{
  "name" : "1704.04866.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Effective Warm Start for the Online Actor-Critic Reinforcement Learning based mHealth Intervention",
    "authors" : [ "Feiyun Zhu", "Peng Liao" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 4.\n04 86\n6v 3\n[ cs\nOnline reinforcement learning (RL) is increasingly popular for the personalized mobile health (mHealth)\nintervention. It is able to personalize the type and dose of interventions according to user’s ongoing statuses and changing needs. However, at the beginning of online learning, there are usually too few samples to support the RL updating, which leads to poor performances. A delay in good performance of the online learning algorithms can be especially detrimental in the mHealth, where users tend to quickly disengage with the mHealth app. To address this problem, we propose a new online RL methodology that focuses on an effective warm start. The main idea is to make full use of the data accumulated and the decision rule achieved in a former study. As a result, we can greatly enrich the data size at the beginning of online learning in our method. Such case accelerates the online learning process for new users to achieve good performances not only at the beginning of online learning but also through the whole online learning process. Besides, we use the decision rules achieved in a previous study to initialize the parameter in our online RL model for new users. It provides a good initialization for the proposed online RL algorithm. Experiment results show that promising improvements have been achieved by our method compared with the state-of-the-art method.\nIndex Terms\nMobile Health (mHealth), Online learning, Reinforcement Learning (RL), Warm Start, Actor-Critic\n2 I. INTRODUCTION\nW ITH billions of smart device (i.e., smart-phones and wearable devices) users worldwide, mobile health (mHealth) interventions (MHI) are increasingly popular among the behavioral health, clinical, computer science and statistic communities [1], [2], [3], [4]. The MHI aims to\nmake full use of smart technologies to collect, transport and analyze the raw data (weather, location, social activity, stress, urges to smoke, etc.) to deliver effective treatments that target behavior regularization [2]. For example, the goal of MHI is to optimally prevent unhealthy behaviors, such as alcohol abuse and eating disorders, and to promote healthy behaviors. Particularly, JITAIs\n(i.e., Just in time adaptive intervention) is especially interesting and practical due to the appealing properties [1]: (1) JITAIs could make adaptive and efficacious interventions according to user’s ongoing statuses and changing needs; (2) JITAIs allow for the real-time delivery of interventions, which is very portable, affordable and flexible [5]. Therefore, JITAIs are widely used in a wide\nrange of mHealth applications, such as physical activity, eating disorders, alcohol use, mental illness, obesity/weight management and other chronic disorders etc., that aims to guide people to lead healthy lives [4], [2], [6], [3], [7].\nNormally, JITAIs is formed as an online sequential decision making (SDM) problem that is\naimed to construct the optimal decision rules to decide when, where and how to deliver effective treatments [4], [2], [5]. This is a brand-new topic that lacks of methodological guidance. In 2014, Lei [1] made a first attempt to formulate the mHealth intervention as an online actor-critic contextual bandit problem. Lei’s method is well suited for the small data set problem in the early stage of\nthe mHealth study. However, this method ignores the important delayed effects of the SDM—the current action may affect not only the immediate reward but also the next states and, through that, all subsequent rewards [8]. To consider the delayed effects, it is reasonable to employ the\nreinforcement learning (RL) in the discount reward setting. RL is much more complex than the contextual bandit. It requires much more data to acquire good and stable decision rules [5]. However at the beginning of the online learning, there are too few data to start effective online learning. A simple and widely used method is to collect a fixed length of trajectory (T0 = 10, say) via the micro-randomized trials [4], accumulating a few of samples, then starting the online updating. Such procedure is called the random warm start, i.e. RWS. However, there are two main drawbacks of the RWS: (1) it highly puts off the online RL learning before achieving good decision rules;\n3 (2) it is likely to frustrate the users because the random decision rules achieved at the beginning of online learning are not personalized to the users’ needs. Accordingly, it is easy for users to\nabandon the mHealth app.\nTo alleviate the above problems, we propose a new online RL methodology by emphasizing effective warm starts. It aims to promote the performance of the online learning at the early stage and, through that, the final decision rule. The motivation is to make full use of the data and the\ndecision rules achieved in the previous study, which is similar to the current study (cf. Sec. III). Specifically, we use the decision rules achieved previously to initialize the parameter of the online RL learning for new users. The data accumulated in the former study is also fully used. As a result, the data size is greatly enriched at the beginning of our online learning algorithm. When\nthe online learning goes on, the data gained from new users will have more and more weights to increasingly dominate the objective function. Our decision rule is still personalized according to the new user. Extensive experiment results show the power of the proposed method."
    }, {
      "heading" : "II. MARKOV DECISION PROCESS (MDP) AND ACTOR-CRITIC REINFORCEMENT LEARNING",
      "text" : "MDP: The dynamic system (i.e. the environment) that RL interacts with is generally modeled as a Markov Decision Process (MDP) [8]. An MDP is a tuple {S,A, P, R, γ} [9], [10], [11], where S is (finite) state space and A is (finite) action space. The state transition probability P : S×A×S 7→ [0, 1], from state s to the next state s′ when taking action a, is given by P (s, a, s′). Let St, At and Rt+1 be the random variables at time t representing the state, action and immediate reward respectively. The expected immediate reward R (s, a) = E (Rt+1 | St = s, At = a) is assumed to be bounded over the state and action spaces [2]. γ ∈ [0, 1) is a discount factor to reduce the influence\nof future rewards.\nThe stochastic policy π (· | s) decides the action to take in a given state s. The goal of RL is to interact with the environment to learn the optimal policy π∗ that maximizes the total accumulated reward. Usually, RL uses the value function Qπ (s, a) ∈ R|S|×|A| to quantify the quality of a policy π, which is the expected discounted cumulative reward, starting from state s, first choosing action a and then following the policy π: Qπ (s, a) = E { ∑∞\nt=0 γ tR (st, at) | s0 = s, a0 = a, π} . The value\nQπ (s, a) satisfies the following linear Bellman equation\nQπ (s, a) = Es′,a′|s,a,π {R (s, a) + γQ π (s′, a′)} (1)\n4 The parameterized functions are generally employed to approximate the value and policy functions since [9] the system usually have too many states and actions to achieve an accurate estimation of\nvalue and policy. Instead they have to be iteratively estimated. Due to the great properties of quick convergences [10], the actor-critic RL algorithms are widely accepted to esimate the parameterized value Q w (s, a) = wTx (s, a) ≈ Qπ and stochastic policy πθ (· | s) ≈ π ∗ (· | s), where x (s, a) is a feature function for the Q-value that merges the information in state s and action a. To learn the unknown parameters {w, θ}, we need a 2-step alternating updating rule until convergence: (1) the critic updating (i.e., policy evaluation) for w to estimate the Q-value function for the current policy, (2) the actor updating (i.e., policy improvement) for θ to search a better policy based on the newly estimated Q-value [10], [4].\nSupposing the online learning for a new user is at decision point t, resulting in t tuples drawn from the MDP system, i.e., D = {(si, ai, ri, s ′ i) | i = 1, · · · , t}. Each tuple consists of four elements: the current state, action, reward and the next state. By using the data in D, the Least-Squares Temporal Difference for Q-value (LSTDQ) [11], [8] is used for the critic updating to estimate ŵt at time point t:\nŵt = [ ζcI+ 1\nt\nt∑\ni=1\nxi (xi − γyi+1) ⊺\n]−1( 1\nt\nt∑\ni=1\nxiri\n) , (2)\nwhere xi = x (si, ai) is the feature at decision point i for the value function;\nyi+1 = ∑\na∈A\nx (si+1, a)πθ̂t (a | si+1)\nis the feature at the next time point; ri is the immediate reward at the i th time point. By maximizing the average reward, i.e., a widely accepted criterion [10], we have the objective function for the actor updating (i.e., policy improvement)\nθ̂t = argmax θ\n1 t\nt∑\ni=1\n∑\na∈A\nQ (si, a; ŵt) πθ (a|si)− ζa 2 ‖θ‖2 2 (3)\nwhere Q (si, a; ŵt) = x (si, a) ⊺ ŵt is the newly estimated value; ζc and ζa are the balancing parameters for the ℓ2 constraint to avoid singular failures for the critic and actor update respectively. Note that after each actor update, the feature at the next time point yi+1 has to be re-calculated based on the newly estimated policy parameter θ̂t. When the discount factor γ = 0, the RL algorithm in (4), (5) is equivalent to the state-of-the-art contextual bandit method in the mHealth [1].\n5"
    }, {
      "heading" : "III. OUR METHOD",
      "text" : "The actor-critic RL algorithm in (4), (5) works well when the sample size (i.e. t) is large. However at the beginning of the online learning, e.g., t = 1, there is only one tuple. It is impossible to do the actor-critic updating with so few samples. A popular and widely accepted method is to\naccumulate a small number of tuples via the micro-randomized trials [4] (called RWS). RWS is to draw a fixed length of trajectory (T0 = 10, say) by applying the random policy with probability 0.5 to provide an intervention (i.e., µ (1 | s) = 0.5 for all states s). RWS works to some extent, they are far from the optimal. One direct drawback with RWS is that it is very expensive in time\nto wait the micro-randomized trials to collect data from human, implying that we may still have a small number of samples to start the actor-critic updating. This tough problem badly affects the actor-critic updating not only at the beginning of online learning, but also along the whole learning process. Such case is due to the actor-critic objective functions is non-convex; any bad\nsolution at the early online learning would bias the optimization direction, which easily leads some sub-optimal solution. Besides, the random policy in micro-randomized trials and the decision rules achieved at the early online learning is of bad user experience. Such problem makes it possible for the users to be inactive with or even to abandon the mHealth intervention.\nTo deal with the above problems, we propose a new online actor-critic RL methodology. It emphasizes effective warm starts for the online learning algorithm. The goal is to promote decision rules achieved at the early online learning stage and, through that, guide the optimization in a better direction, leading to a good final policy that is well suited for the new user. Specifically, we make\nfull use of the data accumulated and decision rules learned in the previous study. Note that for the mHealth intervention design, there are usually several rounds of study; each round is pushed forward and slightly different from the former one. By using the data and policy gained in the\nformer study, the RL learning in current study could quickly achieve good decision rules for new users, reducing the total study time and increasing the user experience at the beginning of the online learning.\nSupposing that the former mHealth study is carried out in an off-policy, batch learning setting, we have N̄ (40, say) individuals. Each individual is with a trajectory including T̄ = 42 tuples of states, actions and rewards. Thus in total there are NT = N̄ × T̄ tuples, i.e., D̄ = {(s̄i, āi, r̄i, s̄ ′ i) | i = 1, · · · , NT}. Besides the data in D̄, we employ the decision rule achieved in\n6 the former study to initialize the parameters in the current online learning. Note that we add a bar above the notations to distinguish the data obtained in the previous study from that of the current\nstudy.\nAt the tth decision point, we have both the data D̄ collected in the former study and the t new tuples drawn from the new user in D to update the online actor-critic learning. It has two parts: (1) the critic updating for ŵt via\nŵt = { ζcI+ 1\nt + 1\n[ 1\nNT\nNT∑\nj=1\nx̄j (x̄j − γȳi+1) ⊺ +\nt∑\ni=1\nxi (xi − γyi+1) ⊺ ]}−1 (4)\n[ 1\nt+ 1\n( 1\nNT\nNT∑\nj=1\nx̄j r̄j +\nt∑\ni=1\nxiri\n)]\nand (2) the actor updating via\nθ̂t = argmax θ\n1\nt + 1\n{ 1\nNT\nNT∑\nj=1\n∑\na∈A\nQ (s̄j, a; ŵt)πθ (a|s̄j) + t∑\ni=1\n∑\na∈A\nQ (si, a; ŵt) · π (a|si)\n} −\nζa 2 ‖θ‖2 2 ,\n(5)\nwhere {x̄j} NT\nj=1 is data in the previous study; (xi)\nt i=1 is the data that is collected from the new\nuser; x̄i = x̄ (si, ai) is the feature vector at decision point i for the value function; ȳi+1 = ∑\na∈A x (s̄i+1, a) πθ̂t (a | s̄i+1) is the feature at the next time point; r̄i is the immediate reward\nat the ith point; Q (s̄i, a; ŵt) = x (s̄i, a) ⊺ ŵt is the newly updated value.\nIn (4) and (5), the terms in the blue ink indicate the the previous data, which is with a normalized\nweight 1 NT . In this setting, all the data obtained in the former study is treated as one sample for the current online learning. When current online learning goes on (i.e., t increases), the data collected from the new user gradually dominates the objective functions. Thus, we are still able to achieve\npersonalized JITAIs that is successfully adapted to each new user."
    }, {
      "heading" : "IV. EXPERIMENTS",
      "text" : "To verify the performance, we compare our method (i.e., NWS-RL) with the conventional RL method with the random warm start (RWS-RL) on the HeartSteps application [3]. The HeartSteps\nis a 42-day mHealth intervention that encourages users to increase the steps they take each day by providing positive interventions, such as suggesting taking a walk after sedentary behavior. The\n7 actions are binary including {0, 1}, where a = 1 means providing active treatments, e.g., sending an intervention to the user’s smart device, while a = 0 means no treatment [2]."
    }, {
      "heading" : "A. Simulated Experiments",
      "text" : "In the experiments, we draw T tuples from each user, i.e.,\nDT = {(S0, A0, R0) , (S1, A1, R1) , · · · , (ST , AT , RT )}\n, where the observation St is a column vector with p elements . The initial states and actions are gen-\nerated by S0 ∼ Normalp {0,Σ} and A0 = 0, where Σ =\n  Σ1 0\n0 Ip−3\n  and Σ1 =   1 0.3 −0.3 0.3 1 −0.3\n−0.3 −0.3 1\n .\nFor t ≥ 1, we have the state generation model and immediate reward model as follows\nSt,1 = β1St−1,1 + ξt,1,\nSt,2 = β2St−1,2 + β3At−1 + ξt,2, (6)\nSt,3 = β4St−1,3 + β5St−1,3At−1 + β6At−1 + ξt,3,\nSt,j = β7St−1,j + ξt,j , for j = 4, . . . , p\nRt = β14 × [β8 + At × (β9 + β10St,1 + β11St,2) + β12St,1 − β13St,3 + ̺t] , (7)\nwhere −β13Ot,3 is the treatment fatigue [4], [2]; {ξt,i} p i=1 ∼ Normal (0, σ2s) at the t th point is the noise in the state transition (6) and ̺t ∼ Normal (0, σ 2 r) is the noise in the immediate reward model (7). To generate N different users, we need N different MDPs specified by the value of βs in (6) and (7). The βs are generated in the following two steps: (a) set a basic βbasic = [0.40, 0.25, 0.35, 0.65, 0.10, 0.50, 0.22, 2.00, 0.15, 0.20, 0.32, 0.10, 0.45, 1.50, 800]; (b) to obtain N different βs (i.e., users or MDPs), we set the {βi} N i=1 as βi = βbasic + δi, for i ∈ {1, 2, · · · , N} , where δi ∼ Normal (0, σbI14), σb controls how different the users are and I14 is an identity matrix with 14×14 elements. To generate the MDP for a future user, we will also use this kind of method to generate new βs."
    }, {
      "heading" : "B. Experiment Settings",
      "text" : "The expectation of long run average reward (ElrAR) E [ηπθ̂ ] is used to evaluate the quality of an estimated policy π θ̂ on a set of N=50 individuals. Intuitively, the ElrAR measures how much\n8 average reward in the long run we could totally get by using the learned policy π θ̂ for a number of users. In the HeartSteps application, the ElrAR measures the average steps that users take each\nday in a long period of time; a larger ElrAR corresponds to a better performance. The average reward ηπθ̂ is calculated by averaging the rewards over the last 4, 000 elements in a trajectory of 5, 000 tuples under the policy π θ̂ . Then ElrAR E [ηπθ̂ ] is achieved by averaging the 50 ηπθ̂’s.\nIn the experiment, we assume the parameterized policy in the form\nπθ (a | s) = exp [aθ ⊺φ (s)] / {1 + exp [θ⊺φ (s)]}\n, where θ ∈ Rq is the unknown variance and φ (s) = [1, s⊺]⊺ ∈ Rq is the feature function for policies that stacks constant 1 with the state vector s. The number of individuals in the former study is N̄ = 40. Each is with a trajectory of T̄=42 time points. For the current study (cf., Table I), there are N = 50 individuals. RWS has to accumulate tuples till T0 = 5 and 10 respectively to start the online learning. Our method (i.e., NWS) has the ability to start the RL online learning algorithm immediately when the 1st tuple is available. Since the comparison of early online learning is our focuses, we set the total trajectory length for the online learning as T = 30 and T = 50, respectively. The noises are set σs = σr = 1 and σβ = 0.005. Other variances are p = 3, q = 4, ζa = ζc = 10 −5. The feature processing for the value estimation is x (s, a) = [1, s⊺, a, s⊺a]⊺ ∈ R2p+2 for all the\ncompared methods. Table I summarizes the experiment results of three methods: RWS-RLT0=5, RWS-RLT0=10 and NWS-RLT0=1. It includes two sub-tables: the left one shows the results of early online learning results, i.e., T = 30 and the right displays the results when T = 50. As we shall see, the proposed warm start method (NWS-RL) has an obvious advantage over the conventional RWS-RL, averagely achieving an improvement of 67.57 steps for T = 30 and 69.72 steps for T = 50 compared with the 2nd best policy in blue.\nV. CONCLUSION AND Discussion\nIn this paper, we propose a new online actor-critic reinforcement learning methodology for the mHealth application. The main idea is to provide an effective warm start method for the online RL learning. The state-of-the-art RL method for mHealth has the problem of lacking samples to\nstart the online learning. To solve this problem, we make full use of the data accumulated and decision rules achieved in the former study. As a result, the data size is greatly enriched even at the beginning of online learning. Our method is able to start the online updating when the first\n9\ntuple is available. Experiment results verify that our method achieves clear gains compared with the state-of-the-art method. In the future, we may explore the robust learning [12], [13] and graph learning [14], [15] on the online actor-critic RL learning algorithm."
    }, {
      "heading" : "ACKNOWLEDGEMENTS",
      "text" : "The authors would like to thank the editor and the reviewers for their valuable suggestions. Besides, this work is supported by R01 AA023187, P50 DA039838, U54EB020404, R01 HL125440."
    } ],
    "references" : [ {
      "title" : "An actor-critic contextual bandit algorithm for personalized interventions using mobile devices",
      "author" : [ "H. Lei", "A. Tewari", "S. Murphy" ],
      "venue" : "NIPS 2014 Workshop: Personalization: Methods and Applications, pp. 1 – 9, 2014.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A batch, off-policy, actor-critic algorithm for optimizing the average reward",
      "author" : [ "S.A. Murphy", "Y. Deng", "E.B. Laber", "H.R. Maei", "R.S. Sutton", "K. Witkiewitz" ],
      "venue" : "CoRR, vol. abs/1607.05047, 2016.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Randomised trials for the fitbit generation",
      "author" : [ "W. Dempsey", "P. Liao", "P. Klasnja", "I. Nahum-Shani", "S.A. Murphy" ],
      "venue" : "Significance, vol. 12, pp. 20 – 23, Dec 2016.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Constructing just-in-time adaptive interventions",
      "author" : [ "P. Liao", "A. Tewari", "S. Murphy" ],
      "venue" : "Phd Section Proposal, pp. 1–49, 2015.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Cohesion-based online actor-critic reinforcement learning for mhealth intervention",
      "author" : [ "F. Zhu", "P. Liao", "X. Zhu", "Y. Yao", "J. Huang" ],
      "venue" : "arXiv:1703.10039, 2017.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "An Online Actor Critic Algorithm and a Statistical Decision Procedure for Personalizing Intervention",
      "author" : [ "H. Lei" ],
      "venue" : "PhD thesis, University of Michigan,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2016
    }, {
      "title" : "A smartphone application to support recovery from alcoholism: a randomized clinical trial",
      "author" : [ "D. Gustafson", "F. McTavish", "M. Chih", "A. Atwood", "R. Johnson", "M.B. ...", "D. Shah" ],
      "venue" : "JAMA Psychiatry, vol. 71, no. 5, 2014.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Reinforcement Learning: An Introduction",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2012
    }, {
      "title" : "Algorithmic survey of parametric value function approximation",
      "author" : [ "M. Geist", "O. Pietquin" ],
      "venue" : "IEEE Transactions on Neural Networks and Learning Systems, vol. 24, no. 6, pp. 845–867, 2013.  10",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A survey of actor-critic reinforcement learning: Standard and natural policy gradients",
      "author" : [ "I. Grondman", "L. Busoniu", "G.A.D. Lopes", "R. Babuska" ],
      "venue" : "IEEE Trans. Systems, Man, and Cybernetics, vol. 42, no. 6, pp. 1291–1307, 2012.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Least-squares policy iteration",
      "author" : [ "M.G. Lagoudakis", "R. Parr" ],
      "venue" : "J. of Machine Learning Research (JLMR), vol. 4, pp. 1107– 1149, 2003.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "10,000+ times accelerated robust subset selection (ARSS)",
      "author" : [ "F. Zhu", "B. Fan", "X. Zhu", "Y. Wang", "S. Xiang", "C. Pan" ],
      "venue" : "Proc. Assoc. Adv. Artif. Intell. (AAAI), pp. 3217–3224, 2015.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Robust hyperspectral unmixing with correntropy-based metric",
      "author" : [ "Y. Wang", "C. Pan", "S. Xiang", "F. Zhu" ],
      "venue" : "IEEE Transactions on Image Processing, vol. 24, no. 11, pp. 4027–4040, 2015.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Structured sparse method for hyperspectral unmixing",
      "author" : [ "F. Zhu", "Y. Wang", "S. Xiang", "B. Fan", "C. Pan" ],
      "venue" : "ISPRS Journal of Photogrammetry and Remote Sensing, vol. 88, pp. 101–118, 2014.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A label propagation method using spatial-spectral consistency for hyperspectral image classification",
      "author" : [ "H. Li", "Y. Wang", "S. Xiang", "J. Duan", "F. Zhu", "C. Pan" ],
      "venue" : "International Journal of Remote Sensing, vol. 37, no. 1, pp. 191–211, 2016.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : ", smart-phones and wearable devices) users worldwide, mobile health (mHealth) interventions (MHI) are increasingly popular among the behavioral health, clinical, computer science and statistic communities [1], [2], [3], [4].",
      "startOffset" : 205,
      "endOffset" : 208
    }, {
      "referenceID" : 1,
      "context" : ", smart-phones and wearable devices) users worldwide, mobile health (mHealth) interventions (MHI) are increasingly popular among the behavioral health, clinical, computer science and statistic communities [1], [2], [3], [4].",
      "startOffset" : 210,
      "endOffset" : 213
    }, {
      "referenceID" : 2,
      "context" : ", smart-phones and wearable devices) users worldwide, mobile health (mHealth) interventions (MHI) are increasingly popular among the behavioral health, clinical, computer science and statistic communities [1], [2], [3], [4].",
      "startOffset" : 215,
      "endOffset" : 218
    }, {
      "referenceID" : 3,
      "context" : ", smart-phones and wearable devices) users worldwide, mobile health (mHealth) interventions (MHI) are increasingly popular among the behavioral health, clinical, computer science and statistic communities [1], [2], [3], [4].",
      "startOffset" : 220,
      "endOffset" : 223
    }, {
      "referenceID" : 1,
      "context" : ") to deliver effective treatments that target behavior regularization [2].",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 0,
      "context" : ", Just in time adaptive intervention) is especially interesting and practical due to the appealing properties [1]: (1) JITAIs could make adaptive and efficacious interventions according to user’s ongoing statuses and changing needs; (2) JITAIs allow for the real-time delivery of interventions, which is very portable, affordable and flexible [5].",
      "startOffset" : 110,
      "endOffset" : 113
    }, {
      "referenceID" : 4,
      "context" : ", Just in time adaptive intervention) is especially interesting and practical due to the appealing properties [1]: (1) JITAIs could make adaptive and efficacious interventions according to user’s ongoing statuses and changing needs; (2) JITAIs allow for the real-time delivery of interventions, which is very portable, affordable and flexible [5].",
      "startOffset" : 343,
      "endOffset" : 346
    }, {
      "referenceID" : 3,
      "context" : ", that aims to guide people to lead healthy lives [4], [2], [6], [3], [7].",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 1,
      "context" : ", that aims to guide people to lead healthy lives [4], [2], [6], [3], [7].",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 5,
      "context" : ", that aims to guide people to lead healthy lives [4], [2], [6], [3], [7].",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 2,
      "context" : ", that aims to guide people to lead healthy lives [4], [2], [6], [3], [7].",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 6,
      "context" : ", that aims to guide people to lead healthy lives [4], [2], [6], [3], [7].",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 3,
      "context" : "Normally, JITAIs is formed as an online sequential decision making (SDM) problem that is aimed to construct the optimal decision rules to decide when, where and how to deliver effective treatments [4], [2], [5].",
      "startOffset" : 197,
      "endOffset" : 200
    }, {
      "referenceID" : 1,
      "context" : "Normally, JITAIs is formed as an online sequential decision making (SDM) problem that is aimed to construct the optimal decision rules to decide when, where and how to deliver effective treatments [4], [2], [5].",
      "startOffset" : 202,
      "endOffset" : 205
    }, {
      "referenceID" : 4,
      "context" : "Normally, JITAIs is formed as an online sequential decision making (SDM) problem that is aimed to construct the optimal decision rules to decide when, where and how to deliver effective treatments [4], [2], [5].",
      "startOffset" : 207,
      "endOffset" : 210
    }, {
      "referenceID" : 0,
      "context" : "In 2014, Lei [1] made a first attempt to formulate the mHealth intervention as an online actor-critic contextual bandit problem.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 7,
      "context" : "However, this method ignores the important delayed effects of the SDM—the current action may affect not only the immediate reward but also the next states and, through that, all subsequent rewards [8].",
      "startOffset" : 197,
      "endOffset" : 200
    }, {
      "referenceID" : 4,
      "context" : "It requires much more data to acquire good and stable decision rules [5].",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 3,
      "context" : "A simple and widely used method is to collect a fixed length of trajectory (T0 = 10, say) via the micro-randomized trials [4], accumulating a few of samples, then starting the online updating.",
      "startOffset" : 122,
      "endOffset" : 125
    }, {
      "referenceID" : 7,
      "context" : "the environment) that RL interacts with is generally modeled as a Markov Decision Process (MDP) [8].",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 8,
      "context" : "An MDP is a tuple {S,A, P, R, γ} [9], [10], [11], where S is (finite) state space and A is (finite) action space.",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 9,
      "context" : "An MDP is a tuple {S,A, P, R, γ} [9], [10], [11], where S is (finite) state space and A is (finite) action space.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 10,
      "context" : "An MDP is a tuple {S,A, P, R, γ} [9], [10], [11], where S is (finite) state space and A is (finite) action space.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 0,
      "context" : "The state transition probability P : S×A×S 7→ [0, 1], from state s to the next state s when taking action a, is given by P (s, a, s).",
      "startOffset" : 46,
      "endOffset" : 52
    }, {
      "referenceID" : 1,
      "context" : "The expected immediate reward R (s, a) = E (Rt+1 | St = s, At = a) is assumed to be bounded over the state and action spaces [2].",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 8,
      "context" : "The parameterized functions are generally employed to approximate the value and policy functions since [9] the system usually have too many states and actions to achieve an accurate estimation of value and policy.",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 9,
      "context" : "Due to the great properties of quick convergences [10], the actor-critic RL algorithms are widely accepted to esimate the parameterized value Q w (s, a) = wx (s, a) ≈ Q and stochastic policy πθ (· | s) ≈ π ∗ (· | s), where x (s, a) is a feature function for the Q-value that merges the information in state s and action a.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 9,
      "context" : ", policy improvement) for θ to search a better policy based on the newly estimated Q-value [10], [4].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 3,
      "context" : ", policy improvement) for θ to search a better policy based on the newly estimated Q-value [10], [4].",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 10,
      "context" : "By using the data in D, the Least-Squares Temporal Difference for Q-value (LSTDQ) [11], [8] is used for the critic updating to estimate ŵt at time point t: ŵt = [ ζcI+ 1 t t ∑",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 7,
      "context" : "By using the data in D, the Least-Squares Temporal Difference for Q-value (LSTDQ) [11], [8] is used for the critic updating to estimate ŵt at time point t: ŵt = [ ζcI+ 1 t t ∑",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 9,
      "context" : ", a widely accepted criterion [10], we have the objective function for the actor updating (i.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 0,
      "context" : "When the discount factor γ = 0, the RL algorithm in (4), (5) is equivalent to the state-of-the-art contextual bandit method in the mHealth [1].",
      "startOffset" : 139,
      "endOffset" : 142
    }, {
      "referenceID" : 3,
      "context" : "A popular and widely accepted method is to accumulate a small number of tuples via the micro-randomized trials [4] (called RWS).",
      "startOffset" : 111,
      "endOffset" : 114
    }, {
      "referenceID" : 2,
      "context" : ", NWS-RL) with the conventional RL method with the random warm start (RWS-RL) on the HeartSteps application [3].",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 1,
      "context" : ", sending an intervention to the user’s smart device, while a = 0 means no treatment [2].",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 3,
      "context" : "where −β13Ot,3 is the treatment fatigue [4], [2]; {ξt,i} p i=1 ∼ Normal (0, σ s) at the t th point is the noise in the state transition (6) and ̺t ∼ Normal (0, σ 2 r) is the noise in the immediate reward model (7).",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 1,
      "context" : "where −β13Ot,3 is the treatment fatigue [4], [2]; {ξt,i} p i=1 ∼ Normal (0, σ s) at the t th point is the noise in the state transition (6) and ̺t ∼ Normal (0, σ 2 r) is the noise in the immediate reward model (7).",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 0,
      "context" : "The value of γ specifies different RL methods: (a) γ = 0 means the contextual bandit [1], (b) 0 < γ < 1 indicates the discounted reward RL.",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 11,
      "context" : "In the future, we may explore the robust learning [12], [13] and graph learning [14], [15] on the online actor-critic RL learning algorithm.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 12,
      "context" : "In the future, we may explore the robust learning [12], [13] and graph learning [14], [15] on the online actor-critic RL learning algorithm.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 13,
      "context" : "In the future, we may explore the robust learning [12], [13] and graph learning [14], [15] on the online actor-critic RL learning algorithm.",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 14,
      "context" : "In the future, we may explore the robust learning [12], [13] and graph learning [14], [15] on the online actor-critic RL learning algorithm.",
      "startOffset" : 86,
      "endOffset" : 90
    } ],
    "year" : 2017,
    "abstractText" : "Online reinforcement learning (RL) is increasingly popular for the personalized mobile health (mHealth) intervention. It is able to personalize the type and dose of interventions according to user’s ongoing statuses and changing needs. However, at the beginning of online learning, there are usually too few samples to support the RL updating, which leads to poor performances. A delay in good performance of the online learning algorithms can be especially detrimental in the mHealth, where users tend to quickly disengage with the mHealth app. To address this problem, we propose a new online RL methodology that focuses on an effective warm start. The main idea is to make full use of the data accumulated and the decision rule achieved in a former study. As a result, we can greatly enrich the data size at the beginning of online learning in our method. Such case accelerates the online learning process for new users to achieve good performances not only at the beginning of online learning but also through the whole online learning process. Besides, we use the decision rules achieved in a previous study to initialize the parameter in our online RL model for new users. It provides a good initialization for the proposed online RL algorithm. Experiment results show that promising improvements have been achieved by our method compared with the state-of-the-art method.",
    "creator" : "LaTeX with hyperref package"
  }
}