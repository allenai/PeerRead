{
  "name" : "1704.05051.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Google’s Cloud Vision API Is Not Robust To Noise",
    "authors" : [ "Hossein Hosseini", "Baicen Xiao", "Radha Poovendran" ],
    "emails" : [ "rp3}@uw.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "In this paper, we evaluate the robustness of the Google’s Cloud Vision API to input perturbation. In particular, we show that by adding sufficient noise to the image, the API generates completely different outputs for the noisy image, while a human observer would perceive its original content. We show that the attack is consistently successful, by performing extensive experiments on different image types, including natural images, images containing faces and images with texts. Our findings indicate the vulnerability of the API in adversarial environments. For example, an adversary can bypass an image filtering system by adding noise to an image with inappropriate content.\nI. INTRODUCTION\nIn recent years, Machine Learning (ML) techniques have been extensively deployed for computer vision tasks, particularly visual classification problems, where new algorithms reported to achieve or even surpass the human performance [1]– [3]. Success of ML algorithms has led to an explosion in demand. To further broaden and simplify the use of ML algorithms, cloud-based services offered by Amazon, Google, Microsoft, BigML, and others have developed ML-as-a-service tools. Thus, users and companies can readily benefit from ML applications without having to train or host their own models.\nRecently, Google introduced the Cloud Vision API for image analysis [4]. A demonstration website has been also launched, where for any selected image, the API outputs the image labels, identifies and reads the texts contained in the image and detects the faces within the image. It also determines how likely is that the image contains inappropriate contents, including adult, spoof, medical, or violence contents.\nThe implicit assumption in designing and developing ML models is that they will be deployed in benign settings. Recent research however have pointed out their vulnerability in adversarial environments [5]–[7]. Security evaluation of ML systems is an emerging field of study. Several papers have presented attacks on various ML systems, such as voice interfaces [8], face-recognition systems [9], toxic comment detectors [10], and video annotation systems [11].\nIn this paper, we evaluate the robustness of the Google’s Cloud Vision API in adversarial environments. In particular, we investigate whether we can modify an image in such a\nThis work was supported by ONR grants N00014-14-1-0029 and N0001416-1-2710, ARO grant W911NF-16-1-0485 and NSF grant CNS-1446866.\nway that a human observer would perceive its original content, but the API generates different outputs for it. For modifying the images, we add either impulse noise or Gaussian noise to them. Due to the inherent low-pass filtering characteristic of the humans vision system, humans are capable of perceiving image contents from the noisy image.\nOur experimental results show that by adding sufficient noise to the image, the API is deceived into returning labels which are not related to the original image. Figure 1 illustrates the attack by showing original and noisy images along with the most confident labels returned by the API. We show that the attack is consistently successful, by performing exten-\nar X\niv :1\n70 4.\n05 05\n1v 1\n[ cs\n.C V\n] 1\n6 A\npr 2\n01 7\nsive experiments on different image types, including natural images, images containing faces and images with texts. Our findings indicate the vulnerability of the Google’s cloud vision API in real-world applications. For example, a search engine may suggest irrelevant images to users, or an image filtering system can be bypassed by adding noise to an image with inappropriate content.\nWe then assess the performance of the API when a noise filter is applied before the image analysis algorithms. We show that the API generates mostly the same outputs for restored images as for the original images. This suggests that the cloud vision API can readily benefit from noise filtering, without the need for updating the image recognition algorithms.\nThe rest of this paper is organized as follows. The noise models are presented in Section II. The proposed attack on the Google’s cloud vision API is given in Section III and a discussion about the experimental results is provided in Section IV. Section V describes some countermeasures to the attack and Section VI concludes the paper.\nII. IMAGE NOISE\nA color image x is a three-dimensional array of pixels xi,j,k, where (i, j) is the image coordinate and k ∈ {1, 2, 3} denotes the coordinate in the color space. In this paper, we encode the images in the RGB color space. Most image file formats use 24 bits per pixel (8 bits per color channel), which results in 256 different colors for each color space. Therefore, the minimum and maximum values of each pixel are 0 and 255, respectively, which correspond to the darkest and brightest colors.\nFor modifying the images, we add either impulse noise or Gaussian noise to them. These noise types often occur during image acquisition and transmission [12]. Impulse Noise, also known as Salt-and-Pepper Noise, is commonly modeled by [13]:\nx̃i,j,k =  0 with probability p2\nxi,j,k with probability 1− p 255 with probability p2\nwhere x, x̃ and p are the original and noisy images and the noise density, respectively. Impulse noise can be removed using spatial filters which exploit the correlation of adjacent pixels. We use the weighted-average filtering method, proposed in [13], for restoring the images corrupted by impulse noise.\nA noisy image corrupted by Gaussian noise is obtained as x̂i,j,k = xi,j,k + z, where z is a zero-mean Gaussian random variable. The pixel values of the noisy image should be clipped, so that they remain in the range of 0 to 255. Gaussian noise can be reduced by filtering the input with a low-pass kernel [12].\nFor assessing the quality of the restored image x∗ compared to the original image x, we use the Peak Signal-to-Noise Ratio (PSNR). For images of size d1×d2×3, the PSNR is computed as follows [14]:\nPSNR = 10 · log10\n( 2552\n1 3 d1 d2 ∑ i,j,k(xi,j,k − x∗i,j,k)2\n) .\nPSNR value is measured in dB. Typical values for the PSNR are usually considered to be between 20 and 40 dB, where higher is better [15]."
    }, {
      "heading" : "III. THE PROPOSED ATTACK ON CLOUD VISION API",
      "text" : "In this section, we describe the attack on the Google’s Cloud Vision API. The goal of the attack is to modify a given image in such a way that the API returns completely different outputs than the ones for the original image, while a human observer would perceive its original content. We perform the experiments on different image types, including natural images from the ImageNet dataset [16], images containing faces from the Faces94 dataset [17], and images with text. When selecting an image for analysis, the API outputs the image labels, detects the faces within the image, and identifies and reads the texts contained in the image.\nThe attack procedure is as follows. We first test the API with the original image and record the outputs. We then test the API with a modified image, generated by adding very low-density impulse noise. If we can force the API to output completely different labels, or to fail to detect faces or identify the texts within the image, we declare the noisy image as the adversary’s image. Otherwise, we increase the noise density and retry the attack. We continue to increase the noise density until we can successfully force the API to output wrong labels. In experiments, we start the attack with 5% impulse noise and increase the noise density each time by 5%. 1\nFigure 1 shows the API’s output label with the highest confidence score, for the original and noisy images. As can be seen, unlike the original images, the API wrongly labels the noisy images, despite that the objects in noisy images are easily recognizable. Trying on 100 images of the ImageNet dataset, we needed on average 14.25% impulse noise density to deceive the Google’s cloud vision API. Figure 2 shows the adversary’s success rate versus the noise density. As can be seen, by adding 35% impulse noise, the attack always succeeded on the samples from the ImageNet dataset.\nFigure 3 shows sample images from the Faces94 dataset and the corresponding noisy images. Unlike the original images, the API fails to detect the face in the noisy ones. Trying on the first 20 images of each female and male categories, we needed on average 23.8% impulse noise density to deceive the Google’s cloud vision API. Similarly, figure 4 shows an image with text and the corresponding noisy image. The API correctly reads the text within the original image, but fails to identify any texts in the noisy one, despite that the text within the noisy image is easily readable.\nWe also tested the API with images corrupted by Gaussian noise. The results are similar to the case of impulse noise. That is, by adding zero-mean Gaussian noise with sufficient variance, we can always force the API to generate a different output than the one for the original image, while a human observer would perceive its original content.\n1The experiments are performed on the interface of the Cloud Vision API’s website on Apr. 7, 2017."
    }, {
      "heading" : "IV. DISCUSSION",
      "text" : "Our experimental results indicate that Google’s cloud vision API is vulnerable to random input perturbation. Specifically, we showed that by adding noise, an adversary can deceive the API to output completely irrelevant labels, or to fail to detect faces or identify texts within the image. Such vulnerability undermines the applicability of the API in real-world applications. For example, a search engine based on the API may wrongly suggest a noisy image to users who searched for a different content. Furthermore, an adversary can easily bypass an image filtering system, by adding noise to an image with inappropriate content.\nThe noisy images used in our attack can be viewed as a form of adversarial examples [18]. An adversarial example is defined as a modified input, which causes the classifier to output a different label, while a human observer would recognize its original content. Note that we could deceive the could vision API without having any knowledge about the learning algorithm. Also, unlike the existing black-box attacks on learning systems [19], [20], we have no information about the training data or even the set of output labels of the model. Moreover, unlike the current methods for generating adversarial examples [21], we perturb the input completely randomly, which results in a more serious attack vector in real-world applications."
    }, {
      "heading" : "V. COUNTERMEASURES",
      "text" : "The success of our attack indicates the importance of designing the learning system to be robust to input perturbations. It has been shown that the robustness of ML algorithms can be improved by using regularization or data augmentation during training [22]. In [23], the authors proposed adversarial training, which iteratively creates a supply of adversarial examples and includes them into the training data. Approaches based on robust optimization however may not be practical, since the model needs to be retrained.\nFor image recognition algorithms, a more viable approach is preprocessing the inputs. Natural images have special\nproperties, such as high correlation among adjacent pixels, sparsity in transform domain or having low energy in high frequencies [12]. Noisy inputs typically do not lie in the same space as natural images. Therefore, by projecting the input image down to the space of the natural images, which is often done by passing the image through a filter, we can reverse the effect of the noise or adversarial perturbation.\nWe assess the performance of the cloud vision API when a noise filter is applied before the image analysis algorithms. For this, we test the API with the restored images. Figure 5 shows the screenshots of the API’s output labels for original, noisy and restored images of a sample image from the ImageNet dataset. As can be seen, none of the labels returned for the noisy image are related to the labels of the original image. However, the labels of the restored image are mostly the same as the ones for the original image. Note that the restored image is almost indistinguishable from the original images.\nSimilarly, figure 6 shows the restored images of the images with faces from figure 3 and the image with text from figure 4. Unlike the noisy images, the API correctly detects the same face attributes for restored face images as the original images, and can read the text within the restored text image.\nWe did the experiments on all the sample images from the ImageNet and Faces94 datasets, corrupted by either impulse or Gaussian noise. Restored images are generated by applying the weighted-average filter [13] for impulse noise and a lowpass filter for Gaussian noise. In all cases, when testing on the restored image, the API generates mostly the same output as for the original image. This suggests that the cloud vision API can readily benefit from noise filtering prior to applying image analysis algorithms."
    }, {
      "heading" : "VI. CONCLUSION",
      "text" : "In this paper, we showed that the Google’s Cloud Vision API can be easily deceived by an adversary without compromising the system or having any knowledge about the specific details of the algorithms used. In essence, we found that by adding noise, we can always force the API to output wrong labels or to fail to detect any face or text within the image. We also showed that when testing with the restored images, the API generates mostly the same outputs as for the original images. This suggests that the system’s robustness can be readily improved by applying a noise filter on the inputs, without the need for updating the image recognition algorithms."
    } ],
    "references" : [ {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "Advances in neural information processing systems, pp. 1097–1105, 2012.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "K. Simonyan", "A. Zisserman" ],
      "venue" : "arXiv preprint arXiv:1409.1556, 2014.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Deeplysupervised nets",
      "author" : [ "C.-Y. Lee", "S. Xie", "P.W. Gallagher", "Z. Zhang", "Z. Tu" ],
      "venue" : "AISTATS, vol. 2, p. 5, 2015.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Adversarial machine learning",
      "author" : [ "L. Huang", "A.D. Joseph", "B. Nelson", "B.I. Rubinstein", "J. Tygar" ],
      "venue" : "Proceedings of the 4th ACM workshop on Security and artificial intelligence, pp. 43–58, ACM, 2011.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "The limitations of deep learning in adversarial settings",
      "author" : [ "N. Papernot", "P. McDaniel", "S. Jha", "M. Fredrikson", "Z.B. Celik", "A. Swami" ],
      "venue" : "Security and Privacy (EuroS&P), 2016 IEEE European Symposium on, pp. 372–387, IEEE, 2016. 4",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Concrete problems in ai safety",
      "author" : [ "D. Amodei", "C. Olah", "J. Steinhardt", "P. Christiano", "J. Schulman", "D. Mané" ],
      "venue" : "arXiv preprint arXiv:1606.06565, 2016.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Hidden voice commands",
      "author" : [ "N. Carlini", "P. Mishra", "T. Vaidya", "Y. Zhang", "M. Sherr", "C. Shields", "D. Wagner", "W. Zhou" ],
      "venue" : "25th USENIX Security Symposium (USENIX Security 16), Austin, TX, 2016.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition",
      "author" : [ "M. Sharif", "S. Bhagavatula", "L. Bauer", "M.K. Reiter" ],
      "venue" : "Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, pp. 1528–1540, ACM, 2016.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Deceiving google’s perspective api built for detecting toxic comments",
      "author" : [ "H. Hosseini", "S. Kannan", "B. Zhang", "R. Poovendran" ],
      "venue" : "arXiv preprint arXiv:1702.08138, 2017.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Deceiving google’s cloud video intelligence api built for summarizing videos",
      "author" : [ "H. Hosseini", "B. Xiao", "R. Poovendran" ],
      "venue" : "arXiv preprint arXiv:1703.09793, 2017.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Handbook of image and video processing",
      "author" : [ "A.C. Bovik" ],
      "venue" : "Academic press,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2010
    }, {
      "title" : "Real-time impulse noise suppression from images using an efficient weighted-average filtering",
      "author" : [ "H. Hosseini", "F. Hessar", "F. Marvasti" ],
      "venue" : "IEEE Signal Processing Letters, vol. 22, no. 8, pp. 1050–1054, 2015.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Salt-and-pepper noise removal by median-type noise detectors and detail-preserving regularization",
      "author" : [ "R.H. Chan", "C.-W. Ho", "M. Nikolova" ],
      "venue" : "IEEE Transactions on image processing, vol. 14, no. 10, pp. 1479–1485, 2005.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Reliable and fast structureoriented video noise estimation",
      "author" : [ "A. Amer", "A. Mitiche", "E. Dubois" ],
      "venue" : "Image Processing. 2002. Proceedings. 2002 International Conference on, vol. 1, pp. I–I, IEEE, 2002.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Imagenet: A large-scale hierarchical image database",
      "author" : [ "J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei" ],
      "venue" : "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pp. 248– 255, IEEE, 2009.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Intriguing properties of neural networks",
      "author" : [ "C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I. Goodfellow", "R. Fergus" ],
      "venue" : "arXiv preprint arXiv:1312.6199, 2013.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Practical black-box attacks against deep learning systems using adversarial examples",
      "author" : [ "N. Papernot", "P. McDaniel", "I. Goodfellow", "S. Jha", "Z.B. Celik", "A. Swami" ],
      "venue" : "arXiv preprint arXiv:1602.02697, 2016.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Blocking transferability of adversarial examples in black-box learning systems",
      "author" : [ "H. Hosseini", "Y. Chen", "S. Kannan", "B. Zhang", "R. Poovendran" ],
      "venue" : "arXiv preprint arXiv:1703.04318, 2017.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Towards evaluating the robustness of neural networks",
      "author" : [ "N. Carlini", "D. Wagner" ],
      "venue" : "arXiv preprint arXiv:1608.04644, 2016.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Understanding adversarial training: Increasing local stability of neural nets through robust optimization",
      "author" : [ "U. Shaham", "Y. Yamada", "S. Negahban" ],
      "venue" : "arXiv preprint arXiv:1511.05432, 2015.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Explaining and harnessing adversarial examples",
      "author" : [ "I.J. Goodfellow", "J. Shlens", "C. Szegedy" ],
      "venue" : "arXiv preprint arXiv:1412.6572, 2014. 5",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "In recent years, Machine Learning (ML) techniques have been extensively deployed for computer vision tasks, particularly visual classification problems, where new algorithms reported to achieve or even surpass the human performance [1]– [3].",
      "startOffset" : 232,
      "endOffset" : 235
    }, {
      "referenceID" : 2,
      "context" : "In recent years, Machine Learning (ML) techniques have been extensively deployed for computer vision tasks, particularly visual classification problems, where new algorithms reported to achieve or even surpass the human performance [1]– [3].",
      "startOffset" : 237,
      "endOffset" : 240
    }, {
      "referenceID" : 3,
      "context" : "Recent research however have pointed out their vulnerability in adversarial environments [5]–[7].",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 5,
      "context" : "Recent research however have pointed out their vulnerability in adversarial environments [5]–[7].",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 6,
      "context" : "Several papers have presented attacks on various ML systems, such as voice interfaces [8], face-recognition systems [9], toxic comment detectors [10], and video annotation systems [11].",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 7,
      "context" : "Several papers have presented attacks on various ML systems, such as voice interfaces [8], face-recognition systems [9], toxic comment detectors [10], and video annotation systems [11].",
      "startOffset" : 116,
      "endOffset" : 119
    }, {
      "referenceID" : 8,
      "context" : "Several papers have presented attacks on various ML systems, such as voice interfaces [8], face-recognition systems [9], toxic comment detectors [10], and video annotation systems [11].",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 9,
      "context" : "Several papers have presented attacks on various ML systems, such as voice interfaces [8], face-recognition systems [9], toxic comment detectors [10], and video annotation systems [11].",
      "startOffset" : 180,
      "endOffset" : 184
    }, {
      "referenceID" : 10,
      "context" : "These noise types often occur during image acquisition and transmission [12].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 11,
      "context" : "Impulse Noise, also known as Salt-and-Pepper Noise, is commonly modeled by [13]:",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 11,
      "context" : "We use the weighted-average filtering method, proposed in [13], for restoring the images corrupted by impulse noise.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 10,
      "context" : "Gaussian noise can be reduced by filtering the input with a low-pass kernel [12].",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 12,
      "context" : "For images of size d1×d2×3, the PSNR is computed as follows [14]:",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 13,
      "context" : "are usually considered to be between 20 and 40 dB, where higher is better [15].",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 14,
      "context" : "We perform the experiments on different image types, including natural images from the ImageNet dataset [16], images containing faces from the Faces94 dataset [17], and images with text.",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 15,
      "context" : "The noisy images used in our attack can be viewed as a form of adversarial examples [18].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 16,
      "context" : "attacks on learning systems [19], [20], we have no information about the training data or even the set of output labels of the model.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 17,
      "context" : "attacks on learning systems [19], [20], we have no information about the training data or even the set of output labels of the model.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 18,
      "context" : "Moreover, unlike the current methods for generating adversarial examples [21], we perturb the input completely randomly, which results in a more serious attack vector in real-world applications.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 19,
      "context" : "It has been shown that the robustness of ML algorithms can be improved by using regularization or data augmentation during training [22].",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 20,
      "context" : "In [23], the authors proposed adversarial training, which iteratively creates a supply of adversarial examples and includes them into the training data.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 10,
      "context" : "properties, such as high correlation among adjacent pixels, sparsity in transform domain or having low energy in high frequencies [12].",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 11,
      "context" : "6: The restored images, generated by applying the weighted-average filter [13] on the noisy images of figures 3",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 11,
      "context" : "Restored images are generated by applying the weighted-average filter [13] for impulse noise and a lowpass filter for Gaussian noise.",
      "startOffset" : 70,
      "endOffset" : 74
    } ],
    "year" : 2017,
    "abstractText" : "Google has recently introduced the Cloud Vision API for image analysis. According to the demonstration website, the API “quickly classifies images into thousands of categories, detects individual objects and faces within images, and finds and reads printed words contained within images.” It can be also used to “detect different types of inappropriate content from adult to violent content.” In this paper, we evaluate the robustness of the Google’s Cloud Vision API to input perturbation. In particular, we show that by adding sufficient noise to the image, the API generates completely different outputs for the noisy image, while a human observer would perceive its original content. We show that the attack is consistently successful, by performing extensive experiments on different image types, including natural images, images containing faces and images with texts. Our findings indicate the vulnerability of the API in adversarial environments. For example, an adversary can bypass an image filtering system by adding noise to an image with inappropriate content.",
    "creator" : "LaTeX with hyperref package"
  }
}