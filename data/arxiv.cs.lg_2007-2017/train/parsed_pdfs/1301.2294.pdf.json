{
  "name" : "1301.2294.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Expectation Propagation for Approximate Bayesian Inference",
    "authors" : [ "Thomas P. Minka" ],
    "emails" : [ ],
    "sections" : null,
    "references" : [ {
      "title" : "Tractable inference for complex",
      "author" : [ "D. Koller" ],
      "venue" : null,
      "citeRegEx" : "Boyen and Koller,? \\Q1998\\E",
      "shortCiteRegEx" : "Boyen and Koller",
      "year" : 1998
    }, {
      "title" : "Bayes point machines: Estimating the Bayes point in kernel space",
      "author" : [ "R. Herbrich", "T. Graepel", "C. Campbell" ],
      "venue" : "IJCAI Workshop Support Vector Machines",
      "citeRegEx" : "Herbrich et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Herbrich et al\\.",
      "year" : 1999
    }, {
      "title" : "Factor graphs and the sum-product algorithm",
      "author" : [ "F.R. Kschischang", "B.J. Frey", "Loeliger", "H.-A" ],
      "venue" : "IEEE Trans Info Theory,",
      "citeRegEx" : "Kschischang et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Kschischang et al\\.",
      "year" : 2000
    }, {
      "title" : "Propagation of probabilities, means and variances in mixed graphical association models",
      "author" : [ "S.L. Lauritzen" ],
      "venue" : "J American Statistical Association,",
      "citeRegEx" : "Lauritzen,? \\Q1992\\E",
      "shortCiteRegEx" : "Lauritzen",
      "year" : 1992
    }, {
      "title" : "Stochastic models, estimation and control, chapter 12.7",
      "author" : [ "P.S. Maybeck" ],
      "venue" : null,
      "citeRegEx" : "Maybeck,? \\Q1982\\E",
      "shortCiteRegEx" : "Maybeck",
      "year" : 1982
    }, {
      "title" : "Afamily of algorithms for approximate Bayesian inference",
      "author" : [ "T.P. Minka" ],
      "venue" : "Doctoral dissertation, Massachusetts Institute of Technology. vismod.www.media.mit.edu/-tpminka/",
      "citeRegEx" : "Minka,? \\Q2001\\E",
      "shortCiteRegEx" : "Minka",
      "year" : 2001
    }, {
      "title" : "Loopy-belief propagation for approximate inference: An empirical study",
      "author" : [ "K. Murphy", "Y. Weiss", "M. Jordan" ],
      "venue" : "Uncertainty in AI",
      "citeRegEx" : "Murphy et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Murphy et al\\.",
      "year" : 1999
    }, {
      "title" : "A Bayesian approach to on-line learning. On-Line Learning in Neural Networks",
      "author" : [ "M. Opper", "Winther" ],
      "venue" : null,
      "citeRegEx" : "Opper et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Opper et al\\.",
      "year" : 1999
    }, {
      "title" : "Gaussian processes for classification: Mean field algorithms",
      "author" : [ "M. Opper", "Winther" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Opper et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Opper et al\\.",
      "year" : 2000
    }, {
      "title" : "A linear approximation method for probabilistic inference. Uncertainty in AI",
      "author" : [ "R. Shachter" ],
      "venue" : null,
      "citeRegEx" : "Shachter,? \\Q1990\\E",
      "shortCiteRegEx" : "Shachter",
      "year" : 1990
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "For example, we might re­ place the exact one-step posterior with a Gaussian having the same mean and same variance (Maybeck, 1982; Opper & Winther, 1999).",
      "startOffset" : 116,
      "endOffset" : 154
    }, {
      "referenceID" : 9,
      "context" : ", 1999) and extended Kalman filtering (Shachter, 1990).",
      "startOffset" : 38,
      "endOffset" : 54
    }, {
      "referenceID" : 6,
      "context" : "In belief networks with loops it is known that approximate marginal distributions can be obtained by iterating the be­ lief propagation recursions, a process known as loopy be­ lief propagation (Frey & MacKay, 1997; Murphy et al., 1999).",
      "startOffset" : 194,
      "endOffset" : 236
    }, {
      "referenceID" : 3,
      "context" : "ADF has been indepen­ dently proposed in the statistics (Lauritzen, 1992), artifi­ cial intelligence (Boyen & Koller, 1998; Opper & Winther, 1999), and control (Maybeck, 1982) literatures.",
      "startOffset" : 56,
      "endOffset" : 73
    }, {
      "referenceID" : 4,
      "context" : "ADF has been indepen­ dently proposed in the statistics (Lauritzen, 1992), artifi­ cial intelligence (Boyen & Koller, 1998; Opper & Winther, 1999), and control (Maybeck, 1982) literatures.",
      "startOffset" : 160,
      "endOffset" : 175
    }, {
      "referenceID" : 5,
      "context" : "Regular EP did not converge, but a restricted ver­ sion did (Minka, 2001).",
      "startOffset" : 60,
      "endOffset" : 73
    }, {
      "referenceID" : 1,
      "context" : "This section applies Expectation Propagation to inference in the Bayes Point Machine (Herbrich et al., 1999).",
      "startOffset" : 85,
      "endOffset" : 108
    }, {
      "referenceID" : 5,
      "context" : "This extension can be found in Minka (200 1 ). Interestingly, Opper & Winther (2000) have derived an equivalent algorithm using statistical physics methods.",
      "startOffset" : 31,
      "endOffset" : 85
    }, {
      "referenceID" : 1,
      "context" : "error for EP versus three other algorithms for estimating the Bayes point: the billiard al­ gorithm of Herbrich et al. (1999), the TAP algorithm of Opper & Winther (2000), and the mean-field (MF) algo­ rithm of Opper & Winther (2000).",
      "startOffset" : 103,
      "endOffset" : 126
    }, {
      "referenceID" : 1,
      "context" : "error for EP versus three other algorithms for estimating the Bayes point: the billiard al­ gorithm of Herbrich et al. (1999), the TAP algorithm of Opper & Winther (2000), and the mean-field (MF) algo­ rithm of Opper & Winther (2000).",
      "startOffset" : 103,
      "endOffset" : 171
    }, {
      "referenceID" : 1,
      "context" : "error for EP versus three other algorithms for estimating the Bayes point: the billiard al­ gorithm of Herbrich et al. (1999), the TAP algorithm of Opper & Winther (2000), and the mean-field (MF) algo­ rithm of Opper & Winther (2000). The error is measured by Euclidean distance to the exact solution found by im­ portance sampling.",
      "startOffset" : 103,
      "endOffset" : 234
    } ],
    "year" : 2011,
    "abstractText" : "This paper presents a new deterministic approx­ imation technique in Bayesian networks. This method, \"Expectation Propagation,\" unifies two previous techniques: assumed-density filtering, an extension of the Kalman filter, and loopy be­ lief propagation, an extension of belief propaga­ tion in Bayesian networks. Loopy belief propa­ gation, because it propagates exact belief states, is useful for a limited class of belief networks, such as those which are purely discrete. Expec­ tation Propagation approximates the belief states by only retaining expectations, such as mean and variance, and iterates until these expectations are consistent throughout the network. This makes it applicable to hybrid networks with discrete and continuous nodes. Experiments with Gaussian mixture models show Expectation Propagation to be convincingly better than methods with simi­ lar computational cost: Laplace's method, vari­ ational Bayes, and Monte Carlo. Expectation Propagation also provides an efficient algorithm for training Bayes point machine classifiers.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}