{"conference": "ICLR 2017 conference submission", "title": "Memory-augmented Attention Modelling for Videos", "abstract": "Recent works on neural architectures have demonstrated the utility of attention mechanisms for a wide variety of tasks. Attention models used for problems such as image captioning typically depend on the image under consideration, as well as the previous sequence of words that come before the word currently being generated. While these types of models have produced impressive results, they are not able to model the higher-order interactions involved in problems such as video description/captioning, where the relationship between parts of the video and the concepts being depicted is complex. Motivated by these observations, we propose a novel memory-based attention model for video description. Our model utilizes memories of past attention when reasoning about where to attend to in the current time step, similar to the central executive system proposed in human cognition. This allows the model to not only reason about local attention more effectively, it allows it to consider the entire sequence of video frames while generating each word. Evaluation on the challenging and popular MSVD and Charades datasets show that the proposed architecture outperforms all previously proposed methods and leads to a new state of the art results in the video description.", "reviews": [{"is_meta_review": true, "comments": "This paper addresses video captioning with a TEM-HAM architecture, where a HAM module attends over attended outputs of the TEM module when generating the description. This gives a kind of 2-level attention. The model is evaluated on the Charades and MSVD datasets.\n\n1. Quality/Clarify: I found this paper to be poorly written and relatively hard to understand. As far as I can tell the TEM module of Section 3.1 is a straight-forward attention frame encoder of Bahdanau et al. 2015 or Xu et al. 2015. The decoder of Section 3.3 is a standard LSTM with log likelihood. The HAM module of Section 3.2 is the novel module but is not very well described. It looks to be an attention LSTM where the attention is over the TEM LSTM outputs, but the attention weights are additionally conditioned on the decoder state. There are a lot of small problems with the description, such as notational discrepancy in using \\textbf in equations and then not using it in the text. Also, I spent a long time trying to understand what f_m is. The authors say: \n\n\"In order to let the network remember what has been attended before and the temporal\nstructure of a video, we propose f_m to memorize the previous attention and encoded version of an\ninput video with language model. Using f_m not only enables the network to memorize previous\nattention and frames, but also to learn multi-layer attention over an input video and corresponding\nlanguage.\"\n\nWhere one f_m is bold and the other f_m is not. Due to words such as \"we propose f_m\" assumed this was some kind of a novel technical contribution I couldn't find any details about but it is specified later in Section 3.3 at the end that f_m is in fact just an LSTM. It's not clear why this piece of information is in Section 3.3, which discusses the decoder. The paper is sloppy in other parts. For example in Table 1 some numbers have 1 significant digit and some have 2. The semantics of the horizontal line in Table 2 are not explained in text. \n\n2. Experimental results: The ablation study shows mixed results when adding TEM and HAM to the model. Looking at METEOR which was shown to have the highest correlation to humans in the COCO paper compared to the other evaluation criteria, adding TEM+HAM improves the model from 31.20 to 31.70. It is not clear how significant this improvement is, especially given that the test set is only 670 videos. I have doubts over this result. In Table 2, the METEOR score of Pan et al. 2016a is higher [33.10 vs. 31.80], but this discrepancy is not addressed in text. This is surprising because the authors explicitly claim \"state of the art results\".\n\n3. Originality/Significance: The paper introduces an additional layer of attention over a more standard sequence to sequence setup, which is argued to alleviate the burden on the LSTM's memory. This is moderately novel but I don't believe that the experimental results make it sufficiently clear that it is also worth doing. If the paper made the standard model somehow simpler instead of more complex I would be more inclined to judge it favorably.\n\n\nMinor:\nIn response to the author's comment \"not sure what causes to think of RBM. We don't model any part of our architecture using RBM. We'd be appreciated if you please elaborate more about your confusion about figure 1 so we can address it accordingly.\", I created a diagram to hopefully make this more clear:", "IS_META_REVIEW": true}, {"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "There appears to be consensus among the reviewers that the paper appears the overstate its contributions: the originality of the proposed temporal modeler (TEM) is limited, and the experimental evaluation (which itself is of good quality!) does not demonstrate clear merits of the TEM architecture. As a result, the impact of this paper is expected to be limited.", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Review", "is_meta_review": false, "comments": "The authors propose a \"hierarchical\" attention model for video captioning.  They introduce a model composed of three parts: the temporal modeler (TEM) that takes as input the video sequence and outputs a sequential representation of the video to the HAM; the hierarchical attention/memory mechanism (HAM) implements a soft-attention mechanism over the sequential video representation; and finally a decoder that generates a caption. \n\nRelated to the second series of questions above, it seems as though the authors have chosen to refer to their use of an LSTM (or equivalent RNN) as the output of the Bahdanau et al (2015) attention mechanism as a hierarchical memory mechanism. I am actually sympathetic to this terminology in the sense that the recent popularity of memory-based models seems to neglect the memory implicit in the LSTM state vector, but that said, this seems to seriously misrepresent the significance fo the contribution of this paper. \n\nI appreciate the ablation study presented in Table 1. Not enough researchers bother with this kind of analysis. But it does show that the value of the contributions is not actually clear. In particular the case for the TEM is quite weak.\n\nRegarding the quantitative evaluation presented in Table 2, the authors are carving out a fairly specific set of features to describe the set of \"fair\" comparators from the literature. Given the variability of the models and alternate training datasets that are in use, I would find it more compelling if the authors just set about trying to achieve the best results they can, if that includes the fine-tuning of the frame model, so be it. The value of this work is as an application paper, so the discovery and incorporation of elements that can significantly improve performance would seems warranted. \n\nOverall, at this point, I do not see a sufficient contribution to warrant publication in ICLR.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "17 Dec 2016 (modified: 21 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Question", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "IS_META_REVIEW": false, "DATE": "17 Dec 2016", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Review: Novelty and performance claims do not hold, missing clarity, no significant improvements over ablations.", "is_meta_review": false, "comments": "The paper proposes an attention-based approach for video description. The approach uses three LSTMs and two attention mechanisms to sequentially predict words from a sequence of frames.\nIn the LSTM-encoder of the frames (TEM), the first attention approach predicts a spatial attention per frame, and computes the weighted average. The second LSTM (HAM) predicts an attention over the hidden states of the encoder LSTM.\nThe third LSTM which run temporally in parallel to the second LSTM generates the sentence, one word at a time.\n\n\nStrength:\n===============\n\n-\tThe paper works on a relevant and interesting problem.\n-\tUsing 2 layers of attention in the proposed way have to my knowledge not been used before for video description. The exact architecture is thus novel (but the work claims much more without sufficient attribution, see blow)\n-\tThe experiments are evaluated on two datasets, MSVD and Charades, showing performance on the level of related work for MSVD and improvements for Charades.\n\nWeaknesses:\n===============\n\n1.\tClaims about the contribution/novelty of the model seem not to hold: \n1.1.\tOne of the main contributions is the Hierarchical Attention/Memory (HAM):\n1.1.1.\tIt is not clear to me how the presented model (Eq 6-8), are significantly different from the presented model in Xu et al / Yao et al. While Xu et al. attends over spatial image locations and Yao et al. attend over frames, this model attends over encoded video representations h_v^i. A slight difference might be that Xu et al. use the same LSTM to generate, while this model uses an additional LSTM for the decoding.\n1.1.2.\tThe paper states in section 3.2 \u201cwe propose f_m to memorize the previous attention\u201d, however H_m^{t\u2019-1} only consist of the last hidden state. Furthermore, the model f_m does not have access to the \u201cattention\u201d \\alpha. This was also discussed in comments by others, but remains unclear.\n1.1.3.\tIn the discussion of comments the authors claim that \u201cattention not only is a function a current time step but also a function of all previous attentions and network states.\u201d: While it is true that there is a dependency but that is true also for any LSTM, however the model does not have access to the previous network states as H_g^{t\u2019-1} only consist of the last hidden state, as well as H_m^{t\u2019-1} [at least that is what the formulas say and what Figure 1 suggests]. \n1.1.4.\tThe authors claim to have multi-layer attention in HAM, however it remains unclear where the multi-layer comes from.\n1.2.\tThe paper states that in section 3.1. \u201c[CNN] features tend to discard the low level information useful in modeling the motion in the video (Ballas et al., 2016).\u201d This suggests that the approach which follows attacks this problem. However, it cannot model motion as attention \\rho between frames is not available when predicting the next frame. Also, it is not clear how the model can capture anything \u201clow level\u201d as it operates on rather high level VGG conv 5 features.\n\n2.\tRelated work: The difference of HAM to Yao et al. and Xu et al. should be made more clear / or these papers should be cited in the HAM section.\n\n3.\tConceptual Limitation of the model: The model has two independent attention mechanisms, a spatial one, and a temporal one. The spatial (within a frame) is independent of the sentence generation. It thus cannot attend to different aspects of the frames for different words which would make sense. E.g. if the sentence is \u201cthe dog jumps on the trampoline\u201d, the model should focus on the dog when saying \u201cdog\u201d and on the trampoline when saying \u201ctrampoline\u201d, however, as the spatial attention is fixed this is difficult. Also, the encoder model does not have an explicitly way to look at different aspects in the frame during the encoding so might likely get stuck and always predict the same spatial attention for all frames (or it might e.g. always attend to the dog which moves around, but never on the scene).\n\n4.\tEq 11 contradicts Fig 1: How is the model exactly receiving the previous word as input. Eq. 11 suggests it is the softmax. If this is the case, the authors should emphasize this in the text as this is unusual. More common would be to use the ground truth previous word during training (which Fig 11 suggests) and the \u201chardmax\u201d, i.e. the highest predicted previous word encoded as one-hot vector at test time.\n\n5.\tClarity:\n5.1.\tIt would be helpful if the same notation would be used in Eq 2-5 and 6-9. Why is a different notation required?\n5.2.\tIt would be helpful if Fig 1 could contain more details or additional figures for the corresponding parts would be added. If space is a problem, e.g. the well-known equations for LSTM, softmax (Eq 2), and log likelihood loss (Eq 12) could be omitted or inlined.\n\n6.\tEvaluation:\n6.1.\tThe paper claims that the \u201cthe proposed architecture outperforms all previously proposed methods and leads to a new state of the art results\u201d.\n6.1.1.\tFor the MSVD dataset this clearly is wrong, even given the same feature representation. Pan et al. (2016 a) in Table 2 achieve higher METEOR (33.10).\n6.1.2.\tFor this strong claim, I would also expect that it outperforms all previous results independent of the features used, which is also wrong again, Yu et al achieve higher performance in all compared metrics.\n6.1.3.\tFor Charades dataset, this claim is also too bold as hardly any methods have been evaluated on this dataset, so at least all the ablations reported in Table 1 should also be reported for the Charades dataset, to make for this dataset any stronger claims.\n6.2.\tMissing qualitative results of attention: The authors should show qualitative results of the attention, for both attention mechanisms to understand if anything sensible is happening there. How diverse is the spatial and the temporal attention? Is it peaky or rather uniform?\n6.3.\tPerformance improvement is not significant over model ablations: The improvements over Att+No TEM is only 0.5 Meteor, 0.7 Blue@4 and the performance drops for CIDEr by 1.7.\n6.4.\tMissing human evaluation: I disagree with the authors that a human evaluation is not feasible. 1. An evaluation on a subset of the test data is not so difficult. 2. Even if other authors do not provide their code/model [and some do], they are typically happy to share the predicted sentences which is sufficient and even better for human evaluation [if not I would explicitly mention that some authors did not share sentences, as this seems clearly wrong]. 3. For model ablations the sentences are available to the authors.\n\n7.\tSeveral of the comments raised by reviewers/others have not yet been incorporated in a revised version of the paper and/or are still not clear from the explanations given. E.g. including SPICE evaluation and making fixes seems trivial.\t\n\n8.\tHyperparameters are inconsistent: Why are the hyperparemters inconsistent between the ablation analysis (40 frames are sampled) and the performance comparison (8 frames)? Should this not be selected on the validation set? What is the performance of all the ablations with 8 frames?\n\nOther (minor/discussion points)\n-\tEquation 10: what happens with h_m, and h_g, the LSTM formulas provided only handle two inputs. Are h_m and h_g concatenated.\n-\tThere is a section 4.1 but no 4.2.\n-\tThe paper states in section 4.1 \u201cour proposed architecture can alone not only learn a representation for video that can model the temporal structure of a video sequence, but also a representation that can effectively map visual space to the language space.\u201d However, this seems to be true also for many/most other approaches, e.g. [Venugopalan et al. 2015 ICCV]\n\nSummary:\n===============\n\nWhile the paper makes strong claims w.r.t. to the approach and results, the approach lacks novelty and the results are not convincing over related work and ablations. Furthermore, improved clarity and visualizations of the model and attention results would benefit the paper.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "lacks polish and clarity, moderately original but weak results", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "\nThis paper addresses video captioning with a TEM-HAM architecture, where a HAM module attends over attended outputs of the TEM module when generating the description. This gives a kind of 2-level attention. The model is evaluated on the Charades and MSVD datasets.\n\n1. Quality/Clarify: I found this paper to be poorly written and relatively hard to understand. As far as I can tell the TEM module of Section 3.1 is a straight-forward attention frame encoder of Bahdanau et al. 2015 or Xu et al. 2015. The decoder of Section 3.3 is a standard LSTM with log likelihood. The HAM module of Section 3.2 is the novel module but is not very well described. It looks to be an attention LSTM where the attention is over the TEM LSTM outputs, but the attention weights are additionally conditioned on the decoder state. There are a lot of small problems with the description, such as notational discrepancy in using \\textbf in equations and then not using it in the text. Also, I spent a long time trying to understand what f_m is. The authors say: \n\n\"In order to let the network remember what has been attended before and the temporal\nstructure of a video, we propose f_m to memorize the previous attention and encoded version of an\ninput video with language model. Using f_m not only enables the network to memorize previous\nattention and frames, but also to learn multi-layer attention over an input video and corresponding\nlanguage.\"\n\nWhere one f_m is bold and the other f_m is not. Due to words such as \"we propose f_m\" assumed this was some kind of a novel technical contribution I couldn't find any details about but it is specified later in Section 3.3 at the end that f_m is in fact just an LSTM. It's not clear why this piece of information is in Section 3.3, which discusses the decoder. The paper is sloppy in other parts. For example in Table 1 some numbers have 1 significant digit and some have 2. The semantics of the horizontal line in Table 2 are not explained in text. \n\n2. Experimental results: The ablation study shows mixed results when adding TEM and HAM to the model. Looking at METEOR which was shown to have the highest correlation to humans in the COCO paper compared to the other evaluation criteria, adding TEM+HAM improves the model from 31.20 to 31.70. It is not clear how significant this improvement is, especially given that the test set is only 670 videos. I have doubts over this result. In Table 2, the METEOR score of Pan et al. 2016a is higher [33.10 vs. 31.80], but this discrepancy is not addressed in text. This is surprising because the authors explicitly claim \"state of the art results\".\n\n3. Originality/Significance: The paper introduces an additional layer of attention over a more standard sequence to sequence setup, which is argued to alleviate the burden on the LSTM's memory. This is moderately novel but I don't believe that the experimental results make it sufficiently clear that it is also worth doing. If the paper made the standard model somehow simpler instead of more complex I would be more inclined to judge it favorably.\n\n\nMinor:\nIn response to the author's comment \"not sure what causes to think of RBM. We don't model any part of our architecture using RBM. We'd be appreciated if you please elaborate more about your confusion about figure 1 so we can address it accordingly.\", I created a diagram to hopefully make this more clear:  ", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "is_meta_review": false}, {"TITLE": "Clarifications", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "is_meta_review": false}, {"TITLE": "clarifications", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "is_meta_review": false}, {"TITLE": "Few questions about TEM and HAM", "OTHER_KEYS": "(anonymous)", "comments": "Really interesting paper. I have a few questions below.\n\nabout TEM:\nPage 4 you extracted 1 conv map of size LxD for each N frames, how to ensure that L locations are different part of that frame? Or could you explain this more specifically?\n\nabout HAM:\nPage 5 in equation (6), what is the dimension of $$H_g^{t'-1}$$ and $$H_m^{t'-1}$$? Did you repeat $$h_g^{t'-1}$$ N times to form $$H_g^{t'-1}$$? You called this as Hierarchical Attention Model, but I don't really see the hierarchy structure.\n\nI'd appreciate it if you could explain above questions, thanks.", "IS_META_REVIEW": false, "DATE": "21 Nov 2016", "is_meta_review": false}, {"TITLE": "Interesting Paper: Some Minor Feedback and Questions", "OTHER_KEYS": "(anonymous)", "comments": "Dear authors, thank you for submitting the interesting paper. Good to see the incorporation of memory in the context of video description generation. Although in general I like the paper, I have some feedback and questions. Most of my feedback corresponds to the discussion of the HAM at page 5 on the top and the experimental details.\n\n1. During training, how many video frames 0..N are fed into the TEM? Is there any kind of temporal subsampling of the videos? From the text I get the impression that all the video frames are used during training, but wouldn't this cause unrolling the LSTMs for too many timesteps? (i.e. training difficulties and memory constraints)\n2. Figure 1 would be a lot more useful if the arrows include the tensors that flow between them (e.g. F^ to the memory)\n3. It would be useful to explicitly mention the temporal superscripts for the different LSTMs. Now the t-1 and t'-1 notation can be rather confusing.\n4. The bold typefaces in Eq (6) do not match with the regular fonts in the paragraph just below (i.e. H_v, W_v etc). \n5. Just below Eq (9) you write: H_v = [h_1,...,h_N] but this notation does not match with Figure 1.\n6. Just below Eq (12): \"in a single step optimization\" => does this mean a batch size of 1?\n\nOverall an interesting paper that was nice to read. Hopefully you can clarify with regard to the minor feedback and questions.\n", "IS_META_REVIEW": false, "DATE": "09 Nov 2016", "is_meta_review": false}, {"TITLE": "Wrong citation", "OTHER_KEYS": "(anonymous)", "comments": "It seems that the cited paper \" Jointly modeling embedding and translation to bridge video and language\" appears on CVPR16, not the AAAI15. I think you should revise your bib.", "IS_META_REVIEW": false, "DATE": "08 Nov 2016", "is_meta_review": false}], "SCORE": 4, "authors": "Rasool Fakoor, Abdel-rahman Mohamed, Margaret Mitchell, Sing Bing Kang, Pushmeet Kohli", "KEYWORDS": "We propose a novel memory-based attention model for video description", "accepted": false, "id": ""}
