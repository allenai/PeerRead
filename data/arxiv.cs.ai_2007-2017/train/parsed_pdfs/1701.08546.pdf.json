{
  "name" : "1701.08546.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Survey on Models and Techniques for Root-Cause Analysis",
    "authors" : [ "Marc Solé", "Victor Muntés-Mulero", "Annie Ibrahim Rana", "Giovani Estrada" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 1.\n08 54\n6v 1\n[ cs\n.A I]\n3 0\nJa n\n20 17\nIndex Terms—Big data, failure diagnosis, root-cause analysis.\nI. INTRODUCTION\nWith the onset of the SMAC industry (i.e. Social, Mobile, Analytics and Cloud), Software as a Service, and the Internet of Things (IoT), more organizations in all industry sectors recognize they are evolving into technology and data companies1. While one may be tempted to believe that this may only affect some limited number of markets, all indicators show a much more aggressive trend for companies in all industries to transform businesses through software, exploring much further market adjacencies. 54% of CEOs have entered a new sector or sub-sector, or considered it, in the past three years2. Also from the same source by PricewaterhouseCoopers, 55% of entertainment and media CEOs, 52% of communications CEOs, 48% of power and utilities CEOs and 47% of banking and capital markets CEOs say a significant competitor from the technology sector is emerging or will emerge.\nSeveral factors speed up the growing importance of software. The cloud has become necessary for the survival of technology companies. Revenue for SaaS is expected to grow at a compound annual rate of more than 20% throughout this decade. By 2018, 59% of the total cloud workloads will be SaaS3. Besides, we are seeing an integration of digital\nM. Solé and V. Muntés are with CA Technologies. A. Ibrahim and G. Estrada are with Intel 1McKendrick, J. (2015, April 30). Every Company Now A Technology Company: Latest Round Of Mergers And Acquisitions Confirms It. Forbes. Retrieved from http://tinyurl.com/j6f7ub5\n218th Annual Global CEO Survey. PricewaterhouseCoopers. Retrieved from: http://www.pwc.com/gx/en/ceo-survey/2015/download.jhtml\n3Global Cloud Index: Forecast and Methodology, 20142019. Retrieved from: http://tinyurl.com/q9vxgcx\nsensors, processing, connectivity and security into virtually every industry’s products. As IoT moves out of the hype phase, it will drive demand for network infrastructure, sensors, software applications, and all technologies needed to operate IoT applications including data analytics4. Cisco predicted that IoT will unleash $19 trillion USD in new profits and cost savings globally in the next decade (Burrows, 2014). According to Gartner, there will be nearly 26 billion devices on the Internet of Things by 2020, that will potentially generate zetabytes of data annually.\nThe growth of cloud and IoT poses serious challenges to IT leaders. Developing and deploying smart, connected products and retrofitting existing equipment is very challenging, requiring coordination of network connectivity, application protocols, data analytics, and system management. IoT platforms are being developed5 to simplify the processes of developing, connecting, controlling, and capturing insight from connected products and assets, allowing firms to sense and respond to changing customer needs. In particular, controlling these complex and distributed IoT systems will require advanced Root-Cause Analysis (RCA) capable of precisely synthesizing the status of the system for human beings to make decisions. Specifically, human beings will no longer be capable of controlling so complex system through traditional dashboards and will require a higher level of automation to generate hypotheses of potential root-causes much more accurately.\nWhile several decades of research have produced a large number of algorithms and techniques to perform root cause analysis in many different fields, there is still a lack of understanding on how they can be used and adapted to the growing complexity of IoT and other similar environments, where scalability and real-time reaction become essential. In particular, the appropriate interpretation and management of the vast amounts of data generated in these environments need to be underpinned by IoT and Cloud platforms in order for them to be genuinely viable [1].\nIn this survey we will thus focus on the RCA models available and the existing generation and inference algorithms that have been developed for them, paying special attention to performance aspects. Although there is a vast literature on RCA, we will restrict to techniques that can be applied to IT systems. Our survey builds on the shoulders of previous contributions from other general surveys like [2]. For surveys\n4Global technology M&A 1Q15: first look. Ernst & Young. Retrieved from: http://www.ey.com/GL/en/Industries/Technology/EY-global-technology-ma-1q15-first-look; and Manyika, J., Chui, M., Bisson, P., Woetzel, J., Dobbs, R., Bughin, J. and Aharon, D. (2015, June). Unlocking the potential of the Internet of Things. McKinsey Global Institute. Retrieved from: http://tinyurl.com/jqpymqu\n52015 Technology Industry Outlook. Deloitte. Retrieved from: http://tinyurl.com/o7sb52h\n2 on specific areas, the reader may refer to [3], [4] for computer networks, [5], [6] for software, [7], [8] for industrial systems, [9] for smart buildings, [10] for buildings, [11] for machinery, [12] for swarm systems, [13], [14] for automatic control systems, [15], [16] for automotive systems and [17] for aerospace systems.\nThe remainder of the paper is organized as follows. Section II introduces the main concepts and terminology used in the rest of the paper. Section III describes the main models used for RCA, the different ways in which they can be obtained and some of the learning algorithms available for each model. The inference algorithms that can be used on these models are the subject of Section IV while Section V concludes this paper."
    }, {
      "heading" : "II. MAIN CONCEPTS AND TERMINOLOGY",
      "text" : "In this section, we discuss the main concepts and terminology used in the area of RCA. In particular, we provide some essential terminology that we will use throughout the remainder of the survey and provide a classification of RCA tasks.\nThe core concepts behind RCA are causality and explanation. Although central to any scientific endeavour, there is no consensus on their formal definition despite centuries long discussions on the subject [18], [19]. This is relevant, especially in the case of explanation, because an explanation is often a desired output of RCA, thus, as we will see in Section IV, many alternatives have been proposed.\nA. Terminology\nIn this survey, we follow the terminology proposed in [2]: Event is an exceptional condition occurring in the operation\nof a system. Faults/problems/root causes are events that can cause other\nevents but are not themselves caused by other events. According to their duration they can be classified as: permanent if fault will persist until reparation, intermittent if they are discontinuous and periodic, and transient if temporary.\nError An Error is caused by one or more faults and is a discrepancy between a condition of the system and its theoretically correct condition.\nFailure A Failure is an error that is observable from outside the system.\nSymptom A Symptom is an external manifestations of failures. This includes a direct observation of failures themselves and externally visible indicators that a failure happened that are not failures by themselves, like alarms raised by anomaly detectors.\nRoot Cause Analysis also referred as fault localization, fault isolation or alarm/event correlation, is the process of inferring the set of faults that generated a given set of symptoms. Note that this process might be trivial if faults are directly observable, in which case they are also symptoms as well. However, this is not the usual case in complex systems. In such cases, a model that explains the relationship between faults and symptoms must be used to be able to perform this inference process.\nB. Classification of the RCA challenges\nThere is a wide range of RCA models and techniques. One of the reasons that explains such a large corpus is the fact that different aspects and requirements of the system to be analyzed may need different analysis strategies. In this section we describe some of the relevant dimensions that can affect the nature of a RCA problem.\nAnalysis intent Whether the objective of the analysis is to obtain just the root cause (or causes) of the observed symptoms or an explanation (explaining how the root causes are linked to the symptoms) is desired.\nAnalysis time This is a non-functional requirement affecting the maximum time that the algorithm can spend doing inference. Although this requirement can have a numerical translation, it is sometimes useful to use two broad classes: real-time diagnosis, in which response time is critical and post-mortem diagnosis, in which temporal constraints are not so important. For real-time diagnosis a usual approach is to precompute part or all of the inference process, trading space by online checking time, i.e., the time for analysis still has to be spent during the offline precomputation.\nComplexity Diagnosis can be a more or less challenging task depending on a number of factors:\nSystem Size The number of components in the system to diagnose. This will have an impact in the size of the model used for diagnosis.\nData Size Volume of data that has to be processed during diagnosis. For diagnosing systems which use data as well to generate the RCA model, this metric can be subdivided between Training Data Size, i.e.,the amount of data that is available for learning the model, and Observation Data Size, which is the amount of data that needs to be processed during the inference process.\nInference length Maximum number of components that have to be traversed to reach a fault from a symptom. If this length is zero, it means that all faults are symptoms and diagnosis is simply the output of the anomaly detection task. Since the diagnosis of these type of systems is a task of anomaly detection, we will restrict to systems in which the inference length is at least one.\nEffect propagation time Changes in one component of the system may take time to modify the state of adjacent components. In some diagnosing systems the effect propagation time is ignored or considered immediate, like in medical diagnosis where the time window for relevant observations is decided by the doctor, but in other cases the effect propagation time defines the window of relevant observations, thus affecting the Observation Data Size.\nEvolution rate The speed and extent of changes in the diagnosed system. In slow-changing systems (like human health in medical diagnosis applications for a well-established medical area), it might pay off the effort of creating an accurate manual model from experts, since once complete there will be only minor changes to the model.\nDomain knowledge required. The amount of knowledge on the domain of the system. For instance, if building a diagnosing system for a Hadoop installation, the knowledge on Hadoop that is available to the designer of the diagnosis system.\nSystem knowledge required. Indicates the level of access the diagnosing system will have to the diagnosed system. This dimension ranges from black box, i.e., no information available, to white box, i.e., all internal information available, even with the possibility to alter the system. Note that this is orthogonal to the domain knowledge as, for instance, one could have access to the Hadoop code but have no idea on how Hadoop works.\nC. RCA workflow\nMost RCA techniques share a basic workflow that can be summarized in the diagram of Figure 1. First a model is constructed, combining Domain Knowledge, System Knowledge and observations of the diagnosed system. Not all these types of informations have to be necessarily used, but approaches exist that can consider all of them in the model construction.\nThe output of such process is, obviously, the RCA model that will be used for inference. The model is populated using the observations of the diagnosed system and the outputs, depending on the algorithm and model used can be the rootcauses and/or an explanation of the observations.\nThe type of model has an effect as well on the approach that has to be taken when there is a change in the system (i.e., a change in the System Knowledge) such as the addition/removal of components or connections between them. Some models can be updated incrementally, but for the rest when such changes occur a reconstruction of the model is required. Changes in Domain Knowledge are considered much less frequent than changes in System Knowledge, thus the workflow does not explicitly consider Domain Knowledge updates as it often entails a model reconstruction."
    }, {
      "heading" : "III. GENERATION OF RCA MODELS",
      "text" : "The characteristics of the inference process, in particular its performance, are heavily affected by the type of RCA model\nused. In this section we describe many of the RCA models available in the literature (Section III-A) and the techniques used to learn/construct these models (Section III-B).\nA. Models for RCA\nInference is a fundamental task in Artificial Intelligence (AI) [20], hence most models for RCA come from this field.\nThere are two broad families of models: Deterministic models and Probabilistic models. In deterministic models there is no uncertainty in the known facts or the inferences expressed in the model. On the other hand probabilistic models are able to handle this uncertainty.\nThese two families comprise several techniques, and each of the techniques can have different implementations with different performance implications. For instance decision trees and neural nets are two different implementations of a classifier and the time for diagnosis of the former is usually faster than the latter. Moreover, inside each model there can be subtypes with specific properties in terms of learning/inference complexity. That is particularly relevant for Bayesian Networks, that can be hierarchically classified as shown in Fig. 3.\nTable I shows some of the most well-known models for\nRCA, together with an example reference in which each model was used for diagnosis. For some models we were not able to find any diagnostic application in the literature, a fact that we have denoted by a ”?” in the table. Note, however, that one of these models are Sum-Product networks, which are closely related to Arithmetic Circuits, thus [60] would be a valid reference for them.\nThe table is not complete in the sense that any classifier can be used for RCA (at least for single-fault diagnosis) as long as there is enough training data. Training data would be instances of symptoms with their corresponding fault(s) as label. Since the number of different classifiers currently available is quite\nlarge, we only mention a subset of them in this table. Besides, some of them are mentioned both in the deterministic and the probabilistic families since, depending on the codification of the problem and their construction, they might be able to handle uncertainty on inputs and provide confidence values on outputs.\nAlthough classifiers are attractive specially because their automatic generation has been one of the key researched topics in Machine Learning, they do not dominate the RCA area. Some of the reasons that can help explain this effect are that the majority of the most advanced classifiers, like Neural Nets, (i) only return a predicted root cause and it is difficult to obtain an explanation from them. (ii) Do not yield logical rules, and such approaches are difficult to combine with available domain knowledge, although not impossible [67]. (iii) They are usually tailored for single label classification, which would correspond to a single fault diagnosis task. If multiple-fault diagnosis has to be achieved, then a selection strategy has to be implemented to generate the set of faults out of a multi-class classifier (e.g., like taking all labels above a defined threshold) or work with multi-label classifiers (see [68] for a good survey on the area).\nUsing a table to create a taxonomy of RCA models is helpful to mentally order the landscape of available models, but hides the fact that relationships between models are not as clean as they might seem. For instance, codebooks [33], [34] can be seen as a particular implementation of propositional logic, as they are basically a way to precompute the inference on top of a graph by generating sets of rules that can be quickly checked using a mechanism such as hash tables. Similarly, we have established a distinction between models able to diagnose situations in which time of observation of symptoms is not relevant for inference, and process models which explicitly consider the sequence of observations. However, there are process models that are Bayesian approaches as well, like Dynamic Bayesian Networks or Hidden Markov Models. These relationships can be better appreciated in the diagram of Figure 2.\nIf domain knowledge is provided in a given model, but the preferred model for inference is different, there are ways\n5 in which one model can be (sometimes losslessly) converted into another. For instance sets of (in some cases fuzzy) rules can be extracted from decision trees [69], Bayesian Networks [70], SVMs [71] and Neural Nets [72], [73], Possibilistic Logic derived from Markov Logic Networks [74], Bayesian networks can be generated from first-order logic [75] or fault trees [76] if probabilities are provided, fuzzy fault trees [77] and Sum-Product Networks [78], and Arithmetic circuits and Sum-Product Networks can be converted one into the other [79]. Some of these conversions have a strong effect on diagnosis performance, for instance Bayesian Networks and Relational Bayesian Network (one type of Probabilistic Relational Model) can be compiled into Arithmetic Circuits for particular diagnosing tasks [80]. The compilation process is expensive, but after that the diagnosis process is much faster [81].\nModels have their own characteristics, which can have a large impact on the diagnosis performance:\nSize. The number of elemental analysis elements used to model the system (typically number of components, but the exact definition depends on the abstraction level at which the system is modeled). Depending on the model this can be the number of variables, rules, nodes, etc. Models might have more than one size attribute.\nInference structure. Defines how the different elemental analysis elements (rules, nodes in a Bayesian Network, etc.) relate to each other. Many techniques are tailored for specific structures, as structure can have a large impact on the theoretical complexity as well as final performance. Since it is complex to succinctly specify the structure of a model, a derived metric, Inference length, is useful to distinguish between diagnosing techniques. Inference length is the maximum number of inference steps needed to reach a fault from a symptom in the model.\nThere are three main ways in which a model for RCA can be obtained:\nManual generation In this case a group of experts provides the model. Models produced in this way tend to be very accurate, but knowledge elicitation is a complex and slow process [82]. For systems with a high evolution rate this approach might be unpractical.\nAssisted generation In most of the cases domain knowledge is partially available, for instance in the form of known models for sub-parts of the system that can be replicated several times and arbitrarily connected to other subparts. In these cases, the whole model is produced by assembling the models of the sub-parts based on available data of the system (i.e., available system knowledge), like its topology. In some cases these sub-models are not explicit in a library but implicit in the algorithm that, given the available system knowledge, generates the model for RCA. Assisted generation methods require a fair amount of domain knowledge in the form of a submodel library and/or the specific composition algorithms, plus a detailed system information to be able to produce the final diagnosis model. Most of the RCA systems in the literature applied to industrial environments fall inside\nthis category, as it offers a good compromise between the quality of manual specification and the automatization of the construction of the whole model.\nAutomated generation The model for RCA is generated entirely from the data, using standard non adhoc algorithms, which may include observations as well as available system knowledge. This is the only viable solution if it is not possible or practical to obtain domain knowledge.\nB. Learning models for RCA\nWhen no domain knowledge is available to generate a model, either manually or in some assisted way, the only remaining options is to use learning algorithms on the raw data of the system. For models that are classifiers, this is quite straightforward since they originated in the Machine Learning area. For models that had different origins, like Bayesian Networks, a wide range of techniques have been developed to learn the models from the data. Table II summarizes the algorithms available for that task.\nSome models are complex enough to distinguish between their structure and the values of their internal parameters. This is the case for instance for BNs or Fuzzy Logic in which the structure corresponds to how variables are related (through arcs in BNs and through clauses in Fuzzy Logic) and the probabilities assigned to the elements in that structure are the parameters. Learning algorithms can learn both things (structure and parameters) or just one of them. In general in the table we have listed algorithms that learn everything or just the structure, as they frequently are more complex than the ones just learning the parameters given the structure. Note, however, that for Neural nets a structure is usually assumed and learning algorithms in that case usually refers to parameter learning algorithms.\nIn some cases no specific learning algorithm was found in the literature (e.g., Dempster-Shafer Theory, Fuzzy Fault Tree and Non-axiomatic Logic), although this does not preclude the option of learning an alternative model and then using a conversion between models as the one reported in Section III-A.\nIn some other cases the referenced algorithms learn specific subclasses of the model. For instance, to our knowledge, there are no general algorithms for learning Bipartite BNs. The only related approach is [111], where the Bipartite BN had to satisfy additional constraints, like using a noisy-Or model [128], that each symptom could be only related to a maximum of k different causes, and that the probabilities could not be arbitrary. In that case, their algorithm learns the BN with cost exponential on k, but linear in the number of causes multiplied by the number of symptoms. Similarly, we are not aware of any specific learning algorithm for the class of polytree BNs, other than using a learning algorithm for general BNs trying to constrain the decisions in the learning process to comply with the acyclic property of polytree BNs.\nAmong the fastest learning algorithms are the ones learning rules (either decision trees or directly rule sets) for non-process models. Some of them are even greedy in order to speed up diagnosis in production systems, like MinEntropy [35], which was used at eBay. For process models many fast algorithms\n6\nexist to obtain automata [129] or Petri nets [130], although not very accurate in many cases.\nSome interesting work focuses in creating self-adaptive decision support trees based on streamed data including change detection. Interestingly, this work assumes that the amount of information is so large that the data stream can be considered continuous and in general it is not possible to read data more than once. These assumptions are specially well suited when dealing with IoT systems or other complex systems that change dynamically, which cannot be fully understood in general because of their size and the size of the data generated. Differently from classical methods such as C4.5, these algorithms do not assume that all training data are available simultaneously in memory and deal with change over time. In particular, different methods have been proposed based on Hoeffding Trees or Very Fast Decision Tree method\n(VFDT) [131]. For instance, CVFDT is an adaptive variant of VFDT proposed by Hulten et al [132]. Adaptive Hoeffding Trees were later proposed in [133] for the same purpose, but detecting change and updating the decision tree based exclusively on data analysis. Attempts to parallelize the creation of a decision tree for heavy streams are implemented in Apache Samoa6.\nOther classifier-based RCA models in the table, like Naı̈ve Bayes, SVMs or Neural Nets have very different training costs. While Naı̈ve Bayes can be trained in a single pass of the data, thus, according to [131], it was one of the most widely used learners at Google, the complexity of general (non-linear) SVM classifiers during training is between O(n2) and O(n3), where n is the number of training instances, although with an iterative approximate approach [134] it can go down to O(nr), where r is the number of iterations performed. On the other hand, the general learning problem for Neural nets is NP-complete [135].\nIn this regard, one of the most popular RCA models, BNs do not enjoy any advantage with respect to classifier-based models: learning Bayesian Networks is NP-complete [136]7. However there has been an evolution in the sizes of the systems whose causality can be inferred. For instance, the complexity of the PC algorithm, one of the first algorithms to be used, is O(n log(n)max (pq, p2)), where n is the number of samples, p is the number of variables, and q is the maximal size of the adjacency sets [137], thus worst-case exponential. This made it quite difficult to use it for more than a hundred of variables. More recent approaches, like LGL, based on generating global causality from local causality have raised this limit to one million variables, though running times can range from quadratic time to exponential time depending on parameters provided to the algorithm [138].\nBesides these improvements based on changing the philosophy of the learning algorithm, there have been proposals as well in the line of parallelizing existing algorithms, like parallel PC [115], [116]. Although we only mention this parallel algorithm in the table, the possibility of parallelization has also been explicitly considered for many of the other algorithms [138], [139]8.\nIf the evolution rate of the diagnosed system is high, then this will impose restrictions on the algorithms and/or the models for RCA that can be used. Either the model/learning algorithm allows for incremental changes or it is fast enough to learn the whole model from scratch every time there is a change. For instance algorithms ID5R for decision trees and χ2\ntest on updates for Bayesian Networks can work incrementally. Classifiers in general are not a good option when the\nevolution rate of the diagnosed system is high, specially if the classifier requires lots of instances to attain good results. There are several factors that explain this fact: First, if evolution rate is high, then it is likely that there will be few\n6Apache Samoa: https://samoa.incubator.apache.org 7Learning algorithms for BNs can be classified into constraint-based learners and score-based ones. Although this is sometimes cited as a general rule, in fact [136] shows that the methods based on scores and search are NP-complete\n8In principle any algorithm can be parallelized, albeit with different success in terms of being close to the theoretical maximum speedup.\n7 instances available to train the classifier. Second, assuming that the system is large enough so that fast changes produce a reasonable quantity of diagnosable instances, to correctly label them we will need human diagnosis. Typically this is a slow process (the worth of the automated RCA system is in fact directly related to how time consuming is the manual diagnosis), so throughput could only be increased by having a large pool of humans. Crowdsourcing has been proposed as a solution when large amounts of workers are required, however in this case the task is highly specialized and most of the times involves dealing with sensitive data, thus not very suitable for this approach. A possible way to compensate for scarcity of experts could be systems providing guesses that have to be later on validated by humans. However this approach relies on the assumption that validation is significantly faster than diagnosis, which depends on the feedback provided by the classifier. If just a guessed label is given and nothing else, then the savings would not be as large as if additional information, like the followed reasoning or the facts that support that hypothesis are given to the user.\nIV. INFERENCE IN RCA MODELS\nOnce the model for RCA is available, it is possible to use it to obtain the fault (or faults) that generate a given set of symptoms. This process is called inference or abduction. For classifiers, the process is straightforward as it simply involves the classification of the symptoms using the model. For other models, Tables III and IV show several algorithms available to find the root causes given the set of symptoms.\nIn this table there are two types of algorithms: algorithms that provide equivalent results but with different implementations offering different performances (that is the case of the Rete family of algorithms), and algorithms that have a different concept of what provides a good explanation.\nFor rule sets, the concept of a good explanation is relatively simple: they can provide the conclusion to the user, together with all the rules fired to reach that conclusion from the symptoms. On the other hand, for Bayesian Networks the concept is not so clear. One possibility is to use the computed probabilities of each of the potential causes, their marginals, and take the one with maximum probability (or the group above a given threshold if multiple faults are allowed). However this approach considers the aggregation of all possible worlds compatible with the observations, even if the real world can only be one of those states. In contrast, the Most Probable Explanation (MPE) just outputs the most probable compatible world, that is the assignment of all the variables in the model with highest probability, thus it is most meaningful when the model contains only causes and symptoms and all symptom values are known. The Maximum A Posteriori (MAP) lies somewhere in between the previous two: some of the variables can be abstracted, thus aggregating some of the worlds, before selecting the most probable one. This is done typically with intermediate variables not representing the potential causes or with symptoms for which we have no information, so that only the set of all potential causes is considered.\nRecently new metrics have been proposed, like the Most Reasonable Explanation (MRE) or the Most Inforbable Ex-\nplanation (MIE). All of them try to find, according to some goodness criteria, what subset of variables is the most informative to the user to explain the observations, rather than the user having to provide that set beforehand as in MAP. However this more sophisticated explanations are not needed if the model has clear variables and/or values that are tied to failures, and the number of this potential causes is not very large. For instance, if the model shows the dependency between components and each component has an associated variable with two possible values, ok and failure, given some symptoms, computing the posterior marginal probabilities for each one of the variables in its failure state, or computing the MPE or the MAP would be a good option. The particular selection between Marginals, MEP or MAP depends on whether variables not related to failures have to be abstracted or not and the number of faults that can happen concurrently: marginals would be a better option for single-fault diagnosis, while MPE and MAP are more suited for multiple failure scenarios. On the other hand, if variables represent simply different possible configuration options and none of them is inherently wrong, alternative explanation methods will try to automatically subset which variables had the largest influence in the observed symptoms.\nIn Tables III and IV we have selected the following dimensions to classify the algorithms:\nMultiple Failure If algorithm is capable of finding several concurrent failures (✓) or just returns always at most a single failure (✗). Some methods only work under the assumption that there can be at most k different concurrent failures. This is indicated in the table with a k.\nExact If the algorithm yields the exact answer to the corresponding inference concept (✓) or not (✗). A negative answer could be either because the algorithm uses some heuristic to reduce the search space or because it is an iterative method that converges to the correct answer with time, suitable if an anytime algorithm is required.\nUnknown Symptoms The method is able to diagnose with incomplete symptom information (✓). On the other hand, (✗) indicates that the algorithm requires the values for the symptom variables to be all available, i.e., all symptoms are either positive or negative.\nUnknown Causes The algorithm doesn’t need to know which variables (or variables values) correspond to potential root causes/failures, so it has to evaluate the contribution of each variable in the outcome to decide which subset of variables had a greater impact in generating the symptoms (✓). If potential failure variables need to be known and have identified failure values, this is denoted by (✗).\nNoisy Symptoms Positive/negative symptom information can be wrong and the algorithm can still provide a reasonable (depending on the amount of noise) answer (✓). If algorithm needs precise symptom information, then (✗).\nNoisy Propagation The algorithm is able to consider that failures do not propagate perfectly: there is a chance that errors can be masked in a part of the system and do not propagate to other connected elements (✓).\n8 Adaptive Solving an instance of the problem provides an internal state and a solution that can be reused for similar queries or as new information (evidence) is available.\nIn the tables some of these characteristics have a more subtle interpretation than the binary one that appears in it suggests. For instance, in rule-based approaches, the approximation is exact as long as there are no conflicts (in which case the output will depend on the conflict resolution algorithm used) and typically one would assume that symptoms have to be known, as in propositional logic facts have to be either true or false. However, these systems were used for expert system design for medical diagnosis (the famous MYCIN system) and had to support lack of information. Typically this can be achieved by introducing variables to represent if the value of a specific variable is known or unknown and taking into account specifically this possibility in the rules, albeit at the cost of having more rules in place (see [159] for a comparative survey between rule-based and Bayesian approaches in the context of medical diagnosis).\nIn some cases Tables III and IV only reflect some of the approaches for each type of technique, since, especially for Probabilistic Graphical Models, like Bayesian Networks, the amount of literature is very large. However, the general trend for dealing with complexity is common to all techniques: if the exact algorithm is too expensive (and they quickly become expensive for RCA as even the most basic problem setup based on the set covering algorithm in a bipartite non-probabilistic graph is already NP-hard), then compute an approximation. Approximate algorithms are usually inherently anytime, so they are a reasonable solution for real-time systems, at least if they are able to provide a sufficiently approximated answer in the provided time. For a good survey of anytime algorithms for Bayesian Networks refer to [175].\nThere is a large corpora of work related to resource constrained probabilistic inference9 in Bayesian Networks. For instance [200] reviews a number of strategies that can work real-time, broadly categorized between anytime algorithms [201], which can provide an approximate answer at any time and converge to the correct solution as time goes by, and metalevel reasoners combined with multiple methods. The class of anytime algorithms includes many algorithms for approximate inference, that start from a rough solution and iteratively approach the correct values.\nOne big family of approximate methods are based on stochastic samplings, others are based on local search (e.g., MAP on Bayesian Networks has been approximated using genetic algorithms [202], hill climbing and taboo search [203] or simulated annealing [204] to mention some) and a third class includes model simplification techniques [205], [206] that try to remove least significant elements (nodes/arcs/small probabilities) to obtain a smaller and more tractable system. Despite the approximations, often the resulting problem is still highly complex.\n9The term probabilistic inference has been traditionally abused and can encompass the computation of marginals, MPE and MAP. However in many cases it just applies to the computation of marginals (i.e. posterior marginal probabilities of an arbitrary set of variables given an evidence)\nIn terms of complexity and performance, MPE and MAP, for general BNs, both exact and approximate, are NP-hard [207], [208]. In fact MAP complexity is NPPP complete [209], more specifically O(n exp(wc)), where n is the number of variables and wc is the constrained tree width of the network, a value larger than the regular tree width w. On the other hand, MPE is O(n exp(w)) [81]. For particular types of network topologies, like polytrees, these complexity results somewhat alleviate, as the tree width in a polytree is the maximum number of parents p of any node, so MPE becomes O(n exp(p)) but MAP is still NP-complete [210]. Particular algorithms to compute the MPE are obviously satisfying these complexities: MPE buckettree complexity is O(exp(n)), while Iterative MPE is O(n5) for bipartite graphs and O(n6) if used as an approximate algorithm for polytrees.\nThe approximate approach to compute MAP in [203] simply turns it O(n exp(w)), which is the same complexity as MPE or marginal computation. Nevertheless, exponential worse time computations does not imply that approaches are infeasible in practice. For instance the constraint-based search of [176] guarantees an error ǫ (ǫ < 0.5) in the marginal computation for a fraction 1−Ψ of all possible systems in time O(nc logn) where c ≥ 1 is a constant that depends on ǫ, Ψ and n among other factors [211], and Loopy Belief Propagation can yield close approximations to the marginals in case it converges, each iteration cost being dependent on the diameter of the network, although the actual conditions that make it converge and be accurate are not generally understood [177].\nTwo other approaches by which MAP and MPE computation time can be reduced are either by using some precomputation strategy (like codebooks are precomputations of rules) or by assuming some fact that allows reducing the search space. For the first option Arithmetic Circuits have been proposed (denoted as MPE (AC) and MAP (AC) in Table III). Their evaluation is linear on the circuit size, but they need to be compiled from the Bayesian Network and their size can be exponential in the number of variables (note that ACs can be learned directly, though). However, from a practical point of view they can reuse computations in the circuit, thus they are viewed as one of the fastest methods to compute MPE or MAP. For diagnosis they are good candidates, as they optimize the computation at the expense of having to recompile when the Bayesian Network changes its structure. An exponent of the second strategy, incorporating assumptions that allow reducing complexity, is [188], where they assume that faults are rare and, at any point, there will be at most k simultaneous root causes of the observed symptoms (algorithm MPE (at most k problems) in Table IV). In such a case, the complexity is O(fk) where f is the number of possible root causes.\nThe easiest type of BN (besides the Naı̈ve bayes classifier) is the bipartite BN, in which one set of nodes represents causes and the other nodes represent symptoms. The bipartite model is attractive because more complex propagation models can be reduced to this one (e.g., by graph reduction operations [33]) and it enjoys better computational tractability on approximate algorithms (exact solution is still exponential). However, the conversion from a more complex model to a bipartite model can bloat the final result: for instance in [160] they model\n9\nend-to-end failures in a network of size n and the theoretical complexity of IHU and IHU+ in that example is n4. Although classified in the Bayesian Network approaches because it works on a graph annotated with probabilities [190] can be seen as another example of this effect, although it never builds explicitly the bipartite graph, as it has cost O(n3).\nThe Positive Information algorithm is conceptually similar to the Greedy set covering as both try to select the minimum set of causes that would explain the symptoms, but using different although related heuristics. Since both algorithms are based on heuristics, they provide an approximation to the optimal solution in polynomial time. For [166], if probabilities involved in the computation do not have large differences, the solution is likely to be the optimal one. Algorithms IHU and IHU+ work as well with this bipartite fault propagation model and have complexity O(s · f2), where s is the number of observed symptoms and f is the number of potential faults considered, while algorithms Max-covering and MCA+ have cost O(s2 · f). The SWPM algorithm has even a smaller cost O(s · f), akin of HA and QOP algorithms, which, besides using heuristics to reduce complexity, they also assume a small number of concurrent faults.\nWe have seen in Section III-B that decision tree models are fast to train. There are several approaches by which decision trees can be used for root cause analysis. One is to use them\nas classifiers by training them with observations labeled with a root cause. In such a case, the inference is simply done by traversing the decision tree until a leaf. This has a complexity O(d), where d is the size of the decision tree. There are several factors that affect the size of the tree generated, but in the worst case d is O(n), where n is the number of training samples. However, such degenerated cases are rare, and on average d is O(log2 n).\nAnother possible use of decision trees is to distinguish only between observations with and without symptoms. In this case the decisions taken at each node of the tree leading to leafs with symptom observations are potential causes that could explain the symptoms. This is for instance the approach of the Four heuristics method [35], that applies four different heuristics to simplify the set of decisions. Although no complexity cost is detailed in [35], one of the steps involve merging decisions in the tree, thus we have assumed a simple search strategy which yields a complexity O(d2), but wiser implementations could have better cost. Along these lines, [148] mentions a greedy variant, deployed to diagnose at eBay, that only computes the decision on the decision tree that leads to the largest number of failures without actually building the tree. Since the implementation is not disclosed, the complexity is not available, although it is logical to assume that will be faster than [35]. Finally, [149] tries to find the “easiest”\n10\npath (i.e., changes in the decisions taken in the tree) that could change from observations with symptoms to observations without symptoms. They consider that decisions have an associated cost, so they minimize this quantity, which could yield a solution with more changes than the fewest possible changes. In their case, they are able to automatically compute the costs by considering the frequency by which changes in decision naturally occur. Again, no specific implementation or complexity analysis is given in the paper and a naı̈ve implementation by computing all alternative paths in the tree, computing its cost and then taking the minimum cost one would be O(d2).\nMost classifiers have good inference complexities. For instance the forward propagation in Neural nets highly depends on the architecture of the network, but as a rule of thumb, assuming an all connected network between layers, with s input neurons (one per symptom), f output neurons (one per\nfault), h hidden neurons per layer and a total of l layers, the number of operations is O(max(s, f, h)2 · l). Most classifiers can be trained as single-class or multiple-class classifiers. In the table we assumed that the multiple-class neural network had the same structure as the single-class (for instance a threshold can be used to select the output labels), but other approaches are possible with different costs. On the other hand SVM classification cost is highly dependant on the type of kernel they use. For lineal SVMs, the cost is linear in s, assuming the symptoms are the features used. For kernel SVMs, it can be O(v·s) considering there are v support vectors if the kernel is linear on the number of features, however more complex kernels will increase that cost.\nNot surprisingly, other models that have a reasonably fast inference are the ones based on codebooks, as they can be viewed as a type of nearest neighbor classifier. For instance the complexity of [33] is the complexity of a minimum\n11\nHamming distance decoding. There are several possible implementations ranging from exhaustive comparison with each symptom vector for each fault, with the cost O(f · s) that appears on the table, to Standard Array or Syndrome decoding that are faster but require more space (so they have a larger learning cost). The disadvantage of codebooks is that they consider only a single-failure model. The technique in [140] extended it to multiple faults by allowing considering up to k simultaneous faults, by constructing a codebook with all the possible combinations of faults, thus the cost of inference is the one proportional to a codebook of fk entries that, as we have seen before, depends on the actual implementation of the minimum Hamming distance decoding. The table reports the cost for the basic exhaustive search strategy (O(fk · s)).\nResults on logics are as usual in consonance with their expressiveness: logic abduction in first-order logic is undecidable because the set of explanations may be infinite, thus deciding if an explanation is minimal is generally undecidable [144]. Logic abduction on propositional logic is decidable, but its complexity is large (deciding if there is an explanation is more complex than NP-complete) and depends on the types of clauses allowed (e.g., Horn clauses are easier than the general proposition logic), see [212] for a detailed explanation and [213] for a summary of approaches to tackle that complexity, including heuristics, reduction to QBF, compilation and approximations.\nDempster-Shafer theory allows associating degrees of belief and plausability to logical propositions. These propositions can be combined using the Dempster-Shafer combination rule. However the application of this rule is #P − complete [214], thus the straightforward implementation of the belief update algorithm [145] is exponential on the size of the propositions. Consequently, approximate ways to compute the resulting beliefs have been proposed [146]. For a comprehensive review of alternative approaches and complexities see [215].\nAn interesting family of models that have tractable inference are Sum-Product Networks (closely related to Arithmetic Circuits), that have inference algorithms to compute marginals or MPE linear in the size of the model. Relational SumProduct Networks do not have specific inference algorithms but just work as templates to generate SPNs and then apply the inference algorithms for them.\nMarkov Logic Networks are an alternative formalism to Bayesian Networks. By default they support deduction, but adding some logical formulas it is possible to make them perform abduction as well, as shown by [151] and [152]. These two techniques differ in the amount of nodes they add to the MLN and how they impact the treewidth of the graph, to whom there is an exponential dependency in terms of complexity. Marginal computation can be done using lifted inference [153], [154]. In the latter case they do it by transforming the MLN into a first order deterministic decomposable negation normal form, which has an inference complexity polynomial on the size of the formula. Finally [155] takes the approach from [154] but transforms the inference problem into C++ code that can be compiled, thus has the same asymptotic complexity but can yield notable accelerations from a practical point of view.\nThe Most frugal explanation [189] is a heuristic approach\nto compute MAP. Although its complexity is NPPP PP\n−hard, thus, more complex than MAP itself, it becomes tractable under a set of constraints that is less strict than the one that makes MAP tractable.\nThe Divide and conquer algorithm is a two-phase algorithm in which first the nodes of the graph are aggregated by maximum mutual dependency and then a subset is selected that explains all symptoms and contains a fault. Its complexity is O(n3), where n is the number of nodes in the graph.\nIt is possible to diagnose not only snapshots of symptoms but as well when symptoms appear as sequences or processes. For instance the approach in [66] uses partially stochastic Petri nets to find the most likely sequence of transitions (faults) that happened in the system to produce the observed sequence of symptoms.\nBesides Marginal, MPE and MAP inference concepts, a number of extensions have been proposed, since in many cases they can produce overspecified explanations [216]. For instance explanatory MAP [191] identifies the most relevant variables from a subset of the non-observed variables, where the relevancy can be a metric like the Bayes factor or the Likelihood [191]. [193], [194] follow a similar approach but using a different metric based on the generalized Bayes factor (they compute the ratio between the probability of a particular hypothesis over the probability of the rest of hypotheses). In these cases we have marked these approaches as able to work with unknown causes, as the whole set of non-observed variables could be used as the target set, effectively letting the task of deciding what variables had greater effect on the observations to the algorithm. The search space for MRE is very large, thus exact algorithms have conjectured complexity NPPP−hard [196] and several local search algorithms have been developed [195]–[197].\nOn the other hand the Most Inforbable Explanation [192] tries to introduce a balance between how informative should be an explanation and how probable it is, two requirements that are generally opposed: a more specific information (thus less probable than a generic one) is considered more informative than a general one. Computing MIE has complexity NPPP−hard although it becomes tractable under a similar set of constraints that render MAP tractable.\nBesides these algorithms that provide an assignment of a subset of variables as an output, tree-based approaches have been investigated as well [198], [199]. The idea behind these models is that if the number of variables in the subset is very large, users should have a help to understand the set, by creating a tree in which every node is a variable and sorting them by relevance. At each step, the tree is grown considering the evidence plus the hypothesis previously seen in that branch of the tree. The difference between the methods is the metric used to choose the best variable to add, how they decide that growth should stop, and how they then evaluate the resulting hypotheses.\nSome of these extended explanatory models beyond MAP, that we have collectively labeled as MAP simplification techniques do not have a complexity cost specified as in the previous techniques. Rather they have to evaluate the a posteriori probability of some hypothesis given some evidence\n12\nat each step of their computation. Thus the complexity we give in some cases is in the number of calls to an underlying computational model (a BN, for instance) used to compute these posterior marginals (denoted by the suffix inferences in the table). As we have seen, for many models, the complexity of such a computation might be prohibitive in the general case, a fact that might partially explain why these more advanced explanatory models had not a more widespread adoption.\nThe complexity of Causal explanation trees [199], in terms of number of calls to an inference engine per node in the constructed tree, is O(nd), where n is the number of explanatory variables and d is the average domain size of the variables, e.g., 2 for binary variables. Explanation trees [198] approach is O(n2d2).\nFinally, one way to cope with the complexity of the inference task is to parallelize the algorithms. The survey in [217] covers parallel inference in Bayesian Networks, most of them parallelizing the Junction Tree algorithm. Approaches ranged from multi/many-core algorithms based on OpenMP [218], with cost O(n2w/p + wdwn/p + n log p), where n is the number of nodes in the Bayesian network, w is the clique width, d is the number of possible states of the variables and p is the number of processors, and scalable over 1 ≤ p ≤ nw/ logn, to multi-core with pthreads [219], GPUs [220], hybrid CPU-GPGPU systems [221] or FPGAs [222]."
    }, {
      "heading" : "V. CONCLUSION",
      "text" : "In this survey we have reviewed the models and algorithms to perform inference that have been used for Root-Cause analysis. The survey shows a wide spectrum of techniques with the usual trade-off between tractability and expressiveness. As future work we plan to extend this survey to consider specifically RCA for Big Data, by providing a users’ guide to RCA in this setting."
    }, {
      "heading" : "ACKNOWLEDGMENT",
      "text" : "The authors would like to thank LeanBigData (FP7-619606) project."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Automation and computer intelligence to support<lb>complex human decisions becomes essential to manage large and<lb>distributed systems in the Cloud and IoT era. Understanding<lb>the root cause of an observed symptom in a complex system<lb>has been a major problem for decades. As industry dives into<lb>the IoT world and the amount of data generated per year<lb>grows at an amazing speed, an important question is how to<lb>find appropriate mechanisms to determine root causes that can<lb>handle huge amounts of data or may provide valuable feedback<lb>in real-time. While many survey papers aim at summarizing<lb>the landscape of techniques for modelling system behavior and<lb>infering the root cause of a problem based in the resulting<lb>models, none of those focuses on analyzing how the different<lb>techniques in the literature fit growing requirements in terms<lb>of performance and scalability. In this survey, we provide a<lb>review of root-cause analysis, focusing on these particular aspects.<lb>We also provide guidance to choose the best root-cause analysis<lb>strategy depending on the requirements of a particular system<lb>and application.",
    "creator" : "LaTeX with hyperref package"
  }
}