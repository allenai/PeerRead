{"conference": "ICLR 2017 conference submission", "title": "Learning Visual Servoing with Deep Features and Fitted Q-Iteration", "abstract": "Visual servoing involves choosing actions that move a robot in response to observations from a camera, in order to reach a goal configuration in the world. Standard visual servoing approaches typically rely on manually designed features and analytical dynamics models, which limits their generalization capability and often requires extensive application-specific feature and model engineering. In this work, we study how learned visual features, learned predictive dynamics models, and reinforcement learning can be combined to learn visual servoing mechanisms. We focus on target following, with the goal of designing algorithms that can learn a visual servo using low amounts of data of the target in question, to enable quick adaptation to new targets. Our approach is based on servoing the camera in the space of learned visual features, rather than image pixels or manually-designed keypoints. We demonstrate that standard deep features, in our case taken from a model trained for object classification, can be used together with a bilinear predictive model to learn an effective visual servo that is robust to visual variation, changes in viewing angle and appearance, and occlusions. A key component of our approach is to use a sample-efficient fitted Q-iteration algorithm to learn which features are best suited for the task at hand. We show that we can learn an effective visual servo on a complex synthetic car following benchmark using just 20 training trajectory samples for reinforcement learning. We demonstrate substantial improvement over a conventional approach based on image pixels or hand-designed keypoints, and we show an improvement in sample-efficiency of more than two orders of magnitude over standard model-free deep reinforcement learning algorithms. Videos are available at", "histories": [], "reviews": [{"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "All reviewers viewed the paper favourably, although there were some common criticisms. In particular, the demonstration would be more convincing on a more difficult task, and this seems like an intermediate step on the way to an end-to-end solution. There were also questions of being able to reproduce the results. I would strongly recommend that the authors take this suggestions into account.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Good approach for visual servoing, but could use better experimentation", "APPROPRIATENESS": 2, "RECOMMENDATION_UNOFFICIAL": 2, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper proposes a novel approach for learning visual servoing based on Q-iteration. The main contributions of the paper are:\n\n1. Bilinear dynamics model for predicting next frame (features) based on action and current frame\n2. Formulation of servoing with a Q-function that learns weights for different feature channels\n3. An elegant method for optimizing the Bellman error to learn the Q-function\n\nPros:\n+ The paper does a good job of exploring different ways to connect the action (u_t) and frame representation (y_t) to predict next frame features (y_{t+1}). They argue in favour of a locally connected bilinear model which strikes the balance between computation and expressive ability. \n\nCons:\n- While, sec. 4 makes good arguments for different choices, I would have liked to see more experimental results comparing the 3 approaches: fully connected, convolutional and locally connected dynamics.\n\nPros: \n+ The idea of weighting different channels to capture the importance of obejcts in different channels seems more effective than treating errors across all channels equally. This is also validated experimentally, where unweighted performance suffers consistently.\n+ Solving the Bellman error is a difficult problem in Q-learning approaches. The current paper presents a solid optimization scheme based on the key-observation that scaling Q-function parameters does not affect the best policy chosen. This enables a more elegant FQI approach as opposed to typical optimization schemes which (c_t + \\gamma min_u Q_{t+1}) fixed. \n\nCons:\n- However, I would have liked to see the difference between FQI and such an iterative approach which holds the second term in Eq. 5 fixed.\n\nExperimental results:\n- Overall, I find the experimental results unsatisfying given the small scale and toy simulations. However, the lack of benchmarks in this domain needs to be recognized.\n- Also, as pointed out in pre-review section, the idea of modifying the VGG needs to be experimentally validated. In its current form, it is not clear whether the modified VGG would perform better than the original version.\n\nOverall, the contribution of the paper is solid in terms of technical novelty and problem formulations. However, the paper could use stronger experiments as suggested to earlier to bolster its claims.\n", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "CLARITY": 2, "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "TITLE": "Excellent technical paper, but emphasis on control rather than representation", "comments": "This paper investigates the benefits of visual servoing using a learned\nvisual representation. The authors  propose to first learn an action-conditional\nbilinear model of the visual features (obtained from a pre-trained VGG net) from\nwhich a policy can be derived using a linearization of the dynamics. A multi-scale,\nmulti-channel and locally-connected variant of the bilinear model is presented.\nSince the bilinear model only predicts the dynamics one step ahead, the paper\nproposes a weighted objective which incorporates the long-term values of the\ncurrent policy. The evaluation problem is addressed using a fitted-value approach.\n\nThe paper is well written, mathematically solid, and conceptually exhaustive.\nThe experiments also demonstrate the benefits of using a value-weighted objective\nand is an important contribution of this paper. This paper also seems to be the\nfirst to outline a trust-region fitted-q iteration algorithm. The use of\npre-trained visual features is also shown to help, empirically, for generalization.\n\nOverall, I recommend this paper as it would benefit many researchers in robotics.\nHowever, in the context of this conference, I find the contribution specifically on\nthe \"representation\" problem to be limited. It shows that a pre-trained VGG\nrepresentation is useful, but does not consider learning it end-to-end. This is not\nto say that it should be end-to-end, but proportionally speaking, the paper\nspends more time on the control problem than the representation learning one.\nAlso, the policy representation is fixed and the values are approximated\nin linear form using problem-specific features. This doesn't make the paper\nless valuable, but perhaps less aligned with what I think ICLR should be about.\n", "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "16 Dec 2016 (modified: 17 Dec 2016)", "REVIEWER_CONFIDENCE": 3}, {"APPROPRIATENESS": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Interesting reinforcement learning paper needing maybe a little bit more work on improving the benchmark and exposition", "comments": "1) Summary\n\nThis paper proposes to tackle visual servoing (specifically target following) using spatial feature maps from convolutional networks pre-trained on general image classification tasks. The authors combine bilinear models of one-step dynamics of visual feature maps at multiple scales with a reinforcement learning algorithm to learn a servoing policy. This policy is learned by minimizing a regularized weighted average of distances to features predicted by the aforementioned model of visual dynamics.\n\n2) Contributions\n\n+ Controlled experiments in simulation quantifying the usefulness of pre-trained deep features for visual servoing.\n+ Clear performance benefits with respect to many sensible baselines, including ones using ground truth bounding boxes.\n+ Principled learning of multi-scale visual feature weights with an efficient trust-region fitted Q-iteration algorithm to handle the problem of distractors.\n+ Good sample efficiency thanks to the choice of Q-function approximator and the model-based one-step visual feature dynamics.\n+ Open source virtual city environment to benchmark visual servoing.\n\n3) Suggestions for improvement\n\n- More complex benchmark:\nAlthough the environment is not just a toy synthetic one, the experiments would benefit greatly from more complex visual conditions (clutter, distractors, appearance and motion variety, environment richness and diversity, etc). At least, the realism and diversity of object appearances could be vastly improved by using a larger number of 3D car models, including more realistic and diverse ones that can be obtained from Google SketchUp for instance, and populating the environment with more distractor cars (in traffic or parked). This is important as the main desired quality of the approach is robustness to visual variations.\n\n- End-to-end and representation learning:\nAlthough the improvements are already significant in the current synthetic experiments, it would be interesting to measure the impact of end-to-end training (i.e. also fine-tuning the convnet), as it is possibly needed for better generalization in more challenging visual conditions. It would also allow to measure the benefit of deep representation learning for visual servoing, which would be relevant to ICLR (there is no representation learning so far, although the method can be straightforwardly adapted as the authors mention briefly).\n\n- Reproducibility:\nThe formalism and algorithms are clearly explained, but there is a slightly overwhelming mass of practical tricks and implementation details described with varying levels of details throughout the paper and appendix. Grouping, simplifying, or reorganizing the exposition of these implementation details would help, but a better way would probably consist in only summarizing the most important ones in section and link to an open source implementation of the method for completeness.\n\n- Typos:\np.2: \"learning is a relative[ly] recent addition\"\np.2: \"be applied [to] directly learn\"\n\n4) Conclusion\n\nIn spite of the aforementioned limits of the experiments, this paper is interesting and solid, in part thanks to the excellent reply to the pre-review questions and the subsequent improved revision. This leads me to believe the authors are more than capable of following to a significant extent the aforementioned suggestions for improvement, thus leading to an even better paper.", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Clarifications on modified VGG used for feature dynamics", "APPROPRIATENESS": 2, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "RECOMMENDATION_UNOFFICIAL": 2, "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "08 Dec 2016", "CLARITY": 2}, {"TITLE": "Sample complexity, relation to attention, formulation", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "", "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "03 Dec 2016"}, {"APPROPRIATENESS": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Pre-review questions", "comments": "", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "30 Nov 2016", "CLARITY": 5}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "All reviewers viewed the paper favourably, although there were some common criticisms. In particular, the demonstration would be more convincing on a more difficult task, and this seems like an intermediate step on the way to an end-to-end solution. There were also questions of being able to reproduce the results. I would strongly recommend that the authors take this suggestions into account.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Good approach for visual servoing, but could use better experimentation", "APPROPRIATENESS": 2, "RECOMMENDATION_UNOFFICIAL": 2, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper proposes a novel approach for learning visual servoing based on Q-iteration. The main contributions of the paper are:\n\n1. Bilinear dynamics model for predicting next frame (features) based on action and current frame\n2. Formulation of servoing with a Q-function that learns weights for different feature channels\n3. An elegant method for optimizing the Bellman error to learn the Q-function\n\nPros:\n+ The paper does a good job of exploring different ways to connect the action (u_t) and frame representation (y_t) to predict next frame features (y_{t+1}). They argue in favour of a locally connected bilinear model which strikes the balance between computation and expressive ability. \n\nCons:\n- While, sec. 4 makes good arguments for different choices, I would have liked to see more experimental results comparing the 3 approaches: fully connected, convolutional and locally connected dynamics.\n\nPros: \n+ The idea of weighting different channels to capture the importance of obejcts in different channels seems more effective than treating errors across all channels equally. This is also validated experimentally, where unweighted performance suffers consistently.\n+ Solving the Bellman error is a difficult problem in Q-learning approaches. The current paper presents a solid optimization scheme based on the key-observation that scaling Q-function parameters does not affect the best policy chosen. This enables a more elegant FQI approach as opposed to typical optimization schemes which (c_t + \\gamma min_u Q_{t+1}) fixed. \n\nCons:\n- However, I would have liked to see the difference between FQI and such an iterative approach which holds the second term in Eq. 5 fixed.\n\nExperimental results:\n- Overall, I find the experimental results unsatisfying given the small scale and toy simulations. However, the lack of benchmarks in this domain needs to be recognized.\n- Also, as pointed out in pre-review section, the idea of modifying the VGG needs to be experimentally validated. In its current form, it is not clear whether the modified VGG would perform better than the original version.\n\nOverall, the contribution of the paper is solid in terms of technical novelty and problem formulations. However, the paper could use stronger experiments as suggested to earlier to bolster its claims.\n", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "CLARITY": 2, "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "TITLE": "Excellent technical paper, but emphasis on control rather than representation", "comments": "This paper investigates the benefits of visual servoing using a learned\nvisual representation. The authors  propose to first learn an action-conditional\nbilinear model of the visual features (obtained from a pre-trained VGG net) from\nwhich a policy can be derived using a linearization of the dynamics. A multi-scale,\nmulti-channel and locally-connected variant of the bilinear model is presented.\nSince the bilinear model only predicts the dynamics one step ahead, the paper\nproposes a weighted objective which incorporates the long-term values of the\ncurrent policy. The evaluation problem is addressed using a fitted-value approach.\n\nThe paper is well written, mathematically solid, and conceptually exhaustive.\nThe experiments also demonstrate the benefits of using a value-weighted objective\nand is an important contribution of this paper. This paper also seems to be the\nfirst to outline a trust-region fitted-q iteration algorithm. The use of\npre-trained visual features is also shown to help, empirically, for generalization.\n\nOverall, I recommend this paper as it would benefit many researchers in robotics.\nHowever, in the context of this conference, I find the contribution specifically on\nthe \"representation\" problem to be limited. It shows that a pre-trained VGG\nrepresentation is useful, but does not consider learning it end-to-end. This is not\nto say that it should be end-to-end, but proportionally speaking, the paper\nspends more time on the control problem than the representation learning one.\nAlso, the policy representation is fixed and the values are approximated\nin linear form using problem-specific features. This doesn't make the paper\nless valuable, but perhaps less aligned with what I think ICLR should be about.\n", "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "16 Dec 2016 (modified: 17 Dec 2016)", "REVIEWER_CONFIDENCE": 3}, {"APPROPRIATENESS": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Interesting reinforcement learning paper needing maybe a little bit more work on improving the benchmark and exposition", "comments": "1) Summary\n\nThis paper proposes to tackle visual servoing (specifically target following) using spatial feature maps from convolutional networks pre-trained on general image classification tasks. The authors combine bilinear models of one-step dynamics of visual feature maps at multiple scales with a reinforcement learning algorithm to learn a servoing policy. This policy is learned by minimizing a regularized weighted average of distances to features predicted by the aforementioned model of visual dynamics.\n\n2) Contributions\n\n+ Controlled experiments in simulation quantifying the usefulness of pre-trained deep features for visual servoing.\n+ Clear performance benefits with respect to many sensible baselines, including ones using ground truth bounding boxes.\n+ Principled learning of multi-scale visual feature weights with an efficient trust-region fitted Q-iteration algorithm to handle the problem of distractors.\n+ Good sample efficiency thanks to the choice of Q-function approximator and the model-based one-step visual feature dynamics.\n+ Open source virtual city environment to benchmark visual servoing.\n\n3) Suggestions for improvement\n\n- More complex benchmark:\nAlthough the environment is not just a toy synthetic one, the experiments would benefit greatly from more complex visual conditions (clutter, distractors, appearance and motion variety, environment richness and diversity, etc). At least, the realism and diversity of object appearances could be vastly improved by using a larger number of 3D car models, including more realistic and diverse ones that can be obtained from Google SketchUp for instance, and populating the environment with more distractor cars (in traffic or parked). This is important as the main desired quality of the approach is robustness to visual variations.\n\n- End-to-end and representation learning:\nAlthough the improvements are already significant in the current synthetic experiments, it would be interesting to measure the impact of end-to-end training (i.e. also fine-tuning the convnet), as it is possibly needed for better generalization in more challenging visual conditions. It would also allow to measure the benefit of deep representation learning for visual servoing, which would be relevant to ICLR (there is no representation learning so far, although the method can be straightforwardly adapted as the authors mention briefly).\n\n- Reproducibility:\nThe formalism and algorithms are clearly explained, but there is a slightly overwhelming mass of practical tricks and implementation details described with varying levels of details throughout the paper and appendix. Grouping, simplifying, or reorganizing the exposition of these implementation details would help, but a better way would probably consist in only summarizing the most important ones in section and link to an open source implementation of the method for completeness.\n\n- Typos:\np.2: \"learning is a relative[ly] recent addition\"\np.2: \"be applied [to] directly learn\"\n\n4) Conclusion\n\nIn spite of the aforementioned limits of the experiments, this paper is interesting and solid, in part thanks to the excellent reply to the pre-review questions and the subsequent improved revision. This leads me to believe the authors are more than capable of following to a significant extent the aforementioned suggestions for improvement, thus leading to an even better paper.", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Clarifications on modified VGG used for feature dynamics", "APPROPRIATENESS": 2, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "RECOMMENDATION_UNOFFICIAL": 2, "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "08 Dec 2016", "CLARITY": 2}, {"TITLE": "Sample complexity, relation to attention, formulation", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "", "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "03 Dec 2016"}, {"APPROPRIATENESS": 3, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Pre-review questions", "comments": "", "SOUNDNESS_CORRECTNESS": 5, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "30 Nov 2016", "CLARITY": 5}], "authors": "Alex X. Lee, Sergey Levine, Pieter Abbeel", "accepted": true, "id": "320"}