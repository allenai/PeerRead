{
  "name" : "1510.07586.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "raosudha@cs.umd.edu,", "yogarshi@cs.umd.edu,", "hal@cs.umd.edu,", "resnik@umd.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n51 0.\n07 58\n6v 1\n[ cs\n.C L\n] 2\n6 O\nct 2\n01 5\nParser for Abstract Meaning Representation using Learning to Search\nSudha Rao1,3∗, Yogarshi Vyas1,3∗, Hal Daumé III1,3, Philip Resnik2,3 1Computer Science, 2Linguistics, 3UMIACS\nUniversity of Maryland raosudha@cs.umd.edu, yogarshi@cs.umd.edu, hal@cs.umd.edu, resnik@umd.edu\nAbstract\nWe develop a novel technique to parse English sentences into Abstract Meaning Representation (AMR) using SEARN, a Learning to Search approach, by modeling the concept and the relation learning in a unified framework. We evaluate our parser on multiple datasets from varied domains and show an absolute improvement of 2% to 6% over the state-of-the-art. Additionally we show that using the most frequent concept gives us a baseline that is stronger than the state-of-the-art for concept prediction. We plan to release our parser for public use."
    }, {
      "heading" : "1 Introduction",
      "text" : "Abstract Meaning Representation (Banarescu et al., 2013) is a semantic representation which is a rooted, directed, acyclic graph where the nodes represent concepts (words, PropBank (Palmer et al., 2005) framesets or special keywords) and the edges represent relations between these concepts. Figure 1 shows the complete AMR for a sample sentence.\nThe key motivation behind developing AMR was to have a comprehensive and broad-coverage semantic formalism that puts together the best insights from a variety of semantic annotations (like named entities, co-reference, semantic relations, discourse connectives, temporal entities, etc.) in a way that would enable it to have the same kind of impact that syntactic treebanks have on natural language processing tasks. Currently, there are approximately 20,000 sentences which have been annotated with their AMRs, but for such a representation to be useful for almost any NLP task, a larger set of annotations would be needed. Algorithms that can perform automatic semantic pars-\n∗The first two authors contributed equally to this work.\ning of sentences into AMR can help alleviate the problem of paucity of manual annotations.\nAutomatic semantic parsing for AMR is still in a nascent stage. There have been two published approaches for automatically parsing English sentences into AMR. Flanigan et al. (2014) use a semi-Markov model to first identify the concepts, and then find a maximum spanning connected subgraph that defines the relations between these concepts. The other approach (Wang et al., 2015) uses a transition-based algorithm to convert the dependency representation of a sentence to its AMR.\nIn this work, we develop a novel technique for AMR parsing that uses SEARN (Daumé III et al., 2009), a Learning to Search (L2S) algorithm. SEARN and other L2S algorithms have proven to be highly effective for tasks like part-of-speech tagging, named entity recognition (Daumé III et al., 2014), and for even more complex structured prediction tasks like coreference resolution (Ma et al., 2014) and dependency parsing (He et al., 2013). Using SEARN allows us\nto model the learning of concepts and relations in a unified framework which aims to minimize the loss over the entire predicted structure, as opposed to minimizing the loss over concepts and relations in two separate stages, as is done by Flanigan et al (2014).\nThere are three main contributions of this work. Firstly, we provide a novel algorithm based on SEARN to parse sentences into AMRs. Additionally, our parser extracts possible ‘candidates’ for the right concepts and relations from the entire training data, but only uses smaller sentences to train the learning algorithm. This is important since AMR annotations are easier to obtain for smaller sentences. Secondly, we evaluate our parser on datasets from various domains, unlike previous works, which have been restricted to newswire text. We observe that our parser performs better than the existing state-of-the-art parser, with an absolute improvement of 2 to 6 % over the different datasets. Finally, we show that using the most frequently aligned concept for each word in the sentence (as seen in the training data) as the predicted concept, proves to be a strong baseline for concept prediction. This baseline does better than existing approaches, and we show that our parser performs as well as the baseline at this part of the task in some datasets, and\neven better in some others. The rest of this paper is organized as follows. In the next section, we briefly review SEARN and explain its various components with respect to our AMR parsing task. Section 3 describes our main algorithm along with the strategies we use to deal with the large search space of the search problem. We then describe our experiments and results (Section 4)."
    }, {
      "heading" : "2 Using SEARN",
      "text" : "The task of generating an AMR given a sentence is a structured prediction task where the structure that we are trying to predict is a singly rooted, connected directed graph with concepts (nodes) and relations (edges). In this work, we design an AMR parser that learns to predict this structure using SEARN. SEARN solves complex structured prediction problems by decomposing it into classification problems. It does so by decomposing the structured output, y, into a sequence of decisions y1, y2, ..., ym and then using a classifier to make predictions for each component in turn, allowing for dependent predictions. We decompose the AMR prediction problem into the three problems of predicting the concepts of the AMR, predicting the root and then predicting the relations between the predicted concepts (explained in more\ndetail under section 3). Below, we explain how we use SEARN, with reference to a running example in Figure 2.\nSEARN works on the notion of a policy which can be defined as “what is the best next action (yi) to take” in a search space given the current state (s = (x, y1, y2, .., yi−1)), where x is the input. For our problem, a state during the concept prediction phase is defined as the concepts predicted for a part of the input sentence. Similarly, a state during the relation prediction phase is defined as the set of relations predicted for certain pairs of concepts obtained during the concept prediction stage. (In Figure 2a (concept prediction), the current state corresponds to the concepts {‘i’, ‘read01’, ‘book’} predicted for a part of the sentence. In Figure 2c (relation prediction), the current state corresponds to the relation ‘ARG0’ predicted between ‘r’ and ‘i’ )\nAt training time, SEARN operates in an iterative fashion. It starts with some initial policy and given an input x, makes a prediction y = y1, y2, ..., ym using the current policy. For each prediction yi it generates a set of cost-sensitive multi-class classification examples each of which correspond to a possible action (a) the algorithm can take given the current state. Each example can be defined using local features and features that depend on previous predictions. The possible set of next actions in our concept prediction phase corresponds to the set of possible concepts the next word can take. The possible set of next actions in our relation prediction phase corresponds to the set of possible relations the next pair of concepts can take. (In Figure 2a (concept prediction), the next action is assigning one of {‘call-01’, ‘called’, NULL} to the word ‘called’. In Figure 2c (relation prediction), the next action is assigning one of {‘ARG1’, ‘mod’, NO-EDGE} to the pair of concept ‘b’ and ‘i’).\nDuring training, SEARN has access to an “oracle” policy which gives the true best action (a∗) given the current state . Our oracle returns the correct concept and relation labels in the concept prediction and relation prediction phase respectively. (In Figure 2a (concept prediction), the oracle will return NULL and in Figure 2c (relation prediction), the oracle will return NO-EDGE). SEARN then calculates the loss between a and a∗ using a prespecified loss function. It then computes a new policy based on this loss and interpolates it with\nthe current policy to get an updated policy, before moving on to the next iteration.\nAt test time, predictions are made greedily using the policy learned during training time."
    }, {
      "heading" : "3 Methodology",
      "text" : ""
    }, {
      "heading" : "3.1 Learning technique",
      "text" : "Algorithm 1\n1: for each span si do 2: ci = predict concept(si) 3: end for 4: croot = predict root([c1, ..., cn]) 5: for each concept ci do 6: for each j < i do 7: r(i,j) = predict relation(ci, cj) 8: r(j,i) = predict relation(cj , ci) 9: end for\n10: end for\nWe use SEARN as described in section 2 to learn a model that can successfully predict the AMR y for a sentence x. The sentence x is composed of a sequence of spans (s1, s2, ..., sn) each of which can be a single word or a span of words (We describe how we go from a raw sentence to a sequence of spans in Section 4.2). Given that our input has n spans, we first decompose the structure into a sequence of n2 + 1 predictions D = (C,ROOT,R), where\nC = c1, c2, ..., cn - where ci is the concept predicted for span si\nROOT is the decision of choosing one of the predicted concepts as the root (croot) of the AMR\nR = r2,∗, r∗,2, r3,∗, r∗,3, ..., rn,∗, r∗,n - where ri,∗ are the predictions for the directed relations from ci to cj ∀j < i, and r∗,i are the predictions for the directed relations from cj to ci ∀j < i. We constrain our algorithm to not predict any incoming relations to croot.\nDuring training time, the possible set of actions for each prediction is given by the k-best list, which we will describe in Section 3.2. We use Hamming Loss as our loss function. Under Hamming Loss, the oracle policy is simply choosing the right action for each prediction. Since this loss is defined on the entire predicted output, the model learns to minimize the loss for concepts and relations jointly.\nAlgorithm 1 describes the sequence of predictions to be made in our problem. We learn\nthree different policies corresponding to each of the functions predict concept, predict root and predict relation. The learner in each stage uses features that depend on predictions made in the previous stages. Tables 1, 2 and 3 describe the set of features we use for the concept prediction, relation prediction and root prediction stages respectively.\n3.2 Selecting k-best lists\nFor predicting the concepts and relations using SEARN, we need a candidate-list (possible set of actions) to make predictions from.\nConcept candidates: For a span si, the candidate-list of concepts, CL-CONsi is the set of\nall concepts that were aligned to si in the training data. If si has not been seen in the training data, CL-CONsi consists of the lemmatized span, PropBank frames (for verbs) obtained using the Unified Verb Index (Schuler, 2005) and the NULL concept.\nRelation candidates: The candidate list of relations for a relation from concept ci to concept cj , CL-RELij, is the union of the following three sets:\n• pairwisei,j - All directed relations from ci to cj when ci and cj occurred in the same AMR, • outgoingi - All outgoing relations from ci, and • incomingj - All incoming relations into cj .\nIn the case when both ci and cj have not been\nseen in the training data, CL-RELij consists of all relations seen in the training data. In both cases, we also provide an option NO-EDGE which indicates that there is no relation between ci and cj ."
    }, {
      "heading" : "3.3 Pruning the search space",
      "text" : "To prune the search space of our learning task, and to improve the quality of predictions, we use two observations about the nature of the edges of the AMR of a sentence, and its dependency tree, within our algorithm.\nFirst, we observe that a large fraction of the edges in the AMR for a sentence are between concepts whose underlying spans (more specifically, the words in these underlying spans) are within two edges of each other in the dependency tree of the sentence. Thus, we refrain from calling the predict relation function in Algorithm 1 between concepts ci and cj if each word in wi is three or more edges away from all words in wj in the dependency tree of the sentence under consideration, and vice versa. This implies that there will be no relation rij in the predicted AMR of that sentence. This doesn’t affect the number of calls to predict relation in the worst case (n2 − n, for a sentence with n spans), but practically, the number of calls are far fewer. Also, to make sure that this method does not filter out too many AMR edges, we calculated the percentage of AMR edges that are more than two edges away in dependency tree. We found this number to be only about 5% across all our datasets.\nSecondly, and conversely, we observe that for a large fraction of words which have a dependency edge between them, there is an edge in the AMR between the concepts corresponding to those two words. Thus, when we observe two concepts ci and cj which satisfy this property, we force our predict relation function to assign a relation rij that is not NULL."
    }, {
      "heading" : "3.4 Training on smaller sentences",
      "text" : "For a sentence containing n spans, Algorithm 1 has to make n2 predictions in the worst case, and this can be inhibitive for large values of n. To deal with this, we use a parameter to indicate a cut-off on the length of a sentence (C), and only use sentences whose length (number of spans) is less than or equal to C . This parameter can be varied based on the size of the training data and the distribution of the length of the sentences in the training data. Setting a higher values of C will\ncause the model to use more sentences for training, but spend longer time, whereas lower values will train quickly on fewer sentences. In our experiments, a C-value between 10 and 15 gave us the best balance between training time, and number of examples considered."
    }, {
      "heading" : "4 Experiments and Results",
      "text" : ""
    }, {
      "heading" : "4.1 Dataset and Method of Evaluation",
      "text" : "We use the publicly available AMR Annotation Release 1.0 (LDC2014T12) corpus for our experiments. This corpus consists of datasets from varied domains such as online discussion forums, blogs, and newswire, with about 13,000 sentenceAMR pairs. Previous works have only used one of these datasets for evaluation (proxy), but we evaluate our parser on all of them. Additionally, we also use the freely available AMRs for The Little Prince, (lp) 1 which is from a more literary domain. All datasets have a pre-specified training and test split (Table 4).\nAs stated earlier (Sections 3.2 and 3.4), we use the entire training set to extract the candidate lists for concept prediction and relation prediction, but train our learning algorithm on only a subset of the sentence-AMR pairs in the training data, which is obtained by selecting sentences having less than a fixed number of spans (C , set to 10 for all our experiments). Table 4 also mentions the number of sentences in each training dataset that are of length ≤ C (column Training (≤ C)).\nWe compare our results against those of the JAMR parser 2 of Flanigan et. al (2014) 3. We run the parser with the configuration that is specified to give the best results.\n1http://amr.isi.edu/download.html 2https://github.com/jflanigan/jamr 3The transition-based parser by Wang et al. () is newer, but the latest release of JAMR performs better, hence we do not compare against the former.\nThe evaluation of predicted AMRs is done using Smatch (Cai and Knight, 2013) 4, which compares two AMRs using precision, recall and F1. Additionally, we also evaluate how good we are at predicting the concepts of the AMRs, by calculating precision, recall and F1 against the gold-concepts that are aligned to the induced spans during test time."
    }, {
      "heading" : "4.2 Preprocessing",
      "text" : "JAMR Aligner: The training data for AMR parsing consists of sentences paired with corresponding AMRs. To convert a raw sentence into a sequence of spans (as required by our algorithm), we obtain alignments between words in the sentence and concepts in the AMR using the automatic aligner of JAMR. The alignments obtained can be of three types (Examples refer to Figure 1):\n• A single word aligned to a single concept: E.g., word ‘read’ aligned to concept ‘read01’. • Span of words aligned to a graph fragment: E.g., span ‘Stories from Nature’ aligned to the graph fragment rooted at ’name’. This usually happens for named entities and multiword expressions such as those related to date and time. • A word aligned to NULL concept: Most function words like ‘about’, ‘a’, ‘the’, etc are not aligned to any particular concept. These are considered to be aligned to the NULL concept.\nForced alignments: The JAMR aligner does not align all concepts in a given AMR to a span in the sentence. We use a heuristic to forcibly align these leftover concepts and improve the quality of alignments. For every unaligned concept, we count the number of times an unaligned word occurs in the same sentence with the unaligned concept across all training examples. We then align every leftover concept in every sentence with the unaligned word in the sentence with which it has maximally coocurred.\nSpan identification: During training time, the aligner takes in a sentence and its AMR graph and splits each sentence into spans that can be aligned to the concepts in the AMR. However, during test time, we do not have access to the AMR graph. Hence, given a test sentence, we need to split the\n4http://amr.isi.edu/download/smatch-v2.0.tar.gz\nsentence into spans, on which we can predict concepts. We consider each word as a single span except for two cases. First, we detect possible multiword spans corresponding to named entities, using a named entity recognizer (Lafferty et al., 2001). Second, we use some basic regular expressions to identify time and date expressions in sentences."
    }, {
      "heading" : "4.3 Experiments",
      "text" : "To train our model, we use SEARN as implemented in the Vowpal Wabbit machine learning library (Langford et al., 2007; Daumé III et al., 2014).\nFor each dataset, we run three kinds of experiments. They differ in how they get the concepts during test time. All of them use the approach described in Section 3.1 for predicting the relations.\n• Oracle Concept - Use the true concept aligned with each span. • 1-Best Concept - Use the concept with which the span was most aligned in the training data. • Fully automatic - Use the concepts predicted using the approach described in Section 3.1."
    }, {
      "heading" : "4.4 Connectivity",
      "text" : "Algorithm 1 does not place explicit constraints on the structure of the AMR. Hence, the predicted output can have disconnected components. Since we want the predicted AMR to be connected, we connect the disconnected components (if any) using the following heuristic. For each component, we find its roots (i.e. concepts with no incoming relations). We then connect the components together by simply adding an edge from our predicted root croot to each of the component roots. To decide what edge to use between our predicted root croot and the root of a component, we get the k-best list (as described in section 3.2) between them and choose the most frequent edge from this list."
    }, {
      "heading" : "4.5 Acyclicity",
      "text" : "The post-processing step described in the previous section ensures that the predicted AMRs are rooted, connected, graphs. However, an AMR, by definition, is also acyclic. We do not model this constraint explicitly within our learning framework. Despite this, we observe that only a very small number of AMRs predicted using our fully automatic approach have cycles in them. Out of the total 1,444 AMRs predicted in all test sets, less\nthan 5% have cycles in them. Besides, almost all cycles that are predicted consist of only two nodes, i.e. both rij and rji have non-NO-EDGE values for concepts ci and cj . To get an acyclic graph, we can greedily select one of rij or rji, without any loss in parser performance."
    }, {
      "heading" : "4.6 Results",
      "text" : "Table 5 shows the result of running our parser on all five datasets. By running our fully automatic approach, we get an absolute improvement of about 2% to 6% on most datasets as compared to JAMR. Surprisingly, we observe a large improvement of 21% on the online discussion forum dataset (dfa). In all cases, our results indicate a more balanced output in terms of precision and recall as compared to JAMR, with consistently higher recall.\nIt should be noted that selecting the 1-best concept also gives better results than JAMR. This indicates that the 1-best baseline is strong, and possibly, not very easy to beat. To reinforce this, we evaluate our concept predictions separately. The results are shown in Table 6. First, observe that going from the fully learned concept prediction to the 1-best concept shows only a small (or in some cases, no) drop in performance. Second, note that we show a consistent absolute improvement of 10% to 12% over the concept prediction\nresults of JAMR. As in the full prediction case, we observe a large performance increase (27%) on the online discussion forum dataset."
    }, {
      "heading" : "5 Related work",
      "text" : "Semantic representations and techniques for parsing them have a rich and varied history. AMR itself is based on propositional logic and neoDavidsonian semantics (Davidson, 1967). AMR is not intended to be an interlingua, but due to the various assumptions made while creating an AMR (dropping tense, function words, morphology, etc.), it does away with languagespecific idiosyncrasies and interlingual representations (Dorr, 1992) are thus, important predecessors to AMR.\nLike the task of AMR parsing, there have been various attempts to parse sentences into a logical form, given raw sentences annotated with such forms (Kate et al., 2005; Wong and Mooney, 2006). The work by Zettlemoyer and Collins (Zettlemoyer and Collins, 2005) attempts to map natural language sentences to a lambdacalculus encoding of their semantics. They do so by treating the problem as a structured learning task, and use a log-linear model to learn a Probabilistic Combinatory Categorical Grammar\n(CCG) (Steedman and Baldridge, 2011), which is a grammar formalism based on lambda calculus.\nAMR aims to combine various semantic annotations to produce a unified annotation, but it mainly builds on top of PropBank (Palmer et al., 2005). PropBank has found extensive use in other semantic tasks such as shallow semantic parsing (Giuglea and Moschitti, 2006),\nIn our work we used SEARN to build an AMR parser. SEARN comes from a family of algorithms called ”Learning to Search (L2S)” that solves structured prediction problems by decomposing the structured output in terms of an explicit search space and then learning a policy that can take actions in this search space in the optimal way. Incremental structured perceptron (Collins and Roark, 2004; Huang et al., 2012), DAGGER (Ross et al., 2011), AGGREVATE (Ross and Bagnell, 2014), etc. (Daumé III and Marcu, 2005; Xu and Fern, 2007; Xu et al., 2007; Ratliff et al., 2007; Syed and Schapire, 2010; Doppa et al., 2012) are other algorithms that also belong to this family."
    }, {
      "heading" : "6 Conclusion and Future work",
      "text" : "We have presented a novel technique for parsing english sentences into AMR using a learning to search approach. We model the concept and the relation learning in a unified framework using SEARN which allows us to optimize over the loss of the entire predicted output. We evaluate our parser on multiple datasets from varied domains and show that our parser performs better than the state-of-the-art across all the datasets. We also show that a simple technique of choosing the most frequent concept gives us a baseline that is better than the state-of-the-art for concept prediction.\nCurrently we ensure various properties of AMR, such as connectedness and acyclicity using heuristics. In the future, we plan to incorporate these as constraints in our learning technique."
    } ],
    "references" : [ {
      "title" : "Abstract meaning representation for sembanking",
      "author" : [ "Claire Bonial", "Shu Cai", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Kevin Knight", "Philipp Koehn", "Martha Palmer", "Nathan Schneider" ],
      "venue" : null,
      "citeRegEx" : "Banarescu et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Banarescu et al\\.",
      "year" : 2013
    }, {
      "title" : "Smatch: an evaluation metric for semantic",
      "author" : [ "Cai", "Knight2013] Shu Cai", "Kevin Knight" ],
      "venue" : null,
      "citeRegEx" : "Cai et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Cai et al\\.",
      "year" : 2013
    }, {
      "title" : "Incremental parsing with the perceptron algorithm",
      "author" : [ "Collins", "Roark2004] Michael Collins", "Brian Roark" ],
      "venue" : "In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,",
      "citeRegEx" : "Collins et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Collins et al\\.",
      "year" : 2004
    }, {
      "title" : "Learning as search optimization: Approximate large margin methods for structured prediction",
      "author" : [ "III Daumé", "III Marcu2005] Hal Daumé", "Marcu. Daniel" ],
      "venue" : "In Proceedings of the 22nd international conference on Machine learning,",
      "citeRegEx" : "Daumé et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Daumé et al\\.",
      "year" : 2005
    }, {
      "title" : "Search-based structured prediction",
      "author" : [ "John Langford", "Daniel Marcu" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "III et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "III et al\\.",
      "year" : 2009
    }, {
      "title" : "Efficient programmable learning to search. arXiv preprint arXiv:1406.1837",
      "author" : [ "John Langford", "Stephane Ross" ],
      "venue" : null,
      "citeRegEx" : "III et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "III et al\\.",
      "year" : 2014
    }, {
      "title" : "The logical form of action sentences",
      "author" : [ "Donald Davidson" ],
      "venue" : null,
      "citeRegEx" : "Davidson.,? \\Q1967\\E",
      "shortCiteRegEx" : "Davidson.",
      "year" : 1967
    }, {
      "title" : "Output space search for structured prediction",
      "author" : [ "Alan Fern", "Prasad Tadepalli" ],
      "venue" : "arXiv preprint arXiv:1206.6460",
      "citeRegEx" : "Doppa et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Doppa et al\\.",
      "year" : 2012
    }, {
      "title" : "The use of lexical semantics in interlingual machine translation",
      "author" : [ "Bonnie J Dorr" ],
      "venue" : "Machine Translation,",
      "citeRegEx" : "Dorr.,? \\Q1992\\E",
      "shortCiteRegEx" : "Dorr.",
      "year" : 1992
    }, {
      "title" : "A discriminative graph-based parser for the abstract meaning representation",
      "author" : [ "Sam Thomson", "Jaime G. Carbonell", "Chris Dyer", "Noah A. Smith" ],
      "venue" : "In Proceedings of the 52nd Annual Meeting of the Association",
      "citeRegEx" : "Flanigan et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Flanigan et al\\.",
      "year" : 2014
    }, {
      "title" : "Semantic role labeling via framenet, verbnet and propbank",
      "author" : [ "Giuglea", "Moschitti2006] Ana-Maria Giuglea", "Alessandro Moschitti" ],
      "venue" : "ACL",
      "citeRegEx" : "Giuglea et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Giuglea et al\\.",
      "year" : 2006
    }, {
      "title" : "Dynamic feature selection for dependency parsing",
      "author" : [ "He et al.2013] He He", "Hal Daumé III", "Jason Eisner" ],
      "venue" : "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "He et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2013
    }, {
      "title" : "Structured perceptron with inexact search",
      "author" : [ "Huang et al.2012] Liang Huang", "Suphan Fayong", "Yang Guo" ],
      "venue" : "In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
      "citeRegEx" : "Huang et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2012
    }, {
      "title" : "Learning to transform natural to formal languages",
      "author" : [ "Kate et al.2005] Rohit J. Kate", "Yuk Wah Wong", "Raymond J. Mooney" ],
      "venue" : "In Proceedings,",
      "citeRegEx" : "Kate et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Kate et al\\.",
      "year" : 2005
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "Andrew McCallum", "Fernando CN Pereira" ],
      "venue" : null,
      "citeRegEx" : "Lafferty et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Lafferty et al\\.",
      "year" : 2001
    }, {
      "title" : "Vowpal wabbit online learning project",
      "author" : [ "Lihong Li", "Alexander Strehl" ],
      "venue" : null,
      "citeRegEx" : "Langford et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Langford et al\\.",
      "year" : 2007
    }, {
      "title" : "Prune-andscore: Learning for greedy coreference resolution",
      "author" : [ "Ma et al.2014] Chao Ma", "Janardhan Rao Doppa", "J Walker Orr", "Prashanth Mannem", "Xiaoli Fern", "Tom Dietterich", "Prasad Tadepalli" ],
      "venue" : "In Proceedings of Conference on Empirical Methods",
      "citeRegEx" : "Ma et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2014
    }, {
      "title" : "The proposition bank: An annotated corpus of semantic roles",
      "author" : [ "Palmer et al.2005] Martha Palmer", "Paul Kingsbury", "Daniel Gildea" ],
      "venue" : null,
      "citeRegEx" : "Palmer et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Palmer et al\\.",
      "year" : 2005
    }, {
      "title" : "Boosting structured prediction for imitation learning",
      "author" : [ "David Bradley", "J Andrew Bagnell", "Joel Chestnutt" ],
      "venue" : "Robotics Institute,",
      "citeRegEx" : "Ratliff et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Ratliff et al\\.",
      "year" : 2007
    }, {
      "title" : "Reinforcement and imitation learning via interactive no-regret learning",
      "author" : [ "Ross", "Bagnell2014] Stephane Ross", "J Andrew Bagnell" ],
      "venue" : "arXiv preprint arXiv:1406.5979",
      "citeRegEx" : "Ross et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ross et al\\.",
      "year" : 2014
    }, {
      "title" : "A reduction of imitation learning and structured prediction to no-regret online learning",
      "author" : [ "Ross et al.2011] Stéphane Ross", "Geoff J. Gordon", "J. Andrew Bagnell" ],
      "venue" : "In Proceedings of the Workshop on Artificial Intelligence and Statistics (AIStats)",
      "citeRegEx" : "Ross et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Ross et al\\.",
      "year" : 2011
    }, {
      "title" : "Verbnet: A broad-coverage, comprehensive verb lexicon",
      "author" : [ "Karin Kipper Schuler" ],
      "venue" : null,
      "citeRegEx" : "Schuler.,? \\Q2005\\E",
      "shortCiteRegEx" : "Schuler.",
      "year" : 2005
    }, {
      "title" : "A reduction from apprenticeship learning to classification",
      "author" : [ "Syed", "Schapire2010] Umar Syed", "Robert E Schapire" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Syed et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Syed et al\\.",
      "year" : 2010
    }, {
      "title" : "A transitionbased algorithm for amr parsing",
      "author" : [ "Wang et al.2015] Chuan Wang", "Nianwen Xue", "Sameer Pradhan" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning for semantic parsing with statistical machine translation",
      "author" : [ "Wong", "Mooney2006] Yuk Wah Wong", "Raymond J. Mooney" ],
      "venue" : "In Human Language Technology Conference of the North American Chapter of the Association of Compu-",
      "citeRegEx" : "Wong et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Wong et al\\.",
      "year" : 2006
    }, {
      "title" : "On learning linear ranking functions for beam search",
      "author" : [ "Xu", "Fern2007] Yuehua Xu", "Alan Fern" ],
      "venue" : "In Proceedings of the 24th international conference on Machine learning,",
      "citeRegEx" : "Xu et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2007
    }, {
      "title" : "Discriminative learning of beamsearch heuristics for planning",
      "author" : [ "Xu et al.2007] Yuehua Xu", "Alan Fern", "Sung Wook Yoon" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "Xu et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2007
    }, {
      "title" : "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars",
      "author" : [ "Zettlemoyer", "Collins2005] Luke S. Zettlemoyer", "Michael Collins" ],
      "venue" : "In UAI ’05, Proceedings of the 21st Conference in Uncertainty",
      "citeRegEx" : "Zettlemoyer et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Zettlemoyer et al\\.",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Abstract Meaning Representation (Banarescu et al., 2013) is a semantic representation which is a rooted, directed, acyclic graph where the nodes represent concepts (words, PropBank (Palmer et al.",
      "startOffset" : 32,
      "endOffset" : 56
    }, {
      "referenceID" : 17,
      "context" : ", 2013) is a semantic representation which is a rooted, directed, acyclic graph where the nodes represent concepts (words, PropBank (Palmer et al., 2005) framesets or special keywords) and the edges represent relations between these concepts.",
      "startOffset" : 132,
      "endOffset" : 153
    }, {
      "referenceID" : 23,
      "context" : "The other approach (Wang et al., 2015) uses a transition-based algorithm to convert the dependency representation of a sentence to its AMR.",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 9,
      "context" : "Flanigan et al. (2014) use a semi-Markov model to first identify the concepts, and then find a maximum spanning connected subgraph that defines the relations between these concepts.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 16,
      "context" : ", 2014), and for even more complex structured prediction tasks like coreference resolution (Ma et al., 2014) and dependency parsing (He et al.",
      "startOffset" : 91,
      "endOffset" : 108
    }, {
      "referenceID" : 11,
      "context" : ", 2014) and dependency parsing (He et al., 2013).",
      "startOffset" : 31,
      "endOffset" : 48
    }, {
      "referenceID" : 21,
      "context" : "If si has not been seen in the training data, CL-CONsi consists of the lemmatized span, PropBank frames (for verbs) obtained using the Unified Verb Index (Schuler, 2005) and the NULL concept.",
      "startOffset" : 154,
      "endOffset" : 169
    }, {
      "referenceID" : 14,
      "context" : "First, we detect possible multiword spans corresponding to named entities, using a named entity recognizer (Lafferty et al., 2001).",
      "startOffset" : 107,
      "endOffset" : 130
    }, {
      "referenceID" : 15,
      "context" : "To train our model, we use SEARN as implemented in the Vowpal Wabbit machine learning library (Langford et al., 2007; Daumé III et al., 2014).",
      "startOffset" : 94,
      "endOffset" : 141
    }, {
      "referenceID" : 6,
      "context" : "AMR itself is based on propositional logic and neoDavidsonian semantics (Davidson, 1967).",
      "startOffset" : 72,
      "endOffset" : 88
    }, {
      "referenceID" : 8,
      "context" : "), it does away with languagespecific idiosyncrasies and interlingual representations (Dorr, 1992) are thus, important predecessors to AMR.",
      "startOffset" : 86,
      "endOffset" : 98
    }, {
      "referenceID" : 13,
      "context" : "Like the task of AMR parsing, there have been various attempts to parse sentences into a logical form, given raw sentences annotated with such forms (Kate et al., 2005; Wong and Mooney, 2006).",
      "startOffset" : 149,
      "endOffset" : 191
    }, {
      "referenceID" : 17,
      "context" : "AMR aims to combine various semantic annotations to produce a unified annotation, but it mainly builds on top of PropBank (Palmer et al., 2005).",
      "startOffset" : 122,
      "endOffset" : 143
    }, {
      "referenceID" : 12,
      "context" : "Incremental structured perceptron (Collins and Roark, 2004; Huang et al., 2012), DAGGER (Ross et al.",
      "startOffset" : 34,
      "endOffset" : 79
    }, {
      "referenceID" : 20,
      "context" : ", 2012), DAGGER (Ross et al., 2011), AGGREVATE (Ross and Bagnell, 2014), etc.",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 25,
      "context" : "(Daumé III and Marcu, 2005; Xu and Fern, 2007; Xu et al., 2007; Ratliff et al., 2007; Syed and Schapire, 2010; Doppa et al., 2012) are other algorithms that also belong to this family.",
      "startOffset" : 0,
      "endOffset" : 130
    }, {
      "referenceID" : 18,
      "context" : "(Daumé III and Marcu, 2005; Xu and Fern, 2007; Xu et al., 2007; Ratliff et al., 2007; Syed and Schapire, 2010; Doppa et al., 2012) are other algorithms that also belong to this family.",
      "startOffset" : 0,
      "endOffset" : 130
    }, {
      "referenceID" : 7,
      "context" : "(Daumé III and Marcu, 2005; Xu and Fern, 2007; Xu et al., 2007; Ratliff et al., 2007; Syed and Schapire, 2010; Doppa et al., 2012) are other algorithms that also belong to this family.",
      "startOffset" : 0,
      "endOffset" : 130
    } ],
    "year" : 2015,
    "abstractText" : "We develop a novel technique to parse English sentences into Abstract Meaning Representation (AMR) using SEARN, a Learning to Search approach, by modeling the concept and the relation learning in a unified framework. We evaluate our parser on multiple datasets from varied domains and show an absolute improvement of 2% to 6% over the state-of-the-art. Additionally we show that using the most frequent concept gives us a baseline that is stronger than the state-of-the-art for concept prediction. We plan to release our parser for public use.",
    "creator" : "LaTeX with hyperref package"
  }
}