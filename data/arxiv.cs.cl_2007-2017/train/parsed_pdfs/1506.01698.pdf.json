{
  "name" : "1506.01698.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "The Long-Short Story of Movie Description",
    "authors" : [ "Anna Rohrbach", "Marcus Rohrbach", "Bernt Schiele" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Automatic description of visual content has lately received a lot of interest in our community. Multiple works have successfully addressed the image captioning problem [6, 16, 17, 35]. Many of the proposed methods rely on Long-Short Term Memory networks (LSTMs) [13]. In the meanwhile, two large-scale movie description datasets have been proposed, namely MPII Movie Description (MPIIMD) [28] and Montreal Video Annotation Dataset (M-VAD) [31]. Both are based on movies with associated textual descriptions and allow studying the problem how to generate movie description for visually disabled people. Works addressing these datasets [28, 33, 39] show that they are indeed challenging in terms of visual recognition and automatic description. This results in a significantly lower performance then on simpler video datasets (e.g. MSVD [2]), but a detailed analysis of the difficulties is missing. In this work we address this by taking a closer look at the performance of existing methods on the movie description task.\nThis work contributes a) an approach to build robust visual classifiers which distinguish verbs, objects, and places extracted from weak sentence annotations; b) based on the visual classifiers we evaluate different design choices to train an LSTM for generating descriptions. This outperforms related work on the MPII-MD dataset, both using automatic and human evaluation; c) we perform a detailed analysis of prior work and our approach to understand the challenges of the movie description task.\nar X\niv :1\n50 6.\n01 69\n8v 1\n[ cs\n.C V\n] 4\nJ un\n2 01\n5"
    }, {
      "heading" : "2 Related Work",
      "text" : "Image captioning. Automatic image description has been studied in the past [9, 19, 20, 24], however it regained attention just recently. Multiple works have been proposed [6, 8, 16, 17, 23, 35, 37]. Many of them rely on Recurrent Neural Networks (RNNs) and in particular on Long-Short Term Memory networks (LSTMs). Also new datasets have been released, Flickr30k [40] and MS COCO Captions [3], where [3] additionally presents a standardized setup for image captioning evaluation. There are also attempts to analyze the performance of recent methods. E.g. [5] compares them with respect to the novelty of generated descriptions and additionally proposes a nearest neighbor baseline that improves over recent methods.\nVideo description. In the past video description has been addressed in semirealistic settings [1, 18], on a small scale [4, 11, 30] or in constrained scenarios like cooking [27, 29]. Most works (with a few exceptions, e.g. [27]) study the task of describing a short clip with a single sentence. [6] first proposed to describe videos using an LSTM, relying on precomputed CRF scores from [27]. [34] extended this work to extract CNN features from frames which are max-pooled over time. They show the benefit of pre-training the LSTM network for image captioning and fine-tuning it to video description. [25] proposes a framework that consists of a 2-D and/or 3-D CNN and the LSTM is trained jointly with a visual-semantic embedding to ensure better coherence between video and text. [38] jointly addresses the language generation and video/language retrieval tasks by learning a joint embedding model for a deep video model and compositional semantic language model.\nMovie description. Recently two large-scale movie description datasets have been proposed, MPII Movie Description (MPII-MD) [28] and Montreal Video Annotation Dataset (M-VAD) [31]. Given that they are based on movies, they cover a much broader domain then previous video description datasets. Consequently they are much more varied and challenging with respect to the visual content and the associated description. They also do not have any additional annotations, as e.g. TACoS Multi-Level [27], thus one has to rely on the weak annotations of the sentence descriptions. To handle this challenging scenario [39] proposes an attention based model which selects the most relevant temporal segments in a video and incorporates 3-D CNN and generates a sentence using an LSTM. [33] proposes an encoder-decoder framework, where a single LSTM encodes the input video frame by frame and decodes it into a sentence, outperforming [39]. Our approach for sentence generation is most similar to [6] and we also rely on their LSTM implementation based on Caffe [15]. However, we analyze different aspects and variants of this architecture for movie description. To extract labels from sentences we rely on the semantic parser of [28], however we treat the labels differently to handle the weak supervision (see Section 3.1). We show that this improves over [28] and [33]."
    }, {
      "heading" : "3 Approach",
      "text" : "In this section we present our two-step approach to video description. The first step performs visual recognition, while the second step generates textual descriptions. For the visual recognition we propose to use the visual classifiers trained according to the labels’ semantics and “visuality”. For the language generation we rely on a LSTM network which has been successfully used for image and video description [6, 33]. We discuss various design choices for building and training the LSTM. An overview of our approach is given in Figure 1."
    }, {
      "heading" : "3.1 Visual Labels for Robust Visual Classifiers",
      "text" : "For training we rely on a parallel corpus of videos and weak sentence annotations. As in [28] we parse the sentences to obtain a set of labels (single words or short phrases, e.g. look up) to train our visual classifiers. However, in contrast to [28] we do not want to keep all of these initial labels as they are noisy, but select only visual ones which actually can be robustly recognized.\nAvoiding parser failure. Not all sentences can be parsed successfully, as e.g. some sentences are incomplete or grammatically incorrect. To avoid loosing the potential labels in these sentences, we match our set of initial labels to the sentences which the parser failed to process.\nSemantic groups. Our labels correspond to different semantic groups. In this work we consider three most important groups: verbs (actions), objects and places, as they are typically visual. One could also consider e.g. groups like mood or emotions, which are naturally harder for visual recognition. We propose to treat each label group independently. First, we rely on a different representation for the each semantic groups, which is targeted to the specific group. Namely we use the activity recognition feature Improved Dense Trajectories\n(DT) [36] for verbs, large scale object detector responses (LSDA) [14] for objects and scene classification scores (PLACES) [41] for places. Second, we train one-vs-all SVM classifiers for each group separately. The intuition behind this is to discard “wrong negatives” (e.g. using object “bed” as negative for place “bedroom”).\nVisual labels. Now, how do we select visual labels for our semantic groups? In order to find the verbs among the labels we rely on the semantic parser of [28]. Next, we look up the list of “places” used in [41] and search for corresponding words among our labels. We look up the object classes used in [14] and search for these “objects”, as well as their base forms (e.g. “domestic cat” and “cat”). We discard all the labels that do not belong to any of our three groups of interest as we assume that they are likely not visual and thus are difficult to recognize. Finally, we discard labels which the classifiers could not learn, as these are likely to be noisy or not visual. For this we require the classifiers to have have minimum area under the ROC-curve (Receiver Operating Characteristic)."
    }, {
      "heading" : "3.2 LSTM for Sentence Generation",
      "text" : "We rely on the basic LSTM architecture proposed in [6] for video description. As shown in Figures 1 and 2(a), at each time step, an LSTM generates a word and receives the visual classifiers (input-vis) as well as as the previous generated word (input-lang) as input. To handle natural words we encode each word with a one-hot-vector according to their index in a dictionary and a lower dimensional embedding. The embedding is jointly learned during training of the LSTM. [6] compares three variants: (a) an encoder-decoder architecture, (b) a decoder architecture with visual max predictions, and (c) a decoder architecture with visual probabilistic predictions. In this work we rely on variant (c) which was shown to work best as it can rely on the richest visual input. We analyze the following aspects for this architecture:\nLayer structure: We compare a 1-layer architecture with a 2-layer architecture. In the 2-layer architecture, the output of the first layer is used as input for the second layer (Figure 2b) and was used by [6] for video description. Additionally\nwe also compare to a 2-layer factored architecture [6], where the first layer only gets the language as input and the second gets the output of the first layer as well as the visual input.\nDropout placement: To learn a more robust network which is less likely to overfit we rely on a dropout [12]. Using dropout a ratio r of randomly selected units is set to 0 during training (while all others are multiplied with 1/r). We explore different ways to place dropout in the network, i.e. either for language input (lang-drop) or visual (vis-drop) input only, for both inputs (concat-drop) or for the LSTM output (lstm-drop), see Figure 2(d). While the default dropout ratio is r = 0.5, we evaluate the effect of different ratios.\nLearning strategy: By default we rely on a step-based learning strategy, where a learning rate is halved after a certain number of steps. We find the best learning rate and step size on the validation set. Additionally we compare this to a polynomial learning strategy, where the learning rate is continuously decreased. The polynomial learning strategy has been shown to give good results faster without tweaking step size for GoogleNet implemented by Sergio Guadarrama in Caffe [15]."
    }, {
      "heading" : "4 Evaluation",
      "text" : "In this section we first analyze our approach on the MPII-MD [28] dataset and explore different design choices. Then, we compare our best system to prior work."
    }, {
      "heading" : "4.1 Analysis of our approach",
      "text" : "Experimental setup. We build on the labels discovered by our semantic parser [28] and additionally match these labels to sentences which the parser failed to process. To be able to learn classifiers we select the labels that appear at least 30 times, resulting in 1,263 labels. The parser additionally tells us whether the label is a verb. We use the visual features (DT, LSDA, PLACES) provided with the MPII-MD dataset [28]. The LSTM output/hidden unit as well as memory cell have each 500 dimensions. We train the SVM classifiers on the Training set (56,861 clips). We evaluate our method on the validation set (4,930 clips) using the METEOR [21] score, which, according to [7, 32], supersedes other popular measures, such as BLEU [26], ROUGE [22], in terms of agreement with human judgments. The authors of CIDEr [32] showed that METEOR also outperforms CIDEr when the number of references is small and in the case of MPII-MD we have typically only a single reference.\nRobust visual classifiers. In a first set of experiments we analyze our proposal to consider groups of labels to learn different classifiers and also to use different visual representations for these groups (see Section 3.1). Table 1 we evaluate our generated sentences using different input features to the LSTM. In our baseline,\nin the top part of Table 1, we treat all labels equally, i.e. we use the same visual descriptors for all labels. The PLACES feature is best with 7.1 METEOR. Combination by stacking all features (DT + LSDA + PLACES) improves further to 7.24 METEOR.\nThe second part of the table demonstrates the effect of introducing different semantic label groups. We first split the labels into “Verbs” and all remaining. Given that some labels appear in both roles, the total number of labels increases to 1328. We analyze two settings of training the classifiers. In the case of “Retrieved” we retrieve the classifier scores from the general classifiers trained in the previous step. “Trained” corresponds to training the SVMs specifically for each label type (e.g. for “verbs”). Next, we further divide the non-verbal labels into “Places” and “Others”, and finally into “Places” and “Objects”. We discard the unused labels and end up with 913 labels. Out of these labels, we select the labels where the classifier obtains a ROC higher or equal to 0.7 (threshold selected on the validation set). After this we obtain 263 labels and the best performance in the “Trained” setting. To support our intuition about the importance of the label discrimination (i.e. using different features for different semantic groups of labels), we propose another baseline (last line in the table). Here we use the same set of 263 labels but provide the same feature for all of them, namely the best performing combination DT + LSDA + PLACES. As we see, this results in an inferior performance.\nWe make several observations from Table 1 which lead to robust visual classifiers from the weak sentence annotations. a) It is beneficial to select features based on the label semantics. b) Training one-vs-all SVMs for specific label groups consistently improves the performance as it avoids “wrong” negatives. c) Focusing on more “visual” labels helps: we reduce the LSTM input dimensionality to 263 while improving the performance.\nLSTM architectures. Now, as described in Section 3.2, we look at different LSTM architectures and training configurations. In the following we use the best performing “Visual Labels” approach, Table 1 (8).\nWe start with examining the architecture, where we explore different configurations of LSTM and dropout layers. Table 2a shows the performance of three different networks: “1 layer”, “2 layers unfactored” and “2 layers factored” introduced in Section 3.2. As we see, the “1 layer” and “2 layers unfactored” perform equally well, while “2 layers factored” is inferior to them. In following experiments we use the simplest “1 layer” network. We then compare different dropout placements as illustrated in (Figure 2b). We obtain the best result when applying dropout after the LSTM layer (“lstm-drop”), while having no dropout or applying it only to language leads to stronger over-fitting to the visual features. Putting dropout after the LSTM (and prior to a final prediction layer) makes the entire system more robust. As for the best dropout ratio, we find that 0.5 works best with lstm-dropout Table 2c.\nWe compare different learning rates and learning strategies in Tables 3a and 3b. We find that the best learning rate in the step-based learning is 0.01, while\nstep 4000 slightly improves over step 2000 (which we used in Table 1). We explore an alternative learning strategy, namely decreasing learning rate according to a polynomial decay. We experiment with different exponents (0.5 and 0.7) and numbers of iterations (25K and 10K), using the base-learning rate 0.01. Our results show that the step-based learning is superior to the polynomial learning.\nIn most of experiments we trained our networks for 25,000 iterations. After looking at the METEOR performance for intermediate iterations we found that for the step size 4000 at iteration 15,000 we achieve best performance overall. Additionally we train multiple LSTMs with different random orderings of the training data. In our experiments we combine three in an ensemble, averaging the resulting word predictions. In most cases the ensemble improves over the single networks in terms of METEOR score (see Table 4).\nTo summarize, the most important aspects that decrease over-fitting and lead to a better sentence generation are: (a) a correct learning rate and step size, (b) dropout after the LSTM layer, (c) choosing the training iteration based on METEOR score as opposed to only looking at the LSTM accuracy/loss which can be misleading, and (d) building ensembles of multiple networks with different random initializations. In the following section we evaluate our best ensemble (last line of Table 4) on the test set of MPII-MD."
    }, {
      "heading" : "4.2 Comparison to related work",
      "text" : "Experimental setup. We compare the best method of [28], the recently proposed method S2VT [33] and our proposed “Visual Labels”-LSTM on the test set of the MPII-MD dataset (6,578 clips). We report all popular automatic evaluation measures, CIDEr [32], BLEU [26], ROUGE [22] and METEOR [21], computed using the evaluation code of [3]. We also perform a human evaluation, by randomly selecting 1300 video snippets and asking AMT turkers to rank three systems (the best SMT of [28], S2VT [33] and ours) with respect to Correctness, Grammar and Relevance, similar to [28].\nResults. Table 5 summarizes the results on the test set of MPII-MD. While we rely on identical features and similar labels as [28], we significantly improve the performance in all automatic measures, specifically by 1.44 METEOR points.\nMoreover, we improve over the recent approach of [33], which also uses LSTM to generate video descriptions. Exploring different strategies to label selection and classifier training, as well as various LSTM configurations allows to obtain best result to date on the MPII-MD dataset. Human evaluation mainly agrees with the automatic measures. We outperform both prior works in terms of Correctness and Relevance, however we lose to S2VT in terms of Grammar. This is due to the fact that S2VT produces overall shorter (7.4 versus 8.7 words per sentence) and simpler sentences, while our system generates longer sentences and therefore has higher chances to make mistakes.\nWe also propose a retrieval upperbound (last line in Table 5). For every test sentence we retrieve the closest training sentence according to the METEOR. The rather low METEOR score of 19.43 reflects the difficulty of the dataset.\nA closer look at the sentences produced by all three methods gives us additional insights. An interesting characteristic is the output vocabulary size, which is 94 for [28], 86 for [33] and 605 for our method, while the test set contains 6422 unique words. This clearly shows a higher diversity of our output. Among the words generated by our system and absent in the outputs of others are such verbs as grab, drive, sip, climb, follow, objects as suit, chair, cigarette, mirror, bottle and places as kitchen, corridor, restaurant. We showcase some qualitative results in Figure 3. Here, e.g. the verb pour, object drink and place courtyard only appear in our output. We attribute this, on one hand, to our diverse and robust visual classifiers. On the other hand, the architecture and parameter choices of our LSTM allow us to learn better correspondance between words and visual classifiers’ scores."
    }, {
      "heading" : "5 Analysis",
      "text" : "Despite the recent advances in the video description domain, including our proposed approach, the video description performance on the movie description datasets (MPII-MD [28] and M-VAD [31]) remains relatively low. In this section we want to take a closer look at three methods, best SMT of [28], S2VT [33] and ours, in order to understand where these methods succeed and where they fail. In the following we evaluate all three methods on the MPII-MD test set."
    }, {
      "heading" : "5.1 Difficulty versus performance",
      "text" : "As the first study we suggest to sort the reference sentences (from the test set) by difficulty, where difficulty is defined in multiple ways.\nSentence length and Word frequency. Two of the simplest sentence difficulty measures are its length and average frequency of words. When sorting the data by difficulty (increasing sentence length or decreasing average word frequency), we find that all three methods have the same tendency to obtain lower METEOR score as the difficulty increases (Figures 4a and 4b). For the\nword frequency the correlation is stronger. Our method consistently outperforms the other two, most notable as the difficulty increases.\nTextual and Visual Nearest Neighbors. Next, for each reference test sentence we search for the closest training sentence (in terms of the METEOR score). We use the obtained best scores to sort the reference sentences by textual difficulty, i.e. the “easy” sentences are more likely to be retrieved. If we consider all training sentences, we obtain a Textual Nearest Neighbor. We sort the test sentences according to these scores (decreasing) and plot the performance of three methods in Figure 5a. All methods “agree” and ours is best throughout the difficulty range, in particular in the more challenging part of the plot. We can also use visual features to find the k Nearest Neighbors in the Training set, select the best one (in terms of the METEOR score) and use this score to sort the reference sentences. We call this a Visual k Nearest Neighbor. The intuition behind it is to consider a video clip as visually “easy” if the most similar training clips also have similar descriptions (the “difficult” clip might have no close visual neighbours). We rely on our best visual representation (8) from Table 1 and cos similarity measure to define the Visual kNN and sort the reference sentences according to it with k = 10 (Figure 5b). We see a clear correlation between the visual difficulty and the performance of all methods (Figure 5b).\nSummary. a) All methods perform better on shorter, common sentences and our method notably wins on longer sentences. b) Our method also wins on sentences that are more difficult to retrieve. c) Visual difficulty, defined by cos similarity and representation (8) from Table 1, strongly correlates with the performance of all methods. (d) When comparing all four plots (Figures 4a and 4b, Figures 5a and 5b), we find that the strongest correlation between the methods’ performance and the difficulty is observed for the Textual difficulty, while the least correlation we observe for the Sentence length."
    }, {
      "heading" : "5.2 Semantic analysis",
      "text" : "WordNet Verb Topics. We closer analyze the test sentences with respect to different verbs. For this we rely on WordNet topics (high level entries in the WordNet ontology, e.g. “motion”, “perception”, “competition”, “emotion”), defined for most synsets in WordNet [10]. We obtain the sense information from the semantic parser of [28], thus senses might be noisy. We showcase the 5 most frequent verbs for each topic in Table 6. We select sentences with a single verb, group them according to the verb topic and compute an average METEOR score for each topic, see Figure 6. We find that our method is best for all topics except “communication”, where [28] wins. The most frequent verbs in this topic are “look up” and “nod”, which are also frequent in the dataset and in the sentences produced by [28]. The best performing topic, “cognition”, is highly biased to “look at” verb. The most frequent topics, “motion” and “contact”,\nwhich are also visual (e.g. “turn”, “walk”, “open”, “sit”), are nevertheless quite challenging, which we attribute to their high diversity (see their entropy w.r.t. different verbs and their frequencies in Table 6). At the same time “perception” is far less diverse and mainly focuses on verbs like “look” or “stare”, which are quite frequent in the dataset, resulting in better performance. Topics with more abstract verbs (e.g. “be”, “have”, “start”) tend to get lower scores.\nTop 100 best and worst sentences. We look at 100 Test sentences, where our method obtains highest and lowest METEOR scores. Out of 100 best sentences 44 contain the verb “look” (including verb phrases such as “look at”). The other frequent verbs are “walk”, “turn”, “smile”, “nod”, “shake”, “stare”, “sit”, i.e. mainly visual verbs. Overall the sentences are simple and common. Among the 100 lowest scoring sentences we observe more diversity: 12 sentences contain no verb, 10 mention unusual words (specific to the movie), 24 contain no subject, 29 have a non-human subject. Altogether this leads to a lower performance, in particular, as most training sentences contain “Someone” as subject and generated sentences are biased towards it.\nSummary. a) The test sentences that mention the verb “look” (and similar) get higher METEOR scores due to their high frequency in the dataset. b) The sentences with more “visual” verbs tend to get higher scores. c) The sentences without verbs (e.g. describing a scene), without subjects or with non-human subjects get lower scores, which can be explained by a dataset bias towards “Someone” as subject."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We propose an approach to automatic movie description which trains visual classifiers and uses the classifier scores as input to LSTM. To handle the weak sentence annotations we rely on three main ingredients. First, we distinguish three semantic groups of labels (verbs, objects and places), second we train them discriminatively, removing potentially noisy negatives, and third, we select only a small number of the most reliable classifiers. For sentence generation we show the benefits of exploring different LSTM architectures and learning configurations. As the result we obtain the highest performance on the MPII-MD dataset as shown by all automatic evaluation measures and extensive human evaluation.\nWe analyze the challenges in the movie description task using our and two prior works. We find that the factors which contribute to higher performance include: presence of frequent words, sentence length and simplicity as well as presence of “visual” verbs (e.g. “nod”, “walk”, “sit”, “smile”). Textual and visual difficulties of sentences/clips strongly correlate with the performance of all methods. We observe a high bias in the data towards humans as subjects and verbs similar to “look”. Future work has to focus on dealing with less frequent words and handle less visual descriptions. This potentially requires to consider external text corpora, modalities other than video, such as audio and dialog, and to look across multiple sentences. This would allow exploiting long- and shortrange context and thus understanding and describing the story of the movie.\nAcknowledgements. Marcus Rohrbach was supported by a fellowship within the FITweltweit-Program of the German Academic Exchange Service (DAAD). The authors thank Niket Tandon for help with the WordNet Topics analysis."
    } ],
    "references" : [ {
      "title" : "Video in sentences out",
      "author" : [ "A. Barbu", "A. Bridge", "Z. Burchill", "D. Coroian", "S. Dickinson", "S. Fidler", "A. Michaux", "S. Mussman", "S. Narayanaswamy", "D. Salvi", "L. Schmidt", "J. Shangguan", "J.M. Siskind", "J. Waggoner", "S. Wang", "J. Wei", "Y. Yin", "Z. Zhang" ],
      "venue" : "Proceedings of the conference on Uncertainty in Artificial Intelligence (UAI)",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Collecting highly parallel data for paraphrase evaluation",
      "author" : [ "D. Chen", "W. Dolan" ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Microsoft coco captions: Data collection and evaluation server",
      "author" : [ "X. Chen", "H. Fang", "T. Lin", "R. Vedantam", "S. Gupta", "P. Dollr", "C.L. Zitnick" ],
      "venue" : "arXiv:1504.00325",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching",
      "author" : [ "P. Das", "C. Xu", "R. Doell", "J. Corso" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Language models for image captioning: The quirks and what works",
      "author" : [ "J. Devlin", "H. Cheng", "H. Fang", "S. Gupta", "L. Deng", "X. He", "G. Zweig", "M. Mitchell" ],
      "venue" : "arXiv:1505.01809",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Long-term recurrent convolutional networks for visual recognition and description",
      "author" : [ "J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Image description using visual dependency representations",
      "author" : [ "D. Elliott", "F. Keller" ],
      "venue" : "EMNLP. pp. 1292–1302",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "From captions to visual concepts and back",
      "author" : [ "H. Fang", "S. Gupta", "F.N. Iandola", "R. Srivastava", "L. Deng", "P. Dollár", "J. Gao", "X. He", "M. Mitchell", "J.C. Platt", "C.L. Zitnick", "G. Zweig" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Every picture tells a story: Generating sentences from images",
      "author" : [ "A. Farhadi", "M. Hejrati", "M. Sadeghi", "P. Young", "C. Rashtchian", "J. Hockenmaier", "D. Forsyth" ],
      "venue" : "Proceedings of the European Conference on Computer Vision (ECCV)",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "WordNet: An Electronical Lexical Database",
      "author" : [ "C. Fellbaum" ],
      "venue" : "The MIT Press",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shoot recognition",
      "author" : [ "S. Guadarrama", "N. Krishnamoorthy", "G. Malkarnenkar", "S. Venugopalan", "R. Mooney", "T. Darrell", "K. Saenko" ],
      "venue" : "Proceedings of the IEEE International Conference on Computer Vision (ICCV)",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Improving neural networks by preventing co-adaptation of feature detectors",
      "author" : [ "G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov" ],
      "venue" : "arXiv:1207.0580",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Long short-term memory",
      "author" : [ "S. Hochreiter", "J. Schmidhuber" ],
      "venue" : "Neural Computation",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "LSDA: Large scale detection through adaptation",
      "author" : [ "J. Hoffman", "S. Guadarrama", "E. Tzeng", "J. Donahue", "R. Girshick", "T. Darrell", "K. Saenko" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Caffe: Convolutional architecture for fast feature embedding",
      "author" : [ "Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell" ],
      "venue" : "arXiv:1408.5093",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Deep visual-semantic alignments for generating image descriptions",
      "author" : [ "A. Karpathy", "L. Fei-Fei" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Unifying visual-semantic embeddings with multimodal neural language models",
      "author" : [ "R. Kiros", "R. Salakhutdinov", "R.S. Zemel" ],
      "venue" : "Transactions of the Association for Computational Linguistics (TACL)",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Natural language description of human activities from video images based on concept hierarchy of actions",
      "author" : [ "A. Kojima", "T. Tamura", "K. Fukunaga" ],
      "venue" : "International Journal of Computer Vision (IJCV)",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Baby talk: Understanding and generating simple image descriptions",
      "author" : [ "G. Kulkarni", "V. Premraj", "S. Dhar", "S. Li", "Y. Choi", "A.C. Berg", "T.L. Berg" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Treetalk: Composition and compression of trees for image descriptions",
      "author" : [ "P. Kuznetsova", "V. Ordonez", "T.L. Berg", "U.C. Hill", "Y. Choi" ],
      "venue" : "Transactions of the Association for Computational Linguistics (TACL)",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Meteor universal: Language specific translation evaluation for any target language",
      "author" : [ "M.D.A. Lavie" ],
      "venue" : "ACL 2014 p. 376",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Rouge: A package for automatic evaluation of summaries",
      "author" : [ "C.Y. Lin" ],
      "venue" : "Text Summarization Branches Out: Proceedings of the ACL-04 Workshop. pp. 74–81",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Deep captioning with multimodal recurrent neural networks (m-rnn)",
      "author" : [ "J. Mao", "W. Xu", "Y. Yang", "J. Wang", "A.L. Yuille" ],
      "venue" : "arXiv:1412.6632",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Midge: Generating image descriptions from computer vision detections",
      "author" : [ "M. Mitchell", "J. Dodge", "A. Goyal", "K. Yamaguchi", "K. Stratos", "X. Han", "A. Mensch", "A.C. Berg", "T.L. Berg", "H.D. III" ],
      "venue" : "Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics (EACL)",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Jointly modeling embedding and translation to bridge video and language",
      "author" : [ "Y. Pan", "T. Mei", "T. Yao", "H. Li", "Y. Rui" ],
      "venue" : "arXiv:1505.01861",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "BLEU: a method for automatic evaluation of machine translation",
      "author" : [ "K. Papineni", "S. Roukos", "T. Ward", "W.J. Zhu" ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Coherent multi-sentence video description with variable level of detail",
      "author" : [ "A. Rohrbach", "M. Rohrbach", "W. Qiu", "A. Friedrich", "M. Pinkal", "B. Schiele" ],
      "venue" : "Proceedings of the German Confeence on Pattern Recognition (GCPR)",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A dataset for movie description",
      "author" : [ "A. Rohrbach", "M. Rohrbach", "N. Tandon", "B. Schiele" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Translating video content to natural language descriptions",
      "author" : [ "M. Rohrbach", "W. Qiu", "I. Titov", "S. Thater", "M. Pinkal", "B. Schiele" ],
      "venue" : "Proceedings of the IEEE International Conference on Computer Vision (ICCV)",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Integrating language and vision to generate natural language descriptions of videos in the wild",
      "author" : [ "J. Thomason", "S. Venugopalan", "S. Guadarrama", "K. Saenko", "R.J. Mooney" ],
      "venue" : "Proceedings of the International Conference on Computational Linguistics (COLING)",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Using descriptive video services to create a large data source for video annotation research",
      "author" : [ "A. Torabi", "C. Pal", "H. Larochelle", "A. Courville" ],
      "venue" : "arXiv:1503.01070v1",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Cider: Consensus-based image description evaluation",
      "author" : [ "R. Vedantam", "C.L. Zitnick", "D. Parikh" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Sequence to sequence – video to text",
      "author" : [ "S. Venugopalan", "M. Rohrbach", "J. Donahue", "R. Mooney", "T. Darrell", "K. Saenko" ],
      "venue" : "arXiv:1505.00487",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Translating videos to natural language using deep recurrent neural networks",
      "author" : [ "S. Venugopalan", "H. Xu", "J. Donahue", "M. Rohrbach", "R. Mooney", "K. Saenko" ],
      "venue" : "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Show and tell: A neural image caption generator",
      "author" : [ "O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Action recognition with improved trajectories",
      "author" : [ "H. Wang", "C. Schmid" ],
      "venue" : "Proceedings of the IEEE International Conference on Computer Vision (ICCV)",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Show, attend and tell: Neural image caption generation with visual attention",
      "author" : [ "K. Xu", "J. Ba", "R. Kiros", "A. Courville", "R. Salakhutdinov", "R. Zemel", "Y. Bengio" ],
      "venue" : "arXiv:1502.03044",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Jointly modeling deep video and compositional text to bridge vision and language in a unified framework",
      "author" : [ "R. Xu", "C. Xiong", "W. Chen", "J.J. Corso" ],
      "venue" : "Proceedings of the Conference on Artificial Intelligence (AAAI)",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Describing videos by exploiting temporal structure",
      "author" : [ "L. Yao", "A. Torabi", "K. Cho", "N. Ballas", "C. Pal", "H. Larochelle", "A. Courville" ],
      "venue" : "arXiv:1502.08029v4",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
      "author" : [ "P. Young", "A. Lai", "M. Hodosh", "J. Hockenmaier" ],
      "venue" : "Transactions of the Association for Computational Linguistics (TACL) 2, 67–78",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Learning Deep Features for Scene Recognition using Places Database",
      "author" : [ "B. Zhou", "A. Lapedriza", "J. Xiao", "A. Torralba", "A. Oliva" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 27,
      "context" : "The recent advances in image captioning as well as the release of large-scale movie description datasets such as MPII-MD [28] allow to study this task in more depth.",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 5,
      "context" : "Multiple works have successfully addressed the image captioning problem [6, 16, 17, 35].",
      "startOffset" : 72,
      "endOffset" : 87
    }, {
      "referenceID" : 15,
      "context" : "Multiple works have successfully addressed the image captioning problem [6, 16, 17, 35].",
      "startOffset" : 72,
      "endOffset" : 87
    }, {
      "referenceID" : 16,
      "context" : "Multiple works have successfully addressed the image captioning problem [6, 16, 17, 35].",
      "startOffset" : 72,
      "endOffset" : 87
    }, {
      "referenceID" : 34,
      "context" : "Multiple works have successfully addressed the image captioning problem [6, 16, 17, 35].",
      "startOffset" : 72,
      "endOffset" : 87
    }, {
      "referenceID" : 12,
      "context" : "Many of the proposed methods rely on Long-Short Term Memory networks (LSTMs) [13].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 27,
      "context" : "In the meanwhile, two large-scale movie description datasets have been proposed, namely MPII Movie Description (MPIIMD) [28] and Montreal Video Annotation Dataset (M-VAD) [31].",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 30,
      "context" : "In the meanwhile, two large-scale movie description datasets have been proposed, namely MPII Movie Description (MPIIMD) [28] and Montreal Video Annotation Dataset (M-VAD) [31].",
      "startOffset" : 171,
      "endOffset" : 175
    }, {
      "referenceID" : 27,
      "context" : "Works addressing these datasets [28, 33, 39] show that they are indeed challenging in terms of visual recognition and automatic description.",
      "startOffset" : 32,
      "endOffset" : 44
    }, {
      "referenceID" : 32,
      "context" : "Works addressing these datasets [28, 33, 39] show that they are indeed challenging in terms of visual recognition and automatic description.",
      "startOffset" : 32,
      "endOffset" : 44
    }, {
      "referenceID" : 38,
      "context" : "Works addressing these datasets [28, 33, 39] show that they are indeed challenging in terms of visual recognition and automatic description.",
      "startOffset" : 32,
      "endOffset" : 44
    }, {
      "referenceID" : 1,
      "context" : "MSVD [2]), but a detailed analysis of the difficulties is missing.",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 8,
      "context" : "Automatic image description has been studied in the past [9, 19, 20, 24], however it regained attention just recently.",
      "startOffset" : 57,
      "endOffset" : 72
    }, {
      "referenceID" : 18,
      "context" : "Automatic image description has been studied in the past [9, 19, 20, 24], however it regained attention just recently.",
      "startOffset" : 57,
      "endOffset" : 72
    }, {
      "referenceID" : 19,
      "context" : "Automatic image description has been studied in the past [9, 19, 20, 24], however it regained attention just recently.",
      "startOffset" : 57,
      "endOffset" : 72
    }, {
      "referenceID" : 23,
      "context" : "Automatic image description has been studied in the past [9, 19, 20, 24], however it regained attention just recently.",
      "startOffset" : 57,
      "endOffset" : 72
    }, {
      "referenceID" : 5,
      "context" : "Multiple works have been proposed [6, 8, 16, 17, 23, 35, 37].",
      "startOffset" : 34,
      "endOffset" : 60
    }, {
      "referenceID" : 7,
      "context" : "Multiple works have been proposed [6, 8, 16, 17, 23, 35, 37].",
      "startOffset" : 34,
      "endOffset" : 60
    }, {
      "referenceID" : 15,
      "context" : "Multiple works have been proposed [6, 8, 16, 17, 23, 35, 37].",
      "startOffset" : 34,
      "endOffset" : 60
    }, {
      "referenceID" : 16,
      "context" : "Multiple works have been proposed [6, 8, 16, 17, 23, 35, 37].",
      "startOffset" : 34,
      "endOffset" : 60
    }, {
      "referenceID" : 22,
      "context" : "Multiple works have been proposed [6, 8, 16, 17, 23, 35, 37].",
      "startOffset" : 34,
      "endOffset" : 60
    }, {
      "referenceID" : 34,
      "context" : "Multiple works have been proposed [6, 8, 16, 17, 23, 35, 37].",
      "startOffset" : 34,
      "endOffset" : 60
    }, {
      "referenceID" : 36,
      "context" : "Multiple works have been proposed [6, 8, 16, 17, 23, 35, 37].",
      "startOffset" : 34,
      "endOffset" : 60
    }, {
      "referenceID" : 39,
      "context" : "Also new datasets have been released, Flickr30k [40] and MS COCO Captions [3], where [3] additionally presents a standardized setup for image captioning evaluation.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 2,
      "context" : "Also new datasets have been released, Flickr30k [40] and MS COCO Captions [3], where [3] additionally presents a standardized setup for image captioning evaluation.",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 2,
      "context" : "Also new datasets have been released, Flickr30k [40] and MS COCO Captions [3], where [3] additionally presents a standardized setup for image captioning evaluation.",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 4,
      "context" : "[5] compares them with respect to the novelty of generated descriptions and additionally proposes a nearest neighbor baseline that improves over recent methods.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "In the past video description has been addressed in semirealistic settings [1, 18], on a small scale [4, 11, 30] or in constrained scenarios like cooking [27, 29].",
      "startOffset" : 75,
      "endOffset" : 82
    }, {
      "referenceID" : 17,
      "context" : "In the past video description has been addressed in semirealistic settings [1, 18], on a small scale [4, 11, 30] or in constrained scenarios like cooking [27, 29].",
      "startOffset" : 75,
      "endOffset" : 82
    }, {
      "referenceID" : 3,
      "context" : "In the past video description has been addressed in semirealistic settings [1, 18], on a small scale [4, 11, 30] or in constrained scenarios like cooking [27, 29].",
      "startOffset" : 101,
      "endOffset" : 112
    }, {
      "referenceID" : 10,
      "context" : "In the past video description has been addressed in semirealistic settings [1, 18], on a small scale [4, 11, 30] or in constrained scenarios like cooking [27, 29].",
      "startOffset" : 101,
      "endOffset" : 112
    }, {
      "referenceID" : 29,
      "context" : "In the past video description has been addressed in semirealistic settings [1, 18], on a small scale [4, 11, 30] or in constrained scenarios like cooking [27, 29].",
      "startOffset" : 101,
      "endOffset" : 112
    }, {
      "referenceID" : 26,
      "context" : "In the past video description has been addressed in semirealistic settings [1, 18], on a small scale [4, 11, 30] or in constrained scenarios like cooking [27, 29].",
      "startOffset" : 154,
      "endOffset" : 162
    }, {
      "referenceID" : 28,
      "context" : "In the past video description has been addressed in semirealistic settings [1, 18], on a small scale [4, 11, 30] or in constrained scenarios like cooking [27, 29].",
      "startOffset" : 154,
      "endOffset" : 162
    }, {
      "referenceID" : 26,
      "context" : "[27]) study the task of describing a short clip with a single sentence.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 5,
      "context" : "[6] first proposed to describe videos using an LSTM, relying on precomputed CRF scores from [27].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 26,
      "context" : "[6] first proposed to describe videos using an LSTM, relying on precomputed CRF scores from [27].",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 33,
      "context" : "[34] extended this work to extract CNN features from frames which are max-pooled over time.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "[25] proposes a framework that consists of a 2-D and/or 3-D CNN and the LSTM is trained jointly with a visual-semantic embedding to ensure better coherence between video and text.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 37,
      "context" : "[38] jointly addresses the language generation and video/language retrieval tasks by learning a joint embedding model for a deep video model and compositional semantic language model.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 27,
      "context" : "Recently two large-scale movie description datasets have been proposed, MPII Movie Description (MPII-MD) [28] and Montreal Video Annotation Dataset (M-VAD) [31].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 30,
      "context" : "Recently two large-scale movie description datasets have been proposed, MPII Movie Description (MPII-MD) [28] and Montreal Video Annotation Dataset (M-VAD) [31].",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 26,
      "context" : "TACoS Multi-Level [27], thus one has to rely on the weak annotations of the sentence descriptions.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 38,
      "context" : "To handle this challenging scenario [39] proposes an attention based model which selects the most relevant temporal segments in a video and incorporates 3-D CNN and generates a sentence using an LSTM.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 32,
      "context" : "[33] proposes an encoder-decoder framework, where a single LSTM encodes the input video frame by frame and decodes it into a sentence, outperforming [39].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 38,
      "context" : "[33] proposes an encoder-decoder framework, where a single LSTM encodes the input video frame by frame and decodes it into a sentence, outperforming [39].",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 5,
      "context" : "Our approach for sentence generation is most similar to [6] and we also rely on their LSTM implementation based on Caffe [15].",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 14,
      "context" : "Our approach for sentence generation is most similar to [6] and we also rely on their LSTM implementation based on Caffe [15].",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 27,
      "context" : "To extract labels from sentences we rely on the semantic parser of [28], however we treat the labels differently to handle the weak supervision (see Section 3.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 27,
      "context" : "We show that this improves over [28] and [33].",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 32,
      "context" : "We show that this improves over [28] and [33].",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 35,
      "context" : "We first train the visual classifiers for verbs, objects and places, using different visual features: DT (dense trajectories [36]), LSDA (large scale object detector [14]) and PLACES (Places-CNN [41]).",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 13,
      "context" : "We first train the visual classifiers for verbs, objects and places, using different visual features: DT (dense trajectories [36]), LSDA (large scale object detector [14]) and PLACES (Places-CNN [41]).",
      "startOffset" : 166,
      "endOffset" : 170
    }, {
      "referenceID" : 40,
      "context" : "We first train the visual classifiers for verbs, objects and places, using different visual features: DT (dense trajectories [36]), LSDA (large scale object detector [14]) and PLACES (Places-CNN [41]).",
      "startOffset" : 195,
      "endOffset" : 199
    }, {
      "referenceID" : 5,
      "context" : "For the language generation we rely on a LSTM network which has been successfully used for image and video description [6, 33].",
      "startOffset" : 119,
      "endOffset" : 126
    }, {
      "referenceID" : 32,
      "context" : "For the language generation we rely on a LSTM network which has been successfully used for image and video description [6, 33].",
      "startOffset" : 119,
      "endOffset" : 126
    }, {
      "referenceID" : 27,
      "context" : "As in [28] we parse the sentences to obtain a set of labels (single words or short phrases, e.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 27,
      "context" : "However, in contrast to [28] we do not want to keep all of these initial labels as they are noisy, but select only visual ones which actually can be robustly recognized.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 35,
      "context" : "(DT) [36] for verbs, large scale object detector responses (LSDA) [14] for objects and scene classification scores (PLACES) [41] for places.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 13,
      "context" : "(DT) [36] for verbs, large scale object detector responses (LSDA) [14] for objects and scene classification scores (PLACES) [41] for places.",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 40,
      "context" : "(DT) [36] for verbs, large scale object detector responses (LSDA) [14] for objects and scene classification scores (PLACES) [41] for places.",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 27,
      "context" : "Now, how do we select visual labels for our semantic groups? In order to find the verbs among the labels we rely on the semantic parser of [28].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 40,
      "context" : "Next, we look up the list of “places” used in [41] and search for corresponding words among our labels.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 13,
      "context" : "We look up the object classes used in [14] and search for these “objects”, as well as their base forms (e.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 5,
      "context" : "We rely on the basic LSTM architecture proposed in [6] for video description.",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 5,
      "context" : "[6] compares three variants: (a) an encoder-decoder architecture, (b) a decoder architecture with visual max predictions, and (c) a decoder architecture with visual probabilistic predictions.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "In the 2-layer architecture, the output of the first layer is used as input for the second layer (Figure 2b) and was used by [6] for video description.",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 5,
      "context" : "we also compare to a 2-layer factored architecture [6], where the first layer only gets the language as input and the second gets the output of the first layer as well as the visual input.",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 11,
      "context" : "Dropout placement: To learn a more robust network which is less likely to overfit we rely on a dropout [12].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 14,
      "context" : "The polynomial learning strategy has been shown to give good results faster without tweaking step size for GoogleNet implemented by Sergio Guadarrama in Caffe [15].",
      "startOffset" : 159,
      "endOffset" : 163
    }, {
      "referenceID" : 27,
      "context" : "In this section we first analyze our approach on the MPII-MD [28] dataset and explore different design choices.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 27,
      "context" : "We build on the labels discovered by our semantic parser [28] and additionally match these labels to sentences which the parser failed to process.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 27,
      "context" : "We use the visual features (DT, LSDA, PLACES) provided with the MPII-MD dataset [28].",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 20,
      "context" : "We evaluate our method on the validation set (4,930 clips) using the METEOR [21] score, which, according to [7, 32], supersedes other popular measures, such as BLEU [26], ROUGE [22], in terms of agreement with human judgments.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 6,
      "context" : "We evaluate our method on the validation set (4,930 clips) using the METEOR [21] score, which, according to [7, 32], supersedes other popular measures, such as BLEU [26], ROUGE [22], in terms of agreement with human judgments.",
      "startOffset" : 108,
      "endOffset" : 115
    }, {
      "referenceID" : 31,
      "context" : "We evaluate our method on the validation set (4,930 clips) using the METEOR [21] score, which, according to [7, 32], supersedes other popular measures, such as BLEU [26], ROUGE [22], in terms of agreement with human judgments.",
      "startOffset" : 108,
      "endOffset" : 115
    }, {
      "referenceID" : 25,
      "context" : "We evaluate our method on the validation set (4,930 clips) using the METEOR [21] score, which, according to [7, 32], supersedes other popular measures, such as BLEU [26], ROUGE [22], in terms of agreement with human judgments.",
      "startOffset" : 165,
      "endOffset" : 169
    }, {
      "referenceID" : 21,
      "context" : "We evaluate our method on the validation set (4,930 clips) using the METEOR [21] score, which, according to [7, 32], supersedes other popular measures, such as BLEU [26], ROUGE [22], in terms of agreement with human judgments.",
      "startOffset" : 177,
      "endOffset" : 181
    }, {
      "referenceID" : 31,
      "context" : "The authors of CIDEr [32] showed that METEOR also outperforms CIDEr when the number of references is small and in the case of MPII-MD we have typically only a single reference.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 27,
      "context" : "We compare the best method of [28], the recently proposed method S2VT [33] and our proposed “Visual Labels”-LSTM on the test set of the MPII-MD dataset (6,578 clips).",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 32,
      "context" : "We compare the best method of [28], the recently proposed method S2VT [33] and our proposed “Visual Labels”-LSTM on the test set of the MPII-MD dataset (6,578 clips).",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 31,
      "context" : "We report all popular automatic evaluation measures, CIDEr [32], BLEU [26], ROUGE [22] and METEOR [21], computed using the evaluation code of [3].",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 25,
      "context" : "We report all popular automatic evaluation measures, CIDEr [32], BLEU [26], ROUGE [22] and METEOR [21], computed using the evaluation code of [3].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 21,
      "context" : "We report all popular automatic evaluation measures, CIDEr [32], BLEU [26], ROUGE [22] and METEOR [21], computed using the evaluation code of [3].",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 20,
      "context" : "We report all popular automatic evaluation measures, CIDEr [32], BLEU [26], ROUGE [22] and METEOR [21], computed using the evaluation code of [3].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 2,
      "context" : "We report all popular automatic evaluation measures, CIDEr [32], BLEU [26], ROUGE [22] and METEOR [21], computed using the evaluation code of [3].",
      "startOffset" : 142,
      "endOffset" : 145
    }, {
      "referenceID" : 27,
      "context" : "We also perform a human evaluation, by randomly selecting 1300 video snippets and asking AMT turkers to rank three systems (the best SMT of [28], S2VT [33] and ours) with respect to Correctness, Grammar and Relevance, similar to [28].",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 32,
      "context" : "We also perform a human evaluation, by randomly selecting 1300 video snippets and asking AMT turkers to rank three systems (the best SMT of [28], S2VT [33] and ours) with respect to Correctness, Grammar and Relevance, similar to [28].",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 27,
      "context" : "We also perform a human evaluation, by randomly selecting 1300 video snippets and asking AMT turkers to rank three systems (the best SMT of [28], S2VT [33] and ours) with respect to Correctness, Grammar and Relevance, similar to [28].",
      "startOffset" : 229,
      "endOffset" : 233
    }, {
      "referenceID" : 27,
      "context" : "While we rely on identical features and similar labels as [28], we significantly improve the performance in all automatic measures, specifically by 1.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 27,
      "context" : "Best SMT of [28] 8.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 32,
      "context" : "08 S2VT [33] 9.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 32,
      "context" : "Moreover, we improve over the recent approach of [33], which also uses LSTM to generate video descriptions.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 27,
      "context" : "An interesting characteristic is the output vocabulary size, which is 94 for [28], 86 for [33] and 605 for our method, while the test set contains 6422 unique words.",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 32,
      "context" : "An interesting characteristic is the output vocabulary size, which is 94 for [28], 86 for [33] and 605 for our method, while the test set contains 6422 unique words.",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 27,
      "context" : "Despite the recent advances in the video description domain, including our proposed approach, the video description performance on the movie description datasets (MPII-MD [28] and M-VAD [31]) remains relatively low.",
      "startOffset" : 171,
      "endOffset" : 175
    }, {
      "referenceID" : 30,
      "context" : "Despite the recent advances in the video description domain, including our proposed approach, the video description performance on the movie description datasets (MPII-MD [28] and M-VAD [31]) remains relatively low.",
      "startOffset" : 186,
      "endOffset" : 190
    }, {
      "referenceID" : 27,
      "context" : "In this section we want to take a closer look at three methods, best SMT of [28], S2VT [33] and ours, in order to understand where these methods succeed and where they fail.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 32,
      "context" : "In this section we want to take a closer look at three methods, best SMT of [28], S2VT [33] and ours, in order to understand where these methods succeed and where they fail.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 27,
      "context" : "SMT [28] Someone is a man, someone is a man.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 32,
      "context" : "S2VT [33] Someone looks at him, someone turns to someone.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 27,
      "context" : "SMT [28] The car is a water of the water.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 32,
      "context" : "S2VT [33] On the door, opens the door opens.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 27,
      "context" : "SMT [28] Someone is down the door, someone is a back of the door, and someone is a door.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 32,
      "context" : "S2VT [33] Someone shakes his head and looks at someone.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 27,
      "context" : "<−short Sentences long−> M E T E O R Best SMT of [28] S2VT [33] Our",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 32,
      "context" : "<−short Sentences long−> M E T E O R Best SMT of [28] S2VT [33] Our",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 27,
      "context" : "<−short Sentences long−> M E T E O R Best SMT of [28] S2VT [33] Our",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 32,
      "context" : "<−short Sentences long−> M E T E O R Best SMT of [28] S2VT [33] Our",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 27,
      "context" : "<−easy Sentences difficult−> M E T E O R Best SMT of [28] S2VT [33] Our",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 32,
      "context" : "<−easy Sentences difficult−> M E T E O R Best SMT of [28] S2VT [33] Our",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 27,
      "context" : "<−easy Sentences difficult−> M E T E O R Best SMT of [28] S2VT [33] Our",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 32,
      "context" : "<−easy Sentences difficult−> M E T E O R Best SMT of [28] S2VT [33] Our",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 27,
      "context" : "w e a th e r (7 ) M E T E O R Best SMT of [28] S2VT [33] Our",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 32,
      "context" : "w e a th e r (7 ) M E T E O R Best SMT of [28] S2VT [33] Our",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 9,
      "context" : "“motion”, “perception”, “competition”, “emotion”), defined for most synsets in WordNet [10].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 27,
      "context" : "We obtain the sense information from the semantic parser of [28], thus senses might be noisy.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 27,
      "context" : "We find that our method is best for all topics except “communication”, where [28] wins.",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 27,
      "context" : "The most frequent verbs in this topic are “look up” and “nod”, which are also frequent in the dataset and in the sentences produced by [28].",
      "startOffset" : 135,
      "endOffset" : 139
    } ],
    "year" : 2015,
    "abstractText" : "Generating descriptions for videos has many applications including assisting blind people and human-robot interaction. The recent advances in image captioning as well as the release of large-scale movie description datasets such as MPII-MD [28] allow to study this task in more depth. Many of the proposed methods for image captioning rely on pre-trained object classifier CNNs and Long-Short Term Memory recurrent networks (LSTMs) for generating descriptions. While image description focuses on objects, we argue that it is important to distinguish verbs, objects, and places in the challenging setting of movie description. In this work we show how to learn robust visual classifiers from the weak annotations of the sentence descriptions. Based on these visual classifiers we learn how to generate a description using an LSTM. We explore different design choices to build and train the LSTM and achieve the best performance to date on the challenging MPII-MD dataset. We compare and analyze our approach and prior work along various dimensions to better understand the key challenges of the movie description task.",
    "creator" : "TeX"
  }
}