{
  "name" : "1202.6009.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "josep.domingo@urv.cat" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n20 2.\n60 09\nv1 [\nThe purpose of statistical disclosure control (SDC) of microdata, a.k.a. data anonymization or privacy-preserving data mining, is to publish data sets containing the answers of individual respondents in such a way that the respondents corresponding to the released records cannot be re-identified and the released data are analytically useful. SDC methods are either based on masking the original data, generating synthetic versions of them or creating hybrid versions by combining original and synthetic data. The choice of SDC methods for categorical data, especially nominal data, is much smaller than the choice of methods for numerical data. We mitigate this problem by introducing a numerical mapping for hierarchical nominal data which allows computing means, variances and covariances on them. Keywords: Statistical disclosure control; Data anonymization; Privacypreserving data mining; Variance of hierarchical data; Hierarchical nominal data"
    }, {
      "heading" : "1 Introduction",
      "text" : "Statistical disclosure control (SDC, [1, 4, 8, 3, 5]), a.k.a. data anonymization and sometimes as privacy-preserving data mining, aims at making possible the\n∗This work was partly supported by the Government of Catalonia under grant 2009 SGR 1135, by the Spanish Government through projects TSI2007-65406-C03-01 “E-AEGIS” and CONSOLIDER INGENIO 2010 CSD2007-00004 “ARES”, and by the European Comission under FP7 project “DwB”. The author is partially supported as an ICREA Acadèmia researcher by the Government of Catalonia.\npublication of statistical data in such a way that the individual responses of specific users cannot be inferred from the published data and background knowledge available to intruders. If the data set being published consists of records corresponding to individuals, usual SDC methods operate by masking original data (via perturbation or detail reduction), by generating synthetic (simulated) data preserving some statistical features of the original data or by producing hybrid data obtained as a combination of original and synthetic data. Whatever the protection method chosen, the resulting data should still preserve enough analytical validity for their publication to be useful to potential users.\nA microdata set can be defined as a file with a number of records, where each record contains a number of attributes on an individual respondent. Attributes can be classified depending on their range and the operations that can be performed on them:\n1. Numerical. An attribute is considered numerical if arithmetical operations can be performed on it. Examples are income and age. When designing methods to protect numerical data, one has the advantage that arithmetical operations are possible, and the drawback that every combination of numerical values in the original data set is likely to be unique, which leads to disclosure if no action is taken.\n2. Categorical. An attribute is considered categorical when it takes values over a finite set and standard arithmetical operations on it do not make sense. Two main types of categorical attributes can be distinguished:\n(a) Ordinal. An ordinal attribute takes values in an ordered range of categories. Thus, the ≤, max and min operators are meaningful and can be used by SDC techniques for ordinal data. The instruction level and the political preferences (left-right) are examples of ordinal attributes.\n(b) Nominal. A nominal attribute takes values in an unordered range of categories. The only possible operator is comparison for equality. Nominal attributes can further be divided into two types:\ni. Hierarchical. A hierarchical nominal attribute takes values from a hierarchical classification. For example, plants are classified using Linnaeus’s taxonomy, the type of a disease is also selected from a hierarchical taxonomy, and the type of an attribute can be selected from the hierarchical classification we propose in this section.\nii. Non-hierarchical. A non-hierarchical nominal attribute takes values from a flat hierarchy. Examples of such attributes could be the preferred soccer team, the address of an individual, the civil status (married, single, divorced, widow/er), the eye color, etc.\nThis paper focuses on finding a numerical mapping of nominal attributes, and more precisely hierarchical nominal attributes. In addition to other conceivable applications not dealt with in this paper, such a mapping can be used\nto anonymize nominal data in ways so far reserved to numerical data. The interest of this is that many more SDC methods exist for anonymizing numerical data than categorical and especially nominal data.\nAssuming a hierarchy is less restrictive than it would appear, because very often a non-hierarchical attribute can be turned into a hierarchical one if its flat hierarchy can be developed into a multilevel hierarchy. For instance, the preferred soccer and the address of an individual have been mentioned as nonhierarchical attributes; however, a hierarchy of soccer teams by continent and country could be conceived, and addresses can be hierarchically clustered by neighborhood, city, state, country, etc. Furthermore, well-known approaches to anonynimization, like k-anonymity [7], assume that any attribute can be generalized, i.e. that an attribute hierarchy can be defined and values at lower levels of the hierarchy can be replaced by values at higher levels."
    }, {
      "heading" : "1.1 Contribution and plan of this paper",
      "text" : "We propose to associate a number to each categorical value of a hierarchical nominal attribute, namely a form of centrality of that category within the attribute’s hierarchy. We show how this allows computation of centroids, variances and covariances of hierarchical nominal data.\nSection 2 gives background on the variance of hierarchical nominal attributes. Section 3 defines a tree centrality measure called marginality and presents the numerical mapping. Section 4 exploits the numerical mapping to compute means, variances and covariances of hierarchical nominal data. Conclusions are drawn in Section 5."
    }, {
      "heading" : "2 Background",
      "text" : "We next recall the variance measure for hierarchical nominal attributes introduced in [2]. To the best of our knowledge, this is the first measure which captures the variability of a sample of values of a hierarchical nominal attribute by taking into account the semantics of the hierarchy. The intuitive idea is that a set of nominal values belonging to categories which are all children of the same parent category in the hierarchy has smaller variance that a set with children from different parent categories.\nAlgorithm 1 (Nominal variance in [2])\n1. Let the hierarchy of categories of a nominal attribute X be such that b is the maximum number of children that a parent category can have in the hierarchy.\n2. Given a sample TX of nominal categories drawn from X, place them in the tree representing the hierarchy of X. Prune the subtrees whose nodes have no associated sample values. If there are repeated sample values, there will\nbe several nominal values associated to one or more nodes (categories) in the pruned tree.\n3. Label as follows the edges remaining in the tree from the root node to each of its children:\n• If b is odd, consider the following succession of labels l0 = (b− 1)/2, l1 = (b−1)/2−1, l2 = (b−1)/2+1, l3 = (b−1)/2−2, l4 = (b−1)/2+2, · · · , lb−2 = 0, lb−1 = b− 1. • If b is even, consider the following succession of labels l0 = (b− 2)/2, l1 = (b−2)/2+1, l2 = (b−2)/2−1, l3 = (b−2)/2+2, l4 = (b−2)/2−2, · · · , lb−2 = 0, lb−1 = b− 1. • Label the edge leading to the child with most categories associated to its descendant subtree as l0, the edge leading to the child with the second highest number of categories associated to its descendant subtree as l1, the one leading to the child with the third highest number of categories associated to its descendant subtree as l2 and, in general, the edge leading to the child with the i-th highest number of categories associated to its descendant subtree as li−1. Since there are at most b children, the set of labels {l0, · · · , lb−1} should suffice. Thus an edge label can be viewed as a b-ary digit (to the base b).\n4. Recursively repeat Step 3 taking instead of the root node each of the root’s child nodes.\n5. Assign to values associated to each node in the hierarchy a node label consisting of a b-ary number constructed from the edge labels, more specifically as the concatenation of the b-ary digits labeling the edges along the path from the root to the node: the label of the edge starting from the root is the most significant one and the edge label closest to the specific node is the least significant one.\n6. Let L be the maximal length of the leaf b-ary labels. Append as many l0 digits as needed in the least significant positions to the shorter labels so that all of them eventually consist of L digits.\n7. Let TX(0) be the set of b-ary digits in the least significant positions of the node labels (the “units” positions); let TX(1) be the set of b-ary digits in the second least significant positions of the node labels (the “tens” positions), and so on, until TX(L−1) which is the set of digits in the most significant positions of the node labels.\n8. Compute the variance of the sample as\nV arH(TX) = V ar(TX(0)) + b 2 · V ar(TX(1)) + · · ·\n+ b2(L−1) · V ar(TX(L− 1)) (1) where V ar(·) is the usual numerical variance.\nIn Section 4.2 below we will show that an equivalent measure can be obtained in a simpler and more manageable way."
    }, {
      "heading" : "3 A numerical mapping for nominal hierarchical",
      "text" : "data\nConsider a nominal attribute X taking values from a hierarchical classification. Let TX be a sample of values of X . Each value x ∈ TX can be associated two numerical values:\n• The sample frequency of x;\n• Some centrality measure of x within the hierarchy of X .\nWhile the frequency depends on the particular sample, centrality measures depend both on the attribute hierarchy and the sample. Known tree centralities attempt to determine the “middle” of a tree [6]. We are rather interested in finding how far from the middle is each node of the tree, that is, how marginal it is. We next propose an algorithm to compute a new measure of the marginality of the values in the sample TX .\nAlgorithm 2 (Marginality of nominal values)\n1. Given a sample TX of nominal categorical values drawn from X, place them in the tree representing the hierarchy of X. There is a one-to-one mapping between the set of tree nodes and the set of categories where X takes values. Prune the subtrees whose nodes have no associated sample values. If there are repeated sample values, there will be several nominal values associated to one or more nodes (categories) in the pruned tree.\n2. Let L be the depth of the pruned tree. Associate weight 2L−1 to edges linking the root of the hierarchy to its immediate descendants (depth 1), weight 2L−2 to edges linking the depth 1 descendants to their own descendants (depth 2), and so on, up to weight 20 = 1 to the edges linking descendants at depth L− 1 with those at depth L. In general, weight 2L−i is assigned to edges linking nodes at depth i− 1 with those at depth i, for i = 1 to L.\n3. For each nominal value xj in the sample, its marginality m(xj) is defined and computed as\nm(xj) = ∑\nxl∈TX−{xj}\nd(xj , xl)\nwhere d(xj , xl) is the sum of the edge weights along the shortest path from the tree node corresponding to xj and the tree node corresponding to xl.\nClearly, the greater m(xj), the more marginal (i.e. the less central) is xj . Some properties follow which illustrate the rationale of the distance and the weights used to compute the marginality.\nLemma 1 d(·, ·) is a distance in the mathematical sense.\nBeing the length of a path, it is immediate to check that d(·, ·) satisfies reflexivity, symmetry and subadditivity. The rationale of the above exponential weight scheme is to give more weight to differences at higher levels of the hierarchy; specifically, the following property is satisfied.\nLemma 2 The distance between any non-root node nj and its immediate ancestor is greater than the distance between nj and any of its descendants.\nProof: Let L be the depth of the overall tree and Lj be the depth of nj . The distance between nj and its immediate ancestor is 2\nL−Lj . The distance between nj and its most distant ancestor is\n1 + 2 + · · ·+ 2L−Lj−1 = 2L−Lj − 1\nLemma 3 The distance between any two nodes at the same depth is greater than the longest distance within the subtree rooted at each node.\nProof: Let L be the depth of the overall tree and Lj be the depth of the two nodes. The shortest distance between both nodes occurs when they have the same parent and it is\n2 · 2L−Lj = 2L−Lj+1.\nThe longest distance within any of the two subtrees rooted at the two nodes at depth Lj is the length of the path between two leaves at depth L, which is\n2 · (1 + 2 + · · ·+ 2L−Lj−1) = 2(2L−Lj − 1) = 2L−Lj+1 − 2"
    }, {
      "heading" : "4 Statistical analysis of numerically mapped nom-",
      "text" : "inal data\nIn the previous section we have shown how a nominal value xj can be associated a marginality measure m(xj). In this section, we show how this numerical magnitude can be used in statistical analysis."
    }, {
      "heading" : "4.1 Mean",
      "text" : "The mean of a sample of nominal values cannot be computed in the standard sense. However, it can be reasonably approximated by the least marginal value, that is, by the most central value in terms of the hierarchy.\nDefinition 1 (Marginality-based approximated mean) Given a sample TX of a hierarchical nominal attribute X, the marginality-based approximated mean is defined as\nMeanM (TX) = arg min xj∈TX m(xj)\nif one wants the mean to be a nominal value, or\nNum meanM (TX) = min xj∈TX m(xj)\nif one wants a numerical mean value."
    }, {
      "heading" : "4.2 Variance",
      "text" : "In Section 2 above, we recalled a measure of variance of a hierarchical nominal attribute proposed in [2] which takes the semantics of the hierarchy into account. Interestingly, it turns out that the average marginality of a sample is an equivalent way to capture the same notion of variance.\nDefinition 2 (Marginality-based variance) Given a sample TX of n values drawn from a hierarchical nominal attribute X, the marginality-based sample variance is defined as\nV arM (TX) =\n∑\nxj∈TX m(xj)\nn\nThe following lemma is proven in the Appendix.\nLemma 4 The V arM (·) measure and the V arH(·) specified by Algorithm 1 in Section 2 are equivalent."
    }, {
      "heading" : "4.3 Covariance matrix",
      "text" : "It is not difficult to generalize the sample variance introduced in Definition 2 to define the sample covariance of two nominal attributes.\nDefinition 3 (Marginality-based covariance) Given a bivariate sample T(X,Y ) consisting of n ordered pairs of values {(x1, y1), · · · , (xn, yn)} drawn from the ordered pair of nominal attributes (X,Y ), the marginality-based sample covariance is defined as\nCovarM (T(X,Y )) =\n∑n\nj=1\n√\nm(xj)m(yj)\nn\nThe above definition yields a non-negative covariance whose value is higher when the marginalities of the values taken by X and Y are positively correlated: as the values taken by X become more marginal, so become the values taken by Y .\nGiven a multivariate data set T containing a sample of d nominal attributes X1, · · · , Xd, using Definitions 2 and 3 yields a covariance matrix S = {sjl}, for 1 ≤ j ≤ d and 1 ≤ l ≤ d, where sjj = V arM (Tj), sjl = CovarM (Tjl) for j 6= l, Tj is the column of values taken by X\nj in T and Tjl = (Tj, Tl). We can use the following distance definition for records with numerical,\nnominal or hierarchical attributes.\nDefinition 4 (SSE-distance) The SSE-distance between two records x1 and x2 in a data set with d attributes is\nδ(x1,x2) =\n√\n(S2)112 (S2)1 + · · ·+ (S 2)d12 (S2)d (2)\nwhere (S2)l12 is the variance of the l-th attribute over the group formed by x1 and x2, and (S 2)l is the variance of the l-th attribute over the entire data set.\nWe prove in the Appendix the following two theorems stating that the distance above satisfies the properties of a mathematical distance.\nTheorem 1 The SSE-distance on multivariate records consisting of nominal attributes based on the nominal variance computed as per Definition 2 is a distance in the mathematical sense.\nTheorem 2 The SSE-distance on multivariate records consisting of ordinal or numerical attributes based on the usual numerical variance is a distance in the mathematical sense.\nBy combining the proofs of Theorems 1 and 2, the next corollary follows.\nCorollary 1 The SSE-distance on multivariate records consisting of attributes of any type, where the nominal variance is used for nominal attributes and the usual numerical variance is used for ordinal and numerical attributes, is a distance in the mathematical sense."
    }, {
      "heading" : "5 Conclusions",
      "text" : "We have presented a centrality-based mapping of hierarchical nominal data to numbers. We have shown how such a numerical mapping allows computing means, variances and covariances of nominal attributes, and distances between records containing any kind of attributes. Such enhanced flexility of manipulation of nominal attributes can be used, e.g. to adapt anonymization methods intented for numerical data to the treament of nominal and hierarchical attributes. The only requirement is that, whatever the treatment, it should not\nmodify the numerical values assigned by marginality, in order for the numerical mapping to be reversible and allow recovering the original nominal values after treatment.\nAppendix\nProof (Lemma 4): We will show that, given two samples TX = {x1, · · · , xn} and T ′X = {x′1, · · · , x′n} of a nominal attributeX , both with the same cardinality n, it holds that V arM (TX) < V arM (T ′ X) if and only if V arH(TX) < V arH(T ′ X).\nAssume that V arM (TX) < V arM (T ′ X). Since both samples have the same\ncardinality, this is equivalent to\nn ∑\nj=1\nm(xj) <\nn ∑\nj=1\nm(x′j)\nBy developing the marginalities, we obtain\nn ∑\nj=1\n∑\nxl∈TX−{xj}\nd(xj , xl) <\nn ∑\nj=1\n∑\nx′ l ∈T ′ X −{x′ j }\nd(x′j , x ′ l)\nSince distances are sums of powers of 2, from 1 to 2L−1, we can write the above inequality as\nd0 + 2d1 + · · ·+ 2L−1dL−1 < d′0 + 2d′1 + · · ·+ 2L−1d′L−1 (3)\nBy viewing dL−1 · · · d1d0 and d′L−1 · · · d′1d′0 as binary numbers, it is easy to see that Inequality (3) implies that some i must exist such that di < d\n′ i and dî ≤ d′î\nfor i < î ≤ L − 1. This implies that there are less high-level edge differences associated to the values of TX than to the values of T ′ X . Hence, in terms of V arH(·), we have that V ar(TX(i)) < V ar(T ′X(i)) and V ar(TX (̂i)) ≤ V ar(T ′X (̂i) for i < î ≤ L− 1. This yields V arH(TX) < V arH(T ′X).\nIf we now assume V arH(TX) < V arH(T ′ X) we can prove V arM (TX) < V arM (T ′ X) by reversing the above argument. . Lemma 5 Given non-negative A,A′, A′′, B,B′, B′′ such that √ A ≤ √ A′+ √ A′′ and √ B ≤ √ B′ +\n√ B′′ it holds that √ A+B ≤ √ A′ +B′ + √ A′′ +B′′ (4)\nProof (Lemma 5): Squaring the two inequalities in the lemma assumption, we obtain\nA ≤ ( √ A′ + √ A′′)2 B ≤ ( √ B′ + √ B′′)2\nAdding both expressions above, we get the square of the left-hand side of Expression (4)\nA+B ≤ ( √ A′ + √ A′′)2 + ( √ B′ + √ B′′)2\n= A′ +A′′ +B′ +B′′ + 2( √ A′A′′ + √ B′B′′) (5)\nSquaring the right-hand side of Expression (4), we get\n( √ A′ +B′ + √ A′′ +B′′)2\n= A′ +B′ +A′′ +B′′ + 2 √ (A′ +B′)(A′′ +B′′) (6)\nSince Expressions (5) and (6) both contain the terms A′ + B′ + A′′ + B′′, we can neglect them. Proving Inequality (4) is equivalent to proving\n√ A′A′′ + √ B′B′′ ≤ √ (A′ +B′)(A′′ +B′′)\nSuppose the opposite, that is,\n√ A′A′′ + √ B′B′′ > √ (A′ +B′)(A′′ +B′′) (7)\nSquare both sides: A′A′′ +B′B′′ + 2 √ A′A′′B′B′′ >\n(A′ +B′)(A′′ +B′′) = A′A′′ +B′B′′ +A′B′′ +B′A′′\nSubtract A′A′′ +B′B′′ from both sides to obtain\n2 √ A′A′′B′B′′ > A′B′′ +B′A′′\nwhich can be rewritten as\n( √ A′B′′ − √ B′A′′)2 < 0\nSince a real square cannot be negative, the assumption in Expression (7) is false and the lemma follows.\nProof (Theorem 1): We must prove that the SSE-distance is non-negative, reflexive, symmetrical and subadditive (i.e. it satisfies the triangle inequality).\nNon-negativity. The SSE-distance is defined as a non-negative square root, hence it cannot be negative.\nReflexivity. If x1 = x2, then δ(x1,x2) = 0. Conversely, if δ(x2,x2) = 0, the variances are all zero, hence x1 = x2.\nSymmetry. It follows from the definition of the SSE-distance. Subadditivity. Given three records x1, x2 and x3, we must check whether\nδ(x1,x3) ? ≤ δ(x1,x2) + δ(x2,x3)\nBy expanding the above expression using Expression (2), we obtain\n√\n(S2)113 (S2)1 + · · ·+ (S 2)d13 (S2)d ? ≤\n√ (S2)112 (S2)1 + · · ·+ (S 2)d12 (S2)d + √ (S2)123 (S2)1 + · · ·+ (S 2)d23 (S2)d (8)\nLet us start with the case d = 1, that is, with a single attribute, i.e. xi = xi for i = 1, 2, 3. To check Inequality (8) with d = 1, we can ignore the variance in the denominators (it is the same on both sides) and we just need to check\n√\nS213 ? ≤\n√ S212 + √ S223 (9)\nWe have\nS213 = V ar({x1, x3}) = m(x1) +m(x3)\n2\n= d(x1, x3)\n2 +\nd(x3, x1)\n2 = d(x1, x3) (10)\nSimilarly S212 = d(x1, x2) and S 2 23 = d(x2, x3). Therefore, Expression (9) is equivalent to subaddivitity for d(·, ·) and the latter holds by Lemma 1. Let us now make the induction hypothesis for d− 1 and prove subadditivity for any d. Call now\nA := (S2)113 (S2)1 + · · ·+ (S 2)d−113 (S2)d−1\nA′ := (S2)112 (S2)1 + · · ·+ (S 2)d−112 (S2)d−1\nA′′ := (S2)123 (S2)1 + · · ·+ (S 2)d−123 (S2)d−1\nB := (S2)d13 (S2)d ; B′ := (S2)d12 (S2)d ; B′′ := (S2)d23 (S2)d\nSubadditivity for d amounts to checking whether\n√ A+B ? ≤ √ A′ +B′ + √ A′′ +B′′ (11)\nwhich holds by Lemma 5 because, by the induction hypothesis for d−1, we have√ A ≤ √ A′ + √ A′′ and, by the proof for d = 1, we have √ B ≤ √ B′ + √ B′′.\nProof (Theorem 2): Non-negativity, reflexivity and symmetry are proven in a way analogous as in Theorem 1. As to subaddivity, we just need to prove the case d = 1, that is, the inequality analogous to Expression (9) for numerical variances. The proof for general d is the same as in Theorem 1. For d = 1, we have\nS213 = (x1 − x3)2\n2 ; S212 = (x1 − x2)2 2 ; S223 = (x2 − x3)2\n2 Therefore, Expression (9) obviously holds with equality in the case of numerical variances because\n√ S213 = x1 − x3√\n2 = (x1 − x2) + (x2 − x3)√ 2 = √ S212 + √ S223\nAcknowledgments and disclaimer\nThanks go to Klara Stokes for useful help. The authors are with the UNESCO Chair in Data Privacy, but they are solely responsible for the views expressed in this paper, which do not necessarily reflect the position of UNESCO nor commit that organization."
    } ],
    "references" : [ {
      "title" : "A survey of inference control methods for privacypreserving data mining",
      "author" : [ "J. Domingo-Ferrer" ],
      "venue" : "Advances in Database Systems,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2008
    }, {
      "title" : "A measure of nominal variance for hierarchical nominal attributes",
      "author" : [ "J. Domingo-Ferrer", "A. Solanas" ],
      "venue" : "Information Sciences,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2008
    }, {
      "title" : "Salazar-González. Statistical Confidentiality: Principles and Practice",
      "author" : [ "G.T. Duncan", "M. Elliot", "J.-J" ],
      "venue" : "New York: Springer,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2011
    }, {
      "title" : "Handbook on Statistical Disclosure Control (version 1.2)",
      "author" : [ "A. Hundepool", "J. Domingo-Ferrer", "L. Franconi", "S. Giessing", "R. Lenz", "J. Longhurst", "E. Schulte-Nordholt", "G. Seri", "P.-P. DeWolf" ],
      "venue" : "ESSNET SDC Project,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2010
    }, {
      "title" : "Methoden der Geheimhaltung wirtschaftsstatistischer Einzeldaten und ihre Schutzwirkung",
      "author" : [ "R. Lenz" ],
      "venue" : "Statistik und Wissenschaft,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2010
    }, {
      "title" : "Centrality measures in trees",
      "author" : [ "K.B. Reid" ],
      "venue" : "In Advances in Interdisciplinary Applied Discrete Mathematics",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2010
    }, {
      "title" : "Protecting respondents’ identities in microdata release",
      "author" : [ "P. Samarati" ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2001
    }, {
      "title" : "DeWaal. Elements of Statistical Disclosure Control",
      "author" : [ "T.L. Willenborg" ],
      "venue" : "New York: Springer,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2001
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Statistical disclosure control (SDC, [1, 4, 8, 3, 5]), a.",
      "startOffset" : 37,
      "endOffset" : 52
    }, {
      "referenceID" : 3,
      "context" : "Statistical disclosure control (SDC, [1, 4, 8, 3, 5]), a.",
      "startOffset" : 37,
      "endOffset" : 52
    }, {
      "referenceID" : 7,
      "context" : "Statistical disclosure control (SDC, [1, 4, 8, 3, 5]), a.",
      "startOffset" : 37,
      "endOffset" : 52
    }, {
      "referenceID" : 2,
      "context" : "Statistical disclosure control (SDC, [1, 4, 8, 3, 5]), a.",
      "startOffset" : 37,
      "endOffset" : 52
    }, {
      "referenceID" : 4,
      "context" : "Statistical disclosure control (SDC, [1, 4, 8, 3, 5]), a.",
      "startOffset" : 37,
      "endOffset" : 52
    }, {
      "referenceID" : 6,
      "context" : "Furthermore, well-known approaches to anonynimization, like k-anonymity [7], assume that any attribute can be generalized, i.",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 1,
      "context" : "We next recall the variance measure for hierarchical nominal attributes introduced in [2].",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 1,
      "context" : "Algorithm 1 (Nominal variance in [2])",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 5,
      "context" : "Known tree centralities attempt to determine the “middle” of a tree [6].",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 1,
      "context" : "2 Variance In Section 2 above, we recalled a measure of variance of a hierarchical nominal attribute proposed in [2] which takes the semantics of the hierarchy into account.",
      "startOffset" : 113,
      "endOffset" : 116
    } ],
    "year" : 2013,
    "abstractText" : "The purpose of statistical disclosure control (SDC) of microdata, a.k.a. data anonymization or privacy-preserving data mining, is to publish data sets containing the answers of individual respondents in such a way that the respondents corresponding to the released records cannot be re-identified and the released data are analytically useful. SDC methods are either based on masking the original data, generating synthetic versions of them or creating hybrid versions by combining original and synthetic data. The choice of SDC methods for categorical data, especially nominal data, is much smaller than the choice of methods for numerical data. We mitigate this problem by introducing a numerical mapping for hierarchical nominal data which allows computing means, variances and covariances on them.",
    "creator" : "LaTeX with hyperref package"
  }
}