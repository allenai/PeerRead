{
  "name" : "1709.00575.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Grasping the Finer Point: A Supervised Similarity Network for Metaphor Detection",
    "authors" : [ "Marek Rei", "Luana Bulat", "Douwe Kiela", "Ekaterina Shutova" ],
    "emails" : [ "marek.rei@cl.cam.ac.uk,", "luana.bulat@cl.cam.ac.uk,", "ekaterina.shutova@cl.cam.ac.uk,", "dkiela@fb.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Metaphor is pervasive in our everyday communication, enriching it with sophisticated imagery and helping us to reconcile our experience in the world with our conceptual system (Lakoff and Johnson, 1980). In the most influential account of metaphor to date, Lakoff and Johnson explain the phenomenon through the presence of systematic metaphorical associations between two distinct concepts or domains. For instance, when we talk about “curing juvenile delinquency” or “corruption transmitting through the government ranks”, we view the general concept of crime (the target concept) in terms of the properties of a disease (the source concept). Such metaphorical associations are broad generalisations that allow us to project knowledge and inferences across domains; and our metaphorical use of language is a reflection of this process.\nGiven its ubiquity, metaphorical language poses an important problem for natural language understanding (Cameron, 2003; Shutova and Teufel, 2010). A number of approaches to metaphor processing have thus been proposed, focusing pre-\ndominantly on classifying linguistic expressions as literal or metaphorical. They experimented with a range of features, including lexical and syntactic information (Hovy et al., 2013; Beigman Klebanov et al., 2016) and higher-level features such as semantic roles (Gedigian et al., 2006), domain types (Dunn, 2013), concreteness (Turney et al., 2011), imageability (Strzalkowski et al., 2013) and WordNet supersenses (Tsvetkov et al., 2014). While reporting promising results, all of these approaches used hand-engineered features and relied on manually-annotated resources to extract them. In order to reduce the reliance on manual annotation, other researchers experimented with sparse distributional features (Shutova et al., 2010; Shutova and Sun, 2013) and dense neural word embeddings (Bracewell et al., 2014; Shutova et al., 2016). Their experiments have demonstrated that corpus-driven lexical representations already encode information about semantic domains needed to learn the patterns of metaphor usage from linguistic data.\nWe take this intuition a step further and present the first deep learning architecture designed to capture metaphorical composition. Deep learning methods have already been shown successful in many other semantic tasks (e.g. Hermann et al., 2015; Kumar et al., 2015; Zhao et al., 2015), which suggests that designing a specialised neural network architecture for metaphor detection will lead to improved performance. In this paper, we present a novel architecture which (1) models the interaction between the source and target domains in the metaphor via a gating function; (2) specialises word representations for the metaphor identification task via supervised training; (3) quantifies metaphoricity via a weighted similarity function that automatically selects the relevant dimensions of similarity. We experimented with two types of word representations ar X iv :1\n70 9.\n00 57\n5v 1\n[ cs\n.C L\n] 2\nS ep\n2 01\n7\nas inputs to the network: the standard skip-gram word embeddings (Mikolov et al., 2013a) and the cognitively-driven attribute-based vectors (Bulat et al., 2017), as well as a combination thereof.\nWe evaluate our method in the metaphor identification task, focusing on adjective–noun, verb– subject and verb–direct object constructions where the verbs and adjectives can be used metaphorically. Our results show that our architecture outperforms both a metaphor agnostic deep learning baseline (a basic feed forward network) and the previous corpus-based approaches to metaphor identification. We also investigate the effects of training data on this task, and demonstrate that with a sufficiently large training set our method also outperforms the best existing systems based on hand-coded lexical knowledge."
    }, {
      "heading" : "2 Related Work",
      "text" : "The majority of approaches to metaphor processing cast the problem as classification of linguistic expressions as metaphorical or literal. Gedigian et al. (2006) classified verbs related to MOTION and CURE within the domain of financial discourse. They used the maximum entropy classifier and the verbs’ nominal arguments and their FrameNet roles (Fillmore et al., 2003) as features, reporting encouraging results. Dunn (2013) used a logistic regression classifier and high-level properties of concepts extracted from SUMO ontology, including domain types (ABSTRACT, PHYSICAL, SOCIAL, MENTAL) and event status (PROCESS, STATE, OBJECT). Tsvetkov et al. (2014) used random forest classifier and coarse semantic features, such as concreteness, animateness, named entity types and WordNet supersenses. They have shown that the model learned with such coarse semantic features is portable across languages. The work of Hovy et al. (2013) is notable as they focused on compositional rather than categorical features. They trained an SVM with dependency-tree kernels to capture compositional information, using lexical, part-of-speech tag and WordNet supersense representations of sentence trees. Mohler et al. (2013) aimed at modelling conceptual information. They derived semantic signatures of texts as sets of highly-related and interlinked WordNet synsets. The semantic signatures served as features to train a set of classifiers (maximum entropy, decision trees, SVM, random forest) that mapped new metaphors to the semantic signatures\nof the known ones.\nWith the aim of reducing the dependence on manually-annotated lexical resources, other research focused on modelling metaphor using corpus-driven information alone. Shutova et al. (2010) pointed out that the metaphorical uses of words constitute a large portion of the dependency features extracted for abstract concepts from corpora. For example, the feature vector for politics would contain GAME or MECHANISM terms among the frequent features. As a result, distributional clustering of abstract nouns with such features identifies groups of diverse concepts metaphorically associated with the same source domain. Shutova et al. (2010) exploit this property of co-occurrence vectors to identify new metaphorical mappings starting from a set of examples. Shutova and Sun (2013) used hierarchical clustering to derive a network of concepts in which metaphorical associations are learned in an unsupervised way. Do Dinh and Gurevych (2016) investigated metaphors through the task of sequence labelling, detecting metaphor related words in context. Gutiérrez et al. (2016) investigated metaphorical composition in the compositional distributional semantics framework. Their method learns metaphors as linear transformations in a vector space and they demonstrated that it produces superior phrase representations for both metaphorical and literal language, as compared to the traditional ”single-sense” compositional distributional model. They then used these representations in the metaphor identification task, achieving promising results.\nThe more recent approaches of Shutova et al. (2016) and Bulat et al. (2017) used dense skipgram word embeddings (Mikolov et al., 2013a) instead of the sparse distributional features. Shutova et al. (2016) investigated a set of metaphor identification methods using linguistic and visual features. They learned linguistic and visual representations for both words and phrases, using skipgram and convolutional neural networks (Kiela and Bottou, 2014) respectively. They then measured the difference between the phrase representation and those of its component words in terms of their cosine similarity, which served as a predictor of metaphoricity. They found basic cosine similarity between the component words in the phrase to be a powerful measure – the neural embeddings of the words were compared with cosine similar-\nity and a threshold was tuned on the development set to distinguish between literal and metaphorical phrases. This approach was their best performing linguistic model, outperformed only by a multimodal system which included both linguistic and visual features.\nBulat et al. (2017) presented a metaphor identification method that uses representations constructed from human property norms (McRae et al., 2005). They first learn a mapping from the skip-gram embedding vector space to the property norm space using linear regression, which allows them to generate property norm representations for unseen words. The authors then train an SVM classifier to detect metaphors using these representations as input. Bulat et al. (2017) have shown that the cognitively-driven property norms outperform standard skip-gram representations in this task."
    }, {
      "heading" : "3 Supervised Similarity Network",
      "text" : "Our method is inspired by the findings of Shutova et al. (2016), who showed that the cosine similarity between neural embeddings of the two words in a phrase is indicative of its metaphoricity. For example, the phrase ‘colourful personality’ receives a score:\ns = cos(xc, xp) (1)\nwhere xc is the embedding for colourful and xp is the embedding for personality. The combined phrase is classified as being metaphorical based on a threshold, which is optimised on a development dataset. In this paper, we propose several extensions to this general idea, creating a supervised version of the cosine similarity metric which can be optimised on training data to be more suitable for metaphor detection."
    }, {
      "heading" : "3.1 Word Representation Gating",
      "text" : "Directly comparing the vector representations of both words treats each of the embeddings as an independent unit. In reality, however, word meanings vary and adapt based on the context. In case of metaphorical language (e.g. “cure crime”), the source domain properties of the verb (e.g. cure) are projected onto the target domain noun (e.g. crime), resulting in the interaction of the two domains in the interpretation of the metaphor.\nIn order to integrate this idea into the metaphor detection method, we can construct a gating function that modulates the representation of one word based on the other. Given embeddings x1 and x2, the gating values are predicted as a non-linear transformation of x1 and applied to x2 through element-wise multiplication:\ng = σ(Wgx1) (2)\nx̃2 = x2 g (3)\nwhereWg is a weight matrix that is optimised during training, σ is the sigmoid activation function, and represents element-wise multiplication. In an adjective-noun phrase, this architecture allows the network to first look at the adjective, then use its meaning to change the representation of the noun. The sigmoid activation function makes it act as a filter, choosing which information from the original embedding gets through to the rest of the network. While learning a more complex gating function could be beneficial for very large training resources, the filtering approach is more suitable for the annotated metaphor datasets which are relatively small in size."
    }, {
      "heading" : "3.2 Vector Space Mapping",
      "text" : "As the next step, we implement position-specific mappings for the word embeddings. The original method uses word embeddings that have been pretrained using the distributional skip-gram objective (Mikolov et al., 2013a). While this tunes the vectors for predicting context words, there is no reason to believe that the same space is also optimal for the task of metaphor detection. In order to address this shortcoming, we allow the model to learn a mapping from the skip-gram vector space to a new metaphor-specific vector space:\nz1 = tanh(Wz1x1) (4)\nz2 = tanh(Wz2 x̃2) (5)\nwhere Wz1 and Wz2 are weight matrices, z1 and z2 are the new position-specific word representations. While the original embeddings x1 and x2 are pre-trained on a large unannotated corpus, the transformation process is optimised using annotated metaphor examples, resulting in word representations that are more suitable for this task. Furthermore, the adjectives and nouns use separate mapping weights, which allows the model to better distinguish between the different functionalities of these words. In contrast, the original cosine similarity is not position-specific and would give the same result regardless of the word order."
    }, {
      "heading" : "3.3 Weighted Cosine",
      "text" : "If the vectors x1 and x2 are normalised to unit length, the cosine similarity between them is equal to their dot product, which in turn is equal to their elementwise multiplication followed by a sum over all elements:\ncos(x1, x2) ∝ ∑ i x1,ix2,i (6)\nThis calculation of cosine similarity can be formulated as a small neural network where the two unit-normalised input vectors are directly multiplied together. This is followed by a single output neuron, with all the intermediate weights set to value 1. Such a network would calculate the same sum over the element-wise multiplication, outputting the value of cosine similarity.\nSince there is no reason to assume that all the embedding dimensions are equally important when detecting metaphors, we can explore other strategies for weighting the similarity calculation.\nRei and Briscoe (2014) used a fixed formula to calculate weights for different dimensions of cosine similarity and showed that it helped in recovering hyponym relations. We extend this even further and allow the network to use multiple different weighting strategies which are all optimised during training. This is done by first creating a vector m, which is an element-wise multiplication of the two word representations:\nmi = z1,iz2,i (7)\nwhere mi is the i-th element of vector m and z1,i is the i-th element of vector z1. After that, the resulting vector is used as input for a hidden neural layer:\nd = γ(Wdm) (8)\nwhereWd is a weight matrix and γ is an activation function. If the length of d is 1, all the weights in Wd have value 1, and γ is a linear activation, then this formula is equivalent to a regular cosine similarity. However, we use a larger length for d to capture more features, use tanh as the activation function, and optimise the weights of Wd during training, giving the framework more flexibility to customise the model for the task of metaphor detection."
    }, {
      "heading" : "3.4 Prediction and Optimisation",
      "text" : "Based on vector d we can output a prediction for the word pair, showing whether it is literal or metaphorical:\ny = σ(Wyd) (9)\nwhere Wy is a weight matrix, σ is the logistic activation function, and y is a real-valued prediction with values between 0 and 1.\nWe optimise the model based on an annotated training dataset, while minimising the following hinge loss function:\nE = ∑ k qk (10)\nqk = { (ỹ − y)2 if |ỹ − y| > 0.4 0, otherwise\n(11)\nwhere y is the predicted value, ỹ is the true label, and k iterates over all training examples. Equation 11 optimises the model to minimise the squared error between the predicted and true labels. However, this is only done for training examples where the predicted error is not already close enough to the desired result. The condition |ỹ − y| > 0.4 only updates training examples where the difference from the true label is greater than 0.4. The true labels ỹ can only take values 0 (literal) or 1 (metaphorical), and the threshold 0.4 is chosen so that datapoints that are on the correct side of the decision boundary by more than 0.1 would be ignored, which helps reduce overfitting and allows the model to focus on the misclassified examples.\nThe diagram of the complete network can be seen in Figure 1."
    }, {
      "heading" : "4 Word Representations",
      "text" : "Following Bulat et al. (2017) we experiment with two types of semantic vectors: skip-gram word embeddings and attribute-based representations.\nThe word embeddings are 100-dimensional and were trained using the standard log-linear skipgram model with negative sampling of Mikolov et al. (2013b) on Wikipedia for 3 epochs, using a symmetric window of 5 and 10 negative samples per word-context pair.\nWe use the 2526-dimensional attribute-based vectors trained by Bulat et al. (2017), following Fagarasan et al. (2015). These representations were induced by using partial least squares regression to learn a cross-modal mapping function between the word embeddings described above and the McRae et al. (2005) property-norm semantic space."
    }, {
      "heading" : "5 Datasets",
      "text" : "We evaluate our method using two datasets of phrases manually annotated for metaphoricity.\nSince these datasets include examples for different senses (both metaphorical and literal) of the same verbs or adjectives, they allow us to test the extent to which our model is able to discriminate between different word senses, as opposed to merely selecting the most frequent class for a given word.\nMohammad et al. dataset (MOH) Mohammad et al. (2016) used WordNet to find verbs that had between three and ten senses and extracted the sentences exemplifying them in the corresponding glosses, yielding a total of 1639 verb uses in sentences. Each of these was annotated for metaphoricity by 10 annotators via the crowdsourcing platform CrowdFlower1. Mohammad et al. selected the verbs that were tagged by at least 70% of the annotators as metaphorical or literal to create their dataset. We extracted verb–direct object and verb–subject relations of the annotated verbs from this dataset, discarding the instances with pronominal or clausal subject or object. This resulted in a dataset of 647 verb–noun pairs (316 metaphorical and 331 literal). Some examples of annotated verb phrases from MOH are presented in Table 1.\nTsvetkov et al. dataset (TSV) Tsvetkov et al. (2014) construct a dataset of adjective–noun pairs annotated for metaphoricity. This is divided into a training set consisting of 884 literal and 884 metaphorical pairs (TSV-TRAIN) and a test set containing 100 literal and 100 metaphorical pairs (TSV-TEST). Table 2 shows a portion of annotated adjective-noun phrases from TSV-TEST. TSV-TRAIN was collected from publicly available metaphor collections on the web and manually\n1www.crowdflower.com\ncurated by removing duplicates and metaphorical phrases that depend on wider context for their interpretation (e.g. drowning students). TSVTEST was constructed by extracting nouns that co-occur with a list of 1000 frequent adjectives in the TenTen Web Corpus2 using SketchEngine. The selected adjective-noun pairs were annotated for metaphoricity by 5 annotators with an interannotator agreement of κ = 0.76. Since TSVTRAIN and TSV-TEST were constructed differently, we follow previous work (Tsvetkov et al., 2014; Shutova et al., 2016; Bulat et al., 2017) and report performance on TSV-TEST. We randomly separated 200 (out of the 1536) examples from the training set to use for development experiments."
    }, {
      "heading" : "6 Experiments and Results",
      "text" : "The word representations in our model were initialised with either the 100-dimensional skip-gram embeddings or the 2,526-dimensional attribute vectors (Section 4). These were kept fixed and not updated, which reduces overfitting on the available training examples. For both word representations we use the same embeddings as Bulat et al. (2017), which makes the results directly comparable and shows that the improvements are coming from the novel architecture and are not due to a different embedding initialisation.\nThe network was optimised using AdaDelta (Zeiler, 2012) for controlling adaptive learning rates. The models were evaluated after each full pass over the training data and training was stopped if the F-score on the development set had not improved for 5 epochs. The transformed embeddings z1 and z2 were set to size 300, layer d was set to size 50. The values for these hyperparameters were chosen experimentally using the development dataset. In order to avoid drawing conclusions based on outlier results due to random initialisations, we ran each experiment 25 times with random seeds and present the averaged results in this paper. We implemented the framework using Theano (Al-Rfou et al., 2016) and are making the source code publicly available.3\nTable 3 contains results of different system configurations on the TSV dataset. The original Fscore by Tsvetkov et al. (2014) is still the highest, as they used a range of highly-engineered features that require manual annotation, such as\n2https://www.sketchengine.co.uk/ententen-corpus/ 3http://www.marekrei.com/projects/ssn\nthe lexical abstractness, imageability scores and the relative number of supersenses for each word in the dataset. Our setup is more similar to the linguistic experiments by Shutova et al. (2016), where metaphor detection is performed using pretrained word embeddings. They also proposed combining the linguistic model with a system using visual word representations and achieved performance improvements. Recently, Bulat et al. (2017) compared different types of embeddings and showed that attribute-based representations can outperform regular skip-gram embeddings.\nAs an additional baseline, we report the performance on metaphor detection using a basic feedforward network (FFN). In this configuration, the word embeddings x1 and x2 are directly connected to the hidden layer d, skipping all the intermediate network structure. The FFN achieves 74.4% F-score on TSV-TEST, showing that even such a simple model can perform relatively well in a supervised setting. Using attribute vectors instead of skip-gram embeddings gives a slight improvement, especially on the recall metric, which is consistent with the findings by Bulat et al. (2017).\nThe architecture described in Section 3, which we refer to as a supervised similarity network (SSN), outperforms the baseline and achieves 80.1% F-score using skip-gram embeddings and 80.6% with attribute-based representations. We also created a fusion of these two models where the predictions from both are combined as a weighted average. In this setting, the two networks are trained in tandem and a real-valued weight, which is also optimised during training, is\nused to combine them together. This configuration achieves 81.1% F-score, indicating that the the skip-gram embeddings and attribute vectors capture somewhat complementary information. Excluding the system by Tsvetkov et al. (2014) which requires hand-annotated features, the proposed similarity network outperforms all the previous systems, even improving over the multimodal system by Shutova et al. (2016) without requiring any visual information. The attribute-based SSN also improves over Bulat et al. (2017) by 5.6% absolute, using the same word representations as input.\nTable 4 contains results of different system architectures on the MOH dataset. Shutova et al. (2016) reported 75% F-score on this dataset with a multimodal system, after randomly separating a subset for testing. Since this corpus contains only 647 annotated examples, we instead evaluated the systems using 10-fold cross-validation. The feedforward baseline with skip-gram embeddings returns an F-score that is close to the linguistic configuration of Shutova et al, whereas the best results are achieved by the similarity network with skip-gram embeddings. In this setting, the attribute-based representations did not improve performance – this is expected, as the attribute norms by McRae et al. (2005) are designed for nouns, whereas the MOH dataset is centered on verbs.\nTable 5 contains examples from the TSV development set, together with gold annotations and predicted scores. The system confidently detects literal phrases such as sunny country and meaningless discussion, along with metaphorical phrases such as unforgiving heights and blind hope. The predicted output disagrees with the annotation on\ncases such as humane treatment and rich programmer – some of these examples could also be argued as being metaphorical, depending on the specific sense of the words. While the system was relatively unsure about the false positives (the scores were close to 0.5), it tended to assign more decisive scores to the false negatives."
    }, {
      "heading" : "7 The Effects of Training Data",
      "text" : "Results in Section 6 show that performance on the TSV dataset is higher than the MOH dataset, likely due to the former having more examples available for training. Therefore, we ran an additional experiment to investigate the effect of dataset size on the performance of metaphor detection. Gutiérrez et al. (2016) annotated a dataset of adjective-noun phrases as being literal or metaphorical, and we are able to use this as an additional training resource. While it contains only 23 unique adjectives, the total number of phrases reaches 8,592. We remove any phrases that occur in the development or test data of TSV, then incrementally add the remaining examples to the TSV training data and evaluate on the TSV-TEST.\nFigure 2 shows a graph of the system performance, when increasing the training data at intervals of 500. There is a very rapid increase in performance until around 2,000 training points, whereas the existing TSV-TRAIN is limited to 1,336 examples. Providing even more data to the system gives an additional increase that is more gradual. The final performance of the system us-\ning both datasets is 88.3 F-score, which is the highest result reported on the TSV dataset and translates to 36% relative error reduction with respect to the same system trained only on the original dataset.\nWe report the exact values in Table 6 for the different training sets. The value on the Tsvetkov training data is different from the result in Table 3, which is due to the original attribute embeddings by Bulat et al. (2017) only containing representations for the vocabulary in the TSV dataset. In order to include the data from Gutiérrez et al. (2016), we recreated the attribute vectors for a larger vocabulary, which results in a slightly different baseline performance."
    }, {
      "heading" : "8 Qualitative analysis",
      "text" : "The architecture in Section 3 also acts as a semantic composition model, extracting the meaning of the phrase by combining the meanings of its component words. Therefore, we performed a qualitative experiment to investigate: (1) how well do traditional compositional methods capture metaphors, without any fine-tuning; and (2) whether the supervised representations still retain their domain-specific semantic information. For this purpose, we construct three vector spaces and visualise some examples from the TSV training set,\nusing t-SNE (Van Der Maaten and Hinton, 2008). Figure 3 contains examples for three different composition methods: the additive method simply sums the skip-gram embeddings for both words (top); the multiplicative method multiplies the skip-gram embeddings (middle); the final system uses layer m from the SSN model to represent the\nphrases (bottom). The visualisation shows that the additive and multiplicative models are both comparable when it comes to semantic clustering of the phrases, but metaphorical examples are mixed together with literal clusters. The SSN is optimised for metaphor classification and therefore it produces representations with a very clear boundary for metaphoricity. Interestingly, the graph also reveals a misannotated example in the dataset, since ‘fiery temper’ should be labeled as a metaphor. At the same time, this space also retains the general semantic information, as similar phrases with the same label are still positioned close together. Future work could investigate models of multi-task training where metaphor detection is trained together with an unsupervised objective, allowing the system to take better advantage of unlabeled data while still learning to separate metaphors."
    }, {
      "heading" : "9 Conclusion",
      "text" : "In this paper, we introduced the first deep learning architecture designed to capture metaphorical composition and evaluated it on a metaphor identification task.\nFirstly, we demonstrated that the proposed framework outperforms both a metaphor-agnostic baseline (a feed-forward neural network) as well as previous corpus-driven approaches to metaphor identification. The results showed that it is beneficial to construct a specialised network architecture for metaphor detection, which includes a gating function for capturing the interaction between the source and target domains, word embeddings mapped to a metaphor-specific space, and optimisation using a hinge loss function.\nSecondly, our qualitative analysis indicates that our supervised similarity network learns phrase representations with a very clear boundary for metaphoricity, in contrast to traditional compositional methods.\nFinally, we show that with a sufficiently large training set our model can also outperform the state-of-the art metaphor identification systems based on hand-coded lexical knowledge."
    }, {
      "heading" : "Acknowledgments",
      "text" : "Ekaterina Shutova’s research is supported by the Leverhulme Trust Early Career Fellowship."
    } ],
    "references" : [ {
      "title" : "Theano: A Python framework for fast computation of mathematical expressions",
      "author" : [ "Bisson", "Josh Bleecher Snyder", "Nicolas Bouchard", "Nicolas Boulanger-Lewandowski", "Others." ],
      "venue" : "arXiv e-prints, abs/1605.0:19.",
      "citeRegEx" : "Bisson et al\\.,? 2016",
      "shortCiteRegEx" : "Bisson et al\\.",
      "year" : 2016
    }, {
      "title" : "Semantic classifications for detection of verb metaphors",
      "author" : [ "Beata Beigman Klebanov", "Chee Wee Leong", "E. Dario Gutierrez", "Ekaterina Shutova", "Michael Flor." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Klebanov et al\\.,? 2016",
      "shortCiteRegEx" : "Klebanov et al\\.",
      "year" : 2016
    }, {
      "title" : "A tiered approach to the recognition of metaphor",
      "author" : [ "David Bracewell", "Marc Tomlinson", "Michael Mohler", "Bryan Rink." ],
      "venue" : "Computational Linguistics and Intelligent Text Processing, 8403:403–414.",
      "citeRegEx" : "Bracewell et al\\.,? 2014",
      "shortCiteRegEx" : "Bracewell et al\\.",
      "year" : 2014
    }, {
      "title" : "Modelling metaphor with attribute-based semantics",
      "author" : [ "Luana Bulat", "Stephen Clark", "Ekaterina Shutova." ],
      "venue" : "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2017).",
      "citeRegEx" : "Bulat et al\\.,? 2017",
      "shortCiteRegEx" : "Bulat et al\\.",
      "year" : 2017
    }, {
      "title" : "Metaphor in Educational Discourse",
      "author" : [ "Lynne Cameron." ],
      "venue" : "Continuum, London.",
      "citeRegEx" : "Cameron.,? 2003",
      "shortCiteRegEx" : "Cameron.",
      "year" : 2003
    }, {
      "title" : "TokenLevel Metaphor Detection using Neural Networks",
      "author" : [ "Erik-Lân Do Dinh", "Iryna Gurevych." ],
      "venue" : "Proceedings of the Fourth Workshop on Metaphor in NLP.",
      "citeRegEx" : "Dinh and Gurevych.,? 2016",
      "shortCiteRegEx" : "Dinh and Gurevych.",
      "year" : 2016
    }, {
      "title" : "Evaluating the premises and results of four metaphor identification systems",
      "author" : [ "Jonathan Dunn." ],
      "venue" : "Proceedings of CICLing’13, pages 471–486, Samos, Greece.",
      "citeRegEx" : "Dunn.,? 2013",
      "shortCiteRegEx" : "Dunn.",
      "year" : 2013
    }, {
      "title" : "From distributional semantics to feature norms: grounding semantic models in human perceptual data",
      "author" : [ "Luana Fagarasan", "Eva Maria Vecchi", "Stephen Clark." ],
      "venue" : "Proceedings of the 11th International Conference on Computational Semantics",
      "citeRegEx" : "Fagarasan et al\\.,? 2015",
      "shortCiteRegEx" : "Fagarasan et al\\.",
      "year" : 2015
    }, {
      "title" : "Background to FrameNet",
      "author" : [ "Charles Fillmore", "Christopher Johnson", "Miriam Petruck." ],
      "venue" : "International Journal of Lexicography, 16(3):235–250.",
      "citeRegEx" : "Fillmore et al\\.,? 2003",
      "shortCiteRegEx" : "Fillmore et al\\.",
      "year" : 2003
    }, {
      "title" : "Catching metaphors",
      "author" : [ "Matt Gedigian", "John Bryant", "Srini Narayanan", "Branimir Ciric." ],
      "venue" : "In Proceedings of the 3rd Workshop on Scalable Natural Language Understanding, pages 41–48, New York.",
      "citeRegEx" : "Gedigian et al\\.,? 2006",
      "shortCiteRegEx" : "Gedigian et al\\.",
      "year" : 2006
    }, {
      "title" : "Literal and Metaphorical Senses",
      "author" : [ "E. Darı́o Gutiérrez", "Ekaterina Shutova", "Tyler Marghetis", "Benjamin K. Bergen" ],
      "venue" : null,
      "citeRegEx" : "Gutiérrez et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gutiérrez et al\\.",
      "year" : 2016
    }, {
      "title" : "Teaching machines to read and comprehend",
      "author" : [ "Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 1693–",
      "citeRegEx" : "Hermann et al\\.,? 2015",
      "shortCiteRegEx" : "Hermann et al\\.",
      "year" : 2015
    }, {
      "title" : "Identifying metaphorical word use with tree kernels",
      "author" : [ "Dirk Hovy", "Shashank Shrivastava", "Sujay Kumar Jauhar", "Mrinmaya Sachan", "Kartik Goyal", "Huying Li", "Whitney Sanders", "Eduard Hovy." ],
      "venue" : "Proceedings of the First Workshop on Metaphor in NLP,",
      "citeRegEx" : "Hovy et al\\.,? 2013",
      "shortCiteRegEx" : "Hovy et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning Image Embeddings using Convolutional Neural Networks for Improved Multi-Modal Semantics",
      "author" : [ "Douwe Kiela", "Léon Bottou." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-14).",
      "citeRegEx" : "Kiela and Bottou.,? 2014",
      "shortCiteRegEx" : "Kiela and Bottou.",
      "year" : 2014
    }, {
      "title" : "Ask me anything: Dynamic memory networks for natural language processing",
      "author" : [ "Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher." ],
      "venue" : "CoRR, abs/1506.07285.",
      "citeRegEx" : "Kumar et al\\.,? 2015",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2015
    }, {
      "title" : "Metaphors We Live By",
      "author" : [ "George Lakoff", "Mark Johnson." ],
      "venue" : "University of Chicago Press, Chicago.",
      "citeRegEx" : "Lakoff and Johnson.,? 1980",
      "shortCiteRegEx" : "Lakoff and Johnson.",
      "year" : 1980
    }, {
      "title" : "Semantic feature production norms for a large set of living and nonliving things",
      "author" : [ "Ken McRae", "George S Cree", "Mark S Seidenberg", "Chris McNorgan." ],
      "venue" : "Behavior Research Methods, 37.",
      "citeRegEx" : "McRae et al\\.,? 2005",
      "shortCiteRegEx" : "McRae et al\\.",
      "year" : 2005
    }, {
      "title" : "Efficient Estimation of Word Representations in Vector Space",
      "author" : [ "Tomáš Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "Proceedings of the International Conference on Learning Representations (ICLR 2013).",
      "citeRegEx" : "Mikolov et al\\.,? 2013a",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "Proceedings of ICLR, Scottsdale, AZ.",
      "citeRegEx" : "Mikolov et al\\.,? 2013b",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Metaphor as a medium for emotion: An empirical study",
      "author" : [ "Saif M Mohammad", "Ekaterina Shutova", "Peter D Turney." ],
      "venue" : "Proceedings of *SEM 2016.",
      "citeRegEx" : "Mohammad et al\\.,? 2016",
      "shortCiteRegEx" : "Mohammad et al\\.",
      "year" : 2016
    }, {
      "title" : "Semantic signatures for example-based linguistic metaphor detection",
      "author" : [ "Michael Mohler", "David Bracewell", "Marc Tomlinson", "David Hinote." ],
      "venue" : "Proceedings of the First Workshop on Metaphor in NLP, pages 27–35, Atlanta, Georgia.",
      "citeRegEx" : "Mohler et al\\.,? 2013",
      "shortCiteRegEx" : "Mohler et al\\.",
      "year" : 2013
    }, {
      "title" : "Looking for Hyponyms in Vector Space",
      "author" : [ "Marek Rei", "Ted Briscoe." ],
      "venue" : "Proceedings of the Eighteenth Conference on Computational Natural Language Learning (CoNLL 2014), pages 68–77.",
      "citeRegEx" : "Rei and Briscoe.,? 2014",
      "shortCiteRegEx" : "Rei and Briscoe.",
      "year" : 2014
    }, {
      "title" : "Black Holes and White Rabbits : Metaphor Identification with Visual Features",
      "author" : [ "Ekaterina Shutova", "Douwe Kiela", "Jean Maillard." ],
      "venue" : "Proceedings of NAACL-HLT 2016.",
      "citeRegEx" : "Shutova et al\\.,? 2016",
      "shortCiteRegEx" : "Shutova et al\\.",
      "year" : 2016
    }, {
      "title" : "Unsupervised metaphor identification using hierarchical graph factorization clustering",
      "author" : [ "Ekaterina Shutova", "Lin Sun." ],
      "venue" : "Proceedings of NAACL 2013, Atlanta, GA, USA.",
      "citeRegEx" : "Shutova and Sun.,? 2013",
      "shortCiteRegEx" : "Shutova and Sun.",
      "year" : 2013
    }, {
      "title" : "Metaphor identification using verb and noun clustering",
      "author" : [ "Ekaterina Shutova", "Lin Sun", "Anna Korhonen." ],
      "venue" : "Proceedings of Coling 2010, pages 1002–1010, Beijing, China.",
      "citeRegEx" : "Shutova et al\\.,? 2010",
      "shortCiteRegEx" : "Shutova et al\\.",
      "year" : 2010
    }, {
      "title" : "Metaphor corpus annotated for source - target domain mappings",
      "author" : [ "Ekaterina Shutova", "Simone Teufel." ],
      "venue" : "Proceedings of LREC 2010, pages 3255– 3261, Malta.",
      "citeRegEx" : "Shutova and Teufel.,? 2010",
      "shortCiteRegEx" : "Shutova and Teufel.",
      "year" : 2010
    }, {
      "title" : "Robust extraction of metaphor from novel data",
      "author" : [ "Tomek Strzalkowski", "George Aaron Broadwell", "Sarah Taylor", "Laurie Feldman", "Samira Shaikh", "Ting Liu", "Boris Yamrom", "Kit Cho", "Umit Boz", "Ignacio Cases", "Kyle Elliot." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Strzalkowski et al\\.,? 2013",
      "shortCiteRegEx" : "Strzalkowski et al\\.",
      "year" : 2013
    }, {
      "title" : "Metaphor Detection with Cross-Lingual Model Transfer",
      "author" : [ "Yulia Tsvetkov", "Leonid Boytsov", "Anatole Gershman", "Eric Nyberg", "Chris Dyer." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014),",
      "citeRegEx" : "Tsvetkov et al\\.,? 2014",
      "shortCiteRegEx" : "Tsvetkov et al\\.",
      "year" : 2014
    }, {
      "title" : "Literal and metaphorical sense identification through concrete and abstract context",
      "author" : [ "Peter D. Turney", "Yair Neuman", "Dan Assaf", "Yohai Cohen." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,",
      "citeRegEx" : "Turney et al\\.,? 2011",
      "shortCiteRegEx" : "Turney et al\\.",
      "year" : 2011
    }, {
      "title" : "Visualizing high-dimensional data using tsne",
      "author" : [ "Laurens Van Der Maaten", "Geoffrey Hinton." ],
      "venue" : "Journal of Machine Learning Research, 9.",
      "citeRegEx" : "Maaten and Hinton.,? 2008",
      "shortCiteRegEx" : "Maaten and Hinton.",
      "year" : 2008
    }, {
      "title" : "ADADELTA: An Adaptive Learning Rate Method",
      "author" : [ "Matthew D. Zeiler." ],
      "venue" : "arXiv preprint arXiv:1212.5701.",
      "citeRegEx" : "Zeiler.,? 2012",
      "shortCiteRegEx" : "Zeiler.",
      "year" : 2012
    }, {
      "title" : "Self-adaptive hierarchical sentence model",
      "author" : [ "Han Zhao", "Zhengdong Lu", "Pascal Poupart." ],
      "venue" : "arXiv preprint arXiv:1504.05070.",
      "citeRegEx" : "Zhao et al\\.,? 2015",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "Metaphor is pervasive in our everyday communication, enriching it with sophisticated imagery and helping us to reconcile our experience in the world with our conceptual system (Lakoff and Johnson, 1980).",
      "startOffset" : 176,
      "endOffset" : 202
    }, {
      "referenceID" : 4,
      "context" : "Given its ubiquity, metaphorical language poses an important problem for natural language understanding (Cameron, 2003; Shutova and Teufel, 2010).",
      "startOffset" : 104,
      "endOffset" : 145
    }, {
      "referenceID" : 25,
      "context" : "Given its ubiquity, metaphorical language poses an important problem for natural language understanding (Cameron, 2003; Shutova and Teufel, 2010).",
      "startOffset" : 104,
      "endOffset" : 145
    }, {
      "referenceID" : 12,
      "context" : "They experimented with a range of features, including lexical and syntactic information (Hovy et al., 2013; Beigman Klebanov et al., 2016) and higher-level features such as semantic roles (Gedigian et al.",
      "startOffset" : 88,
      "endOffset" : 138
    }, {
      "referenceID" : 9,
      "context" : ", 2016) and higher-level features such as semantic roles (Gedigian et al., 2006), domain",
      "startOffset" : 57,
      "endOffset" : 80
    }, {
      "referenceID" : 6,
      "context" : "types (Dunn, 2013), concreteness (Turney et al.",
      "startOffset" : 6,
      "endOffset" : 18
    }, {
      "referenceID" : 28,
      "context" : "types (Dunn, 2013), concreteness (Turney et al., 2011), imageability (Strzalkowski et al.",
      "startOffset" : 33,
      "endOffset" : 54
    }, {
      "referenceID" : 26,
      "context" : ", 2011), imageability (Strzalkowski et al., 2013) and WordNet supersenses (Tsvetkov et al.",
      "startOffset" : 22,
      "endOffset" : 49
    }, {
      "referenceID" : 27,
      "context" : ", 2013) and WordNet supersenses (Tsvetkov et al., 2014).",
      "startOffset" : 32,
      "endOffset" : 55
    }, {
      "referenceID" : 24,
      "context" : "In order to reduce the reliance on manual annotation, other researchers experimented with sparse distributional features (Shutova et al., 2010; Shutova and Sun, 2013) and dense neural word",
      "startOffset" : 121,
      "endOffset" : 166
    }, {
      "referenceID" : 23,
      "context" : "In order to reduce the reliance on manual annotation, other researchers experimented with sparse distributional features (Shutova et al., 2010; Shutova and Sun, 2013) and dense neural word",
      "startOffset" : 121,
      "endOffset" : 166
    }, {
      "referenceID" : 2,
      "context" : "embeddings (Bracewell et al., 2014; Shutova et al., 2016).",
      "startOffset" : 11,
      "endOffset" : 57
    }, {
      "referenceID" : 22,
      "context" : "embeddings (Bracewell et al., 2014; Shutova et al., 2016).",
      "startOffset" : 11,
      "endOffset" : 57
    }, {
      "referenceID" : 14,
      "context" : "Deep learning methods have already been shown successful in many other semantic tasks (e.g. Hermann et al., 2015; Kumar et al., 2015; Zhao et al., 2015), which suggests that designing a specialised neural network architecture for metaphor detection will lead to improved performance.",
      "startOffset" : 86,
      "endOffset" : 152
    }, {
      "referenceID" : 31,
      "context" : "Deep learning methods have already been shown successful in many other semantic tasks (e.g. Hermann et al., 2015; Kumar et al., 2015; Zhao et al., 2015), which suggests that designing a specialised neural network architecture for metaphor detection will lead to improved performance.",
      "startOffset" : 86,
      "endOffset" : 152
    }, {
      "referenceID" : 17,
      "context" : "word embeddings (Mikolov et al., 2013a) and the cognitively-driven attribute-based vectors (Bulat et al.",
      "startOffset" : 16,
      "endOffset" : 39
    }, {
      "referenceID" : 3,
      "context" : ", 2013a) and the cognitively-driven attribute-based vectors (Bulat et al., 2017), as well as a combination thereof.",
      "startOffset" : 60,
      "endOffset" : 80
    }, {
      "referenceID" : 9,
      "context" : "Gedigian et al. (2006) classified verbs related to MOTION and CURE within the domain of financial",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 8,
      "context" : "They used the maximum entropy classifier and the verbs’ nominal arguments and their FrameNet roles (Fillmore et al., 2003) as features, reporting encouraging results.",
      "startOffset" : 99,
      "endOffset" : 122
    }, {
      "referenceID" : 6,
      "context" : "Dunn (2013) used a logistic regression classifier and high-level prop-",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 26,
      "context" : "Tsvetkov et al. (2014) used random forest classifier and coarse semantic features, such as concreteness, animateness, named entity types and WordNet supersenses.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 12,
      "context" : "The work of Hovy et al. (2013) is notable as they focused on compositional rather than categorical features.",
      "startOffset" : 12,
      "endOffset" : 31
    }, {
      "referenceID" : 22,
      "context" : "Shutova et al. (2010) pointed out that the metaphorical uses of words constitute a large portion of the dependency features extracted for abstract concepts from corpora.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 22,
      "context" : "Shutova et al. (2010) pointed out that the metaphorical uses of words constitute a large portion of the dependency features extracted for abstract concepts from corpora. For example, the feature vector for politics would contain GAME or MECHANISM terms among the frequent features. As a result, distributional clustering of abstract nouns with such features identifies groups of diverse concepts metaphorically associated with the same source domain. Shutova et al. (2010) exploit this property of co-occurrence vectors to identify new",
      "startOffset" : 0,
      "endOffset" : 473
    }, {
      "referenceID" : 23,
      "context" : "Shutova and Sun (2013) used hierarchical clustering to derive a network of concepts in which metaphorical associations are learned in an unsupervised way.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 10,
      "context" : "Gutiérrez et al. (2016) investigated metaphorical composition in the compositional distributional semantics framework.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 17,
      "context" : "(2017) used dense skipgram word embeddings (Mikolov et al., 2013a) instead of the sparse distributional features.",
      "startOffset" : 43,
      "endOffset" : 66
    }, {
      "referenceID" : 13,
      "context" : "They learned linguistic and visual representations for both words and phrases, using skipgram and convolutional neural networks (Kiela and Bottou, 2014) respectively.",
      "startOffset" : 128,
      "endOffset" : 152
    }, {
      "referenceID" : 18,
      "context" : "The more recent approaches of Shutova et al. (2016) and Bulat et al.",
      "startOffset" : 30,
      "endOffset" : 52
    }, {
      "referenceID" : 3,
      "context" : "(2016) and Bulat et al. (2017) used dense skipgram word embeddings (Mikolov et al.",
      "startOffset" : 11,
      "endOffset" : 31
    }, {
      "referenceID" : 3,
      "context" : "(2016) and Bulat et al. (2017) used dense skipgram word embeddings (Mikolov et al., 2013a) instead of the sparse distributional features. Shutova et al. (2016) investigated a set of metaphor identification methods using linguistic and visual features.",
      "startOffset" : 11,
      "endOffset" : 160
    }, {
      "referenceID" : 16,
      "context" : "(2017) presented a metaphor identification method that uses representations constructed from human property norms (McRae et al., 2005).",
      "startOffset" : 114,
      "endOffset" : 134
    }, {
      "referenceID" : 3,
      "context" : "Bulat et al. (2017) have",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 22,
      "context" : "Our method is inspired by the findings of Shutova et al. (2016), who showed that the cosine similarity between neural embeddings of the two words in a phrase is indicative of its metaphoricity.",
      "startOffset" : 42,
      "endOffset" : 64
    }, {
      "referenceID" : 17,
      "context" : "trained using the distributional skip-gram objective (Mikolov et al., 2013a).",
      "startOffset" : 53,
      "endOffset" : 76
    }, {
      "referenceID" : 3,
      "context" : "Following Bulat et al. (2017) we experiment with two types of semantic vectors: skip-gram word embeddings and attribute-based representations.",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 17,
      "context" : "The word embeddings are 100-dimensional and were trained using the standard log-linear skipgram model with negative sampling of Mikolov et al. (2013b) on Wikipedia for 3 epochs, using a symmetric window of 5 and 10 negative samples per word-context pair.",
      "startOffset" : 128,
      "endOffset" : 151
    }, {
      "referenceID" : 3,
      "context" : "We use the 2526-dimensional attribute-based vectors trained by Bulat et al. (2017), following Fagarasan et al.",
      "startOffset" : 63,
      "endOffset" : 83
    }, {
      "referenceID" : 3,
      "context" : "We use the 2526-dimensional attribute-based vectors trained by Bulat et al. (2017), following Fagarasan et al. (2015). These representations were induced by using partial least squares regression to learn a cross-modal mapping function between the word embeddings described above and the McRae et al.",
      "startOffset" : 63,
      "endOffset" : 118
    }, {
      "referenceID" : 3,
      "context" : "We use the 2526-dimensional attribute-based vectors trained by Bulat et al. (2017), following Fagarasan et al. (2015). These representations were induced by using partial least squares regression to learn a cross-modal mapping function between the word embeddings described above and the McRae et al. (2005) property-norm semantic space.",
      "startOffset" : 63,
      "endOffset" : 308
    }, {
      "referenceID" : 27,
      "context" : "Since TSVTRAIN and TSV-TEST were constructed differently, we follow previous work (Tsvetkov et al., 2014; Shutova et al., 2016; Bulat et al., 2017) and report performance on TSV-TEST.",
      "startOffset" : 82,
      "endOffset" : 147
    }, {
      "referenceID" : 22,
      "context" : "Since TSVTRAIN and TSV-TEST were constructed differently, we follow previous work (Tsvetkov et al., 2014; Shutova et al., 2016; Bulat et al., 2017) and report performance on TSV-TEST.",
      "startOffset" : 82,
      "endOffset" : 147
    }, {
      "referenceID" : 3,
      "context" : "Since TSVTRAIN and TSV-TEST were constructed differently, we follow previous work (Tsvetkov et al., 2014; Shutova et al., 2016; Bulat et al., 2017) and report performance on TSV-TEST.",
      "startOffset" : 82,
      "endOffset" : 147
    }, {
      "referenceID" : 3,
      "context" : "For both word representations we use the same embeddings as Bulat et al. (2017), which makes the results directly comparable and",
      "startOffset" : 60,
      "endOffset" : 80
    }, {
      "referenceID" : 30,
      "context" : "The network was optimised using AdaDelta (Zeiler, 2012) for controlling adaptive learning",
      "startOffset" : 41,
      "endOffset" : 55
    }, {
      "referenceID" : 27,
      "context" : "The original Fscore by Tsvetkov et al. (2014) is still the highest, as they used a range of highly-engineered features that require manual annotation, such as",
      "startOffset" : 23,
      "endOffset" : 46
    }, {
      "referenceID" : 21,
      "context" : "(2014) - - - 85 Shutova et al. (2016) linguistic - 73 80 76 multimodal - 67 96 79 Bulat et al.",
      "startOffset" : 16,
      "endOffset" : 38
    }, {
      "referenceID" : 3,
      "context" : "(2016) linguistic - 73 80 76 multimodal - 67 96 79 Bulat et al. (2017) - 85 71 77",
      "startOffset" : 51,
      "endOffset" : 71
    }, {
      "referenceID" : 22,
      "context" : "Our setup is more similar to the linguistic experiments by Shutova et al. (2016), where metaphor detection is performed using pretrained word embeddings.",
      "startOffset" : 59,
      "endOffset" : 81
    }, {
      "referenceID" : 3,
      "context" : "Recently, Bulat et al. (2017) compared different types of embeddings and showed that attribute-based representations",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 3,
      "context" : "Using attribute vectors instead of skip-gram embeddings gives a slight improvement, especially on the recall metric, which is consistent with the findings by Bulat et al. (2017).",
      "startOffset" : 158,
      "endOffset" : 178
    }, {
      "referenceID" : 25,
      "context" : "cluding the system by Tsvetkov et al. (2014) which requires hand-annotated features, the proposed similarity network outperforms all the previous systems, even improving over the multimodal system by Shutova et al.",
      "startOffset" : 22,
      "endOffset" : 45
    }, {
      "referenceID" : 22,
      "context" : "(2014) which requires hand-annotated features, the proposed similarity network outperforms all the previous systems, even improving over the multimodal system by Shutova et al. (2016) without requiring any",
      "startOffset" : 162,
      "endOffset" : 184
    }, {
      "referenceID" : 3,
      "context" : "The attribute-based SSN also improves over Bulat et al. (2017) by 5.",
      "startOffset" : 43,
      "endOffset" : 63
    }, {
      "referenceID" : 22,
      "context" : "Shutova et al. (2016) reported 75% F-score on this dataset with a multimodal system, after randomly separating a subset for testing.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 16,
      "context" : "In this setting, the attribute-based representations did not improve performance – this is expected, as the attribute norms by McRae et al. (2005) are designed for nouns, whereas the MOH dataset is centered on verbs.",
      "startOffset" : 127,
      "endOffset" : 147
    }, {
      "referenceID" : 10,
      "context" : "Gutiérrez et al. (2016) annotated a dataset of adjective-noun phrases as being literal or metaphorical, and we are able to use this as an additional training resource.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 3,
      "context" : "The value on the Tsvetkov training data is different from the result in Table 3, which is due to the original attribute embeddings by Bulat et al. (2017) only containing representations for the vocabulary in the TSV dataset.",
      "startOffset" : 134,
      "endOffset" : 154
    }, {
      "referenceID" : 3,
      "context" : "The value on the Tsvetkov training data is different from the result in Table 3, which is due to the original attribute embeddings by Bulat et al. (2017) only containing representations for the vocabulary in the TSV dataset. In order to include the data from Gutiérrez et al. (2016), we recreated the attribute vectors for a larger vocabulary, which results in a slightly different baseline performance.",
      "startOffset" : 134,
      "endOffset" : 283
    } ],
    "year" : 2017,
    "abstractText" : "The ubiquity of metaphor in our everyday communication makes it an important problem for natural language understanding. Yet, the majority of metaphor processing systems to date rely on handengineered features and there is still no consensus in the field as to which features are optimal for this task. In this paper, we present the first deep learning architecture designed to capture metaphorical composition. Our results demonstrate that it outperforms the existing approaches in the metaphor identification task.",
    "creator" : "LaTeX with hyperref package"
  }
}