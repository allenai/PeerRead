{
  "name" : "1709.05774.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Direction-Aware Semi-Dense SLAM",
    "authors" : [ "Julian Straub", "Randi Cabezas", "John Leonard", "John W. Fisher" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Future perception systems in applications such as autonomous cars, autonomous robots, or augmented reality will integrate scene understanding into the purely geometric localization and mapping task. This is likely to improve both simultaneous localization and mapping (SLAM), provide a basis for higher-level reasoning about the scene, and richer information for human operators. In current systems scene understanding is used in two ways: (1) to improve the operation of the 3D perception system and (2) to provide additional information for higher-level inference or a human operator. We argue that only systems in the first class actually “understand” aspects of the scene because they are able to use inferred concepts to improve on their other inferential tasks (i.e. localization and mapping). The number of systems that fall into this class is still small.\nIn order to improve 3D reconstruction and localization via scene understanding most approaches rely on geometric scene priors such as planarity [10, 39, 20, 28], the Manhattan World [37] (MW) or the Stata Center World [6] (SCW) assumption. The assumption of planarity is a local assumption that cannot explain the connection between disparate scene parts like all the parallel planes in typical man-made environments. Such connections can be captured and explained by global assumptions such as the MW and the SCW assumption. Because the MW assumption is limited\nto very specific environments, we instead explore the flexible Stata Center World model to improve 3D reconstruction and camera tracking. As shown in [43, 44], the directional clustering of a scene’s surface normals under the Stata Center World implies a segmentation that captures scene-wide regularities of the environment [45] as can be seen in the segmentation in Fig. 1.\nBased on the SCW scene prior, we propose the first semi-dense nonparametric direction-aware SLAM system. It performs joint inference over the Bayesian nonparametric SCW scene segmentation and the world map using Gibbssampling without precluding real-time operation. To connect the scene-wide Stata Center World model with local surface properties we model the assumption that nearby areas in the same directional segment are likely planar using a CRF. We demonstrate experimentally that using the directional segmentation improves SLAM accuracy and camera tracking efficiency via guided observation selection."
    }, {
      "heading" : "1. Related Work",
      "text" : "Among the wealth of recent 3D SLAM systems [13, 25, 33, 32, 49, 50, 29, 15, 12], there are only few who\n1\nar X\niv :1\n70 9.\n05 77\n4v 1\n[ cs\n.C V\n] 1\n8 Se\np 20\n17\njointly reason about 3D structure, geometric segmentation, and camera trajectory. The most common geometric prior is planarity of the environment. Castle et al. [10] are among the visual SLAM systems to incorporate planar geometry. They augment their visual SLAM system with the ability to detect known planar patches and use them to improve SLAM. Salas-Moreno et al. [39] integrate plane segmentation into the tracking and reconstruction pipeline of a dense surfel-based reconstruction system. Results show that utilizing a plane segmentation of the environment leads to improved tracking accuracy. Kaess [20] explores a plane-based SLAM formulation wherein the map directly consists of infinite planes which are being jointly optimized with the camera pose in a smoothing and mapping (SAM). Ma et al. [28] demonstrate joint inference over a key-frame-based map and a plane segmentation of the environment. The joint formulation with soft planeassignments reduces drift of the SLAM system. In comparison to the plane-based approaches, the proposed system does not need to explicitly extract planes and imposes scene-wide constraints as opposed to local constraints. Furthermore sampling-based inference allows soft associations to directions that can be refined and corrected whereas all but the EM-based CPA-SLAM [28] make hard assignments to specific planes that are not revisited [10, 39, 20]. More related to the proposed SCW-based approach is the system by Peasley et al. [37] who use the Manhattan World assumption to impose global constraints on the 2D trajectory of a robot. They show that this yields drift-free SLAM, eliminating the need for loop-closures, given that the MW assumption holds. Bosse et al. [6] essentially use the SCW assumption in the image space via vanishing point (VP) detection. They incorporate VP tracking into a SLAM system to jointly estimate a robot’s trajectory and the sparse 3D location of lines in the environment.\nBeyond the aforementioned planar, MW and SCW assumptions several approaches have been proposed that incorporate human-annotated semantic labels and shapes into SLAM. Bao et al. [3, 2] jointly estimate object and region segmentation of a sparse point cloud in the batch structure from motion framework. Similarly, Fioraio et al. [16] jointly perform incremental object detection, mapping and camera pose estimation in what they call semantic bundle adjustment. Xiao et al. [51] show how enforcing semantic label consistency in a 3D reconstruction system leads to better 3D reconstruction by decreasing drift and correcting loop closures. Kundun et al. [26] jointly use dense image segmentation and the raw RGB image captured from a single camera to infer the camera trajectory and, using a conditional random field (CRF) defined over an occupancy grid, a semantic 3D reconstruction. Working in the realm of RGBD cameras as well, Kim et al. [24] use a voxelbased world representation and, for a given RGBD image,\ninfer the 3D occupancy (i.e. the 3D structure) and the segmentation of the environment into semantic classes. SalasMoreno et al. [40] are the first to demonstrate a SLAM system that utilizes dense 3D object models as beacons for camera tracking and map representation. Closer in spirit to our approach, Cabezas et al. [8] use a mixture-model over scene features (appearance, surface normals and semantic observations) as a prior-probability model to discover and encourage scene-wide structure. They show that the learned scene-specific priors improve the 3D reconstruction. In comparison, our method not only discovers scenewide structure but also connects the scene-wide model to the local 3D reconstruction."
    }, {
      "heading" : "2. Direction-Aware Semi-Dense SLAM",
      "text" : "We define direction-aware SLAM as reasoning about the joint distribution of a world map m, the trajectory of the perception system and the directional segmentation z given observations x. Concretely, we represent the map as a set of surfels [21, 48, 17]. Surfels are localized planes with position pi, orientation ni, color Ii and radius ri. For notational clarity let si = {pi, ni, Ii, ri} collect all properties of surfel i. The SCW segmentation is expressed via surfel labels {zi}. The world is observed via a RGB-D camera at poses {Tt} where t indexes the pose at the reception of the tth camera frame. From the RGBD image, we obtain point observations xp, surface normal observations xn, and surface color Ic. We collect all observations of the tth frame in the variable xt = {xn, xp, Ic}. Hence, the direction-aware SLAM problem amounts to inference over the posterior:\np ({si}, {zi}, {Tt} | {xt}) dir.-aware SLAM . (1)\nWe perform inference on this direction-aware SLAM posterior by interleaving inference about the three subproblems of localization, mapping and directional segmentation:\np ({Tt} | {zi}, {si}, {xt}) dir.-aware loc , (2) p ({si} | {zi}, {Tt}, {xt}) dir.-aware mapping , (3) p ({zi} | {si}, {Tt}, {xt}) dir. segmentation . (4)\nTo accommodate operation at camera frame-rate the inference is split into two main parts: (1) real-time maximum likelihood camera pose estimation (Eq. (2)) and (2) sampling-based joined inference on segmentation and map (Eq. (3) and (4)) which runs in the background. An overview of the direction-aware SLAM system is depicted in Fig. 2.\nInstead of aiming to represent all surfaces in the environment densely, as in related work [21, 48, 17], we sample the surfaces of the environment sparsely with a bias towards high intensity gradient areas for two reasons: (1) a sparse sampling of environment surface captures the majority of surfaces and scene structure, (2) a bias towards high intensity gradient areas captures visually salient regions for camera tracking [15]. To substantiate the first point we show the percentage of inlier scene-points to a randomly sampled set of planes as a function of the number of planes in Fig. 3 across all 1449 scenes of the NYU v2 dataset [30]. As little as 50 planes are enough to describe an average of 60% of the scene."
    }, {
      "heading" : "3. Direction-aware Camera Pose Estimation",
      "text" : "For each observed RGBD frame we run the iterative closest point (ICP) algorithm to find a local optimum of the camera pose as well as data association between the observations and the global map surfels. The optimization of the camera pose given a projective data association A amounts to maximizing the negative log likelihood of the\ncamera pose:\nT ? = arg minT∈SE(3) fp2pl(T ) + λIfphoto(T ) (5) fp2pl = ∑ i∈A\n1 σ2p2pl,i ‖nTi (Tx p i − pi)‖22 (6) fphoto = ∑ i∈A\n1 σ2I ‖Ic(π(TT pi))− Ii‖22 , (7)\nwhere π(·) projects a 3D point into the image space. Note that while we have use the familiar notation for surfel properties (here ni, pi), in practice sample-based estimates, computed as described in Sec. 6, are used.\nThis cost function combines a point-to-plane (p2pl) and a photometric (photo) cost as employed by [50]. The probabilistic interpretation was developed in [41] and an extension to include a photometric term is straight forward [23]. The common strategy to obtain a camera pose estimate given data association is to Taylor-expand the error terms around the current transformation estimate:\nf = ∑ i∈A ‖ei(T ) + ∂ei(TExp(ω))∂ω ω‖ 2 2 = ‖Jω − b‖22 , (8)\nwhere we have collected the individual derivatives and error terms into the rows of J and b respectively. The variable ω ∈ se(3) is a small perturbation of the transformation in the tangent space to SE(3) at the current estimate of the transformation T . The least-squares solution for the (small) motion ω? along the manifold SE(3) is obtained via the standard pseudo inverse (JTJ)−1JT b . As noted previously by Kerl et al. [22] the term JTJ is the Fisher information matrix of the estimator. The variance of the estimate can be lower-bound by the Fisher information matrix using the Cramer-Rao bound [11]. Therefore, the entropy of the estimate ω? is lower-bound via\nH(ω) ≥ 3 log(2πe)− 12 log (∣∣JTJ∣∣) . (9)\nThe task of the perception system is to improve the lowerbound on the true variance and entropy to enable more certain estimates.\nOur variant of ICP, outlined in Algorithm 1, incrementally adds planes to the cost function until low enough entropy is reached. Planes are chosen in a round-robin style from each of the Stata Center World segments in order of decreasing surfel texture gradient strength. Intuitively a diverse set of observed plane orientations provides better constrain the point-to-plane cost function (at least three differently orientated planes have to be observed to constrain the system fully). Preference for high gradient image regions is important for the photometric part of the ICP cost function. Similar to the approach by Dellaert et al. [14], the proposed ICP variant selectively integrates informative observations which decreases the number of necessary observations in practice and thus speeds up camera tracking.\nAlgorithm 1 Direction-aware incremental ICP. 1: get observable surfels by rendering the map 2: while ICP not converged do 3: k = 0 4: while uncertainty too large do 5: pick surfel with next lower ‖∇I‖2 in dir. seg. k 6: if plane passes occlusion reasoning then 7: add to ICP JTJ and JT b; updated entropy 8: end if 9: k = (k + 1)%K 10: end while 11: compute transformation update 12: end while"
    }, {
      "heading" : "4. Directional Segmentation",
      "text" : "Under the Stata Center World model we make the assumption that the surface normal distribution of surfels has characteristic, low-entropy patterns as leveraged in related work by Straub et al. [45, 43, 44]. Similar to [43], we capture the notion of the Stata Center World model, that the surfel surface normal distribution consists of some variable, unknown number of clusters by a Dirichlet process von-Mises-Fisher mixture model. Following the proposal of [36], we impose spatial smoothness of the Stata Center World segmentation by assuming a Markov random field (MRF) over the segmentation z that encourages uniform labeling inside a set Ni of neighboring surfels of surfel i.\nFrom a generative standpoint, this model first samples a countably infinite set of cluster weights πk, von-MisesFisher means µk, and concentrations τk from a Dirichlet process with concentration α and base measure G0:\n{πk, µk, τk}∞k=1 ∼ DP(α,G0) (10)\nTo define the base measure, we utilize the conjugate prior for the von-Mises-Fisher distribution which in general is only known up to proportionality [35]:\np(µ, τ | µ0, a, b) ∝ ( τD/2−1\nID/2−1(τ)\n)a exp ( bτµTµ0 ) , (11)\nwhere 0 < b < a. The parameters of the prior are the directional mode µ0 and a and b where a can be understood as pseudo-counts and b as the concentration mode. Second, given the cluster weights πk and the local neighborhood Ni, a label zi ∈ {1, . . . ,∞} is sampled to assign each surfel to a von-Mises-Fisher distribution zi:\nzi ∼ MRFz (zi, {zj}j∈Ni ;λ) Cat ({πk}∞k=1) (12)\nThe MRF smoothness component in practice helps speed up inference and leads to more uniform segmentations in the face of noise. It takes the form:\nMRFz = exp ( λ ∑ j∈Ni 1 zi zj − λ|Ni| ) , (13)\nwhere λ is the weight of the MRF contribution and 1ab is 1 if a = b and 0 otherwise."
    }, {
      "heading" : "5. Direction-aware Mapping",
      "text" : "We use another Markov random field over neighboring surfels to express a local planarity assumption over points in the same directional segment. The MRF connects the scenewide directional segmentation with local spatial properties. The MRF potential Ψplij(pi, ni, pj , nj , z) that encapsulates local planarity is obtained by symmetrizing the well known point-to-plane distance function used in implementations of ICP [38]:\nexp ( −1zizj 2σ2pl ( ‖nTi (pj − pi)‖22 + ‖nTj (pi − pj)‖22 )) . (14)\nWhile the point-to-plane cost function penalizes the outof-plane deviation of a point, the MRF potential employed herein can be seen as the product of two Gaussians with variance σ2pl over the out-of-plane deviation of the respective other surfel location. This geometry is shown in Fig. 4."
    }, {
      "heading" : "5.1. Observation Models for Mapping",
      "text" : "Surfel locations and orientations are observed via the camera located at the estimated pose T . Associations between RGB-D observations and map surfels are established using projective data association [32]. Back-facing and occluded surfels are pruned. Occlusion is detected if a surfel observation has low probability. Capturing the cameraframe times at which observations of surfel i were taken in the set Oi, we assume an iid Gaussian observation model for locations {xpj}j∈Oi :\np ( {xpj}Oi | pi ) = ∏ j∈Oi N (x p j ;T −1 t pi,Σp,j) . (15)\nThe observation covariances Σp,j are computed according to a realistic depth camera noise model [34] and incorporate the linearized camera pose uncertainty:\nΣp,j = ΣO,j + JTΣTJ T T . (16)\nThe surfel orientation observations {xpj}j∈Oi are assumed to be iid von-Mises-Fisher distributed:\np({xnj }Oi | ni; τO) = ∏ j∈Oi vMF(x n j ;R T t ni, τO) , (17)\nwhere we have used the inferred camera rotations {Rt}j∈Oi . Surface normals are extracted using the fast yet robust unconstrained scatter-matrix approach by Badino et al. [1]. It is unclear how camera pose noise and depth image noise influences the surface normal concentration. Hence, we use a conservative observation concentration of τO = 100 which makes the realistic assumption that 99% of the observed surface normals lie within a solid angle of about 18◦ around the true surface normal. A more detailed model could be obtained with a controlled experiment similar to [34]."
    }, {
      "heading" : "6. Sampling-based Inference over SCW Map",
      "text" : "We now turn to describing how to perform posterior inference on the joint SCW map model given observations {xp, xn, Ic} from inferred camera poses Tt. Because the directional segmentation involves a Bayesian nonparametric Dirichlet process prior, we rely on Gibbs sampling inference, which in the limit of sampling guarantees samples from the true posterior distribution. The Gibbs sampler iterates sampling from the different conditional distributions of each random variable in the join SCW map model. In the following we provide details on sampling from each conditional distributions before detailing how samples are used to inform camera tracking.\nSampling Normals ni Via Bayes’ law, the conditional distribution of surfel direction ni, p(ni|{xnj }i, p, z), is proportional to\np(ni|µzi , τzi)p ( {xnj }i|ni )∏ j∈Ni Ψ pl ij\n∝ vMF(ni;µzi , τzi) ∏ j∈Oi vMF(x n j , R\nT t ni, τO)∏\nj∈Ni N (n T i pi;n T i pj , σ 2 pl)\n1 zi zj ,\n(18)\nwhere we have abbreviated {·}j∈Oi with {·}i and used that only one of the two out-of-plane Gaussians in the MRF depends on ni. The first factor stems from the directional Stata Center World mixture model, and the second from the surface normal observation model. To sample from this distribution we derive a close approximation to the out-ofplane Gaussian that has the form of a vMF distribution. This makes the posterior over surface normals von-Mises-Fisher distributed which can be sampled efficiently. The Gaussian distribution on out-of-plane deviations of neighboring points can be re-arranged as\nN (nTi pi;nTi pj , σ2pl) 1 zi zj ∝ exp ( − 12n T i Sini ) , (19)\nwhere Si = ∑ j∈Ni 1 zi zj σ2pl (pi − pj)(pi − pj)T . This distribution has the form of a Bingham distribution [4]. To keep in the realm of the von-Mises-Fisher distribution, we approximate this Bingham with a vMF distribution using the eigen decomposition of Si with eigenvalues e1 < e2 < e3 and associated eigenvectors q1, q2, q3:\nexp(− 12n T i Sini) ≈ exp ( 2e2e2 e2+e3 qT1 ni ) , (20)\nwhich is proportional to a vMF distribution with mode q1 and concentration 2e2e2e2+e3 . Figure 5 shows that the vMF approximation is close to the Bingham distribution for several realistic standard deviations of planar and slightly curved surfaces. In practice, since Si incorporates only neighbors in the same directional segment (which are therefore likely to lie roughly in the same plane), we find the approximation to work well.\nUnder this approximation the posterior p(ni|{xnj }i, p, z) over surface normal ni is indeed proportional to a vonMises-Fisher:\np({xnj }i|ni)p(ni|µ, τ, zi)p(ni|p, z) ∝ vMF(dϑe, ‖ϑ‖2) ϑ = ∑ j∈Oi x n j τO + µziτzi + 2e2e2 e2+e3 q1 , (21)\nwhere xnj have been rotated into the world camera frame using the appropriate Rt and dϑe = ϑ‖ϑ‖2 . An efficient method for sampling from a von-Mises-Fisher distribution is outlined in [47].\nSampling Directional Segmentation Labels zi We use the Chinese restaurant process (CRP) representation of the Dirichlet process [5, 31] since it lends itself to straightforward sampling-based inference. The posterior for the directional segmentation label of surfel i is:\np(zi=k|zNi , µ, τ) ∝ exp ( λ ∑ j∈Ni 1 zi zj − λ|Ni| ) ( 1zik Nk vMF(ni;µk, τk) + 1 z1 K+1αp(ni;G0) ) (22)\nwhere Nk is the number of surfels associated to cluster k excepting the ith surfel, α is the Dirichlet process concentration and λ is the weight of the MRF contribution. The marginal distribution of surface normal ni under the prior on the vMF component distribution, p(ni;G0), can be derived in closed form for the vMF prior parameters a = 1 and 0 < b < 1 in D = 3 dimensions (see Sec. 2.6.3 [42])\np(ni;µ0, a = 1, b) = bτ 2π2 exp\n( bτµTµ0 ) tan ( bπ 2 ) sinh(τ) . (23)\nSampling vMF Parameters µk and τk Given sampled normals ni assigned to von-Mises-Fisher clusters via labels zi the posterior over the kth vMF mixture component mode µk and concentration τk is:\np(µk, τk|n, z;G0) ∝ p(µk, τk;G0) ∏ i∈Ik p(ni|µk, τk)\n∝ p(µk, τk | µ̃k0 , ãk, b̃k) , (24)\nwhere Ik collects all surfels associated to cluster k. With ϑ = ∑ i∈Ik ni + bµ0, the posterior parameters ãk and b̃k are computed as\nãk = a+ |Ik| , b̃k = ‖ϑ‖2 , µ̃k0 = dϑe . (25)\nSampling Locations pi Conditioned on point observations {xpj}j∈Oi , and a surfel’s neighborhood Ni, a surfel’s position is distributed as:\np(pi|{xpj}i, n, p, z)∝ ∏ j∈Ni Ψplij ∏ j∈Oi p(xpj |pi) (26)\nwhere the observation model p(xpj | pi) is Gaussian as defined in Eq. (15). The MRF potential Ψplij from Eq. (14) is proportional to:\nexp\n( −1zizj\n2 p T i Iijpi + 1 zi zjp T i Iijpj\n) (27)\nwhere Iij = 1σ2pl (nin T i +njn T j ) is the information matrix of a degenerate Gaussian in information form and Iijpj is its scaled mean. Since the individual distributions are all Gaussian the posterior over surfel location pi is also Gaussian [7] with the following mean and variance:\np(pi | {xpi }, ni, p, z) ∝ N (pi; µ̃i, Σ̃i) (28) Σ̃−1i = ∑ j∈Oi Σ̃ −1 p,j + ∑ j∈Ni 1 zi zjIij (29)\nΣ̃−1i µ̃i = ∑ j∈Oi Σ̃ −1 p,jTtx p j + ∑ j∈Ni 1 zi zjIijpj . (30)\nwhere Σ̃−1p,j = RtΣ −1 p,jR T t . Note that there is always at least one observation (i.e. |Oi| > 0) and therefore the inversion to compute the variance is always determined."
    }, {
      "heading" : "6.1. Estimates Computed from the Samples",
      "text" : "We use the Gibbs-sampler samples to approximately compute means and variances of surfel locations and orientations. Via the law of large numbers and by the construction of the Gibbs sampler this approach will in the limit converge to the true means and variances [9]. In practice, since the marginal distributions p(pi) or p(ni) are mostly concentrated about a single mode, the estimates converge quickly. In our experiments in the order of ten samples were sufficient to get usable estimates for real-time camera tracking as described in Sec. 3.\nGiven a set of samples {ξpj }Si from the distribution of surfel locations pi, we estimate the mean p̄i and variance Σ̄i of the surfel location using the accumulated statistics p̃i = ∑ j∈Si ξ p j and Opi = ∑ j∈Si ξ p j ξ pT j :\np̄i= Epi [pi] ≈ p̃i |Si| , Σ̄i= Var (pi) ≈ Opi |Si| − p̃ip̃ T i . (31)\nNote that samples ξpj are not samples from a Gaussian distribution but the maximum entropy distribution of pi is a Gaussian with the aforementioned mean and variance. The entropy of this Gaussian is an upper bound on the true entropy of the surfel location distribution and can serve as a scalar indicator of the uncertainty.\nFrom the surfel normal samples {ξnj }j∈Oi we compute the mode n̄i of a vMF distribution for camera tracking using the accumulated statistics ñi:\nn̄i = dñie , ñi = ∑ j∈Si ξ n j . (32)\nTo compute the most likely directional segment z̄i of surfel i we would ideally keep a count of the number of times the surfel is assigned to each directional cluster via label zi. Since the number of clusters keeps growing and we aim for this estimation to be efficient for large numbers of surfels, we only keep track of the K̃ = 3 most likely cluster assignments incrementally."
    }, {
      "heading" : "7. Implementation",
      "text" : "In practice, to use the proposed approach we architect a multi-threaded system as depicted in Fig. 2. The main five threads are (1) a real-time data acquisition, camera tracking and observation extraction thread, (2) a nearest neighborhood graph builder thread and (3-5) three Gibbs sampler threads. Camera tracking utilizes RGBD frames and the current most likely estimate of the segmentation and surfel map to infer the current camera pose T . To be able to deal with fast motions we perform photometric rotational pre-alignment [27] from image pyramid level 3 down to 1. For the same reason, we run direction-aware ICP from scale pyramid levels 1 down to 0. The observation extraction algorithm adds new surfels by uniformly sampling so-far unobserved surfaces with a bias towards high gradient surface\nareas similar to [15]. The graph builder thread uses the initial locations of all surfels to maintain a k-nearest-neighbor graph over surfels (here k = 12) using the negative log MRF potential from Eq. (14) as the distance function. Valid neighbors have to be within a Euclidean radius of 0.2 m. This is an approximation to the directed graph that could be obtained by connecting all surfels within some distance. Retaining only the top k closest (under the potential) surfels improves algorithm efficiency without notable differences in the reconstruction results. To deal with deleted and newly added surfels, the thread additionally randomly revisits and potentially updates the nearest neighbors of already incorporated surfels. We split the Gibbs sampler into three threads each sampling (at its own speed) from the respective posterior given samples from the other threads. There exists only preliminary research on parallel Gibbs sampling under the name Hogwild Gibbs sampling [19] and it is unclear if\nthere are theoretical guarantees. In practice breaking the samplers into parallel threads seems to make no difference."
    }, {
      "heading" : "8. Evaluation and Results",
      "text" : "In the following we evaluate the proposed directionaware 3D reconstruction system on various challenging datasets quantitatively as well as qualitatively. All experiments are performed on a machine with an Intel Xeon CPU with 16 cores at 2.4 GHz and a Nvidia GTX-1080 graphics card. As described in Sec. 7, the algorithm utilizes a total of 5 CPU cores for the main inference tasks. Surface normals are computed only sparsely on CPU wherever needed. The GPU is used for the full-frame operations of SO(3) prealignment and data preprocessing.\nQualitative Reconstruction Results In Figures 1, 6, and 7 we show the RGB-colored and the SCW segmented 3D reconstructions of different scenes. As can be seen, the maximum a-posteriori estimate of the Stata Center World segmentation sensibly partitions the environment according to the surfel directions. The inference extracts the main peaks of the distribution which correspond to planar regions in the scene. Additionally, low concentration clusters are inferred that capture noisy, non-planar regions (green in top and yellow in bottom row of Fig. 6, yellow in Fig. 7).\nAlgorithm Operation and Properties To explore the properties of the algorithm, we discuss timings, surfel and sampling statistics collected during the reconstruction of the\nfr2 xyz dataset [46] displayed in Fig. 6. Figure 8 (left) shows that the main camera tracking thread mostly runs in less than 50ms per frame. Runtime increases when the camera moves far away from the scene and ICP processes more points for confident camera tracking (see Fig. 8 middle). The runtime of the surfel parameter sampling threads scales with the size of the map (compare Fig. 8 middle). As can be seen in Fig. 8 (middle), the number of surfels utilized for camera tracking is usually less than 1000 surfels even if a magnitude more surfels are in view. This is enabled by the direction and gradient-aware selection of surfel observations. The statistics in Fig. 8 (right) show that while the number of surfels in the map keeps growing, the sampling threads yield sufficient samples per surfel.\nCamera Tracking Accuracy Comparison We use the TUM indoor dataset [46] and the synthetic dataset by Handa et al. [18] to evaluate the camera tracking accuracy against groundtruth via the absolute trajectory error (ATE) [46] and compare our system to related 3D SLAM systems in Fig. 9. Fig. 9 demonstrates that the proposed directional SLAM system is on par or better than related algorithms in terms of camera trajectory estimation for datasets\nwithout the need for loop closures. The Dir. SLAM Random system uses direct surfel fusion and randomly selects ICP observations. As can be seen, disregarding the directional segmentation decreases tracking accuracy especially on the real datasets fr2 xyz and fr2 desk."
    }, {
      "heading" : "9. Conclusion",
      "text" : "We have introduced the first direction-aware semi-dense SLAM system which performs joint inference over directional segmentation, surfel-based map and camera pose. Its direction-awareness manifests in that it can utilize the directional segmentation for its other tasks. The use of Gibbssampling-based inference on the complex Bayesian nonparametric segmentation and map model in a real-time reconstruction system has not been demonstrated before. Due to the flexibility of Gibbs-sampling this opens up exciting possibilities for inference on more complex and detailed environment models. Having access to samples from the posterior also allows reasoning about uncertainty which is not possible with the commonly employed mode-seeking inference methods."
    } ],
    "references" : [ {
      "title" : "Fast and accurate computation of surface normals from range images",
      "author" : [ "H. Badino", "D. Huber", "Y. Park", "T. Kanade" ],
      "venue" : "ICRA, pages 3084–3091.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Semantic structure from motion with points, regions, and objects",
      "author" : [ "S.Y. Bao", "M. Bagra", "Y.-W. Chao", "S. Savarese" ],
      "venue" : "CVPR, pages 2703–2710.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Semantic structure from motion",
      "author" : [ "S.Y. Bao", "S. Savarese" ],
      "venue" : "CVPR, pages 2025–2032.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "An antipodally symmetric distribution on the sphere",
      "author" : [ "C. Bingham" ],
      "venue" : "The Annals of Statistics, 2(6):1201–1225,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1974
    }, {
      "title" : "Ferguson distributions via pólya urn schemes",
      "author" : [ "D. Blackwell", "J.B. MacQueen" ],
      "venue" : "The Annals of Statistics, pages 353– 355,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1973
    }, {
      "title" : "Vanishing points and three-dimensional lines from omni-directional video",
      "author" : [ "M. Bosse", "R. Rikoski", "J. Leonard", "S. Teller" ],
      "venue" : "The Visual Computer, 19(6):417–430,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Products and convolutions of Gaussian probability density functions",
      "author" : [ "P. Bromiley" ],
      "venue" : "Technical Report Tina Memo No. 2003-003,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2003
    }, {
      "title" : "Semantically- Aware Aerial Reconstruction from Multi-Modal Data",
      "author" : [ "R. Cabezas", "J. Straub", "J.W. Fisher III" ],
      "venue" : "ICCV,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Explaining the gibbs sampler",
      "author" : [ "G. Casella", "E. George" ],
      "venue" : "The American Statistician, 46(3):167–174,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Towards simultaneous recognition, localization and mapping for hand-held and wearable cameras",
      "author" : [ "R.O. Castle", "D. Gawley", "G. Klein", "D.W. Murray" ],
      "venue" : "ICRA,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Mathematical Methods of Statistics, volume 9",
      "author" : [ "H. Cramér" ],
      "venue" : "Princeton University Press,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "BundleFusion: Real-time Globally Consistent 3D Reconstruction using On-the-fly Surface Re-integration TOG, 2017",
      "author" : [ "A. Dai", "M. Nießner", "M. Zollhöfer", "S. Izadi", "C. Theobalt" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2017
    }, {
      "title" : "Real-time simultaneous localisation and mapping with a single camera",
      "author" : [ "A.J. Davison" ],
      "venue" : "ICCV,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Fast image-based tracking by selective pixel integration",
      "author" : [ "F. Dellaert", "R. Collins" ],
      "venue" : "Proceedings of the ICCV Workshop on Frame-Rate Vision,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "LSD-SLAM: Largescale direct monocular SLAM",
      "author" : [ "J. Engel", "T. Schöps", "D. Cremers" ],
      "venue" : "ECCV,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Joint detection, tracking and mapping by semantic bundle adjustment",
      "author" : [ "N. Fioraio", "L. Di Stefano" ],
      "venue" : "CVPR,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A surface-growing approach to multi-view stereo reconstruction",
      "author" : [ "M. Habbecke", "L. Kobbelt" ],
      "venue" : "CVPR,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "A benchmark for RGB-D visual odometry, 3D reconstruction and SLAM",
      "author" : [ "A. Handa", "T. Whelan", "J. McDonald", "A.J. Davison" ],
      "venue" : "ICRA,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Analyzing hogwild parallel Gaussian Gibbs sampling",
      "author" : [ "M. Johnson", "J. Saunderson", "A. Willsky" ],
      "venue" : "NIPS,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Simultaneous localization and mapping with infinite planes",
      "author" : [ "M. Kaess" ],
      "venue" : "ICRA,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Real-time 3D reconstruction in dynamic scenes using point-based fusion",
      "author" : [ "M. Keller", "D. Lefloch", "M. Lambers", "S. Izadi", "T. Weyrich", "A. Kolb" ],
      "venue" : "International Conference on 3DTV-Conference,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Dense visual SLAM for RGB-D cameras",
      "author" : [ "C. Kerl", "J. Sturm", "D. Cremers" ],
      "venue" : "IROS,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Robust odometry estimation for RGB-D cameras",
      "author" : [ "C. Kerl", "J. Sturm", "D. Cremers" ],
      "venue" : "ICRA,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "3D scene understanding by voxel-CRF",
      "author" : [ "B.-s. Kim", "P. Kohli", "S. Savarese" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2013
    }, {
      "title" : "Parallel tracking and mapping on a camera phone",
      "author" : [ "G. Klein", "D. Murray" ],
      "venue" : "ISMAR,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Joint semantic segmentation and 3D reconstruction from monocular video",
      "author" : [ "A. Kundu", "Y. Li", "F. Dellaert", "F. Li", "J.M. Rehg" ],
      "venue" : "ECCV,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Real-time spherical mosaicing using whole image alignment",
      "author" : [ "S. Lovegrove", "A.J. Davison" ],
      "venue" : "ECCV,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "CPA-SLAM: Consistent plane-model alignment for direct RGB-D SLAM",
      "author" : [ "L. Ma", "C. Kerl", "J. Stueckler", "D. Cremers" ],
      "venue" : "ICRA,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "ORB- SLAM: a versatile and accurate monocular SLAM system",
      "author" : [ "R. Mur-Artal", "J.M.M. Montiel", "J.D. Tardos" ],
      "venue" : "Transactions on Robotics, 31(5):1147–1163,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Indoor segmentation and support inference from RGBD images",
      "author" : [ "P.K. Nathan Silberman", "Derek Hoiem", "R. Fergus" ],
      "venue" : "In ECCV,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2012
    }, {
      "title" : "Markov chain sampling methods for Dirichlet process mixture models",
      "author" : [ "R. Neal" ],
      "venue" : "Journal of computational and graphical statistics, 9(2):249–265,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Kinectfusion: Real-time dense surface mapping and tracking",
      "author" : [ "R.A. Newcombe", "A.J. Davison", "S. Izadi", "P. Kohli", "O. Hilliges", "J. Shotton", "D. Molyneaux", "S. Hodges", "D. Kim", "A. Fitzgibbon" ],
      "venue" : "ISMAR,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "DTAM: Dense tracking and mapping in real-time",
      "author" : [ "R.A. Newcombe", "S.J. Lovegrove", "A.J. Davison" ],
      "venue" : "ICCV,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Modeling Kinect sensor noise for improved 3D reconstruction and tracking",
      "author" : [ "C.V. Nguyen", "S. Izadi", "D. Lovell" ],
      "venue" : "3DIMPVT,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A Bayesian analysis of directional data using the von Mises-Fisher distribution",
      "author" : [ "G. Nunez-Antonio", "E. Gutiérrez-Pena" ],
      "venue" : "Communications in StatisticsSimulation and Computation R  ©, 34(4):989–999,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Smooth image segmentation by nonparametric Bayesian inference",
      "author" : [ "P. Orbanz", "J. Buhmann" ],
      "venue" : "ECCV,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Accurate on-line 3D occupancy grids using Manhattan world constraints",
      "author" : [ "B. Peasley", "S. Birchfield", "A. Cunningham", "F. Dellaert" ],
      "venue" : "IROS,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Efficient variants of the ICP algorithm",
      "author" : [ "S. Rusinkiewicz", "M. Levoy" ],
      "venue" : "International Conference on 3-D Digital Imaging and Modeling,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Dense planar SLAM",
      "author" : [ "R.F. Salas-Moreno", "B. Glocken", "P.H. Kelly", "A.J. Davison" ],
      "venue" : "ISMAR,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "SLAM++: Simultaneous localisation and mapping at the level of objects",
      "author" : [ "R.F. Salas-Moreno", "R.A. Newcombe", "H. Strasdat", "P.H. Kelly", "A.J. Davison" ],
      "venue" : "CVPR,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Generalized-ICP",
      "author" : [ "A. Segal", "D. Haehnel", "S. Thrun" ],
      "venue" : "RSS,",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Nonparametric Directional Perception",
      "author" : [ "J. Straub" ],
      "venue" : "PhD thesis, Massachusetts Institute of Technology,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Smallvariance nonparametric clustering on the hypersphere",
      "author" : [ "J. Straub", "T. Campbell", "J.P. How", "J.W. Fisher III" ],
      "venue" : "CVPR,",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A dirichlet process mixture model for spherical data",
      "author" : [ "J. Straub", "J. Chang", "O. Freifeld", "J.W. Fisher III" ],
      "venue" : "AISTATS,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "The Manhattan frame model – Manhattan world inference in the space of surface normals",
      "author" : [ "J. Straub", "G. Rosman", "O. Freifeld", "J.J. Leonard", "J.W. Fisher III" ],
      "venue" : "TPAMI,",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "A benchmark for the evaluation of RGB-D SLAM systems",
      "author" : [ "J. Sturm", "N. Engelhard", "F. Endres", "W. Burgard", "D. Cremers" ],
      "venue" : "IROS,",
      "citeRegEx" : "46",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Computer generation of distributions on the msphere",
      "author" : [ "G. Ulrich" ],
      "venue" : "Applied Statistics, pages 158–163,",
      "citeRegEx" : "47",
      "shortCiteRegEx" : null,
      "year" : 1984
    }, {
      "title" : "In-hand scanning with online loop closure",
      "author" : [ "T. Weise", "T. Wismer", "B. Leibe", "L. Van Gool" ],
      "venue" : "ICCV Workshops,",
      "citeRegEx" : "48",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Kintinuous: Spatially extended kinectfusion",
      "author" : [ "T. Whelan", "M. Kaess", "M. Fallon", "H. Johannsson", "J. Leonard", "J. McDonald" ],
      "venue" : null,
      "citeRegEx" : "49",
      "shortCiteRegEx" : "49",
      "year" : 2012
    }, {
      "title" : "Elasticfusion: Real-time dense SLAM and light source estimation",
      "author" : [ "T. Whelan", "R.F. Salas-Moreno", "B. Glocker", "A.J. Davison", "S. Leutenegger" ],
      "venue" : "IJRR, pages 1697–1716,",
      "citeRegEx" : "50",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Sun3D: A database of big spaces reconstructed using SFM and object labels",
      "author" : [ "J. Xiao", "A. Owens", "A. Torralba" ],
      "venue" : "ICCV,",
      "citeRegEx" : "51",
      "shortCiteRegEx" : null,
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "In order to improve 3D reconstruction and localization via scene understanding most approaches rely on geometric scene priors such as planarity [10, 39, 20, 28], the Manhattan World [37] (MW) or the Stata Center World [6] (SCW) assumption.",
      "startOffset" : 144,
      "endOffset" : 160
    }, {
      "referenceID" : 38,
      "context" : "In order to improve 3D reconstruction and localization via scene understanding most approaches rely on geometric scene priors such as planarity [10, 39, 20, 28], the Manhattan World [37] (MW) or the Stata Center World [6] (SCW) assumption.",
      "startOffset" : 144,
      "endOffset" : 160
    }, {
      "referenceID" : 19,
      "context" : "In order to improve 3D reconstruction and localization via scene understanding most approaches rely on geometric scene priors such as planarity [10, 39, 20, 28], the Manhattan World [37] (MW) or the Stata Center World [6] (SCW) assumption.",
      "startOffset" : 144,
      "endOffset" : 160
    }, {
      "referenceID" : 27,
      "context" : "In order to improve 3D reconstruction and localization via scene understanding most approaches rely on geometric scene priors such as planarity [10, 39, 20, 28], the Manhattan World [37] (MW) or the Stata Center World [6] (SCW) assumption.",
      "startOffset" : 144,
      "endOffset" : 160
    }, {
      "referenceID" : 36,
      "context" : "In order to improve 3D reconstruction and localization via scene understanding most approaches rely on geometric scene priors such as planarity [10, 39, 20, 28], the Manhattan World [37] (MW) or the Stata Center World [6] (SCW) assumption.",
      "startOffset" : 182,
      "endOffset" : 186
    }, {
      "referenceID" : 5,
      "context" : "In order to improve 3D reconstruction and localization via scene understanding most approaches rely on geometric scene priors such as planarity [10, 39, 20, 28], the Manhattan World [37] (MW) or the Stata Center World [6] (SCW) assumption.",
      "startOffset" : 218,
      "endOffset" : 221
    }, {
      "referenceID" : 42,
      "context" : "As shown in [43, 44], the directional clustering of a scene’s surface normals under the Stata Center World implies a segmentation that captures scene-wide regularities of the environment [45] as can be seen in the segmentation in Fig.",
      "startOffset" : 12,
      "endOffset" : 20
    }, {
      "referenceID" : 43,
      "context" : "As shown in [43, 44], the directional clustering of a scene’s surface normals under the Stata Center World implies a segmentation that captures scene-wide regularities of the environment [45] as can be seen in the segmentation in Fig.",
      "startOffset" : 12,
      "endOffset" : 20
    }, {
      "referenceID" : 44,
      "context" : "As shown in [43, 44], the directional clustering of a scene’s surface normals under the Stata Center World implies a segmentation that captures scene-wide regularities of the environment [45] as can be seen in the segmentation in Fig.",
      "startOffset" : 187,
      "endOffset" : 191
    }, {
      "referenceID" : 12,
      "context" : "Among the wealth of recent 3D SLAM systems [13, 25, 33, 32, 49, 50, 29, 15, 12], there are only few who",
      "startOffset" : 43,
      "endOffset" : 79
    }, {
      "referenceID" : 24,
      "context" : "Among the wealth of recent 3D SLAM systems [13, 25, 33, 32, 49, 50, 29, 15, 12], there are only few who",
      "startOffset" : 43,
      "endOffset" : 79
    }, {
      "referenceID" : 32,
      "context" : "Among the wealth of recent 3D SLAM systems [13, 25, 33, 32, 49, 50, 29, 15, 12], there are only few who",
      "startOffset" : 43,
      "endOffset" : 79
    }, {
      "referenceID" : 31,
      "context" : "Among the wealth of recent 3D SLAM systems [13, 25, 33, 32, 49, 50, 29, 15, 12], there are only few who",
      "startOffset" : 43,
      "endOffset" : 79
    }, {
      "referenceID" : 48,
      "context" : "Among the wealth of recent 3D SLAM systems [13, 25, 33, 32, 49, 50, 29, 15, 12], there are only few who",
      "startOffset" : 43,
      "endOffset" : 79
    }, {
      "referenceID" : 49,
      "context" : "Among the wealth of recent 3D SLAM systems [13, 25, 33, 32, 49, 50, 29, 15, 12], there are only few who",
      "startOffset" : 43,
      "endOffset" : 79
    }, {
      "referenceID" : 28,
      "context" : "Among the wealth of recent 3D SLAM systems [13, 25, 33, 32, 49, 50, 29, 15, 12], there are only few who",
      "startOffset" : 43,
      "endOffset" : 79
    }, {
      "referenceID" : 14,
      "context" : "Among the wealth of recent 3D SLAM systems [13, 25, 33, 32, 49, 50, 29, 15, 12], there are only few who",
      "startOffset" : 43,
      "endOffset" : 79
    }, {
      "referenceID" : 11,
      "context" : "Among the wealth of recent 3D SLAM systems [13, 25, 33, 32, 49, 50, 29, 15, 12], there are only few who",
      "startOffset" : 43,
      "endOffset" : 79
    }, {
      "referenceID" : 9,
      "context" : "[10] are among the visual SLAM systems to incorporate planar geometry.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 38,
      "context" : "[39] integrate plane segmentation into the tracking and reconstruction pipeline of a dense surfel-based reconstruction system.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "Kaess [20] explores a plane-based SLAM formulation wherein the map directly consists of infinite planes which are being jointly optimized with the camera pose in a smoothing and mapping (SAM).",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 27,
      "context" : "[28] demonstrate joint inference over a key-frame-based map and a plane segmentation of the environment.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 27,
      "context" : "Furthermore sampling-based inference allows soft associations to directions that can be refined and corrected whereas all but the EM-based CPA-SLAM [28] make hard assignments to specific planes that are not revisited [10, 39, 20].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 9,
      "context" : "Furthermore sampling-based inference allows soft associations to directions that can be refined and corrected whereas all but the EM-based CPA-SLAM [28] make hard assignments to specific planes that are not revisited [10, 39, 20].",
      "startOffset" : 217,
      "endOffset" : 229
    }, {
      "referenceID" : 38,
      "context" : "Furthermore sampling-based inference allows soft associations to directions that can be refined and corrected whereas all but the EM-based CPA-SLAM [28] make hard assignments to specific planes that are not revisited [10, 39, 20].",
      "startOffset" : 217,
      "endOffset" : 229
    }, {
      "referenceID" : 19,
      "context" : "Furthermore sampling-based inference allows soft associations to directions that can be refined and corrected whereas all but the EM-based CPA-SLAM [28] make hard assignments to specific planes that are not revisited [10, 39, 20].",
      "startOffset" : 217,
      "endOffset" : 229
    }, {
      "referenceID" : 36,
      "context" : "[37] who use the Manhattan World assumption to impose global constraints on the 2D trajectory of a robot.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 5,
      "context" : "[6] essentially use the SCW assumption in the image space via vanishing point (VP) detection.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[3, 2] jointly estimate object and region segmentation of a sparse point cloud in the batch structure from motion framework.",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 1,
      "context" : "[3, 2] jointly estimate object and region segmentation of a sparse point cloud in the batch structure from motion framework.",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 15,
      "context" : "[16] jointly perform incremental object detection, mapping and camera pose estimation in what they call semantic bundle adjustment.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 50,
      "context" : "[51] show how enforcing semantic label consistency in a 3D reconstruction system leads to better 3D reconstruction by decreasing drift and correcting loop closures.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 25,
      "context" : "[26] jointly use dense image segmentation and the raw RGB image captured from a single camera to infer the camera trajectory and, using a conditional random field (CRF) defined over an occupancy grid, a semantic 3D reconstruction.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "[24] use a voxelbased world representation and, for a given RGBD image, Sensor",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 39,
      "context" : "[40] are the first to demonstrate a SLAM system that utilizes dense 3D object models as beacons for camera tracking and map representation.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 7,
      "context" : "[8] use a mixture-model over scene features (appearance, surface normals and semantic observations) as a prior-probability model to discover and encourage scene-wide structure.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 20,
      "context" : "Concretely, we represent the map as a set of surfels [21, 48, 17].",
      "startOffset" : 53,
      "endOffset" : 65
    }, {
      "referenceID" : 47,
      "context" : "Concretely, we represent the map as a set of surfels [21, 48, 17].",
      "startOffset" : 53,
      "endOffset" : 65
    }, {
      "referenceID" : 16,
      "context" : "Concretely, we represent the map as a set of surfels [21, 48, 17].",
      "startOffset" : 53,
      "endOffset" : 65
    }, {
      "referenceID" : 29,
      "context" : "The plots summarize statistics over all of the scenes in the NYU v2 dataset [30].",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 20,
      "context" : "Instead of aiming to represent all surfaces in the environment densely, as in related work [21, 48, 17], we sample the surfaces of the environment sparsely with a bias towards high intensity gradient areas for two reasons: (1) a sparse sampling of environment surface captures the majority of surfaces and scene structure, (2) a bias towards high intensity gradient areas captures visually salient regions for camera tracking [15].",
      "startOffset" : 91,
      "endOffset" : 103
    }, {
      "referenceID" : 47,
      "context" : "Instead of aiming to represent all surfaces in the environment densely, as in related work [21, 48, 17], we sample the surfaces of the environment sparsely with a bias towards high intensity gradient areas for two reasons: (1) a sparse sampling of environment surface captures the majority of surfaces and scene structure, (2) a bias towards high intensity gradient areas captures visually salient regions for camera tracking [15].",
      "startOffset" : 91,
      "endOffset" : 103
    }, {
      "referenceID" : 16,
      "context" : "Instead of aiming to represent all surfaces in the environment densely, as in related work [21, 48, 17], we sample the surfaces of the environment sparsely with a bias towards high intensity gradient areas for two reasons: (1) a sparse sampling of environment surface captures the majority of surfaces and scene structure, (2) a bias towards high intensity gradient areas captures visually salient regions for camera tracking [15].",
      "startOffset" : 91,
      "endOffset" : 103
    }, {
      "referenceID" : 14,
      "context" : "Instead of aiming to represent all surfaces in the environment densely, as in related work [21, 48, 17], we sample the surfaces of the environment sparsely with a bias towards high intensity gradient areas for two reasons: (1) a sparse sampling of environment surface captures the majority of surfaces and scene structure, (2) a bias towards high intensity gradient areas captures visually salient regions for camera tracking [15].",
      "startOffset" : 426,
      "endOffset" : 430
    }, {
      "referenceID" : 29,
      "context" : "3 across all 1449 scenes of the NYU v2 dataset [30].",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 49,
      "context" : "This cost function combines a point-to-plane (p2pl) and a photometric (photo) cost as employed by [50].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 40,
      "context" : "The probabilistic interpretation was developed in [41] and an extension to include a photometric term is straight forward [23].",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 22,
      "context" : "The probabilistic interpretation was developed in [41] and an extension to include a photometric term is straight forward [23].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 21,
      "context" : "[22] the term JJ is the Fisher information matrix of the estimator.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "The variance of the estimate can be lower-bound by the Fisher information matrix using the Cramer-Rao bound [11].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 13,
      "context" : "[14], the proposed ICP variant selectively integrates informative observations which decreases the number of necessary observations in practice and thus speeds up camera tracking.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 44,
      "context" : "[45, 43, 44].",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 42,
      "context" : "[45, 43, 44].",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 43,
      "context" : "[45, 43, 44].",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 42,
      "context" : "Similar to [43], we capture the notion of the Stata Center World model, that the surfel surface normal distribution consists of some variable, unknown number of clusters by a Dirichlet process von-Mises-Fisher mixture model.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 35,
      "context" : "Following the proposal of [36], we impose spatial smoothness of the Stata Center World segmentation by assuming a Markov random field (MRF) over the segmentation z that encourages uniform labeling inside a set Ni of neighboring surfels of surfel i.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 34,
      "context" : "To define the base measure, we utilize the conjugate prior for the von-Mises-Fisher distribution which in general is only known up to proportionality [35]:",
      "startOffset" : 150,
      "endOffset" : 154
    }, {
      "referenceID" : 37,
      "context" : "The MRF potential Ψ ij(pi, ni, pj , nj , z) that encapsulates local planarity is obtained by symmetrizing the well known point-to-plane distance function used in implementations of ICP [38]:",
      "startOffset" : 185,
      "endOffset" : 189
    }, {
      "referenceID" : 31,
      "context" : "Associations between RGB-D observations and map surfels are established using projective data association [32].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 33,
      "context" : "The observation covariances Σp,j are computed according to a realistic depth camera noise model [34] and incorporate the linearized camera pose uncertainty:",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 0,
      "context" : "[1].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 33,
      "context" : "A more detailed model could be obtained with a controlled experiment similar to [34].",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 3,
      "context" : "This distribution has the form of a Bingham distribution [4].",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 46,
      "context" : "An efficient method for sampling from a von-Mises-Fisher distribution is outlined in [47].",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 4,
      "context" : "Sampling Directional Segmentation Labels zi We use the Chinese restaurant process (CRP) representation of the Dirichlet process [5, 31] since it lends itself to straightforward sampling-based inference.",
      "startOffset" : 128,
      "endOffset" : 135
    }, {
      "referenceID" : 30,
      "context" : "Sampling Directional Segmentation Labels zi We use the Chinese restaurant process (CRP) representation of the Dirichlet process [5, 31] since it lends itself to straightforward sampling-based inference.",
      "startOffset" : 128,
      "endOffset" : 135
    }, {
      "referenceID" : 41,
      "context" : "3 [42])",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 6,
      "context" : "Since the individual distributions are all Gaussian the posterior over surfel location pi is also Gaussian [7] with the following mean and variance:",
      "startOffset" : 107,
      "endOffset" : 110
    }, {
      "referenceID" : 8,
      "context" : "Via the law of large numbers and by the construction of the Gibbs sampler this approach will in the limit converge to the true means and variances [9].",
      "startOffset" : 147,
      "endOffset" : 150
    }, {
      "referenceID" : 26,
      "context" : "To be able to deal with fast motions we perform photometric rotational pre-alignment [27] from image pyramid level 3 down to 1.",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 14,
      "context" : "areas similar to [15].",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 18,
      "context" : "There exists only preliminary research on parallel Gibbs sampling under the name Hogwild Gibbs sampling [19] and it is unclear if there are theoretical guarantees.",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 45,
      "context" : "Figure 8: Thread timings (left) and surfel (middle) and sample count (right) statistics for the directional SLAM system running on the fr2 xyz dataset [46].",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 45,
      "context" : "Figure 9: Comparison of the absolute trajectory error (ATE) in meters as defined in [46] of different SLAM systems for different synthetic datasets from the benchmark dataset by Handa et al.",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 17,
      "context" : "[18] (kt0-2) and the TUM indoor dataset [46].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 45,
      "context" : "[18] (kt0-2) and the TUM indoor dataset [46].",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 45,
      "context" : "fr2 xyz dataset [46] displayed in Fig.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 45,
      "context" : "Camera Tracking Accuracy Comparison We use the TUM indoor dataset [46] and the synthetic dataset by Handa et al.",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 17,
      "context" : "[18] to evaluate the camera tracking accuracy against groundtruth via the absolute trajectory error (ATE) [46] and compare our system to related 3D SLAM systems in Fig.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 45,
      "context" : "[18] to evaluate the camera tracking accuracy against groundtruth via the absolute trajectory error (ATE) [46] and compare our system to related 3D SLAM systems in Fig.",
      "startOffset" : 106,
      "endOffset" : 110
    } ],
    "year" : 2017,
    "abstractText" : "To aide simultaneous localization and mapping (SLAM), future perception systems will incorporate forms of scene understanding. In a step towards fully integrated probabilistic geometric scene understanding, localization and mapping we propose the first direction-aware semi-dense SLAM system. It jointly infers the directional Stata Center World (SCW) segmentation and a surfel-based semi-dense map while performing real-time camera tracking. The joint SCW map model connects a scene-wide Bayesian nonparametric Dirichlet Process von-Mises-Fisher mixture model (DP-vMF) prior on surfel orientations with the local surfel locations via a conditional random field (CRF). Camera tracking leverages the SCW segmentation to improve efficiency via guided observation selection. Results demonstrate improved SLAM accuracy and tracking efficiency at state of the art performance. Future perception systems in applications such as autonomous cars, autonomous robots, or augmented reality will integrate scene understanding into the purely geometric localization and mapping task. This is likely to improve both simultaneous localization and mapping (SLAM), provide a basis for higher-level reasoning about the scene, and richer information for human operators. In current systems scene understanding is used in two ways: (1) to improve the operation of the 3D perception system and (2) to provide additional information for higher-level inference or a human operator. We argue that only systems in the first class actually “understand” aspects of the scene because they are able to use inferred concepts to improve on their other inferential tasks (i.e. localization and mapping). The number of systems that fall into this class is still small. In order to improve 3D reconstruction and localization via scene understanding most approaches rely on geometric scene priors such as planarity [10, 39, 20, 28], the Manhattan World [37] (MW) or the Stata Center World [6] (SCW) assumption. The assumption of planarity is a local assumption that cannot explain the connection between disparate scene parts like all the parallel planes in typical man-made environments. Such connections can be captured and explained by global assumptions such as the MW and the SCW assumption. Because the MW assumption is limited Figure 1: We propose the first direction-aware semi-dense SLAM system. Based on the Stata Center World assumption the system jointly infers a directional segmentation (right) and a semi-dense surfel-based map (left) in real-time. to very specific environments, we instead explore the flexible Stata Center World model to improve 3D reconstruction and camera tracking. As shown in [43, 44], the directional clustering of a scene’s surface normals under the Stata Center World implies a segmentation that captures scene-wide regularities of the environment [45] as can be seen in the segmentation in Fig. 1. Based on the SCW scene prior, we propose the first semi-dense nonparametric direction-aware SLAM system. It performs joint inference over the Bayesian nonparametric SCW scene segmentation and the world map using Gibbssampling without precluding real-time operation. To connect the scene-wide Stata Center World model with local surface properties we model the assumption that nearby areas in the same directional segment are likely planar using a CRF. We demonstrate experimentally that using the directional segmentation improves SLAM accuracy and camera tracking efficiency via guided observation selection.",
    "creator" : "LaTeX with hyperref package"
  }
}