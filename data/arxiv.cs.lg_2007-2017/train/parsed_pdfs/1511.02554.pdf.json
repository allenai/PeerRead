{
  "name" : "1511.02554.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Deep Recurrent Neural Networks for Sequential (1).pages",
    "authors" : [ "Farhad Pouladi", "Hojjat Salehinejad", "Amir Mohammad Gilani" ],
    "emails" : [ "pouladi@ictfaculty.ir", "hsalehin@uoguelph.ca", "am.gilani@mci.ir" ],
    "sections" : [ {
      "heading" : null,
      "text" : "I. INTRODUCTION The genome-wide association (GWA) studies have discovered many convincingly replicated associations for complex human diseases using high-throughput single-nucleotide olymorphism (SNP) genotypes [1], [2]. The genotype imputation has been used for fine-map associations and facilitates the combination of results across studies [1]. The issue of missing genotype data and its imputation implies creating individualistic genotype data [2]. Impact of even small amounts of missing data on a multi-SNP analysis is of great importance for the complex diseases research [2]. There are several programs such as BEAGLE [3], MaCH [4], and IMPUTE2 [5], which provide imputation capability of untyped variants. In the past few year, machine learning and optimization algorithms have been widely used to deal with problems in nature and technology [12], [15]. The evolutionary algorithms are efficient in solving high-dimensional, multi-modal problems [16]. Some examples are in localization [17], [20],navigation [21], and spectrum assignment [22] problems. The sparse partial least squares (SPLS) and least absolute shrinkage and selection operator (LASSO) methods are well-known for simultaneous dimension reduction and variable selection [6], [7]. The LASSO is a shrinkage and selection method for linear regression, which attempts to minimize an error function. This function is typically the sum of squared errors with a bound on the sum of the absolute values of the coefficients [7]. The partial least squares (PLS) regression is used as an alternative approach to the ordinary least squares (OLS) regression method [6]. The SPLS method is the sparse\nPage 1\nversion of PLS method, which simultaneously works to achieve good predictive performance and variable selection by producing sparse linear combinations of the original predictors [6].\nIn general, matrix factorization is a technique to decompose a matrix for multivariate data into two matrices with F latent features [10]. Many matrix factorization techniques have been proposed to increase its performance, such as non-negative [13], sparse [14], non-linear [9], and kernel-based approaches [8]. A kernel non-negative matrix factorization method is proposed for feature extraction and classification of microarray data in [8]. Performance evaluation of this method for eight different gene samples has showed better performance over linear as well as other well-known kernel-based matrix factorization approaches.\nA sparse matrix factorization method has been proposed for tumor classification using gene expression data, [14]. In this approach, the gens are selected using a sparse matrix factorization method and then the features are extracted to be fed into a support vector machine (SVM) for tumor samples classification. It is reported that the performance results have been improved versus the non-sparse matrix factorization techniques. The artificial neural network (ANN) is another successful machine learning approach for prediction and classification applications [11]. As an example, a feed-forward ANNs model and a Bayesian approach are utilized to impute missing genotype data of SNPs in [2]. Sequence modelling is one of the most areas in machine learning. This is due to the fact that a large class of phenomenal and data around us is made of sequences of data with particular patterns. Some examples are retail data, speed recognition, natural language modelling, music generation and genotype data for medical applications. With the great practical advances in deep learning, this state-of-the-art machine learning technique is the key for many problems in science and engineering. Recurrent neural network (RNN), due to its recurrent connections is considered as a subcategory of deep learning methods. This powerful model is capable of learning temporal patters and in sequential data. The power of RNN arises from its hidden state, which works as the “memory” of system to remember past important features for the future decision makings. The hidden state is consisted of highdimensional non-linear dynamics which enables modelling any phenomena, if trained well [31], [30].\nPage 2\nIn this paper, we are proposing a new system model for missing genotype imputation and phenotype prediction using matrix factorization and RNNs. In this model, a simple but efficient matrix factorization method is used for missing genotype imputation. Then, the imputed genotypes are used along the sequence of available phenotype data to train our RNN with the recently developed ReLU learning approach. In order to evaluate performance of the ReLU approach in learning longterm dependencies in phenotype data, it is compared with the LSTM-RNN and SRNN approaches.\nIn the next section, the data structure of the dataset used for the experiments is described. In Section III, the methodologies based on the matrix factorization technique and DRNN is discussed in detail. The experimental results as well are comparative analysis are provided in Section IV. Finally, the paper is concluded in Section V and some guidelines are further developments are provided."
    }, {
      "heading" : "II. DATA STRUCTURE",
      "text" : "For the experiments, we are using a set of data provided for our research by Afzalipour research hospital. The genotype data contains genotypes of 1980 SNPs for 604 observations and the phenotype data provides measurements of two phenotype, called trait 1 and trait 2. Out of 1980 SNPs provided in the genotype data, 5% contain missing genotypes. The percentage of observations with missing genotypes for each SNP varies from 1 to 25. For each trait, 30 randomly selected observations have missing values."
    }, {
      "heading" : "III. METHODOLOGY",
      "text" : "In order to deal with the missing genotype and phenotype problem, we are utilizing the matrix factorization (MF) and RNNs techniques to fit prediction models as in Figure 1. To do so, after data pre-processing, the genotype dataset with missing values is imported into the MF system to predict the missing genotype values. By having the estimated genotype dataset and\nPage 3\ncorresponding phenotypes, the RNN is utilized in a supervised manner to train a network model for prediction of phenotypes, based on the known genotype-phenotype pairs. Each stage is described in details in the following subsections."
    }, {
      "heading" : "A. Data Pre-processing",
      "text" : "In general, the SNP genotypes (AA, BB, AB, or Null) are denoted with integer numbers for computational purposes, however, some programs may be able to work with this AA/AB/BB format directly. The data pre-processing step is an opportunity to clean data, remove noise, and translate the genotype data and indicate/distinguish missing values from the available data."
    }, {
      "heading" : "B. Matrix Factorization Model",
      "text" : "The proposed MF structure for genotype data imputation is presented in Figure 2. In this model we consider U number of samples and V number of SNPs. Therefore, the genotype data is structured as a U X V matrix, called GU x V. MF technique is to estimate two matrices, PU x F and Q V x F with F latent features, such that their product G’ U x V estimates G U x V as :\nwhere each element of the genotype matrix G’ is computed by using the dot product such as :\nIn order to find the best values for the matrices P and Q, we need to minimize the objective function which describes the difference between the G and G0 genotype matrices [10]. To do so, the gradient descent algorithm is utilized as the optimizer in Figure 2 to update the feature matrices P and Q iteratively.\nThe above procedure is illustrated in pseudocode as in Algorithm 1. As it is demonstrated, the parameters are set in the initialization step and random values in range [a,b] are allocated to the feature matrices P and Q, [10]. Based on the availability of each genotype such as G u , v ≠ 0 for all { u .v } ϵ {{1,….,U} , {1, …,V}}, the estimated genotype matrix G u , v is computed. The objective function is then formulated with respect to P and Q as:\nPage 4\nx1 x2 xN\ny1 y1 yP h1 h2 hM ht+1\nt t+1\nh1 h2 hM x1 x2 xN\ny1 y1 yP\nh1 h2 hM\nt+2\nht\nWHH\nWHH WIH\nWHO\nWIH\nWHO\nTime\nx1 x2 xN\ny1 y1 yP\nh1 h2 hM\nht+1\nInput\nHidden\nOutput Layer\nLayer\nLayer\nt\nWIH\nWHO WHH\nwhere Forbenius norms of P and Q are used for regularization under control of parameter β to prevent over-fitting of model by penalizing it with extreme parameter values [10]. Normally β is set to some values in the range of 0.02, such that P and Q can approximate G without having to contain large numbers. The feature matrices of P and Q are updated as:\nand\nrespectively, where α represents the learning rate and is practically set to 0.0001."
    }, {
      "heading" : "C. Recurrent Neural Network with Rectified Linear Unit Model",
      "text" : "The utilized RNN in the proposed model in Figure 1 is consisted of input, hidden, and output layers, where each layer is consisted of corresponding units. The input layer is consisted of N input units, where its inputs are defined as a sequence of vectors through time t such as {…, Xt-1 , Xt ,Xt+1 , …} where Xt = (x1,x2,…,xn). In a fully connected RNN the inputs units are connected to hidden units in the hidden layer, where the connections are defined with a weight matrix W I H. The hidden layer is consisted of M hidden units ht = (h1,h2,…,h M), which are connected to each other through time with recurrent connections. As it is demonstrated in Figure 3b, the hidden units are initiated before feeding the inputs. The hidden layer structure defines the state space or “memory” of the system, defined as\nWhere f H(.) is hidden layer activation function and bh is the bias vector of the hidden units. The hidden units are connected to the output layer with weighted connections WH O.The output layer has P units such yt = (y1, y2 ,…, yp) which are estimated as\nPage 5\nwhere f O (.) is the output layer activations functions and bO is the bias vector. Learning long term dependencies in RNNs is difficult task [30]. This is due to two major problems which are vanishing gradients and exploding gradients. The long-shortterm memory (LSTM) method is one of the popular methods to overcome the vanishing radient problem. A recent proposed method suggests that proper initialization of the RNN weights with rectified linear units has good performance in modeling longrange dependencies [30]. In this approach, the model is trained by utilizing back-propagation through time (BPTT) technique to compute the derivatives of error with respect to the weights. The reported performance analysis show that this method has comparable results in comparision to the LSTM method, with much less complexity.\nIn this model, each new hidden state vector is inherited from the previous hidden vector by copying its values, adding the effects of inputs, and finally, replacing negative state values by zero. In other words, this means that the recurrent weight matrix is initialized to an identity matrix and the biases are set to zero. This procedure is in fact replacing the ”tanh” activation function (Figure 4) with a rectified linear unit (ReLU)(Figure 5). The ReLU in fact is modelling the behaviour of LSTM. In LSTM, the gates are set in a way that there is no decay to model long-term dependencies. In ReLU, when the error derivatives for the hidden units are back-propagated through time they remain constant provided no extra error-derivatives are added [30].\nPage 6"
    }, {
      "heading" : "IV. EXPERIMENTAL RESULTS",
      "text" : "The proposed model is implemented for parallel processing using Theano in Python [23], [18], [19]. In this section, we are presenting the performance result from comparison between simple RNN, LSTM, and ReLU training methods for phenotype sequence prediction. The ReLU method is then compared with the well-known sparse partial least square (SPLS) method."
    }, {
      "heading" : "A. Parameter Setting",
      "text" : "As it is described in Section II, it is assumed that the genotypes data is either missing or observed. Therefore, the observed genotypes are represented as Gu,v ∈ {0, 1, 2} and the missing (null) data is represented as Gu,v = 5 for the experiments [1]. The genotype dataset GU,V is consisted of V = 1980 SNPs for U = 604 observations. Parameter setting for all the experiments are presented in Table I adapted from the literature, [14], [19], [29], unless a change is mentioned. As recommended in the literature, the 10-fold cross-validation is used to tune the parameters eta and K for the SPLS algorithm using the SPLS package in R programming language [19], [20], [6]. The parameter tuning is conducted separately for each provided trait in the phenotype dataset for 1 ≤ K ≤ 10. The optimal values are provided in\nTable I.\nDue to small size of data samples, %80 of available genotype and phenotype data is used for training the ANN, %10 for validation, and %10 for testing. Regarding the SPLS algorithm, %80 of the provided data is considered as training data and %20 as test data."
    }, {
      "heading" : "B. Simulation Results Analysis",
      "text" : "In this subsection, performance results of the proposed method is presented and compared with the SPLS method for the described dataset in Section II. Performance of the methods is evaluated by measuring the correlation between the original genotype and phenotype values with the corresponding predicted values.\nThe average of cost for fitting the MF model for different epochs is presented in Figure 6. In order to have a deeper look in details, the figure is presented in linear and logarithmic scales. The measure to compute the average of cost is the mean-square error, which is the average of\nPage 7\nsquared difference between targets and the output of the MF model. As the results show, the more number of latent features F results in less average of cost. In addition, the model is\ntrying to fit better that before in each epoch, however, after epoch 110 the progress is not very significant. The performance results for the success percentile in missing genotype data and success percentile in the whole genotype data construction is provided in Table II. There is a trade-off between the number of features and performance. This is due to the fact that increasing the number of features increases the computational complexity.\nPage 8\nPage 9\nIn Table III, performance of the ReLU-RNN for the genotype prediction is presented. As it is showed, the ReLU method has better training performance comparing to the results of the SPLS method in Table IV. The missing genotype represents the error for the test dataset in percentile while the original genotype represents the training error values in percentile. Since the training dataset has been seen by the model during training, it is reasonable to see that performance of the methods for the training dataset is better than the unseen test data.\nIn Figure 7, the training error of the SRNN, LSTM- RNN, and ReLU-RNN algorithms are compared. For better illustration, the first 100 training epochs are presented. As the results show, the SRNN algorithm has more training error than the LSTM-RNN method. This is while the ReLU-RNN approach has the least training error comparing to LSTM- RNN. At the early epochs, we see that the LSTM and ReLU are almost at the same training loss, however, the ReLU achieves less error in further epochs."
    }, {
      "heading" : "V. CONCLUSION AND FUTURE WORKS",
      "text" : "In this paper, a novel model is proposed which utilizes matrix factorization and deep recurrent neural networks (DRNN) for genotype imputation and phenotype sequences prediction. Sine we are interested in keeping track of sequences with long- term dependencies in genomics, the state-of-the-art recited linear unit learning method is used.\nThe performance results show the with the ReLU methods has a better performance in training comparing to the LSTM-RNN and simple RNN methods. The ReLU learning methods also has less computational complexity comparing to the LSTM method. For future research, it is interesting to analyze other recent advances in deep learning for genotype- phenotype application; particularly that these algorithms are moving toward more simple designs which is suitable for big data application."
    } ],
    "references" : [ {
      "title" : "Genotype imputation for genome-wide association studies",
      "author" : [ "J. Marchini", "B. Howie" ],
      "venue" : "Nature Reviews Genetics, vol. 11, pp. 499-511, 2010.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Kardia1, “Imputing missing genotypic data of single-nucleotide polymorphisms using neural networks,",
      "author" : [ "Y.V. Sun", "S. LR" ],
      "venue" : "European Journal of Human Genetics,",
      "citeRegEx" : "Sun and LR.,? \\Q2008\\E",
      "shortCiteRegEx" : "Sun and LR.",
      "year" : 2008
    }, {
      "title" : "Rapid and accurate haplotype phasing and missing-data inference for whole-genome association studies by use of localized haplotype clustering,",
      "author" : [ "SR. Browning", "BL. Browning" ],
      "venue" : "American journal of human genetics,",
      "citeRegEx" : "Browning and Browning,? \\Q2007\\E",
      "shortCiteRegEx" : "Browning and Browning",
      "year" : 2007
    }, {
      "title" : "MaCH: using sequence and genotype data to estimate haplotypes and unobserved genotypes,",
      "author" : [ "Y. Li", "CJ. Willer", "J. Ding", "P. Scheet", "GR. Abecasis" ],
      "venue" : "Genetic Epidemiology,",
      "citeRegEx" : "Li et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2010
    }, {
      "title" : "A flexible and accurate genotype imputation method for the next generation of genome-wide association studies,“ PLoS genetics",
      "author" : [ "BN. Howie", "P. Donnelly", "J. Marchini" ],
      "venue" : null,
      "citeRegEx" : "Howie et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Howie et al\\.",
      "year" : 2009
    }, {
      "title" : "Sparse partial least squares regression for simultaneous dimension reduction and variable selection,",
      "author" : [ "H. Chun", "S. Kele" ],
      "venue" : "J R Stat Soc Series B Stat Methodol,",
      "citeRegEx" : "Chun and Kele,? \\Q2010\\E",
      "shortCiteRegEx" : "Chun and Kele",
      "year" : 2010
    }, {
      "title" : "Regression shrinkage and selection via the lasso,",
      "author" : [ "R. Tibshirani" ],
      "venue" : "J. Royal. Statist. Soc B.,",
      "citeRegEx" : "Tibshirani,? \\Q1996\\E",
      "shortCiteRegEx" : "Tibshirani",
      "year" : 1996
    }, {
      "title" : "A New Kernel Non-Negative Matrix Factor- ization and Its Application in Microarray Data Analysis,",
      "author" : [ "Y. Li", "A. Ngom" ],
      "venue" : "in Proc. IEEE Symposium on Computational Intelligence in Bioinformatics and Computational Biology,",
      "citeRegEx" : "Li and Ngom,? \\Q2012\\E",
      "shortCiteRegEx" : "Li and Ngom",
      "year" : 2012
    }, {
      "title" : "Nonlinear nonnegative matrix factorization based on Mercer kernel construction,",
      "author" : [ "P. Binbin", "L. Jianhuang", "C. Wen-Sheng" ],
      "venue" : "Pattern Recognition,",
      "citeRegEx" : "Binbin et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Binbin et al\\.",
      "year" : 2011
    }, {
      "title" : "Dictionary IdentificationSparse Matrix- Factorization via l1-Minimization,",
      "author" : [ "R. Gribonval", "K. Schnass" ],
      "venue" : "IEEE Transactions on Information Theory, vol. 56,",
      "citeRegEx" : "Gribonval and Schnass,? \\Q2010\\E",
      "shortCiteRegEx" : "Gribonval and Schnass",
      "year" : 2010
    }, {
      "title" : "A Pistachio Nuts Classification Technique: An ANN Based Signal Processing Scheme,",
      "author" : [ "S. Mahdavi-Jafari", "H. Salehinejad", "S. Talebi" ],
      "venue" : "Computational Intelligence for Modelling Control & Automation, International Conference on,",
      "citeRegEx" : "Mahdavi.Jafari et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Mahdavi.Jafari et al\\.",
      "year" : 2008
    }, {
      "title" : "Micro-Differential Evolution with Vectorized Random Mutation Fac- tor,",
      "author" : [ "H. Salehinejad", "S. Rahnamayan", "H.R. Tizhoosh", "S.Y. Chen" ],
      "venue" : "in Proc. IEEE Congress on Evolutionary Computation,",
      "citeRegEx" : "Salehinejad et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Salehinejad et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning the parts of objects by non-negative matrix factorization,",
      "author" : [ "D.D. Lee", "S. Seung" ],
      "venue" : "Nature,",
      "citeRegEx" : "Lee and Seung,? \\Q1999\\E",
      "shortCiteRegEx" : "Lee and Seung",
      "year" : 1999
    }, {
      "title" : "Tumor Classification Based on Non-Negative Matrix Factorization Using Gene Expression Data,",
      "author" : [ "C. Zheng", "et. al" ],
      "venue" : "IEEE Transactions on NanoBioscience,",
      "citeRegEx" : "Zheng and al.,? \\Q2011\\E",
      "shortCiteRegEx" : "Zheng and al.",
      "year" : 2011
    }, {
      "title" : "Type-II Opposition-based Differential Evolutions,",
      "author" : [ "H. Salehinejad", "S. Rahnamayan", "H.R. Tizhoosh" ],
      "venue" : "in Proc. IEEE Congress on Evolutionary Computation,",
      "citeRegEx" : "Salehinejad et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Salehinejad et al\\.",
      "year" : 2014
    }, {
      "title" : "Micro-Differential Evolution: Diversity Enhancement and Comparative Study,",
      "author" : [ "H. Salehinejad" ],
      "venue" : "University of Ontario Institute of Technology,",
      "citeRegEx" : "Salehinejad,? \\Q2014\\E",
      "shortCiteRegEx" : "Salehinejad",
      "year" : 2014
    }, {
      "title" : "3D localization in large-scale Wireless Sensor Networks: A microdifferential evolution approach,” in Personal, Indoor, and Mobile Radio Communication (PIMRC), 2014",
      "author" : [ "H. Salehinejad", "R. Zadeh", "R. Liscano", "S. Rahnamayan" ],
      "venue" : "IEEE 25th Annual International Symposium on, vol.,",
      "citeRegEx" : "Salehinejad et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Salehinejad et al\\.",
      "year" : 2014
    }, {
      "title" : "SciPy: Open Source Scientific Tools for Python,",
      "author" : [ "E. Jones", "E. Oliphant", "P. Peterson", "et. al" ],
      "venue" : null,
      "citeRegEx" : "Jones et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Jones et al\\.",
      "year" : 2001
    }, {
      "title" : "Theano: A CPU and GPU Math Expression Compiler,“ in Proceedings of the Python for Scientific Computing",
      "author" : [ "J. Bergstra", "O. Breuleux", "F. Bastien", "et. al" ],
      "venue" : null,
      "citeRegEx" : "Bergstra et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Bergstra et al\\.",
      "year" : 2010
    }, {
      "title" : "Optimum Localization of Wind Turbine Sites Using Opposition Based Ant Colony Optimization,",
      "author" : [ "F. Pouladi", "A.M. Gilani", "B. Nikpour", "H. Salehinejad" ],
      "venue" : "in Developments in eSystems Engineering (DeSE),",
      "citeRegEx" : "Pouladi et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Pouladi et al\\.",
      "year" : 2013
    }, {
      "title" : "Dynamic Fuzzy Logic-Ant Colony System-Based Route Selection System,",
      "author" : [ "H. Salehinejad", "S. Talebi" ],
      "venue" : "Applied Computational Intelligence and Soft Computing,",
      "citeRegEx" : "Salehinejad and Talebi,? \\Q2010\\E",
      "shortCiteRegEx" : "Salehinejad and Talebi",
      "year" : 2010
    }, {
      "title" : "Cognitive radio networks spectrum allocation: An ACS perspective",
      "author" : [ "F. Koroupi", "S. Talebi", "H. Salehinejad" ],
      "venue" : "Scientia Iranica, vol. 19, Issue 3, pp. 767-773, 2012.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "An Algorithm for Least-Squares Estimation of Non- linear Parameters,",
      "author" : [ "D. Marquardt" ],
      "venue" : "SIAM Journal on Applied Mathematics,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 1963
    }, {
      "title" : "The Levenberg-Marquardt Algorithm,",
      "author" : [ "A. Ranganathan" ],
      "venue" : null,
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2004
    }, {
      "title" : "An Introduction to the spls Package, Version 1.0",
      "author" : [ "D. Chung", "et. al." ],
      "venue" : "June 10, 2012.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Package spls",
      "author" : [ "D. Chung", "et. al." ],
      "venue" : "R programming language, February 20, 2015.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Theano: new features and speed improvements",
      "author" : [ "F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I. Goodfellow", "A. Bergeron", "N. Bouchard", "D. Warde-Farley", "Y. Bengio" ],
      "venue" : "NIPS 2012 deep learning workshop",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2012
    }, {
      "title" : "A Simple Way to Initialize Recurrent Networks of Rectified Linear Units,",
      "author" : [ "Le", "Q. V", "N. Jaitly", "G.E. Hinton Google" ],
      "venue" : null,
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2015
    }, {
      "title" : "Modeling Temporal Dependencies in High-Dimensional Sequences: Application to Polyphonic Music Generation and Transcription,",
      "author" : [ "N. Boulanger-Lewandowski", "Y. Bengio", "P. Vincent" ],
      "venue" : "Appearing in Proc. ICML,",
      "citeRegEx" : "Boulanger.Lewandowski et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Boulanger.Lewandowski et al\\.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "The genome-wide association (GWA) studies have discovered many convincingly replicated associations for complex human diseases using high-throughput single-nucleotide olymorphism (SNP) genotypes [1], [2].",
      "startOffset" : 195,
      "endOffset" : 198
    }, {
      "referenceID" : 0,
      "context" : "The genotype imputation has been used for fine-map associations and facilitates the combination of results across studies [1].",
      "startOffset" : 122,
      "endOffset" : 125
    }, {
      "referenceID" : 21,
      "context" : "Some examples are in localization [17], [20],navigation [21], and spectrum assignment [22] problems.",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 27,
      "context" : "The hidden state is consisted of highdimensional non-linear dynamics which enables modelling any phenomena, if trained well [31], [30].",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 27,
      "context" : "Learning long term dependencies in RNNs is difficult task [30].",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 27,
      "context" : "A recent proposed method suggests that proper initialization of the RNN weights with rectified linear units has good performance in modeling longrange dependencies [30].",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 27,
      "context" : "In ReLU, when the error derivatives for the hidden units are back-propagated through time they remain constant provided no extra error-derivatives are added [30].",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 0,
      "context" : "Therefore, the observed genotypes are represented as Gu,v ∈ {0, 1, 2} and the missing (null) data is represented as Gu,v = 5 for the experiments [1].",
      "startOffset" : 145,
      "endOffset" : 148
    }, {
      "referenceID" : 26,
      "context" : "Parameter setting for all the experiments are presented in Table I adapted from the literature, [14], [19], [29], unless a change is mentioned.",
      "startOffset" : 108,
      "endOffset" : 112
    } ],
    "year" : 2015,
    "abstractText" : "In analyzing of modern biological data, we are often dealing with ill-posed problems and missing data, mostly due to high dimensionality and multicollinearity of the dataset. In this paper, we have proposed a system based on matrix factorization (MF) and deep recurrent neural networks (DRNNs) for genotype imputation and phenotype sequences prediction. In order to model the long-term dependencies of phenotype data, the new Recurrent Linear Units (ReLU) learning strategy is utilized for the first time. The proposed model is implemented for parallel processing on central processing units (CPUs) and graphic processing units (GPUs). Performance of the proposed model is compared with other training algorithms for learning long-term dependencies as well as the sparse partial least square (SPLS) method on a set of genotype and phenotype data with 604 samples, 1980 single-nucleotide polymorphisms (SNPs), and two traits. The results demonstrate performance of the ReLU training algorithm in learning long-term dependencies in RNNs.",
    "creator" : "Pages"
  }
}