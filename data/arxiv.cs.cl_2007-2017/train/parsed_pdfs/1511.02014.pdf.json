{
  "name" : "1511.02014.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Population size predicts lexical diversity, but so does the mean sea level – one problem in the analysis of temporal data",
    "authors" : [ "Alexander Koplenig", "Carolin Müller-Spitzer" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "We document an apparently spectacular relationship between population size and lexical diversity: for five out of seven investigated languages, there is a strong relationship between population size and lexical diversity of the primary language in this country. We show that this relationship is the result of a misspecified model that does not consider the temporal aspect of the data by presenting a similar but nonsensical relationship between the global annual mean sea level and lexical diversity. Given the fact that in the recent past, several studies were published that seem to suffer from this problem [12– 18], we explain the cause of the misspecification and show that it has profound consequences. We demonstrate how simple transformation of the time series can often solve problems of this type and argue that the evaluation of the plausibility of a relationship is important in this context. Following [19], we hope that our paper will help both researchers and reviewers to understand why it is important to use special models for the analysis of data with a natural temporal ordering."
    }, {
      "heading" : "Introduction",
      "text" : "In principle, we could start our paper like this: As complex adaptive systems [1], languages are constantly changing on all fundamental levels [2]. In this context, several studies have indicated that population size is an important factor that influences the (rate of) language change [3–5]. Based on a very large sample of the written language record [6], we present quantitative evidence that clearly supports the claim that population size strongly influences the lexical diversity of eight languages. The problem is: an almost identical pattern emerges if we correlate the lexical diversity in a given year with the global mean sea level [7] instead of population size. This indicates that there seems to be something wrong with our analysis. While we want to point out upfront that the problem we describe is fairly common knowledge in econometric time-series analysis [8–11], we nevertheless felt the need to outline it once again in this paper, given the fact that in the recent past, several studies were published– some in journals with a good reputation – that seem to suffer from exactly this problem [12–18]. Following [19], we consider this problematic, because the studies sometimes present surprising or even spectacular relationships, especially between different (socio-)demographical variables on the one hand and cultural or linguistic characteristics (like the one described above) on the other hand. As a result, the studies also receive widespread media attention and can, in turn, affect public opinion and decision making. We therefore hope that our paper can help both researchers and reviewers detect the problem and ultimately avoid it.\nThe remainder of this paper is organized in the following way: First, we document an apparently spectacular relationship between population size and lexical diversity. We then try to show that this relationship is the result of a misspecified model that does not consider the temporal aspect of the data by presenting a similar but nonsensical relationship between the global annual mean sea level and lexical diversity. On this basis, we\nexplain the cause of the misspecification, show that it has profound consequences and demonstrate how simple data transformation can often solve the problem, before arguing that checking for plausibility is important in this context. This paper ends with some concluding remarks."
    }, {
      "heading" : "Results and Discussion",
      "text" : "The problem of spurious correlations\nWe ‘investigated’ the correlation between population size and lexical diversity for American English, British English, Chinese (simplified), French, German, Italian, Russian and Spanish on the basis of data on population size and the type-token ratio based on the Google Books dataset (see Materials and Methods for more details). At first glance, Figure 1seems to document a spectacular and fascinating relationship: There is a strong and statistically significant correlation (at p < .05 or better) between the level of the population size and the level of the type-token ratio as a measure of lexical diversity for all investigated languages, except Chinese. On this basis, we could argue for a general relationship. For example, we assume that with increasing populations, the number of language speakers naturally also increases. We could then continue and elaborate our argument by assuming that a larger number of speakers is most likely associated with a greater degree of variance in demographic background and sociocultural environments [26], and that this greater diversity leads to an increase of the type-token ratio as a measure of lexical diversity. The fact that there is virtually no correlation for Chinese and – compared to the other languages – only a small correlation for the Russian data could then be incorporated in our \"theory\" by referring to the political background in those two countries, e.g. that in socialist countries, increasing population sizes do not\naffect the lexical diversity, because socialist coercion policies suppresses linguistic development by defining one common \"linguistic\" or \"cultural\" standard or something like that. This is exactly what [19] alerted to, since this result could be used in the media or even by politicians to criticize socialism.\nFigure 2 demonstrates why the analysis presented in Figure 1 is severely flawed: Compared to the cross-linguistic relationship between population size and lexical diversity, which it would be possible to argue for, the relationship between the global mean sea level and the lexical diversity is highly similar. In this context, all correlations except for Chinese and Russian are strong and highly significant (at p < .001 or better). However, arguing for any potential relationship seems to be out of the question in this context.\npopulation size. Notes on the bottom right: Pearson correlation (all ps < .001 except for Chinese were p = 0.95 and Russian where p = .10)\nFigure3 \"explains\" these apparent relationships: all type-token ratio time-series, again except for Chinese and Russian, clearly exhibit an upward trend. The series are said to have a unit root or to be non-stationary [11]. The same is true for both the population sizes and the global mean sea level, which also increased throughout the 20 th century. For the analysis of temporal data, this has important ramifications because the following statement is true per definition: values that are later in time will be above the average of the mean value of the series, while values that are earlier in time will be below average. Since the Pearson product-moment correlation measures whether values of one series that are above/below average tend to co-occur with values of another series that are above/below average, by mathematical necessity, the correlation coefficient for two trending time-series will then be high when in fact they are not related in any substantial sense [8]. An augmented Dickey-Fuller test (a formal test for a unit root) with a lag length of 1 reveals that all time series that include the population sizes and the global mean sea level are non-stationary (all ps > .05), except for the Chinese and the Russian type-token ration series where p < .05.\nChanges instead of levels\nOne common approach to avoid spurious correlations is to transform the series prior to the analysis, for example by detrending the series (estimating the trend and subtracting it from the actual series). Another more general solution that often results in stationary series, that is a series in which the mean and the variance of the investigated series do not change as a function of time, is to correlate period-to-period changes instead of the\nactual levels of the two series [11]. It is worth emphasizing that this also re-formulates the research question as it excludes the upward trends of the correlated series [27]. It determines if period-to-period changes that are above or below the average of the first series correspond mainly to changes that are above or below the average of the second series. Therefore, [8] suggest as a rule of thumb to generally model data on a combination of both levels and changes. In our case, correlating year-to-year changes (or decade-to-decade changes) seems to be even better suited to answer our \"research question\": if the population increases from last year to this year, then – on average –lexical diversity should also increase from last year to this year, if both series are related. If we correlate year-to-year changes all correlations between population size / global mean sea level and lexical diversity become virtually nonexistent (rmax = .10) and insignificant at all common levels of significance (pmin = .31).\nThe problem of temporal autocorrelation\nTo demonstrate why it is problematic to correlate two trending time-series, we have simulated 10,000 random walks with drift (cf. Materials and Methods). Each resulting time-series has an average upward trend, but otherwise behaves in a completely random manner. This means that the random walks serve as a proxy for time series with a general upward trend. All series are then correlated with the annual global mean sea level. Figure 4 shows that 7,519 of the 10,000 random walks correlate moderately (r > .30) and more than 50% (5,432) even correlate strongly (r > .75) with the global mean sea level, if we calculate the correlation based on the level of the series (rmean= .52). This result is, of course, far from what we should actually expect for the distribution of correlation coefficients where one variable is a random quantity per definition: only a few series should – by chance – substantially correlate with the global mean sea level, while most correlation coefficients should be close to zero. If we instead correlate year-to-year\nchanges of the two series, the maximum correlation coefficients amounts to r = .37 and – as expected – a distribution that closely resembles a normal distribution (blue line in Figure 4) with a mean close to zero (rmean= .00).\nThis leaves little room for debate: Whenever two variables evolve through time, those variables will almost always look highly correlated even if they are not related in any\nsubstantial sense. The reason why standard statistical models fail when it comes to the analysis of time-series has to do with the fact that there is basically no such thing as a univariate time-series: analyzing univariate time series is always \"the analysis of the bivariate relationship between the variable of interest and time.\" [11, p. 92]. In the Materials and Methods section, we demonstrate why temporal autocorrelation is problematic from a statistical point of view, if regular models for cross-sectional data (that assume independence between individual observations) are used.\nPlausibility\nTo demonstrate why it is also important – especially for a spectacular and unexpected result – to remain skeptical and to carefully check plausibility, let us briefly give an example what our initial “analysis” of the relationship between lexical diversity and population size would actually imply: f we regress the level of the lexical diversity in the Spanish Google Books data on the population size of Spain, we obtain a coefficient of determination of r 2 = .69. This means that almost 70% of the variance of the lexical diversity variable is “explained” by the population size (for the American English data it would be even more than 95% of the variance). This model would also imply that every 10 new inhabitants of Spain are equal to 4.56 additional word types (per 1 million word tokens) in the Spanish Google Books data (that also includes books written and published in Latin America). We believe that this would be an extraordinary result. In fact, this result would be so extraordinary that it seems wise to first ask: is this result plausible? Can we come up with any good theory regarding this relationship?\nA few words on the Google Books data are in order here, as they are the basis of all but one [16] study quoted above. Here, we want to echo [31, p. 1203]: just because a datasets is big, does not mean that “one can ignore foundational issues of measurement and construct validity and reliability and dependencies among data.” However, this\nseems to be precisely the case regarding the Google Books Ngram data. After all, for ngrams where n is ranges from one (single words) to five (five word units), the data consist of only year-wise aggregated overall frequencies of occurrence and the number of books each n-gram appears in. Since n-grams do not occur independently across distinct books, this aggregation of individual book frequencies means that we cannot account for the distributions of n-grams which can have profound consequences for the analysis of textual data [32,33]. In addition, it is a largely overlooked fact that the Google Books Ngram data only includes the counts for n-grams that occur at least 40 times across the entire corpus. At least from a (corpus) linguistic point of view, this certainly matters since most n-grams are very infrequent. So in terms of what we know about word frequency distributions [34], this procedure eliminates approximately 95% of all different 1-gram types; for n-grams where n>1 this figure is even higher [35]. To the best of our knowledge, the question of whether this arbitrary data truncation does not impose a systematic bias on the data is something that remains to be demonstrated empirically, given the fact that corpus size for each year strongly increases as a function of time. At the same time, this means that we would have to further extend our analysis illustrated above: nearly every second new inhabitant of Spain is \"responsible\" for one new word type that occurs more than 40 times in the Spanish Google Books data.\nTo check the plausibility of this result, we would have to face the fact that we still do not have any reliable information about the books included in the corpora. According to the FAQs of the Culturomics project behind the GB data [22] the team has “not received permission […] yet” to release the full 5.2 million book bibliography, containing information about each book included in the corpus for each language. This statement has not changed in the last few years and we are rather pessimistic that it will change anytime soon; but we would love to be proven wrong. This, in turn, means that we currently have no way of finding out whether the different diachronic book samples really repre-\nsent similar things at different moments in time and, as the Culturomics team themselves pointed out [36, p. 13], it is very likely that the types of books that are published are changing as a function of time. The lack of metadata can have important ramifications for any interpretation of potential results based on the Google Books Ngram data [37,38]. So at the moment, all we can be sure about, again according to the aforementioned FAQs, is that “the vast majority of the books from 1800-2000 come from Google's library partners, and so the composition of the corpus reflects the kind of books that libraries tend to acquire.”\nReturning to our research question – the correlation between population size and lexical diversity – population growth is affected by the birth of children and the influx of immigrants. Babies do not write books, and only a few immigrants publish books which are acquired by libraries shortly after immigration. So, the strong relationship between lexical diversity and population size would indicate that nearly every second new inhabitant (babies and immigrants alike) is \"responsible\" for one new word type that occurs more than 40 times in the Spanish Google Books data. This would really be an extraordinary relationship.\nTo drive home this point, if we regress the level of lexical diversity in the German Google Books data to the population size of China, we obtain a very strong correlation of r = .89 that is significant at all standard levels. Our model predicts that with every 1,000 new inhabitants of China, we will find roughly 15 additional word types in the German Google Books data. While it is certainly possible to “rationalize nearly everything” [9, p. 2], we just do not think that this result makes any sense – which mechanism could generate a relationship like this? If we use year-to-year changes instead of the actual levels, we obtain an insignificant correlation of 0.10 (p = 0.34). This implies\nthat knowing the Chinese population size does not help in predicting the lexical diversity in the Google Books data, a result which we believe fits reality more closely.\nFrom a statistical point of view, this demonstrates why it can be a good idea to model a potential relationship between two trending time series with changes instead of levels. This is also important from a methodological point of view: just because two series are trending, does not necessarily imply any substantial relationship [30]. Therefore we strongly advise against using the fact that two series are evolving in a predicted way as evidence in order to substantiate a specific theoretical claim.\nThe general question concerning the Google Books data itself, whether the acquisition strategy of major libraries really can serve as an (temporarily) unbiased proxy for the evolution of subjective or even latent cognitive traits, is an open research question. Again, we are rather skeptical. For example, a change in the acquisition strategy of one major library is not necessarily motivated by one of the factors we might be interested in; nevertheless in aggregation of the frequency counts of different n-grams, it might look like one. Once again, we want to refer to [31, p. 1203] regarding such naïve mapping:\n“All empirical research stands on a foundation of measurement. Is the instrumentation actually capturing the theoretical construct of interest? Is measurement stable and comparable across cases and over time? Are measurement errors systematic?”\nThe outlined problems all have to do with the fact that – in making the data freely available (which is a fantastic thing) – Google wanted to avoid breaking any copyright laws, and it goes without saying that legal restrictions also have to be taken seriously in this case. However, while we are – as many other empirically-minded researchers – fascinated by the possibilities that the analysis of “big data” offers, we believe that the\nseemingly prevailing view that the size of the (Google Books Ngram) data will stand in for fundamental methodological problems, is not justified."
    }, {
      "heading" : "Concluding remarks",
      "text" : "All recently published studies that we mentioned in the introduction do not explicitly model the underlying temporal structure of the data [12–18]. This certainly has to do with the fact that time series analysis is a relatively young statistical discipline [11, p. xxi-xxii]. We hope that this paper will help both researchers and reviewers understand why it is important to use special models for the analysis of such data. Standard statistical models that work for cross-sectional data run the risk of incorrect statistical inference in time-series analysis, where (potentially strong) effects are meaningless and therefore can potentially lead to wrong conclusions."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank Sascha Wolfer for valuable comments on earlier drafts of this article and Sarah Signer for proofreading."
    }, {
      "heading" : "Materials and Methods",
      "text" : "S1 contains the population data, compiled from [20]. The time-series of the global mean sea level was presented in [7] and is available at [21]. The type-token ratio, a common way to measure lexical diversity, is based on the Google Books datasets that were made available by [6] at [22]. For the present study, the 1-gram datasets of Version 2 (July 2012) of the following languages were used: American English, British English, Chi-\nnese (simplified), French, German, Italian, Russian and Spanish. The type-token ratio for each year and each language is calculated by dividing the number of unique strings by the total number of strings. Higher type-token ratios are indicative of higher lexical diversity. Since this measure is known to be heavily text-length dependent [23] and given the fact that the corpus size based on the Google Books data strongly increases as a function of time, calculating the type-token ratio based on the actual corpus sizes would systematically bias the results. To solve this problem, random samples of 1,000,000 tokens were drawn from the data as described in [24]. The analysis is restricted to the 20 th century, execpt for Chinese, which is restricted to the time span 1950-2000 since the size of the Google Books base corpora is not sufficient (< 1,000,000 tokens) for earlier periods.\nAdditionally, we simulated i = 10,000 random walks with drift [25,11] that are defined as:\nxt,i = di + xt-1,i + et,i\nwhere xt is the value of the ith random walk at time point t; the constant drift term di is randomly drawn from a uniformly distributed interval [0.02,0.2) and et is white noise, normally distributed over the interval [0,1).\nFor each resulting series this means that the current value of the series depends on its previous value plus a positive drift term and a white noise error term. At each point in time, the series takes one random step away from the last position, but as result of the drift term, the series will have an upward trend in the long-run.\nAll analyses were carried out using Stata/MP2 14.0 for Windows (64-bit version). To ensure maximal replicability, S2 contains a Stata script (‘do-file’) that automatically downloads the data and reproduces all results presented in this article, while S3 is a de-\nlimited text file (comma-separated) of the final dataset that can be used to replicate our findings with another software package.\nTemporal autocorrelation\nFrom a statistical point of view, temporal autocorrelation is problematic because it biases our estimators. If, for example, we fit a simple time-series regression that can be written as:\nyt = β0 + β1x1t + εt\nwhere yt represents the level of our outcome variable in t and x1t is the level of predictor variable, β0 is the regression constant and β1 is the regression coefficient, εt is the error term. OLS analysis assumes that there is no autocorrelation between the residuals (\uD835\uDC36\uD835\uDC5C\uD835\uDC63(\uD835\uDF00\uD835\uDC60, \uD835\uDF00\uD835\uDC61) = 0 for all \uD835\uDC60 ≠ \uD835\uDC61). In this context, first-order autocorrelation εt can be written as:\nεt = ρεt-1 + ηt\nwhere ρ is the autoregressive parameter and ηt is a white-noise process. In the presence of first-order autocorrelation, the OLS estimators are biased and lead to incorrect statistical inferences [28]. To see, why this is also the problem of our simulation, Figure 5 shows the correlation between current and lagged residuals of an OLS regression for each of the simulated random walks with drift on the annual global mean seal level. While the regression residuals of levels are strongly autocorrelated (rmean= .91), we obtain a normal distribution with a mean close to zero (rmean= -.02) for the regression residuals of year-to-year changes."
    } ],
    "references" : [ {
      "title" : "Politics and the German language: Testing Orwell’s hypothesis using the Google N-Gram corpus. Digit Scholarsh Humanit",
      "author" : [ "P. Caruana-Galizia" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2015
    }, {
      "title" : "Recent evolution of learnability in American English from 1800",
      "author" : [ "Hills TT", "Adelman JS" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2000
    }, {
      "title" : "Historical Analysis of National Subjective Wellbeing Using Millions of Digitized Books [Internet",
      "author" : [ "T Hills", "E Protio", "D. Sgroi" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2015
    }, {
      "title" : "A decline in prosocial language helps explain public disapproval of the US Congress",
      "author" : [ "JA Frimer", "K Aquino", "JE Gebauer", "(Lei) Zhu L", "H. Oakes" ],
      "venue" : "Proc Natl Acad Sci",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2015
    }, {
      "title" : "Male and Female Pronoun Use in U.S. Books Reflects Women’s Status, 1900–2008",
      "author" : [ "JM Twenge", "WK Campbell", "B. Gentile" ],
      "venue" : "Sex Roles",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2012
    }, {
      "title" : "Cultural evolution over the last 40 years in China: Using the Google Ngram Viewer to study implications of social and political change for cultural values: CULTURAL EVOLUTION IN CHINA",
      "author" : [ "Zeng R", "Greenfield PM" ],
      "venue" : "Int J Psychol",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2015
    }, {
      "title" : "Linguistic Diversity and Traffic Accidents: Lessons from Statistical Studies of Cultural Traits",
      "author" : [ "S Roberts", "J. Winters" ],
      "venue" : "Emmert-Streib F, editor. PLoS ONE. 2013;8:",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2013
    }, {
      "title" : "Language Is a Complex Adaptive System: Position Paper",
      "author" : [ "C Beckner", "R Blythe", "J Bybee", "MH Christiansen", "W Croft", "NC Ellis" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2009
    }, {
      "title" : "Principles of linguistic change",
      "author" : [ "W. Labov" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1994
    }, {
      "title" : "Rate of language evolution is affected by population size",
      "author" : [ "L Bromham", "X Hua", "TG Fitzpatrick", "SJ. Greenhill" ],
      "venue" : "Proc Natl Acad Sci",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1419
    }, {
      "title" : "Is the rate of linguistic change constant? Lingua",
      "author" : [ "D. Nettle" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1999
    }, {
      "title" : "Population Size and Rates of Language Change",
      "author" : [ "Wichmann S", "Holman EW" ],
      "venue" : "Hum Biol. 2009;81",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2009
    }, {
      "title" : "The Google Books Team, et al. Quantitative Analysis of Culture",
      "author" : [ "J-B Michel", "YK Shen", "AP Aiden", "A Verses", "MK Gray" ],
      "venue" : "Using Millions of Digitized Books. Science",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1199
    }, {
      "title" : "Probabilistic reanalysis of twentiethcentury sea-level rise",
      "author" : [ "CC Hay", "E Morrow", "RE Kopp", "JX. Mitrovica" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2015
    }, {
      "title" : "Spurious regressions in econometrics",
      "author" : [ "CWJ Granger", "P. Newbold" ],
      "venue" : "J Econom",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1974
    }, {
      "title" : "Why do we sometimes get nonsense correlations between time series? A study in sampling and the nature of time series",
      "author" : [ "Yule GU" ],
      "venue" : "J R Stat Soc",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1926
    }, {
      "title" : "The analysis of time series: an introduction",
      "author" : [ "C. Chatfield" ],
      "venue" : "6th ed. Boca Raton, FL: Chapman ",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2004
    }, {
      "title" : "Introduction to time series using Stata",
      "author" : [ "S. Becketti" ],
      "venue" : "1st ed. College Station, Tex: Stata Press;",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2013
    }, {
      "title" : "Speaker Input Variability Does Not Explain Why Larger Populations Have Simpler Languages",
      "author" : [ "M Atkinson", "S Kirby", "K. Smith" ],
      "venue" : "Caldwell CA, editor. PLOS ONE. 2015;10:",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2015
    }, {
      "title" : "Statistics with Stata: updated for version 12 (8th ed.)",
      "author" : [ "Hamilton LC" ],
      "venue" : "Belmont: Cengage;",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2013
    }, {
      "title" : "The Parable of Google Flu: Traps in Big Data Analysis",
      "author" : [ "D Lazer", "R Kennedy", "G King", "A. Vespignani" ],
      "venue" : null,
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2014
    }, {
      "title" : "Significance testing of word frequencies in corpora. Digit Scholarsh Humanit",
      "author" : [ "J Lijffijt", "T Nevalainen", "T Säily", "P Papapetrou", "K Puolamaki", "H. Mannila" ],
      "venue" : null,
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2014
    }, {
      "title" : "Significant or random?: A critical review of sociolinguistic generalisations based on large corpora",
      "author" : [ "V Brezina", "M. Meyerhoff" ],
      "venue" : "Int J Corpus Linguist",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2014
    }, {
      "title" : "Word Frequency Distributions",
      "author" : [ "Baayen RH" ],
      "venue" : "Dordrecht: Kluwer Academic Publishers;",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2001
    }, {
      "title" : "Distributions in text. In: Lüdeling A, Kytö M, editors. Corpus linguistics: an international handbook",
      "author" : [ "M. Baroni" ],
      "venue" : null,
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2009
    }, {
      "title" : "The Google Books Team, et al. Quantitative Analysis of Culture Using Millions of Digitized Books (Supporting Online Material)",
      "author" : [ "J-B Michel", "YK Shen", "AP Aiden", "A Verses", "MK Gray" ],
      "venue" : null,
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 1996
    }, {
      "title" : "The impact of lacking metadata for the measurement of cultural and linguistic change using the Google Ngram datasets – reconstructing the composition of the German corpus in times of WWII",
      "author" : [ "A. Koplenig" ],
      "venue" : "Digit Scholarsh Humanit",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2015
    }, {
      "title" : "Characterizing the Google Books corpus: Strong limits to inferences of socio-cultural and linguistic evolution [Internet",
      "author" : [ "EA Pechenick", "CM Danforth", "PS. Dodds" ],
      "venue" : null,
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2015
    }, {
      "title" : "Spurious correlations. First edition",
      "author" : [ "T. Vigen" ],
      "venue" : "New York: Hachette Books;",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2015
    }, {
      "title" : "POPULATION STATISTICS: historical demography of all countries, their divisions and towns. In: http://www.populstat.info/ [Internet",
      "author" : [ "J. Lahmeyer" ],
      "venue" : "[cited",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2006
    }, {
      "title" : "Probabilistic reanalysis of twentiethcentury sea-level rise (Excel source data)",
      "author" : [ "CC Hay", "E Morrow", "RE Kopp", "JX. Mitrovica" ],
      "venue" : null,
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2015
    }, {
      "title" : "How Variable May a Constant be? Measures of Lexical Richness in Perspective",
      "author" : [ "Tweedie FJ", "Baayen RH" ],
      "venue" : "Comput Humanit",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 1998
    }, {
      "title" : "Principles of econometrics. In: Principles of Econometrics, 3rd Edition (accompanying website) [Internet",
      "author" : [ "Hill RC" ],
      "venue" : "[cited",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2008
    }, {
      "title" : "Spurious regressions in econometrics",
      "author" : [ "CWJ Granger", "P. Newbold" ],
      "venue" : "J Econom",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 1974
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "Given the fact that in the recent past, several studies were published that seem to suffer from this problem [12– 18], we explain the cause of the misspecification and show that it has profound consequences.",
      "startOffset" : 109,
      "endOffset" : 117
    }, {
      "referenceID" : 17,
      "context" : "Following [19], we hope that our paper will help both researchers and reviewers to understand why it is important to use special models for the analysis of data with a natural temporal ordering.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 0,
      "context" : "In principle, we could start our paper like this: As complex adaptive systems [1], languages are constantly changing on all fundamental levels [2].",
      "startOffset" : 143,
      "endOffset" : 146
    }, {
      "referenceID" : 1,
      "context" : "In this context, several studies have indicated that population size is an important factor that influences the (rate of) language change [3–5].",
      "startOffset" : 138,
      "endOffset" : 143
    }, {
      "referenceID" : 2,
      "context" : "In this context, several studies have indicated that population size is an important factor that influences the (rate of) language change [3–5].",
      "startOffset" : 138,
      "endOffset" : 143
    }, {
      "referenceID" : 3,
      "context" : "In this context, several studies have indicated that population size is an important factor that influences the (rate of) language change [3–5].",
      "startOffset" : 138,
      "endOffset" : 143
    }, {
      "referenceID" : 4,
      "context" : "Based on a very large sample of the written language record [6], we present quantitative evidence that clearly supports the claim that population size strongly influences the lexical diversity of eight languages.",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 5,
      "context" : "identical pattern emerges if we correlate the lexical diversity in a given year with the global mean sea level [7] instead of population size.",
      "startOffset" : 111,
      "endOffset" : 114
    }, {
      "referenceID" : 6,
      "context" : "While we want to point out upfront that the problem we describe is fairly common knowledge in econometric time-series analysis [8–11], we nevertheless felt the need to outline it once again in this paper, given the fact that in the recent past, several studies were published– some in journals with a good reputation – that seem to suffer from exactly this problem [12–18].",
      "startOffset" : 127,
      "endOffset" : 133
    }, {
      "referenceID" : 7,
      "context" : "While we want to point out upfront that the problem we describe is fairly common knowledge in econometric time-series analysis [8–11], we nevertheless felt the need to outline it once again in this paper, given the fact that in the recent past, several studies were published– some in journals with a good reputation – that seem to suffer from exactly this problem [12–18].",
      "startOffset" : 127,
      "endOffset" : 133
    }, {
      "referenceID" : 8,
      "context" : "While we want to point out upfront that the problem we describe is fairly common knowledge in econometric time-series analysis [8–11], we nevertheless felt the need to outline it once again in this paper, given the fact that in the recent past, several studies were published– some in journals with a good reputation – that seem to suffer from exactly this problem [12–18].",
      "startOffset" : 127,
      "endOffset" : 133
    }, {
      "referenceID" : 9,
      "context" : "While we want to point out upfront that the problem we describe is fairly common knowledge in econometric time-series analysis [8–11], we nevertheless felt the need to outline it once again in this paper, given the fact that in the recent past, several studies were published– some in journals with a good reputation – that seem to suffer from exactly this problem [12–18].",
      "startOffset" : 127,
      "endOffset" : 133
    }, {
      "referenceID" : 10,
      "context" : "While we want to point out upfront that the problem we describe is fairly common knowledge in econometric time-series analysis [8–11], we nevertheless felt the need to outline it once again in this paper, given the fact that in the recent past, several studies were published– some in journals with a good reputation – that seem to suffer from exactly this problem [12–18].",
      "startOffset" : 365,
      "endOffset" : 372
    }, {
      "referenceID" : 11,
      "context" : "While we want to point out upfront that the problem we describe is fairly common knowledge in econometric time-series analysis [8–11], we nevertheless felt the need to outline it once again in this paper, given the fact that in the recent past, several studies were published– some in journals with a good reputation – that seem to suffer from exactly this problem [12–18].",
      "startOffset" : 365,
      "endOffset" : 372
    }, {
      "referenceID" : 12,
      "context" : "While we want to point out upfront that the problem we describe is fairly common knowledge in econometric time-series analysis [8–11], we nevertheless felt the need to outline it once again in this paper, given the fact that in the recent past, several studies were published– some in journals with a good reputation – that seem to suffer from exactly this problem [12–18].",
      "startOffset" : 365,
      "endOffset" : 372
    }, {
      "referenceID" : 13,
      "context" : "While we want to point out upfront that the problem we describe is fairly common knowledge in econometric time-series analysis [8–11], we nevertheless felt the need to outline it once again in this paper, given the fact that in the recent past, several studies were published– some in journals with a good reputation – that seem to suffer from exactly this problem [12–18].",
      "startOffset" : 365,
      "endOffset" : 372
    }, {
      "referenceID" : 14,
      "context" : "While we want to point out upfront that the problem we describe is fairly common knowledge in econometric time-series analysis [8–11], we nevertheless felt the need to outline it once again in this paper, given the fact that in the recent past, several studies were published– some in journals with a good reputation – that seem to suffer from exactly this problem [12–18].",
      "startOffset" : 365,
      "endOffset" : 372
    }, {
      "referenceID" : 15,
      "context" : "While we want to point out upfront that the problem we describe is fairly common knowledge in econometric time-series analysis [8–11], we nevertheless felt the need to outline it once again in this paper, given the fact that in the recent past, several studies were published– some in journals with a good reputation – that seem to suffer from exactly this problem [12–18].",
      "startOffset" : 365,
      "endOffset" : 372
    }, {
      "referenceID" : 16,
      "context" : "While we want to point out upfront that the problem we describe is fairly common knowledge in econometric time-series analysis [8–11], we nevertheless felt the need to outline it once again in this paper, given the fact that in the recent past, several studies were published– some in journals with a good reputation – that seem to suffer from exactly this problem [12–18].",
      "startOffset" : 365,
      "endOffset" : 372
    }, {
      "referenceID" : 17,
      "context" : "Following [19], we",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 24,
      "context" : "We could then continue and elaborate our argument by assuming that a larger number of speakers is most likely associated with a greater degree of variance in demographic background and sociocultural environments [26], and that this greater diversity leads to an increase of the type-token ratio as a",
      "startOffset" : 212,
      "endOffset" : 216
    }, {
      "referenceID" : 17,
      "context" : "This is exactly what [19] alerted to, since this result could be used in the media or even by politicians to criticize socialism.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 9,
      "context" : "The series are said to have a unit root or to be non-stationary [11].",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 6,
      "context" : "Since the Pearson product-moment correlation measures whether values of one series that are above/below average tend to co-occur with values of another series that are above/below average, by mathematical necessity, the correlation coefficient for two trending time-series will then be high when in fact they are not related in any substantial sense [8].",
      "startOffset" : 350,
      "endOffset" : 353
    }, {
      "referenceID" : 9,
      "context" : "actual levels of the two series [11].",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 6,
      "context" : "Therefore, [8] suggest as a rule of thumb to generally model data on a combination of both levels and changes.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 14,
      "context" : "A few words on the Google Books data are in order here, as they are the basis of all but one [16] study quoted above.",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 29,
      "context" : "Since n-grams do not occur independently across distinct books, this aggregation of individual book frequencies means that we cannot account for the distributions of n-grams which can have profound consequences for the analysis of textual data [32,33].",
      "startOffset" : 244,
      "endOffset" : 251
    }, {
      "referenceID" : 30,
      "context" : "Since n-grams do not occur independently across distinct books, this aggregation of individual book frequencies means that we cannot account for the distributions of n-grams which can have profound consequences for the analysis of textual data [32,33].",
      "startOffset" : 244,
      "endOffset" : 251
    }, {
      "referenceID" : 31,
      "context" : "So in terms of what we know about word frequency distributions [34], this procedure eliminates approximately 95% of all different 1-gram types; for n-grams where n>1 this figure is even higher [35].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 20,
      "context" : "According to the FAQs of the Culturomics project behind the GB data [22] the team has “not received",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 33,
      "context" : "The lack of metadata can have important ramifications for any interpretation of potential results based on the Google Books Ngram data [37,38].",
      "startOffset" : 135,
      "endOffset" : 142
    }, {
      "referenceID" : 27,
      "context" : "This is also important from a methodological point of view: just because two series are trending, does not necessarily imply any substantial relationship [30].",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 10,
      "context" : "All recently published studies that we mentioned in the introduction do not explicitly model the underlying temporal structure of the data [12–18].",
      "startOffset" : 139,
      "endOffset" : 146
    }, {
      "referenceID" : 11,
      "context" : "All recently published studies that we mentioned in the introduction do not explicitly model the underlying temporal structure of the data [12–18].",
      "startOffset" : 139,
      "endOffset" : 146
    }, {
      "referenceID" : 12,
      "context" : "All recently published studies that we mentioned in the introduction do not explicitly model the underlying temporal structure of the data [12–18].",
      "startOffset" : 139,
      "endOffset" : 146
    }, {
      "referenceID" : 13,
      "context" : "All recently published studies that we mentioned in the introduction do not explicitly model the underlying temporal structure of the data [12–18].",
      "startOffset" : 139,
      "endOffset" : 146
    }, {
      "referenceID" : 14,
      "context" : "All recently published studies that we mentioned in the introduction do not explicitly model the underlying temporal structure of the data [12–18].",
      "startOffset" : 139,
      "endOffset" : 146
    }, {
      "referenceID" : 15,
      "context" : "All recently published studies that we mentioned in the introduction do not explicitly model the underlying temporal structure of the data [12–18].",
      "startOffset" : 139,
      "endOffset" : 146
    }, {
      "referenceID" : 16,
      "context" : "All recently published studies that we mentioned in the introduction do not explicitly model the underlying temporal structure of the data [12–18].",
      "startOffset" : 139,
      "endOffset" : 146
    }, {
      "referenceID" : 18,
      "context" : "S1 contains the population data, compiled from [20].",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 5,
      "context" : "sea level was presented in [7] and is available at [21].",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 19,
      "context" : "sea level was presented in [7] and is available at [21].",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 4,
      "context" : "The type-token ratio, a common way to measure lexical diversity, is based on the Google Books datasets that were made available by [6] at [22].",
      "startOffset" : 131,
      "endOffset" : 134
    }, {
      "referenceID" : 20,
      "context" : "The type-token ratio, a common way to measure lexical diversity, is based on the Google Books datasets that were made available by [6] at [22].",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 21,
      "context" : "Since this measure is known to be heavily text-length dependent [23] and given the fact that the corpus size based on the Google Books data strongly increases as a function of time, calculating the type-token ratio based on the actual corpus sizes would systematically bias the results.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 22,
      "context" : "To solve this problem, random samples of 1,000,000 tokens were drawn from the data as described in [24].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 23,
      "context" : "Additionally, we simulated i = 10,000 random walks with drift [25,11] that are defined as:",
      "startOffset" : 62,
      "endOffset" : 69
    }, {
      "referenceID" : 9,
      "context" : "Additionally, we simulated i = 10,000 random walks with drift [25,11] that are defined as:",
      "startOffset" : 62,
      "endOffset" : 69
    }, {
      "referenceID" : 25,
      "context" : "In the presence of first-order autocorrelation, the OLS estimators are biased and lead to incorrect statistical inferences [28].",
      "startOffset" : 123,
      "endOffset" : 127
    } ],
    "year" : 2015,
    "abstractText" : "We document an apparently spectacular relationship between population size and lexical diversity: for five out of seven investigated languages, there is a strong relationship between population size and lexical diversity of the primary language in this country. We show that this relationship is the result of a misspecified model that does not consider the temporal aspect of the data by presenting a similar but nonsensical relationship between the global annual mean sea level and lexical diversity. Given the fact that in the recent past, several studies were published that seem to suffer from this problem [12– 18], we explain the cause of the misspecification and show that it has profound consequences. We demonstrate how simple transformation of the time series can often solve problems of this type and argue that the evaluation of the plausibility of a relationship is important in this context. Following [19], we hope that our paper will help both researchers and reviewers to understand why it is important to use special models for the analysis of data with a natural temporal ordering.",
    "creator" : "Microsoft® Word 2010"
  }
}