{
  "name" : "1501.05222.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Plug-and-play dual-tree algorithm runtime analysis",
    "authors" : [ "Ryan R. Curtin", "Dongryeol Lee" ],
    "emails" : [ "ryan@ratml.org", "drselee@gmail.com", "march@ices.utexas.edu", "p.ram@gatech.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 1.\n05 22\n2v 1\nKeywords: dual-tree algorithms, branch and bound, nearest neighbor search, kernel density estimation, range search"
    }, {
      "heading" : "1. Dual-tree algorithms",
      "text" : "A surprising number of machine learning algorithms have computational bottlenecks that can be expressed as pairwise statistical problems. By this, we mean computational tasks that can be evaluated directly by iterating over all pairs of input points. Nearest neighbor search is one such problem, since for every query point, we can evaluate its distance to every reference point and keep the closest one. This naively requires O(N) time to answer in single query in a reference set of size N ; answering O(N) queries subsequently requires\nprohibitive O(N2) time. Kernel density estimation is also a pairwise statistical problem, since we compute a sum over all reference points for each query point. This again requires O(N2) time to answer O(N) queries if done directly. The reference set is typically indexed with spatial data structures to accelerate this type of computation (Finkel and Bentley, 1974; Beygelzimer et al., 2006); these result in O(logN) runtime per query under favorable conditions.\nBuilding upon this intuition, Gray and Moore (2001) generalized the fast multipole method from computational physics to obtain dual-tree algorithms. These are extremely useful when there are large query sets, not just a few query points. Instead of building a tree on the reference set and searching with each query point separately, Gray and Moore suggest also building a query tree and traversing both the query and reference trees simultaneously (a dual-tree traversal, from which the class of algorithms takes its name).\nDual-tree algorithms can be easily understood through the recent framework of Curtin et al. (2013b): two trees (a query tree and a reference tree) are traversed by a pruning dual-tree traversal. This traversal visits combinations of nodes from the trees in some sequence (each combination consisting of a query node and a reference node), calling a problem-specific Score() function to determine if the node combination can be pruned. If not, then a problem-specific BaseCase() function is called for each combination of points held in the query node and reference node. This has significant similarity to the more common singletree branch-and-bound algorithms, except that the algorithm must recurse into child nodes of both the query tree and reference tree.\nThere exist numerous dual-tree algorithms for problems as diverse as kernel density estimation (Gray and Moore, 2003), mean shift (Wang et al., 2007), minimum spanning tree calculation (March et al., 2010), n-point correlation function estimation (March et al., 2012), max-kernel search (Curtin et al., 2013c), particle smoothing (Klaas et al., 2006), variational inference (Amizadeh et al., 2012), range search (Gray and Moore, 2001), and embedding techniques Van Der Maaten (2014), to name a few.\nSome of these algorithms are derived using the cover tree (Beygelzimer et al., 2006), a data structure with compelling theoretical qualities. When cover trees are used, Dual-tree all-nearest-neighbor search and approximate kernel density estimation have O(N) runtime guarantees for O(N) queries (Ram et al., 2009a); minimum spanning tree calculation scales as O(N logN) (March et al., 2010). Other problems have similar worst-case guarantees (Curtin and Ram, 2014; March, 2013).\nIn this work we combine the generalization of Curtin et al. (2013b) with the theoretical results of Beygelzimer et al. (2006) and others in order to develop a worst-case runtime bound for any dual-tree algorithm when the cover tree is used.\nSection 2 lays out the required background, notation, and introduces the cover tree and its associated theoretical properties. Readers familiar with the cover tree literature and dual-tree algorithms (especially Curtin et al., 2013b) may find that section to be review. Following that, we introduce an intuitive measure of cover tree imbalance, an important property for understanding the runtime of dual-tree algorithms, in Section 3. This measure of imbalance is then used to prove the main result of the paper in Section 4, which is a worst-case runtime bound for generalized dual-tree algorithms. We apply this result to three specific problems: nearest neighbor search (Section 5), approximate kernel density estimation (Section 6), and range search / range count (Section 7), showing linear runtime\nbounds for each of those algorithms. Each of these bounds is an improvement on the state-of-the-art, and in the case of range search, is the first such bound."
    }, {
      "heading" : "2. Preliminaries",
      "text" : "For simplicity, the algorithms considered in this paper will be presented in a tree-independent context, as in Curtin et al. (2013b), but the only type of tree we will consider is the cover tree (Beygelzimer et al., 2006), and the only type of traversal we will consider is the cover tree pruning dual-tree traversal, which we will describe later.\nAs we will be making heavy use of trees, we must establish notation (taken from Curtin et al., 2013b). The notation we will be using is defined in Table 1."
    }, {
      "heading" : "2.1 The cover tree",
      "text" : "The cover tree is a leveled hierarchical data structure originally proposed for the task of nearest neighbor search by Beygelzimer et al. (2006). Each node Ni in the cover tree is associated with a single point pi. An adequate description is given in their work (we have adapted notation slightly):\nA cover tree T on a dataset S is a leveled tree where each level is a “cover” for the level beneath it. Each level is indexed by an integer scale si which decreases as the tree is descended. Every node in the tree is associated with a point in S. Each point in S may be associated with multiple nodes in the tree; however, we require that any point appears at most once in every level. Let Csi denote the set of points in S associated with the nodes at level si. The cover tree obeys the following invariants for all si:\n• (Nesting). Csi ⊂ Csi−1. This implies that once a point p ∈ S appears in Csi then every lower level in the tree has a node associated with p.\n• (Covering tree). For every pi ∈ Csi−1, there exists a pj ∈ Csi such that d(pi, pj) < 2\nsi and the node in level si associated with pj is a parent of the node in level si − 1 associated with pi.\n• (Separation). For all distinct pi, pj ∈ Csi , d(pi, pj) > 2 si .\nAs a consequence of this definition, if there exists a node Ni, containing the point pi at some scale si, then there will also exist a self-child node Nic containing the point pi at scale si − 1 which is a child of Ni. In addition, every descendant point of the node Ni is contained within a ball of radius 2si+1 centered at the point pi; therefore, λi = 2\nsi+1 and µi = pi (Table 1).\nNote that the cover tree may be interpreted as an infinite-leveled tree, with C∞ containing only the root point, C−∞ = S, and all levels between defined as above. Beygelzimer et al. (2006) find this representation (which they call the implicit representation) easier for description of their algorithms and some of their proofs. But clearly, this is not suitable for implementation; hence, there is an explicit representation in which all nodes that have only a self-child are coalesced upwards (that is, the node’s self-child is removed, and the children of that self-child are taken to be the children of the node).\nIn this work, we consider only the explicit representation of a cover tree, and do not concern ourselves with the details of tree construction1."
    }, {
      "heading" : "2.2 Expansion constant",
      "text" : "The explicit representation of a cover tree has a number of useful theoretical properties based on the expansion constant (Karger and Ruhl, 2002); we restate its definition below.\nDefinition 1 Let BS(p,∆) be the set of points in S within a closed ball of radius ∆ around some p ∈ S with respect to a metric d: BS(p,∆) = {r ∈ S : d(p, r) ≤ ∆}. Then, the expansion constant of S with respect to the metric d is the smallest c ≥ 2 such that\n|BS(p, 2∆)| ≤ c|BS(p,∆)| ∀ p ∈ S, ∀ ∆ > 0. (1)\nThe expansion constant is used heavily in the cover tree literature. It is, in some sense, a notion of instrinic dimensionality, and previous work has shown that there are many scenarios where c is independent of the number of points in the dataset (Karger and Ruhl, 2002; Beygelzimer et al., 2006; Krauthgamer and Lee, 2004; Ram et al., 2009a). Note also that if points in S ⊂ H are being drawn according to a stationary distribution f(x), then c will converge to some finite value cf as |S| → ∞. To see this, define cf as a generalization of the expansion constant for distributions. cf ≥ 2 is the smallest value such that\n∫\nBH(p,2∆) f(x)dx ≤ cf\n∫\nBH(p,∆) f(x)dx (2)\nfor all p ∈ H and ∆ > 0 such that ∫\nBH(p,∆) f(x)dx > 0, and with BH(p,∆) defined as the\nclosed ball of radius ∆ in the space H. As a simple example, take f(x) as a uniform spherical distribution inRd: for any |x| ≤ 1, f(x) is a constant; for |x| > 1, f(x) = 0. It is easy to see that cf in this situation is 2 d, and thus for some dataset S, c must converge to that value as more and more points are added to S. Closed-form solutions for cf for more complex distributions are less easy to derive; however, empirical speedup results from Beygelzimer et al. (2006) suggest the existence of datasets where c is not strongly dependent on d. For instance, the covtype dataset has\n1. A batch construction algorithm is given by Beygelzimer et al. (2006), called Construct.\n54 dimensions but the expansion constant is much smaller than other, lower-dimensional datasets.\nThere are some other important observations about the behavior of c. Adding a single point to S may increase c arbitrarily: consider a set S distributed entirely on the surface of a unit hypersphere. If one adds a single point at the origin, producing the set S′, then c explodes to |S′| whereas before it may have been much smaller than |S|. Adding a single point may also decrease c significantly. Suppose one adds a point arbitrarily close to the origin to S′; now, the expansion constant will be |S′|/2. Both of these situations are degenerate cases not commonly encountered in real-world behavior; we discuss them in order to point out that although we can bound the behavior of c as |S| → ∞ for S from a stationary distribution, we are not able to easily say much about its convergence behavior.\nThe expansion constant can be used to show a few useful bounds on various properties of the cover tree; we restate these results below, given some cover tree built on a dataset S with expansion constant c and |S| = N :\n• Width bound: no cover tree node has more than c4 children (Lemma 4.1, Beygelzimer et al. (2006)).\n• Depth bound: the maximum depth of any node isO(c2 logN) (Lemma 4.3, Beygelzimer et al. (2006)).\n• Space bound: a cover tree has O(N) nodes (Theorem 1, Beygelzimer et al. (2006)).\nLastly, we introduce a convenience lemma of our own which is a generalization of the packing arguments used by Beygelzimer et al. (2006). This is a more flexible version of their argument.\nLemma 1 Consider a dataset S with expansion constant c and a subset C ⊆ S such that every point in C is separated by δ. Then, for any point p (which may or may not be in S), and any radius ρδ > 0:\n|BS(p, ρδ) ∩ C| ≤ c 2+⌈log 2 ρ⌉. (3)\nProof The proof is based on the packing argument from Lemma 4.1 in Beygelzimer et al. (2006). Consider two cases: first, let d(p, pi) > ρδ for any pi ∈ S. In this case, BS(p, ρδ) = ∅ and the lemma holds trivially. Otherwise, let pi ∈ S be a point such that d(p, pi) ≤ ρδ. Observe that BS(p, ρδ) ⊆ BS(pi, 2ρδ). Also, |BS(pi, 2ρδ)| = c 2+⌈log 2 ρ⌉|BS(pi, δ/2)| by the definition of the expansion constant. Because each point in C is separated by δ, the number of points in BS(p, ρδ)∩C is bounded by the number of disjoint balls of radius δ/2 that can be packed into BS(p, ρδ). In the worst case, this packing is perfect, and\n|BS(p, ρδ)| ≤ |BS(pi, 2ρδ)|\n|BS(pi, δ/2)| ≤ c2+⌈log2 ρ⌉. (4)"
    }, {
      "heading" : "3. Tree imbalance",
      "text" : "It is well-known that imbalance in trees leads to degradation in performance; for instance, a kd-tree node with every descendant in its left child except one is effectively useless. A kd-tree full of nodes like this will perform abysmally for nearest neighbor search, and it is not hard to generate a pathological dataset that will cause a kd-tree of this sort.\nThis sort of imbalance applies to all types of trees, not just kd-trees. In our situation, we are interested in a better understanding of this imbalance for cover trees, and thus endeavor to introduce a more formal measure of imbalance which is correlated with tree performance. Numerous measures of tree imbalance have already been established; one example is that proposed by Colless (1982), and another is Sackin’s index (Sackin, 1972), but we aim to capture a different measure of imbalance that utilizes the leveled structure of the cover tree.\nWe already know each node in a cover tree is indexed with an integer level (or scale). In the explicit representation of the cover tree, each non-leaf node has children at a lower level. But these children need not be strictly one level lower; see Figure 1. In Figure 1a, each cover tree node has children that are strictly one level lower; we will refer to this as a perfectly balanced cover tree. Figure 1b, on the other hand, contains the node Nm which has two children with scale two less than sm. We will refer to this as an imbalanced cover tree. Note that in our definition, the balance of a cover tree has nothing to do with differing number of descendants in each child branch but instead only missing levels.\nAn imbalanced cover tree can happen in practice, and in the worst cases, the imbalance may be far worse than the simple graphs of Figure 1. Consider a dataset with a single outlier which is very far away from all of the other points2. Figure 2 shows what happens in this situation: the root node has two children; one of these children has only the outlier as a descendant, and the other child has the rest of the points in the dataset as a descendant. In fact, it is easy to find datasets with a handful of outliers that give rise to a chain-like structure at the top of the tree: see Figure 3 for an illustration3.\n2. Note also that for an outlier sufficiently far away, the expansion constant is N − 1, so we should expect poor performance with the cover tree anyway. 3. As a side note, this behavior is not limited to cover trees, and can happen to mean-split kd-trees too, especially in higher dimensions.\nA tree that has this chain-like structure all the way down, which is similar to the kd-tree example at the beginning of this section, is going to perform horrendously; motivated by this observation, we define a measure of tree imbalance.\nDefinition 2 The cover node imbalance in(Ni) for a cover tree node Ni with scale si in the cover tree T is defined as the cumulative number of missing levels between the node and its parent Np (which has scale sp). If the node is a leaf child (that is, si = −∞), then number of missing levels is defined as the difference between sp and smin − 1 where smin is the smallest scale of a non-leaf node in T . If Ni is the root of the tree, then the cover node imbalance is 0. Explicitly written, this calculation is\nin(Ni) =\n\n \n \nsp − si − 1 if Ni is not a leaf and not the root node\nmax(sp − smin − 1, 0) if Ni is a leaf\n0 if Ni is the root node.\n(5)\nThis simple definition of cover node imbalance is easy to calculate, and using it, we can generalize to a measure of imbalance for the full tree.\nDefinition 3 The cover tree imbalance it(T ) for a cover tree T is defined as the cumulative number of missing levels in the tree. This can be expressed as a function of cover node imbalances easily:\nit(T ) = ∑\nNi∈T\nin(Ni). (6)\nA perfectly balanced cover tree Tb with no missing levels has imbalance it(Tb) = 0 (for instance, Figure 1a). A worst-case cover tree Tw which is entirely a chain-like structure with maximum scale smax and minimum scale smin will have imbalance it(Tw) ∼ N(smax−smin). Because of this chain-like structure, each level has only one node and thus there are at least N levels; or, smax − smin ≥ N , meaning that in the worst case the imbalance is quadratic in N4.\nHowever, for most real-world datasets with the cover tree implementation in mlpack (Curtin et al., 2013a) and the reference implementation (Beygelzimer et al., 2006), the tree imbalance is near-linear with the number of points. Generally, most of the cover tree imbalance is contributed by leaf nodes whose parent has scale greater than smin. At this time, no cover tree construction algorithm specifically aims to minimize imbalance."
    }, {
      "heading" : "4. General runtime bound",
      "text" : "Perhaps more interesting than measures of tree imbalance is the way cover trees are actually used in dual-tree algorithms. Although cover trees were originally intended for nearest neighbor search (See Algorithm Find-All-Nearest, Beygelzimer et al., 2006), they can be adapted to a wide variety of problems: minimum spanning tree calculation (March et al., 2010), approximate nearest neighbor search (Ram et al., 2009b), Gaussian processes posterior calculation (Moore and Russell, 2014), and max-kernel search (Curtin and Ram, 2014) are some examples. Further, through the tree-independent dual-tree algorithm abstraction of Curtin et al. (2013b), other existing dual-tree algorithms can easily be adapted for use with cover trees.\nIn the framework of tree-independent dual-tree algorithms, all that is necessary to describe a dual-tree algorithm is a point-to-point base case function (BaseCase()) and a node-to-node pruning rule (Score()). These functions, which are often very straightforward, are then paired with a type of tree and a pruning dual-tree traversal to produce a working algorithm. In later sections, we will consider specific examples.\nWhen using cover trees, the typical pruning dual-tree traversal is an adapted form of the original nearest neighbor search algorithm (see Find-All-Nearest, Beygelzimer et al., 2006); this traversal is implemented in both the cover tree reference implementation and in the more flexible mlpack library (Curtin et al., 2013a). The problem-independent traversal is given in Algorithm 1 and was originally presented by Curtin and Ram (2014). Initially, it is called with the root of the query tree and a reference set R containing only the root of the reference tree.\nThis dual-tree recursion is a depth-first recursion in the query tree and a breadth-first recursion in the reference tree; to this end, the recursion maintains one query node Nq and a reference set R. The set R may contain reference nodes with many different scales; the maximum scale in the reference set is smaxr (line 3). Each single recursion will descend either the query tree or the reference tree, not both; the conditional in line 4, which determines whether the query or reference tree will be recursed, is aimed at keeping the relative scales of query nodes and reference nodes close.\nA query recursion (lines 13–18) is straightforward: for each child Nqc of Nq, the node combinations (Nqc,Nr) are scored for each Nr in the reference set R. If possible, these\n4. Note that in this situation, c ∼ N also.\nAlgorithm 1 The standard pruning dual-tree traversal for cover trees.\n1: Input: query node Nq, set of reference nodes R 2: Output: none\n3: smaxr ← maxNr∈R sr 4: if (sq < s max r ) then 5: {Perform a reference recursion.} 6: for each Nr ∈ R do 7: BaseCase(pq, pr) 8: end for\n9: Rr ← {Nr ∈ R : sr = smaxr } 10: Rr−1 ← {C (Nr) : Nr ∈ Rr} ∪ (R \\Rr) 11: R′r−1 ← {Nr ∈ Rr−1 : Score(Nq,Nr) 6= ∞} 12: recurse with Nq and R ′ r−1 13: else 14: {Perform a query recursion.} 15: for each Nqc ∈ C (Nq) do 16: R′ ← {Nr ∈ R : Score(Nq,Nr) 6= ∞} 17: recurse with Nqc and R ′ 18: end for\n19: end if\ncombinations are pruned to form the set R′ (line 17) by checking the output of the Score() function, and then the algorithm recurses with Nqc and R\n′. A reference recursion (lines 4–12) is similar to a query recursion, but the pruning strategy is significantly more complicated. Given R, we calculate Rr, which is the set of nodes in R that have scale smaxr . Then, for each node Nr in the set of children of nodes in Rr, the node combinations (Nq,Nr) are scored and pruned if possible. Those reference nodes that were not pruned form the set R′r−1. Then, this set is combined with R \\Rr—that is, each of the nodes in R that was not recursed into—to produce R′, and the algorithm recurses with Nq and the reference set R\n′. The reference recursion only recurses into the top-level subset of the reference nodes in order to preserve the separation invariant. It is easy to show that every pair of points held in nodes in R is separated by at least 2s max r :\nLemma 2 For all nodes Ni,Nj ∈ R (in the context of Algorithm 1) which contain points pi and pj, respectively, d(pi, pj) > 2 smaxr , with smaxr defined as in line 3.\nProof This proof is by induction. If |R| = 1, such as during the first reference recursion, the result obviously holds. Now consider any reference set R and assume the statement of the lemma holds for this set R, and define smaxr as the maximum scale of any node in R. Construct the set Rr−1 as in line 10 of Algorithm 1; if |Rr−1| ≤ 1, then Rr−1 satisfies the desired property.\nOtherwise, take any Ni,Nj in Rr−1, with points pi and pj, respectively, and scales si and sj, respectively. Clearly, if si = sj = s max r − 1, then by the separation invariant d(pi, pj) > 2 smaxr −1.\nNow suppose that si < s max r − 1. This implies that there exists some implicit cover tree node with point pi and scale s max r − 1 (as well an implicit child of this node pi with scale smaxr − 2 and so forth until one of these implicit nodes has child pi with scale si). Because the separation invariant applies to both implicit and explicit representations of the tree, we conclude that d(pi, pr) > 2\nsmaxr − 1. The same argument may be made for the case where sj < s max r − 1, with the same conclusion.\nWe may therefore conclude that each point of each node in Rr−1 is separated by 2 smaxr −1. Note that R′r−1 ⊆ Rr−1 and that R \\Rr−1 ⊆ R in order to see that this condition holds for all nodes in R′ = R′r−1 ∪ (R \\Rr−1).\nBecause we have shown that the condition holds for the initial reference set and for any reference set produced by a reference recursion, we have shown that the statement of the lemma is true.\nThis observation means that the set of points P held by all nodes in R is always a subset of Csmaxr . This fact will be useful in our later runtime proofs.\nNext, we develop notions with which to understand the behavior of the cover tree dualtree traversal when the datasets are of significantly different scale distributions.\nIf the datasets are similar in scale distribution (that is, inter-point distances tend to follow the same distribution), then the recursion will alternate between query recursions and reference recursions. But if the query set contains points which are, in general, much farther apart than the reference set, then the recursion will start with many query recursions before reaching a reference recursion. The converse case also holds. We are interested in formalizing this notion of scale distribution; therefore, define the following dataset-dependent constants for the query set Sq and the reference set Sr:\n• ηq: the largest pairwise distance in Sq\n• δq: the smallest nonzero pairwise distance in Sq\n• ηr: the largest pairwise distance in Sr\n• δr: the smallest nonzero pairwise distance in Sr\nThese constants are directly related to the aspect ratio of the datasets; indeed, ηq/δq is exactly the aspect ratio of Sq. Further, let us define and bound the top and bottom levels of each tree:\n• The top scale sTq of the query tree Tq is such that as ⌈log2(ηq)⌉− 1 ≤ s T q ≤ ⌈log2(ηq)⌉.\n• The minimum scale of the query tree Tr is defined as s min q = ⌈log2(δq)⌉.\n• The top scale sTr of the reference tree Tr is such that as ⌈log2(ηr)⌉ − 1 ≤ s T r ≤\n⌈log2(ηr)⌉.\n• The minimum scale of the reference tree Tr is defined as s min r = ⌈log2(δr)⌉.\nNote that the minimum scale is not the minimum scale of any cover tree node (that would be −∞), but the minimum scale of any non-leaf node in the tree.\nSuppose that our datasets are of a similar scale distribution: sTq = s T r , and s min q = s min r . In this setting we will have alternating query and reference recursions. But if this is not the case, then we have extra reference recursions before the first query recursion or after the last query recursion (situations where both these cases happen are possible). Motivated by this observation, let us quantify these extra reference recursions:\nLemma 3 For a dual-tree algorithm with Sq ∼ Sr ∼ O(N) using cover trees and the traversal given in Algorithm 1, the number of extra reference recursions that happen before the first query recursion is bounded by\nmin (O(N), log2(ηr/ηq)− 1) . (7)\nProof The first query recursion happens once sq ≥ s max r . The number of reference recursions before the first query recursion is then bounded as the number of levels in the reference tree between sTr and s T q that have at least one explicit node. Because there are O(N) nodes in the reference tree, the number of levels cannot be greater than O(N) and thus the result holds.\nThe second bound holds by applying the definitions of sTr and s T q to the expression\nsTr − s T q − 1:\nsTr − s T q − 1 ≤ ⌈log2(ηr)⌉ − (⌈log2(ηq)⌉ − 1)− 1 (8)\n≤ log2(ηr) + 1− log2(ηq) (9)\nwhich gives the statement of the lemma after applying logarithmic identities.\nNote that the O(N) bound may be somewhat loose, but it suffices for our later purposes. Now let us consider the other case:\nLemma 4 For a dual-tree algorithm with Sq ∼ Sr ∼ O(N) using cover trees and the traversal given in Algorithm 1, the number of extra reference recursions that happen after the last query recursion is bounded by\nθ = max { min [ O(N log2(δq/δr)), O(N 2) ] , 0 } . (10)\nProof Our goal here is to count the number of reference recursions after the final query recursion at level sminq ; the first of these reference recursions is at scale s max r = s min q . Because query nodes are not pruned in this traversal, each reference recursion we are counting will be duplicated over the whole set of O(N) query nodes. The first part of the bound follows by observing that sminq − s min r ≤ ⌈log2(δq)⌉ − ⌈log2(δr)⌉ − 1 ≤ log2(δq/δr).\nThe second part follows by simply observing that there are O(N) reference nodes.\nThese two previous lemmas allow us a better understanding of what happens as the reference set and query set become different. Lemma 3 shows that the number of extra\nrecursions caused by a reference set with larger pairwise distances than the query set (ηr larger than ηq) is modest; on the other hand, Lemma 4 shows that for each extra level in the reference tree below sminq , O(N) extra recursions are required. Using these lemmas and this intuition, we will prove general runtime bounds for the cover tree traversal.\nTheorem 1 Given a reference set Sr of size N with an expansion constant cr and a set of queries Sq of size O(N), a standard cover tree based dual-tree algorithm (Algorithm 1) takes\nO ( c4r |R ∗|χψ(N + it(Tq) + θ) )\n(11)\ntime, where |R∗| is the maximum size of the reference set R (line 1) during the dual-tree recursion, χ is the maximum possible runtime of BaseCase(), ψ is the maximum possible runtime of Score(), and θ is defined as in Lemma 4.\nProof First, split the algorithm into two parts: reference recursions (lines 4–12) and query recursions (lines 13–18). The runtime of the algorithm is bounded as the runtime of a reference recursion times the total number of reference recursions plus the total runtime of all query recursions.\nConsider a reference recursion (lines 4–12). Define R∗ to be the largest set R for any scale smaxr and any query node Nq during the course of the algorithm; then, it is true that |R| ≤ |R∗|. The work done in the base case loop from lines 6–8 is thus O(χ|R|) ≤ O(χ|R∗|). Then, lines 10 and 11 take O(c4rψ|R|) ≤ O(c 4 rψ|R\n∗|) time, because each reference node has up to c4r children. So, one full reference recursion takes O(c 4 rψχ|R\n∗|) time. Now, note that there are O(N) nodes in Tq. Thus, line 17 is visited O(N) times. The amount of work in line 16, like in the reference recursion, is bounded as O(c4rψ|R ∗|). Therefore, the total runtime of all query recursions is O(c4rψ|R ∗|N).\nLastly, we must bound the total number of reference recursions. Reference recursions happen in three cases: (1) smaxr is greater than the scale of the root of the query tree (no query recursions have happened yet); (2) smaxr is less than or equal to the scale of the root of the query tree, but is greater than the minimum scale of the query tree that is not −∞; (3) smaxr is less than the minimum scale of the query tree that is not −∞.\nFirst, consider case (1). Lemma 3 shows that the number of reference recursions of this type is bounded by O(N). Although there is also a bound that depends on the sizes of the datasets, we only aim to show a linear runtime bound, so the O(N) bound is sufficient here.\nNext, consider case (2). In this situation, each query recursion implies at least one reference recursion before another query recursion. For some query node Nq, the exact number of reference recursions before the children of Nq are recursed into is bounded above by in(Nq)+1: if Nq has imbalance 0, then it is exactly one level below its parent, and thus there is only one reference recursion. On the other hand, if Nq is many levels below its parent, then it is possible that a reference recursion may occur for each level in between; this is a maximum of in(Nq) + 1.\nBecause each query node in Tq is recursed into once, the total number of reference recursions before each query recursion is\n∑\nNq∈Tq\nin(Nq) + 1 = it(Tq) +O(N) (12)\nsince there are O(N) nodes in the query tree.\nLastly, for case (3), we may refer to Lemma 4, giving a bound of θ reference recursions in this case.\nWe may now combine these results for the runtime of a query recursions with the total number of reference recursions in order to give the result of the theorem:\nO ( c4r |R ∗|ψχ (N + it(Tq) + θ) ) +O ( c4r|R ∗|ψN ) ∼ O ( c4r |R ∗|ψχ (N + it(Tq) + θ) ) . (13)\nWhen we consider the monochromatic case (where Sq = Sr), the results trivially simplify.\nCorollary 1 Given the situation of Theorem 1 but with Sq = Sr = S so that cq = cr = c and Tq = Tr = T , a dual-tree algorithm using the standard cover tree traversal (Algorithm 1) takes\nO ( c4|R∗|χψ (N + it(T )) )\n(14)\ntime, where |R∗| is the maximum size of the reference set R (line 1) during the dualtree recursion, χ is the maximum possible runtime of BaseCase(), and ψ is the maximum possible runtime of Score().\nAn intuitive understanding of these bounds is best achieved by first considering the monochromatic case (this case arises, for instance, in all-nearest-neighbor search). The linear dependence on N arises from the fact that all query nodes must be visited. The dependence on the reference tree, however, is encapsulated by the term c4|R∗|, with |R∗| being the maximum size of the reference set R; this value must be derived for each specific problem. The bad performance of poorly-behaved datasets with large c (or, in the worst case, c ∼ N) is then captured in both of those terms. Poorly-behaved datasets may also have a high cover tree imbalance it(T ); the linear dependence of runtime on imbalance is thus sensible for well-behaved datasets.\nThe bichromatic case (Sq 6= Sr) is a slightly more complex result which deserves a bit more attention. The intuition for all terms except θ remain virtually the same.\nThe term θ captures the effect of query and reference datasets with different widths, and has one unfortunate corner case: when δq > ηr, then the query tree must be entirely descended before any reference recursion. This results in a bound of the form O(N log(ηr/δr)), or O(N2) (See Lemma 4). This is because the reference tree must be descended individually for each query point.\nThe quantity |R∗| bounds the amount of work that needs to be done for each recursion. In the worst case, |R∗| can be N . However, dual-tree algorithms rely on branch-and-bound techniques to prune away work (Lines 11 and 16 in Algorithm 1). A small value of |R∗| will imply that the algorithm is extremely successful in pruning away work. An (upper) bound on |R∗| (and the algorithm’s success in pruning work) will depend on the problem and the data. As we will show, bounding |R∗| is often possible. For many dual-tree algorithms,\nχ ∼ ψ ∼ O(1); often, cached sufficient statistics (Moore, 2000) can enable O(1) runtime implementations of BaseCase() and Score().\nThese results hold for any dual-tree algorithm regardless of the problem. Hence, the runtime of any dual-tree algorithm would be at least O(N) using our bound, which matches the intuition that answering O(N) queries would take at least O(N) time. For a particular problem and data, if cr, |R\n∗|, χ, ψ are bounded by constants independent of N and θ is no more than linear in N (for large enough N), then the dual-tree algorithm for that problem has a runtime linear in N . Our theoretical result separates out the problem-dependent and the problem-independent elements of the runtime bound, which allows us to simply plug in the problem-dependent bounds in order to get runtime bounds for any dual-tree algorithm without requiring an analysis from scratch.\nOur results are similar to that of Ram et al. (2009a), but those results depend on a quantity called the constant of bichromaticity, denoted κ, which has unclear relation to cover tree imbalance. The dependence on κ is given as c4κq , which is not a good bound, especially because κ may be much greater than 1 in the bichromatic case (where Sq 6= Sr).\nThe more recent results of Curtin and Ram (2014) are more related to these results, but they depend on the inverse constant of bichromaticity ν which suffers from the same problem as κ. Although the dependence on ν is linear (that is, O(νN)), bounding ν is difficult and it is not true that ν = 1 in the monochromatic case.\nν corresponds to the maximum number of reference recursions between a single query recursion, and κ corresponds to the maximum number of query recursions between a single reference recursion. The respective proofs that use these constants then apply them as a worst-case measure for the whole algorithm: when using κ, Ram et al. (2009a) assume that every reference recursion may be followed by κ query recursions; similarly, Curtin and Ram (2014) assume that every query recursion may be followed by ν reference recursions. Here, we have simply used it(Tq) and θ as an exact summation of the total extra reference recursions, which gives us a much tighter bound than ν or κ on the running time of the whole algorithm.\nFurther, both ν and κ are difficult to empirically calculate and require an entire run of the dual-tree algorithm. On the other hand, bounding it(Tq) (and θ) can be done in one pass of the tree (assuming the tree is already built). Thus, not only is our bound tighter when the cover tree imbalance is sublinear in N , it more closely reflects the actual behavior of dual-tree algorithms, and the constants which it depends upon are straightforward to calculate.\nIn the following sections, we will apply our results to specific problems and show the utility of our bound in simplifying runtime proofs for dual-tree algorithms."
    }, {
      "heading" : "5. Nearest neighbor search",
      "text" : "The standard task of nearest neighbor search can be simply described: given a query set Sq and a reference set Sr, for each query point pq ∈ Sq, find the nearest neighbor pr in the reference set Sr. The task is well-studied and well-known, and there exist numerous approaches for both exact and approximate nearest neighbor search, including the cover tree nearest neighbor search algorithm due to Beygelzimer et al. (2006). We will consider that algorithm, but in a tree-independent sense as given by Curtin et al. (2013b); this\nAlgorithm 2 Nearest neighbor search BaseCase()\nInput: query point pq, reference point pr, list of candidate neighbors N and distances D\nOutput: distance d between pq and pr\nif d(pq, pr) < D[pq] and BaseCase(pq, pr) not yet called then D[pq] ← d(pq, pr), and N [pq] ← pr end if return d(pq, pr)\nAlgorithm 3 Nearest neighbor search Score()\nInput: query node Nq, reference node Nr Output: a score for the node combination (Nq,Nr), or ∞ if the combination should be pruned\nif dmin(Nq,Nr) < B(Nq) then return dmin(Nq,Nr) end if return ∞\nmeans that to describe the algorithm, we require only a BaseCase() and Score() function; these are given in Algorithms 2 and 3, respectively. The point-to-point BaseCase() function compares a query point pq and a reference point pr, updating the list of candidate neighbors for pq if necessary.\nThe node-to-node Score() function determines if the entire subtree of nodes under the reference node Nr can improve the candidate neighbors for all descendant points of the query node Nq; if not, the node combination is pruned. The Score() function depends on the function dmin(·, ·), which represents the minimum possible distance between any two descendants of two nodes. Its definition for cover tree nodes is\ndmin(Nq,Nr) = d(pq, pr)− 2 sq+1 − 2sr+1. (15)\nGiven a type of tree and traversal, these two functions store the current nearest neighbor candidates in the array N and their distances in the array D. (See Curtin et al., 2013b, for a more complete discussion of how this algorithm works and a proof of correctness.) The Score() function depends on a bound function B(Nq) which represents the maximum distance that could possibly improve a nearest neighbor candidate for any descendant point of the query node Nq. The standard bound function B(Nq) used for cover trees is adapted from (Beygelzimer et al., 2006):\nB(Nq) := D[pq] + 2 sq+1 (16)\nIn this formulation, the query node Nq holds the the query point pq, the quantity D[pq] is the current nearest neighbor candidate distance for the query point pq, and 2 sq+1 corresponds to the furthest descendant distance of Nq. For notational convenience in the following proof, take cqr = max((maxpq∈Sq c ′ r), cr), where c ′ r is the expansion constant of the set Sr ∪ {pq}.\nTheorem 2 Using cover trees, the standard cover tree pruning dual-tree traversal, and the nearest neighbor search BaseCase() and Score() as given in Algorithms 2 and 3, respectively, and also given a reference set Sr with expansion constant cr, and a query set Sq, the running time of the algorithm is bounded by O(c4rc 5 qr(N + it(Tq) + θ)) with it(Tq and θ defined as in Definition 3 and Lemma 4, respectively.\nProof The running time of BaseCase() and Score() are clearly O(1). Due to Theorem 1, we therefore know that the runtime of the algorithm is bounded by O(c4r |R\n∗|(N+it(Tq)+θ)). Thus, the only thing that remains is to bound the maximum size of the reference set, |R∗|.\nAssume that when R∗ is encountered, the maximum reference scale is smaxr and the query node is Nq. Every node Nr ∈ R\n∗ satisfies the property enforced in line 11 that dmin(Nq,Nr) ≤ B(Nq). Using the definition of dmin(·, ·) and B(·), we expand the equation. Note that pq is the point held in Nq and pr is the point held in Nr. Also, take p̂r to be the current nearest neighbor candidate for pq; that is, D[pq] = d(pq, p̂r) and N [pq] = p̂r. Then,\ndmin(Nq,Nr) ≤ B(Nq) (17)\nd(pq, pr) ≤ d(pq, p̂r) + 2 sq+1 + 2sr+1 + 2sq+1 (18)\n≤ d(pq, p̂r) + 2(2 smaxr +1) (19)\nwhere the last step follows because sq + 1 ≤ s max r and sr ≤ s max r . Define the set of points P as the points held in each node in R∗ (that is, P = {pr ∈ P(Nr) : Nr ∈ R ∗}). Then, we can write\nP ⊆ BSr(pq, d(pq, p̂r) + 2(2 smaxr +1)). (20)\nSuppose that the true nearest neighbor is p∗r and d(pq, p ∗ r) > 2 smaxr +1. Then, p∗r must be held as a descendant point of some node in R∗ which holds some point p̃r. Using the triangle inequality,\nd(pq, p̂r) ≤ d(pq, p̃r) ≤ d(pq, p ∗ r) + d(p̃r, p ∗ r) ≤ d(pq, p ∗ r) + 2 smaxr +1. (21)\nThis gives that P ⊆ BSr∪{pq}(pq, d(pq, p ∗ r)+3(2 smaxr +1)). The previous step is necessary: to apply the definition of the expansion constant, the ball must be centered at a point in the set; now, the center (pq) is part of the set.\n|BSr∪{pq}(pq, d(pq, p ∗ r) + 3(2 smaxr +1))| ≤ |BSr∪{pq}(pq, 4d(pq, p ∗ r))| (22)\n≤ c3qr|BSr∪{pq}(pq, d(pq, p ∗ r)/2)| (23)\nwhich follows because the expansion constant of the set Sr ∪ {pq} is bounded above by cqr. Next, we know that p∗r is the closest point to pq in Sr ∪ {pq}; thus, there cannot exist a point p′r 6= pq ∈ Sr ∪ {pq} such that p ′ r ∈ BSqr(pq, d(pq, p ∗ r)/2) because that would imply that d(pq, p ′ r) < d(pq, p ∗ r), which is a contradiction. Thus, the only point in the ball is pq, and we have that |BSr∪{pq}(pq, d(pq, p ∗ r)/2)| = 1, giving the result that |R| ≤ c 3 qr in this case.\nThe other case is when d(pq, p ∗ r) ≤ 2 smaxr +1, which means that d(pq, p̂r) ≤ 2 smaxr +2. Note\nthat P ∈ Csmaxr , and therefore\nP ⊆ BSr(pq, d(pq, p ∗ r) + 3(2 smaxr +1)) ∩ Csmaxr (24)\n⊆ BSr(pq, 4(2 smaxr +1)) ∩Csmaxr . (25)\nEvery point in Csmaxr is separated by at least 2 smaxr . Using Lemma 1 with δ = 2s max r and\nρ = 8 yields that |P | ≤ c5r . This gives the result, because c 5 r ≤ c 5 qr.\nIn the monochromatic case where Sq = Sr, the bound is O(c 9(N + it(T )) because c = cr = cqr and θ = 0. For well-behaved trees where it(Tq) is linear or sublinear in N , this represents the current tightest worst-case runtime bound for nearest neighbor search."
    }, {
      "heading" : "6. Approximate kernel density estimation",
      "text" : "Ram et al. (2009a) present a clever technique for bounding the running time of approximate kernel density estimation based on the properties of the kernel, when the kernel is shiftinvariant and satisfies a few assumptions. We will restate these assumptions and provide an adapted proof using Theorem 1, which gives a tighter bound.\nApproximate kernel density estimation is a common application of dual-tree algorithms (Gray and Moore, 2003, 2001). Given a query set Sq, a reference set Sr of size N , and a kernel function K(·, ·), the true kernel density estimate for a query point pq is given as\nf∗(pq) = ∑\npr∈Sr\nK(pq, pr). (26)\nIn the case of an infinite-tailed kernel K(·, ·), the exact computation cannot be accelerated; thus, attention has turned towards tractable approximation schemes. Two simple schemes for the approximation of f∗(pq) are well-known: absolute value approximation and relative value approximation. Absolute value approximation requires that each density estimate f(pq) is within ǫ of the true estimate f ∗(pq):\n|f(pq)− f ∗(pq)| < ǫ ∀pq ∈ Sq. (27)\nRelative value approximation is a more flexible approximation scheme; given some parameter ǫ, the requirement is that each density estimate is within a relative tolerance of f∗(pq) :\n|f(pq)− f ∗(pq)|\n|f∗(pq)| < ǫ ∀pq ∈ Sq. (28)\nKernel density estimation is related to the well-studied problem of kernel summation, which can also be solved with dual-tree algorithms (Lee and Gray, 2006, 2009). In both of those problems, regardless of the approximation scheme, simple geometric observations can be made to accelerate computation: when K(·, ·) is shift-invariant, faraway points have very small kernel evaluations. Thus, trees can be built on Sq and Sr, and node combinations can be pruned when the nodes are far apart while still obeying the error bounds.\nAlgorithm 4 Approximate kernel density estimation BaseCase()\n1: Input: query point pq, reference point pr, list of kernel point estimates f̂p 2: Output: kernel value K(pq, pr)\n3: fp(pq) ← fp(pq) +K(pq, pr) 4: return K(pq, pr)\nIn the following two subsections, we will separately consider both the absolute value approximation scheme and the relative value approximation scheme, under the assumption of a shift-invariant kernel K(pq, pr) = K(‖pq − pr‖) which is monotonically decreasing and non-negative. In addition, we assume that there exists some bandwidth h such that K(d) must be concave for d ∈ [0, h] and convex for d ∈ [h,∞). This assumption implies that the magnitude of the derivative |K′(d)| is maximized at d = h. These are not restrictive assumptions; most standard kernels fall into this class, including the Gaussian, exponential, and Epanechnikov kernels."
    }, {
      "heading" : "6.1 Absolute value approximation",
      "text" : "A tree-independent algorithm for solving approximate kernel density estimation with absolute value approximation under the previous assumptions on the kernel is given as a BaseCase() function in Algorithm 4 and a Score() function in Algorithm 5 (a correctness proof can be found in Curtin et al., 2013b). The list fp holds partial kernel density estimates for each query point, and the list fn holds partial kernel density estimates for each query node. At the beginning of the dual-tree traversal, the lists fp and fn, which are both of size O(N), are each initialized to 0. As the traversal proceeds, node combinations are pruned if the difference between the maximum kernel value K(dmin(Nq,Nr)) and the minimum kernel value K(dmax(Nq,Nr)) is sufficiently small (line 3). If the node combination can be pruned, then the partial node estimate is updated (line 4). When node combinations cannot be pruned, BaseCase() may be called, which simply updates the partial point estimate with the exact kernel evaluation (line 3).\nAfter the dual-tree traversal, the actual kernel density estimates f must be extracted. This can be done by traversing the query tree and calculating f(pq) = fp(pq)+ ∑\nNi∈T fn(Ni),\nwhere T is the set of nodes in Tq that have pq as a descendant. Each query node needs to be visited only once to perform this calculation; it may therefore be accomplished in O(N) time.\nNote that this version is far simpler than other dual-tree algorithms that have been proposed for approximate kernel density estimation (see, for instance Gray and Moore, 2003); however, this version is sufficient for our runtime analysis. Real-world implementations, such as the one found in mlpack (Curtin et al., 2013a), tend to be far more complex.\nTheorem 3 Assume that K(·, ·) is a kernel satisfying the assumptions of the previous subsection. Then, given a query set Sq and a reference set Sr with expansion constant cr, and using the approximate kernel density estimation BaseCase() and Score() as given in Algorithms 4 and 5, respectively, with the traversal given in Algorithm 1, the running time of approximate kernel density estimation for some error parameter ǫ is bounded by\nAlgorithm 5 Absolute-value approximate kernel density estimation Score()\n1: Input: query node Nq, reference node Nr, list of node kernel estimates f̂n 2: Output: a score for the node combination (Nq,Nr), or ∞ if the combination should\nbe pruned\n3: if K(dmin(Nq,Nr))−K(dmax(Nq,Nr)) < ǫ then 4: fn(Nq) ← fn(Nq) + |D\np(Nr)| (K(dmin(Nq,Nr)) +K(dmax(Nq,Nr))) / 2 5: return ∞ 6: end if 7: return K(dmin(Nq,Nr))−K(dmax(Nq,Nr))\nO(c 8+⌈log 2 ζ⌉ r (N + it(Tq) + θ)) with ζ = −K ′(h)K−1(ǫ)ǫ−1, it(Tq) defined as in Definition 3, and θ defined as in Lemma 4.\nProof It is clear that BaseCase() and Score() both take O(1) time, so Theorem 1 implies the total runtime of the dual-tree algorithm is bounded by O(c4r |R\n∗|(N+ it(Tq)+θ)). Thus, we will bound |R∗| using techniques related to those used by Ram et al. (2009a). The bounding of |R∗| is split into two sections: first, we show that when the scale smaxr is small enough, R∗ is empty. Second, we bound R∗ when smaxr is larger.\nThe Score() function is such that any node in R∗ for a given query node Nq obeys\nK(dmin(Nq,Nr))−K(dmax(Nq,Nr)) ≥ ǫ. (29)\nThus, we are interested in the maximum possible value K(a) − K(b) for a fixed value of b− a > 0. Due to our assumptions, the maximum value of K′(·) is K′(h); therefore, the maximum possible value of K(a) − K(b) is when the interval [a, b] is centered on h. This allows us to say that K(a)−K(b) ≤ ǫ when (b− a) ≤ (−ǫ/K′(h)). Note that\ndmax(Nq,Nr)− dmin(Nq,Nr) ≤ d(pq, pr) + 2 smaxr +1 − d(pq, pr) + 2 smaxr +1 (30)\n≤ 2s max r +2. (31)\nTherefore, R∗ = ∅ when 2s max r +2 ≤ −ǫ/K′(h), or when smaxr ≤ log2(−ǫ/K ′(h)) − 2. Consider, then, the case when smaxr > log2(−ǫ/K\n′(h)) − 2. Because of the pruning rule, for any Nr ∈ R\n∗, K(dmin(Nq,Nr)) > ǫ; we may refactor this by applying definitions to show d(pq, pr) < K −1(ǫ) + 2s max r +1. Therefore, bounding the number of points in the set BSr(pq,K −1(ǫ) + 2s max r +1) ∩ Csmaxr is sufficient to bound |R\n∗|. For notational convenience, define ω = (K−1(ǫ)/2s max r +1) + 1, and the statement may be more concisely written as BSr(pq, ω2 smaxr +1) ∩ Csmaxr .\nUsing Lemma 1 with δ = 2s max r and ρ = 2ω gives |R∗| = c 3+⌈log 2 ω⌉\nr . The value ω is maximized when smaxr is minimized. Using the lower bound on s max r , ω is bounded as ω = −2K′(h)K−1(ǫ)ǫ−1. Finally, with ζ = −K′(h)K−1(ǫ)ǫ−1, we are able to conclude that |R∗| ≤ c 3+⌈log 2 (2ζ)⌉ r = c 4+⌈log 2 ζ⌉ r . Therefore, the entire dual-tree traversal takes O(c 8+⌈log 2 ζ⌉\nr (N + θ)) time. The postprocessing step to extract the estimates f(·) requires one traversal of the tree Tr; the tree has O(N) nodes, so this takes only O(N) time. This is less than the runtime of\nthe dual-tree traversal, so the runtime of the dual-tree traversal dominates the algorithm’s runtime, and the theorem holds.\nThe dependence on ǫ (through ζ) is expected: as ǫ → 0 and the search becomes exact, ζ diverges both because ǫ−1 diverges and also because K−1(ǫ) diverges, and the runtime goes to the worst-case O(N2); exact kernel density estimation means no nodes can be pruned at all.\nFor the Gaussian kernel with bandwidth σ defined by Kg(d) = exp(−d 2/(2σ2)), ζ does not depend on the kernel bandwidth; only the approximation parameter ǫ. For this kernel, h = σ and therefore −K′g(h) = σ −1e−1/2. Additionally, K−1g (ǫ) = σ √ 2 ln(1/ǫ). This means that for the Gaussian kernel, ζ = √\n(−2 ln ǫ)/(eǫ2). Again, as ǫ → 0, the runtime diverges; however, note that there is no dependence on the kernel bandwidth σ. To demonstrate the relationship of runtime to ǫ, see that for a reasonably chosen ǫ = 0.05, the runtime is approximately O(c8.89r (N +θ)); for ǫ = 0.01, the runtime is approximately O(c 11.52 r (N +θ)). For very small ǫ = 0.00001, the runtime is approximately O(c22.15r (N + θ)).\nNext, consider the exponential kernel: Kl(d) = exp(−d/σ). For this kernel, h = 0 (that is, the kernel is always convex), so then K′l(h) = σ\n−1. Simple algebraic manipulation gives K−1l (ǫ) = −σ ln ǫ, resulting in ζ = −K ′ l(h)K −1 l (ǫ)ǫ\n−1 = ǫ−1 ln ǫ. So both the exponential and Gaussian kernels do not exhibit dependence on the bandwidth.\nTo understand the lack of dependence on kernel bandwidth more intuitively, consider that as the kernel bandwidth increases, two things happen: (a) the reference set R becomes empty at larger scales, and (b) K−1(ǫ) grows, allowing less pruning at higher levels. These effects are opposite, and for the Gaussian and exponential kernels they cancel each other out, giving the same bound regardless of bandwidth."
    }, {
      "heading" : "6.2 Relative Value Approximation",
      "text" : "Approximate kernel density estimation using relative-value approximation may be bounded by reducing the absolute-value approximation algorithm (in linear time or less) to relativevalue approximation. This is the same strategy as performed by Ram et al. (2009a).\nFirst, we must establish a Score() function for relative value approximation. The difference between Equations 27 and 28 is the division by the term |f∗(pq)|. But we can quickly bound |f∗(pq)|:\n|f∗(pq)| ≥ NK\n(\nmax pr∈Sr d(pq, pr)\n)\n. (32)\nThis is clearly true: each point in Sr must contribute more than K(maxpr∈Sr d(pq, pr)) to f∗(pq). Now, we may revise the relative approximation condition in Equation 28:\n|f(pq)− f ∗(pq)| ≤ ǫK max (33)\nwhere Kmax is lower bounded by K(maxpr∈Sr d(pq, pr)). Assuming we have some estimate Kmax, this allows us to create a Score() algorithm, given in Algorithm 6.\nUsing this, we may prove linear runtime bounds for relative value approximate kernel density estimation.\nAlgorithm 6 Relative-value approximate kernel density estimation Score()\n1: Input: query node Nq, reference node Nr, list of node kernel estimates f̂n 2: Output: a score for the node combination (Nq,Nr), or ∞ if the combination should\nbe pruned\n3: if K(dmin(Nq,Nr))−K(dmax(Nq,Nr)) < ǫK max then 4: fn(Nq) ← fn(Nq) + |D p(Nr)| (K(dmin(Nq,Nr)) +K(dmax(Nq,Nr))) / 2 5: return ∞ 6: end if 7: return K(dmin(Nq,Nr))−K(dmax(Nq,Nr))\nTheorem 4 Assume that K(·, ·) is a kernel satisfying the same assumptions as Theorem 3. Then, given a query set Sq and a reference set Sr both of size O(N), it is possible to perform relative value approximate kernel density estimation (satisfying the condition of Equation 28) in O(N) time, assuming that the expansion constant cr of Sr is not dependent on N .\nProof It is easy to see that Theorem 3 may be adapted to the very slightly different Score() rule of Algorithm 6 while still providing an O(N) bound. With that Score() function, the dual-tree algorithm will return relative-value approximate kernel density estimates satisfying Equation 28.\nWe now turn to the calculation of Kmax. Given the cover trees Tq and Tr with root nodes N Rr and N R r , respectively, we may calculate a suitable K max value in constant time:\nKmax = dmax(N R q ,N R r ) = d(p R q , p R r ) + 2 smaxq +1 + 2s max r +1. (34)\nThis proves the statement of the theorem.\nIn this case, we have not shown tighter bounds because the algorithm we have proposed is not useful in practice. For an example of a better relative-value approximate kernel density estimation dual-tree algorithm, see the work of Gray and Moore (2003)."
    }, {
      "heading" : "7. Range search and range count",
      "text" : "In the range search problem, the task is to find the set of reference points\nS[pq] = {pr ∈ Sr : d(pq, pr) ∈ [l, u]} (35)\nfor each query point pq, where [l, u] is the given range. The range count problem is practically identical, but only the size of the set, |S[pq]|, is desired. Our proof works for both of these algorithms similarly, but we will focus on range search. A BaseCase() and Score() function are given in Algorithms 7 and 8, respectively (a correctness proof can be found in Curtin et al., 2013b). The sets N [pq] (for each pq) are initialized to ∅ at the beginning of the traversal.\nIn order to bound the running time of dual-tree range search, we require better notions for understanding the difficulty of the problem. Observe that if the range is sufficiently large, then for every query point pq, S[pq] = Sr. Clearly, for Sq ∼ Sr ∼ O(N), this cannot\nAlgorithm 7 Range search BaseCase()\n1: Input: query point pq, reference point pr, range sets N [pq] and range [l, u] 2: Output: distance d between pq and pr 3: if d(pq, pr) ∈ [rmin, rmax] and BaseCase(pq, pr) not yet called then 4: S[pq] ← S[pq] ∪ {pr} 5: end if 6: return d\nAlgorithm 8 Range search Score()\n1: Input: query node Nq, reference node Nr 2: Output: a score for the node combination (Nq,Nr), or ∞ if the combination should\nbe pruned\n3: if dmin(Nq,Nr) ∈ [l, u] or dmax(Nq,Nr) ∈ [l, u] then 4: return dmin(Nq,Nr) 5: end if 6: return ∞\nbe solved in anything less than quadratic time simply due to the time required to fill each output array S[pq]. Define the maximum result size for a given query set Sq, reference set Sr, and range [l, u] as\n|Smax| = max pq∈Sq |S[pq]|. (36)\nSmall |Smax| implies an easy problem; large |Smax| implies a difficult problem. For bounding the running time of range search, we require one more notion of difficulty, related to how |Smax| changes due to changes in the range [l, u].\nDefinition 4 For a range search problem with query set Sq, reference set Sr, range [l, u], and results S[pq] for each query point pq given as\nS[pq] = {pr : pr ∈ Sr, l ≤ d(pq, pr) ≤ u}, (37)\ndefine the α-expansion of the range set S[pq] as the slightly larger set\nSα[pq] = {pr : pr ∈ Sr, (1− α)l ≤ d(pq, pr) ≤ (1 + α)u}. (38)\nWhen the α-expansion of the set Smax is approximately the same size as Smax, then the problem would not be significantly more difficult if the range [l, u] was increased slightly. Using these notions, then, we may now bound the running time of range search.\nTheorem 5 Given a reference set Sr of size N with expansion constant cr, and a query set Sq of size O(N), a search range of [l, u], and using the range search BaseCase() and Score() as given in Algorithms 7 and 8, respectively, with the standard cover tree pruning dual-tree traversal as given in Algorithm 1, and also assuming that for some α > 0,\n|Sα[pq] \\ S[pq]| ≤ C ∀ pq ∈ Sq, (39)\nthe running time of range search or range count is bounded by\nO ( c4r max ( c4+βr , |Smax|+ C ) (N + it(Nq) + θ) )\n(40)\nwith θ defined as in Lemma 4, β = ⌈log2(1 + α −1)⌉, and Smax as defined in Equation\n36.\nProof Both BaseCase() (Algorithm 7) and Score() (Algorithm 8) take O(1) time. Therefore, using Lemma 1, we know that the runtime of the algorithm is bounded by O(c4r |R\n∗|(N + it(Nq) + θ)). As with the previous proofs, then, our only task is to bound the maximum size of the reference set, |R∗|.\nBy the pruning rule, for a query node Nq, the reference set R ∗ is made up of reference\nnodes Nr that are within a margin of 2 sq+1+2sr+1 ≤ 2s max r +2 of the range [l, u]. Given that pr is the point in Nr,\npr ∈ ( BSr(pq, u+ 2 smaxr +2) ∩ Csmaxr ) \\ ( BSr(pq, l − 2 smaxr +2) ∩ Csmaxr ) . (41)\nA bound on the number of elements in this set is a bound on |R∗|. First, consider the case where u ≤ α−12s max r +2. Ignoring the smaller ball, take δ = 2s max r and ρ = 4(1 + α−1) and apply Lemma 1 to produce the bound\n|R∗| ≤ c 4+⌈log 2 (1+α−1)⌉\nr . (42)\nNow, consider the other case: u > α−12s max r +1. This means\nBSr(pq, u+ 2 smaxr +1) \\BSr(pq, l − 2 smaxr +1) ⊆ BSr(pq, (1 + α)u) \\BSr(pq, (1 − α)l). (43)\nThis set is necessarily a subset of Sα[pq]; by assumption, the number of points in this set is bounded above by |Smax| + C. We may then conclude that |R\n∗| ≤ |Smax| + C. By taking the maximum of the sizes of |R∗| in both cases above, we obtain the statement of the theorem.\nThis bound displays both the expected dependence on cr and |Smax|. As the largest range set Smax increases in size (with the worst case being Smax ∼ N), the runtime degenerates to quadratic. But for adequately small Smax the runtime is instead dependent on cr and the parameter C of the α-expansion of Smax. This situation leads to a simplification.\nCorollary 2 For sufficiently small |Smax| and sufficiently small C, the runtime of range search under the conditions of Theorem 5 simplifies to\nO(c8+βr (N + it(Nq) + θ)). (44)\nIn this setting we can more easily consider the relation of the running time to α. Consider α = (1/3); this yields a running time of O(c8(N+θ)). α = (1/7) yields O(c9(N+it(Nq+θ)),\nα = (1/15) yields O(c10(N + it(Nq)+ θ)), and so forth. As α gets smaller, the exponent on c gets larger, and diverges as α → 0.\nFor reasonable runtime it is necessary that the α-expansion of Smax be bounded. This is because the dual-tree recursion must retain reference nodes which may contain descendants in the range set S[pq] for some query pq. The parameter C of the α-expansion allows us to bound the number of reference nodes of this type, and if α increases but C remains small enough that Corollary 2 applies, then we are able to obtain tighter running bounds."
    }, {
      "heading" : "8. Conclusion",
      "text" : "We have presented a unified framework for bounding the runtimes of dual-tree algorithms that use cover trees and the standard cover tree pruning dual-tree traversal (Algorithm 1). In order to produce an understandable bound, we have introduced the notion of cover tree imbalance; one possible interesting direction of future work is to empirically and theoretically minimize this quantity by way of modified tree construction algorithms; this is likely to provide both tighter runtime bounds and also accelerated empirical results.\nOur main result, Theorem 1, allows plug-and-play runtime bounding of these algorithms. We have shown that Theorem 1 is useful for bounding the runtime of nearest neighbor search (Theorem 2), approximate kernel density estimation (Theorem 3), exact range count, and exact range search (Theorem 5). With our contribution, bounding a cover tree dual-tree algorithm is straightforward and only involves bounding the maximum size of the reference set, |R∗|."
    } ],
    "references" : [ {
      "title" : "Variational dual-tree framework for largescale transition matrix approximation",
      "author" : [ "S. Amizadeh", "B. Thiesson", "M. Hauskrecht" ],
      "venue" : "In Proceedings of the Twenty-Eighth Annual Conference on Uncertainty in Artificial Intelligence",
      "citeRegEx" : "Amizadeh et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Amizadeh et al\\.",
      "year" : 2012
    }, {
      "title" : "Cover trees for nearest neighbor",
      "author" : [ "A. Beygelzimer", "S.M. Kakade", "J. Langford" ],
      "venue" : "In Proceedings of the 23rd International Conference on Machine Learning (ICML",
      "citeRegEx" : "Beygelzimer et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Beygelzimer et al\\.",
      "year" : 2006
    }, {
      "title" : "Review of ‘Phylogenetics: The Theory and Practice of Phylogenetic Systematics’, by E.O",
      "author" : [ "D.H. Colless" ],
      "venue" : "Wiley. Systematic Zoology,",
      "citeRegEx" : "Colless.,? \\Q1982\\E",
      "shortCiteRegEx" : "Colless.",
      "year" : 1982
    }, {
      "title" : "Dual-tree fast exact max-kernel search",
      "author" : [ "R.R. Curtin", "P. Ram" ],
      "venue" : "Statistical Analysis and Data Mining,",
      "citeRegEx" : "Curtin and Ram.,? \\Q2014\\E",
      "shortCiteRegEx" : "Curtin and Ram.",
      "year" : 2014
    }, {
      "title" : "MLPACK: A scalable C++ machine learning library",
      "author" : [ "R.R. Curtin", "J.R. Cline", "N.P. Slagle", "W.B. March", "P. Ram", "N.A. Mehta", "A.G. Gray" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Curtin et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Curtin et al\\.",
      "year" : 2013
    }, {
      "title" : "Treeindependent dual-tree algorithms",
      "author" : [ "R.R. Curtin", "W.B. March", "P. Ram", "D.V. Anderson", "A.G. Gray", "C.L. Isbell Jr." ],
      "venue" : "In Proceedings of The 30th International Conference on Machine Learning (ICML",
      "citeRegEx" : "Curtin et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Curtin et al\\.",
      "year" : 2013
    }, {
      "title" : "Fast exact max-kernel search",
      "author" : [ "R.R. Curtin", "P. Ram", "A.G. Gray" ],
      "venue" : "In Proceedings of the 13th SIAM International Conference on Data Mining (SDM",
      "citeRegEx" : "Curtin et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Curtin et al\\.",
      "year" : 2013
    }, {
      "title" : "Quad trees a data structure for retrieval on composite keys",
      "author" : [ "R.A. Finkel", "J.L. Bentley" ],
      "venue" : "Acta Informatica,",
      "citeRegEx" : "Finkel and Bentley.,? \\Q1974\\E",
      "shortCiteRegEx" : "Finkel and Bentley.",
      "year" : 1974
    }, {
      "title" : "N-body problems in statistical learning",
      "author" : [ "A.G. Gray", "A.W. Moore" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Gray and Moore.,? \\Q2000\\E",
      "shortCiteRegEx" : "Gray and Moore.",
      "year" : 2000
    }, {
      "title" : "Nonparametric density estimation: Toward computational tractability",
      "author" : [ "A.G. Gray", "A.W. Moore" ],
      "venue" : "In Proceedings of the 3rd SIAM International Conference on Data Mining (SDM",
      "citeRegEx" : "Gray and Moore.,? \\Q2003\\E",
      "shortCiteRegEx" : "Gray and Moore.",
      "year" : 2003
    }, {
      "title" : "Finding nearest neighbors in growth-restricted metrics",
      "author" : [ "D.R. Karger", "M. Ruhl" ],
      "venue" : "In Proceedings of the Thirty-Fourth Annual ACM Symposium on Theory of Computing (STOC",
      "citeRegEx" : "Karger and Ruhl.,? \\Q2002\\E",
      "shortCiteRegEx" : "Karger and Ruhl.",
      "year" : 2002
    }, {
      "title" : "Fast particle smoothing: if I had a million particles",
      "author" : [ "M. Klaas", "M. Briers", "N. De Freitas", "A. Doucet", "S. Maskell", "D. Lang" ],
      "venue" : "In Proceedings of the 23rd International Conference on Machine Learning (ICML",
      "citeRegEx" : "Klaas et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Klaas et al\\.",
      "year" : 2006
    }, {
      "title" : "Navigating nets: simple algorithms for proximity search",
      "author" : [ "R. Krauthgamer", "J.R. Lee" ],
      "venue" : "In Proceedings of the Fifteenth Annual ACM-SIAM Symposium on Discrete Algorithms",
      "citeRegEx" : "Krauthgamer and Lee.,? \\Q2004\\E",
      "shortCiteRegEx" : "Krauthgamer and Lee.",
      "year" : 2004
    }, {
      "title" : "Faster Gaussian summation: Theory and Experiment",
      "author" : [ "D. Lee", "A.G. Gray" ],
      "venue" : "In Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence",
      "citeRegEx" : "Lee and Gray.,? \\Q2006\\E",
      "shortCiteRegEx" : "Lee and Gray.",
      "year" : 2006
    }, {
      "title" : "Fast high-dimensional kernel summations using the monte carlo multipole method",
      "author" : [ "D. Lee", "A.G. Gray" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Lee and Gray.,? \\Q2008\\E",
      "shortCiteRegEx" : "Lee and Gray.",
      "year" : 2008
    }, {
      "title" : "Multi-tree algorithms for computational statistics and physics",
      "author" : [ "W.B. March" ],
      "venue" : "PhD thesis, Georgia Institute of Technology,",
      "citeRegEx" : "March.,? \\Q2013\\E",
      "shortCiteRegEx" : "March.",
      "year" : 2013
    }, {
      "title" : "Fast Euclidean minimum spanning tree: algorithm, analysis, and applications",
      "author" : [ "W.B. March", "P. Ram", "A.G. Gray" ],
      "venue" : "In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD",
      "citeRegEx" : "March et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "March et al\\.",
      "year" : 2010
    }, {
      "title" : "Fast algorithms for comprehensive n-point correlation estimates",
      "author" : [ "W.B. March", "A.J. Connolly", "A.G. Gray" ],
      "venue" : "In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD",
      "citeRegEx" : "March et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "March et al\\.",
      "year" : 2012
    }, {
      "title" : "The Anchors hierarchy: Using the triangle inequality to survive high dimensional data",
      "author" : [ "A.W. Moore" ],
      "venue" : "In Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence",
      "citeRegEx" : "Moore.,? \\Q2000\\E",
      "shortCiteRegEx" : "Moore.",
      "year" : 2000
    }, {
      "title" : "Fast Gaussian process posteriors with product trees",
      "author" : [ "D.A. Moore", "S.J. Russell" ],
      "venue" : "In Proceedings of the Thirtieth Conference on Uncertainty in Artificial Intelligence (UAI14),",
      "citeRegEx" : "Moore and Russell.,? \\Q2014\\E",
      "shortCiteRegEx" : "Moore and Russell.",
      "year" : 2014
    }, {
      "title" : "Linear-time algorithms for pairwise statistical problems",
      "author" : [ "P. Ram", "D. Lee", "W.B. March", "A.G. Gray" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Ram et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Ram et al\\.",
      "year" : 2009
    }, {
      "title" : "Rank-approximate nearest neighbor search: Retaining meaning and speed in high dimensions",
      "author" : [ "P. Ram", "D. Lee", "H. Ouyang", "A.G. Gray" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Ram et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Ram et al\\.",
      "year" : 2009
    }, {
      "title" : "Accelerating t-sne using tree-based algorithms",
      "author" : [ "L. Van Der Maaten" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Maaten.,? \\Q2014\\E",
      "shortCiteRegEx" : "Maaten.",
      "year" : 2014
    }, {
      "title" : "Fast mean shift with accurate and stable convergence",
      "author" : [ "P. Wang", "D. Lee", "A.G. Gray", "J.M. Rehg" ],
      "venue" : "In Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics (AISTATS",
      "citeRegEx" : "Wang et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "The reference set is typically indexed with spatial data structures to accelerate this type of computation (Finkel and Bentley, 1974; Beygelzimer et al., 2006); these result in O(logN) runtime per query under favorable conditions.",
      "startOffset" : 107,
      "endOffset" : 159
    }, {
      "referenceID" : 1,
      "context" : "The reference set is typically indexed with spatial data structures to accelerate this type of computation (Finkel and Bentley, 1974; Beygelzimer et al., 2006); these result in O(logN) runtime per query under favorable conditions.",
      "startOffset" : 107,
      "endOffset" : 159
    }, {
      "referenceID" : 9,
      "context" : "There exist numerous dual-tree algorithms for problems as diverse as kernel density estimation (Gray and Moore, 2003), mean shift (Wang et al.",
      "startOffset" : 95,
      "endOffset" : 117
    }, {
      "referenceID" : 23,
      "context" : "There exist numerous dual-tree algorithms for problems as diverse as kernel density estimation (Gray and Moore, 2003), mean shift (Wang et al., 2007), minimum spanning tree calculation (March et al.",
      "startOffset" : 130,
      "endOffset" : 149
    }, {
      "referenceID" : 16,
      "context" : ", 2007), minimum spanning tree calculation (March et al., 2010), n-point correlation function estimation (March et al.",
      "startOffset" : 43,
      "endOffset" : 63
    }, {
      "referenceID" : 17,
      "context" : ", 2010), n-point correlation function estimation (March et al., 2012), max-kernel search (Curtin et al.",
      "startOffset" : 49,
      "endOffset" : 69
    }, {
      "referenceID" : 11,
      "context" : ", 2013c), particle smoothing (Klaas et al., 2006), variational inference (Amizadeh et al.",
      "startOffset" : 29,
      "endOffset" : 49
    }, {
      "referenceID" : 0,
      "context" : ", 2006), variational inference (Amizadeh et al., 2012), range search (Gray and Moore, 2001), and embedding techniques Van Der Maaten (2014), to name a few.",
      "startOffset" : 31,
      "endOffset" : 54
    }, {
      "referenceID" : 1,
      "context" : "Some of these algorithms are derived using the cover tree (Beygelzimer et al., 2006), a data structure with compelling theoretical qualities.",
      "startOffset" : 58,
      "endOffset" : 84
    }, {
      "referenceID" : 16,
      "context" : ", 2009a); minimum spanning tree calculation scales as O(N logN) (March et al., 2010).",
      "startOffset" : 64,
      "endOffset" : 84
    }, {
      "referenceID" : 3,
      "context" : "Other problems have similar worst-case guarantees (Curtin and Ram, 2014; March, 2013).",
      "startOffset" : 50,
      "endOffset" : 85
    }, {
      "referenceID" : 15,
      "context" : "Other problems have similar worst-case guarantees (Curtin and Ram, 2014; March, 2013).",
      "startOffset" : 50,
      "endOffset" : 85
    }, {
      "referenceID" : 0,
      "context" : "The reference set is typically indexed with spatial data structures to accelerate this type of computation (Finkel and Bentley, 1974; Beygelzimer et al., 2006); these result in O(logN) runtime per query under favorable conditions. Building upon this intuition, Gray and Moore (2001) generalized the fast multipole method from computational physics to obtain dual-tree algorithms.",
      "startOffset" : 134,
      "endOffset" : 283
    }, {
      "referenceID" : 0,
      "context" : "The reference set is typically indexed with spatial data structures to accelerate this type of computation (Finkel and Bentley, 1974; Beygelzimer et al., 2006); these result in O(logN) runtime per query under favorable conditions. Building upon this intuition, Gray and Moore (2001) generalized the fast multipole method from computational physics to obtain dual-tree algorithms. These are extremely useful when there are large query sets, not just a few query points. Instead of building a tree on the reference set and searching with each query point separately, Gray and Moore suggest also building a query tree and traversing both the query and reference trees simultaneously (a dual-tree traversal, from which the class of algorithms takes its name). Dual-tree algorithms can be easily understood through the recent framework of Curtin et al. (2013b): two trees (a query tree and a reference tree) are traversed by a pruning dual-tree traversal.",
      "startOffset" : 134,
      "endOffset" : 856
    }, {
      "referenceID" : 0,
      "context" : ", 2006), variational inference (Amizadeh et al., 2012), range search (Gray and Moore, 2001), and embedding techniques Van Der Maaten (2014), to name a few.",
      "startOffset" : 32,
      "endOffset" : 140
    }, {
      "referenceID" : 0,
      "context" : ", 2006), variational inference (Amizadeh et al., 2012), range search (Gray and Moore, 2001), and embedding techniques Van Der Maaten (2014), to name a few. Some of these algorithms are derived using the cover tree (Beygelzimer et al., 2006), a data structure with compelling theoretical qualities. When cover trees are used, Dual-tree all-nearest-neighbor search and approximate kernel density estimation have O(N) runtime guarantees for O(N) queries (Ram et al., 2009a); minimum spanning tree calculation scales as O(N logN) (March et al., 2010). Other problems have similar worst-case guarantees (Curtin and Ram, 2014; March, 2013). In this work we combine the generalization of Curtin et al. (2013b) with the theoretical results of Beygelzimer et al.",
      "startOffset" : 32,
      "endOffset" : 703
    }, {
      "referenceID" : 0,
      "context" : ", 2006), variational inference (Amizadeh et al., 2012), range search (Gray and Moore, 2001), and embedding techniques Van Der Maaten (2014), to name a few. Some of these algorithms are derived using the cover tree (Beygelzimer et al., 2006), a data structure with compelling theoretical qualities. When cover trees are used, Dual-tree all-nearest-neighbor search and approximate kernel density estimation have O(N) runtime guarantees for O(N) queries (Ram et al., 2009a); minimum spanning tree calculation scales as O(N logN) (March et al., 2010). Other problems have similar worst-case guarantees (Curtin and Ram, 2014; March, 2013). In this work we combine the generalization of Curtin et al. (2013b) with the theoretical results of Beygelzimer et al. (2006) and others in order to develop a worst-case runtime bound for any dual-tree algorithm when the cover tree is used.",
      "startOffset" : 32,
      "endOffset" : 761
    }, {
      "referenceID" : 4,
      "context" : "See Curtin et al. (2013b) for details.",
      "startOffset" : 4,
      "endOffset" : 26
    }, {
      "referenceID" : 1,
      "context" : "(2013b), but the only type of tree we will consider is the cover tree (Beygelzimer et al., 2006), and the only type of traversal we will consider is the cover tree pruning dual-tree traversal, which we will describe later.",
      "startOffset" : 70,
      "endOffset" : 96
    }, {
      "referenceID" : 3,
      "context" : "For simplicity, the algorithms considered in this paper will be presented in a tree-independent context, as in Curtin et al. (2013b), but the only type of tree we will consider is the cover tree (Beygelzimer et al.",
      "startOffset" : 111,
      "endOffset" : 133
    }, {
      "referenceID" : 1,
      "context" : "The cover tree is a leveled hierarchical data structure originally proposed for the task of nearest neighbor search by Beygelzimer et al. (2006). Each node Ni in the cover tree is associated with a single point pi.",
      "startOffset" : 119,
      "endOffset" : 145
    }, {
      "referenceID" : 1,
      "context" : "Beygelzimer et al. (2006) find this representation (which they call the implicit representation) easier for description of their algorithms and some of their proofs.",
      "startOffset" : 0,
      "endOffset" : 26
    }, {
      "referenceID" : 10,
      "context" : "The explicit representation of a cover tree has a number of useful theoretical properties based on the expansion constant (Karger and Ruhl, 2002); we restate its definition below.",
      "startOffset" : 122,
      "endOffset" : 145
    }, {
      "referenceID" : 10,
      "context" : "It is, in some sense, a notion of instrinic dimensionality, and previous work has shown that there are many scenarios where c is independent of the number of points in the dataset (Karger and Ruhl, 2002; Beygelzimer et al., 2006; Krauthgamer and Lee, 2004; Ram et al., 2009a).",
      "startOffset" : 180,
      "endOffset" : 275
    }, {
      "referenceID" : 1,
      "context" : "It is, in some sense, a notion of instrinic dimensionality, and previous work has shown that there are many scenarios where c is independent of the number of points in the dataset (Karger and Ruhl, 2002; Beygelzimer et al., 2006; Krauthgamer and Lee, 2004; Ram et al., 2009a).",
      "startOffset" : 180,
      "endOffset" : 275
    }, {
      "referenceID" : 12,
      "context" : "It is, in some sense, a notion of instrinic dimensionality, and previous work has shown that there are many scenarios where c is independent of the number of points in the dataset (Karger and Ruhl, 2002; Beygelzimer et al., 2006; Krauthgamer and Lee, 2004; Ram et al., 2009a).",
      "startOffset" : 180,
      "endOffset" : 275
    }, {
      "referenceID" : 1,
      "context" : "Closed-form solutions for cf for more complex distributions are less easy to derive; however, empirical speedup results from Beygelzimer et al. (2006) suggest the existence of datasets where c is not strongly dependent on d.",
      "startOffset" : 125,
      "endOffset" : 151
    }, {
      "referenceID" : 1,
      "context" : "A batch construction algorithm is given by Beygelzimer et al. (2006), called Construct.",
      "startOffset" : 43,
      "endOffset" : 69
    }, {
      "referenceID" : 1,
      "context" : "1, Beygelzimer et al. (2006)).",
      "startOffset" : 3,
      "endOffset" : 29
    }, {
      "referenceID" : 1,
      "context" : "1, Beygelzimer et al. (2006)). • Depth bound: the maximum depth of any node isO(c2 logN) (Lemma 4.3, Beygelzimer et al. (2006)).",
      "startOffset" : 3,
      "endOffset" : 127
    }, {
      "referenceID" : 1,
      "context" : "• Space bound: a cover tree has O(N) nodes (Theorem 1, Beygelzimer et al. (2006)).",
      "startOffset" : 55,
      "endOffset" : 81
    }, {
      "referenceID" : 1,
      "context" : "Lastly, we introduce a convenience lemma of our own which is a generalization of the packing arguments used by Beygelzimer et al. (2006). This is a more flexible version of their argument.",
      "startOffset" : 111,
      "endOffset" : 137
    }, {
      "referenceID" : 1,
      "context" : "1 in Beygelzimer et al. (2006). Consider two cases: first, let d(p, pi) > ρδ for any pi ∈ S.",
      "startOffset" : 5,
      "endOffset" : 31
    }, {
      "referenceID" : 2,
      "context" : "Numerous measures of tree imbalance have already been established; one example is that proposed by Colless (1982), and another is Sackin’s index (Sackin, 1972), but we aim to capture a different measure of imbalance that utilizes the leveled structure of the cover tree.",
      "startOffset" : 99,
      "endOffset" : 114
    }, {
      "referenceID" : 1,
      "context" : ", 2013a) and the reference implementation (Beygelzimer et al., 2006), the tree imbalance is near-linear with the number of points.",
      "startOffset" : 42,
      "endOffset" : 68
    }, {
      "referenceID" : 16,
      "context" : ", 2006), they can be adapted to a wide variety of problems: minimum spanning tree calculation (March et al., 2010), approximate nearest neighbor search (Ram et al.",
      "startOffset" : 94,
      "endOffset" : 114
    }, {
      "referenceID" : 19,
      "context" : ", 2009b), Gaussian processes posterior calculation (Moore and Russell, 2014), and max-kernel search (Curtin and Ram, 2014) are some examples.",
      "startOffset" : 51,
      "endOffset" : 76
    }, {
      "referenceID" : 3,
      "context" : ", 2009b), Gaussian processes posterior calculation (Moore and Russell, 2014), and max-kernel search (Curtin and Ram, 2014) are some examples.",
      "startOffset" : 100,
      "endOffset" : 122
    }, {
      "referenceID" : 1,
      "context" : "Although cover trees were originally intended for nearest neighbor search (See Algorithm Find-All-Nearest, Beygelzimer et al., 2006), they can be adapted to a wide variety of problems: minimum spanning tree calculation (March et al., 2010), approximate nearest neighbor search (Ram et al., 2009b), Gaussian processes posterior calculation (Moore and Russell, 2014), and max-kernel search (Curtin and Ram, 2014) are some examples. Further, through the tree-independent dual-tree algorithm abstraction of Curtin et al. (2013b), other existing dual-tree algorithms can easily be adapted for use with cover trees.",
      "startOffset" : 107,
      "endOffset" : 525
    }, {
      "referenceID" : 1,
      "context" : "Although cover trees were originally intended for nearest neighbor search (See Algorithm Find-All-Nearest, Beygelzimer et al., 2006), they can be adapted to a wide variety of problems: minimum spanning tree calculation (March et al., 2010), approximate nearest neighbor search (Ram et al., 2009b), Gaussian processes posterior calculation (Moore and Russell, 2014), and max-kernel search (Curtin and Ram, 2014) are some examples. Further, through the tree-independent dual-tree algorithm abstraction of Curtin et al. (2013b), other existing dual-tree algorithms can easily be adapted for use with cover trees. In the framework of tree-independent dual-tree algorithms, all that is necessary to describe a dual-tree algorithm is a point-to-point base case function (BaseCase()) and a node-to-node pruning rule (Score()). These functions, which are often very straightforward, are then paired with a type of tree and a pruning dual-tree traversal to produce a working algorithm. In later sections, we will consider specific examples. When using cover trees, the typical pruning dual-tree traversal is an adapted form of the original nearest neighbor search algorithm (see Find-All-Nearest, Beygelzimer et al., 2006); this traversal is implemented in both the cover tree reference implementation and in the more flexible mlpack library (Curtin et al., 2013a). The problem-independent traversal is given in Algorithm 1 and was originally presented by Curtin and Ram (2014). Initially, it is called with the root of the query tree and a reference set R containing only the root of the reference tree.",
      "startOffset" : 107,
      "endOffset" : 1469
    }, {
      "referenceID" : 18,
      "context" : "χ ∼ ψ ∼ O(1); often, cached sufficient statistics (Moore, 2000) can enable O(1) runtime implementations of BaseCase() and Score().",
      "startOffset" : 50,
      "endOffset" : 63
    }, {
      "referenceID" : 17,
      "context" : "χ ∼ ψ ∼ O(1); often, cached sufficient statistics (Moore, 2000) can enable O(1) runtime implementations of BaseCase() and Score(). These results hold for any dual-tree algorithm regardless of the problem. Hence, the runtime of any dual-tree algorithm would be at least O(N) using our bound, which matches the intuition that answering O(N) queries would take at least O(N) time. For a particular problem and data, if cr, |R ∗|, χ, ψ are bounded by constants independent of N and θ is no more than linear in N (for large enough N), then the dual-tree algorithm for that problem has a runtime linear in N . Our theoretical result separates out the problem-dependent and the problem-independent elements of the runtime bound, which allows us to simply plug in the problem-dependent bounds in order to get runtime bounds for any dual-tree algorithm without requiring an analysis from scratch. Our results are similar to that of Ram et al. (2009a), but those results depend on a quantity called the constant of bichromaticity, denoted κ, which has unclear relation to cover tree imbalance.",
      "startOffset" : 51,
      "endOffset" : 942
    }, {
      "referenceID" : 3,
      "context" : "The more recent results of Curtin and Ram (2014) are more related to these results, but they depend on the inverse constant of bichromaticity ν which suffers from the same problem as κ.",
      "startOffset" : 27,
      "endOffset" : 49
    }, {
      "referenceID" : 3,
      "context" : "The more recent results of Curtin and Ram (2014) are more related to these results, but they depend on the inverse constant of bichromaticity ν which suffers from the same problem as κ. Although the dependence on ν is linear (that is, O(νN)), bounding ν is difficult and it is not true that ν = 1 in the monochromatic case. ν corresponds to the maximum number of reference recursions between a single query recursion, and κ corresponds to the maximum number of query recursions between a single reference recursion. The respective proofs that use these constants then apply them as a worst-case measure for the whole algorithm: when using κ, Ram et al. (2009a) assume that every reference recursion may be followed by κ query recursions; similarly, Curtin and Ram (2014) assume that every query recursion may be followed by ν reference recursions.",
      "startOffset" : 27,
      "endOffset" : 661
    }, {
      "referenceID" : 3,
      "context" : "The more recent results of Curtin and Ram (2014) are more related to these results, but they depend on the inverse constant of bichromaticity ν which suffers from the same problem as κ. Although the dependence on ν is linear (that is, O(νN)), bounding ν is difficult and it is not true that ν = 1 in the monochromatic case. ν corresponds to the maximum number of reference recursions between a single query recursion, and κ corresponds to the maximum number of query recursions between a single reference recursion. The respective proofs that use these constants then apply them as a worst-case measure for the whole algorithm: when using κ, Ram et al. (2009a) assume that every reference recursion may be followed by κ query recursions; similarly, Curtin and Ram (2014) assume that every query recursion may be followed by ν reference recursions.",
      "startOffset" : 27,
      "endOffset" : 771
    }, {
      "referenceID" : 1,
      "context" : "The task is well-studied and well-known, and there exist numerous approaches for both exact and approximate nearest neighbor search, including the cover tree nearest neighbor search algorithm due to Beygelzimer et al. (2006). We will consider that algorithm, but in a tree-independent sense as given by Curtin et al.",
      "startOffset" : 199,
      "endOffset" : 225
    }, {
      "referenceID" : 1,
      "context" : "The task is well-studied and well-known, and there exist numerous approaches for both exact and approximate nearest neighbor search, including the cover tree nearest neighbor search algorithm due to Beygelzimer et al. (2006). We will consider that algorithm, but in a tree-independent sense as given by Curtin et al. (2013b); this",
      "startOffset" : 199,
      "endOffset" : 325
    }, {
      "referenceID" : 1,
      "context" : "The standard bound function B(Nq) used for cover trees is adapted from (Beygelzimer et al., 2006):",
      "startOffset" : 71,
      "endOffset" : 97
    }, {
      "referenceID" : 20,
      "context" : "Thus, we will bound |R∗| using techniques related to those used by Ram et al. (2009a). The bounding of |R∗| is split into two sections: first, we show that when the scale smax r is small enough, R∗ is empty.",
      "startOffset" : 67,
      "endOffset" : 86
    }, {
      "referenceID" : 20,
      "context" : "This is the same strategy as performed by Ram et al. (2009a). First, we must establish a Score() function for relative value approximation.",
      "startOffset" : 42,
      "endOffset" : 61
    }, {
      "referenceID" : 8,
      "context" : "For an example of a better relative-value approximate kernel density estimation dual-tree algorithm, see the work of Gray and Moore (2003).",
      "startOffset" : 117,
      "endOffset" : 139
    } ],
    "year" : 2015,
    "abstractText" : "Numerous machine learning algorithms contain pairwise statistical problems at their core— that is, tasks that require computations over all pairs of input points if implemented naively. Often, tree structures are used to solve these problems efficiently. Dual-tree algorithms can efficiently solve or approximate many of these problems. Using cover trees, rigorous worstcase runtime guarantees have been proven for some of these algorithms. In this paper, we present a problem-independent runtime guarantee for any dual-tree algorithm using the cover tree, separating out the problem-dependent and the problem-independent elements. This allows us to just plug in bounds for the problem-dependent elements to get runtime guarantees for dual-tree algorithms for any pairwise statistical problem without re-deriving the entire proof. We demonstrate this plug-and-play procedure for nearest-neighbor search and approximate kernel density estimation to get improved runtime guarantees. Under mild assumptions, we also present the first linear runtime guarantee for dual-tree based range search.",
    "creator" : "LaTeX with hyperref package"
  }
}