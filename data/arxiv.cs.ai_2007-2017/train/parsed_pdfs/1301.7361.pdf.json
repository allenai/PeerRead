{
  "name" : "1301.7361.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Structured Reachability Analysis for Markov Decision Processes",
    "authors" : [ "Craig Boutilier", "Christopher Geibt" ],
    "emails" : [ "@cs.ubc.ca", "@cs.bgu.ac.il", "@htc.honeywell.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 Introduction\nWhile Markov decision processes (MDPs) have proven to be useful as conceptual and computational models for deci sion theoretic planning (DTP), there has been considerable effort devoted within the AI community to enhancing the computational power of these models. One of the key draw backs of classic algorithms such as policy iteration [16] or value iteration [1] is the need to explicitly \"sweep through\" state space, making these techniques impractical for realis tic problems. Recent research on the use of MDPs for DTP has focussed on methods for solving MDPs that avoid ex plicit state space enumeration while constructing optimal or approximately optimal policies. These include the use of function approximators for value functions [2], aggregation and abstraction techniques [5, 6, 12, 8], reachability analy sis [9], and decomposition techniques [11].\n• This work was supported by NSERC Research Grant OGP0121843 and IRIS-II Project IC-7.\nt Part of this work was undertaken at U.B.C. and was sup ported by NSERC.\nt Part of this work was undertaken at U.B.C. and was sup ported by IRIS-II Project IC-7.\nIn this paper we address the problem of integrating reach ability considerations into the construction of abstract MDPs. In particular, we develop techniques whereby knowledge of an initial state (or initial conditions) and the concomitant reachability considerations influence the ab stractions produced for an MDP, forming what is termed by Knoblock [ 17] a problem specific abstraction. We assume that the MDP is described in terms of random variables us ing dynamic Bayes nets (DBNs) [6]; we also assume an ini tial state has been given. Given this representation, several useful (exact and approximate) abstraction techniques can be employed to solve an MDP without explicit state space enumeration [5, 6, 12].1 These all rely on the identification of conditions under which a variable can influence a value function or action choice, and can be viewed as decision theoretic generalizations of goal regression [4]. Reachability analysis allows one to determine that certain states are not reachable in an MDP given a particular initial state (or a set of possible initial states, or an initial distri bution), no matter what actions are performed. This can be used to restrict dynamic programming to reachable states, reducing the computational burden of solving an MDP (for instance, see the envelope approach of [9]). However, ap proaches that determine reachability using explicit transi tion matrix operations [13] or state-based search cannot be exploited by variable-based abstraction techniques.\nOur aim is to develop methods that determine the set of reachable states implicitly by explicitly considering the variable values or combinations of variable values that can or cannot be realized. This knowledge can be inte grated into abstraction techniques by eliminating abstract states that contain unreachable variable combinations. The method we develop is based on the GRAPHPLAN algo rithm [3]: the graph-building phase of GRAPHPLAN can be viewed as performing an approximate reachability analysis that is used to prune subsequent goal-regression search in a classical planning framework. However, we make certain modifications designed to deal with the DBN action repre sentation. In particular, the distributed nature of this repre sentation, while often more compact than (say) probabilis tic STRIPS operators [15], requires that care be taken to deal with correlated effects. A second way in which we general ize the graph-building phase of GRAPHPLAN is to consider\n1The integration of reachability with state aggregation has been considered in the verification community (e.g., see [19]). There explicit state space representations are used however.\n32 Boutilier, Brafman, and Geib\nThe more sophisticated techniques of [4, 5, 6] can, of course, benefit in the same way as the simple abstrac tion method. However, these algorithms create dynamic, nonuniform abstractions, creating and recreating tree rep resentations of value functions and policies. As such, the simple approach of using the reachable set to reduce MDP descriptions once and for all does not take full advantage of the reachability analysis. Algorithms like these can be augmented slightly to fully exploit the output of REACH ABLEK: since these algorithms put together combinations of variables dynamically, they may start to compute policy values for states satisfying variable values that are marked as exclusive. It is a simple matter to add tests that use the constraints returned by REACHABLEK to rule out unrealiz able variable value combinations when building policy and value trees. Roughly, when a tree is constructed that con tains a branch labeled with variables values that are exclu sive, a suitable subtree can be removed from the tree. This results in smaller trees being maintained, which is the main factor in reducing the complexity of these algorithms.U Once again, the fact that reachability analysis is performed using the structured representation of the MDP is the key to integrating reachability with abstraction."
    }, {
      "heading" : "5 Concluding Remarks",
      "text" : "We have described an algorithm for performing structured reachability analysis of an MDP, using DBN action repre sentations, that allows one to trade off the complexity of the analysis with completeness. Our approach is inspired by the GRAPHPLAN algorithm, but generalizes the concepts introduced there by handling conditional effects and cor relations, and by providing the flexibility to adjust the de gree of reachability considered. We have illustrated how this can be exploited in certain MDP abstraction algorithms to provide deeper MDP reductions when initial states are fixed. One cannot expect reachability to play a substan tial role in MDP reduction in all cases (ergodic MDPs, for example); but it can be significant in many circumstances, such as when rewards are conditional, when certain vari ables are observable, but uncontrollable (e.g., road condi tions, the weather, interest rates), or when resource con straints restrict the set of reachable states. Our preliminary empirical results on resource-constrained problems suggest that reachability can offer dramatic MDP reductions in cer tain circumstances. A number of interesting directions emerge for future re search. The application of these ideas to classical planning settings, when viewed as extensions to GRAPHPLAN, could prove useful. We are also interested in exploiting the distri butional information in our action descriptions to perform approximate reachability analysis, focusing attention on the most \"important\" reachable states (e.g., perhaps the most probable, but accounting for improbable states that have a high impact on value).\nAcknowledgements: Thanks to Bob Givan and Matthew Spears for their comments and for pointing out a correction to an earlier version of the algorithm.\nReferences\n[1] Richard E. Bellman. Dynamic Programming. Princeton Uni-\n11 We defer full details to a longer version of this paper.\nversity Press, Princeton, 1957.\n[2] Dimitri P. Bertsekas and John. N. Tsitsiklis. Neuro-dynamic Programming. Athena, Belmont, MA, 1996.\n[3] Avrim L. Blum and Merrick L. Furst. Fast planning through graJ?h analysis. In Proceedings of the Fourteenth Inter natwnal Joint Conference on Artificial Intelligence, pages 1636-1642, Montreal, 1995.\n[4] Cr�ig Boutilier. Correlated action effects in decision theo retic regression. In Proceedings of the Thirteenth Confer ence on Uncertainty in Artificial Intelligence, pages 30-37, Providence, RI, 1997.\n[5] Craig Boutilier and Richard Dearden. Approximating value trees i� structured dyna_mic programming. In Proceedings of the Thtrteenth Internatwnal Conference on Machine Learn ing, pages 54-62, Bari, Italy, 1996.\n[6] Craig Boutilier, Richard Dearden, and Moises Goldszmidt. Exrloiting structure in policy construction. In Proceedin_gs of the Fourteenth International Joint Conference on Artip ciallntelligence, pages 1104-1111, Montreal, 1995.\n[7) Craig Boutilier and Moises Goldszmidt. The frame problem and Bayesian network action representations. In Proceed ings of the Eleventh Biennial Canadian Conference on Arti ficial Intelligence, pages 69-83, Toronto, 1996.\n[8] Thomas Dean and Robert Givan. Model minimization in markov decision processes. In Proceedings of the Four teenth National Conference on Artificial Intelligence, pages 1 06-111, Providence, 1997.\n[9] Thomas Dean, Leslie Pack Kaelbling, Jak Kirman, and Ann Nicholson. Planning with deadlines in stochastic domains. In Proceedingsof the Eleventh National Conference onArti jiciallntelligence, pages 574-579, Washington, D.C., 1993.\n[10] Thomas Dean and Keiji Kanazawa. A model for reason ing about persistence and causation. Computational Intel ligence, 5(3):142-150, 1989.\n[11] Thomas Dean and Shieu-Hong Lin. Decomposition tech niques for planning in stochastic domains. In Proceedings of the Fourteenth International Joint Conference on Artifi cial Intelligence, pages 1121-1127, Montreal, 1995.\n[12] Richard Dearden and Craig Boutilier. Abstraction and ap proximate decision theoretic planning. Artificial Intelli gence,89:219-283, 1997.\n[13] B. L. Fox and D. M. Landi. An algorithm for identifying the ergodic subchains and transient states of a stochastic matrix. Communications of the ACM, 2:619-621, 1968.\n[14] B. Cenk Gazen and Craig A. Knoblock. Combining the ex pressiveness of UCPOP with the efficieny of Graphplan. In Proceedings of the Fourth European Conference on Plan ning, pages ??-??, Toulouse, 1997.\n[ 15] Steve Hanks and Drew V. McDermott. Mode lin� a dynamic and uncertain world I: Symbolic and probabilistic reasoning about change. Artificial Intelligence, 1994.\n[16]\n[17]\n[18]\n[19]\nRonald A. Howard. Dynamic Programming and Markov Processes. MIT Press, Cambridge, 1960.\nCraig A. Knoblock. Automatically generating abstractions for planning. Artificial Intelligence, 68:243-302, 1994.\nJ. Koehler, B. Nebel, J. Hoffman, andY. Dimopolous. Ex tending Graphplan to a subset of ADL. In Proceedinf!s of the Fourth European Conference on Planning, pages ?7-?1, Toulouse, 1997.\nD. Lee and M. Yannakakis. Online miminization of transi tion systems. In Proceedings of the 24thAnnualACM Sym posium on the Theory of Computing (STOC-92), pages 264- 274, Victoria, BC, 1992."
    } ],
    "references" : [ {
      "title" : "Dynamic Programming. Princeton Uni- 11 We defer full details to a longer version of this paper",
      "author" : [ "Richard E. Bellman" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1957
    }, {
      "title" : "Fast planning through graJ?h analysis",
      "author" : [ "Avrim L. Blum", "Merrick L. Furst" ],
      "venue" : "In Proceedings of the Fourteenth Inter­ natwnal Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1995
    }, {
      "title" : "Correlated action effects in decision theo­ retic regression",
      "author" : [ "Cr�ig Boutilier" ],
      "venue" : "In Proceedings of the Thirteenth Confer­ ence on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1997
    }, {
      "title" : "Approximating value trees i� structured dyna_mic programming",
      "author" : [ "Craig Boutilier", "Richard Dearden" ],
      "venue" : "In Proceedings of the Thtrteenth Internatwnal Conference on Machine Learn­ ing,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1996
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "One of the key draw­ backs of classic algorithms such as policy iteration [16] or value iteration [1] is the need to explicitly \"sweep through\" state space, making these techniques impractical for realis­ tic problems.",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 3,
      "context" : "These include the use of function approximators for value functions [2], aggregation and abstraction techniques [5, 6, 12, 8], reachability analy­ sis [9], and decomposition techniques [11].",
      "startOffset" : 112,
      "endOffset" : 125
    }, {
      "referenceID" : 3,
      "context" : "enumeration [5, 6, 12].",
      "startOffset" : 12,
      "endOffset" : 22
    }, {
      "referenceID" : 2,
      "context" : "1 These all rely on the identification of conditions under which a variable can influence a value function or action choice, and can be viewed as decision­ theoretic generalizations of goal regression [4].",
      "startOffset" : 201,
      "endOffset" : 204
    }, {
      "referenceID" : 1,
      "context" : "The method we develop is based on the GRAPHPLAN algo­ rithm [3]: the graph-building phase of GRAPHPLAN can be viewed as performing an approximate reachability analysis that is used to prune subsequent goal-regression search in a classical planning framework.",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 2,
      "context" : "The more sophisticated techniques of [4, 5, 6] can, of course, benefit in the same way as the simple abstrac­ tion method.",
      "startOffset" : 37,
      "endOffset" : 46
    }, {
      "referenceID" : 3,
      "context" : "The more sophisticated techniques of [4, 5, 6] can, of course, benefit in the same way as the simple abstrac­ tion method.",
      "startOffset" : 37,
      "endOffset" : 46
    } ],
    "year" : 2011,
    "abstractText" : "Recent research in decision theoretic planning has fo­ cussed on making the solution of Markov decision pro­ cesses (MDPs) more feasible. We develop a family of algorithms for structured reachability analysis of MDPs that are suitable when an initial state (or set of states) is known. Using compact, structured represen­ tations of MDPs (e.g., Bayesian networks), our meth­ ods, which vary in the tradeoff between comiJiexity and accuracy, produce structured descriptions of (esti­ mated) reachable states that can be used to eliminate variables or variable values from the problem descrip­ tion, reducing the size of the MDP and making it eas­ ier to solve. One contribution of our work is the exten­ sion of ideas from GRAPH PLAN to deal with the dis­ tributed nature of action representations typically em­ bodied within Bayes nets and the problem of correlated action effects. We also demonstrate that our algorithm can be made more complete by using k-ary constraints instead of binary constraints. Another contribution is the illustration of how the compact representation of reachability constraints can be exploited by several ex­ isting (exact and approximate) abstraction algorithms for MDPs.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}