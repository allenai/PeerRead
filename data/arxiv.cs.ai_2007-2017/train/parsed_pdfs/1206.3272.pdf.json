{
  "name" : "1206.3272.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Improving Gradient Estimation by Incorporating Sensor Data",
    "authors" : [ "Gregory Lawrence" ],
    "emails" : [ "gregl@cs.berkeley.edu", "russell@cs.berkeley.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "An efficient policy search algorithm should estimate the local gradient of the objective function, with respect to the policy parameters, from as few trials as possible. Whereas most policy search methods estimate this gradient by observing the rewards obtained during policy trials, we show, both theoretically and empirically, that taking into account the sensor data as well gives better gradient estimates and hence faster learning. The reason is that rewards obtained during policy execution vary from trial to trial due to noise in the environment; sensor data, which correlates with the noise, can be used to partially correct for this variation, resulting in an estimator with lower variance."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Policy search algorithms have been very effective in learning good policies in the reinforcement learning setting. Successful applications include helicopter flight [8], quadruped locomotion [5], and baseball hitting [10]. These methods work by adjusting the parameters of a policy to improve its value, i.e., the expected sum of rewards (possibly discounted) obtained during policy execution. To do this, the algorithms repeatedly estimate the gradient of the value with respect to the parameters, using information observed during policy trials, and then adjust the parameters in the “uphill” direction. Because trials can be expensive, especially in physical environments, a number of authors have presented techniques to reduce the number of required trials—mainly by reducing the variance of the gradient estimator [1, 3, 6, 7, 10, 9, 11, 13].\nGenerally speaking, these methods estimate the gradient from the policy parameter settings on each trial and the score (the actual sum of rewards), ignoring\nthe sensor data.1 The main point of this paper is that the sensor data obtained during each trial also provides a useful signal that can reduce the variance of gradient estimators. To understand how this may be so, consider first a case in which it is not so: that is, the noise-free case where the score is a deterministic function of the policy parameters. In that case, the local gradient can be estimated exactly from a small set of trials with policy parameter settings closely spaced around the setting of interest, and the sensor data can provide no more information.2 In the noisy case, however, a gradient estimator can be easily misled by trials of bad policies that yield fortuitously good scores and vice versa. In essence, what we propose is that sensor data can account, at least partially, for the deviation in the score of each trial from its expected value. Conditioned on the sensor data, therefore, the posterior estimate for the policy value will be closer to the true value.\nAs an example, consider the problem of firing a cannon at a distant target (Figure 1). Imagine that, after firing several shots that fall short, you increase the firing angle to θ= θ0. The next shot sails over the target. Normally, you would decrease θ again. Suppose, however, that the sound of this last shot was much louder than usual; this suggests that the muzzle velocity was higher than intended and accounts for the poor outcome. It might therefore be sensible to stay at θ0 for the next shot. Notice, though, that this decision re-\n1Of course, the policies themselves may use the sensor data to select actions. One might also interpret the approach of [6] as using observed perturbations to improve gradient estimation—but only for the restricted case of perfect sensing of noise applied directly to the policy parameters. The current paper removes these unrealistic assumptions, allowing the method to apply to any real physical system; it also provides a more general explanation of the benefits of sensor data for gradient estimation.\n2If we are willing to step outside the policy search framework, of course, then we can use the sensor data to learn a transition model from which the optimal policy can be obtained by offline methods.\nquires some knowledge of the relationship between the sensor data (loudness of bang) and the score; learning such relationships is a key element of our method.\nThe remainder of the paper is organized as follows. Section 2 introduces the basic technical approach: instead of estimating the gradient by fitting a local linear model for the value as a function of the policy parameters, we fit a linear model of the value as a function of the policy parameters and the (transformed) sensor values. We show that estimator variance is minimized if the sensor data is transformed by a projection that renders it statistically independent of the policy parameters while remaining correlated with the perturbations in the score; we also present a practical technique that approximates this ideal transformation. Section 3 describes how we applied these techniques to a dart throwing task and a quadruped locomotion task, in both cases obtaining improved learning curves compared to a method that does not use the sensor data. Section 4 discusses possibilities for future improvements."
    }, {
      "heading" : "2 INCORPORATING SENSOR INFORMATION",
      "text" : "A policy π determines how an agent chooses its actions given its past observations, and the reinforcement learning goal is to find a policy π∗ that maximizes the performance. Each policy trial generates a history h, a sequence of percept-action-reward tuples; the score f(h) is the sum of the reward values. The optimal policy π∗ maximizes the value function V (π) = E[f(h)|π] where the histories h are generated from π. In this paper we perform policy search by hill-climbing through a space of parameterized policies π ∈ Rd. We climb through this space by adjusting our current policy π0 in the direction of the gradient ∇πV (π)|π=π0 . At each hill-climbing step, the gradient is estimated from n policy trials where we vary the parameters of each trial for exploration purposes."
    }, {
      "heading" : "2.1 TOY EXAMPLE",
      "text" : "To illustrate the main contributions of this paper, we will examine the toy cannon problem (Figure 1). Here, the policy π = (πv, πθ)T consists of a desired cannon angle, πθ ∈ [0, π/2] and a desired initial velocity πv > 0. Following [6], we assume the policy itself is perturbed by noise to give the actual controls u = (uv, uθ)T . We assume that the agent has access to a noisy sensor that measures the perturbation (u− π) and let s = (sv, sθ) denote its value. There is additive, zero-mean Gaussian noise in both the control and sensor values; the control noise has covariance matrix Σu and the sensor noise has covariance matrix Σs. The history h for this problem has a single tuple with the desired action π, the sensor value s, and the reward −d2, where d is the distance from the target to where the cannon ball lands. The score is just the reward in this case, so maximizing V (π) = E[f(h)|π] is equivalent to minimizing the expected squared distance error.\nTo demonstrate the benefits of incorporating the sensor data, suppose that we had a perfect sensor. In that case, we have u = π + s. Furthermore let us assume that f(h) is (locally) linear in the actual control u. Then the score function can be written as f(π, s) = (π + s)TAπ0 + bπ0 . A typical approach to gradient estimation in this setting would be to ignore the sensor data and use samples of π and f(π, s) to fit a local linear approximation to V around π0:\nV̂ (π) = πTAπ0 + bπ0 .\nUsing least-squares regression, we estimate the gradient by learning Aπ0 from n policy trials. The control noise causes perturbations in the scores with variance given by Aπ0\nTΣuAπ0 . Hence, the estimator Aπ0 will not be exact and we may need many samples to be confident in our estimate of the gradient.\nTo obtain the benefits of incorporating the sensor data, we can instead fit a sensor-data-dependent linear approximation to the scoring function f itself (rather than its expectation V ):\nf̂(π, s) = πTAπ0 + s TAs + bπ0 .\nFrom samples of π, f(π, s), and s we can use linear regression to learn both Aπ0 and As in this equation. The value that we learn for Aπ0 can be used as our estimate of the gradient of V. In the case of perfect sensing, f̂(π, s) can be a locally exact fit because the score will be a deterministic function of π and s. Thus, we have a perfect gradient estimator. Intuitively, the more informative the sensor, the better our gradient estimator.\nThere are two important issues to note with this analysis. As we will see in Section 2.2, the estimator Aπ0\nwill be unbiased provided the sensor data are independent of the policy parameters. Therefore, it will be advantageous to project the sensor data in such a way as to render it independent of the policy parameters to the extent possible, while maintaining its correlation with the perturbations in the score. Second, the sensor-data-dependent fit for f̂ requires extra parameters to be learned for As (two extra parameters in our example), which may in turn require extra trials. We will see in the next subsection that the relative efficiency of this estimator depends on the amount of noise in the observed scores that can be corrected for using the sensor data and the number of extra parameters to be learned.\nFigures 2(a-d) demonstrates how incorporating the sensor data is beneficial. For ease of graphical depiction, we consider a restricted version of the cannon problem in which uθ is fixed at 45 degrees. We consider a linear approximation to the score around the nominal cannon velocity πv0 = 20ms−1. Figure 2(a) shows the true value function with four policy trials superimposed. Figure 2(b) shows the least squares fit of these four trials. The slope of this line is an estimate of the gradient of V (π). Figure 2(c) shows the least squares fit of the four trials as a function of π and s and Figure 2(d) shows this least squares fit projected onto the plane that spans the πv and f(h) axes. Notice that it perfectly fits the true value. This is because deviations from the expected score are explained away\nby the sensor values. We expand upon this in the next section.\nFigure 3 shows different learning curves for the cannon problem as we increase the level of actuator noise (Σu = [ 1 0 0 4 ] ). At each hill-climbing step we ran 10 trials, each with a different policy in the neighborhood of the current policy, and we averaged the resulting learning curve over 100 hill-climbing runs. One curve shows the learning performance achieved while ignoring the sensor data while the other curve shows what happens when incorporating the sensor data. Notice that the performance gains from using sensor data become more pronounced as the noise level increases."
    }, {
      "heading" : "2.2 VARIANCE REDUCTION",
      "text" : "In this section we compare the variance of a gradient estimator that incorporates sensor data to an estimator that ignores it. We assume that the score function is an unknown linear function of both the policy parameters and the sensor data. The gradient estimators take n policy trials from the current hill-climbing step and return an estimate of ∇πV (π)|π=π0 .\nWe will show that an agent can reduce the variance of its gradient estimates by choosing a sensor encoding that correlates with the noise-induced perturbations in the score. To get an unbiased estimate, the sensor data must be uncorrelated with π. In Section 2.2.3 we\ngive expressions for the bias and variance of a gradient estimator in the correlated setting. A biased estimator that incorporates sensor data may still outperform one that ignores it as long as the bias remains small.\nLet Π = [π1, . . . , πn]T be the matrix of policy parameters and let S = [s1, . . . , sn]T be a matrix whose rows contain the corresponding sensor values. We assume that the sensors have a Gaussian distribution with variance Σs and mean µs. Let f = [f1, . . . , fn]T be a column vector whose entries are the score values and let w = [w1, . . . , wn] be a column vector of zero-mean noise which is added to the output with variance σw2. The score is written as f2(π, s, w) = πTAπ + sTAs + b + w. The scores for the current set of policy trials are given by the following equation:\nf = ΠAπ + SAs + 1nb+ w, (1)\nwhere 1n is a column vector of ones.\nEach estimator learns a local linear model of the scoring function using linear regression on the data obtained from n policy trials. For exploration purposes the policy parameters of each trial are assumed to be distributed around a nominal policy π0 according to a Gaussian distribution with variance Σe. We assume that the policy parameters, scores, and sensor data have been centered around zero."
    }, {
      "heading" : "2.2.1 Ignore Sensor Data",
      "text" : "From the point of view of a gradient estimator that ignores the sensor data, additional noise will appear to be added to the scores that will not be explained by perturbations in the sensor data. Let v represent this noise where each element is given by the equation v = sTAs+w. The variance of each entry in v is given by the expression AsTΣsAs + σ2. The score function f1(π, v) = πTAπ+b+v is equivalent in value to f2. We can learn the linear relationship between the policy parameters and the score by performing linear regression on the set of n policy trials. In other words we find a suitable estimate for Aπ in the following equation:\nf = ΠAπ + 1nb+ v. (2)\nWe are interested in the gradient of E[f1(π,v)|π0], where π and v represent random variables whose values are distributed according to the exploration distribution N (π0,Σe) and the output distribution respectively. The gradient with respect to π0 is written as ∇π0E[f1(π,v)|π0] = Aπ and therefore, a sensible gradient estimator returns an estimate of Aπ from the n policy trials. The gradient estimator, which we denote by g1(Π, f) is given by the linear regression equation:\ng1(Π, f) = (ΠTΠ)−1ΠT f\nThe variance of the g1 is written as\nvar[g1(Π, f)] = (ΠTΠ)−1ΠTE[vvT ]Π(ΠTΠ)−1\n= (ΠTΠ)−1ΠT (AsTΣsAs + σ2)Π(ΠTΠ)−1 = (ΠTΠ)−1(AsTΣsAs + σ2),\nwhere f is a column vector random variable whose entries are distributed according to the output distribution. This quantity is for a fixed set of policies Π. The variance of the g1 averaged over the randomness of the policies drawn for exploration purposes can be determined by observing that the distribution of the matrix (ΠTΠ)−1 is an inverted Wishart with n degrees of freedom where d is the number of policy parameters.\nvar[g1(Π, f)] = Σe−1(AsTΣsAs + σ2)\n(N − d− 1) , (3)\nwhere Π is a random variable where each row is distributed according to the exploration distribution."
    }, {
      "heading" : "2.2.2 Include Sensor Data",
      "text" : "A linear model that predicts the score as a function of both the policy parameters and sensor data will have the noise on the output partially explained by the sensor data. An estimate of the score as a linear function of the policy parameters and sensor data can be written as[\ng2(Π, S, f) β2(Π, S, f) ] = [ ΠTΠ ΠTS STΠ STS ]−1 [ΠT f ST f ] ,\nwhere β2(Π, S, f) determines how the sensor values affect the perturbations in the score. The gradient of E[f2(π, s, f)|π0] with respect to π0 is written as ∇πE[f2(π, s, f)|π0] = Aπ and so we can use g2(Π, S, f), the first d entries in the above vector, as our estimate of the gradient. The variance of the above expression is written as\nvar [ g2(Π, S, f) β2(Π, S, f) ] = [ ΠTΠ ΠTS STΠ STS ]−1 σ2.\nWe take the inverse of the Schur complement with respect to STS to find the variance of g2:\nvar[g2(Π, S, f)] = (ΠTΠ−ΠTS(STS)−1STΠ)−1σ2.\nThis quantity is for a fixed set of policies Π and sensor values S. The variance of g2 averaged over different exploration policies and sensor values, assuming that the sensors are independent of both the policy parameters and output noise w, is given by the following equation:\nvar [ g2(Π,S, f) β2(Π,S, f) ] = [ Σe 0 0 Σs ]−1 σ2 (N − d− ds − 1)\nvar[g2(Π,S, f)] = Σe−1σ2\n(N − d− ds − 1) , (4)\nwhere ds is the dimensionality of the sensor data.\nThe expressions for the variance of the two gradient estimators (equations 3 and 4) differ from each other in two factors. The variance of the estimator that ignores the sensor data has a factor of (AsTΣsAs + σ2) which is reduced to σ2 in the estimator that incorporates the sensor data. We see that we get bigger reductions whenever the sensor information provides more information about the score. The second difference between the two estimators favors the estimator that ignores the sensor data because the denominator in equation 3 has a term that is larger than the corresponding term in equation 4. The difference in the denominators is the dimensionality of the sensor data ds, which suggests that we should choose sensor encodings of low dimensionality. Whether g2 is more efficient than g1 depends on the relative strength of these two factors."
    }, {
      "heading" : "2.2.3 Correlated Sensors",
      "text" : "If the sensors are correlated with the policy parameters then the gradient estimator that incorporates sensor data will be biased. In this situation we can represent the distribution over sensors as a linear Gaussian distribution s ∼ N (Aπ,sTπ+bπ,s,Σs). Inserting this into score function f2 gives the following equation:\nf3(π, s, w,wπ,s) = πTAπ + πTAπ,sAs + bπ,sTAs+\nwπ,s TAs + b+ w\nwhere wπ,s is a zero-mean Gaussian random variable with variance Σs. The gradient with respect to π0 is written as ∇π0E[f3(π, s,w,wπ,s)|π0] = Aπ + Aπ,sAs which means that gradient estimator g2 is biased by Aπ,sAs whenever the sensors are correlated with the policy parameters. The variance of the estimator also changes in the case of correlated sensors:\nvar[g2(Π,S, f)] = (Σe −D)−1σ2\n(N − d− ds − 1) D := Σes(Aπ,sTΣeAπ,s + Σs)−1ΣesT ,\nwhere Σes = ΣeAπ,s is the covariance of the policy parameters and sensor values. Thus we see that it is best to choose sensor encodings where the sensor values are uncorrelated with the policy parameters."
    }, {
      "heading" : "2.3 FINDING GOOD SENSOR ENCODINGS",
      "text" : "In the prior subsection we saw that S should be independent of Π to give an unbiased estimate of the gradient. This is often not true in many problems. The analyzes also suggests that to get an improved gradient\nestimator, we should prefer low-dimensional sensor encodings that have a great influence on the score. This section presents a heuristic that can be used to find good sensor encodings that give performance gains in problems where the assumptions do not hold.\nWe find good sensor encodings by taking sensor variables that lie in a high dimensional space and projecting them down to a low-dimensional subspace. Intuitively, we should prefer directions that maintain the influence of the sensor data on the score. We also want the sensor data to be uncorrelated with the policy parameters to minimize the bias. Our approach is to search over possible sensor encodings to find the optimal projection at each hill-climbing step. We use cross-validation to measure the quality of each projection. Let Π-i equal Π with the ith row removed, let S-i equal S with the ith row removed, and let f-i equal f with the ith entry removed. Let B be a matrix that projects the raw sensor data down to a lowdimensional subspace. We use a quasi-Newton method to minimize the following cost function:\nJ = n∑ i=1 (πiT g(Π-i, S-iB, f-i) + o(Π-i, S-iB, f-i)− fi)2,\nwhere g estimates the gradient using g2 after the inputs have been centered around zero and o estimates the appropriate offset term. Thus we learn the gradient and corresponding offset terms using the data from a collection of policy trials with a single policy trial held out at a time. This gradient and offset is then used to predict the score of the held out sample. Given the optimal projection B∗ from the above procedure, we estimate the gradient using the full set of policy trials:\n∇πV (π)|π=π0 ≈ g2(Π, SB∗, f). (5)"
    }, {
      "heading" : "3 RESULTS",
      "text" : "In this section we describe how an agent can use its sensor data to improve its learning performance on a dart throwing task [6] and a quadruped locomotion task. These partially observable sequential tasks are complicated partly due to the fact that the transition and sensor models and their structures are unknown. These properties are characteristic of a wide range of real-world tasks.\nFigure 4(a) shows a single frame of the dart throwing problem where the objective is to throw a dart with minimal mean squared error (measured from where the dart hits the wall to the center of the dart board). The arm is modeled as a three-link rigid body with dimensions based on biological measurements [2]. The links correspond to the upper arm, forearm, and hand and are connected using a single degree of freedom\nrotational joint. The upper arm is connected to the shoulder at a fixed location. The arm is controlled by applying torques at each joint. These torques are generated by a PD-controller that attempts to move the arm through a desired trajectory, specified by a cubic spline for each joint angle. The starting posture of the arm is fixed in advance and the control policy specifies three spline knot positions for each joint, yielding 9 policy parameters in all. The controller is simulated for approximately 0.2 seconds and then the dart is released. The torques calculated by the PD-controller are perturbed by additive and multiplicative noise,3 and there is Gaussian noise added to the release time with σ = 0.01.\nFigure 4(b) shows a single frame of the quadruped locomotion problem where the objective is to maximize the sustained speed of the robot. Each leg of the quadruped has four degrees of freedom (three at the shoulder joints and one at the elbow). The quadruped is controlled by applying torques at each joint which gives the system 16 degrees of controllable freedom. The torques are generated by a PD-controller that attempts to move each leg through a desired trajectory, specified by a truncated Fourier Series for each joint angle. Each controllable degree of freedom has three corresponding parameters and the right side of the body is constrained to move the same as the left except offset by 180 degrees; there are 24 policy parameters. The controller is simulated for 3 seconds for each policy trial and the distance travelled is the objective function. As with the dart thrower, noise enters the system by perturbing the torques given by the PD-controller by additive and multiplicative noise.\n3Multiplicative noise has been shown to explain some aspects of biological motion [4, 12]."
    }, {
      "heading" : "3.1 SENSORS FOR MOTOR CONTROL",
      "text" : "The systems described in the previous subsection are capable of measuring the state of the observable joint angles during each policy trial. In these two tasks, the observable joints are the same as the controllable joints. In the quadruped task, this means that while the agent can sense the positions of each leg relative to the body, it does not have access to the absolute position and rotation of the torso.\nOur task is to take the sensed trajectories and to transform the values to something that gives us improvements in our gradient estimator. The sensor encodings should be independent of the policy parameters and so we attempt to find the difference between the observed motion of the system and the expected motion at each time step. The idea of using sensory data to cancel out the effect of one’s own motion is also present in the biology literature [14]. We approximate this difference by learning a crude approximation to the dynamical system in a pre-processing phase. Using the joint-space representation of each system, the dynamics are governed by the following second-order nonlinear differential equation:\nm(x)ẍ = u(t) + g(x) + c(x, ẋ) + w(x, ẋ, t).\nwhere x is the physical state of the system, m(x) is the joint-space inertia matrix, u(t) are the forces and torques applied to the system, g(x) is the gravitational force, c(x, ẋ) are the Centrifugal and Coriolis forces, and w(t) is the noise plus any external forces (e.g., the ground pushing up on the feet of the quadruped). A discrete time version of this equation is written as:\nm(x)a = u(t) + g(x) + c(x, v) + w(x, v, t).\nwhere v are the velocities and a is the acceleration.\nWe approximate the expected acceleration by predicting the following quantities as a function of the observable states and velocities:\nvec(m(x)−1) ≈ AMφ(x(o)) vec(m(x)−1g(x)) ≈ AGφ(x(o))\nvec(m(x)−1c(x, v)) ≈ ACφ( [ x(o) T , v(o) T ]T ),\nwhere x(o) and v(o) contain the observable components of the state and velocity terms and where φ(x) = triu([1, xT ]T [1, xT ]) is a function that augments its input with quadratic terms (triu(X) is a function that returns the upper triangular part of X stacked as a single column vector). We use linear regression to learn a model of each of these components as a function of the observable state variables. For example in the quadruped problem we will learn these linear models\nwithout regard to the absolute rotation of the system. This is clearly an approximation for the terms that involve gravity because the direction of the gravitational force, from a frame of reference attached to the torso, depends on its rotation relative to the ground frame.\nWe learn these parameters in a pre-processing stage by examining random states (xi, vi) in the dynamical system and examining the mass matrix m(xi), the gravity forces g(xi), and the Coriolis and Centrifugal forces c(xi, vi). The samples are drawn from a distribution of states that are likely to be encountered during policy execution. We learn the linear relationships using the following equations:\nAM := (Φ1TΦ1)−1ΦT1 M\nAG := (Φ1TΦ1)−1ΦT1 G\nAg := (Φ2TΦ2)−1ΦT2 C,\nwhere\nαi = φ(xi(o))\nβi = φ([xi(o)T , vi(o)T ]T )\nΦ1 = [α1T , . . . , αnf T ]T Φ2 = [β1T , . . . , βnf T ]T M = [vec(m(x1)−1)T , . . . , vec(m(xnf ) −1)T ]T\nG = [vec(m(x1)−1g(x1))T , . . . ,\nvec(m(xnf ) −1g(xnf )) T ]T\nC = [vec(m(x1)−1c(x1, v1))T , . . . ,\nvec(m(xnf ) −1c(xnf , vnf )) T ]T .\nDuring each policy execution we can take these parameters to estimate the expected acceleration at each time step as follows:\nâ(π, t, x(o), v(o)) = reshM (AMφ(x(o)))u(π, t)+ reshG(AGφ(x(o)))+\nreshC(ACφ( [ x(o) T , v(o) T ]T )),\nwhere the resh function reshapes a matrix to its original size. The difference in velocity is computed as the actual velocity at each time step v(o)(t) minus the velocity predicted using the following expression:\nv(o)(t) ≈ v(o)(t− 1) + â(π, t, x(o)(t− 1), v(o)(t− 1))∆t,\nwhere ∆t is the time between sensor measurements.\nWe reduce the number of sensor values by projecting these difference curves down onto a set of basis functions. We chose the same basis functions that we used to encode the policy for the two tasks; we used\nsplines for the dart and a truncated Fourier series for the quadruped. This is the sensor data that we give to the gradient estimator algorithms as described in Section 2.2. We also include a sensor that detects the release time for the dart throwing task."
    }, {
      "heading" : "3.2 LEARNING PERFORMANCE",
      "text" : "Figure 5 shows the learning curves for the dart throwing problem. We get a substantial improvement in the learning performance when using an algorithm that incorporates sensor data when compared to an algorithm that ignores the sensor data. At each hill-climbing step we drew a single sample from 12 different policies and we averaged over 48 hill-climbing runs. The policies for each hill-climbing step were drawn from a Gaussian distribution for exploration purposes.\nFigure 6 shows the learning curves for the quadruped locomotion problem. We get an improvement in the learning performance when using of an algorithm that incorporates sensor data when compared to an algorithm that ignores the sensor data. At each hillclimbing step we drew a single sample from 30 different policies and we averaged over 48 hill-climbing runs. The policies for each hill-climbing step were drawn from a Gaussian distribution for exploration purposes."
    }, {
      "heading" : "4 DISCUSSION",
      "text" : "In this paper we demonstrated how one may incorporate sensor data into the gradient estimation task to improve the performance of policy search. We showed that the level of improvement depends on the amount of information that the sensors provide about\nthe noise-induced perturbations on the score. We also showed that the performance gains depend on the dimensionality of the sensor data. It is important to choose sensor encodings that are independent of the policy parameters to minimize the bias. We also presented a technique to find good sensor encodings for problems in which these assumptions (lowdimensionality and statistical indepedence) do not hold. Finally, we presented learning curves that show improvements in the learning performance for a toy cannon problem, dart throwing task, and quadruped locomotion task.\nIn this paper the distribution of every random variable was approximated by using a linear Gaussian relationship. Improvements in performance may be realized if we use non-linear mappings. Other improvements may come from incorporating our knowledge of the physics behind each task. For example, in the cannon problem we already know the equations of projectile motion. Thus, given the actual controls, we should be able to accurately predict the score. Even in cases in which we do not know the equations of motion, we often know qualitative information about the motion, such as the fact that increasing the desired velocity causes the cannon ball to fly further (i.e., the distance travelled is monotonically increasing as a function of the desired velocity). One possible approach to incorporating this information is to place a prior on the parameters that reflects these constraints."
    } ],
    "references" : [ {
      "title" : "Infinite-horizon policygradient estimation",
      "author" : [ "J. Baxter", "P.L. Bartlett" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2001
    }, {
      "title" : "A kinematic model of the upper limb based on the visible human project (vhp) image dataset",
      "author" : [ "B. Garner", "M. Pandy" ],
      "venue" : "Computer Methods in Biomechanics and Biomedical Engineering,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1999
    }, {
      "title" : "Variance reduction techniques for gradient estimates in reinforcement learning",
      "author" : [ "Evan Greensmith", "Peter L. Bartlett", "Jonathan Baxter" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2001
    }, {
      "title" : "Signaldependent noise determines motor",
      "author" : [ "Christopher Harris", "Daniel Wolpert" ],
      "venue" : "planning. Nature,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1998
    }, {
      "title" : "Machine learning for fast quadrupedal locomotion",
      "author" : [ "N. Kohl", "P. Stone" ],
      "venue" : "In Proceedings of the Nineteenth National Conference on Artificial Intelligence,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2004
    }, {
      "title" : "Efficient gradient estimation for motor control learning",
      "author" : [ "Gregory Lawrence", "Noah Cowan", "Stuart Russell" ],
      "venue" : "In Proceedings of the Nineteenth International Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2003
    }, {
      "title" : "Pegasus: A policy search method for large MDPs and POMDPs",
      "author" : [ "Andrew Y. Ng", "Michael Jordan" ],
      "venue" : "In Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2000
    }, {
      "title" : "Autonomous helicopter flight via reinforcement learning",
      "author" : [ "Andrew Y. Ng", "H. Jin Kim", "Michael Jordan", "Shankar Sastry" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2003
    }, {
      "title" : "Learning from scarce experience",
      "author" : [ "Leon Peshkin", "Christian R. Shelton" ],
      "venue" : "In Proceedings of the 19th International Conference on Machine Learning,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2002
    }, {
      "title" : "Natural actor-critic",
      "author" : [ "Jan Peters", "Sethu Vijayakumar", "Stefan Schaal" ],
      "venue" : "In Proceedings of the 16th European Conference on Machine Learnin,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2003
    }, {
      "title" : "Policy improvement for POMDPs using normalized importance sampling",
      "author" : [ "Christian R. Shelton" ],
      "venue" : "In Proceedings of the Seventeenth International Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2001
    }, {
      "title" : "Optimal control as a theory of motor coordination",
      "author" : [ "Emanuel Todorov", "Michael I. Jordan" ],
      "venue" : "Nature Neuroscience,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2002
    }, {
      "title" : "The optimal reward baseline for gradient-based reinforcement learning",
      "author" : [ "Lex Weaver", "Nigel Tao" ],
      "venue" : "In Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2001
    }, {
      "title" : "Perspectives and problems in motor learning",
      "author" : [ "D. Wolpert", "Z. Ghahramani", "J. Flanagan" ],
      "venue" : "Trends in Cognitive Science,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2001
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "Successful applications include helicopter flight [8], quadruped locomotion [5], and baseball hitting [10].",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 4,
      "context" : "Successful applications include helicopter flight [8], quadruped locomotion [5], and baseball hitting [10].",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 9,
      "context" : "Successful applications include helicopter flight [8], quadruped locomotion [5], and baseball hitting [10].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 0,
      "context" : "Because trials can be expensive, especially in physical environments, a number of authors have presented techniques to reduce the number of required trials—mainly by reducing the variance of the gradient estimator [1, 3, 6, 7, 10, 9, 11, 13].",
      "startOffset" : 214,
      "endOffset" : 241
    }, {
      "referenceID" : 2,
      "context" : "Because trials can be expensive, especially in physical environments, a number of authors have presented techniques to reduce the number of required trials—mainly by reducing the variance of the gradient estimator [1, 3, 6, 7, 10, 9, 11, 13].",
      "startOffset" : 214,
      "endOffset" : 241
    }, {
      "referenceID" : 5,
      "context" : "Because trials can be expensive, especially in physical environments, a number of authors have presented techniques to reduce the number of required trials—mainly by reducing the variance of the gradient estimator [1, 3, 6, 7, 10, 9, 11, 13].",
      "startOffset" : 214,
      "endOffset" : 241
    }, {
      "referenceID" : 6,
      "context" : "Because trials can be expensive, especially in physical environments, a number of authors have presented techniques to reduce the number of required trials—mainly by reducing the variance of the gradient estimator [1, 3, 6, 7, 10, 9, 11, 13].",
      "startOffset" : 214,
      "endOffset" : 241
    }, {
      "referenceID" : 9,
      "context" : "Because trials can be expensive, especially in physical environments, a number of authors have presented techniques to reduce the number of required trials—mainly by reducing the variance of the gradient estimator [1, 3, 6, 7, 10, 9, 11, 13].",
      "startOffset" : 214,
      "endOffset" : 241
    }, {
      "referenceID" : 8,
      "context" : "Because trials can be expensive, especially in physical environments, a number of authors have presented techniques to reduce the number of required trials—mainly by reducing the variance of the gradient estimator [1, 3, 6, 7, 10, 9, 11, 13].",
      "startOffset" : 214,
      "endOffset" : 241
    }, {
      "referenceID" : 10,
      "context" : "Because trials can be expensive, especially in physical environments, a number of authors have presented techniques to reduce the number of required trials—mainly by reducing the variance of the gradient estimator [1, 3, 6, 7, 10, 9, 11, 13].",
      "startOffset" : 214,
      "endOffset" : 241
    }, {
      "referenceID" : 12,
      "context" : "Because trials can be expensive, especially in physical environments, a number of authors have presented techniques to reduce the number of required trials—mainly by reducing the variance of the gradient estimator [1, 3, 6, 7, 10, 9, 11, 13].",
      "startOffset" : 214,
      "endOffset" : 241
    }, {
      "referenceID" : 5,
      "context" : "One might also interpret the approach of [6] as using observed perturbations to improve gradient estimation—but only for the restricted case of perfect sensing of noise applied directly to the policy parameters.",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 5,
      "context" : "Following [6], we assume the policy itself is perturbed by noise to give the actual controls u = (uv, uθ) .",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 0,
      "context" : "Figure 3 shows different learning curves for the cannon problem as we increase the level of actuator noise (Σu = [ 1 0 0 4 ] ).",
      "startOffset" : 113,
      "endOffset" : 124
    }, {
      "referenceID" : 3,
      "context" : "Figure 3 shows different learning curves for the cannon problem as we increase the level of actuator noise (Σu = [ 1 0 0 4 ] ).",
      "startOffset" : 113,
      "endOffset" : 124
    }, {
      "referenceID" : 5,
      "context" : "In this section we describe how an agent can use its sensor data to improve its learning performance on a dart throwing task [6] and a quadruped locomotion task.",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 1,
      "context" : "The arm is modeled as a three-link rigid body with dimensions based on biological measurements [2].",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 3,
      "context" : "Multiplicative noise has been shown to explain some aspects of biological motion [4, 12].",
      "startOffset" : 81,
      "endOffset" : 88
    }, {
      "referenceID" : 11,
      "context" : "Multiplicative noise has been shown to explain some aspects of biological motion [4, 12].",
      "startOffset" : 81,
      "endOffset" : 88
    }, {
      "referenceID" : 13,
      "context" : "The idea of using sensory data to cancel out the effect of one’s own motion is also present in the biology literature [14].",
      "startOffset" : 118,
      "endOffset" : 122
    } ],
    "year" : 2008,
    "abstractText" : "An efficient policy search algorithm should estimate the local gradient of the objective function, with respect to the policy parameters, from as few trials as possible. Whereas most policy search methods estimate this gradient by observing the rewards obtained during policy trials, we show, both theoretically and empirically, that taking into account the sensor data as well gives better gradient estimates and hence faster learning. The reason is that rewards obtained during policy execution vary from trial to trial due to noise in the environment; sensor data, which correlates with the noise, can be used to partially correct for this variation, resulting in an estimator with lower variance.",
    "creator" : "TeX"
  }
}