{
  "name" : "1610.01807.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Parallel Large-Scale Attribute Reduction on Cloud Systems",
    "authors" : [ "Junbo Zhang", "Tianrui", "Li", "Yi Pan" ],
    "emails" : [ "jbzhang@my.swjtu.edu.cn,", "trli@swjtu.edu.cn);", "pan@cs.gsu.edu)." ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms—Attribute Reduction, SPARK, Model Parallelism, Big Data, Cloud"
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Enormous amounts of data are generated every day with the amazing spread of computers and sensors in a widerange of domains, including social media, search engines, insurance companies, health care organizations, financial industry and many others [2]. Now we are in the era of big data, which is characterized by 5Vs [3]: 1) Volume means the amount of data that needs to be managed is very huge; 2) Velocity means that the speed of data update is very high; 3) Variety means that the data is varied in nature and there are\nThis is an extended version of the paper presented at 2013 International Conference on Parallel and Distributed Computing, Applications and Technologies, 2013. An early version (in Chinese) can be found in Chapter 3 of the book [1]. Junbo Zhang and Tianrui Li are with the School of Information Science and Technology, Southwest Jiaotong University, Chengdu 610031, China (e-mail: jbzhang@my.swjtu.edu.cn, trli@swjtu.edu.cn); Yi Pan is with the Department of Computer Science, Georgia State University, Atlanta, GA 30303, USA (e-mail: pan@cs.gsu.edu).\nmany different types of data that need to be properly combined to make the most of the analysis; 4) Value means high yield will be achieved if the big data is handled correctly and accurately; 5) Veracity means the inherent uncertainty and ambiguity of data. Big data is currently a fast growing field both from an application and a research point of view. Since big data contains huge values, mastering big data means mastering resources. However, mining knowledge from big data is a tremendously challenging task because of data uncertainty and inconsistency. Attribute reduction, also known as feature selection in pattern recognition & machine learning, can not only be used as an effective preprocessing step which reduces the complexity of handling big data, but also exploits the data redundancy to reduce the uncertainty from big data. It also helps people better understand the data by telling them which are key features, and has been attracted much attention in recent years [4], [5], [6], [7], [8], [9]. Rough set theory, introduced by Pawlak in 1982, is a soft computing tool for dealing with inconsistent information in decision situations [10], [11], [12], and plays an important role in the fields of pattern recognition, feature selection and knowledge discovery [13]. Attribute reduction in rough set theory provides a theoretic framework for consistencybased feature selection, which can retain the discernible ability of original features for the objects from the universe [6].\nAttribute reduction from large data is an expensive preprocessing step. To accelerate this process, incremental techniques combined with traditional rough set based methods are widely researched [14], [15], [16], [8], [6], [17], [18]. For example, Li et al. proposed an incremental method for dynamic attribute generalization, which can accelerate a heuristic process of attribute reduction by updating approximations incrementally. Qian et al. introduced positive approximation, which is a theoretic framework for accelerating a heuristic process [6]. Zhang et al. presented a matrix-based incremental method for fast computing rough approximations. Liang et al. developed a group of incremental feature selection algorithms based on rough sets [18].\nAs these methods are still sequential and can not process big data with a cluster. MapReduce, by Google, is a popular parallel programming model and a framework for processing big data on certain kinds of distributable problems using a large number of computers, collectively referred to as a cluster [19]. It can help arrange the application in the cluster easily. Some MapReduce runtime systems were\nar X\niv :1\n61 0.\n01 80\n7v 1\n[ cs\n.D C\n] 6\nO ct\n2 01\n6\nimplemented, such as Hadoop [20], Twister [21], Phoenix [22] and Mars [23], which all can help developers to parallelize traditional algorithms by using MapReduce model. For example, Apache Mahout [24] is machine learning libraries, and produces implementations of parallel scalable machine learning algorithms on Hadoop platform by using MapReduce.\nIn the previous work, Zhang et al. developed a parallel algorithm for computing rough set approximations based on MapReduce [25]. Based on that, Zhang et al. proposed a parallel rough set based knowledge acquisition method using MapReduce [26]. Afterwards, Qian et al. presented a parallel attribute reduction algorithm based on MapReduce [27]. However, all of these existing parallel methods make use of the classical MapReduce framework and are implemented on the Hadoop platform [20].\nIn this paper, we develop a computational method for attribute reduction that meets all the following criteria: 1) it can support devise attribute significance measures; 2) it is highly parallelizable; and 3) it can evaluate feature candidates simultaneously without evaluating one by one. Our main contributions are four-folds:\n• We propose a novel parallel method, called PLAR, for large-scale attribute reduction, which supports data parallelism and model parallelism. • Our method is a unified framework for attribute reduction, which means that various attribute reduction algorithms can be integrated into ours. • We implement our method, PLAR, on a generalpurpose in-memory dataflow system, SPARK [28], and conduct extensive experimental evaluation. In contrast, the most existing methods [6], [27] are designed 1) either for a single machine which means that the entire data must fit in the main memory and the parallelism is limited; 2) or for Hadoop which means that the data have to be loaded into the distributed memory frequently. • We test on the large sample size & high-demensional data, SDSS, with 320,000 samples and 5201 features, which is a real-world astronomical dataset. It takes about 7,000 seconds for one iteration.\nThe rest of this paper is organized as follows. Section 2 gives the elementary background introduction to feature selection and SPARK. In Section 3, we propose a parallel framework for feature selection and a unified representation of attributes’ significance evaluation functions. We describe our implementation in Section 4, and present the experimental analysis in Section 5. The paper ends with conclusions and future work in Section 6."
    }, {
      "heading" : "2 PRELIMINARIES",
      "text" : "In this section, we first review a unified framework for original feature selection as well as four representative significance measures of attributes, then introduce the distributed data processing platform used in this paper."
    }, {
      "heading" : "2.1 A Unified Framework for Feature Selection",
      "text" : "Figure 1 shows a unified framework for sequential feature selection. The first step is generating several attribute subsets. The second step is to employ an attribute significance\nmeasure to evaluate all generated candidates, and outputs the current optimal feature subset. The third step is checking whether the stopping criterion (e.g., number of attributes) is satisfied, a) if yes, output the current optimal feature subset as final optimal subset; b) otherwise, it goes back the first step, generating candidates then evaluating until the selected feature subset meets the stopping criterion.\nIn this framework, the key component is the evaluating and almost the computational work comes from this step. Hence, the efficiency of feature selection depends on evaluating. The main contribution of this paper is to extend this framework and propose a parallel framework to accelerate the whole process of evaluating. For the evaluation of attributes, there are mainly two general methods: a) wrapper which employs a learning algorithm (e.g. support vector machine) to evaluate; b) filter which measures the attributes’ significance with a metric, including information gain [29], distance [30], dependency [31] and consistency [4], [32]. The representative significance measures of attributes used in this paper are all based on rough set theory. It would also be interesting to study how to use our proposed framework to scale up on other attribute reduction algorithms."
    }, {
      "heading" : "2.1.1 Heuristic attribute reduction algorithm",
      "text" : "To select optimal feature subset efficiently and effectively, many heuristic attribute reduction algorithms were proposed during pasting two decades [33], [29], [34], [35], most of which make use of forward search strategy. In each forward heuristic attribute reduction algorithm, starting with the attributes (called core) with the satisfied inner importance (e.g., greater than a threshold), it takes the attribute with the maximal outer significance into the feature subset iteratively until the selected feature subset meets the stopping criterion, and finally we can get an attribute reduct [6].\nLet S = (U,C∪D) be a decision table,B ⊆ C . We denote the inner and outer importance measures of an attribute a as Siginner∆ (a,B,D) and Sig outer ∆ (a,B,D), respectively. In general, inner is used to measure an internal attribute a ∈ B by removing it from B; outer is employed to measure an external attribute a by adding it into B. Four representative metrics ∆ ∈ {PR, SCE,LCE,CCE} for measuring the attribute’s importance will be introduced in Section 2.1.2. Based on these notation, we give the unified definition for computing the attribute core and selecting the best attribute from the candidates as follows. Definition 2.1 (Attribute Core). Let S = (U,C ∪ D) be a\ndecision table. Siginner∆ (a,C,D) is an inner importance measure of an attribute a, and is a threshold. The core of attributes, termed Core, is defined as\nCore = {a|Siginner∆ (a,C,D) > , a ∈ C} (1)\nDefinition 2.2 (Optimal Attribute). Let S = (U,C ∪ D) be a decision table, B ⊆ C . Sigouter∆ (a,B,D) is an outer importance measure of an attribute a. The optimal attribute aopt is defined as\naopt = arg max a∈C\\B\n{Sigouter∆ (a,B,D)} (2)\nFigure 2 shows the relationship between core and reduct. In detail, a general forward heuristic attribute reduction algorithm can be written as follows. core_and_reduct\nAlgorithm 1: A general forward heuristic attribute reduction algorithm\nInput: A decision table S = (U,C ∪D), attribute importance measure metric ∆, threshold\nOutput: An attribute reduct R 1 Core = {a|Siginner∆ (a,C,D) > , a ∈ C} ;\n// Siginner∆ (a,C,D) is the inner importance measure of the attribute a\n2 R ←− Core ; 3 while stopping criterion not met & C \\ R 6= ∅ do 4 aopt = arg max\na∈C\\R {Sigouter∆ (a,R, D)} ;\n// Sigouter∆ (a,R, D) is the outer importance measure of the attribute a\n5 R ←− R∪ {aopt}; 6 return R"
    }, {
      "heading" : "2.1.2 Representative significance measures of attributes",
      "text" : "For efficient attribute reduction, many heuristic attribute reduction methods have been developed in rough set theory [33], [29], [34], [35]. Further, from the viewpoint of heuristic functions, Qian et al. classified these attribute reduction methods into four categories: positive-region reduction, Shannon’s conditional entropy reduction, Liang’s conditional entropy reduction and combination conditional entropy reduction [6]. We here also only focus on these four representative attribute reduction methods.\nGiven a decision table S = (U,C ∪ D), B ⊆ C , the condition partition U/B = {E1, E2, · · · , Ee} and the decision partition U/D = {D1, D2, · · · , Dm} can be obtained. Through these notations, we briefly review four types of significance measures of attributes as follows.\n(1) Positive-region based method Definition 2.3 (PR). Let S = (U,C ∪D) be a decision table,\nB ⊆ C . The dependency degree of D to B is defined as γB(D) = |POSB(D)| |U | ,\nwhere POSB(D) = m⋃ i=1 {Ei ∈ U/B : Xi ⊆ Y1 ∨ Xi ⊆\nY2 ∨ · · · ∨Xi ⊆ Yn} = m⋃ i=1 {Xi ∈ U/B : |Xi/D| = 1}.\nDefinition 2.4 (PR significance). Let S = (U,C ∪ D) be a decision table, B ⊆ C . The inner and outer significance measure of a based on PR, denoted by SiginnerPR (a,B,D) and SigouterPR (a,B,D), are respectively defined as\nSiginnerPR (a,B,D) = γB(D)− γB\\{a}(D),∀a ∈ B SigouterPR (a,B,D) = γB∪{a}(D)− γB(D), ∀a ∈ C\\B\n(2) Shannon’s conditional entropy based method (SCE) Definition 2.5 (SCE). Shannon’s conditional entropy of D\nwith respect to B is defined as\nH(D|B) = − e∑\ni=1 p(Ei) m∑ j=1 p(Dj |Ei) log(p(Dj |Ei)) (3)\nwhere p(Ei) = |Ei| |U | , p(Dj |Ei) = |Ei∩Dj | |Ei| .\nDefinition 2.6 (SCE significance). Let S = (U,C ∪ D) be a decision table, B ⊆ C . The inner and outer significance measure of a based on SCE, denoted by SiginnerSCE (a,B,D) and Sig outer SCE (a,B,D), are respec-\ntively defined as\nSiginnerSCE (a,B,D) = H(D|B\\{a})−H(D|B), ∀a ∈ B SigouterSCE (a,B,D) = H(D|B)−H(D|B ∪ {a}), ∀a ∈ C\\B\n(3) Liang’s conditional entropy based method (LCE) Definition 2.7 (LCE). Liang’s conditional entropy of D with\nrespect to B is defined as\nHL(D|B) = e∑\ni=1 m∑ j=1 |Dj ∩ Ei| |U | |Dcj \\ Eci | |U | (4)\nwhere Ec means the complement of the set E.\nDefinition 2.8 (LCE significance). Let S = (U,C ∪ D) be a decision table, B ⊆ C . The inner and outer significance measure of a based on LCE, denoted by SiginnerLCE (a,B,D) and Sig outer LCE (a,B,D), are respec-\ntively defined as\nSiginnerLCE (a,B,D) = HL(D|B\\{a})−HL(D|B),∀a ∈ B SigouterLCE (a,B,D) = HL(D|B)−HL(D|B ∪ {a}), ∀a ∈ C\\B\n(4) Combination conditional entropy based method (CCE) Definition 2.9 (CCE). Combination conditional entropy of\nD with respect to B is defined as\nHQ(D|B) = e∑\ni=1\n |Ei| |U | C2|Ei| C2|U | − m∑ j=1 |Ei ∩Dj | |U | C2|Ei∩Dj | C2|U |  (5)\nwhere C2|Ei| = |Ei|×(|Ei|−1)\n2 denotes the number of the pairs of the objects which are not distinguishable each other in the equivalence class Ei.\nDefinition 2.10 (CCE significance). Let S = (U,C ∪ D) be a decision table, B ⊆ C . The inner and outer significance measure of a based on CCE, denoted by SiginnerCCE (a,B,D) and Sig outer CCE (a,B,D), are respec-\ntively defined as\nSiginnerCCE (a,B,D) = HQ(D|B\\{a})−HQ(D|B), ∀a ∈ B SigouterCCE (a,B,D) = HQ(D|B)−HQ(D|B ∪ {a}), ∀a ∈ C\\B\nIntuitively, these four significance measures of attributes are listed in Table 1. To keep the notation consistent, we define γ(D|B) def= −γB(D), hence, γ(·), H(·), HL(·) and HQ(·) can be written as a unified form, i.e., Θ(·). And, the inner and outer importance measures can be computed by Siginner∆ = Θ(D|B\\{a}) − Θ(D|B) and Sigouter∆ = Θ(D|B)−Θ(D|B∪{a}) , respectively. Thus, it is easy to see that the attribute’s significance measure can be transformed to the computation of the Θ(D|B)."
    }, {
      "heading" : "2.2 SPARK",
      "text" : "We propose a unified framework for parallel large-scale attribute reduction, which is distributed in nature. Therefore, in principle, it can be implemented on any distributed data processing platforms (e.g. HADOOP, SPARK). In our implementation, SPARK [28] is chosen because of its properties: a) SPARK has currently developed into a full-fledged, generalpurpose distributed computation engine for large-scale data processing, and facilitates in-memory cluster computing, which is essential for iterative algorithms; b) it provides easy-to-use operations to build parallel, distributed and fault-tolerant applications; c) it is implemented in Scala, programming APIs in Scala, Java, R, and Python makes SPARK much more accessible to a range of data scientists who can take fast and full advantage of the Spark engine.\nAt a high level, every SPARK application consists of a driver program that runs the users main function and executes various parallel operations on a cluster. The main abstraction SPARK provides is a Resilient Distributed Dataset (RDD) [36] that allows applications to keep data in the shared memory of multiple machines and can be operated on in parallel. RDDs are fault-tolerant since SPARK can automatically recover lost data. Formally, an RDD is a read-only, partitioned collection of records, where two types of operations over RDDs are available: transformations which create a new RDD from an existing one, and actions which return a value to the driver program after running a computation on the RDD. Transformations used in our implementations mainly include map, flatMap, and reduceByKey. Specifically, map passes each dataset element through a function and returns a new RDD representing the results which is typically one-to-one mapping of the input RDD, and flatMap constructs a one-to-many of the input; reduceByKey works on an RDD of key-value pairs and generates a new RDD of key-value pairs where the values for each key are aggregated using the given reduce function. These operations are similar to the map and reduce operations in the traditional MAPREDUCE [19] framework. We use two actions: collect and sum, the former returns all elements of an RDD and the latter returns the sum of all elements of an RDD. In additional to these operations, one can call cache or persist to indicate which RDD to be\nreused in future and SPARK will cache persistent RDDs in memory, which radically reduces the computational time."
    }, {
      "heading" : "3 PARALLEL LARGE-SCALE ATTRIBUTE REDUCTION",
      "text" : "In this section, we first propose a hybrid parallel framework for attribute reduction, then present a unified representation of evaluation functions, finally give a SPARK-based algorithm for large-scale attribute reduction."
    }, {
      "heading" : "3.1 A Parallel Framework for Attribute Reduction",
      "text" : "We here give a parallel framework for attribute reduction. It mainly has the following stages: 1) Generating candidates using a certain search strategy (e.g., heuristic search in this paper). In general, this stage generates a pool of attributes. We propose to leverage a multi-processing method to parallelize the processing of evaluation of multiple attributes. As this procedure is model-related, we call the method as Model-Parallelism (MP). 2) With lunching a pool of worker processes, each of which is used to compute the significance of an attribute in SPARK. Here, R is the attribute reduct in the current loop. This processing makes use of SPARK to parallelize the processing of the computation of an attribute’s importance. Hence it is named as Data-Parallelism (DP). 3) After computing all features’ significance, the optimal feature is selected according to the best significance and inserted into the R. Then, it goes into next loop. By combining MP and DP, our proposed framework formally supports both model and data parallelism. Therefore, we term this framework model-and-data-parallelism (MDP)."
    }, {
      "heading" : "3.2 A Unified Representation of Evaluation Functions",
      "text" : "In this section, we first introduce the simplification and decomposition of evaluation functions which is a unified view for computing evaluation functions’ value. Based on it, we give a MapReduce-based method."
    }, {
      "heading" : "3.2.1 Simplification and Decomposition of Evaluation Functions",
      "text" : "Corollary 3.1. Let S = (U,C ∪ D) be a decision ta-\nble, B ⊆ C . U/B = {E1, E2, · · · , Ee} and U/D = {D1, D2, · · · , Dm} are condition and decision partitions, respectively. The positive region POSB(D) is calculated as follows\nPOSB(D) = e⋃\ni=1\n{Ei ∈ U/B : |Ei/D| = 1} (6)\nwhere Ei/D is the decision partition of Ei. Proof 1. Given U/D = {D1, D2, · · · , Dm}, ∀Ei ∈ U/B. i) If ∃Dj ∈ U/D, Ei ⊆ Dj , then Ei ⊆ R(Dj) ⊆ POSB(D), and |Ei/D| = 1; ii) If @Dj ∈ U/D, Ei ⊆ Dj , then Ei 6⊆ POSB(D) and |Ei/D| > 1. To sum up, we have\nPOSB(D) = m⋃ j=1\n( e⋃\ni=1\n{Ei ∈ U/B : Ei ⊆ Dj} )\n= e⋃\ni=1  m⋃ j=1 {Ei ∈ U/B : Ei ⊆ Dj} \n= e⋃\ni=1\n{Ei ∈ U/B : Ei ⊆ D1 ∨ Ei ⊆ D2∨\n· · · ∨ Ei ⊆ Dm}\n= e⋃\ni=1\n{Ei ∈ U/B : |Ei/D| = 1}.\nDefinition 3.1 (Multiset). Let S = (U,C ∪D) be a decision table, B ⊆ C . U/B = {E1, E2, · · · , Ee} and U/D = {D1, D2, · · · , Dm} are condition and decision partitions, respectively. ∀Ei ∈ U/B, the multiset of the decision partition of Ei, denoted by Ei//D, is defined as\nEi//D = {Di1, Di2, · · · , Dim} (7) where Dij = Ei ∩Dj , ∀Dj ∈ U/D.\nCorollary 3.2. Let S = (U,C ∪ D) be a decision table, B ⊆ C . U/B = {E1, E2, · · · , Ee} and U/D = {D1, D2, · · · , Dm} are condition and decision partitions of U , respectively. Ei/D = {Yi1, Yi2, · · · , Yil} is the decision partition ofEi, ∀Ei ∈ U/B. Then, the multiset of the decision partition of Ei, Ei//D = {Di1, Di2, · · · , Dim} can be computed by\nDij =\n{ Y, ∃!Y ∈ Ei/D, ⇀ Y = ⇀ Dij\n∅, else ,∀j = 1, · · · ,m\nProof 2. If ∃!Y ∈ Ei/D, ⇀ Y = ⇀ Dij , because ⇀ Dij = ⇀ Dj ,\ntherefore ⇀ Y = ⇀ Dj . As Y ⊆ Ei ⊆ U and Dj ∈ U/D, according to the definition of equivalence partition [37], we have Y ⊆ Dj . Thus, Y = Ei ∩ Dj = Dij , that is Dij = Y .\n(1) Decomposition of PR-based Evaluation Function According to Corollary 3.1, we have\nγ(D|B) = −|POSB(D)||U | = e∑\ni=1\n( −|Ei|sgnPR(Ei)|U | )\nwhere sgnPR(Ei) = {\n1, |Ei/D| = 1 0, else .\n(2) Decomposition of SCE-based Evaluation Function According to Corollary 3.2, we have\nH(D|B) = − e∑\ni=1 p(Ei) m∑ j=1 p(Dj |Ei) log(p(Dj |Ei))\n= − e∑\ni=1\n|Ei| |U | m∑ j=1 |Ei ∩Dj | |Ei| log ( |Ei ∩Dj | |Ei| )\n= e∑\ni=1 − 1|U | m∑ j=1 |Dij | log |Dij | |Ei|  (3) Decomposition of LCE-based Evaluation Function\nAccording to Corollary 3.2, we have\nHL(D|B) = e∑\ni=1 m∑ j=1 |Dj ∩ Ei| |U | |Dcj − Eci | |U |\n= e∑\ni=1 m∑ j=1 |Dj ∩ Ei| |U | |Ei −Dj | |U |\n= e∑\ni=1  m∑ j=1 |Dj ∩ Ei| |U | |Ei| − |Dj ∩ Ei| |U |  =\ne∑ i=1  m∑ j=1 |Dij |(|Ei| − |Dij |) |U |2  (4) Decomposition of CCE-based Evaluation Function\nAccording to Corollary 3.2, we have\nHQ(D|B) = e∑\ni=1\n |Ei| |U | C2|Ei| C2|U | − m∑ j=1 |Ei ∩Dj | |U | C2|Ei∩Dj | C2|U |  =\ne∑ i=1 ( |Ei| |U | |Ei| × (|Ei| − 1) C2|U | − m∑ j=1 |Ei ∩Dj | |U | |Ei ∩Dj | × (|Ei ∩Dj | − 1) C2|U |\n =\ne∑ i=1\n( |Ei|2 × (|Ei| − 1)\n|U |C2|U | −\nm∑ j=1 |Ei ∩Dj |2 × (|Ei ∩Dj | − 1) |U |C2|U |  =\ne∑ i=1\n( |Ei|2 × (|Ei| − 1)\n|U |C2|U | −\nm∑ j=1 |Dij |2 × (|Dij | − 1) |U |C2|U |  According to the above corollaries and transformation, these four evaluation functions can be written as a unified form as follows\nΘ(D|B) = e∑\ni=1\nθ(Si) (8)\nwhere Si def = (Ei, D), four evaluation functions’ sub function θ are shown in Table 2."
    }, {
      "heading" : "3.2.2 MapReduce-based Method",
      "text" : "According to the aforementioned simplification and decomposition, we describe the basic MapReduce-based method below, as shown in Figure 4. The detailed SPARK-based algorithm will be introduced in Section 4. Note that the basic parallel idea is similar, but SPARK provides much more exhaustive API beyond the native MAP and REDUCE functions.\n• MAP phase: each Map worker reads data split Uk from distributed file system (e.g. HDFS), and then mapping its elements into key-value pair ( ⇀ xB , ⇀ xD).\nThe main function of this processing is dividing Uk into equivalence classes w.r.t. the attribute set B. • REDUCE phase: each Reduce worker receives a group\nof data, whose key and value are ⇀ EiB and Si =\n(Ei, D) respectively. Then, it computes the subevaluation function’s value θ(Si). • SUM phase: finally, the main function collects the results from all Reduce workers, and calculates the sum over these collected values that is the evaluation function’s value Θ(D|B)."
    }, {
      "heading" : "3.3 Granularity Representation",
      "text" : "We propose to employ a granularity representation method to accelerate the processing procedure of feature selection. From the perspective of Granular Computing (GrC), U in a decision table can be partitioned into different-scale (namely granularity) disjoint sets by different attribute sets. We first define the granularity representation of a decision table below. Definition 3.2 (Granularity Representation). Let S =\n(U,A) be a decision table where A = {a1, · · · , a|A|}. U/A is a partition over U w.r.t. A. The granularity representation of S w.r.t. A, denoted as G(A), defined as follows,\nG(A) = {( ⇀ EA, |EA|) : EA ∈ U/A} (9)\nwhere ⇀ EA def = 〈va1 , · · · , va|A|〉 is the feature vector representation of the equivalence class EA w.r.t. A, and vai is the value on the attribute ai, ∀ai ∈ A ; |EA| denotes the cardinality of EA.\nExample 1. A decision table S = (U,C ∪ D) is given in Table 3, where C = {a1, a2} is the conditional attribute set and D is the decision set. Assume that A = C ∪ D, the partition over U w.r.t. A can be computed as U/A = {E1, E2, · · · , E5}, where E1 = {x1, x2}, E2 = {x3}, E3 = {x4, x5, x6}, E4 = {x7}, and E5 = {x8}. Taking E1 for example, its feature vector is ⇀ E1 = 〈0, 0, Y 〉 and\nthe associated cardinality is |E1| = 2. The more details of the granularity representation can be found in Table 4.\nLet S = (U,A) be a decision table, two attribute sets P and Q, P ⊆ Q ⊆ A. We define partial relations , as follows: ⇀ EP ⇀ EQ (or ⇀ EQ ⇀ EP ) if and only if ∀pi ∈ P , there exists qj = {pi} ∩ Q such that vqj = vpi , where P = {p1, · · · , p|P |}, Q = {q1, · · · , q|Q|}, ⇀ EP =\n〈vp1 , vp2 , · · · , vp|P |〉 and ⇀ EQ = 〈vq1 , vq2 , · · · , vq|Q|〉. With these notation, we define the granulating relation as follows.\nDefinition 3.3 (Granulating Relation). Let G(P ) and G(Q)\nbe two granularity representations of the decision table S = (U,A) w.r.t. P and Q, P ⊆ Q ⊆ A. G(P ) v G(Q) (or G(P ) w G(Q)), if and only if ∀EP ∈ U/P , ∃EQ ∈ U/Q such that ⇀ EP ⇀ EQ and EQ ⊆ EP .\nThe granulating relation reveals the relationships between different granularity representations. Subsequently, we introduce two operations, coarsing and refining, that formally describes how to switch between two granularity representations.\nCorollary 3.3 (Coarsing). Given a granularity representation\nG(Q) = {( ⇀ EQ, |EQ|) : EQ ∈ U/Q}, ∀P ⊆ Q, the coarse granularity representation G(P ) = {( ⇀ EP , |EP |) : EP ∈ U/P} is computed by\n∀EP ∈ U/P,EP = ∪{EQ ∈ U/Q| ⇀ EP ⇀ EQ}\nCorollary 3.4 (Refining). Given a granularity representation\nG(P ) = {( ⇀ EP , |EP |) : EP ∈ U/P}, ∀Q ⊇ P , the refining granularity representation G(Q) = {( ⇀ EQ, |EQ|) : EQ ∈ U/Q} is computed by\nG(Q) = {( ⇀ EQ, |EQ|) : EQ ∈ {EP /Q− P |EP ∈ U/P}}\nExample 2 (Example 1 continued). Let P = {a2} and Q = {a2, D}. So we have U/P = {{x1, x2, x3, x7}, {x4, x5, x6, x8}} and U/Q = {{x1, x2}, {x3, x7}, {x4, x5, x6, x8}}. Figure 5 depicts the coarsing and refining operations."
    }, {
      "heading" : "4 IMPLEMENTATION",
      "text" : "In this section, we describe the implementation of our algorithms on top of SPARK [28]. As introduced in Section 3,\nthe key components of the MDP framework are modelparallelism and data-parallelism. To achieve the best possible performance, we consider both GrC-based initialization and MDP. Algorithm 2 outlines the details of the parallel large-scale attribute reduction (PLAR) algorithm.\n• The first stage leverages the granularity representation introduced in Section 3.3 to do the GrC-based initialization (lines 1-2), which is consisted of 1) loading data from distributed file system (e.g. HDFS) via the function “sc.textFile()”; 2) constructing the granularity representation of the decision table; 3) caching the granularity representation in the distributed memory (line 2). It means that loading data and constructing the granularity representation are only executed once in the whole computing processing. • The second stage is getting the attribute core (lines 3- 8). We first generates the feature candidates (line 3), then use MP to execute a parallel-for loop to compute evaluation function’s value simultaneously (lines 4- 5) where COMPUTINGEF is a DP-based method for computing evaluation function’s value. After that, we compute the significance of all candidates (lines 6-7), and select the satisfied attribute (i.e. greater than a threshold here) into the attribute core (line 8). • The third stage is computing the attribute reduct (lines 9-14). First, we initialize the attribute reduct R as Core (line 9). We invoke the iterative processing of feature selection which selects the optimal feature into the attribute reduct R (lines 10-14). In each iteration, we compute evaluation function’s value using MP simultaneously (lines 11-12). Then we select the best attribute (line 13) and put it into R (line 14). • Procedure COMPUTINGEF describes the processing for computing evaluation function’s value using the SPARK-style language. Given the granularity representation aRDD def= GC∪D , we first compute the granularity representation GB∪D according to Corollary 3.3 (line 16). Then, we use DP to compute the evaluation function’s value (line 17) and the details are already described in Section 3.2. To easily understand it, we give an illustration as follows.\nExample 3 (Example 1 continued, ∆ = PR). Figure 6 shows an example for computing evluation function’s value on SPARK. (1) The original decision table is stored in HDFS. With GrC initialization, we can get the granularity representa-\nAlgorithm 2: PLAR\nInput: Input files: input (i.e., decision table S = (U,C ∪D)), attribute importance measure metric ∆, threshold Output: An attribute reduct R // Initialization: constructing a granularity representation G(C∪D), namely aRDD\n1 aRDD def = G(C∪D) ←− sc.textFile(input)\n.map(line⇒ parseVector())\n.reduceByKey(add); 2 aRDD.cache() ; 3 Cands←− {C} ∪ {C \\ {a}|a ∈ C}; 4 for B ∈ Cands do in parallel // Model parallelism: multiprocessing execution 5 Θ(D|B) = COMPUTINGEF(aRDD,D,B,∆); 6 for a ∈ C do 7 Siginner∆ (a,C,D)←− Θ(D|C \\ {a})−Θ(D|C); 8 Core = {a|Siginner∆ (a,C,D) > , a ∈ C}; 9 R ←− Core;\n10 while stopping criterion not met & C \\ R 6= ∅ do 11 for a ∈ C \\ R do in parallel // Model parallelism: multiprocessing execution 12 Θ(D|R ∪ {a}) = COMPUTINGEF(aRDD,D,R∪ {a},∆); 13 aopt = arg min\na∈C\\R {Θ(D|R ∪ {a})};\n14 R ←− R∪ {aopt}; 15 return R // Computing Evaluation Function’s Value Procedure COMPUTINGEF(aRDD,D,B,∆)\n16 bRDD def = G(B∪D) ←− aRDD.map( ( ⇀ EC∪D, |EC∪D| ) ⇒ ( ⇀ EB∪D, |EB∪D| ) )\n17 Θ(D|B) = bRDD.map( ( ⇀ EB∪D, |EB∪D| ) ⇒ ( ⇀ EB , ( ⇀ ED, |EB∪D|) ) )\n.reduceByKey(Si ⇒ θ(Si))\n.sum(); 18 return Θ(D|B)\ntion of the decision table. (2) Here, we suppose to evaluateB = {a2}. Using map(), it produces ( ⇀ EB , ( ⇀ ED, |EB∪D|) ) . Taking “〈0, 0, Y 〉, 2”\nfor example, we have ⇀ EB = ⇀ Ea2 = 〈0〉, ⇀ ED = 〈Y 〉 and |EB∪D| = 2. Therefore, its output is “〈0〉, (〈Y 〉, 2)”. So are others. (3) reduceByKey() is employed to aggregate the values with same key and then compute the sub-evaluation function’s value. Taking “key = 〈1〉” for example, according to PR’s definition (see Table 2),− |Ei|sgnPR(Ei)|U | = − 48 . (4) sum() is computing the sum of all sub-evaluation functions’ value."
    }, {
      "heading" : "4.1 Complexity Analysis",
      "text" : "Assume that S = (U,A) is a decision table, therefore it requires O(|U ||A|) to store the data. With the GrC-based initialization, the data is only read once when loading, and only the feature vectors of equivalence classes and associated cardinality are stored. Thus, the space complexity is O(|U/A|(|A|+ 1)) = O(|U/A||A|) where |U/A| is the total number of equivalence classes over U w.r.t. A."
    }, {
      "heading" : "5 EXPERIMENTAL RESULTS",
      "text" : "In this section, we evaluate PLAR experimentally. We first describe the experimental setup in Section 5.1. Then we report the results in terms of effectiveness and efficiency in Section 5.2 and Section 5.3, respectively. We show effect of PLAR’s components in Section 5.4."
    }, {
      "heading" : "5.1 Setup",
      "text" : "Cluster. We perform all experiments on a cluster of 19 machines, each with at least 8-core 2.0 GHz processors, more than 8 GB RAM, running Cent OS 6.5. All machines are connected via a Gigabit network. We deploy SPARK of\nversion 1.x on each machine in the cluster, and configure one machine as the master node and the other ones as slaves. The total number of cores used by SPARK is 128. All our algorithms are implemented in the Python programming language and PySpark. Datasets. We tested on 13 benchmark datasets of various scales. The description of each dataset, including its samples and features, is shown in Table 5. Among them, Datasets 1-9 are small datasets, downloaded from UCI dataset repository1; Datasets 10 and 11 have millions of samples, and are respectively obtained from KDD CUP 19992 and generated by the WEKA3 data generator. Dataset 12 is a highdimensional dataset, which was selected as the NIPS 2003 feature selection challenge4; Dataset 13 is really big dataset (large & high-dimension), collected from the astronomical data repository5. All datasets are uploaded into the HDFS before computation.\nParameter. The model parallelism level is a parameter in PLAR, which means the maximum concurrency number for computing attributes’ significance measures."
    }, {
      "heading" : "5.2 Effectiveness",
      "text" : "To evaluate the effectiveness of PLAR, we compare it with two baselines, including HAR and FSPA, which are described below.\n• HAR is an original forward Heuristic Attribute Reduction algorithm (see Algorithm 1). • FSPA [6] is a general Feature Selection based on the Positive Approximation, which is a state-of-the-art algorithm in a single machine.\nWe evaluate PLAR by four representative significance measures of attributes, including PR, SCE, LCE and CCE (see details in Section 2.1). Since computing and storing for the traditional single-machine-based approaches is impractical for large datasets, we use Datasets 1-9 of Table 5 for this test. The used cores’ number and the model parallelism level are both set as 8 for PLAR.\n1. http://archive.ics.uci.edu/ml/ 2. http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html 3. http://www.cs.waikato.ac.nz/ml/weka/ 4. https://archive.ics.uci.edu/ml/datasets/Gisette 5. http://www.sdss.org/data/\nTables 6-9 show the elapsed time and attribute reduction of the algorithms HAR, FSPA and PLAR using four different attribute measures. Taking PR for example, from Table 6, we can observe that the feature subset obtained by PLAR is the same as the algorithms HAR and FSPA. The similar results in Tables 7-9 demonstrate that our proposed parallel algorithm can produce the consistent attribute reduction. We find that FSPA is much faster in several small datasets such as Breast-cancer-wisconsin, and PLAR can achieve better performance in some large datasets, including Shuttle, Letter-recognition and Ticdata2000."
    }, {
      "heading" : "5.3 Efficiency",
      "text" : "To measure the efficiency of PLAR, we evaluate it on four aspects below.\n• Comparison with single-machine algorithms: see Section 5.3.1. • Comparison with distributed algorithms: see Section 5.3.2. • Effect of model parallelism level: see Section 5.4.2. • Speedup on large & high-dimensional data: see Sec-\ntion 5.3.3."
    }, {
      "heading" : "5.3.1 Comparison with single-machine algorithms",
      "text" : "In this section, we compare PLAR with the state-of-the-art single machine algorithms on 9 small datasets. The used datasets and compared baseline algorithms are same with ones in Section 5.2. Besides, we perform an experiment on an algorithm called PLAR-DP, which is a simplified version of PLAR that only utilizes the data parallelism without the model parallelism. Because the algorithms HAR and FSPA can only run in a single machine, we perform PLAR and PLAR-DP on a 8-core single machine as well. PLAR’s model parallelism level is set as 8. We here use speedup = running time of a certain algorithmrunning time of HAR to measure all algorithms. From Figure 7, we can see FSPA outperform other algorithms in the datasets Tic-tac-toe, Dermatology, Breast-cancer-wisconsin and Backup-large.test, whose samples are very few, all less than 1,000. As the sample size increases, PLAR performs better and better, especially in the datasets Shuttle and Letter-recognition whose samples are more than 10,000, where PLAR can achieve 100×, even 1000× speedup. Taking LCE for example, against HAR, PLAR is about 6400× faster. The experimental results show that PLAR is always faster than PLAR-DP which verifies the benefit of the model parallelism."
    }, {
      "heading" : "5.3.2 Comparison with distributed algorithms",
      "text" : "In this experiment, we compare PLAR with some distributed algorithms, including HadoopAR, SparkAR and PLAR-DP, which are described as follows.\n• HadoopAR [38] is a parallel attribute reduction algorithm which is implemented in the Hadoop6 platform. • SparkAR is a modified version of HadoopAR which is implemented on the Spark platform.\n6. http://hadoop.apache.org/\nIn all of these experiments, we set the same core number, i.e., 16. Figure 8 shows the performance comparison of these 4 distributed algorithms. It is easy to see that SparkAR is much faster than HadoopAR because HadoopAR has to read the data from HDFS each time when evaluating the attribute. On the contrary, Spark-based algorithms always read the data once into the distributed memory which makes the same parallel method can achieve up to 100× speedup on Spark than on Hadoop. PLAR always outperforms other algorithms.\nTo display the results intuitively, speedup = running time of a certain algorithm\nrunning time of HadoopAR is employed here, as\nshown in Table 8. It demonstrates that SparkAR is always 6 − 7× faster than HadoopAR in the dataset KDD99, and reaches 2.48 − 3.24× in the dataset WEKA15360. PLAR-DP and PLAR perform outstandingly, and can speed up more than 200 times in KDD99. Specifically, when using the attribute measure LCE, PLAR achieves 500-fold improvement. In WEKA15360, PLAR is 50-fold faster than HadoopAR over all four attribute measures, and obtains 75.88× speedup when using the attribute measure PR."
    }, {
      "heading" : "5.3.3 Speedup on large & high-dimensional data",
      "text" : "In this experiment, we test our algorithms with different numbers of cores for attribute reduction on the dataset SDSS\nof Table 5. Because the overall running time for this dataset is very large, we test two configurations: 32 cores and 128 cores. Taking SCE and 128 cores for example, we record the running time for each iteration, the first 5 iterations cost 7312, 6696, 6793, 7659 and 7035 seconds, respectively. For the first iteration, there are totally 5201 feature candidates. Therefore, the average elapsed time for evaluating one attribute is about 1.406 seconds. When the same test is ran on 32 cores, the first iteration takes 24180 seconds that means 4.649 seconds per attribute. Hence, 4 times cores can achieve 4.649 1.406 ≈ 3.3× speedup.\nWe do the similar experiments on all four attribute measure methods, shown in Table 11. Comparing with 32 cores, performance of PR, SCE, LCE and CCE on 128 cores is 3.27, 3.31, 3.38 and 3.29 times faster, respectively. It demonstrates that it can efficiently reduce the running time as the core number increases."
    }, {
      "heading" : "5.4 Effect of components of PLAR",
      "text" : "The key components of PLAR are GrC-based initialization, data-parallelism, and model-parallelism. The effect of dataparallelism has been richly verified in Section 5.3. Here, we mainly shows the effects of the GrC-based initialization and model-parallelism."
    }, {
      "heading" : "5.4.1 Effect of GrC-based initialization",
      "text" : "In this experiment, we test the effect of GrC-based initialization on Datasets KDD99 and WEKA15360 of Table 5, as depicted in Figure 9. We can observe that the running with the GrC-based initialization are extremely less on both datasets KDD99 and WEKA15360 using four different attribute significance measures which demonstrates the GrC-based initialization can efficiently accelerate the whole processing of feature selection."
    }, {
      "heading" : "5.4.2 Effect of model parallelism level",
      "text" : "In this section, we test our algorithms with different model parallelism levels for attribute reduction on the dataset\nGisette of Table 5. We use 64 cores and the attribute measure method SCE for this experiment. Gisette is consisted of 5000 features and 6000 samples. Suppose that the attribute core is empty, it sequentially evaluates 5000 feature candidates in 1st iteration, 4999 ones in 2nd iteration, 4998 ones in 3rd iteration, . . ., which requires model parallelism more obviously. We here set different model parallelism levels: 1, 2, 4, 8, 16, 32 and 64. When the model parallelism level is equal to 1, PLAR is degraded into PLAR-DP (only data parallelism). We record the first 5 iterations’ running time, shown in Table 12. When the model parallelism level is 2, PLAR is twice faster than PLAR-DP. As the model parallelism level increases, the running time reduces obviously. Intuitively, we employ speedup = running time of PLARrunning time of PLAR-DP to show the importance of the model parallelism in PLAR. From Figure 10, we can clearly observe the speedup is the highest (i.e. 17.8×) when model parallelism level = 32."
    }, {
      "heading" : "6 CONCLUSIONS",
      "text" : "We have proposed and implemented a highly parallelizable method, PLAR, for large-scale attribute reduction. It consists of GrC-based initialization, model-parallelism and data-parallelism. The GrC-based initialization converts the original decision table into a granularity representation which reduce the space complexity from O(|U ||A|) into\nO(|U/A||A|), and can efficiently accelerate the computation. The model-parallelism is a natural parallel strategy which means we can evaluate all feature candidates at the same time. It becomes much more efficient when there are thousands of features. The data-parallelism means that we can compute a single attribute’s significance in parallel which benefits from our decomposition method to the evaluation functions. Extensive experimental results show that PLAR is more efficient and scalable than the existing solutions for large-scale problems."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "This work is supported by the National Science Foundation of China (Nos. 61573292, 61572406)."
    } ],
    "references" : [ {
      "title" : "The Principles and Methodologies of Big Data Mining—-From the Perspectives of Granular Computing and Rough Sets",
      "author" : [ "T. Li", "C. Luo", "H. Chen", "J. Zhang" ],
      "venue" : "Science Press (In Chinese),",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2016
    }, {
      "title" : "Pickt: A solution for big data analysis",
      "author" : [ "T. Li", "C. Luo", "H. Chen", "J. Zhang" ],
      "venue" : "International Conference on Rough Sets and Knowledge Technology. Springer, 2015, pp. 15–25.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Significance and challenges of big data research",
      "author" : [ "X. Jin", "B.W. Wah", "X. Cheng", "Y. Wang" ],
      "venue" : "Big Data Research, vol. 2, no. 2, pp. 59–64, 2015.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Consistency-based search in feature selection",
      "author" : [ "M. Dash", "H. Liu" ],
      "venue" : "Artificial Intelligence, vol. 151, no. 1, pp. 155–176, 2003.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Selecting discrete and continuous features based on neighborhood decision error minimization",
      "author" : [ "Q. Hu", "W. Pedrycz", "D. Yu", "J. Lang" ],
      "venue" : "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), vol. 40, no. 1, pp. 137–150, 2010.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Positive approximation: An accelerator for attribute reduction in rough set theory",
      "author" : [ "Y.H. Qian", "J.Y. Liang", "W. Pedrycz", "C.Y. Dang" ],
      "venue" : "Artificial Intelligence, vol. 174, no. 9-10, pp. 597–618, Jun. 2010.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Hybrid attribute reduction based on a novel fuzzy-rough model and information granulation",
      "author" : [ "Q. Hu", "Z. Xie", "D. Yu" ],
      "venue" : "Pattern Recognition, vol. 40, no. 12, pp. 3509–3521, 2007.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "An efficient accelerator for attribute reduction from incomplete data in rough set framework",
      "author" : [ "Y. Qian", "J. Liang", "W. Pedrycz", "C. Dang" ],
      "venue" : "Pattern Recognition, vol. 44, no. 8, pp. 1658–1670, 2011.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Attribute reduction for massive data based on rough set theory and mapreduce",
      "author" : [ "Y. Yang", "Z. Chen", "Z. Liang", "G. Wang" ],
      "venue" : "International Conference on Rough Sets and Knowledge Technology. Springer, 2010, pp. 672–678.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Rough sets: Theoretical aspects of reasoning about data",
      "author" : [ "Z. Pawlak" ],
      "venue" : "Dordrecht: Kluwer Academic Publishers,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1991
    }, {
      "title" : "Rudiments of rough sets",
      "author" : [ "Z. Pawlak", "A. Skowron" ],
      "venue" : "Information Sciences, vol. 177, no. 1, pp. 3–27, 2007.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Rough sets: Some extensions",
      "author" : [ "Z. Pawlak", "A. Skowron" ],
      "venue" : "Information Sciences, vol. 177, no. 1, pp. 28 – 40, 2007, zdzis?aw Pawlak life and work (19262006). [Online]. Available: http:// www.sciencedirect.com/science/article/pii/S0020025506001496  TECHNICAL REPORT  14",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Discovery through rough set theory",
      "author" : [ "W. Ziarko" ],
      "venue" : "Communications of the ACM, vol. 42, no. 11, pp. 54–57, 1999.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "A rough sets based characteristic relation approach for dynamic attribute generalization in data mining",
      "author" : [ "T.R. Li", "D. Ruan", "W. Geert", "J. Song", "Y. Xu" ],
      "venue" : "Knowledge-Based Systems, vol. 20, no. 5, pp. 485–494, 2007.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Rough sets based matrix approaches with dynamic attribute variation in set-valued information systems",
      "author" : [ "J. Zhang", "T. Li", "D. Ruan", "D. Liu" ],
      "venue" : "International Journal of Approximate Reasoning, vol. 53, no. 4, pp. 620–635, 2012.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "The incremental method for fast computing the rough fuzzy approximations",
      "author" : [ "Y. Cheng" ],
      "venue" : "Data & Knowledge Engineering, vol. 70, no. 1, pp. 84–100, 2011.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Neighborhood rough set based heterogeneous feature subset selection",
      "author" : [ "Q.H. Hu", "D.R. Yu", "J.F. Liu", "C.X. Wu" ],
      "venue" : "Information Sciences, vol. 178, no. 18, pp. 3577–3594, Sep. 2008.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "A group incremental approach to feature selection applying rough set technique",
      "author" : [ "J. Liang", "F. Wang", "C. Dang", "Y. Qian" ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering, vol. 26, no. 2, pp. 294–308, 2014.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Mapreduce: simplified data processing on large clusters",
      "author" : [ "J. Dean", "S. Ghemawat" ],
      "venue" : "Communications of the ACM, vol. 51, no. 1, pp. 107–113, 2008.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Hadoop: The definitive guide. ",
      "author" : [ "T. White" ],
      "venue" : "O’Reilly Media, Inc.”,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2012
    }, {
      "title" : "Twister: a runtime for iterative mapreduce",
      "author" : [ "J. Ekanayake", "H. Li", "B. Zhang", "T. Gunarathne", "S.-H. Bae", "J. Qiu", "G. Fox" ],
      "venue" : "Proceedings of the 19th ACM International Symposium on High Performance Distributed Computing. ACM, 2010, pp. 810–818.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Phoenix++: modular mapreduce for shared-memory systems",
      "author" : [ "J. Talbot", "R.M. Yoo", "C. Kozyrakis" ],
      "venue" : "Proceedings of the second international workshop on MapReduce and its applications. ACM, 2011, pp. 9–16.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Mars: a mapreduce framework on graphics processors",
      "author" : [ "B. He", "W. Fang", "Q. Luo", "N.K. Govindaraju", "T. Wang" ],
      "venue" : "Proceedings of the 17th international conference on Parallel architectures and compilation techniques. ACM, 2008, pp. 260–269.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Mahout in action",
      "author" : [ "S. Owen", "R. Anil", "T. Dunning", "E. Friedman" ],
      "venue" : "2012.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A parallel method for computing rough set approximations",
      "author" : [ "J.B. Zhang", "T.R. Li", "D. Ruan", "Z.Z. Gao", "C.B. Zhao" ],
      "venue" : "Information Sciences, vol. 194, pp. 209–223, 2012.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Parallel rough set based knowledge acquisition using mapreduce from big data",
      "author" : [ "J. Zhang", "T. Li", "Y. Pan" ],
      "venue" : "Proceedings of the 1st International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications. ACM, 2012, pp. 20–27.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Parallel attribute reduction algorithms using mapreduce",
      "author" : [ "J. Qian", "D. Miao", "Z. Zhang", "X. Yue" ],
      "venue" : "Information Sciences, vol. 279, pp. 671 – 690, 2014. [Online]. Available: http://www. sciencedirect.com/science/article/pii/S0020025514004666",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "The algorithm on knowledge reduction in incomplete information systems",
      "author" : [ "J.Y. Liang", "Z.B. Xu" ],
      "venue" : "International Journal of Uncertainty Fuzziness and Knowledge-based Systems, vol. 10, no. 1, pp. 95–103, Feb. 2002.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "The feature selection problem: Traditional methods and a new algorithm",
      "author" : [ "K. Kira", "L.A. Rendell" ],
      "venue" : "Proceedings of the 10th National Conference on Artificial Intelligence, 1992, pp. 129–134.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Feature selection using rough sets theory",
      "author" : [ "M. Modrzejewski" ],
      "venue" : "Machine Learning: ECML-93. Springer, 1993, pp. 213–226.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Consistency measure, inclusion degree and fuzzy measure in decision tables",
      "author" : [ "Y. Qian", "J. Liang", "C. Dang" ],
      "venue" : "Fuzzy sets and systems, vol. 159, no. 18, pp. 2353–2377, 2008.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "A new method for measuring uncertainty and fuzziness in rough set theory",
      "author" : [ "J.Y. Liang", "K.S. Chin", "C.Y. Dang", "R.C.M. Yam" ],
      "venue" : "International Journal of General Systems, vol. 31, no. 4, pp. 331–342, 2002.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Combination entropy and combination granulation in rough set theory",
      "author" : [ "Y.H. Qian", "J.Y. Liang" ],
      "venue" : "International Journal of Uncertainty Fuzziness and Knowledge-based Systems, vol. 16, no. 2, pp. 179–193, Apr. 2008.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Approximate entropy reducts",
      "author" : [ "D. Slezak" ],
      "venue" : "Fundamenta Informaticae, vol. 53, no. 3-4, pp. 365–390, Dec. 2002.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Resilient distributed  datasets: A fault-tolerant abstraction for in-memory cluster computing",
      "author" : [ "M. Zaharia", "M. Chowdhury", "T. Das", "A. Dave", "J. Ma", "M. McCauley", "M.J. Franklin", "S. Shenker", "I. Stoica" ],
      "venue" : "Proceedings of the 9th USENIX conference on Networked Systems Design and Implementation. USENIX Association, 2012, pp. 2–2.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Rough sets and boolean reasoning",
      "author" : [ "Z. Pawlak", "A. Skowron" ],
      "venue" : "Information Sciences, vol. 177, no. 1, pp. 41–73, 2007.",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Plar: Parallel large-scale attribute reduction on cloud systems",
      "author" : [ "J. Zhang", "T. Li", "Y. Pan" ],
      "venue" : "2013 International Conference on Parallel and Distributed Computing, Applications and Technologies. IEEE, 2013, pp. 184–191.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Enormous amounts of data are generated every day with the amazing spread of computers and sensors in a widerange of domains, including social media, search engines, insurance companies, health care organizations, financial industry and many others [2].",
      "startOffset" : 248,
      "endOffset" : 251
    }, {
      "referenceID" : 2,
      "context" : "Now we are in the era of big data, which is characterized by 5Vs [3]: 1) Volume means the amount of data that needs to be managed is very huge; 2) Velocity means that the speed of data update is very high; 3) Variety means that the data is varied in nature and there are",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 0,
      "context" : "An early version (in Chinese) can be found in Chapter 3 of the book [1].",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 3,
      "context" : "It also helps people better understand the data by telling them which are key features, and has been attracted much attention in recent years [4], [5], [6], [7], [8], [9].",
      "startOffset" : 142,
      "endOffset" : 145
    }, {
      "referenceID" : 4,
      "context" : "It also helps people better understand the data by telling them which are key features, and has been attracted much attention in recent years [4], [5], [6], [7], [8], [9].",
      "startOffset" : 147,
      "endOffset" : 150
    }, {
      "referenceID" : 5,
      "context" : "It also helps people better understand the data by telling them which are key features, and has been attracted much attention in recent years [4], [5], [6], [7], [8], [9].",
      "startOffset" : 152,
      "endOffset" : 155
    }, {
      "referenceID" : 6,
      "context" : "It also helps people better understand the data by telling them which are key features, and has been attracted much attention in recent years [4], [5], [6], [7], [8], [9].",
      "startOffset" : 157,
      "endOffset" : 160
    }, {
      "referenceID" : 7,
      "context" : "It also helps people better understand the data by telling them which are key features, and has been attracted much attention in recent years [4], [5], [6], [7], [8], [9].",
      "startOffset" : 162,
      "endOffset" : 165
    }, {
      "referenceID" : 8,
      "context" : "It also helps people better understand the data by telling them which are key features, and has been attracted much attention in recent years [4], [5], [6], [7], [8], [9].",
      "startOffset" : 167,
      "endOffset" : 170
    }, {
      "referenceID" : 9,
      "context" : "Rough set theory, introduced by Pawlak in 1982, is a soft computing tool for dealing with inconsistent information in decision situations [10], [11], [12], and plays an important role in the fields of pattern recognition, feature selection and knowledge discovery [13].",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 10,
      "context" : "Rough set theory, introduced by Pawlak in 1982, is a soft computing tool for dealing with inconsistent information in decision situations [10], [11], [12], and plays an important role in the fields of pattern recognition, feature selection and knowledge discovery [13].",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 11,
      "context" : "Rough set theory, introduced by Pawlak in 1982, is a soft computing tool for dealing with inconsistent information in decision situations [10], [11], [12], and plays an important role in the fields of pattern recognition, feature selection and knowledge discovery [13].",
      "startOffset" : 150,
      "endOffset" : 154
    }, {
      "referenceID" : 12,
      "context" : "Rough set theory, introduced by Pawlak in 1982, is a soft computing tool for dealing with inconsistent information in decision situations [10], [11], [12], and plays an important role in the fields of pattern recognition, feature selection and knowledge discovery [13].",
      "startOffset" : 264,
      "endOffset" : 268
    }, {
      "referenceID" : 5,
      "context" : "Attribute reduction in rough set theory provides a theoretic framework for consistencybased feature selection, which can retain the discernible ability of original features for the objects from the universe [6].",
      "startOffset" : 207,
      "endOffset" : 210
    }, {
      "referenceID" : 13,
      "context" : "To accelerate this process, incremental techniques combined with traditional rough set based methods are widely researched [14], [15], [16], [8], [6], [17], [18].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 14,
      "context" : "To accelerate this process, incremental techniques combined with traditional rough set based methods are widely researched [14], [15], [16], [8], [6], [17], [18].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 15,
      "context" : "To accelerate this process, incremental techniques combined with traditional rough set based methods are widely researched [14], [15], [16], [8], [6], [17], [18].",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 7,
      "context" : "To accelerate this process, incremental techniques combined with traditional rough set based methods are widely researched [14], [15], [16], [8], [6], [17], [18].",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 5,
      "context" : "To accelerate this process, incremental techniques combined with traditional rough set based methods are widely researched [14], [15], [16], [8], [6], [17], [18].",
      "startOffset" : 146,
      "endOffset" : 149
    }, {
      "referenceID" : 16,
      "context" : "To accelerate this process, incremental techniques combined with traditional rough set based methods are widely researched [14], [15], [16], [8], [6], [17], [18].",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 17,
      "context" : "To accelerate this process, incremental techniques combined with traditional rough set based methods are widely researched [14], [15], [16], [8], [6], [17], [18].",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 5,
      "context" : "introduced positive approximation, which is a theoretic framework for accelerating a heuristic process [6].",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 17,
      "context" : "developed a group of incremental feature selection algorithms based on rough sets [18].",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 18,
      "context" : "MapReduce, by Google, is a popular parallel programming model and a framework for processing big data on certain kinds of distributable problems using a large number of computers, collectively referred to as a cluster [19].",
      "startOffset" : 218,
      "endOffset" : 222
    }, {
      "referenceID" : 19,
      "context" : "implemented, such as Hadoop [20], Twister [21], Phoenix [22] and Mars [23], which all can help developers to parallelize traditional algorithms by using MapReduce model.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 20,
      "context" : "implemented, such as Hadoop [20], Twister [21], Phoenix [22] and Mars [23], which all can help developers to parallelize traditional algorithms by using MapReduce model.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 21,
      "context" : "implemented, such as Hadoop [20], Twister [21], Phoenix [22] and Mars [23], which all can help developers to parallelize traditional algorithms by using MapReduce model.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 22,
      "context" : "implemented, such as Hadoop [20], Twister [21], Phoenix [22] and Mars [23], which all can help developers to parallelize traditional algorithms by using MapReduce model.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 23,
      "context" : "For example, Apache Mahout [24] is machine learning libraries, and produces implementations of parallel scalable machine learning algorithms on Hadoop platform by using MapReduce.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 24,
      "context" : "developed a parallel algorithm for computing rough set approximations based on MapReduce [25].",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 25,
      "context" : "proposed a parallel rough set based knowledge acquisition method using MapReduce [26].",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 26,
      "context" : "presented a parallel attribute reduction algorithm based on MapReduce [27].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 19,
      "context" : "However, all of these existing parallel methods make use of the classical MapReduce framework and are implemented on the Hadoop platform [20].",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 5,
      "context" : "In contrast, the most existing methods [6], [27] are designed 1) either for a single machine which means that the entire data must fit in the main memory and the parallelism is limited; 2) or for Hadoop which means that the data have to be loaded into the distributed memory frequently.",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 26,
      "context" : "In contrast, the most existing methods [6], [27] are designed 1) either for a single machine which means that the entire data must fit in the main memory and the parallelism is limited; 2) or for Hadoop which means that the data have to be loaded into the distributed memory frequently.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 27,
      "context" : "support vector machine) to evaluate; b) filter which measures the attributes’ significance with a metric, including information gain [29], distance [30], dependency [31] and consistency [4], [32].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 28,
      "context" : "support vector machine) to evaluate; b) filter which measures the attributes’ significance with a metric, including information gain [29], distance [30], dependency [31] and consistency [4], [32].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 29,
      "context" : "support vector machine) to evaluate; b) filter which measures the attributes’ significance with a metric, including information gain [29], distance [30], dependency [31] and consistency [4], [32].",
      "startOffset" : 165,
      "endOffset" : 169
    }, {
      "referenceID" : 3,
      "context" : "support vector machine) to evaluate; b) filter which measures the attributes’ significance with a metric, including information gain [29], distance [30], dependency [31] and consistency [4], [32].",
      "startOffset" : 186,
      "endOffset" : 189
    }, {
      "referenceID" : 30,
      "context" : "support vector machine) to evaluate; b) filter which measures the attributes’ significance with a metric, including information gain [29], distance [30], dependency [31] and consistency [4], [32].",
      "startOffset" : 191,
      "endOffset" : 195
    }, {
      "referenceID" : 31,
      "context" : "1 Heuristic attribute reduction algorithm To select optimal feature subset efficiently and effectively, many heuristic attribute reduction algorithms were proposed during pasting two decades [33], [29], [34], [35], most of which make use of forward search strategy.",
      "startOffset" : 191,
      "endOffset" : 195
    }, {
      "referenceID" : 27,
      "context" : "1 Heuristic attribute reduction algorithm To select optimal feature subset efficiently and effectively, many heuristic attribute reduction algorithms were proposed during pasting two decades [33], [29], [34], [35], most of which make use of forward search strategy.",
      "startOffset" : 197,
      "endOffset" : 201
    }, {
      "referenceID" : 32,
      "context" : "1 Heuristic attribute reduction algorithm To select optimal feature subset efficiently and effectively, many heuristic attribute reduction algorithms were proposed during pasting two decades [33], [29], [34], [35], most of which make use of forward search strategy.",
      "startOffset" : 203,
      "endOffset" : 207
    }, {
      "referenceID" : 33,
      "context" : "1 Heuristic attribute reduction algorithm To select optimal feature subset efficiently and effectively, many heuristic attribute reduction algorithms were proposed during pasting two decades [33], [29], [34], [35], most of which make use of forward search strategy.",
      "startOffset" : 209,
      "endOffset" : 213
    }, {
      "referenceID" : 5,
      "context" : ", greater than a threshold), it takes the attribute with the maximal outer significance into the feature subset iteratively until the selected feature subset meets the stopping criterion, and finally we can get an attribute reduct [6].",
      "startOffset" : 231,
      "endOffset" : 234
    }, {
      "referenceID" : 31,
      "context" : "2 Representative significance measures of attributes For efficient attribute reduction, many heuristic attribute reduction methods have been developed in rough set theory [33], [29], [34], [35].",
      "startOffset" : 171,
      "endOffset" : 175
    }, {
      "referenceID" : 27,
      "context" : "2 Representative significance measures of attributes For efficient attribute reduction, many heuristic attribute reduction methods have been developed in rough set theory [33], [29], [34], [35].",
      "startOffset" : 177,
      "endOffset" : 181
    }, {
      "referenceID" : 32,
      "context" : "2 Representative significance measures of attributes For efficient attribute reduction, many heuristic attribute reduction methods have been developed in rough set theory [33], [29], [34], [35].",
      "startOffset" : 183,
      "endOffset" : 187
    }, {
      "referenceID" : 33,
      "context" : "2 Representative significance measures of attributes For efficient attribute reduction, many heuristic attribute reduction methods have been developed in rough set theory [33], [29], [34], [35].",
      "startOffset" : 189,
      "endOffset" : 193
    }, {
      "referenceID" : 5,
      "context" : "classified these attribute reduction methods into four categories: positive-region reduction, Shannon’s conditional entropy reduction, Liang’s conditional entropy reduction and combination conditional entropy reduction [6].",
      "startOffset" : 219,
      "endOffset" : 222
    }, {
      "referenceID" : 34,
      "context" : "The main abstraction SPARK provides is a Resilient Distributed Dataset (RDD) [36] that allows applications to keep data in the shared memory of multiple machines and can be operated on in parallel.",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 18,
      "context" : "These operations are similar to the map and reduce operations in the traditional MAPREDUCE [19] framework.",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 35,
      "context" : "As Y ⊆ Ei ⊆ U and Dj ∈ U/D, according to the definition of equivalence partition [37], we have Y ⊆ Dj .",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 5,
      "context" : "• FSPA [6] is a general Feature Selection based on the Positive Approximation, which is a state-of-the-art algorithm in a single machine.",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 36,
      "context" : "• HadoopAR [38] is a parallel attribute reduction algorithm which is implemented in the Hadoop6 platform.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 5,
      "context" : "Dataset Original features HAR-PR [6] FSPA-PR [6] PLAR-PR Time (s) Selected features Time (s) Selected features Time (s) Selected features Mushroom 22 24.",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 5,
      "context" : "Dataset Original features HAR-PR [6] FSPA-PR [6] PLAR-PR Time (s) Selected features Time (s) Selected features Time (s) Selected features Mushroom 22 24.",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 5,
      "context" : "Dataset Original features HAR-SCE [6] FSPA-SCE [6] PLAR-SCE Time (s) Selected features Time (s) Selected features Time (s) Selected features Mushroom 22 162.",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 5,
      "context" : "Dataset Original features HAR-SCE [6] FSPA-SCE [6] PLAR-SCE Time (s) Selected features Time (s) Selected features Time (s) Selected features Mushroom 22 162.",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 5,
      "context" : "Dataset Original features HAR-LCE [6] FSPA-LCE [6] PLAR-LCE Time (s) Selected features Time (s) Selected features Time (s) Selected features Mushroom 22 300.",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 5,
      "context" : "Dataset Original features HAR-LCE [6] FSPA-LCE [6] PLAR-LCE Time (s) Selected features Time (s) Selected features Time (s) Selected features Mushroom 22 300.",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 5,
      "context" : "Dataset Original features HAR-CCE [6] FSPA-CCE [6] PLAR-CCE Time Selected features Time (s) Selected features Time (s) Selected features Mushroom 22 166.",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 5,
      "context" : "Dataset Original features HAR-CCE [6] FSPA-CCE [6] PLAR-CCE Time Selected features Time (s) Selected features Time (s) Selected features Mushroom 22 166.",
      "startOffset" : 47,
      "endOffset" : 50
    } ],
    "year" : 2016,
    "abstractText" : "The rapid growth of emerging information technologies and application patterns in modern society, e.g., Internet, Internet of Things, Cloud Computing and Tri-network Convergence, has caused the advent of the era of big data. Big data contains huge values, however, mining knowledge from big data is a tremendously challenging task because of data uncertainty and inconsistency. Attribute reduction (also known as feature selection) can not only be used as an effective preprocessing step, but also exploits the data redundancy to reduce the uncertainty. However, existing solutions are designed 1) either for a single machine that means the entire data must fit in the main memory and the parallelism is limited; 2) or for the Hadoop platform which means that the data have to be loaded into the distributed memory frequently and therefore become inefficient. In this paper, we overcome these shortcomings for maximum efficiency possible, and propose a unified framework for Parallel Large-scale Attribute Reduction, termed PLAR, for big data analysis. PLAR consists of three components: 1) Granular Computing (GrC)-based initialization: it converts a decision table (i.e. original data representation) into a granularity representation which reduces the amount of space and hence can be easily cached in the distributed memory: 2) model-parallelism: it simultaneously evaluates all feature candidates and makes attribute reduction highly parallelizable; 3) dataparallelism: it computes the significance of an attribute in parallel using a MapReduce-style manner. We implement PLAR with four representative heuristic feature selection algorithms on SPARK, and evaluate them on various huge datasets, including UCI and astronomical datasets, finding our method’s advantages beyond existing solutions.",
    "creator" : "LaTeX with hyperref package"
  }
}