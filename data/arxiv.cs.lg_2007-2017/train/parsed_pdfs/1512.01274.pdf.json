{
  "name" : "1512.01274.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems",
    "authors" : [ "Tianqi Chen", "Mu Li", "Yutian Li", "Min Lin", "Naiyan Wang", "Minjie Wang", "Tianjun Xiao", "Bing Xu", "Chiyuan Zhang", "Zheng Zhang" ],
    "emails" : [ "(muli@cs.cmu.edu)" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The scale and complexity of machine learning (ML) algorithms are becoming increasingly large. Almost all recent ImageNet challenge [12] winners employ neural networks with very deep layers, requiring billions of floating-point operations to process one single sample. The rise of structural and computational complexity poses interesting challenges to ML system design and implementation.\nMost ML systems embed a domain-specific language (DSL) into a host language (e.g. Python, Lua, C++). Possible programming paradigms range from imperative, where the user specifies exactly “how” computation needs to be performed, and declarative, where the user specification focuses on “what” to be done. Examples of imperative programming include numpy and Matlab, whereas packages such as Caffe, CXXNet program over layer definition which abstracts away and hide the inner-working of actual implementation. The dividing line between the two can be muddy at times. Frameworks such as Theano and the more recent Tensorflow can also be viewed as a mixture of both, they declare a computational graph, yet the computation within the graph is imperatively specified.\nRelated to the issue of programming paradigms is how the computation is carried out. Execution can be concrete, where the result is returned right away on the same thread, or asynchronize or delayed, where the statements are gathered and transformed into a dataflow graph as an intermediate representation first, before released to available devices. These two execution models have different implications on how inherent parallelisms are discovered. Concrete execution is restrictive (e.g. parallelized matrix multiplication), whereas asynchronize/delayed execution additionally identified all parallelism within the scope of an instance of dataflow graph automatically.\nThe combination of the programming paradigm and execution model yields a large design space, some of which are more interesting (and valid) than others. In fact, our team has collectively explored a number of them, as does the rest of the community. For example, Minerva [14] combines imperative programming with asynchronize execution. While Theano takes an declarative approach,\n∗Corresponding author (muli@cs.cmu.edu)\nar X\niv :1\n51 2.\n01 27\n4v 1\n[ cs\n.D C\n] 3\nD ec\n2 01\nenabling more global graph-aware optimization. Similar discipline was adopted in Purine2 [10]. Instead, CXXNet adopts declarative programming (over tensor abstraction) and concrete execution, similar to Caffe [7]. Table 1 gives more examples.\nOur combined new effort resulted in MXNet (or “mix-net”), intending to blend advantages of different approaches. Declarative programming offers clear boundary on the global computation graph, discovering more optimization opportunity, whereas imperative programs offers more flexibility. In the context of deep learning, declarative programming is useful in specifying the computation structure in neural network configurations, while imperative programming are more natural for parameter updates and interactive debugging. We also took the effort to embed into multiple host languages, including C++, Python, R, Go and Julia.\nDespite the support of multiple languages and combination of different programming paradigm, we are able to fuse the execution to the same backend engine. The engine tracks data dependencies across computation graphs and imperative operations, and schedules them efficiently jointly. We aggressively reduce memory footprint, performing in-place update and memory space reuse whenever possible. Finally, we designed a compact communication API so that a MXNet program runs on multiple machines with little change.\nComparing to other open-source ML systems, MXNet provides a superset programming interface to Torch7 [3], Theano [1], Chainer [5] and Caffe [7], and supports more systems such as GPU clusters. Besides supporting the optimization for declarative programs as TensorFlow [11] do, MXNet additionally embed imperative tensor operations to provide more flexibility. MXNet is lightweight, e.g. the prediction codes fit into a single 50K lines C++ source file with no other dependency, and has more languages supports. More detailed comparisons are shown in Table 2."
    }, {
      "heading" : "2 Programming Interface",
      "text" : "2.1 Symbol: Declarative Symbolic Expressions KV Store Symbolic Expr\nBinder\nDep EngineBLAS\nCPU GPU Android\nND Array\nC/C++ Python R Julia...\n... iOS Comm\nFigure 1: MXNet Overview\nMXNet uses multi-output symbolic expressions, Symbol, declare the computation graph. Symbols are composited by operators, such as simple matrix operations (e.g. “+”), or a complex neural network layer (e.g. convolution layer). An operator can take several input variables, produce more than one output variables, and have internal\nstate variables. A variable can be either free, which we can bind with value later, or an output of another symbol. Figure 2 shows the construction of a multi-layer perception symbol by chaining a variable , which presents the input data, and several layer operators.\nusing MXNet mlp = @mx.chain mx.Variable(:data) =>\nmx.FullyConnected(num_hidden=64) => mx.Activation(act_type=:relu) => mx.FullyConnected(num_hidden=10) => mx.Softmax()\nFigure 2: Symbol expression construction in Julia.\n>>> import mxnet as mx >>> a = mx.nd.ones((2, 3), ... mx.gpu()) >>> print (a * 2).asnumpy() [[ 2. 2. 2.] [ 2. 2. 2.]]\nFigure 3: NDArray interface in Python\nTo evaluate a symbol we need to bind the free variables with data and declare the required outputs. Beside evaluation (“forward”), a symbol supports auto symbolic differentiation (“backward”). Other functions, such as load, save, memory estimation, and visualization, are also provided for symbols.\n2.2 NDArray: Imperative Tensor Computation\nMXNet offers NDArray with imperative tensor computation to fill the gap between the declarative symbolic expression and the host language. Figure 3 shows an example which does matrix-constant multiplication on GPU and then prints the results by numpy.ndarray.\nNDArray abstraction works seamlessly with the executions declared by Symbol, we can mix the imperative tensor computation of the former with the latter. For example, given a symbolic neural network and the weight updating function, e.g. w = w − ηg. Then we can implement the gradient descent by\nwhile(1) { net.foward_backward(); net.w -= eta * net.g };\nThe above is as efficient as the implementation using a single but often much more complex symbolic expression. The reason is that MXNet uses lazy evaluation of NDArray and the backend engine can correctly resolve the data dependency between the two.\n2.3 KVStore: Data Synchronization Over Devices\nThe KVStore is a distributed key-value store for data synchronization over multiple devices. It supports two primitives: push a key-value pair from a device to the store, and pull the value on a key from the store. In addition, a user-defined updater can specify how to merge the pushed value. Finally, model divergence is controlled via consistency model [8]. Currently, we support the sequential and eventual consistency.\nThe following example implements the distributed gradient descent by data parallelization.\nwhile(1){ kv.pull(net.w); net.foward_backward(); kv.push(net.g); }\nwhere the weight updating function is registered to the KVStore, and each worker repeatedly pulls the newest weight from the store and then pushes out the locally computed gradient.\nThe above mixed implementation has the same performance comparing to a single declarative program, because the actual data push and pull are executed by lazy evaluation, which are scheduled by the backend engine just like others."
    }, {
      "heading" : "2.4 Other Modules",
      "text" : "MXNet ships with tools to pack arbitrary sized examples into a single compact file to facilitate both sequential and random seek. Data iterators are also provided. Data pre-fetching and pre-processing are multi-threaded, reducing overheads due to possible remote file store reads and/or image decoding and transformation.\nThe training module implements the commonly used optimization algorithms, such as stochastic gradient descent. It trains a model on a given symbolic module and data iterators, optionally distributedly if an additional KVStore is provided."
    }, {
      "heading" : "3 Implementation",
      "text" : "3.1 Computation Graph fullc\nrelu\n∂W W\nX ∂X\nb\n∂ fullc\n∂ relu\n∂b\nFigure 4: Computation graph for both forward and backward.\nA binded symbolic expression is presented as a computation graph for evaluation. Figure 4 shows a part of the graph of both forward and backward of the MLP symbol in Figure 2. Before evaluation, MXNet transforms the graph to optimize the efficiency and allocate memory to internal variables.\nGraph Optimization. We explore the following straightforward optimizations. We note first that only the subgraph required to obtain the outputs specified during binding is needed. For example, in prediction only the forward graph is needed, while for extracting features from internal layers, the last layers can be skipped. Secondly, operators can be grouped into a single one. For example, a × b + 1 is replaced by a single BLAS or GPU call. Finally, we manually implemented welloptimized “big” operations, such as a layer in neural network.\nMemory Allocation. Note that each variable’s life time, namely the period between the creation and the last time will be used, is known for a computation graph. So we can reuse memory for nonintersected variables. However, an ideal allocation strategy requires O(n2) time complexity, where n is the number of variables.\nWe proposed two heuristics strategies with linear time complexity. The first, called inplace, simulates the procedure of traversing the graph, and keeps a reference counter of depended nodes that are not used so far. If the counter reaches zero, the memory is recycled. The second, named co-share, allows two nodes to share a piece of memory if only if they cannot be run in parallel. Exploring co-share imposes one additional dependency constraint. In particular, each time upon scheduling, among the pending paths in the graph, we find the longest path and perform needed memory allocations."
    }, {
      "heading" : "3.2 Dependency Engine",
      "text" : "In MXNet, each source units, including NDArray, random number generator and temporal space, is registered to the engine with a unique tag. Any operations, such as a matrix operation or data communication, is then pushed into the engine with specifying the required resource tags. The engine continuously schedules the pushed operations for execution if dependencies are resolved. Since there usually exists multiple computation resources such as CPUs, GPUs, and the memory/PCIe buses, the engine uses multiple threads to scheduling the operations for better resource utilization and parallelization.\nDifferent to most dataflow engines [14], our engine tracks mutation operations as an existing resource unit. That is, ours supports the specification of the tags that a operation will write in addition to read. This enables scheduling of array mutations as in numpy and other tensor libraries. It also enables easier memory reuse of parameters, by representing parameter updates as mutating the parameter arrays. It also makes scheduling of some special operations easier. For example, when generating two random numbers with the same random seed, we can inform the engine they will write the seed so that they should not be executed in parallel. This helps reproducibility.\n3.3 Data Communication\nWe implemented KVStore based on the parameter server [8, 9, 4](Figure 5). It differs to previous works in two aspects: First, we use the engine to schedule the KVStore operations and manage the data consistency. The strategy not only makes the data synchronization works seamless with computation, and also greatly simplifies the implementation. Second, we adopt an two-level structure. A level-1 server manages the data synchronization between the devices within a single machine, while a level-2 server manages intermachine synchronization. Outbound data from a level-1 server can\nbe aggregated, reducing bandwidth requirement; intra- and inter-machine synchronization can use different consistency model (e.g. intra- is sequential and inter- is eventual)."
    }, {
      "heading" : "4 Evaluation",
      "text" : "Raw performance We fist compare MXNet with Torch7, Caffe, and TensorFlow on the popular “convnet-benchmarks” [2]. All these systems are compiled with CUDA 7.5 and CUDNN 3 except for TensorFlow, which only supports CUDA 7.0 and CUDNN 2. We use batch size 32 for all networks and run the experiments on a single Nvidia GTX 980 card. Results are shown in Figure 6. As expected that MXNet has similar performance comparing to Torch7 and Caffe, because most computations are spent on the CUDA/CUDNN kernels. TensorFlow is always 2x slower, which might be due its use of a lower CUDNN version.\nMemory usage Figure 7 shows the memory usages of the internal variables excepts for the outputs. As can be seen, both “inplace” and “co-share” can effective reduce the memory footprint. Combing them leads to a 2x reduction for all networks during model training, and further improves to 4x for model prediction. For instance, even for the most expensive VGG net, training needs less than 16MB extra.\nScalability We run the experiment on Amazon EC2 g2.8x instances, each of which is shipped with four Nvidia GK104 GPUs and 10G Ethernet. We train googlenet with batch normalization [6] on the ILSVRC12 dataset [13] which consists of 1.3 million images and 1,000 classes. We fix the learning rate to .05, momentum to .9, weight decay to 10−4, and feed each GPU with 36 images in one batch.\nThe convergence results are shown in Figure 8. As can be seen, comparing to single machine, the distributed training converges slower at the beginning, but outperforms after 10 data passes. The average cost of a data pass is 14K and 1.4K sec on a single machine and 10 machines, respectively. Consequently, this experiment reveals a super-linear speedup."
    }, {
      "heading" : "5 Conclusion",
      "text" : "MXNet is a machine learning library combining symbolic expression with tensor computation to maximize efficiency and flexibility. It is lightweight and embeds in multiple host languages, and can be run in a distributed setting. Experimental results are encouraging. While we continue to explore new design choices, we believe it can already benefit the relevant research community. The codes are available at http://dmlc.io.\nAcknowledgment. We sincerely thanks Dave Andersen, Carlos Guestrin, Tong He, Chuntao Hong, Qiang Kou, Hu Shiwen, Alex Smola, Junyuan Xie, Dale Schuurmans and all other contributors."
    } ],
    "references" : [ {
      "title" : "Theano: new features and speed improvements",
      "author" : [ "Frédéric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "David Warde-Farley", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1211.5590,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2012
    }, {
      "title" : "Easy benchmarking of all public open-source implementations of convnets",
      "author" : [ "Soumith Chintala" ],
      "venue" : "https://github.com/soumith/convnet-benchmarks",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2015
    }, {
      "title" : "Torch7: A matlab-like environment for machine learning",
      "author" : [ "Ronan Collobert", "Koray Kavukcuoglu", "Clément Farabet" ],
      "venue" : "In BigLearn, NIPS Workshop, number EPFL-CONF-192376,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2011
    }, {
      "title" : "Large scale distributed deep networks",
      "author" : [ "J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "Q. Le", "M. Mao", "M. Ranzato", "A. Senior", "P. Tucker", "K. Yang", "A. Ng" ],
      "venue" : "In Neural Information Processing Systems,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2012
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "Sergey Ioffe", "Christian Szegedy" ],
      "venue" : "arXiv preprint arXiv:1502.03167,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2015
    }, {
      "title" : "Caffe: Convolutional architecture for fast feature embedding",
      "author" : [ "Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell" ],
      "venue" : "In Proceedings of the ACM International Conference on Multimedia,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2014
    }, {
      "title" : "Scaling distributed machine learning with the parameter server",
      "author" : [ "M. Li", "D.G. Andersen", "J. Park", "A.J. Smola", "A. Amhed", "V. Josifovski", "J. Long", "E. Shekita", "B.Y. Su" ],
      "venue" : "In OSDI,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2014
    }, {
      "title" : "Communication efficient distributed machine learning with the parameter server",
      "author" : [ "M. Li", "D.G. Andersen", "A.J. Smola", "K. Yu" ],
      "venue" : "In Neural Information Processing Systems,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    }, {
      "title" : "Purine: A bi-graph based deep learning framework",
      "author" : [ "Min Lin", "Shuo Li", "Xuan Luo", "Shuicheng Yan" ],
      "venue" : "arXiv preprint arXiv:1412.6249,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2014
    }, {
      "title" : "Tensorflow: Large-scale machine learning on heterogeneous systems",
      "author" : [ "Abadi Martn", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin", "Sanjay Ghemawat", "Ian Goodfellow", "Andrew Harp", "Geoffrey Irving", "Michael Isard", "Yangqing Jia", "Rafal Jozefowicz", "Lukasz Kaiser", "Manjunath Kudlur", "Josh Levenberg", "Dan Mane", "Rajat Monga", "Sherry Moore", "Derek Murray", "Chris Olah", "Mike Schuster", "Jonathon Shlens", "Benoit Steiner", "Ilya Sutskever", "Kunal Talwar", "Paul Tucker", "Vincent Vanhoucke", "Vijay Vasudevan", "Fernanda Viegas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2015
    }, {
      "title" : "ImageNet Large Scale Visual Recognition Challenge",
      "author" : [ "Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein", "Alexander C. Berg", "Li Fei- Fei" ],
      "venue" : "International Journal of Computer Vision (IJCV),",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2015
    }, {
      "title" : "Imagenet large scale visual recognition challenge",
      "author" : [ "Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein" ],
      "venue" : "International Journal of Computer Vision,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Minerva: A scalable and highly efficient training platform",
      "author" : [ "Minjie Wang", "Tianjun Xiao", "Jianpeng Li", "Jiaxing Zhang", "Chuntao Hong", "Zheng Zhang" ],
      "venue" : "for deep learning,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "Almost all recent ImageNet challenge [12] winners employ neural networks with very deep layers, requiring billions of floating-point operations to process one single sample.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 12,
      "context" : "For example, Minerva [14] combines imperative programming with asynchronize execution.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 5,
      "context" : "System Core Binding Devices Distri- Imperative Declarative Lang Langs (beyond CPU) buted Program Program Caffe [7] C++ Python/Matlab GPU × × √",
      "startOffset" : 111,
      "endOffset" : 114
    }, {
      "referenceID" : 2,
      "context" : "Torch7 [3] Lua GPU/FPGA × √ × Theano [1] Python GPU × × √",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 0,
      "context" : "Torch7 [3] Lua GPU/FPGA × √ × Theano [1] Python GPU × × √",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 9,
      "context" : "TensorFlow [11] C++ Python GPU/Mobile √ × √",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 8,
      "context" : "Similar discipline was adopted in Purine2 [10].",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 5,
      "context" : "Instead, CXXNet adopts declarative programming (over tensor abstraction) and concrete execution, similar to Caffe [7].",
      "startOffset" : 114,
      "endOffset" : 117
    }, {
      "referenceID" : 2,
      "context" : "Comparing to other open-source ML systems, MXNet provides a superset programming interface to Torch7 [3], Theano [1], Chainer [5] and Caffe [7], and supports more systems such as GPU clusters.",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 0,
      "context" : "Comparing to other open-source ML systems, MXNet provides a superset programming interface to Torch7 [3], Theano [1], Chainer [5] and Caffe [7], and supports more systems such as GPU clusters.",
      "startOffset" : 113,
      "endOffset" : 116
    }, {
      "referenceID" : 5,
      "context" : "Comparing to other open-source ML systems, MXNet provides a superset programming interface to Torch7 [3], Theano [1], Chainer [5] and Caffe [7], and supports more systems such as GPU clusters.",
      "startOffset" : 140,
      "endOffset" : 143
    }, {
      "referenceID" : 9,
      "context" : "Besides supporting the optimization for declarative programs as TensorFlow [11] do, MXNet additionally embed imperative tensor operations to provide more flexibility.",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 6,
      "context" : "Finally, model divergence is controlled via consistency model [8].",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 12,
      "context" : "Different to most dataflow engines [14], our engine tracks mutation operations as an existing resource unit.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 6,
      "context" : "We implemented KVStore based on the parameter server [8, 9, 4](Figure 5).",
      "startOffset" : 53,
      "endOffset" : 62
    }, {
      "referenceID" : 7,
      "context" : "We implemented KVStore based on the parameter server [8, 9, 4](Figure 5).",
      "startOffset" : 53,
      "endOffset" : 62
    }, {
      "referenceID" : 3,
      "context" : "We implemented KVStore based on the parameter server [8, 9, 4](Figure 5).",
      "startOffset" : 53,
      "endOffset" : 62
    }, {
      "referenceID" : 1,
      "context" : "Raw performance We fist compare MXNet with Torch7, Caffe, and TensorFlow on the popular “convnet-benchmarks” [2].",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 4,
      "context" : "We train googlenet with batch normalization [6] on the ILSVRC12 dataset [13] which consists of 1.",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 11,
      "context" : "We train googlenet with batch normalization [6] on the ILSVRC12 dataset [13] which consists of 1.",
      "startOffset" : 72,
      "endOffset" : 76
    } ],
    "year" : 2015,
    "abstractText" : "MXNet is a multi-language machine learning (ML) library to ease the development of ML algorithms, especially for deep neural networks. Embedded in the host language, it blends declarative symbolic expression with imperative tensor computation. It offers auto differentiation to derive gradients. MXNet is computation and memory efficient and runs on various heterogeneous systems, ranging from mobile devices to distributed GPU clusters. This paper describes both the API design and the system implementation of MXNet, and explains how embedding of both symbolic expression and tensor operation is handled in a unified fashion. Our preliminary experiments reveal promising results on large scale deep neural network applications using multiple GPU machines.",
    "creator" : "LaTeX with hyperref package"
  }
}