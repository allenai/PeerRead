{
  "name" : "1705.01206.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Local Shrunk Discriminant Analysis (LSDA)",
    "authors" : [ "Zan Gao", "Guotai Zhang", "Feiping Nie", "Hua Zhang" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Local Shrunk Discriminant Analysis (LSDA)\nZan Gao1,2, Guotai Zhang1,2, Feiping Nie3* and Hua Zhang1,2, Member, IEEE 1 Key Laboratory of Computer Vision and System,Ministry of Education,Tianjin University of Technology,Tianjin,300384,China\n2Tianjin Key Laboratory of Intelligence Computing and Novel Software Technology,Tianjin University of Technology,300384, China 3Center for OPTical IMagery Analysis and Learning(OPTIMAL), Northwestern Polytechnical University, Xian, Shanxi, 710072, China\nAbstract:Dimensionality reduction is a crucial step for pattern recognition and data mining tasks to overcome the curse of dimensionality. Principal component analysis (PCA) is a traditional technique for unsupervised dimensionality reduction, which is often employed to seek a projection to best represent the data in a least-squares sense, but if the original data is nonlinear structure, the performance of PCA will quickly drop. An supervised dimensionality reduction algorithm called Linear discriminant analysis (LDA) seeks for an embedding transformation, which can work well with Gaussian distribution data or single-modal data, but for non-Gaussian distribution data or multimodal data, it gives undesired results. What is worse, the dimension of LDA cannot be more than the number of classes. In order to solve these issues, Local shrunk discriminant analysis (LSDA) is proposed in this work to process the nonGaussian distribution data or multimodal data, which not only incorporate both the linear and nonlinear structures of original data, but also learn the pattern shrinking to make the data more flexible to fit the manifold structure. Further, LSDA has more strong generalization performance, whose objective function will become local LDA and traditional LDA when different extreme parameters are utilized respectively. What is more, a new efficient optimization algorithm is introduced to solve the non-convex objective function with low computational cost. Compared with other related approaches, such as PCA, LDA and local LDA, the proposed method can derive a subspace which is more suitable for non-Gaussian distribution and real data. Promising experimental results on different kinds of data sets demonstrate the effectiveness of the proposed approach1.\nIndex Terms—Dimensionality Reduction, Shrunk Pattern, Discriminant Analysis, Transformation matrix\nI. INTRODUCTION\nIN pattern recognition and data mining tasks, we are oftenconfront with the curse of dimensionality, which may make it hard for us to train a stable classifier and it will take a long time to train the classifier. Thus, dimensionality reduction is a hot and classical topic [1], which attempts to overcome the curse of the dimensionality and to extract relevant features [2], [3]. For example, although the dimension of original feature of all images of the same subject is very high, its intrinsic\n1This work was supported in part by the National Natural Science Foundation of China (No.61572357, No.61202168). Zan Gao, Guotai Zhang and Hua Zhang is with Key Laboratory of Computer Vision and System,Tianjin University of Technology, Ministry of Education,Tianjin Key Laboratory of Intelligence Computing and Novel Software Technology, Tianjin University of Technology, Tianjin, 300384, China. Feiping Nie is with the Center for OPTical IMagery Analysis and Learning (OPTIMAL), Northwestern Polytechnical University, Xian, Shanxi, 710072, China. (E-mail: feipingnie@gmail.com)\ndimensionality is usually very low [4]. In dimensionality reduction [5], [6], feature selection [7], [8], [9], [10], where a subset of features of the original set are selected, and feature transformation [11], [12], [13], [14], where the original features are transformed to a new feature subspace, are the two main ways. In contrast to feature selection, feature transformation will obtain much more compact representation of the variables. So far, many dimensionality reduction approaches have been proposed which can be categorized into supervised learning (e.g LDA [11]; Kernel LDA [12]; DiscLDA [13]; LPP [14]; SDRHF [15], Local fisher discriminant analysis (LFDA) [31]) and unsupervised learning (PCA [16], [17]; Kernal PCA [18]; LLE [19]; LPP [14]; NPE [20]; Isomap [22]; Laplacian eigenmaps, [21], [22]) dimensionality reduction methods. The difference between supervised and unsupervised dimensionality reduction algorithms lies in whether the ground truth is utilized or not in learning the transformation matrix [23]. If the ground truth is employed in the subspace learning, the method belongs to the supervised learning method, otherwise, it will be unsupervised learning method. The related references (PCA, [16], [17]; LDA [11]; DiscLDA [12], LFDA [31] show that the supervised learning methods can obtain much better performance than the unsupervised learning methods, and has been applied into different research domains. Among the supervised learning approaches, the most popular and successful one is LDA, which is very suitable for the Gaussian distribution data, and LDA performs well in many applications, but LDA also has some drawbacks: 1) it is only suitable for Gaussian distribution data, since the objective function of LDA is to make the distance between different categories as far as possible, and the same categories as close as possible, thus, when the non-Gaussian distribution data is utilized, the projection direction will be wrong; 2) the dimension of LDA is limited, which must be smaller than the number of classes. If there are only two classes in our task, thus, the dimensionality reduction must be one dimension, which may not represent the original data distribution.\nIn addition, in our daily life, there are so many unlabeled data on internet, which is very helpful for our future life, thus, unsupervised learning dimensionality reduction algorithms also play an important role. Among these methods, different motivations and objective functions are designed. For example, Turk et al. [16], [17] proposed Principal Component Analysis which is the most frequently used dimensionality reduction method. The motivation of PCA seeks a projection which can\nar X\niv :1\n70 5.\n01 20\n6v 1\n[ cs\n.L G\n] 3\nM ay\n2 01\n7\nbest represent the data in a least-squares sense. He et al. [14], [20] proposed Locality Preserving Projections (LPP) in 2003 and Neighborhood Preserving Embedding (NPE) in 2005. Both of them are different from PCA which aims at preserving the global Euclidean structure, but they are linear projective maps whose motivations optimally preserve the neighborhood local structure of the data set. In addition, several nonlinear dimensionality reduction approaches are proposed, such as, locally linear embedding (LLE), isometric feature mapping (Isomap) and Laplacian Eigenmaps, which also preservers the neighborhood relation of data points. However, the motivations between PCA, LPP, NPE and LLE, Isomap and Laplacian Eigenmaps, are very different, and the original LLE, Isomap and Laplcacian Eigenmaps cannot deal with the out-of-sample problem directly [24], that is to say, they only can deal with the training samples, and obtain the low dimension embedding, but for test samples, they cannot directly calculated, analytically or cannot calculated at all. As for PCA, LPP and NPE, they can easily and directly calculate the low dimensional embedding for both training samples and testing samples.\nRecently, pattern shrinking [25], [26] is often utilized in clustering algorithms, which not only characterizes the linear and nonlinear structures of data, but also reflects the requirements of clustering, what is more, it can obtain satisfying performance on high dimensional data and non-Gaussian distribution data [30], [32], [33], [34]. Inspired by them, in this paper, we propose a new general unsupervised and supervised learning dimensionality reduction algorithm, called Local shrunk discriminant analysis, where the shrunk pattern and the projection matrix are simultaneously optimized in our objective function, and whose neighborhood structure can be preserved in the dimensionality reduced space. Since the shrunk pattern is utilized in our model, which makes the data more flexible to fit the manifold structure, thus, our proposed model is suitable for non-Gaussian distribution data and the dimensionality reduction is irrelative to the number of classes. What is more, a new efficient optimization algorithm is introduced to solve the non-convex objective function with low computational cost. Promising experimental results on different kinds of datasets demonstrate the effectiveness of the proposed approach. It is worthwhile to highlight the following merits of our work:\n• The proposed algorithm is more capable of uncovering the manifold structure. Particularly, the shrunk pattern does not have the orthogonal constraint, making it more flexible to fit the manifold structure. • The pattern shrinking and transformation matrix are simultaneously learnt, which will make the pattern shrinking more suitable for the transformation matrix. • The transformation matrix not only learns from the original data points, but also learns from their pattern shrinking, which will make the pattern shrinking more convenient to find a suitable subspace for dimensionality reduction.\nThe rest of this paper is organized as follows. The related work will be given in Section II, and then we detail our proposed algorithm. After that, extensive experimental results\nare introduced and Section V concludes this paper."
    }, {
      "heading" : "II. RELATED WORK",
      "text" : "A. Linear Discriminant Analysis\nSince the objective function of LDA is to make the distance between different categories as far as possible, and the same categories as close as possible. Thus, assume there are n samples for dimensionality reduction, and an r−dimensional feature vector is utilized to represent each sample, i.e, {x1, x2, ..., xn} where xi ∈ Rr for i = 1, 2, ...n . The learning subsapce of {x1, x2, ..., xn} is represented by {z1, z2, ..., zn}, where zi ∈ Rd for i = 1, 2, ...n , and d is the dimension of learning subspace. The goal of subspace learning is to find a optimized transformation matrix W ∈ Rr×d, and each sample is then projected into a low-dimensional subspace by zi = WTxi. Denote X = {x1, x2, ..., xn} and Z = {z1, z2, ..., zn} , therefore, Z = WTX . As for n sample, we assume that each image belongs to one of C classes, thus, these original data can also be represented by {X1, X2, ..., Xc}, where the number of each class Xi is ni . And then, the mean value µ of all samples are computed, after that, the mean value µi of each class Xi be also calculated, thus, let the between-class scatter matrix be defined as\nSB = C∑ i=1 ni(µi − µ)(µi − µ)T (1)\nAnd the within-class scatter matrix be defined as\nSW = C∑ i=1 ∑ xk∈Xi (xk − µi)(xk − µi)T (2)\nWhere Xi denotes the set of samples in class i. The goal of LDA is also to obtain a transformation matrix W ∈ Rr×d, and the low-dimensional feature vector can be calculated by zi = W\nTxi. If SW is nonsingular, the optimal projection Wopt is chosen as the matrix with orthonormal columns which maximizes the ratio of the determinant of the between-class scatter matrix of the projected samples to the determinant of the within-class scatter matrix of the projected samples, i.e.,\nWopt = arg max W |WTSBW | |WTSWW | = [w1, w2, ...wm] (3)\nWhere {wi|i = 1, 2, ...,m} is the set of the generalized eigen-vectors of SB and SW corresponding to the m largest generalized eigen-values {λi|i = 1, 2, ...,m} , i.e.,\nSBwi = λiSWwi, i = 1, 2, ...,m (4)\nNote that there are at most C − 1 nonzero generalized eigenvalues, and so an upper bound on m is C− 1, where C is the number of classes.\nB. Locality Preserving Projections\nThe linear projective maps (LPP), which is a variational problem, optimally preserves the neighborhood structure of the data set. In fact, LPP also can be seen as an alternative to PCA, which is a classical linear technique that projects the data\nalong the directions of maximal variance. Thus, the objective function of LPP is defined as follows:\nWopt = arg min WTXDXTW=I\ntr(WTXLXTW ) (5)\nWhere aij is utilized to measure the similarity of xi and xj . The similarity matrix [28], [29] A ∈ Rn×n is composed of all the aij , which is utilized to characterize the manifold structure of original data points, i.e. x1, x2, ..., xn . In detail, a k-nearest neighborhood graph is firstly constructed, and the points are considered as nodes, then connecting every point to its k nearest neighbors is utilized as theirs edges.\nAccording to whether the label information is utilized or not, the construction of aij is divided into supervised learning and unsupervised learning. In supervised learning, denote that Ns(xi) is the index set of the points, which are k nearest neighbors of xi, and the class of these points are same to xi. Thus, the weight matrix A in supervised learning, associated with this k -nearest neighborhood graph, is computed by the following equation:\naij=aji=\n{ e− ‖xi−xj‖ 2 2\n2σ2 xi ∈ Ns(xj) or xj ∈ Ns(xi) 0 otherwise\n(6) Where σ is the width parameter to control the Gaussian distribution.\nAs for unsupervised learning, denote that Nu(xi) is the index set of the points, which are k nearest neighbors of xi and the classes of these points can be from different classes. Thus, the weight matrix A in unsupervised learning can be defined as follows:\naij=aji=\n{ e− ‖xi−xj‖ 2 2\n2σ2 xi ∈ Nu(xj) or xj ∈ Nu(xi) 0 otherwise\n(7) In addition,L = D − A , where D is a diagonal matrix\nwith entries Dij = ∑ j Aij . In fact, it is the Laplacian matrix of the above defined k-nearest graph with weight matrix S. Specially, in the construction of similarity matrix, if knearest neighborhood does not contain the label information of samples, thus, LPP is considered as unsupervised learning dimensionality reduction, or it will be known as supervised learning dimensionality reduction.\nFor the optimization of Eq. (5), it can be obtained by computing the eigenvectors and eigenvalues for the generalized eigenvector problem:\nXLXTW = λXDXTW (8)\nLet the columns vectors w0, w1, ..., wd−1 be the solutions of Eq. (5), which are ordered by their eigenvalues, λ0 < λ1 < ... < λd−1. Thus, the dimensionality reduction can be obtained by\nxi → yi = WTxi,W = (w0, w1, ..., wd−1) (9)\nC. Subspace Learning via Pattern Shrinking\nAssume there are n samples for clustering, and an r−dimensional feature vector is utilized to represent each sample, i.e,{x1, x2, ..., xn} where xi ∈ Rr for i = 1, 2, ...n. The\nshrunk pattern of these samples are defined by {y1, y2, ..., yn} , where yi ∈ Rr for i = 1, 2, ...n. The learning subspace of {x1, x2, ..., xn} is represented by {z1, z2, ..., zn}, where zi ∈ Rd for i = 1, 2...n, and d is the dimension of learning subspace. The goal of subspace learning is to find a optimized transformation matrix W ∈ Rr×d, and each sample is then projected into a low-dimensional subspace by zi = WTxi. Denote X = {x1, x2, ..., xn} Y = {y1, y2, ..., xn} and Z = {z1, z2, ..., zn} , therefore, Z = WTX or Z = WTY . Since the manifold structures of the original data may be nonlinear, but the linear projection approaches cannot fully consider about them. Thus, the shrunk patterns are employed in the model, which can make the data more flexible to fit the manifold structure. The model is defined as follows:\narg min WTW=I,Z=WTY {(1− δ) n∑ i=1 n∑ j=1 aij ‖yi − yj‖22\n+ δ n∑ i=1 ‖ xi − yi‖22 −β n∑ i=1 ‖ zi − 1 n n∑ j=1 zj‖22} (10)\nWhere δ and β are used to control the balance of each regular term. The meaning of other parameters in Eq.(10) is same to Eq.(5) and Eq.(6).\nIn this objective function, there are three kinds of regularization terms, and the reason why the first term is utilized, is that nearby points are more likely to belong to the same cluster, and the shrunk pattern should maintain the similarities of the original data, which is measured by a weight matrix; As for the second regular term, the reason is that the consistency between the original data and their shrunk pattern should be kept; Finally, after learning the shrunk pattern, a subspace is expected to learn, where the projections of shrunk patterns should maintain their original separations. It guarantees that the information loss in deriving shrunk data is as little as possible."
    }, {
      "heading" : "III. LOCAL SHRUNK DISCRIMINANT ANALYSIS",
      "text" : "In this section, we will introduce some notations and formulate the dimensionality reduction approach by our pattern shrinking technique. After that, we show how to derive the approximated solution in a quick way. Finally, some preliminary discussions are provided.\nA. Problem Formulation\nIn our real-life, there are often non-Gaussian distribution data, when LDA is employed to reduce the dimension, the projection direction may be wrong. Fortunately the pattern shrinking [25], [26] may be helpful for us to find a suitable transformation matrix, thus, the pattern shrinking is employed in our model. In addition, in subspace learning via pattern shrinking, it cannot directly deal with the out-of-the-sample problem [24], where only the low dimensional embedding map of training samples can be calculated but the samples out of the training set (i.e. testing samples) cannot be computed directly, thus, the transformation matrix is simultaneously optimized from the original features and shrunk pattern. For\nthe construction of this objective function, our motivations are as follows:\n1) Basic assumption of clustering or subspace learning. Intuitively, if two points are nearby, they should belong to the same cluster and the similarity weight should be large. On the contrary, if two points are far away, the corresponding similarity weight should be small. This objective can be implemented by employing the similarity matrix S. After obtaining the pattern shrinking, the similarity between the shrunk patterns yi and yj should keep the consistency with the similarity between the original data xi and xj . 2) The shrunk pattern should keep the consistency with the original data. More concretely, the shrunk pattern and the original data should not be far away. Compared with the first motivation, which uses the local similarity for shrinking data, this objective function can be regarded as keeping dissimilarity in firtst motivation, and we also require that the pattern shrinking data should be close to the original data. 3) After learning the shrunk pattern, we expect to learn a projection matrix in which the similarity of the projections between different shrunk patterns, and the difference of the projections between the original data points and shrunk pattern, should maintain their original separations. By this way, it guarantees that the loss of information in deriving shrunk data and the embedding is as little as possible. In addition, the projection matrix and the pattern shrinking are simultaneously optimized, which makes the project matrix more suitable for dimensionality reduction.\nFor the simplification of writing, the notation in Section II is utilized. In order to keep the local similarity, the shrunk pattern should inherit the local similarity of the original data. Moreover, the shrunk pattern should not be far way from the original data. Thus, it also requires that the shrunk pattern consists with the original data. Thus, the following loss function can be directly minimized:\narg min Z n∑ i=1 n∑ j=1 aij‖zi − zj‖22 +γ n∑ i=1 ‖ xi − zi‖22 (11)\nAlthough the shrunk pattern can well represent the original data, the dimension may be very high, and it will be difficlut to train classifiers or other computations. Thus,we hope the learnt feature representation not only can has robust feature representation, but also it can has low dimension. Thus, the loss function can be defined as follows:\narg min WTStW=I,Z n∑ i=1 n∑ j=1 aij‖WT zi −WT zj‖22\n+ γ n∑ i=1 ‖WTxi −WT zi‖22\n(12)\nWhere St ∈ Rr×r is the covariance matrix of the original data points. I ∈ Rd×d is the unit matrix, and γ is used to control the balance between the original data and shrunk patterns. As for other parameters, their means are same to Eq. (10). In this objective function, the first term is employed\nto meet the first motivation, and then the second motivation is satisfied by the second term. As for last motivation, it is achieved by W in each regularization term.\nB. Optimization\nSince the Eq.(12) is non-convex, thus, it is difficult for us to directly optimize it. In order to simply write, we denote that fi = W\nT zi ∈ Rd and fi = WT zi ∈ Rd are the embedding of corresponding zi and zj , thus, the problem of Eq.(12) is equivalent to\narg min WTStW=I,F n∑ i=1 n∑ j=1 aij‖fi − fj‖22 +γ n∑ i=1 ‖WTxi − fi‖22\n(13)\nWhere F = {f1, f2...fi...fn} ∈ Rd×n is the embedding matrix of corresponding {z1, z2...zn}. However, the problem is still non-convex, it seems difficult to find the optimal solution, even to find a solution, since:\nWhen fix W , the original problem equivalent to :\narg min F n∑ i=1 n∑ j=1 aij‖fi − fj‖22 +γ n∑ i=1 ‖WTxi − fi‖22 (14)\nWhen fix F , the new objective function will be:\narg min WTStW=I n∑ i=1 ‖WTxi − fi‖22 (15)\nHowever, for the optimization of Eq.(15), it is still difficult for us to obtain the optimization solution. Interestingly, we can find the closed form (and thus optimal) solution to the problem of Eq.(13). First, the objective function in Eq.(13), becomes\narg min WTStW=I,F\nTr(FTLF ) + γ ‖ XTW − F ‖2F (16)\nWhere L is the so called graph Laplacian induced from the graph structure. Specifically, L = D − A , where A is the pre-computed similarity matrix among the original data, and D is a diagonal matrix, whose element can be obtained by Di,i = ∑ iAi,j .As for ‖ • ‖F , it denotes the F − norm. Denote Γ(W,F ) = Tr(FTLF ) + γ ‖ XTW − F ‖2F , and setting the gradients with respect to F to zero, and we can calculate\n∂Γ(W,F )\n∂F = 0⇒ F = γ(L+ γI)−1XTW (17)\nSubstituting F = γ(L+γI)−1XTW into the Eq.(16), we can obtain\narg min WTStW=I\nTr(WTX(γI − γ2(L+ γI)−1)XTW ) (18)\nFor the optimization of Eq.(18), the constrained minimization can then be done using the method of Lagrange multipliers:\nΓ(W ) =Tr(WTX(γI − γ2(L+ γI)−1)XTW ) + λ(I −WTStW )\n(19)\nSetting the gradients with respect to W to zero, we have\n∂Γ(W )\n∂W =2X(γI − γ2(L+ γI)−1)XTW )\n− 2λStW ) = 0 (20)\nBy defining\nH = X(γI − γ2(L+ γI)−1)XT (21)\nThe transformation vector in Eq.(18) that minimizes the objective function is given by the minimum eigenvalue solution to the following generalized eigenvector problem\nHW = λStW (22)\nSince St is the covariance matrix of the original data points, thus, it may be nonsingular, thus, the Generalized Singular Value Decomposition (GSVD) is employed. Note that at most C − 1 nonzero generalized eigen-valuse in LDA is not requested in our model. After obtaining the projection vector,Xi can be mapped to a low dimensional space zi by zi = WTxi .\nC. Algorithm Analysis\nIn LSDA model, it not only can make the data more flexible to fit the manifold structure, which will be useful for nonGaussian distribution data, but also it is the generalized form of LDA. In what follows, we will introduce the form of objetive function when different parameters are utilized, such as, γ → ∞, γ → 0 and γ ∈ R, and we will carefully explain it in the following section.\nWhen γ → ∞ , thus, the second regular term in Eq.(16) will be zero, and the optimization of Eq.(16) is equal to\narg min WTStW=I,F\nTr(FTLF )\ns.t.F = XTW (23)\nSubstituting F = XTW into the Eq.(23), we can obtain\narg min WTStW=I\nTr(WTXLXTW ) (24)\nIn fact, this objective function is one kind of local LDA. When γ → 0, it is clear that the optimization solution is Tr(FTLF ) = 0, and then we just optimize\narg min WTStW=I,F n∑ i=1 ‖WTxi − fi ‖22 (25)\nAssume there are P connected component in matrix L, in which it is obvious that P is much bigger than the number of classes C, and for class k, the number of its connected component is vk, thus, the connected components of class k can be denoted by {C1k , C2k , ..., C vk k }. Among all connected component, fi = fj . If there is xi ∈ Clk , thus fi = f lk. Therefore, the optimization of Eq.(23) becomes\narg min WTStW=I,F c∑ k=1 vk∑ l=1 ∑ xi∈Clk ‖WTxi − f lk ‖22 (26)\nWe can obviously know that the optimization solution is f lk = 1 |Clk| ∑ xi∈Clk WTxi . If we assume mlk = 1 |Clk| ∑ xi∈Clk\nxi, thus, the objective funcition Eq.(26) can be represented by\narg min WTStW=I,F c∑ k=1 vk∑ l=1 ∑ xi∈Clk ‖WTxi −WTmlk ‖22 (27)\nIn fact, from the objective function, we can know that it is another local LDA [31]. Further, when vk = 1 , the objective function will become\narg min WTStW=I,F\nTr(WTSwW ) (28)\nIt can be observed that it is the traditional LDA. Furthermore, for any γ, when vk = 1 and aij is required to\naij= { 1 nk\nxi, xj ∈ Ck\n0 otherwise (29)\nSince it is obvious that I − γ(L+ γI)−1 is block diagonal matrix, and for each diagonal block, we can know that\n(L+ γI)−1 = ((1 + γ)I − 1 ni 11T )−1 (30)\n= 1\n1 + γ I +\n1\nniγ(1 + γ) 11T\nI − γ(L+ γI)−1 = 1 1 + γ (I − 1 ni 11T ) (31)\nThus, Eq.(21) will become X(γI − (γ)2(L+ γI)−1XT = γ 1+γSw. Thus, as for any γ, the objective function of our proposed method is equal to the traditional LDA.\nThus, we can conclude that LSDA has more strong generalization performance, whose objective function with different extreme parameters will become local LDA and traditional LDA."
    }, {
      "heading" : "IV. EXPERIMENTS AND DISCUSSION",
      "text" : "In order to evaluate the performance of our proposed LSDA, we perform extensive experiments on three different kinds of tasks, such as handwritten digit recognition task, face recognition task and object recognition task. Specially, on face recognition, four face recognition databases are utilized. At the same time, in order to fair comparison, we divide the existing dimensionality reduction algorithms into unsupervised learning (including PCA, NPE, LPP and LSDA with unsupervised learning) and supervised learning algorithms (LDA, LPP,LFDA and LSDA with supervised learning). For the code of PCA, NPE, LDA and LPP, we download them from the internet2 , and also strictly keep the parameters same with them.\n2http://www.ews.uiuc.edu/ dengcai2/Data/data.html\nuse all the images under different illuminations, lighting and expressions which leaves us 170 near frontal face images for each individual. Fig.5 shows their samples.\n• Coil-1008\nIt contains 100 objects. The images of each object were taken 5 degrees apart as the object is rotated on a turntable and each object has 72 images.\nB. Pre-processing Step\nAll images are manually aligned and cropped, and expect for USPS dataset, the size of each cropped image is 32x32 pixels, with 256 gray levels per pixel. The pixel values are then scaled to [0,1] (divided by 256). For the vector-based approaches, the image is represented as a 1024-dimesional vector. For classification, the nearest-neighbor classifier for its simplicity is utilized in our experiments, and following most work on recognition task, we adopt recognition accuracy as our evaluation metrics in our experiments. In the computation of the similarity matrix, the Euclidean metric is employed as our distance measure.\nC. Parameter Setup\nFor unsupervised learning, the size of neighborhood k is set to 30 for all the dimensionality reduction algorithms, but for supervised learning, the size of neighborhood k is decided by the number of samples of each subject, and if the number of samples of one subject is small than 50, thus, k is set to its number of samples, otherwise, k is equal to 50.\nIn all experiments, we tune the sigma parameter in the range of [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], and gamma parameter in the range of [2−10, 2−9, ..., 29, 210]. Since the accuracies of PCA, NPE and LDA are only affected by the dimension without tuning other parameters, thus, their accuracies are stable for each dimension. However, for LPP, its accuracy will be different with the change of sigma parameter, thus, the sigma parameter will be traversed for each dimension, and then the best result is reported. As for LSDA, the change of sigma and gamma parameters will affect the performance of LSDA, thus, the alternating method is utilized [27], where at each iteration, we first fix the one variable, and then optimize\n8http://www.cs.columbia.edu/CAVE/software/softlib/coil-100.php\nthe other variables, at the same time, we update these variables by repeated iterative, and then keep the best accuracy.\nNote that the results of recognition algorithms vary on dataset split, in order to reduce the influence of statistical errors, different training and testing datasets are constructed, where different number of images are chosen for different datasets according by the number of samples. In our experiments, we randomly choose L (=2, 3, 4, 5, 6, 7, 8) images for Yale and ORL databases, L (=5, 10, 20, 30, 40, 50) images for YaleB and Coil-100 databases, and L (=5, 10, 20, 30) images for CMU-PIE database per individual to form the training set, and then the rest of them is utilized to form the testing dataset. What is more, for each given L, we will repeat 50 times, and then average the results over 50 random splits. As for USPS dataset, the fixed training and testing datasets are given, thus, we also strictly follow it.\nD. Toy Example\nTaking Fig. 7 as an intuitive example, and from it, we can know that the original data is non-Gaussian data, where there are two class man-made data points, and the number of each ellipse is 1000. Since the goal of LDA is to make the distance between different categories as far as possible, and the same categories as close as possible, thus, when LDA is utilized, the projector direction is wrong. However, in our LSDA model, the shrunk pattern is learnt, which will make the data more flexible to fit the manifold structure, and the project direct is correct.\nE. Experimental Results of Unsupervised Learning Methods\nIn the construction of similarity matrix A, aij is built by Eq.(7) where the k-nearest neighbor samples without their label information are employed, thus, LSDA is considered as unsupervised learning dimensionality reduction method. Therefore, we will firstly assess the performance of unsupervised learning dimensionality reduction method. In our experiments, for all algorithms, we will evaluate the performance when the dimensionality changes from 10 to 100 on top four databases, and from 10 to 200 on the last three databases. In addition, in order to fairly and conveniently comparison, if the dimension of these dimensionality reduction algorithms cannot reach 100 or 200, but we will still keep\nthe best performance for the later dimensions, for example, if the maximum dimension of PCA only can obtain 60 whose performance is 85%, thus, all the performances for 70, 80, 90 and 100 dimensions will be 85%. At the same time, for each dataset split, we will repeat 50 times, and then average recognition accuracy is utilized as the evaluation criterion. As for the baseline method in all datasets, the recognition is simply performed in the original 1024-dimensional image space without any dimensionality reduction. The experimental results of USPS, Yale, ORL, YaleB, CMU-PIE and Coil-100 are showed on Fig.8, Fig.9, Fig.10, Fig.11, Fig.12 and Fig.13 respectively, but since the limitation of the space, only the results of the first and last dataset splits are given for USPS, Yale, ORL, YALE, YaleB, CMU-PIE and Coil-100. In these figures, the horizontal axis is the dimension index, and the vertical coordinates means the average recognition accuracy. At the same time, we also choose the best results from them to compare to baseline method, whose results are given in Table I, Table II, Table III, Table IV, and Table V respectively. In these Tables, the first row denotes different dataset splits, and the most left column means different dimensionality reduction algorithms. As for the data in these tables, they are average recognition accuracy and the standard deviation of them. From these figures and tables, we can observe that:\n• For PCA methods, we can observe that although the performances of PCA on all datasets expect for COIL100 are lower than the baseline, their performances are relative stable with the variation of different databases, and their accuracies always are comparable with the baseline even when their dimensions are under 100. In fact, with the addition of dimensions, their performance can relatively improve, but the dimension will be very high which will take a long time for recognition task. • For NPE reduction method, it can obtain relative good accuracy on Yale, YaleB and ORL databases whose performances are comparable with the baseline, but on USPS, CMU-PIE and COIL-100 database, its performance quickly decreases, which is much worse than the baseline. Thus, the method is not stable. Furthermore, the performance of NPE is little worse than PCA, but the improve speed of its accuracy is very quick with the increase of the dimension. • For LPP with unsupervised learning method, it preserves the neighborhood structure of the data point, and it can obtain good accuracy on USPS, YaleB and CMUPIE database whose performance is comparable with the baseline, but on Yale, ORL and COIL-100 databases, their performance quickly decreases, which is also much worse than the baseline. The reason why the performance of LPP with unsupervised learning method changes so much, is that different databases have different data distributions, and in some databases, there are some noise points in the neighborhood structure of the data point, and the noise points affect the following recognition task. • For LSDA with unsupervised learning method, although the neighborhood structure of the data points is also preserved in LPP and NPE, the pattern shrinking is employed\nin LSDA, what is more, the pattern shrinking and the transformation matrix are simultaneously learnt. Experimental results show that with the variation of different databases and the number of dimensions, our proposed LSDA method are almost always consistently the best algorithm, whose improvements can reach about 3% to 10% improvement on all databases when comparing with other dimensionality reduction algorithms. Further, when comparing with the baseline on all databases, whose improvement achieves 10% to 30%. The experimental results show that our method is very efficient and effective. In other words, our method is stable and efficient. • From these experimental results, we also can know that no matter what kinds of dimensionality reduction algorithms, the number of samples of each subject will affect their performances, and with the increase of the number of samples, their accuracies greatly improve, but for each dataset split, when the dimension is added, their accuracies will keep stable.\nF. Experimental Results of Supervised Learning Algorithms\nIn these experiments, we constructed similarity matrix A by Eq.(6), and only same class neighbor samples are kept, thus, LSDA is also considered as supervised learning dimensionality reduction method. The experimental results of USPS, Yale, ORL, YaleB, CMU-PIE and Coil-100 are given on Fig.14, Fig.15, Fig.16, Fig.17, Fig.18 and Fig.19 respectively, at the\nsame time, we also choose the best results from them to compare to baseline method, whose results are given in Table I, Table II, Table III, Table IV, and Table V respectively. The meaning of the horizontal axis and the vertical coordinates in these figures are same to Fig.8 and Fig.9. Therefore, we will analyze the proposed approach in several different aspects. First, we will discuss the effect of the number of classes.\nSince the dimension of LDA is affected by the number of classes C, whose dimension is at most C-1. On USPS dataset, although there are several hundred samples of each object in training dataset, the number of classes is only ten. Thus, the dimension of LDA is at most nine. Fig.14 gives its results with the change of dimensions. From it, we can observe that the the accuracy of LDA obtains improvement with the increase of dimensions, but the maximum dimension of LDA only can be nine on this dataset. At the same time, we also can know that LPP and LSDA will not be affected by the number of classes, and the best accuracy of Baseline, LPP, LDA and LSDA is 96.9%, 96.8%, 91.6% and 98.3% respectively. Among these algorithms, LSDA can always obtain the best accuracy, and we also can observe the same conditions in other figures.\nSecond, the effect of the number of each object in the training dataset is discussed. In our experiments, three kinds of tasks are assessed, and their accuracies are given in Table.I, Table.II, Table.III, Table.IV and Table.V respectively. From these Tables, there are mainly three observations. (1) With the increase of the number of each object in the training dataset, the accuracies of all approaches are improved; (2) LFDA effective combines the ideas of LDA and LPP,whose performace are a litter better than that of LPP and LDA. (3) The accuracy of LSDA always is the best when different datasets are utilized and different numbers of training samples are chosen. (4) Although the neighborhood graph is also reserved in LPP, the shrunk pattern is ignored in LPP. Thus, its performance is a little better than Baseline and LDA, but it is a litter worse than LSDA. Thus, the proposed approach is effective and efficient.\nG. The Comparison Between Unsupervised Learning Methods and Supervising Learning Method\nIn above two sections, we have discussed unsupervised and supervised learning dimensionality reduction algorithms, and\nwe can know that: 1) if the label information of samples can be employed, most of time, the performance of supervised learning algorithms is much better than that of unsupervised learning algorithms; For example, the performances of the 10th row in Table.I, Table.II, Table.III and Table.IV, which belongs to LDA, are much better than the performances of the 5-th row in these corresponding Tables, which belongs to PCA; 2) As for LPP with unsupervised and supervised learning, the difference between them is that whether the label information in the construction of similarity matrix is utilized or not, but we still can observe that the accuracy of supervised learning outperforms the accuracy of unsupervised learning, where the improvement can reach about 5% to 30%; 3) Similarly, we also can find the similar case in LSDA method, where the label information of samples is very helpful, but we still observe that since the shrunk pattern is learnt in this model, which will make the data more flexible to fit the manifold structure, thus, the difference between unsupervised and supervised learning algorithms is not so great. That is to say, our proposed LSDA model is effective and stable.\nH. Parameter Sensitivity Analysis\nIn above Sections, we have proved the effective of LSDA, but in this Section, we will further evaluate the parameter sensitivity of it. Due to space limitation, we only choose one dataset for each task, and their performances on USPS, YaleB and Coil-100 datasets are shown in Fig.20, Fig.21 and Fig.22, Fig.23 and Fig.24 respectively. In these figures, Fig.20 (a), Fig.21 (a),Fig.22 (a) and Fig.23 (a) are the performance variance of sigma when gamma and dimension are fixed, where all gammas are equal to 2−10, Fig.20 (b), Fig.21(b), Fig.22(b) and Fig.23 (b) are the performance variance of gamma when sigma and dimension are fixed, where all sigmas are set to 0.9, 0.3 and 0.9 respectively. Further, for USPS and Coil-100, the dimension is set to 100, but the dimension of YaleB is assigned to 200.\nThere are mainly two observations from these figures. 1) When Gamma is fixed, the performance of LSDA is stable with the change of Sigma, especial for intermediate value; 2) When Sigma is fixed, the performance of LSDA has some fluctuation with the change of Gamma, and both ends also have the best performance, 3) The number of each subject sample in the training dataset will slightly affect the choice of Gamma. Thus, the optimization of LSDA will be easier and we can quickly choose the best parameters.\nI. The Generalization Analysis\nFrom Subection C in algorithm analysis of Section III, we can know that the objective function LSDA has strong generalized form, and LDA and local LDA are the special case of it when different extreme parameters are utilzied. Thus, in this experiment, we will assess their performances on USPS, YaleB and Coil-100 datasets when extreme gamma parameters are utilized, and the results are shown in Fig.25, Fig.26 and Fig.27 respectively. In the optimization, and the dimensions on all datasets are set to 200, and sigma is set to 0.7. From them, we can observe that when gamma is zero or positive infinity respectively, their accuracies are still comparable to the performance of LDA or PCA, but the optimization parameter of LSDA can obtain the best performance.For example, the accuracies of LSDA, LSDA (Gamma=0) and LSDA (Gamma = INF) are 97.8%, 94.6% and 95.4% respectively in Fig.25 where the dimension is equal to 60, and we also can observe the same case in other figures. Thus, it further proves that our proposed approach has better generalized performance."
    }, {
      "heading" : "V. CONCLUSION",
      "text" : "In this paper, we have proposed a novel and universal unsupervised and supervised learning dimensionality reduction method. It is mainly based on pattern shrinking technique. The main idea is to simultaneously learn the pattern shrinking and the projector matrix, and make the data more flexible to fit the manifold structure, which is more convenient for nonGaussian distribution data and real-life data. The advantage of our method is three-fold. First, uncovering the manifold structure can be mined by our method. Particularly, the shrunk\nFig. 26: Performance analysis on YaleB database when Gamm=0, Gamma = INF and the optimized Gamma are employed respectively, and from left to right, they are unsupervised and supervised learning algorithms respectively.\nFig. 27: Performance analysis on Coil-100 database when Gamm=0, Gamma = INF and the optimized Gamma are employed respectively, and from left to right, they are unsupervised and supervised learning algorithms respectively.\npattern learned by the proposed algorithm does not have the orthogonal constraint, which makes it more flexible to fit the manifold structure. The learned manifold knowledge is particularly helpful for achieving better dimensionality reduction result. Second, the transformation matrix is learnt from the original space and the pattern shrinking space, which contributes to more precise structural information for dimensionality reduction and recognition. Third, the pattern shrinking and transformation matrix are simultaneously learnt, which makes it easy to reduce dimension and data representation. Experimental results on several datasets show that when compared with the state-of-the-art unsupervised and supervised learning dimensionality reduction methods, it performs better. Moreover, it has much better generalized form, and LDA and local LDA are the special case of it. In addition, an efficient optimization algorithm is introduced to solve the non-convex objective function with low computational cost. Thus, LSDA is effective and efficient."
    } ],
    "references" : [ {
      "title" : "Dimensionality reduction:A comparative review",
      "author" : [ "L.J.P.V.-D. Maaten", "E.O. Postma", "H.J. vandenHerik" ],
      "venue" : "Tech. rep.,Tilburg University,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2009
    }, {
      "title" : "Local linear transformation embedding",
      "author" : [ "C. Hou", "J. Wang", "Y. Wu", "D. Yi" ],
      "venue" : "Neurocomputing, vol. 72, no. 10, pp. 2368-2378",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "and F",
      "author" : [ "C. Hou", "C. Zhang", "Y. Wu" ],
      "venue" : "Nie, Multiple view semi-supervised dimensionality reduction,Pattern Recogn., vol. 43, no. 3, pp. 720-730",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Laplacian Eigenmaps for Dimensionality Reduction and Data Representation",
      "author" : [ "M. Belkin", "P. Niyogi" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2003
    }, {
      "title" : "Local coordinates alignment with global preservation for dimensionality reduction",
      "author" : [ "J. Chen", "Z. Ma", "Y. Liu" ],
      "venue" : "IEEE Trans. Neural Netw. Learn. Syst.,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "Feature learning for image classification via multiobjective genetic programming",
      "author" : [ "L. Shao", "L. Liu", "X. Li" ],
      "venue" : "IEEE Trans. Neural Netw. Learn. Syst.,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2014
    }, {
      "title" : "and Y",
      "author" : [ "L. Wang", "S. Chen" ],
      "venue" : "Wang, A unified algorithm for mixed L2,pminimizations and its application in feature selection, Comput. Optim.Appl., vol. 58, no. 2, pp. 409-421",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Discriminant analysis for unsupervised feature selection",
      "author" : [ "J. Tang", "X. Hu", "H. Gao", "H. Liu" ],
      "venue" : "in Proc. 2014 SIAM Int. Conf. Data Min.,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    }, {
      "title" : "Trace ratio criterion for feature selection",
      "author" : [ "F. Nie", "S. Xiang", "Y. Jia", "C. Zhang", "S. Yan" ],
      "venue" : "in Proc. 23rd AAAI Conf. Artif. Intell.,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2008
    }, {
      "title" : "Eigenfaces vs",
      "author" : [ "P.N. Belhumeur", "J.P. Hepanha", "D.J. Kriegman" ],
      "venue" : "fisherfaces: recognition using class specific linear projection, IEEE. Trans. Pattern Analysis and Machine Intelligence, vol. 19, no. 7",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Face Recognition Using Kernel Based Fisher Discriminant Analysis",
      "author" : [ "Q. Liu", "R. Huang", "H. Lu", "S. Ma" ],
      "venue" : "Fifth Intl Conf. Automatic Face and Gesture Recognition",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "In D",
      "author" : [ "S. Lacoste-Julien", "F. Sha", "M.I. Jordan" ],
      "venue" : "Koller, Y. Bengio, D. Schuurmans and L. Bottou (Eds.), DiscLDA: Discriminative learning for dimensionality reduction and classification., Advances in Neural Information Processing Systems (NIPS), 21",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Locality Preserving Projections",
      "author" : [ "X. He", "P. Niyogi" ],
      "venue" : "Advances in Neural Information Processing Systems 16, Vancouver, British Columbia, Canada",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Semi-supervised Dimensionality Reduction via Harmonic Functions",
      "author" : [ "C. Hou", "F.Nie", "Y. Wu" ],
      "venue" : "Proceedingsof 8th International Conference,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2011
    }, {
      "title" : "Eigenfaces for recognition",
      "author" : [ "M. Turk", "A. Pentland" ],
      "venue" : "Journal of Cognitive Neuroscience",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Neighborhood preserving embedding",
      "author" : [ "Xiaofei He", "Deng Cai", "Shuicheng Yan", "Hong-Jiang Zhang" ],
      "venue" : "In Proceedings of the 10th IEEE International Conference on Computer Vision,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2005
    }, {
      "title" : "Face recognition using laplacianfaces",
      "author" : [ "X. He", "S. Yan", "Y. Hu", "P. Niyogi", "H.-J. Zhang" ],
      "venue" : "IEEE Trans. on Pattern Analysis and Machine Intelligence",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Laplacian eigenmaps for dimensionality reduction and data representation",
      "author" : [ "M. Belkin", "P. Niyogi" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2003
    }, {
      "title" : "Effective Discriminative Feature Selection with Non-trivial Solutions",
      "author" : [ "H. Tao", "C. Hou", "F. Nie", "Y.Jiao", "D. Yi" ],
      "venue" : "IEEE TNNLS, eprint arXiv:1504.05408,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2015
    }, {
      "title" : "Quimet,M.: Out-of-sample Extensions for LLE, Isomap, MDS, Eigenmaps, and Spectral Clustering",
      "author" : [ "Y. Bengio", "J. Paiement", "P. Vincent", "O. Dellallaeu", "Roux", "N.L" ],
      "venue" : "Neural Information Processing Systems,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2003
    }, {
      "title" : "Learning a subspace for clustering via pattern shrinking",
      "author" : [ "C. Hou", "F. Nie", "Y. Jiao", "C. Zhang", "Y. Wu" ],
      "venue" : "Inf. Process. Manage",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2013
    }, {
      "title" : "A Convex Formulation for Spectral Shrunk Clustering",
      "author" : [ "X.-J. Chang", "F.-P. Nie", "Z.-G. Ma", "Y. Yang", "X.-F. Zhou" ],
      "venue" : "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Convergence theorems for generalized alternating minimization procedures",
      "author" : [ "A. Gunawardana", "W. Byrne" ],
      "venue" : "The Journal of Machine Learning Research, vol. 6, pp. 2049-2073",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Clustering and projected clustering with adaptive neighbors,In The 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD",
      "author" : [ "F. Nie", "X. Wang", "H. Huang" ],
      "venue" : null,
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2014
    }, {
      "title" : "A new simplex sparse learning model to measure data similarity for clustering",
      "author" : [ "J. Huang", "F. Nie", "H. Huang" ],
      "venue" : "In Proceedings of the 24th International Conference on Artificial Intelligence,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2015
    }, {
      "title" : "Unsupervised Large Graph Embedding",
      "author" : [ "Feiping Nie", "Wei Zhu", "Xuelong Li" ],
      "venue" : "The 31st AAAI Conference on Artificial Intelligence (AAAI), San Francisco,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2017
    }, {
      "title" : "Reduction of Multimodal Labeled Data by Local Fisher Discriminant Analysis",
      "author" : [ "Masashi Sugiyama", "Dimensionality" ],
      "venue" : "Journal of Machine Learning",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2007
    }, {
      "title" : "Subspace Clustering via New Low-Rank Model with Discrete Group Structure Constraint",
      "author" : [ "Feiping Nie", "Heng Huang" ],
      "venue" : "The 25th International Joint Conference on Artificial Intelligence (IJCAI),New York,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2016
    }, {
      "title" : "A New Sparse Subspace Clustering Algorithm for Hyperspectral Remote Sensing Imagery,IEEE",
      "author" : [ "Han Zhai", "Hongyan Zhang", "Liangpei Zhang" ],
      "venue" : "GEOSCIENCE AND REMOTE SENSING LETTERS,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Thus, dimensionality reduction is a hot and classical topic [1], which attempts to overcome the curse of the dimensionality and to extract relevant features [2], [3].",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 1,
      "context" : "Thus, dimensionality reduction is a hot and classical topic [1], which attempts to overcome the curse of the dimensionality and to extract relevant features [2], [3].",
      "startOffset" : 157,
      "endOffset" : 160
    }, {
      "referenceID" : 2,
      "context" : "Thus, dimensionality reduction is a hot and classical topic [1], which attempts to overcome the curse of the dimensionality and to extract relevant features [2], [3].",
      "startOffset" : 162,
      "endOffset" : 165
    }, {
      "referenceID" : 3,
      "context" : "com) dimensionality is usually very low [4].",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 4,
      "context" : "In dimensionality reduction [5], [6], feature selection [7], [8], [9], [10], where a subset of features of the original set are selected, and feature transformation [11], [12], [13], [14], where the original features are transformed to a new feature subspace, are the two main ways.",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 5,
      "context" : "In dimensionality reduction [5], [6], feature selection [7], [8], [9], [10], where a subset of features of the original set are selected, and feature transformation [11], [12], [13], [14], where the original features are transformed to a new feature subspace, are the two main ways.",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 6,
      "context" : "In dimensionality reduction [5], [6], feature selection [7], [8], [9], [10], where a subset of features of the original set are selected, and feature transformation [11], [12], [13], [14], where the original features are transformed to a new feature subspace, are the two main ways.",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 7,
      "context" : "In dimensionality reduction [5], [6], feature selection [7], [8], [9], [10], where a subset of features of the original set are selected, and feature transformation [11], [12], [13], [14], where the original features are transformed to a new feature subspace, are the two main ways.",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 8,
      "context" : "In dimensionality reduction [5], [6], feature selection [7], [8], [9], [10], where a subset of features of the original set are selected, and feature transformation [11], [12], [13], [14], where the original features are transformed to a new feature subspace, are the two main ways.",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 9,
      "context" : "In dimensionality reduction [5], [6], feature selection [7], [8], [9], [10], where a subset of features of the original set are selected, and feature transformation [11], [12], [13], [14], where the original features are transformed to a new feature subspace, are the two main ways.",
      "startOffset" : 165,
      "endOffset" : 169
    }, {
      "referenceID" : 10,
      "context" : "In dimensionality reduction [5], [6], feature selection [7], [8], [9], [10], where a subset of features of the original set are selected, and feature transformation [11], [12], [13], [14], where the original features are transformed to a new feature subspace, are the two main ways.",
      "startOffset" : 171,
      "endOffset" : 175
    }, {
      "referenceID" : 11,
      "context" : "In dimensionality reduction [5], [6], feature selection [7], [8], [9], [10], where a subset of features of the original set are selected, and feature transformation [11], [12], [13], [14], where the original features are transformed to a new feature subspace, are the two main ways.",
      "startOffset" : 177,
      "endOffset" : 181
    }, {
      "referenceID" : 12,
      "context" : "In dimensionality reduction [5], [6], feature selection [7], [8], [9], [10], where a subset of features of the original set are selected, and feature transformation [11], [12], [13], [14], where the original features are transformed to a new feature subspace, are the two main ways.",
      "startOffset" : 183,
      "endOffset" : 187
    }, {
      "referenceID" : 9,
      "context" : "g LDA [11]; Kernel LDA [12]; DiscLDA [13]; LPP [14]; SDRHF [15], Local fisher discriminant analysis (LFDA) [31]) and unsupervised learning (PCA [16], [17]; Kernal PCA [18]; LLE [19]; LPP [14]; NPE [20]; Isomap [22]; Laplacian eigenmaps, [21], [22]) dimensionality reduction methods.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 10,
      "context" : "g LDA [11]; Kernel LDA [12]; DiscLDA [13]; LPP [14]; SDRHF [15], Local fisher discriminant analysis (LFDA) [31]) and unsupervised learning (PCA [16], [17]; Kernal PCA [18]; LLE [19]; LPP [14]; NPE [20]; Isomap [22]; Laplacian eigenmaps, [21], [22]) dimensionality reduction methods.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 11,
      "context" : "g LDA [11]; Kernel LDA [12]; DiscLDA [13]; LPP [14]; SDRHF [15], Local fisher discriminant analysis (LFDA) [31]) and unsupervised learning (PCA [16], [17]; Kernal PCA [18]; LLE [19]; LPP [14]; NPE [20]; Isomap [22]; Laplacian eigenmaps, [21], [22]) dimensionality reduction methods.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 12,
      "context" : "g LDA [11]; Kernel LDA [12]; DiscLDA [13]; LPP [14]; SDRHF [15], Local fisher discriminant analysis (LFDA) [31]) and unsupervised learning (PCA [16], [17]; Kernal PCA [18]; LLE [19]; LPP [14]; NPE [20]; Isomap [22]; Laplacian eigenmaps, [21], [22]) dimensionality reduction methods.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 13,
      "context" : "g LDA [11]; Kernel LDA [12]; DiscLDA [13]; LPP [14]; SDRHF [15], Local fisher discriminant analysis (LFDA) [31]) and unsupervised learning (PCA [16], [17]; Kernal PCA [18]; LLE [19]; LPP [14]; NPE [20]; Isomap [22]; Laplacian eigenmaps, [21], [22]) dimensionality reduction methods.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 26,
      "context" : "g LDA [11]; Kernel LDA [12]; DiscLDA [13]; LPP [14]; SDRHF [15], Local fisher discriminant analysis (LFDA) [31]) and unsupervised learning (PCA [16], [17]; Kernal PCA [18]; LLE [19]; LPP [14]; NPE [20]; Isomap [22]; Laplacian eigenmaps, [21], [22]) dimensionality reduction methods.",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 14,
      "context" : "g LDA [11]; Kernel LDA [12]; DiscLDA [13]; LPP [14]; SDRHF [15], Local fisher discriminant analysis (LFDA) [31]) and unsupervised learning (PCA [16], [17]; Kernal PCA [18]; LLE [19]; LPP [14]; NPE [20]; Isomap [22]; Laplacian eigenmaps, [21], [22]) dimensionality reduction methods.",
      "startOffset" : 150,
      "endOffset" : 154
    }, {
      "referenceID" : 12,
      "context" : "g LDA [11]; Kernel LDA [12]; DiscLDA [13]; LPP [14]; SDRHF [15], Local fisher discriminant analysis (LFDA) [31]) and unsupervised learning (PCA [16], [17]; Kernal PCA [18]; LLE [19]; LPP [14]; NPE [20]; Isomap [22]; Laplacian eigenmaps, [21], [22]) dimensionality reduction methods.",
      "startOffset" : 187,
      "endOffset" : 191
    }, {
      "referenceID" : 15,
      "context" : "g LDA [11]; Kernel LDA [12]; DiscLDA [13]; LPP [14]; SDRHF [15], Local fisher discriminant analysis (LFDA) [31]) and unsupervised learning (PCA [16], [17]; Kernal PCA [18]; LLE [19]; LPP [14]; NPE [20]; Isomap [22]; Laplacian eigenmaps, [21], [22]) dimensionality reduction methods.",
      "startOffset" : 197,
      "endOffset" : 201
    }, {
      "referenceID" : 17,
      "context" : "g LDA [11]; Kernel LDA [12]; DiscLDA [13]; LPP [14]; SDRHF [15], Local fisher discriminant analysis (LFDA) [31]) and unsupervised learning (PCA [16], [17]; Kernal PCA [18]; LLE [19]; LPP [14]; NPE [20]; Isomap [22]; Laplacian eigenmaps, [21], [22]) dimensionality reduction methods.",
      "startOffset" : 210,
      "endOffset" : 214
    }, {
      "referenceID" : 16,
      "context" : "g LDA [11]; Kernel LDA [12]; DiscLDA [13]; LPP [14]; SDRHF [15], Local fisher discriminant analysis (LFDA) [31]) and unsupervised learning (PCA [16], [17]; Kernal PCA [18]; LLE [19]; LPP [14]; NPE [20]; Isomap [22]; Laplacian eigenmaps, [21], [22]) dimensionality reduction methods.",
      "startOffset" : 237,
      "endOffset" : 241
    }, {
      "referenceID" : 17,
      "context" : "g LDA [11]; Kernel LDA [12]; DiscLDA [13]; LPP [14]; SDRHF [15], Local fisher discriminant analysis (LFDA) [31]) and unsupervised learning (PCA [16], [17]; Kernal PCA [18]; LLE [19]; LPP [14]; NPE [20]; Isomap [22]; Laplacian eigenmaps, [21], [22]) dimensionality reduction methods.",
      "startOffset" : 243,
      "endOffset" : 247
    }, {
      "referenceID" : 18,
      "context" : "The difference between supervised and unsupervised dimensionality reduction algorithms lies in whether the ground truth is utilized or not in learning the transformation matrix [23].",
      "startOffset" : 177,
      "endOffset" : 181
    }, {
      "referenceID" : 14,
      "context" : "The related references (PCA, [16], [17]; LDA [11]; DiscLDA [12], LFDA [31] show that the supervised learning methods can obtain much better performance than the unsupervised learning methods, and has been applied into different research domains.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 9,
      "context" : "The related references (PCA, [16], [17]; LDA [11]; DiscLDA [12], LFDA [31] show that the supervised learning methods can obtain much better performance than the unsupervised learning methods, and has been applied into different research domains.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 10,
      "context" : "The related references (PCA, [16], [17]; LDA [11]; DiscLDA [12], LFDA [31] show that the supervised learning methods can obtain much better performance than the unsupervised learning methods, and has been applied into different research domains.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 26,
      "context" : "The related references (PCA, [16], [17]; LDA [11]; DiscLDA [12], LFDA [31] show that the supervised learning methods can obtain much better performance than the unsupervised learning methods, and has been applied into different research domains.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 14,
      "context" : "[16], [17] proposed Principal Component Analysis which is the most frequently used dimensionality reduction method.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 12,
      "context" : "[14], [20] proposed Locality Preserving Projections (LPP) in 2003 and Neighborhood Preserving Embedding (NPE) in 2005.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[14], [20] proposed Locality Preserving Projections (LPP) in 2003 and Neighborhood Preserving Embedding (NPE) in 2005.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 19,
      "context" : "However, the motivations between PCA, LPP, NPE and LLE, Isomap and Laplacian Eigenmaps, are very different, and the original LLE, Isomap and Laplcacian Eigenmaps cannot deal with the out-of-sample problem directly [24], that is to say, they only can deal with the training samples, and obtain the low dimension embedding, but for test samples, they cannot directly calculated, analytically or cannot calculated at all.",
      "startOffset" : 214,
      "endOffset" : 218
    }, {
      "referenceID" : 20,
      "context" : "Recently, pattern shrinking [25], [26] is often utilized in clustering algorithms, which not only characterizes the linear and nonlinear structures of data, but also reflects the requirements of clustering, what is more, it can obtain satisfying performance on high dimensional data and non-Gaussian distribution data [30], [32], [33], [34].",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 21,
      "context" : "Recently, pattern shrinking [25], [26] is often utilized in clustering algorithms, which not only characterizes the linear and nonlinear structures of data, but also reflects the requirements of clustering, what is more, it can obtain satisfying performance on high dimensional data and non-Gaussian distribution data [30], [32], [33], [34].",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 25,
      "context" : "Recently, pattern shrinking [25], [26] is often utilized in clustering algorithms, which not only characterizes the linear and nonlinear structures of data, but also reflects the requirements of clustering, what is more, it can obtain satisfying performance on high dimensional data and non-Gaussian distribution data [30], [32], [33], [34].",
      "startOffset" : 318,
      "endOffset" : 322
    }, {
      "referenceID" : 27,
      "context" : "Recently, pattern shrinking [25], [26] is often utilized in clustering algorithms, which not only characterizes the linear and nonlinear structures of data, but also reflects the requirements of clustering, what is more, it can obtain satisfying performance on high dimensional data and non-Gaussian distribution data [30], [32], [33], [34].",
      "startOffset" : 324,
      "endOffset" : 328
    }, {
      "referenceID" : 28,
      "context" : "Recently, pattern shrinking [25], [26] is often utilized in clustering algorithms, which not only characterizes the linear and nonlinear structures of data, but also reflects the requirements of clustering, what is more, it can obtain satisfying performance on high dimensional data and non-Gaussian distribution data [30], [32], [33], [34].",
      "startOffset" : 330,
      "endOffset" : 334
    }, {
      "referenceID" : 23,
      "context" : "The similarity matrix [28], [29] A ∈ Rn×n is composed of all the aij , which is utilized to characterize the manifold structure of original data points, i.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 24,
      "context" : "The similarity matrix [28], [29] A ∈ Rn×n is composed of all the aij , which is utilized to characterize the manifold structure of original data points, i.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 20,
      "context" : "shrinking [25], [26] may be helpful for us to find a suitable transformation matrix, thus, the pattern shrinking is employed in our model.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 21,
      "context" : "shrinking [25], [26] may be helpful for us to find a suitable transformation matrix, thus, the pattern shrinking is employed in our model.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 19,
      "context" : "In addition, in subspace learning via pattern shrinking, it cannot directly deal with the out-of-the-sample problem [24], where only the low dimensional embedding",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 26,
      "context" : "In fact, from the objective function, we can know that it is another local LDA [31].",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 0,
      "context" : "The pixel values are then scaled to [0,1] (divided by 256).",
      "startOffset" : 36,
      "endOffset" : 41
    }, {
      "referenceID" : 22,
      "context" : "As for LSDA, the change of sigma and gamma parameters will affect the performance of LSDA, thus, the alternating method is utilized [27], where at each iteration, we first fix the one variable, and then optimize",
      "startOffset" : 132,
      "endOffset" : 136
    } ],
    "year" : 2017,
    "abstractText" : "Dimensionality reduction is a crucial step for pattern recognition and data mining tasks to overcome the curse of dimensionality. Principal component analysis (PCA) is a traditional technique for unsupervised dimensionality reduction, which is often employed to seek a projection to best represent the data in a least-squares sense, but if the original data is nonlinear structure, the performance of PCA will quickly drop. An supervised dimensionality reduction algorithm called Linear discriminant analysis (LDA) seeks for an embedding transformation, which can work well with Gaussian distribution data or single-modal data, but for non-Gaussian distribution data or multimodal data, it gives undesired results. What is worse, the dimension of LDA cannot be more than the number of classes. In order to solve these issues, Local shrunk discriminant analysis (LSDA) is proposed in this work to process the nonGaussian distribution data or multimodal data, which not only incorporate both the linear and nonlinear structures of original data, but also learn the pattern shrinking to make the data more flexible to fit the manifold structure. Further, LSDA has more strong generalization performance, whose objective function will become local LDA and traditional LDA when different extreme parameters are utilized respectively. What is more, a new efficient optimization algorithm is introduced to solve the non-convex objective function with low computational cost. Compared with other related approaches, such as PCA, LDA and local LDA, the proposed method can derive a subspace which is more suitable for non-Gaussian distribution and real data. Promising experimental results on different kinds of data sets demonstrate the effectiveness of the proposed approach.",
    "creator" : "LaTeX with hyperref package"
  }
}