{
  "name" : "1105.5442.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Oleg Ledeniov", "Shaul Markovitch" ],
    "emails" : [ "olleg@cs.technion.ac.il", "shaulm@cs.technion.ac.il" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Journal of Arti cial Intelligence Research 9 (1998) 37-97 Submitted 2/98; published 9/98The Divide-and-Conquer Subgoal-Ordering Algorithmfor Speeding up Logic InferenceOleg Ledeniov olleg@cs.technion.ac.ilShaul Markovitch shaulm@cs.technion.ac.ilComputer Science DepartmentTechnion { Israel Institute of TechnologyHaifa 32000, Israel AbstractIt is common to view programs as a combination of logic and control: the logic partde nes what the program must do, the control part { how to do it. The Logic Program-ming paradigm was developed with the intention of separating the logic from the control.Recently, extensive research has been conducted on automatic generation of control forlogic programs. Only a few of these works considered the issue of automatic generation ofcontrol for improving the e ciency of logic programs. In this paper we present a novel al-gorithm for automatic nding of lowest-cost subgoal orderings. The algorithm works usingthe divide-and-conquer strategy. The given set of subgoals is partitioned into smaller sets,based on co-occurrence of free variables. The subsets are ordered recursively and merged,yielding a provably optimal order. We experimentally demonstrate the utility of the algo-rithm by testing it in several domains, and discuss the possibilities of its cooperation withother existing methods.1. IntroductionIt is common to view programs as a combination of logic and control (Kowalski, 1979). Thelogic part de nes what the program must do, the control part { how to do it. Traditionalprogramming languages require that the programmers supply both components. The LogicProgramming paradigm was developed with the intention of separating the logic from thecontrol (Lloyd, 1987). The goal of the paradigm is that the programmer speci es the logicwithout bothering about the control, which should be supplied by the interpreter.Initially, most practical logic programming languages, such as Prolog (Clocksin & Mel-lish, 1987; Sterling & Shapiro, 1994), did not include the means for automatic generation ofcontrol. As a result, a Prolog programmer had to implicitly de ne the control by the order ofclauses and of subgoals within the clauses. Recently, extensive research has been conductedon automatic generation of control for logic programs. A major part of this research is con-cerned with control that a ects correctness and termination of logic programs (De Schreye& Decorte, 1994; Somogyi, Henderson, & Conway, 1996b; Cortesi, Le Charlier, & Rossi,1997). Only a few of these works consider the issue of automatic generation of control forimproving the e ciency of logic programs. Finding a good ordering that leads to e cientexecution requires a deep understanding of the logic inference mechanism. Hence, in manycases, only expert programmers are able to generate e cient programs. The problem inten-si es with the recent development of the eld of inductive logic programming (Muggletonc 1998 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.\nLedeniov & Markovitch& De Raedt, 1994). There, logic programs are automatically induced by learning. Suchlearning algorithms are commonly built with the aim of speeding up the induction processwithout considering the e ciency of resulting programs.The goal of the research described in this paper is to design algorithms that automat-ically nd e cient orderings of subgoal sequences. Several researchers have explored theproblem of automatic reordering of subgoals in logic programs (Warren, 1981; Naish, 1985b;Smith & Genesereth, 1985; Natarajan, 1987; Markovitch & Scott, 1989). The general sub-goal ordering problem is known to be NP-hard (Ullman, 1982; Ullman & Vardi, 1988).Smith and Genesereth (1985) and Markovitch and Scott (1989) present search algorithmsfor nding optimal orderings. These algorithms are general and carry exponential costs fornon-trivial sets of subgoals. Natarajan (1987) describes an e cient algorithm for the specialcase where subgoals in the set do not share free variables.In this paper we present a novel algorithm for subgoal ordering. We call two subgoalsthat share a free variable dependent. Unlike Natarajan's approach, which can only handlesubgoal sets that are completely independent, our algorithm can deal with any subgoalset, while making maximal use of the existing dependencies for acceleration of the orderingprocess. In the worst case the algorithm { like that of Smith and Genesereth { is exponential.Still, in most practical cases, our algorithm exploits subgoal dependencies and nds optimalorderings in polynomial time.We start with an analysis of the ordering problem and demonstrate its importancethrough examples. We then show how to compute the cost of a given ordering based onthe cost and the number of solutions of the individual subgoals. We describe the algorithmof Natarajan and the algorithm of Smith and Genesereth and show how the two can becombined into an algorithm that is more e cient and general than each of the two. Weshow drawbacks of the combined algorithm and introduce the new algorithm, which avoidsthese drawbacks. We call it the Divide-and-Conquer algorithm (dac algorithm). We provethe correctness of the algorithm, discuss its complexity and compare it to the combinedalgorithm. The dac algorithm assumes knowledge of the cost and the number of solutionsof the subgoals. This knowledge can be obtained by machine learning techniques such asthose employed by Markovitch and Scott (1989). Finally, we test the utility of our algorithmby running a set of experiments on arti cial and real domains.The dac algorithm for subgoal ordering can be combined with many existing methodsin logic programming, such as program transformation, compilation, termination control,correctness veri cation, and others. We discuss the possibilities of such combinations in theconcluding section.Section 2 states the ordering problem. Section 3 describes existing ordering algorithmsand their combination. Section 4 presents the new algorithm. Section 5 discusses theacquisition of the control knowledge. Section 6 contains experimental results. Section 7contains a discussion of practical issues, comparison with other works and conclusions.2. Background: Automatic Ordering of SubgoalsWe start by describing the conventions and assumptions accepted in this paper. Then wedemonstrate the importance of subgoal ordering and discuss its validity. Finally, we presenta classi cation of ordering methods and discuss related work.38\nThe Divide-and-Conquer Subgoal-Ordering Algorithm2.1 Conventions and AssumptionsAll constant, function and predicate symbols in programs begin with lower case letters,while capital letters are reserved for variables. Braces are used to denote unordered sets(e.g., fa; b; cg), and angle brackets are used for ordered sequences (e.g., ha; b; ci). Parallellines (k) denote concatenations of ordered sequences of subgoals. When speaking aboutabstract subgoals (and not named predicates of concrete programs), we denote separatesubgoals by capital letters (A;B : : :), ordered sequences of subgoals by capitalized vectors( ~B; ~OS : : :), and sets of subgoals by calligraphic capitals (B;S : : :). (S) denotes the set ofall permutations of S.We assume that the programs we work with are written in pure Prolog, i.e., without cutoperators, meta-logical or extra-logical predicates. Alternatively, we can assume that onlypure Prolog sub-sequences of subgoals are subject to ordering. For example, given a rule ofthe form A B1; B2; B3; !; B4; B5; B6:only its nal part fB4; B5; B6g can be ordered (without a ecting the solution set).In this work we focus upon the task of nding all the solutions to a set of subgoals.2.2 Ordering of Subgoals in Logic ProgramsA logic program is a set of clauses:A B1; B2; : : : ; Bn: (n 0)where A;B1; : : : ; Bn are literals (predicates with arguments). To use such a clause forproving a goal that matches A, we must prove that all B-s hold simultaneously, underconsistent bindings of the free variables. A solution is such a set of variable bindings. Thesolution set of a goal is the bag of all its solutions created by its program.A computation rule de nes which subgoal will be proved next. In Prolog, the compu-tation rule always selects the leftmost subgoal in a goal. If a subgoal fails, backtracking isperformed { the proof of the previous subgoal is re-entered to generate another solution.For a detailed de nition of the logic inference process, see Lloyd (1987).Theorem 1 The solution set of a set of subgoals does not depend on the order of theirexecution.Proof: When we are looking for all solutions, the solution set does not depend on thecomputation rule chosen (Theorems 9.2 and 10.3 in Lloyd, 1987). Since a transposition ofsubgoals in an ordered sequence can be regarded as a change of the computation rule (thesubgoals are selected in di erent order), such transposition does not change the solutionset. 2This theorem implies that we may reorder subgoals during the proof derivation. Yet thee ciency of the derivation strongly depends on the chosen order of subgoals. The followingexample illustrates how two di erent orders can lead to a large di erence in executione ciency. 39\nLedeniov & Markovitchparent(abraham,isaac).parent(sarah,isaac).parent(abraham,ishmael).parent(isaac,esav).parent(isaac,jakov).... More parent clauses ... male(abraham).male(isaac).male(ishmael).male(jakov).male(esav).... More male clauses ...brother(X,Y) male(X), parent(W,X), parent(W,Y), X=/=Y.father(X,Y) male(X), parent(X,Y).uncle(X,Y) parent(Z,Y), brother(X,Z).... More rules of relations ...Figure 1: A small fragment of a Biblical database describing family relationships.Example 1Consider a Biblical family database such as the one listed in Figure 1 (a similar databaseappears in the book by Sterling & Shapiro, 1994). The body of the rule de ning theuncle-nephew (or uncle-niece) relation can be ordered in two ways:1. uncle(X,Y) brother(X,Z), parent(Z,Y).2. uncle(X,Y) parent(Z,Y), brother(X,Z).To prove the goal uncle(ishmael,Y) using the rst version of the rule, the interpreter will rst look for Ishmael's siblings (and nd Isaac) and then for the siblings' children (Esavand Jacov). The left part of Figure 2 shows the associated proof tree with a total of 10nodes. If we use the second version of the rule, the interpreter will create all the parent-child pairs available in the database, and will test for each parent whether he (or she) isIshmael's sibling. The right part of Figure 2 shows the associated proof tree with a totalof 4(N 2) + 6 2 + 2 = 4N + 6 nodes, where N is the number of parent-child pairs in thedatabase. The tree contains two success branches and N 2 failure branches; in the gurewe show one example of each. While the two versions of the rule yield identical solutionsets, the rst version leads to a much smaller tree and to a faster execution.Note that this result is true only for the given mode (bound,free) of the head literal;for the mode (free,bound), as in uncle(X,jacov), the outcome is the contrary: the secondversion of the rule yields a smaller tree.2.3 Categories of Subgoal Ordering MethodsAssume that the current conjunctive goal (the current resolvent) is fA1; A2g. Assume thatwe use the rule \\A1 A11; A12:\" to reduce A1. According to Theorem 1, the producedresolvent, fA11; A12; A2g, can be executed in any order. We call ordering methods thatallow any permutation of the resolvent interleaving ordering methods, since they permit40\nThe Divide-and-Conquer Subgoal-Ordering Algorithmuncle(X,Y) brother(X,Z), parent(Z,Y). uncle(X,Y) parent(Z,Y), brother(X,Z). parent(ishmael,Y) ishmael =/= ishmael, uncle(ishmael,Y) brother(ishmael,Z), parent(Z,Y) male(ishmael), parent(W,ishmael), parent(W,Z), ishmael =/= Z, parent(Z,Y) parent(W,ishmael), parent(W,Z), ishmael =/= Z, parent(Z,Y) W=abraham parent(abraham,Z), ishmael=/=Z, parent(Z,Y) Z=ishmael Z=isaac isaac =/= ishmael, parent(isaac,Y)\nparent(isaac,Y)\nY=esav Y=jacov\nW=abraham W=abraham\nZ=isaac, Y=cain Z=adam,\nother parent-child\npairs\nY=jacov\nparent(Z,Y), brother(ishmael,Z)\nbrother(ishmael,isaac)\nparent(W,isaac), ishmael=/=isaac male(ishmael), parent(W,ishmael), parent(W,ishmael), parent(W,isaac), ishmael =/= isaac parent(abraham,isaac), ishmael=/=isaac\nishmael =/= isaac\nbrother(ishmael,adam)\nparent(W,adam), ishmael =/= adam male(ishmael), parent(W,ishmael), parent(W,ishmael), parent(W,adam), ishmael =/= adam parent(abraham,adam), ishmael =/=adam\nuncle(ishmael,Y)\nFigure 2: Two proof trees obtained with di erent orderings of a single rule in Example 1.interleaving of subgoals from di erent rule bodies. When ordering is performed only onrule bodies before using them for reduction, the method is non-interleaving. In the aboveexample, interleaving methods will consider all 6 permutations of the resolvent, while non-interleaving methods will consider only two orderings: hA11; A12; A2i and hA12; A11; A2i.Interleaving ordering methods deal with signi cantly more possible orderings than non-interleaving methods. That means that they can nd more e cient orderings. On theother hand, the space of possible orderings may become prohibitively large, requiring toomany computational resources.Subgoal ordering can take place at various stages of the proof process. We divide allsubgoal ordering methods into static, semi-dynamic and dynamic. Static ordering: The rule bodies are ordered before the execution starts. No order-ing takes place during the execution. Semi-dynamic ordering: Whenever a rule is selected for reduction, its body isordered. The order of its subgoals does not change after the reduction takes place. Dynamic ordering: The ordering decision is made at each inference step.Static methods add no overhead to the execution time. However, the optimal orderingof a rule often depends on a particular binding of a variable, which can be known only atrun-time. For instance, in Example 1 we saw that the rst ordering of the rule is betterfor proving the goal uncle(ishmael,Y). And yet, for the goal uncle(X,jacov), it is thesecond ordering that yields more e cient execution. To handle such cases statically, wemust compute the optimal ordering for each possible binding.41\nLedeniov & MarkovitchObviously, static ordering can only be non-interleaving. The dynamic method is more exible, since it can use more updated knowledge about variable bindings, but it also carriesthe largest runtime overhead, since it is invoked several times for each use of a rule body.The semi-dynamic method is a compromise between the two: it is more powerful than thestatic method, because it can dynamically propose di erent orderings for di erent instancesof the same rule; it also carries less overhead than the dynamic method, because it is invokedonly once for each use of a rule body.The total time of proving a goal is the sum of the ordering time and the inference time.Interleaving and dynamic methods have the best potential for reducing the inference time,but may signi cantly augment the ordering time. Static methods do not devote time toordering (it is done o -line), but have a limited potential for reducing the inference time.The algorithms described in this paper can be used for all categories of ordering methods,although in the experiments described in Section 6 we have only implemented semi-dynamic,non-interleaving ordering methods: on each reduction, the rule body is ordered and addedto the left end of the resolvent, and then the leftmost literal of the resolvent is selected forthe next reduction step.2.4 Related WorkThe problem of computational ine ciency of logic inference was the subject of extensiveresearch. The most obvious aspect of this ine ciency is the possible non-termination ofa proof. Several researchers developed compile-time and run-time techniques to detectand avoid in nite computations (De Schreye & Decorte, 1994). A certain success wasachieved in providing more advanced control through employment of co-routining for inter-predicate synchronization purposes (Clark & McCabe, 1979; Porto, 1984; Naish, 1984).Also, in nite computations can be avoided by pruning in nite branches that do not containsolutions (Vasak & Potter, 1985; Smith, Genesereth, & Ginsberg, 1986; Bol, Apt, & Klop,1991). In the NAIL! system (Morris, 1988) subgoals are automatically reordered to avoidnontermination.Still, even when the proof is nite, it is desirable to make it more e cient. Severalresearchers studied the problem of clause ordering (Smith, 1989; Cohen, 1990; Etzioni,1991; Laird, 1992; Mooney & Zelle, 1993; Greiner & Orponen, 1996). If we are looking forall the solutions of a goal, then the e ciency does not depend on the clause order (assumingno cuts). Indeed, if some predicate has m clauses, and for some argument bindings theseclauses produce all their solutions in times t1; t2 : : : tm, then all solutions of the predicateunder these bindings are obtained in time t1 + t2 + : : :+ tm, regardless of the order inwhich the clauses are applied. Di erent clause orderings correspond to di erent orders inwhich branches are selected in a proof tree; if we traverse the entire tree, then the numberof traversal steps does not depend on the order of branch selection, though the order ofsolutions found does depend on it.Subgoal ordering, as was demonstrated in Example 1, can signi cantly a ect the e -ciency of proving a goal. There are two major approaches to subgoal ordering. The rstapproach uses various heuristics to order subgoals, for example: Choose a subgoal whose predicate has the smallest number of matching clauses (Minker,1978). 42\nThe Divide-and-Conquer Subgoal-Ordering Algorithm Prefer a subgoal with more constants (Minker, 1978). Choose a subgoal with the largest size, where the size is de ned as the number ofoccurrences of predicate symbols, function symbols, and variables (Nie & Plaisted,1990). Choose a subgoal with the largest mass, where the mass of a subgoal depends onthe frequency of its arguments and sub-arguments in the entire goal (Nie & Plaisted,1990). Choose a subgoal with the least number of solutions (Warren, 1981; Nie & Plaisted,1990). Apply \\tests\" before \\generators\" (Naish, 1985a). Prefer calls that fail quickly (Naish, 1985b).The heuristic methods usually execute quickly, but may yield suboptimal orderings.The second approach, which is adopted in this paper, aims at nding optimal order-ings (Smith & Genesereth, 1985; Natarajan, 1987; Markovitch & Scott, 1989). Natarajanproposed an e cient way to order a special sort of subgoal set (where all subgoals are in-dependent), while Smith and Genesereth proposed a general, but ine cient algorithm. Inthe following section we build a unifying framework for dealing with subgoal ordering anddescribe variations on Natarajan's and Smith and Genesereth's algorithms. We also showhow the two can be combined for increased e ciency.3. Algorithms for Subgoal Ordering in Logic ProgramsThe goal of the work presented here is to order subgoals for speeding up logic programs. Thissection starts with an analysis of the cost of executing a sequence of subgoals. The resultingformula is the basis for the subsequent ordering algorithms. Then we discuss dependenceof subgoals and present existing ordering algorithms for independent and dependent sets ofsubgoals. Finally, we combine these algorithms into a more general and e cient one.3.1 The Cost of Executing a Sequence of SubgoalsIn this subsection we analyze the cost of executing a sequence of subgoals. The analysisbuilds mainly on the work of Smith and Genesereth (1985).Let S = fA1; A2; : : :Akg be a set of subgoals and b be a binding. We denote Sols(S) tobe the solution set of S, and de ne Sols(;) = f;g. We denote Aijb to be Ai whose variablesare bound according to b (Aij; = Ai). Finally, we denote Cost(Aijb) to be the amount ofresources needed for proving Aijb. Cost(Aijb) should re ect the time complexity of provingAi under binding b. For example, the number of uni cation steps is a natural measure ofcomplexity for logic programs (Itai & Makowsky, 1987).To obtain the cost of nding all the solutions of an ordered sequence of subgoals~S = hA1; A2; A3; : : :Ani; (1)43\nLedeniov & Markovitchwe note that the proof-tree of A1 is traversed only once, the tree of A2 is traversed oncefor each solution generated by A1, the tree of A3 { once for each solution of fA1; A2g, etc.Consequently, the total cost of proving Equation 1 isCost(hA1; : : :Ani) = Cost(A1) + Xb2Sols(fA1g)Cost(A2jb) + : : :+ Xb2Sols(fA1;:::An 1g)Cost(Anjb) == nXi=1 Xb2Sols(fA1;:::Ai 1g)Cost(Aijb): (2)To compute Equation 2 one must know the cost and the solution set for each subgoalunder each binding. To reduce the amount of information needed, we derive an equivalentformula, which uses average cost and average number of solutions.De nition: Let B be a set of subgoals, A a subgoal. De ne cost(A)jB to be the averagecost of A over all solutions of B and nsols(A)jB to be its average number of solutions overall solutions of B:cost(A)jB = 8><>>: Cost(A); B = ;Pb2Sols(B)Cost(Ajb)jSols(B)j ; B 6= ;; Sols(B) 6= ;unde ned; B 6= ;; Sols(B) = ;nsols(A)jB = 8>><>: jSols(fAg)j; B = ;Pb2Sols(B) jSols(fAjbg)jjSols(B)j ; B 6= ;; Sols(B) 6= ;unde ned; B 6= ;; Sols(B) = ;From the rst de nition, it follows that:Xb2Sols(fA1;:::Ai 1g)Cost(Aijb) = jSols(fA1; : : :Ai 1g)j cost(Ai)jfA1;:::Ai 1g: (3)If we apply the second de nition recursively, we obtainjSols(fA1; : : :Aig)j = Xb2Sols(fA1;:::Ai 1g)jSols(fAijbg)j= jSols(fA1; : : :Ai 1g)j nsols(Ai)jfA1;:::Ai 1g= : : : = iYj=1 nsols(Aj)jfA1:::Aj 1g: (4)Note that we de ned Sols(;) = f;g; thus, these equations hold also for i = 1. Incorporationof Equations 3 and 4 into Equation 2 yieldsCost(hA1; A2; : : :Ani) = nXi=1 240@i 1Yj=1 nsols(Aj)jfA1:::Aj 1g1A cost(Ai)jfA1:::Ai 1g35 : (5)44\nThe Divide-and-Conquer Subgoal-Ordering AlgorithmFor each subgoal Ai, its average cost is multiplied by the total number of solutions ofall the preceding subgoals. We can de ne average cost and number of solutions for everycontinuous sub-sequence of Equation 1: 8k1; k2; 1 k1 k2 n,cost(hAk1 ; : : :Ak2i)jfA1;:::Ak1 1g = cost(hA1; : : :Ak2i)j; cost(hA1; : : :Ak1 1i)j;nsols(hA1; : : :Ak1 1i)j; (6)= k2Xi=k1 240@ i 1Yj=k1 nsols(Aj)jfA1;:::Aj 1g1A cost(Ai)jfA1;:::Ai 1g35nsols(hAk1 ; : : :Ak2i)jfA1;:::Ak1 1g = nsols(hA1; : : :Ak2i)j;nsols(hA1; : : :Ak1 1i)j; = k2Yi=k1 nsols(Ai)jfA1;:::Ai 1g (7)The values of cost(Ai) and nsols(Ai) depend on the position of Ai in the ordered se-quence. For example, assume that we want to nd Abraham's sons, using the domain ofExample 1. The unordered conjunctive goal is fmale(Y),parent(abraham,Y)g. Let therebe N males in the database (two of them, Isaac and Ishmael, are Abraham's sons):nsols(male(Y))j; = N nsols(parent(abraham,Y))j; = 2nsols(male(Y))jfparent(abraham,Y)g = 1 nsols(parent(abraham,Y))jfmale(Y)g = 2=NNote that nsols(hmale(Y),parent(abraham,Y)i) = 2 = nsols(hparent(abraham,Y),male(Y)i),exactly as Theorem 1 predicts.Having de ned the cost of a sequence of subgoals, we can now de ne the objective ofour ordering algorithms:De nition: Let S be a set of subgoals. De ne (S) to be set of all permutations ofS. ~OS 2 (S) is a minimal ordering of S (denoted Min( ~OS ;S)), if its cost according toEquation 5 is minimal over all possible permutations of S:Min( ~OS ;S) () 8O0S 2 (S) : Cost( ~OS) Cost(O0S):The total execution time is the sum of the time which is spent on ordering, and theinference time spent by the interpreter on the ordered sequence. In this paper we focusupon developing algorithms for minimizing the inference time. Elsewhere (Ledeniov &Markovitch, 1998a, 1998b) we present algorithms that attempt to reduce the total executiontime.The values of cost and number of solutions can be obtained in various ways: by exactcomputation, by estimation and bounds, and by learning. Let us assume at the momentthat there exists a mechanism that returns the average cost and number of solutions of asubgoal in time . In Section 5 we show how this control knowledge can be obtained byinductive learning.3.2 Ordering of Independent Sets of SubgoalsThe general subgoal ordering problem is NP-hard (Ullman & Vardi, 1988). However, thereis a special case where ordering can be performed e ciently: if all the subgoals in the45\nLedeniov & Markovitchgiven set are independent, i.e. do not share free variables. This section begins with thede nition of subgoal dependence and related concepts. We then show an ordering algorithmfor independent sets and prove its correctness.3.2.1 Dependence of SubgoalsDe nition: Let S and B be sets of subgoals (B is called the binding set of S). A pair ofsubgoals in S is directly dependent under B, if they share a free variable not bound by asubgoal of B.A pair of subgoals is indirectly dependent with respect to S and B if there exists a thirdsubgoal in S which is directly dependent on one of them under B, and dependent (directlyor indirectly) on the other one under B. A pair of subgoals of S is independent under B ifit is not dependent under B (either directly or indirectly). A subgoal is independent of Sunder B if it is independent of all members of S under B.Two subsets S1 S and S2 S are mutually independent under the binding set B ifevery pair of subgoals (A1; A2), such that A1 2 S1 and A2 2 S2, is independent under B.The entire set S is called independent under the binding set B if all its subgoal pairsare independent under B, and is called dependent otherwise. A dependent set of subgoalsis called indivisible if all its subgoal pairs are dependent under B, and divisible otherwise.A divisibility partition of S under B, DPart(S;B), is a partition of S into subsets thatare mutually independent and indivisible under B, except at most one subset which containsall the subgoals independent of S under B. It is easy to show that DPart(S;B) is unique.For example, let S0 = fa; b(X); c(Y ); d(X; Y ); e(Z); f(Z;V ); h(W )g. With respect toS0 and an empty binding set, the pair fb(X); d(X;Y )g is directly dependent, fb(X); c(Y )gis indirectly dependent and fb(X); e(Z)g is independent. If we represent a set of subgoalsas a graph, where subgoals are vertices and directly dependent subgoals are connected byedges, then dependence is equivalent to connectivity and indivisible subsets are equivalentto connected components of size greater than 1. The divisibility partition is the partitionof a graph into connected components, with all the \\lonely\" vertices collected together, ina special component. Figure 3 shows an example of such a graph for the set S0 and foran empty binding set. The whole set is divisible into four mutually independent subsets.The subsets fe(Z); f(Z; V )g and fb(X); c(Y ); d(X; Y )g are indivisible. Elements of thedivisibility partition DPart(S0; ;) are shown by dotted lines.If a subgoal is independent of the set, then its average cost and number of solutions donot depend on its position within the ordered sequence:cost(A)jB = Pb2Sols(B) Cost(Ajb)jSols(B)j = jSols(B)j Cost(A)jSols(B)j = Cost(A);nsols(A)jB = Pb2Sols(B) jSols(fAjbg)jjSols(B)j = jSols(B)j jSols(fAg)jjSols(B)j = jSols(fAg)j:In this case we can omit the binding information and write cost(Ai) instead of cost(Ai)jfA1:::Ai 1g,and nsols(Ai) instead of nsols(Ai)jfA1:::Ai 1g.In practice, program rule bodies rarely feature independent sets of literals. An exampleis the following clause, which states that children like candy:46\nThe Divide-and-Conquer Subgoal-Ordering Algorithmfa, b(X), c(Y), d(X,Y), e(Z), f(Z,V), h(W)g) e(Z)\nh(W) f(Z,V)\nc(Y)a b(X)\nd(X,Y)Figure 3: An example of a graph representing a set of subgoals. Directly dependent subgoalsare connected by edges. Independent subgoals and indivisible subsets are equivalent toconnected components (surrounded by dashed lines). The divisibility partition (underthe empty binding set) is shown by dotted lines.likes(X,Y) child(X), candy(Y).More often, independent rule bodies appear not because they are written as such in theprogram text, but because some variables are bound in (initially dependent) rule bodies, asa result of clause head uni cation. For example, if the rulefather(X,Y) male(X), parent(X,Y).is used to reduce father(abraham,W), then X is bound to abraham, and the rule bodybecomes independent. Rule bodies often become independent after substitutions are per-formed in the course of the inference process.3.2.2 Algorithm for Ordering Independent Sets by SortingLet ~S be an ordered sub-sequence of subgoals, B a set of subgoals. We denotecn(~S)jB = nsols(~S)jB 1cost(~S)jB :The name \\cn\" re ects the participation of cost and nsols in the de nition. When the sub-sequence ~S is independent of other subgoals, the binding information (jB) can be omitted.Together, the average cost, average number of solutions, and cn value of a subgoal will becalled the control values of this subgoal.For independent sets, there exists an e cient ordering algorithm, listed in Figure 4. Thecomplexity of this algorithm is O(n( + log n)): O(n ) to obtain the control values of nsubgoals, and O(n logn) to perform the sorting (Knuth, 1973). To enable the division, wemust de ne the cost so that cost(Ai) is always positive. If we de ne the cost as the numberof uni cations performed, then always cost(Ai) 1, under a reasonable assumption thatpredicates of all rule body subgoals are de ned in the program. (In this case, at least oneuni cation is performed for each subgoal). Similar algorithms were proposed by Simon andKadane (1975) and Natarajan (1987).Example 2 Let the set of independent subgoals be fp; q; rg, with the following control val-ues: 47\nLedeniov & MarkovitchAlgorithm 1Let S = fA1; A2; : : :Ang be a set of subgoals.Sort S using cn(Ai) = nsols(Ai) 1cost(Ai) as the key for Ai, and return the result.Figure 4: The algorithm for ordering subgoals by sorting.p q rcost 10 20 5nsols 1 5 0:1cn 0 0:2 0:18We compute the costs of all possible orderings, using Equation 5:Cost(hp; q; ri) = 10 + 1 20 + 1 5 5 = 55Cost(hp; r; qi) = 10 + 1 5 + 1 0:1 20 = 17Cost(hq; p; ri) = 20 + 5 10 + 5 1 5 = 95Cost(hq; r; pi) = 20 + 5 5 + 5 0:1 10 = 50Cost(hr; p; qi) = 5 + 0:1 10 + 0:1 1 20 = 8Cost(hr; q; pi) = 5 + 0:1 20 + 0:1 5 10 = 12The minimal ordering is hr; p; qi, and this is exactly the ordering which is found muchmore quickly by Algorithm 1 for the set fp; q; rg: r has the smallest cn value, 0:18, thengoes p with cn(p) = 0, and nally q with cn(q) = 0:2.Note that the sorting algorithm re ects a well-known principle: The best implementa-tions of generate-and-test programs are obtained with the tests placed as early as possiblein the rule body and the generations as late as possible (Naish, 1985a). Of course, thecheap tests should come rst, while the expensive ones should come last. If one looks atthe cn measure, one quickly realizes that tests should be put in front (because nsols < 1,so cn < 0), while generator subgoals should move towards the end (nsols > 1, so cn > 0).The weakness of the \\test- rst\" principle is in the fact that not every subgoal can be easilytagged as a test or a generator. If one subgoal has nsols < 1 and another one has nsols > 1,then their order is obvious even without looking at the costs (because their cn values havedi erent signs). But if both subgoals have nsols < 1, or both have nsols > 1, then thedecision is not so simple. Sorting by cn can correctly handle all the possible cases.3.2.3 Correctness Proof of the Sorting Algorithm for Independent SetsWe saw that Algorithm 1 found a minimal ordering in Example 2. We are now going toprove that Algorithm 1 always nds a minimal ordering for independent sets. First weshow an important lemma which will also be used in further discussion. This lemma states48\nThe Divide-and-Conquer Subgoal-Ordering Algorithmthat substitution of a sub-sequence by its cheaper permutation makes the entire sequencecheaper.Lemma 1Let ~S = ~Ak ~Bk ~C, ~S0 = ~Ak ~B0k ~C, where ~B and ~B0 are permutations of one another, and ~Aeither is empty or has nsols( ~A) > 0. ThenCost(~S) < Cost(~S 0) () cost( ~B)j ~A < cost( ~B0)j ~A;Cost(~S) = Cost(~S 0) () cost( ~B)j ~A = cost( ~B0)j ~A:Proof: If ~A and ~C are not empty,Cost(~S) Cost(~S0) = Cost( ~Ak ~Bk~C) Cost( ~Ak ~B0k~C) =(5)= cost( ~A)j; + nsols( ~A)j; cost( ~B)j ~A + nsols( ~Ak ~B)j; cost(~C)j ~Ak~B cost( ~A)j; + nsols( ~A)j; cost( ~B0)j ~A + nsols( ~Ak ~B0)j; cost(~C)j ~Ak~B0 :By Theorem 1, ~B and ~B0 produce the same solution sets. Hence, the third terms in theparentheses above are equal, andCost(~S) Cost(~S 0) = nsols( ~A)j; cost( ~B)j ~A cost( ~B0)j ~A :Since nsols( ~A) > 0, the sign of Cost(~S) Cost(~S 0) coincides with the sign of cost( ~B)j ~A cost( ~B0)j ~A.If ~A or ~C is empty, the proof is similar. 2De nition: Let ~S = ~Ak ~B1k ~Ck ~B2k ~D be an ordered sequence of subgoals ( ~A, ~C and ~D maybe empty sequences). With respect to ~S, the pair h ~B1; ~B2i is cn-ordered, if cn( ~B1)j ~A cn( ~B2)j ~A[ ~B1[~C cn-inverted, if cn( ~B1)j ~A > cn( ~B2)j ~A[~B1[~CWe now show that two adjacentmutually independent sequences of subgoals in a minimalordering must be cn-ordered.Lemma 2Let ~S = ~Ak ~B1k ~B2k ~C, ~S0 = ~Ak ~B2k ~B1k ~C, where ~B1, ~B2 are mutually independent under ~A.Let ~A either be empty or have nsols( ~A) > 0. ThenCost(~S) < Cost(~S0) () cn( ~B1)j ~A < cn( ~B2)j ~A;Cost(~S) = Cost(~S0) () cn( ~B1)j ~A = cn( ~B2)j ~A:49\nLedeniov & MarkovitchProof:Cost(~S) < Cost(~S 0) Lemma 1() cost( ~B1k ~B2)j ~A < cost( ~B2k ~B1)j ~A() cost( ~B1)j ~A+ nsols( ~B1)j ~A cost( ~B2)j ~A[~B1 <cost( ~B2)j ~A+ nsols( ~B2)j ~A cost( ~B1)j ~A[~B2indep.f ~B1 ; ~B2g() cost( ~B1)j ~A+ nsols( ~B1)j ~A cost( ~B2)j ~A <cost( ~B2)j ~A+ nsols( ~B2)j ~A cost( ~B1)j ~A() nsols( ~B1)j ~A cost( ~B2)j ~A cost( ~B2)j ~A <nsols( ~B2)j ~A cost( ~B1)j ~A cost( ~B1)j ~Acost( ~Bi)j ~A>0() nsols( ~B1)j ~A 1cost( ~B1)j ~A < nsols( ~B2)j ~A 1cost( ~B2)j ~A() cn( ~B1)j ~A < cn( ~B2)j ~ACost(~S) = Cost(~S 0) () cn( ~B1)j ~A = cn( ~B2)j ~A | similar. 2In an independent set, all subgoal pairs are independent, in particular all adjacent pairs.So, in a minimal ordering of an independent set, all adjacent subgoal pairs must be cn-ordered; otherwise, the cost of the sequence can be reduced by a transposition of such pair.This conclusion is expressed in the following theorem.Theorem 2Let S be an independent set. Let ~S be an ordering of S. ~S is minimal i all the subgoals in~S are sorted in non-decreasing order by their cn values.Proof:1. Let ~S be a minimal ordering of S. If ~S contains a cn-inverted adjacent pair of subgoals,then transposition of this pair reduces the cost of ~S (Lemma 2), contradicting theminimality of ~S.2. Let ~S be some ordering of S, whose subgoals are sorted in non-decreasing order bycn. Let ~S0 be a minimal ordering of S. According to item 1, ~S0 is also sorted bycn. The only possible di erence between the two sequences is the internal orderingof sub-sequences with equal cn values. The ordering of each such sub-sequence in~S can be transformed to the ordering of its counterpart sub-sequence in ~S 0 by a nite number of transpositions of adjacent subgoals. By Lemma 2, transpositions ofadjacent independent subgoals with equal cn values cannot change the cost of thesequence. Therefore, Cost(~S) = Cost(~S 0), and ~S is a minimal ordering of S (since ~S 0is minimal). 2Corollary 1 Algorithm 1 nds a minimal ordering of an independent set of subgoals.50\nThe Divide-and-Conquer Subgoal-Ordering Algorithm3.3 Ordering of Dependent Sets of SubgoalsAlgorithm 1 does not guarantee nding a minimal ordering when the given set of subgoalsis dependent, as the following proposition shows.Proposition 1 When the given set of subgoals is dependent, then:1. The result of Algorithm 1 on it is not always de ned.2. Even when the result is de ned, it is not always a minimal ordering of the set.Proof: Both claims are proved by counter-examples.1. We show a set of subgoals that cannot be ordered by sorting.The program:a(c1). b(c1).a(c2). b(c2). Control values:a(X)j; a(X)jfb(X)g b(X)j; b(X)jfa(X)gcost 2 2 2 2nsols 2 1 2 1cn 12 0 12 0The set fa(X), b(X)g has two possible orderings, ha(X); b(X)i and hb(X); a(X)i.Both orderings have minimal cost, though neither one is sorted by cn: each orderinghas cn = 12 for its rst subgoal, and cn = 0 for the second one. Sorting by cn isimpossible here: when we transpose subgoals, their cn values are changed, and thepair becomes cn-inverted again.2. We show a set of subgoals that can be ordered by sorting, but its sorted ordering isnot minimal.The program:a(c1).a(c1).b(c1).b(c2) a(c1), a(c2). Control values:a(X)j; a(X)jfb(X)g b(X)j; b(X)jfa(X)gcost 2 2 8 2nsols 2 2 1 1cn 12 12 0 0Let the unordered set of subgoals be fa(X), b(X)g. Its ordering hb(X); a(X)i is sortedby cn, while ha(X); b(X)i is not. But ha(X); b(X)i is cheaper than hb(X); a(X)i:cost(ha(X); b(X)i) = 2 + 2 2 = 6 cost(hb(X); a(X)i) = 8 + 1 2 = 10 2Since sorting cannot guarantee minimal ordering for dependent subgoals, we now con-sider alternative ordering algorithms. The simplest algorithm checks every possible permu-tation of the set and returns the one with the minimal cost. The listing for this algorithmis shown in Figure 5.This algorithm runs in O( n!) time, where is the time it takes to compute the controlvalues for one subgoal, and n is the number of subgoals.The following observation can help to reduce the ordering time at the expense of addi-tional space. Ordered sequences can be constructed incrementally, by adding subgoals to51\nLedeniov & MarkovitchAlgorithm 2For each permutation of subgoals, nd its cost according to Equation 5.Store the currently cheapest permutation and update it when a cheaperone is found.Finally, return the cheapest permutation.Figure 5: The algorithm for subgoal ordering by an exhaustive check of all permutations.Algorithm 3Order(S)let P0 f;g; n jSjloop for k = 1 to nP 0k n ~P kB ~P 2 Pk 1; B 2 S n ~P oPk n ~P 2 P 0k 8 ~P 0 2 P 0k; hpermutation( ~P ; ~P 0)) Cost( ~P ) Cost( ~P 0)ioReturn the single member of Pn.Figure 6: The ordering algorithm which checks permutations of ordered pre xes.the right ends of ordered pre xes. By Lemma 1, if a cheaper permutation of a pre x exists,then this pre x cannot belong to a minimal ordering. The ordering algorithm can buildpre xes with increasing lengths, at each step adding to the right end of each pre x one ofthe subgoals that do not appear in it already, and for each subset keeping only its cheapestpermutation (if several permutations have equal cost, any one of them can be chosen). Thelisting for this algorithm is shown in Figure 6. At each step k, P 0k stores the set of pre xesfrom step k 1 extended by every subgoal not appearing there already. Pk P 0k, andin Pk each subset of subgoals is represented only by its cheapest permutation. Obviously,jPkj = (nk) (one pre x is kept for every subset of S of size k). For each pre x of length k 1,there are n (k 1) possible continuations of length k. The size of P 0k is as follows:jP 0kj = ( nk 1) (n (k 1)) = n!(n (k 1))!(k 1)! (n (k 1)) = kk n!(n k)!(k 1)! = k (nk):For each pre x, we compute its cost in time. The permutation test can be completedin O(n) time, by using, for example, a trie structure (Aho et al., 1987), where subgoals inpre xes are sorted lexicographically. Each step k takes O((n+ ) k (nk)) time, and the52\nThe Divide-and-Conquer Subgoal-Ordering Algorithmwhole algorithm runs innXk=1O((n+ ) k (nk )) = O(n (n+ ) nXk=1(nk)) = O(n (n+ ) 2n):If = O(n), this makes O(n2 2n).Smith and Genesereth (1985) and Natarajan (1987) point out that in a minimal orderedsequence every adjacent pair of subgoals must satisfy an adjacency restriction. The mostgeneral form of such a restriction in our notation says that two adjacent subgoals Ak andAk+1 in a minimal ordering hA1; A2 : : :Ani must satisfycost(hAk; Ak+1i)jfA1:::Ak 1g cost(hAk+1; Aki)jfA1:::Ak 1g: (8)The restriction follows immediately from Lemma 1. However, it can only help to nd alocally minimal ordering, i.e., an ordering that cannot be improved by transpositions ofadjacent subgoals. It is possible that all adjacent subgoal pairs satisfy Equation 8, but theordering is still not minimal. The following example illustrates this statement.Example 3 Let the unordered set be fp(X); q(X); r(X)g, where the predicates are de nedby the following program: p(c1): q(c1): r(c1):p(c2) f: q(c2): r(c1):q(c3) f:f fails after 50 uni cations.The ordering hp(X); q(X); r(X)i satis es the adjacency restriction (Equation 8):cost(p(X); q(X))j; = 55 cost(q(X); r(X))jp(X) = 5cost(q(X); p(X))j; = 107 cost(r(X); q(X))jp(X) = 8But it is not minimal: cost(hp(X); q(X); r(X)i) = 57cost(hr(X); p(X); q(X)i) = 12To nd a globally minimal ordering, it seems bene cial to combine the pre x algorithmwith the adjacency restriction: if a pre x does not satisfy the adjacency restriction, thenthere is a cheaper permutation of this pre x. The adjacency test can be performed fasterthan the permutation test, since it must only consider the two last subgoals of each pre- x. Nevertheless, the number of pre xes remaining after each step of Algorithm 3 is notreduced: if a pre x is rejected due to a violation of the adjacency restriction, it would havealso been rejected by the permutation test. Furthermore, if the adjacency restriction testdoes not fail, we should still perform the permutation test to avoid local minima (as inExample 3). The adjacency test succeeds in at least half of the cases: if we examine apre x hA1; : : :Ak ; B1; B2i, we shall also examine hA1; : : :Ak; B2; B1i, and the adjacency testcannot fail in both. Consequently, addition of the adjacency test can only halve the totalrunning time of the ordering algorithm, leaving it O(n2 2n) in the worst case.53\nLedeniov & MarkovitchSmith and Genesereth propose performing a best- rst search in the space of orderedpre xes, preferring pre xes with lower cost. The best- rst search can be combined withthe permutation test and the adjacency restriction. In addition, when the subgoals notin a pre x are independent under its binding, they can be sorted, and the sorted resultconcatenated to the pre x. By Lemma 1 and Corollary 1, this produces the cheapestcompletion of this pre x. When we perform completion, there is no need to perform theadjacency or permutation test: if a complete sequence is not minimal, it will never be chosenas the cheapest pre x; even if it is added to the list of pre xes, it will never be extractedtherefrom. The resulting algorithm is shown in Figure 7.Algorithm 4Order(S)let pre x-list ;, pre x ;, rest Sloop until empty(rest)if Independent(restjpre x)thenlet completion pre xkSort-by-cn(restjpre x)Insert-By-Cost(completion, pre x-list)elseloop for subgoal 2 restlet extension pre xksubgoalif Adjacency-Restriction-Test(extension)and Permutation-Test(extension)thenInsert-By-Cost(extension, pre x-list)pre x Cheapest(pre x-list)Remove-from-list(pre x, pre x-list)rest Snpre xReturn pre xFigure 7: An algorithm for subgoal ordering, incorporating the ideas of earlier researchers.The advantage of using best- rst search is that it avoids expanding pre xes whose costis higher than the cost of the minimal ordering. The policy used by the algorithm may,however, be suboptimal or even harmful. It often happens that the best completion of acheaper pre x is much more expensive than the best completion of a more expensive pre x.When the number of solutions is large, it is better to place subgoals with high costs closerto the beginning of the ordering to reduce the number of times that their cost is multiplied.For example, let the set be fa(X); b(X)g, with cost(a(X)) = 10, cost(b(X)) = nsols(a(X))= nsols(b(X)) = 2. Then a minimal ordering starts with the most expensive pre x:Cost(ha(X); b(X)i) = 10 + 2 2 = 1454\nThe Divide-and-Conquer Subgoal-Ordering AlgorithmCost(hb(X); a(X)i) = 2 + 2 10 = 22If there are many pre xes whose cost is higher than the cost of the minimal ordering, thenbest- rst search saves time. But if the number of such pre xes is small, using best- rstsearch can increase the total time, due to the need to perform insertion of a pre x into apriority queue, according to its cost.A sample run of Algorithm 4 will be shown later (in Section 4.7).4. The Divide-and-Conquer Subgoal Ordering AlgorithmAlgorithm 1 presented in Section 3.2 is very e cient, but is applicable only when the entireset of subgoals is independent. Algorithm 3 can handle a dependent set of subgoals but isvery ine cient. Algorithm 4, a combination of the two, can exploit independence of sub-goals for better e ciency. However, the obtained bene t is quite limited. In this section,we present the Divide-and-Conquer (dac) algorithm, which is able to exploit subgoal inde-pendence in a more elaborate way. The algorithm divides the set of subgoals into smallersubsets, orders these subsets recursively and combines the results.4.1 Divisibility Trees of Subgoal SetsIn this subsection we de ne a structure that represents all the ways of breaking a subgoalset into independent parts. Our algorithm will work by traversing this structure.De nition: Let S and B be sets of subgoals. The divisibility tree of S under B,DTree(S;B),is an AND-OR tree de ned as follows:DTree(S;B) = 8>><>>>: leaf(S;B) S is independent under BOR(S;B; fDTree(S n fBig;B [ fBig) j Bi 2 Sg) S is indivisible under BAND(S;B; fDTree(Si;B) j Si 2 DPart(S;B)g) S is divisible under BEach node N in the tree DTree(S0;B0) has an associated set of subgoals S(N) S0 andan associated binding set B(N) B0. For the root node, S(N) = S0, B(N) = B0. If thebinding set of the root is not speci ed explicitly, we assume it to be empty. For AND-nodesand OR-nodes we also de ne the sets of children. If S(N) is independent under B(N), then N is a leaf. If S(N) is indivisible under B(N), then N is an OR-node. Each subgoal Bi in S(N)de nes a child node whose set of subgoals is S(N) n fBig and the binding set isB(N) [ fBig. We call Bi the binder of the generated child. Note that the bindingset of every node in a divisibility tree is the union of the binders of all its indivisibleancestors and of the root's binding set. If S(N) is divisible under B(N), then N is an AND-node. Each subset Si in thedivisibility partition DPart(S(N);B(N)) de nes a child node with associated set ofsubgoals Si and binding set B(N). Divisibility partition was de ned in Section 3.2.1.55\nLedeniov & Markovitch OB(n2) = OB(n1) = OB(n3) =\nn4 n5 n6\nn2\nn1\nn3\nB(n5) = {d(X)}\nS(n2) = {a, b}\nS(n6) = {c(X), d(X)} B(n6) = {e(X)} S(n4) = {d(X), e(X)} B(n4) = {c(X)}\nS(n5) = {c(X), e(X)}\nS(n1) = {a, b, c(X), d(X), e(X)}\nS(n3) = {c(X), d(X), e(X)}\nFigure 8: The divisibility tree of fa; b; c(X); d(X); e(X)g under empty initial binding set. The setassociated with node n1 is divisible, and is represented by an AND-node. Its childrencorrespond to its divisibility subsets { one independent, S(n2) = fa; bg, and one indivis-ible, S(n3) = fc(X); d(X); e(X)g. n3 is an OR-node, whose children correspond to itsthree subgoals (each subgoal serves as a binder in one of the children). The sets S(n2),S(n4), S(n5) and S(n6) are independent under their respective binding sets, and theirnodes are leaves. Here we assumed that the subgoals c(X), d(X) and e(X) bind X as aresult of their proof.It is easy to show that the divisibility tree of a set of subgoals is unique up to the order ofchildren of each node. Figure 8 shows the divisibility tree of the set fa; b; c(X); d(X); e(X)gunder empty initial binding set. The associated sets and binding sets are written next tothe nodes.The following lemma expresses an important property of divisibility trees: subgoals ofeach node are independent of the rest of subgoals under the binding set of the node.Lemma 3 Let S0 be a set of subgoals. Then for every node N in DTree(S0; ;), for everysubgoal A 2 S(N), and for every subgoal Y 2 S0 n (S(N)[B(N)), A and Y are independentunder B(N).Proof: by induction on the depth of N in the divisibility tree.Inductive base: N is the root node, S0 n S(N) is empty, and no such Y exists.Inductive hypothesis: The lemma holds for M , the parent node of N .Inductive step: Let A 2 S(N), Y 2 S0 n (S(N) [ B(N)). A 2 S(M), and for M thelemma holds, thus either A and Y are independent under B(M), or Y 2 S(M).If A and Y are independent under B(M), then they are also independent under B(N),since B(M) B(N). Otherwise, A and Y are dependent under B(M), and Y 2 S(M).56\nThe Divide-and-Conquer Subgoal-Ordering Algorithm If M is an AND-node, and A and Y are dependent under B(M), then A andY belong to the same element of DPart(S(M);B(M)), and Y 2 S(N) { acontradiction. If M is an OR-node and Y 2 S(M) n S(N), then Y must be the binder of N .But then B(N) = B(M) [ fY g and Y 2 B(N) { a contradiction again. 2The lemma relates to subgoal independence inside divisibility trees. We shall sometimesneed to argue about independence inside ordered sequences of subgoals. The followingcorollary provides the necessary connecting link.Corollary 2 Let S0 be a set of subgoals, N be a node in the divisibility tree of S0, ~S anordering of S0, ~S = ~S1k~S2, where B(N) ~S1 and S(N) ~S2. Then S(N) is mutuallyindependent of ~S2 n S(N) under ~S1.Proof: Let A 2 S(N), Y 2 ~S2 n S(N). A and Y are independent under B(N), by thepreceding lemma. Since B(N) ~S1, A and Y are independent under ~S1. Every subgoal ofS(N) is independent of every subgoal of ~S2 nS(N) under ~S1; therefore, S(N) and ~S2 nS(N)are mutually independent under ~S1. 24.2 Valid Orderings in Divisibility TreesThe aim of our ordering algorithm is to nd a minimal ordering of a given set of subgoals.We construct orderings following a divide-and-conquer policy: larger sets are split intosmaller ones, and orderings of the smaller sets are combined to produce an ordering of thelarger set. To implement this policy, we perform a post-order traversal of the divisibilitytree corresponding to the given set of subgoals under an empty initial binding set. Whenorderings of child nodes are combined to produce an ordering of the parent node, the innerorder of their subgoals is not changed: smaller orderings are consistentwith larger orderings.De nition: Let S and G S be sets of subgoals. An ordering ~OG of G and an ordering~OS of S are consistent (denoted Cons( ~OG; ~OS)), if the order of subgoals of G in ~OG and in~OS is the same.The divide-and-conquer process described above seems analogous to Merge Sort (Knuth,1973). There, the set of numbers is split into two (or more) subsets, each subset is inde-pendently ordered to a sequence consistent with the global order, and these sequences aremerged. Is it possible to use a similar method for subgoal ordering? Assume that a setof subgoals is partitioned into two mutually independent subsets, A and B. Can we buildan algorithm that, given A, produces its ordering consistent with a minimal ordering ofA [ B, independently of B? Unfortunately, the answer is negative. An ordering of A maybe consistent with a minimal ordering of A [ B1 but at the same time not be consistentwith a minimal ordering of A [ B2 for some B1 6= B2.For example, let A = fa1(X); a2(X)g, B1 = fbg, B2 = fdg and the control values be asspeci ed in Figure 9. The single minimal ordering of A[B1 is ha2(X); b; a1(X)i, while thesingle minimal ordering of A[B2 is hd; a1(X); a2(X)i. There is no ordering of A consistentwith both these minimal global orderings. 57\nLedeniov & MarkovitchThe program:a1(c1). b a1(X).a1(c1). b d.a2(c1). d.a2(c1).a2(c2) a1(c2). The control values:a1(X)j; a1(X)jfa2(X)g a2(X)j; a2(X)jfa1(X)g b dcost 2 2 5 3 5 1nsols 2 2 2 2 3 1Cost(b; a1(X); a2(X)) = 5 + 3 2 + 3 2 3 = 29Cost(b; a2(X); a1(X)) = 5 + 3 5 + 3 2 2 = 32Cost(a1(X); b; a2(X)) = 2 + 2 5 + 2 3 3 = 30Cost(a1(X); a2(X); b) = 2 + 2 3 + 2 2 5 = 28Cost(a2(X); b; a1(X)) = 5 + 2 5 + 2 3 2 = 27Cost(a2(X); a1(X); b) = 5 + 2 2 + 2 2 5 = 29 Cost(d; a1(X); a2(X)) = 1 + 1 2 + 1 2 3 = 9Cost(d; a2(X); a1(X)) = 1 + 1 5 + 1 2 2 = 10Cost(a1(X); d; a2(X)) = 2 + 2 1 + 2 1 3 = 10Cost(a1(X); a2(X); d) = 2 + 2 3 + 2 2 1 = 12Cost(a2(X); d; a1(X)) = 5 + 2 1 + 2 1 2 = 11Cost(a2(X); a1(X); d) = 5 + 2 2 + 2 2 1 = 13Figure 9: We show a small program and the control values it de nes. Then we compute costs of allpermutations of the sets fb; a1(X); a2(X)g and fd; a1(X); a2(X)g. Di erent orderings offa1(X); a2(X)g are consistent with minimal orderings of these sets.Since, unlike the case of Merge Sort, we cannot always identify a single ordering of thesubset consistent with a minimal ordering of the whole set, our algorithm will deal withsets of candidate orderings. Our requirement from such a set is that it contain at leastone local ordering consistent with a global minimal ordering, if such a local ordering exists(\\local\" ordering is an ordering of the set of the node, \\global\" ordering is an ordering ofthe set of the root). Such a set will be called valid. The following de nition de nes validsets formally, together with several other concepts.De nition: Let S0 be a set of subgoals and N be a node in the divisibility tree of S0.Recall that (S) denotes the set of all permutations of S.1. ~OS 2 (S0) is binder-consistent with ~ON 2 (S(N)) (denoted BCN ( ~ON ; ~OS)), if theyare consistent, and all subgoals of B(N) appear in ~OS before all subgoals of ~ON :BCN ( ~ON ; ~OS) () 9 ~OB 2 (B(N)) : Cons( ~OBk ~ON ; ~OS):~OS 2 (S0) is binder-consistent with the node N (denoted BCN ( ~OS)), if it is binder-consistent with some ordering of S(N):BCN ( ~OS) () 9 ~ON 2 (S(N)) : BCN ( ~ON ; ~OS):2. ~ON 2 (S(N)) is min-consistent with ~OS 2 (S0) (denotedMCN;S0( ~ON ; ~OS)), if theyare binder-consistent, and ~OS is minimal:MCN;S0( ~ON ; ~OS) () BCN ( ~ON ; ~OS) ^Min( ~OS ;S0):~ON 2 (S(N)) is min-consistent (denoted MCN;S0( ~ON)), if it is min-consistent withsome ordering of S0:MCN;S0( ~ON) () 9 ~OS 2 (S0) :MCN;S0( ~ON ; ~OS):58\nThe Divide-and-Conquer Subgoal-Ordering Algorithm3. An ordering ~ON 2 (S(N)) is MC-contradicting, if it is not min-consistent:MCCN;S0( ~ON) () :MCN;S0( ~ON):4. Two orderings ~O1; ~O2 2 (S(N)) are MC-equivalent, if one of them is min-consistenti the other one is:MCEN;S0( ~O1; ~O2) () [MCN;S0( ~O1) () MCN;S0( ~O2)]:5. A set of orderings CN (S(N)) is valid, if CN contains a min-consistent ordering(when at least one min-consistent ordering of S(N) exists):V alidN;S0(CN) () [9 ~O0N 2 (S(N)) :MCN;S0( ~O0N)]! [9 ~ON 2 CN :MCN;S0( ~ON)]:An important property of valid sets is that a valid set of orderings of the root ofDTree(S0; ;) must contain a minimal ordering of S0. Indeed, in the root S(N) = S0,and consistency becomes identity. Also, B(N) = ;, so that binder-consistency becomesconsistency, and min-consistency becomes minimality. Since there always exists a minimalordering of S0, a valid set of orderings of the root must contain a minimal ordering of S0.4.3 The Outline of the Divide-and-Conquer AlgorithmWe propose an algorithm that is based on producing valid sets of orderings. Each node ina divisibility tree produces a valid set for its associated set of subgoals, and passes it toits parent node. After the valid set of the root node is found, we compare costs of all itsmembers, and return the cheapest one.The set of orderings produced by the algorithm for a node N is called a candidate setof N . Its members are called candidate orderings of N , or simply candidates. To nda candidate set of N , we rst consider the set of all possible orderings of S(N) that areconsistent with candidates of N 's children. This set is called the consistency set of N .Given the candidate sets of N 's children, the consistency set of N is de ned uniquely. Acandidate set of N is usually not unique.De nition: Let N be a node in a divisibility tree of S0. The consistency set of N , denotedConsSet(N), and the candidate set of N , denoted CandSet(N), are de ned recursively: If N is a leaf, its consistency set contains all permutations of S(N):ConsSet(N) = (S(N)): If N is an AND-node, and its child nodes are N1; N2; : : :Nk, we de ne the consistencyset of N as the set of all possible orderings of S(N) consistent with candidates ofN1; N2; : : :Nk:ConsSet(N) = n~ON 2 (S(N)) 8i (1 i k); 9 ~Oi 2 CandSet(Ni) : Cons( ~Oi; ~ON)o :59\nLedeniov & Markovitch If N is an OR-node, and its child node corresponding to every binder A 2 S(N) isNA, then the consistency set of N is obtained by adding binders as the rst elementsto the candidates of the children:ConsSet(N) = nAk ~OA A 2 S(N); ~OA 2 CandSet(NA)o : A candidate set of N is any set of orderings produced by removing MC-contradictingand MC-equivalent orderings out of the consistency set of N , while keeping at leastone representative for each group of MC-equivalent orderings:CandSet(N) ConsSet(N);~ON 2 (ConsSet(N) nCandSet(N)) ) MCCN;S0( ~ON) _h9 ~O0N 2 CandSet(N) : MCEN;S0( ~ON ; ~O0N)i :(In other words, if some ordering is rejected, it is either MC-contradicting, or MC-equivalent to some other ordering, which is not rejected.)There are two kinds of orderings which can be removed from ConsSet(N) while re-taining its validity: MC-contradicting and MC-equivalent orderings. Removal of an MC-contradicting ordering cannot change the number of min-consistent orderings in the set; ifwe remove an MC-equivalent ordering, then even if it is min-consistent, some other min-consistent ordering is retained in the set. If there exists a min-consistent ordering of the setof the node, then its candidate set must contain a min-consistent ordering, and thereforethe candidate set is valid.Note that when our algorithm treats an OR-node, the binder of each child is alwaysplaced as the rst subgoal of the produced ordering of this node. On higher levels the innerorder of subgoals in the ordering does not change (consistency is preserved). Therefore,our algorithm can only produce binder-consistent orderings. This explains the choice ofthe names \\binder\" and \\binding set\": the subgoals of B(N) bind some common variablesof S(N), since they stand to the left of them in any global ordering that our algorithmproduces. In particular, if S(N) is independent under B(N), then the subgoals of B(N)bind all the shared free variables of S(N).To implement the DPart function, we can use the Union-Find data structure (Cormen,Leiserson, & Rivest, 1991, Chapter 22), where subgoals are elements, and indivisible setsare groups. In the beginning, every subgoal constitutes a group by itself. Whenever wediscover that two subgoals share a free variable not bound by subgoals of the binding set,we unite their groups into one. To complete the procedure, we need a way to determinewhich variables are bound by the given binding set. Section 7.1 contains a discussion ofthis problem and proposes some practical solutions. Finally, we collect all the indivisiblesubgoals into a separate group. These operations can be implemented in O(n (n; n)) amor-tized time, where (n; n) is the inverse Ackermann function, which can be considered O(1)for all values of n that can appear in realistic logic programs. Thus, the whole process of nding the divisibility partition of n subgoals can be performed in O(n) average time.The formal listing of the ordering algorithm discussed above is shown in Figure 10.The algorithm does not specify explicitly how candidate sets are created from consis-tency sets. To complete this algorithm, we must provide the three ltering procedures60\nThe Divide-and-Conquer Subgoal-Ordering AlgorithmAlgorithm 5Order(S0)RootCandSet CandidateSet(S0; ;)Return the cheapest member of RootCandSetCandidateSet(S;B)case (S under B)independent:let ConsSetN (S)let CandSetN ValidLeafFilter(ConsSetN )divisible:let fS1;S2; : : :Skg DPart(S;B)loop for i = 1 to klet Ci CandidateSet(Si;B)let ConsSetN n~ON 2 (S(N)) j 8i = 1 : : :k; 9 ~Oi 2 Ci : Cons( ~Oi; ~ON)olet CandSetN ValidANDFilter(ConsSetN ; fS1; : : :Skg; fC1; : : :Ckg)indivisible:loop for A 2 Slet C(A) CandidateSet(S n fAg;B [ fAg)let C 0(A) nAk ~OA j ~OA 2 C(A)olet ConsSetN SA2S C 0(A)let CandSetN ValidORFilter(ConsSetN )Return CandSetNFigure 10: The skeleton of the dac ordering algorithm. For each type of node in a divisibility tree,a consistency set is created and re ned through validity lters. The produced candidateset of the root is valid; hence, its cheapest member is a minimal ordering of the givenset.{ ValidLeafFilter, ValidANDFilter and ValidORFilter. Trivially, we can de ne themall as null lters that return the sets they receive unchanged. In this case the candidateset of every node will contain all the permutations of its subgoals, and will surely be valid.This will, however, greatly increase the ordering time. Our intention is to reduce the sizesof candidate sets as far as possible, while keeping them valid.In the following two subsections we discuss the ltering procedures. Section 4.4 dis-cusses detection of MC-contradicting orderings, and Section 4.5 discusses detection of MC-equivalent orderings. Finally, in Section 4.6 we present the complete ordering algorithm,incorporating the lters into the skeleton of Algorithm 5.61\nLedeniov & Markovitch4.4 Detection of MC-Contradicting OrderingsIn this subsection we show su cient conditions for an ordering to be MC-contradicting.Such orderings can be safely discarded, leaving the set of orderings valid, but reducing itssize. The subsection is divided into three parts, one for each type of node in a divisibilitytree.4.4.1 Detection of MC-Contradicting Orderings in LeavesThe following lemma shows that subgoals in a min-consistent ordering of a leaf node mustbe sorted by cn.Lemma 4Let S0 be a set of subgoals, N be a leaf in the divisibility tree of S0. Let ~ON be an ordering ofS(N). If the subgoals of ~ON are not sorted by cn under B(N), then ~ON is MC-contradicting.Proof: Let ~OS be any ordering of S0, binder-consistent with ~ON . We show that ~OS cannotbe a minimal ordering of S0, thus ~ON is not min-consistent.~ON is not sorted by cn, i.e., it contains an adjacent cn-inverted pair of subgoals hA1; A2i.(Recall that a pair is cn-inverted if the rst element has a larger cn value than the secondone { Section 3.2.3). Since ~OS is consistent with ~ON , we can write ~OS = ~XkA1k~Y kA2k~Z,where ~X, ~Y and ~Z are (possibly empty) sequences of subgoals. Since ~OS is binder-consistentwith ~ON , B(N) ~X .If ~Y is empty, then A1 and A2 are adjacent in ~OS . Since B(N) ~X , A1 and A2 areindependent under ~X . Therefore, the cost of the whole ordered sequence can be reducedby transposing A1 and A2, according to Lemma 2 (they are adjacent, independent andcn-inverted).If ~Y is not empty, then no subgoal of ~Y belongs to S(N), since otherwise it would appearin ~ON between A1 and A2. By Corollary 2, ~Y is mutually independent of both A1 and A2under ~X. If cn(~Y )j ~X < cn(A1)j ~X then, by Lemma 2, a transposition of ~Y with A1 produces anordering with lower cost. Otherwise, cn(~Y )j ~X cn(A1)j ~X . Since the pair hA1; A2i is cn-inverted, cn(A1)j ~X >cn(A2)j ~X . Hence, cn(~Y )j ~X > cn(A2)j ~X , and transposition of ~Y with A2 reduces thecost, by Lemma 2.In either case, there is a way to reduce the cost of ~OS . Therefore, ~OS cannot be minimal,and ~ON is MC-contradicting. 24.4.2 Detection of MC-Contradicting Orderings in AND-nodesEvery member of the consistency set of an AND-node is consistent with some combinationof candidates of its child nodes. If there are k child nodes, and for each child Ni the sizesof subgoal and candidate sets are jS(Ni)j = ni and jCandSet(Ni)j = ci, then the totalnumber of possible consistent orderings is c1 c2 : : : ck (n1+n2+:::+nk)!n1! n2 !::: nk! . Fortunately, mostof these orderings are MC-contradicting and can be discarded from the candidate set. The62\nThe Divide-and-Conquer Subgoal-Ordering Algorithmfollowing lemma states that it is forbidden to insert other subgoals between two cn-invertedsub-sequences. If such insertion takes place, the ordering is MC-contradicting and can besafely discarded.Lemma 5Let S0 be a set of subgoals, N a node in the divisibility tree of S0, and ~OS an ordering ofS0, binder-consistent with an ordering ~ON of S(N).If ~ON contains an adjacent cn-inverted pair of sub-sequences h ~A1; ~A2i, ~A1 and ~A2 appearin ~OS not mixed with other subgoals, and ~A1 and ~A2 are not adjacent in ~OS, then ~OS isnot minimal.Proof: Let ~OS be such an ordering of S0, binder-consistent with ~ON :~OS = ~Xk ~A1k~Y k ~A2k~Z;where ~Y is not empty. No subgoal of ~Y belongs to S(N), since otherwise it would standin ~ON between ~A1 and ~A2. ~OS is binder-consistent with ~ON ; therefore, B(N) ~X. ByCorollary 2, ~Y must be mutually independent of both ~A1 and ~A2 under ~X, and by Lemma 2a transposition of ~Y with either ~A1 or ~A2 reduces the cost { exactly as in the proof ofLemma 4. 2If a pair of adjacent subgoals hAi; Ai+1i is cn-inverted, then by the previous lemma anyattempt to insert subgoals inside it results in a non-minimal global ordering. Thereuponwe may join Ai and Ai+1 into a block Ai;i+1, which can further participate in a larger block.The formal recursive de nition of a block follows. For convenience, we consider separatesubgoals to be blocks of length 1.De nition:1. A sub-sequence ~A of an ordered sequence of subgoals is a block if it is either a singlesubgoal, or ~A = ~A1k ~A2, where h ~A1; ~A2i is a cn-inverted pair of blocks.2. A block is maximal (max-block) if it is not a sub-sequence of a larger block.3. Let N be a node in a divisibility tree, M be some descendant of N , ~ON 2 (S(N))and ~OM 2 (S(M)) be two consistent orderings of these nodes. A block ~A of ~OM isviolated in ~ON if there are two adjacent subgoals in ~A that are not adjacent in ~ON (inother words, alien subgoals are inserted between the subgoals of the block).4. Let N be a node, M be its descendant, ~ON 2 (S(N)) and ~OM 2 (S(M)) be twoconsistent orderings of these nodes. ~OM is called the projection of ~ON on M . Weshall usually speak about projection of an ordering on a child node.The concept of max-block is similar to themaximal indivisible block introduced by Simonand Kadane (1975) in the context of satis cing search. The following corollary presents theresult of Lemma 5 in a more convenient way.Corollary 3 Let N be a node in a divisibility tree, M be one of its children, ~ON be anordering of N , and ~OM be the projection of ~ON on M . If ~OM contains a block that isviolated in ~ON , then ~ON is MC-contradicting.63\nLedeniov & MarkovitchProof: Let ~A be the smallest block of ~OM violated in ~ON . According to the de nitionof a block, ~A = ~A1k ~A2, where ~A1 and ~A2 are not violated in ~ON , and the pair h ~A1; ~A2iis cn-inverted. Let ~OS be any ordering of the root node binder-consistent with ~ON . ~OSviolates ~A, since ~ON violates ~A. To show that ~ON is MC-contradicting, we must prove that~OS is not minimal. If ~A1 and ~A2 are not violated in ~OS , then they are not adjacent in ~OS , and ~OS is notminimal, by Lemma 5. Otherwise, ~A1 or ~A2 is violated in ~OS . Without loss of generality, let it be ~A1. Let ~A0be the smallest sub-block of ~A1 violated in ~OS . According to the de nition of a block,~A0 = ~A01k ~A02, where the pair h ~A01; ~A02i is cn-inverted, ~A1 and ~A2 are not violated andnot adjacent in ~OS . By Lemma 5, ~OS is not minimal. 2For example, if control values of subgoals are as shown in Figure 9, then ha1(X); a2(X)iis a block, since cn(a1(X))j; = 2 12 = 12 , cn(a2(X))jfa1(X)g = 2 13 = 13 . As one can see fromthe gure, insertion of b or d inside this block results in a non-minimal ordering.As was already noted above, the consistency set of an AND-node can be large. Inmany of its orderings, however, blocks of projections are violated, and we can discardthese orderings as MC-contradicting. In the remaining orderings, no block of a projectionis violated, and each such ordering can be represented as a sequence of max-blocks of theprojections. In each projection, its max-blocks stand in cn-ascending order (otherwise, thereis an adjacent cn-inverted pair of blocks, and a larger block can be formed, which contradictstheir maximality). As the following lemma states, in the parent AND-node these blocksmust also be ordered by their cn values; otherwise, the ordering is MC-contradicting.Lemma 6 If an ordering of an AND-node contains an adjacent cn-inverted pair of max-blocks of its projections on the children, then this ordering is MC-contradicting.Proof: If these blocks are violated in the binder-consistent global ordering, the globalordering is not minimal by Corollary 3. If the blocks are not violated, the proof is similarto the proof of Lemma 4. 2The two su cient conditions for detection of MC-contradicting orderings expressed inCorollary 3 and Lemma 6 allow us to reduce the size of the candidate set signi cantly.Assume, for example, that the set of our current node N is split into two mutually indepen-dent subsets whose candidates are ha1; a2i and hb1; b2i (one candidate for each child). Thereare six possible orderings of S(N), all shown in Figure 11. Assume that both ha1; a2i andhb1; b2i are blocks, and cn(ha1; a2i)jB(N) < cn(hb1; b2i)jB(N). Out of six consistent orderings,four (2{5) can be rejected due to block violation, and one of the remaining two (number 6)puts the blocks in the wrong order. So, only one ordering (number 1) can be left in the can-didate set of N . Even if neither ha1; a2i nor hb1; b2i are blocks, Lemma 6 dictates a uniqueinterleaving of their elements (max-blocks), assuming that cn(a1)jB(N) 6= cn(a2)jB(N)[fa1g6= cn(b1)jB(N) 6= cn(b2)jB(N)[fb1g.4.4.3 Detection of MC-Contradicting Orderings in OR-nodesThe following lemma states that if a block has a cheaper permutation, then the ordering isMC-contradicting (and can be discarded from the candidate set).64\nThe Divide-and-Conquer Subgoal-Ordering Algorithm a1 2a\nb1b2 22b1\na1 2a\nb1b2 b\n1 2a1 2a\nb1b2\na 2a\nb1bb1b2 a a1 2a a1 1. 2. 3. 4. 5. 6.Figure 11: The possible ways to combine ha1, a2i and hb1, b2iLemma 7 Let N be a node in the divisibility tree of ~S0, ~ON 2 (~S(N)). Let ~A be a leadingblock of ~ON : ~ON = ~Ak~R. If there is a permutation of ~A, ~A0, such that cost( ~A0)jB(N) <cost( ~A)jB(N), then ~ON is MC-contradicting.Proof: Let ~OS 2 (~S0) be binder-consistent with ~ON . If ~A is violated in ~OS , ~OS cannotbe minimal (Corollary 3). Otherwise, ~A occupies a continuous segment in ~OS , and itsreplacement by a cheaper permutation reduces the cost of the global ordering (Lemma 1).Thus, ~OS cannot be minimal. 2This check should be done only for leading blocks of OR-nodes: Every ordering of a leaf node that has not been rejected due to Lemma 4 must besorted by cn. Consequently, it contains no cn-inverted adjacent pair of subgoals, andno block of size 2 can be formed. Every ordering of an AND-node that has not been rejected due to Corollary 3 orLemma 6 must have its blocks unbroken and in cn-ascending order. Consequently,new blocks cannot be formed here either. In OR-nodes, new blocks can be formed when we add a binder as the rst element ofan ordering, if the cn value of the binder is greater than that of the subsequent block.All new blocks start from the binder, and we must perform the permutation test onlyon the leading max-block of an ordering.4.5 Detection of MC-Equivalent OrderingsIn the previous subsection we presented su cient conditions for detecting MC-contradictingorderings. In this subsection we specify su cient conditions for identifying MC-equivalentorderings. Recall that two orderings of a node are MC-equivalent if minimal consistencyof one implies minimal consistency of the other. Finding such su cient conditions willallow us to eliminate orderings without loss of validity of the candidate set. We startwith de ning a specialization of the MC-equivalence relation: blockwise equivalence. Wethen show that orderings whose max-blocks are sorted by cn are blockwise-equivalent, andtherefore MC-equivalent. 65\nLedeniov & MarkovitchDe nition: Let S0 be a set of subgoals and N be a node in the divisibility tree of S0. Let~O1 and ~O2 be two orderings of S(N) with an equal number of max-blocks. Let ~OS be anordering of S0, binder-consistent with ~O1, where blocks of ~O1 are not violated.~OS j~O2~O1 is the ordering obtained by replacing in ~OS every max-block of ~O1 with a max-block of ~O2, while preserving the order of max-blocks (the i-th max-block of ~O1 is replacedby the i-th max-block of ~O2).~O1 and ~O2 are blockwise-equivalent if the following condition holds: ~O1 is min-consistentwith ~OS i ~O2 is min-consistent with ~OS j~O2~O1 .As can be easily seen, if two orderings are blockwise-equivalent, then they are MC-equivalent. Now we show that a transposition of adjacent, mutually independent cn-equalmax-blocks in an ordering of a node produces a blockwise-equivalent ordering. The proofof the following lemma is found in Appendix A.Lemma 8Let S0 be a set of subgoals, N be a node in the divisibility tree of S0, ~ON = ~Qk ~A1k ~A2k~R bean ordering of S(N), where ~A1 and ~A2 are max-blocks, mutually independent and cn-equalunder the bindings of B(N)[ ~Q. Then ~ON is blockwise-equivalent with ~O0N = ~Qk ~A2k ~A1k~R.Corollary 4 All sorted by cn orderings of a leaf node are blockwise-equivalent.For example, if S(N) = fA;B;C;Dg, cn(A)jB(N) = 0:1, cn(B)jB(N) = cn(C)jB(N) = 0:3,cn(D)jB(N) = 0:5, then the orderings hA;B;C;Di and hA;C;B;Di are blockwise-equivalent,and we can remove from the candidate set any one of them (but not both).Corollary 5 All orderings of an AND-node, where blocks of projections are not violatedand adjacent max-blocks from di erent children projections are cn-ordered, are blockwise-equivalent.For example, if the candidates of the children are ~Ak ~B and ~Ck ~D, where ~A; ~B; ~C; ~D aremax-blocks, cn( ~A)jB(N) = 0:1, cn( ~B)jB(N)[~A = cn( ~C)jB(N) = 0:3 and cn( ~D)jB(N)[~C = 0:5,then the orderings ~Ak ~Bk ~Ck ~D and ~Ak ~Ck ~Bk ~D are blockwise-equivalent, and we can removefrom the candidate set any one of them (but not both).To prove both Corollaries 4 and 5, we note that in each case one of the mentionedorderings can be obtained from the other by a nite number of transpositions of adjacent,mutually independent and cn-equal max-blocks. According to Lemma 8, each such transpo-sition yields a blockwise-equivalent ordering. It is easy to show that blockwise equivalenceis transitive.The following corollary states that subgoals within a block can be permuted, providedthat the cost of the block is not changed.Corollary 6 All orderings of a node, identical up to cost-preserving permutations of sub-goals inside blocks, are blockwise-equivalent.The proof of the corollary follows immediately from Lemma 1. For example, if the setis fa(X); b(X)g, and the control values are as in the rst counter-example of Proposition 1,66\nThe Divide-and-Conquer Subgoal-Ordering AlgorithmNode Set MC-contradicting blockwise-equivalentLeaf Independent Subgoals not sorted by cn Subgoals sorted by cn| Lemma 4 | Corollary 4Contains violated blocks Max-blocks not violated,AND Divisible | Corollary 3 sorted by cnMax-blocks not sorted by cn | Corollary 5| Lemma 6The leading max-block has Cost-preserving permutationsOR Indivisible a cheaper permutation of blocks| Lemma 7 | Corollary 6Table 1: Summary of su cient conditions for detection of MC-contradicting and blockwise-equivalent orderings.i.e. cn(a(X)j;) = cn(b(X)j;) = 12 , and cn(a(X)jfb(X)g) = cn(b(X)jfa(X)g) = 0, then in bothpossible orderings, ha(X); b(X)i and hb(X); a(X)i, the two subgoals are united into a block,and these blocks have equal cost. In any global ordering containing the block ha(X); b(X)i,we can replace this block with hb(X); a(X)i without changing the total cost. Thereforeha(X); b(X)i is blockwise-equivalent to hb(X); a(X)i.The su cient condition expressed in Corollary 6 should be checked only in OR-nodes,since in leaves and AND-nodes no new blocks are created, as was argued in Section 4.4.3.4.6 The Revised Ordering AlgorithmIn the two preceding subsections we saw several su cient conditions of MC-contradictionand MC-equivalence, summarized in Table 1. These results permit us to close the gapsin Algorithm 5 by providing the necessary validity lters. Each lter tests the su cientconditions of MC-contradiction and MC-equivalence on every ordering in the consistencyset. If some of these su cient conditions hold, the ordering is rejected. The formal listingof these procedures is shown in Figure 12.While the generate-and-test approach described above served us well for methodologicalpurposes, it is obviously not practical because of its computational limitations. For example,for an independent set of size n, the algorithm creates n! orderings, then rejects n! 1and keeps only one. This process takes O(n! n) time and produces an ordering whichis sorted by cn. The same result could be obtained in just O(n logn) time, by a singlesorting. So, instead of uncontrolled creation of orderings and selective rejection, we want toperform a selective creation of orderings. In other words, we want to revise our algorithm todeal directly with candidate sets, instead of generating large consistency sets. The revisedalgorithm produces the candidate set of a node N as follows: If N is a leaf, the subgoals of S(N) are sorted by cn under the bindings of B(N), andthe produced ordering is the sole candidate of N . If N is an AND-node, then for each combination of its children's candidates a candi-date of N is created, where the max-blocks of the children's candidates are ordered67\nLedeniov & MarkovitchValidLeafFilter(ConsSetN )let CandSetN ;loop for ~ON 2 ConsSetNif ~ON is sorted by cnand there is no ~O0N 2 CandSetN which is sorted by cnthen CandSetN CandSetN [ f ~ONgReturn CandSetNValidANDFilter(ConsSetN ; fS1; : : :Skg; fC1; : : :Ckg)let CandSetN ;loop for ~ON 2 ConsSetNloop for i = 1 to klet ~Oi be the projection of ~ON on Siif 8i ~Oi 2 Ciand max-blocks of ~Oi-s are not violated in ~ON ,and max-blocks of ~Oi-s are ordered by cn in ~ON ,and there is no ~O0N 2 CandSetN consistent with all ~Oi-s,then CandSetN CandSetN [ f ~ONgReturn CandSetNValidORFilter(ConsSetN )let CandSetN ;loop for ~ON 2 ConsSetNif ~ON does not start with a block having a cheaper permutation,and there is no ~O0N 2 CandSetN , identical to ~ON up tocost-preserving permutations in blocks,then CandSetN CandSetN [ f ~ONgReturn CandSetNFigure 12: The three lter procedures that convert a consistency set into a candidate set. Togetherwith Algorithm 5, they form a complete ordering algorithm. The e ciency of thealgorithm can be improved, as we shall see in Algorithm 6.by cn. The candidate is produced by merging: moving in parallel on the candidatesof the children and extracting max-blocks that are minimal by cn. If N is an OR-node, then for each candidate of its child an ordering of N is createdby adding the binder to the left end of the child candidate. If this results in creationof a block that has a cheaper permutation, the ordering is rejected; otherwise, it isadded to the candidate set. It su ces to check only the leading max-block.68\nThe Divide-and-Conquer Subgoal-Ordering AlgorithmNote that the revised algorithm does not include a test for cost-preserving permutationsof blocks in di erent orderings (expressed in Corollary 6), because of the high expense ofsuch a test.The revised algorithm described above contains manipulations of blocks. For this pur-pose, we need an easy and e cient way to detect blocks in orderings. Since we do notpermit block violation (by Corollary 3), we can unite all the subgoals of a max-block intoone entity, and treat it as an ordinary subgoal. The procedure of joining subgoals intoblocks is called folding, and the resulting sequence of max-blocks { a folded sequence. Aftersubgoals are folded into a block, there is no need to unfold this block back to separatesubgoals: on upper levels of the tree, these subgoals will again be joined into a block, unlessthe block is violated. The unfolding operation is carried out only once before returning thecheapest ordering of the set (of the root node). The candidate sets of the nodes are nowde ned as sets of folded orderings.As was already stated, new blocks can only be created in the candidates of OR-nodes,when the binder is added as the rst element of the ordering, if the cn value of the binderis greater than the cn value of the rst max-block of the child projection. Therefore, in therevised algorithm we only build new blocks that start from the binder: the max-blocks inthe rest of the ordering remain from the child's candidate. First we try to make a blockout of the binder and the rst max-block of the child's candidate. If they are cn-ordered,we stop the folding. If they are cn-inverted, we unite them into a larger block, and tryto unite it with the second max-block of the child's candidate, and so on. The producedfolded ordering contains only maximal blocks: the rst block is maximal, since we could notexpand it further to the right, and the other blocks are maximal, since they were maximalin the child's candidate.Lemma 7 states that an ordering whose leading max-block has a cheaper permutationis MC-contradicting. One way to detect such a block is to exhaustively test all its permu-tations, computing and comparing their costs. This procedure is very expensive. Instead,in our revised algorithm we employ the adjacency restriction test (Equation 8). The test isapplied to every pair of adjacent subgoals of a block, and if some adjacent pair has a cheapertransposition, then the whole block has a cheaper permutation, by Lemma 1. Since blocksare created by concatenation of smaller blocks, it su ces to test the adjacency restrictiononly at the points where blocks are joined (for other adjacent pairs of subgoals, the testswere performed on the lower levels, when smaller blocks were formed). The adjacency re-striction test does not guarantee detection of all not-cheapest permutations (as was shownin Example 3), but it detects such blocks in many cases, and works in linear time.The nal version of the dac subgoal ordering algorithm is presented in Figure 13. Thecomplete correctness proof of Algorithm 6 is found in Appendix B.4.7 Sample Run and Comparison of Ordering AlgorithmsWe illustrate the work of the dac algorithm, using the subgoal set shown in Figure 8,S0 = fa; b; c(X); d(X); e(X)g. After proving c(X), d(X) or e(X), we can assume that X isbound. Let the control values for the subgoals be as shown in Table 2. The column c(free)contains control values for the subgoal c(X) when X is not yet bound by the precedingsubgoals (i.e., the binding set does not contain d(X) or e(X)). The column c(bound)69\nLedeniov & MarkovitchAlgorithm 6 : The Divide-and-Conquer AlgorithmOrder(S0)let RootCandSet CandidateSet(S0; ;)Return Unfold(the cheapest element of RootCandSet)CandidateSet(S;B)let fS1;S2; : : :Skg DPart(S;B)case k = 1, shared-vars(S1) = ; (S is independent under B):Return fSort-by-cn(S;B)g k = 1, shared-vars(S1) 6= ; (S is indivisible under B):loop for A 2 Slet C(A) CandidateSet(S n fAg;B [ fAg)let C 0(A) nFold(Ak ~OA;B) ~OA 2 C(A)oReturn SA2S C0(A) k > 1 (S is divisible under B):loop for i = 1 to klet Ci CandidateSet(Si ;B)Return nMerge(f ~O1; ~O2; : : : ~Okg;B) ~O1 2 C1; ~O2 2 C2; : : : ~Ok 2 CkoMerge(f ~O1; ~O2; : : : ~Okg;B)let min-cn-candidate ~Oi that minimizes cn( rst-max-block( ~Oi))jB, 1 i klet min-cn-block rst-max-block(min-cn-candidate)remove- rst-max-block(min-cn-candidate)Return min-cn-blockkMerge(f ~O1; ~O2; : : : ~Okg;B [min-cn-block)Fold(hA1; A2 : : :Aki;B)if k 1 or cn(A1)jB cn(A2)jBkA1then Return hA1; A2 : : :Akielseif the last subgoal of A1 and the rst subgoal of A2 satisfy the adjacency restrictionthenlet A0 block(A1; A2)Return Fold(hA0; A3 : : :Aki;B)else Return ;Figure 13: The revised version of the dac algorithm. The candidate sets are built selectively,without explicit creation of consistency sets. Candidate sets contain folded orderings,and unfolding is performed only on the returned global ordering. The code of theUnfold and Sort-by-cn procedures is not listed, due to its straightforwardness. Themerging procedure recursively extracts from the given folded orderings max-blocks thatare minimal by cn. The folding procedure joins two leading blocks into a larger one, aslong as they are cn-inverted. 70\nThe Divide-and-Conquer Subgoal-Ordering Algorithma b c(free) c(bound) d(free) d(bound) e(free) e(bound)cost 10 5 5 5 10 5 20 10nsols 0.8 2 2 0.5 4 1 0.4 0.1cn -0.02 0.2 0.2 -0.1 0.3 0 -0.03 -0.09Table 2: Control values for the sample runs of the ordering algorithms.contains cost values of c(X) when d(X) or e(X) have already bound X . For example,cost(c(X))jfa;d(X)g = cost(c(bound)) = 5. The dac algorithm traverses the divisibility treeof S0 as follows. (The names of the nodes are as in Figure 8.)1. The root of the divisibility tree, n1, has empty binding set B(n1) = ;, and theassociated subgoal set S(n1) = fa; b; c(X); d(X); e(X)g. The set S(n1) is parti-tioned into two subsets under B(n1): one independent { fa; bg, and one indivisible {fc(X); d(X); e(X)g. These two subsets correspond to two child nodes of the AND-node n1: n2 and n3, both with empty binding sets.2. S(n2) is independent under B(n2). Therefore, n2 is a leaf, and its sole candidateordering is obtained by sorting its subgoals by cn under B(n2). cn(a)j; = 0:02,cn(b)j; = 0:2, thus CandSet(n2) = fha; big.3. S(n3) is indivisible under B(n3). Therefore, n3 is an OR-node, and its three childrenare created { one for each subgoal of S(n3) serving as the binder. Binder c(X) yields the child node n4 with the associated set S(n4) = fd(X); e(X)gand the binding set B(n4) = fc(X)g. S(n4) is independent under B(n4). There-fore, n4 is a leaf, and its sole candidate is obtained by sorting its subgoals bycn: cn(d(X))jfc(X)g = 0; cn(e(X))jfc(X)g = 0:09;thus, the candidate of n4 is he(X); d(X)i. Binder d(X) yields the child node n5 with the associated set S(n5) = fc(X); e(X)gand the binding set B(n5) = fd(X)g. S(n5) is independent under B(n5), and itssorting by cn produces the candidate hc(X); e(X)i. Binder e(X) yields the child node n6 with the associated set S(n6) = fc(X); d(X)gand the binding set B(n6) = fe(X)g. S(n6) is independent under B(n6), and itssorting by cn produces the candidate hc(X); d(X)i.4. We now add each binder to its corresponding child's candidate and obtain three order-ings of the OR-node n3: hc(X); e(X); d(X)i, hd(X); c(X); e(X)i, he(X); c(X); d(X)i.5. We now perform folding of these orderings and check violations of the adjacencyrestriction, in order to determine whether a block has a cheaper permutation.71\nLedeniov & Markovitch First, we perform the folding of hc(X); e(X); d(X)i. The pair hc(X); e(X)i iscn-inverted: cn(c(X))j; = 0:2, cn(e(X))jfc(X)g = 0:09. We thus unite it into ablock. This block does not pass the adjacency restriction test (Equation 8):cost(hc(X); e(X)i)j; = 5 + 2 10 = 25;cost(he(X); c(X)i)j; = 20+ 0:4 5 = 22:Therefore, this ordering is MC-contradicting and can be discarded. We perform the folding of hd(X); c(X); e(X)i. cn(d(X))j; = 0:3, cn(c(X))jfd(X)g = 0:1, the pair is cn-inverted, and we unite it into a block. This block does notpass the adjacency restriction test:cost(hd(X); c(X)i)j; = 10 + 4 5 = 30;cost(hc(X); d(X)i)j; = 5 + 2 5 = 15:This ordering is rejected too, even before its folding is nished. If we continuethe folding process, we shall see that the subgoal e(X) must also be added to thisblock, since cn(hd(X); c(X)i)j; = 4 0:5 130 = 0:0333, and cn(e(X))jhd(X);c(X)i = 0:09. We perform the folding of he(X); c(X); d(X)i. cn(e(X))j; = 0:03, cn(c(X))jfe(X)g= 0:1, the pair is cn-inverted, and we form a block ec(X) = he(X); c(X)i, whichpasses the adjacency restriction test:cost(he(X); c(X)i)j; = 20 + 0:4 5 = 22;cost(hc(X); e(X)i)j; = 5 + 2 10 = 25:We compute the control values of the new block:cost(ec(X))j; = 20 + 0:4 5 = 22nsols(ec(X))j; = 0:4 0:5 = 0:2cn(ec(X))j; = 0:2 122 = 0:0363636cn(d(X))jfec(X)g = 0, thus the pair hec(X); d(X)i is cn-ordered, no more foldingis needed, and we add the folded candidate hec(X); d(X)i to the candidate setof n3.6. We now perform merging of the candidate set of n2, fha; big, with the candidate setof n3, fhec(X); d(X)ig. In the resulting sequence max-blocks must be sorted by cn.cn(a) = 0:02; cn(b) = 0:2; cn(ec(X))j; = 0:0363636; cn(d(X))jfec(X)g = 0:The merged ordering, hec(X); a; d(X); bi, is added to the candidate set of n1.7. We compare the costs of all candidates of n1, and output the cheapest one. In our case,there is only one candidate, hec(X); a; d(X); bi. The algorithm returns this candidateunfolded, he(X); c(X); a; d(X); bi. 72\nThe Divide-and-Conquer Subgoal-Ordering AlgorithmCheapest pre x Extension/Completion Cost; hai 10hbi 5hc(X)i 5hd(X)i 10he(X)i 20hbi hb; ai the adjacency restriction test failshb; c(X)i 5 + 2 5 = 15hb; d(X)i 5 + 2 10 = 25hb; e(X)i the adjacency restriction test failshc(X)i hc(X); e(X); a; d(X); bi 5 + 2(10 + 0:1(10 + 0:8(5 + 1 5))) = 28:6hai ha; bi 10 + 0:8 5 = 14ha; c(X)i 10 + 0:8 5 = 14ha; d(X)i 10 + 0:8 10 = 18ha; e(X)i the adjacency restriction test failshd(X)i hd(X); c(X); e(X); a; bi 10 + 4(5 + 0:5(10 + 0:1(10 + 0:8 5))) = 52:8ha; bi ha; b; c(X)i 14 + 0:8 2 5 = 22ha; b; d(X)i 14 + 0:8 2 10 = 30ha; b; e(X)i the adjacency restriction test failsha; c(X)i ha; c(X); e(X); d(X); bi 14 + 0:8 2(10 + 0:1(5 + 1 5)) = 31:6hb; c(X)i hb; c(X); e(X); a; d(X)i 15 + 2 2(10 + 0:1(10 + 0:8 5)) = 60:6ha; d(X)i ha; d(X); c(X); e(X); bi 18 + 0:8 4(5 + 0:5(10 + 0:1 5)) = 50:8he(X)i he(X); c(X); a; d(X); bi 20 + 0:4(5 + 0:5(10 + 0:8(5 + 1 5))) = 25:6ha; b; c(X)i ha; b; c(X); e(X); d(X)i 22 + 0:8 2 2(10 + 0:1 5) = 55:6hb; d(X)i hb; d(X); c(X); e(X); ai 25 + 2 4(5 + 0:5(10 + 0:1 10)) = 109he(X); c(X); a; d(X); bi complete orderingTable 3: A trace of a sample run of Algorithm 4 on the set of Figure 8. The left column shows thecheapest pre x extracted from the list on each step, the middle column { its extensionsor completions that are added to the list, and the right column { their associated costs.For comparison, we now show how the same task is performed by Algorithm 4. Thealgorithm maintains a list of pre xes, sorted by their cost values, and which initially containsan empty sequence. On each step the algorithm extracts from the list its cheapest element,and adds to the list the extensions or completions of this pre x. Extensions are created whenthe set of remaining subgoals is dependent, by appending each of the remaining subgoalsto the end of the pre x. Completions are created when the set of remaining subgoals isindependent, by sorting them and appending the entire resulting sequence to the pre x. Anextension is added to the list only when the adjacency restriction test succeeds on its twolast subgoals. To make the list operations faster, we can implement it as a heap structure(Cormen et al., 1991).The trace of Algorithm 4 on the set S0 is shown in Table 3. The left column shows thecheapest pre x extracted from the list on each step, the middle column { its extensions orcompletions that are added to the list, and the right column { their associated costs.It looks as if the dac algorithm orders the given set S0 more e ciently than Algorithm 4.We can compare several discrete measurements to show this. For example, Algorithm 673\nLedeniov & Markovitch p5 4X X8 X9 X10( , , , ) p1 1X X2 X3 4X( , , , ) p3 1X X5 X6 X8( , , , ) 2p X2 X5 X7 X9( , , , )\np4 X7 X10X6 X3( , , , )Figure 14: An example of the worst case for ordering. When all variables are initially free, everysubset of subgoals is indivisible under the binding of the rest of subgoals, and the overallcomplexity of ordering by Algorithm 6 is O(n!).performs 4 sorting sessions, each one with 2 elements, while Algorithm 4 performs 5 sortingswith 2 elements, and 3 sortings with 3 elements. The adjacency restriction is tested only 3times by Algorithm 6, and 11 times by Algorithm 4. Algorithm 6 creates totally 8 di erentordered sub-sequences, with total length 22, while Algorithm 4 creates 24 ordered pre xes,with total length 55.4.8 Complexity AnalysisBoth Algorithm 4 and Algorithm 6 nd a minimal ordering, and both sort independentsubsets of subgoals whenever possible. Algorithm 6, however, o ers several advantages dueto its divide-and-conquer strategy.Let n be the number of subgoals in the initial set. For convenience, we assume thatthe time of computing the control values for one subgoal is O(1); otherwise, if this timeis , all the complexities below must be multiplied by . The worst case complexity ofAlgorithm 6 is O(n!). Figure 14 shows an example of such a case for n = 5. In this setevery two subgoals share a variable that does not appear in other subgoals. Thus, othersubgoals cannot bind it. The set of the root is indivisible, and no matter which binder ischosen, the sets of the children are indivisible. So, in each child of the root, we must selectevery remaining subgoal as the binder, and so on. The overall complexity of this executionis O(n!). This is indeed the worst-case complexity: presence of AND-nodes in the tree canonly reduce it.Note that even when n is small, such a complex rule body with (n2) free variables isvery improbable in practical programs. Also, the worst-case complexity can be reducedto O(n2 2n), if we move from divisibility trees to divisibility graphs (DAGs), where allidentical nodes of a divisibility tree (same subgoal set, same binding set) are representedby a single vertex. The equivalence test of the tree nodes can be performed e ciently withthe help of trie structures (Aho et al., 1987), where subgoals are sorted lexicographically.Let there be n subgoals, with v shared variables appearing in m subgoals. As wasalready noted in Section 4.3, the partition of subgoals into subsets can be performed in74\nThe Divide-and-Conquer Subgoal-Ordering AlgorithmO(n) average time, using a Union-Find data structure (Cormen et al., 1991, Chapter 22).In the worst possible case, there are no AND-nodes in the divisibility tree, apart from theroot node (whose set is divisible into a dependent set of size m and an independent set ofsize n m). The overall complexity of the dac algorithm in such a case isT (n;m; v) = O(n) | divisibility partition+ O((n m) log(n m)) | ordering of independent subgoals+ O((Qki=0(m i)) log(m k)) | ordering of dependent subgoals+ O(m Qk 1i=0 (m i)) | folding+ O(n Qk 1i=0 (m i)) | mergingwhere k is the maximal possible number of bindings performed before the remaining subsetis independent. If we assume that every subgoal binds all its free variables (which happensvery frequently in practical logic programs), then k = minfv;m 1g; otherwise k = m 1.k is equal to the maximal number of OR-nodes on a path from the root to a leaf of thedivisibility tree. Therefore, the height of the divisibility tree is limited by k + 1. Actually,the tree can be shallower, since some binders can bind more than one shared variable each.This means that the number of shared variables can decrease by more than 1 in each OR-node. Below we simplify the above formula for several common cases, when k is small andwhen the abovementioned assumption holds (every subgoal binds all its free variables afterits proof terminates). If v < m n: T (n;m; v) = O(n mv + n logn) If m v n: T (n;m; v) = O(n mm 1 + n logn) If v m ' n: T (n;m; v) = O(nv+1 log n) If m v ' n: T (n;m; v) = O(n m! + n logn)Generally, for a small number v of shared variables, the complexity of the algorithm isroughly bounded by O(nv+1 log n). In particular, if all subgoals are independent (v = 0),the complexity is O(n logn). In most practical cases, the number of shared free variablesin a rule body is relatively small, and every subgoal binds all its free variables; therefore,the algorithm has polynomial complexity. Note that even if a rule body in the programtext contains many free variables, most of them usually become bound after the rule headuni cation is performed (i.e., before we start the ordering of the instantiated body).5. Learning Control Knowledge for OrderingThe ordering algorithms described in the previous sections assume the availability of correctvalues of average cost and number of solutions for various predicates under various argumentbindings. In this section we discuss how this control knowledge can be obtained by learning.Instead of static exploration of the program text (Debray & Lin, 1993; Etzioni, 1993),we adopt the approach of Markovitch and Scott (1989) and learn the control knowledgeby collecting statistics on the literals that were proved in the past. This learning can beperformed on-line or o -line. In the latter case, the ordering system rst works with atraining set of queries, while collecting statistics. This training set can be built on the75\nLedeniov & Markovitchdistribution of user queries seen in the past. We assume that the distribution of queriesreceived by the system does not change signi cantly with time; hence, the past distributiondirects the system to learn relevant knowledge for the future queries.While proving queries, the learning component accumulates information about the con-trol values (average cost and number of solutions) of various literals. Storing a separatevalue for each literal is not practical, for two reasons. The rst is the large space requiredby this approach. The second is the lack of generalization: the ordering algorithm is quitelikely to encounter literals which have not been seen before, and whose control values areunknown. Recall that when we transformed Equation 2 into Equation 5, we moved fromcontrol values of single literals to average control values over sets of literals. To obtain theprecise averages for these sets, we still needed the control values of individual literals. Here,we take a di erent approach, that of learning and using control values for more generalclasses of literals. The estimated cost (nsols) value of a class can be de ned as the averagereal cost (nsols) value of all examples of this class that were proved in the past.The more re ned the classes, the smaller the variance of real control values inside eachclass, the more precise the cost and nsols estimations that the classes assign to their mem-bers, and the better orderings we obtain. One easy way to de ne classes is by modesor binding patterns (Debray & Warren, 1988; Ullman & Vardi, 1988): for each argu-ment we denote whether it is free or bound. For example, for the predicate father thepossible classes are father(free,free), father(bound,free), father(free,bound) andfather(bound,bound). Now, if we receive a literal (for example, father(abraham,X)),we can easily determine its binding pattern (in this case, father(bound,free)) and re-trieve the control information stored for this class. Of course, to nd the binding patternof a subgoal with a given binding set, we need a method to determine which variables arebound by the subgoals of the binding set. The same problem arose in DPart computation(Section 4.3). We shall discuss some practical ways to solve this problem in Section 7.1.For the purpose of class de nition we can also use regression trees { a type of decision treethat classi es to continuous numeric values and not to discrete classes (Breiman et al., 1984;Quinlan, 1986). Two separate regression trees can be stored for every program predicate,one for its cost values, and one for the nsols. The tests in the tree nodes can be de nedin various ways. If we only use the test \\is argument i bound?\", then the classes of literalsde ned by regression trees coincide with the classes de ned by binding patterns. But wecan also apply more sophisticated tests, both syntactic (e.g., \\is the third argument a termwith functor f?\") and semantic (e.g., \\is the third argument female?\"), which leads tomore re ned classes and better estimations. A possible regression tree for estimating the ofnumber of solutions for predicate father is shown in Figure 15.Semantic tests about the arguments require logic inference (in the example of Figure 15{ invoking the predicate female on the rst argument of the literal). Therefore, they mustbe as e cient as possible. Otherwise the retrieval of control values will take too much time.The problem of e cient learning of control values is further considered elsewhere (Ledeniov& Markovitch, 1998a).Several researchers applied machine learning techniques for accelerating logic inference(Cohen, 1990; Dejong & Mooney, 1986; Langley, 1985; Markovitch & Scott, 1993; Minton,1988; Mitchell, Keller, & Kedar-Cabelli, 1986; Mooney & Zelle, 1993; Prieditis & Mostow,1987). Some of these works used explanation-based learning or generalized caching tech-76\nThe Divide-and-Conquer Subgoal-Ordering Algorithm yes nono\nnoyes\nyes\nyes no\nAverage: 1.0\nTest: bound(arg2)? Average: 0.98\nTest: bound(arg2)? Average: 0.3 Test: female(arg1)?\nAverage: 0.0\nAverage: 0.0001\nAverage: 0.5 Average: 50\nAverage: 3.1416 Test: bound(arg1)?\nAverage: 5Figure 15: A regression tree that estimates the number of solutions for father(arg1,arg2).niques to avoid repeated computation. Others utilized the acquired knowledge for the prob-lem of clause selection. None of these works, however, dealt with the problem of subgoalreordering.6. ExperimentationTo test the e ectiveness of our ordering algorithm, we experimented with it on variousdomains, and compared its performance to other ordering algorithms. Most experimentswere performed on randomly created arti cial domains. We also tested the performance ofthe system on several real domains.6.1 Experimental MethodologyAll experiments described below consist of a training session, followed by a testing session.Training and testing sets of queries are randomly drawn from a xed distribution. In thetraining session we collect the control knowledge for literal classes. In the testing session weprove the queries of the testing set using di erent ordering algorithms, and compare theirperformance using various measurements.The goal of ordering is to reduce the time spent by the Prolog interpreter when itproves queries of the testing set. This time is the sum of the time spent by the orderingprocedure (ordering time) and the time spent by the interpreter (inference time). Since theCPU time is known to be very sensitive to irrelevant factors such as hardware, softwareand programming quality, we also show two alternative discrete measurements: the totalnumber of clause uni cations, and the total number of clause reductions performed. Thenumber of reductions re ects the size of the proof tree.For experimentation we used a new version of the lassy system (Markovitch & Scott,1989), using regression trees for learning, and the ordering algorithms discussed in thispaper. 77\nLedeniov & Markovitch6.2 Experiments with Arti cial DomainsIn order to ensure the statistical signi cance of the results of comparing di erent orderingalgorithms, we experimented with many di erent domains. For this purpose, we created aset of 100 arti cial domains, each with a small xed set of predicates, but with a randomnumber of clauses in each predicate, and with random rule lengths. Predicates in therule bodies, and arguments in both rule heads and bodies are randomly drawn from xeddistributions. Each domain has its own training and testing sets (these two sets do notintersect).The more training examples are fed into the system on the learning phase, the betterestimations of control values it produces. On the other hand, the learning time must be lim-ited, because after seeing a certain number of training examples, new examples do not bringmuch new information, and additional learning becomes wasteful. We have experimentallybuilt a learning curve which shows the dependence of the quality of the control knowledgeon the amount of training. The curve suggests that after control values were learned forapproximately 400 literals, there is no signi cant improvement in the quality of orderingwith new training examples. Therefore, in the subsequent experiments we stopped trainingafter 600 cost values were learned. The training time was always small: one learned costvalue corresponds to a complete proof of a literal. Thus, if every predicate in a program hasfour clauses that de ne it, then 600 cost values are learned after 2400 uni cations, which isa very small time.The control values were learned by means of regression trees (Section 5), with simplesyntactic tests that only checked whether some argument is bound or whether some argu-ment is a term with a certain functor (the list of functors was created automatically whenthe domain was loaded). However, as we shall see, even these simple tests succeeded inmaking good estimations of control values.We tested the following ordering methods: Random: The subgoals are permuted randomly and the control knowledge is notused. Algorithm 3: Building ordered pre xes. Out of all pre xes that are permutation ofone another, only the cheapest one is retained. Algorithm 3a: As Algorithm 3, but with best- rst search method used to de ne thenext processed pre x. A similar algorithm was used in the lassy system of Markovitchand Scott (1989). Algorithm 3b: As Algorithm 3a, but with adjacency restriction test added. Asimilar algorithm was described by Smith and Genesereth (1985). Algorithm 4: As Algorithm 3b, but whenever all the subgoals that are not in thepre x are independent (under the binding of the pre x), they are sorted and the resultis appended to the pre x as one unit. Algorithm 6: The dac algorithm.In our experiments we always used the Bubble-Sort algorithm to sort literals in inde-pendent sets. This algorithm is easy to implement, and it is known to be e cient for small78\nThe Divide-and-Conquer Subgoal-Ordering AlgorithmOrdering Uni cations Reductions Ordering Inference Total Ord.TimeMethod Time Time Time ReductionsRandom 86052.06 27741.52 8.1910 27.385 35.576 0.00029Algorithm 3 2600.32 911.04 504.648 1.208 505.856 0.55Algorithm 3.a 2829.00 978.59 347.313 1.178 348.491 0.35Algorithm 3.b 2525.34 881.12 203.497 1.137 204.634 0.23Algorithm 4 2822.27 976.02 40.284 1.191 41.475 0.04Algorithm 6 2623.82 914.67 2.3620 1.102 3.464 0.0025Table 4: The e ect of ordering on the tree sizes and the CPU time (mean results over 100 arti cialdomains).sets, when the elements are already ordered, or nearly ordered. In practice, programmersorder most program rules optimally, and the sorting stops early.Since the non-deterministic nature of the random method introduces additional noise,we performed on each arti cial domain 20 experiments with this method, and the tablepresents the average values of these measurements.Table 4 shows the obtained results over 100 domains: the rows correspond to the orderingmethods used, and the columns to the measurements taken. The rightmost column showsthe ratio of the ordering time and the number of reductions performed, which re ects theaverage ordering time of one rule body. The inference time was not measured separately,but was set as the di erence of the total time and the ordering time.Several observations can be made:1. Using the dac ordering algorithm helps to reduce the total time of proving the testingset of queries by a factor of 10, compared to the random ordering. The inference timeis reduced by a factor of 25.2. All deterministic ordering methods have similar number of uni cations and reductions,and similar inference time, which is predictable, since they all nd minimal orderings.Small uctuations of these values can be explained by the fact that some rules haveseveral minimal orderings under the existing control knowledge, and di erent orderingalgorithms select di erent minimal orderings. Since the control knowledge is notabsolutely precise, the real execution costs of these orderings may be di erent, whichleads to the di erences. The random ordering method builds much larger trees, withlarger inference time.3. When we compare the performance of the deterministic algorithms (3 { 6), we seethat the dac algorithm performs much better than the algorithms that build orderedpre xes. In the latter ones, the ordering is expensive, and smaller inference timecannot compensate for the increase in ordering time. Only Algorithm 4, a combinationof several ideas of previous researchers, has total time comparable with the time ofthe random method (though still greater).79\nLedeniov & Markovitch4. It may seem strange that the simple random ordering method has larger ordering timethan the sophisticated Algorithm 6. To explain this, note that the random methodcreates much larger proof trees (on average), therefore the number of ordered rulesincreases, and even the cheap operations, like random ordering of a rule, sum up toa considerable time. The average time spent on ordering of one rule is shown in thelast column of Table 4; this value is very small in the random method.6.3 Experiments with Real DomainsWe tested our ordering algorithm also on real domains obtained from various sources. Thesedomains allow us to compare orderings performed by our algorithm with orderings per-formed by human programmers.The following domains were used: Moral-reasoner: Taken from the Machine Learning Repository at the Universityof California, Irvine1. The domain qualitatively simulates moral reasoning: whethera person can be considered guilty, given various aspects of his character and of thecrime performed. Depth- rst planner: Program 14.11 from the book \\The Art of Prolog\" (Sterling& Shapiro, 1994). The program implements a simple planner for the blocks world. Biblical Family Database: A database similar to that described in Example 1. Appletalk: A domain describing the physical layout of a local computer network(Markovitch, 1989). Benchmark: A Prolog benchmark taken from the CMUArti cial Intelligence Reposi-tory2. The predicate names are not informative: it is an example of a program wheremanual ordering is di cult. Slow reverse: Another benchmark program from the same source. Geography: Also a benchmark program from the CMU Repository. The domaincontains many geographical facts about countries.Table 5 shows the results obtained. For ordering we used the dac algorithm, with literalclasses de ned by binding patterns. It can be seen that the dac algorithm was able to speedup the logic inference in real domains as well. Note that in the Slow Reverse domain theprogrammer's ordering was already optimal; thus, applying the ordering algorithm did notreduce the tree sizes. Still, the overhead of the ordering is not signi cant.7. DiscussionIn this concluding section we discuss several issues concerning the practical implementationof the dac algorithm and several ways to increase its e ciency. Then we survey somerelated areas of logic programming and propose the use of the dac algorithm there.1. URL: http://www.ics.uci.edu/~mlearn/MLRepository.html2. URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/project/ai-repository/ai/html/air.html80\nThe Divide-and-Conquer Subgoal-Ordering AlgorithmDomain Without ordering With ordering Gain ratiouni cations seconds uni cations seconds (time/time)Moral-reasoner 352180 98.39 87020 23.53 4.2Depth- rst planner 10225 19.01 9927 18.16 1.05Biblical Family 347827 112.68 120701 46.08 2.5Appletalk 5036167 1246.30 640092 221.73 5.6Benchmark 62012 554.31 46012 395.04 1.4Slow reverse 6291 10.33 6291 11.92 0.9Geography 428480 141.47 226628 82.76 1.7Table 5: Experiments on real domains.7.1 Practical IssuesIn this subsection we would like to address several issues related to implementation andapplications of the dac algorithm.The computation of the DPart function (Section 3.2.1) requires a procedure for com-puting the set of variables bound by a given binding set of subgoals. The same procedureis needed for computing control values (Section 5). There are several possible ways toimplement such a procedure. For example:1. The easiest way is to assume that every subgoal binds all the variables appearing in itsarguments. This simplistic assumption is su cient for many domains, especially thedatabase-oriented ones. However, it is not appropriate when logic programs are usedto manipulate complex data structures containing free variables (such as di erencelists). This assumption was used for the experiments described in Section 6.2. Some dialects of Prolog and other logic languages support mode declarations providedby the user (Somogyi et al., 1996b). When such declarations are available, it is easyto infer the binding status of each variable upon exiting a subgoal.3. Even when the user did not supply enough mode declarations, they can often beinferred from the structure of the program by means of static analysis (Debray &Warren, 1988). Note, however, that as was pointed out by Somogyi et al. (1996b),no-one has yet demonstrated a mode inference algorithm that is guaranteed to ndaccurate mode information for every predicate in the program.4. We can learn the sets of variables bound by classes of subgoals using methods similarto those described in Section 5 for learning control values.Several researchers advocate user declarations of available (permitted) modes. Suchdeclarations can be elegantly incorporated into our algorithm to prune branches that violateavailable modes. When we x a binder in an OR-node, we compute the set of variablesthat become bound by it. If this results in a violation of an available mode for one of thesubgoals of the corresponding child, then the whole subtree of this child is pruned. Notethat we can detect violations even when the mode of the subgoal is partially unknown81\nLedeniov & MarkovitchCandidateSet(S;B)let fS1;S2; : : :Skg DPart(S;B)case: : : k = 1, shared-vars(S1) 6= ; (S is indivisible under B):loop for A 2 Sif B [ fAg does not violate available modesin any subgoal of S n fAgthenlet C(A) CandidateSet(S n fAg;B [ fAg)let C 0(A) nFold(Ak ~OA;B) ~OA 2 C(A)oelse let C 0(A) ; (don't enter the branch)Return SA2S C0(A)Figure 16: Changes to Algorithm 6 that make use of available mode declarations.The rest of the algorithm remains unchanged.at the moment. For example, if all the available modes require that the rst argumentbe unbound, then binding of the argument by the OR-node binder will trigger pruning,even if the binding status of the other arguments is not yet known. Figure 16 shows howAlgorithm 6 can be changed in order to incorporate declarations of available modes. Anyother correctness requirement can be treated in a similar manner: a candidate ordering willbe rejected whenever we see that it violates the requirement.The experiments described in Section 6 were performed with a Prolog interpreter. Isit possible to combine the dac algorithm with a Prolog compiler? There are several waysto achieve this goal. One way is to allow the compiler to insert code for on-line learning.The compiled code will contain procedures for accumulating control values and for the dacalgorithm. Alternatively, o -line learning can be implemented, with training as a part ofthe compilation process.Another method for combining our algorithm with existing Prolog compilers is to useit for program transformation, and to process the transformed program by a standardcompiler. Elsewhere (Ledeniov & Markovitch, 1998a) we describe the method for classifyingthe orderings produced by the dac algorithm. For each rule we build a classi cation tree,where classes are the di erent orderings of the rule body, and the tests are applied to therule head arguments. These are the same type of tests described in Section 5 for learningcontrol values. Figure 17 shows two examples of such trees.Given such a classi cation tree, we can write a set of Prolog rules, where each rule hasthe same head as the original rule, and has a body built of all the tests on the path fromthe tree root to a leaf node followed by the ordering at the leaf. For example, the secondtree in Figure 17 yields the following set of rules:82\nThe Divide-and-Conquer Subgoal-Ordering AlgorithmA classi cation tree for the ruleuncle(X,Y) of Example 1. [parent(Z,Y),brother(Z,X)]\n[brother(Z,X),parent(Z,Y)] nonvar(Y) ? noyesA possible classi cation tree for the rulehead(X,Y) p1(X), p2(Y), p3(X,Y).\n[p2(Y),p3(X,Y),p1(X)]\n[p3(X,Y),p1(X),p2(Y)]\n[p1(X),p3(X,Y),p2(Y)]\n[p1(X),p2(Y),p3(X,Y)]\nnonvar(X) ?\nmale(X) ? nonvar(Y) ?\nno\nno yes noyes yesFigure 17: Examples of classi cation trees that learn rule body orderings.head(X,Y) nonvar(X), male(X), p1(X), p3(X,Y), p2(Y).head(X,Y) nonvar(X), not(male(X)), p1(X), p2(Y), p3(X,Y).head(X,Y) var(X), nonvar(Y), p2(Y), p3(X,Y), p1(X).head(X,Y) var(X), var(Y), p3(X,Y), p1(X), p2(Y).From Table 4 we can see that while the dac algorithm helped to reduce the inferencetime by a factor of 25, the total time was reduced only by a factor of 10. This di erenceis caused by the additional computation of the ordering procedure. There is a danger thatthe bene t obtained by ordering will be outweighed by the cost of the ordering process.This is a manifestation of the so-called utility problem (Minton, 1988; Markovitch & Scott,1993). In systems that are strongly-moded (such as Mercury { Somogyi et al., 1996b) we canemploy the dac algorithm statically at compilation time for each one of the available modes,thus reducing the run-time ordering time to zero. The mode-based approach performs onlysyntactic tests of the subgoal arguments. The classi cation tree method, described above,is a generalization of the mode-based approach, allowing semantic tests as well.Due to insu cient learning experience or lack of meaningful semantic tests, it is quitepossible that the classi cation trees contain leaves with large degrees of error. In such caseswe still need to perform the ordering dynamically. To reduce the harmfulness of the utilityproblem in the case of dynamic ordering, we can use a cost-sensitive variation of the dacalgorithm (Ledeniov & Markovitch, 1998a, 1998b). This modi ed algorithm deals with theproblem by explicit reasoning about the economy of the control process. The algorithm isanytime, that is, it can be stopped at any moment and return its currently best ordering(Boddy & Dean, 1989). We learn a resource-investment function to compute the expectedreturn in speedup time for additional control time. This function is used to determine astopping condition for the anytime procedure. We have implemented this framework andfound that indeed we have succeeded in reducing ordering time, without signi cant increaseof inference time. 83\nLedeniov & Markovitch7.2 Relationship to Other WorksThe work described in this paper is a continuation of the line of research initiated by Smithand Genesereth (1985) and continued by Natarajan (1987) and Markovitch and Scott (1989).This line of research aims at nding the most e cient ordering of a set of subgoals. Thesearch for minimal-cost ordering is based on cost analysis that utilizes available informationabout the cost and number-of-solutions of individual subgoals.Smith and Genesereth (1985) performed an exhaustive search over the space of allpermutations of the given set of subgoals, using the adjacency restriction to reduce thesize of the search space (Equation 8). This restriction was applied on pairs of adjacentsubgoals in the global ordering of the entire set. When applied to an independent set ofsubgoals, the adjacency restriction is easily transformed into the sorting restriction: thesubgoals in a minimal ordering must be sorted by their cn values. Natarajan (1987) arrivedat this conclusion and presented an e cient ordering algorithm for independent sets.The dac algorithm uses subgoal dependence to break the set into smaller subsets. In-dependent subsets are sorted. Dependent subsets are recursively ordered, and the resultingorderings are merged using a generalization of the adjacency restriction that manipulatesblocks of subgoals. Therefore the dac algorithm is a generalization of both algorithms.During the last decade, a signi cant research e ort went into static analysis (SA) oflogic programs. There are three types of SA that can be exploited by the dac algorithm toreduce the ordering time.A major part of the SA research deals with program termination (De Schreye & Decorte,1994). The dac algorithm solves the termination problem, as a special case of the e ciencyproblem (it always nds a terminating ordering, if such orderings exist). During learning,we set limits on the computation resources available for subgoal execution. If a subgoal isnon-terminating (in a certain mode), the learning module will associate a very high costwith this particular mode. Consequently, the dac algorithm will not allow orderings withthis mode of the subgoal. Nevertheless, while the use of static termination analysis isnot mandatory for a proper operation of the dac algorithm, we can exploit such analysisto increase the e ciency of both the learning process and the ordering process. Duringlearning, the limit that we set on the computation resources devoted to the execution ofa subgoal must be high, to increase the reliability of the cost estimation. However, sucha high limit can lead to a signi cant increase in learning time when many subgoals arenon-terminating. If termination information obtained by SA is available, we can use it toavoid entering in nite branches of proof trees. During ordering, termination information canserve to reduce the size of space of orderings searched by the algorithm. If the terminationinformation comes in the form of allowed modes (Somogyi et al., 1996b), orderings thatviolate these modes are ltered out, as in the modi ed algorithm shown in Figure 16. If thetermination information comes in the form of a partial order between subgoals, orderingsthat violate this partial order can be ltered out in a similar manner.The second type of SA research that can be combined with the dac algorithm is cor-rectness analysis, where the program is tested against speci cations given by the user.The folon environment (Henrard & Le Charlier, 1992) was designed to support themethodology for logic program construction that aims at reconciling the declarative seman-tics with an e cient implementation (Deville, 1990). The construction process starts with84\nThe Divide-and-Conquer Subgoal-Ordering Algorithma speci cation, converts it into a logic description and nally, into a Prolog program. Ifthe rules of the program are not correct with respect to the initial speci cation, the sys-tem performs transformations such as reordering literals in a clause, adding type checkingliterals and so on. De Boeck and Le Charlier (1990) mention this reordering, but do notspecify an ordering algorithm di erent from the simple generate-and-test method. Cortesi,Le Charlier, and Rossi (1997) present an analyzer for verifying the correctness of a Prologprogram relative to a speci cation which provides a list of input/output annotations for thearguments and parameters that can be used to establish program termination. Again, noordering algorithm is given explicitly. The purpose of the dac algorithm is complementaryto the purpose of folon, and it could serve as an auxiliary aid to make the resulting Prologprogram more e cient.Recently, the Mercury language was developed at the University of Melbourne (Somogyiet al., 1996a, 1996b). Mercury is a strongly typed and strongly moded language. Type andmode declarations should be supplied by the programmer (though recent releases of theMercury system already support partial inference of types and modes { Somogyi et al.,1996a). The compiler checks that mode declarations for all predicates are satis ed; ifnecessary, it reorders subgoals in the rule body to ensure mode correctness (and rejects theprogram if neither ordering satis es the mode declaration constraints). When the compilerperforms this reordering, it does not consider the e ciency issue. It often happens thatseveral orderings of a rule body satisfy the mode declaration constraints: in such casesthe Mercury compiler could call the static version of the dac algorithm to select the moste cient ordering. Another alternative is to augment the dac algorithm by mode declarationchecks, as was shown in Figure 16.Note that Mercury is a purely declarative logic programming language, and is thereforemore suitable for subgoal reordering than Prolog. It has no non-logical constructs thatcould destroy the declarative semantics which give logic programs their power; in Mercuryeven I/O is declarative.The third type of relevant SA is the cost analysis of logic programs (Debray & Lin,1993; Braem et al., 1994; Debray et al., 1997). Cortesi et al. (1997) describe a cost formulasimilar to Equation 5 to select a lowest-cost ordering. However, they used a generate-and-test approach which can sometimes be prohibitively expensive. Static analysis of cost andnumber of solutions can be used to obtain the control values, instead of learning them.The e ciency of logic programs can also be increased by methods of program trans-formation (Pettorossi & Proietti, 1994, 1996). One of the most popular approaches is the\\rules+strategies\" approach, which consists in starting from an initial program and thenapplying one or more elementary transformation rules. Transformation strategies are meta-rules which prescribe suitable sequences of applications of transformation rules.One of the possible transformation rules is the goal rearrangement rule which transformsa program by transposing two adjacent subgoals in a rule body. Obviously, any orderingof a rule body can be transformed into any other ordering by a nite number of suchtranspositions. Thus, static subgoal ordering can be considered a special case of programtransformation where only the goal rearrangement rule is used. On the other hand, dynamicand semi-dynamic ordering methods cannot be represented by simple transformation rules,since they make use of run-time information (expressed in bindings that rule body subgoals85\nLedeniov & Markovitchobtain through uni cations of rule heads), and may order the same rule body di erentlyunder di erent circumstances.A program transformation technique called compiling control (Bruynooghe, De Schreye,& Krekels, 1989; Pettorossi & Proietti, 1994) follows an approach di erent from that oftrying to improve the control strategy of logic programs. Instead of enhancing the naiveProlog evaluator using a better (and often more complex) computation rule, the program istransformed so that the derived program behaves under the naive evaluator exactly as theinitial program would behave under an enhanced evaluator. Most forms of compiling control rst translate the initial program into some standard representation (for example, into anunfolding tree), while the complex computation rule is used, and then the new program isconstructed from this representation, with the naive computation rule in mind.Reordering of rule body subgoals can be regarded as moving to a complex computationrule which selects subgoals in the order dictated by the ordering algorithm. In the case ofthe dac algorithm, this computation rule may be too complex for simple use of compilingcontrol methods. Nevertheless, it can be easily incorporated into a special compiling controlmethod. In Section 7.1 we described a method of program rewriting which rst buildsclassi cation trees based on the orderings that were performed in the past, and then usesthese classi cation trees for constructing clauses of a derived program. The derived programcan be e ciently executed under the naive computation rule of Prolog. This technique isin fact a kind of compiling control. Its important property is the use of knowledge collectedfrom experience (the orderings that were made in the past).One transformation method that can signi cantly bene t from the dac algorithm isunfolding (Tamaki & Sato, 1984). During the unfolding process subgoals are replaced bytheir associated rule bodies. Even if the initial rules were ordered optimally by a humanprogrammer or a static ordering procedure, the resulting combined sequence may be far fromoptimal. Therefore it could be very advantageous to use the dac algorithm for reorderingof the unfolded rule. As the rules become longer, the potential bene t of ordering grows.The danger of high complexity of the ordering procedure can be overcome by using thecost-sensitive version of the dac algorithm (Section 7.1).7.3 ConclusionsIn this work we study the problem of subgoal ordering in logic programs. We present both atheoretical base and a practical implementation of the ideas, and show empirical results thatcon rm our theoretical predictions. We combine the ideas of Smith and Genesereth (1985),Simon and Kadane (1975) and Natarajan (1987) into a novel algorithm for ordering ofconjunctive goals. The algorithm is aimed at minimizing the time which the logic interpreterspends on the proof of the given conjunctive goal.The main algorithm described in this paper is the dac algorithm (Algorithm 6, Sec-tion 4.6). It works by dividing the sets of subgoals into smaller sets, producing candidatesets of orderings for the smaller sets, and combining these candidate sets to obtain orderingsof the larger sets. We prove that the algorithm nds a minimal ordering of the given setof subgoals, and we show its e ciency under practical assumptions. The algorithm canbe employed statically (to reorder rule bodies in the program text before the execution86\nThe Divide-and-Conquer Subgoal-Ordering Algorithmstarts), semi-dynamically (to reorder the rule body before the reduction is performed) ordynamically (to reorder the resolvent after every reduction of a subgoal by a rule body).Several researchers (Minker, 1978; Warren, 1981; Naish, 1985a, 1985b; Nie & Plaisted,1990) proposed various heuristics for subgoal ordering. Though fast, these methods donot guarantee nding minimal-cost orderings. Our algorithm provably nds a minimal-costordering, though the ordering itself may take more time than with the heuristic methods. Inthe future it seems promising to incorporate heuristics into the dac algorithm. For example,heuristics can be used to grade binders in OR-nodes: rather than exhaustively trying allsubgoals as binders, we could try just one, or several binders, thus reducing the orderingtime. Also, the current version of our ordering algorithm is suitable only for nding allsolutions to a conjunctive goal. We would like to extend it to the problem of nding onesolution, or a xed number of solutions.Another interesting issue for further research is the adaptation of the dac algorithm tointerleaving ordering methods (Section 2.3). There, if subgoals of a rule body are addedto an ordered resolvent, it seems wasteful to start a complete ordering process; we shoulduse the information stored in the existing ordering of the resolvent. Perhaps the wholedivisibility tree of the resolvent should be stored, and its nodes updated when subgoals ofa rule body are added to the resolvent.The ordering algorithm needs control knowledge for its work. This control knowledge isthe average cost and number of solutions of literals, and it can be learned by training andcollecting statistics. We make an assumption that the distribution of queries received bythe system does not change with time; thus, if the training set is based on the distributionseen in the past, the system learns relevant knowledge for future queries. We consider theissue of learning control values more thoroughly in another paper (Ledeniov & Markovitch,1998a), together with other issues concerning the dac algorithm (such as minimizing thetotal time, instead of minimizing the inference time only).Ullman and Vardi (1988) showed that the problem of ordering subgoals to obtain ter-mination is inherently exponential in time. The problem we work with is substantiallyharder: we must not only nd an order whose execution terminates in nite time, but onethat terminates in minimal nite time. It is impossible to nd an e cient algorithm forall cases. The dac algorithm, however, is e cient in most practical cases, when the graphrepresenting the subgoal dependence (Figure 3) is sparsely connected.We have implemented the dac algorithm and tested it on arti cial and real domains.The experiments show a speedup factor of up to 10 compared with random ordering, andup to 13 compared with some alternative ordering algorithms.The dac algorithm can be useful for many practical applications. Formal hardwareveri cation has become extremely important in the semiconductor industry. While modelchecking is currently the most widely used technique, it is generally agreed that coping withthe increasing complexity of VLSI design requires methods based on theorem proving. Themain obstacle preventing the use of automatic theorem proving is its high computationaldemands. The dac algorithm may be used for speeding up logic inference, making the useof automatic theorem provers more practical.Logic has gained increasing popularity for representation of common-sense knowledge.It has several advantages, including exibility and well-understood semantics. Indeed, theCYC project (Lenat, 1995) has recently moved from frame-based representation to logic-87\nLedeniov & Markovitchbased representation. However, the large scale of such knowledge bases is likely to presentsigni cant e ciency problems to the inference engines. Using automatic subgoal orderingtechniques, such as those described here, may help to solve these problems.The issue of subgoal ordering obtains a new signi cance with the development of Induc-tive Logic Programming (Lavrac & Dzeroski, 1994; Muggleton & De Raedt, 1994). Systemsusing this approach, such as FOIL (Quinlan & Cameron-Jones, 1995), try to build correctprograms as fast as possible, without considering the e ciency of the produced programs.Combining the dac algorithm with Inductive Logic Programming and other techniques forthe synthesis of logic programs (such as the deductive and the constructive approaches)looks like a promising direction.Appendix A. Proof of Lemma 8In this appendix we present the proof of a lemma which was omitted from the main text ofthe paper for reasons of compactness. Before we prove it we show two auxiliary lemmas.Lemma 9Let ~A1 and ~A2 be two ordered sequences of subgoals, and B a set of subgoals. The value ofcn( ~A1k ~A2)jB lies between the values cn( ~A1)jB and cn( ~A2)jB[ ~A1.Proof:Denote c1 = cost( ~A1)jB n1 = nsols( ~A1)jB cn1 = cn( ~A1)jBc2 = cost( ~A2)jB[~A1 n2 = nsols( ~A2)jB[ ~A1 cn2 = cn( ~A2)jB;[~A1c1;2 = cost( ~A1k ~A2)jB n1;2 = nsols( ~A1k ~A2)jB cn1;2 = cn( ~A1k ~A2)jBcn1;2 = n1;2 1c1;2 = n1n2 1c1;2 = (n1 1) + n1(n2 1)c1;2 == c1n1 1c1 + n1c2n2 1c2c1;2 = c1cn1 + n1c2cn2c1;2 = c1c1;2 cn1 + n1c2c1;2 cn2So, cn1;2 always lies between cn1 and cn2 (because c1c1;2 and n1c2c1;2 are positive and sumto 1). More exactly, the point cn1;2 divides the segment [cn1; cn2] with ratio(cn1;2 cn1) : (cn2 cn1;2) = n1c2 : c1:In other words, cn1;2 is a weighted average of cn1 and cn2. Note that c1 is the amountof resources spent in the proof-tree of ~B1, n1c2 { the resources spent in the tree of ~B2, andc1;2 is their sum. So, the more time (relatively) we dedicate to the proof of ~B1, the closercn1;2 is to cn1. This conclusion can be generalized to a larger number of components in aconcatenation (the proof is by induction):cn( ~A1k ~A2k : : : ~Ak)jB = cost( ~A1)jBcost( ~A1k ~A2k : : : ~Ak)jB cn( ~A1)jB ++ nsols( ~A1)jB cost( ~A2)jB[ ~A1cost( ~A1k ~A2k : : : ~Ak)jB cn( ~A2)jB[~A1 + : : :++ nsols( ~A1k ~A2k : : : ~Ak 1)jB cost( ~Ak)jB[ ~A1[::: ~Ak 1cost( ~A1k ~A2k : : : ~Ak)jB cn( ~Ak)jB[ ~A1[::: ~Ak 188\nThe Divide-and-Conquer Subgoal-Ordering Algorithm 2Lemma 10Let S0 be a set of subgoals and N be a node in the divisibility tree of S0. Let ~ON =~Qk ~A1k ~A2k~R be an ordering of S(N), where ~A1 and ~A2 are cn-equal max-blocks: cn( ~A1)jB(N)[~Q =cn( ~A2)jB(N)[~Q[ ~A1.Let M be an ancestor of N and ~OM be an ordering of S(M) consistent with ~ON , where~A1 and ~A2 are not violated. Then either ~A1 and ~A2 are both max-blocks in ~OM and allmax-blocks that stand between them are cn-equal to them, or ~A1 and ~A2 belong to the samemax-block in ~OM , or ~OM is MC-contradicting.Proof: By induction on the distance between N and M . If M = N , then ~A1 and ~A2are max-blocks, and the lemma holds. Let M 6= N , and let M 0 be the child of M whosedescendant is N . By inductive hypothesis, the lemma holds for N and M 0. Let ~O0M be theprojection of ~OM on M 0. ~A1 and ~A2 are not violated in ~O0M , since they are not violated in~OM . If ~A1 and ~A2 are both max-blocks in ~O0M , then by the inductive hypothesis all max-blocks that stand between them are cn-equal to them. If M is an OR-node, no newsubgoals can enter between ~A1 and ~A2. If M is an AND-node, the insertion of newsubgoals is possible, but if it violates blocks, or places max-blocks not ordered by cn,then ~OM is MC-contradicting, by Corollary 3 or Lemma 6. So, if ~OM is not MC-contradicting, then all new max-blocks inserted between ~A1 and ~A2 must be cn-equalto them both.Assume that ~A1 and ~A2 are not both max-blocks in ~OM . Without loss of generality,let ~A1 be member of a larger max-block in ~OM . We show that ~A2 must also participatein the same max-block.Since ~A1 joined a larger block, there must exist another block, ~B, adjacent to ~A1,such that their pair is cn-inverted. Let ~B stand to the left of ~A1 (in the opposite case,the proof is similar): ~OM = ~Xk ~Bk ~A1k~Y k ~A2k~Z. The pair h ~B; ~A1i is cn-inverted, i.e.,cn( ~B)jB(M)[~X > cn( ~A1)jB(M)[~X[~B. From Lemma 9, cn( ~Bk ~A1)jB(M)[~X > cn( ~A1)jB(M)[~X[ ~B,and we must add to the block ~Bk ~A1 all blocks from ~Y , because they are all cn-equalto ~A1. Also, cn( ~A1)jB(M)[~X[ ~B = cn( ~A2)jB(M)[~X[~B[ ~A1 , and ~A2 must also be addedto the block. Thus, ~A1 and ~A2 belong to the same max-block in ~OM . If ~A1 and ~A2 belong to the same max-block in ~O0M , then this block is either violatedin ~OM , or not. In the former case, ~OM is MC-contradicting, by Corollary 3. In thelatter case, ~A1 and ~A2 belong to the same max-block in ~OM . If ~O0M is MC-contradicting, then ~OM is MC-contradicting too (the proof is easy). 2Now we can prove Lemma 8:Lemma 8Let S0 be a set of subgoals, N be a node in the divisibility tree of S0 and ~ON = ~Qk ~A1k ~A2k~R89\nLedeniov & Markovitchbe an ordering of S(N), where ~A1 and ~A2 are max-blocks, mutually independent andcn-equal under the bindings of B(N) [ ~Q. Then ~ON is blockwise-equivalent with ~O0N =~Qk ~A2k ~A1k~R.Proof:Let ~S be a minimal ordering of S0 binder-consistent with ~ON . By Corollary 3, ~S does notviolate the blocks of ~ON , in particular ~A1 and ~A2: ~S = ~Xk ~A1k~Y k ~A2k~Z. Let ~S0 = ~Sj~O0N~ON =~Xk ~A2k~Y k ~A1k~Z. We must show that ~S0 is minimal, which implies blockwise equivalence of~ON and ~O0N .If ~Y is empty, then Cost(~S) = Cost(~S0) by Lemma 2 ( ~A1 and ~A2 are adjacent, mutuallyindependent and cn-equal; thus, their transposition does not change the cost).If ~Y is not empty, then by Corollary 2 ~Y is mutually independent of both ~A1 and ~A2(~S is binder-consistent with ~ON , therefore B(N) ~X, and consequently ~Y \\ B(N) = ;).~Y can be divided into several blocks, each one of them cn-equal to ~A1 and ~A2: since ~Sis minimal, ~ON cannot be MC-contradicting, and the claim follows from Lemma 10. ByLemma 9, cn(~Y )j ~X = cn( ~A1)j ~X = cn( ~A2)j ~X . By Lemma 2:Cost(~S) = Cost( ~Xk ~A1k~Y k ~A2k~Z) = == swap(Y;A2)= Cost( ~Xk ~A1k ~A2k~Y k~Z) = == swap(A1; A2)= Cost( ~Xk ~A2k ~A1k~Y k~Z) = == swap(A1; Y )= Cost( ~Xk ~A2k~Y k ~A1k~Z) = Cost(~S0)Minimality of ~S0 implies blockwise equivalence of ~ON and ~O0N . 2Appendix B. Correctness of the dac AlgorithmIn this section we show that the dac algorithm is correct, i.e., given a set of subgoals S0,it returns its minimal ordering. It su ces to show that the candidate set of the root nodeof DTree(S0; ;) is valid. In such a case, as follows from the de nition of valid sets, it mustcontain a minimal ordering. The algorithm returns one of the cheapest candidates of theroot. Therefore, if the candidate set of the root is valid, the dac algorithm must return aminimal ordering of S0.We start by de ning strong validity of sets of orderings. We then prove that strongvalidity implies validity. Finally, we use induction to prove a theorem, showing that thecandidate set produced for each node in the divisibility tree is strongly valid.De nition: Let S0 be a set of subgoals, N be a node in the divisibility tree of S0. The setCN (S(N)) is strongly valid, if every ordering in (S(N))nCN is either MC-contradictingor blockwise-equivalent to some member of CN , unless no ordering of S(N) is min-consistent.StronglyV alidN;S0(CN) ()[9 ~O0N 2 (S(N)) : MCN;S0( ~O0N)]! [ ~ON 2 (S(N)) n CN !MCCN;S0( ~ON)_(9 ~O00N 2 CN ^MCEN;S0( ~ON ; ~O00N))]Lemma 11 A strongly valid set of orderings is valid.90\nThe Divide-and-Conquer Subgoal-Ordering AlgorithmProof: Let S0 be a set of subgoals, N be a node in the divisibility tree of S0, C(N) be astrongly valid set of orderings of N .If there is no min-consistent ordering of N , then C(N) is valid, by the de nition of avalid set (Section 4.2).Otherwise, there exists at least one minimal ordering of S0, binder-consistent with N .Every ordering in (S(N)) n C(N) is either MC-contradicting or blockwise-equivalent tosome member of C(N). To prove that C(N) is valid, we must show that it contains anordering ~ON , which is binder-consistent with some minimal ordering ~S of S0.Let ~S0 be a minimal ordering of S0, binder-consistent with N . Let ~O0N be the projectionof ~S0 on N . If ~O0N 2 C(N), we are done ( ~ON = ~O0N , ~S = ~S0). Otherwise, ~O0N 2 (S(N)) nC(N). ~O0N cannot be MC-contradicting (it is min-consistent to ~S0), therefore it must beblockwise-equivalent to some ~O00N 2 C(N). Blocks of ~O0N are not violated in ~S 0, since ~S 0 isminimal (Corollary 3). Therefore the substitution ~S00 = ~S 0j~O00N~O0N is well de ned. ~S00 is minimal,since ~S 0 is minimal and ~O0N and ~O00N are blockwise-equivalent. ~S00 is binder-consistent with~O00N , since ~S0 was binder-consistent with ~O0N . Thereupon ~S 00 and ~O00N satisfy the requirementsof validity ( ~ON = ~O00N , ~S = ~S 00). 2Theorem 3Let S0 be a set of subgoals. For each node N of the divisibility tree of S0, Algorithm 6creates a strongly valid candidate set of orderings.Proof: By induction on the height of N 's subtree.Inductive base: N is a leaf node, which means that S(N) is independent under B(N).The candidate set of N contains one element, whose subgoals are sorted by cn. Allorderings that belong to (S(N)) n CandSet(N) are either not sorted by cn, andhence are MC-contradicting (Lemma 4), or are sorted by cn, and hence are blockwise-equivalent to the candidate (Corollary 4). Consequently, CandSet(N) is stronglyvalid.Inductive hypothesis: For all children of N , Algorithm 6 produces strongly valid candi-date sets.Inductive step: An internal node in a divisibility tree is either an AND-node or an OR-node.1. N is an AND-node. Let N1; N2; : : :Nk be the children of N . First we show thatConsSet(N) is strongly valid.Let ~ON 2 (S(N)) nConsSet(N). For all 1 i k, let ~Oi be the projection of~ON on Ni. The set of projections f ~O1; ~O2; : : : ~Okg can belong to one of the threefollowing types, with regard to ~ON .(a) The sets of the rst type contain at least one MC-contradicting projection. Insuch a case ~ON is MC-contradicting too. Assume the contrary: there existsa minimal ordering ~S of S0, binder-consistent with ~ON . Let ~Oi be an MC-contradicting projection. Since ~Oi is consistent with ~ON , it is also consistent91\nLedeniov & Markovitchwith ~S. Since B(Ni) = B(N), all subgoals of B(Ni) appear in ~S beforesubgoals of S(Ni). Therefore, ~Oi is binder-consistent with ~S, and since ~S isminimal, ~Oi is min-consistent and not MC-contradicting { a contradiction.(b) The sets of the second type do not contain MC-contradicting projections, butin ~ON some block of some projection is violated, or max-blocks from di erentprojections are not ordered by cn. In such a case, ~ON is MC-contradicting,by Corollary 3 and Lemma 6.(c) The sets of the third type do not contain MC-contradicting projections, andmax-blocks of the projections are not violated in ~ON and are sorted by cn.Every projection ~Oi either belongs to CandSet(Ni), or not. If ~Oi 62 CandSet(Ni),then there exists ~O0i 2 CandSet(Ni) such that ~Oi is blockwise-equivalent to~O0i (because CandSet(Ni) is strongly valid by the inductive hypothesis, and~Oi is not MC-contradicting). If ~Oi 2 CandSet(Ni), we can set ~O0i = ~Oi.Let ~O0N = ~ON j~O01~O1 j~O02~O2 : : : j~O0k~Ok . This substitution is well de ned, since each ~Oihas the same number of max-blocks as ~O0i, and max-blocks of the projectionsare not violated in ~ON . Let ~S be a minimal ordering of S0, binder-consistentwith ~ON . Since ~S is minimal, blocks of ~O1 are not violated in ~S. Since ~O1is blockwise-equivalent to ~O01, the ordering ~S1 = ~Sj~O01~O1 is well-de ned andminimal. In ~S1 the positions of the subgoals from B(N) did not change;thus, ~O2 is min-consistent with ~S1, and blockwise equivalence of ~O2 and ~O02entails minimality of the ordering ~S2 = ~S1j~O02~O2 = ~Sj~O01~O1 j~O02~O2. We continue withother ~Oi-s, and nally obtain that ~S 0 = ~Sj~O01~O1j~O02~O2 : : : j~O0k~Ok is minimal. From thede nition of ~O0N , ~S 0 = ~Sj~O0N~ON (note that we introduced blockwise equivalenceand strong validity only to be able to perform this transition). ~S0 is minimal,therefore ~ON is blockwise-equivalent to ~O0N . ~O0N 2 ConsSet(N), since all itsprojections are candidates of the child nodes. Thereupon, ~ON is blockwise-equivalent to a member of ConsSet(N).So, ConsSet(N) is strongly valid. To prove that CandSet(N) is strongly valid,it su ces to show that all the members of ConsSet(N) that are not included inCandSet(N) by Algorithm 6, are either MC-contradicting or blockwise-equivalentto members of CandSet(N). Such orderings can be of three types:(a) Orderings that violate blocks of the children projections. They are MC-contradicting by Corollary 3.(b) Orderings that do not violate blocks, but where max-blocks of children pro-jections are not ordered by cn. They are MC-contradicting by Lemma 6.(c) Orderings that do not violate blocks and have them sorted by cn. For eachcombination of projections, one consistent ordering of N is retained in thecandidate set, and all the other are rejected. By Corollary 5, the rejectedorderings are blockwise-equivalent to the retained candidate.Consequently, CandSet(N) is strongly valid.92\nThe Divide-and-Conquer Subgoal-Ordering Algorithm2. N is an OR-node. Again, we start with showing that ConsSet(N) is stronglyvalid.Let ~ON 2 (S(N)) n ConsSet(N). ~ON is constructed from a binder H and a\\tail\" sequence ~T : ~ON = Hk~T . Let NH be the child of N that correspondsto the binder H . By the inductive hypothesis, CandSet(NH) is strongly valid.~T 62 CandSet(NH), since otherwise ~ON 2 ConsSet(N). Therefore, ~T is ei-ther MC-contradicting, or blockwise-equivalent to some ~T 0 2 CandSet(NH).If ~T is MC-contradicting, ~ON is MC-contradicting too (proof by contradic-tion, as for AND-nodes). If ~T is blockwise-equivalent to ~T 0, then ~ON = Hk~Tis blockwise-equivalent to Hk~T 0 2 ConsSet(N) (the proof is easy). Hence,ConsSet(N) is strongly valid. The only orderings of ConsSet(N) that are not in-cluded in CandSet(N) by the dac algorithm have cheaper permutations of theirleading max-blocks, and therefore are MC-contradicting, by Lemma 7. Hence,CandSet(N) is strongly valid. 2Corollary 7 The candidate set found by Algorithm 6 for the root node is valid.Corollary 8 Algorithm 6 nds a minimal ordering of the given set of subgoals.ReferencesAho, A. V., Hopcroft, J. E., & Ullman, J. D. (1987). Data Structures and Algorithms.Addison-Wesley.Boddy, M., & Dean, T. (1989). Solving time-dependent planning problems. In Sridha-ran, N. S. (Ed.), Proceedings of the 11th International Joint Conference on Arti cialIntelligence, pp. 979{984, Detroit, MI, USA. Morgan Kaufmann.Bol, R. N., Apt, K. R., & Klop, J. W. (1991). An analysis of loop checking mechanisms forlogic programs. Theoretical Computer Science, 86 (1), 35{79.Braem, C., Le Charlier, B., Modar, S., & Van Hentenryck, P. (1994). Cardinality Analysisof Prolog. In Bruynooghe, M. (Ed.), Logic Programming - Proceedings of the 1994International Symposium, pp. 457{471, Massachusetts Institute of Technology. TheMIT Press.Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. (1984). Classi cation andRegression Trees. Wadsworth International Group, Belmont, CA.Bruynooghe, M., De Schreye, D., & Krekels, B. (1989). Compiling control. The Journal ofLogic Programming, 6, 135{162.Clark, K. L., & McCabe, F. (1979). The control facilities of IC-Prolog. In Michie, D. (Ed.),Expert Systems in The Microelectronic Age., pp. 122{149. University of Edinburgh,Scotland.Clocksin, W. F., & Mellish, C. S. (1987). Programming in Prolog (Third edition). Springer-Verlag, New York. 93\nLedeniov & MarkovitchCohen, W. W. (1990). Learning approximate control rules of high utility. In Proceedings ofthe Seventh International Machine Learning Workshop, pp. 268{276, Austin, Texas.Morgan Kaufmann.Cormen, T. H., Leiserson, C. E., & Rivest, R. L. (1991). Introduction To Algorithms. MITPress, Cambridge, Mass.Cortesi, A., Le Charlier, B., & Rossi, S. (1997). Speci cation-based automatic veri cationof Prolog programs. In Gallagher, J. (Ed.), Proceedings of the 6th International Work-shop on Logic Program Synthesis and Transformation, Vol. 1207 of LNCS, pp. 38{57,Stockholm, Sweden. Springer-Verlag.De Boeck, P., & Le Charlier, B. (1990). Static type analysis of Prolog procedures for ensuringcorrectness. In Deransart, P., & Ma luszy nski, J. (Eds.), Programming LanguagesImplementation and Logic Programming, Vol. 456 of LNCS, pp. 222{237, Link oping,Sweden. Springer-Verlag.De Schreye, D., & Decorte, S. (1994). Termination of logic programs: The never-endingstory. The Journal of Logic Programming, 19 & 20, 199{260.Debray, S., L opez-Garc a, P., Hermenegildo, M., & Lin, N.-W. (1997). Lower bound costestimation for logic programs. In Ma luszy nski, J. (Ed.), Proceedings of the Interna-tional Symposium on Logic Programming (ILPS-97), pp. 291{306, Cambridge. MITPress.Debray, S. K., & Lin, N.-W. (1993). Cost analysis of logic programs. ACM Transactionson Programming Languages and Systems, 15 (5), 826{875.Debray, S. K., & Warren, D. S. (1988). Automatic mode inference for logic programs. TheJournal of Logic Programming, 5, 207{229.Dejong, G., & Mooney, R. (1986). Explanation-based learning: An alternative view. Ma-chine Learning, 1, 145{176.Deville, Y. (1990). Logic Programming: Systematic Program Development. InternationalSeries in Logic Programming, Addison-Wesley.Etzioni, O. (1991). STATIC: A problem-space compiler for PRODIGY. In Dean, ThomasL.; McKeown, K. (Ed.), Proceedings of the 9th National Conference on Arti cialIntelligence, pp. 533{540, Anaheim, California. MIT Press.Etzioni, O. (1993). Acquiring search-control knowledge via static analysis. Arti cial Intel-ligence, 62, 255{301.Greiner, R., & Orponen, P. (1996). Probably approximately optimal satis cing strategies.Arti cial Intelligence, 82 (1-2), 21{44.Henrard, J., & Le Charlier, B. (1992). FOLON: An environment for declarative constructionof logic programs. In Bruynooghe, M., & Wirsing, M. (Eds.), Proceedings of the FourthInternational Symposium on Programming Language Implementation and Logic Pro-gramming, Vol. 631 of LNCS, pp. 217{231, Leuven, Belgium. Springer-Verlag.94\nThe Divide-and-Conquer Subgoal-Ordering AlgorithmItai, A., & Makowsky, J. A. (1987). Uni cation as a complexity measure for logic program-ming. The Journal of Logic Programming, 4, 105{117.Knuth, D. E. (1973). The Art Of Computer Programming, Vol. 3. Addison-Wesley, Reading,Mass.Kowalski, R. A. (1979). Algorithm = Logic + Control. Communications of the ACM, 22(7),424{436.Laird, P. D. (1992). E cient dynamic optimization of logic programs. In Proceedings ofthe ML92 Workshop on Knowledge Compilation and Speedup Learning Aberdeen,Scotland.Langley, P. (1985). Learning to search: From weak methods to domain-speci c heuristics.Cognitive Science, 9, 217{260.Lavrac, N., & Dzeroski, S. (1994). Inductive Logic Programming: Techniques and Applica-tions. Arti cial Intelligence. Ellis Harwood, New York.Ledeniov, O., & Markovitch, S. (1998a). Controlled utilization of control knowledge forspeeding up logic inference. Tech. rep. CIS9812, Technion, Haifa, Israel.Ledeniov, O., & Markovitch, S. (1998b). Learning investment functions for controlling theutility of control knowledge. In Proceedings of the Fifteenth National Conference onArti cial Intelligence, pp. 463{468, Madison, Wisconsin. Morgan Kaufmann.Lenat, D. B. (1995). CYC: A large-scale investment in knowledge infrastructure. Commu-nications of the ACM, 38 (11), 33{38.Lloyd, J. W. (1987). Foundations of Logic Programming (Second edition). Springer-Verlag,Berlin.Markovitch, S., & Scott, P. D. (1989). Automatic ordering of subgoals | a machine learningapproach. In Lusk, E. L., & Overbeek, R. A. (Eds.), Proceedings of the North AmericanConference on Logic Programming, pp. 224{242, Cleveland, Ohio. MIT Press.Markovitch, S. (1989). Information Filtering: Selection Mechanisms in Learning Systems.Ph.D. thesis, EECS Department, University of Michigan.Markovitch, S., & Scott, P. D. (1993). Information ltering: Selection mechanisms inlearning systems. Machine Learning, 10, 113{151.Minker, J. (1978). Search strategy and selection function for an inferential relational system.In ACM Transactions on Database Systems, Vol. 3, pp. 1{31.Minton, S. (1988). Learning Search Control Knowledge: An Explanation-Based Approach.Kluwer, Boston, MA.Mitchell, T. M., Keller, R. M., & Kedar-Cabelli, S. T. (1986). Explanation-based general-ization: A unifying view. Machine Learning, 1, 47{80.95\nLedeniov & MarkovitchMooney, R. J., & Zelle, J. M. (1993). Combining FOIL and EBG to speed-up logic programs.In Bajcsy, R. (Ed.), Proceedings of The Thirteenth International Joint Conference forArti cial Intelligence, pp. 1106{1111, Chambery, France. Morgan Kaufmann.Morris, K. A. (1988). An algorithm for ordering subgoals in NAIL!. In Proceedings of theSeventh ACM SIGACT-SIGMOD Symposium on Principles of Database Systems, pp.82{88, Austin, TX. ACM Press, New York.Muggleton, S., & De Raedt, L. (1994). Inductive logic programming: Theory and methods.The Journal of Logic Programming, 19 & 20, 629{680.Naish, L. (1984). MU-Prolog 3.1db Reference Manual. Dept. of Computer Science, Univ.of Melbourne.Naish, L. (1985a). Automatic control for logic programs. The Journal of Logic Programming,3, 167{183.Naish, L. (1985b). Prolog control rules. In Joshi, A. (Ed.), Proceedings of the 9th Inter-national Joint Conference on Arti cial Intelligence, pp. 720{723, Los Angeles, CA.Morgan Kaufmann.Natarajan, K. S. (1987). Optimizing backtrack search for all solutions to conjunctive prob-lems. In McDermott, J. (Ed.), Proceedings of the 10th International Joint Conferenceon Arti cial Intelligence, pp. 955{958, Milan, Italy. Morgan Kaufmann.Nie, X., & Plaisted, D. A. (1990). Experimental results on subgoal ordering. In IEEETransactions On Computers, Vol. 39, pp. 845{848.Pettorossi, A., & Proietti, M. (1994). Transformation of logic programs: Foundations andtechniques. The Journal of Logic Programming, 19 & 20, 261{320.Pettorossi, A., & Proietti, M. (1996). Rules and strategies for transforming functional andlogic programs. ACM Computing Surveys, 28 (2), 360{414.Porto, A. (1984). Epilog: A language for extended programming. In Campbell, J. (Ed.),Implementations of Prolog. Ellis Harwood.Prieditis, A. E., & Mostow, J. (1987). PROLEARN: Towards a prolog interpreter thatlearns. In Forbus, Kenneth; Shrobe, H. (Ed.), Proceedings of the 6th National Con-ference on Arti cial Intelligence, pp. 494{498, Seattle, WA. Morgan Kaufmann.Quinlan, J. R. (1986). Induction of decision trees. Machine Learning, 1, 81{106.Quinlan, J. R., & Cameron-Jones, R. M. (1995). Induction of logic programs: FOIL andrelated systems. New Generation Computing, Special Issue on Inductive Logic Pro-gramming, 13 (3-4), 287{312.Simon, H. A., & Kadane, J. B. (1975). Optimal problem-solving search: All-or-none solu-tions. Arti cial Intelligence, 6, 235{247.Smith, D. E. (1989). Controlling backward inference. Arti cial Intelligence, 39 (1), 145{208.96\nThe Divide-and-Conquer Subgoal-Ordering AlgorithmSmith, D. E., & Genesereth, M. R. (1985). Ordering conjunctive queries. Arti cial Intelli-gence, 26, 171{215.Smith, D. E., Genesereth, M. R., & Ginsberg, M. L. (1986). Controlling recursive inference.Arti cial Intelligence, 30 (3), 343{389.Somogyi, Z., Henderson, F., Conway, T., Bromage, A., Dowd, T., Je ery, D., & al. (1996a).Status of the Mercury system. In Proc. of the JICSLP '96 Workshop on Parallelismand Implementation Technology for (Constraint) Logic Programming Languages, pp.207{218, Bonn, Germany.Somogyi, Z., Henderson, F., & Conway, T. (1996b). The execution algorithm of Mercury,an e cient purely declarative logic programming language. Journal of Logic Program-ming, 29 (1{3), 17{64.Sterling, L., & Shapiro, E. (1994). The Art of Prolog (Second edition). MIT Press, Cam-bridge, MA.Tamaki, H., & Sato, T. (1984). Unfold/fold transformation of logic programs. In T arnlund,S.- A. (Ed.), Proceedings of the Second International Conference on Logic Program-ming, pp. 127{138, Uppsala, Sweden.Ullman, J. D., & Vardi, M. Y. (1988). The complexity of ordering subgoals. In Proceedings ofthe Seventh ACM SIGACT-SIGMOD Symposium on Principles of Database Systems,pp. 74{81, Austin, TX. ACM Press, New York.Ullman, J. D. (1982). Principles of Database Systems. Computer Science Press, Rockville,MD.Vasak, T., & Potter, J. (1985). Metalogical control for logic programs. Journal of LogicProgramming, 2 (3), 203{220.Warren, D. H. D. (1981). E cient processing of interactive relational database queriesexpressed in logic. In Zaniola, & Delobel (Eds.), Proceedings of the 7th InternationalConference on Very Large Data Bases, pp. 272{281, Cannes, France. IEEE ComputerSociety Press. 97"
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2011,
    "abstractText" : null,
    "creator" : "dvipsk 5.58f Copyright 1986, 1994 Radical Eye Software"
  }
}