{
  "name" : "1609.07082.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Large Margin Nearest Neighbor Classification using Curved Mahalanobis Distances∗",
    "authors" : [ "Frank Nielsen", "Boris Muzellec", "Richard Nock" ],
    "emails" : [ "Frank.Nielsen@acm.org" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: classification; metric learning; Cayley-Klein metrics; LMNN; Voronoi diagrams."
    }, {
      "heading" : "1 Introduction",
      "text" : ""
    }, {
      "heading" : "1.1 Metric learning",
      "text" : "The Mahalanobis distance between point p and q of Rd is defined for a symmetric positive definite matrix Q 0 by:\ndM (p, q) = √ (p− q)>Q(p− q). (1)\nIt is a metric distance that satisfies the three metric axioms: indiscernibility (dM (p, q) = 0 iff. p = q), symmetry (dM (p, q) = dM (q, p)), and triangle inequality (dM (p, q) + dM (q, r) ≥ dM (p, r)). The Mahalanobis distance generalizes the Euclidean distance by choosing Q = I, the identity matrix: DI(p, q) = ‖p− q‖. Given a finite point set P = {x1, . . . , xn}, matrix Q is often chosen as the precision matrix Σ−1 where Σ is the covariance matrix of P:\nΣ = 1\nn ∑ i (xi − µ)(xi − µ)>,with (2)\nµ = 1\nn ∑ i xi. (3)\nµ is the center of mass of P (called sample mean in Statistics). ∗A preliminary work appeared at IEEE International Conference on Image Processing (ICIP) 2016 [16]. †École Polytechnique, France, and Sony Computer Science Laboratories, Japan. e-mail:Frank.Nielsen@acm.org ‡École Polytechnique, France. §Data61, The Australian National University (ANU) & The University of Sydney, Australia.\nar X\niv :1\n60 9.\n07 08\n2v 2\n[ cs\n.L G\n] 2\n6 Se\np 20\n16\nIn machine learning, given a labeled point set P = {(x1, y1), . . . , (xn, yn)} with yi ∈ Y denoting the label of xi ∈ X , the classification task consists in building a classifier h(·) : X 7→ Y to tag newly unlabelled points x as y = h(x). The classification task is binary when Y = {−1, 1}, otherwise it is said multi-task. A simple but powerful classifier consists in retrieving the k nearest neighbor(s) NNk(x) of an unlabeled query point x, and to associate to x the dominant label of its neighbor(s). This rule yields the so-called k-Nearest Neighbor classifier (or k-NN for short). The k-NN rule depends on the chosen distance between elements of X . When the distance is parametric like the Mahalanobis distance, one has to learn the appropriate distance parameter (eg., matrix Q for the Mahalanobis distance). This hot topic of machine learning bears the name metric learning. Weinberger et al. [26] proposed an efficient method to learn a Mahalanobis distance: The Large Margin Nearest Neighbor (LMNN) algorithm. The LMNN algorithm was further extended to elliptic Cayley-Klein geometries in [4]. In this work, we further extend the LMNN framework in hyperbolic Cayley-Klein geometries, and also consider mixed hyperbolic/elliptic Cayley-Klein distances."
    }, {
      "heading" : "1.2 Contributions and outline",
      "text" : "We summarize our key contributions as follows:\n• We extend the LMNN to hyperbolic Cayley-Klein geometries (§ 4.3),\n• We introduce a linearly mixed Cayley-Klein distance and investigate its experimental performance (§ 5.2),\n• We show that Cayley-Klein Voronoi diagrams are affine and equivalent to power diagrams (§ 3.1), and\n• We prove that Cayley-Klein balls have Mahalanobis shapes with displaced centers (§ 3.2).\nThe paper is organized as follows: Section 2 concisely introduces the basic notions of CayleyKlein geometries and present formula for the elliptic/hyperbolic Cayley-Klein distances. Those elliptic/hyperbolic Cayley-Klein distances are reinterpreted as curved Mahalanobis distances in Section 2.4. Section 3 studies some facts useful for computational geometry [10]: First, we show that the Cayley-Klein bisector is a (clipped) hyperplane, and that the Cayley-Klein Voronoi diagrams can be built from equivalent (clipped) power diagrams (Section 3.1). Second, we notice that Cayley-Klein balls have Mahalanobis shapes with displaced centers (Section 3.2). Section 4 introduces the LMNN framework: First, we review LMNN for learning a squared Mahalanobis distance in § 4.1. Then we report the extension of Bi et al. [4] to elliptic Cayley-Klein geometries, and describe our novel extension to hyperbolic Cayley-Klein geometries in § 4.3. Experimental results are presented in Section 5, and a mixed Cayley-Klein distance is considered in § 5.2 that further improve experimentally classification performance. Fast nearest neighbor queries in Cayley-Klein geometries are briefly touched upon in § 5.1. Finally, Section 6 concludes this work and hints at further perspectives of the role of Cayley-Klein distances in machine learning."
    }, {
      "heading" : "2 Cayley-Klein geometry",
      "text" : "The real projective space [22] RPd can be understood as the set of lines passing through the origin of the vector space Rd+1. Projective spaces is different from spherical geometry because antipodal\npoints of the unit sphere are identified (they yield the same line passing through the origin). Let RPd = (Rd+1\\{0})/̃ denote the real projective space with the equivalence class relation∼: (λx, λ) ∼ (x, 1) for λ 6= 0. A point x in Rd is mapped to a point x̃ ∈ RPd using homogeneous coordinates x 7→ x̃ = (x,w = 1) by adding an extra coordinate w. Conversely, a projective point x̃ ∈ Rd+1 = (x,w) is dehomogeneized by “perspective division” x̃ 7→ xw ∈ Rd provided that w 6= 0. The projective points at infinity have the coordinate w = 0. Thus the projective space is a compactification of the Euclidean space. The non-infinite points of the projective space RPd is often visualized in Rd+1 as the points lying on the hyperplane H passing through the (d + 1)-th coordinate w = 1 (with each point on H defining a line passing through the origin of Rd+1). In projective geometry, two distinct lines always intersect in exactly one point, and a bundle of Euclidean parallel lines intersect at the same projective point at infinity.\nIn projective geometry [22], the cross-ratio (Figure 1) of four collinear points p, q, P,Q on a line is defined by:\n(p, q;P,Q) = (p− P )(q −Q) (p−Q)(q − P ) . (4)\nThe cross-ratio is a measure that is invariant by projectivities [22] (see Figure 1 (a)), also called collineations or homographies. The cross-ratio enjoys the following key properties:\n• (p, p;P, P ) = 1,\n• (p, q;Q,P ) = 1(p,q;P,Q) ,\n• (p, q;P,Q) = (p, r;P,Q)× (r, q;P,Q) when r is collinear with p, q, P,Q.\nA gentle introduction to projective geometry and Cayley-Klein geometries can be found in [22, 24, 25]. We also refer the reader to a more advanced textbook [20] handling invariance and isometries, and to the historical seminal paper [7] of Cayley (1859)."
    }, {
      "heading" : "2.1 Cayley-Klein distances from cross-ratio measures",
      "text" : "A Cayley-Klein geometry is a triple K = (F , cdist, cangle), where:\n1. F is a fundamental conic,\n2. cdist ∈ C is a constant unit for measuring distances, and\n3. cangle ∈ C is constant unit for measuring angles.\nThe distance in Cayley-Klein geometries (see Figure 2) is defined by:\ndist(p, q) = cdist Log((p, q;P,Q)), (5)\nwhere P and Q are the intersection points of line l = (pq) with the fundamental conic F . Historically, the fundamental conic was called the “absolute” [7]. The logarithm function Log denotes the principal value of the complex logarithm. That is, since complex logarithm values are defined up to modulo 2πi, we define the principal value of the complex logarithm as the unique value with imaginary part lying in the range (−π, π].\nSimilarly, the angle in Cayley-Klein geometries (see Figure 2) is measured as follows:\nangle(l,m) = cangle Log((l,m;L,M)), (6)\nwhere L andM are tangent lines to the fundamental conic F passing through the intersection point p of line l and line m (see Figure 2). This formula generalizes the Laguerre formula that calculates the acute angle between two distinct real lines [22].\nThe Cayley-Klein geometries can further be extended to Hilbert projective geometries [9] by replacing the conic object F with a bounded convex subset of Rd. Interestingly, the convex objects delimiting the Hilbert geometry domain do not need to be strictly convex [8].\nThe properties of Cayley-Klein distances are:\n• Law of the indiscernibles: dist(p, q) = 0 iff. p = q,\n• Signed distances : dist(p, q) = −dist(q, p), and\n• When p, q, r are collinear, dist(p, q) = dist(p, r) + dist(r, q). That is, shortest-path geodesics1 in Cayley-Klein geometries are straight lines (clipped within the conic domain D).\n1Cayley-Klein geometries can also be studied from the viewpoint of Riemannian geometry.\nNotice that the logarithm of Cayley-Klein measurement formula is transferring multiplicative properties of the cross-ratio to additive properties of Cayley-Klein distances: For example, it follows from the cross-ratio identity (p, q;P,Q) = (p, r;P,Q) × (r, q;P,Q) for collinear p, q, P,Q that dist(p, q) = dist(p, r) + dist(r, q)."
    }, {
      "heading" : "2.2 Dual conics and taxonomy of Cayley-Klein geometries",
      "text" : "In projective geometry, points and lines are dual concepts, and theorems on points can be translated equivalently to theorems on lines. For example, Pascal’s theorem is dual to Brianchon’s theorem [22].\nA conic object F can be described as the convex hull of its extreme points (points lying on its border), or equivalently as the intersection of all half-spaces tangent at its border and fully containing the conic. This is similar to the dual H-representation and V -representation of finite convex polytopes [13] (’H’ standing for Halfspaces, and ’V’ for Vertex). This point/line duality yields a dual parameterizations of the fundamental conic F = (A,A∆) by two matrices, where A∆ = A−1|A| is the adjoint matrix (transpose of its cofactor matrix). Observe that the adjoint matrix can be computed even when A is not invertible (|A| = 0).\nTo a symmetric positive semi-definite matrix (d + 1) × (d + 1)-dimensional A, we associate a homogeneous polynomial called the quadratic form QA(x) = x̃>Ax̃. The primal conic is thus described as the set of border points CA = {p̃ ∈ RPd : QA(p̃) = 0} using matrix A, and the dual conic as the set of tangent hyperplanes C∗A = {l̃ ∈ RPd : QA∆(l̃) = 0} using the dual adjoint matrix A∆.\nThe signature of matrix is a triple (n, z, p) counting the signs of the eigenvalues (in {−1, 0,+1}) of its eigendecomposition, where n denotes the number of negative eigenvalue(s), 0 the number of null eigenvalue(s), and p the number of positive eigenvalue(s) (with n+z+p = d+1). For example, a (d + 1) × (d + 1) symmetric positive-definite matrix S 0 has signature (0, 0, d + 1), while a semi-definite rank-deficient matrix S 0 of rank r < d+ 1 has signature (0, d+ 1− r, r).\nTable 1 displays the seven types of planar Cayley-Klein geometries (induced by a pair of 3× 3 dual conic matrices (A,A∆)). All degenerate cases can be obtained as the limit of non-degenerate cases, see [22]. Another way to classify the Cayley-Klein geometries is to consider the type of measurements for distances and angles. Each type of measurement is of three kinds [22]: elliptic or hyperbolic for non-degenerate geometries or parabolic for degenerate cases. Using this classification, we obtain nine combinations for the planar Cayley-Klein geometries.\nTraditionally, hyperbolic geometry [2] considers objects inside the unit ball in the BeltramiKlein model. In that case, the fundamental conic that is the unit ball. However, using CayleyKlein geometry, complex-valued measures are also possible even when points/lines fall outside the fundamental conic. With the following choice cdist = −12 and cangle = i2 , we obtain [22] (Chapter 20):\n• A real measurement for angles when points p, q lie inside the primal conic,\n• When both points p and q lie outside the conic, with l = (pq) denoting the line passing through them:\n– A real hyperbolic measure if l does not intersect the conic,\n– A pure imaginary elliptical measure if l does not intersect the conic,\n• A complex measure (a+ ib) if one point is inside, and the other outside the conic.\nTherefore it may be convenient to use in general the module of Cayley-Klein measures to handle all those possible situations.\nIn higher dimensions [14, 22], Cayley-Klein geometries unify common space geometries (euclidean, elliptical, and hyperbolic) with other space-time geometries (Minkowskian, Galilean, de Sitter, etc.) In the remainder, we consider the non-degenerate hyperbolic Cayley-Klein geometry (signature (0, 0, d+ 1), a real conic) and the non-degenerate elliptic Cayley-Klein geometry (signature (1, 0, d), a complex conic)."
    }, {
      "heading" : "2.3 Bilinear form and formula for the hyperbolic/elliptic Cayley-Klein distances",
      "text" : "For getting real-value Cayley-Klein distances, we choose the constants as follows (with κ denoting the curvature) :\n• Elliptic (κ > 0): cdist = κ2i ,\n• Hyperbolic (κ < 0): cdist = −κ2 .\nBy introducing the bilinear form for a (d+ 1)× (d+ 1) matrix S:\nSpq = (p >, 1)>S(q, 1) = p̃>Sq̃, (7)\nwe get rid of the cross-ratio expression in distance/angle formula of Eq. 5 and Eq. 6 using [22]:\n(p, q;P,Q) = Spq +\n√ S2pq − SppSqq\nSpq − √ S2pq − SppSqq . (8)\nThus, we end up with the following equivalent expressions for the elliptic/hyperbolic CayleyKlein distances:\nHyperbolic Cayley-Klein distance. When p, q ∈ DS = {p : Spp < 0} (the hyperbolic domain), we have the following equivalent hyperbolic Cayley-Klein distances:\ndH(p, q) = − κ\n2 log\nSpq + √ S2pq − SppSqq\nSpq − √ S2pq − SppSqq  , (9) dH(p, q) = −κ arctanh (√ 1− SppSqq\nS2pq\n) , (10)\ndH(p, q) = −κ arccosh (\nSpq√ SppSqq\n) , (11)\nwhere arccosh(x) = log(x+ √ x2 − 1) and arctanh(x) = 12 log 1+x1−x .\nElliptic Cayley-Klein distance. When p, q ∈ Rd+1, we have the following equivalent elliptic Cayley-Klein distances:\ndE(p, q) = κ\n2i Log\nSpq + √ S2pq − SppSqq\nSpq − √ S2pq − SppSqq  , (12) dE(p, q) = κ arccos ( Spq√ SppSqq ) . (13)\nNotice that dE(p, q) < κπ, and that p and q always belong to the domain DS = Rd in the case of elliptic geometry. The link between the principal logarithm of Eq. 5 and the arccos function of Eq. 13 is explained by the following identity: Log(x) = 2i arccos ( x+1 2 √ x ) .\nSince the elliptic/hyperbolic case is induced by the signature of matrix S, we shall denote generically by dS the Cayley-Klein distance in either the elliptic or hyperbolic case.\nThose elliptic/hyperbolic distances can be interpreted from projections [22, 18], as depicted in Figure 3.\nIt is somehow surprising that we can derive metric structures from projective geometry. Arthur Cayley (1821-1895), a British mathematician, said “Projective geometry is all geometry”."
    }, {
      "heading" : "2.4 Cayley-Klein elliptic/hyperbolic distances: Curved Malahanobis distances",
      "text" : "Bi et. al [4] rewrote the bilinear form as follows: Let\nS = [ Σ a a> b ] = SΣ,a,b, (14)\nwith Σ 0 a d× d-dimensional matrix and a, b ∈ Rd so that:\nSp,q = p̃ >Sq̃ = p>Σq + p>a+ a>q + b. (15)\nLet µ = −Σ−1a ∈ Rd (so that a = −Σµ) and b = µ>Σµ+ sign(κ) 1 κ2 so that:\nκ = { (b− µ>µ)− 12 b > µ>µ −(µ>µ− b)− 12 b < µ>µ\n(16)\nThen the bilinear form can be rewritten as:\nS(p, q) = SΣ,µ,κ(p, q) = (p− µ)>Σ(q − µ) + sign(κ) 1\nκ2 . (17)\nFurthermore, it is proved in [4] that:\nlim κ→0+ DΣ,µ,κ(p, q) = lim κ→0− DΣ,µ,κ(p, q) = DΣ(p, q) (18)\nTherefore the hyperbolic/elliptic Cayley-Klein distances can be interpreted as curved Mahalanobis distances (or κ-Mahalanobis distances). Indeed, we choose to term those hyperbolic/elliptic Cayley-Klein distances “curved Mahalanobis distances” to constrast with the fact that (squared) Mahalanobis distances are symmetric Bregman divergences that induce a (self-dual) flat geometry in information geometry [1].\nNotice that when S = diag(1, 1, ..., 1,−1), we recover the canonical hyperbolic distance [17] in Cayley-Klein model:\nDh(p, q) = arccosh\n( 1− 〈p, q〉√\n1− 〈p, p〉 √ 1− 〈q, q〉\n) , (19)\ndefined inside the interior of a unit ball since we have:\nSpq = ( p 1 )>( I 0 0 −1 )( q 1 ) = p>Iq − 1 = p>q − 1. (20)"
    }, {
      "heading" : "3 Computational geometry in Cayley-Klein geometries",
      "text" : ""
    }, {
      "heading" : "3.1 Cayley-Klein Voronoi diagrams",
      "text" : "Define the bisector Bi(p, q) of points p and q as:\nBi(p, q) = {x ∈ DS : distS(p, x) = distS(x, q)}. (21)\nThen it comes that the bisector is a hyperplane (eventually clipped to the domain D) with equation: 〈\nx, √ |S(p, p)|Σq − √ |S(q, q)|Σp 〉 + √ |S(p, p)|(a>(q + x) + b)− √ |S(q, q)|(a>(p+ x) + b) = 0 (22)\nFigure 4 displays two examples of the bisectors of two points in planar hyperbolic Cayley-Klein geometry.\nThus the Cayley-Klein Voronoi diagram is an affine diagram. Therefore the Cayley-Klein Voronoi diagram can be computed as an equivalent (clipped) power diagram [15, 5, 17], using the following conversion formula:\nci = Σpi + a 2 √ Spipi , (23) r2i = ‖Σpi + a‖2\n4Spipi + a>pi + b√ Spipi , (24)\nwhere Bi = (ci, ri) is the equivalent ball of point pi ∈ P. More precisely, let B = {Bi = (ci, ri) : i ∈ [n]} denote the set of associated balls of P. Then the Cayley-Klein Voronoi diagram VorCKS (P) of P amounts to the intersection of the power Voronoi diagram VorPow(B) of equivalent balls clipped to the domain D:\nVorCKS (P) = VorPow(B) ∩ DS . (25)\nFigure 5 and a short online video2 illustrates the Cayley-Klein Voronoi diagrams."
    }, {
      "heading" : "3.2 Cayley-Klein balls have Mahalanobis shapes with displaced centers",
      "text" : "A Cayley-Klein ball B of center c and radius r is defined by:\nBCK(c, r) = {x : dCK(x, c) ≤ r}. (26)\nThe Cayley-Klein sphere S = ∂BCK has equation dCK(x, c) = r. Figure 6 shows Cayley-Klein spheres in the elliptic case (red), and in the hyperbolic case (green) at different center positions (but for fixed elliptic and hyperbolic geometries). For comparison, the Mahalanobis spheres are displayed (blue): This drawing let us visualize the anisotropy of CayleyKlein spheres that have shape depending on the center location, while Mahalanobis spheres have identical shapes everywhere (isotropy).\nIt can be noticed in Figure 6 that Cayley-Klein balls have Mahalanobis ball shapes with displaced centers. We shall give the corresponding conversion formula. Let\n(x− c′)>Σ′(x− c′) = r′2, (27)\ndenote the equation of a Mahalanobis sphere of center c′, radius r′, and shape Σ′ 0. Then a hyperbolic/elliptic sphere can be interpreted as a Mahalanobis sphere as follows:\nHyperbolic Cayley-Klein sphere case:\nΣ′ = aa> − r̃2Σ c′ = Σ′−1(r̃2a− b′a′) r′2 = r̃2b− b′2 + 〈c′, c′〉Σ′ with r̃ =\n√ Sc,ccosh(r)\na′ = Σc+ a\nb′ = a>c+ b\n2https://www.youtube.com/watch?v=YHJLq3-RL58\nElliptic Cayley-Klein sphere case:\nΣ′ = r̃2Σ− aa>\nc′ = Σ′−1(b′a′ − r̃2a) r′2 = b′2 − r̃2b+ 〈c′, c′〉Σ′\nwith r̃ =\n√ Sc,c cos(r)\na′ = Σc+ a\nb′ = a>c+ b\nFurthermore, by using the Cholesky decomposition of Σ = LL> = Σ> = L>L, a Mahalanobis sphere can be interpreted as an ordinary Euclidean sphere after performing an affine transformation xL ← Lx.\n(x− c′)>Σ′(x− c′) = r′2, (28) (L(x− c′))>(L(x− c′)) = r′2, (29)\n(xL − c′L)>(xL − c′L) = r′ 2 , (30)\n‖xL − c′L‖2 = r′. (31)"
    }, {
      "heading" : "4 Learning curved Mahalanobis metrics",
      "text" : "Supervised learning techniques rely on labelled information, or at least on side information based on similarities/dissimilarities. In the technique called Mahalanobis Metric for Clustering (MMC) [27], Xing and al. use pairwise information to learn a global Mahalanobis metric. Given two sets S and D of input describing respectively the pairs of points that are similar to each other (eg., share the same label) and the pairs which are dissimilar (eg., have different labels), Xing and al. [27] learn a matrix M 0 by gradient descent such that the total pairwise distance in D in maximized, while keeping the total pairwise distance in S constant. While good performances are experimentally obtained, this MMC method tends to cluster similar points together and may thus perform poorly in the case of multi-modal data. Furthermore, MMC requires two computationally costly projections at each gradient step: One projection on the cone of positive semi-definite matrices, and the other projection on the set of constraints.\nLMNN [26] on the other hand is a projection-free metric learning method. LMNN learns a global Mahalanobis metric using triplet information: For each point, we take as input a set of k target neighbors which should be brought close by the learned metric, while enforcing a unit margin with respect to points which are differently labelled. Contrary to MMC, LMNN handles well multimodal data, but would optimally require oracle information of which points should be considered as targets of a given point. In practice, this is achieved by computing for each point the list of its k nearest neighbors according to euclidean distance beforehand, but in specific applications the “point neighborhoods” can be gained using additional structural properties of the problems at hand.\nWhile we consider in the remainder the LMNN framework, another more flexible approach in metric learning consists in learning local metrics, which allow to obtain a non-linear pseudo-metric while staying in a Mahalanobis framework3, at the cost of greatly amplifying spatial complexity. Therefore, most works on the subject try to obtain a sparse encoding of such metrics. For example, in [12], Fetaya and Ullman learn one Mahalanobis metric per data point using only negative examples (eg., only information on dissimilarity), and obtain sparse metrics thanks to an equivalence with Support Vector Machines (SVMs). In [23], Shi et. al. sparsely combine low-rank (one-dimensional) local metrics into a global metric. For a comprehensive survey on local metric learning, we refer the reader to [21]."
    }, {
      "heading" : "4.1 Large Margin Nearest Neighbors (LMNN)",
      "text" : "Given a labeled input data-set P = {(x1, y1), . . . , (x1, y1)} of n points x1, . . . , xn of Rd, the Large Margin Nearest Neighbors4 (LMNN) [26] learns a Mahalanobis distance (ie., matrix M 0). Since the k-NN classification does not change by taking any monotonically increasing function of the base distance (like its square), it is often more convenient mathematically to use the squared Mahalanobis distance that get rid of the square root. However, the squared Mahalanobis distance does not satisfy the triangle inequality. (It is a Bregman divergence [3, 15, 5].)\nIn LMNN, for each point, we take as input the set of k target neighbors which should be brought close by the learned metric, while enforcing a unit margin with respect to points which have different labels.\nTo define the objective cost function [26] in LMNN, we consider two sets S and R, or target neighbors and impostors:\n• Distance of each point to its target neighbors shrink, pull(L):\nS = {(xi, xj) : yi = yj and xj ∈ N(xj)}, (32) where N(x) denotes the neighbors of point x.\n• Keep a distance margin of each point to its impostors, push(L):\nR = {(xi, xj , xl) : (xi, xj) ∈ S and yi 6= yl} (33) 3In Riemannian geometry, the distance is a geodesic length L(γ) = ∫ b a √ gγ(t)(γ̇(t), γ̇(t))dt that can be interpreted\nas locally integrating Mahalanobis infinitesimal distances: L(γ) = ∫ b a Dg(γ(t))(γ̇(t), γ̇(t))dt for a metric tensor g.\n4http://www.cs.cornell.edu/~kilian/code/lmnn/lmnn.html\nUsing Cholesky decomposition M = L>L 0, the LMNN cost function [26] is then defined as:\npull(L) = Σi,i→j‖L(xi − xj)‖2, (34) push(L) = Σi,i→jΣj(1− yil) [ 1 + ‖L(xi − xj)‖2 − ‖L(xi − xl)‖2 ] + , (35)\n(L) = (1− µ) pull(L) + µ push(L), (36)\nwhere [x]+ = max(0, x) and µ is a trade-off parameter for tuning target/impostor relative importance, and i→ j indicates that xj is a target neighbor of xi. We define yil = 1 if and only if xi and xj have same label, yil = 0 otherwise.\nThus the training of the Mahalanobis matrix M = L>L is done by minimizing a linear combination of a pull function which brings points closer to their target neighbors with a push function that keeps the impostors away by penalizing the violation of the margin with a hinge loss.\nThe LMNN cost function is convex and piecewise linear [26]. Replacing the hinge loss by slack variables, we obtain a semidefinite program, which allows us to solve the minimization problem with standard solver packages.\nInstead, Weinberger and Saul [26] propose a gradient descent where the set of impostors is re-computed every 10 to 20 iterations.\nIn our implementation, we optimize the cost function by gradient descent:\n(Lt+1) = (Lt)− γ ∂ (Lt)\n∂L , (37)\nwhere γ > 0 is the learning rate, and:\n∂ ∂L = (1− µ)Σi,i→jCij + µΣ(i,j,l)∈Rt(Cij − Cil) (38)\nwith Cij = (xi − xj)>(xi − xj). LMNN is a projection-free metric learning method that is quite easy to implement. There is no projection mechanism like for the Mahalanobis Metric for Clustering (MMC) [27] method. We shall now consider extensions of the LMNNmethod to Cayley-Klein elliptic [4] and hyperbolic geometries."
    }, {
      "heading" : "4.2 Elliptic Cayley-Klein LMNN",
      "text" : "Bi et al. [4] consider the extension of LMNN to the case of elliptic Cayley-Klein geometry. The cost function is defined as:\n(L) = (1− µ) ∑ i,i→j dE(xi, xj) + µ ∑ i,i→j ∑ l (1− yil)ζijl (39)\nwith ζijl = [1 + dE(xi, xj)− dE(xi, xl)]+ . (40)\nThe gradient5 with respect to lower triangular matrix L is computed as:\n∂ (L)\n∂L = (1− µ) ∑ i,i→j ∂dE(xi, xj) ∂L + µ ∑ i,i→j ∑ l (1− yil) ∂ζijl ∂L , (41)\n5There is minor error in the expression of ∂ (L) ∂L in the original paper of Bi et al. [4], as Cij +Cji was replaced by 2Cij , which cannot be the distance gradient that must be symmetric with respect to xi and xj .\nwith Cij = (x>i , 1) >(x>j , 1).\nThe gradient terms of Eq. 41 are calculated as follows:\n∂dE(xi, xj)\n∂L = k√ SiiSjj − S2ij L ( Sij Sii Cii + Sij Sjj Cjj − (Cij + Cji) )\n(42)\n∂ζijl ∂L =\n{ ∂dE(xi,xj)\n∂L − ∂dE(xi,xl) ∂L , if ζijl ≥ 0, 0, otherwise.\n(43)\nThe elliptic LMNN loss is not convex, and thus the performance of the algorithm greatly depends on the chosen initialization for M = L>L. We may initialize the elliptic CK-LMNN either by the sample mean m = 1n ∑ i xi of the point set P, and either the precision matrix (inverse covariance matrix) of P or the matrix obtained by Mahalanobis-LMNN. We then build initial matrix S as follows:\nG+ =\n( Σ −Σm\n−m>Σ m>Σm+ 1 κ2\n) . (44)\nSuch a matrix is called a generalized Mahalanobis matrix in [4]. We term them curved Mahalanobis matrices.\nNote that elliptic Cayley-Klein geometry are defined on the full domain Rd, and furthermore the elliptic distance is bounded (by π when κ = 1)."
    }, {
      "heading" : "4.3 Hyperbolic Cayley-Klein LMNN",
      "text" : "To ensure that the (d+1)× (d+1)-dimensional matrix S keeps the correct signature (1, 0, d) during the LMNN gradient descent, we decompose S = L>DL (with L 0) and perform a gradient descent on L with the following gradient:\n∂dH(xi, xj)\n∂L = k√ S2ij − SiiSjj DL ( Sij Sii Cii + Sij Sjj Cjj − (Cij + Cji) ) . (45)\nWe initialize L = ( L′\n1\n) and D so that P ∈ DS as follows: Let Σ−1 = L′>L′ (eg., by taking\nprecision matrix Σ−1 of P), and then choose the diagonal matrix as:\nD =  −1 . . . −1\nκ maxx ‖L′x‖2  , (46) with κ > 1.\nLet DSt denote the domain at a given iteration t induced by the bilinear form St. It may happen that the point set P 6∈ DSt since we do not know the optimal learning rate γ beforehand, and thus might have overshoot the domain. When this case happens, we reduce γ ← γ2 , otherwise when the point set P is fully contained inside the real conic domain, we let γ ← 1.01γ.\nLike in the elliptic case, we initialize the hyperbolic CK-LMNN either by calculating the sample mean m = 1n ∑ i xi of the point set P, and either the precision matrix of P or the matrix obtained by Mahalanobis-LMNN. We then build initial matrix S as follows:\nG− =\n( Σ −Σm\n−m>Σ m>Σm− 1 κ2\n) . (47)\nFigure 7 displays a hyperbolic Cayley-Klein Voronoi diagram for a set of 8 generators (with labels ±1 displayed in blue/red), and the bichromatic Voronoi diagram in case of binary classification. Notice that the decision frontier of the nearest-neighbor classifier (k = 1) is the union of Voronoi facets (in 2D, edges) supporting different label cells. A similar result holds for the Cayley-Klein kNN classifier: Its decision boundary is piecewise linear since the bisectors are (clipped) hyperplanes."
    }, {
      "heading" : "5 Experimental results",
      "text" : "We report on our experimental results on some UCI data-sets.6 Descriptions of those labelled data-sets are concisely summarized in Table 3.\nWe performed k = 3 nearest neighbor classification. As in [4], we performed leave-one-out cross validation for the wine data-set, whereas for balance, pima and vowel data-sets, we trained the model on random subsets of size 250, testing it on the remaining data and repeating this procedure 10 times.\nWe observe that the elliptic CK-LMNN performs quite better than the Mahalanobis LMNN and the hyperbolic CK-LMNN."
    }, {
      "heading" : "5.1 Spectral decomposition and proximity queries in Cayley-Klein geometry",
      "text" : "To avoid to compute dE or dH for arbitrary matrix S, we apply the matrix factorization (elliptic case S = L>L, or hyperbolic case S = L>DL ) and perform coordinate changes so that it is enough\n6https://archive.ics.uci.edu/ml/datasets.html\nTable 2: Characteristics of the UCI data-sets.\nData-set # Data points # Attributes # Classes Wine 178 13 3 Sonar 208 60 2 Vowel 528 10 11 Balance 625 4 3 Pima 768 8 2\nTable 3: UCI data-sets chosen for the experiments.\nto consider the canonical metric distances:\ndE(x ′, y′) = arccos ( 〈x′, y′〉 ‖x′‖‖y′‖ ) , (48)\ndH(x ′, y′) = arccosh ( 1− 〈x′, y′〉√\n1− 〈x′, x′〉 √ 1− 〈y′, y′〉\n) . (49)\nAlternatively, consider the spectral decomposition of matrix S = OΛO> obtained by eigenvalue decomposition (with diagonal matrix Λ = diag(Λ1,1, . . . ,Λd+1,d+1)), and let us write canonically:\nS = OD 1 2 [ I 0 0 λ ] D 1 2O>, (50)\nwhere λ =∈ {−1, 1} and O is an orthogonal matrix (with O−1 = O>). The diagonal matrix D has\nall positive values, with Di,i = Λi,i and Dd+1,d+1 = |Λd+1,d+1| so that D 1 2 is defined as the diagonal matrix obtained by taking element-wise the square root values of the matrix.\nWe rewrite the bilinear form into a canonical form by mapping the points x to x̃′ = D 1 2O> ( x 1 ) =(\nx′′\nw\n) . Since x̃′ = ( x′\n1\n) , we can then find x′ = x ′′\nw . When λ > 0 (elliptical case with Dd+1,d+1 > 0),\nwe have SS(p, q) = SE(p′, q′) = SI(p′, q′). When λ < 0 (hyperbolic case with Dd+1,d+1 < 0), we have SS(p, q) = SH(p′, q′), with H = diag(1, ..., 1,−1) the canonical matrix form for hyperbolic Cayley-Klein spaces.\nNotice that in the ordinary Mahalanobis case, instead of using the Cholesky decomposition, we may also use the L1DL>1 matrix decomposition where L1 is a unit lower triangular matrix (with diagonal elements all 1), and D is a diagonal matrix of positive elements. The mapping is then x′ = D\n1 2L>1 or x′ = (L1D 1 2 )> since D = D>. Thus by transforming the input space into one of the\ncanonical Euclidean/elliptical/hyperbolic spaces, we avoid to perform costly matrix multiplications required in the general bilinear form, and once the structure (say, a k-NN decision boundary or a Voronoi diagram) has been recovered, we can map back to the original space (say, for classifying new observations using the original coordinate system).\nNearest neighbor proximity queries can then be answered using various spatial data-structures. For example, we may consider the Vantage Point Tree data-structures [28, 19].\nIn small dimensions, we can compute the k-order elliptic/hyperbolic Voronoi affine diagram, as depicted in Figure 8. The k-order Voronoi diagram is affine since the bisectors are affine. Neighbor queries can then be reported efficiently in logarithmic time in 2D after preprocessing time, see [10] for further details."
    }, {
      "heading" : "5.2 Mixed curved Mahalanobis distance",
      "text" : "We consider the mixed elliptic/hyperbolic Cayley-Klein distance:\nd(x, y) = αdE(x, y) + (1− α)dH(x, y). (51)\nSince the sum of (Riemannian) metric distances is a (Riemannian) metric distance, we deduce that d(x, y) is a (Riemannian) metric distance. However, this “blending” of positive with negative constant curvature (Riemannian) geometries does not yield a constant curvature (Riemannian) geometry. Indeed, although that the metric tensors blend locally, the Ordinary Differential Equation (ODE) characterizing the geodesics solves differently.\nNotice that we mix a bounded distance (elliptic CK) with an unbounded distance (hyperbolic CK) via the hyperparameter α that needs to be tuned. Table 5 shows the preliminary experimental results. Those results indicate better performance for the mixed model in most (but not all) cases. This should not be surprising as a smooth non-constant Riemannian manifold will better model data-sets than a constant-curvature manifold."
    }, {
      "heading" : "6 Conclusion and perspectives",
      "text" : "We considered Cayley-Klein geometries for super-vised classification purposes in machine learning. First, we studied some nice properties of the Voronoi diagrams and balls in Cayley-Klein geometries: We proved that the Cayley-Klein Voronoi diagram is affine, and reported formula to build it as an\nequivalent (clipped) power diagram. We then showed that Cayley-Klein balls have Mahalanobis shapes with displaced centers, and gave the explicit conversion formula. Second, we extended the LMNN framework to hyperbolic Cayley-Klein geometries that were not considered in [4], and proposed learning a mixed elliptic/hyperbolic distance that experimentally shows good improvement over constant-curvature Cayley-Klein geometries.\nThe fact that the Cayley-Klein bisectors are hyperplanes offers nice computational perspectives in machine learning and computational geometry. For example, it would be interesting to study Multi-Dimensional Scaling [11] or Support Vector Machines (SVMs) in Cayley-Klein geometries, or to mesh anisotropically [6] in Cayley-Klein geometries.\nSupplemental information is available online at: https://www.lix.polytechnique.fr/~nielsen/CayleyKlein/"
    } ],
    "references" : [ {
      "title" : "Information Geometry and Its Applications. Applied Mathematical Sciences",
      "author" : [ "S. Amari" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2016
    }, {
      "title" : "Clustering with Bregman divergences",
      "author" : [ "Arindam Banerjee", "Srujana Merugu", "Inderjit S Dhillon", "Joydeep Ghosh" ],
      "venue" : "Journal of machine learning research,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2005
    }, {
      "title" : "Beyond Mahalanobis metric: Cayley-Klein metric learning",
      "author" : [ "Yanhong Bi", "Bin Fan", "Fuchao Wu" ],
      "venue" : "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2015
    }, {
      "title" : "Anisotropic Delaunay mesh generation",
      "author" : [ "Jean-Daniel Boissonnat", "Camille Wormser", "Mariette Yvinec" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2015
    }, {
      "title" : "Hilbert geometry for convex polygonal domains",
      "author" : [ "Bruno Colbois", "Constantin Vernicos", "Patrick Verovic" ],
      "venue" : "Journal of Geometry,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "Hilbert geometry for strictly convex domains",
      "author" : [ "Bruno Colbois", "Patrick Verovic" ],
      "venue" : "Geometriae Dedicata,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2004
    }, {
      "title" : "Foundations of multi-dimensional metric scaling in Cayley-Klein geometries",
      "author" : [ "Jan Drösler" ],
      "venue" : "British Journal of Mathematical and Statistical Psychology,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1979
    }, {
      "title" : "Learning local invariant mahalanobis distances",
      "author" : [ "E. Fetaya", "S. Ullman" ],
      "venue" : "International Conference on Machine Learning (ICML),",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2015
    }, {
      "title" : "Convex Polytopes, volume 221",
      "author" : [ "Branko Grünbaum" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2013
    }, {
      "title" : "Geometry, Kinematics, and Rigid Body Mechanics in Cayley-Klein Geometries",
      "author" : [ "C. Gunn" ],
      "venue" : "PhD thesis, Technische Universität Berlin,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2011
    }, {
      "title" : "On Bregman Voronoi diagrams",
      "author" : [ "Frank Nielsen", "Jean-Daniel Boissonnat", "Richard Nock" ],
      "venue" : "In Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2007
    }, {
      "title" : "Classification with mixtures of curved Mahalanobis metrics",
      "author" : [ "Frank Nielsen", "Boris Muzellec", "Richard Nock" ],
      "venue" : "In IEEE International Conference on Image Processing (ICIP),",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2016
    }, {
      "title" : "Hyperbolic Voronoi diagrams made easy",
      "author" : [ "Frank Nielsen", "Richard Nock" ],
      "venue" : "In IEEE International Conference on Computational Science and Its Applications (ICCSA),",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2010
    }, {
      "title" : "Further results on the hyperbolic Voronoi diagrams",
      "author" : [ "Frank Nielsen", "Richard Nock" ],
      "venue" : "CoRR, abs/1410.1036,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2014
    }, {
      "title" : "Bregman vantage point trees for efficient nearest neighbor queries",
      "author" : [ "Frank Nielsen", "Paolo Piro", "Michel Barlaud" ],
      "venue" : "In IEEE International Conference on Multimedia and Expo,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2009
    }, {
      "title" : "Projective and Cayley-Klein Geometries",
      "author" : [ "Arkadij L Onishchik", "Rolf Sulanke" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2006
    }, {
      "title" : "Local distance functions: A taxonomy, new algorithms, and an evaluation",
      "author" : [ "D. Ramanan", "S. Baker" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2011
    }, {
      "title" : "Perspectives on Projective Geometry: A Guided Tour Through Real and Complex",
      "author" : [ "Jürgen Richter-Gebert" ],
      "venue" : null,
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2011
    }, {
      "title" : "Sparse compositional metric learning",
      "author" : [ "Yuan Shi", "Aurélien Bellet", "Fei Sha" ],
      "venue" : "In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2014
    }, {
      "title" : "Projective spaces with Cayley-Klein metrics",
      "author" : [ "Horst Struve", "Rolf Struve" ],
      "venue" : "Journal of Geometry,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2004
    }, {
      "title" : "Non-euclidean geometries: the Cayley-Klein approach",
      "author" : [ "Horst Struve", "Rolf Struve" ],
      "venue" : "Journal of Geometry,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2010
    }, {
      "title" : "Distance metric learning for large margin nearest neighbor classification",
      "author" : [ "Kilian Q. Weinberger", "John Blitzer", "Lawrence K. Saul" ],
      "venue" : null,
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2006
    }, {
      "title" : "Distance metric learning, with application to clustering with side-information",
      "author" : [ "Eric P. Xing", "Andrew Y. Ng", "Michael I. Jordan", "Stuart Russell" ],
      "venue" : "In Advances in neural information processing",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2003
    }, {
      "title" : "Data structures and algorithms for nearest neighbor search in general metric spaces",
      "author" : [ "Peter N. Yianilos" ],
      "venue" : "In Proceedings of the Fourth Annual ACM-SIAM Symposium on Discrete Algorithms,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 1993
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "∗A preliminary work appeared at IEEE International Conference on Image Processing (ICIP) 2016 [16].",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 21,
      "context" : "[26] proposed an efficient method to learn a Mahalanobis distance: The Large Margin Nearest Neighbor (LMNN) algorithm.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 2,
      "context" : "The LMNN algorithm was further extended to elliptic Cayley-Klein geometries in [4].",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 2,
      "context" : "[4] to elliptic Cayley-Klein geometries, and describe our novel extension to hyperbolic Cayley-Klein geometries in § 4.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 17,
      "context" : "2 Cayley-Klein geometry The real projective space [22] RP can be understood as the set of lines passing through the origin of the vector space Rd+1.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 17,
      "context" : "In projective geometry [22], the cross-ratio (Figure 1) of four collinear points p, q, P,Q on a line is defined by:",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 17,
      "context" : "The cross-ratio is a measure that is invariant by projectivities [22] (see Figure 1 (a)), also called collineations or homographies.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 17,
      "context" : "A gentle introduction to projective geometry and Cayley-Klein geometries can be found in [22, 24, 25].",
      "startOffset" : 89,
      "endOffset" : 101
    }, {
      "referenceID" : 19,
      "context" : "A gentle introduction to projective geometry and Cayley-Klein geometries can be found in [22, 24, 25].",
      "startOffset" : 89,
      "endOffset" : 101
    }, {
      "referenceID" : 20,
      "context" : "A gentle introduction to projective geometry and Cayley-Klein geometries can be found in [22, 24, 25].",
      "startOffset" : 89,
      "endOffset" : 101
    }, {
      "referenceID" : 15,
      "context" : "We also refer the reader to a more advanced textbook [20] handling invariance and isometries, and to the historical seminal paper [7] of Cayley (1859).",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 17,
      "context" : "This formula generalizes the Laguerre formula that calculates the acute angle between two distinct real lines [22].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 5,
      "context" : "The Cayley-Klein geometries can further be extended to Hilbert projective geometries [9] by replacing the conic object F with a bounded convex subset of Rd.",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 4,
      "context" : "Interestingly, the convex objects delimiting the Hilbert geometry domain do not need to be strictly convex [8].",
      "startOffset" : 107,
      "endOffset" : 110
    }, {
      "referenceID" : 17,
      "context" : "For example, Pascal’s theorem is dual to Brianchon’s theorem [22].",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 8,
      "context" : "This is similar to the dual H-representation and V -representation of finite convex polytopes [13] (’H’ standing for Halfspaces, and ’V’ for Vertex).",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 17,
      "context" : "All degenerate cases can be obtained as the limit of non-degenerate cases, see [22].",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 17,
      "context" : "Each type of measurement is of three kinds [22]: elliptic or hyperbolic for non-degenerate geometries or parabolic for degenerate cases.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 17,
      "context" : "With the following choice cdist = − 2 and cangle = i 2 , we obtain [22] (Chapter 20):",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 9,
      "context" : "In higher dimensions [14, 22], Cayley-Klein geometries unify common space geometries (euclidean, elliptical, and hyperbolic) with other space-time geometries (Minkowskian, Galilean, de Sitter, etc.",
      "startOffset" : 21,
      "endOffset" : 29
    }, {
      "referenceID" : 17,
      "context" : "In higher dimensions [14, 22], Cayley-Klein geometries unify common space geometries (euclidean, elliptical, and hyperbolic) with other space-time geometries (Minkowskian, Galilean, de Sitter, etc.",
      "startOffset" : 21,
      "endOffset" : 29
    }, {
      "referenceID" : 17,
      "context" : "6 using [22]:",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 17,
      "context" : "Those elliptic/hyperbolic distances can be interpreted from projections [22, 18], as depicted in Figure 3.",
      "startOffset" : 72,
      "endOffset" : 80
    }, {
      "referenceID" : 13,
      "context" : "Those elliptic/hyperbolic distances can be interpreted from projections [22, 18], as depicted in Figure 3.",
      "startOffset" : 72,
      "endOffset" : 80
    }, {
      "referenceID" : 2,
      "context" : "al [4] rewrote the bilinear form as follows: Let",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 2,
      "context" : "Furthermore, it is proved in [4] that:",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 0,
      "context" : "Indeed, we choose to term those hyperbolic/elliptic Cayley-Klein distances “curved Mahalanobis distances” to constrast with the fact that (squared) Mahalanobis distances are symmetric Bregman divergences that induce a (self-dual) flat geometry in information geometry [1].",
      "startOffset" : 268,
      "endOffset" : 271
    }, {
      "referenceID" : 12,
      "context" : ", 1,−1), we recover the canonical hyperbolic distance [17] in Cayley-Klein model:",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 10,
      "context" : "Therefore the Cayley-Klein Voronoi diagram can be computed as an equivalent (clipped) power diagram [15, 5, 17], using the following conversion formula:",
      "startOffset" : 100,
      "endOffset" : 111
    }, {
      "referenceID" : 12,
      "context" : "Therefore the Cayley-Klein Voronoi diagram can be computed as an equivalent (clipped) power diagram [15, 5, 17], using the following conversion formula:",
      "startOffset" : 100,
      "endOffset" : 111
    }, {
      "referenceID" : 22,
      "context" : "In the technique called Mahalanobis Metric for Clustering (MMC) [27], Xing and al.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 22,
      "context" : "[27] learn a matrix M 0 by gradient descent such that the total pairwise distance in D in maximized, while keeping the total pairwise distance in S constant.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "LMNN [26] on the other hand is a projection-free metric learning method.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 7,
      "context" : "For example, in [12], Fetaya and Ullman learn one Mahalanobis metric per data point using only negative examples (eg.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 18,
      "context" : "In [23], Shi et.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 16,
      "context" : "For a comprehensive survey on local metric learning, we refer the reader to [21].",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 21,
      "context" : ", xn of Rd, the Large Margin Nearest Neighbors4 (LMNN) [26] learns a Mahalanobis distance (ie.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 1,
      "context" : "(It is a Bregman divergence [3, 15, 5].",
      "startOffset" : 28,
      "endOffset" : 38
    }, {
      "referenceID" : 10,
      "context" : "(It is a Bregman divergence [3, 15, 5].",
      "startOffset" : 28,
      "endOffset" : 38
    }, {
      "referenceID" : 21,
      "context" : "To define the objective cost function [26] in LMNN, we consider two sets S and R, or target neighbors and impostors:",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 21,
      "context" : "Using Cholesky decomposition M = L>L 0, the LMNN cost function [26] is then defined as: pull(L) = Σi,i→j‖L(xi − xj)‖, (34) push(L) = Σi,i→jΣj(1− yil) [ 1 + ‖L(xi − xj)‖ − ‖L(xi − xl)‖ ] + , (35) (L) = (1− μ) pull(L) + μ push(L), (36) where [x]+ = max(0, x) and μ is a trade-off parameter for tuning target/impostor relative importance, and i→ j indicates that xj is a target neighbor of xi.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 21,
      "context" : "The LMNN cost function is convex and piecewise linear [26].",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 21,
      "context" : "Instead, Weinberger and Saul [26] propose a gradient descent where the set of impostors is re-computed every 10 to 20 iterations.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 22,
      "context" : "There is no projection mechanism like for the Mahalanobis Metric for Clustering (MMC) [27] method.",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 2,
      "context" : "We shall now consider extensions of the LMNNmethod to Cayley-Klein elliptic [4] and hyperbolic geometries.",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 2,
      "context" : "[4] consider the extension of LMNN to the case of elliptic Cayley-Klein geometry.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[4], as Cij +Cji was replaced by 2Cij , which cannot be the distance gradient that must be symmetric with respect to xi and xj .",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "Such a matrix is called a generalized Mahalanobis matrix in [4].",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 2,
      "context" : "As in [4], we performed leave-one-out cross validation for the wine data-set, whereas for balance, pima and vowel data-sets, we trained the model on random subsets of size 250, testing it on the remaining data and repeating this procedure 10 times.",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 23,
      "context" : "For example, we may consider the Vantage Point Tree data-structures [28, 19].",
      "startOffset" : 68,
      "endOffset" : 76
    }, {
      "referenceID" : 14,
      "context" : "For example, we may consider the Vantage Point Tree data-structures [28, 19].",
      "startOffset" : 68,
      "endOffset" : 76
    }, {
      "referenceID" : 2,
      "context" : "Second, we extended the LMNN framework to hyperbolic Cayley-Klein geometries that were not considered in [4], and proposed learning a mixed elliptic/hyperbolic distance that experimentally shows good improvement over constant-curvature Cayley-Klein geometries.",
      "startOffset" : 105,
      "endOffset" : 108
    }, {
      "referenceID" : 6,
      "context" : "For example, it would be interesting to study Multi-Dimensional Scaling [11] or Support Vector Machines (SVMs) in Cayley-Klein geometries, or to mesh anisotropically [6] in Cayley-Klein geometries.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 3,
      "context" : "For example, it would be interesting to study Multi-Dimensional Scaling [11] or Support Vector Machines (SVMs) in Cayley-Klein geometries, or to mesh anisotropically [6] in Cayley-Klein geometries.",
      "startOffset" : 166,
      "endOffset" : 169
    } ],
    "year" : 2016,
    "abstractText" : "We consider the supervised classification problem of machine learning in Cayley-Klein projective geometries: We show how to learn a curved Mahalanobis metric distance corresponding to either the hyperbolic geometry or the elliptic geometry using the Large Margin Nearest Neighbor (LMNN) framework. We report on our experimental results, and further consider the case of learning a mixed curved Mahalanobis distance. Besides, we show that the Cayley-Klein Voronoi diagrams are affine, and can be built from an equivalent (clipped) power diagrams, and that Cayley-Klein balls have Mahalanobis shapes with displaced centers.",
    "creator" : "LaTeX with hyperref package"
  }
}