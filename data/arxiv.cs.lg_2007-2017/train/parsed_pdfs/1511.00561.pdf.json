{
  "name" : "1511.00561.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation",
    "authors" : [ "Vijay Badrinarayanan", "Alex Kendall", "Roberto Cipolla" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms—Deep Convolutional Neural Networks, Semantic Pixel-Wise Segmentation, Encoder, Decoder, Pooling, Upsampling.\nF"
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Semantic segmentation has a wide array of applications ranging from scene understanding, inferring support-relationships among objects to autonomous driving. Early methods that relied on lowlevel vision cues have fast been superseded by popular machine learning algorithms. In particular, deep learning has seen huge success lately in handwritten digit recognition, speech, categorising whole images and detecting objects in images [4], [5]. Now there is an active interest for semantic pixel-wise labelling [6] [7], [8], [2], [9], [10], [11], [12], [13], [14]. However, some of these recent approaches have tried to directly adopt deep architectures designed for category prediction to pixel-wise labelling [6]. The results, although very encouraging, appear coarse [14]. This is primarily because max pooling and sub-sampling reduce feature map resolution. Our motivation to design SegNet arises from this need to map low resolution features to input resolution for pixelwise classification. This mapping must produce features which are useful for accurate boundary localization.\nOur architecture, SegNet, is designed to be a core segmentation engine for pixel-wise semantic segmentation. It is primarily motivated by road scene understanding applications which require the ability to model appearance (road, building), shape (cars, pedestrians) and understand the spatial-relationship (context) be-\n• V. Badrinarayanan, A. Kendall, R. Cipolla are with the Machine Intelligence Lab, Department of Engineering, University of Cambridge, UK. E-mail: vb292,agk34,cipolla@eng.cam.ac.uk\ntween different classes such as road and side-walk. In typical road scenes, the majority of the pixels belong to large classes such as road, building and hence the network must produce smooth segmentations. The engine must also have the ability to delineate moving and other objects based on their shape despite their small size. Hence it is important to retain boundary information in the extracted image representation. From a computational persepective, it is necessary for the network to be efficient in terms of both memory and computation time during inference. It must also be able to train end-to-end in order to jointly optimise all the weights in the network using an efficient weight update technique such as stochastic gradient descent (SGD) [15]. Networks that are trained end-to-end or equivalently those that do not use multi-stage training [2] or other supporting aids such as region proposals [9] help establish benchmarks that are more easily repeatable. The design of SegNet arose from a need to match these criteria.\nThe encoder network in SegNet is topologically identical to the convolutional layers in VGG16 [1]. We remove the fully connected layers of VGG16 which makes the SegNet encoder network significantly smaller and easier to train than many other recent architectures [2], [9], [11], [16]. The key component of SegNet is the decoder network which consists of a hierarchy of decoders one corresponding to each encoder. Of these, the appropriate decoders use the max-pooling indices received from the corresponding encoder to perform non-linear upsampling of their input feature maps. This idea was inspired from an architecture designed for unsupervised feature learning [17]. Reusing max-pooling indices in the decoding process has several practical\nar X\niv :1\n51 1.\n00 56\n1v 1\n[ cs\n.C V\n] 2\nN ov\n2 01\n5\n2\nadvantages; (i) it improves boundary delineation , (ii) it reduces the number of parameters enabling end-to-end training, and (iii) this form of upsampling can be incorporated into any encoder-decoder architecture such as [2], [10] with only a little modification.\nOne of the main contributions of this paper is our analysis of the SegNet decoding technique and the widely used Fully Convolutional Network (FCN) [2]. This is in order to convey the practical trade-offs involved in designing segmentation architectures. Most recent deep architectures for segmentation have identical encoder networks, i.e VGG16, but differ in the form of the decoder network, training and inference. Another common feature is they have trainable parameters in the order of hundreds of millions and thus encounter difficulties in performing end-toend training [9]. The difficulty of training these networks has led to multi-stage training [2], appending networks to a pre-trained core segmentation engine such as FCN [10], use of supporting aids such as region proposals for inference [9], disjoint training of classification and segmentation networks [16] and use of additional training data for pre-training [11] [18] or for full training [10]. In addition, performance boosting post-processing techniques [14] have also been popular. Although all these factors improve performance on challenging benchmarks [19], it is unfortunately difficult from their quantitative results to disentangle the key design factors necessary to achieve good performance. We therefore analysed the decoding process used in some of these approaches [2], [9] and reveal their pros and cons.\nWe evaluate the performance of SegNet on PascalVOC12 salient object(s) segmentation [19], [20] and scene understanding challenges such as CamVid road scene segmentation [3] and SUN RGB-D indoor scene segmentation [21]. Pascal VOC12 [19] is the benchmark for segmentation due to its size and challenges, but the\nmajority of this task has one or two foreground classes surrounded by a highly varied background. This implicitly favours techniques used for detection as shown by the recent work on a decoupled classification-segmentation network [16] where the classification network can be trained with a large set of weakly labelled data and the independent segmentation network performance is improved. The method of [14] also use the feature maps of the classification network with an independent CRF post-processing technique to perform segmentation. The performance can also be boosted by the use additional inference aids such as region proposals [9], [22]. Therefore, it is different from scene understanding where the idea is to exploit co-occurences of objects and other spatial-context to perform robust segmentation. To demonstrate the efficacy of SegNet, we present a real-time online demo of road scene segmentation into 11 classes of interest for autonomous driving (see Fig. 1). Some example test results produced on randomly sampled road scene images from Google are shown in Fig. 1.\nThe remainder of the paper is organized as follows. In Sec. 2 we review related recent literature. We describe the SegNet architecture and its analysis in Sec. 3. In Sec. 4 we evaluate the performance of SegNet on several benchmark datasets. This is followed by a general discussion regarding our approach with pointers to future work in Sec. 5. We conclude in Sec. 6."
    }, {
      "heading" : "2 LITERATURE REVIEW",
      "text" : "Semantic pixel-wise segmentation is an active topic of research, fuelled by challenging datasets [3], [19], [21], [23], [24]. Before the arrival of deep networks, the best performing methods mostly relied on hand engineered features classifiying pixels independently. Typically, a patch is fed into a classifier e.g. Random\n3 Forest [25], [26] or Boosting [27], [28] to predict the class probabilities of the center pixel. Features based on appearance [25] or SfM and appearance [26], [27], [28] have been explored for the CamVid road scene understanding test [3]. These per-pixel noisy predictions (often called unary terms) from the classifiers are then smoothed by using a pair-wise or higher order CRF [27], [28] to improve the accuracy. More recent approaches have aimed to produce high quality unaries by trying to predict the labels for all the pixels in a patch as opposed to only the center pixel. This improves the results of Random Forest based unaries [29] but thin structured classes are classfied poorly. Dense depth maps computed from the CamVid video have also been used as input for classification using Random Forests [30]. Another approach argues for the use of a combination of popular hand designed features and spatio temporal super-pixelization to obtain higher accuracy [31]. The best performing technique on the CamVid test [28] addresses the imbalance among label frequencies by combining object detection outputs with classifier predictions in a CRF framework. The result of all these techniques indicate the need for improved features for classification.\nIndoor RGBD pixel-wise semantic segmentation has also gained popularity since the release of the NYU dataset [23]. This dataset showed the usefulness of the depth channel to improve segmentation. Their approach used features such as RGB-SIFT, depth-SIFT and pixel location as input to a neural network classifier to predict pixel unaries. The noisy unaries are then smoothed using a CRF. Improvements were made using a richer feature set including LBP and region segmentation to obtain higher accuracy [32] followed by a CRF. In more recent work [23], both class segmentation and support relationships are inferred together using a combination of RGB and depth based cues. Another approach focusses on real-time joint reconstruction and semantic segmentation, where Random Forests are used as the classifier [33]. Gupta et al. [34] use boundary detection and hierarchical grouping before performing category segmentation. The common attribute in all these approaches is the use of hand engineered features for classification of either RGB or RGBD images.\nThe success of deep convolutional neural networks for object classification has more recently led to researchers to exploit their feature learning capabilities for structured prediction problems such as segmentation. There have also been attempts to apply networks designed for object categorization to segmentation, particularly by replicating the deepest layer features in blocks to match image dimensions [6], [35], [36], [37]. However, the resulting classification is blocky [36]. Another approach using recurrent neural networks [38] merges several low resolution predictions to create input image resolution predictions. These techniques are already an improvement over hand engineered features [6] but their ability to delineate boundaries is poor.\nNewer deep architectures [2], [9], [10], [13], [16] particularly designed for segmentation have advanced the state-of-the-art by learning to decode or map low resolution image representations to pixel-wise predictions. The encoder network which produces these low resolution representations in all of these architectures is the VGG16 classification network [1] which has 13 convolutional layers and 3 fully connected layers. This encoder network weights are typically pre-trained on the large ImageNet object classification dataset [39]. The decoder network varies between these architectures and is the part which is responsible for producing multi-dimensional features for each pixel for classification.\nEach decoder in the Fully Convolutional Network (FCN)\narchitecture [2] learns to upsample its input feature map(s) and combines them with the corresponding encoder feature map to produce the input to the next decoder. It is a core segmentation engine which has a large number of trainable parameters in the encoder network (134M) but a very small decoder network (0.5M). The overall large size of this network makes it hard to train end-to-end on a relevant task. Therefore, the authors use a stage-wise training process. Here each decoder in the decoder network is progressively added to an existing trained network. The network is grown until no further increase in performance is observed. This growth is stopped after three decoders thus ignoring high resolution feature maps can certainly lead to loss of edge information [9]. Apart from training related issues, the need to reuse the encoder feature maps in the decoder makes it memory intensive in test time. We study this network in more detail as it the core of other recent architectures [10], [11].\nThe predictive performance of FCN has been improved further by appending the FCN with a recurrent neural network (RNN) [10] and fine-tuning them on large datasets [19], [40]. The RNN layers mimic the sharp boundary delineation capabilities of CRFs while exploiting the feature representation power of FCN’s. They show a significant improvement over FCN-8 but also show that this difference is reduced when more training data is used to train FCN-8. The main advantage of the CRF-RNN is when it is jointly trained with a core segmentation engine such as the FCN8. The fact that joint training helps is also shown in other recent results [41], [42]. Interestingly, the deconvolutional network [9] performs significantly better than FCN although at the cost of a more complex training and inference. This however raises the question as to whether the perceived advantage of the CRF-RNN would be reduced as the core feed-forward segmentation engine is made better. In any case, the CRF-RNN network can be appended to any core segmentation engine including SegNet.\nMulti-scale deep architectures are also being pursued [13], [42]. They come in two flavours, (i) those which use input images at a few scales and corresponding deep feature extraction networks, and (ii) those which combine feature maps from different layers of a single deep architecture [43] [11]. The common idea is to use features extracted at multiple scales to provide both local and global context [44] and the using feature maps of the early encoding layers retain more high frequency detail leading to sharper class boundaries. Some of these architectures are difficult to train due to their parameter size [13]. Thus a multi-stage training process is employed along with data augmentation. The inference is also expensive with multiple convolutional pathways for feature extraction. Others [42] append a CRF to their multi-scale network and jointly train them. However, these are not feed-forward at test time and require optimization to determine the MAP labels.\nSeveral of the recently proposed deep architectures for segmentation are not feed-forward in inference time [9], [14], [16]. They require either MAP inference over a CRF [42], [41] or aids such as region proposals [9] for inference. We believe the perceived performance increase obtained by using a CRF is due to the lack of good decoding techniques in their core feed-forward segmentation engine. SegNet on the other hand uses decoders to obtain features for accurate pixel-wise classification.\nThe recently proposed Deconvolutional Network [9] and its semi-supervised variant the Decoupled network [16] use the max locations of the encoder feature maps (pooling indices) to perform non-linear upsampling in the decoder network. The authors of these architectures, independently of SegNet (first submitted to\n4 Convolutional Encoder-Decoder\nPooling Indices\nInput\nSegmentation\nOutput\nConv + Batch Normalisation + ReLU Pooling Upsampling Softmax\nRGB Image\nFig. 2. An illustration of the SegNet architecture. There are no fully connected layers and hence it is only convolutional. A decoder upsamples its input using the transferred pool indices from its encoder to produce a sparse feature map(s). It then performs convolution with a trainable filter bank to densify the feature map. The final decoder output feature maps are fed to a soft-max classifier for pixel-wise classification.\nCVPR 2015 [12]), proposed this idea of decoding in the decoder network. However, their encoder network consists of the fully connected layers from the VGG-16 network which consists of about 90% of the parameters of their entire network. This makes training of their network very difficult and thus require additional aids such as the use of region proposals to enable training. Moreover, during inference these proposals are used and this increases inference time significantly. From a benchmarking point of view, this also makes it difficult to evaluate the performance of their core segmentation engine (encoder-decoder network) without other aids. In this work we discard the fully connected layers of the VGG16 encoder network which enables us to train the network using the relevant training set using SGD optimization. Another recent method [14] shows the benefit of reducing the number of parameters significantly without sacrificing performance, reducing memory consumption and improving inference time.\nOur work was inspired by the unsupervised feature learning architecture proposed by Ranzato et al. [17]. The key learning module is an encoder-decoder network. An encoder consists of convolution with a filter bank, element-wise tanh non-linearity, max-pooling and sub-sampling to obtain the feature maps. For each sample, the indices of the max locations computed during pooling are stored and passed to the decoder. The decoder upsamples the feature maps by using the stored pooled indices. It convolves this upsampled map using a trainable decoder filter bank to reconstruct the input image. This architecture was used for unsupervised pre-training for classification. A somewhat similar decoding technique is used for visualizing trained convolutional networks [45] for classification. The architecture of Ranzato et al. mainly focussed on layer-wise feature learning using small input patches. This was extended by Kavukcuoglu et. al. [46] to accept full image sizes as input to learn hierarchical encoders. Both these approaches however did not attempt to use deep encoder-decoder networks for unsupervised feature training as they discarded the decoders after each encoder training. Here, SegNet differs from these architectures as the deep encoder-decoder network is trained jointly for a supervised learning task and hence the decoders are an integral part of the network in test time.\nOther applications where pixel wise predictions are made using deep networks are image super-resolution [47] and depth map prediction from a single image [48]. The authors in [48] discuss the need for learning to upsample from low resolution feature maps which is the central topic of this paper."
    }, {
      "heading" : "3 ARCHITECTURE",
      "text" : "SegNet has an encoder network and a corresponding decoder network, followed by a final pixelwise classification layer. This architecture is illustrated in Fig. 3. The encoder network consists of 13 convolutional layers which correspond to the first 13 convolutional layers in the VGG16 network [1] designed for object classification. We can therefore initialize the training process from weights trained for classification on large datasets [39]. We can also discard the fully connected layers in favour of retaining higher resolution feature maps at the deepest encoder output. This also reduces the number of parameters in the SegNet encoder network significantly (from 134M to 14.7M) as compared to other recent architectures [2], [9] (see. Table 5). Each encoder layer has a corresponding decoder layer and hence the decoder network has 13 layers. The final decoder output is fed to a multi-class soft-max classifier to produce class probabilities for each pixel independently.\nEach encoder in the encoder network performs convolution with a filter bank to produce a set of feature maps. These are then batch normalized [49]). Then an element-wise rectified-linear non-linearity (ReLU) max(0, x) is applied. Following that, maxpooling with a 2 × 2 window and stride 2 (non-overlapping window) is performed and the resulting output is sub-sampled by a factor of 2. Max-pooling is used to achieve translation invariance over small spatial shifts in the input image. Sub-sampling results in a large input image context (spatial window) for each pixel in the feature map. While several layers of max-pooling and sub-sampling can achieve more translation invariance for robust classification correspondingly there is a loss of spatial resolution of the feature maps. The increasingly lossy (boundary detail) image representation is not beneficial for segmentation where boundary delineation is vital. Therefore, it is necessary to capture and store boundary information in the encoder feature maps before sub-sampling is performed. If memory during inference is not constrained, then all the encoder feature maps (after subsampling) can be stored. This is usually not the case in practical applications and hence we propose a more efficient way to store this information. It involves storing only the max-pooling indices, i.e, the locations of the maximum feature value in each pooling window is memorized for each encoder feature map. In principle, this can be done using 2 bits for each 2 × 2 pooling window and is thus much more efficient to store as compared to memorizing feature map(s) in float precision. As we show later in this work,\n5 this lower memory storage results in a slight loss of accuracy but is still suitable for practical applications.\nThe appropriate decoder in the decoder network upsamples its input feature map(s) using the memorized max-pooling indices from the corresponding encoder feature map(s). This step produces sparse feature map(s). This SegNet decoding technique is illustrated in Fig. 3. These feature maps are then convolved with a trainable decoder filter bank to produce dense feature maps. A batch normalization step is then applied to each of these maps. Note that the decoder corresponding to the first encoder (closest to the input image) produces a multi-channel feature map, although its encoder input has 3 channels (RGB). This is unlike the other decoders in the network which produce feature maps with the same number of size and channels as their encoder inputs. The high dimensional feature representation at the output of the final decoder is fed to a trainable soft-max classifier. This soft-max classifies each pixel independently. The output of the soft-max classifier is a K channel image of probabilities where K is the number of classes. The predicted segmentation corresponds to the class with maximum probability at each pixel."
    }, {
      "heading" : "3.1 Decoder Variants",
      "text" : "Many segmentation architectures [2], [9], [14] share the same encoder network and they only vary in the form of their decoder network. Of these we choose to compare the SegNet decoding technique with the widely used Fully Convolutional Network (FCN) decoding technique [2], [10].\nIn order to analyse SegNet and compare its performance with FCN (decoder variants) we use a smaller version of SegNet, termed SegNet-Basic 1, which has 4 encoders and 4 decoders. All the encoders in SegNet-Basic perform max-pooling and subsampling and the corresponding decoders upsample its input using the received max-pooling indices. Batch normalization is used after each convolutional layer in both the encoder and decoder network. No biases are used after convolutions and no ReLU nonlinearity is present in the decoder network. Further, a constant kernel size of 7× 7 over all the layers is chosen to provide a wide context for smooth labelling i.e. a pixel in the deepest layer feature map (layer 4) can be traced back to a context window in the input image of 106×106 pixels. This small size of SegNet-Basic allows us to explore many different variants (decoders) and train them in reasonable time. Similarly we create FCN-Basic, a comparable version of FCN for our analysis which shares the same encoder network as SegNet-Basic but with the FCN decoding technique in the corresponding decoders.\nOn the left in Fig. 3 is the decoding technique used by SegNet (also SegNet-Basic), where there is no learning involved in the upsampling step. However, the upsampled maps are convolved with trainable multi-channel decoder filters to densify the sparse inputs. Each decoder filter has the same number of channels as the number of upsampled feature maps. A smaller variant is one where the decoder filters are single channel, i.e they only convolve their corresponding upsampled feature map. This variant (SegNetBasic-SingleChannelDecoder) reduces the number of trainable parameters and inference time significantly.\nOn the right in Fig. 3 is the FCN (also FCN-Basic) decoding technique. The important design element of the FCN model is dimensionality reduction step of the encoder feature maps. This\n1. SegNet-Basic was earlier termed SegNet in a archival version of this paper [12]\ncompresses the encoder feature maps which are then used in the corresponding decoders. Dimensionality reduction of the encoder feature maps, say of 64 channels, is performed by convolving them with 1 × 1 × 64 × K trainable filters, where K is the number of classes. The compressed K channel final encoder layer feature maps are the input to the decoder network. In a decoder of this network, upsampling is performed by convolution using a trainable multi-channel upsampling kernel. We set the kernel size to 8 × 8. This manner of upsampling is also termed as deconvolution. Note that in SegNet the multi-channel convolution using trainable decoder filters is performed after upsampling to densifying feature maps. The upsampled feature map in FCN is then added to the corresponding resolution encoder feature map to produce the output decoder feature map. The upsampling kernels are initialized using bilinear interpolation weights [2].\nThe FCN decoder model requires storing encoder feature maps during inference. This can be memory intensive, for e.g. storing 64 feature maps of the first layer of FCN-Basic at 180 × 240 resolution in 32 bit floating point precision takes 11MB. This can be made smaller using dimensionality reduction to the 11 feature maps which requires ≈ 1.9MB storage. SegNet on the other hand requires almost negligible storage cost for the pooling indices (.17MB if stored using 2 bits per 2× 2 pooling window). We can also create a variant of the FCN-Basic model which discards the encoder feature map addition step and only learns the upsampling kernels (FCN-Basic-NoAddition).\nIn addition to the above variants, we study upsampling using fixed bilinear interpolation weights which therefore requires no learning for upsampling (Bilinear-Interpolation). At the other extreme, we can add 64 encoder feature maps at each layer to the corresponding output feature maps from the SegNet decoder to create a more memory intensive variant of SegNet (SegNetBasic-EncoderAddition). Another and more memory intensive FCN-Basic variant (FCN-Basic-NoDimReduction) is where there is no dimensionality reduction performed for the encoder feature maps. Finally, please note that to encourage reproduction of our results we release the Caffe implementation of all the variants 2.\nWe also tried other generic variants where feature maps are simply upsampled by replication [6], or by using a fixed (and sparse) array of indices for upsampling. These performed quite poorly in comparison to the above variants. A variant without max-pooling and sub-sampling in the encoder network (decoders are redundant) consumes more memory, takes longer to converge and performs poorly."
    }, {
      "heading" : "3.2 Training",
      "text" : "We use the CamVid road scenes dataset to benchmark the performance of the decoder variants. This dataset is small, consisting of 367 training and 233 testing RGB images (day and dusk scenes) at 360×480 resolution. The challenge is to segment 11 classes such as road, building, cars, pedestrians, signs, poles, side-walk etc. We perform local contrast normalization [50] to the RGB input.\nThe encoder and decoder weights were all initialized using the technique described in He et al. [51]. To train all the variants we use stochastic gradient descent (SGD) with a fixed learning rate of 0.1 and momentum of 0.9 [15] using our Caffe implementation of SegNet-Basic [52]. We train the variants for a maximum of 200 epochs where each epoch is one pass through the full dataset. Before each epoch, the training set is shuffled and each mini-batch\n2. See http://mi.eng.cam.ac.uk/projects/segnet/ for code and web demo.\n6 \uD835\uDC4E \uD835\uDC4F \uD835\uDC50 \uD835\uDC51 \uD835\uDC4E 0 0 0 0 0 0 \uD835\uDC51 0 0 0 \uD835\uDC4F 0 0 0 \uD835\uDC50 Max-pooling Indices\nSegNet\nDeconvolution for upsampling\nEncoder feature map\n+\n\uD835\uDC661\n\uD835\uDC663 \uD835\uDC662 \uD835\uDC664 \uD835\uDC665 \uD835\uDC668 \uD835\uDC666 \uD835\uDC667\n\uD835\uDC6613 \uD835\uDC6614 \uD835\uDC6615 \uD835\uDC6616\nFCN\n\uD835\uDC6612 \uD835\uDC6610 \uD835\uDC6611 \uD835\uDC669\n\uD835\uDC4E \uD835\uDC4F \uD835\uDC50 \uD835\uDC51\n\uD835\uDC651\n\uD835\uDC653 \uD835\uDC652 \uD835\uDC654 \uD835\uDC655 \uD835\uDC658 \uD835\uDC656 \uD835\uDC657\n\uD835\uDC6512 \uD835\uDC659 \uD835\uDC6510 \uD835\uDC6511 \uD835\uDC6514 \uD835\uDC6513 \uD835\uDC6515 \uD835\uDC6516\nConvolution with trainable decoder filters\nDimensionality reduction\n(12 images) is then picked in order thus ensuring that each image is used only once in an epoch. We use no validation set to stop training as the training set is small and also because this enables us to compare these variants against other methods (see Table 2).\nWe use the cross-entropy loss [2] as the objective function for training the network. The loss is summed up over all the pixels in a mini-batch. When there is large variation in the number of pixels in each class in the training set (e.g road, sky and building pixels dominate the CamVid dataset) then there is a need to weight the loss differently based on the true class. This is termed class balancing. We use median frequency balancing [13] where the weight assigned to a class in the loss function is the ratio of the median of class frequencies computed on the entire training set divided by the class frequency. This implies that larger classes in the training set have a weight smaller than 1 and the weights of the smallest classes are the highest. We also experimented with training the different variants without class balancing or equivalently using natural frequency balancing."
    }, {
      "heading" : "3.3 Analysis",
      "text" : "To compare the quantitative performance of the different decoder variants, we use three commonly used performance measures: global accuracy (G) which measures the percentage of pixels correctly classified in the dataset, class average accuracy (C) is the mean of the predictive accuracy over all classes and mean intersection over union (I/U) over all classes as used in the Pascal VOC12 challenge [19]. The mean I/U metric is the hardest metric since it penalizes false positive predictions unlike class average accuracy. However, I/U metric is not optimized for directly through the class balanced cross-entropy loss.\nWe test each variant at five points, 2K, 3K, 4K, 5K, 6K iterations of optimization, on the CamVid test set. With a training mini-batch size of 12 this corresponds to testing approximately at 65, 98, 131, 163, 196 epochs (passes) through the training set. We select the iteration wherein the global accuracy is highest amongst the five evaluations and report all the three measures of performance at this point. Although we use class balancing while train-\n7 ing the variants, it is still important to achieve high global accuracy to result in an overall smooth segmentation. Another reason is that the contribution of segmentation towards autonomous driving is mainly for delineating classes such as roads, buildings, side-walk, sky. These classes dominate the majority of the pixels in an image and a high global accuracy corresponds to good segmentation of these important classes. We also observed that reporting the numerical performance when class average is highest can often correspond to low global accuracy indicating a perceptually noisy segmentation output.\nIn Table 1 we report the numerical results of our analysis. We also show the size of the trainable parameters, highest resolution feature map or pooling indices storage memory, i.e, of the first layer feature maps after max-pooling and sub-sampling, average time for one forward pass based on 50 measurements using a 360 × 480 input and also the iterations required to get the best global accuracy. We show the results for both testing and training for all the variants at the selected iteration. The results are also tabulated without class balancing (natural frequency) for training and testing accuracies. Below we analyse the results with class balancing.\nFrom the Table 1, we see that bilinear interpolation based upsampling without any learning performs the worst based on all the three measures of accuracy. All the other methods which either use learning for upsampling (FCN-Basic and variants) or learning decoder filters after upsampling (SegNet-Basic and its variants) perform significantly better. This emphasizes the need to learn decoders for segmentation. This is also supported by experimental evidence gathered by other authors when comparing FCN with SegNet-type decoding techniques [9].\nWhen we compare SegNet-Basic and FCN-Basic we see that both perform equally well on this test over all the three measures of accuracy. The difference is that SegNet uses less memory during inference since it only stores max-pooling indices. On the other hand FCN-Basic stores encoder feature maps in full which consumes much more memory (11 times more). SegNetBasic has a decoder with 64 feature maps in each decoder layer. In comparison FCN-Basic, which uses dimensionality reduction, has fewer (11) feature maps in each decoder layer. This reduces the number of convolutions in the decoder network and hence FCN-Basic is faster during inference (forward pass). From another perspective, the decoder network in SegNet-Basic makes it overall a larger network than FCN-Basic. This endows it with more flexibility and hence achieves higher training accuracy than FCNBasic for the same number of iterations. Overall we see that SegNet-Basic has an advantage over FCN-Basic when memory during inference is constrained but where inference time can be compromised to an extent.\nSegNet-Basic is most similar to FCN-Basic-NoAddition in terms of their decoders, although the decoder of SegNet is larger. Both learn to produce dense feature maps, either directly by learning to perform deconvolution as in FCN-Basic-NoAddition or by first upsampling and then convolving with trained decoder filters. The performance of SegNet-Basic is superior, in part due to its larger decoder size. Now, the accuracy of FCN-Basic-NoAddition is also lower as compared to FCN-Basic. This shows that it is important to capture the information present in the encoder feature maps for better performance. This can also explain the part of the reason why SegNet-Basic outperforms FCN-Basic-NoAddition.\nThe size of the FCN-Basic-NoAddition-NoDimReduction model is slightly larger than SegNet-Basic and this makes it a\nfair comparison. The performance of this FCN variant is poorer than SegNet-Basic in test but also its training accuracy is lower for the same number of training epochs. This shows that using a larger decoder is not enough but it is also important to capture encoder feature map information to learn better. Here it is also interesting to see that SegNet-Basic has a competitive training accuracy when compared to larger models such FCN-Basic-NoDimReduction.\nAnother interesting comparison between FCN-BasicNoAddition and SegNet-Basic-SingleChannelDecoder shows that using max-pooling indices for upsampling and an overall larger decoder leads to better performance. This also lends evidence to SegNet being a good architecture for segmentation, particularly when there is a need to find a compromise between storage cos, accuracy versus inference time. In the best case, when both memory and inference time is not constrained, larger models such as FCN-Basic-NoDimReduction and SegNet-EncoderAddition are both more accurate than the other variants. Particularly, discarding dimensionality reduction in the FCN-Basic model leads to the best performance amongst the FCN-Basic variants. This once again emphasizes the trade-off involved between memory and accuracy in segmentation architectures.\nThe last two columns of Table 1 show the result when no class balancing is used (natural frequency). Here, we can observe that without weighting the results are poorer for all the variants, particularly for class average accuracy and mean I/U metric. The global accuracy is the highest without weighting since the majority of the scene is dominated by sky, road and building pixels. Apart from this all the inference from the comparative analysis of variants holds true for natural frequency balancing too. SegNet-Basic performs as well as FCN-Basic and is better than the larger FCN-Basic-NoAddition-NoDimReduction. The bigger but less efficient models FCN-Basic-NoDimReduction and SegNetEncoderAddition perform better than the other variants.\nWe can now summarize the above analysis with the following general points.\n1) The best performance is achieved when encoder feature maps are stored in full. 2) When memory during inference is constrained, then compressed forms of encoder feature maps (dimensionality reduction, max-pooling indices) can be stored and used with an appropriate decoder (e.g. SegNet type) to improve performance. 3) Larger decoders increase performance for a given encoder network."
    }, {
      "heading" : "4 BENCHMARKING",
      "text" : "We quantify the performance of SegNet on three different benchmarks using our Caffe implementation 3. Through this process we demonstrate the efficacy of SegNet for various scene segmentation tasks which have practical applications. In the first experiment, we test the performance of SegNet on the CamVid road scene dataset (see Sec. 3.2 for more information about this data). We use this result to compare SegNet with several methods including Random Forests [25], Boosting [25], [27] in combination with CRF based methods [28]. We also trained SegNet on a larger dataset of road scenes collected from various publicly available datasets [3], [53], [54] and show that this leads to a large improvement in accuracy.\n3. Our web demo and Caffe implementation is available for evaluation at http://mi.eng.cam.ac.uk/projects/segnet/\n8 SUN RGB-D [21] is a very challenging and large dataset of indoor scenes with 5285 training and 5050 testing images. The images are captured by different sensors and hence come in various resolutions. The task is to segment 37 indoor scene classes including wall, floor, ceiling, table, chair, sofa etc. This task is made hard by the fact that object classes come in various shapes, sizes and in different poses. There are frequent partial occlusions since there are typically many different classes present in each of the test images. These factors make this one of the hardest segmentation challenges. We only use the RGB modality for our training and testing. Using the depth modality would necessitate architectural modifications/redesign [2]. Also the quality of depth images from current cameras require careful post-processing to fill-in missing measurements. They may also require using fusion of many frames to robustly extract features for segmentation. Therefore we believe using depth for segmentation merits a separate body of work which is not in the scope of this paper. We also note that an earlier benchmark dataset NYUv2 [23] is included as part of this dataset.\nPascal VOC12 [19] is a RGB dataset for segmentation with 12031 combined training and validation images of indoor and outdoor scenes. The task is to segment 21 classes such as bus, horse, cat, dog, boat from a varied and large background class. The foreground classes often occupy a small part of an image. The evaluation is performed remotely on 1456 images.\nIn all three benchmark experiments, we scale the images to 224 × 224 resolution for training and testing. We used SGD with momentum to train SegNet. The learning rate was fixed to 0.001 and momentum to 0.9. The mini-batch size was 4. The optimization was performed for 100 epochs and then tested."
    }, {
      "heading" : "4.1 CamVid Road Scenes",
      "text" : "A number of outdoor scene datasets are available for semantic parsing [3], [24], [55], [56]. Of these we choose to benchmark SegNet using the CamVid dataset [3] as it contains video sequences. This enables us to compare our proposed architecture with those which use motion and structure [26], [27], [28] and video segments [31]. We also use an ensemble of training images from some of these datasets totalling 3433 images to train SegNet for an additional benchmark. For a web demo (see the link 3) of road scene segmentation, we include the CamVid test set to this larger dataset.\nThe qualitative comparisons of SegNet-Basic and SegNet predictions with several well known algorithms (unaries, unaries+CRF) are shown in Fig. 4 along with the input modalities used to train the methods. The qualitative results show the ability of the proposed architectures to segment small (cars, pedestrians, bicyclist) classes while producing a smooth segmentation of the overall scene. The unary only methods like Random Forests, Boosting which are trained to predict the label of the center-pixel of a small patch produce low quality segmentations. Smoothing unaries with CRF’s improve the segmentation quality considerably with higher order CRF’s performing best. Although the CRF based results appear smooth, upon close scrutiny, we see that shape segmentation of smaller but important classes such as bicyclists, pedestrians are poor. In addition, natural shapes of classes like trees are not preserved and the details like the wheels of cars are lost. More dense CRF models [57] can be better but with additional cost of inference. SegNet-Basic and SegNet (without large dataset training) clearly indicate their ability to retain the natural\nshapes of classes such as bicyclists, cars, trees, poles etc better than CRF based approaches. The overall segmentation quality is also smooth except for the side-walk class. This is because the side-walk class is highly varied in terms of size and texture and this cannot be captured with a small training set. Another explanation could be that the size of the receptive fields of the deepest layer feature units are smaller than their theoretical estimates [11], [58] and hence unable to group all the side-walk pixels into one class. Illumination variations also affect performance on cars in the dusk examples. However, several of these issues can be ameliorated by using larger amounts of training data. In particular, we see that smaller classes such as pedestrians, cars, bicyclists, columnpole are segmented better than other methods in terms of shape retention. The side-walk class is also segmented smoothly.\nThe quantitative results in Table 2 show SegNet-Basic, SegNet obtain competitive results even without CRF based processing. This shows the ability of the deep architecture to extract meaningful features from the input image and map it to accurate and smooth class segment labels. SegNet is better in performance than SegNet-Basic although trained with the same (small) training set. This indicates the importance of using pre-trained encoder weights and choice of the optimization technique . Interestingly, the use of the bigger and deeper SegNet architecture improves the accuracy of the larger classes as compared to SegNet-Basic and not the smaller classes as one might expect. We also find that SegNetBasic [12] trained in a layer-wise manner using L-BFGS [59] also performs competitively and is better than SegNet-Basic trained with SGD (see Sec. 3.2). This is an interesting training approach but needs further research in order for it scale to larger datasets.\nThe most interesting result is the approximately 15% performance improvement in class average accuracy that is obtained when a large training dataset is utilized to train SegNet. The mean of intersection over union metric is also very high. Correspondingly, the qualitative results of SegNet (see Fig. 4) are clearly superior to the rest of the methods. It is able to segment both small and large classes well. In addition, there is an overall smooth quality of segmentation much like what is typically obtained with CRF post-processing. Although the fact that results improve with larger training sets is not surprising, the percentage improvement obtained using pre-trained encoder network and this training set indicates that this architecture can potentially be deployed for practical applications. Our random testing on urban and highway images from Google (see Fig. 1) demonstrates that SegNet can absorb a large training set and generalize well to unseen images. It also indicates the contribution of the prior (CRF) can be lessened when sufficient amount of training data is made available."
    }, {
      "heading" : "4.2 SUN RGB-D Indoor Scenes",
      "text" : "Road scene images have limited variation, both in terms of the classes of interest and their spatial arrangements, especially when captured from a moving vehicle. In comparison, images of indoor scenes are more complex since the view points can vary a lot and there is less regularity in both the number of classes present in a scene and their spatial arrangement. Another difficulty is caused by the widely varying sizes of the object classes in the scene. Some test samples from the recent SUN RGB-D dataset [21] are shown in Fig. 5. We observe some scenes with few large classes and some others with dense clutter (bottom row and right). The appearance (texture and shape) can also widely vary in indoor scenes. Therefore, we believe this is the hardest challenge for\n9 Test samples\nGround Truth\nRandom Forest with SfM - Height above camera - Surface normals - Depth + Texton features Boosting with - SfM (see c above) -Texton -Color -HOG -Location\nBoosting (see d above) + pair-wise CRF\nBoosting (see d above) + Higher order CRF\nBoosting (see d above) + Detectors trained on CamVid dataset + CRF\nSegNet-Basic with only local contrast normalized RGB as input (median freq. balancing)\nSegNet with only local contrast normalized RGB as input (pre-trained encoder, median freq. balancing)\nSegNet with only local contrast normalized RGB as input (pretrained encoder , median freq. balancing + large training set)\nFig. 4. Results on CamVid day and dusk test samples. The evolution of results from various patch based predictions [25], [26], then combined with CRF smoothing models [27], [28]. Unlike CRF based methods, SegNet-Basic and SegNet predictions retain the shape of small categories such as poles (column 2,4), bicyclist (column 3), far side side-walk (column 2) better. Large sized side-walk is not smoothly segmented possibly because the empirical size of the receptive fields of deep feature units is not large enough [11], [58]. When SegNet is trained on a large dataset it produces the most accurate predictions with an overall smooth appearance.\n10\nMethod B ui\nld in\ng\nTr ee\nSk y\nC ar\nSi gn\n-S ym\nbo l\nR oa\nd\nPe de\nst ri\nan\nFe nc\ne\nC ol\num n-\nPo le\nSi de\n-w al\nk\nB ic\nyc lis\nt\nC la\nss av\ng.\nG lo\nba la\nvg .\nM ea\nn I/\nU\nSfM+Appearance [26] 46.2 61.9 89.7 68.6 42.9 89.5 53.6 46.6 0.7 60.5 22.5 53.0 69.1 n/a Boosting [27] 61.9 67.3 91.1 71.1 58.5 92.9 49.5 37.6 25.8 77.8 24.7 59.8 76.4 n/a\nDense Depth Maps [30] 85.3 57.3 95.4 69.2 46.5 98.5 23.8 44.3 22.0 38.1 28.7 55.4 82.1 n/a Structured Random Forests [29] n/a 51.4 72.5 n/a\nNeural Decision Forests [60] n/a 56.1 82.1 n/a Local Label Descriptors [61] 80.7 61.5 88.8 16.4 n/a 98.0 1.09 0.05 4.13 12.4 0.07 36.3 73.6 n/a\nSuper Parsing [31] 87.0 67.1 96.9 62.7 30.1 95.9 14.7 17.9 1.7 70.0 19.4 51.2 83.3 n/a SegNet-Basic 80.6 72.0 93.0 78.5 21.0 94.0 62.5 31.4 36.6 74.0 42.5 62.3 82.8 46.3\nSegNet-Basic (layer-wise training [12]) 75.0 84.6 91.2 82.7 36.9 93.3 55.0 37.5 44.8 74.1 16.0 62.9 84.3 n/a SegNet 88.0 87.3 92.3 80.0 29.5 97.6 57.2 49.4 27.8 84.8 30.7 65.9 88.6 50.2\nSegNet (3.5K dataset training) 73.8 90.7 90.1 83.0 83.9 95.21 86.8 68.0 74.6 95.3 53.0 81.3 86.8 69.1 CRF based approaches Boosting + pairwise CRF [27] 70.7 70.8 94.7 74.4 55.9 94.1 45.7 37.2 13.0 79.3 23.1 59.9 79.8 n/a Boosting+Higher order [27] 84.5 72.6 97.5 72.7 34.1 95.3 34.2 45.7 8.1 77.6 28.5 59.2 83.8 n/a Boosting+Detectors+CRF [28] 81.5 76.6 96.2 78.7 40.2 93.9 43.0 47.6 14.3 81.5 33.9 62.5 83.8 n/a\nTABLE 2 Quantitative results on CamVid [3] consisting of 11 road scene categories. SegNet outperforms all the other methods, including those using depth, video and/or CRF’s. In comparison with the CRF based methods SegNet predictions are more accurate in 8 out of the 11 classes. It also shows a good ≈ 15% improvement in class average accuracy when trained on a large dataset of 3.5K images and this sets a new benchmark for the majority of the individual classes. Particularly noteworthy are the significant improvements in accuracy for the smaller/thinner classes.\nsegmentation architectures and methods in computer vision. Other challenges such as Pascal VOC12 [19] salient object segmentation have occupied researchers more, but indoor scene segmentation is more challenging and has more practical applications such as in robotics. To encourage more research in this direction we set a new benchmark on the large SUN RGB-D dataset.\nThe qualitative results of SegNet on some images of indoor scenes of different types such as bedroom, kitchen, bathroom, classroom etc. are shown in Fig. 5. We see that SegNet obtains sharp boundaries between classes when the scene consists of reasonable sized classes but even when view point changes are present (see bed segmentation from different view points). This is particularly interesting since the input modality is only RGB. It indicates the ability of SegNet to extract features from RGB images which are useful for view-point invariant segmentation provided there is sufficient training data (here 5285 images). RGB images are also useful to segment thinner structures such as the legs of chairs and tables, lamps which is difficult to achieve using depth images from currently available sensors. It is also useful to segment decorative objects such as paintings on the wall.\nIn Table 4 we report the quantitative results on the 37 class segmentation task. We first note here that the other methods that have been benchmarked are not based on deep architectures and they only report class average accuracy. The existing top performing method [32] relies on hand engineered features using colour, gradients and surface normals for describing super-pixels and then smooths super-pixel labels with a CRF. For SegNet we achieve a high global accuracy which correlates with an overall smooth segmentation. This is also an indicator that largest classes such as wall, floor, bed, table, sofa are segmented well in spite of view-point changes and appearance variations. However, the class average accuracy and mean I/U metric are poor, but at the same level as the hand engineered method which also includes the depth channel as input. This shows that smaller and thinner classes which have lesser training data are not segmented well. The individual class accuracies are reported in Table 3. From these we see that there is a clear correlation between the size and\nnatural frequency of occurrence of classes and their individual accuracies. It is also informative to note RGB input is useful to segment widely varying (shape, texture) categories such as wall, floor, ceiling, table, chair, sofa with reasonable accuracy."
    }, {
      "heading" : "4.3 Pascal VOC12 Segmentation Challenge",
      "text" : "The Pascal VOC12 segmentation challenge [19] consists of segmenting a few salient object classes from a widely varying background class. It is unlike the segmentation for scene understanding benchmarks described earlier which require learning both classes and their spatial context. A number of techniques have been proposed based on this challenge which are increasingly more accurate and complex 4. Our efforts in this benchmarking experiment have not been diverted towards attaining the top rank by either using multi-stage training [2], other datasets for pre-training such as MS-COCO [10], [40], training and inference aids such as object proposals [9], [22] or post-processing using CRF based methods [9], [14]. Although these supporting techniques clearly have value towards increasing the performance it unfortunately does not reveal the true performance of the deep architecture which is the core segmentation engine. It however does indicate that some of the large deep networks are difficult to train end-toend on this task even with pre-trained encoder weights. Therefore,\n4. See the leader board at http://host.robots.ox.ac.uk:8080/leaderboard\n11\n12\nWall Floor Cabinet Bed Chair Sofa Table Door Window Bookshelf Picture Counter Blinds 86.6 92.0 52.4 68.4 76.0 54.3 59.3 37.4 53.8 29.2 49.7 32.5 31.2 Desk Shelves Curtain Dresser Pillow Mirror Floor mat Clothes Ceiling Books Fridge TV Paper 17.8 5.3 53.2 28.8 36.5 29.6 0.0 14.4 67.7 32.4 10.2 18.3 19.2 Towel Shower curtain Box Whiteboard Person Night stand Toilet Sink Lamp Bathtub Bag 11.5 0.0 8.9 38.7 4.9 22.6 55.6 52.7 27.9 29.9 8.1\nTABLE 3 Class average accuracy of SegNet predictions for the 37 indoor scene classes in the SUN RGB-D benchmark dataset.\nto encourage more controlled benchmarking, we trained SegNet end-to-end without other aids and report this performance.\nIn Table 5 we show the class average accuracy for some recent methods based on deep architectures. To the best of our ability, we have tried to gather the performance measures of the competing methods for their runs using minimum supporting techniques. We also specify when a method reports the performance of its core engine on the smaller validation set of 346 images [14]. We find the performance on the full test set to be approximately 1% less as compared to the smaller validation set.\nFrom the results in Table 5 we can see that the best performing networks are either very large (and slow) [9] and/or they use a CRF [10]. The CRF encourages large segments with a single label and this suits the Pascal challenge wherein there are one or two salient objects in the center of the image. This prior also has a larger impact when training data is limited. This is shown in the experiments using CRF-RNN [10] wherein the core FCN-8 model predictions are less accurate without extra training data.\nIt is interesting that the DeepLab [14] architecture which is simply upsampling the FCN encoder features using bilinear interpolation performs reasonably well (on the validation set). The fact that a coarse segmentation is enough to produce this performance shows that this challenge is unlike scene understanding wherein many classes of varying shape and size need to be segmented.\nMethods using object proposals during training and/or inference [9], [43] are very slow in inference time and it is hard to measure their true performance. These aids are necessitated by the very large size of their deep network [9] and also because the Pascal data can also be processed by a detect and segment approach. In comparison, SegNet is smaller by virtue of discarding the fully connected layers in the VGG16 [1]. The authors of DeepLab [14] have also reported little loss in performance by reducing the size of the fully connected layers. The smaller size of SegNet makes end-to-end training possible for benchmarking. Although it may be argued that larger networks perform better, it is at the cost of a complex training mechanism, increased memory and inference time. This makes them unsuitable for real-time applications such as road scene understanding.\nThe individual class accuracies for SegNet predictions on Pascal VOC12 are shown in Table 6. From this we once again see larger and more frequently occurring classes such as aeroplane, bus, cat etc. have higher accuracy and smaller/thinner classes such as potted plant, bicycle are poorly segmented. We believe more training data [40] can help improve the performance of SegNet."
    }, {
      "heading" : "5 DISCUSSION AND FUTURE WORK",
      "text" : "The success of deep learning is due largely because of the availability of massive datasets and to a lesser extent the use of very deep (and often big) models. The use of millions of training parameters in these networks has resulted in performance gains and this effect can be observed in our analysis of various models\n(see Sec. 3.3). However, less attention has been paid to smaller and more efficient models for real-time applications (of segmentation) such as road scene understanding. This was the primary motivation behind the proposal of SegNet, which is smaller and more efficient than other competing architectures but efficient, for tasks such as road scene understanding.\nAlthough in some tasks it may be sufficient to simply replicate the encoder features and post-process with a CRF, it is less elegant and does not exploit the potential of deep learning for feed-forward segmentation. Using SegNet as a candidate architecture, we have shown the need for designing trainable decoders which can learn to map the output of the encoder network to input resolution feature maps for classification. Our controlled analysis reveals the tradeoffs involved in designing architectures for segmentation, mostly involving inference memory, time and accuracy. In particular, transferring more (feature) information to the decoders can lead to simpler decoding process but with additional storage costs. When compressed forms of encoder feature maps are stored and transferred to the decoders then, to achieve similar performance, the decoders need to be more complex (more training parameters).\nOur experience with benchmarking experiments have shown the efficacy of our proposed architecture for segmentation on both indoor and outdoor scenes. We outperform all the state-of-the-art methods for road scene understanding. Indoor scene understanding is very challenging and SegNet has set a new benchmark on a large dataset for RGB based scene understanding. An important issue we faced while conducting these experiments is the lack of controlled baselines for many of the recent architectures. Many of these architectures have used a host of supporting techniques to arrive at high accuracies on datasets but this makes it difficult to gather conclusive evidence about their true performance. This is further worsened by the fact that different variants of SGD is used to train these architectures and the learning rate is tuned manually by observing the progress of the loss. To avoid this and in the interest of future research, we trained SegNet endto-end using SGD with a fixed learning rate and momentum throughout the training process for a predefined number of epochs. We also provide our Caffe implementation of SegNet-Basic (and its variants), SegNet and a web demo for evaluation of the realtime road scene segmentation. For the future, we would like to exploit our understanding of segmentation architectures gathered from our analysis to design more efficient architectures for realtime applications. We are also interested in experimenting with Dropout [63] during training and testing. This can help estimate the uncertainty of the predictions for scene understanding."
    }, {
      "heading" : "6 CONCLUSION",
      "text" : "We presented SegNet, a deep convolutional network architecture for semantic segmentation. The main motivation behind SegNet was the need to design an efficient architecture for road scene understanding which is efficient both in terms of memory and\n13\ncomputational time. We analysed SegNet and compared it with other important variants to reveal the trade-offs involved in designing architectures for segmentation. Those which store the encoder network feature maps in full perform best but consume more memory during inference time. SegNet on the other hand is more efficient since it only stores the max-pooling indices of the feature maps and uses them in its decoder network to achieve good performance. However, the inference time is increased since the decoder network is required to be more larger. On large and well known datasets SegNet performs competitively on challenges such as indoor scene understanding and Pascal VOC12. It sets a new benchmark for road scene understanding when trained on a large dataset. With its efficient architecture and competitive performance SegNet is well suited for scene understanding applications. In future, we would also like to estimate the uncertainty of the labelling using techniques such as Dropout."
    } ],
    "references" : [ {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "K. Simonyan", "A. Zisserman" ],
      "venue" : "arXiv preprint arXiv:1409.1556, 2014.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Fully convolutional networks for semantic segmentation",
      "author" : [ "J. Long", "E. Shelhamer", "T. Darrell" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3431–3440, 2015.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Semantic object classes in video: A high-definition ground truth database",
      "author" : [ "G. Brostow", "J. Fauqueur", "R. Cipolla" ],
      "venue" : "PRL, vol. 30(2), pp. 88– 97, 2009.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Going deeper with convolutions",
      "author" : [ "C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich" ],
      "venue" : "CoRR, vol. abs/1409.4842, 2014.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "K. Simonyan", "A. Zisserman" ],
      "venue" : "CoRR, vol. abs/1409.1556, 2014.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Learning hierarchical features for scene labeling",
      "author" : [ "C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun" ],
      "venue" : "IEEE PAMI, vol. 35, no. 8, pp. 1915–1929, 2013.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1915
    }, {
      "title" : "Fast semantic segmentation of rgb-d scenes with gpu-accelerated deep neural networks",
      "author" : [ "N. Hft", "H. Schulz", "S. Behnke" ],
      "venue" : "KI 2014: Advances in Artificial Intelligence (C. Lutz and M. Thielscher, eds.), vol. 8736 of  14 Lecture Notes in Computer Science, pp. 80–85, Springer International Publishing, 2014.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Parsing natural scenes and natural language with recursive neural networks",
      "author" : [ "R. Socher", "C.C. Lin", "C. Manning", "A.Y. Ng" ],
      "venue" : "ICML, pp. 129– 136, 2011.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Learning deconvolution network for semantic segmentation",
      "author" : [ "H. Noh", "S. Hong", "B. Han" ],
      "venue" : "CoRR, vol. abs/1505.04366, 2015.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Conditional random fields as recurrent neural networks",
      "author" : [ "S. Zheng", "S. Jayasumana", "B. Romera-Paredes", "V. Vineet", "Z. Su", "D. Du", "C. Huang", "P. Torr" ],
      "venue" : "arXiv preprint arXiv:1502.03240, 2015.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Parsenet: Looking wider to see better",
      "author" : [ "W. Liu", "A. Rabinovich", "A.C. Berg" ],
      "venue" : "CoRR, vol. abs/1506.04579, 2015.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Segnet: A deep convolutional encoder-decoder architecture for robust semantic pixel-wise labelling",
      "author" : [ "V. Badrinarayanan", "A. Handa", "R. Cipolla" ],
      "venue" : "CoRR, vol. abs/1505.07293, 2015.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture",
      "author" : [ "D. Eigen", "R. Fergus" ],
      "venue" : "arXiv preprint arXiv:1411.4734, 2014.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Semantic image segmentation with deep convolutional nets and fully connected crfs",
      "author" : [ "L. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille" ],
      "venue" : "CoRR, vol. abs/1412.7062, 2014.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Large-scale machine learning with stochastic gradient descent",
      "author" : [ "L. Bottou" ],
      "venue" : "Proceedings of COMPSTAT’2010, pp. 177–186, Springer, 2010.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Decoupled deep neural network for semisupervised semantic segmentation",
      "author" : [ "S. Hong", "H. Noh", "B. Han" ],
      "venue" : "CoRR, vol. abs/1506.04924, 2015.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Unsupervised learning of invariant feature hierarchies with applications to object recognition",
      "author" : [ "M. Ranzato", "F.J. Huang", "Y. Boureau", "Y. LeCun" ],
      "venue" : "CVPR, 2007.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "The role of context for object detection and semantic segmentation in the wild",
      "author" : [ "R. Mottaghi", "X. Chen", "X. Liu", "N.-G. Cho", "S.-W. Lee", "S. Fidler", "R. Urtasun" ],
      "venue" : "Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pp. 891–898, IEEE, 2014.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "The pascal visual object classes challenge: A retrospective",
      "author" : [ "M. Everingham", "S.A. Eslami", "L. Van Gool", "C.K. Williams", "J. Winn", "A. Zisserman" ],
      "venue" : "International Journal of Computer Vision, vol. 111, no. 1, pp. 98–136.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 0
    }, {
      "title" : "Semantic contours from inverse detectors",
      "author" : [ "B. Hariharan", "P. Arbeláez", "L. Bourdev", "S. Maji", "J. Malik" ],
      "venue" : "Computer Vision (ICCV), 2011 IEEE International Conference on, pp. 991–998, IEEE, 2011.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Sun rgb-d: A rgb-d scene understanding benchmark suite",
      "author" : [ "S. Song", "S.P. Lichtenberg", "J. Xiao" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 567–576, 2015.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Edge boxes: Locating object proposals from edges",
      "author" : [ "C.L. Zitnick", "P. Dollár" ],
      "venue" : "Computer Vision–ECCV 2014, pp. 391–405, Springer, 2014.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Indoor segmentation and support inference from rgbd images",
      "author" : [ "N. Silberman", "D. Hoiem", "P. Kohli", "R. Fergus" ],
      "venue" : "ECCV, pp. 746–760, Springer, 2012.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Are we ready for autonomous driving? the KITTI vision benchmark suite",
      "author" : [ "A. Geiger", "P. Lenz", "R. Urtasun" ],
      "venue" : "CVPR, pp. 3354–3361, 2012.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Semantic texton forests for image categorization and segmentation",
      "author" : [ "J. Shotton", "M. Johnson", "R. Cipolla" ],
      "venue" : "CVPR, 2008.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Segmentation and recognition using structure from motion point clouds",
      "author" : [ "G. Brostow", "J.J. Shotton", "R. Cipolla" ],
      "venue" : "ECCV, Marseille, 2008.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Combining appearance and structure from motion features for road scene understanding",
      "author" : [ "P. Sturgess", "K. Alahari", "L. Ladicky", "P.H.S.Torr" ],
      "venue" : "BMVC, 2009.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "What, where and how many? combining object detectors and crfs",
      "author" : [ "L. Ladicky", "P. Sturgess", "K. Alahari", "C. Russell", "P.H.S. Torr" ],
      "venue" : "ECCV, pp. 424–437, 2010.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Structured class-labels in random forests for semantic image labelling",
      "author" : [ "P. Kontschieder", "S.R. Bulo", "H. Bischof", "M. Pelillo" ],
      "venue" : "ICCV, pp. 2190–2197, IEEE, 2011.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Semantic segmentation of urban scenes using dense depth maps",
      "author" : [ "C. Zhang", "L. Wang", "R. Yang" ],
      "venue" : "ECCV, pp. 708–721, Springer, 2010.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Superparsing",
      "author" : [ "J. Tighe", "S. Lazebnik" ],
      "venue" : "IJCV, vol. 101, no. 2, pp. 329– 349, 2013.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Rgb-(d) scene labeling: Features and algorithms",
      "author" : [ "X. Ren", "L. Bo", "D. Fox" ],
      "venue" : "CVPR, pp. 2759–2766, IEEE, 2012.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Dense 3D Semantic Mapping of Indoor Scenes from RGB-D Images",
      "author" : [ "A. Hermans", "G. Floros", "B. Leibe" ],
      "venue" : "ICRA, 2014.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Perceptual organization and recognition of indoor scenes from rgb-d images",
      "author" : [ "S. Gupta", "P. Arbelaez", "J. Malik" ],
      "venue" : "CVPR, pp. 564–571, IEEE, 2013.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Scene parsing with multiscale feature learning, purity trees, and optimal covers",
      "author" : [ "C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun" ],
      "venue" : "ICML, 2012.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Deep convolutional networks for scene parsing",
      "author" : [ "D. Grangier", "L. Bottou", "R. Collobert" ],
      "venue" : "ICML Workshop on Deep Learning, 2009.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Unrolling loopy top-down semantic feedback in convolutional deep networks",
      "author" : [ "C. Gatta", "A. Romero", "J. van de Weijer" ],
      "venue" : "CVPR Workshop on Deep Vision, 2014.",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Recurrent convolutional neural networks for scene labeling",
      "author" : [ "P. Pinheiro", "R. Collobert" ],
      "venue" : "ICML, pp. 82–90, 2014.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "ImageNet Large Scale Visual Recognition Challenge",
      "author" : [ "O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei" ],
      "venue" : "International Journal of Computer Vision (IJCV), pp. 1–42, April 2015.",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Microsoft coco: Common objects in context",
      "author" : [ "T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Dollár", "C.L. Zitnick" ],
      "venue" : "Computer Vision–ECCV 2014, pp. 740–755, Springer, 2014.",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Fully connected deep structured networks",
      "author" : [ "A.G. Schwing", "R. Urtasun" ],
      "venue" : "arXiv preprint arXiv:1503.02351, 2015.",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Efficient piecewise training of deep structured models for semantic segmentation",
      "author" : [ "G. Lin", "C. Shen", "I. Reid" ],
      "venue" : "arXiv preprint arXiv:1504.01013, 2015.",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Hypercolumns for object segmentation and fine-grained localization",
      "author" : [ "B. Hariharan", "P.A. Arbeláez", "R.B. Girshick", "J. Malik" ],
      "venue" : "CoRR, vol. abs/1411.5752, 2014.",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Feedforward semantic segmentation with zoom-out features",
      "author" : [ "M. Mostajabi", "P. Yadollahpour", "G. Shakhnarovich" ],
      "venue" : "arXiv preprint arXiv:1412.0774, 2014.",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Deconvolutional networks",
      "author" : [ "M.D. Zeiler", "D. Krishnan", "G.W. Taylor", "R. Fergus" ],
      "venue" : "CVPR, pp. 2528–2535, IEEE, 2010.",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Learning convolutional feature hierarchies for visual recognition",
      "author" : [ "K. Kavukcuoglu", "P. Sermanet", "Y. Boureau", "K. Gregor", "M. Mathieu", "Y. LeCun" ],
      "venue" : "NIPS, pp. 1090–1098, 2010.",
      "citeRegEx" : "46",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Learning a deep convolutional network for image super-resolution",
      "author" : [ "C. Dong", "C.C. Loy", "K. He", "X. Tang" ],
      "venue" : "ECCV, pp. 184–199, Springer, 2014.",
      "citeRegEx" : "47",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Depth map prediction from a single image using a multi-scale deep network",
      "author" : [ "D. Eigen", "C. Puhrsch", "R. Fergus" ],
      "venue" : "arXiv preprint arXiv:1406.2283, 2014.",
      "citeRegEx" : "48",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "S. Ioffe", "C. Szegedy" ],
      "venue" : "CoRR, vol. abs/1502.03167, 2015.",
      "citeRegEx" : "49",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "What is the best multi-stage architecture for object recognition",
      "author" : [ "K. Jarrett", "K. Kavukcuoglu", "M. Ranzato", "Y. LeCun" ],
      "venue" : "ICCV, pp. 2146– 2153, 2009.",
      "citeRegEx" : "50",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
      "author" : [ "K. He", "X. Zhang", "S. Ren", "J. Sun" ],
      "venue" : "arXiv preprint arXiv:1502.01852, 2015.",
      "citeRegEx" : "51",
      "shortCiteRegEx" : null,
      "year" : 1852
    }, {
      "title" : "Caffe: Convolutional architecture for fast feature embedding",
      "author" : [ "Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell" ],
      "venue" : "arXiv preprint arXiv:1408.5093, 2014.",
      "citeRegEx" : "52",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Labelme: online image annotation and applications",
      "author" : [ "A. Torralba", "B.C. Russell", "J. Yuen" ],
      "venue" : "tech. rep., MIT CSAIL Technical Report., 2009.",
      "citeRegEx" : "53",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Vision-based offline-online perception paradigm for autonomous driving",
      "author" : [ "G. Ros", "S. Ramos", "M. Granados", "A. Bakhtiary", "D. Vazquez", "A. Lopez" ],
      "venue" : "WACV, 2015.",
      "citeRegEx" : "54",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Decomposing a scene into geometric and semantically consistent regions",
      "author" : [ "S. Gould", "R. Fulton", "D. Koller" ],
      "venue" : "ICCV, pp. 1–8, IEEE, 2009.",
      "citeRegEx" : "55",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Labelme: a database and web-based tool for image annotation",
      "author" : [ "B.C. Russell", "A. Torralba", "K.P. Murphy", "W.T. Freeman" ],
      "venue" : "IJCV, vol. 77, no. 1-3, pp. 157–173, 2008.",
      "citeRegEx" : "56",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Efficient inference in fully connected crfs with gaussian edge potentials",
      "author" : [ "V. Koltun" ],
      "venue" : "In: NIPS (2011, 2011.",
      "citeRegEx" : "57",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Object detectors emerge in deep scene cnns",
      "author" : [ "B. Zhou", "A. Khosla", "A. Lapedriza", "A. Oliva", "A. Torralba" ],
      "venue" : "arXiv preprint arXiv:1412.6856, 2014.",
      "citeRegEx" : "58",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Neural decision forests for semantic image labelling",
      "author" : [ "Bulo", "S. Rota", "P. Kontschieder" ],
      "venue" : "CVPR, 2014.",
      "citeRegEx" : "60",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Local label descriptor for example based semantic image labeling",
      "author" : [ "Y. Yang", "Z. Li", "L. Zhang", "C. Murphy", "J. Ver Hoeve", "H. Jiang" ],
      "venue" : "ECCV, pp. 361–375, Springer, 2012.",
      "citeRegEx" : "61",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Sift flow: Dense correspondence across different scenes",
      "author" : [ "C. Liu", "J. Yuen", "A. Torralba", "J. Sivic" ],
      "venue" : "ECCV, Marseille, 2008.",
      "citeRegEx" : "62",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Dropout as a bayesian approximation: Insights and applications",
      "author" : [ "Y. Gal", "Z. Ghahramani" ],
      "venue" : "Deep Learning Workshop, ICML, 2015.",
      "citeRegEx" : "63",
      "shortCiteRegEx" : null,
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "VGG16 network [1].",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 1,
      "context" : "network [2] architecture and its variants.",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 3,
      "context" : "In particular, deep learning has seen huge success lately in handwritten digit recognition, speech, categorising whole images and detecting objects in images [4], [5].",
      "startOffset" : 158,
      "endOffset" : 161
    }, {
      "referenceID" : 4,
      "context" : "In particular, deep learning has seen huge success lately in handwritten digit recognition, speech, categorising whole images and detecting objects in images [4], [5].",
      "startOffset" : 163,
      "endOffset" : 166
    }, {
      "referenceID" : 5,
      "context" : "Now there is an active interest for semantic pixel-wise labelling [6] [7], [8], [2], [9], [10], [11], [12], [13], [14].",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 6,
      "context" : "Now there is an active interest for semantic pixel-wise labelling [6] [7], [8], [2], [9], [10], [11], [12], [13], [14].",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 7,
      "context" : "Now there is an active interest for semantic pixel-wise labelling [6] [7], [8], [2], [9], [10], [11], [12], [13], [14].",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 1,
      "context" : "Now there is an active interest for semantic pixel-wise labelling [6] [7], [8], [2], [9], [10], [11], [12], [13], [14].",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 8,
      "context" : "Now there is an active interest for semantic pixel-wise labelling [6] [7], [8], [2], [9], [10], [11], [12], [13], [14].",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 9,
      "context" : "Now there is an active interest for semantic pixel-wise labelling [6] [7], [8], [2], [9], [10], [11], [12], [13], [14].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 10,
      "context" : "Now there is an active interest for semantic pixel-wise labelling [6] [7], [8], [2], [9], [10], [11], [12], [13], [14].",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 11,
      "context" : "Now there is an active interest for semantic pixel-wise labelling [6] [7], [8], [2], [9], [10], [11], [12], [13], [14].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 12,
      "context" : "Now there is an active interest for semantic pixel-wise labelling [6] [7], [8], [2], [9], [10], [11], [12], [13], [14].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 13,
      "context" : "Now there is an active interest for semantic pixel-wise labelling [6] [7], [8], [2], [9], [10], [11], [12], [13], [14].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 5,
      "context" : "However, some of these recent approaches have tried to directly adopt deep architectures designed for category prediction to pixel-wise labelling [6].",
      "startOffset" : 146,
      "endOffset" : 149
    }, {
      "referenceID" : 13,
      "context" : "The results, although very encouraging, appear coarse [14].",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 14,
      "context" : "It must also be able to train end-to-end in order to jointly optimise all the weights in the network using an efficient weight update technique such as stochastic gradient descent (SGD) [15].",
      "startOffset" : 186,
      "endOffset" : 190
    }, {
      "referenceID" : 1,
      "context" : "Networks that are trained end-to-end or equivalently those that do not use multi-stage training [2] or other supporting aids such as region proposals [9] help establish benchmarks that are more easily repeatable.",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 8,
      "context" : "Networks that are trained end-to-end or equivalently those that do not use multi-stage training [2] or other supporting aids such as region proposals [9] help establish benchmarks that are more easily repeatable.",
      "startOffset" : 150,
      "endOffset" : 153
    }, {
      "referenceID" : 0,
      "context" : "The encoder network in SegNet is topologically identical to the convolutional layers in VGG16 [1].",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 1,
      "context" : "We remove the fully connected layers of VGG16 which makes the SegNet encoder network significantly smaller and easier to train than many other recent architectures [2], [9], [11], [16].",
      "startOffset" : 164,
      "endOffset" : 167
    }, {
      "referenceID" : 8,
      "context" : "We remove the fully connected layers of VGG16 which makes the SegNet encoder network significantly smaller and easier to train than many other recent architectures [2], [9], [11], [16].",
      "startOffset" : 169,
      "endOffset" : 172
    }, {
      "referenceID" : 10,
      "context" : "We remove the fully connected layers of VGG16 which makes the SegNet encoder network significantly smaller and easier to train than many other recent architectures [2], [9], [11], [16].",
      "startOffset" : 174,
      "endOffset" : 178
    }, {
      "referenceID" : 15,
      "context" : "We remove the fully connected layers of VGG16 which makes the SegNet encoder network significantly smaller and easier to train than many other recent architectures [2], [9], [11], [16].",
      "startOffset" : 180,
      "endOffset" : 184
    }, {
      "referenceID" : 16,
      "context" : "This idea was inspired from an architecture designed for unsupervised feature learning [17].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 2,
      "context" : "[3].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "advantages; (i) it improves boundary delineation , (ii) it reduces the number of parameters enabling end-to-end training, and (iii) this form of upsampling can be incorporated into any encoder-decoder architecture such as [2], [10] with only a little modification.",
      "startOffset" : 222,
      "endOffset" : 225
    }, {
      "referenceID" : 9,
      "context" : "advantages; (i) it improves boundary delineation , (ii) it reduces the number of parameters enabling end-to-end training, and (iii) this form of upsampling can be incorporated into any encoder-decoder architecture such as [2], [10] with only a little modification.",
      "startOffset" : 227,
      "endOffset" : 231
    }, {
      "referenceID" : 1,
      "context" : "One of the main contributions of this paper is our analysis of the SegNet decoding technique and the widely used Fully Convolutional Network (FCN) [2].",
      "startOffset" : 147,
      "endOffset" : 150
    }, {
      "referenceID" : 8,
      "context" : "Another common feature is they have trainable parameters in the order of hundreds of millions and thus encounter difficulties in performing end-toend training [9].",
      "startOffset" : 159,
      "endOffset" : 162
    }, {
      "referenceID" : 1,
      "context" : "The difficulty of training these networks has led to multi-stage training [2], appending networks to a pre-trained core segmentation engine such as FCN [10], use of supporting aids such as region proposals for inference [9], disjoint training of classification and segmentation networks [16] and use of additional training data for pre-training [11] [18] or for full training [10].",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 9,
      "context" : "The difficulty of training these networks has led to multi-stage training [2], appending networks to a pre-trained core segmentation engine such as FCN [10], use of supporting aids such as region proposals for inference [9], disjoint training of classification and segmentation networks [16] and use of additional training data for pre-training [11] [18] or for full training [10].",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 8,
      "context" : "The difficulty of training these networks has led to multi-stage training [2], appending networks to a pre-trained core segmentation engine such as FCN [10], use of supporting aids such as region proposals for inference [9], disjoint training of classification and segmentation networks [16] and use of additional training data for pre-training [11] [18] or for full training [10].",
      "startOffset" : 220,
      "endOffset" : 223
    }, {
      "referenceID" : 15,
      "context" : "The difficulty of training these networks has led to multi-stage training [2], appending networks to a pre-trained core segmentation engine such as FCN [10], use of supporting aids such as region proposals for inference [9], disjoint training of classification and segmentation networks [16] and use of additional training data for pre-training [11] [18] or for full training [10].",
      "startOffset" : 287,
      "endOffset" : 291
    }, {
      "referenceID" : 10,
      "context" : "The difficulty of training these networks has led to multi-stage training [2], appending networks to a pre-trained core segmentation engine such as FCN [10], use of supporting aids such as region proposals for inference [9], disjoint training of classification and segmentation networks [16] and use of additional training data for pre-training [11] [18] or for full training [10].",
      "startOffset" : 345,
      "endOffset" : 349
    }, {
      "referenceID" : 17,
      "context" : "The difficulty of training these networks has led to multi-stage training [2], appending networks to a pre-trained core segmentation engine such as FCN [10], use of supporting aids such as region proposals for inference [9], disjoint training of classification and segmentation networks [16] and use of additional training data for pre-training [11] [18] or for full training [10].",
      "startOffset" : 350,
      "endOffset" : 354
    }, {
      "referenceID" : 9,
      "context" : "The difficulty of training these networks has led to multi-stage training [2], appending networks to a pre-trained core segmentation engine such as FCN [10], use of supporting aids such as region proposals for inference [9], disjoint training of classification and segmentation networks [16] and use of additional training data for pre-training [11] [18] or for full training [10].",
      "startOffset" : 376,
      "endOffset" : 380
    }, {
      "referenceID" : 13,
      "context" : "In addition, performance boosting post-processing techniques [14] have also been popular.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 18,
      "context" : "Although all these factors improve performance on challenging benchmarks [19], it is unfortunately difficult from their quantitative results to disentangle the key design factors necessary to achieve good performance.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 1,
      "context" : "We therefore analysed the decoding process used in some of these approaches [2], [9] and reveal their pros and cons.",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 8,
      "context" : "We therefore analysed the decoding process used in some of these approaches [2], [9] and reveal their pros and cons.",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 18,
      "context" : "We evaluate the performance of SegNet on PascalVOC12 salient object(s) segmentation [19], [20] and scene understanding challenges such as CamVid road scene segmentation [3] and SUN RGB-D indoor scene segmentation [21].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 19,
      "context" : "We evaluate the performance of SegNet on PascalVOC12 salient object(s) segmentation [19], [20] and scene understanding challenges such as CamVid road scene segmentation [3] and SUN RGB-D indoor scene segmentation [21].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 2,
      "context" : "We evaluate the performance of SegNet on PascalVOC12 salient object(s) segmentation [19], [20] and scene understanding challenges such as CamVid road scene segmentation [3] and SUN RGB-D indoor scene segmentation [21].",
      "startOffset" : 169,
      "endOffset" : 172
    }, {
      "referenceID" : 20,
      "context" : "We evaluate the performance of SegNet on PascalVOC12 salient object(s) segmentation [19], [20] and scene understanding challenges such as CamVid road scene segmentation [3] and SUN RGB-D indoor scene segmentation [21].",
      "startOffset" : 213,
      "endOffset" : 217
    }, {
      "referenceID" : 18,
      "context" : "Pascal VOC12 [19] is the benchmark for segmentation due to its size and challenges, but the majority of this task has one or two foreground classes surrounded by a highly varied background.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 15,
      "context" : "This implicitly favours techniques used for detection as shown by the recent work on a decoupled classification-segmentation network [16] where the classification network can be trained with a large set of weakly labelled data and the independent segmentation network performance is improved.",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 13,
      "context" : "The method of [14] also use the feature maps of the classification network with an independent CRF post-processing technique to perform segmentation.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 8,
      "context" : "The performance can also be boosted by the use additional inference aids such as region proposals [9], [22].",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 21,
      "context" : "The performance can also be boosted by the use additional inference aids such as region proposals [9], [22].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 2,
      "context" : "Semantic pixel-wise segmentation is an active topic of research, fuelled by challenging datasets [3], [19], [21], [23], [24].",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 18,
      "context" : "Semantic pixel-wise segmentation is an active topic of research, fuelled by challenging datasets [3], [19], [21], [23], [24].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 20,
      "context" : "Semantic pixel-wise segmentation is an active topic of research, fuelled by challenging datasets [3], [19], [21], [23], [24].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 22,
      "context" : "Semantic pixel-wise segmentation is an active topic of research, fuelled by challenging datasets [3], [19], [21], [23], [24].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 23,
      "context" : "Semantic pixel-wise segmentation is an active topic of research, fuelled by challenging datasets [3], [19], [21], [23], [24].",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 24,
      "context" : "Forest [25], [26] or Boosting [27], [28] to predict the class probabilities of the center pixel.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 25,
      "context" : "Forest [25], [26] or Boosting [27], [28] to predict the class probabilities of the center pixel.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 26,
      "context" : "Forest [25], [26] or Boosting [27], [28] to predict the class probabilities of the center pixel.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 27,
      "context" : "Forest [25], [26] or Boosting [27], [28] to predict the class probabilities of the center pixel.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 24,
      "context" : "Features based on appearance [25] or SfM and appearance [26], [27], [28] have been explored for the CamVid road scene understanding test [3].",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 25,
      "context" : "Features based on appearance [25] or SfM and appearance [26], [27], [28] have been explored for the CamVid road scene understanding test [3].",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 26,
      "context" : "Features based on appearance [25] or SfM and appearance [26], [27], [28] have been explored for the CamVid road scene understanding test [3].",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 27,
      "context" : "Features based on appearance [25] or SfM and appearance [26], [27], [28] have been explored for the CamVid road scene understanding test [3].",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 2,
      "context" : "Features based on appearance [25] or SfM and appearance [26], [27], [28] have been explored for the CamVid road scene understanding test [3].",
      "startOffset" : 137,
      "endOffset" : 140
    }, {
      "referenceID" : 26,
      "context" : "These per-pixel noisy predictions (often called unary terms) from the classifiers are then smoothed by using a pair-wise or higher order CRF [27], [28] to improve the accuracy.",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 27,
      "context" : "These per-pixel noisy predictions (often called unary terms) from the classifiers are then smoothed by using a pair-wise or higher order CRF [27], [28] to improve the accuracy.",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 28,
      "context" : "This improves the results of Random Forest based unaries [29] but thin structured classes are classfied poorly.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 29,
      "context" : "Dense depth maps computed from the CamVid video have also been used as input for classification using Random Forests [30].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 30,
      "context" : "Another approach argues for the use of a combination of popular hand designed features and spatio temporal super-pixelization to obtain higher accuracy [31].",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 27,
      "context" : "The best performing technique on the CamVid test [28] addresses the imbalance among label frequencies by combining object detection outputs with classifier predictions in a CRF framework.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 22,
      "context" : "Indoor RGBD pixel-wise semantic segmentation has also gained popularity since the release of the NYU dataset [23].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 31,
      "context" : "Improvements were made using a richer feature set including LBP and region segmentation to obtain higher accuracy [32] followed by a CRF.",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 22,
      "context" : "In more recent work [23], both class segmentation and support relationships are inferred together using a combination of RGB and depth based cues.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 32,
      "context" : "Another approach focusses on real-time joint reconstruction and semantic segmentation, where Random Forests are used as the classifier [33].",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 33,
      "context" : "[34] use boundary detection and hierarchical grouping before performing category segmentation.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 5,
      "context" : "There have also been attempts to apply networks designed for object categorization to segmentation, particularly by replicating the deepest layer features in blocks to match image dimensions [6], [35], [36], [37].",
      "startOffset" : 191,
      "endOffset" : 194
    }, {
      "referenceID" : 34,
      "context" : "There have also been attempts to apply networks designed for object categorization to segmentation, particularly by replicating the deepest layer features in blocks to match image dimensions [6], [35], [36], [37].",
      "startOffset" : 196,
      "endOffset" : 200
    }, {
      "referenceID" : 35,
      "context" : "There have also been attempts to apply networks designed for object categorization to segmentation, particularly by replicating the deepest layer features in blocks to match image dimensions [6], [35], [36], [37].",
      "startOffset" : 202,
      "endOffset" : 206
    }, {
      "referenceID" : 36,
      "context" : "There have also been attempts to apply networks designed for object categorization to segmentation, particularly by replicating the deepest layer features in blocks to match image dimensions [6], [35], [36], [37].",
      "startOffset" : 208,
      "endOffset" : 212
    }, {
      "referenceID" : 35,
      "context" : "However, the resulting classification is blocky [36].",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 37,
      "context" : "Another approach using recurrent neural networks [38] merges several low resolution predictions to create input image resolution predictions.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 5,
      "context" : "These techniques are already an improvement over hand engineered features [6] but their ability to delineate boundaries is poor.",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 1,
      "context" : "Newer deep architectures [2], [9], [10], [13], [16] particularly designed for segmentation have advanced the state-of-the-art by learning to decode or map low resolution image representations to pixel-wise predictions.",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 8,
      "context" : "Newer deep architectures [2], [9], [10], [13], [16] particularly designed for segmentation have advanced the state-of-the-art by learning to decode or map low resolution image representations to pixel-wise predictions.",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 9,
      "context" : "Newer deep architectures [2], [9], [10], [13], [16] particularly designed for segmentation have advanced the state-of-the-art by learning to decode or map low resolution image representations to pixel-wise predictions.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 12,
      "context" : "Newer deep architectures [2], [9], [10], [13], [16] particularly designed for segmentation have advanced the state-of-the-art by learning to decode or map low resolution image representations to pixel-wise predictions.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 15,
      "context" : "Newer deep architectures [2], [9], [10], [13], [16] particularly designed for segmentation have advanced the state-of-the-art by learning to decode or map low resolution image representations to pixel-wise predictions.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 0,
      "context" : "The encoder network which produces these low resolution representations in all of these architectures is the VGG16 classification network [1] which has 13 convolutional layers and 3 fully connected layers.",
      "startOffset" : 138,
      "endOffset" : 141
    }, {
      "referenceID" : 38,
      "context" : "This encoder network weights are typically pre-trained on the large ImageNet object classification dataset [39].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 1,
      "context" : "Each decoder in the Fully Convolutional Network (FCN) architecture [2] learns to upsample its input feature map(s) and combines them with the corresponding encoder feature map to produce the input to the next decoder.",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 8,
      "context" : "This growth is stopped after three decoders thus ignoring high resolution feature maps can certainly lead to loss of edge information [9].",
      "startOffset" : 134,
      "endOffset" : 137
    }, {
      "referenceID" : 9,
      "context" : "We study this network in more detail as it the core of other recent architectures [10], [11].",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 10,
      "context" : "We study this network in more detail as it the core of other recent architectures [10], [11].",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 9,
      "context" : "The predictive performance of FCN has been improved further by appending the FCN with a recurrent neural network (RNN) [10] and fine-tuning them on large datasets [19], [40].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 18,
      "context" : "The predictive performance of FCN has been improved further by appending the FCN with a recurrent neural network (RNN) [10] and fine-tuning them on large datasets [19], [40].",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 39,
      "context" : "The predictive performance of FCN has been improved further by appending the FCN with a recurrent neural network (RNN) [10] and fine-tuning them on large datasets [19], [40].",
      "startOffset" : 169,
      "endOffset" : 173
    }, {
      "referenceID" : 40,
      "context" : "The fact that joint training helps is also shown in other recent results [41], [42].",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 41,
      "context" : "The fact that joint training helps is also shown in other recent results [41], [42].",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 8,
      "context" : "Interestingly, the deconvolutional network [9] performs significantly better than FCN although at the cost of a more complex training and inference.",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 12,
      "context" : "Multi-scale deep architectures are also being pursued [13], [42].",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 41,
      "context" : "Multi-scale deep architectures are also being pursued [13], [42].",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 42,
      "context" : "They come in two flavours, (i) those which use input images at a few scales and corresponding deep feature extraction networks, and (ii) those which combine feature maps from different layers of a single deep architecture [43] [11].",
      "startOffset" : 222,
      "endOffset" : 226
    }, {
      "referenceID" : 10,
      "context" : "They come in two flavours, (i) those which use input images at a few scales and corresponding deep feature extraction networks, and (ii) those which combine feature maps from different layers of a single deep architecture [43] [11].",
      "startOffset" : 227,
      "endOffset" : 231
    }, {
      "referenceID" : 43,
      "context" : "The common idea is to use features extracted at multiple scales to provide both local and global context [44] and the using feature maps of the early encoding layers retain more high frequency detail leading to sharper class boundaries.",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 12,
      "context" : "Some of these architectures are difficult to train due to their parameter size [13].",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 41,
      "context" : "Others [42] append a CRF to their multi-scale network and jointly train them.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 8,
      "context" : "Several of the recently proposed deep architectures for segmentation are not feed-forward in inference time [9], [14], [16].",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 13,
      "context" : "Several of the recently proposed deep architectures for segmentation are not feed-forward in inference time [9], [14], [16].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 15,
      "context" : "Several of the recently proposed deep architectures for segmentation are not feed-forward in inference time [9], [14], [16].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 41,
      "context" : "They require either MAP inference over a CRF [42], [41] or aids such as region proposals [9] for inference.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 40,
      "context" : "They require either MAP inference over a CRF [42], [41] or aids such as region proposals [9] for inference.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 8,
      "context" : "They require either MAP inference over a CRF [42], [41] or aids such as region proposals [9] for inference.",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 8,
      "context" : "The recently proposed Deconvolutional Network [9] and its semi-supervised variant the Decoupled network [16] use the max locations of the encoder feature maps (pooling indices) to perform non-linear upsampling in the decoder network.",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 15,
      "context" : "The recently proposed Deconvolutional Network [9] and its semi-supervised variant the Decoupled network [16] use the max locations of the encoder feature maps (pooling indices) to perform non-linear upsampling in the decoder network.",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 11,
      "context" : "CVPR 2015 [12]), proposed this idea of decoding in the decoder network.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 13,
      "context" : "Another recent method [14] shows the benefit of reducing the number of parameters significantly without sacrificing performance, reducing memory consumption and improving inference time.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 16,
      "context" : "[17].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 44,
      "context" : "A somewhat similar decoding technique is used for visualizing trained convolutional networks [45] for classification.",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 45,
      "context" : "[46] to accept full image sizes as input to learn hierarchical encoders.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 46,
      "context" : "Other applications where pixel wise predictions are made using deep networks are image super-resolution [47] and depth map prediction from a single image [48].",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 47,
      "context" : "Other applications where pixel wise predictions are made using deep networks are image super-resolution [47] and depth map prediction from a single image [48].",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 47,
      "context" : "The authors in [48] discuss the need for learning to upsample from low resolution feature maps which is the central topic of this paper.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 0,
      "context" : "The encoder network consists of 13 convolutional layers which correspond to the first 13 convolutional layers in the VGG16 network [1] designed for object classification.",
      "startOffset" : 131,
      "endOffset" : 134
    }, {
      "referenceID" : 38,
      "context" : "We can therefore initialize the training process from weights trained for classification on large datasets [39].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 1,
      "context" : "7M) as compared to other recent architectures [2], [9] (see.",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 8,
      "context" : "7M) as compared to other recent architectures [2], [9] (see.",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 48,
      "context" : "These are then batch normalized [49]).",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 1,
      "context" : "Many segmentation architectures [2], [9], [14] share the same encoder network and they only vary in the form of their decoder network.",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 8,
      "context" : "Many segmentation architectures [2], [9], [14] share the same encoder network and they only vary in the form of their decoder network.",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 13,
      "context" : "Many segmentation architectures [2], [9], [14] share the same encoder network and they only vary in the form of their decoder network.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 1,
      "context" : "Of these we choose to compare the SegNet decoding technique with the widely used Fully Convolutional Network (FCN) decoding technique [2], [10].",
      "startOffset" : 134,
      "endOffset" : 137
    }, {
      "referenceID" : 9,
      "context" : "Of these we choose to compare the SegNet decoding technique with the widely used Fully Convolutional Network (FCN) decoding technique [2], [10].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 11,
      "context" : "SegNet-Basic was earlier termed SegNet in a archival version of this paper [12] compresses the encoder feature maps which are then used in the corresponding decoders.",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 1,
      "context" : "The upsampling kernels are initialized using bilinear interpolation weights [2].",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 5,
      "context" : "We also tried other generic variants where feature maps are simply upsampled by replication [6], or by using a fixed (and sparse) array of indices for upsampling.",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 49,
      "context" : "We perform local contrast normalization [50] to the RGB input.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 50,
      "context" : "[51].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "9 [15] using our Caffe implementation of SegNet-Basic [52].",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 51,
      "context" : "9 [15] using our Caffe implementation of SegNet-Basic [52].",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 1,
      "context" : "An illustration of SegNet and FCN [2] decoders.",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 1,
      "context" : "We use the cross-entropy loss [2] as the objective function for training the network.",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 12,
      "context" : "We use median frequency balancing [13] where the weight assigned to a class in the loss function is the ratio of the median of class frequencies computed on the entire training set divided by the class frequency.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 18,
      "context" : "To compare the quantitative performance of the different decoder variants, we use three commonly used performance measures: global accuracy (G) which measures the percentage of pixels correctly classified in the dataset, class average accuracy (C) is the mean of the predictive accuracy over all classes and mean intersection over union (I/U) over all classes as used in the Pascal VOC12 challenge [19].",
      "startOffset" : 398,
      "endOffset" : 402
    }, {
      "referenceID" : 8,
      "context" : "This is also supported by experimental evidence gathered by other authors when comparing FCN with SegNet-type decoding techniques [9].",
      "startOffset" : 130,
      "endOffset" : 133
    }, {
      "referenceID" : 24,
      "context" : "We use this result to compare SegNet with several methods including Random Forests [25], Boosting [25], [27] in combination with CRF based methods [28].",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 24,
      "context" : "We use this result to compare SegNet with several methods including Random Forests [25], Boosting [25], [27] in combination with CRF based methods [28].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 26,
      "context" : "We use this result to compare SegNet with several methods including Random Forests [25], Boosting [25], [27] in combination with CRF based methods [28].",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 27,
      "context" : "We use this result to compare SegNet with several methods including Random Forests [25], Boosting [25], [27] in combination with CRF based methods [28].",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 2,
      "context" : "We also trained SegNet on a larger dataset of road scenes collected from various publicly available datasets [3], [53], [54] and show that this leads to a large improvement in accuracy.",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 52,
      "context" : "We also trained SegNet on a larger dataset of road scenes collected from various publicly available datasets [3], [53], [54] and show that this leads to a large improvement in accuracy.",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 53,
      "context" : "We also trained SegNet on a larger dataset of road scenes collected from various publicly available datasets [3], [53], [54] and show that this leads to a large improvement in accuracy.",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 20,
      "context" : "SUN RGB-D [21] is a very challenging and large dataset of indoor scenes with 5285 training and 5050 testing images.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 1,
      "context" : "Using the depth modality would necessitate architectural modifications/redesign [2].",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 22,
      "context" : "We also note that an earlier benchmark dataset NYUv2 [23] is included as part of this dataset.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 18,
      "context" : "Pascal VOC12 [19] is a RGB dataset for segmentation with 12031 combined training and validation images of indoor and outdoor scenes.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 2,
      "context" : "A number of outdoor scene datasets are available for semantic parsing [3], [24], [55], [56].",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 23,
      "context" : "A number of outdoor scene datasets are available for semantic parsing [3], [24], [55], [56].",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 54,
      "context" : "A number of outdoor scene datasets are available for semantic parsing [3], [24], [55], [56].",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 55,
      "context" : "A number of outdoor scene datasets are available for semantic parsing [3], [24], [55], [56].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 2,
      "context" : "Of these we choose to benchmark SegNet using the CamVid dataset [3] as it contains video sequences.",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 25,
      "context" : "This enables us to compare our proposed architecture with those which use motion and structure [26], [27], [28] and video segments [31].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 26,
      "context" : "This enables us to compare our proposed architecture with those which use motion and structure [26], [27], [28] and video segments [31].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 27,
      "context" : "This enables us to compare our proposed architecture with those which use motion and structure [26], [27], [28] and video segments [31].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 30,
      "context" : "This enables us to compare our proposed architecture with those which use motion and structure [26], [27], [28] and video segments [31].",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 56,
      "context" : "More dense CRF models [57] can be better but with additional cost of inference.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 10,
      "context" : "Another explanation could be that the size of the receptive fields of the deepest layer feature units are smaller than their theoretical estimates [11], [58] and hence unable to group all the side-walk pixels into one class.",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 57,
      "context" : "Another explanation could be that the size of the receptive fields of the deepest layer feature units are smaller than their theoretical estimates [11], [58] and hence unable to group all the side-walk pixels into one class.",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 11,
      "context" : "We also find that SegNetBasic [12] trained in a layer-wise manner using L-BFGS [59] also performs competitively and is better than SegNet-Basic trained with SGD (see Sec.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 20,
      "context" : "Some test samples from the recent SUN RGB-D dataset [21] are shown in Fig.",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 24,
      "context" : "The evolution of results from various patch based predictions [25], [26], then combined with CRF smoothing models [27], [28].",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 25,
      "context" : "The evolution of results from various patch based predictions [25], [26], then combined with CRF smoothing models [27], [28].",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 26,
      "context" : "The evolution of results from various patch based predictions [25], [26], then combined with CRF smoothing models [27], [28].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 27,
      "context" : "The evolution of results from various patch based predictions [25], [26], then combined with CRF smoothing models [27], [28].",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 10,
      "context" : "Large sized side-walk is not smoothly segmented possibly because the empirical size of the receptive fields of deep feature units is not large enough [11], [58].",
      "startOffset" : 150,
      "endOffset" : 154
    }, {
      "referenceID" : 57,
      "context" : "Large sized side-walk is not smoothly segmented possibly because the empirical size of the receptive fields of deep feature units is not large enough [11], [58].",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 25,
      "context" : "SfM+Appearance [26] 46.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 26,
      "context" : "1 n/a Boosting [27] 61.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 29,
      "context" : "4 n/a Dense Depth Maps [30] 85.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 28,
      "context" : "1 n/a Structured Random Forests [29] n/a 51.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 58,
      "context" : "5 n/a Neural Decision Forests [60] n/a 56.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 59,
      "context" : "1 n/a Local Label Descriptors [61] 80.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 30,
      "context" : "6 n/a Super Parsing [31] 87.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 11,
      "context" : "3 SegNet-Basic (layer-wise training [12]) 75.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 26,
      "context" : "1 CRF based approaches Boosting + pairwise CRF [27] 70.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 26,
      "context" : "8 n/a Boosting+Higher order [27] 84.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 27,
      "context" : "8 n/a Boosting+Detectors+CRF [28] 81.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 2,
      "context" : "TABLE 2 Quantitative results on CamVid [3] consisting of 11 road scene categories.",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 18,
      "context" : "Other challenges such as Pascal VOC12 [19] salient object segmentation have occupied researchers more, but indoor scene segmentation is more challenging and has more practical applications such as in robotics.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 31,
      "context" : "The existing top performing method [32] relies on hand engineered features using colour, gradients and surface normals for describing super-pixels and then smooths super-pixel labels with a CRF.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 60,
      "context" : "[62] n/a 9.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 60,
      "context" : "[62] n/a 10.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 31,
      "context" : "al [32] n/a 36.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 31,
      "context" : "SegNet RGB based predictions have a high global accuracy and also matches the RGB-D based predictions [32] in terms of class average accuracy.",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 18,
      "context" : "The Pascal VOC12 segmentation challenge [19] consists of segmenting a few salient object classes from a widely varying background class.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 1,
      "context" : "Our efforts in this benchmarking experiment have not been diverted towards attaining the top rank by either using multi-stage training [2], other datasets for pre-training such as MS-COCO [10], [40], training and inference aids such as object proposals [9], [22] or post-processing using CRF based methods [9], [14].",
      "startOffset" : 135,
      "endOffset" : 138
    }, {
      "referenceID" : 9,
      "context" : "Our efforts in this benchmarking experiment have not been diverted towards attaining the top rank by either using multi-stage training [2], other datasets for pre-training such as MS-COCO [10], [40], training and inference aids such as object proposals [9], [22] or post-processing using CRF based methods [9], [14].",
      "startOffset" : 188,
      "endOffset" : 192
    }, {
      "referenceID" : 39,
      "context" : "Our efforts in this benchmarking experiment have not been diverted towards attaining the top rank by either using multi-stage training [2], other datasets for pre-training such as MS-COCO [10], [40], training and inference aids such as object proposals [9], [22] or post-processing using CRF based methods [9], [14].",
      "startOffset" : 194,
      "endOffset" : 198
    }, {
      "referenceID" : 8,
      "context" : "Our efforts in this benchmarking experiment have not been diverted towards attaining the top rank by either using multi-stage training [2], other datasets for pre-training such as MS-COCO [10], [40], training and inference aids such as object proposals [9], [22] or post-processing using CRF based methods [9], [14].",
      "startOffset" : 253,
      "endOffset" : 256
    }, {
      "referenceID" : 21,
      "context" : "Our efforts in this benchmarking experiment have not been diverted towards attaining the top rank by either using multi-stage training [2], other datasets for pre-training such as MS-COCO [10], [40], training and inference aids such as object proposals [9], [22] or post-processing using CRF based methods [9], [14].",
      "startOffset" : 258,
      "endOffset" : 262
    }, {
      "referenceID" : 8,
      "context" : "Our efforts in this benchmarking experiment have not been diverted towards attaining the top rank by either using multi-stage training [2], other datasets for pre-training such as MS-COCO [10], [40], training and inference aids such as object proposals [9], [22] or post-processing using CRF based methods [9], [14].",
      "startOffset" : 306,
      "endOffset" : 309
    }, {
      "referenceID" : 13,
      "context" : "Our efforts in this benchmarking experiment have not been diverted towards attaining the top rank by either using multi-stage training [2], other datasets for pre-training such as MS-COCO [10], [40], training and inference aids such as object proposals [9], [22] or post-processing using CRF based methods [9], [14].",
      "startOffset" : 311,
      "endOffset" : 315
    }, {
      "referenceID" : 20,
      "context" : "Qualitative assessment of SegNet predictions on RGB indoor test scenes from the recently released SUN RGB-D dataset [21].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 13,
      "context" : "We also specify when a method reports the performance of its core engine on the smaller validation set of 346 images [14].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 8,
      "context" : "From the results in Table 5 we can see that the best performing networks are either very large (and slow) [9] and/or they use a CRF [10].",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 9,
      "context" : "From the results in Table 5 we can see that the best performing networks are either very large (and slow) [9] and/or they use a CRF [10].",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 9,
      "context" : "This is shown in the experiments using CRF-RNN [10] wherein the core FCN-8 model predictions are less accurate without extra training data.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 13,
      "context" : "It is interesting that the DeepLab [14] architecture which is simply upsampling the FCN encoder features using bilinear interpolation performs reasonably well (on the validation set).",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 8,
      "context" : "Methods using object proposals during training and/or inference [9], [43] are very slow in inference time and it is hard to measure their true performance.",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 42,
      "context" : "Methods using object proposals during training and/or inference [9], [43] are very slow in inference time and it is hard to measure their true performance.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 8,
      "context" : "These aids are necessitated by the very large size of their deep network [9] and also because the Pascal data can also be processed by a detect and segment approach.",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 0,
      "context" : "In comparison, SegNet is smaller by virtue of discarding the fully connected layers in the VGG16 [1].",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 13,
      "context" : "The authors of DeepLab [14] have also reported little loss in performance by reducing the size of the fully connected layers.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 39,
      "context" : "We believe more training data [40] can help improve the performance of SegNet.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 61,
      "context" : "We are also interested in experimenting with Dropout [63] during training and testing.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 18,
      "context" : "Qualitative assessment of SegNet predictions on test samples from Pascal VOC12 [19] dataset.",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 57,
      "context" : "This can be perhaps be attributed to the smaller empirical size of the receptive field of the feature units in the deepest encoder layer size [58].",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 13,
      "context" : "DeepLab [14] (validation set) n/a n/a < 134.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 1,
      "context" : "5 58 n/a n/a FCN-8 [2] (multi-stage training) 134 0.",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 42,
      "context" : "2 210 n/a Hypercolumns [43] (object proposals) n/a n/a > 134.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 8,
      "context" : "6 n/a n/a DeconvNet [9] (object proposals) 138.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 9,
      "context" : "26 (× 50) CRF-RNN [10] (multi-stage training) n/a n/a > 134.",
      "startOffset" : 18,
      "endOffset" : 22
    } ],
    "year" : 2015,
    "abstractText" : "We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1]. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the fully convolutional network [2] architecture and its variants. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. The design of SegNet was primarily motivated by road scene understanding applications. Hence, it is efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than competing architectures and can be trained end-to-end using stochastic gradient descent without complex training protocols. We also benchmark the performance of SegNet on Pascal VOC12 salient object segmentation and the recent SUN RGB-D indoor scene understanding challenge. These quantitative assessments show that SegNet provides competitive performance although it is significantly smaller than other architectures. We also provide a Caffe implementation of SegNet and a webdemo at http://mi.eng.cam.ac.uk/projects/segnet/.",
    "creator" : "LaTeX with hyperref package"
  }
}