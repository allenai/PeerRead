{
  "name" : "1301.3845.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Separation Properties of Sets of Probability Measures",
    "authors" : [ "Fabio G. Cozman" ],
    "emails" : [ "fgcozman@usp.br," ],
    "sections" : [ {
      "heading" : null,
      "text" : "This paper analyzes independence concepts for sets of probability measures associated with directed acyclic graphs. The paper shows that epistemic independence and the standard Markov condition violate desirable separation properties. The adoption of a contraction condition leads to d-separation but still fails to guarantee a belief separa tion property. To overcome this unsatisfac tory situation, a strong Markov condition is proposed, based on epistemic independence. The main result is that the strong Markov condition leads to strong independence and does enforce separation properties; this re sult implies that (1) separation properties of Bayesian networks do extend to epistemic in dependence and sets of probability measures, and (2) strong independence has a clear justi fication based on epistemic independence and the strong Markov condition.\n1 Introduction\nSets of probability measures, called credal sets by Levi [17], are quite flexible representations for uncer tainty, employed in several theories [3, 12, 14, 15, 17, 18, 20, 21, 23]. Credal sets can be used to represent the opinions of a group of experts, to model the impre cision and incompleteness of realistic decision making, and to investigate the effect of perturbations in stan dard probabilistic models.\nThis paper focuses on credal sets that are represented by directed acyclic graphs. Such structures are called credal networks1 and have been investigated by sev eral authors [1, 4, 11, 22]. The idea is to associate\nThe author was partially supported by a grant from CNPq, Brazil.\n1The author has employed the term Quasi-Bayesian\na local credal set with each node in a directed acyclic graph, mimicking the theory of Bayesian networks [19]. Figure 1 shows a simple credal network; note that all nodes are associated with probability intervals. Sec tions 2 and 3 briefly review the main ideas connected with credal sets and credal networks.\nThere are two difficulties with credal networks. First, there are several concepts of independence for credal sets. Second, there are several ways to combine locally defined credal sets in a credal network, i.e., several extensions can be defined for a given network. Conse quently, it is hard to tell exactly what a credal network represents. Take the network in Figure 1: Is it repre senting a credal set where W and Y are epistemically independent given X , or a credal set where W and Y are strongly independent given X? Is it representing the largest credal set for a given concept of indepen dence, or a credal set constructed in some special way?\nThe objective of this paper is to present a theory of credal networks that overcomes the difficulties de scribed in the previous paragraph. The idea is to ana lyze concepts of independence and methods of exten sion through their separation properties. Sections 4 and 5 present current approaches to credal networks and analyze their weaknesses. Based on this analysis, a new condition, called the strong Markov condition, is presented (Section 6). The strong Markov condition can alone organize the theory of credal sets: it implies that a small number of intuitive elements leads to a unique, computationally tractable type of extension for credal networks, with pleasant separation proper ties.\nThe argument leading to the strong Markov condition can be summarized as follows. There are two impor tant concepts of independence for credal sets: strong and epistemic independence (Section 2). Strong in dependence leads to strong extensions, which inherit\nnetworks [7, 8]; the term credal networks, proposed by Zaf falon [24], seems more appropriate.\n108 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\n®-0-0-0\n0. 2 � p(w) � 0. 3 0. 1 � p(xlw) � 0. 2 0. 4 � p(ylx) � 0.5 0. 7 � p(zly) � 0. 8 0. 8 � p(xlwc) � 0.9 0.5 � p(ylxc) � 0. 6 0. 1 � p(zlyc) � 0. 2\nFigure 1: Credal network with four binary variables (superscript c indicates negation).\nseveral nice features of Bayesian networks, including computational simplicity (Section 4). But strong in dependence is mostly a mathematical generalization of standard stochastic independence without a clear, direct justification. Section 5 moves the focus from strong independence to epistemic independence. Sec tion 5 investigates the separation properties of exten sions based on epistemic independence and the stan dard Markov condition. The rationale for this inves tigation is that the separation properties of standard stochastic independence are fundamental in the theory of Bayesian networks. In fact, separation properties are not just implied by stochastic independence, sep aration properties validate the concept of stochastic independence. Section 5 presents a number of novel examples to demonstrate that epistemic independence and the standard Markov condition violate desirable separation properties; the examples also indicate that epistemic independence leads to extensions that are intractable in practice. Given the weaknesses of the extensions analyzed in Sections 4 and 5 , we are led to ask whether there is some intuitive condition on a credal network that (1) is based on epistemic inde pendence, and (2) produces a suitable, tractable type of extension. The strong Markov condition is such a condition, as argued in Section 7.\n2 Credal sets\nThis section contains basic definitions and notation used in the paper. For discrete random variables X and Y, p(X) denotes the probability density of X , p(XIy) denotes the conditional density of X given the event {Y = y } , Ep[f(X)) denotes the expectation of function f(X) with respect to p(X), and Ep[f(X)Iy) denotes the expectation of function f(X) with respect to p(XIy) .\nA credal set K is a collection of probability mea sures. A credal set defined by a collection of den sities p(X) is denoted by K(X). Given a credal set K(X) and a function f(X), the lower and up per expectations of f(X) are defined respectively as\nE[f(X)) = minl(X)EK(X) Ep[f(X)) and E[f(X)) = maxl(X)EK(X) Ep[f(X)]. A credal set defines a unique lower expectation for every bounded function. Sim ilarly, the lower probability and the upper probabil ity of event A are defined respectively as P(A) = minl(X)EK(X) P(A) and P(A) = maxl(X)EK(X) P(A).\nA set of probability measures and its convex hull pro duce the same lower and upper expectations. The no tation 0K indicates the convex hull of a set of mea sures K and the notation extK indicates the extreme points of a credal set K.\nInference is performed by applying Bayes rule to each measure in a credal set; the posterior credal set is the union of all posterior probability measures [14). A con ditional credal set K(XIy) contains densities p(XIy) for random variables X and Y.\nThere are several concepts of independence that can be applied to credal sets [6, 10). To understand them, consider the two definitions of stochastic independence in probability theory. Variables X and Y are stochas tically independent either if p(XIy) = p(X) for any value y , or if p(X, Y) = p(X) p(Y) (appropriate pos itivity conditions may be required). Several authors have tried to adapt both methods to the theory of credal sets using various multiplication operators and conditioning rules. The next subsections describe con cepts that can be cast in the language of credal sets.\n2.1 Epistemic independence\nThe idea of epistemic independence [23, Chap. 9) is to start from an asymmetric irrelevance relation and then define independence from irrelevance.\nDefinition 1 Variable Y is epistemically irrelevant to X given Z, denoted by (Y EIR X I Z), if K(XIz) and K(XIy, z) have the same convex hull for all possible values of Y and Z.\nIn terms of lower expectations, (Y EIR X I Z) iff E[f(X)Iy, z ] = E[f(X)Iz) for any function f(X) and all possible values of Y and Z. Epistemic irrelevance gets an intuitive significance when we think that lower and upper expectations are the practical consequences of beliefs, as they are used for decision-making and in ference. Irrelevance of Y to X means that the practical consequences of our beliefs about X are not affected by knowledge of Y. If we take two credal sets to be equivalent when their convex hulls agree, we have that (Y EIR X I Z) iff K(XIz) and K(XIy, z) are equivalent for all possible values of Y and Z. A more stringent definition would be to impose equality between K(XIz) and K(XIy, z) for all possible values of Y and Z. If K(XIz) and K(XIy, z) are non-convex, then equality\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 109\nand equivalence have different meanings.\nEpistemic independence is just a symmetrization of epistemic irrelevance.\nDefinition 2 Variables X and Y are epistemically independent given Z, denoted by (X EIN Y I Z), if (X EIR Y I Z) and (Y EIR X I Z).\n2.2 Strong independence\nConsider the following concept:\nDefinition 3 Variables X and Y are strongly inde pendent when every extreme point of K(X, Y) satisfies standard stochastic independence of X and Y.\nThis concept is called independence in the selection by Couso et al [6]; they employ strong independence when the joint credal set is the largest credal set that satisfies Definition 3. Here we prefer to use strong independence and take the largest credal set as the strong extension (Section 4) .\nWe can even ask that every probability density in a credal set factorizes according to stochastic indepen dence. In this case the resulting credal set is non convex [16]. Note that from a behavioral point of view, insisting on stochastic independence for all probabili ties in a credal set is excessive because it suggests that a credal set and its convex hull are different. But two individuals with the exact same lower and upper ex pectations should have equivalent beliefs, at least from a behavioral perspective. On the other hand, adopting convexity significantly diminishes the justification for stochastic independence. Even if all extreme points of a convex credal set display independence between two variables, there may be densities in the credal set that display dependence between the variables [2, 5]. To accommodate these conflicting views, the definition of strong independence does not require (nor prohibits) convexity.\nStrong independence is reasonable in a \"sensitivity analysis\" interpretation of credal sets: a credal set is then viewed as a set containing a \"true\" probability, and not as a representation of uncertainty in itself [23). Strong independence is also reasonable when a collec tion of experts, specifying a credal set, agrees that ev ery lower expectation must be computed with respect to a density that displays stochastic independence.\nDespite the intuitive appeal of strong independence, there is no known direct justification for it - a justi fication that does not employ stochastic independence as a starting point. Definitions of strong independence usually assume stochastic independence and standard probability - a most undesirable situation, as the\ntheory of credal sets purports to offer a more basic approach to uncertainty, an approach that contains standard probability theory as a limiting case. Strong independence stands as a mathematically-minded gen eralization of the factorization property of standard stochastic independence.\nOne concept of independence that might character ize strong independence through lower expectations is Kuznetsov's independence. Variables X and Y are (Kuznetsov-)independent given Z if, for any functions f(X) and g(Y) and all values of Z,\nE[J(X)g(Y)Iz ] = E[f(X)Iz ] x E[g(Y)Iz], (1)\nwhere E[f(X) ] indicates [E[f(X) ], E[f(X)]] and x in dicates interval multiplication. It is an open question whether this concept is equivalent to some form of strong independence; even if it were, Expression (1) cannot be easily justified except as a mathematical generalization of standard stochastic independence. 2\n3 Graphical models\nMultivariate statistical models can be elegantly repre sented by graphical models, as demonstrated by the theory of Bayesian networks (19) . A Bayesian net work is composed of a directed acyclic graph and a collection of variables X. Each node in the graph is associated with a variable Xi and with a conditional density p(Xilpa(Xi)). Bayesian networks satisfy the Markov condition: Every variable is independent of its nondescendants non-parents given its parents. The Markov condition implies that every Bayesian network represents a unique joint probability\n(2)\nCredal networks are structures that represent joint credal sets through directed acyclic graphs; for this purpose, define [8]:\nDefinition 4 A locally defined credal network for a collection of variables X is a directed acyclic graph where every node is associated with a variable Xi and with a local credal set K(Xilpa(Xi)).\nA joint credal set K(X) that satisfies all constraints in a credal network is an extension of the network. Note that Definition 4 does not generate a unique ex tension for a given credal network [8]. The flexibility of Definition 4 seems justified as a starting point, as\n2Unfortunately, Kuznetsov died in an accident at the beginning of 1998, and little is known about his concept of independence.\n110 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nit would be unduly restrictive to select an extension, among the many possible extensions, as the \"correct\" one. The next sections look at strategies that automat ically specify the independence judgements in credal networks; the analysis will lead to the strong Markov condition (Section 6) .\n4 Strong extensions\nWhat happens if every variable in a credal network is strongly independent of its nondescendants non parents given its parents? We can ask for the largest possible credal set that satisfies this type of Markov condition; call this credal set the strong extension of the network.\nStrong extensions have a rather simple form. As proved in Appendix A:\nTheorem 1 The strong extension of a locally de fined credal network with convex local credal sets is the convex hull of all probability densities that sat isfy the Markov condition on the network when each conditional probability p(Xilpa(Xi)), for each value of pa(Xi), is selected from K(Xilpa(Xi)):\n(3)\nThis result is apparently assumed, or at least known informally, in most of the literature that associates credal sets with directed acyclic graphs [4, 7, 11, 22]. The convexity of the local credal sets K(Xilpa(Xi)) is quite a reasonable assumption that is usually taken for granted. Most of the literature starts from Expres sion (3) and argues that this expression is the \"correct\" extension of a credal network, rather than obtaining it as a consequence of strong independence. 3\nStrong extensions have an intuitive similarity with standard Bayesian networks, and they satisfy one of the most important properties of Bayesian networks (as proved in Appendix A):\nTheorem 2 Given a credal network where all com binations of variables have positive probability, every d-separation relation in the network corresponds to a strong independence in the strong extension of the net work.\nActually, d-separation relations even imply epistemic independence in strong extensions [8], so the similarity to Bayesian networks is quite remarkable.\n3The author has employed the term \"type-1 extension\" to refer to Expression (3) [7, 8].\nOn the negative side, strong extensions are just as hard to justify as strong independence. They make sense in a \"sensitivity analysis\" interpretation of credal sets, but they can only have as compelling a justification as strong independence.\n5 Extensions based on epistemic independence\nSuppose we adopt epistemic independence as a solid and compelling concept of independence, and we adopt the standard Markov condition: Every variable in a credal network is epistemically independent of its non descendants non-parents given its parents. What sep aration properties are valid in this model? The follow ing example shows that d-separation does not imply epistemic independence.\nExample 1 A group of experts models a domain by the credal network in Figure 1. The experts reach a preliminary model by adopting all extreme points of the strong extension that satisfy either (p(zly) ,p(zlyc)) = (0. 7, 0. 2) or (p(zly) , p(zlyc)) = (0. 8, 0. 1) . Denote the resulting extension by K'(Z, Y, X, W) (this extension has 64 extreme points). Table 1 shows two extreme points of K'(Z, Y, X, W) (denoted by p�, p�). After additional discussion, the experts conclude that an additional probability mea sure p* must be added to the extension (also shown in Table 1) . The experts agree on the extension K\"(Z, Y, X, W) = {p* UK'(Z, Y, X, W)}, because this credal set satisfies (tightly) all probability bounds and also satisfies the Markov condition for epistemic inde pendence: (W EIN Y I X) and ((W, X) EIN Z I Y).\nUnfortunately, d-separation does not imply epistemic independence in the extension K\"(Z, Y, X, W). Note that Z and W are d-separated by X, but it is not true that (W EIN Z I X). For example, p(zlx) 8501/22707 < 19/50 = E(zlx , w) . - o\nOf course, if d-separation were to be maintained at all costs, we could simply impose it. We would say that, in any extension of a credal network, d-separation must imply epistemic independence. If d-separation seems too convoluted to be adopted outright, we can optionally adopt the following condition:\nDefinition 5 (Contraction condition) Suppose W, X, Y, and Z are collections of variables in a credal network and X, Y and Z are non-descendants of W. If Y is epistemically irrelevant to X given Z, and Y is epistemicallly irrelevant to W given (X, Z) , then Y is epistemically irrelevant to (W, X) given Z.\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 111\nThe Markov and the contraction conditions lead to d-separation properties for epistemic independence, when we require equivalence of credal sets for epistemic independence (Definition 1) . In this case, the con ditions allow a duplication of the d-separation proof for Bayesian networks [13]. The contraction condi tion is necessary because epistemic independence does not satisfy precisely the graphoid property of contrac tion employed in Verma, Geiger and Pearl's proof of d-separation [9].\nNote that the contraction condition is not satisfied in Example 1. Note also that if we require equali ty between credal sets for epistemic independence, other graphoid properties may fail and the contraction con dition may not suffice for d-separation.\nThe contraction condition may itself look too con voluted and a bit excessive, for the contraction and Markov conditions guarantee d-separation for al l ex tensions of a network. Instead, we may ask for the sep aration properties of particular extensions. Take the largest credal set that complies with (1) the constraints that define the local credal sets in a credal network; and with (2) the Markov condition for epistemic inde pendence. Call this credal set the independen t natural extension of the credal network. In general, a natural extension is simply the largest credal set satisfying a collection of constraints [23].\nIndependent natural extensions are quite intuitive, but little is known about them - except that the Markov condition generates far too many constraints. The in dependent natural extension of the network in Fig ure 1 has more than 6 million extreme points! Whether or not d-separation implies epistemic independence in independent natural extensions is an important open question with no obvious answer. Finding a counterex ample for d-separation relations, if there is one, is quite an undertaking, as even simple networks produce quite complex independent natural extensions.\nOn top of the complexity issues, independent natural extensions do not satisfy a property that we might call be lief separation, as illustrated in the next example.\nExample 2 Consider two binary variables X and Y, such that (X EIN Y) and p(X) E [2/5, 1/2], p(Y) E [2/5, 1/2] (23, Sect. 9. 3. 4]. This can be represented by a trivial credal network with two unconnected nodes. The strong extension of this network has four extreme points (vectors [p(x,y) ,p(x,yc) ,p(xc,y) ,p(xc,yc)]):\n(1/4, 1/4, 1/4, 1/4], (4/25, 6/25, 6/25, 9/25 ],\n[1/5, 1/5, 3/10, 3/10], (1/5, 3/10, 1/5 , 3/10].\nThe independent natural extension has six extreme points: the four points of the strong extension and [2/9, 2/9, 2/9, 1/3}, [2/11, 3/11, 3/11,3/11}. An expert then communicates that p(y) is exactly 2/5 . To sim plify our calculations, we simply go through the six densities in the independent natural extension, chang ing p(y) to 2/5. As X and Y are independent, we reason that there is no need to change the conditional densities p(XIY) and recompute the independent nat ural extension from scratch. Doing so, we obtain four different extreme points:\n[4/25, 6/25, 6/25, 9/25], [1/5, 3/10, 1/5 , 3/10],\n[1/5, 6/25, 1/5, 9/25 }, [4/25, 3/10, 6/25, 3/10}.\nThe problem is that X and Y are not epistemically independent in this new extension; for example, p(y) = 2/5 but p(yix) E (2/5, 4/7]. o\nA similar situation can be informally described as fol lows. Take three variables, X as {rain, no rain} , Y as {airport on, airport of}, W as {good, bad} for the weather at the remote location. Suppose we have the structure of Figure 1, including the relation (Y EIN W I X). The independent natural extension of this network introduces a form of \"linkage\" between the variables, because conditional densities for X and Y may be tied to particular marginal densities for W. If we find that an extreme point (with linkage) is inad equate, we cannot change p(W) or p(XIW) or p(YIX) for that extreme point in isolation. Independent ma nipulation of credal sets is not possible with epistemic independence and the standard Markov condition.\n1 1 2 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\n6 The strong Markov condition\nThe previous sections made a detailed analysis of independence concepts and methods of extension in credal networks, and the result is far from satisfac tory. Strong extensions are quite manageable com putationally and satisfy d-separation, but there is no justification for strong extension as long as strong inde pendence remains only a mathematical generalization of standard stochastic independence. Epistemic inde pendence has a much better justification, but does not guarantee desirable separation properties.\nThe key idea here is to recognize that the standard Markov condition, and not epistemic independence, is too weak. In essence, the standard Markov condition demands that beliefs about a situation should not be affected by the past once we know everything that causes that situation. But in fact we can say more: our beliefs about a situation should not be affected by the past, and by changes in our beliefs about the past, once we know everything that causes that situation.\nTo obtain an improvement on the standard Markov condition, consider the following interpretation for the expression \"changes in our beliefs.\" Suppose a joint credal set K(X, Y) is given. Every extreme point of K(X, Y) can be written as p(XIY) p(Y). Select an extreme point p(X, Y) , remove it from the collec tion of extreme points of the credal set and modify p(X, Y) : replace p(Y) by an arbitrary p'(Y) . Then in sert p( X I Y) p' (Y) into the collection of extreme points and take the convex hull of all densities there. Call any such modification of K(X, Y) a belief change with re spect to Y.\nThis rationale leads to the following condition:\nDefinition 6 (Strong Markov condition) Every variable xi is epistemically independent of its non descendants non-parents given its parents, regardless of any sequence of belief changes with respect to the nondescendants of xi.\nDenote the nondescendants non-parents of Xi by nD(Xi)· Intuitively, the strong Markov condition requires that (Xi EIR nD(Xi) I pa(Xi)) regardless of K(pa(Xi), nD(Xi)).\nDespite the intuitive character of the strong Markov condition, it is not clear whether this condition is use ful or not - i.e., whether or not it can lead to in teresting separation properties and important compu tational simplifications. The main result of the pa per, expressed by the next theorem, is that the strong Markov condition implies the standard Markov con dition with strong independence (the proof is in Ap-\npendix A). The impact of this result is discussed in Section 7.\nTheorem 3 In a credal network with convex local credal sets, the strong Markov condition holds if and only if every variable Xi is strongly independent of its nondescendants non-parents given its parents.\nTo illustrate the consequences of Theorem 3, consider a quite common model in statistics: an experiment is independently repeated several times. Here we must differentiate between the assumption that the exper iment is independently repeated and the assumption that the repeated experiments are in fact exchangeable - that is, the exact same probabilistic model should represent every repetition of the experiment. Here we assume only independence; Walley discusses at length the concept of exchangeability and its implications in the theory of credal sets [23, Chap. 9].\nExample 3 Take a collection of variables Xj, j E { 1, ... , n} , where every variable is independent of all others and all variables can be modeled by the same equivalent credal sets. Suppose the marginal convex credal set K(Xj) is given for all variables Xj. If we assume the strong Markov condition, then the nat ural extension of this credal network is the strong extension 181 flt=I PJ(Xj), for every combination of PJ(XJ) E extK(Xj). o\n7 Discussion\nThe main contribution of this paper is the strong Markov condition. To understand the place of the strong Markov condition in the theory of credal net works, note that Theorems 1 and 3 demonstrate the equivalence of:\n1. Strong Markov condition (with epistemic inde pendence) plus natural extension.\n2. Standard Markov condition (with strong indepen dence) plus natural extension.\n3. Strong extension.\nSo, epistemic independence does satisfy the \"belief separation\" property through the strong Markov con dition is adopted. Note also that, in strong exten sions, d-separation implies epistemic independence [7], so epistemic independence and natural extension do display the same d-separation relations displayed by strong extensions and standard Bayesian networks. This argument proves the \"soundness\" of d-separation (that every d-separation relation produces an indepen dence relation). The \"completeness\" of d-separation\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 113\n(there is always a joint credal set where variables that are not d-separated are dependent) comes from the fact that every standard Bayesian network is a credal network.\nConsequently, epistemic independence and natural ex tension do display the essential separation properties of standard Bayesian networks, provided we accept the strong Markov condition. Using the strong Markov condition, we obtain the computational simplicity as sociated with the strong extension. As a by-product, we obtain a clear justification for strong independence; namely, that strong independence is a consequence of epistemic independence and the strong Markov condi tion.\nAs discussed in Section 2.2, the definition of strong independence has two drawbacks: it deals with indi vidual probabilities in a credal set, and, rather un fortunately, it requires an understanding of stochastic independence. The strong Markov condition solves the latter, more serious, problem as it does not require any concept of independence for individual probabilities. The strong Markov condition still relies on individual probabilities. But note that individual probabilities appear in a progression that is quite satisfactory in the theory of credal networks described here. The theory first adopts epistemic independence, then deals with multivariate and graphical representations, and only then introduces individual probabilities to define be lief changes. Future work should attempt to improve the results here by defining belief changes in terms of credal sets.\nIn short, if we accept epistemic independence as the definition of independence and natural extension as the method of extension, we have a pleasant relationship:\nStrong Markov condition¢:> Strong extension. (4)\nNote that Expression ( 4) closely resembles the theory of Bayesian networks, where\nstandard Markov condition<=? Expression (2).\nSo we can build a powerful theory of credal networks starting from five elements: directed acyclic graphs and local credal sets, epistemic independence, the strong Markov condition and natural extension. Ul timately, Expression (4) is the reason why the strong Markov condition can be the foundation of a solid and appealing theory of credal networks.\nA Proofs\nTheorem 1: The strong extension must be equal or larger than l8l fLp(X;jpa(X;)), because this credal set satisfies the standard Markov condition for strong independence. But this set is the largest possi-\nble extension, because any larger set will have some p(X;jpa(X;), nD(X;)) as a non-constant function of nD(X;) - violating a strong independence relation required by the standard Markov condition. <>\nTheorem 2: Suppose X and Y are d-separated by Z; then every extreme point of the strong extension satisfies stochastic independence of X and Y given Z and consequently X and Y are strongly independent given z. <>\nTheorem 3: � If for every variable X;, X; and nD(X;) are strongly independent given pa(X;), then the strong Markov condition is satisfied. To see that, note that as any extreme point of an extension must satisfy p(X;Ipa(X;), nD(X;)) = p(X;Ipa(X;)), any ex treme point must satisfy (X; SIN nD(X;) I pa(X;)) re gardless of K(nD(X;)). => Suppose the strong Markov condition holds for a given extension K(X). Denote D; = nD(X;) and A; = pa(X;). Take a particular variable X;, and se lect K'(A;, D;) to be a singleton set containing an ar bitrary everywhere positive density p'(A;, D;). This can be done arbitrarily because the strong Markov condition is valid for any sequence of belief changes, and a sequence can always be built to produce an ar bitrary p'(A;, D;). Note that K'(D;Ia;) is a single ton set for any value of A;, with density p'(D;Ia;). Consider the extreme points of K(X;, A;, D;) (the marginal credal set of extension K(X)) and work by reductio. Suppose that for one of the extreme points of K(X;, A;, D;), denoted by p*(X;, A;, D;), the density p* (X; Ia;, d;) is a non-constant function of D; for some value of A;. If this is true, there must be some value (x;, a;, d;) of (X;, A;, D;) such that p* (x;ja;, d;) < p* (x;la;). Now consider the unique ex tension of p'(A;ID;) and K(X;Ia;, d;) (the conditional credal set of extension K(X)); this unique extension is equal to p'(A;ID;) x K(X;IA;, D;), where x indi cates elementwise scalar multiplication. Note that the density p'(A;, D;)p*(X;IA;, D;) must belong to this unique extension. For some value (x;, a;, d;):\n=> p' (d;la;) p* (x; Ia;, d;)jp* (x;la;) < p' (d;la;),\n=> p* (d;lx;, a;) < p'(d;la;).\nThen p(d;lx;,a;) < p(a;ld;), contradicting the strong Markov conditioll. So every extreme point of K(X;, A;, D;) must have its conditional density p(X;jA;, D;) as a constant function of D;, and con sequently every variable X; is strongly independent of D; given A;. <>\n114 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nAcknowledgements\nI greatly benefited from joint work with Peter Walley on graphoid properties; I learned about Kuznetsov's independence from him. Thanks to David A vis for the Irs package, used in Example 1 (Irs package is currently available at ftp:/ jmutt.cs.mcgill.cajpub/C/lrs.html).\nReferences\n[1] K. A. Andersen and J. N. Hooker. Bayesian logic. Decision Support Systems, 11:191-210, 1994.\n[2] J. Berger and E. Moreno. Bayesian robustness in bidimensional models: Prior independence. Jour nal of Statistical Planning and Inference, 40:161- 176, 1994.\n[3] J. 0. Berger. Robust Bayesian analysis: Sensitiv ity to the prior. Journal of Statistical Planning and Inference, 25:303-328, 1990.\n[4] J. Cano, M. Delgado, and S. Moral. An ax iomatic framework for propagating uncertainty in directed acyclic networks. International Journal of Approximate Reasoning, 8:253-280, 1993.\n[5] L. Chrisman. Independence with lower and upper probabilities. In E. Horvitz and F. Jensen, editors, CUAI, pages 169-177, Morgan Kaufmann, San Francisco, California, 1996.\n[6] I. Couso, S. Moral, and P. Walley. Examples of independence for imprecise probabilities. In G. de Cooman, F. Cozman, S. Moral, and P. Wal ley, editors, Proc. First Int. Symp. on Imprecise Probabilities and Their Applications, pages 121- 130, Ghent, Belgium, 1999.\n[7] F. G. Cozman. Robustness analysis of Bayesian networks with local convex sets of distributions. In D. Geiger and P. Shenoy, editors, CUAI, pages 108-115, Morgan Kaufmann, San Francisco, Cal ifornia, 1997.\n[8] F. G. Cozman. Irrelevance and independence in quasi-Bayesian networks. In G. Cooper and S. Moral, editors, CUAI, pages 89-96, Morgan Kauf mann, San Francisco, California, 1998.\n[9] F. G. Cozman. Irrelevance and independence ax ioms in quasi-Bayesian theory. In A. Hunter and S. Parsons, editors, ECSQARU, pages 128-136. Springer, London, England, 1999.\n[10] L. de Campos and S. Moral. Independence con cepts for convex sets of probabilities. In P. Besnard and S. Hanks, editors, CUAI, pages 108- 115, Morgan Kaufmann, San Francisco, Califor nia, 1995.\n[11] E. Fagiuoli and M. Zaffalon. 2U: An exact interval propagation algorithm for polytrees with binary variables. Artificial Intelligence, 106(1) :77-107, 1998.\n(12] T. L. Fine. Lower probability models for uncer tainty and nondeterministic processes. Journal of Statistical Planning and Inference, 20:389-411, 1988.\n[13] D. Geiger, T. Verma, and J. Pearl. Identifying independence in Bayesian networks. Networks, 20:507-534, 1990.\n[14] F. J. Giron and S. Rios. Quasi-Bayesian be haviour: A more realistic approach to decision making? In J. M. Bernardo, J. H. DeGroot, D. V. Lindley, and A. F. M. Smith, editors, Bayesian Statistics, pages 17-38. University Press, Valen cia, Spain, 1980.\n[15] H. E. Kyburg Jr. Bayesian and non-Bayesian ev idential updating. Artificial Intelligence, 31:271- 293, 1987.\n[16] H. E. Kyburg and M. Pittarelli. Set-based Bayesianism. IEEE Transactions on Systems, Man and Cybernetics A, 26(3):324-339, 1996.\n[17] I. Levi. The Enterprise of Knowledge. MIT Press, Cambridge, Massachusetts, 1980.\n[18] N. J. Nilsson. Probabilistic logic. Artificial Intel ligence, 28:71-87, 1986.\n[19] J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Mor gan Kaufmann, San Mateo, California, 1988.\n[20] T. Seidenfeld, M. J. Schervish, and J. B. Kadane. A representation of partially ordered preferences. Annals of Statistics, 23(6):2168-2217, 1995.\n[21] P. Suppes. The measurement of belief. Journal Royal Statistical Society B, 2:160-191, 1974.\n[22] B. Tessem. Interval probability propagation. In ternational Journal of Approximate Reasoning, 7:95-120, 1992.\n[23] P. Walley. Statistical Reasoning with Imprecise Probabilities. Chapman and Hall, London, 1991.\n[24] M. Zaffalon. A credal approach to naive classi fication. In G. de Cooman, F. G. Cozman, S. Moral, and P. Walley, editors, Proc. First Int. Symp. on Imprecise Probabilities and Their Ap plications, pages 405-414, Ghent, Belgium, 1999."
    } ],
    "references" : [ {
      "title" : "Bayesian robustness in bidimensional models: Prior independence",
      "author" : [ "J. Berger", "E. Moreno" ],
      "venue" : "Jour­ nal of Statistical Planning and Inference,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1994
    }, {
      "title" : "Robust Bayesian analysis: Sensitiv­ ity to the prior",
      "author" : [ "J. 0. Berger" ],
      "venue" : "Journal of Statistical Planning and Inference,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1990
    }, {
      "title" : "An ax­ iomatic framework for propagating uncertainty in directed acyclic networks",
      "author" : [ "J. Cano", "M. Delgado", "S. Moral" ],
      "venue" : "International Journal of Approximate Reasoning,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1993
    }, {
      "title" : "Independence with lower and upper probabilities",
      "author" : [ "L. Chrisman" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1996
    }, {
      "title" : "Examples of independence for imprecise probabilities",
      "author" : [ "I. Couso", "S. Moral", "P. Walley" ],
      "venue" : "Proc. First Int. Symp. on Imprecise Probabilities and Their Applications,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1999
    }, {
      "title" : "Robustness analysis of Bayesian networks with local convex sets of distributions",
      "author" : [ "F.G. Cozman" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1997
    }, {
      "title" : "Irrelevance and independence in quasi-Bayesian networks",
      "author" : [ "F.G. Cozman" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1998
    }, {
      "title" : "Irrelevance and independence ax­ ioms in quasi-Bayesian theory",
      "author" : [ "F.G. Cozman" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1999
    }, {
      "title" : "Independence con­ cepts for convex sets of probabilities",
      "author" : [ "L. de Campos", "S. Moral" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1995
    }, {
      "title" : "2U: An exact interval propagation algorithm for polytrees with binary variables",
      "author" : [ "E. Fagiuoli", "M. Zaffalon" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1998
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Sets of probability measures, called credal sets by Levi [17], are quite flexible representations for uncer­ tainty, employed in several theories [3, 12, 14, 15, 17, 18, 20, 21, 23].",
      "startOffset" : 146,
      "endOffset" : 181
    }, {
      "referenceID" : 2,
      "context" : "Such structures are called credal networks1 and have been investigated by sev­ eral authors [1, 4, 11, 22].",
      "startOffset" : 92,
      "endOffset" : 106
    }, {
      "referenceID" : 9,
      "context" : "Such structures are called credal networks1 and have been investigated by sev­ eral authors [1, 4, 11, 22].",
      "startOffset" : 92,
      "endOffset" : 106
    }, {
      "referenceID" : 5,
      "context" : "networks [7, 8]; the term credal networks, proposed by Zaf­",
      "startOffset" : 9,
      "endOffset" : 15
    }, {
      "referenceID" : 6,
      "context" : "networks [7, 8]; the term credal networks, proposed by Zaf­",
      "startOffset" : 9,
      "endOffset" : 15
    }, {
      "referenceID" : 4,
      "context" : "This concept is called independence in the selection by Couso et al [6]; they employ strong independence when the joint credal set is the largest credal set that satisfies Definition 3.",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 0,
      "context" : "display dependence between the variables [2, 5].",
      "startOffset" : 41,
      "endOffset" : 47
    }, {
      "referenceID" : 3,
      "context" : "display dependence between the variables [2, 5].",
      "startOffset" : 41,
      "endOffset" : 47
    }, {
      "referenceID" : 6,
      "context" : "Credal networks are structures that represent joint credal sets through directed acyclic graphs; for this purpose, define [8]:",
      "startOffset" : 122,
      "endOffset" : 125
    }, {
      "referenceID" : 6,
      "context" : "Note that Definition 4 does not generate a unique ex­ tension for a given credal network [8].",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 2,
      "context" : "This result is apparently assumed, or at least known informally, in most of the literature that associates credal sets with directed acyclic graphs [4, 7, 11, 22].",
      "startOffset" : 148,
      "endOffset" : 162
    }, {
      "referenceID" : 5,
      "context" : "This result is apparently assumed, or at least known informally, in most of the literature that associates credal sets with directed acyclic graphs [4, 7, 11, 22].",
      "startOffset" : 148,
      "endOffset" : 162
    }, {
      "referenceID" : 9,
      "context" : "This result is apparently assumed, or at least known informally, in most of the literature that associates credal sets with directed acyclic graphs [4, 7, 11, 22].",
      "startOffset" : 148,
      "endOffset" : 162
    }, {
      "referenceID" : 6,
      "context" : "Actually, d-separation relations even imply epistemic independence in strong extensions [8], so the similarity to Bayesian networks is quite remarkable.",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 5,
      "context" : "to refer to Expression (3) [7, 8].",
      "startOffset" : 27,
      "endOffset" : 33
    }, {
      "referenceID" : 6,
      "context" : "to refer to Expression (3) [7, 8].",
      "startOffset" : 27,
      "endOffset" : 33
    }, {
      "referenceID" : 7,
      "context" : "The contraction condi­ tion is necessary because epistemic independence does not satisfy precisely the graphoid property of contrac­ tion employed in Verma, Geiger and Pearl's proof of d-separation [9].",
      "startOffset" : 198,
      "endOffset" : 201
    }, {
      "referenceID" : 5,
      "context" : "sions, d-separation implies epistemic independence [7], so epistemic independence and natural extension do display the same d-separation relations displayed by strong extensions and standard Bayesian networks.",
      "startOffset" : 51,
      "endOffset" : 54
    } ],
    "year" : 2011,
    "abstractText" : "This paper analyzes independence concepts for sets of probability measures associated with directed acyclic graphs. The paper shows that epistemic independence and the standard Markov condition violate desirable separation properties. The adoption of a contraction condition leads to d-separation but still fails to guarantee a belief separa­ tion property. To overcome this unsatisfac­ tory situation, a strong Markov condition is proposed, based on epistemic independence. The main result is that the strong Markov condition leads to strong independence and does enforce separation properties; this re­ sult implies that (1) separation properties of Bayesian networks do extend to epistemic in­ dependence and sets of probability measures, and (2) strong independence has a clear justi­ fication based on epistemic independence and the strong Markov condition.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}