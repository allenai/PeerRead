{
  "name" : "1307.3549.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Performance Analysis of Clustering Algorithms for Gene Expression Data",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "It is used to identify the co-expressed genes in specific cells or tissues that are actively used to make proteins, This method is used to analysis the gene expression, an important task in bioinformatics research. Cluster analysis of gene expression data has proved to be a useful tool for identifying co-expressed genes, biologically relevant groupings of genes and samples. In this paper we analysed K-Means with Automatic Generations of Merge Factor for ISODATA- AGMFI, to group the microarray data sets on the basic of ISODATA. AGMFI is to generate initial values for merge and Spilt factor, maximum merge times instead of selecting efficient values as in ISODATA. The initial seeds for each cluster were normally chosen either sequentially or randomly. The quality of the final clusters was found to be influenced by these initial seeds. For the real life problems, the suitable number of clusters cannot be predicted. To overcome the above drawback the current research focused on developing the clustering algorithms without giving the initial number of clusters.\nIndex Terms— Bioinformatics, Clustering, K-Means, Microarray gene expression data.\n——————————  ——————————"
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "LUSTERING has been used in a number of applications such as engineering, biology, medicine and data mining. Cluster analysis of gene expression data has proved to be a useful tool for identifying co-expressed genes. DNA microarrays are emerged as the leading technology to measure gene expression levels primarily, because of their high throughput. Results from these experiments are usually presented in the form of a data matrix in which rows represent genes and columns represent conditions [12]. Each entry in the matrix is a measure of the expression level of a particular gene under a specific condition. Analysis of these data sets reveals genes of unknown functions and the discovery of functional relationships between genes [18]. The most popular clustering algorithms in microarray gene expression analysis are Hierarchical clustering [11], K-Means clustering [3], and SOM [8]. Of these K-Means clustering is very simple and fast efficient. The KMeans clustering algorithm which is developed by Mac Queen [6]. The K-Means algorithm is effective in producing clusters for many practical applications. One drawback in the K-Means algorithm is that of a priori fixation of number of clusters [2, 3, 4, 17].\nIterative Self-Organizing Data Analysis Techniques (ISODATA) tries to find the best cluster centres through iterative approach, until some convergence criteria are met. One significant feature of ISODATA over K-Means is that the initial number of clusters may be merged or split, and so the final number of clusters may be different from the number of clus-\nters specified as part of the input. In [10] Karteeka Pavan et al proposed an algorithm AGMFI to initialize merge factor for ISODATA. This paper studies an initialization of centroids proposed in [17] for microarray data to get the best quality of clusters.\nThis paper is organised as follows. Section 2 presents an overview of Existing works K-Means algorithm, Iterative Self – Organizing Data Analysis Techniques and Automatic Generation of Merge Factor for ISODATA (AGMFI) methods. Section 3 describes the centre initialization algorithm. Section 4 describes performance study of the above methods for UCI data sets. Section 5 describes the conclusion and future work."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : ""
    }, {
      "heading" : "2.1 K- Means Clustering",
      "text" : "The main objective in cluster analysis is to group objects that are similar in one cluster and separate objects that are dissimilar by assigning them to different clusters. One of the most popular clustering methods is K-Means clustering algorithm [3, 9, 12, 17]. It is classifies objects to a pre-defined number of clusters, which is given by the user (assume K clusters). The idea is to choose random cluster centres, one for each cluster. These centres are preferred to be as far as possible from each other. In this algorithm mostly Euclidean distance is used to find distance between data points and centroids [6]. The Euclidean distance between two multi-dimensional data points X = (x1, x2, x3, ..., xm) and Y = (y1, y2, y3, ..., ym) is described as follows:\nD(X, Y) = The K-Means method aims to minimize the sum of squared distances between all points and the cluster centre. This procedure consists of the following steps, as described below\nC\n————————————————\n T.Chandrasekhar is with the Computer Science Department, Periyar University, Salem, Tamilnadu-636 011, India, PH- +91-9942925467. E-mail: ch_ansekh80@rediffmail.com  K.Thangavel is with the Computer Science Department, Periyar University, Salem, Tamilnadu-636 011, India, E-mail: drktvel@yahoo.com  E. Elayaraja is with the Computer Science Department, Periyar University, Salem, Tamilnadu-636 011, India, E-mail: elayarajaphd.e@gmail.com\nAlgorithm 1: K-Means clustering algorithm [13]\nRequire: D = {d1, d2, d3, ..., dn } // Set of n data points.\nK - Number of desired clusters\nEnsure: A set of K clusters. Steps: 1. Arbitrarily choose k data points from D as initial centroids;"
    }, {
      "heading" : "2. Repeat",
      "text" : "Assign each point di to the cluster which has the closest cen-\ntroid;\nCalculate the new mean for each cluster;\nUntil convergence criteria is met.\nThough the K-Means algorithm is simple, it has some drawbacks of quality of the final clustering, since it highly depends on the arbitrary selection of the initial centroids [1]."
    }, {
      "heading" : "2.2 Iterative Self-Organizing Data Analysis Techniques",
      "text" : "ISODATA algorithms variation is to permit splitting and merging of the resulting clusters. Typically, a cluster is split when its variance is above a pre-specified threshold, and two clusters are merged when the distance between their centroids is below another pre-specified threshold [14]. Using this variant, it is possible to obtain the optimal partition starting from any arbitrary initial partition, provided proper threshold values are specified. The well-known ISODATA algorithm uses more clustering technique of merging and splitting clusters. If ISODATA is given the “ellipse” partitioning shown in Fig.1 as an initial partitioning, it will produce the optimal threecluster partitioning ISODATA will first merge the clusters {A} and {B,C} into one cluster because the distance between their centroids is small and then split the cluster {D,E,F,G}, which has a large variance, into two clusters {D,E} and {F,G}.\nAlgorithm 2: ISODATA algorithm [15]\nInput: D = {d1, d2, d3, ..., dn } // Set of n data points.\nK - Number of desired clusters. Θn - a threshold value point for discarding cluster. Θs - a threshold value for spilt operation. Θc - a threshold value for merge operations.\nOutput: A set of K clusters. Steps: 1. Select a K- initial partition of the patterns with a fixed\nnumber of clusters and cluster centers;\n2. Assign each pattern to its closest cluster center and\ncompute the new cluster centers as the centroids of the clusters. Repeat this step until convergence is achieved, i.e., until the cluster membership is stable;\n3. Merge and split clusters based on some heuristic\ninformation, optionally repeating step 2."
    }, {
      "heading" : "2.3 Automatic Generation of Merge Factor for ISODATA (AGMFI) Algorithm",
      "text" : "The clusters produced in the K-Means clustering are further optimized by ISODATA algorithm. Some of the parameters are fixed by user during the merging and partitioning the clusters. In [10], Automatic Generation of Merge Factor is proposed to initialize merge factor for ISODATA. AGMFI uses different heuristics to determine when to split. Decision of merging is done based upon merge factor which is the function of distances between the clusters. The step by step procedure of AGMFI is given here under.\nAlgorithm 3: The AGMFI algorithm [10]\nInput: D = {d1, d2, d3, ..., dn } // Set of n data points.\nK - Number of desired clusters. m- minimum number of samples in a cluster. n – maximum number of iterations. Θs – a threshold value for spilt_size. Θc - a threshold value for merge_size.\nOutput: A set of K clusters. Steps: 1. Identify clusters using K-Means algorithms; 2. Find the inter distance in all other cluster to minimum\naverage inter distances clusters point in C;\n3. Discard the m and merging operations of cluster ≥ 2*K,\nIf n is even go to step 4 or 5;\n4. Distance between two centroids < C, merge the cluster\nAnd update centroid, otherwise repeat up to K/2 times;\n5. K ≤ K/2 or n is odd go to step 6 or 7; 6. Find the standard division of all clusters that has exceeds S * standard division of D; 7. Executed n times or no changes occurred in clusters since the last time then stop, otherwise take the centroids of the clusters as new seed points and find the clusters using K- Means and go to step 3.\nThe main difference between AGMFI and ISODATA is ISODATA uses heuristic values to merge the clusters, AGMFI generates automatically and the choice of c is not fixed but is to be decided to have better performance. The distance measure used here is the Euclidean distance. To assess the quality of the clusters, we used the silhouette measure proposed by Rousseeuw [14]."
    }, {
      "heading" : "3 CLUSTER CENTRE INITIALIZATION ALGORITHM",
      "text" : "(CCIA)\nPerformance of iterative clustering algorithms which converges to numerous local minima depends highly on initial cluster centers. Generally initial cluster centers are selected randomly. In this section, the cluster centre initialization algorithm is studied to improve the performance of the K-Means algorithm.\nAlgorithm 4: Finding the initial centroids [17]\nInput: D = {d1, d2, ..., dn} // set of n data items\nK // Number of desired clusters\nOutput: A set of K initial centroids.\nSteps: 1. Set m = 1; 2. Compute the distance between each data point and all\nother data points in the set D;\n3. Find the closest pair of data points from the set D and\nform a data point set Am (1 ≤ m ≤ K) which contains these two data points, Delete these two data points from the set D;\n4. Find the data point in D that is closest to the data point\nset Am, Add it to Am and delete it from D;\n5. Repeat step 4 until the number of data points in Am\nreaches 0.75 * (n/K);\n6. If m < K, then m = m+1, find another pair of data points\nfrom D between which the distance is the shortest, form\nanother data-point set Am and delete them from D, Go to step 4;\n7. For each data point set Am (1 ≤ m ≤ K) find the\narithmetic mean of the vectors of data points in Am, these means will be the initial centroids."
    }, {
      "heading" : "4 EXPERIMENTAL ANALYSIS AND DISCUSSION",
      "text" : "The following data sets are used to analyse the methods studied in sections 2 and 3."
    }, {
      "heading" : "4.1 Serum Data",
      "text" : "This data set is described and used in [10]. It can be downloaded from: http://www.sciencemag.org/feature/data/ 984559.shl and corresponds to the selection of 517 genes whose expression varies in response to serum concentration inhuman fibroblasts."
    }, {
      "heading" : "4.2 Yeast data",
      "text" : "This data set is downloaded from Gene Expression Omnibusdatabases. The Yeast cell cycle dataset contains 2884 genes and 17 conditions. To avoid distortion or biases arising from the presence of missing values in the data matrix we removal all the genes that had any missing value. This step results in a matrix of size 2882 * 17. The proposed matrix contains integer in the range of 1 to 500."
    }, {
      "heading" : "4.3 Simulated Data",
      "text" : "It is downloaded from http://www.igbmc.ustrasbg.fr/projets/fcm/y3c.txt. The set contains 300 Genes [3]. Above the microarray data set values are all normalized in every gene average values zero and standard deviation equal to 1."
    }, {
      "heading" : "4.4 Comparative Analysis",
      "text" : "The K-Means, CCIA with K-Means and AGMFI are applied on serum data set when numbers of clusters are taken as 10 and 7 times running to EIAGMFI clusters data into 6. K-Means, CCIA with K-Means and AGMFI are applied on Yeast data set when number of clusters initialized to 10 and 7 times running on EIAGMFI clusters data into 6 . Fig. 2. Performance Comparison chart for serum data -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 50 150 250 350 450 Si lh o u e tt e V al u e s Genes Serum Data K CCIAKME AGMFI CCIAGMFI\nFig. 3. Performance Comparison chart for Yeast data\n-0.6\n-0.4\n-0.2\n0\n0.2\n0.4\n0.6\n0.8\n50 150 250 350 450\nSi lh\no u\ne tt\ne V\nal u\ne s\nGenes\nSerum Data\nK\nCCIAKME\nAGMFI\nCCIAGMFI\nThe K-Means, CCIA with K-Means and AGMFI are applied on simulated data set when no of clusters initialized to 10 and 7 times running to EIAGMFI clusters data into 5.\nTABLE 1 COMPARATIVE ANALYSIS OF CLUSTERING QUALITY\nData set\nInitial number of cluster Finalized number of clus-\nter\nCluster Quality by KMeans Cluster Quality by CCIA\nwith KMean\nCluster Quality by AGMFI Cluster Quality by EIAGM\nFI\nSerum 10 6 -0.013 -17.162 18.476 21.101 simulated 10 5 38.407 25.962 54.347 57.552 Yeast 10 6 -6.072 10.425 43.559 50.397"
    }, {
      "heading" : "5 CONCLUSION",
      "text" : "In this paper AGMFI was studied to improve the quality of clusters. The Evaluation of Improved Automatic Generation of Merge Factor for Clustering Microarray Data based on KMeans and AGMFI clustering algorithms were also studied. One of the demerits of AGMFI is random selection of initial seed point of desired clusters. This was overcome with CCIA for finding the initial centroids algorithms to avoidance for initial values at random. Therefore, the EIAGMFI algorithm not depending upon the any choice of the number of cluster and automatic evaluation initial seed of centroids it produces different better results. Both the algorithms were tested with gene expression data."
    }, {
      "heading" : "255, 2008.",
      "text" : "[16] Margaret H. Dunham, “Data Mining- Introductory and Advanced Con-\ncepts”, Pearson education, 2006.\n[17] Madhu Yedla, Srinivasa Rao Pathakota, T M Srinivasa , “Enhancing K-Means\nClustering Algorithm with Improved Initial Center” , Madhu Yedla et al. /\n(IJCSIT) International Journal of Computer Science and Information Technologies, Vol. 1 (2), pp121-125, 2010,\n[18] Sunnyvale, Schena M. “ Microarray biochip technology ”. CA: Eaton Publish-\ning; 2000.\n[19] Wei Zhong, Gulsah Altun, Robert Harrison, Phang C. Tai, and Yi Pan, “Im-\nproved K-Means Clustering Algorithm for Exploring Local Protein Sequence\nMotifs Representing Common Structural Property”, IEEE transactions on nanobioscience, vol. 4, no. 3, september 2005.\n[20] Y. Lu, S. Lu, F. Fotouhi, Y. Deng, and S. Brown, “Incremental Genetic K-\nMeans Algorithm and its Application in Gene Expression Data Analysis”,\nBMC Bioinformatics, 2004.\nFig. 4. Performance Comparison chart for simulated data\n-0.6\n-0.4\n-0.2\n0\n0.2\n0.4\n0.6\n0.8\n50 150 250 350 450\nSi lh\no u\ne tt\ne V\nal u\ne s\nGenes\nSerum Data\nK\nCCIAKME\nAGMFI\nCCIAGMFI"
    } ],
    "references" : [ {
      "title" : "Initializing K-Means using Genetic Algorithms",
      "author" : [ "Bashar Al-Shboul", "Sung-Hyon Myaeng" ],
      "venue" : "World Academy of Science, Engineering and Technology",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2009
    }, {
      "title" : "K-Means Clustering Algorithm with Improved Initial center",
      "author" : [ "Chen Zhang", "Shixiong Xia" ],
      "venue" : "Second International Workshop on Knowledge Discovery and Data Mining (WKDD), pp. 790-792, 2009.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "K - Modes clustering",
      "author" : [ "A Chaturvedi J.C.", "P Green" ],
      "venue" : "Journals of Classification, (18):35–55, 2001.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Fuzzy C means method for clustering microarray data",
      "author" : [ "Doulaye Dembele", "Philippe Kastner" ],
      "venue" : "Bioinformatics, vol.19,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2003
    }, {
      "title" : "An Interactive Approach to mining Gene Expression Data",
      "author" : [ "Daxin Jiang", "Jian Pei", "Aidong Zhang" ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2005
    }, {
      "title" : "Network constrained clustering for gene microarray Data”, doi:10.1093 /bioinformatics",
      "author" : [ "Dongxiao Zhu", "Alfred O Hero", "Hong Cheng", "Ritu Khanna", "Anand Swaroop" ],
      "venue" : "bti 655,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2005
    }, {
      "title" : "An Efficient enhanced K-Means clustering algorithm",
      "author" : [ "A.M Fahim", "M Salem A", "A Torkey", "A Ramadan M" ],
      "venue" : "Journal of Zhejiang University,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2006
    }, {
      "title" : "Automatic Generation of Merge Factor for Clustering Microarray Data",
      "author" : [ "K Karteeka Pavan", "Allam Appa Rao", "A V Dattatreya Rao", "GR Sridhar" ],
      "venue" : "IJCSNS International Journal of Computer Science and Network Security,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2008
    }, {
      "title" : "Divisive Correlation Clustering Algorithm (DCCA) for grouping of genes: detecting varying Patterns in expression profiles",
      "author" : [ "K.R De", "A. Bhattacharya" ],
      "venue" : "bioinformatics, Vol. 24, pp. 1359-1366, 2008.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Improving the accuracy and efficiency of the K-Means clustering algorithm”, in nternational Conference on Data Mining and Knowledge Engineering (ICDMKE)",
      "author" : [ "K.A. Abdul Nazeer", "M.P. Sebastian" ],
      "venue" : "Proceedings of the World Congress on Engineering (WCE-2009),London, UK",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2009
    }, {
      "title" : "Selecting variables for K- Means cluster analysis by using a genetic algorithm that optimises the silhouettes",
      "author" : [ "R. Lletí", "M.C. Ortiz", "L.A. Sarabia", "M.S. Sánchez" ],
      "venue" : "Analytica Chimica Acta,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2004
    }, {
      "title" : "An Efficient Approach for Computing Silhouette Coefficients",
      "author" : [ "Moh'd Belal Al- Zoubi", "Mohammad al Rawi" ],
      "venue" : "Journal of Computer Science",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2008
    }, {
      "title" : "Data Mining- Introductory and Advanced Concepts",
      "author" : [ "Margaret H. Dunham" ],
      "venue" : "Pearson education,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2006
    }, {
      "title" : "Microarray biochip technology ",
      "author" : [ "Sunnyvale", "Schena M" ],
      "venue" : "CA: Eaton Publishing;",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2000
    }, {
      "title" : "Improved K-Means Clustering Algorithm for Exploring Local Protein Sequence Motifs Representing Common Structural Property",
      "author" : [ "Wei Zhong", "Gulsah Altun", "Robert Harrison", "Phang C. Tai", "Yi Pan" ],
      "venue" : "IEEE transactions on nanobioscience,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2005
    }, {
      "title" : "Genetic K- Means Algorithm and its Application in Gene Expression Data Analysis",
      "author" : [ "Y. Lu", "S. Lu", "F. Fotouhi", "Y. Deng", "S. Brown", "“Incremental" ],
      "venue" : "BMC Bioinformatics,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "Results from these experiments are usually presented in the form of a data matrix in which rows represent genes and columns represent conditions [12].",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 13,
      "context" : "Analysis of these data sets reveals genes of unknown functions and the discovery of functional relationships between genes [18].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 1,
      "context" : "The most popular clustering algorithms in microarray gene expression analysis are Hierarchical clustering [11], K-Means clustering [3], and SOM [8].",
      "startOffset" : 131,
      "endOffset" : 134
    }, {
      "referenceID" : 5,
      "context" : "The most popular clustering algorithms in microarray gene expression analysis are Hierarchical clustering [11], K-Means clustering [3], and SOM [8].",
      "startOffset" : 144,
      "endOffset" : 147
    }, {
      "referenceID" : 3,
      "context" : "The KMeans clustering algorithm which is developed by Mac Queen [6].",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 0,
      "context" : "One drawback in the K-Means algorithm is that of a priori fixation of number of clusters [2, 3, 4, 17].",
      "startOffset" : 89,
      "endOffset" : 102
    }, {
      "referenceID" : 1,
      "context" : "One drawback in the K-Means algorithm is that of a priori fixation of number of clusters [2, 3, 4, 17].",
      "startOffset" : 89,
      "endOffset" : 102
    }, {
      "referenceID" : 7,
      "context" : "In [10] Karteeka Pavan et al proposed an algorithm AGMFI to initialize merge factor for ISODATA.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 1,
      "context" : "One of the most popular clustering methods is K-Means clustering algorithm [3, 9, 12, 17].",
      "startOffset" : 75,
      "endOffset" : 89
    }, {
      "referenceID" : 6,
      "context" : "One of the most popular clustering methods is K-Means clustering algorithm [3, 9, 12, 17].",
      "startOffset" : 75,
      "endOffset" : 89
    }, {
      "referenceID" : 8,
      "context" : "One of the most popular clustering methods is K-Means clustering algorithm [3, 9, 12, 17].",
      "startOffset" : 75,
      "endOffset" : 89
    }, {
      "referenceID" : 3,
      "context" : "In this algorithm mostly Euclidean distance is used to find distance between data points and centroids [6].",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 9,
      "context" : "Algorithm 1: K-Means clustering algorithm [13]",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 10,
      "context" : "Typically, a cluster is split when its variance is above a pre-specified threshold, and two clusters are merged when the distance between their centroids is below another pre-specified threshold [14].",
      "startOffset" : 195,
      "endOffset" : 199
    }, {
      "referenceID" : 11,
      "context" : "Algorithm 2: ISODATA algorithm [15]",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 7,
      "context" : "In [10], Automatic Generation of Merge Factor is proposed to initialize merge factor for ISODATA.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 7,
      "context" : "Algorithm 3: The AGMFI algorithm [10]",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 10,
      "context" : "To assess the quality of the clusters, we used the silhouette measure proposed by Rousseeuw [14].",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 7,
      "context" : "1 Serum Data This data set is described and used in [10].",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 1,
      "context" : "The set contains 300 Genes [3].",
      "startOffset" : 27,
      "endOffset" : 30
    } ],
    "year" : 2013,
    "abstractText" : "Microarray technology is a process that allows thousands of genes simultaneously monitor to various experimental conditions. It is used to identify the co-expressed genes in specific cells or tissues that are actively used to make proteins, This method is used to analysis the gene expression, an important task in bioinformatics research. Cluster analysis of gene expression data has proved to be a useful tool for identifying co-expressed genes, biologically relevant groupings of genes and samples. In this paper we analysed K-Means with Automatic Generations of Merge Factor for ISODATAAGMFI, to group the microarray data sets on the basic of ISODATA. AGMFI is to generate initial values for merge and Spilt factor, maximum merge times instead of selecting efficient values as in ISODATA. The initial seeds for each cluster were normally chosen either sequentially or randomly. The quality of the final clusters was found to be influenced by these initial seeds. For the real life problems, the suitable number of clusters cannot be predicted. To overcome the above drawback the current research focused on developing the clustering algorithms without giving the initial number of clusters.",
    "creator" : "Microsoft® Office Word 2007"
  }
}