{
  "name" : "1702.08039.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Criticality and Deep Learning, Part I: Theory vs. Empirics",
    "authors" : [ "Dan Oprisa", "Peter Toth" ],
    "emails" : [ "dan.oprisa@critical.ai", "peter.toth@critical.ai" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Motivated by the idea that criticality and universality of phase transitions might play a crucial role in achieving and sustaining learning and intelligent behaviour in biological and artificial networks, we analyse a theoretical and a pragmatic experimental set up for critical phenomena in deep learning. On the theoretical side, we use results from statistical physics to carry out critical point calculations in feed-forward/fully connected networks, while on the experimental side we set out to find traces of criticality in deep neural networks. This is our first step in a series of upcoming investigations to map out the relationship between criticality and learning in deep networks."
    }, {
      "heading" : "1 Introduction",
      "text" : "Various systems in nature display patterns, forms, attractors and recurrent behavior, which are not caused by a law per se; the ubiquity of such systems and similar statistical properties of their exhibit order has lead to the term ”universality”, since such phenomena show up in cosmology, the fur of animals [1], chemical and physical systems [2], landscapes, biological preypredator systems and endless many others [3]. Furthermore, because of universality, it turns out that the most simplistic mathematical models exhibit the same statistical properties when their parameters are tuned correctly. As such it suffices to study N-particle systems with simple, ”atomistic” components and interactions since they already exhibit many non-trivial emergent properties in the large N limit. Certain ”order” parameters change behavior in a non-classical fashion, for specific noise levels. Using the rich and deep knowledge gained in statistical physics about those systems, we map the mathematical properties and learn about novel behaviors in deep learning set ups. Specifically we look at a collection of N units on a lattice with various pair interactions; when the units are binary spins with values (±1), the model is known\nas a Curie-Weiss model. From a physical point of view, this is one of the basic, analytically solvable models, which still possesses the rich emergent properties of critical phenomena. However, given its general mathematical structure, the model has already been used to explain population dynamics in biology [4], opinion formation in society [5], machine learning [6, 7, 8] and many others [9, 10]. All those systems, with a rich and diverse origination, posses almost identical behavior at criticality. In the latter case of machine learning, the Curie-Weiss model encodes information about fully connected and feed-forward architectures to first order. Similar work was done in [11, 12], where insights from Ising models and fully connected layers are drawn and applied to net architectures; in [13] a natural link between the energy function and an autoencoder is established. We will address the generalisation of fully connected system and understand its properties, before moving to the deep learning network and applying there the same techniques and intuition.\nThe article is organised as follows: section 2 gives a short introduction of critical systems and appropriate examples from physics; in section 3 we map a concrete, non-linear, feed forward net to its physical counterpart and discuss other architectures as well; then we turn to investigating the practical question whether we can spot traces of criticality in current deep learning nets in 4. Finally we summarise our findings in 5 and hint at future directions for the rich map between statistical systems and deep learning."
    }, {
      "heading" : "2 Brief summary of critical phenomena",
      "text" : "Critical phenomena were first thoroughly explained and analysed in the field of statistical mechanics, although they were observed in various other systems, but lacking a theoretical understanding. The study of criticality belongs to statistical physics and is an incredibly rich and wide field, hence we can only briefly summarise some few results of interest for the present article; definitely a much more comprehensive coverage can be found, see e.g.[14, 15, 16, 17]. In a nutshell, the subject is concerned with the behavior of\nar X\niv :1\n70 2.\n08 03\n9v 1\n[ cs\n.A I]\n2 6\nFe b\n20 17\nsystems in the neighbourhood of their critical points, [18]. One thus looks at systems composed of (families of) many, identical particles, trying to derive properties for macroscopic parameters, such as density or polarisation from the microscopic properties and interactions of the particles; statistical mechanics can hence be understood as a bridge between macroscopic phenomenology (e.g. thermodynamics) and microscopic dynamics (e.g. molecular or quantum-mechanical interacting collections of particles). In a nutshell, criticality is achieved when macroscopic parameters show anomalous, divergent behavior at a phase transition. Depending on the system at hand, the parameters might be magnetisation, polarisation, correlation, density, etc. Specifically it is the correlation function of the ”components” which then displays divergent behavior, and signals strong coordinated group behavior over a wide range of magnitudes. Usually it is the noise (temperature) which at certain values will induce the phase transition accompanied by the critical anomalous behavior. Given its relevance in physics and also its mathematical analogy to our deep learning networks, we will briefly review here the CurieWeiss model with non-constant coupling and examine its behavior at criticality."
    }, {
      "heading" : "2.1 Curie-Weiss model",
      "text" : "A simplistic, fully solvable model for a magnet is the Curie-Weiss model (CW), [19]. It possesses many interesting features, exhibits critical behavior and correctly predicts some of the experimental findings. As its mathematics is later on used in our deep learning setup, we will briefly present main properties and solutions for the sake of self-consistency.\nThe Hamiltonian of the CW model is given by\nH = − J 2N N∑ ij sisj − b N∑ i si (1)\nHere the si are a collection of interacting ”particles”, in our physical case, spins, that interact with each other via the coupling J ; they take values (±1) and interact pairwise with each other, at long distances; the inclusion of a factor of 1N multiplying the quadratic spin term makes this long-range interaction tractable in the large N limit. Furthermore, there is a directed external magnetic field which couples to every spin via b. Since the coupling between spins is a constant and since every spin interacts with every other spin (except self-interactions, which is accounted by a factor of 12 ) the Hamiltonian can be rewritten to\nH = − J 2N ( N∑ i si )2 − b N∑ i si (2)\nWith β = 1/kT being the inverse temperature the par-\ntition function can be formulated\nZ = ∑\nsi∈{±1}\ne−βH(s) (3)\n= ∑\nsi∈{±1}\nexpβ\n[ J\n2N\n(∑N i si )2 + b ∑N i si ] (4)\nwhich can be fully solved, [19], summing over each of the 2N states; given an explicit partition Z, the free energy can be computed via\nF = −kT lnZ (5)\nOnce we have F various macroscopic values of interest can be inferred such as the magnetisation of the system, aka first derivative of F wrt. b. This is a so called ”order parameter”, which carries various other denominations, such as polarisation, density, opinion imbalance, etc. depending on the system at hand. It basically measures how arranged or homogeneous the system is under the influence of the outside field which couples to the spins via b. A full treatment and derivation of the model including all its critical behavior can be found in [20], from where we get the equation of state for the magnetisation\nm = b tanh (K b m+ b T ) (6)\nwith K = ( JT ) 1/2. The analysis of this equation for various temperatures T and couplings J , b reveals a phase transition at the critical temperature Tc = J . Introducing the dimensionless parameter t = (T/Tc−1) and expanding (6) in small couplings the famous power law dependence on temperature for the magnetisation emerges:\nm ' √ 3 (K − 1)1/2\nK3/2 ∼ |t|1/2 (7)\nHere we recognise one of the very typical power laws which are ubiquitous to critical systems. The quantity we are most interested in though is the second derivative of the free energy F wrt. b, which is basically the 2-point correlation function of the spins si. Again, expanding the second derivative of the free energy in small couplings and looking in the neighbourhood of the critical temperature Tc yields\n〈si, sj〉 ∼ b2\nTc |t|−1 (8)\nagain displaying power law behavior with a power coefficient γ = 1. The innocent looking equation 8 has actually tremendous consequences, as it implies that correlation does not simply restrict to nearest neighbours but goes over very long distances only slowly decaying; further, because of the power law behavior, there will be self-similar, fractal patterns in the system: islands of equal magnetisation will form within other islands and so on, all the way through all scales. Also, the correlation diverges at the criticality point Tc. We will carry out the explicit calculations for our case of interest - non-constant matrix couplings - later one, in section 3.1."
    }, {
      "heading" : "2.2 Criticality in real-world networks",
      "text" : "Two of the main motivations why we look for criticality and exploit on it in artificial networks, are the universal arising of this phenomenon as well as various hints of its occurrence in biological [4] and neural systems [25, 22]; once systems get ”sizable” enough, gaining complexity, critical behavior emerges, which also applies to man-made nets [23]. Various measures can be formulated to detect criticality, and they all show power law distribution behavior. In the world wide web, e.g. the number of links to a source, and the number of links away from a source, both exhibit power law distribution\nP (k) ∼ k−γ (9)\nfor some power coefficient γ 6= 0. Similar behavior can be uncovered in various other networks, if sizable enough, such as citation behavior of scientific articles, social networks, etc. A simple, generic metric to detect criticality in networks is the degree distribution, defined as the number of (weighted) links connecting to one node.\nFurther, also the correlation between nodes is nontrivial, such that nodes with similar degree have higher probability of being connected than nodes of different degree [23], chapter VII. We will follow a similar path as proposed above and grow an experimental network with new nodes having the simplest preferential and directed attachment towards existing nodes, as a function of their degree:\nΠ(k) ∼ kα (10)\nHere, Π(k) denotes the probability that some node will grow a link to another node of degree k. Hence, every new node, will prefer nodes with higher degrees, leading to the overall power distribution observed in the real world systems. Additional metrics we look at are single neuron activity as well as layer activity and pattern behavior; more details on that in section 4."
    }, {
      "heading" : "3 Criticality in deep learning nets",
      "text" : ""
    }, {
      "heading" : "3.1 From feed-forward to fully connected architecture",
      "text" : "We will focus now on a feed-forward network, with two layers, ai and bj connected via a weight matrix wij ; In order to probe our system for criticality, we write down its Hamiltonian\nH = − 1 2N N∑ ij wijaibj − h N∑ i bi (11)\nwhich has been first formulated in the seminal paper [9]. Here, the values of the a and b are {0, 1}. Further, by absorbing the biases bi in the usual way we can assume our weight matrix has the form:\nW =  2Nh 0 · · · 0 2Nh w11 · · · w1n\n... ...\n. . . ...\n−2Nh wn1 · · · wnn  (12) while the Vi read (1, V1, · · · , VN ). This Hamiltonian describes a two layer net containing rectified linear units (ReLU) in the b-layer with a common bias term h. The weight matrix wij sums the binary inputs coming from the ai and those are fed into bi; depending whether the ReLU threshold has been reached, ai is activated, hence the binary values allowed for both, inputs and b-layer.\nFurther, we show in appendix A, that the partition function is up to a constant the same for the units taking values in {±1} or {0, 1}. By redefining N+1→ N We can then formulate the partition function as\nZ = ∑\na,b∈{±1}\ne− β 2N ∑ ijWijaibj (13)\nwhere β is the inverse temperature 1/T . This is the partition function of a bipartite graph with non-constant connection matrix w.\nHowever, it turns out, that the partition function of the fully connected layer is the highest contribution (1st order) of our feed forward network (see appendix B), hence further simplifying the expression to\nZ = ∑\nsi∈{±1}\ne− β 2N ∑ ijWijsisj (14)\nWe will now proceed and compute the free energy F , defined as F = −T lnZ, using the procedure presented in [10]. From the free energy we then find all quantities of interest, especially the 2-point correlation function of the neurons."
    }, {
      "heading" : "3.2 Fully connected architecture with non-constant weights",
      "text" : "In order to solve the CW model analytically, one has to perform the sum over spins, which is hindered by the quadratic term sisj . The standard way to overcome this problem is the gaussian linearisation trick which replaces quadratic term by its square root - linear in si and one additional continuous variable - the ”mean” field, which is being integrated over entire R:\nea 2 = 1√ 2π ∫ ∞ −∞ dxe−x 2/2+ √ 2ax (15)\nwhich in physics, is known as the HubbardStratonovich transform.\nUnfortunately our coupling is not scalar, and hence we will linearise the sum term by term to keep track of all the weight matrix entries. First we will insert N identities via the Dirac delta function into our Hamiltonian as used in (14):\nH(s) = − 1 2N N∑ ij siWijsj (16)\n= − 1 2N ∏ k ∫ ∞ −∞ dVkδ(sk − Vk) N∑ ij ViWijVj\n= ∏ k ∫ ∞ −∞ dVkδ(sk − Vk)H(V )\nWith the definition of the delta function δ(x) = 1 2πi ∫ i∞ −i∞ dye xy the partition function (14) reads now\nZ(s) = ∏ k ∫ ∞ −∞ dVkδ(sk − Vk) ∑\nsi∈{±1}\ne−βH(V ) (17)\n∼ ∏ k ∫ ∞ −∞ dVk ∫ i∞ −i∞ dUk ∑\nsi∈{±1}\neUk(sk−Vk)e−βH(V )\n= ∏ k ∫ ∞ −∞ dVk ∫ i∞ −i∞ dUke −UkVk+ln(coshUk)e−βH(V )\nAs already stated, we could perform the sum over the binary units si, since they show up linearly in the exponential after the change of variables via delta identity1; we effectively converted the sum over binary values {±1} into integrals over R, leading to\n1In general we’re not interested in numerical multiplicative constants, as later on, when logging the partition and computing the free energy, those terms will be simple additive constants without any contribution after differentiating the expression\nZ = c ∏ i ∫ ∞ −∞ dVi ∫ i∞ −i∞ dUi e −Hg(V,U,T ) (18)\nwith a generalised Hamiltonian\nHg =− β 2N ∑ ij WijViVj + ∑ i [ UiVi − ln (coshUi) ] =− β\n2N\n∑ ij wijViVj − βh ∑ i Vi\n+ ∑ i [ UiVi − ln (coshUi) ] (19)\nUltimately we are interested in the free energy per unit, which contains the partition function, via\nF = lim N→∞\n(−T lnZ)/N (20)\nFrom F we can now obtain all quantities of interest via derivatives, in our case with respect to h. The partition function Z still contains a product of double integrals, which can be solved via the saddle point approximation; we recall here the one-dimensional case∫ ∞\n−∞ dxe−f(x) ≈ ( 2π f ′′(x0) )1/2 e−f(x0) (21)\nwhere x0 is the stationary value of f and f ′′(x0) is in our case the Hessian evaluated at the stationary point:\nHgViVj = − β\nN wij (22)\nHgUiUj = −δij(1− tanh 2 Ui) HgViUj = δij\n(23)\nwhile Hg is given in (19).\nThe expression 18 can now be computed by applying simultaneously the saddle point conditions for both integrals. The stationarity conditions2 for Vi and Ui give\n∂Hg\n∂Vi = − β N ∑ j WijVj + Ui = 0 (24)\n= −β( ∑ j wijVj/N + h) + Ui = 0\n∂Hg ∂Ui = Vi − tanhUi = 0\n2We keep in mind that we enlarged W to contain h as well, hence the explicit equations are h dependent\nwhich combined deliver the self consistency mean field equation of the fully connected layer (29). Further, denoting Hg0 the the Hamiltonian satisfying the stationarity conditions, it reads\nHg0 = β\n2N\n∑ ij wijViVj (25)\n− ∑ i ln coshβ( ∑ j wijVj/N + h)\nEquation (25) already displays manifestly the consistency equation for the mean field, as taking the first derivative wrt. Vi leaves exactly the consistency equation over per its construction;\nNow we can rewrite the free energy (20) as\nF = lim N→∞\nT\nN\n[ Hg0 + ln detH g hh ] ∼ lim N→∞\n(26)[ 1 2N2 ∑ ij wijViVj + 1 N2 ∑ ij ln[wij(1− V 2i )− 1]\n− T N ∑ i ln coshβ( ∑ j wijVj/N + h) ] We need to address now the large N limit; obviously the second term coming from the determinant clearly vanishes in the large-N limit, as the logarithm is slowly increasing, while we divide through N2; the first term - a double sum over Vi is of order N\n2 and hence a well defined average in the limit; the last term - ln cosh, when expanded, is again linear in the sum3, and hence a well defined average after dividing through N , hence we’re left with the free energy\nF = T\nN Hg0 (27)\n= 1\n2N2 ∑ ij wijViVj\n− T N ∑ i ln coshβ( ∑ j wijVj/N + h) ] We’re at the point now, where all quantities of interest can be derived from the free energy F ; the order parameter (aka magnetisation when dealing with spins) per unit is defined as\nm ≡ dF dh = ∂F ∂h ∣∣∣∣ V st + 7 0 ∂F ∂Vi ∂Vi ∂h ∣∣∣∣ V st\n(28)\n3The interior sum over j is an average, hence well defined in the limit; after expansion, we’re left with the outer sum (over i), which is again a well defined average when divided by N\nThe second term on the right vanishes identically, as we recognize it being evaluated at the stationarity condition V st for the Hamiltonian. The contribution of the first term is:\n∑ ik wikVk/N = 1 N ∑ i tanhβ( ∑ k wikVk + h) (29)\nm Vi = tanhβ( ∑ k wikVk/N + h)\nwhich is (the weighted sum version of) the iconic selfconsistency mean field equation of the CW magnet (6).\nThe critical point, Pc is located where the correlation function diverges for h → 0; the 2-point correlation function (aka susceptibility when dealing with spins) is the second derivative of F, i.e. the derivative of (29) wrt. h:\nPc ≡ d2F dh2 = dm dh (30)\nm ∂Vi ∂h = β(1− V 2i )(1 + ∑ k wik ∂Vk ∂h /N)\nwhere we used the original equation (29) for taking the derivatives. It is worth contemplating first equations (29) and (30). They both capture the essence of the criticality of our system, including it’s power law behavior. When the weight matrix reduces to a scalar coupling, both equations reduce to the classical CW system and display the behavior shown in (7) and (8). Furthermore, eq. (30) encodes all the information needed for finding the critical point of matrix system at hand; we recall that all V s (and their derivatives) are already implicitly ”solved” in terms of h and wij via the stationarity equation (29) and hence the Vi are just place holders for functions of w and h; we’re thus left with a non-linear system of first order differential equations in N variables, which will produce poles for specific values of the couplings and temperature at criticality."
    }, {
      "heading" : "4 Experimental results",
      "text" : "After investigating criticality through the partition function in our theoretical setup, now we turn to a practical question: do current deep learning networks exhibit critical behaviour, or put it differently, can we spot traces of critical phenomena in them? Instead of directly attacking the partition function of real world deep neural nets, we start with the practical observation, that systems at around criticality show off power law distributions in certain internal attributes.\nConcretely for networks [23, 24] we look for traces of power laws in weight distributions, layer activation pattern frequencies, single node activation frequencies and average layer activations. In the following we will present experimental results for multilayer feed-\nforward networks, convolutional neural nets and autoencoders.\nFor all networks we ran experiments on the CIFAR10 dataset, training each models for 200 epochs using ReLU activations and Adam Optimizer without gradient clipping and run inferences for 100 epochs. The feed forward network had 3 layers with 500, 400 and 200 nodes, the CNN had 3 convolutional layers followed by 3 fully connected layers and the autoencoder had one layer with 500 nodes.\nFor weight distributions we looked at sums of absolute values of the outgoing weights at each node, as a weighted order of the node. In fig. 1 we have a log-log plot of counts versus the node order as defined above, and detect no linear behavior.\nFor layer activation patterns we counted the frequency of each layer activations through the inference epochs. Figures 2 and 3 are log-log plots of layer activation frequencies versus their respective counts for the feedforward layer the autoencoder. As we see, the hidden layer activation pattern frequencies of the Autoencoder resembles a truncated straight line, indicating that learning hidden features in unsupervised manner can give rise for scale free, power law phenomena in accordance with the findings of [24], but no other architectures show traces of any power law.\nFor single node activation frequencies we counted the frequency of each node activations through the inference epochs.\nFigures 6 and 7 depict the behavior of feed-forward and CN network. The flat, nearly horizontal line in the latter architecture is again a sign of missing exponent whatsoever.\nAs a last measure we employed the sum of activations defined as the average activations on each layer throughout the inference epochs.\nSpontaneous and detectable criticality did not arise in classical architectures so the next step will be to create and experiment with systems that have induced criticality and learning rules that take into account criticality. Our first approach was to grow a fully connected net using the preferential attachment algorithm to induce at least some power law in node weights, and use the fully connected net as a hidden to hidden module. We further experimented with different solutions, regarding input and read out of activations from this hidden to hidden module, without changing the power law distribution. (This would simulate a system located at a critical state, with power law weight distribution). Our findings so far show that learning in these systems is very unstable without any advancement in learning and inference. The fundamental missing part is how to naturally induce a critical state in a network, which is equipped with learning rules that inherently take into account the critical state. For that we need new architectures and new learning rules, derived from the critical point equations (30)."
    }, {
      "heading" : "5 Summary and outlook",
      "text" : "Summary: In this article we make our first steps in investigating the relationship between criticality and deep learning networks. After a short introduction of criticality in statistical physics and real world networks we started with the theoretical setup of a fully connected layer. We used continuous mean field approximation techniques to tackle the partition function of the system ending up with a system of differential equations that determine the critical behaviour of the system. These equations can be the starting point for a possible network architecture with induced criticality and learning rules exploiting criticality. After that we presented results of experiments aiming to find traces of power law distributions in current deep learning networks such as multilayer feed-forward nets, convolutional networks and autoencoders. The results - except for the autoencoder - were affirmative in the negative sense, setting up as next the necessity to create networks with induced criticality and learning rules that exploit the critical state.\nOutlook: Obviously the fully connected layer, which can be solved analytically on the theoretical side is of limited importance, as it translates into a rather simplistic architecture; more realistic, widely used setups, e.g. convolutional or recurrent nets, do very well contain the feed-forward mechanism, but are strongly deviating and hence only partially mapped to our theoretical treatment; it would definitely be essential to address theoretically the convolution mechanism of deep\nnets and establish a link between the theoretical and experimental side; also inducing criticality into the net via eq. (30) could prove beneficial and might very well affect learning behavior and flow on the surface on the loss function.\nAppendix"
    }, {
      "heading" : "A Different unit values",
      "text" : "We here show that the partition function with Hamiltonian H{0,1} = ∑ ij aiwijaj + hiai (31)\nwho’s units are taking values in {0, 1} has the same qualities as encoded in the partition function with Hamiltonian H{±1}, who’s units take values in {±1}.\nWe rewrite the Hamiltonian in (31) with units taking values in {±1} (using Einstein’s summation convention over double indices) :\nH = 1\n4 (1− ui)wij(1− uj) +\n1 2 hi(1− ui) (32)\nwhere the ui and vi take values in {±1}. Carrying now the multiplications in (32) yields\nH = 1\n4 ∑ ij wij + 1 4 uiwijuj\n− 2 4 uiwij1j − 1 2 hiui + 1 2 hi (33) = 1\n2 (c+ uiwijvj + h\n′ iui)\nwith h′i = −wij1j − hi. Hence when computing the partition Z with (32) we obtain\nZ = ∑\nu∈{±1}\neH = ec ∑\na∈{0,1}\neaiwijaj+h ′ iai (34)\nwhere the right hand side is the original Hamiltonian with a shifted coupling h′. The additional constant c factors out completely and hence when taking the logarithm and the second derivative it won’t change the outcome. Also we note that the second derivative wrt. h′ is ∂h′h′ = ∂hh."
    }, {
      "heading" : "B First order contribution",
      "text" : "We consider here the Hamiltonian of the bi-partite graph connected via weight matrix w (with Einstein summation convention):\nHb = uiwijvj (35)\nwith the free energy\nFb = − ln ∑\nu,v∈{±1}\nexp(uiwijvj) (36)\nWithout any loss of generality we set the temperature T = 1, and we won’t keep track of it. Carrying the partial sum over vi yields\nFb = − ln ∑ u ∏ j [ exp(uiwij) + exp(−uiwij) ] (37)\n= − ln ∑ u ∏ j [ 2 cosh(uiwij) ] The sum over the vi is understood as a collection of 2N terms, each corresponding to a unique combination of 0’s and 1’s in the vector of length N representing that specific state of the spins; however, the sum can be conveniently written as a product of N binary summands, where each contains exactly the two possible states of the ith spin - this is where the product over j comes from in upper formula. Expanding now to lowest order in w we obtain\nFb ∼ − ln ∑ u ∏ j (1 + (uiwij) 2 2 ) (38)\n∼ − ln ∑ u exp ∑ j (uiwij) 2 2\n= − ln ∑ u eH(ui)\nwhere H(ui) is the Hamiltonian of the fully connected graph, defined as (Einstein summation convention)\nH(ui) = ∑ j (uiwij) 2 2 (39)\n= 1\n2 ∑ ik ( ∑ j wijwjk)︸ ︷︷ ︸ w′ik uiuk\n= 1\n2 ∑ ik uiw ′ ikuk\nA few notes are in place regarding eq. (39): the matrix w′ik is now symmetric by construction and hence mediates between equally sized (actually identical) layers; further, all higher terms of the cosh function are even, hence all contributions are higher order, symmetric interactions of the layer ui with itself."
    } ],
    "references" : [ {
      "title" : "How Nature Works: the science of selforganized criticality",
      "author" : [ "Per Bak" ],
      "venue" : "Copernicus Springer-Verlag,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1996
    }, {
      "title" : "Scale Invariance, From Phase Transitions to Turbulence",
      "author" : [ "Lesne Annick", "Lagues Michel" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2012
    }, {
      "title" : "Self-organised criticality",
      "author" : [ "Gunnar Pruessner" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2012
    }, {
      "title" : "Neocognitron: A Selforganizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position",
      "author" : [ "Kunihiko Fukushima" ],
      "venue" : "www.cs.princeton.edu/courses/archive/",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1980
    }, {
      "title" : "Neural Networks and physical systems with emergent collective computational abilities,",
      "author" : [ "J.J. Hopfield" ],
      "venue" : "Proceedings of the National Academy of Science, USA,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1982
    }, {
      "title" : "A Mean Field Theory Learning Algorithm for Neural Networks",
      "author" : [ "Carsten Peterson", "James R . Anderson" ],
      "venue" : "Complex Systems",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1987
    }, {
      "title" : "Complexity and Criticality (Advanced Physics Texts)",
      "author" : [ "Kim Christensen", "Nicholas R. Moloney" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2005
    }, {
      "title" : "One rule of life: Are we poised on the border of order?",
      "author" : [ "Philip Ball" ],
      "venue" : "New Scientist,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2014
    }, {
      "title" : "Renormalization Group: Applications in Statistical Physics",
      "author" : [ "Uwe C. Tauber" ],
      "venue" : "Nuclear Physics B,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2011
    }, {
      "title" : "The Origins of Order: Self- Organization and Selection in Evolution",
      "author" : [ "Stuart A. Kauffman" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1993
    }, {
      "title" : "Introduction to statistical physics",
      "author" : [ "Silvio Salinas" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2001
    }, {
      "title" : "Weak pairwise correlations imply strongly correlated network states in a neural population",
      "author" : [ "Elad Schneidman", "Michael J. Berry II", "Ronen Segev", "William Bialek" ],
      "venue" : null,
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Various systems in nature display patterns, forms, attractors and recurrent behavior, which are not caused by a law per se; the ubiquity of such systems and similar statistical properties of their exhibit order has lead to the term ”universality”, since such phenomena show up in cosmology, the fur of animals [1], chemical and physical systems [2], landscapes, biological preypredator systems and endless many others [3].",
      "startOffset" : 310,
      "endOffset" : 313
    }, {
      "referenceID" : 1,
      "context" : "Various systems in nature display patterns, forms, attractors and recurrent behavior, which are not caused by a law per se; the ubiquity of such systems and similar statistical properties of their exhibit order has lead to the term ”universality”, since such phenomena show up in cosmology, the fur of animals [1], chemical and physical systems [2], landscapes, biological preypredator systems and endless many others [3].",
      "startOffset" : 345,
      "endOffset" : 348
    }, {
      "referenceID" : 2,
      "context" : "Various systems in nature display patterns, forms, attractors and recurrent behavior, which are not caused by a law per se; the ubiquity of such systems and similar statistical properties of their exhibit order has lead to the term ”universality”, since such phenomena show up in cosmology, the fur of animals [1], chemical and physical systems [2], landscapes, biological preypredator systems and endless many others [3].",
      "startOffset" : 418,
      "endOffset" : 421
    }, {
      "referenceID" : 3,
      "context" : "However, given its general mathematical structure, the model has already been used to explain population dynamics in biology [4], opinion formation in society [5], machine learning [6, 7, 8] and many others [9, 10].",
      "startOffset" : 181,
      "endOffset" : 190
    }, {
      "referenceID" : 4,
      "context" : "However, given its general mathematical structure, the model has already been used to explain population dynamics in biology [4], opinion formation in society [5], machine learning [6, 7, 8] and many others [9, 10].",
      "startOffset" : 207,
      "endOffset" : 214
    }, {
      "referenceID" : 5,
      "context" : "However, given its general mathematical structure, the model has already been used to explain population dynamics in biology [4], opinion formation in society [5], machine learning [6, 7, 8] and many others [9, 10].",
      "startOffset" : 207,
      "endOffset" : 214
    }, {
      "referenceID" : 6,
      "context" : "[14, 15, 16, 17].",
      "startOffset" : 0,
      "endOffset" : 16
    }, {
      "referenceID" : 7,
      "context" : "[14, 15, 16, 17].",
      "startOffset" : 0,
      "endOffset" : 16
    }, {
      "referenceID" : 8,
      "context" : "[14, 15, 16, 17].",
      "startOffset" : 0,
      "endOffset" : 16
    }, {
      "referenceID" : 9,
      "context" : "[14, 15, 16, 17].",
      "startOffset" : 0,
      "endOffset" : 16
    }, {
      "referenceID" : 10,
      "context" : "1 Curie-Weiss model A simplistic, fully solvable model for a magnet is the Curie-Weiss model (CW), [19].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 10,
      "context" : "which can be fully solved, [19], summing over each of the 2 states; given an explicit partition Z, the free energy can be computed via",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 11,
      "context" : "Two of the main motivations why we look for criticality and exploit on it in artificial networks, are the universal arising of this phenomenon as well as various hints of its occurrence in biological [4] and neural systems [25, 22]; once systems get ”sizable” enough, gaining complexity, critical behavior emerges, which also applies to man-made nets [23].",
      "startOffset" : 223,
      "endOffset" : 231
    }, {
      "referenceID" : 4,
      "context" : "which has been first formulated in the seminal paper [9].",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 5,
      "context" : "We will now proceed and compute the free energy F , defined as F = −T lnZ, using the procedure presented in [10].",
      "startOffset" : 108,
      "endOffset" : 112
    } ],
    "year" : 2017,
    "abstractText" : "Motivated by the idea that criticality and universality of phase transitions might play a crucial role in achieving and sustaining learning and intelligent behaviour in biological and artificial networks, we analyse a theoretical and a pragmatic experimental set up for critical phenomena in deep learning. On the theoretical side, we use results from statistical physics to carry out critical point calculations in feed-forward/fully connected networks, while on the experimental side we set out to find traces of criticality in deep neural networks. This is our first step in a series of upcoming investigations to map out the relationship between criticality and learning in deep networks.",
    "creator" : "LaTeX with hyperref package"
  }
}