{
  "name" : "1702.06762.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "CHESS DIFFERENTLY", "Muthuraman Chidambaram", "Yanjun Qi" ],
    "emails" : [ "mc4xf@virginia.edu", "yanjun@virginia.edu" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Gatys et al. (2015) showed that a convolutional neural network (CNN) model could be trained to transfer the unique styles present in human art onto other images. However, the style transfer loss used in their paper, as well the losses used in the follow-up work of Ulayanov et al. (2016) and Johnson et al. (2016), were specific to image-based tasks. This makes it difficult to extend their work on style transfer to other tasks where unique human styles are present, such as playing games.\nMotivated by this problem, we present a general framework for style transfer, which we term style transfer generative adversarial networks (STGANs) as an extension of the generative adversarial networks (GANs) described by Goodfellow et al. (2014). Our proposed framework consists of a generator G, which learns to perform a given task, and a discriminator D, which learns to predict whether the same task was performed in a specific style. These two models are trained in an adversarial fashion by using the discriminator to regularize the generator, so that the generator learns to perform the given task in a way that is consistent with the style designated by the discriminator.\nIn this paper, we examine an application of STGANs to the task of learning to play chess in the style of a designated player. Essentially, a generator is trained to evaluate chess board positions, and is then combined with a search function to generate moves. A discriminator is trained to distinguish the moves selected using the generator from the moves of a designated player, and is used to bias the generator’s evaluations towards the style of the designated player."
    }, {
      "heading" : "2 STGAN MODEL",
      "text" : "The key difference between our proposed STGAN model and the GAN model is that the generator loss in our model is not structured purely in terms of the discriminator. Instead, the generator loss is defined to be specific to the given task, which we take to be generating optimal chess board evaluations in this paper. We then define a style transfer generator loss by regularizing the original generator loss using the discriminator."
    }, {
      "heading" : "2.1 GENERATOR",
      "text" : "We structured our generator to be similar to the Deep Pink model described by Bernhardsson (2014). The generator G is thus a fully connected feedforward neural network with a 768 unit-wide input layer, two 2048 unit-wide hidden layers with ReLU activations, and a single linear output unit. The generator takes as input a chess board, which is represented as a 768 element vector corresponding to the locations of the 12 different chess pieces, and outputs a real number as an evaluation. Positive\nar X\niv :1\n70 2.\n06 76\n2v 1\n[ cs\n.L G\n] 2\n2 Fe\nb 20\n17\nevaluations signify that the board is in white’s favor, while negative evaluations signify that the board is in black’s favor. We train the generator using triplets of chess boards (xG, yG, rG) taken from games played by top chess players, where xG is an initial board, yG is the board after a player has made a move on xG, and rG is the board after a random move has been made on xG. The function G is learned such that G(xG) = G(yG) and G(yG) > G(rG) if it is white’s turn to move and G(yG) < G(rG) if it is black’s turn to move. The assumption made here is that the generator is being trained on boards taken from masters’ games, so board evaluations should not change much after a move has been played (neither player gives the other a significant advantage). Consequently, a random non-master move is considered to be much worse, so the board evaluation should become more positive if the random move was played by black and more negative if the random move was played by white. We formulate the generator loss J (G)(θG) as:\nJ (G)(θG) = − 1\nm\nm∑\ni=1\n[ log(σ(G(x (i) G )−G(y (i) G ))) + log(σ(G(y (i) G )−G(x (i) G )))\n+ log(σ(pi(G(y (i) G )−G(r (i) G ))))]\n(1)\nWhere m is the batch size, σ is the sigmoid function, and pi is 1 if it is white’s turn to move on the input board, and -1 otherwise. The terms log(σ(G(x(i)G ) − G(y (i) G ))) and log(σ(G(y (i) G ) − G(x (i) G ))) enforce the inequalities G(xG) > G(yG) and G(yG) > G(xG), thereby attempting to learn G(xG) = G(yG). The term log(σ(pi(G(y (i) G ) − G(r (i) G )))) enforces the inequality G(yG) > G(rG) if it is white’s turn to move, and G(yG) < G(rG) if it is black’s turn to move."
    }, {
      "heading" : "2.2 DISCRIMINATOR",
      "text" : "The discriminator, which learns a function D, is set up identically to the generator, save for a 1536 unit-wide input layer and a sigmoid output. The discriminator takes as input a valid chess move, which is represented as the concatenation of the vector representations of a pair of boards, and outputs the probability that the move was played by a designated player. Training is done using pairs of sequential boards (xD, yD) taken from the games of a designated player, as well as fake move pairs (xD,M(xD)) generated by selecting moves using the generator G. The board M(xD) is chosen using the negamax search described by Campbell & Marsland (1983) with a search depth of one and the generator as the board evaluation function. The discriminator is optimized by maximizing D((xD, yD)) and minimizing D((xD,M(xD)), which corresponds to minimizing the following discriminator loss J (D)(θD):\nJ (D)(θD) = − 1\nm\nm∑\ni=1\nD((x (i) D , y (i) D )) +\n1\nm\nm∑\ni=1\nD((x (i) D ,M(x (i) D )) (2)\nHere we have opted to structure the discriminator loss after the loss described by Arjovsky et al. (2017) for training Wasserstein GANs (WGANs)."
    }, {
      "heading" : "2.3 STYLE TRANSFER",
      "text" : "Style transfer is done by using the discriminator to regularize the generator. This is achieved by defining a style transfer generator loss J (G)ST (θG) as:\nJ (G) ST (θG) = J (G)(θG)− 1\nm\nm∑\ni=1\nkD((x (i) D ,M(x (i) D )) (3)\nWhere k is a hyperparameter that controls the level of influence the style designated by the discriminator should have on the generator. Since certain boards x(i)G may not be represented in the discriminator’s training data, we choose to use initial boards x(i)D for the regularization term."
    }, {
      "heading" : "2.4 TRAINING",
      "text" : "The discriminator and the generator are updated simultaneously by gradient descent on J (D) and J (G) (ST ), but the discriminator is updated 5 times for each generator update, as described by Arjovsky et al. (2017) in the WGAN paper. Examples taken from the most recent training batch of the discriminator are used for regularization in each generator update. The discriminator’s weights are also clamped to be in the range [−0.01, 0.01], once again consistent with the WGAN approach."
    }, {
      "heading" : "3 RESULTS",
      "text" : "Training data for the generator was obtained by extracting all standard chess games played in 2016 between players with ratings above 2000 from the FICS games database. For the discriminator, we chose to predict the style of late chess grandmaster Mikhail Tal, and extracted his 2431 available games from PGN Mentor as training data.\nWe trained multiple generator networks with varying values of the regularization parameter k, with k = 0 being treated as the baseline. Due to the cost of having to perform a negamax search for each generated move during training, all networks were trained for 10 epochs with only 100 batches of size 64 sampled from the training data in each epoch. After training, each network was tested by generating moves (once again using a negamax search with depth one) on boards taken from Tal’s games.\nFigure 2 shows the moves selected by the generator networks for a given input board, as well as an actual move commonly played by Tal in the given position. The style transfer networks select the Tal move, whereas the baseline network selects a move that was never played by Tal (at least within the data) in the given position. Table 1 shows the difference in move evaluations between the networks, and it can be seen that the Tal move (d7d5) becomes more favored as k is increased. It should be noted that the style transfer networks still learn that the move f8e7 is a good move (positive negamax evaluation), so they are not simply overfitting to Tal’s moves."
    } ],
    "references" : [ {
      "title" : "Deep learning for.",
      "author" : [ "Erik Bernhardsson" ],
      "venue" : "URL https://erikbern.com/2014/ 11/29/deep-learning-for-chess/",
      "citeRegEx" : "Bernhardsson.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bernhardsson.",
      "year" : 2014
    }, {
      "title" : "A comparison of minimax tree search algorithms",
      "author" : [ "Martin S. Campbell", "T.A. Marsland" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Campbell and Marsland.,? \\Q1983\\E",
      "shortCiteRegEx" : "Campbell and Marsland.",
      "year" : 1983
    }, {
      "title" : "A neural algorithm of artistic style",
      "author" : [ "Leon A. Gatys", "Alexander S. Ecker", "Matthias Bethge" ],
      "venue" : null,
      "citeRegEx" : "Gatys et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gatys et al\\.",
      "year" : 2015
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "Ian J. Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Big Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio" ],
      "venue" : "Advances in Neural Processing Systems,",
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Perceptual losses for real-time style transfer and super-resolution",
      "author" : [ "Justin Johnson", "Alexandre Alahi", "Fei-Fei Li" ],
      "venue" : "European Conference on Computer Vision,",
      "citeRegEx" : "Johnson et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2016
    }, {
      "title" : "Texture networks: Feedforward synthesis of textures and stylized images",
      "author" : [ "Dmitry Ulayanov", "Vadim Lebedev", "Andrea Vedaldi", "Victor Lempitsky" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Ulayanov et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ulayanov et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "1 GENERATOR We structured our generator to be similar to the Deep Pink model described by Bernhardsson (2014). The generator G is thus a fully connected feedforward neural network with a 768 unit-wide input layer, two 2048 unit-wide hidden layers with ReLU activations, and a single linear output unit.",
      "startOffset" : 90,
      "endOffset" : 110
    } ],
    "year" : 2017,
    "abstractText" : "The idea of style transfer has largely only been explored in image-based tasks, which we attribute in part to the specific nature of loss functions used for style transfer. We propose a general formulation of style transfer as an extension of generative adversarial networks, by using a discriminator to regularize a generator with an otherwise separate loss function. We apply our approach to the task of learning to play chess in the style of a specific player, and present empirical evidence for the viability of our approach.",
    "creator" : "LaTeX with hyperref package"
  }
}