{
  "name" : "1706.00439.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Tensor Contraction Layers for Parsimonious Deep Nets",
    "authors" : [ "Jean Kossaifi", "Aran Khanna", "Zachary C. Lipton", "Tommaso Furlanello", "Anima Anandkumar" ],
    "emails" : [ "jean.kossaifi@imperial.ac.uk", "arankhan@amazon.com", "zlipton@cs.ucsd.edu", "furlanel@usc.edu", "anima@amazon.com" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Following their successful application to computer vision, speech recognition, and natural language processing, deep neural networks have become ubiquitous in the machine learning community. And yet many questions remain unanswered: Why do deep neural networks work? How many parameters are really necessary to achieve state of the\nart performance? Recently, tensor methods have been used in attempts to better understand the success of deep neural networks [4, 6]. One class of broadly useful techniques within tensor methods are tensor decompositions. While the properties of tensors have long been studied, in the past decade they have come to prominence in machine learning in such varied applications as learning latent variable models [1], and developing recommender systems [10]. Several recent papers apply tensor learning and tensor decomposition to deep neural networks for the purpose of devising neural network learning algorithms with theoretical guarantees of convergence [17, 9].\nOther lines of research have investigated practical applications of tensor decomposition to deep neural networks with aims including multi-task learning [20], sharing residual units [3], and speeding up convolutional neural networks [15]. Several recent papers apply decompositions for either initialization [20] or post-training [16]. These techniques then often require additional fine-tuning to compensate for the loss of information [11]. However, to our knowledge, no attempt has been made to apply tensor contractions as a generic layer directly on the activations or weights of a deep neural network and to train the resulting network endto-end.\nIn deep convolutional neural networks, the output of each layer is a tensor. We posit that tensor algebraic techniques can exploit multidimensional dependencies in the activation tensors. We propose to leverage that structure by incorporating Tensor Contraction Layers (TCLs) into neural networks. Specifically, in our experiments, we apply TCLs directly to the third-order activation tensors produced by the final convolutional layer of an image recognition network. Traditional networks flatten this activation tensor, passing it to subsequent fully-connected layers. However,\nar X\niv :1\n70 6.\n00 43\n9v 1\n[ cs\n.L G\n] 1\nJ un\nthe flattening process loses information about the multidimensional structure of the tensor. Our experiments show that incorporating TCLs into several popular deep convolutional networks can improve their performance, despite reducing the number of parameters. Moreover, inference on TCL-equipped networks, which contain less parameters, requires considerably fewer floating point operations.\nWe organize the rest of this paper as follows: Section 1.1 introduces prerequisite concepts needed to understand the TCL; Section 2 explains the TCL in detail; Section 3 experimentally evaluates the TCL."
    }, {
      "heading" : "1.1. Tensor Contraction",
      "text" : "Notation: We define tensors as multidimensional arrays, denoting first-order tensors v as vectors, second-order tensors M as matrices and by X̃ , refer to tensors of order 3 or greater. M> denotes the transpose of M.\nTensor unfolding: Given a tensor, X̃ ∈ RD1×D2×···×DN , the mode-n unfolding of X̃ is a matrix X[n] ∈ RDn,D(−n) , with D(−n) = ∏N k=1, k 6=n Dk and is defined by the mapping from element (d1, d2, · · · , dN ) to (dn, e), with e =\n∑N k=1, k 6=n dk × ∏N m=k+1 Dm.\nn-mode product: For a tensor X̃ ∈ RD1×D2×···×DN and a matrix M ∈ RR×Dn , the n-mode product of X̃ by M is a tensor of size (D1 × · · · ×Dn−1 ×R×Dn+1 × · · · ×DN ) and can be expressed using the unfolding of X̃ and the classical matrix multiplication as:\nX̃ ×n M = MX̃[n] ∈ RD1×···×Dn−1×R×Dn+1×···×DN (1)\nTensor contraction: Given a tensor X̃ ∈ RD1×D2×···×DN , we can decompose it into a lowdimensional core tensor G̃ ∈ RR1×R2×···×RN through projection along each of its modes by projection factors( U(1), · · · ,U(N) ) , with U(k) ∈ RRk,Dk , k ∈ (1, · · · , N). In other words, we can write:\nG̃ = X̃ ×1 U(1) ×2 U(2) × · · · ×N U(N) (2)\nor, in short:\nG̃ = JX̃ ; U(1), · · · ,U(N)K (3) In the case of tensor decomposition, the factors of the contraction are obtained by solving a least squares problem. In particular, closed form solutions can be obtained for the factor by considering the n−mode unfolding of X̃ that can be expressed as:\nG[n] = U (n)X[n]\n( U(1) ⊗ · · ·U(n−1) ⊗U(n+1) ⊗ · · · ⊗U(N) )T (4)\nWe refer the interested reader to the seminal work of Kolda and Bader [12]."
    }, {
      "heading" : "1.2. Networks with Large fully connected layers",
      "text" : "Many popular convolutional neural networks for computer vision, e.g. AlexNet, ResNet, and Inception, require hundreds of millions of parameters to achieve the reported results. This can be problematic when running these networks for inference on resource-constrained devices, where it may not be easy to execute hundreds of millions of calculations just to classify a single image.\nWhile these widely used architectures exhibit considerable variety, they also exhibit some commonalities. Often, they consist of blocks containing convolution, activation and pooling layers followed by fully-connected layers before the final classification layer. Both the popular networks AlexNet [14] and VGG [19] follow this meta-architecture, with both containing two fully-connected layers of 4096 hidden units each. In both networks, these fully-connected layers hold over 80 percent of the parameters. In VGG, the hidden units contain 119,545,856 of the 138,357,544 total parameters, and in AlexNet the hidden units contain 54,534,144 out the 62,378,344 total parameters.\nGiven the enormous computational costs for both training and running inference in these networks, we desire techniques that preserve high accuracy while reducing the number of parameters in the network. Notable work in this direction includes approaches to induce and exploit sparsity in the parameters during training [7]."
    }, {
      "heading" : "2. Tensor Contraction Layer",
      "text" : "In this paper, we propose to incorporate the tensor contraction into convolutional neural networks as an end-to-end trainable layer, applying it to the third order activation tensor output by the final convolutional layer.\nIn particular, given an activation tensor X̃ of size (D1, · · · , DN ), we seek a low dimensional core G̃ of smaller size (R1, · · · , RN ) such that:\nG̃ = X̃ ×1 V(1) ×2 V(2) × · · · ×N V(N) (5)\nwith V(k) ∈ RRk,Dk , k ∈ (1, · · · , N). We leverage this formulation and define a new layer that takes the activation tensor X̃ obtained from a previous layer and applies such a projection to it (Figure. 1). We optimize the projection factors ( V(k) ) k∈[1,···N ] to obtain a low dimensional projection of the activation tensor as the output of the layer. We learn the projection factors by backpropagation jointly with the rest of the network’s parameters. We call this new layer the tensor contraction layer and denote by size–(R1, · · · , RN ) TCL, or TCL–(R1, · · · , RN ) a TCL producing a contracted output of size (R1, · · · , RN ).\nThe gradients with respect to each of the factors can be derived easily from 4. Specifically, for each k ∈ 1, · · · , N , we use the following equivalences:\n∂G̃ ∂V(k) = ∂X̃ ×1 V(1) ×2 V(2) × · · · ×N V(N) ∂V(k) = ∂G̃[k] ∂V(k)\n= ∂V(k)X[k]\n( V(1) ⊗ · · ·V(k−1) ⊗V(k+1) ⊗ · · · ⊗V(N) )T ∂V(k)\nIn practice, with minibatch training, we might think of the first mode of an activation tensor as corresponding to the batch-size. Technically, it is possible to applying a transformation along this dimension too, but we leave this consideration for future work. It is trivial to address this case by\neither starting the n−mode products at the second mode or by setting the first factor to be the Identity and not optimize over it. Therefore, in the remainder of the paper, we consider the activation tensor for a single sample for clarity, without loss of generality.\nFigure. 2 presents the symbolic graph of the tensor contraction layer. Note that when taking the n-mode product over different modes, the order in which the n-mode products are computed does not matter."
    }, {
      "heading" : "2.1. Complexity of the TCL",
      "text" : "In this section, we detail the number of parameters and complexity of the tensor contraction layer.\nNumber of parameters Let X̃ be an activation tensor of size (D1, · · · , DN ) which we pass through a size– (R1, · · · , RN ) tensor contraction layer.\nThis TCL has a total of ∑N\nk=1 Dk×Rk parameters (corresponding to the factors of the N n−mode products) and produces as input a tensor of size (R1, · · · , RN ).\nBy comparison, a fully-connected layer producing an output of the same size, i.e. with H = ∏N k=1 Rk hidden units, and taking the same (flattened) tensor as input would have a total of ∏N k=1 Dk × ∏N k=1 Rk parameters.\nComplexity As previously exposed, one way to look at the TCL is as a series of matrix multiplications between the factors of the contraction and the unfolded activation tensor. Let’s place ourselves in the setting previously detailed with an activation tensor X̃ of size (D1, · · · , DN ) and a TCL–(R1, · · · , RN ) of complexity O(CTCL). We can write CTCL = ∑N k=1 Ck where Ck is the complexity of the k th n−mode product. Note that the order in which the products are taken does not matter due to the commutativity of the n−mode product over disjoint modes (e.g. it is commutative for X̃ ×i U(i) ×j U(j) as long as i 6= j). However, for illustrative purposes, we consider them to be done in order, from the first mode to the N th. We then have:\nCk = Rk ×Dk k−1∏ i=1 Ri N∏ j=k+1 Dj (6)\nIt follows that the overall complexity of the TCL is:\nCTCL = N∑ k=1 k∏ i=1 Ri N∏ j=k Dj (7)\nComparison with a fully-connected layer A fullyconnected layer with H hidden units has complexity O(CFC), with:\nCFC = H N∏ i=1 Di (8)\nConsider a TCL that maintains the size of its input, i.e., for any k in [1 . . N ], Rk = Dk. In other words, Ck = Dk ∏N i=1 Di. Therefore,\nCTCL = N∑ k=1 Dk N∏ i=1 Di (9)\nBy comparison, a fully-connected layer that also maintains the size of its input, i.e. H = ∏N k=1 Dk, would have a complexity of:\nCFC = ( N∏ i=1 Di )2 (10)\nNotice the product in the fully-connected case versus a sum for the TCL case."
    }, {
      "heading" : "2.2. Incorporating TCL in a network",
      "text" : "We see several straightforward ways to incorporate the TCL into existing neural network architectures.\nTCL as An Additional Layer First, we can insert a tensor contraction layer following the last pooling layer, reducing the dimensionality of the activation tensor before feeding it to the subsequent two fully-connected layers and softmax output of the network. In general, flattening induces a loss of information. By applying tensor contraction we reduce dimensionality efficiently by leveraging the multidimensional dependencies in the activation tensor.\nTCL as Replacement of a Fully Connected Layer We can also incorporate the TCL into existing architectures by completely replacing fully-connected layers. This has the advantage of significantly reducing the number of parameters in our model. Concretely, consider an activation tensor of size (256, 7, 7) that is fed to either a fully-connected\nlayer (after having been flattened) or to a TCL. A fullyconnected layer with 4096 hidden units has 256 × 7 × 7 × 4096 = 51, 380, 224 parameters. A TCL that preserves the size of its input, on the other hand, only has 2562 + 72 + 72 = 1, 712, 622 parameters. The TCL has 30 times fewer parameters than the fully-connected layer. Similarly, a TCL–(128, 5, 5) (approximately half size) will have only 256×128+7×5+7×5 = 32, 838 parameters, or 1, 500 times fewer parameters than a fully-connected layer."
    }, {
      "heading" : "3. Experiments",
      "text" : "Our experiments investigate the representational power of the TCL, demonstrating results on the CIFAR100 dataset [13]. Subsequently, we offer some preliminary results on the ImageNet 1k dataset [5]. We hypothesize that a TCL can efficiently represent an activation tensor for processing by subsequent layers of the network, allowing for a large reduction in parameters without a reduction in accuracy.\nWe conduct our investigation on CIFAR100 using the AlexNet [14] and VGG [19] architectures, each modified to take 32 × 32 images as inputs. We also present results with a traditional AlexNet on ImageNet. In all cases we report the accuracy (top-1) as well as the space saved, which we quantify as:\nspace savings = 1− nTCL noriginal\nwhere noriginal is the number of parameters in the fullyconnected layers of the standard network and nTCL is the number of parameters in the fully-connected layers of the network modified to include the TCL.\nTo avoid vanishing or exploding gradients, and to make the TCL more robust to changes in the initialization of the factors, we added a batch normalization layer [8] before and after the TCL."
    }, {
      "heading" : "3.1. Results on CIFAR100",
      "text" : "The CIFAR100 dataset is composed of 100 classes containing 600 32× 32 images each, with 500 training images and 100 testing images per class. In all cases, we report performance on the testing set in term of accuracy (Top1). We implemented all models using the MXNet library [2] and ran all experiments training with data parallelism across multiple GPUs on Amazon Web Services, with two NVIDIA k80 GPUs.\nBecause both the original AlexNet and VGG architectures were defined for the ImageNet data set, which has a larger input image size, to adapt them for CIFAR100 by adjusting the stride size on the input convolution layer of both networks so that they would take 32× 32 input images. We investigate two sets of experiments, described below.\nAdded TCL In the first experiments, we added a TCL as additional layer after the last pooling layer and perform the contraction along the two spacial modes of the image, leaving the modes corresponding to the channel and the batch size untouched. We gradually reduced the number of hidden units in these last two layers with and without the TCL included and retrain the nets until convergence to demonstrate how the TCL can learn more compact representations without compromising accuracy.\nTCL substitution In this case, we completely replace one or both of the fully-connected layers by a tensor contraction layer. We reduce the number of hidden units in the subsequent layers proportionally to the reduction in the size of the activation tensor.\nNetwork architectures We experimented with an AlexNet, with an adjusted stride and filter size in the final convolutional layer. From the last convolutional layer, we get an activation tensor of size (batch size, 256, 3, 3). Similarly, in the case of the VGG network, we obtain activation tensors of size (batch size, 512, 3, 3). We experiment with\nseveral variations of the tensor contraction layer. First, we consider the case where we project the activations to a tensor of identical shape. Additionally, we evaluate the effect of reducing the dimensionality of the activation tensor by 25% and by 50%. For AlexNet, because the spatial modes already compact are already, we preserve the spatial dimensions, and reduce dimensionality along the channel."
    }, {
      "heading" : "3.1.1 Results",
      "text" : "Table 1 summarizes our results on CIFAR100 using the AlexNet, while results with VGG are presented in Table 2. The first column presents the method, the second specifies whether a tensor contraction was added and when this is the case, the size of the contracted core. Columns 3 and 4 specify the number of hidden units in the fully connected layers or the size of the TCL used instead when relevant. Column 5 presents the top-1 accuracy on the validation. Finally, the last column presents the reduction factor in the number of parameters in the fully connected layers (which represent, as previously mentioned, more than 80% of the total number of parameters of the networks) where the reference is the original network without any modification (Baseline).\nA first observation is that adding a tensor contraction layer (Added TCL in Tables 1 and 2) consistently increases performance while having minimal impact on the overall number of parameters. Replacing the first fully-connected layer (1 TCL substitution in the Tables) allows us to reduce the number of parameters in the fully connected layers by a factor of more than 3, while observing the same performance as the original network. By replacing both fully connected layers (2 TCL substitutions in the Tables) we can obtain a reduction of more than 92×, with only a 2.5% decrease in performance."
    }, {
      "heading" : "3.2. Results on ImageNet",
      "text" : "In this section, we present preliminary experiments using the larger ILSVRC 2012 (ImageNet) dataset [5], using\nthe AlexNet architecture. ImageNet is composed of 1.2 millions image for testing and 50,000 for validation and comprises 1,000 labeled classes.\nFor these experiments, we trained each network simultaneously on 4 NVIDIA k80 GPUs using data parallelism and report preliminary results. We report Top-1 accuracy on the validation set, across all 1000 classes. All experiments were run using the same setting.\nNetwork architecture We use a standard AlexNet [14]. From the last convolutional layer, we get an activation tensor of size (batch size, 256, 5, 5). As in the CIFAR100 case, we experiment with several variations of the tensor contraction layer. We first insert a TCL before the fullyconnected layers, either a size-preserving TCL (i.e. projecting to a tensor of the same size) or with a smaller size TCL and a proportionally smaller number of hidden units in the subsequent fully-connected layers. We then experiment with replacing completely the first fully-connected layer with a TCL."
    }, {
      "heading" : "3.2.1 Results",
      "text" : "In Table 3 we summarize the results from a standard AlexNet (Baseline, first row), with an added tensor contraction layer (Added TCL) that preserves the dimensionality of its input (row 2) or reduces it (last row). We also report result for substituting the first fully connected layer with a TCL (1 TCL substitution, last row). Simply adding the TCL improves performance while the increase in number of parameters in the fullly connected layers is negligible. We can obtain similar performance by first adding a TCL to reduce the dimensionality of the activation tensor and reducing the number of hidden units in the fully-connected layers, leading to a large space saving with virtually no decrease in performance. Replacing the first fully-connected layer with a size-preserving TCL results in a similar space savings while maintaining the same performance as the standard network."
    }, {
      "heading" : "4. Discussion",
      "text" : "We introduced a new neural network layer that performs a tensor contraction on an activation tensor to yield a low\ndimensional representation of it. By exploiting the natural multi-linear structure of the data in the activation tensor, where each mode corresponds to a distinct modality (i.e. the dimensions of the image and the channels), we are able to decrease the size of the data representation passed to subsequent layers in the network without compromising accuracy on image recognition tasks.\nThe biggest practical contribution of the TCL is the drastic reduction in the number of parameters with little to no performance penalty. This also allows neural networks to perform faster inference with fewer parameters by increasing their representational power. We demonstrated this via the performance of TCLs on the widely used CIFAR100 dataset with two established architectures, namely AlexNet and VGG. We also show results with AlexNet on the ImageNet dataset. Our proposed tensor contraction layer seems to be able to capture the underlying structure in the activation tensor and improve performance when added to an existing network. When we replace fully-connected layers with TCLs, we significantly reduce the number of parameters and nevertheless maintain (or in some cases even improve) performance.\nGoing forward, we plan to extend our work to more network architectures, especially in settings where raw data or learned representations exhibit natural multi-modal structure that we might capture via high-order tensors. We also endeavor to advance our experimental study of TCLS for large-scale, high-resolutions vision datasets. Given the time required to train a large network on such datasets we are investigating ways to reduce the dimension of the tensor contractions of an already trained model and simply fine tune. In addition, recent work [18] has shown that new extended BLAS primitives can avoid transpositions needed to compute the tensor contractions. This will further speed up the computations and we plan to implement it in future. Furthermore, we will look into methods to induce and exploit sparsity in the TCL, to understand the parameter reductions this method can yield over existing state-of-the-art pruning methods. Finally, we are working on an extension to the TCL: a tensor regression layer to replace both the fullyconnected and final output layers, potentially yielding increased accuracy with even greater parameter reductions."
    } ],
    "references" : [ {
      "title" : "Tensor decompositions for learning latent variable models",
      "author" : [ "A. Anandkumar", "R. Ge", "D.J. Hsu", "S.M. Kakade", "M. Telgarsky" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2014
    }, {
      "title" : "Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems",
      "author" : [ "T. Chen", "M. Li", "Y. Li", "M. Lin", "N. Wang", "M. Wang", "T. Xiao", "B. Xu", "C. Zhang", "Z. Zhang" ],
      "venue" : "CoRR, abs/1512.01274,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2015
    }, {
      "title" : "Sharing residual units through collective tensor factorization in deep neural networks. 2017",
      "author" : [ "Y. Chen", "X. Jin", "B. Kang", "J. Feng", "S. Yan" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2017
    }, {
      "title" : "On the expressive power of deep learning: A tensor analysis",
      "author" : [ "N. Cohen", "O. Sharir", "A. Shashua" ],
      "venue" : "CoRR, abs/1509.05009,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2015
    }, {
      "title" : "Imagenet: A large-scale hierarchical image database",
      "author" : [ "J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei- Fei" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2009
    }, {
      "title" : "Global optimality in tensor factorization, deep learning, and beyond",
      "author" : [ "B.D. Haeffele", "R. Vidal" ],
      "venue" : "CoRR, abs/1506.07540,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2015
    }, {
      "title" : "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding",
      "author" : [ "S. Han", "H. Mao", "W.J. Dally" ],
      "venue" : "International Conference on Learning Representations (ICLR),",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2016
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate",
      "author" : [ "S. Ioffe", "C. Szegedy" ],
      "venue" : "shift. CoRR,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2015
    }, {
      "title" : "Generalization bounds for neural networks through tensor factorization",
      "author" : [ "M. Janzamin", "H. Sedghi", "A. Anandkumar" ],
      "venue" : "CoRR, abs/1506.08473,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "Multiverse recommendation: n-dimensional tensor factorization for context-aware collaborative filtering",
      "author" : [ "A. Karatzoglou", "X. Amatriain", "L. Baltrunas", "N. Oliver" ],
      "venue" : "In Proceedings of the fourth ACM conference on Recommender systems,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2010
    }, {
      "title" : "Compression of deep convolutional neural networks for fast and low power mobile applications",
      "author" : [ "Y. Kim", "E. Park", "S. Yoo", "T. Choi", "L. Yang", "D. Shin" ],
      "venue" : "CoRR, abs/1511.06530,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2015
    }, {
      "title" : "Tensor decompositions and applications",
      "author" : [ "T.G. Kolda", "B.W. Bader" ],
      "venue" : "SIAM REVIEW,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2009
    }, {
      "title" : "Learning multiple layers of features from tiny images",
      "author" : [ "A. Krizhevsky", "G. Hinton" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2009
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2012
    }, {
      "title" : "Speeding-up convolutional neural networks using fine-tuned cp-decomposition",
      "author" : [ "V. Lebedev", "Y. Ganin", "M. Rakhuba", "I.V. Oseledets", "V.S. Lempitsky" ],
      "venue" : "CoRR, abs/1412.6553,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2014
    }, {
      "title" : "Tensorizing neural networks",
      "author" : [ "A. Novikov", "D. Podoprikhin", "A. Osokin", "D. Vetrov" ],
      "venue" : "In Proceedings of the 28th International Conference on Neural Information Processing Systems,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2015
    }, {
      "title" : "Training input-output recurrent neural networks through spectral methods",
      "author" : [ "H. Sedghi", "A. Anandkumar" ],
      "venue" : "CoRR, abs/1603.00954,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2016
    }, {
      "title" : "Tensor contractions with extended blas kernels on cpu and gpu",
      "author" : [ "Y. Shi", "U.N. Niranjan", "A. Anandkumar", "C. Cecka" ],
      "venue" : "In 2016 IEEE 23rd International Conference on High Performance Computing (HiPC),",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2016
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "K. Simonyan", "A. Zisserman" ],
      "venue" : "CoRR, abs/1409.1556,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2014
    }, {
      "title" : "Deep multi-task representation learning: A tensor factorisation approach",
      "author" : [ "Y. Yang", "T.M. Hospedales" ],
      "venue" : "CoRR, abs/1605.06391,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Recently, tensor methods have been used in attempts to better understand the success of deep neural networks [4, 6].",
      "startOffset" : 109,
      "endOffset" : 115
    }, {
      "referenceID" : 5,
      "context" : "Recently, tensor methods have been used in attempts to better understand the success of deep neural networks [4, 6].",
      "startOffset" : 109,
      "endOffset" : 115
    }, {
      "referenceID" : 0,
      "context" : "While the properties of tensors have long been studied, in the past decade they have come to prominence in machine learning in such varied applications as learning latent variable models [1], and developing recommender systems [10].",
      "startOffset" : 187,
      "endOffset" : 190
    }, {
      "referenceID" : 9,
      "context" : "While the properties of tensors have long been studied, in the past decade they have come to prominence in machine learning in such varied applications as learning latent variable models [1], and developing recommender systems [10].",
      "startOffset" : 227,
      "endOffset" : 231
    }, {
      "referenceID" : 16,
      "context" : "Several recent papers apply tensor learning and tensor decomposition to deep neural networks for the purpose of devising neural network learning algorithms with theoretical guarantees of convergence [17, 9].",
      "startOffset" : 199,
      "endOffset" : 206
    }, {
      "referenceID" : 8,
      "context" : "Several recent papers apply tensor learning and tensor decomposition to deep neural networks for the purpose of devising neural network learning algorithms with theoretical guarantees of convergence [17, 9].",
      "startOffset" : 199,
      "endOffset" : 206
    }, {
      "referenceID" : 19,
      "context" : "Other lines of research have investigated practical applications of tensor decomposition to deep neural networks with aims including multi-task learning [20], sharing residual units [3], and speeding up convolutional neural networks [15].",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 2,
      "context" : "Other lines of research have investigated practical applications of tensor decomposition to deep neural networks with aims including multi-task learning [20], sharing residual units [3], and speeding up convolutional neural networks [15].",
      "startOffset" : 182,
      "endOffset" : 185
    }, {
      "referenceID" : 14,
      "context" : "Other lines of research have investigated practical applications of tensor decomposition to deep neural networks with aims including multi-task learning [20], sharing residual units [3], and speeding up convolutional neural networks [15].",
      "startOffset" : 233,
      "endOffset" : 237
    }, {
      "referenceID" : 19,
      "context" : "Several recent papers apply decompositions for either initialization [20] or post-training [16].",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 15,
      "context" : "Several recent papers apply decompositions for either initialization [20] or post-training [16].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 10,
      "context" : "These techniques then often require additional fine-tuning to compensate for the loss of information [11].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 11,
      "context" : "We refer the interested reader to the seminal work of Kolda and Bader [12].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 13,
      "context" : "Both the popular networks AlexNet [14] and VGG [19] follow this meta-architecture, with both containing two fully-connected layers of 4096 hidden units each.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 18,
      "context" : "Both the popular networks AlexNet [14] and VGG [19] follow this meta-architecture, with both containing two fully-connected layers of 4096 hidden units each.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 6,
      "context" : "Notable work in this direction includes approaches to induce and exploit sparsity in the parameters during training [7].",
      "startOffset" : 116,
      "endOffset" : 119
    }, {
      "referenceID" : 12,
      "context" : "Our experiments investigate the representational power of the TCL, demonstrating results on the CIFAR100 dataset [13].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 4,
      "context" : "Subsequently, we offer some preliminary results on the ImageNet 1k dataset [5].",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 13,
      "context" : "We conduct our investigation on CIFAR100 using the AlexNet [14] and VGG [19] architectures, each modified to take 32 × 32 images as inputs.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 18,
      "context" : "We conduct our investigation on CIFAR100 using the AlexNet [14] and VGG [19] architectures, each modified to take 32 × 32 images as inputs.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 7,
      "context" : "To avoid vanishing or exploding gradients, and to make the TCL more robust to changes in the initialization of the factors, we added a batch normalization layer [8] before and after the TCL.",
      "startOffset" : 161,
      "endOffset" : 164
    }, {
      "referenceID" : 1,
      "context" : "We implemented all models using the MXNet library [2] and ran all experiments training with data parallelism across multiple GPUs on Amazon Web Services, with two NVIDIA k80 GPUs.",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 4,
      "context" : "In this section, we present preliminary experiments using the larger ILSVRC 2012 (ImageNet) dataset [5], using",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 13,
      "context" : "Network architecture We use a standard AlexNet [14].",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 17,
      "context" : "In addition, recent work [18] has shown that new extended BLAS primitives can avoid transpositions needed to compute the tensor contractions.",
      "startOffset" : 25,
      "endOffset" : 29
    } ],
    "year" : 2017,
    "abstractText" : "Tensors offer a natural representation for many kinds of data frequently encountered in machine learning. Images, for example, are naturally represented as third order tensors, where the modes correspond to height, width, and channels. Tensor methods are noted for their ability to discover multi-dimensional dependencies, and tensor decompositions in particular, have been used to produce compact low-rank approximations of data. In this paper, we explore the use of tensor contractions as neural network layers and investigate several ways to apply them to activation tensors. Specifically, we propose the Tensor Contraction Layer (TCL), the first attempt to incorporate tensor contractions as end-to-end trainable neural network layers. Applied to existing networks, TCLs reduce the dimensionality of the activation tensors and thus the number of model parameters. We evaluate the TCL on the task of image recognition, augmenting two popular networks (AlexNet, VGG). The resulting models are trainable end-to-end. Applying the TCL to the task of image recognition, using the CIFAR100 and ImageNet datasets, we evaluate the effect of parameter reduction via tensor contraction on performance. We demonstrate significant model compression without significant impact on the accuracy and, in some cases, improved performance.",
    "creator" : "LaTeX with hyperref package"
  }
}