{
  "name" : "1411.4114.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Definition of Visual Speech Element and Research on a Method of Extracting Feature Vector for Korean Lip-Reading",
    "authors" : [ "Ha Jong Won", "Li", "Gwang Chol", "Kim", "Hyok Chol", "Kum Song", "Kim Il Sung" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "We defined the 10 visemes based on vowel by analyzing of Korean utterance and proposed the method of extracting the 20-dimensional visual feature vector, combination of static features and dynamic features.\nLastly, we took an experiment in recognizing words based on 3-viseme HMM and evaluated the efficiency.\nKeywords- lip-reading, viseme, feature extraction, visual feature vector, HMM\n1. Introduction Lip-reading is a process of recognizing the speech from the visual information such as\nthe movement of lip, tongue and tooth.\nA lots of researches shows that lip-reading plays an important role in improving the speech recognition of normal people as well as in expressing phonological expression of people with paralyzed auditory organ.\nIn the middle of 1970s, it was known that lip-reading has limited function that assists auditory with its speech recognition in a noisy environment.\nBut later researches based of the analysis of the process of speech discrimination of ordinary people has explored that when we see the speaker’s face, audio-visual information becomes an part of audio-visual complex improving the encoding of speech signal.\nMcGruk effect arises when we sense (or hear) the speech signal.[1] It is McGruk effect that is at the bottom of the attempt to use visual information in speech recognition. McGruk effect means that when a man is stimulated by contrary aural and visual stimulation, man recognize nothing. McGruk effect shows that recognized sound depends on not only speech signal but also visual information such as lip movement.\nResearch on Lip-reading is in progress in 2 parts.\nThe first one is to assist the improving accuracy of speech recognition and the other one is to recognize the speech contents from only visual information, that is, the movement of lip.\nThe latter one is more difficult than traditional speech recognition.\nIn these researches, HMM is used as a recognition model and individual words were set as a basic unit of recognition.\nHowever, Lip-reading systems which don’t use speech signal are proper for current number and individual words but, it is difficult to extend word dictionary on a large scale.\nTherefore, reference [2] made scientific researches on defining the viseme corresponding phoneme of traditional speech recognition not word as a recognition unit and recognizing words as a sequence of visemes.\nA viseme is a virtual speech which has a unique lip shape of mouth so it is similar to phoneme of speech domain. But between phoneme and viseme there exists m to 1 relation, but not the 1 to 1 relation.\nReference [3] described about the definitions of 9 English visemes such as AB,LDF,IDF,LSH,ALF,LLL,RRR,PAL,WWW. LSH Class is classified into small groups: gum-closed nasal speech t, d, n and palate glottis closed - nasal speech g, k, ng.\nWhile reference [4] described about the definitions of 14 English visemes based on the MPEG-4 Multimedia Standard rule, in reference [5] sip and sp, phonemes to represent no sound used in HTK was introduced. And they were classified into 7 consonant visemes, 4 vowel visemes according to the shape of lip formed by vowel and 1 gum sound semivowel viseme to define 13 visemes and to recognize the viseme using HTK.\nViseme can be defined as several ways by using image processing method to get linguistic and visual features. It is “Visemes separated by visual feature” and “linguistic ambiguity of final recognized result” that is important to define viseme.\nLip-reading can convert word sequences into viseme sequences and as ordinary speech recognition based on viseme, i.e., recognition unit, and can get the final sentence using recognition model such as HMM. But when we use lip-reading, it is difficult to get good result as speech recognition because of its own specialty. So reducing the ambiguity came into arise.\nThus, up to now, practical lip-reading system was not developed yet and still have been staying in experiment step was only useful for limited linguistic dictionary\nEspecially Research on Korean lip-reading is no more than initial step and there are many problems such as the definition and feature extraction of Korean Viseme and designing recognition model.\nVisual features for lip-reading can be classified in to two groups, i.e. region feature and contour feature.\nA lot of methods such as PCA based region feature extracting method [5], DCT based region feature extracting method [6], region feature extracting method using 2DPCA called BDPCA (Bidirectional PCA)[7], AAM(Active Appearance Model) based lip region feature extracting method [8] and Hi-LDA based region feature extracting method [9] are\nused to extract region feature.\nAs region features are got by extracting lip region and applying the transformation such as DCT, PCA or LDA, it is more simple than extracting contour feature, but easy to be affected by illumination and feature vectors extracted using affine transformation such as movement and rotation are much different each other.\nThere are some methods of extracting lip contour features such as contour feature extracting method applying ASM to extracted lip region [10], after extracting contour of lip, applying affine robust Fourier transform to get Fourier coefficients and to use as feature vector to represent the type of contour [11], a method that gets extracted 10 geometrical parameters from outer and inner contour of lip and use them as a visual feature vector for lip-reading[4] and so on.\nThe change of lip contour usually reflect the shape features extracted from contour and visual features for lip-reading. So it is more effective than region feature that they represent visual information of speech directly.\nBut contour based shape features have some disadvantages, that is, the features works properly on assumption that extracted lip contour from lip region should be correct and lip contour can’t represent the presence of tooth and the state of tongue itself.\nTo improve the performance of lip-reading system, visual features reflecting many speech information correctly must be extracted as possible and more efficient recognition algorithm must be used.\nIn preceding researches, recognition models used in speech recognition were used as the lip-reading used to conduct lip regions extracted from image frames is similar to the process of speech recognition used to conduct speech signals.\nEspecially, HMM (Hidden Markov Model), the recognition model used in speech recognition whose efficiency was vividly proved are often used.\nWhereas in reference [12], it describes about the implementation of lip – reading system based on the HMMs for each viseme, in reference [13], HMMs for each words were formed to use for lip-reading of word.\nReference [14] proposed a method to apply SVM to lip-reading system. SVM is a strong classifier used in pattern recognition such as face detection and face recognition known that it is highly efficient.\nIn reference [15], it proposed a method of implementing AVSR using TDNN (Time Delayed Neural Network).\nIn this paper, we put emphasis on the definition of the Korean viseme and extracting visual feature vector.\nIn Section 2, we define the viseme for Korean lip-reading based on the analysis of Korean Speech.\nIn Section 3, we describe about the method of extracting static features and dynamic features for Korean lip-reading.\nLastly, Section 4 shows the experimental results for recognizing Korean words using 3 viseme HMM and visual feature vector.\n2. The definition of Korean Viseme It is the sub-word that is often used in speech information based speech recognition as\na recognition unit.\nWe can get sub-word by dividing a word into phone and a sub-word is composed of monophone consisted of one phone, biphone depended on the left or right phone, and triphone with left phone and right pone.\nIn general, sub-word model is are very suitable for composing word model or continuous sentence model, so it is widely used in HMM based speech recognition.\nIn Korean Speech Recognition, phones are usually defined by Korean consonants and vowels and each word is expressed as the sequence of these phones,\nIn lip movement based lip-reading system, viseme is defined as a recognition unit and all words are expressed by the sequence of visemes. And 2viseme or 3viseme HMM based recognition model can be configured.\nIn lip-reading, viseme, a recognition unit, corresponds to a phone in speech recognition and in the case of Korean, it can be defined by Korean consonants and vowels as speech recognition.\nBut in lip movement based lip-reading, there are many consonants and vowels whose lip movements are similar, so that it is impossible to affirm that there exist 1:1 relation between consonants and vowels and visemes.\nOf course, we can define the unique visemes for all Korean consonants and vowels if we consider the states of lip shape, tongue, tooth and vocal cords characterizing utterance of Korean, but in general, it is only lip shape and presence of teeth that can be extracted exactly by image processing.\nAnd it is difficult to extract the others and even though they are extracted, we can’t ensure their correctness.\nThus, when we define viseme by only lip shape and presence of teeth, the m: 1 relation, not 1:1 comes into being between consonants-vowels and visemes.\nViseme should be defined to reduce the ambiguity linguistically and to be clear to distinguish as possible.\nIn general, for the number of visemes, linguistic ambiguity is inversely proportionated to visual division ability. In other words, when we define viseme in detail similarly to consonants and vowels, repetition frequency in viseme representation of words linguistically decreases, but visual recognition accuracy goes down.\nIn lip-reading for English, we defined 14 visemes discriminated visually and clearly, and matched them with English alphabet.\nUnlikely English, each Korean letter is composed of consonant, vowel, and consonant at the end of a syllable. And each consonant, vowel, and consonant at the end of a syllable has certain features visually in lip movement.\nA Viseme should contains enough linguistic meaning and it must be possible to discriminate it from another by the image of lip movement.\nThe utterance of each Korean letter is caused by the activity of different elements composing the human utterance organ (lip, tongue, tooth, vocal cords).\nBut here, the only information observable visually are shape of mouth, tongue, and state of teeth.\nAmong these 3 kinds of information, i.e. shape of mouth, tongue and state of teeth, the color information of tongue and lip are so similar that they are indistinguishable.\nThus we must define viseme by using state of teeth and shape of mouse.\nKorean letter is composed of consonant + vowel + consonant at the end syllable and here as consonant and consonant at the end of syllable are formed by creating an obstacle to outbreath in mouth and throat, they are mainly determined by the state of tongue rather than shape of mouth, so it is hard to observe.\nAmong the consonants, it is only \"m\", \"b\", \"p\" and \"pp\" that are determined clearly by shape of mouth and these consonants have closed shape of mouth.\nFor example, when we pronounce \"ma\", after shape of mouth become closed shape, it will be opened shape to pronounce vowel \"a\".\nThe 4 consonants above, unlikely the other ones, are the most discriminable ones that can be determined by only shape of mouth.\nOn the one hand, vowel is mainly discriminated by the difference of shape of mouth. And it is an information visually appeared on the external.\nLike this, in general, the shape of mouse and the presence of teeth are changed by Korean vowel and the other consonants and consonants at the end of syllable except \"m\", \"b\", \"p\", \"pp\" are not expressed by the shape of mouse and the presence of teeth.\nHence, we defined Korean visemes for Korean lip-reading with Korean vowels as its axis as follows. (Table–1)\nTable–1. Definition of Korean visemes\nKorean visemes Denotation\na, ya a\no, yo o\nu, yu u\ni i\ne e\nwe we\nwi, ui wi\nwa wa\nwo wo\nm, b, p m\nAmong the visemes above, a, o, i, u, e, m are called “single viseme” and they are determined by one of the basic shapes of mouths as Figure–1.\nFigure-1. Basic shapes of mouths determining single viseme.\nInformation about single viseme is mainly reflected to the static feature among static and dynamic features for lip reading, so as a result, static feature about some lip image frame shows which simple viseme that frame represents.\nOther hand, the rest 4 visemes we, wi, wa, wo except 6 simple visemes among Korean visemes are called double visemes and they are represented by the combination of 2 simple visemes.\nDifferently from simple visemes, double visemes are not defined by one main lip shape and are ruled by the dynamic process of 2 main lip shapes.\nFor example, double viseme ‘we’ is represented by the combination of simple visemes ‘u’ and ‘e’ and is defined by the main lip shape that reflects ‘u’ and ‘e’.\nThe main lip shapes and moving process which represents double viseme is like as the following Figure-2.\nFigure-2. Basic lip shapes that represents double viseme.\nThe information about double viseme is reflected to the static feature which represents the main lip shape and the dynamic feature which represents the moving processes of the lip shapes.\nAs above, based on defined Korean viseme, we can express all Korean words as a sequence of visemes.\n3. Visual feature vector for Korean lip reading Lip reading, as a process to get the string spoken from the lip moving change, the utterance of each letter is characterized by a certain time delay and moving process at the lip shape.\nThe moving process of a lip shape when a letter is spoken has 2 characters, that is, one is that main lip shape which specifies the letter exists in the moving process and the other is that this moving process varies from what the four-letter (the letter before) of the letter is.\nFor example, when a Korean word “Ah” is spoken, we can say that the main lip shape that defines “Ah” is the shape of opened state.\nAnd the main lip shape about “Ah” utters any sentence or doesn’t differ according to the speaker but the moving process of lip shape about “Ah” changes according to the front letter of “Ah” in the sentence.\nSo although the main lip shape of the last state for all Korean letter has almost no change, the moving process of lip shape, dynamic process that characterizes the letter differs from the front letter.\nFrom this, we used the static feature that reflects the main lip shape according to the each letter and dynamic feature that reflects the moving process of lip shape as a visual feature for Korean lip reading.\n3.1 Static feature Static feature, a feature related to the lip image frame itself, is a feature that reflect the\ninformation that frame got.\nWe can say that such a information for lip reading is included in lip image region and lip outlet, and is expressed as a distribution of shade values of lip pixels and lip surrounding pixels in lip image region and geometric feature of pixel points that consist the outlet of lip contour.\nTherefore, the static feature about all lip image frame can be extracted from the lip image region and the lip outlet."
    }, {
      "heading" : "3.1.1 Region feature",
      "text" : "When man is speaking, the lip shape is shown as various shapes according to the\nspeaking letter and the speaker but exists the main lip shape which defines each letter.\nIn this paper, we defined the 8 main lip shapes (shown in Figure-3) and defined the other lip shapes that is different from the main shapes, as the middle lip-shape that appears during the progress between the main lip shapes and regarded them as those in which the 8 main lip shapes are included in different degrees.\nTherefore, every lip shape contains information concerned with all 8 main lip shapes in it and is determined differently according to the degrees of the main lip shapes.\nFigure–3. Basic shapes of lip\nThus, regional specification of a lip image is specified by the inclusion degrees how much the 8 main lip shapes are included in it and each degree is defined as the probabilitythe similarity between the selected lip shape and the main lip shape.\nTo do this, at first, we have to determine the main probability model pi )8,1( i about 8 lip shapes.\nThe main probability mode pi is determined using the Gaussian Mixing Model (GMM) and is expressed as the followings.\n8,1,)()( 1   \nig iK\nk ikikip xx \n(1)\nIn previous expression, Ki represents the count of Gaussian component model and πik\nis the mixing coefficient and is defined as  \n iK k ik 1 1 . And the Gaussian component gik is\nexpressed as the followings.\n        \n  \nD\nd ikd\nikdd D\nd ikd\nD ik xg 1\n2\n2\n1\n)( 2 1exp\n)2(\n1)(  \n x\n(2)\nD is the dimension of feature vector x and 2, ikdikd  are the average and square offset of the Gaussian component gik.\nWe can get the feature vector for the probability modeling from the lip region image by the following method.\nGet W×H DCT coefficients applying 2D-DCT to the lip image in size W×H. In Data compression, low-frequency components, to which the energy is concentrated, is important among the DCT coefficients, but it doesn’t mean that those components are also important in Pattern Recognition.\nTherefore, in this paper, we extract the coefficients, that contains important information for Pattern Recognition, among all DCT coefficients and use it in probability modeling and select those coefficients in principle of Entropy minimization.\nLet’s be the set and the size of whole lip images as S and N, and the set and size of study lip images concerned to 8 main lip shapes as Si and Ni.\nWe can get the division entropy H(S) of when other division is done on S by the following expression\n \n 8\n1 )ln()( i\nii N N N NSH\n(3)\nWhen considering the k-th DCT coefficient, we can divide the train data of S into 8 classes according to the distance minimization principle using this coefficient as a feature valve.\nLet )8,1(, ins ii be 8 subsets and size got by this division, then for each Si, we can think entropy samely in S.\n8,1),ln()( 8\n1    i n n n n sH i ij j i ij i\n(4) At the expression above, nij is the number of study data of j-th main lip class in Si. Then the division entropy HD(S, k) is like this\n \n 8\n1 )(),( i i i sH N nkSHD\n(5)\nWhen we divide S into 8 Si using kth DCT coefficient. The information gain IG(k) that we can get when we use the kth DCT coefficient can be defined as the difference between the entropy H(S) that we calculated before the segmentation of S and the segmentation entropy HD(S,k) that we get after we segmented S into 8 Si by kth DCT coefficient.\n),()()( kSHDSHkIG  (6)\nIG(k) is the scale ,that expresses how well we have segmented the train data by kth DCT coefficient, and the bigger that value is the more important related DCT coefficient plays the role in Pattern Recognition.\n10\nThen, after determine the information gain IG(k) about HW  DCT coefficients and rearrange DCT coefficients according to the size of IG(k) and select the biggest d values.\nThen, determine the mixing coefficients, the average and the square offset of the main probability model pi about 8 lip shapes after applying EM algorithm that use Si as train da ta set.\nThe main probability model pi is the probability model that we dependently found after using Si as the study set about every type, so it hasn’t considered its relativity between the lip shapes.\nTherefore, we can use the main probability model p i and can estimate the final probability model pri, that considered the relativity between the lip shapes.\n \n 8\n1 8,1),()( k kiki ipPr xx (7)\n   \n    \n\n    \n\n\n    \n \niS\nj j\nk\ni ik\np p N x x x 8\n1 )(\n)(1 (8)\nThe Region feature vector )(' nrgnf about the lip image of the nth frame can be configured applying d-dimensional vector x, that we can get after applying 2D-DCT to the lip image and multiplying IG value, to expression (5).\n         8 8 7 7 6 6 5 5 4 4 3 3 2 2 1 1' )(, )( , )( , )( , )( , )( , )( , )( )( Pr Pr Pr Pr Pr Pr Pr Pr Pr Pr Pr Pr Pr Pr Pr Prnrgn xxxxxxxxf (9)\nIn the above expression, iPr is the average probability about the i-th lip shape and is\ndefined by    iS i i i PrN Pr x x)(1 ."
    }, {
      "heading" : "3.1.2 Contour feature",
      "text" : "We can get contour feature from the lip contour extracted in every lip image frame.\nd5\nd2\nd1\nd3\nd4 d6\nFigure–4. The landmark points used in extraction of the contour feature.\nLet landmark points of lip contour taken by n-th lip image frame 20,1),,(  iyxpt iii ,\nIn contour feature extraction, only 8 landmark points 191613119631 ,,,,,,, ptptptptptptptpt is used and we can get following parameters from x,y positions of this landmark points as shown by Figure-4.\n139619341665\n191339311111\n,,\n,,\nyydyydyyd\nxxdxxdxxd\n\n\n(10)\nContour feature from this parameters are defined by rates of corresponding amount in closed lip.\nThat is, in closed lip let the values corresponding above 654321 ,,,,, dddddd\nc6c5c4c3c2c1 ,,,,, dddddd then contour feature for n-th lip image frame )(n'contf is\n       c6 6 c5 5 c4 4 c3 3 c2 2 c1 1 ,,,,,)( d d d d d d d d d d d dn'contf (11)\nContour feature )(n'contf takes the relative values to closed lip and this has nothing to do with frame size and has features that reflect the variation of lip contour.\nHere, setting the closed lip to the basis is because lip keeps closed usually when the human doesn’t speak or when there is no-sound.\nTo calculate contour feature )(n'contf , we must get 6 parameters )6,1(c kd k for closed lip first.\nTo do this, we must get closed lip from any video but maybe the frame that has closed lip shape appear in the video or not. So consider 2 cases to get parameters for closed lip.\nFirst, in case that closed lip appear in the video, we can extract the closed lip from the video and estimate the parameters directly.\nWe can determine from the values of region feature whether the lip in every frame of the video is closed or not.\nThat is, let the corresponding component to closed lip shape among 8-d region feature vector i-th component. Then if it is greater than i-th component fi extracted from lip region of a frame then this lip of the frame is decided by closed lip.\nThen the parameter for the closed lip 6,1,c kkd is setted by average of tkd which is getted from the frames of which fi is greater than 1 respectively.\nThat is\n  6,1,)1( )1( 1 1\n1\nc \n  \n\n  \n       kdfsgn fsgn\nd T\nt\nt k t iT\nt\nt i\nk (12)\nIn the above expression T is frame count in the video, tif is i-th component of region\nfeature )(t'rgnf extracted from t-th frame.\nAnd sgn(x) is sign function and   \n   0,0 0,1 )( x x xsgn\nIn case that there is no closed lip in the video, i-th component tif of region feature is smaller than 1.\nIn this case, the parameters for the closed lip is estimated from other not closed lips indirectly.\nThen the parameters for closed lip ckd is determined by the following expression (13).\n6,1,c  kk j kk dd  (13)\nIn the above expression jk shows dimension rate between k-th parameter of j-th lipshape which is not closed lip among 8 lip-shapes and k-th parameter of closed lip and comes from statistical analyisis of training data of lip-shapes.\nAnd jk is unique parameter for a talker that is set the other value as the lip shape by one fixed talker and that is different according to a talker.\nStatic feature vector )(' nstcf for n-th frame is 14-d vector combined with 8-d region feature vector and 6-d contour feature vector\n))(),(()( '' nsnsn contcontrgnrgnstc 'fff  (14)\nIn the above expression srgn, scont is respectively scale transform factor for the region feature and the contour feature.\nRegion feature in static feature reflects the distribute state for the feature pixels such as the teeth but contour feature reflects the geometrical feature of the landmark points of which lip contour consists such as lip-open extent.\nAnd To reduce the noise effect, the final static feature vector )(nstcf of n-th lip image frame is averaged static feature vector of all frames in the window region which is W+1 wide and centered by n-th frame.\n   \n 2/\n2/\n' )( )1(\n1)( W\nWk stcstc knW n ff (15)\n3.2 Dynamic feature Static feature reflects lip shape of every frame but dynamic feature reflects variation\nprogress of lip shape between from one frame to another frame.\nSpecially, Dynamic feature is variation of lip shape between neighbor frames centered by current frame, that is, it reflects front-back context so it is very important on reading sequence using triviseme HMM.\nFigure-5 shows how lip shape change by left-right context when the human speaks.\nFigure–5. Variation Example of lip shape by left-right context\nThis shows that the basic lip shape which defines a character is different according to a speaker and a left-right context of the character.\nThe basic lip shape which defines a character is different according to a speaker and a left-right context but in the array of lip images lip shape the variation feature is not different a lot when the different speakers speak the same sentence (word) but it is different according to which sentence is spoken.\nAs you can see the above example, as you see the array of lip images for the spoken sentence “Boado”, the lip shape, the change progress of lip shape, is “Round lip shape”>”opened lip shape->”Round lip shape” and for “Chazaga”, the change progress of lip shape is “bit opened lip shaped”->”large opened lip shape”. As a result it is different according to the spoken sentence and it is independent on a speaker.\nThis shows that the feature reflected lip shape change progress plays very important role to construct the reading sequence system which is independent on a speaker and to perform lip reading using triviseme HMM considering the left-right context.\nDynamic feature reflecting the change progress of lip shape is extracted from transform information of lip contour according to time.\nDynamic feature is defined by position variation according to time of 8 feature points among the 20 landmark points of which lip contour is consisted as the figure-4.\nAmong the 8 landmark points, for pt1and pt11 corresponding to the corner point of the\nlip contour the variation of only x-direction is considered, but for pt6 and pt16 the variation of only y-direction is considered.\nAnd for the rest 4 landmark points pt3, pt9, pt13, pt19 the variation of both of x, y direction is considered and totally they consist 12-d dynamic feature.\nNow let the x, y coordinate of landmark point ptk according to t express Ck(x, t), Ck(y, t) then dynamic feature )(t'dynf at the time t is expressed by derivation value of correspond x, y coordinate of time t.\n \n\n \n \ndt tydC dt txdC dt tydC dt tydC dt txdC dt txdC\ndt tydC dt txdC dt tydC dt tydC dt txdC dt txdC tdyn\n),( , ),( , ),( , ),( , ),( , ),(\n),( , ),( , ),( , ),( , ),( , ),( )(\n191916131311\n996331`f\n(16)\nInput object is the array of lip image so the derivation of x, y coordinate at time t is simply the variation of x, y coordinate between n th frame and n-1 th frame. That is\n)1,(),(),( ),(  nxCnxCnxC dt txdC kkk k\n)1,(),(),( ),(\n nyCnyCnyC dt\ntydC kkk k\n(17)\nAnd dynamic feature )(n'dynf on the n-th frame is\n ),(),,(),,(),,(),,(),,( ),(),,(),,(),,(),,(),,()(\n191916131311\n996331 ` nyCnxCnyCnyCnxCnxC nyCnxCnyCnyCnxCnxCndyn  f\n(18)\nBut actually the variation of lip contour between the neighbor frames in the lip video is very small and the higher the fps of the camera is the smaller this variation gets.\nAnd it is affected by the noisy a little detecting the lip contour so we don’t extract the dynamic feature by every frame as a unit but extract the dynamic feature by the frame win dow as a unit, the window is the pack of the frames which is W+1 wide.\nThen dynamic feature )(ndynf is defined as the difference between the averaged x, y coordinate in nth window and the averaged x, y coordinate in n-1th frame pack.\n ),(),,(),,(),,(),,(),,( ),(),,(),,(),,(),,(),,()(\n191916311311\n996331\nnyCnxCnyCnyCnxCnxC nyCnxCnyCnyCnxCnxCndyn  f\n(19)\nHere,\n)1,(),(),(  nxCnxCnxC kkk . And ),( nxCk , ),( nyCk is the average of x, y of k th landmark point of every frame in the nth window. That is\n     \n 2/\n2/\n2/\n2/\n),( )1( 1),(,),( )1( 1),( W Wk kk W Wk kk knyCW nyCknxC W nxC (20)\n4. Experiment Result In this paper, we evaluate the recognize performance of Korean lip reading system in\nthe different environment and analysis of this."
    }, {
      "heading" : "4.1 Recognition performance evaluation according to utterance unit",
      "text" : "In this paper, we evaluate the propriety of suggested visual feature by recognizing a\nisolated word which utterance unit of speaker has a word.\nFirst, we defined triviseme HMMs based on single visemes defined in Sec.2 and estim ated the models using HTK3.4.\nTrain data for estimation of triviseme HMMs is the feature vector data extracted from 5, 6000 lip motion video spoken about standard training sentences. Video is the video spoken the standard study sentences by 7 speaker and it is the front faced video.\nTo evaluate recognize performance of the isolated word we used 300 words which is in the training sentences and 200 words which is not in the training sentences.\nIn case of N-best recognition, the recognition result is evaluated as truth when 3 candidates result include correct word .\nWe also took recognition in cases when visemes were given manually, and also when they were not given. The recognition result is as following table.\nTable–2. Recognition result according to utterance unit\nRecognition accuracy(%)\nManual viseme information No manual viseme information\nTraining word 65.4 89.7\nNon-training word 56.5 78.9\nIn this table, you can see that recognition accuracy of training words is better than of non-training words.\nAnd you can also know that the more definite visual speech element for word was given, the higher accuracy the result is.\nAs result of test about scale of word dictionary, the larger the dictionary is, the lower accuracy is.\nIn the case of recognition of isolated word, language model can’t be apply and recognition proceed only by triviseme HMM, the accuracy is low and it’s only for validation test of accuracy of 3 visual speech element."
    }, {
      "heading" : "4.2 Evaluation of Recognition according to speaker",
      "text" : "In the thesis, we have experiments about different speakers for robustness of visual fe\nature vector which we proposed.\nFor that, we made 50 lip videos of every 3 speakers, one person is training speaker and the others are non-training speakers. And then we have recognition test with 150 lip videos.\nAt that time, we have given same viseme information for same sentence of every speakers.\nThe result of recognition experiment for speakers are as follow.\nTable–3. Recognition result according to speaker\nRecognition accuracy(%)\nManual viseme information No manual viseme information\nTraining speaker 52.5 92.3\nNon-training\nspeaker1 48.1 83.8\nNon-training\nSpeaker2 47.6 82.1.\nAs you can see in the table, the recognition accuracy for the non-training speaker is approximately 9~10% lower than study speaker.\nAs the result of analyzing visual feature for the 3 speakers, the variation of the static feature is large but the variation of the dynamic feature is not large for the speaking of the same sentence.\nThis shows that the basic lip shape is different according to the speaker but the variation feature of the lip for the spoken content is not different a lot.\nTherefore we can see the stronger robustness of the dynamic feature than the static feature.\nAs the result of the above experiment we verified the validity of the Korean visemes which is defined in this paper and the effectiveness of the visual feature vector.\nIn the future, we are going to concentrate on the selection of reasonable visual features and enhancement of the robustness in the preprocessing step, and the research of effective\ntrain algorithm to reduce the train time in training step.\nAnd in the recognize section we are going to intensify the research about effective using the dictionary related spoken content in the recognize progress and the design method of advanced language model.\nReferences\n[1] Ue Meier and Rainer Stiefelhagen,”Towards unrestricted lipreading”, Interactive Systems Laboratories , Carnegie Mellon University, Pittsburgh, USA,University of Karlsruhe, Karlsruhe,Germany, 1997\n[2] Chalapathy Neti and Gerasimos Potamianos,”Audio-Visual Speech Recognition”, Workshop 2000 Final Report, 2000.12\n[3] Trent W.Lewis and David M.W.Powers,”Audio-Visual Speech Reconition using Red Exclusion and Neural NetWorks”, School of Informatics and Engineering Flinders University of South Australia, PO Box 2100,Adelaide,South Australia 5001,2001\n[4] Say Wei Foo, Yong Lian and Liang Dong,” Recognition of Visual Speech Elements Using Adaptively Boosted Hidden Markov Models”, IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 14, NO. 5, MAY 2004,pp.693-705\n[5] Luhong Liang, Xiaoxing Liu and Yibao Zhao, “Speaker Independent AudioVisual Continuous Speech Recognition”, Microcomputer Research Labs,Intel Corporation Santa Clara,CA,95052,2003\n[6] Patrick Lucey and Gerasimos Potamianos,”An Extended Pose-Invariant Lipreading System”, IBM Thomas J.Watson Research Center, Yorktown Heights,NY 10598,USA, 2008\n[7] Guanyong Wu,”MODULAR BDPCA BASED VISUAL FEATURE REPRESENTATION FOR LIP-READING”, ICIP 2008,pp.1328-1331\n[8] Iain Matthews, Timothy F.Cootes and J.Andrew Bangham,”Extraction of Visual Features fr Lipreading”, IEEE Trans on Pattern Analysis and Machine Intelligence, Vol.24, No.2, 2002,pp.198-213\n[9] Yuxuan Lan, Barry-John. T and Richard .H, “Improving Visual Features for Lipreading”, School of Computing Sciences, University of East Anglia,UK, School of Electronics and Physical Sciences, University of Surrey,UK,2010\n[10] S.L. Wang, W.H. Lau, S.H. Leung and H. Yan,” A REAL-TIME AUTOMATIC LIPREADING SYSTEM”, ISCAS 2004, pp.101-104\n[11] Juergen Luettin, Neil A. Thacker and Steve W. Beet,” SPEAKER\nIDENTIFICATION BY LIPREADING”, Dept.of Electronic and Electrical Engineering University of Sheffield,Sheffield S1 3JD,UK, 1997\n[12] Hong-xun Yao, Wen Gao1 and Wei Shan,” Visual Features Extracting & Selecting for Lipreading”, AVBPA 2003, LNCS 2688,pp.251-259, 2003\n[13] Kazuhiro Nakamura,Noriaki Murakami,Kazuyoshi Takagi and Naofumi Takagi,”A Real-time Lipreading LSI for Word Recognition”, IEEE,2002,pp.301-306\n[14] Mihaela Gordan,Constantine Kotropoulos and Ioannis Pitas,”Application of Support Vector Machines Classifiers to Visual Speech Recognition”,IEEE ICIP 2002,pp.129-132\n[15] D.G.Stork,G.Wolf and E.Levine,”Neural network lipreading system for improved speech recognition”,IJCNN,1992.6"
    } ],
    "references" : [ {
      "title" : "Stiefelhagen,”Towards unrestricted lipreading”, Interactive Systems Laboratories",
      "author" : [ "Ue Meier", "Rainer" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1997
    }, {
      "title" : "M.W.Powers,”Audio-Visual Speech Reconition using Red Exclusion and Neural NetWorks”, School of Informatics and Engineering Flinders University of South Australia, PO Box 2100,Adelaide,South Australia",
      "author" : [ "Trent W.Lewis", "David" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2001
    }, {
      "title" : "Speaker Independent Audio- Visual Continuous Speech Recognition”, Microcomputer Research Labs,Intel Corporation Santa Clara,CA,95052,2003",
      "author" : [ "Luhong Liang", "Xiaoxing Liu", "Yibao Zhao" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2003
    }, {
      "title" : "Extended Pose-Invariant Lipreading System",
      "author" : [ "Patrick Lucey", "Gerasimos Potamianos", "”An" ],
      "venue" : "IBM Thomas J.Watson Research Center, Yorktown Heights,NY",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2008
    }, {
      "title" : "F.Cootes and J.Andrew Bangham,”Extraction of Visual Features fr Lipreading",
      "author" : [ "Iain Matthews", "Timothy" ],
      "venue" : "IEEE Trans on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2002
    }, {
      "title" : "H, “Improving Visual Features for Lipreading”, School of Computing Sciences, University of East Anglia,UK",
      "author" : [ "Yuxuan Lan", "Barry-John. T", "Richard" ],
      "venue" : "School of Electronics and Physical Sciences,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2010
    }, {
      "title" : "A REAL-TIME AUTOMATIC LIPREADING SYSTEM",
      "author" : [ "S.L. Wang", "W.H. Lau", "S.H. Leung", "H. Yan" ],
      "venue" : "ISCAS 2004,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2004
    }, {
      "title" : "SPEAKER  18  IDENTIFICATION BY LIPREADING”, Dept.of Electronic and Electrical Engineering University of Sheffield,Sheffield S1 3JD,UK",
      "author" : [ "Juergen Luettin", "Neil A. Thacker", "Steve W. Beet" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1997
    }, {
      "title" : "Visual Features Extracting & Selecting for Lipreading",
      "author" : [ "Hong-xun Yao", "Wen Gao", "Wei Shan" ],
      "venue" : "AVBPA",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2003
    }, {
      "title" : "Real-time Lipreading LSI for Word Recognition",
      "author" : [ "Kazuhiro Nakamura", "Noriaki Murakami", "Kazuyoshi Takagi", "Naofumi Takagi", "”A" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2002
    }, {
      "title" : "Pitas,”Application of Support Vector Machines Classifiers to Visual Speech Recognition”,IEEE",
      "author" : [ "Mihaela Gordan", "Constantine Kotropoulos", "Ioannis" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2002
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "[1] It is McGruk effect that is at the bottom of the attempt to use visual information in speech recognition.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "Reference [3] described about the definitions of 9 English visemes such as AB,LDF,IDF,LSH,ALF,LLL,RRR,PAL,WWW.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 2,
      "context" : "While reference [4] described about the definitions of 14 English visemes based on the MPEG-4 Multimedia Standard rule, in reference [5] sip and sp, phonemes to represent no sound used in HTK was introduced.",
      "startOffset" : 133,
      "endOffset" : 136
    }, {
      "referenceID" : 2,
      "context" : "A lot of methods such as PCA based region feature extracting method [5], DCT based region feature extracting method [6], region feature extracting method using 2DPCA called BDPCA (Bidirectional PCA)[7], AAM(Active Appearance Model) based lip region feature extracting method [8] and Hi-LDA based region feature extracting method [9] are",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 3,
      "context" : "A lot of methods such as PCA based region feature extracting method [5], DCT based region feature extracting method [6], region feature extracting method using 2DPCA called BDPCA (Bidirectional PCA)[7], AAM(Active Appearance Model) based lip region feature extracting method [8] and Hi-LDA based region feature extracting method [9] are",
      "startOffset" : 116,
      "endOffset" : 119
    }, {
      "referenceID" : 4,
      "context" : "A lot of methods such as PCA based region feature extracting method [5], DCT based region feature extracting method [6], region feature extracting method using 2DPCA called BDPCA (Bidirectional PCA)[7], AAM(Active Appearance Model) based lip region feature extracting method [8] and Hi-LDA based region feature extracting method [9] are",
      "startOffset" : 275,
      "endOffset" : 278
    }, {
      "referenceID" : 5,
      "context" : "A lot of methods such as PCA based region feature extracting method [5], DCT based region feature extracting method [6], region feature extracting method using 2DPCA called BDPCA (Bidirectional PCA)[7], AAM(Active Appearance Model) based lip region feature extracting method [8] and Hi-LDA based region feature extracting method [9] are",
      "startOffset" : 329,
      "endOffset" : 332
    }, {
      "referenceID" : 6,
      "context" : "There are some methods of extracting lip contour features such as contour feature extracting method applying ASM to extracted lip region [10], after extracting contour of lip, applying affine robust Fourier transform to get Fourier coefficients and to use as feature vector to represent the type of contour [11], a method that gets extracted 10 geometrical parameters from outer and inner contour of lip and use them as a visual feature vector for lip-reading[4] and so on.",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 7,
      "context" : "There are some methods of extracting lip contour features such as contour feature extracting method applying ASM to extracted lip region [10], after extracting contour of lip, applying affine robust Fourier transform to get Fourier coefficients and to use as feature vector to represent the type of contour [11], a method that gets extracted 10 geometrical parameters from outer and inner contour of lip and use them as a visual feature vector for lip-reading[4] and so on.",
      "startOffset" : 307,
      "endOffset" : 311
    }, {
      "referenceID" : 8,
      "context" : "Whereas in reference [12], it describes about the implementation of lip – reading system based on the HMMs for each viseme, in reference [13], HMMs for each words were formed to use for lip-reading of word.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 9,
      "context" : "Whereas in reference [12], it describes about the implementation of lip – reading system based on the HMMs for each viseme, in reference [13], HMMs for each words were formed to use for lip-reading of word.",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 10,
      "context" : "Reference [14] proposed a method to apply SVM to lip-reading system.",
      "startOffset" : 10,
      "endOffset" : 14
    } ],
    "year" : 2014,
    "abstractText" : "In this paper, we defined the viseme (visual speech element) and described about the method of extracting visual feature vector. We defined the 10 visemes based on vowel by analyzing of Korean utterance and proposed the method of extracting the 20-dimensional visual feature vector, combination of static features and dynamic features. Lastly, we took an experiment in recognizing words based on 3-viseme HMM and evaluated the efficiency. Keywordslip-reading, viseme, feature extraction, visual feature vector, HMM",
    "creator" : "PScript5.dll Version 5.2.2"
  }
}