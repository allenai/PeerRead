{
  "name" : "1705.09021.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning to Pour",
    "authors" : [ "Yongqiang Huang", "Yu Sun" ],
    "emails" : [ "yongqiang@mail.usf.edu,", "yusun@cse.usf.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "I. INTRODUCTION\nTasks that are found in manufacturing facilities are often precisely defined, repetitive, and tolerates very little error. Those tasks can be easily handled by industrial robots but proves difficult for human workers. In contrast, each time a daily activity is performed by human, its execution is adjusted according to the environment and is different from last time. Programming an industrial robot to accomplish the same is prohibitively difficult, whereas human completes those tasks with ease. To make robots more widely useful, researchers have been trying to help robots learn a task and generalize to different situations, to which the approach of teaching robots by providing examples has received considerable attention, known as programming by demonstration (PbD) [1]. In this work, we consider learning a task on the level of motion trajectories, and to perform a task, we generate a new trajectory.\nPouring is a simple task that human practice daily. It is the second most frequently executed motion in cooking scenarios after pick-and-place [2]. It relies on rotating a cup (or a container in general) that holds certain material. Sufficient rotation of the cup makes the material come out and sufficient recovery makes the pouring stop. Human typically pour using vision, while sensing the force resulted from the cup. We aim to learn from data how to pour using force feedback alone.\nOne popular framework for motion trajectory generation is dynamical movement primitives (DMP) [3]. DMP is a stable non-linear dynamical system, and is capable of modeling discrete movement such as swinging a tennis racket [4], playing table tennis [5] as well as rhythmic movement such as drumming [6] and walking [7]. DMP consists of a non-linear forcing function, a canonical system and a transformation system. The forcing function defines the desired task trajectory. The transformation system is a point attractor system for discrete movement or a limit cycle system for rhythmic\nThe authors are with the Department of Computer Science and Engineering, University of South Florida, Tampa, FL 33620, USA. Email: yongqiang@mail.usf.edu, yusun@cse.usf.edu\nmovement that is modulated by the forcing function. The canonical system is a well-understood dynamical system that is guaranteed to converge and also serves to provide the phase variable of a task. The parameters that represent the forcing function can be learned from human demonstrations using locally weighted regression [8] or other regression methods.\nBased on DMP, [9] introduced interactive primitives for two-agent collaborative tasks. A predictive distribution of the parameters of DMP is maintained and is used to infer the collaborative activity of one agent while observing that of the other. [10] extends [9] by using a Gaussian mixture of interactive primitives. Such extension allows the correlation between two agents of a collaboration task to be non-linear, and it also enables modeling multiple collaborative tasks.\nAnother approach for motion generation is based on Gaussian mixture model (GMM) and Gaussian mixture regression (GMR) [11]. GMM is used to model the trajectories of a task and GMR is used for task reproduction. GMM is learned using all the variables of a movement including time stamps, and GMR is conducted by infering the movement variables using the learned GMM conditioned on the time stamp. The parameters of GMM can be learned using the ExpectationMaximization algorithm [12]. Several constraints can be incorporated using the product property of Gaussians while performing a new task. The approach can be applied to both world and joint space to consider the trade-off of variations in difference spaces and thus achieves a more accurate control [13]. Also, task-parameterized GMM models a movement using multiple candidate frames of reference, and thus enables more detailed motion production [14]. Using time as the index the approach can produce a task trajectory in a single shot. In comparison, the approach can be extended to model a dynamical system which produces a trajectory step by step [15].\nPrincipal Component Analysis (PCA) also proves useful for motion generation. Known as a dimension reduction technique used on the dimentionality axis of the data, PCA can be used on the time axis of motion trajectories instead to retrieve geometric variations [16]. Besides, PCA can also be applied to find variations in how the motion progresses in time, which, combined with the variations in geometry enables generating motions with more flexibility [17]. Functional PCA (fPCA) extends PCA by introducing continuous-time basis functions and treating trajectories as functions instead of collections of points [18]. [19] applies fPCA for producing trajectories of gross motion such as answering phone and punching, and for making the trajectories avoid obstacles with the guidance of quality via points. [2] uses fPCA for\nar X\niv :1\n70 5.\n09 02\n1v 1\n[ cs\n.R O\n] 2\n5 M\nay 2\n01 7\ngenerating trajectories of fine motion such as pouring. Recently, recurrent neural networks (RNN) receives increasing attention. At any time step, RNN takes a given input and the output emitted from the last time step, and emits an output which is passed to the next time step. The mechanism of RNN makes it inherently suitable for handling sequential data. Similar to DMP [3] and GMR based approach [15], RNN is also capable of modeling general dynamical systems [20], [21]. RNN can be readily used to generate trajectories by relating the emitted output to future inputs. For example, [22] generates English hand writing trajectories by predicting the location offset of the tip of the pen and the end of a stroke. [23] applies a similar strategy to generate Chinese characters. [24] generates motion capture trajectories by directly predicting the joint angle vector for the next time step.\nThe paper goes as follows. In Section II, we review the fundamentals of RNN and particularly LSTM, and present our pouring system. In Section III, we describe the data collection and preparation process, training the system, and creating and training a separate force estimation system. In Section IV, we conduct experiments to evaluate whether our system generalizes to unseen situations. We discuss the performance of our pouring system in Section V."
    }, {
      "heading" : "II. METHODOLOGY FOR POURING TRAJECTORY GENERATION",
      "text" : "In this section, we describe in detail our system of generating a pouring trajectory which builds on long short-term memory. To explain why we choose RNN as the building block, prior to the system description, we review the basics of traditional RNN, and of one particular structure, the long short-term memory."
    }, {
      "heading" : "A. Recurrent Neural Network",
      "text" : "Recurrent neural network (RNN) conducts its computation one step at a time, and at any step its input consists of two parts: a given input, and its own output from the previous time step. The idea is shown in Eq. (1) where xt is the given input, ht−1 and ht are output from the previous and at the current step. The weight W and bias b can be learned using Backpropagation Through Time [25].\nht = tanh ( W [ht−1, xt] > + b )\n(1)\nIn theory, by including its past output in its input, RNN takes the entire history of given inputs into account when it conducts computation at any step, and therefore is inherently suitable for handling sequential data. However, the traditional RNN as shown in Eq. (1) is difficult to train and has vanishing gradients problem, and therefore is inadequate for problems involving long-term dependency [26], [27]. Long short-term memory (LSTM) is a specific RNN design that overcomes the vanishing gradient problem [27]. We use a version of LSTM whose working mechanism is described\nby [28]:\ni = sigm ( Wi[ht−1, xt] > + bi )\n(2) o = sigm ( Wo[ht−1, xt] > + bo ) (3)\nf = sigm ( Wf [ht−1, xt] > + bf )\n(4) g = tanh ( Wg[ht−1, xt] > + bg )\n(5) ct = f ct−1 + i g (6) ht = o tanh(ct) (7)\nwhere i, o, f are the input, output, and forget gates respectively, c is the cell, sigm is short for sigmoid, and represent element-wise multiplication. Fig. 1 gives an illustration.\nLSTM has been proven successful for sequential generation applications including generating hand written characters [22], [23], captioning images [29] and videos [30], drawing images [31], translating natural languages [32], [33], and executing computer programs [34].\nWe identify RNN, and specifically LSTM, as the architecture with which we build our pouring system. The reasons include:\n1) The structure of RNN makes it inherently fit for handling sequences. 2) RNN is capable of modeling dynamical systems. Since a dynamical system is powered by velocity (or acceleration), it has the ability to react to changes of the environment. 3) RNN has proven ability to generate both categorical and continuous-valued sequences. 4) RNN eliminates the needs for temporally aligning sequences before modeling, and therefore preserves the dynamics in a sequence. 5) LSTM supercedes the traditional RNN, and has proven ability to handle long-term dependency."
    }, {
      "heading" : "B. Generating Pouring Trajectory",
      "text" : "The pouring system predicts the velocity of rotation using the force feedback produced by the cup, which is shown as (middle) in Fig. 1.\nWe assume n trials of pouring motion are available. The data of trial i are represented by (θ1...Ti , f1...Ti , z)\n(i), where θ1...Ti is the sequence of cup rotation, Ti is the sequence length, f1...Ti is the sequence of sensed force,\nand z represents static data that characterize the trial. For simplicity, we assume θ, f, z are all one-dimensional.\nWe refer to the system that predicts the velocity of rotation as vel. The actual velocity is computed by\nωt = θt+1 − θt, t = 1 . . . Ti − 1. (8)\nAt step t, vel takes [θt, ft, z]> as input, and generates predicted velocity ω̂t:\nht = LSTM([θt, ft, z]>) (9) ω̂t = fc(ht) (10)\nwhere ‘fc’ is short for ‘fully connected’. The loss is defined using Euclidean distance:\nLvel = 1\nn n∑ i=1 1 Ti − 1 Ti−1∑ t=1 (ω (i) t − ω̂ (i) t ) 2. (11)\nIn order to automatically stop the generation process after the pouring task has completed, we create a stopping system. We refer to the system that stops the pouring motion as stp shown as (right) in Fig 2, which is a binary classifier. At step t, stp takes [θt, ft, z] as input, and outputs a 2-vector rt. We define class 0 as ‘continue’, and class 1 as ‘stop’.\nht = LSTM([θt, ft, z]>) (12) rt = fc(ht) (13) st = softmax(rt) (14)\nLet the target be represented by a trivial one-hot vector s′t = [s′t,1, s ′ t,2] >, where s′t,1, s ′ t,1 ∈ {0, 1} and s′t,1 + s′t,2 = 1. The loss is defined using cross entropy:\nLstp = − n∑\ni=1 Ti∑ t=1 ( s ′(i) t,1 lns (i) t,1 + s ′(i) t,2 lns (i) t,2 ) (15)\nThe initial state of LSTM includes c0 and h0, which are obtained by\nc0 = fc([θ1, f1, z]>), (16) h0 = tanh(c0), (17)\nas shown in Fig. 3. The trajectory is generated by first initializing vel and stp, and then keep generating and executing rotational velocities. Specifically, the trajectory generation process is described in Alg. 1.\nAlgorithm 1 Trajectory Generation 1: Initialize vel and stp using [θ1, f1, z]>\n2: t← 1 3: while True do 4: ωt ← vel([θt, ft, z]>) 5: θt+1 ← θt + ωt 6: s← argmax stp([θt, ft, z]>) 7: t← t+ 1 8: if s == 1 then 9: Break\n10: end if 11: end while"
    }, {
      "heading" : "III. DATA PREPARATION AND TRAINING",
      "text" : "The equipment for data collection includes six different cups, ten different containers, one ATI mini40 force and torque (FT) sensor, and one Polhemus Patriot motion tracker. We refer to the pour-from container as cup and the pour-to container as container. All cups are mutually different and so are all the containers. The FT sensor records (fx, fy, fz, τx, τy, τz) at 1KHz. The motion tracker records (x, y, z, yaw, pitch, roll) at 60Hz. The cup, the force sensor, and the motion tracker are connected by 3D printed adapters, shown in Fig. 4. The materials that are poured include water, beans, and ice.\nWe obtain the empty reading by keeping an empty cup in a level position, taking 500 FT samples (which takes 0.5 second), and then taking the average. Similarly, for each trial, we obtain the initial reading right before the trial with material in the cup, and the final reading right after the trial with or without material in the cup depending on the trial.\nWe define the sensed force as f = √ f2x + f 2 y + f 2 z . (18)\nIn total we collected 1,138 trials which involves 3 subjects. Each trial is represented by a sequence {at}Tit=1 where at ∈ R10 and\nat = [\nθt rotation angle at time t (degree) ft sensed force at time t (lbf) finit sensed force before pouring (lbf) fempty sensed force while cup is empty (lbf) ffinal sensed force after pouring (lbf) dcup diameter of the cup (mm) hcup height of the cup (mm) dctn diameter of the container (mm) hctn height of the container (mm) ρ material density / water density (unitless)\n] We pad all the sequences to the maximum length in the data: Tmax = max({Ti}). For vel, we pad using zero because zero padding makes it easy to compute the original length of a sequence during training. For stp, we pad using the end value of the sequence because stp is intended to be used on generated motions which will not have zero padding.\nWe train using the Adam optimizer [35] and set the learning rate to 0.01. We trained each system for a fixed number of epochs: 4,000 for vel, and 2,000 for stp. The training error for vel ranges from 0.002 to 0.005 (mm), and the accuracy of stp ranges from 0.9 to 0.98."
    }, {
      "heading" : "A. Training Force Estimation",
      "text" : "In order to run our approach in simulation, we need to have force feedback after we have arrived at a new rotation. Real force feedback is not applicable in simulation. The movement of the liquid during pouring forms a complex dynamical system and is difficult to calculate analytically. Thus, to get force feedback, we decide to generate the force by ourselves. To that end, we learn from data the mapping relationship from rotation angles to force, and then use the learned model to estimate the force corresponding to current rotation.\nThus, we need to train a new system. We refer to the system that estimates the sensed force from rotation as frc, shown as (left) in Fig. 2. At step t, frc takes [θt, z]> as input, and produces estimated force f̂t:\nht = LSTM([θt, z]>) (19) f̂t = fc(ht) (20)\nThe loss is defined using Euclidean distance:\nLfrc = 1\nn n∑ i=1 1 Ti Ti∑ t=1 (f (i) t − f̂ (i) t ) 2. (21)\nThe initialization of frc includes\nc0 = fc([θ1, z]>), (22) h0 = tanh(c0), (23)\nas shown in Fig. 3. The data preparation for frc uses zero padding. We train the frc with a fixed 2000 epochs, and the error ranges between 0.002 to 0.003 (lbf).\nWith frc, the trajectory generation process needs modification. Force can no longer be assumed to be available, but must be produced explicitly by frc. The modified trajectory generation process is shown in Alg. 2.\nAlgorithm 2 Trajectory generation for simulation 1: Initialize frc using [θ1, z]>\n2: f1 ← frc([θ1, z]>) 3: Initialize vel and stp using [θ1, f1, z]> 4: t← 1 5: while t < Tmax do 6: ωt ← vel([θt, ft, z]>) 7: θt+1 ← θt + ωt 8: s← argmax stp([θt, ft, z]>) 9: ft+1 ← frc([θt+1, z]>)\n10: t← t+ 1 11: if s == 1 then 12: Break 13: end if 14: end while"
    }, {
      "heading" : "IV. EXPERIMENT ON GENERALIZATION",
      "text" : "We evaluate the generalization ability of our approach and see if it can generate pouring motion in unseen situations. Given a test sequence, we extract θ1 and z, and generate a sequence using Alg. 2. The evaluation is conducted in simulation.\nWe test the system using unseen 1) cup, 2) container, 3) material, 4) cup and container, 5) container and material, 6) cup and material, 7) cup and container and material.\nA. Identifying success\nWe evaluate the generalization ability of the pouring system using dynamic time warping (DTW) [36], which gives the minimum normalized distance between two trajectories.\nWe provide a set of test sequence which include an element that is unseen during training and see if the system is able to adapt to the changes. Let the set of test sequences be {xi}mi=1. We first compute the distance between each pair of test sequences and draw a histogram:\nh1 = hist({dtw(xi, xj)}i 6=j) i, j = 1, 2, . . . ,m. (24)\nEach xi can be used to generate a new trajectory x′i. We compute the distance between x′i and every test sequence xj and draw another histogram.\nh2 = hist({dtw(x′i, xj)}) i, j = 1, 2, . . . ,m. (25)\nBoth histograms are normalized. We visually compare the similarity between h1 and h2. If they are similar, then it means the generated trajectories are similar to the trajectories executed by human, which identifies that the generalization succeeds. The system fails to generalize if otherwise."
    }, {
      "heading" : "B. Results",
      "text" : "The results for the seven cases of unseen elements of the pouring characteristics are shown in Fig. 5 to 11. Generalization on cup, or container, or material alone is successful because the pairing histograms are similar (Fig. 5, 6 and 7). Generalizing on cup and container (Fig. 8) and container and material (Fig. 9) can be considered successful because of the similarity in the concentration of the small-distances, despite the difference on mid to high-valued distance parts, which occupy only a small portion of all the distances. Generalizing on cup and material fails as well as on cup and container and materials, as shown in Fig. 10 and Fig. 11. For cup and container and material, only 8 test sequences are available, which may partly contribute to the difference between the two histograms.\nFig. 7. Generalizing on an unseen material\nFig. 8. Generalizing on an unseen cup and container"
    }, {
      "heading" : "V. DISCUSSION",
      "text" : "We have presented an approach of generating pouring trajectories by learning from pouring motions demonstrated by human subjects. The approach uses force feedback from the cup to determine the future velocity of pouring. We aim to make the system generalize its learned knowledge to unseen situations. The system successfully generalize when either a cup, a container, or the material changes, and starts to stumble when changes of more than one element are present. Since the total size of data does not change, the more that is left out for testing (more unseen elements), the less there is available for training. Thus, the system accepts weaker training and after which faces more demanding challenges. The observed results of degrading performance with increading generalization difficulty is expected.\nWe have started evaluating the system on an industrial robot that is equipped with a force sensor. The evaluation is still under way.\nFuture work includes finishing the evaluation on the industrial robot, designing a quantitative measure that measures the degree of success of a generated trajectory, modifying the architecture to emphasize the role of initial and final force, and getting help from reinforcement learning."
    } ],
    "references" : [ {
      "title" : "Robot Programming by Demonstration",
      "author" : [ "A. Billard", "S. Calinon", "R. Dillmann", "S. Schaal" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2008
    }, {
      "title" : "Functional object-oriented network for manipulation learning",
      "author" : [ "D. Paulius", "Y. Huang", "R. Milton", "W.D. Buchanan", "J. Sam", "Y. Sun" ],
      "venue" : "2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Oct 2016, pp. 2655–2662.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Dynamical movement primitives: Learning attractor models for motor behaviors",
      "author" : [ "A. Ijspeert", "J. Nakanishi", "H. Hoffmann", "P. Pastor", "S. Schaal" ],
      "venue" : "Neural Computation, vol. 25, no. 2, pp. 328–373, 2 2013.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Movement imitation with nonlinear dynamical systems in humanoid robots",
      "author" : [ "A.J. Ijspeert", "J. Nakanishi", "S. Schaal" ],
      "venue" : "Proceedings 2002 IEEE International Conference on Robotics and Automation (Cat. No.02CH37292), vol. 2, 2002, pp. 1398–1403.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Movement templates for learning of hitting and batting",
      "author" : [ "J. Kober", "K. Mlling", "O. Krmer", "C.H. Lampert", "B. Schlkopf", "J. Peters" ],
      "venue" : "2010 IEEE International Conference on Robotics and Automation, May 2010, pp. 853–858.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Movement planning and imitation by shaping nonlinear attractors",
      "author" : [ "S. Schaal" ],
      "venue" : "Proceedings of the 12th Yale Workshop on Adaptive and Learning Systems, Yale University, New Haven, CT, 2003. [Online]. Available: http://www-clmc.usc.edu/publications/ S/schaal-YWALS2003.pdf",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Learning from demonstration and adaptation of biped locomotion",
      "author" : [ "J. Nakanishi", "J. Morimoto", "G. Endo", "G. Cheng", "S. Schaal", "M. Kawato" ],
      "venue" : "vol. 47, no. 2-3, pp. 79–91, 2004. [Online]. Available: http://www-clmc.usc.edu/publications/N/nakanishi-RAS2004.pdf",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Constructive incremental learning from only local information",
      "author" : [ "S. Schaal", "C.G. Atkeson" ],
      "venue" : "Neural Comput., vol. 10, no. 8, pp. 2047– 2084, Nov. 1998.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Interaction primitives for human-robot cooperation tasks",
      "author" : [ "H.B. Amor", "G. Neumann", "S. Kamthe", "O. Kroemer", "J. Peters" ],
      "venue" : "2014 IEEE International Conference on Robotics and Automation (ICRA), May 2014, pp. 2831–2837.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Learning multiple collaborative tasks with a mixture of interaction primitives",
      "author" : [ "M. Ewerton", "G. Neumann", "R. Lioutikov", "H.B. Amor", "J. Peters", "G. Maeda" ],
      "venue" : "2015 IEEE International Conference on Robotics and Automation (ICRA), May 2015, pp. 1535–1542.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Robot Programming by Demonstration: A Probabilistic Approach",
      "author" : [ "S. Calinon" ],
      "venue" : "EPFL/CRC Press,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2009
    }, {
      "title" : "Maximum likelihood from incomplete data via the em algorithm",
      "author" : [ "A.P. Dempster", "N.M. Laird", "D.B. Rubin" ],
      "venue" : "JOURNAL OF THE ROYAL STATISTICAL SOCIETY, SERIES B, vol. 39, no. 1, pp. 1–38, 1977.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1977
    }, {
      "title" : "A probabilistic programming by demonstration framework handling skill constraints in joint space and task space",
      "author" : [ "S. Calinon", "A. Billard" ],
      "venue" : "Proc. IEEE/RSJ Intl Conf. on Intelligent Robots and Systems (IROS), September 2008, pp. 367–372.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Statistical dynamical systems for skills acquisition in humanoids",
      "author" : [ "S. Calinon", "Z. Li", "T. Alizadeh", "N.G. Tsagarakis", "D.G. Caldwell" ],
      "venue" : "2012 12th IEEE-RAS International Conference on Humanoid Robots (Humanoids 2012), Nov 2012, pp. 323–329.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Handling of multiple constraints and motion alternatives in a robot programming by demonstration framework",
      "author" : [ "S. Calinon", "F. D’halluin", "D.G. Caldwell", "A.G. Billard" ],
      "venue" : "2009 9th IEEE-RAS International Conference on Humanoid Robots, Dec 2009, pp. 582–588.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Movement primitives, principal component analysis, and the efficient generation of natural motions",
      "author" : [ "B. Lim", "S. Ra", "F.C. Park" ],
      "venue" : "Proceedings of the 2005 IEEE International Conference on Robotics and Automation, April 2005, pp. 4630–4635.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Interactive generation of human animation with deformable motion models",
      "author" : [ "J. Min", "Y.-L. Chen", "J. Chai" ],
      "venue" : "ACM Trans. Graph., vol. 29, no. 1, pp. 9:1–9:12, Dec. 2009. [Online]. Available: http://doi.acm.org/10.1145/1640443.1640452",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Functional Data Analysis with R and Matlab",
      "author" : [ "J.O. Ramsay", "G. Hooker", "S. Graves" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2009
    }, {
      "title" : "Generating manipulation trajectory using motion harmonics",
      "author" : [ "Y. Huang", "Y. Sun" ],
      "venue" : "2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Sept 2015, pp. 4949–4954.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Modeling Dynamic System by Recurrent Neural Network with State Variables",
      "author" : [ "M. Han", "Z. Shi", "W. Wang" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2004
    }, {
      "title" : "Synthesis of recurrent neural networks for dynamical system simulation",
      "author" : [ "A.P. Trischler", "G.M. DEleuterio" ],
      "venue" : "Neural Networks, vol. 80, pp. 67 – 78, 2016. [Online]. Available: //www.sciencedirect. com/science/article/pii/S0893608016300314",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Generating sequences with recurrent neural networks",
      "author" : [ "A. Graves" ],
      "venue" : "CoRR, vol. abs/1308.0850, 2013. [Online]. Available: http://arxiv.org/ abs/1308.0850",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Drawing and recognizing chinese characters with recurrent neural network",
      "author" : [ "X. Zhang", "F. Yin", "Y. Zhang", "C. Liu", "Y. Bengio" ],
      "venue" : "CoRR, vol. abs/1606.06539, 2016. [Online]. Available: http://arxiv.org/abs/ 1606.06539",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Recurrent network models for human dynamics",
      "author" : [ "K. Fragkiadaki", "S. Levine", "P. Felsen", "J. Malik" ],
      "venue" : "Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV), ser. ICCV ’15. Washington, DC, USA: IEEE Computer Society, 2015, pp. 4346– 4354. [Online]. Available: http://dx.doi.org/10.1109/ICCV.2015.494",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Backpropagation through time: what it does and how to do it",
      "author" : [ "P.J. Werbos" ],
      "venue" : "Proceedings of the IEEE, vol. 78, no. 10, pp. 1550–1560, Oct 1990.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Learning long-term dependencies with gradient descent is difficult",
      "author" : [ "Y. Bengio", "P. Simard", "P. Frasconi" ],
      "venue" : "Trans. Neur. Netw., vol. 5, no. 2, pp. 157–166, Mar. 1994. [Online]. Available: http://dx.doi.org/10.1109/72.279181",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Long short-term memory",
      "author" : [ "S. Hochreiter", "J. Schmidhuber" ],
      "venue" : "Neural computation, vol. 9, no. 8, pp. 1735–1780, 1997.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Recurrent neural network regularization",
      "author" : [ "W. Zaremba", "I. Sutskever", "O. Vinyals" ],
      "venue" : "CoRR, vol. abs/1409.2329, 2014. [Online]. Available: http://arxiv.org/abs/1409.2329",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Deep visual-semantic alignments for generating image descriptions",
      "author" : [ "A. Karpathy", "F. Li" ],
      "venue" : "CoRR, vol. abs/1412.2306, 2014. [Online]. Available: http://arxiv.org/abs/1412.2306",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Sequence to sequence - video to text",
      "author" : [ "S. Venugopalan", "M. Rohrbach", "J. Donahue", "R.J. Mooney", "T. Darrell", "K. Saenko" ],
      "venue" : "CoRR, vol. abs/1505.00487, 2015. [Online]. Available: http://arxiv.org/abs/1505.00487",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "DRAW: A recurrent neural network for image generation",
      "author" : [ "K. Gregor", "I. Danihelka", "A. Graves", "D. Wierstra" ],
      "venue" : "CoRR, vol. abs/1502.04623, 2015. [Online]. Available: http://arxiv.org/abs/1502. 04623",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "I. Sutskever", "O. Vinyals", "Q.V. Le" ],
      "venue" : "CoRR, vol. abs/1409.3215, 2014. [Online]. Available: http://arxiv.org/abs/1409.3215",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Learning phrase representations using RNN encoderdecoder for statistical machine translation",
      "author" : [ "K. Cho", "B. van Merrienboer", "Ç. Gülçehre", "F. Bougares", "H. Schwenk", "Y. Bengio" ],
      "venue" : "CoRR, vol. abs/1406.1078, 2014. [Online]. Available: http://arxiv.org/abs/1406.1078",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Learning to execute",
      "author" : [ "W. Zaremba", "I. Sutskever" ],
      "venue" : "CoRR, vol. abs/1410.4615, 2014. [Online]. Available: http://arxiv.org/abs/1410. 4615",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "D.P. Kingma", "J. Ba" ],
      "venue" : "CoRR, vol. abs/1412.6980, 2014. [Online]. Available: http://arxiv.org/abs/1412.6980",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Dynamic programming algorithm optimization for spoken word recognition",
      "author" : [ "H. Sakoe", "S. Chiba" ],
      "venue" : "IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 26, no. 1, pp. 43–49, Feb 1978.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 1978
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "To make robots more widely useful, researchers have been trying to help robots learn a task and generalize to different situations, to which the approach of teaching robots by providing examples has received considerable attention, known as programming by demonstration (PbD) [1].",
      "startOffset" : 276,
      "endOffset" : 279
    }, {
      "referenceID" : 1,
      "context" : "It is the second most frequently executed motion in cooking scenarios after pick-and-place [2].",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 2,
      "context" : "One popular framework for motion trajectory generation is dynamical movement primitives (DMP) [3].",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 3,
      "context" : "DMP is a stable non-linear dynamical system, and is capable of modeling discrete movement such as swinging a tennis racket [4], playing table tennis [5] as well as rhythmic movement such as drumming [6] and walking [7].",
      "startOffset" : 123,
      "endOffset" : 126
    }, {
      "referenceID" : 4,
      "context" : "DMP is a stable non-linear dynamical system, and is capable of modeling discrete movement such as swinging a tennis racket [4], playing table tennis [5] as well as rhythmic movement such as drumming [6] and walking [7].",
      "startOffset" : 149,
      "endOffset" : 152
    }, {
      "referenceID" : 5,
      "context" : "DMP is a stable non-linear dynamical system, and is capable of modeling discrete movement such as swinging a tennis racket [4], playing table tennis [5] as well as rhythmic movement such as drumming [6] and walking [7].",
      "startOffset" : 199,
      "endOffset" : 202
    }, {
      "referenceID" : 6,
      "context" : "DMP is a stable non-linear dynamical system, and is capable of modeling discrete movement such as swinging a tennis racket [4], playing table tennis [5] as well as rhythmic movement such as drumming [6] and walking [7].",
      "startOffset" : 215,
      "endOffset" : 218
    }, {
      "referenceID" : 7,
      "context" : "The parameters that represent the forcing function can be learned from human demonstrations using locally weighted regression [8] or other regression methods.",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 8,
      "context" : "Based on DMP, [9] introduced interactive primitives for two-agent collaborative tasks.",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 9,
      "context" : "[10] extends [9] by using a Gaussian mixture of interactive primitives.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 8,
      "context" : "[10] extends [9] by using a Gaussian mixture of interactive primitives.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 10,
      "context" : "Another approach for motion generation is based on Gaussian mixture model (GMM) and Gaussian mixture regression (GMR) [11].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 11,
      "context" : "The parameters of GMM can be learned using the ExpectationMaximization algorithm [12].",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 12,
      "context" : "The approach can be applied to both world and joint space to consider the trade-off of variations in difference spaces and thus achieves a more accurate control [13].",
      "startOffset" : 161,
      "endOffset" : 165
    }, {
      "referenceID" : 13,
      "context" : "Also, task-parameterized GMM models a movement using multiple candidate frames of reference, and thus enables more detailed motion production [14].",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 14,
      "context" : "In comparison, the approach can be extended to model a dynamical system which produces a trajectory step by step [15].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 15,
      "context" : "Known as a dimension reduction technique used on the dimentionality axis of the data, PCA can be used on the time axis of motion trajectories instead to retrieve geometric variations [16].",
      "startOffset" : 183,
      "endOffset" : 187
    }, {
      "referenceID" : 16,
      "context" : "Besides, PCA can also be applied to find variations in how the motion progresses in time, which, combined with the variations in geometry enables generating motions with more flexibility [17].",
      "startOffset" : 187,
      "endOffset" : 191
    }, {
      "referenceID" : 17,
      "context" : "Functional PCA (fPCA) extends PCA by introducing continuous-time basis functions and treating trajectories as functions instead of collections of points [18].",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 18,
      "context" : "[19] applies fPCA for producing trajectories of gross motion such as answering phone and punching, and for making the trajectories avoid obstacles with the guidance of quality via points.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 1,
      "context" : "[2] uses fPCA for ar X iv :1 70 5.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "Similar to DMP [3] and GMR based approach [15], RNN is also capable of modeling general dynamical systems [20], [21].",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 14,
      "context" : "Similar to DMP [3] and GMR based approach [15], RNN is also capable of modeling general dynamical systems [20], [21].",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 19,
      "context" : "Similar to DMP [3] and GMR based approach [15], RNN is also capable of modeling general dynamical systems [20], [21].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 20,
      "context" : "Similar to DMP [3] and GMR based approach [15], RNN is also capable of modeling general dynamical systems [20], [21].",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 21,
      "context" : "For example, [22] generates English hand writing trajectories by predicting the location offset of the tip of the pen and the end of a stroke.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 22,
      "context" : "[23] applies a similar strategy to generate Chinese characters.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "[24] generates motion capture trajectories by directly predicting the joint angle vector for the next time step.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "The weight W and bias b can be learned using Backpropagation Through Time [25].",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 25,
      "context" : "(1) is difficult to train and has vanishing gradients problem, and therefore is inadequate for problems involving long-term dependency [26], [27].",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 26,
      "context" : "(1) is difficult to train and has vanishing gradients problem, and therefore is inadequate for problems involving long-term dependency [26], [27].",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 26,
      "context" : "Long short-term memory (LSTM) is a specific RNN design that overcomes the vanishing gradient problem [27].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 27,
      "context" : "Mechanism inside an LSTM unit, Zaremba’s version [28]",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 27,
      "context" : "by [28]:",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 21,
      "context" : "LSTM has been proven successful for sequential generation applications including generating hand written characters [22], [23], captioning images [29] and videos [30], drawing images [31], translating natural languages [32], [33], and executing computer programs [34].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 22,
      "context" : "LSTM has been proven successful for sequential generation applications including generating hand written characters [22], [23], captioning images [29] and videos [30], drawing images [31], translating natural languages [32], [33], and executing computer programs [34].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 28,
      "context" : "LSTM has been proven successful for sequential generation applications including generating hand written characters [22], [23], captioning images [29] and videos [30], drawing images [31], translating natural languages [32], [33], and executing computer programs [34].",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 29,
      "context" : "LSTM has been proven successful for sequential generation applications including generating hand written characters [22], [23], captioning images [29] and videos [30], drawing images [31], translating natural languages [32], [33], and executing computer programs [34].",
      "startOffset" : 162,
      "endOffset" : 166
    }, {
      "referenceID" : 30,
      "context" : "LSTM has been proven successful for sequential generation applications including generating hand written characters [22], [23], captioning images [29] and videos [30], drawing images [31], translating natural languages [32], [33], and executing computer programs [34].",
      "startOffset" : 183,
      "endOffset" : 187
    }, {
      "referenceID" : 31,
      "context" : "LSTM has been proven successful for sequential generation applications including generating hand written characters [22], [23], captioning images [29] and videos [30], drawing images [31], translating natural languages [32], [33], and executing computer programs [34].",
      "startOffset" : 219,
      "endOffset" : 223
    }, {
      "referenceID" : 32,
      "context" : "LSTM has been proven successful for sequential generation applications including generating hand written characters [22], [23], captioning images [29] and videos [30], drawing images [31], translating natural languages [32], [33], and executing computer programs [34].",
      "startOffset" : 225,
      "endOffset" : 229
    }, {
      "referenceID" : 33,
      "context" : "LSTM has been proven successful for sequential generation applications including generating hand written characters [22], [23], captioning images [29] and videos [30], drawing images [31], translating natural languages [32], [33], and executing computer programs [34].",
      "startOffset" : 263,
      "endOffset" : 267
    }, {
      "referenceID" : 34,
      "context" : "We train using the Adam optimizer [35] and set the learning rate to 0.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 35,
      "context" : "We evaluate the generalization ability of the pouring system using dynamic time warping (DTW) [36], which gives the minimum normalized distance between two trajectories.",
      "startOffset" : 94,
      "endOffset" : 98
    } ],
    "year" : 2017,
    "abstractText" : "Pouring is a simple task people perform daily. It is the second most frequently executed motion in cooking scenarios, after pick-and-place. We present a pouring trajectory generation approach, which uses force feedback from the cup to determine the future velocity of pouring. The approach uses recurrent neural networks as its building blocks. We collected the pouring demonstrations which we used for training. To test our approach in simulation, we also created and trained a force estimation system. The simulated experiments show that the system is able to generalize to single unseen element of the pouring characteristics.",
    "creator" : "LaTeX with hyperref package"
  }
}