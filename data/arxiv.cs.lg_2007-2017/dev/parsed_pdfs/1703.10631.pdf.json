{
  "name" : "1703.10631.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Interpretable Learning for Self-Driving Cars by Visualizing Causal Attention",
    "authors" : [ "Jinkyu Kim" ],
    "emails" : [ "canny}@berkeley.edu" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Self-driving vehicle control has made dramatic progress in the last several years, and many auto vendors have pledged large-scale commercialization in a 2-3 year time frame. These controllers use a variety of approaches but recent successes [3] suggests that neural networks will be widely used in self-driving vehicles. But neural networks are notoriously cryptic - both network architecture and hidden layer activations may have no obvious relation to the function being estimated by the network. An exception to the rule is visual attention networks [26, 21, 7]. These networks provide spatial attention maps - areas of the image\nthat the network attends to - that can be displayed in a way that is easy for users to interpret. They provide their attention maps instantly on images that are input to the network, and in this case on the stream of images from automobile video. As we show from our examples later, visual attention maps lie over image areas that have intuitive influence on the vehicle’s control signal.\nBut attention maps are only part of the story. Attention is a mechanism for filtering out non-salient image content. But attention networks need to find all potentially salient image areas and pass them to the main recognition network (a CNN here) for a final verdict. For instance, the attention network will attend to trees and bushes in areas of an image where road signs commonly occur. Just as a human will use peripheral vision to determine that ”there is something there”, and then visually fixate on the item to determine what it actually is. We therefore post-process the attention network’s output, clustering it into attention ”blobs” and then mask (set the attention weights to zero) each blob to determine the effect on the end-to-end network output. Blobs that have an causal effect on network output are retained while those that do not are removed from the visual map presented to the user.\nFigure 1 shows an overview of our model. Our approach can be divided into three steps: (1) Encoder: convolutional feature extraction, (2) Coarse-grained decoder by visual attention mechanism, and (3) Fine-grained decoder: causal visual saliency detection and refinement of attention map. Our contributions are as follows:\n• We show that visual attention heat maps are suitable ”explanations” for the behavior of a deep neural vehicle controller, and do not degrade control accuracy. • We show that attention maps comprise ”blobs” that can\nbe segmented and filtered to produce simpler and more accurate maps of visual saliency. • We demonstrate the effectiveness of using our model\nwith three large real-world driving datasets that contain over 1,200,000 video frames (approx. 16 hours).\nar X\niv :1\n70 3.\n10 63\n1v 1\n[ cs\n.C V\n] 3\n0 M\nar 2\n• We illustrate typical spurious attention sources in driving video and quantify the reduction in explanation complexity from causal filtering."
    }, {
      "heading" : "2. Related Works",
      "text" : ""
    }, {
      "heading" : "2.1. End-to-End Learning for Self-driving Cars",
      "text" : "Self-driving vehicle controllers can be classified as: mediated perception approaches and end-to-end learning approaches. The mediated perception approach depends on recognizing human-designated features (i.e., lane markings and cars) in a controller with if-then-else rules. Some examples include Urmson et al. [24], Buehler et al. [4], and Levinson et al. [18].\nRecently there is growing interest in end-to-end learning vehicle control. Most of these approaches learn a controller by supervised regression to recordings from human drivers. The training data comprise video from one or more vehicle cameras, and the control outputs (steeting and possible acceleration and braking) from the driver. ALVINN (Autonomous Land Vehicle In a Neural Network) [19] was the first attempt to use neural network for directly mapping images to navigate the direction of the vehicle. More recently Bojarski et al. [3] demonstrated good performance with convolutional neural networks (CNNs) to directly map images from a front-view camera to steering controls. Xu et al. [25] proposed an end-to-end egomotion prediction approach that takes raw pixels and prior vehicle state signals as inputs and predicts several a sequence of discretized actions (i.e., straight, stop, left-turn, and right-turn). These\nmodels show good performance but their behavior is opaque and uninterpretable.\nAn intermediate approach was explored in Chen et al. [6] who defined human-interpretable intermediate features such as the curvature of lane, distances to neighboring lanes, and distances from the front-located vehicles. A CNN is trained to produce these features, and a simple controller maps them to steering angle. They also generated deconvolution maps to show image areas that affected network output. However, there were several difficulties with that work: (i) use of the intermediate layer caused significant degradation (40% or more) of control accuracy (ii) the intermediate feature descriptors provide a limited and adhoc vocabulary for explanations and (iii) the authors noted the presence of spurious input features but there was no attempt to remove them. By contrast, our work shows that state-of-the-art driving models can be made interpretable without sacrificing accuracy, that attention models provide more robust image annotation, and causal analysis further improves explanation saliency."
    }, {
      "heading" : "2.2. Visual Explanation",
      "text" : "In a landmark work, Zeiler and Fergus [28] used ”deconvolution” to visualize layer activations of convolutional networks. LeCun et al. [16] provides textual explanations of images as automatically-generated captions. Building on this work, Bojarski et al. [2] developed a richer notion of ”contribution” of a pixel to the output. However a difficulty with deconvolution-style approaches is the lack of formal\nmeasures of how the network output is affected by spatiallyextended features (rather than pixels). Attention-based approaches like ours directly extract areas of the image that did not affect network output (because they were masked out by the attention model), and causal filtering further removes spurious image areas. Hendricks et al. [11] trains a deep network to generate species specific explanation without explicitly identifying semantic features. Also, Justin Johnson et al. [14] proposes DenseCap which uses fully convolutional localization networks for dense captioning, their paper achieves both localizing objects and describing salient regions in images using natural langauge. In reinforcement learning, Zrihem et al. [27] proposes a visualization method to interpret the agents action by describing Markov Decision Process model as a directed graph on a t-SNE map."
    }, {
      "heading" : "3. Method",
      "text" : ""
    }, {
      "heading" : "3.1. Preprocessing",
      "text" : "Our model predicts continuous steering angle commands from input raw pixels in an end-to-end manner. As discussed by Bojarski et al. [3], our model predicts the inverse turning radius ût (= r−1t , where rt is the turning radius) at every timestep t instead of steering angle commands, which depends on the vehicle’s steering geometry and also result in numerical instability when predicting near zero steering angle commands. The relationship between the inverse turning radius ut and the steering angle command θt can be approximated by Ackermann steering geometry [20] as follows:\nθt = fsteers(ut) = utdwKs(1 +Kslipvt 2) (1)\nwhere θt in degrees and vt (m/s) is a steering angle and a velocity at time t, respectively. Ks, Kslip, and dw are vehicle-specific parameters. Ks is a steering ratio between the turn of the steering and the turn of the wheels. Kslip represents the relative motion between a wheel and the surface of road. dw is the length between the front and rear wheels. Our model therefore needs two measurements for training: timestamped vehicle’s speed and steering angle commands.\nTo reduce computational cost, each raw input image is down-sampled and resized to 80×160×3 with nearestneighbor scaling algorithm. For images with different raw aspect ratios, we cropped the height to match the ratio before down-sampling. We also normalized pixel values to [0, 1] in HSV colorspace.\nWe utilize a single exponential smoothing method [13] to reduce the effect of human factors-related performance variation and the effect of measurement noise. Formally, given a smoothing factor 0 ≤ αs ≤ 1, the simple exponen-\ntial smoothing method is defined as follows:( θ̂t v̂t ) = αs ( θt vt ) + (1− αs) ( θ̂t−1 v̂t−1 ) (2)\nwhere θ̂t and v̂t are the smoothed time-series of θt and vt, respectively. Note that they are same as the original timeseries when αs = 1, while values of αs closer to zero have a greater smoothing effect and are less responsive to recent changes. The effect of applying smoothing methods is summarized in Section 4.4."
    }, {
      "heading" : "3.2. Encoder: Convolutional Feature Extraction",
      "text" : "We use a convolutional neural network to extract a set of encoded visual feature vector, which we refer to as a convolutional feature cube xt. Each feature vectors may contain high-level object descriptions that allow the attention model to selectively pay attention to certain parts of an input image by choosing a subset of feature vectors.\nAs depicted in Figure 1, we use a 5-layered convolution network that is utilized by Bojarski et al. [3] to learn a model for self-driving cars. As discussed by Lee et al. [17], we omit max-pooling layers to prevent spatial locational information loss as the strongest activation propagates through the model. We collect a three-dimensional convolutional feature cube xt from the last layer by pushing the preprocessed image through the model, and the output feature cube will be used as an input of the LSTM layers, which we will explain in Section 3.3. Using this convolutional feature cube from the last layer has advantages in generating high-level object descriptions, thus increasing interpretability and reducing computational burdens for a real-time system.\nFormally, a convolutional feature cube of size W×H×D is created at each timestep t from the last convolutional layer. We then collect xt, a set of L = W × H vectors, each of which is a D-dimensional feature slice for different spatial parts of the given input.\nxt = {xt,1, xt,2, . . . , xt,L} (3)\nwhere xt,i ∈ RD for i ∈ {1, 2, . . . , L}. This allows us to focus selectively on different spatial parts of the given image by choosing a subset of these L feature vectors."
    }, {
      "heading" : "3.3. Coarse-Grained Decoder: Visual Attention",
      "text" : "The goal of soft deterministic attention mechanism π({xt,1, xt,2, . . . , xt,L}) is to search for a good context vector yt, which is defined as a combination of convolutional feature vectors xt,i, while producing better prediction accuracy. We utilize a deterministic soft attention mechanism that is trainable by standard back-propagation methods, which thus has advantages over a hard stochastic attention mechanism that requires reinforcement learning. Our\nmodel feeds α weighted context yt to the system as discuss by several works [21, 26]:\nyt = fflatten(π({αt,i}, {xt,i})) = fflatten({αt,ixt,i})\n(4)\nwhere i = {1, 2, . . . , L}. αt,i is a scalar attention weight value associated with a certain grid of input image in such that ∑ i αt,i = 1. These attention weights can be interpreted as the probability over L convolutional feature vectors that the location i is the important part to produce better estimation accuracy. fflatten is a flattening function. yt is thus D×L-dimensional vector that contains convolutional feature vectors weighted by attention weights. Note that, our attention mechanism π({αt,i}, {xt,i}) is different from the previous works [21, 26], which use the α weighted average context yt = ∑L i=1 αt,ixt,i. We observed that this change significantly improves overall prediction accuracy. The performance comparison is explained in Section 4.5.\nAs we summarize in Figure 1, we use a long shortterm memory (LSTM) network [12] that predicts the inverse turning radius ût and generates attention weights {αt,i} at each timestep t conditioned on the previous hidden state ht and a current convolutional feature cube xt. More formally, let us assume a hidden layer fattn(xt,i, ht−1) conditioned on the previous hidden state ht−1 and the current feature vectors {xt,i}. The attention weight {αt,i} for each spatial location i is then computed by multinomial logistic regression (i.e., softmax regression) function as follows:\nαt,i = exp(fattn(xt,i, ht−1))∑L j=1 exp(fattn(xt,j , ht−1))\n(5)\nOur network also predicts inverse turning radius ût as an output with additional hidden layer fout(yt, ht) conditioned on the current hidden state ht and α weighted context yt.\nTo initialize memory state ct and hidden state ht of LSTM network, we follow Xu et al. [26] by averaging of the feature slices x0,i at initial time fed through two additional hidden layers: finit,c and finit,h.\nc0 = finit,c\n( 1\nL L∑ i=1 x0,i\n) , h0 = finit,h ( 1\nL L∑ i=1 x0,i ) (6)\nAs discussed by Xu et al. [26], doubly stochastic regularization can encourage the attention model to at different parts of the image. At time t, our attention model predicts a scalar β=sigm(fβ(ht−1)) with an additional hidden layer fβ conditioned on the previous hidden state ht−1 such that\nyt = sigm(fβ(ht−1))fflatten({αt,ixt,i}) (7)\nWe use the following penalized loss function L1:\nL1(ut, ût) = T∑ t=1 |ut − ût|+ λ L∑ i=1\n( 1−\nT∑ t=1 αt,i\n) (8)\nwhere T is the length of time steps, and λ is a penalty coefficient that encourages the attention model to see different parts of the image at each time frame. Section 4.3 describes the effect of using regularization."
    }, {
      "heading" : "3.4. Fine-Grained Decoder: Causality Test",
      "text" : "The last step of our pipeline is a fine-grained decoder, in which we refine a map of attention and detect local visual saliencies. Though an attention map from our coarsegrained decoder provides probability of importance over a 2D image space, our model needs to determine specific regions that cause a causal effect on prediction performance. To this end, we assess a decrease in performance when a local visual saliency on an input raw image is masked out.\nWe first collect a consecutive set of attention weights {αt,i} and input raw images {It} for a user-specified T timesteps. We then create a map of attention, which we referMt as defined:Mt = fmap({αt,i}). Our 5-layer convolutional neural network uses a stack of 5 × 5 and 3 × 3 filters without any pooling layer, and therefore the input image of size 80× 160 is processed to produce the output feature cube of size 10 × 20 × 64, while preserving its aspect ratio. Thus, we use fmap({αt,i}) as up-sampling function by the factor of eight followed by Gaussian filtering [5] as discussed in [26] (see Figure 2 (A,B)).\nTo extract a local visual saliency, we first randomly sample 2D N particles with replacement over an input raw image conditioned on the attention map Mt. Note that, we also use time-axis as the third dimension to consider temporal features of visual saliencies. We thus store spatiotemporal 3D particles P ← P ∪ {Pt, t} (see Figure 2 (C)).\nWe then apply a clustering algorithm to find a local visual saliency by grouping 3D particles P into clusters {C} (see Figure 2 (D)). In our experiment, we use DBSCAN [9], a density-based clustering algorithm that has advantages to deal with a noisy dataset because they group particles together that are closely packed, while marking particles as outliers that lie alone in low-density regions. For points of each cluster c and each time frame t, we compute a convex hull H(c) to find a local region of each visual saliency detected (see Figure 2 (E, F)).\nFor points of each cluster c and each time frame t, we iteratively measure a decrease of prediction performance with an input image which we mask out a local visual saliency. We compute a convex hull H(c) to find a local, and mask out each visual saliency by assigning zero values for all pixels lying inside each convex hull. Each causal visual saliency is generated by warping into a fixed spatial resolution (=64×64) as shown in Figure 2 (G, H)."
    }, {
      "heading" : "4. Result and Discussion",
      "text" : ""
    }, {
      "heading" : "4.1. Datasets",
      "text" : "As explained in Table 1, we obtain two large-scale datasets that contain over 1,200,000 frames (≈16 hours) collected from Comma.ai [8], Udacity [23], and Hyundai Center of Excellence in Integrated Vehicle Safety Systems and Control (HCE) under a research contract. These three datasets acquired contain video clips captured by a single front-view camera mounted behind the windshield of the vehicle. Alongside the video data, a set of time-stamped sensor measurement is contained, such as vehicle’s velocity, acceleration, steering angle, GPS location and gyroscope angles. Thus, these datasets are ideal for self-driving studies. Note that, for sensor logs unsynced with the timestamps of video data, we use the estimates of the interpolated measurements. Videos are mostly captured during highway driving in clear weather on daytime, and there included driving on other road types, such as residential roads (with and without lane markings), and contains the whole driver’s activities such as staying in a lane and switching lanes. Note also that, we exclude frames when the vehicle stops which happens when v̂t <1 m/s."
    }, {
      "heading" : "4.2. Training and Evaluation Details",
      "text" : "To obtain a convolutional feature cube xt, we train the 5- layer CNNs explained in Section 3.2 by using additional 5- layer fully connected layers (i.e., # hidden variables: 1164, 100, 50, and 10, respectively), of which output predicts the measured inverse turning radius ut. Incidentally, instead of using addition fully-connected layers, we could also obtain a convolutional feature cube xt by training from scratch with the whole network. In our experiment, we obtain the 10×20×64-dimensional convolutional feature cube, which is then flattened to 200×64 and is fed through the coarsegrained decoder. Other recent types of more recent expres-\nsive networks may give a performance boost over our CNN configuration. However, exploration of other convolutional architectures would be out of our scope.\nWe experiment with various numbers of LSTM layers (1 to 5) of the soft deterministic visual attention model but did not observe any significant improvements in model performance. Unless otherwise stated, we use a single LSTM layer in this experiment. For training, we use Adam optimization algorithm [15] and also use dropout [22] of 0.5 at hidden state connections and Xavier initialization [10]. We randomly sample a mini-batch of size 128, each of batch contains a set Consecutive frames of length T = 20. Our model took less than 24 hours to train on a single NVIDIA Titan X Pascal GPU. Our implementation is based on Tensorflow [1] and code will be publicly available upon publication.\nTwo datasets (Comma.ai [8] and HCE) we used were available with images captured by a single front-view cam-\nera. This makes it hard to use the data augmentation technique proposed by Bojarski et al. [3], which generated images with artificial shifts and rotations by using two additional off-center images (left-view and right-view) captured by the same vehicle. Data augmentation may give a performance boost, but we report performance without data augmentation."
    }, {
      "heading" : "4.3. Effect of Choosing Penalty Coefficient λ",
      "text" : "Our model provides a better way to understand the rationale of the models decision by visualizing where and what the model sees to control a vehicle. Figure 3 shows a consecutive input raw images (with sampling period of 5 seconds) and their corresponding attention maps (i.e., Mt = fmap({αt,i})). We also experiment with three different penalty coefficients λ ∈ {0, 10, 20}, where the model is encouraged to pay attention to wider parts of the image (see differences between the bottom 3 rows in Figure 3 ) as we have larger λ. For better visualization, an attention map is overlaid by an input raw image and color-coded; for example, red parts represent where the model pays attention. For quantitative analysis, prediction performance in terms of mean absolute error (MAE) is explained on the bottom of each figure. We observe that our model is indeed able to pay attention on road elements, such as lane markings, guardrails, and vehicles ahead, which is essential for human to drive."
    }, {
      "heading" : "4.4. Effect of Varying Smoothing Factors",
      "text" : "Recall from Section 3.1 that the single exponential smoothing method [13] is used to reduce the effect of human factors variation and the effect of measurement noise for two sensor inputs: steering angle and velocity. A robust\nmodel for autonomous vehicles would yield consistent performance, even when some measurements are noisy. Figure 4 shows the prediction performance in terms of mean absolute error (MAE) on a comma.ai testing data set. Various smoothing factors αs ∈ {0.01, 0.05, 0.1, 0.3, 0.5, 1.0} are used to assess the effect of using smoothing methods. With setting αs=0.05, our model for the task of steering estimation performs the best. Unless otherwise stated, we will use αs as 0.05."
    }, {
      "heading" : "4.5. Quantitative Analysis",
      "text" : "In Table 2, we compare the prediction performance with alternatives in terms of MAE. We implement alternatives that include the work by Bojarski et al. [3], which used an identical base CNN and a fully-connected network (FCN) without attention. To see the contribution of LSTMs, we also test a CNN and LSTM, which is identical to ours but does not use the attention mechanism. For our model, we\ntest with three different values of penalty coefficients λ ∈ {0, 10, 20}.\nOur model shows competitive prediction performance than alternatives. Our model shows 1.18–4.15 in terms of MAE on testing dataset. This confirms that incorporation of attention does not degrade control accuracy. The average run-time for our model and alternatives took less than a day to train each dataset."
    }, {
      "heading" : "4.6. Effect of Causal Visual Saliencies",
      "text" : "Recall from Section 3.4, we post-process the attention networks output by clustering it into attention blobs and filtering if they have an causal effect on network output. Figure 5 (A) shows typical examples of an input raw image, an attention networkss output with spurious attention sources, and our refined attention heat map. We observe our model can produce a simpler and more accurate map of visual saliency by filtering out spurious attention blobs. In our experiment, 62% and 58% out of all attention blobs are indeed spurious attention sources on Comma.ai [8] and\nHCE datasets (see Figure 5 (B))."
    }, {
      "heading" : "5. Conclusion",
      "text" : "We described an interpretable visualization for deep selfdriving vehicle controllers. It uses a visual attention model augmented with an additional layer of causal filtering. We tested with three large-scale real driving datasets that contain over 16 hours of video frames. We showed that (i) incorporation of attention does not degrade control accuracy compared to an identical base CNN without attention (ii) raw attention highlights interpretable features in the image and (iii) causal filtering achieves a useful reduction in explanation complexity by removing features which do not significantly affect the output."
    } ],
    "references" : [ {
      "title" : "Tensorflow: Large-scale machine learning on heterogeneous 7  Dataset  Model  MAE in degree [SD] Training Testing Comma.ai",
      "author" : [ "M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin" ],
      "venue" : "HCE",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2015
    }, {
      "title" : "Visualbackprop: visualizing cnns for autonomous driving",
      "author" : [ "M. Bojarski", "A. Choromanska", "K. Choromanski", "B. Firner", "L. Jackel", "U. Muller", "K. Zieba" ],
      "venue" : "arXiv preprint arXiv:1611.05418,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "End to end learning for self-driving cars",
      "author" : [ "M. Bojarski", "D. Del Testa", "D. Dworakowski", "B. Firner", "B. Flepp", "P. Goyal", "L.D. Jackel", "M. Monfort", "U. Muller", "J. Zhang" ],
      "venue" : "arXiv preprint arXiv:1604.07316,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2016
    }, {
      "title" : "The DARPA urban challenge: autonomous vehicles in city traffic, volume 56",
      "author" : [ "M. Buehler", "K. Iagnemma", "S. Singh" ],
      "venue" : "springer,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "The laplacian pyramid as a compact image code",
      "author" : [ "P. Burt", "E. Adelson" ],
      "venue" : "IEEE Transactions on communications, 31(4):532–540,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1983
    }, {
      "title" : "Deepdriving: Learning affordance for direct perception in autonomous driving",
      "author" : [ "C. Chen", "A. Seff", "A. Kornhauser", "J. Xiao" ],
      "venue" : "Proceedings of the IEEE International Conference on Computer Vision, pages 2722–2730,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Brain inspired cognitive model with attention for self-driving cars",
      "author" : [ "S. Chen", "S. Zhang", "J. Shang", "B. Chen", "N. Zheng" ],
      "venue" : "arXiv preprint arXiv:1702.05596,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "A densitybased algorithm for discovering clusters in large spatial databases with noise",
      "author" : [ "M. Ester", "H.-P. Kriegel", "J. Sander", "X. Xu" ],
      "venue" : "In Kdd,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1996
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "X. Glorot", "Y. Bengio" ],
      "venue" : "Aistats, volume 9, pages 249–256,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Generating visual explanations",
      "author" : [ "L.A. Hendricks", "Z. Akata", "M. Rohrbach", "J. Donahue", "B. Schiele", "T. Darrell" ],
      "venue" : "European Conference on Computer Vision, pages 3–19. Springer,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Long short-term memory",
      "author" : [ "S. Hochreiter", "J. Schmidhuber" ],
      "venue" : "Neural computation, 9(8):1735–1780,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Forecasting with exponential smoothing: the state space approach",
      "author" : [ "R. Hyndman", "A.B. Koehler", "J.K. Ord", "R.D. Snyder" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Densecap: Fully convolutional localization networks for dense captioning",
      "author" : [ "J. Johnson", "A. Karpathy", "L. Fei-Fei" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4565–4574,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "D. Kingma", "J. Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Deep learning",
      "author" : [ "Y. LeCun", "Y. Bengio", "G. Hinton" ],
      "venue" : "Nature, 521(7553):436–444,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations",
      "author" : [ "H. Lee", "R. Grosse", "R. Ranganath", "A.Y. Ng" ],
      "venue" : "Proceedings of the 26th annual international conference on machine learning, pages 609–616. ACM,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Towards fully autonomous driving: Systems and algorithms",
      "author" : [ "J. Levinson", "J. Askeland", "J. Becker", "J. Dolson", "D. Held", "S. Kammel", "J.Z. Kolter", "D. Langer", "O. Pink", "V. Pratt" ],
      "venue" : "In Intelligent Vehicles Symposium (IV),",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2011
    }, {
      "title" : "Alvinn, an autonomous land vehicle in a neural network",
      "author" : [ "D.A. Pomerleau" ],
      "venue" : "Technical report, Carnegie Mellon University, Computer Science Department,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Vehicle dynamics and control",
      "author" : [ "R. Rajamani" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Action recognition using visual attention",
      "author" : [ "S. Sharma", "R. Kiros", "R. Salakhutdinov" ],
      "venue" : "arXiv preprint arXiv:1511.04119,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Dropout: a simple way to prevent neural networks from overfitting",
      "author" : [ "N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov" ],
      "venue" : "Journal of Machine Learning Research, 15(1):1929–1958,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Autonomous driving in urban environments: Boss and the urban challenge",
      "author" : [ "C. Urmson", "J. Anhalt", "D. Bagnell", "C. Baker", "R. Bittner", "M. Clark", "J. Dolan", "D. Duggins", "T. Galatali", "C. Geyer" ],
      "venue" : "Journal of Field Robotics,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2008
    }, {
      "title" : "End-to-end learning of driving models from large-scale video datasets",
      "author" : [ "H. Xu", "Y. Gao", "F. Yu", "T. Darrell" ],
      "venue" : "arXiv preprint arXiv:1612.01079,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Show, attend and tell: Neural image caption generation with visual attention",
      "author" : [ "K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A.C. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio" ],
      "venue" : "ICML, volume 14, pages 77–81,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Graying the black box: Understanding dqns",
      "author" : [ "T. Zahavy", "N.B. Zrihem", "S. Mannor" ],
      "venue" : "arXiv preprint arXiv:1602.02658,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Visualizing and understanding convolutional networks",
      "author" : [ "M.D. Zeiler", "R. Fergus" ],
      "venue" : "European Conference on Computer Vision, pages 818–833. Springer,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "These controllers use a variety of approaches but recent successes [3] suggests that neural networks will be widely used in self-driving vehicles.",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 23,
      "context" : "An exception to the rule is visual attention networks [26, 21, 7].",
      "startOffset" : 54,
      "endOffset" : 65
    }, {
      "referenceID" : 19,
      "context" : "An exception to the rule is visual attention networks [26, 21, 7].",
      "startOffset" : 54,
      "endOffset" : 65
    }, {
      "referenceID" : 6,
      "context" : "An exception to the rule is visual attention networks [26, 21, 7].",
      "startOffset" : 54,
      "endOffset" : 65
    }, {
      "referenceID" : 21,
      "context" : "[24], Buehler et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 3,
      "context" : "[4], and Levinson et al.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 16,
      "context" : "[18].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "ALVINN (Autonomous Land Vehicle In a Neural Network) [19] was the first attempt to use neural network for directly mapping images to navigate the direction of the vehicle.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 2,
      "context" : "[3] demonstrated good performance with convolutional neural networks (CNNs) to directly map images from a front-view camera to steering controls.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 22,
      "context" : "[25] proposed an end-to-end egomotion prediction approach that takes raw pixels and prior vehicle state signals as inputs and predicts several a sequence of discretized actions (i.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 5,
      "context" : "[6] who defined human-interpretable intermediate features such as the curvature of lane, distances to neighboring lanes, and distances from the front-located vehicles.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 25,
      "context" : "In a landmark work, Zeiler and Fergus [28] used ”deconvolution” to visualize layer activations of convolutional networks.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 14,
      "context" : "[16] provides textual explanations of images as automatically-generated captions.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 1,
      "context" : "[2] developed a richer notion of ”contribution” of a pixel to the output.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "[11] trains a deep network to generate species specific explanation without explicitly identifying semantic features.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[14] proposes DenseCap which uses fully convolutional localization networks for dense captioning, their paper achieves both localizing objects and describing salient regions in images using natural langauge.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "[27] proposes a visualization method to interpret the agents action by describing Markov Decision Process model as a directed graph on a t-SNE map.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 2,
      "context" : "[3], our model predicts the inverse turning radius ût (= r−1 t , where rt is the turning radius) at every timestep t instead of steering angle commands, which depends on the vehicle’s steering geometry and also result in numerical instability when predicting near zero steering angle commands.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 18,
      "context" : "The relationship between the inverse turning radius ut and the steering angle command θt can be approximated by Ackermann steering geometry [20] as follows:",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 0,
      "context" : "We also normalized pixel values to [0, 1] in HSV colorspace.",
      "startOffset" : 35,
      "endOffset" : 41
    }, {
      "referenceID" : 11,
      "context" : "We utilize a single exponential smoothing method [13] to reduce the effect of human factors-related performance variation and the effect of measurement noise.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 2,
      "context" : "[3] to learn a model for self-driving cars.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 15,
      "context" : "[17], we omit max-pooling layers to prevent spatial locational information loss as the strongest activation propagates through the model.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "model feeds α weighted context yt to the system as discuss by several works [21, 26]:",
      "startOffset" : 76,
      "endOffset" : 84
    }, {
      "referenceID" : 23,
      "context" : "model feeds α weighted context yt to the system as discuss by several works [21, 26]:",
      "startOffset" : 76,
      "endOffset" : 84
    }, {
      "referenceID" : 19,
      "context" : "Note that, our attention mechanism π({αt,i}, {xt,i}) is different from the previous works [21, 26], which use the α weighted average context yt = ∑L i=1 αt,ixt,i.",
      "startOffset" : 90,
      "endOffset" : 98
    }, {
      "referenceID" : 23,
      "context" : "Note that, our attention mechanism π({αt,i}, {xt,i}) is different from the previous works [21, 26], which use the α weighted average context yt = ∑L i=1 αt,ixt,i.",
      "startOffset" : 90,
      "endOffset" : 98
    }, {
      "referenceID" : 10,
      "context" : "As we summarize in Figure 1, we use a long shortterm memory (LSTM) network [12] that predicts the inverse turning radius ût and generates attention weights {αt,i} at each timestep t conditioned on the previous hidden state ht and a current convolutional feature cube xt.",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 23,
      "context" : "[26] by averaging of the feature slices x0,i at initial time fed through two additional hidden layers: finit,c and finit,h.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "[26], doubly stochastic regularization can encourage the attention model to at different parts of the image.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 4,
      "context" : "Thus, we use fmap({αt,i}) as up-sampling function by the factor of eight followed by Gaussian filtering [5] as discussed in [26] (see Figure 2 (A,B)).",
      "startOffset" : 104,
      "endOffset" : 107
    }, {
      "referenceID" : 23,
      "context" : "Thus, we use fmap({αt,i}) as up-sampling function by the factor of eight followed by Gaussian filtering [5] as discussed in [26] (see Figure 2 (A,B)).",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 7,
      "context" : "In our experiment, we use DBSCAN [9], a density-based clustering algorithm that has advantages to deal with a noisy dataset because they group particles together that are closely packed, while marking particles as outliers that lie alone in low-density regions.",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 7,
      "context" : "(C) We randomly sample 3D N = 500 particles over the attention map, and (D) we apply a density-based clustering algorithm (DBSCAN [9]) to find a local visual saliency by grouping particles into clusters.",
      "startOffset" : 130,
      "endOffset" : 133
    }, {
      "referenceID" : 13,
      "context" : "For training, we use Adam optimization algorithm [15] and also use dropout [22] of 0.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 20,
      "context" : "For training, we use Adam optimization algorithm [15] and also use dropout [22] of 0.",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 8,
      "context" : "5 at hidden state connections and Xavier initialization [10].",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 0,
      "context" : "Our implementation is based on Tensorflow [1] and code will be publicly available upon publication.",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 2,
      "context" : "[3], which generated images with artificial shifts and rotations by using two additional off-center images (left-view and right-view) captured by the same vehicle.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 11,
      "context" : "1 that the single exponential smoothing method [13] is used to reduce the effect of human factors variation and the effect of measurement noise for two sensor inputs: steering angle and velocity.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 2,
      "context" : "[3], which used an identical base CNN and a fully-connected network (FCN) without attention.",
      "startOffset" : 0,
      "endOffset" : 3
    } ],
    "year" : 2017,
    "abstractText" : "Deep neural perception and control networks are likely to be a key component of self-driving vehicles. These models need to be explainable they should provide easy-tointerpret rationales for their behavior so that passengers, insurance companies, law enforcement, developers etc., can understand what triggered a particular behavior. Here we explore the use of visual explanations. These explanations take the form of real-time highlighted regions of an image that causally influence the network’s output (steering control). Our approach is two-stage. In the first stage, we use a visual attention model to train a convolution network endto-end from images to steering angle. The attention model highlights image regions that potentially influence the network’s output. Some of these are true influences, but some are spurious. We then apply a causal filtering step to determine which input regions actually influence the output. This produces more succinct visual explanations and more accurately exposes the network’s behavior. We demonstrate the effectiveness of our model on three datasets totaling 16 hours of driving. We first show that training with attention does not degrade the performance of the end-to-end network. Then we show that the network causally cues on a variety of features that are used by humans while driving.",
    "creator" : "LaTeX with hyperref package"
  }
}