{
  "name" : "1602.07320.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Zachary C. Lipton" ],
    "emails" : [ "zlipton@cs.ucsd.edu" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "In the worst case, solving for the optimal weights in a neural network is an NP-Hard problem. Further, the error surfaces of neural networks are highly non-convex, presenting seemingly formidable obstacles to learning by gradient descent. And yet practitioners train deep neural networks everyday by stochastic gradient descent, achieving state of the art results on a broad range of tasks. In fact, for many problems, this method easily achieves zero loss on the training set. Thus, while optimization presents a tremendous problem in theory, one might argue that in practice, regularization is the greater concern.\nThis disparity, between the apparent hopelessness of the optimization problem and the de facto ease of training has spurred several researchers to attempt both theoretically and empirically to characterize the error surfaces of deep neural networks. Notably Goodfellow et al. (2014), plotted loss along straight lines through weight space, between two converged models, showing monotonic increases and then decreases in loss. One might ask: Once symmetry is broken, is the problem convex? Of course, the gradient at any point along this line doesn’t necessarily point directly towards the nearest minimum. Dauphin et al. (2014) presented a case based on both empirical study and results from statistical physics suggesting that the ratio of saddle points to local minima on a neural network’s loss surface grows exponentially in the number of parameters. Janzamin et al. (2015) presented a theoretical study, showing that under reasonable conditions on the data, the optimization problem can be made and solved via tensor decomposition."
    }, {
      "heading" : "1.1 CONTRIBUTIONS",
      "text" : "In this paper, we conduct preliminary experiments, training a standard three layer convolutional neural network with 819557 parameters on the MNIST dataset (LeCun et al., 1998), using dropout\n∗http://zacklipton.com\nar X\niv :1\n60 2.\n07 32\n0v 1\n[ cs\n.L G\n] 2\n3 Fe\nb 20\n16\nregularization and `22 weight decay. We analyze the paths through weight space taken over the course of gradient descent, presenting the following findings:\n• Weights do not converge to critical points, instead traveling large (euclidean) distances through flat basins in weight space.\n• While a straight line in weight-space from initialization to solution may correspond to monotonically decreasing loss, the path actually taken by gradient descent seems far from straight.\n• A small number of principal components explains most of the variance along a training trajectory.\n• Even once symmetry is broken, neural network error surfaces are neither convex nor quasiconvex but continue to diverge towards many different low error basins. Starting from the same initialization, but then feeding each network examples in shuffled order is sufficient to diverge each network along a different path. This suggests that the error surface is not only globally non-convex, but also locally non-convex even for a partially trained net.\n• All pairs of solutions after a fixed number of epochs appear to be roughly the same euclidean distance from the origin and from each other. This is true even with identical initializations, and pretraining before cloning."
    }, {
      "heading" : "2 EXPERIMENTS",
      "text" : "Rather than plotting straight lines through weight space like Goodfellow et al. (2014), we investigate the paths through weight space taken as models are trained. We analyze these trajectories qualitatively by visualizing them via 2D PCA, and quantitatively by analyzing the variance explained by the largest principal components.\nWe train a single model for 200 epochs, capturing its full parameters after each epoch. We plot this trajectory with a 2D PCA showing the high degree of non-linearity in the learned path. We then train 5 models for 200 epochs each, starting starting each model from different random initializations, and capturing their parameters every ten epochs.\nNext, we repeat this same experiment but with the same initialization. We train each of the 5 networks on a different shuffle of the data from the same starting point in weight space.\nFinally, we train one model for 10 epochs. Then we clone it 5 times. Each clone is trained from these partially learned starting weights but with a different random shuffle."
    }, {
      "heading" : "3 RESULTS",
      "text" : "To visually demonstrate that the paths taken though weight space are highly nonlinear, we plot a 2D PCA of a single 200 epoch trajectory (Figure 1). The first two principal components explain 81.39% of the variance. The top 10 principal components explain 95.63% of the variance. Speculatively, it\nseems that the low dimension of the trajectories, together with the smoothness of the curves might be useful properties for projecting where to look next.\nWhen we train models from the same initialization, seen in Figure 2, they nevertheless diverge, finding solutions far apart as measured by euclidean distance. Interestingly, all pairs of solutions were equally far apart from each other and equally far from the origin, suggesting strong symmetry in weight space. These observations hold even when we first pretrain the network for 10 epochs (achieving training set error around 1%) before cloning and shuffling (Figure 3)."
    }, {
      "heading" : "4 CONCLUSION",
      "text" : "In these experiments, we present several novel observations about the error surfaces of neural networks. We showed that paths through weight space are highly nonlinear, and that local minima (albeit good ones) are abundant. Further, we showed that even after symmetry is broken by random initialization, the error surfaces of neural networks appears to be highly non-convex. The stochasticity introduced by reshuffling data appears to be enough to diverge otherwise identical (and partially trained) networks towards different local minima. Further, we showed that regions of weight space with near 0 loss are not critical points, but large flat basins through which weights continue to travel as training continues.\nWhile these results are interesting, this work should be expanded in future iterations to verify that these observations hold on larger datasets and other common neural network topologies, including multilayer perceptrons and LSTM recurrent neural networks. Further, this work should be evaluated on datasets where it is impossible for a net trained by gradient descent to achieve arbitrarily low loss. It’s possible that on such datasets, when the net is not so badly over-complete the weights truly do arrive at some critical point."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "Zachary C. Lipton is supported by the Division of Biomedical Informatics at the University of California, San Diego, via training grant (T15LM011271) from the NIH/NLM. Thanks to NVIDIA Corporation for their generous hardware donations. Thanks to John Berkowitz, Anima Anandkumar, Charles Elkan, and Julian McAuley."
    } ],
    "references" : [ {
      "title" : "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization",
      "author" : [ "Yann N Dauphin", "Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Surya Ganguli", "Yoshua Bengio" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Dauphin et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Dauphin et al\\.",
      "year" : 2014
    }, {
      "title" : "Qualitatively characterizing neural network optimization problems",
      "author" : [ "Ian J Goodfellow", "Oriol Vinyals", "Andrew M Saxe" ],
      "venue" : "arXiv preprint arXiv:1412.6544,",
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods",
      "author" : [ "Majid Janzamin", "Hanie Sedghi", "Anima Anandkumar" ],
      "venue" : "CoRR abs/1506.08473,",
      "citeRegEx" : "Janzamin et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Janzamin et al\\.",
      "year" : 2015
    }, {
      "title" : "The mnist database of handwritten digits",
      "author" : [ "Yann LeCun", "Corinna Cortes", "Christopher JC Burges" ],
      "venue" : null,
      "citeRegEx" : "LeCun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Notably Goodfellow et al. (2014), plotted loss along straight lines through weight space, between two converged models, showing monotonic increases and then decreases in loss.",
      "startOffset" : 8,
      "endOffset" : 33
    }, {
      "referenceID" : 0,
      "context" : "Dauphin et al. (2014) presented a case based on both empirical study and results from statistical physics suggesting that the ratio of saddle points to local minima on a neural network’s loss surface grows exponentially in the number of parameters.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : "Dauphin et al. (2014) presented a case based on both empirical study and results from statistical physics suggesting that the ratio of saddle points to local minima on a neural network’s loss surface grows exponentially in the number of parameters. Janzamin et al. (2015) presented a theoretical study, showing that under reasonable conditions on the data, the optimization problem can be made and solved via tensor decomposition.",
      "startOffset" : 0,
      "endOffset" : 272
    }, {
      "referenceID" : 3,
      "context" : "In this paper, we conduct preliminary experiments, training a standard three layer convolutional neural network with 819557 parameters on the MNIST dataset (LeCun et al., 1998), using dropout ∗http://zacklipton.",
      "startOffset" : 156,
      "endOffset" : 176
    }, {
      "referenceID" : 1,
      "context" : "Rather than plotting straight lines through weight space like Goodfellow et al. (2014), we investigate the paths through weight space taken as models are trained.",
      "startOffset" : 62,
      "endOffset" : 87
    } ],
    "year" : 2016,
    "abstractText" : "Deep learning researchers commonly suggest that converged models are stuck in local minima. More recently, some researchers observed that under reasonable assumptions, the vast majority of critical points are saddle points, not true minima. Both descriptions suggest that weights converge around a point in weight space, be it a local optima or merely a critical point. However, it’s possible that neither interpretation is accurate. As neural networks are typically over-complete, it’s easy to show the existence of vast continuous regions through weight space with equal loss. In this paper, we build on recent work empirically characterizing the error surfaces of neural networks. We analyze training paths through weight space, presenting evidence that apparent convergence of loss does not correspond to weights arriving at critical points, but instead to large movements through flat regions of weight space. While it’s trivial to show that neural network error surfaces are globally non-convex, we show that error surfaces are also locally nonconvex, even after breaking symmetry with a random initialization and also after partial training.",
    "creator" : "LaTeX with hyperref package"
  }
}