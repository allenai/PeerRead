{"conference": "ICLR 2017 conference submission", "title": "Nonparametric Neural Networks", "abstract": "Automatically determining the optimal size of a neural network for a given task without prior information currently requires an expensive global search and training many networks from scratch. In this paper, we address the problem of automatically finding a good network size during a single training cycle. We introduce {\\it nonparametric neural networks}, a non-probabilistic framework for conducting optimization over all possible network sizes and prove its soundness when network growth is limited via an $\\ell_p$ penalty. We train networks under this framework by continuously adding new units while eliminating redundant units via an $\\ell_2$ penalty. We employ a novel optimization algorithm, which we term ``Adaptive Radial-Angular Gradient Descent'' or {\\it AdaRad}, and obtain promising results.", "reviews": [{"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "The paper presents a clean framework for optimizing for the network size during the training cycle. While the complexity of each iteration is increased, they argue that overall, the cost is significantly reduced since we do not need to train networks of varying sizes and cross-validate across them. The reviewers recommend acceptance of the paper and I am in agreement with them.", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"TITLE": "New paper version with large dataset", "OTHER_KEYS": "George Philipp", "comments": "I just added a new version of the paper with experiments on a large dataset (> 1m datapoints, ", "IS_META_REVIEW": false, "DATE": "26 Jan 2017", "is_meta_review": false}, {"TITLE": "New version and further comments", "OTHER_KEYS": "George Philipp", "comments": "Dear all,\n\nBelow are some more responses to two points raised. Both responses are reflected in a new version of the paper I just uploaded.\n\nThe following has changed in the paper:\n\n - section 3.1 (self-similar nonlinearities) is new\n - the final paragraph of section 3 is new\n - section 7.2 (proof of new proposition) is new\n\nThe paper is now longer than 8 pages. Having looked at many other submitted papers, it appears that the 8 page requirement is not very serious. If the area chair would prefer an 8 page paper, I can move some more stuff to the appendix.\n\n** Reducing the number of hyperparameters **\n\nTwo questions that came up throughout the reviews is whether our method reduces the number of hyperparameters and whether there is an automatic way to set lambda. \n\nOne advantage of nonparametric networks is that instead of having one hyperparameter per layer (size) there is one hyperparameter for the entire network (lambda) that controls size. It is worth pointing out that this reduction in complexity is not arbitrary. In fact, one can prove that in a ReLU network, one regularization parameter lambda captures all the complexity of having one regularization parameter per layer because we could replace all of these regularization parameters with their geometric mean without changing the objective. (See the newly included-in-the-paper proposition 1.) Hence, nonparametric networks apportion regularization to each layer automatically.\n\nWhile we don't (yet) have a great way of efficiently picking the single remaining lambda, this reduction in complexity certainly contributes to reducing hyperparameter complexity.\n\n** Computational cost of AdaRad (in response to reviewer 1) **\n\n(From the paper:) Using AdaRad over SGD incurs additional computational cost. However, that cost scales more gracefully than the cost of, for example, RMSprop. AdaRad normalizes each fan-in instead of each individual weight, so many of its operations scale only with the number of units and not with the number of weights in the network. In Table \\ref{costTable}, we compare the costs of SGD, AdaRad and RMSprop. Further, RMSprop has a larger memory footprint than AdaRad. It requires a cache of size equal to the number of weights, whereas AdaRad only requires 2 caches of size equal to the number of neurons.\n\nCosts (per minibatch and weight)\n\nSGD, no $\\ell_2$ shrinkage: 1 multiplication\nSGD with $\\ell_2$ shrinkage: 3 multiplications\nAdaRad, no $\\ell_2$ shrinkage: 4 multiplications\nAdaRad with $\\ell_2$ shrinkage: 4 multiplications\nRMSprop, no $\\ell_2$ shrinkage: 4 multiplications, 1 division, 1 square root\nRMSprop with $\\ell_2$ shrinkage: 6 multiplications, 1 division, 1 square root\n\nBest,\nGeorge\n\n", "IS_META_REVIEW": false, "DATE": "13 Jan 2017", "is_meta_review": false}, {"TITLE": "To Reviewer 2", "OTHER_KEYS": "George Philipp", "comments": "Dear Reviewer 2,\n\nThank you again for your review. Did you have a chance to look at my response? It would be great to hear your thoughts on it.\n\nBest,\nGeorge", "IS_META_REVIEW": false, "DATE": "04 Jan 2017", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "No Title", "is_meta_review": false, "comments": "This paper proposes a nonparametric neural network model, which automatically learns the size of the model during the training process. The key idea is to randomly add zero units and use sparse regularizer to automatically null out the weights that are irrelevant. The idea sounds to be a random search approach over discrete space with the help of sparse regularization to eliminate useless units. This is an important problem and the paper gives interesting results. My main comments are listed below:\n\nWhat is the additional computation complexity of the algorithm? The decomposition of each fan-in weights into a parallel component and an orthogonal component and the transformation into radial-angular coordinates may require a lot of extra computation time. The authors may need to discuss the extra amount of operations relative to the parametric neural network. Furthermore, it would be useful to show some running time experiments.\n\nIt is observed that nonparametric networks return small networks on the convex dataset so that it is inferior to parametric networks. Any insight on this?", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "27 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Useful idea, limited experiments and discussion of theoretical result", "is_meta_review": false, "comments": "I agree with reviewer 2 on the interesting part of the paper. The idea of removing or adding units is definitely an interesting direction, that will make a model grow or shrink along the lines required by the problem and the data, not the user prior knowledge.\n\nThe authors offer an interesting theoretical result that proves that under fan out or fan in regularization the optimum of the error function is achieved for finite number of parameters - so the net does not grow indefinitely, until it over-fits perfectly the data.\nThat reminds me of more traditional approaches such as Lasso or Elastic Net, in which the regularization produces sparse weights. I would  have like more intuition to be given for this theorem. It is a nice result, somewhat expected (at last for me it is intuitive) and I would have liked such intuition to be given some space in the paper. For example, less discussion of prior work (that is nice too, but not as important as discussing and studying the main result of the paper) could make more room for addressing the theoretical results. Please also see below (point 2) for some suggestions.\n\nI have a few other comments to make:\n\n1. An interesting experiment would be to show that a model such as yours, where the nodes (neurons) are added or removed automatically can outperform a net with the same number of nodes (at the end, after complete learning), in which the size and number of nodes per layer are fixed from the start. This would prove the efficiency of the idea. This is where your method is interesting: do you save nodes that are not needed and replace them with nodes that are needed? Do you optimize performance vs. memory?\n\nI understand that experiments along this line are given in Figure 2, with mixed results. The Figure i must say, is not very clear, but it is possible to interpret under careful inspection.\n\nIn some the non-parametric nets are doing better and others are doing worse than the parametric ones. Even in such case i could see the usefulness of the method as it helps discovering the structure. \n\nWhat i don't fully understand is why they can do better sometimes than the end net which could be trained from scratch: why is the nonparametric version of learning better than the parametric version, when the final net is known in advance? Could you give more insight?\n\n2. Can you better discuss the meaning and implications of Theorem 1. I feel this theorem is just put there with no proper discussion. Beyond the proofs, from the Appendix, what is the key insight of the Theorem? What does it say, in plain English? To me, the conclusion seems almost natural and obvious. Is there some powerful insight? \n\nAs i have mentioned previously, i feel this theoretical result deserves more space, with even more experiments to back it up. For example, can regularizer parameter lambda  be predicted given the data - is there a property in the data that can help guessing the right lambda? My feeling is that lambda is the key factor for determining the final net structure. Is this true? \n\nHow much does the structure of the final net depend on the initialization? Do you get different nets if you start from different random weights? How different are they?\n\nWhat happens when fan in and fan out regularizers are combined? Do you still have the same theoretical result?\n\nI have a few additional questions:\n\n1. Why do you say that adding zero units changes the regularizer value? For example, does L2 norm change if you add zero values?\n\n2. Zero units are defined as having either the fan in or the fan out weights being zero. I think that what you meant is that both fan in and fan out weights are zero, otherwise you cannot remove the unit and keep the same output f. This should be clarified better I think.\n\nI changed my rating to 7, while hoping that the authors will address my comments above.\n\n\n \n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "21 Dec 2016 (modified: 25 Dec 2016)", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Interesting paper with some limitations on demonstrated utility", "is_meta_review": false, "comments": "This paper addresses the problem of allowing networks to change the number of units that are used during training.  This is done in a simple but elegant and well-motivated way.  Units with zero input or output weights are added or removed during training, while a group sparsity norm for regularization is used to encourage unit weights to go to zero.  The main theoretical contribution is to show that with proper regularization, the loss is minimized by a network with a finite number of units.  In practice, this result does not guarantee that the resulting network will not over- or under-fit the training data, but some initial experiments show that this does not seem to be the case.\n\nOne potential advantage of approaches that learn the number of units to use in a network is to ease the burden of tuning hyperparameters.   One disadvantage of this approach (and maybe any such approach) is that it does not really solve this problem.  The network still has several hyperparameters that implicitly control the number of units that will emerge, including parameters that control how often new units are added and how rapidly weights may decay to zero.  It is not clear whether these hyperparameters will be easier or harder to tune than the ones in standard approaches.  In fairness, the authors do not claim that they have made training easier, but it is a little disappointing that this does not seem to be the case.\n\nThe authors do emphasize that they are able to train networks that use fewer units to achieve comparable performance to networks trained parametrically.  This is potentially important, because smaller networks can reduce run-time at testing, and power consumption and memory footprint, which is important on mobile devices in particular.  However, the authors do not compare experimentally to existing approaches that attempt to reduce the size of parametrically trained networks (eg., by pruning trained networks) so it is not clear whether this approach is really competitive with the best current approaches to reducing the size of trained networks.\n\nAnother potential disadvantage of the proposed approach is that the same hyperparameters control both the number of units that will appear in the network and the training time.  Therefore, training might potentially be much slower for this approach than for a parametric approach with fixed hyperparameters. In practice, many parametric approaches require methods like grid search to choose hyperparameters, which can be very slow, but in many other cases experience with similar problems can make the choice of hyperparameters relatively easy.  This means that the cost of grid search is not always paid, but the slowness of the authors\u2019 approach may be endemic.  The authors do not discuss how this issue will scale as much larger networks are trained.  It is a concern that this approach may not be practical for large-scale networks, because training will be very slow.\n\nIn general, the experiments are helpful and encouraging, but not comprehensive or totally convincing.  I would want to see experiments on much larger problems before I was convinced that this approach can really be practical or widely useful.  \n\nOverall, I found this to be an interesting and clearly written paper that makes a potentially useful point.  The overall vision of building networks that can grow and adapt through life-long learning is inspiring, and this type of work might be needed to realize such a vision.  But the current results remain pretty speculative.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Uploaded new revision", "OTHER_KEYS": "George Philipp", "comments": "Dear Reviewers,\n\nIn response to the first question posed to me, I revised the paper to include a discussion of pruning, and added several references. I would like to thank reviewer 2 for his / her feedback. I was not fully aware of how prominent this topic had become in the deep learning community recently as well as the increased use of l2 and l1 regularization.\n\nThe main changes are:\n\n - abstract\n - introduction\n - section 3 before 3.1\n - further background (first half)\n - conclusion\n\nPlus, I improved the grammar / wording in a few places.\n\nI look forward to further comments.\n\nBest,\nGeorge", "IS_META_REVIEW": false, "DATE": "01 Dec 2016 (modified: 02 Dec 2016)", "is_meta_review": false}, {"TITLE": "Removing units", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "IS_META_REVIEW": false, "DATE": "29 Nov 2016", "is_meta_review": false}], "authors": "George Philipp, Jaime G. Carbonell", "KEYWORDS": "We automatically set the size of an MLP by adding and removing units during training as appropriate.", "accepted": true, "id": ""}
