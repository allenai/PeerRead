{
  "name" : "1706.03824.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Attention-based Vocabulary Selection for NMT Decoding",
    "authors" : [ "Baskaran Sankaran" ],
    "emails" : [ "bsankara@us.ibm.com", "freitagm@us.ibm.com", "onaizan@us.ibm.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Due to the fact that Neural Machine Translation (NMT) is reaching comparable or even better performance compared to the statistical machine translation (SMT) models (Jean et al., 2015; Luong et al., 2015), it has become very popular in the recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014). With the recent success of NMT, attention has shifted towards making it more practical. Compared to the traditional phrase-based machine translation engines, NMT decoding tends to be significantly slower. One of the most expensive parts in NMT is the softmax calculation over the\nfull target vocabulary. Recent work show that we can restrict the softmax to a subset of likely candidates given the source. The candidates are based on a dictionary built from Viterbi word alignments, or by matching phrases in a phrase-table, or by using the most frequent words in the target language. In this work, we present a novel approach which extracts the candidates during training based on the attention weights within the network. One advantage is that we do not need to determine the candidates with an external tool and also generate a reliable candidate pool for NMT systems whose vocabularies are based on subword units. The risk with Viterbi alignments is that we could miss some words in the target that are not fully explained by a Viterbi word alignment. As we train the candidate list with the model parameters, the candidate list is highly adapted to the current model which makes it very unlikely that we miss a high scoring word due to the candidate restriction. In this work, we show that it is sufficient to use only the top 100 candidates per source word and speed up decoding by up to a factor of 7 without losing any translation performance."
    }, {
      "heading" : "2 Neural Machine Translation",
      "text" : "The attention-based NMT (Bahdanau et al., 2014) is an encoder-decoder network. The encoder employs a bi-directional RNN to encode the source sentence x = (x1, ..., xl) into a sequence of hidden states h = (h1, ..., hl), where l is the length of the source sentence. Each hi is a concatenation of a left-to-right −→ hi and a right-to-left ←− hi RNN:\nhi = [←− h i−→ h i ] = [←− f (xi, ←− h i+1)−→ f (xi, −→ h i−1) ]\nwhere ←− f and −→ f are two gated recurrent units (GRU) introduced by (Cho et al., 2014).\nar X\niv :1\n70 6.\n03 82\n4v 1\n[ cs\n.C L\n] 1\n2 Ju\nn 20\n17\nGiven the encoded h, the decoder predicts the target translation by maximizing the conditional log-probability of the correct translation y∗ = (y∗1, ...y ∗ m), where m is the length of the target. At each time t, the probability of each word yt from a target vocabulary Vy is:\np(yt|h, y∗t−1..y∗1) = g(st, y∗t−1, Ht), (1)\nwhere g is a two layer feed-forward network (ot being an intermediate state) over the embedding of the previous target word (y∗t−1), the decoder hidden state (st), and the weighted sum of encoder states h (Ht). A single feedforward layer then projects ot to the target vocabulary and applies softmax to predict the probability distribution over the output vocabulary.\nWe compute st with a two layer GRU as:\ns′t = u(st−1, y ∗ t−1). (2)\nst = q(s ′ t, Ht) (3)\nwhere s′t is an intermediate state. The two GRU units u and q together with the attention constitute the conditional GRU layer. And Ht is computed as:\nHt =\n[∑l i=1 (αt,i ·\n←− h i)∑l\ni=1 (αt,i · −→ h i)\n] , (4)\nThe attention model (in the right box) is a two layer feed-forward network r, with At,j being an intermediate state and another layer converting it into a real number et,j . The alignment weights α, are computed from the two layer feed-forward network r as:\nαt,i = exp{r(s′t, hi)}∑l j=1 exp{r(s′t, hj)}\n(5)\nαt,j are actually the soft alignment probabilities, denoting the probability of aligning the target word at timestep t to source position j."
    }, {
      "heading" : "3 Our Approach",
      "text" : "In this section we describe our approach for learning alignments from the attention.\n0Same as the decoder GRU introduced in session2 of the dl4mt-tutorial: https://github.com/nyu-dl/dl4mttutorial/tree/master/session2."
    }, {
      "heading" : "3.1 Learning Alignments from Attention",
      "text" : "At each time step t of the decoder, the attention mechanism determines which word to attend to based on the previous target word yt−1, decoder hidden state st−1 and the source annotations −→ hi and ←− hi . This attention implicitly captures the alignment between the target word to be generated at this time step and source words. We formalize this implicit notion into soft alignments, by aligning the generated target word yt to the source word(s) attended to in the current timestep. The strength of the alignment is determined by the weight of the attention weights αtj . While, the attention weights are probabilities, we treat them as fractional counts distribution over source words for the current target word.\nOur method simply accumulates these (normalized) attention weights into a matrix as the training progresses. A naive implementation of this would need a matrix of dimensions |Vs| × |Vt|, which would be infeasible in the typical memory available. Instead, we maintain a sparse matrix to keep track of these raw word counts, where we only update the cells that are touched by the alignments observed in each minibatch. We further delay the accumulation of alignments during the first epoch of training, to ensure that the network can produce reasonably good alignments. And finally, we also employ a threshold αthr and only record the alignments where the attention weights are larger than this threshold. This filters out large number of spurious alignments especially for frequent words, which are unlikely to be of any use.\nIt should be noted that, the idea of treating the attention weights as soft alignments is already being used in certain cases during decoding. For example, it is a standard practice to get the alignments for the UNK tokens in the decoder postprocessing in order to replace it with appropriate target translation using external alignments such as Model-1 (Jean et al., 2015; Luong et al., 2015; Mi et al., 2016; L’Hostis et al., 2016). Some of these works, notably Jean et al. (2015) and Mi et al. (2016) have relied on the alignments generated by external aligners to identify candidate vocabulary for softmax for each training sentence. Additionally, we also propose a way for using these alignments during training.\nAn attractive aspect of our approach is that the alignments could be learned even for previously trained models by continuing the training for one\nor two epochs. As we show later, it is usually sufficient to learn alignments by accumulating the attention weights for just one additional epoch (see Section 4.2)."
    }, {
      "heading" : "3.2 Vocabulary Selection for Decoding",
      "text" : "As mentioned earlier, vocabulary selection to speedup decoding has been widely employed in NMT (Jean et al., 2015; Mi et al., 2016, inter alia). In this work, we use the alignments that are learned during training for vocabulary selection. It should be noted that the accumulated attention weights are fractional counts and not probabilities. Secondly, these counts as learned from the attention characterize target to source alignments. During decoding we are interested in getting the target vocabulary given the source words. So, we first normalize the distribution matrix along target axis and then use the normalized distribution to obtain top-n target words for each source token.\nThis obviates any need for external tools for generating alignments and also simplifies the decoding pipeline. Following the findings by L’Hostis et al. (2016), we only rely on learned alignments and do not use top-k1 most frequent words or any other resource for decoding. Our experiments (see Section. 4.2) show that the alignments learned from the NMT training are sufficient and we do not lose translation performance."
    }, {
      "heading" : "4 Experiments",
      "text" : "We test our vocabulary selection approach on two language pairs: German→English and Italian→English. The alignments from which we extract the candidate lists are learned either during the full training (from scratch) or only during the final epoch (continue training)."
    }, {
      "heading" : "4.1 Setup",
      "text" : "For the German→English translation task, we train an NMT system based on the WMT 2016 training data (Bojar et al., 2016) (3.9 parallel sentences) and use newstest-2014 and newstest-2015 as our dev/ test sets. For the Italian→English translation task, we train our system on a large data set consisting of 20 million parallel sentences. The sentences come from varied resources such as Europarl, news-commentary, Wikipedia, openSubti-\n1It is typical to set k to be 2000 (Jean et al., 2015; Mi et al., 2016).\ntles among others. As test set, we use Newstest2009.\nIn all our experiments, we use our in-house attention-based NMT implementation which is similar to (Bahdanau et al., 2014). We use subword units extracted by byte pair encoding (Sennrich et al., 2015) instead of words, which shrinks the vocabulary to 40k sub-word symbols for both source and target. For comparison, we also run a word model only for German→English. We limit our word vocabularies to be the top 80K most frequent words for both source and target. Words not in these vocabularies are mapped to a single unknown token. The oov-rate for the 80K wordbased model on the dev and test sets is about 4.5% and 4.3% respectively. During translation, we use the alignments (learned from the attention mechanism) to replace the unknown tokens either with potential targets (obtained from an Model1 trained on the parallel data) or with the source word itself (if no target was found). For all our experiments, we use an embedding dimension of 620 and fix the recurrent GRU layers to be of 1000 cells each. For the training procedure, we use uAdam (Kingma and Ba, 2014) to update model parameters with a mini-batch size of 80. The training data is shuffled after each epoch.\nFor evaluation, we compare the BLEU and TER scores for the baseline decoding and the vocabulary selection decoding. For the baseline decoding, we use full search without using the candidate vocabulary from the learned alignments. We further compare the candidate lists of different sizes, where we limit the maximum number of target words per source word to be 20, 50, 100 or 200. For the continued training setup, we train models to convergence without learning alignments and then train one epoch to learn the alignments. For decoding speed comparison, we report relative speedup gains with respect to the baseline, by averaging across 10 runs."
    }, {
      "heading" : "4.2 German→English",
      "text" : "The results for the German→English translation task are shown in Table 1. Applying vocabulary selection during decoding speeds up the decoding by up to 7x compared to the baseline decoding without any vocabulary selection. In our experiments across both languages, candidate list of size 100 best target candidates per word, seems to be a good trade-off setting between speedup gain (over\n3x) without loosing any performance. The average numbers of candidates (per source word) in this case is just a tiny fraction (≈ 0.1%) of the full target vocabulary used in the baseline setting.\nOne of the interesting trends, we note is that continue training turns out to be extremely competitive to learning alignments throughout NMT training over several epochs. It should be stressed again that we only ran the continue training setup for one epoch to learn alignments. This suggests that the attention weights are very stable once the model is reasonably trained. While the word based models are slightly worse than the BPE models, we observe the same trends that we noted earlier in terms of speedup and number of candidates.\nFor both BPE and word-based models, we also compare our vocabulary selection with one generated from traditional IBM models typically used. We employ vocabulary selection from alignments generated by fastalign (Dyer et al., 2013), which is reparameterized model based on IBM Model-2. We limit the number of top-n target words to be 100 to match our chosen setting. As can be seen in the table (last rows in BPE and Words blocks), the BLEU and TER scores are similar to the numbers using our approach for the same top-n setting. However, the average number of candidates from the fastalign is significantly larger: about 10% for BPE and 30% for words. We plan to compare the target candidates from both approaches in future and we also hope that, it would give us some insights to improve our approach.\nWe show the effect of learning alignments for different number of epochs for German→English translation setting in Figure 1 for our devset (newstest-2014). In these experiments, we fix the NMT model and only change the source-target word distribution for vocabulary selection during decoding. The left plot shows the effect on BLEU scores while the one on the right-side plots average number of candidates per source word. The word distribution from early epochs seems to negatively affect the smaller candidate lists (top-20 in figure), where the BLEU increases by one point. For larger candidate sizes, the effect is only marginal in BLEU (≈ 0.2). As for the candidates, we see a flat curve, when the candidate list size is small (say 20 or even 50) and it starts to have some variance for 100 or more candidates.\nTable 2 shows the effect of different αthr thresholds for accumulating the alignments. The αthr\ncould be used to strike a balance between desired coverage in source-target word distribution and avoiding spurious source-target links. We observe that the thresholds up to 0.25 result in similar performance levels (shown for both dev and test sets), with smaller candidate vocabulary size as the threshold is increasing. We also noticed at larger thresholds, the accumulated count matrices lacked variety in the source words distribution, leading to poor coverage. This is to be expected because, for such large thresholds, the alignments will be accumulated only when attention exhibits a peaked distribution it that it is strongly confident about some particular source-target link. We believe 0.1 or 0.2 would be practical αthr values for most data sets/ language pairs."
    }, {
      "heading" : "4.3 Italian→English",
      "text" : "Empirical results for the Italian→English translation task are shown in Table 3. We can speed up the decoding speed by a factor of 3.6x to 3.9x by using a candidate list coming from the attention of our NMT model. The sweet spot candidate size is 100. We can speed up the decoding by a factor of 3.7 while losing only 0.1 point in BLEU. The continue training (in which we only learn the candidate list in the final epoch), works as good as the full trained candidate list. The average candidate per words are even smaller compared to the full trained candidate list which makes the decoding even a little bit faster."
    }, {
      "heading" : "4.4 Dynamic Vocabulary Selection during Training",
      "text" : "As we accumulate the attention weights into a sparse alignment matrix, we could also exploit this to dynamically select the target vocabulary during training. This would be exactly same as the large vocabulary NMT; but unlike other approaches we would not be relying on external resource/ tools such as Model-1 alignments, phrase tables etc.\nWe now explain the recipe for doing this. We first normalize the sparse matrix and obtain the top-n target tokens for each source word as explained in Section 3.2.2\nWe begin the NMT training without any vocabulary selection and train with entire target vocabulary during the initial stages. We switch to vocabulary selection mode, once the alignment matrix\n2In order to avoid stale probabilities, we normalize/ trim the alignments at the beginning of each epoch for dynamic vocabulary selection.\nis seeded with initial alignments from at least one full sweep over data. Given a mini batch of source sentences X , we identify for each source word, top-n target words and use the set of all unique target words Yc as candidate vocabulary for that batch.\nYc = {fa(xi)} ∀xi ∈ X (6)\nThe dynamic vocabulary can then be used as the target vocabulary to train the present batch. Dynamic selection for each mini batch during training could add to the computational cost and poten-\ntially slow it down. One simple solution would be to just do an offline vocabulary selection based on the alignments at the beginning of each epoch. We leave this for future experimentation."
    }, {
      "heading" : "5 Related Works",
      "text" : "Vocabulary selection has been studied widely in the context of NMT decoding (Luong et al., 2015; Mi et al., 2016; L’Hostis et al., 2016). All these works are inspired by the early work by Jean et al. (2015) and use some kind of external strategy (based on word alignments or phrase tables or co-\noccurrence counts etc.) in order to do vocabulary selection. In contrast, we use the alignments that are learned from the attention weights in the early training, for selecting target vocabulary. The other difference is that the selected vocabulary remains stale throughout the training under these earlier approaches. However in this work, the alignments learned in the previous epoch could be used to select target vocabulary for next epoch.\nHierarchical softmax (Morin and Bengio, 2005; Mnih and Hinton, 2009) is well-known way to reduce softmax over a large number of target words. It uses a hierarchical binary tree representation of the output layer with all words as its leaves. It allows exponentially faster computation of word probabilities and their gradients, but the predic-\ntive performance of the resulting model is heavily dependent on the tree used, which is often constructed heuristically. Moreover, by relaxing the constraint of a binary structure, Le et al. (2011) and Baltescu and Blunsom (2014) introduce a structured output layer with an arbitrary tree structure constructed from word clustering. All these methods speed up both the model training and evaluation considerably but are heavily depend on the quality of the word cluster. NMT experiments with hierarchical softmax showed improvement for smaller datasets with about 2m sentence pairs (Baltescu and Blunsom, 2014)."
    }, {
      "heading" : "6 Summary",
      "text" : "We presented a simple approach for directly learning the source-target alignments from the attention layer in Neural Machine Translation. We showed that the alignments could be used for vocabulary selection in decoding, without requiring any external resources such as aligners, phrase-tables etc. We recommend setting αthr = 0.1 and top-n candidates to be 100 for good performance and faster decoding for most language pairs/ datasets. Our experiments showed decoding speedup of up to a factor of 7 for different settings. We also showed how this could be used for dynamic vocabulary selection during training."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Neural Machine Translation (NMT) models usually use large target vocabulary sizes to capture most of the words in the target language. The vocabulary size is a big factor when decoding new sentences as the final softmax layer normalizes over all possible target words. To address this problem, it is widely common to restrict the target vocabulary with candidate lists based on the source sentence. Usually, the candidate lists are a combination of external word-to-word aligner, phrase table entries or most frequent words. In this work, we propose a simple and yet novel approach to learn candidate lists directly from the attention layer during NMT training. The candidate lists are highly optimized for the current NMT model and do not need any external computation of the candidate pool. We show significant decoding speedup compared with using the entire vocabulary, without losing any translation quality for two language pairs.",
    "creator" : "LaTeX with hyperref package"
  }
}