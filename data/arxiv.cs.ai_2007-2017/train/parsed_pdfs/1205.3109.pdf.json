{
  "name" : "1205.3109.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search",
    "authors" : [ "Arthur Guez" ],
    "emails" : [ "aguez@gatsby.ucl.ac.uk", "d.silver@cs.ucl.ac.uk", "dayan@gatsby.ucl.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Bayesian model-based reinforcement learning is a formally elegant approach to learning optimal behaviour under model uncertainty. In this setting, a Bayesoptimal policy captures the ideal trade-off between exploration and exploitation. Unfortunately, finding Bayes-optimal policies is notoriously taxing due to the enormous search space in the augmented belief-state MDP. In this paper we exploit recent advances in sample-based planning, based on Monte-Carlo tree search, to introduce a tractable method for approximate Bayes-optimal planning. Unlike prior work in this area, we avoid expensive applications of Bayes rule within the search tree, by lazily sampling models from the current beliefs. Our approach outperformed prior Bayesian model-based RL algorithms by a significant margin on several well-known benchmark problems."
    }, {
      "heading" : "1 Introduction",
      "text" : "An important objective in the theory of Markov Decision Processes (MDPs) is to maximize the expected sum of discounted rewards when the dynamics of the MDP are (perhaps partially) unknown. A discount factor pressures the agent to favor short-term rewards, but potentially costly exploration is necessary to discover how to get such rewards. This conflict leads to the well-known explorationexploitation trade-off. Early on, it was recognized [Bellman and Kalaba, 1959, Feldbaum, 1960] that one solution to the resulting partially observable MDP was to augment the regular state of the agent in the world with sufficient statistics for what it knows about the dynamics. One formulation of this idea is the augmented Bayes-Adaptive MDP (BAMDP) [Martin, 1967, Duff, 2002], in which the extra information is the posterior distribution over the dynamics, given the data so far observed. The agent starts in the belief state that corresponds to its prior and, by executing the greedy policy in the BAMDP whilst learning, acts optimally with respect to its belief in the original MDP. An attractive property of this framework for Bayesian reinforcement learning is that rich prior knowledge about statistics of the environment can be naturally incorporated into the planning process, potentially leading to more efficient exploration and exploitation of the uncertain world.\nThe major obstacle to applying Bayesian reinforcement learning is its computational intractability. Various algorithms have been devised to approximate optimal learning, but often at rather large cost. In this paper, we present a tractable approach that exploits and extends recent advances in Monte-Carlo tree search [Kocsis and Szepesvári, 2006, Silver and Veness, 2010].\nar X\niv :1\n20 5.\n31 09\nv1 [\ncs .L\nG ]\nAt each iteration in our algorithm, a single MDP is sampled from the agent’s current beliefs. This single MDP is used to simulate a single episode. The outcome of this simulated episode is used to update the value of each node of the search tree traversed during the simulation. By integrating over many simulations, and therefore many sample MDPs, the optimal value of each future sequence is obtained with respect to the agent’s beliefs. We prove that this process converges to the Bayesoptimal policy. To increase computational efficiency, we introduce a further innovation: a lazy sampling scheme that considerably reduces the cost of sampling.\nOur algorithm is more efficient than previous sparse sampling methods for Bayes-adaptive planning [Wang et al., 2005, Castro, 2007, Asmuth and Littman, 2011], since it does not need to update the posterior belief state during the course of each simulation, and so avoids repeated applications of Bayes rule, which can be expensive in large state spaces. We show that it outperforms existing approaches on standard benchmark problems. Moreover, our algorithm is particularly well suited to support planning in domains with richly, structured, prior knowledge — a critical requirement for applications of Bayesian reinforcement learning to large problems.\nWe applied our algorithm to a representative sample of benchmark problems and competitive algorithms from the literature, and found that it achieved state-of-the-art performance. It consistently and significantly outperformed existing Bayesian RL methods, and also recent non-Bayesian approaches."
    }, {
      "heading" : "2 Bayesian RL",
      "text" : "We describe the generic Bayesian formulation of optimal decision-making in an unknown MDP, following [Martin, 1967] and [Duff, 2002]. An MDP is described as a 5-tuple M = 〈S,A,P,R, γ〉, where S is the set of states, A is the set of actions, P : S × A × S → R is the state transition probability kernel, R : S × A → R is a bounded reward function, and γ is the discount factor [Szepesvári, 2010]. When all the components of the MDP tuple are known, standard MDP planning algorithms can be used to compute the optimal value function and policy off-line. In general, the dynamics are unknown, and we assume that P is a latent variable distributed according to a distribution P (P). After observing a history of actions and transitions ht = s1a1s2a2 . . . atst from the MDP, the posterior belief on P is updated using Bayes’ rule P (P|ht) ∝ P (ht|P)P (P). The uncertainty about the dynamics of the model can be transformed into uncertainty about the current state inside an augmented state space S+ = S×H, where S is the state space in the original problem and H is the set of possible histories. The dynamics associated with this augmented state space are described by\nP+(〈s, h〉, a, 〈s′, h′〉) = 1h′=has′ ∫ P P(s, a, s′)P (P|h) dP, R+(〈s, h〉, a) = R(s, a) (1)\nTogether, the 5-tuple M+ = 〈S+, A,P+,R+, γ〉 forms the Bayes-Adaptive MDP (BAMDP) for the MDP problem M . Since the dynamics of the BAMDP are known, it can in principle be solved to obtain the optimal value function associated with each action:\nQ∗(〈st, ht〉, a) = max π Eπ [ ∞∑ t′=t γt ′−trt′ |at = a ] (2)\nfrom which the optimal action for each state can be readily derived. Optimal actions in the BAMDP are executed greedily in the real MDP M and constitute the best course of action for a Bayesian agent with respect to its prior belief over P . It is obvious that the expected performance of the BAMDP policy in the MDPM is bounded above by that of the optimal policy obtained with a fullyobservable model, with equality occurring, for example, in the degenerate case in which the prior only has support on the true model."
    }, {
      "heading" : "3 The BMCP algorithm",
      "text" : ""
    }, {
      "heading" : "3.1 Algorithm Description",
      "text" : "The goal of a BAMDP planning algorithm is to find, for each decision point 〈s, h〉 encountered, the action a that maximizes Equation 2. Our algorithm, Bayes-adaptive Monte-Carlo Planning (BMCP),\ndoes this by performing a forward-search in the space of possible future histories of the BAMDP using Monte-Carlo Tree Search.\nWe employ the UCT algorithm [Kocsis and Szepesvári, 2006] to allocate search effort to promising branches of the state-action tree, and use sample-based rollouts to provide value estimates at each node. Sample-based search in the BAMDP would usually require the generation of samples from P+ at every single node. This operation requires integration over all possible transition models, or at least a sample of a transition model P — an expensive procedure for all but the simplest generative models P (P). We avoid this cost by only sampling a single transition model Pi from the posterior at the root of the search tree at the start of each simulation i, and using Pi to generate all the necessary samples during this simulation. Sample-based tree search then acts as a filter, ensuring that the correct distribution of state successors is obtained at each of the tree nodes, as if it was sampled from P+. This method is a carefully tailored version of the POMCP algorithm [Silver and Veness, 2010], originally developed to solve Partially Observable MDPs.\nThe root node of the search tree at a decision point represents the current state of the BAMDP. The tree is composed of state nodes representing belief states 〈s, h〉 and action nodes representing the effect of particular action from their parent state node. The visit counts: N(〈s, h〉) for state nodes, and N(〈s, h〉, a) for action nodes, are initialized to 0 and updated throughout search. A value Q(〈s, h〉, a) is also maintained for each action node. Each simulation traverses the tree without backtracking by following the UCT policy at state nodes defined by argmaxaQ(〈s, h〉, a) + c √\nlog(N(〈s,h〉)) N(〈s,h〉,a) , where c is an exploration constant that needs to be set appropriately. Given an action, the transition distribution Pi corresponding to the current simulation i is used to sample the next state. That is, at action node (〈s, h〉, a), s′ is sampled from Pi(s, a, ·), and the new state node is set to 〈s′, has′〉. When a simulation reaches a leaf, the tree is expanded by attaching a new state node with its connected action nodes, and a rollout policy πro is used to control the MDP defined by the current Pi to some fixed depth (determined using the discount factor). The rollout provides an estimate of the value Q(〈s, h〉, a) from the leaf action node. This estimate is then used to update the value of all action nodes traversed during the simulation: if R is the sampled discounted return obtained from a traversed action node (〈s, h〉, a) in a given simulation, then we update the value of the action node to Q(〈s, h〉, a) + R−Q(〈s,h〉,a)N(〈s,h〉,a) (i.e., the mean of the sampled returns obtained from that action node over the simulations). A detailed description of the BMCP algorithm is provided in Algorithm 1. A diagram example of BMCP simulations is presented in Figure 7 (Supplementary).\nThe tree policy treats the forward search as a meta-exploration problem, preferring to exploit regions of the tree that currently appear better than others while continuing to explore unknown or less known parts of the tree. This leads to good empirical results even for small number of simulations, because effort is expended where search seems fruitful. Nevertheless all parts of the tree are eventually visited infinitely often, and therefore the algorithm will eventually converge on the Bayes-optimal policy (see Section 3.4).\nFinally, one should remark that the history of transitions is generally not the most compact sufficient statistic of the belief in fully observable MDPs. Indeed, h can be replaced with unordered transition counts ψ, considerably reducing the number of states of the BAMDP and, thereby, reducing in theory the planning complexity. BMCP can be used to search in this reduced space, requiring a search in an expanding lattice — with a need for an adequate addressing scheme — as opposed to a tree. We experimented with this version of BMCP but only found a marginal improvement. This type of result has been observed before with UCT; it stems from the fact that UCT will usually concentrate its search effort on one of several equivalent paths (up to transposition), so reducing the number of those paths does not affect the performance considerably."
    }, {
      "heading" : "3.2 Rollout Policy",
      "text" : "The choice of rollout policy πro is important if simulations are few, especially if the domain does not display substantial locality or if rewards require a carefully selected sequence of actions to be obtained. Otherwise, a simple uniform random policy can be chosen to provide noisy estimates. In this work, we learn Qro, the optimal Q-value in the real MDP, in a model-free manner (e.g., using Q-learning) from samples (st, at, rt, st+1) obtained off-policy as a result of the interaction of the Bayesian agent with the environment. Acting greedily according to Qro translates to pure exploitation of gathered knowledge. A rollout policy in BMCP following Qro could therefore over-\nAlgorithm 1: BMCP 1:\n2: 3: procedure Search( 〈s, h〉 ) 4: repeat 5: P ∼ P (P|h) 6: Simulate(〈s,P〉, h, 0) 7: until Timeout() 8: return argmax\na Q(〈s, h〉, a)\n9: end procedure 10:\n11: 12: procedure Rollout( 〈s,P〉, h,depth ) 13: if γdepth < then 14: return 0 15: end 16: a ∼ πro(〈s,P〉, ·) 17: s′ ∼ P(s, a, ·) 18: r ← R(s, a) 19: return r + γ Rollout( 〈s′,P〉, has′, depth+1) 20: end procedure 21:\n22:\n23:\n1: procedure Simulate( 〈s,P〉, h,depth ) 2: if γdepth < then return 0 3: if N(〈s, h〉) = 0 then 4: for all a ∈ A do 5: N(〈s, h〉, a)← 0, Q(〈s, h〉, a))← 0 6: end 7: a ∼ πro(〈s,P〉, ·) 8: s′ ∼ P(s, a, ·) 9: r ← R(s, a)\n10: R← r+γRollout(〈s′,P〉, has′,depth) 11: N(〈s, h〉)← 1, N(〈s, h〉, a)← 1 12: Q(〈s, h〉, a)← R 13: return R 14: end\n15: a← argmax b\nQ(〈s, h〉, b) + c √\nlog(N(〈s,h〉)) N(〈s,h〉,b)\n16: s′ ∼ P(s, a, ·) 17: r ← R(s, a) 18: R← r + γ Simulate( 〈s′,P〉, has′, depth+1) 19: N(〈s, h〉)← N(〈s, h〉) + 1 20: N(〈s, h〉, a)← N(〈s, h〉, a) + 1 21: Q(〈s, h〉, a)← Q(〈s, h〉, a) + R−Q(〈s,h〉,a)N(〈s,h〉,a) 22: return R 23: end procedure\nexploit. Instead, similar to [Gelly and Silver, 2007], we select an -greedy policy with respect to Qro as our rollout policy πro. This biases rollouts towards observed regions of high rewards. This method provides valuable direction for the rollout policy at negligible computational cost. More complex rollout policies can be considered, for example rollout policies that depend on the sampled model Pi, but this would usually imply some computational overhead."
    }, {
      "heading" : "3.3 Lazy Sampling",
      "text" : "In previous work on sample-based tree search, including POMCP [Silver and Veness, 2010], a complete sample state is drawn from the posterior at the root of the search tree. However, this can be computationally very costly. Instead, we sample P lazily, creating only the particular transition probabilities from that are required as the simulation traverses the tree, and also during the rollout.\nIf P(s, a, ·) is parametrized by a latent variable θs,a for each state and action pair, potentially depending on each other as well as on an additional set of latent variables φ, then the posterior over P can be written as P (Θ|h) = ∫ φ P (Θ|φ, h)P (φ|h), where Θ = {θs,a|s ∈ S, a ∈ A}. Define Θt = {θs1,a1 , · · · , θst,at} to be the (random) set of θ parameters required during the course of a BMCP simulation that starts at time 1 and ends at time t. Using the chain rule, we can rewrite\nP (Θ|φ, h) =P (θs1,a1 |φ, h) P (θs2,a2 |Θ1, φ, h)\n. . . P (θsT ,aT |ΘT−1, φ, h) P (Θ \\ΘT |ΘT , φ, h)\nwhere T is the length of the simulation and Θ \\ΘT denotes the (random) set of parameters that are not required for a simulation. For each simulation i, we sample P (φ|ht) at the root and then lazily sample the θst,at parameters as required, conditioned on φ and all Θt−1 parameters sampled for the current simulation. This process is stopped at the end of the simulation, potentially before all θ parameters have been sampled.\nFor example, if the transition parameters for different states and actions are independent, we can completely forgo sampling a complete P , and instead draw any necessary parameters individually for each state-action pair. This leads to substantial performance improvement, especially in large MDPs where a single simulation only requires a small subset of parameters."
    }, {
      "heading" : "3.4 Theoretical properties",
      "text" : "Define V (〈s, h〉) = max a∈A Q(〈s, h〉, a) ∀〈s, h〉 ∈ S ×H.\nTheorem 1. For all > 0 and a suitably chosen c (e.g. c > Rmax1−γ ), from state 〈st, ht〉, BMCP constructs a value function that converges in probability to an -optimal value function V ∗ , V (〈s, h〉)\np→ V ∗ (〈s, h〉) for all states 〈s, h〉 reachable from 〈st, ht〉 in the finite search horizon of the BAMDP. Moreover, for large enough N(〈s, h〉), the bias of V (〈s, h〉) decreases as O(log(N(〈s, h〉))/N(〈s, h〉)1.\nBy definition, Theorem 1 implies that BMCP converges to the Bayes-optimal solution asymptotically. We confirmed this result empirically using a variety of Bandit problems, for which the Bayesoptimal solution can be computed efficiently using Gittins indices (see supplementary material)."
    }, {
      "heading" : "4 Related Work",
      "text" : "In Section 5, we compare BMCP to a set of existing Bayesian RL algorithms; we describe them briefly in this section. In the interest of space, we do not provide a comprehensive list of planning algorithms for MDP exploration. Instead, we concentrate on related sample-based algorithms for Bayesian RL.\nBayesian DP [Strens, 2000] maintains a posterior distribution over transition models. At each step, a single model is sampled, and the action that is optimal in that model is executed. The Best Of Sampled Set (BOSS) algorithm generalizes this idea [Asmuth et al., 2009]. BOSS samples a number of models from the posterior and combines them optimistically. This drives sufficient exploration to guarantee finite-sample performance guarantees. BOSS is quite sensitive to its parameter that governs the sampling criterion. Unfortunately, this is difficult to select. [Castro and Precup, 2010] propose the SBOSS variant, which provides a more effective adaptive sampling criterion. The BOSS family of algorithms is generally quite robust, but suffers from over-exploration.\nSparse sampling [Kearns et al., 1999] is a sample-based tree search algorithm. The key idea is to sample successor nodes from each state, and apply a Bellman backup to update the value of the parent node from the values of the child nodes. [Wang et al., 2005] apply sparse sampling to search over belief-state MDPs. The tree is expanded non-uniformly by descending the tree in trajectories. At each decision node, a promising action is selected using Thompson sampling — i.e., sampling an MDP from that belief-state, solving the MDP and taking the optimal action. At each chance node, a successor belief-state is sampled from the transition dynamics of the belief-state MDP.\n[Asmuth and Littman, 2011] further extend this idea in their BFS3 algorithm, an adaptation of Forward Search Sparse Sampling [Walsh et al., 2010] to belief-MDPs. Although they describe their algorithm as Monte-Carlo tree search, it in fact uses a Bellman backup rather than Monte-Carlo evaluation. Each Bellman backup updates both lower and upper bounds on the value of each node. Like Wang et al., the tree is expanded non-uniformly by descending the tree in trajectories. However, they use a different descent algorithm. At each decision node, a promising action is selected by maximising the upper bound on value. At each chance node, observations are selected by maximising the uncertainty (upper minus lower bound).\nBayesian Exploration Bonus (BEB) solves the posterior mean MDP, but with an additional reward bonus that depends on visitation counts [Kolter and Ng, 2009]. Similarly, [Sorg et al., 2010] propose an algorithm with a different form of exploration bonus. These algorithms provide performance guarantees after a polynomial number of steps in the environment. However, behavior in the early steps of exploration is very sensitive to the precise exploration bonuses; and it turns out to be hard to translate sophisticated prior knowledge into bonus form.\n1Proof available in supplementary material."
    }, {
      "heading" : "5 Experiments",
      "text" : "We present empirical results of BMCP on a set of standard problems, and compare the results to other popular algorithms."
    }, {
      "heading" : "5.1 Algorithms",
      "text" : "The following algorithms were run: • BMCP - The algorithm presented in Section 3, implemented with lazy sampling. The algorithm\nwas run for different number of simulations (10 to 10000) to span different planning times. In all experiments, we set πro to be an -greedy policy with = 0.5. The UCT exploration constant was left unchanged for all experiments (c = 3), we experimented with other values of c ∈ {0.5, 1, 5} with similar results.\n• SBOSS - The BOSS [Asmuth et al., 2009] variant from [Castro and Precup, 2010] with an adaptive resampling criterion. For each domain, we varied the number of samples K ∈ {2, 4, 8, 16, 32} and the resampling threshold parameter δ ∈ {3, 5, 7}.\n• BEB - The internal reward algorithm in [Kolter and Ng, 2009]. For each domain, we varied the bonus parameter β ∈ {0.5, 1, 1.5, 2, 2.5, 3, 5, 10, 15, 20}.\n• BFS3 - A recent tree search algorithm [Asmuth and Littman, 2011]. For each domain, we varied the branching factor C ∈ {2, 5, 10, 15} and the number of simulations (10 to 2000). The depth of search was set to 15 in all domains except for the larger grid and maze domain where it was set to 50. We also tuned the Vmax parameter for each domain — Vmin was always set to 0.\nIn addition, we report results from [Strens, 2000] for several other prior algorithms."
    }, {
      "heading" : "5.2 Domains",
      "text" : "For all domains, we fix γ = 0.95. The Double-loop domain is a 9-states deterministic MDP with 2 actions [Dearden et al., 1998], 1000 steps are executed in this domain. Grid5 is a 5× 5 grid with no reward anywhere except for a reward state opposite to the reset state. Actions with cardinal directions are executed with small probability of failure for 1000 steps. Grid10 is a 10×10 grid designed like Grid5. We collect 2000 steps in this domain. Dearden’s Maze is a 264-states maze with 3 flags to collect [Dearden et al., 1998]. A special reward state gives the number of flags collected since the last visit as reward, 20000 steps are executed in this domain2.\nTo quantify the performance of each algorithm, we measure the total undiscounted reward over many steps. We chose this measure of performance to enable fair comparisons to be drawn with prior work. In fact, we are optimising a different criterion – the discounted reward from the start state – and so we might expect this evaluation to be unfavourable to our algorithm.\n2The result reported for Dearden’s maze with the Bayesian DP alg. in [Strens, 2000] is for a different version of the task in which the maze layout is given to the agent.\nOne major advantage of Bayesian RL is that one can specify priors about the dynamics. For the Double-loop domain, the Bayesian RL algorithms were run with a simple DirichletMultinomial model with symmetric Dirichlet parameter α = 1|S| . For the grids and the maze domain, the algorithms were run with a sparse Dirichlet-Multinomial model, as described in [Friedman and Singer, 1999].\n5.3 Results\nA summary of the results is presented in Table 1. Figure 1 and 2 report the planning time/performance trade-off for the different algorithms on the Grid5 and Maze domain.\nOver all the domains tested, BMCP performed best. Other algorithms came close on some tasks, but only when their parameters were tuned to that specific domain. This is particularly evident for BEB, which required a different value of exploration bonus to achieve maximum performance in each domain. BMCP’s performance is stable with respect to the choice of its exploration constant c and it did not require tuning to obtain the results.\nBMCP’s performance scales well as a function of planning time, as evident in Figures 1 and 2. In contrast, SBOSS follows the opposite trend. If more samples are employed to build the merged model, SBOSS actually becomes too optimistic and over-explores, degrading its performance. BEB cannot take advantage of prolonged planning time at all. BFS3 generally scales up with more planning time with an appropriate choice of parameters, but it is not obvious how to trade-off the branching factor, depth, and number of simulations in each domain. BMCP greatly benefited from our lazy sampling scheme in the experiments, providing 35× speed improvement over the naive approach in the maze domain for example; this is illustrated in Figure 2(b).\nDearden’s maze aptly illustrates a major drawback of forward search sparse sampling algorithms such as BFS3. Like many maze problems, all rewards are zero for at least k steps, where k is the solution length. Without prior knowledge of the optimal solution length, all upper bounds will be higher than the true optimal value until the tree has been fully expanded up to depth k – even if a simulation happens to solve the maze. In contrast, once BMCP discovers a successful simulation, its Monte-Carlo evaluation will immediately bias the search tree towards the successful trajectory."
    }, {
      "heading" : "6 Future Work",
      "text" : "The UCT algorithm is known to have several drawbacks. First, there are no finite-time regret bounds. It is possible to construct malicious environments, for example in which the optimal policy is hidden in a generally low reward region of the tree, where UCT can be misled for long periods [Coquelin and Munos, 2007]. Second, the UCT algorithm treats every action node as a multiarmed bandit problem. However, there is no actual benefit to accruing reward during planning, and so it is in theory more appropriate to use pure exploration bandits [Bubeck et al., 2009]. Nevertheless, the UCT algorithm has produced excellent empirical performance in many domains.\nWe have focused on learning the dynamics of a fully observable MDP. If the reward function is also unknown, then BMCP could be extended to maintain beliefs over both transition dynamics and reward. Both dynamics and rewards would then be sampled from the posterior distribution at the start of each simulation. Similarly, if the states are not observed directly, then BMCP could sample both state and dynamics from its posterior beliefs, at the start of each simulation. This setting is known as a Bayes-Adaptive Partially Observable MDP (BAPOMDP) [Ross et al., 2011].\nBMCP is able to exploit prior knowledge about the dynamics in a principled manner. In principle, it is possible to encode many aspects of domain knowledge into the prior distribution. An important avenue for future work is to explore rich, structured priors about the dynamics of the MDP. If this prior knowledge matches the class of environments that the agent will encounter, then exploration could be significantly accelerated."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We have suggested a sample-based algorithm for Bayesian RL called BMCP that significantly surpassed the performance of existing algorithms on several standard tasks. The main idea is to employ Monte-Carlo tree search to explore the augmented Bayes-adaptive search space efficiently. Furthermore, BMCP extends prior work on Monte-Carlo tree search (e.g., [Silver and Veness, 2010]): to use a model-free reinforcement learning algorithm to learn a rollout policy adaptively; and to use a lazy sampling scheme to sample the posterior beliefs cheaply. Unlike other sample-based algorithms for Bayesian RL [Wang et al., 2005, Asmuth and Littman, 2011], BMCP is computationally rather tractable: it only requires beliefs to be sampled at the start of each simulation, a process that lazy sampling makes considerably more economic. In addition, BMCP provably converges to the Bayes-optimal solution."
    } ],
    "references" : [ {
      "title" : "A Bayesian sampling approach to exploration in reinforcement learning",
      "author" : [ "Asmuth et al", "J. 2009] Asmuth", "L. Li", "M. Littman", "A. Nouri", "D. Wingate" ],
      "venue" : "In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "al. et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2009
    }, {
      "title" : "Approaching Bayes-optimality using MonteCarlo tree search",
      "author" : [ "Asmuth", "Littman", "J. 2011] Asmuth", "M. Littman" ],
      "venue" : "In Proceedings of the 27th Conference on Uncertainty in Artificial Intelligence",
      "citeRegEx" : "Asmuth et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Asmuth et al\\.",
      "year" : 2011
    }, {
      "title" : "On adaptive control processes",
      "author" : [ "Bellman", "Kalaba", "R. 1959] Bellman", "R. Kalaba" ],
      "venue" : "Automatic Control, IRE Transactions",
      "citeRegEx" : "Bellman et al\\.,? \\Q1959\\E",
      "shortCiteRegEx" : "Bellman et al\\.",
      "year" : 1959
    }, {
      "title" : "Pure exploration in multi-armed bandits problems",
      "author" : [ "Bubeck et al", "S. 2009] Bubeck", "R. Munos", "G. Stoltz" ],
      "venue" : "In Proceedings of the 20th international conference on Algorithmic learning theory,",
      "citeRegEx" : "al. et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2009
    }, {
      "title" : "Smarter sampling in model-based Bayesian reinforcement learning",
      "author" : [ "Castro", "Precup", "P. 2010] Castro", "D. Precup" ],
      "venue" : "Machine Learning and Knowledge Discovery in Databases,",
      "citeRegEx" : "Castro et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Castro et al\\.",
      "year" : 2010
    }, {
      "title" : "Bandit algorithms for tree search",
      "author" : [ "Coquelin", "Munos", "P. 2007] Coquelin", "R. Munos" ],
      "venue" : "Arxiv preprint cs/0703062",
      "citeRegEx" : "Coquelin et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Coquelin et al\\.",
      "year" : 2007
    }, {
      "title" : "Bayesian Q-learning",
      "author" : [ "Dearden et al", "R. 1998] Dearden", "N. Friedman", "S. Russell" ],
      "venue" : "In Proceedings of the National Conference on Artificial Intelligence,",
      "citeRegEx" : "al. et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 1998
    }, {
      "title" : "A",
      "author" : [ "Feldbaum" ],
      "venue" : "(1960). Dual control theory. Automation and Remote Control, 21(9):874–",
      "citeRegEx" : "Feldbaum. 1960",
      "shortCiteRegEx" : null,
      "year" : 1039
    }, {
      "title" : "Efficient Bayesian parameter estimation in large discrete domains. Advances in neural information processing systems, pages 417–423",
      "author" : [ "Friedman", "Singer", "N. 1999] Friedman", "Y. Singer" ],
      "venue" : null,
      "citeRegEx" : "Friedman et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Friedman et al\\.",
      "year" : 1999
    }, {
      "title" : "Combining online and offline knowledge in UCT",
      "author" : [ "Gelly", "Silver", "S. 2007] Gelly", "D. Silver" ],
      "venue" : "In Proceedings of the 24th international conference on Machine learning,",
      "citeRegEx" : "Gelly et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Gelly et al\\.",
      "year" : 2007
    }, {
      "title" : "Multi-armed bandit allocation indices. Wiley Online Library",
      "author" : [ "Gittins et al", "J. 1989] Gittins", "R. Weber", "K. Glazebrook" ],
      "venue" : null,
      "citeRegEx" : "al. et al\\.,? \\Q1989\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 1989
    }, {
      "title" : "A sparse sampling algorithm for nearoptimal planning in large Markov decision processes",
      "author" : [ "Kearns et al", "M. 1999] Kearns", "Y. Mansour", "A. Ng" ],
      "venue" : "In Proceedings of the 16th international joint conference on Artificial intelligence-Volume",
      "citeRegEx" : "al. et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 1999
    }, {
      "title" : "Bandit based Monte-Carlo planning",
      "author" : [ "Kocsis", "Szepesvári", "L. 2006] Kocsis", "C. Szepesvári" ],
      "venue" : "Machine Learning: ECML",
      "citeRegEx" : "Kocsis et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Kocsis et al\\.",
      "year" : 2006
    }, {
      "title" : "Near-Bayesian exploration in polynomial time",
      "author" : [ "Kolter", "Ng", "J. 2009] Kolter", "A. Ng" ],
      "venue" : "In Proceedings of the 26th Annual International Conference on Machine Learning,",
      "citeRegEx" : "Kolter et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Kolter et al\\.",
      "year" : 2009
    }, {
      "title" : "Exploration of multi-state environments: Local measures and back-propagation of uncertainty",
      "author" : [ "Meuleau", "Bourgine", "N. 1999] Meuleau", "P. Bourgine" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Meuleau et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Meuleau et al\\.",
      "year" : 1999
    }, {
      "title" : "A Bayesian approach for learning and planning in Partially Observable Markov Decision Processes",
      "author" : [ "Ross et al", "S. 2011] Ross", "J. Pineau", "B. Chaib-draa", "P. Kreitmann" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "al. et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2011
    }, {
      "title" : "Monte-Carlo planning in large POMDPs",
      "author" : [ "Silver", "Veness", "D. 2010] Silver", "J. Veness" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "Silver et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Silver et al\\.",
      "year" : 2010
    }, {
      "title" : "Variance-based rewards for approximate Bayesian reinforcement learning",
      "author" : [ "Sorg et al", "J. 2010] Sorg", "S. Singh", "R. Lewis" ],
      "venue" : "In Proceedings of the 26th Conference on Uncertainty in Artificial Intelligence",
      "citeRegEx" : "al. et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2010
    }, {
      "title" : "Integrating sample-based planning and model-based reinforcement learning",
      "author" : [ "Walsh et al", "T. 2010] Walsh", "S. Goschin", "M. Littman" ],
      "venue" : "In Proceedings of the Twenty-Fourth Conference on Artificial Intelligence (AAAI)",
      "citeRegEx" : "al. et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2010
    }, {
      "title" : "Bayesian sparse sampling for on-line reward optimization",
      "author" : [ "Wang et al", "T. 2005] Wang", "D. Lizotte", "M. Bowling", "D. Schuurmans" ],
      "venue" : "In Proceedings of the 22nd international conference on Machine learning,",
      "citeRegEx" : "al. et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2005
    } ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Bayesian model-based reinforcement learning is a formally elegant approach to learning optimal behaviour under model uncertainty. In this setting, a Bayesoptimal policy captures the ideal trade-off between exploration and exploitation. Unfortunately, finding Bayes-optimal policies is notoriously taxing due to the enormous search space in the augmented belief-state MDP. In this paper we exploit recent advances in sample-based planning, based on Monte-Carlo tree search, to introduce a tractable method for approximate Bayes-optimal planning. Unlike prior work in this area, we avoid expensive applications of Bayes rule within the search tree, by lazily sampling models from the current beliefs. Our approach outperformed prior Bayesian model-based RL algorithms by a significant margin on several well-known benchmark problems.",
    "creator" : "LaTeX with hyperref package"
  }
}