{
  "name" : "1705.10998.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "The Atari Grand Challenge Dataset",
    "authors" : [ "Vitaly Kurin", "Sebastian Nowozin", "Katja Hofmann" ],
    "emails" : [ "vitaliykurin@gmail.com", "Katja.Hofmann}@microsoft.com", "leibe}@vision.rwth-aachen.de" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In Reinforcement Learning (RL), an agent learns, by trial and error, to perform a task in an initially unknown environment. Recently, this research area has seen dramatic progress in complex interactive tasks in virtual environments Mnih et al. [2015, 2016], Silver et al. [2016], Schulman et al. [2015], largely driven by combinations of RL with deep learning. Yet, despite this recent progress, real-world applications are still largely lacking.\nIn the RL setup, agents learn to solve a task completely from scratch. This causes one of the key limitations of state-of-the-art deep RL approaches — data inefficiency. In comparison to autonomous agents, humans have a lot of prior information about the world. Every day, we base our decisions on knowledge about culture and social relationships, our own experience, and information we get from experience of others. As a result, when learning a new task, people are more effective and need less actions to master it. During training, the agent executes a lot of actions that a human never would. This is ineffective and renders the application of RL in complex or potentially dangerous environments infeasible. A possible solution to the problem is to learn from human demonstrations Schaal [1997], Ng and Russell [2000], Abbeel and Ng [2004], Monfort et al. [2017], Hester et al. [2017].\nUp until now, the RL community has been focused on building environments — test beds for RL models. Today, RL approaches can be trained and compared on diverse tasks in environments such as ALE Bellemare et al. [2013], OpenAI Gym Brockman et al. [2016] or Microsoft’s Project Malmo Johnson et al. [2016]. But there are few publicly available datasets of human demonstrations\nar X\niv :1\n70 5.\n10 99\n8v 1\n[ cs\n.A I]\n3 1\nof tasks in these environments. This lack hampers the progress of research on learning from human demonstration. Examples such as ImageNet Deng et al. [2009] in Computer Vision and Switchboard Godfrey et al. [1992] in Speech Recognition have shown that datasets can catalyze research progress. In order to accelerate research in learning from demonstration, we release, describe, and illustrate the use of the Atari Grand Challenge dataset1 — a dataset of human Atari 2600 replays.\nOur contributions are: (i) We collect, analyze, and release to the research community the largest and most diverse dataset of human Atari 2600 replays to date. The dataset comprises ∼9.7 million frames (∼45 hours) of game play for five games - an order of magnitude larger than previous datasets. (ii) We illustrate one use of this dataset by analyzing the relation between demonstration quality and imitation learning performance. (iii) We discuss research directions that are opened up by our work."
    }, {
      "heading" : "2 Background",
      "text" : "This section outlines key concepts and notation used throughout the paper. We operate in a usual Reinforcement Learning (RL) setup: an agent acts in an environment in response to state observations, and learns from a reward signal that reflects an abstract notion of consequences of the actions taken. This setup can be formulated as Markov Decision Process (MDP), defined by a tuple 〈S,A,R, T 〉, where S is the set of states, A is the set of actions, R(s, a) is the reward function, and T (s, a, s′) is the transition function that returns a probability over states, given a state and an action: p(s′|s, a), s, s′ ∈ S, a ∈ A. In each iteration of its interaction with the environment, the agent observes the state, takes an action and gets some reward for the transition. An agent’s behavior is characterized by a policy π(s), a function that returns an action given a state. The policy can be stochastic.\nOur experimental analysis (section 5), is based on a recently proposed imitation algorithm suggested in Hester et al. [2017], in turn based on Q-learning Watkins and Dayan [1992] and in particular DDQN Van Hasselt et al. [2016]. We next give an overview of the relevant approaches.\nQ-learning Watkins and Dayan [1992] is a “model-free” RL algorithm. It centers on learning to approximate the so-called action-value function Q(s, a). The Q-function reflects the expected discounted cumulative value of taking a particular action a in state s, and following the particular policy thereafter. The optimal action-value function Q∗(s, a) should satisfy the Bellman equation:\nQ∗(s, a) = Es′ [ R(s, a) + γmax\na′∈A Q∗(s′, a′)|s, a\n] , (1)\nwhere γ is a discount factor that trades off immediate versus longer term rewards. The optimal policy π∗(s) is the policy which takes the best possible decision on each time step:\nπ∗(s) = argmax a∈A Q∗(s, a). (2)\nDQN Mnih et al. [2015] is a variant of Q-learning that uses a neural network (called Deep Q-Network) to approximate Q-values. The network Q(s, ·; θ) returns action values for all the actions available given the current state. A separate target network is used to compute Q-values in training updates, as well as a so-called replay memory of past experience for minibatch sampling. Both were shown to improve training stability and resulted in breakthrough results when learning to play Atari games. Double DQN Van Hasselt et al. [2016] is an extension of DQN which decouples the selection and value estimate of actions in the max operator, which was shown to result in more accurate Q-value approximations both theoretically and in practice. The learning objective looks as follows:\nJDQ(Q) = [ R(s, a) + γQ(s′, a′max; θ ′)−Q(s, a; θ) ]2 , (3)\nwhere a′max = argmaxa′∈AQ(s ′, a′; θ).\n1http://atarigrandchallenge.com/data\nRecent work Hester et al. [2017] suggests an approach to imitation learning that combines the Double DQN objective and a large margin classification loss aimed at keeping a learned policy close to demonstrated behavior:\nJ(Q) = JDQ(Q) + λ1JE(Q) + λ2JL2(Q), (4)\nwhere JL2(Q) is the L2 regularization, JE(Q) is the supervised learning loss:\nJE(Q) = max a∈A\n[ Q(s, a) + l(s, aE , a) ] −Q(s, aE), (5)\nand l(s, aE , a) is the large margin classification loss that returns some positive number if the expert’s action aE 6= a and zero otherwise. The large margin classification loss prevents the learner from over-estimating Q-values for previously unseen states. The Q-values for actions taken by an expert are forced to be a margin higher than those of the unseen ones.\nAll the methods described above can also be applied completely off-line on data collected in the process of a human’s interaction with the environment."
    }, {
      "heading" : "3 Constructing the Atari Grand Challenge dataset",
      "text" : "This section details our approach to collecting the Atari Grand Challenge dataset. All described tools are made public with the data set."
    }, {
      "heading" : "3.1 Collecting the dataset",
      "text" : "We collected our dataset using a web application built around Javatari2, an Atari 2600 emulator written in JavaScript. Given an initial state and the full sequence of human inputs, the emulator is completely deterministic, and we can avoid the excessive burden of saving screenshots of the game at every single time step while it is being played. Instead, we can only record the initial state and player inputs, and generate the dataset of images offline by playback. This makes data collection at large scale feasible with limited resources.\nAfter processing, for each time step we have a screenshot of the game, the action taken at that time step, the reward, the current score, and the information about the time step being terminal or not. Since incomplete (non-terminal) episodes still carry useful information, we save the episode each time a player closes the application tab or browser window as well as when the game ends.\nAll the Atari-related functionality is entirely processed on the client side — within a browser. We support all the major web browsers: Google Chrome, Mozilla Firefox, Microsoft Edge, Safari. The server is built using Flask3. It is only responsible for saving the data and, later, for loading the data when replaying. All the data is saved in a PostgreSQL database. The replay process is automated with Selenium4.\nOur case is a good example of gamified crowdsoursing — using people’s desire to play to do useful things. In order to engage people more, we added two progress bars: one compares players’ performance with the best human player result, the other shows the same comparison with DQN performance taken from Mnih et al. [2015]."
    }, {
      "heading" : "3.2 Dataset post-processing",
      "text" : "There are two steps of dataset post-processing. First, we try to eliminate the differences between the Javatari emulator and the ALE. The only difference we have found is that the states we get in Javatari are vertically shifted by several pixels in comparison to ALE states. We eliminate this difference by shifting the states as in ALE and padding them with zeroes at the bottom and top borders.\n2http://javatari.org/ 3http://flask.pocoo.org/ 4https://github.com/SeleniumHQ\nDuring the first frames that we record, the emulated Atari memory is not fully initialized and we might get an excessively large score, which is not correct. We therefore fix the first several frames’ rewards to zero. We are not also interested in games where the person did not interact with the application more than just opening and closing it. We filter these cases by simply removing games with a final score of zero. The post-processing is fully automated, and new data can be processed with the code provided."
    }, {
      "heading" : "4 Properties of the Atari Grand Challenge dataset",
      "text" : ""
    }, {
      "heading" : "4.1 Description of the dataset",
      "text" : "In this section we briefly describe what the Atari Grand Challenge dataset consists of and we show some of its properties that we deem particularly relevant to research on learning from human demonstrations."
    }, {
      "heading" : "4.1.1 Scale",
      "text" : "The dataset consists of human replays for five popular Atari 2600 games: Video Pinball, Q*bert, Space Invaders, Ms.Pacman and Montezuma’s Revenge. The choice of the games is not random: we want to vary the level of difficulty according to the results in Mnih et al. [2015]. The DQN was able to play the first game significantly better than human players, the results for the second and the third were comparable to human performance and the latter two were very hard for DQN.\nThe Atari Grand Challenge dataset consists of 2367 game episodes with positive final score, that is ∼9.7 million frames or ∼45 hours of playing time at 60 frames per second. Table 1 shows per-game statistics of the dataset and Fig. 1 shows sample screenshots."
    }, {
      "heading" : "4.1.2 Diversity",
      "text" : "Since the data is collected in the wild, some of the players were good, some of them were bad. As a result, Fig. 2 shows that the Atari Grand Challenge dataset is quite diverse in terms of the final score distribution. From the episode final score and the time played, we can already make some assumptions about the different players’ level of expertise. What else can we do to show the player diversity quantitatively?\nIt is natural to assume that the more experienced a player is, the more effective he will be. A new player cannot achieve a challenging reward when an experienced player can. Fig. 3 shows that all the players have equal access to the rewards (at least for the games in question). From the comparison of “advanced” and “expert” groups we can see, that “expert” players are faster in achieving the rewards: the rightmost column data points look more shifted to the left. Given that the final score for the “advanced” group is higher, they achieve more in shorter periods of time."
    }, {
      "heading" : "4.1.3 Extensibility",
      "text" : "Currently the dataset comprises five Atari 2600 games, but it can be easily extended by adding a new game. We do not only publish the dataset, but the code for data collection as well5. Any of the Atari 2600 non-paddle games available in ALE Bellemare et al. [2013] can be added within few hours of work. We do not support paddle games like Breakout or Pong since it is almost impossible to exactly repeat the noisy controller to collect the states off-line."
    }, {
      "heading" : "5 Influence of data quality on the imitation learning performance",
      "text" : ""
    }, {
      "heading" : "5.1 Experiments description",
      "text" : "As shown in Section 4, we have replays of good players as well as replays of bad players. In this section we show how the dataset can be used to study how the demonstrator expertise can influence the performance of imitation learning.\nIn this experiment, we filter the training data by a minimum score. We train our model on the frames of the episodes with final score above a threshold: 50 percentile, 75 percentile (top 25% of the data) and 95 percentile (top 5%). We also train the model on the whole dataset.\nWe train the model completely off-line as in Eq. 5, but we do not use the regularization term from Hester et al. [2017] since we have more data to train on. We use λ1 = 1.0 and l = 0.8 for non-expert actions as suggested in the same paper. We use the same network architecture as in Mnih et al. [2015] and train over 106 iterations. We use the Adam Kingma and Ba [2014] optimizer with learning rate = 0.00025 and β1 = β2 = 0.95. The target network update interval is 10, 000, mini-batch size is 32, and training is run for one million updates. The code is written in Chainer Tokui et al. [2015] within the Malmopy6 framework. Since we do not have any frameskip during data collection, we use frameskip coefficient k = 1. We normalize reward values by dividing the raw rewards by the largest reward value observed in our data. There were no negative rewards in our case. The experimentation code can be found on our github page7."
    }, {
      "heading" : "5.2 Results",
      "text" : "During training we evaluate the performance of the model on 100 episodes every 100,000 mini-batch updates. After training we take the model with the best average results and re-evaluate it on 100 games and report the average score and standard error of the mean. Table 2 is in line with our hypothesis: the higher the filter value for the data, the better the performance. The sub-par performance of the imitation model on our dataset in three out of five games can be explained by looking at Table 3: our data has lower and more diverse human demonstrator scores than those of Hester et al. [2017]. At the same time, in Video Pinball, where our data has better human scores, the model performs better.\n5https://github.com/yobibyte/atarigrandchallenge/ 6https://github.com/Microsoft/malmo-challenge 7https://github.com/yobibyte/atarigrandchallenge/"
    }, {
      "heading" : "6 Related work",
      "text" : "There are two directions of RL research which are working on leveraging demonstration data for training an autonomous agent: Inverse Reinforcement Learning (IRL) and Imitation Learning. The former group addresses scenarios where there is no access to the reward function. It is true that in RL tasks the goal is often underspecified, and sometimes it is hard to provide a reward that represents all the useful information from expert’s demonstration. The general idea is to approximate the reward function and learn a policy using this approximation Ng and Russell [2000], Abbeel and Ng [2004].\nWhilst IRL can benefit from the Atari Grand Challenge dataset by ignoring the reward information, Imitation Learning is the direct benefactor of our dataset. Imitation learning exploits the reward information to learn an action-value function, or directly a policy. Schaal [1997] uses a pre-trained model to speed up training, and has an interesting comparison of pre-training influence on modelfree and model-based RL. The paper notes that model-based learning benefits more from using demonstration data. The latest work on Learning from Demonstration shows that model-free RL can also greatly benefit from using human player data Hester et al. [2017], Subramanian et al. [2016], Hosu and Rebedea [2016].\nThe datasets collected for the learning from demonstration research described above are either small, or not available for public use. The Atari Grand Challenge dataset is the largest and the most diverse in terms of the types of the games as well as amount and types of human players release so far.\nUp until now, the RL community has been mostly focusing on building the environments for training autonomous agents: ALE Bellemare et al. [2013], OpenAI’s gym Brockman et al. [2016] and Universe8, Microsoft’s Malmo project Johnson et al. [2016]. The final goal of operating in these environments is in maximizing the final score. Even if we train our models off-line, it sounds reasonable to evaluate the performance within such an environment. That is why, in Table 4 below we describe the datasets coupled with interactive environments.\n8https://github.com/openai/universe\nAtari 2600 games have recently begun to take a similar role as experimentation ground for RL research as MNIST has taken for computer vision and many implementations of RL algorithms have been evaluated on such games. Therefore, it is much easier to compare leveraging the human behavior data with pure RL implementations or even combining them."
    }, {
      "heading" : "7 Discussion and Future work",
      "text" : "Our work opens up a wide range of follow-up work on benefits and uses of human demonstrations for effectively and efficiently learning to interact with complex environments."
    }, {
      "heading" : "7.1 Extending the Atari Grand Challenge dataset",
      "text" : "The Atari Grand Challenge website is still on-line and people keep playing. We plan to update the dataset in the future as more data becomes available. The most important development of the dataset is to collect more data of “professional players” who achieve higher scores. As we have shown, the data quality affects the final performance dramatically, it will be a good improvement, when we do that."
    }, {
      "heading" : "7.2 Exploiting the Atari Grand Challenge dataset",
      "text" : "Video games are a perfect testing ground for evaluating hypotheses and learning how we can use human data to achieve higher sample efficiency and make the RL training process faster. So, our future research will focus on improving sample efficiency of RL algorithms by leveraging the data of diverse quality.\nIn this paper we have shown just one of the possible dataset applications: how data quality influences the final performance of imitation learning Hester et al. [2017]. We hope that researchers in machine learning, game AI and maybe even cognitive science, can find something useful for their own research purposes. We find the following applications particularly appealing.\nRecently, Inverse Reinforcement Learning and Imitation Learning have regained popularity Ho and Ermon [2016], Baram et al. [2016], Hester et al. [2017]. Our dataset has a direct impact on this kind of research. It is interesting to check if can we take something useful out of bad players data. Even experienced players make mistakes. But throwing out this data can waste potentially important information. Shiarlis et al. [2016] investigates this topic in the Inverse Reinforcement Learning domain. It might be interesting to see a similar approach for Atari 2600 in Learning from Demonstration domain.\nThe frameskip coefficient has been shown to be very important in RL Braylan et al. [2015], Sharma et al. [2017], Lakshminarayanan et al. [2016]. It would be interesting to investigate frameskip, using the human data that we can extract from the Atari Grand Challenge dataset.\nCurriculum learning has proven useful in RL Leibfried et al. [2016]. Having data of players with different expertise, we can investigate curriculum learning with respect to this.\nWe have also seen attempts to investigate how humans learn to play Atari Tsividis et al. [2017]. Our dataset might be interesting for this kind of research.\n9https://github.com/udacity/self-driving-car-sim\nIn conclusion, we release a dataset of human Atari 2600 replays of five games, that is ∼9.7 Million frames or ∼45 hours of game play time. We are describing its main properties, i.e. scale and diversity. We show that in order to achieve high performance, it is more important to collect data of players with a high level of expertise, than to collect a lot of low-skilled data. We plan to update the dataset in the future by adding “professional” Atari 2600 players data. We release the code for data collection as well, which gives the opportunity for everybody to extend the dataset. We also show some of the possible research directions the Atari Grand Challenge dataset could be used in. We hope that our release will catalyze the research in sample-efficiency of RL and learning from human demonstration."
    }, {
      "heading" : "Acknowledgments",
      "text" : "Vitaly Kurin would like to thank Microsoft Research Cambridge for hosting him during the project and for the Microsoft Azure for Research grant. The authors would also like to thank Paulo Peccin, Javatari creator, for the emulator and useful discussions."
    } ],
    "references" : [ {
      "title" : "Apprenticeship learning via inverse reinforcement learning",
      "author" : [ "P. Abbeel", "A.Y. Ng" ],
      "venue" : "In Proceedings of the twenty-first international conference on Machine learning,",
      "citeRegEx" : "Abbeel and Ng.,? \\Q2004\\E",
      "shortCiteRegEx" : "Abbeel and Ng.",
      "year" : 2004
    }, {
      "title" : "Model-based adversarial imitation learning",
      "author" : [ "N. Baram", "O. Anschel", "S. Mannor" ],
      "venue" : "arXiv preprint arXiv:1612.02179,",
      "citeRegEx" : "Baram et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Baram et al\\.",
      "year" : 2016
    }, {
      "title" : "The arcade learning environment: An evaluation platform for general agents",
      "author" : [ "M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Bellemare et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bellemare et al\\.",
      "year" : 2013
    }, {
      "title" : "Frame skip is a powerful parameter for learning to play atari",
      "author" : [ "A. Braylan", "M. Hollenbeck", "E. Meyerson", "R. Miikkulainen" ],
      "venue" : "In Workshops at the Twenty-Ninth AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Braylan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Braylan et al\\.",
      "year" : 2015
    }, {
      "title" : "Imagenet: A large-scale hierarchical image database",
      "author" : [ "J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Deng et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Deng et al\\.",
      "year" : 2009
    }, {
      "title" : "Switchboard: Telephone speech corpus for research and development",
      "author" : [ "J.J. Godfrey", "E.C. Holliman", "J. McDaniel" ],
      "venue" : "In Acoustics, Speech, and Signal Processing,",
      "citeRegEx" : "Godfrey et al\\.,? \\Q1992\\E",
      "shortCiteRegEx" : "Godfrey et al\\.",
      "year" : 1992
    }, {
      "title" : "Learning from demonstrations for real world reinforcement learning",
      "author" : [ "T. Hester", "M. Vecerik", "O. Pietquin", "M. Lanctot", "T. Schaul", "B. Piot", "A. Sendonaris", "G. Dulac-Arnold", "I. Osband", "J. Agapiou", "J.Z. Leibo", "A. Gruslys" ],
      "venue" : "arXiv preprint arXiv:1704.03732,",
      "citeRegEx" : "Hester et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Hester et al\\.",
      "year" : 2017
    }, {
      "title" : "Generative adversarial imitation learning",
      "author" : [ "J. Ho", "S. Ermon" ],
      "venue" : "In Neural Information Processing Systems,",
      "citeRegEx" : "Ho and Ermon.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ho and Ermon.",
      "year" : 2016
    }, {
      "title" : "Playing atari games with deep reinforcement learning and human checkpoint replay",
      "author" : [ "I.-A. Hosu", "T. Rebedea" ],
      "venue" : "arXiv preprint arXiv:1607.05077,",
      "citeRegEx" : "Hosu and Rebedea.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hosu and Rebedea.",
      "year" : 2016
    }, {
      "title" : "The malmo platform for artificial intelligence experimentation",
      "author" : [ "M. Johnson", "K. Hofmann", "T. Hutton", "D. Bignell" ],
      "venue" : "In International Joint Conference on Artificial Intelligence (IJCAI),",
      "citeRegEx" : "Johnson et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "D. Kingma", "J. Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980,",
      "citeRegEx" : "Kingma and Ba.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Dynamic frame skip deep",
      "author" : [ "A.S. Lakshminarayanan", "S. Sharma", "B. Ravindran" ],
      "venue" : "Q network. CoRR,",
      "citeRegEx" : "Lakshminarayanan et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lakshminarayanan et al\\.",
      "year" : 2016
    }, {
      "title" : "A deep learning approach for joint video frame and reward prediction in atari games",
      "author" : [ "F. Leibfried", "N. Kushman", "K. Hofmann" ],
      "venue" : "arXiv preprint arXiv:1611.07078,",
      "citeRegEx" : "Leibfried et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Leibfried et al\\.",
      "year" : 2016
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis" ],
      "venue" : "Nature, 518(7540):529–533,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2015
    }, {
      "title" : "Asynchronous methods for deep reinforcement learning",
      "author" : [ "V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T.P. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu" ],
      "venue" : "CoRR, abs/1602.01783,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2016
    }, {
      "title" : "Asynchronous data aggregation for training end to end visual control networks",
      "author" : [ "M. Monfort", "M. Johnson", "A. Oliva", "K. Hofmann" ],
      "venue" : "In International Conference on Autonomous Agents & Multiagent Systems,",
      "citeRegEx" : "Monfort et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Monfort et al\\.",
      "year" : 2017
    }, {
      "title" : "Algorithms for inverse reinforcement learning",
      "author" : [ "A.Y. Ng", "S.J. Russell" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Ng and Russell.,? \\Q2000\\E",
      "shortCiteRegEx" : "Ng and Russell.",
      "year" : 2000
    }, {
      "title" : "Learning from demonstration",
      "author" : [ "S. Schaal" ],
      "venue" : "Neural Information Processing Systems, pages 1040–1046,",
      "citeRegEx" : "Schaal.,? \\Q1997\\E",
      "shortCiteRegEx" : "Schaal.",
      "year" : 1997
    }, {
      "title" : "Trust region policy optimization",
      "author" : [ "J. Schulman", "S. Levine", "P. Moritz", "M.I. Jordan", "P. Abbeel" ],
      "venue" : "CoRR, abs/1502.05477,",
      "citeRegEx" : "Schulman et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Schulman et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning to repeat: Fine grained action repetition for deep reinforcement learning",
      "author" : [ "S. Sharma", "A.S. Lakshminarayanan", "B. Ravindran" ],
      "venue" : "CoRR, abs/1702.06054,",
      "citeRegEx" : "Sharma et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Sharma et al\\.",
      "year" : 2017
    }, {
      "title" : "Inverse reinforcement learning from failure",
      "author" : [ "K. Shiarlis", "J. Messias", "S. Whiteson" ],
      "venue" : "In International Conference on Autonomous Agents & Multiagent Systems,",
      "citeRegEx" : "Shiarlis et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Shiarlis et al\\.",
      "year" : 2016
    }, {
      "title" : "Mastering the game of go with deep neural networks and tree",
      "author" : [ "D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. Van Den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot", "S. Dieleman", "D. Grewe", "J. Nham", "N. Kalchbrenner", "I. Sutskever", "T. Lillicrap", "M. Leach", "K. Kavukcuoglu", "T. Graepel", "D. Hassabis" ],
      "venue" : "search. Nature,",
      "citeRegEx" : "Silver et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Silver et al\\.",
      "year" : 2016
    }, {
      "title" : "Exploration from demonstration for interactive reinforcement learning",
      "author" : [ "K. Subramanian", "C.L. Isbell Jr.", "A.L. Thomaz" ],
      "venue" : "In International Conference on Autonomous Agents & Multiagent Systems,",
      "citeRegEx" : "Subramanian et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Subramanian et al\\.",
      "year" : 2016
    }, {
      "title" : "Chainer: a next-generation open source framework for deep learning",
      "author" : [ "S. Tokui", "K. Oono", "S. Hido", "J. Clayton" ],
      "venue" : "In Proceedings of Workshop on Machine Learning Systems (LearningSys) in The Twenty-Ninth Annual Conference on Neural Information Processing Systems,",
      "citeRegEx" : "Tokui et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Tokui et al\\.",
      "year" : 2015
    }, {
      "title" : "Human learning in atari",
      "author" : [ "P.A. Tsividis", "T. Pouncy", "J.L. Xu", "J.B. Tenenbaum", "S.J. Gershman" ],
      "venue" : "\"The AAAI 2017 Spring Symposium on Science of Intelligence: Computational Principles of Natural and Artificial Intelligence\",",
      "citeRegEx" : "Tsividis et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Tsividis et al\\.",
      "year" : 2017
    }, {
      "title" : "Deep reinforcement learning with double q-learning",
      "author" : [ "H. Van Hasselt", "A. Guez", "D. Silver" ],
      "venue" : "In Association for the Advancement of Artificial Intelligence,",
      "citeRegEx" : "Hasselt et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hasselt et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "Recently, this research area has seen dramatic progress in complex interactive tasks in virtual environments Mnih et al. [2015, 2016], Silver et al. [2016], Schulman et al.",
      "startOffset" : 109,
      "endOffset" : 156
    }, {
      "referenceID" : 9,
      "context" : "Recently, this research area has seen dramatic progress in complex interactive tasks in virtual environments Mnih et al. [2015, 2016], Silver et al. [2016], Schulman et al. [2015], largely driven by combinations of RL with deep learning.",
      "startOffset" : 109,
      "endOffset" : 180
    }, {
      "referenceID" : 9,
      "context" : "Recently, this research area has seen dramatic progress in complex interactive tasks in virtual environments Mnih et al. [2015, 2016], Silver et al. [2016], Schulman et al. [2015], largely driven by combinations of RL with deep learning. Yet, despite this recent progress, real-world applications are still largely lacking. In the RL setup, agents learn to solve a task completely from scratch. This causes one of the key limitations of state-of-the-art deep RL approaches — data inefficiency. In comparison to autonomous agents, humans have a lot of prior information about the world. Every day, we base our decisions on knowledge about culture and social relationships, our own experience, and information we get from experience of others. As a result, when learning a new task, people are more effective and need less actions to master it. During training, the agent executes a lot of actions that a human never would. This is ineffective and renders the application of RL in complex or potentially dangerous environments infeasible. A possible solution to the problem is to learn from human demonstrations Schaal [1997], Ng and Russell [2000], Abbeel and Ng [2004], Monfort et al.",
      "startOffset" : 109,
      "endOffset" : 1124
    }, {
      "referenceID" : 9,
      "context" : "Recently, this research area has seen dramatic progress in complex interactive tasks in virtual environments Mnih et al. [2015, 2016], Silver et al. [2016], Schulman et al. [2015], largely driven by combinations of RL with deep learning. Yet, despite this recent progress, real-world applications are still largely lacking. In the RL setup, agents learn to solve a task completely from scratch. This causes one of the key limitations of state-of-the-art deep RL approaches — data inefficiency. In comparison to autonomous agents, humans have a lot of prior information about the world. Every day, we base our decisions on knowledge about culture and social relationships, our own experience, and information we get from experience of others. As a result, when learning a new task, people are more effective and need less actions to master it. During training, the agent executes a lot of actions that a human never would. This is ineffective and renders the application of RL in complex or potentially dangerous environments infeasible. A possible solution to the problem is to learn from human demonstrations Schaal [1997], Ng and Russell [2000], Abbeel and Ng [2004], Monfort et al.",
      "startOffset" : 109,
      "endOffset" : 1147
    }, {
      "referenceID" : 0,
      "context" : "A possible solution to the problem is to learn from human demonstrations Schaal [1997], Ng and Russell [2000], Abbeel and Ng [2004], Monfort et al.",
      "startOffset" : 111,
      "endOffset" : 132
    }, {
      "referenceID" : 0,
      "context" : "A possible solution to the problem is to learn from human demonstrations Schaal [1997], Ng and Russell [2000], Abbeel and Ng [2004], Monfort et al. [2017], Hester et al.",
      "startOffset" : 111,
      "endOffset" : 155
    }, {
      "referenceID" : 0,
      "context" : "A possible solution to the problem is to learn from human demonstrations Schaal [1997], Ng and Russell [2000], Abbeel and Ng [2004], Monfort et al. [2017], Hester et al. [2017]. Up until now, the RL community has been focused on building environments — test beds for RL models.",
      "startOffset" : 111,
      "endOffset" : 177
    }, {
      "referenceID" : 0,
      "context" : "A possible solution to the problem is to learn from human demonstrations Schaal [1997], Ng and Russell [2000], Abbeel and Ng [2004], Monfort et al. [2017], Hester et al. [2017]. Up until now, the RL community has been focused on building environments — test beds for RL models. Today, RL approaches can be trained and compared on diverse tasks in environments such as ALE Bellemare et al. [2013], OpenAI Gym Brockman et al.",
      "startOffset" : 111,
      "endOffset" : 396
    }, {
      "referenceID" : 0,
      "context" : "A possible solution to the problem is to learn from human demonstrations Schaal [1997], Ng and Russell [2000], Abbeel and Ng [2004], Monfort et al. [2017], Hester et al. [2017]. Up until now, the RL community has been focused on building environments — test beds for RL models. Today, RL approaches can be trained and compared on diverse tasks in environments such as ALE Bellemare et al. [2013], OpenAI Gym Brockman et al. [2016] or Microsoft’s Project Malmo Johnson et al.",
      "startOffset" : 111,
      "endOffset" : 431
    }, {
      "referenceID" : 0,
      "context" : "A possible solution to the problem is to learn from human demonstrations Schaal [1997], Ng and Russell [2000], Abbeel and Ng [2004], Monfort et al. [2017], Hester et al. [2017]. Up until now, the RL community has been focused on building environments — test beds for RL models. Today, RL approaches can be trained and compared on diverse tasks in environments such as ALE Bellemare et al. [2013], OpenAI Gym Brockman et al. [2016] or Microsoft’s Project Malmo Johnson et al. [2016]. But there are few publicly available datasets of human demonstrations ar X iv :1 70 5.",
      "startOffset" : 111,
      "endOffset" : 482
    }, {
      "referenceID" : 4,
      "context" : "Examples such as ImageNet Deng et al. [2009] in Computer Vision and Switchboard Godfrey et al.",
      "startOffset" : 26,
      "endOffset" : 45
    }, {
      "referenceID" : 4,
      "context" : "Examples such as ImageNet Deng et al. [2009] in Computer Vision and Switchboard Godfrey et al. [1992] in Speech Recognition have shown that datasets can catalyze research progress.",
      "startOffset" : 26,
      "endOffset" : 102
    }, {
      "referenceID" : 6,
      "context" : "Our experimental analysis (section 5), is based on a recently proposed imitation algorithm suggested in Hester et al. [2017], in turn based on Q-learning Watkins and Dayan [1992] and in particular DDQN Van Hasselt et al.",
      "startOffset" : 104,
      "endOffset" : 125
    }, {
      "referenceID" : 6,
      "context" : "Our experimental analysis (section 5), is based on a recently proposed imitation algorithm suggested in Hester et al. [2017], in turn based on Q-learning Watkins and Dayan [1992] and in particular DDQN Van Hasselt et al.",
      "startOffset" : 104,
      "endOffset" : 179
    }, {
      "referenceID" : 6,
      "context" : "Our experimental analysis (section 5), is based on a recently proposed imitation algorithm suggested in Hester et al. [2017], in turn based on Q-learning Watkins and Dayan [1992] and in particular DDQN Van Hasselt et al. [2016]. We next give an overview of the relevant approaches.",
      "startOffset" : 104,
      "endOffset" : 228
    }, {
      "referenceID" : 6,
      "context" : "Our experimental analysis (section 5), is based on a recently proposed imitation algorithm suggested in Hester et al. [2017], in turn based on Q-learning Watkins and Dayan [1992] and in particular DDQN Van Hasselt et al. [2016]. We next give an overview of the relevant approaches. Q-learning Watkins and Dayan [1992] is a “model-free” RL algorithm.",
      "startOffset" : 104,
      "endOffset" : 318
    }, {
      "referenceID" : 13,
      "context" : "DQN Mnih et al. [2015] is a variant of Q-learning that uses a neural network (called Deep Q-Network) to approximate Q-values.",
      "startOffset" : 4,
      "endOffset" : 23
    }, {
      "referenceID" : 13,
      "context" : "DQN Mnih et al. [2015] is a variant of Q-learning that uses a neural network (called Deep Q-Network) to approximate Q-values. The network Q(s, ·; θ) returns action values for all the actions available given the current state. A separate target network is used to compute Q-values in training updates, as well as a so-called replay memory of past experience for minibatch sampling. Both were shown to improve training stability and resulted in breakthrough results when learning to play Atari games. Double DQN Van Hasselt et al. [2016] is an extension of DQN which decouples the selection and value estimate of actions in the max operator, which was shown to result in more accurate Q-value approximations both theoretically and in practice.",
      "startOffset" : 4,
      "endOffset" : 536
    }, {
      "referenceID" : 6,
      "context" : "Recent work Hester et al. [2017] suggests an approach to imitation learning that combines the Double DQN objective and a large margin classification loss aimed at keeping a learned policy close to demonstrated behavior:",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 13,
      "context" : "In order to engage people more, we added two progress bars: one compares players’ performance with the best human player result, the other shows the same comparison with DQN performance taken from Mnih et al. [2015].",
      "startOffset" : 197,
      "endOffset" : 216
    }, {
      "referenceID" : 13,
      "context" : "The choice of the games is not random: we want to vary the level of difficulty according to the results in Mnih et al. [2015]. The DQN was able to play the first game significantly better than human players, the results for the second and the third were comparable to human performance and the latter two were very hard for DQN.",
      "startOffset" : 107,
      "endOffset" : 126
    }, {
      "referenceID" : 2,
      "context" : "Any of the Atari 2600 non-paddle games available in ALE Bellemare et al. [2013] can be added within few hours of work.",
      "startOffset" : 56,
      "endOffset" : 80
    }, {
      "referenceID" : 6,
      "context" : "5, but we do not use the regularization term from Hester et al. [2017] since we have more data to train on.",
      "startOffset" : 50,
      "endOffset" : 71
    }, {
      "referenceID" : 6,
      "context" : "5, but we do not use the regularization term from Hester et al. [2017] since we have more data to train on. We use λ1 = 1.0 and l = 0.8 for non-expert actions as suggested in the same paper. We use the same network architecture as in Mnih et al. [2015] and train over 10 iterations.",
      "startOffset" : 50,
      "endOffset" : 253
    }, {
      "referenceID" : 6,
      "context" : "5, but we do not use the regularization term from Hester et al. [2017] since we have more data to train on. We use λ1 = 1.0 and l = 0.8 for non-expert actions as suggested in the same paper. We use the same network architecture as in Mnih et al. [2015] and train over 10 iterations. We use the Adam Kingma and Ba [2014] optimizer with learning rate = 0.",
      "startOffset" : 50,
      "endOffset" : 320
    }, {
      "referenceID" : 6,
      "context" : "5, but we do not use the regularization term from Hester et al. [2017] since we have more data to train on. We use λ1 = 1.0 and l = 0.8 for non-expert actions as suggested in the same paper. We use the same network architecture as in Mnih et al. [2015] and train over 10 iterations. We use the Adam Kingma and Ba [2014] optimizer with learning rate = 0.00025 and β1 = β2 = 0.95. The target network update interval is 10, 000, mini-batch size is 32, and training is run for one million updates. The code is written in Chainer Tokui et al. [2015] within the Malmopy6 framework.",
      "startOffset" : 50,
      "endOffset" : 545
    }, {
      "referenceID" : 6,
      "context" : "The sub-par performance of the imitation model on our dataset in three out of five games can be explained by looking at Table 3: our data has lower and more diverse human demonstrator scores than those of Hester et al. [2017]. At the same time, in Video Pinball, where our data has better human scores, the model performs better.",
      "startOffset" : 205,
      "endOffset" : 226
    }, {
      "referenceID" : 6,
      "context" : "The first four rows use the offline part of the imitation algorithm (without regularization) from Hester et al. [2017] to train on our data.",
      "startOffset" : 98,
      "endOffset" : 119
    }, {
      "referenceID" : 6,
      "context" : "The first four rows use the offline part of the imitation algorithm (without regularization) from Hester et al. [2017] to train on our data. Top 5% means that the training data consists of the episodes with a final score higher or equal than the 95 percentile score. Evaluation is performed with the -greedy policy, = 0.05. Mnih et al. [2015] reports the standard deviation of the scores, but we do not have information about the number of the episodes and thus we cannot report their SEM.",
      "startOffset" : 98,
      "endOffset" : 343
    }, {
      "referenceID" : 6,
      "context" : "The first four rows use the offline part of the imitation algorithm (without regularization) from Hester et al. [2017] to train on our data. Top 5% means that the training data consists of the episodes with a final score higher or equal than the 95 percentile score. Evaluation is performed with the -greedy policy, = 0.05. Mnih et al. [2015] reports the standard deviation of the scores, but we do not have information about the number of the episodes and thus we cannot report their SEM. Space Invaders Q*bert Ms.Pacman Video Pinball Montezuma’s revenge Imitation All data 125± 9.94 146± 14.87 250± 18.09 8,823±745.26 7± 4.32 Imitation top 50% 90± 8.64 127± 13.80 308± 20.66 11,216±801.53 4± 1.97 Imitation top 25% 127± 9.69 179± 17.01 271± 22.15 24,351±2,084.38 22± 8.11 Imitation top 5% 144± 12.40 545± 107.19 418± 19.98 17,775±16.10 36± 7.98 Imitation Hester et al. [2017] n/a 5,133.",
      "startOffset" : 98,
      "endOffset" : 878
    }, {
      "referenceID" : 6,
      "context" : "The first four rows use the offline part of the imitation algorithm (without regularization) from Hester et al. [2017] to train on our data. Top 5% means that the training data consists of the episodes with a final score higher or equal than the 95 percentile score. Evaluation is performed with the -greedy policy, = 0.05. Mnih et al. [2015] reports the standard deviation of the scores, but we do not have information about the number of the episodes and thus we cannot report their SEM. Space Invaders Q*bert Ms.Pacman Video Pinball Montezuma’s revenge Imitation All data 125± 9.94 146± 14.87 250± 18.09 8,823±745.26 7± 4.32 Imitation top 50% 90± 8.64 127± 13.80 308± 20.66 11,216±801.53 4± 1.97 Imitation top 25% 127± 9.69 179± 17.01 271± 22.15 24,351±2,084.38 22± 8.11 Imitation top 5% 144± 12.40 545± 107.19 418± 19.98 17,775±16.10 36± 7.98 Imitation Hester et al. [2017] n/a 5,133.8 692.4 10,655.5 576.3 DQN Mnih et al. [2015] 1,976 10,596 2,311 42,684 0 DDQN Van Hasselt et al.",
      "startOffset" : 98,
      "endOffset" : 934
    }, {
      "referenceID" : 6,
      "context" : "The first four rows use the offline part of the imitation algorithm (without regularization) from Hester et al. [2017] to train on our data. Top 5% means that the training data consists of the episodes with a final score higher or equal than the 95 percentile score. Evaluation is performed with the -greedy policy, = 0.05. Mnih et al. [2015] reports the standard deviation of the scores, but we do not have information about the number of the episodes and thus we cannot report their SEM. Space Invaders Q*bert Ms.Pacman Video Pinball Montezuma’s revenge Imitation All data 125± 9.94 146± 14.87 250± 18.09 8,823±745.26 7± 4.32 Imitation top 50% 90± 8.64 127± 13.80 308± 20.66 11,216±801.53 4± 1.97 Imitation top 25% 127± 9.69 179± 17.01 271± 22.15 24,351±2,084.38 22± 8.11 Imitation top 5% 144± 12.40 545± 107.19 418± 19.98 17,775±16.10 36± 7.98 Imitation Hester et al. [2017] n/a 5,133.8 692.4 10,655.5 576.3 DQN Mnih et al. [2015] 1,976 10,596 2,311 42,684 0 DDQN Van Hasselt et al. [2016] 2,628.",
      "startOffset" : 98,
      "endOffset" : 993
    }, {
      "referenceID" : 6,
      "context" : "Table 3: Comparison of human scores in Hester et al. [2017] and the Atari Grand Challenge dataset.",
      "startOffset" : 39,
      "endOffset" : 60
    }, {
      "referenceID" : 6,
      "context" : "Table 3: Comparison of human scores in Hester et al. [2017] and the Atari Grand Challenge dataset. Hester et al. [2017] Atari Grand Challenge dataset Worst score Best score #transitions Worst score Best score #transitions Space Invaders n/a 5 3355 2,056,741 Q*bert 80700 99450 75472 25 41425 1,599,453 Ms.",
      "startOffset" : 39,
      "endOffset" : 120
    }, {
      "referenceID" : 11,
      "context" : "The general idea is to approximate the reward function and learn a policy using this approximation Ng and Russell [2000], Abbeel and Ng [2004].",
      "startOffset" : 99,
      "endOffset" : 121
    }, {
      "referenceID" : 0,
      "context" : "The general idea is to approximate the reward function and learn a policy using this approximation Ng and Russell [2000], Abbeel and Ng [2004]. Whilst IRL can benefit from the Atari Grand Challenge dataset by ignoring the reward information, Imitation Learning is the direct benefactor of our dataset.",
      "startOffset" : 122,
      "endOffset" : 143
    }, {
      "referenceID" : 0,
      "context" : "The general idea is to approximate the reward function and learn a policy using this approximation Ng and Russell [2000], Abbeel and Ng [2004]. Whilst IRL can benefit from the Atari Grand Challenge dataset by ignoring the reward information, Imitation Learning is the direct benefactor of our dataset. Imitation learning exploits the reward information to learn an action-value function, or directly a policy. Schaal [1997] uses a pre-trained model to speed up training, and has an interesting comparison of pre-training influence on modelfree and model-based RL.",
      "startOffset" : 122,
      "endOffset" : 424
    }, {
      "referenceID" : 0,
      "context" : "The general idea is to approximate the reward function and learn a policy using this approximation Ng and Russell [2000], Abbeel and Ng [2004]. Whilst IRL can benefit from the Atari Grand Challenge dataset by ignoring the reward information, Imitation Learning is the direct benefactor of our dataset. Imitation learning exploits the reward information to learn an action-value function, or directly a policy. Schaal [1997] uses a pre-trained model to speed up training, and has an interesting comparison of pre-training influence on modelfree and model-based RL. The paper notes that model-based learning benefits more from using demonstration data. The latest work on Learning from Demonstration shows that model-free RL can also greatly benefit from using human player data Hester et al. [2017], Subramanian et al.",
      "startOffset" : 122,
      "endOffset" : 798
    }, {
      "referenceID" : 0,
      "context" : "The general idea is to approximate the reward function and learn a policy using this approximation Ng and Russell [2000], Abbeel and Ng [2004]. Whilst IRL can benefit from the Atari Grand Challenge dataset by ignoring the reward information, Imitation Learning is the direct benefactor of our dataset. Imitation learning exploits the reward information to learn an action-value function, or directly a policy. Schaal [1997] uses a pre-trained model to speed up training, and has an interesting comparison of pre-training influence on modelfree and model-based RL. The paper notes that model-based learning benefits more from using demonstration data. The latest work on Learning from Demonstration shows that model-free RL can also greatly benefit from using human player data Hester et al. [2017], Subramanian et al. [2016], Hosu and Rebedea [2016].",
      "startOffset" : 122,
      "endOffset" : 825
    }, {
      "referenceID" : 0,
      "context" : "The general idea is to approximate the reward function and learn a policy using this approximation Ng and Russell [2000], Abbeel and Ng [2004]. Whilst IRL can benefit from the Atari Grand Challenge dataset by ignoring the reward information, Imitation Learning is the direct benefactor of our dataset. Imitation learning exploits the reward information to learn an action-value function, or directly a policy. Schaal [1997] uses a pre-trained model to speed up training, and has an interesting comparison of pre-training influence on modelfree and model-based RL. The paper notes that model-based learning benefits more from using demonstration data. The latest work on Learning from Demonstration shows that model-free RL can also greatly benefit from using human player data Hester et al. [2017], Subramanian et al. [2016], Hosu and Rebedea [2016]. The datasets collected for the learning from demonstration research described above are either small, or not available for public use.",
      "startOffset" : 122,
      "endOffset" : 850
    }, {
      "referenceID" : 0,
      "context" : "The general idea is to approximate the reward function and learn a policy using this approximation Ng and Russell [2000], Abbeel and Ng [2004]. Whilst IRL can benefit from the Atari Grand Challenge dataset by ignoring the reward information, Imitation Learning is the direct benefactor of our dataset. Imitation learning exploits the reward information to learn an action-value function, or directly a policy. Schaal [1997] uses a pre-trained model to speed up training, and has an interesting comparison of pre-training influence on modelfree and model-based RL. The paper notes that model-based learning benefits more from using demonstration data. The latest work on Learning from Demonstration shows that model-free RL can also greatly benefit from using human player data Hester et al. [2017], Subramanian et al. [2016], Hosu and Rebedea [2016]. The datasets collected for the learning from demonstration research described above are either small, or not available for public use. The Atari Grand Challenge dataset is the largest and the most diverse in terms of the types of the games as well as amount and types of human players release so far. Up until now, the RL community has been mostly focusing on building the environments for training autonomous agents: ALE Bellemare et al. [2013], OpenAI’s gym Brockman et al.",
      "startOffset" : 122,
      "endOffset" : 1297
    }, {
      "referenceID" : 0,
      "context" : "The general idea is to approximate the reward function and learn a policy using this approximation Ng and Russell [2000], Abbeel and Ng [2004]. Whilst IRL can benefit from the Atari Grand Challenge dataset by ignoring the reward information, Imitation Learning is the direct benefactor of our dataset. Imitation learning exploits the reward information to learn an action-value function, or directly a policy. Schaal [1997] uses a pre-trained model to speed up training, and has an interesting comparison of pre-training influence on modelfree and model-based RL. The paper notes that model-based learning benefits more from using demonstration data. The latest work on Learning from Demonstration shows that model-free RL can also greatly benefit from using human player data Hester et al. [2017], Subramanian et al. [2016], Hosu and Rebedea [2016]. The datasets collected for the learning from demonstration research described above are either small, or not available for public use. The Atari Grand Challenge dataset is the largest and the most diverse in terms of the types of the games as well as amount and types of human players release so far. Up until now, the RL community has been mostly focusing on building the environments for training autonomous agents: ALE Bellemare et al. [2013], OpenAI’s gym Brockman et al. [2016] and Universe8, Microsoft’s Malmo project Johnson et al.",
      "startOffset" : 122,
      "endOffset" : 1334
    }, {
      "referenceID" : 0,
      "context" : "The general idea is to approximate the reward function and learn a policy using this approximation Ng and Russell [2000], Abbeel and Ng [2004]. Whilst IRL can benefit from the Atari Grand Challenge dataset by ignoring the reward information, Imitation Learning is the direct benefactor of our dataset. Imitation learning exploits the reward information to learn an action-value function, or directly a policy. Schaal [1997] uses a pre-trained model to speed up training, and has an interesting comparison of pre-training influence on modelfree and model-based RL. The paper notes that model-based learning benefits more from using demonstration data. The latest work on Learning from Demonstration shows that model-free RL can also greatly benefit from using human player data Hester et al. [2017], Subramanian et al. [2016], Hosu and Rebedea [2016]. The datasets collected for the learning from demonstration research described above are either small, or not available for public use. The Atari Grand Challenge dataset is the largest and the most diverse in terms of the types of the games as well as amount and types of human players release so far. Up until now, the RL community has been mostly focusing on building the environments for training autonomous agents: ALE Bellemare et al. [2013], OpenAI’s gym Brockman et al. [2016] and Universe8, Microsoft’s Malmo project Johnson et al. [2016]. The final goal of operating in these environments is in maximizing the final score.",
      "startOffset" : 122,
      "endOffset" : 1397
    }, {
      "referenceID" : 7,
      "context" : "The replay data for Hosu and Rebedea [2016] has not been published, but there are Montezuma’s Revenge and Private Eye checkpoints — saved states of the environment that can be used for continuing the episode.",
      "startOffset" : 20,
      "endOffset" : 44
    }, {
      "referenceID" : 7,
      "context" : "The replay data for Hosu and Rebedea [2016] has not been published, but there are Montezuma’s Revenge and Private Eye checkpoints — saved states of the environment that can be used for continuing the episode. Domain Tasks Size (transitions) Open Diverse in player expertise Atari Grand Challenge Atari 2600 5 ∼9.7 mil. 3 3 Udacity self-driving dataset9 Driving simulator 1 8086 (x3 cameras) 3 7 Hosu and Rebedea [2016] Atari 2600 1 ∼1.",
      "startOffset" : 20,
      "endOffset" : 419
    }, {
      "referenceID" : 6,
      "context" : "(7) 7 Hester et al. [2017] Atari 2600 42 ∼1 mil.",
      "startOffset" : 6,
      "endOffset" : 27
    }, {
      "referenceID" : 4,
      "context" : "In this paper we have shown just one of the possible dataset applications: how data quality influences the final performance of imitation learning Hester et al. [2017]. We hope that researchers in machine learning, game AI and maybe even cognitive science, can find something useful for their own research purposes.",
      "startOffset" : 147,
      "endOffset" : 168
    }, {
      "referenceID" : 4,
      "context" : "In this paper we have shown just one of the possible dataset applications: how data quality influences the final performance of imitation learning Hester et al. [2017]. We hope that researchers in machine learning, game AI and maybe even cognitive science, can find something useful for their own research purposes. We find the following applications particularly appealing. Recently, Inverse Reinforcement Learning and Imitation Learning have regained popularity Ho and Ermon [2016], Baram et al.",
      "startOffset" : 147,
      "endOffset" : 484
    }, {
      "referenceID" : 1,
      "context" : "Recently, Inverse Reinforcement Learning and Imitation Learning have regained popularity Ho and Ermon [2016], Baram et al. [2016], Hester et al.",
      "startOffset" : 110,
      "endOffset" : 130
    }, {
      "referenceID" : 1,
      "context" : "Recently, Inverse Reinforcement Learning and Imitation Learning have regained popularity Ho and Ermon [2016], Baram et al. [2016], Hester et al. [2017]. Our dataset has a direct impact on this kind of research.",
      "startOffset" : 110,
      "endOffset" : 152
    }, {
      "referenceID" : 1,
      "context" : "Recently, Inverse Reinforcement Learning and Imitation Learning have regained popularity Ho and Ermon [2016], Baram et al. [2016], Hester et al. [2017]. Our dataset has a direct impact on this kind of research. It is interesting to check if can we take something useful out of bad players data. Even experienced players make mistakes. But throwing out this data can waste potentially important information. Shiarlis et al. [2016] investigates this topic in the Inverse Reinforcement Learning domain.",
      "startOffset" : 110,
      "endOffset" : 430
    }, {
      "referenceID" : 1,
      "context" : "Recently, Inverse Reinforcement Learning and Imitation Learning have regained popularity Ho and Ermon [2016], Baram et al. [2016], Hester et al. [2017]. Our dataset has a direct impact on this kind of research. It is interesting to check if can we take something useful out of bad players data. Even experienced players make mistakes. But throwing out this data can waste potentially important information. Shiarlis et al. [2016] investigates this topic in the Inverse Reinforcement Learning domain. It might be interesting to see a similar approach for Atari 2600 in Learning from Demonstration domain. The frameskip coefficient has been shown to be very important in RL Braylan et al. [2015], Sharma et al.",
      "startOffset" : 110,
      "endOffset" : 694
    }, {
      "referenceID" : 1,
      "context" : "Recently, Inverse Reinforcement Learning and Imitation Learning have regained popularity Ho and Ermon [2016], Baram et al. [2016], Hester et al. [2017]. Our dataset has a direct impact on this kind of research. It is interesting to check if can we take something useful out of bad players data. Even experienced players make mistakes. But throwing out this data can waste potentially important information. Shiarlis et al. [2016] investigates this topic in the Inverse Reinforcement Learning domain. It might be interesting to see a similar approach for Atari 2600 in Learning from Demonstration domain. The frameskip coefficient has been shown to be very important in RL Braylan et al. [2015], Sharma et al. [2017], Lakshminarayanan et al.",
      "startOffset" : 110,
      "endOffset" : 716
    }, {
      "referenceID" : 1,
      "context" : "Recently, Inverse Reinforcement Learning and Imitation Learning have regained popularity Ho and Ermon [2016], Baram et al. [2016], Hester et al. [2017]. Our dataset has a direct impact on this kind of research. It is interesting to check if can we take something useful out of bad players data. Even experienced players make mistakes. But throwing out this data can waste potentially important information. Shiarlis et al. [2016] investigates this topic in the Inverse Reinforcement Learning domain. It might be interesting to see a similar approach for Atari 2600 in Learning from Demonstration domain. The frameskip coefficient has been shown to be very important in RL Braylan et al. [2015], Sharma et al. [2017], Lakshminarayanan et al. [2016]. It would be interesting to investigate frameskip, using the human data that we can extract from the Atari Grand Challenge dataset.",
      "startOffset" : 110,
      "endOffset" : 748
    }, {
      "referenceID" : 1,
      "context" : "Recently, Inverse Reinforcement Learning and Imitation Learning have regained popularity Ho and Ermon [2016], Baram et al. [2016], Hester et al. [2017]. Our dataset has a direct impact on this kind of research. It is interesting to check if can we take something useful out of bad players data. Even experienced players make mistakes. But throwing out this data can waste potentially important information. Shiarlis et al. [2016] investigates this topic in the Inverse Reinforcement Learning domain. It might be interesting to see a similar approach for Atari 2600 in Learning from Demonstration domain. The frameskip coefficient has been shown to be very important in RL Braylan et al. [2015], Sharma et al. [2017], Lakshminarayanan et al. [2016]. It would be interesting to investigate frameskip, using the human data that we can extract from the Atari Grand Challenge dataset. Curriculum learning has proven useful in RL Leibfried et al. [2016]. Having data of players with different expertise, we can investigate curriculum learning with respect to this.",
      "startOffset" : 110,
      "endOffset" : 948
    }, {
      "referenceID" : 1,
      "context" : "Recently, Inverse Reinforcement Learning and Imitation Learning have regained popularity Ho and Ermon [2016], Baram et al. [2016], Hester et al. [2017]. Our dataset has a direct impact on this kind of research. It is interesting to check if can we take something useful out of bad players data. Even experienced players make mistakes. But throwing out this data can waste potentially important information. Shiarlis et al. [2016] investigates this topic in the Inverse Reinforcement Learning domain. It might be interesting to see a similar approach for Atari 2600 in Learning from Demonstration domain. The frameskip coefficient has been shown to be very important in RL Braylan et al. [2015], Sharma et al. [2017], Lakshminarayanan et al. [2016]. It would be interesting to investigate frameskip, using the human data that we can extract from the Atari Grand Challenge dataset. Curriculum learning has proven useful in RL Leibfried et al. [2016]. Having data of players with different expertise, we can investigate curriculum learning with respect to this. We have also seen attempts to investigate how humans learn to play Atari Tsividis et al. [2017]. Our dataset might be interesting for this kind of research.",
      "startOffset" : 110,
      "endOffset" : 1155
    } ],
    "year" : 2017,
    "abstractText" : "Recent progress in Reinforcement Learning (RL), fueled by its combination, with Deep Learning has enabled impressive results in learning to interact with complex virtual environments, yet real-world applications of RL are still scarce. A key limitation is data efficiency, with current state-of-the-art approaches requiring millions of training samples. A promising way to tackle this problem is to augment RL with learning from human demonstrations. However, human demonstration data is not yet readily available. This hinders progress in this direction. The present work addresses this problem as follows. We (i) collect and describe a large dataset of human Atari 2600 replays – the largest and most diverse such data set publicly released to date (ii) illustrate an example use of this dataset by analyzing the relation between demonstration quality and imitation learning performance, and (iii) outline possible research directions that are opened up by our work.",
    "creator" : "LaTeX with hyperref package"
  }
}