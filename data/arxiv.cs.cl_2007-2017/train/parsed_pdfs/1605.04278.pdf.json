{
  "name" : "1605.04278.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Jing Xian Wang", "Keiko Sophie Mori" ],
    "emails" : [ "berzak@mit.edu", "jessk@mit.edu", "cspadine@mit.edu", "jhwang@mit.edu", "lucci@mit.edu", "ksmori@mit.edu", "sjgarza@mit.edu", "boris@mit.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 5.\n04 27\n8v 1\n[ cs\n.C L\n] 1\n3 M"
    }, {
      "heading" : "1 Introduction",
      "text" : "The majority of the English text available worldwide is generated by non-native speakers (Crystal, 2003). Such texts introduce a variety of challenges, most notably grammatical errors, and are of paramount importance for the scientific study of language acquisition as well as for NLP. Despite the ubiquity of non-native English, there is\n1The treebank is released through universaldependencies.org. A web search interface is available at esltreebank.org.\ncurrently no publicly available syntactic treebank for English as a Second Language (ESL).\nTo address this shortcoming, we present the Treebank of Learner English (TLE), a first of its kind resource for non-native English, containing 5,124 sentences manually annotated with POS tags and dependency trees. The TLE sentences are drawn from the FCE dataset (Yannakoudakis et al., 2011), and authored by English learners from 10 different native language backgrounds. The treebank uses the Universal Dependencies (UD) formalism (De Marneffe et al., 2014), which provides a unified annotation framework across different languages and is geared towards multilingual NLP (McDonald et al., 2013). This characteristic allows our treebank to support computational analysis of ESL using not only English based but also multilingual approaches which seek to relate ESL phenomena to native language syntax.\nWhile the annotation inventory and guidelines are defined by the English UD formalism, we build on previous work in learner language analysis (Dıaz-Negrillo et al., 2010; Dickinson and Ragheb, 2013) to formulate an additional set of annotation conventions aiming at a uniform treatment of ungrammatical learner language. Our annotation scheme uses a two-layer analysis, whereby a distinct syntactic annotation is provided for the original and the corrected version of each sentence. This approach is enabled by a pre-existing error annotation of the FCE (Nicholls, 2003) which is used to generate an error corrected variant of the dataset. Our inter-annotator agreement results provide evidence for the ability of the annotation scheme to support consistent annotation of ungrammatical structures.\nFinally, a corpus that is annotated with gram-\nmatical errors as well as dependencies paves the way for empirical studies on the relations between grammaticality and syntax. In the context of NLP, such studies yield data needed for improving tagging and parsing performance on ungrammatical language (Geertzen et al., 2013), as well as utilizing syntactic structure for grammatical error correction (Tetreault et al., 2010; Ng et al., 2014). In this work, we take the first step in this direction by measuring tagging and parsing accuracy on our dataset under different training regimes, and obtaining several estimates for the impact of grammatical errors on these tasks.\nTo summarize, this paper presents three contributions. First, we introduce the first large syntactic treebank for ESL, manually annotated with POS tags and universal dependencies. Second, we describe a linguistically motivated annotation scheme for ungrammatical English and provide empirical support for its consistency via interannotator agreement analysis. Finally, we benchmark a state of the art parser on our dataset and estimate the influence of grammatical errors on the accuracy of automatic POS tagging and dependency parsing.\nThe remainder of this paper is structured as follows. We start by presenting an overview of the treebank in section 2. In sections 3 and 4 we provide background information on the annotation project, and review the main annotation stages leading to the current form of the dataset. The ESL annotation guidelines are summarized in section 5. Inter-annotator agreement analysis is presented in section 6, followed by parsing benchmarks in section 7. Finally, we review related work in section 8 and present the conclusion in section 9."
    }, {
      "heading" : "2 Treebank Overview",
      "text" : "The TLE currently contains 5,124 sentences (97,683 tokens) with POS tag and dependency annotations in the English Universal Dependencies (UD) formalism (De Marneffe et al., 2014). The sentences were obtained from the FCE corpus (Yannakoudakis et al., 2011), a collection of upper intermediate English learner essays, containing error annotations with 75 error categories (Nicholls, 2003). Sentence level segmentation was performed using an adaptation of the NLTK sentence tokenizer2 . Under-segmented sentences were split further manually in the course of the an-\n2http://www.nltk.org/api/nltk.tokenize.html\nnotation. Word level tokenization was generated using the Stanford PTB word tokenizer3 .\nThe treebank represents learners with 10 different native language backgrounds: Chinese, French, German, Italian, Japanese, Korean, Portuguese, Spanish, Russian and Turkish. For every native language, we randomly sampled 500 automatically segmented sentences, under the constraint that selected sentences have to contain at least one grammatical error that is not punctuation or spelling.\nThe TLE annotations are provided in two versions. The first version is the original sentence authored by the learner, containing grammatical errors. The second, corrected sentence version, is a grammatical variant of the original sentence, generated by correcting all the grammatical errors in the sentence according to the manual error annotation provided in the FCE dataset. The resulting corrected sentences constitute a parallel corpus of standard English. Table 1 presents basic statistics of both versions of the annotated sentences.\nTo avoid potential annotation biases, the annotations of the treebank were created manually from scratch, without utilizing any automatic annotation tools. To further assure annotation quality, each annotated sentence was reviewed by two additional annotators. To the best of our knowledge, TLE is the first large scale English treebank constructed in a completely manual fashion."
    }, {
      "heading" : "3 Annotator Training",
      "text" : "The treebank was annotated by six students, five of whom are undergraduates and one is a graduate student. Among the undergraduates, three are linguistics majors and two are engineering majors with a linguistic minor. The graduate student is a linguist specializing in syntax.\nPrior to annotating the treebank sentences, the annotators went through a training period of about\n3http://nlp.stanford.edu/software/tokenizer.shtml\n8 weeks. During the training, the annotators attended tutorials on dependency grammars, and learned the UD guidelines4 , the Penn Treebank POS guidelines (Santorini, 1990), the grammatical error annotation scheme of the FCE (Nicholls, 2003), as well as the ESL guidelines presented in section 5.\nFurthermore, the annotators completed six annotation exercises, in which they were required to annotate POS tags and dependencies for practice sentences. The exercises were done individually, and followed by group meetings in which all the annotation disagreements were discussed and resolved. The first three exercises consisted of 20 sentences from the UD gold standard for English, the English Web Treebank (EWT) (Silveira et al., 2014). Each of the remaining three exercises contained 20-30 ESL sentences from the FCE. Many of the ESL guidelines were introduced or refined based on the disagreements in the ESL practice exercises and the subsequent group discussions. Several additional guidelines were introduced in the course of the annotation process.\nDuring the training period, the annotators learned to use a search tool that enables formulating queries over word and POS tag sequences as regular expressions and obtaining their annotation statistics in the training portion of the EWT. After experimenting with both textual and graphical interfaces for performing the annotations, we converged on a simple text based format described in section 4.1, where the annotations were filled in using a spreadsheet or a text editor, and tested with a script for detecting annotation typos."
    }, {
      "heading" : "4 Annotation Procedure",
      "text" : "The formation of the treebank was carried out in four steps: annotation, review, disagreement resolution and targeted debugging."
    }, {
      "heading" : "4.1 Annotation",
      "text" : "In the first stage, the annotators were given sentences for annotation. We use a CoNLL style textual template in which each word is annotated in a separate line. Each line contains 6 columns, the first of which has the word index (IND) and the second the word itself (WORD). The remaining four columns had to be filled in with a Universal POS tag5 (UPOS), a Penn Treebank POS tag\n4http://universaldependencies.org/#en 5http://universaldependencies.org/en/pos/all.html\n(POS), a head word index (HIND) and a dependency relation6 (REL) according to version 1 of the English UD guidelines.\nThe annotation section of the sentence is preceded by a metadata header. The first field in this header, denoted with SENT, contains the FCE error coded version of the sentence. The annotators were instructed to verify the error annotation, and add new error annotations if needed. Corrections to the sentence segmentation are specified in the SEGMENT field7. Further down, the field TYPO is designated for literal annotation of spelling errors and ill formed words that happen to form valid words (see section 5.2).\nThe example below presents a pre-annotated original sentence given to an annotator.\n#SENT=That time I had to sleep in <ns type= \"MD\"><c>a</c></ns> tent. #SEGMENT= #TYPO=\n#IND WORD UPOS POS HIND REL 1 That 2 time 3 I 4 had 5 to 6 sleep 7 in 8 tent 9 .\nUpon completion of the original sentence, the annotators proceeded to annotate the corrected sentence version. To reduce annotation time, annotators used a script that copies over annotations from the original sentence and updates head indices of tokens that appear in both sentence versions. Head indices and relation labels were filled in only if the head word of the token appeared in both the original and corrected sentence versions. Tokens with automatically filled annotations included an additional # sign in a seventh column of each word’s annotation. The # signs had to be removed, and the corresponding annotations either approved or changed as appropriate. Tokens that did not appear in the original sentence version were annotated from scratch."
    }, {
      "heading" : "4.2 Review",
      "text" : "All annotated sentences were randomly assigned to a second annotator (henceforth reviewer), in a double blind manner. The reviewer’s task was to\n6http://universaldependencies.org/en/dep/all.html 7The released version of the treebank splits the sentences according to the markings in the SEGMENT field when those apply both to the original and corrected versions of the sentence. Resulting segments without grammatical errors in the original version are currently discarded.\nmark all the annotations that they would have annotated differently. To assist the review process, we compiled a list of common annotation errors, available in the released annotation manual.\nThe annotations were reviewed using an active editing scheme in which an explicit action was required for all the existing annotations. The scheme was introduced to prevent reviewers from overlooking annotation issues due to passive approval. Specifically, an additional # sign was added at the seventh column of each token’s annotation. The reviewer then had to either “sign off” on the existing annotation by erasing the # sign, or provide an alternative annotation for the token following the # sign."
    }, {
      "heading" : "4.3 Disagreement Resolution",
      "text" : "In the final stage of the annotation process all annotator-reviewer disagreements were resolved by a third annotator (henceforth judge), whose main task was to decide in favor of the annotator or the reviewer. Similarly to the review process, the judging task was carried out in a double blind manner. Judges were allowed to resolve annotatorreviewer disagreements with a third alternative, as well as introduce new corrections for annotation issues overlooked by reviewers.\nAn additional task performed by the judges was to mark acceptable alternative annotations for ambiguous structures determined through review disagreements or otherwise present in the sentence. These annotations were specified in an additional metada field called #AMBIGUITY. The ambiguity markings are provided along with the resolved version of the annotations."
    }, {
      "heading" : "4.4 Final Debugging",
      "text" : "After applying the resolutions produced by the judges, we queried the corpus with debugging tests for specific linguistics constructions. This additional testing phase further reduced the number of annotation errors and inconsistencies in the treebank. Including the training period, the treebank creation lasted over a year, with an aggregate of over 2,000 hours of annotation."
    }, {
      "heading" : "5 Annotation Scheme for ESL",
      "text" : "Our annotations use the existing inventory of English UD POS tags and dependency relations, and follow the standard UD annotation guidelines for English. However, these guidelines were for-\nmulated with grammatical usage of English in mind and do not cover non canonical syntactic structures arising due to grammatical errors8. To encourage consistent and linguistically motivated annotation of such structures, we formulated a complementary set of ESL annotation guidelines.\nOur annotation guidelines for the original sentences follow the general principle of literal reading, which emphasizes syntactic analysis according to the observed language usage. This strategy continues a line of work in SLA which advocates for centering analysis of learner language around morpho-syntactic surface evidence (Ragheb and Dickinson, 2012; Dickinson and Ragheb, 2013). Similarly to our framework, which includes a parallel annotation of corrected sentences, such strategies are often presented in the context of multi-layer annotation schemes that also account for error corrected sentence forms (Hirschmann et al., 2007; Dıaz-Negrillo et al., 2010; Rosen et al., 2014).\nDeploying a strategy of literal annotation within UD, a formalism which enforces cross-linguistic consistency of annotations, is designed to support cross-lingual studies of learner language, by enabling meaningful comparisons between noncanonical structures in English and canonical structures in the author’s native language."
    }, {
      "heading" : "5.1 Literal Annotation",
      "text" : "With respect to POS tagging, literal annotation implies adhering as much as possible to the observed morphological forms of the words. Syntactically, argument structure is annotated according to the usage of the word rather than its typical distribution in the relevant context. The following list of conventions defines the notion of literal reading for some of the common non canonical structures associated with grammatical errors.\nArgument Structure\nExtraneous prepositions We annotate all nominal dependents introduced by extraneous prepositions as nominal modifiers. In the following sentence, “him” is marked as a nominal modifier (nmod) instead of an indirect object (iobj) of “give”.\n#SENT=...I had to give <ns type=\"UT\"><i>to</i> </ns> him water...\n... 21 I PRON PRP 22 nsubj\n8The English UD guidelines do address several issues encountered in informal genres, such as the relation “goeswith”, which is used for fragmented words resulting from typos.\n22 had VERB VBD 5 parataxis 23 to PART TO 24 mark 24 give VERB VB 22 xcomp 25 to ADP IN 26 case 26 him PRON PRP 24 nmod 27 water NOUN NN 24 dobj ...\nOmitted prepositions We treat nominal dependents of a predicate that are lacking a preposition as arguments rather than nominal modifiers. In the example below, “money” is marked as a direct object (dobj) instead of a nominal modifier (nmod) of “ask”. As “you” functions in this context as a second argument of “ask”, it is annotated as an indirect object (iobj) instead of a direct object (dobj).\n#SENT=...I have to ask you <ns type=\"MT\"> <c>for</c></ns> the money <ns type= \"RT\"> <i>of</i><c>for</c></ns> the tickets back.\n... 12 I PRON PRP 13 nsubj 13 have VERB VBP 2 conj 14 to PART TO 15 mark 15 ask VERB VB 13 xcomp 16 you PRON PRP 15 iobj 17 the DET DT 18 det 18 money NOUN NN 15 dobj 19 of ADP IN 21 case 20 the DET DT 21 det 21 tickets NOUN NNS 18 nmod 22 back ADV RB 15 advmod 23 . PUNCT . 2 punct\nTense\nCases of erroneous tense usage are annotated according to the morphological tense of the verb. For example, below we annotate “shopping” with present participle VBG, while the correction “shop” is annotated in the corrected version of the sentence as VBP.\n#SENT=...when you <ns type=\"TV\"><i>shopping</i> <c>shop</c></ns>...\n... 4 when ADV WRB 6 advmod 5 you PRON PRP 6 nsubj 6 shopping VERB VBG 12 advcl ...\nWord Formation\nErroneous word formations that are contextually plausible and can be assigned with an existing PTB tag are annotated literally. In the following example, “stuffs” is handled as a plural count noun.\n#SENT=...into fashionable <ns type=\"CN\"> <i>stuffs</i><c>stuff</c></ns>...\n... 7 into ADP IN 9 case 8 fashionable ADJ JJ 9 amod 9 stuffs NOUN NNS 2 ccomp ...\nSimilarly, in the example below we annotate “necessaryiest” as a superlative.\n#SENT=The necessaryiest things...\n1 The DET DT 3 det 2 necessaryiest ADJ JJS 3 amod 3 things NOUN NNS 0 root ..."
    }, {
      "heading" : "5.2 Exceptions to Literal Annotation",
      "text" : "Although our general annotation strategy for ESL follows literal sentence readings, several types of word formation errors make such readings uninformative or impossible, essentially forcing certain words to be annotated using some degree of interpretation (Rosén and De Smedt, 2010). We hence annotate the following cases in the original sentence according to an interpretation of an intended word meaning, obtained from the FCE error correction.\nSpelling\nSpelling errors are annotated according to the correctly spelled version of the word. To support error analysis of automatic annotation tools, misspelled words that happen to form valid words are annotated in the metadata field TYPO for POS tags with respect to the most common usage of the misspelled word form. In the example below, the TYPO field contains the typical POS annotation of “where”, which is clearly unintended in the context of the sentence.\n#SENT=...we <ns type=\"SX\"><i>where</i> <c>were</c></ns> invited to visit... #TYPO=5 ADV WRB ... 4 we PRON PRP 6 nsubjpass 5 where AUX VBD 6 auxpass 6 invited VERB VBN 0 root 7 to PART TO 8 mark 8 visit VERB VB 6 xcomp ...\nWord Formation\nErroneous word formations that cannot be assigned with an existing PTB tag are annotated with respect to the correct word form.\n#SENT=I am <ns type=\"IV\"><i>writting</i> <c>writing</c></ns>... 1 I PRON PRP 3 nsubj 2 am AUX VBP 3 aux 3 writting VERB VBG 0 root ...\nIn particular, ill formed adjectives that have a plural suffix receive a standard adjectival POS tag. Such cases also receive an additional marking for unnecessary agreement in the error annotation using the attribute “ua”.\n#SENT=...<ns type=\"IJ\" ua=true> <i>interestings</i><c>interesting</c></ns> things... ... 6 interestings ADJ JJ 7 amod 7 things NOUN NNS 3 dobj ...\nWrong word formations that result in a valid, but contextually implausible word form are also annotated according to the word correction. In the example below, the nominal form “sale” is likely to be an unintended result of an ill formed verb. Similarly to spelling errors that result in valid words, we mark the typical literal POS annotation in the TYPO metadata field.\n#SENT=...they do not <ns type=\"DV\"><i>sale</i> <c>sell</c></ns> them... #TYPO=15 NOUN NN\n... 12 they PRON PRP 15 nsubj 13 do AUX VBP 15 aux 14 not PART RB 15 neg 15 sale VERB VB 0 root 16 them PRON PRP 15 dobj ...\nTaken together, our ESL conventions cover many of the annotation challenges related to grammatical errors present in the TLE. In addition to the presented overview, the complete manual of ESL guidelines used by the annotators will be made publicly available. The manual contains further details on our annotation scheme, additional annotation guidelines and a list of common annotation errors. We plan to extend and refine these guidelines in future releases of the treebank."
    }, {
      "heading" : "6 Editing Agreement",
      "text" : "We utilize our two step review process to estimate agreement rates between annotators9 . We measure agreement as the fraction of annotation tokens approved by the editor. Table 2 presents the agreement between annotators and reviewers, as well as the agreement between reviewers and the final version of the annotations produced by the judges. Agreement measurements are provided for both the original the corrected versions of the dataset.\nOverall, the results indicate a high agreement rate in the two editing tasks. Furthermore, the gap between the agreement on the original and corrected sentences is small. Note that this result is obtained despite the introduction of several ESL annotation guidelines in the course of the annotation process, which inevitably increased the number of edits related to grammatical errors. We interpret this outcome as evidence for the effectiveness of the ESL annotation scheme in supporting consistent annotations of ungrammatical language.\n9All experimental results on agreement and parsing exclude punctuation tokens."
    }, {
      "heading" : "7 Parsing Experiments",
      "text" : "The TLE enables studying parsing for learner language and exploring relationships between grammatical errors and parsing performance. Here, we present parsing benchmarks on our dataset, and provide several estimates for the extent to which grammatical errors degrade the quality of automatic POS tagging and dependency parsing.\nOur first experiment measures tagging and parsing accuracy on the TLE and approximates the global impact of grammatical errors on automatic annotation via performance comparison between the original and error corrected sentence versions. In this, and subsequent experiments, we utilize version 2.2 of the Turbo tagger and Turbo parser (Martins et al., 2013), state of the art tools for statistical POS tagging and dependency parsing.\nTable 3 presents tagging and parsing results on a test set of 500 TLE sentences (9,591 original tokens, 9,703 corrected tokens). Results are provided for three different training regimes. The first training procedure uses the training portion of version 1.2 of the EWT, the UD English treebank, containing 12,543 sentences (204,586 tokens). The second training mode uses a training set of 4,124 sentences (78,541 original tokens, 79,578 corrected tokens) from the TLE corpus. In the third setup we combine these two training corpora. The remaining 500 TLE sentences (9,549 original tokens, 9,696 corrected tokens) are allocated to a development set, not used in this experiment. Parsing of the test sentences was performed on predicted POS tags.\nThe EWT training regime, which uses out of domain texts written in standard English, provides\nthe lowest performance on all the evaluation metrics. An additional factor which negatively affects performance in this regime are systematic differences in the EWT annotation of possessive pronouns, expletives and names compared to the UD guidelines, which are utilized in the TLE. In particular, the EWT annotates possessive pronoun UPOS as PRON rather than DET, which leads the UPOS results in this setup to be lower than the PTB POS results. Improved results are obtained using the TLE training data, which, despite its smaller size, is closer in genre and syntactic characteristics to the TLE test set. The strongest PTB POS tagging and parsing results are obtained by combining the EWT with the TLE training data, yielding 95.77 POS accuracy and a UAS of 89.98 on the original version of the TLE test set.\nOur dual annotation of sentences in their original and error corrected forms enables us to obtain estimates for the impact of grammatical errors on tagging and parsing by examining the performance gap between the two sentence versions. Averaged across the three training conditions, the POS tagging accuracy on the original sentences is lower than the accuracy on the sentence corrections by 1.0 UPOS and 0.69 POS. Parsing performance degrades by 2.0 UAS, 1.57 LA and 2.31 LAS.\nTo further elucidate the influence of grammatical errors on tagging and parsing quality, table 4 compares performance on tokens in original sentences appearing inside grammatical error tags to those appearing outside such tags. Although grammatical errors may lead to tagging and parsing errors with respect to any element in the sentence, we expect erroneous tokens to be more chal-\nlenging to analyze compared to other tokens in the sentence. This comparison indeed reveals a substantial difference between the two types of tokens, with an average gap of 5.13 UPOS, 6.52 POS, 4.43 UAS, 6.77 LA and 7.4 LAS. Note that differently from the global measurements in the first experiment, this analysis, which focuses on the local impact of remove/replace errors, suggests a stronger negative effect on the dependency labels rather than the dependency structure itself.\nFinally, we measure tagging and parsing performance relative to the fraction of sentence tokens marked with grammatical errors. Similarly to the\nprevious experiment, this analysis focuses on remove/replace rather than insert errors. Figure 1 presents the average sentential performance as a function of the percentage of tokens in the original sentence marked with grammatical errors. In this experiment, we train the parser on the EWT training set and test on the entire TLE corpus. Performance curves are presented for POS, UAS and LAS on the original and error corrected versions of the annotations. We observe that while the performance on the corrected sentences is close to constant, original sentence performance is decreasing as the percentage of the erroneous tokens in the sentence grows.\nOverall, our results suggest a negative, albeit limited effect of grammatical errors on parsing. This outcome contrasts a study by Geertzen et al. (2013) which reported a larger performance gap of 7.6 UAS and 8.8 LAS between sentences with and without grammatical errors. We believe that our analysis provides a more accurate estimate of this impact, as it controls for both sentence content and sentence length. The latter factor is crucial, since it correlates positively with the number of grammatical errors in the sentence, and negatively with parsing accuracy."
    }, {
      "heading" : "8 Related Work",
      "text" : "Previous studies on learner language proposed several annotation schemes for both POS tags and syntax (Hirschmann et al., 2007; Dıaz-Negrillo et al., 2010; Dickinson and Ragheb, 2013; Rosen et al., 2014). The unifying theme in these proposals is a multi-layered analysis aiming to decouple the observed language usage from conventional structures in the foreign language.\nIn the context of ESL, Dıaz et al. (2010) propose three parallel POS tag annotations for the lexical, morphological and distributional forms of each word. In our work, we adopt the distinction between distributional word forms, which correspond to the corrected word forms, and morphological forms as the literal readings of words. However, we account for morphological forms only when these constitute valid existing PTB POS tags and are contextually plausible. Furthermore, while the internal structure of invalid word forms is an interesting object of investigation, we believe that it is more suitable for annotation as word features rather than POS tags. Our treebank supports the addition of such features to the exist-\ning annotations.\nThe work of Ragheb and Dickinson (2009; 2012; 2013) proposes ESL annotation guidelines for POS tags and syntactic dependencies based on the CHILDES annotation framework. This approach, called “morphosyntactic dependencies” is related to our annotation scheme in its focus on surface structures. Differently from this proposal, our annotations are grounded in a parallel annotation of grammatical errors and include an additional layer of analysis for the corrected forms. Moreover, we refrain from introducing new syntactic categories and dependency relations specific to ESL, thereby supporting computational treatment of ESL using existing resources for standard English. At the same time, we utilize a multilingual formalism which, in conjunction with our literal annotation strategy, facilitates linking the annotations to native language syntax.\nWhile the above mentioned studies focus on annotation guidelines, attention has also been drawn to the topic of parsing in the learner language domain. However, due to the shortage of syntactic resources for ESL, much of the work in this area resorted to using surrogates for learner data. For example, in (Foster, 2007; Foster et al., 2008) synthetic learner-like data was created by automatic insertion of grammatical errors to well formed English text. In Cahill et al. (2014) a treebank for secondary level native student texts was used to approximate learner text in order to evaluate a parser that utilizes unlabeled learner data.\nSyntactic annotations for ESL were previously developed by Nagata et al. (2011), who annotate an English learner corpus with POS tags and shallow syntactic parses. Our work departs from shallow syntax to full syntactic analysis, and provides annotations on a significantly larger scale. Furthermore, differently from this annotation effort, our treebank covers a wide range of learner native languages, supporting syntax driven studies of first language interference. An additional syntactic dataset for ESL, currently not available publicly, are 1,000 sentences from the EFCamDat dataset (Geertzen et al., 2013), annotated with Stanford dependencies (De Marneffe and Manning, 2008). This dataset was used to measure the impact of grammatical errors on parsing by comparing performance on sentences with grammatical errors to error free sentences. Our work uses a more direct method for estimating the magnitude\nof this performance gap, and suggests a smaller effect than the one reported in this study."
    }, {
      "heading" : "9 Conclusion",
      "text" : "We present the first large scale treebank of learner language, manually annotated and doublereviewed for POS tags and universal dependencies. The annotation is accompanied by a linguistically motivated framework for handling syntactic structures associated with grammatical errors. Finally, we benchmark automatic tagging and parsing on our corpus, and measure the effect of grammatical errors on tagging and parsing quality. The treebank will support empirical study of learner syntax in NLP, corpus linguistics and second language acquisition."
    }, {
      "heading" : "10 Acknowledgements",
      "text" : "We thank Anna Korhonen for helpful discussions, and insightful comments on this paper. We also thank Dora Alexopoulou, Andrei Barbu, Markus Dickinson, Sue Felshin, Jeroen Geertzen, Yan Huang, Detmar Meurers, Sampo Pyysalo, Roi Reichart and the anonymous reviewers for valuable feedback on this work. This material is based upon work supported by the Center for Brains, Minds, and Machines (CBMM), funded by NSF STC award CCF-1231216."
    } ],
    "references" : [ {
      "title" : "Self-training for parsing learner text",
      "author" : [ "Aoife Cahill", "Binod Gyawali", "James V Bruno." ],
      "venue" : "Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages,",
      "citeRegEx" : "Cahill et al\\.,? 2014",
      "shortCiteRegEx" : "Cahill et al\\.",
      "year" : 2014
    }, {
      "title" : "A Coefficient of Agreement for Nominal Scales",
      "author" : [ "J. Cohen." ],
      "venue" : "Educational and Psychological Measurement, 20(1):37.",
      "citeRegEx" : "Cohen.,? 1960",
      "shortCiteRegEx" : "Cohen.",
      "year" : 1960
    }, {
      "title" : "English as a global language",
      "author" : [ "David Crystal." ],
      "venue" : "Ernst Klett Sprachen.",
      "citeRegEx" : "Crystal.,? 2003",
      "shortCiteRegEx" : "Crystal.",
      "year" : 2003
    }, {
      "title" : "Stanford typed dependencies manual",
      "author" : [ "Marie-Catherine De Marneffe", "Christopher D Manning." ],
      "venue" : "Technical report, Technical report, Stanford University.",
      "citeRegEx" : "Marneffe and Manning.,? 2008",
      "shortCiteRegEx" : "Marneffe and Manning.",
      "year" : 2008
    }, {
      "title" : "Universal stanford dependencies: A cross-linguistic typology",
      "author" : [ "Marie-Catherine De Marneffe", "Timothy Dozat", "Natalia Silveira", "Katri Haverinen", "Filip Ginter", "Joakim Nivre", "Christopher D Manning." ],
      "venue" : "Proceedings of LREC, pages 4585–4592.",
      "citeRegEx" : "Marneffe et al\\.,? 2014",
      "shortCiteRegEx" : "Marneffe et al\\.",
      "year" : 2014
    }, {
      "title" : "Towards interlanguage pos annotation for effective learner corpora in sla and flt",
      "author" : [ "Ana Dıaz-Negrillo", "Detmar Meurers", "Salvador Valera", "Holger Wunsch." ],
      "venue" : "Language Forum, 36(1–2):139–154.",
      "citeRegEx" : "Dıaz.Negrillo et al\\.,? 2010",
      "shortCiteRegEx" : "Dıaz.Negrillo et al\\.",
      "year" : 2010
    }, {
      "title" : "Dependency annotation for learner corpora",
      "author" : [ "Markus Dickinson", "Marwa Ragheb." ],
      "venue" : "Proceedings of the Eighth Workshop on Treebanks and Linguistic Theories (TLT-8), pages 59–70.",
      "citeRegEx" : "Dickinson and Ragheb.,? 2009",
      "shortCiteRegEx" : "Dickinson and Ragheb.",
      "year" : 2009
    }, {
      "title" : "Annotation for learner English guidelines, v",
      "author" : [ "Markus Dickinson", "Marwa Ragheb." ],
      "venue" : "0.1. Technical report, Indiana University, Bloomington, IN, June. June 9, 2013.",
      "citeRegEx" : "Dickinson and Ragheb.,? 2013",
      "shortCiteRegEx" : "Dickinson and Ragheb.",
      "year" : 2013
    }, {
      "title" : "Adapting a wsj-trained parser to grammatically noisy text",
      "author" : [ "Jennifer Foster", "Joachim Wagner", "Josef Van Genabith." ],
      "venue" : "Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technolo-",
      "citeRegEx" : "Foster et al\\.,? 2008",
      "shortCiteRegEx" : "Foster et al\\.",
      "year" : 2008
    }, {
      "title" : "Treebanks gone bad",
      "author" : [ "Jennifer Foster." ],
      "venue" : "International Journal of Document Analysis and Recognition (IJDAR), 10(3-4):129–145.",
      "citeRegEx" : "Foster.,? 2007",
      "shortCiteRegEx" : "Foster.",
      "year" : 2007
    }, {
      "title" : "Automatic linguistic annotation of large scale l2 databases: The ef-cambridge open language database (efcamdat)",
      "author" : [ "Jeroen Geertzen", "Theodora Alexopoulou", "Anna Korhonen." ],
      "venue" : "Proceedings of the 31st Second Language Research Forum. Somerville,",
      "citeRegEx" : "Geertzen et al\\.,? 2013",
      "shortCiteRegEx" : "Geertzen et al\\.",
      "year" : 2013
    }, {
      "title" : "Syntactic annotation of noncanonical linguistic structures",
      "author" : [ "Hagen Hirschmann", "Seanna Doolittle", "Anke Lüdeling" ],
      "venue" : null,
      "citeRegEx" : "Hirschmann et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Hirschmann et al\\.",
      "year" : 2007
    }, {
      "title" : "Turning on the turbo: Fast third-order non-projective turbo parsers",
      "author" : [ "André FT Martins", "Miguel Almeida", "Noah A Smith." ],
      "venue" : "ACL (2), pages 617–622. Citeseer.",
      "citeRegEx" : "Martins et al\\.,? 2013",
      "shortCiteRegEx" : "Martins et al\\.",
      "year" : 2013
    }, {
      "title" : "Universal dependency annotation for multilingual parsing",
      "author" : [ "Ryan T McDonald", "Joakim Nivre", "Yvonne QuirmbachBrundage", "Yoav Goldberg", "Dipanjan Das", "Kuzman Ganchev", "Keith B Hall", "Slav Petrov", "Hao Zhang", "Oscar Täckström" ],
      "venue" : null,
      "citeRegEx" : "McDonald et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "McDonald et al\\.",
      "year" : 2013
    }, {
      "title" : "Creating a manually error-tagged and shallow-parsed learner corpus",
      "author" : [ "Ryo Nagata", "Edward Whittaker", "Vera Sheinman." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Nagata et al\\.,? 2011",
      "shortCiteRegEx" : "Nagata et al\\.",
      "year" : 2011
    }, {
      "title" : "The conll-2014 shared task on grammatical error correction",
      "author" : [ "Hwee Tou Ng", "Siew Mei Wu", "Ted Briscoe", "Christian Hadiwinoto", "Raymond Hendy Susanto", "Christopher Bryant." ],
      "venue" : "CoNLL Shared Task, pages 1–14.",
      "citeRegEx" : "Ng et al\\.,? 2014",
      "shortCiteRegEx" : "Ng et al\\.",
      "year" : 2014
    }, {
      "title" : "The cambridge learner corpus: Error coding and analysis for lexicography and elt",
      "author" : [ "Diane Nicholls." ],
      "venue" : "Proceedings of the Corpus Linguistics 2003 conference, pages 572–581.",
      "citeRegEx" : "Nicholls.,? 2003",
      "shortCiteRegEx" : "Nicholls.",
      "year" : 2003
    }, {
      "title" : "Defining syntax for learner language annotation",
      "author" : [ "Marwa Ragheb", "Markus Dickinson." ],
      "venue" : "COLING (Posters), pages 965–974.",
      "citeRegEx" : "Ragheb and Dickinson.,? 2012",
      "shortCiteRegEx" : "Ragheb and Dickinson.",
      "year" : 2012
    }, {
      "title" : "Syntactic annotation of learner corpora",
      "author" : [ "Victoria Rosén", "Koenraad De Smedt." ],
      "venue" : "Systematisk, variert, men ikke tilfeldig, pages 120–132.",
      "citeRegEx" : "Rosén and Smedt.,? 2010",
      "shortCiteRegEx" : "Rosén and Smedt.",
      "year" : 2010
    }, {
      "title" : "Evaluating and automating the annotation of a learner corpus",
      "author" : [ "Alexandr Rosen", "Jirka Hana", "Barbora Štindlová", "Anna Feldman." ],
      "venue" : "Language Resources and Evaluation, 48(1):65–92.",
      "citeRegEx" : "Rosen et al\\.,? 2014",
      "shortCiteRegEx" : "Rosen et al\\.",
      "year" : 2014
    }, {
      "title" : "Part-of-speech tagging guidelines for the penn treebank project (3rd revision)",
      "author" : [ "Beatrice Santorini." ],
      "venue" : "Technical Reports (CIS).",
      "citeRegEx" : "Santorini.,? 1990",
      "shortCiteRegEx" : "Santorini.",
      "year" : 1990
    }, {
      "title" : "A gold standard dependency corpus for english",
      "author" : [ "Natalia Silveira", "Timothy Dozat", "Marie-Catherine de Marneffe", "Samuel R Bowman", "Miriam Connor", "John Bauer", "Christopher D Manning." ],
      "venue" : "Proceedings of the Ninth International Conference",
      "citeRegEx" : "Silveira et al\\.,? 2014",
      "shortCiteRegEx" : "Silveira et al\\.",
      "year" : 2014
    }, {
      "title" : "Using parse features for preposition selection and error detection",
      "author" : [ "Joel Tetreault", "Jennifer Foster", "Martin Chodorow." ],
      "venue" : "Proceedings of the acl 2010 conference short papers, pages 353–358. Association for Computational Linguistics.",
      "citeRegEx" : "Tetreault et al\\.,? 2010",
      "shortCiteRegEx" : "Tetreault et al\\.",
      "year" : 2010
    }, {
      "title" : "A new dataset and method for automatically grading ESOL texts",
      "author" : [ "Helen Yannakoudakis", "Ted Briscoe", "Ben Medlock." ],
      "venue" : "ACL, pages 180–189.",
      "citeRegEx" : "Yannakoudakis et al\\.,? 2011",
      "shortCiteRegEx" : "Yannakoudakis et al\\.",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 23,
      "context" : "The TLE sentences are drawn from the FCE dataset (Yannakoudakis et al., 2011), and authored by English learners from 10 different native language backgrounds.",
      "startOffset" : 49,
      "endOffset" : 77
    }, {
      "referenceID" : 13,
      "context" : ", 2014), which provides a unified annotation framework across different languages and is geared towards multilingual NLP (McDonald et al., 2013).",
      "startOffset" : 121,
      "endOffset" : 144
    }, {
      "referenceID" : 5,
      "context" : "While the annotation inventory and guidelines are defined by the English UD formalism, we build on previous work in learner language analysis (Dıaz-Negrillo et al., 2010; Dickinson and Ragheb, 2013) to formulate an additional set of annotation conventions aiming at a uniform treatment of ungrammatical learner language.",
      "startOffset" : 142,
      "endOffset" : 198
    }, {
      "referenceID" : 7,
      "context" : "While the annotation inventory and guidelines are defined by the English UD formalism, we build on previous work in learner language analysis (Dıaz-Negrillo et al., 2010; Dickinson and Ragheb, 2013) to formulate an additional set of annotation conventions aiming at a uniform treatment of ungrammatical learner language.",
      "startOffset" : 142,
      "endOffset" : 198
    }, {
      "referenceID" : 10,
      "context" : "In the context of NLP, such studies yield data needed for improving tagging and parsing performance on ungrammatical language (Geertzen et al., 2013), as well as utilizing syntactic structure for grammatical error correction (Tetreault et al.",
      "startOffset" : 126,
      "endOffset" : 149
    }, {
      "referenceID" : 22,
      "context" : ", 2013), as well as utilizing syntactic structure for grammatical error correction (Tetreault et al., 2010; Ng et al., 2014).",
      "startOffset" : 83,
      "endOffset" : 124
    }, {
      "referenceID" : 15,
      "context" : ", 2013), as well as utilizing syntactic structure for grammatical error correction (Tetreault et al., 2010; Ng et al., 2014).",
      "startOffset" : 83,
      "endOffset" : 124
    }, {
      "referenceID" : 23,
      "context" : "The sentences were obtained from the FCE corpus (Yannakoudakis et al., 2011), a collection of upper intermediate English learner essays, containing error annotations with 75 error categories (Nicholls, 2003).",
      "startOffset" : 48,
      "endOffset" : 76
    }, {
      "referenceID" : 16,
      "context" : ", 2011), a collection of upper intermediate English learner essays, containing error annotations with 75 error categories (Nicholls, 2003).",
      "startOffset" : 122,
      "endOffset" : 138
    }, {
      "referenceID" : 20,
      "context" : "During the training, the annotators attended tutorials on dependency grammars, and learned the UD guidelines4 , the Penn Treebank POS guidelines (Santorini, 1990), the grammatical error annotation scheme of the FCE (Nicholls, 2003), as well as the ESL guidelines presented in section 5.",
      "startOffset" : 145,
      "endOffset" : 162
    }, {
      "referenceID" : 16,
      "context" : "During the training, the annotators attended tutorials on dependency grammars, and learned the UD guidelines4 , the Penn Treebank POS guidelines (Santorini, 1990), the grammatical error annotation scheme of the FCE (Nicholls, 2003), as well as the ESL guidelines presented in section 5.",
      "startOffset" : 215,
      "endOffset" : 231
    }, {
      "referenceID" : 21,
      "context" : "The first three exercises consisted of 20 sentences from the UD gold standard for English, the English Web Treebank (EWT) (Silveira et al., 2014).",
      "startOffset" : 122,
      "endOffset" : 145
    }, {
      "referenceID" : 17,
      "context" : "This strategy continues a line of work in SLA which advocates for centering analysis of learner language around morpho-syntactic surface evidence (Ragheb and Dickinson, 2012; Dickinson and Ragheb, 2013).",
      "startOffset" : 146,
      "endOffset" : 202
    }, {
      "referenceID" : 7,
      "context" : "This strategy continues a line of work in SLA which advocates for centering analysis of learner language around morpho-syntactic surface evidence (Ragheb and Dickinson, 2012; Dickinson and Ragheb, 2013).",
      "startOffset" : 146,
      "endOffset" : 202
    }, {
      "referenceID" : 11,
      "context" : "Similarly to our framework, which includes a parallel annotation of corrected sentences, such strategies are often presented in the context of multi-layer annotation schemes that also account for error corrected sentence forms (Hirschmann et al., 2007; Dıaz-Negrillo et al., 2010; Rosen et al., 2014).",
      "startOffset" : 227,
      "endOffset" : 300
    }, {
      "referenceID" : 5,
      "context" : "Similarly to our framework, which includes a parallel annotation of corrected sentences, such strategies are often presented in the context of multi-layer annotation schemes that also account for error corrected sentence forms (Hirschmann et al., 2007; Dıaz-Negrillo et al., 2010; Rosen et al., 2014).",
      "startOffset" : 227,
      "endOffset" : 300
    }, {
      "referenceID" : 19,
      "context" : "Similarly to our framework, which includes a parallel annotation of corrected sentences, such strategies are often presented in the context of multi-layer annotation schemes that also account for error corrected sentence forms (Hirschmann et al., 2007; Dıaz-Negrillo et al., 2010; Rosen et al., 2014).",
      "startOffset" : 227,
      "endOffset" : 300
    }, {
      "referenceID" : 1,
      "context" : "Cohen’s Kappa scores (Cohen, 1960) for POS tags and dependency labels in all evaluation conditions are above 0.",
      "startOffset" : 21,
      "endOffset" : 34
    }, {
      "referenceID" : 12,
      "context" : "2 of the Turbo tagger and Turbo parser (Martins et al., 2013), state of the art tools for statistical POS tagging and dependency parsing.",
      "startOffset" : 39,
      "endOffset" : 61
    }, {
      "referenceID" : 10,
      "context" : "This outcome contrasts a study by Geertzen et al. (2013) which reported a larger performance gap of 7.",
      "startOffset" : 34,
      "endOffset" : 57
    }, {
      "referenceID" : 9,
      "context" : "For example, in (Foster, 2007; Foster et al., 2008) synthetic learner-like data was created by automatic insertion of grammatical errors to well formed English text.",
      "startOffset" : 16,
      "endOffset" : 51
    }, {
      "referenceID" : 8,
      "context" : "For example, in (Foster, 2007; Foster et al., 2008) synthetic learner-like data was created by automatic insertion of grammatical errors to well formed English text.",
      "startOffset" : 16,
      "endOffset" : 51
    }, {
      "referenceID" : 0,
      "context" : "In Cahill et al. (2014) a treebank for secondary level native student texts was used to approximate learner text in order to evaluate a parser that utilizes unlabeled learner data.",
      "startOffset" : 3,
      "endOffset" : 24
    }, {
      "referenceID" : 14,
      "context" : "Syntactic annotations for ESL were previously developed by Nagata et al. (2011), who annotate an English learner corpus with POS tags and shallow syntactic parses.",
      "startOffset" : 59,
      "endOffset" : 80
    }, {
      "referenceID" : 10,
      "context" : "Dat dataset (Geertzen et al., 2013), annotated with Stanford dependencies (De Marneffe and Man-",
      "startOffset" : 12,
      "endOffset" : 35
    } ],
    "year" : 2016,
    "abstractText" : "We introduce the Treebank of Learner English (TLE), the first publicly available syntactic treebank for English as a Second Language (ESL). The TLE provides manually annotated POS tags and Universal Dependency (UD) trees for 5,124 sentences from the Cambridge First Certificate in English (FCE) corpus. The UD annotations are tied to a pre-existing error annotation of the FCE, whereby full syntactic analyses are provided for both the original and error corrected versions of each sentence. Further on, we delineate ESL annotation guidelines that allow for consistent syntactic treatment of ungrammatical English. Finally, we benchmark POS tagging and dependency parsing performance on the TLE dataset and measure the effect of grammatical errors on parsing accuracy. We envision the treebank to support a wide range of linguistic and computational research on second language acquisition as well as automatic processing of ungrammatical language1 .",
    "creator" : "dvips(k) 5.991 Copyright 2011 Radical Eye Software"
  }
}