{
  "name" : "1609.06570.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning",
    "authors" : [ "Guillaume Lemâıtre", "Fernando Nogueira", "Christos K. Aridas" ],
    "emails" : [ "g.lemaitre58@gmail.com", "fmfnogueira@gmail.com", "char@upatras.gr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 9.\n06 57\n0v 1\n[ cs\nKeywords: Imbalanced Dataset, Over-Sampling, Under-Sampling, Ensemble Learning, Machine Learning, Python."
    }, {
      "heading" : "1. Introduction",
      "text" : "Real world datasets commonly show the particularity to have a number of samples of a given class under-represented compared to other classes. This imbalance gives rise to the “class imbalance” problem (Prati et al., 2009) (or “curse of imbalanced datasets”) which is the problem of learning a concept from the class that has a small number of samples.\nThe class imbalance problem has been encountered in multiple areas such as telecommunication managements, bioinformatics, fraud detection, and medical diagnosis, and has been considered one of the top 10 problems in data mining and pattern recognition (Yang and Wu, 2006; Rastgoo et al., 2016). Imbalanced data substantially compromises the learning process, since most of the standard machine learning algorithms expect balanced class distribution or an equal misclassification cost (He and Garcia, 2009). For this reason, several ap-\nc©2016 Guillaume Lemâıtre, Fernando Nogueira, and Christos K. Aridas.\nproaches have been specifically proposed to handle such datasets. Such standalone methods have been implemented mainly in R language (Torgo, 2010; Kuhn, 2015; Dal Pozzolo et al., 2013). Up to our knowledge, however, there is no python toolbox allowing such processing while cutting edge machine learning toolboxes are available (Pedregosa et al., 2011; Sonnenburg et al., 2010).\nIn this paper, we present the imbalanced-learn API, a python toolbox to tackle the curse of imbalanced datasets in machine learning. The following sections present the project vision, a snapshot of the API, an overview of the implemented methods, and finally, the conclusion of this paper, including future functionalities for the imbalanced-learn API."
    }, {
      "heading" : "2. Project management",
      "text" : "Quality insurance In order to ensure code quality, a set of unit tests is provided leading to a coverage of 99 % for the release 0.1.8 of the toolbox. Furthermore, the code consistency is ensured by following PEP8 standards and each new contribution is automatically checked through landscape, which provides metrics related to code quality. Continuous integration To allow user and developer to either use or contribute to this toolbox, Travis CI is used to easily integrate new code and ensure back-compatibility.\nCommunity-based development All the development is performed in a collaborative manner. Tools such as git, GitHub, and gitter are used to ease collaborative programming, issue tracking, code integration, and idea discussions.\nDocumentation A consistent API documentation is provided using sphinx and numpydoc. An additional installation guide and examples are also provided and centralized on GitHub1. Project relevance At the edition time, the repository is visited no less than 2, 000 per week, attracting about 300 unique visitors per week. Additionally, the toolbox is supported by scikit-learn through the scikit-learn-contrib projects."
    }, {
      "heading" : "3. Implementation design",
      "text" : "1 from sk l e a rn . d a t a s e t s import mak e c l a s s i f i c a t i o n 2 from sk l e a rn . decompos it ion import PCA 3 from imblearn . over sampl ing import SMOTE 4 5 # Generate the datase t 6 X, y = mak e c l a s s i f i c a t i o n ( n c l a s s e s =2, weights =[0 .1 , 0 . 9 ] , 7 n f e a t u r e s=20, n samples =5000) 8\n9 # Apply the SMOTE over−sampling 10 sm = SMOTE( r a t i o= ’ auto ’ , kind=’ r e gu l a r ’ ) 11 X resampled , y resampled = sm . f i t s amp l e (X, y )\nListing 1: Code snippet to over-sample a dataset using SMOTE.\nThe implementation relies on numpy, scipy, and scikit-learn. Each sampler class implements three main methods inspired from the scikit-learn API: (i) fit computes the parameter values which are later needed to resample the data into a balanced set; (ii)\n1. https://github.com/scikit-learn-contrib/imbalanced-learn\nsample performs the sampling and returns the data with the desired balancing ratio; and (iii) fit sample is equivalent to calling the method fit followed by the method sample. A class Pipeline is inherited from scikit-learn toolbox to automatically combine samplers, transformers, and estimators."
    }, {
      "heading" : "4. Implemented methods",
      "text" : "The imbalanced-learn toolbox provides four different strategies to tackle the problem of imbalanced dataset: (i) under-sampling, (ii) over-sampling, (iii) a combination of both, and (iv) ensemble learning. The following subsections give an overview of the techniques implemented."
    }, {
      "heading" : "4.1 Notation and background",
      "text" : "Let χ an imbalanced dataset with χmin and χmaj being the subset of samples belonging to the minority and majority class, respectively. The balancing ratio of the dataset χ is defined as:\nrχ = |χmin|\n|χmaj | , (1)\nwhere | · | denotes the cardinality of a set. The balancing process is equivalent to resample χ into a new dataset χres such that rχ > rχres ."
    }, {
      "heading" : "4.2 Under-sampling",
      "text" : "Under-sampling refers to the process of reducing the number of samples in χmaj . The implemented methods can be categorized into 2 groups: (i) fixed under-sampling and (ii) cleaning under-sampling.\nFixed under-sampling refer to the methods which perform under-sampling to obtain the appropriate balancing ratio rχres . The implemented methods perform the under-sampling based on different criteria such as: (i) random selection, (ii) clustering, (iii) nearest neighbours rule (i.e., NearMiss (Mani and Zhang, 2003)), and (iv) classification accuracy (i.e., instance hardness threshold (Smith et al., 2014)).\nIn the contrary to the previous methods, cleaning under-sampling do not allow to reach specifically the balancing ratio rχres , but rather clean the feature space based on some empirical criteria. These criteria are derived from the nearest neighbours rule, namely: (i) condensed nearest neighbours (Hart, 1968), (ii) edited nearest neighbours (Wilson, 1972), (iii) one-sided selection (Kubat et al., 1997), (iv) neighbourhood cleaning rule (Laurikkala, 2001), and (v) Tomek links (Tomek, 1976)."
    }, {
      "heading" : "4.3 Over-sampling",
      "text" : "In the contrary to under-sampling, data balancing can be performed by over-sampling such that new samples are generated in χmin to reach the balancing ratio rχres . Two methods are currently available: (i) Random over-sampling is performed by randomly replicating the samples of χmin to obtain the appropriate balancing ratio rχres and SMOTE which randomly generate new samples between tuple of nearest neighbours of χmin (Chawla et al.,\n2002). Different variants of this algorithm have been proposed: SMOTE borderline 1 & 2 (Han et al., 2005) and SMOTE SVM (Nguyen et al., 2011)."
    }, {
      "heading" : "4.4 Combination of over- and under-sampling",
      "text" : "SMOTE over-sampling can lead to over-fitting which can be avoided by applying cleaning under-sampling methods (Prati et al., 2009). In that regard, Batista et al. (2003) combined SMOTE either with Tomek links or edited nearest neighbours."
    }, {
      "heading" : "4.5 Ensemble learning",
      "text" : "Under-sampling methods imply that samples of the majority class are lost during the balancing procedure. Ensemble methods offer an alternative to use most of the samples. In fact, an ensemble of balanced sets is created and used to later train any classifier. Two methods are available to build such ensemble proposed by Liu et al. (2009): EasyEnsemble and BalanceCascade. The former is based on iteratively applying the random under-sampling method to build several sets, each of them with a desired balancing ratio rχres . The latter differs such that a classifier is used at each iteration to determine the class of the randomly selected samples. Misclassified samples are kept and propagated in the next subset."
    }, {
      "heading" : "5. Future plans and conclusion",
      "text" : "In this paper, we shortly presented the foundations of the imbalanced-learn toolbox vision and API. As avenues for future works, additional methods based on prototype/instance selection, generation, and reduction will be added as well as additional user guides."
    } ],
    "references" : [ {
      "title" : "Balancing training data for automated annotation of keywords: a case study",
      "author" : [ "G.E. Batista", "A.L. Bazzan", "M.C. Monard" ],
      "venue" : "In WOB,",
      "citeRegEx" : "Batista et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Batista et al\\.",
      "year" : 2003
    }, {
      "title" : "SMOTE: synthetic minority over-sampling technique",
      "author" : [ "N.V. Chawla", "K.W. Bowyer", "L.O. Hall", "W.P. Kegelmeyer" ],
      "venue" : "Journal of artificial intelligence research,",
      "citeRegEx" : "Chawla et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Chawla et al\\.",
      "year" : 2002
    }, {
      "title" : "Racing for unbalanced methods selection",
      "author" : [ "A. Dal Pozzolo", "O. Caelen", "S. Waterschoot", "G. Bontempi" ],
      "venue" : "In International Conference on Intelligent Data Engineering and Automated Learning,",
      "citeRegEx" : "Pozzolo et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Pozzolo et al\\.",
      "year" : 2013
    }, {
      "title" : "Borderline-smote: a new over-sampling method in imbalanced data sets learning",
      "author" : [ "H. Han", "W.-Y. Wang", "B.-H. Mao" ],
      "venue" : "In International Conference on Intelligent Computing,",
      "citeRegEx" : "Han et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2005
    }, {
      "title" : "The condensed nearest neighbor rule",
      "author" : [ "P. Hart" ],
      "venue" : "Information Theory, IEEE Transactions on,",
      "citeRegEx" : "Hart.,? \\Q1968\\E",
      "shortCiteRegEx" : "Hart.",
      "year" : 1968
    }, {
      "title" : "Learning from imbalanced data",
      "author" : [ "H. He", "E. Garcia" ],
      "venue" : "Knowledge and Data Engineering, IEEE Transactions on,",
      "citeRegEx" : "He and Garcia.,? \\Q2009\\E",
      "shortCiteRegEx" : "He and Garcia.",
      "year" : 2009
    }, {
      "title" : "Addressing the curse of imbalanced training sets: one-sided selection",
      "author" : [ "M. Kubat", "S. Matwin" ],
      "venue" : "In International Conference in Machine Learning,",
      "citeRegEx" : "Kubat and Matwin,? \\Q1997\\E",
      "shortCiteRegEx" : "Kubat and Matwin",
      "year" : 1997
    }, {
      "title" : "Caret: classification and regression training",
      "author" : [ "M. Kuhn" ],
      "venue" : "Astrophysics Source Code Library,",
      "citeRegEx" : "Kuhn.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kuhn.",
      "year" : 2015
    }, {
      "title" : "Improving identification of difficult small classes by balancing class distribution",
      "author" : [ "J. Laurikkala" ],
      "venue" : null,
      "citeRegEx" : "Laurikkala.,? \\Q2001\\E",
      "shortCiteRegEx" : "Laurikkala.",
      "year" : 2001
    }, {
      "title" : "Exploratory undersampling for class-imbalance learning",
      "author" : [ "X.-Y. Liu", "J. Wu", "Z.-H. Zhou" ],
      "venue" : "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics),",
      "citeRegEx" : "Liu et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2009
    }, {
      "title" : "knn approach to unbalanced data distributions: a case study involving information extraction",
      "author" : [ "I. Mani", "I. Zhang" ],
      "venue" : "In Proceedings of Workshop on Learning from Imbalanced Datasets,",
      "citeRegEx" : "Mani and Zhang.,? \\Q2003\\E",
      "shortCiteRegEx" : "Mani and Zhang.",
      "year" : 2003
    }, {
      "title" : "Borderline over-sampling for imbalanced data classification",
      "author" : [ "H.M. Nguyen", "E.W. Cooper", "K. Kamei" ],
      "venue" : "International Journal of Knowledge Engineering and Soft Data Paradigms,",
      "citeRegEx" : "Nguyen et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2011
    }, {
      "title" : "Scikit-learn: Machine learning in python",
      "author" : [ "F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Pedregosa et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Pedregosa et al\\.",
      "year" : 2011
    }, {
      "title" : "Data mining with imbalanced class distributions: concepts and methods",
      "author" : [ "R.C. Prati", "G.E. Batista", "M.C. Monard" ],
      "venue" : "In Indian International Conference Artificial Intelligence,",
      "citeRegEx" : "Prati et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Prati et al\\.",
      "year" : 2009
    }, {
      "title" : "Tackling the problem of data imbalancing for melanoma classification",
      "author" : [ "M. Rastgoo", "G. Lemaitre", "J. Massich", "O. Morel", "F. Marzani", "R. Garcia", "F. Meriaudeau" ],
      "venue" : "Bioimaging,",
      "citeRegEx" : "Rastgoo et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Rastgoo et al\\.",
      "year" : 2016
    }, {
      "title" : "An instance level analysis of data complexity",
      "author" : [ "M.R. Smith", "T. Martinez", "C. Giraud-Carrier" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Smith et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Smith et al\\.",
      "year" : 2014
    }, {
      "title" : "The SHOGUN machine learning toolbox",
      "author" : [ "S.C. Sonnenburg", "S. Henschel", "C. Widmer", "J. Behr", "A. Zien", "F. de Bona", "A. Binder", "C. Gehl", "V. Franc" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Sonnenburg et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Sonnenburg et al\\.",
      "year" : 2010
    }, {
      "title" : "Two modifications of CNN. Systems, Man, and Cybernetics",
      "author" : [ "I. Tomek" ],
      "venue" : "IEEE Transactions on,",
      "citeRegEx" : "Tomek.,? \\Q1976\\E",
      "shortCiteRegEx" : "Tomek.",
      "year" : 1976
    }, {
      "title" : "Data mining with R: learning with case studies",
      "author" : [ "L. Torgo" ],
      "venue" : "Chapman & Hall/CRC,",
      "citeRegEx" : "Torgo.,? \\Q2010\\E",
      "shortCiteRegEx" : "Torgo.",
      "year" : 2010
    }, {
      "title" : "Asymptotic properties of nearest neighbor rules using edited data",
      "author" : [ "D.L. Wilson" ],
      "venue" : "Systems, Man and Cybernetics, IEEE Transactions on,",
      "citeRegEx" : "Wilson.,? \\Q1972\\E",
      "shortCiteRegEx" : "Wilson.",
      "year" : 1972
    }, {
      "title" : "10 challenging problems in data mining research",
      "author" : [ "Q. Yang", "X. Wu" ],
      "venue" : "International Journal of Information Technology & Decision Making,",
      "citeRegEx" : "Yang and Wu.,? \\Q2006\\E",
      "shortCiteRegEx" : "Yang and Wu.",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "This imbalance gives rise to the “class imbalance” problem (Prati et al., 2009) (or “curse of imbalanced datasets”) which is the problem of learning a concept from the class that has a small number of samples.",
      "startOffset" : 59,
      "endOffset" : 79
    }, {
      "referenceID" : 20,
      "context" : "The class imbalance problem has been encountered in multiple areas such as telecommunication managements, bioinformatics, fraud detection, and medical diagnosis, and has been considered one of the top 10 problems in data mining and pattern recognition (Yang and Wu, 2006; Rastgoo et al., 2016).",
      "startOffset" : 252,
      "endOffset" : 293
    }, {
      "referenceID" : 14,
      "context" : "The class imbalance problem has been encountered in multiple areas such as telecommunication managements, bioinformatics, fraud detection, and medical diagnosis, and has been considered one of the top 10 problems in data mining and pattern recognition (Yang and Wu, 2006; Rastgoo et al., 2016).",
      "startOffset" : 252,
      "endOffset" : 293
    }, {
      "referenceID" : 5,
      "context" : "Imbalanced data substantially compromises the learning process, since most of the standard machine learning algorithms expect balanced class distribution or an equal misclassification cost (He and Garcia, 2009).",
      "startOffset" : 189,
      "endOffset" : 210
    }, {
      "referenceID" : 18,
      "context" : "Such standalone methods have been implemented mainly in R language (Torgo, 2010; Kuhn, 2015; Dal Pozzolo et al., 2013).",
      "startOffset" : 67,
      "endOffset" : 118
    }, {
      "referenceID" : 7,
      "context" : "Such standalone methods have been implemented mainly in R language (Torgo, 2010; Kuhn, 2015; Dal Pozzolo et al., 2013).",
      "startOffset" : 67,
      "endOffset" : 118
    }, {
      "referenceID" : 12,
      "context" : "Up to our knowledge, however, there is no python toolbox allowing such processing while cutting edge machine learning toolboxes are available (Pedregosa et al., 2011; Sonnenburg et al., 2010).",
      "startOffset" : 142,
      "endOffset" : 191
    }, {
      "referenceID" : 16,
      "context" : "Up to our knowledge, however, there is no python toolbox allowing such processing while cutting edge machine learning toolboxes are available (Pedregosa et al., 2011; Sonnenburg et al., 2010).",
      "startOffset" : 142,
      "endOffset" : 191
    }, {
      "referenceID" : 10,
      "context" : ", NearMiss (Mani and Zhang, 2003)), and (iv) classification accuracy (i.",
      "startOffset" : 11,
      "endOffset" : 33
    }, {
      "referenceID" : 15,
      "context" : ", instance hardness threshold (Smith et al., 2014)).",
      "startOffset" : 30,
      "endOffset" : 50
    }, {
      "referenceID" : 4,
      "context" : "These criteria are derived from the nearest neighbours rule, namely: (i) condensed nearest neighbours (Hart, 1968), (ii) edited nearest neighbours (Wilson, 1972), (iii) one-sided selection (Kubat et al.",
      "startOffset" : 102,
      "endOffset" : 114
    }, {
      "referenceID" : 19,
      "context" : "These criteria are derived from the nearest neighbours rule, namely: (i) condensed nearest neighbours (Hart, 1968), (ii) edited nearest neighbours (Wilson, 1972), (iii) one-sided selection (Kubat et al.",
      "startOffset" : 147,
      "endOffset" : 161
    }, {
      "referenceID" : 8,
      "context" : ", 1997), (iv) neighbourhood cleaning rule (Laurikkala, 2001), and (v) Tomek links (Tomek, 1976).",
      "startOffset" : 42,
      "endOffset" : 60
    }, {
      "referenceID" : 17,
      "context" : ", 1997), (iv) neighbourhood cleaning rule (Laurikkala, 2001), and (v) Tomek links (Tomek, 1976).",
      "startOffset" : 82,
      "endOffset" : 95
    }, {
      "referenceID" : 3,
      "context" : "Different variants of this algorithm have been proposed: SMOTE borderline 1 & 2 (Han et al., 2005) and SMOTE SVM (Nguyen et al.",
      "startOffset" : 80,
      "endOffset" : 98
    }, {
      "referenceID" : 11,
      "context" : ", 2005) and SMOTE SVM (Nguyen et al., 2011).",
      "startOffset" : 22,
      "endOffset" : 43
    }, {
      "referenceID" : 13,
      "context" : "4 Combination of over- and under-sampling SMOTE over-sampling can lead to over-fitting which can be avoided by applying cleaning under-sampling methods (Prati et al., 2009).",
      "startOffset" : 152,
      "endOffset" : 172
    }, {
      "referenceID" : 0,
      "context" : "In that regard, Batista et al. (2003) combined SMOTE either with Tomek links or edited nearest neighbours.",
      "startOffset" : 16,
      "endOffset" : 38
    }, {
      "referenceID" : 9,
      "context" : "Two methods are available to build such ensemble proposed by Liu et al. (2009): EasyEnsemble and BalanceCascade.",
      "startOffset" : 61,
      "endOffset" : 79
    } ],
    "year" : 2016,
    "abstractText" : "imbalanced-learn is an open-source python toolbox aiming at providing a wide range of methods to cope with the problem of imbalanced dataset frequently encountered in machine learning and pattern recognition. The implemented state-of-the-art methods can be categorized into 4 groups: (i) under-sampling, (ii) over-sampling, (iii) combination of overand under-sampling, and (iv) ensemble learning methods. The proposed toolbox only depends on numpy, scipy, and scikit-learn and is distributed under MIT license. Furthermore, it is fully compatible with scikit-learn and is part of the scikit-learn-contrib supported project. Documentation, unit tests as well as integration tests are provided to ease usage and contribution. The toolbox is publicly available in GitHub https://github.com/scikit-learn-contrib/imbalanced-learn.",
    "creator" : "LaTeX with hyperref package"
  }
}