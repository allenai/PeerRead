{
  "name" : "1708.00781.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Dynamic Entity Representations in Neural Language Models",
    "authors" : [ "Yangfeng Ji", "Chenhao Tan", "Sebastian Martschat", "Yejin Choi", "Noah A. Smith" ],
    "emails" : [ "yangfeng@cs.washington.edu", "chenhao@cs.washington.edu", "yejin@cs.washington.edu", "nasmith@cs.washington.edu", "martschat@cl.uni-heidelberg.de" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 8.\n00 78\n1v 1\n[ cs\n.C L\n] 2\nA ug\n2 01\n7\ntracking how entities are introduced and evolve over time. We present a new type of language model, ENTITYNLM, that can explicitly model entities, dynamically update their representations, and contextually generate their mentions. Our model is generative and flexible; it can model an arbitrary number of entities in context while generating each entity mention at an arbitrary length. In addition, it can be used for several different tasks such as language modeling, coreference resolution, and entity prediction. Experimental results with all these tasks demonstrate that our model consistently outperforms strong baselines and prior work."
    }, {
      "heading" : "1 Introduction",
      "text" : "Understanding a narrative requires keeping track of its participants over a long-term context. As a story unfolds, the information a reader associates with each character in a story increases, and expectations about what will happen next change accordingly. At present, models of natural language do not explicitly track entities; indeed, in today’s language models, entities are no more than the words used to mention them.\nIn this paper, we endow a generative language model with the ability to build up a dynamic representation of each entity mentioned in the text. Our language model defines a probability distribution over the whole text, with a distinct generative story for entity mentions. It explicitly groups those mentions that corefer and associates with each entity a continuous representation that is updated by every contextualized mention of the entity, and that in turn affects the text that follows.\nOur method builds on recent advances in representation learning, creating local probability distributions from neural networks. It can be understood as a recurrent neural network language model, augmented with random variables for entity mentions that capture coreference, and with dynamic representations of entities. We estimate the model’s parameters from data that is annotated with entity mentions and coreference.\nBecause our model is generative, it can be queried in different ways. Marginalizing everything except the words, it can play the role of a language model. In §5.1, we find that it outperforms both a strong n-gram language model and a strong recurrent neural network language model on the English test set of the CoNLL 2012 shared task on coreference evaluation (Pradhan et al., 2012). The model can also identify entity mentions and coreference relationships among them. In §5.2, we show that it can easily be used to add a performance boost to a strong coreference resolution system, by reranking a list of k-best candidate outputs. On the CoNLL 2012 shared task test set, the reranked outputs are significantly better than the original top choices from the same system. Fi-\nnally, the model can perform entity cloze tasks. As presented in §5.3, it achieves state-of-the-art performance on the InScript corpus (Modi et al., 2017)."
    }, {
      "heading" : "2 Model",
      "text" : "A language model defines a distribution over sequences of word tokens; let Xt denote the random variable for the tth word in the sequence, xt denote the value of Xt and xt the distributed representation (embedding) of this word. Our starting point for language modeling is a recurrent neural network (Mikolov et al., 2010), which defines\np(Xt | history) = softmax (Whht−1 + b) (1)\nht−1 = LSTM(ht−2,xt−1) (2)\nwhere Wh and b are parameters of the model (along with word embeddings xt), LSTM is the widely used recurrent function known as “long short-term memory” (Hochreiter and Schmidhuber, 1997), and ht is a LSTM hidden state encoding the history of the sequence up to the tth word.\nGreat success has been reported for this model (Zaremba et al., 2015), which posits nothing explicitly about the words appearing in the text sequence. Its generative story is simple: the value of each Xt is randomly chosen conditioned on the vector ht−1 encoding its history."
    }, {
      "heading" : "2.1 Additional random variables and representations for entities",
      "text" : "To introduce our model, we associate with each word an additional set of random variables. At position t,\n• Rt is a binary random variable that indicates whether xt belongs to an entity men-\ntion (Rt = 1) or not (Rt = 0). Though not explored here, this is easily generalized to a categorial variable for the type of the entity (e.g., person, organization, etc.).\n• Lt ∈ {1, . . . , ℓmax} is a categorical random variable if Rt = 1, which indicates the number of remaining words in this mention, in-\ncluding the current word (i.e., Lt = 1 for the last word in any mention). ℓmax is a predefined maximum length fixed to be 25, which is an empirical value derived from the training corpora used in the experiments. If\nRt = 0, then Lt = 1. We denote the value of Lt by ℓt.\n• Et ∈ Et is the index of the entity referred to, if Rt = 1. The set Et consists of {1, . . . , 1 + maxt′<t et′}, i.e., the indices of all previously mentioned entities plus an additional value\nfor a new entity. Thus Et starts as {1} and grows monotonically with t, allowing for an arbitrary number of entities to be mentioned. We denote the value of Et by et. If Rt = 0, then Et is fixed to a special value ø.\nThe values of these random variables for our running example are shown in Figure 2.\nIn addition to using symbolic variables to encode mentions and coreference relationships, we maintain a vector representation of each entity that evolves over time. For the ith entity, let ei,t be its representation at time t. These vectors are different from word vectors (xt), in that they are not parameters of the model. They are similar to history representations (ht), in that they are derived through parameterized functions of the random variables’ values, which we will describe in the next subsections."
    }, {
      "heading" : "2.2 Generative story",
      "text" : "The generative story for the word (and other variables) at timestep t is as follows; forwardreferenced equations are in the detailed discussion that follows.\n1. If ℓt−1 = 1 (i.e., xt is not continuing an already-started entity mention):\n• Choose rt (Equation 3). • If rt = 0, set ℓt = 1 and et = ø; then go to step 3. Otherwise:\n– If there is no embedding for the\nnew candidate entity with index 1 + maxt′<t et′ , create one following §2.4. – Select the entity et from {1, . . . , 1+ maxt′<t et′} (Equation 4). – Set ecurrent = eet,t−1, which is the entity embedding of et before\ntimestep t.\n– Select the length of the mention, ℓt (Equation 5).\n2. Otherwise,\n• Set ℓt = ℓt−1 − 1, rt = rt−1, et = et−1.\n3. Sample xt from the word distribution given\nthe LSTM hidden state ht−1 and the current (or most recent) entity embedding ecurrent (Equation 6). (If rt = 0, then ecurrent still represents the most recently mentioned entity.)\n4. Advance the RNN, i.e., feed it the word vec-\ntor xt to compute ht (Equation 2).\n5. If rt = 1, update eet,t using eet,t−1 and ht, then set ecurrent = eet,t. Details of the entity updating are given in §2.4.\n6. For every entity eι ∈ Et \\ {et}, set eι,t = eι,t−1 (i.e., no changes to other entities’ rep-\nresentations).\nNote that at any given time step t, ecurrent will always contain the most recent vector representation of the most recently mentioned entity.\nA generative model with a similar hierarchical structure was used by Haghighi and Klein (2010) for coreference resolution. Our approach differs in two important ways. First, our model defines a joint distribution over all of the text, not just the entity mentions. Second, we use representation learning rather than Bayesian nonparametrics, allowing natural integration with the language model."
    }, {
      "heading" : "2.3 Probability distributions",
      "text" : "The generative story above referenced several parametric distributions defined based on vector representations of histories and entities. These are defined as follows.\nFor r ∈ {0, 1},\np(Rt = r | ht−1) ∝ exp(h ⊤ t−1Wrr), (3)\nwhere r is the parameterized embedding associated with r, which paves the way for exploring entity type representations in future work; Wr is a\nparameter matrix for the bilinear score for ht−1 and r.\nTo give the possibility of predicting a new entity, we need an entity embedding beforehand with index (1 + maxt′<t et′), which is randomly sampled from Equation 7. Then, for every e ∈ {1, . . . , 1 + maxt′<t et′}:\np(Et = e | Rt = 1,ht−1)\n∝ exp(h⊤t−1Wentityee,t−1 +w ⊤ dist f(e)),\n(4)\nwhere ee,t−1 is the embedding of entity e at time step t−1 andWentity is the weight matrix for predicting entities using their continuous representations. The score above is normalized over values {1, . . . , 1 + maxt′<t et′}. f(e) represents a vector of distance features associated with e and the mentions of the existing entities. Hence two information sources are used to predict the next entity: (i) contextual information ht−1, and (ii) distance features f(e) from the current mention to the closest mention from each previously mentioned entity. f(e) = 0 if e is a new entity. This term can also be extended to include other surface-form features for coreference resolution (Martschat and Strube, 2015; Clark and Manning, 2016b).\nFor the chosen entity et from Equation 4, the distribution over its mention length is drawn according to\np(Lt = ℓ | ht−1, eet,t−1)\n∝ exp(W⊤length,ℓ[ht−1; eet,t−1]), (5)\nwhere eet,t−1 is the most recent embedding of the entity et, not updated with ht. The intuition is that eet,t−1 will help contextual information ht−1 to select the residual length of entity et. Wlength is the weight matrix for length prediction, with ℓmax = 25 rows.\nFinally, the probability of a word x as the next token is jointly modeled by ht−1 and the vector representation of the most recently mentioned en-\ntity ecurrent :\np(Xt = x | ht−1, ecurrent)\n∝ CFSM(ht−1 +Weecurrent ), (6)\nwhere We is a transformation matrix to adjust the dimensionality of ecurrent . CFSM is a class factorized softmax function (Goodman, 2001; Baltescu and Blunsom, 2015). It uses a twostep prediction with predefined word classes instead of direct prediction on the whole vocabulary, and reduces the time complexity to the log of vocabulary size."
    }, {
      "heading" : "2.4 Dynamic entity representations",
      "text" : "Before predicting the entity at step t, we need an embedding for the new candidate entity with index e′ = 1 + maxt′<t et′ if it does not exist. The new embedding is generated randomly, according to a normal distribution, then projected onto the unit ball:\nu ∼ N (r1, σ 2 I);\nee′,t−1 = u\n‖u‖2 ,\n(7)\nwhere σ = 0.01. The time step t − 1 in ee′,t−1 means the current embedding contains no information from step t, although it will be updated once we have ht and if Et = e ′. r1 is the parameterized embedding for Rt = 1, which will be jointly optimized with other parameters and is expected to encode some generic information about entities. All the initial entity embeddings are centered on the mean r1, which is used in Equation 3 to determine whether the next token belongs to an entity mention. Another choice would be to initialize with a zero vector, although our preliminary experiments showed this did not work as well as random initialization in Equation 7.\nAssume Rt = 1 and Et = et, which means xt is part of a mention of entity et. Then, we need to update eet,t−1 based on the new information we have from ht. The new embedding eet,t is a convex combination of the old embedding (eet,t−1) and current LSTM hidden state (ht) with the interpolation (δt) determined dynamically based on a bilinear function:\nδt = σ(h ⊤ t Wδeet,t−1); u = δteet,t−1 + (1− δt)ht;\neet,t = u\n‖u‖2 , (8)\nThis updating scheme will be used to update et in each of all the following ℓt steps. The projection in the last step keeps the magnitude of the entity embedding fixed, avoiding numeric overflow. A similar updating scheme has been used by Henaff et al. (2016) for the “memory blocks” in their recurrent entity network models. The difference is that their model updates all memory blocks in each time step. Instead, our updating scheme in Equation 8 only applies to the selected entity et at time step t."
    }, {
      "heading" : "2.5 Training objective",
      "text" : "The model is trained to maximize the log of the joint probability of R,E,L, and X:\nℓ(θ) = log P (R,E,L,X;θ)\n= ∑\nt\nlogP (Rt, Et, Lt,Xt;θ), (9)\nwhere θ is the collection of all the parameters in this model. Based on the formulation in §2.3, Equation 9 can be decomposed as the sum of conditional log-probabilities of each random variable at each time step.\nThis objective requires the training data annotated as in Figure 2. We do not assume that these variables are observed at test time."
    }, {
      "heading" : "3 Implementation Details",
      "text" : "Our model is implemented with DyNet (Neubig et al., 2017) and available at https://github.com/jiyfeng/entitynlm. We use AdaGrad (Duchi et al., 2011) with learning rate λ = 0.1 and ADAM (Kingma and Ba, 2014) with default learning rate λ = 0.001 as the candidate optimizers of our model. For all the parameters, we use the initialization tricks recommended by Glorot and Bengio (2010). To avoid overfitting, we also employ dropout (Srivastava et al., 2014) with the candidate rates as {0.2, 0.5}. In addition, there are two tunable hyperparameters of ENTITYNLM: the size of word embeddings and the dimension of LSTM hidden states. For both of them, we consider the values {32, 48, 64, 128, 256}. We also experiment with the option to either use the pretrained GloVe word embeddings (Pennington et al., 2014) or randomly initialized word embeddings (then updated during training). For all experiments, the best configuration of hyperparameters and optimizers is selected based on the objective value on the development data."
    }, {
      "heading" : "4 Evaluation Tasks and Datasets",
      "text" : "We evaluate our model in diverse use scenarios: (i) language modeling, (ii) coreference resolution, and (iii) entity prediction. The evaluation on language modeling shows how the internal entity representation, when marginalized out, can improve the perplexity of language models. The evaluation on coreference resolution experiment shows how our new language model can improve a competitive coreference resolution system. Finally, we employ an entity cloze task to demonstrate the generative performance of our model in predicting the next entity given the previous context.\nWe use two datasets for the three evaluation tasks. For language modeling and coreference resolution, we use the English benchmark data from the CoNLL 2012 shared task on coreference resolution (Pradhan et al., 2012). We employ the standard training/development/test split, which includes 2,802/343/348 documents with roughly 1M/150K/150K tokens, respectively. We follow the coreference annotation in the CoNLL dataset to extract entities and ignore the singleton mentions in texts.\nFor entity prediction, we employ the InScript corpus created by Modi et al. (2017). It consists of 10 scenarios, including grocery shopping, taking a flight, etc. It includes 910 crowdsourced simple narrative texts in total and 18 stories were ignored due to labeling problems (Modi et al., 2017). On average, each story has 12.4 sentences, 24.9 entities and 217.2 tokens. Each entity mention is labeled with its entity index. We use the same training/development/test split as in (Modi et al., 2017), which includes 619, 91, 182 texts, respectively.\nData preprocessing\nFor the CoNLL dataset, we lowercase all tokens and remove any token that only contains a punctuation symbol unless it is in an entity mention. We also replace numbers in the documents with the special token NUM and low-frequency word types with UNK. The vocabulary size of the CoNLL data after preprocessing is 10K. For entity mention extraction, in the CoNLL dataset, one entity mention could be embedded in another. For embedded mentions, only the enclosing entity mention is kept. We use the same preprocessed data for both language modeling and coreference resolution evaluation.\nFor the InScript corpus, we apply similar data preprocessing to lowercase all tokens, and we replace low-frequency word types with UNK. The vocabulary size after preprocessing is 1K."
    }, {
      "heading" : "5 Experiments",
      "text" : "In this section, we present the experimental results on the three evaluation tasks."
    }, {
      "heading" : "5.1 Language modeling",
      "text" : "Task description. The goal of language modeling is to compute the marginal probability:\nP (X) = ∑\nR,E,L\nP (X,R,E,L). (10)\nHowever, due to the long-range dependency in recurrent neural networks, the search space of R,E,L during inference grows exponentially. We thus use importance sampling to approximate the marginal distribution of X. Specifically, with the samples from a proposal distribution Q(R,E,L|X), the approximated marginal probability is defined as\nP (X) = ∑\nR,E,L\nP (X,R,E,L)\n= ∑\nR,E,L\nQ(R,E,L | X) P (X,R,E,L)\nQ(R,E,L | X)\n≈ 1\nN\n∑\n{r(i),e(i),ℓ(i)}∼Q\nP (r(i), e(i), ℓ(i),x)\nQ(r(i), e(i), ℓ(i) | x)\n(11)\nA similar idea of using importance sampling for language modeling evaluation has been used by Dyer et al. (2016).\nFor language modeling evaluation, we train our model on the training set from the CoNLL 2012 dataset with coreference annotation. On the test data, we treat coreference structure as latent variables and use importance sampling to approximate the marginal distribution of X. For each document, the model randomly draws N = 100 samples from the proposal distribution, discussed next.\nProposal distribution. For implementation of Q, we use a discriminative variant of ENTITYNLM by taking the current word xt for predicting the entity-related variables in the same time step. Specifically, in the generative story described in §2.2, we delete step 3 (words are not generated, but rather conditioned upon), move step 4\nbefore step 1, and replace ht−1 with ht in the steps for predicting entity type Rt, entity Et and mention length Lt. This model variant provides a conditional probability Q(Rt, Et, Lt | Xt) at each timestep.\nBaselines. We compare the language modeling performance with two competitive baselines: 5- gram language model implemented in KenLM (Heafield et al., 2013) and RNNLM with LSTM units implemented in DyNet (Neubig et al., 2017). For RNNLM, we use the same hyperparameters described in §3 and grid search on the development data to find the best configuration.\nResults. The results of ENTITYNLM and the baselines on both development and test data are reported in Table 1. For ENTITYNLM, we use the value of 2− 1 T ∑ T t=1 logP (Xt,Rt,Et,Lt) on the development set with coreference annotation to select the best model configuration and report the best number. On the test data, we are able to calculate perplexity by marginalizing all other random variables using Equation 11. To compute the perplexity numbers on the test data, our model only takes account of log probabilities on word prediction. The difference is that coreference information is only used for training ENTITYNLM and not for test. All three models reported in Table 1 share the same vocabulary, therefore the numbers on the test data are directly comparable. As shown in Table 1, ENTITYNLM outperforms both the 5- gram language model and the RNNLM on the test data. Better performance of ENTITYNLM on language modeling can be expected, if we also use the marginalization method defined in Equation 11 on the development data to select the best configuration. However, we plan to use the same experimental setup for all experiments, instead of customizing our model for each individual task."
    }, {
      "heading" : "5.2 Coreference reranking",
      "text" : "Task description. We show how ENTITYLM, which allows an efficient computation of the probability P (R,E,L,X), can be used as a coreference reranker to improve a competitive coreference resolution system due to Martschat and Strube (2015). This task is analogous to the reranking approach used in machine translation (Shen et al., 2004). The specific formulation is as follows:\narg max {r(i),e(i),l(i)}∈K\nP (r(i), e(i), l(i),x) (12)\nwhere K is the k-best list for a given document. In our experiments, k = 100. To the best of our knowledge, the problem of obtaining k-best outputs of a coreference resolution system has not been studied before.\nApproximate k-best decoding. We rerank the output of a system that predicts an antecedent for each mention by relying on pairwise scores for mention pairs. This is the dominant approach for coreference resolution (Martschat and Strube, 2015; Clark and Manning, 2016a). The predictions induce an antecedent tree, which represents antecedent decisions for all mentions in the document. Coreference chains are obtained by transitive closure over the antecedent decisions encoded in the tree. A mention also can have an empty mention as antecedent, which denotes that the mention is non-anaphoric.\nFor extending Martschat and Strube’s greedy decoding approach to k-best inference, we cannot simply take the k highest scoring trees according to the sum of edge scores, because different trees may represent the same coreference chain. Instead, we use an heuristic that creates an approximate k-best list on candidate antecedent trees. The idea is to generate trees from the original system output by considering suboptimal antecedent choices that lead to different coreference chains. For each mention pair (mj ,mi), we compute the difference of its score to the score of the optimal antecedent choice for mj . We then sort pairs in ascending order according to this difference and iterate through the list of pairs. For each pair (mj,mi), we create a tree tj,i by replacing the antecedent of mj in the original system output with mi. If this yields a tree that encodes different coreference chains from all chains encoded by trees in the k-best list, we add ti,j to the k-best list.\nIn the case that we cannot generate a given number of trees (particularly for a short document with a large k), we pad the list with the last item added to the list.\nEvaluation measures. For coreference resolution evaluation, we employ the CoNLL scorer (Pradhan et al., 2014). It computes three commonly used evaluation measures MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFe (Luo, 2005). We report the F1 score of each evaluation measure and their average as the CoNLL score.\nCompeting systems. We employed CORT1 (Martschat and Strube, 2015) as our baseline coreference resolution system. Here, we compare with the original (one best) outputs of CORT’s latent ranking model, which is the bestperforming model implemented in CORT. We consider two rerankers based on ENTITYNLM. The first reranking method only uses the log probability for ENTITYNLM to sort the candidate list (Equation 12). The second method uses a linear combination of both log probabilities from ENTITYNLM and the scores from CORT, where the coefficients were found via grid search with the CoNLL score on the development set.\nResults. The reranked results on the CoNLL 2012 test set are reported in Table 2. The numbers of the baseline are higher than the results reported in Martschat and Strube (2015) since the feature set of CORT was subsequently extended. Lines 2 and 3 in Table 2 present the reranked best results. As shown in this table, both reranked results give more than 1% of CoNLL score improvement on the test set over CORT, which are significant based on an approximate randomization test2.\nAdditional experiments also found that increasing k from 100 to 500 had a minor effect. That is because the diversity of each k-best list is limited by (i) the number of entity mentions in the document, (ii) the performance of the baseline coreference resolution system, and possibly (iii) the approximate nature of our k-best inference procedure. We suspect that a stronger baseline system (such as that of Clark and Manning, 2016a) could give greater improvements, if it can be adapted to provide k-best lists. Future work might incorpo-\n1https://github.com/smartschat/cort, we used version 0.2.4.5.\n2 https://github.com/smartschat/art\nprediction task requires predicting xxxx given the preceding text either by choosing a previously mentioned entity or deciding that this is a “new entity”. In this example, the ground-truth prediction is [tire]4. For training, ENTITYNLM attempts to predict every entity. While, for testing, it predicts a maximum of 30 entities after the first three sentences, which is consistent with the experimental setup suggested by Modi et al. (2017).\nrate the techniques embedded in such systems into ENTITYNLM."
    }, {
      "heading" : "5.3 Entity prediction",
      "text" : "Task description. Based on Modi et al. (2017), we introduce a novel entity prediction task that tries to predict the next entity given the preceding text. For a given text as in Figure 3, this task makes a forward prediction based on only the left context. This is different from coreference resolution, where both left and right contexts from a given entity mention are used in decoding. It is also different from language modeling, since this task only requires predicting entities. Since ENTITYNLM is generative, it can be directly applied to this task. To predict entities in test data, Rt is always given and ENTITYNLM only needs to predict Et when Rt = 1.\nBaselines and human prediction. We introduce two baselines in this task: (i) the always-new baseline that always predicts “new entity”; (ii) a linear classification model using shallow features from Modi et al. (2017), including the recency of an entity’s last mention and the frequency. We also compare with the model proposed by Modi et al. (2017). Their work assumes that the model has prior knowledge of all the participant types, which are specific to each scenario and fine-grained, e.g., rider in the bicycle narrative, and predicts participant types for new entities. This assumption is unrealistic for pure generative models like ours. Therefore, we remove this assumption and adapt\ntheir prediction results to our formulation by mapping all the predicted entities that have not been mentioned to “new entity”. We also compare to the adapted human prediction used in the InScript corpus. For each entity slot, Modi et al. (2017) acquired 20 human predictions, and the majority vote was selected. More details about human predictions are discussed in (Modi et al., 2017).\nResults. Table 3 shows the prediction accuracies. ENTITYNLM (line 4) significantly outperforms both baselines (line 1 and 2) and prior work (line 3) (p ≪ 0.01, paired t-test). The comparison between line 4 and 5 shows our model is even close to the human prediction performance."
    }, {
      "heading" : "6 Related Work",
      "text" : "Rich-context language models. The originally proposed recurrent neural network language models only capture information within sentences. To extend the capacity of RNNLMs, various researchers have incorporated information beyond sentence boundaries. Previous work focuses on contextual information from previous sentences (Ji et al., 2016a) or discourse relations between adjacent sentences (Ji et al., 2016b), showing improvements to language modeling and related tasks like coherence evaluation and discourse relation prediction. In this work, ENTITYNLM adds explicit entity information to the language model, which is another way of adding a memory network for language modeling. Unlike the work\nby Tran et al. (2016), where memory blocks are used to store general contextual information for language modeling, ENTITYNLM assigns each memory block specifically to only one entity.\nEntity-related models. Two recent approaches to modeling entities in text are closely related to our model. The first is the “reference-aware” language models proposed by Yang et al. (2016), where the referred entities are from either a predefined item list, an external database, or the context from the same document. Yang et al. (2016) present three models, one for each case. For modeling a document with entities, they use coreference links to recover entity clusters, though they only model entity mentions as containing a single word (an inappropriate assumption, in our view). Their entity updating method takes the latest hidden state (similar to ht whenRt = 1 in our model) as the new representation of the current entity; no long-term history of the entity is maintained, just the current local context. In addition, their language model evaluation assumes that entity information is provided at test time (Yang, personal communication), which makes a direct comparison with our model impossible. Our entity updating scheme is similar to the “dynamic memory” method used by Henaff et al. (2016). Our entity representations are dynamically allocated and updated only when an entity appears up, while the EntNet from Henaff et al. (2016) does not model entities and their relationships explicitly. In their model, entity memory blocks are pre-allocated and updated simultaneously in each timestep. So there is no dedicated memory block for every entity and no distinction between entity mentions and non-mention words. As a consequence, it is not clear how to use their model for coreference reranking and entity prediction.\nCoreference resolution. The hierarchical structure of our entity generation model is inspired by Haghighi and Klein (2010). They implemented this idea as a probabillistic graphical model with\nthe distance-dependent Chinese Restaurant Process (Pitman, 1995) for entity assignment, while our model is built on a recurrent neural network architecture. The reranking method considered in our coreference resolution evaluation could also be extended with samples from additional coreference resolution systems, to produce more variety (Ng, 2005). The benefit of such a system comes, we believe, from the explicit tracking of each entity throughout the text, providing entity-specific representations. In previous work, such information has been added as features (Luo et al., 2004; Björkelund and Kuhn, 2014) or by computing distributed entity representations (Wiseman et al., 2016; Clark and Manning, 2016b). Our approach complements these previous methods.\nEntity prediction. The entity prediction task discussed in §5.3 is based on work by Modi et al. (2017). The main difference is that we do not assume that all entities belong to a previously known set of entity types specified for each narrative scenario. This task is also closely related to the “narrative cloze” task of Chambers and Jurafsky (2008) and the “story cloze test” of Mostafazadeh et al. (2016). Those studies aim to understand relationships between events, while our task focuses on predicting upcoming entity mentions."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We have presented a neural language model, ENTITYNLM, that defines a distribution over texts and the mentioned entities. It provides vector representations for the entities and updates them dynamically in context. The dynamic representations are further used to help generate specific entity mentions and the following text. This model outperforms strong baselines and prior work on three tasks: language modeling, coreference resolution, and entity prediction."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank anonymous reviewers for the helpful feedback on this work. We also thank the members of Noah’s ARK and XLab at University of Washington for their valuable comments, particularly Eunsol Choi for pointing out the InScript corpus. This research was supported in part by a University of Washington Innovation Award, Samsung GRO, NSF grant IIS-1524371, the DARPA CwC\nprogram through ARO (W911NF-15-1-0543), and gifts by Google and Facebook."
    } ],
    "references" : [ {
      "title" : "Algorithms for scoring coreference chains",
      "author" : [ "Amit Bagga", "Breck Baldwin." ],
      "venue" : "LREC Workshop on Linguistic Coreference.",
      "citeRegEx" : "Bagga and Baldwin.,? 1998",
      "shortCiteRegEx" : "Bagga and Baldwin.",
      "year" : 1998
    }, {
      "title" : "Pragmatic neural language modelling in machine translation",
      "author" : [ "Paul Baltescu", "Phil Blunsom." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Baltescu and Blunsom.,? 2015",
      "shortCiteRegEx" : "Baltescu and Blunsom.",
      "year" : 2015
    }, {
      "title" : "Learning structured perceptrons for coreference resolution with latent antecedents and non-local features",
      "author" : [ "Anders Björkelund", "Jonas Kuhn." ],
      "venue" : "ACL.",
      "citeRegEx" : "Björkelund and Kuhn.,? 2014",
      "shortCiteRegEx" : "Björkelund and Kuhn.",
      "year" : 2014
    }, {
      "title" : "Unsupervised Learning of Narrative Event Chains",
      "author" : [ "Nathanael Chambers", "Daniel Jurafsky." ],
      "venue" : "ACL.",
      "citeRegEx" : "Chambers and Jurafsky.,? 2008",
      "shortCiteRegEx" : "Chambers and Jurafsky.",
      "year" : 2008
    }, {
      "title" : "Deep reinforcement learning for mention-ranking coreference models",
      "author" : [ "Kevin Clark", "Christopher D. Manning." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Clark and Manning.,? 2016a",
      "shortCiteRegEx" : "Clark and Manning.",
      "year" : 2016
    }, {
      "title" : "Improving coreference resolution by learning entitylevel distributed representations",
      "author" : [ "Kevin Clark", "Christopher D. Manning." ],
      "venue" : "ACL.",
      "citeRegEx" : "Clark and Manning.,? 2016b",
      "shortCiteRegEx" : "Clark and Manning.",
      "year" : 2016
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "John Duchi", "Elad Hazan", "Yoram Singer." ],
      "venue" : "Journal of Machine Learning Research, 12:2121–2159.",
      "citeRegEx" : "Duchi et al\\.,? 2011",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2011
    }, {
      "title" : "Recurrent neural network grammars",
      "author" : [ "Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A. Smith." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Dyer et al\\.,? 2016",
      "shortCiteRegEx" : "Dyer et al\\.",
      "year" : 2016
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "Xavier Glorot", "Yoshua Bengio." ],
      "venue" : "AISTATS, pages 249–256.",
      "citeRegEx" : "Glorot and Bengio.,? 2010",
      "shortCiteRegEx" : "Glorot and Bengio.",
      "year" : 2010
    }, {
      "title" : "Classes for fast maximum entropy training",
      "author" : [ "Joshua Goodman." ],
      "venue" : "ICASSP.",
      "citeRegEx" : "Goodman.,? 2001",
      "shortCiteRegEx" : "Goodman.",
      "year" : 2001
    }, {
      "title" : "Coreference resolution in a modular, entity-centered model",
      "author" : [ "Aria Haghighi", "Dan Klein." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Haghighi and Klein.,? 2010",
      "shortCiteRegEx" : "Haghighi and Klein.",
      "year" : 2010
    }, {
      "title" : "Scalable modified Kneser-Ney language model estimation",
      "author" : [ "Kenneth Heafield", "Ivan Pouzyrevsky", "Jonathan H. Clark", "Philipp Koehn." ],
      "venue" : "ACL.",
      "citeRegEx" : "Heafield et al\\.,? 2013",
      "shortCiteRegEx" : "Heafield et al\\.",
      "year" : 2013
    }, {
      "title" : "Tracking the world state with recurrent entity networks",
      "author" : [ "Mikael Henaff", "Jason Weston", "Arthur Szlam", "Antoine Bordes", "Yann LeCun." ],
      "venue" : "arXiv:1612.03969.",
      "citeRegEx" : "Henaff et al\\.,? 2016",
      "shortCiteRegEx" : "Henaff et al\\.",
      "year" : 2016
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural Computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Document context language models",
      "author" : [ "Yangfeng Ji", "Trevor Cohn", "LingpengKong", "Chris Dyer", "Jacob Eisenstein." ],
      "venue" : "ICLR (workshop track).",
      "citeRegEx" : "Ji et al\\.,? 2016a",
      "shortCiteRegEx" : "Ji et al\\.",
      "year" : 2016
    }, {
      "title" : "A latent variable recurrent neural network for discourse-driven language models",
      "author" : [ "Yangfeng Ji", "Gholamreza Haffari", "Jacob Eisenstein." ],
      "venue" : "NAACL-HLT.",
      "citeRegEx" : "Ji et al\\.,? 2016b",
      "shortCiteRegEx" : "Ji et al\\.",
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba." ],
      "venue" : "arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "On coreference resolution performance metrics",
      "author" : [ "Xiaoqiang Luo." ],
      "venue" : "HLT-EMNLP.",
      "citeRegEx" : "Luo.,? 2005",
      "shortCiteRegEx" : "Luo.",
      "year" : 2005
    }, {
      "title" : "A mentionsynchronous coreference resolution algorithm based on the Bell tree",
      "author" : [ "Xiaoqiang Luo", "Abe Ittycheriah", "Hongyan Jing", "Nanda Kambhatla", "Salim Roukos." ],
      "venue" : "ACL.",
      "citeRegEx" : "Luo et al\\.,? 2004",
      "shortCiteRegEx" : "Luo et al\\.",
      "year" : 2004
    }, {
      "title" : "Latent structures for coreference resolution",
      "author" : [ "Sebastian Martschat", "Michael Strube." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 3:405–418.",
      "citeRegEx" : "Martschat and Strube.,? 2015",
      "shortCiteRegEx" : "Martschat and Strube.",
      "year" : 2015
    }, {
      "title" : "Recurrent neural network based language model",
      "author" : [ "Tomas Mikolov", "Martin Karafiát", "Lukas Burget", "Jan Cernockỳ", "Sanjeev Khudanpur." ],
      "venue" : "INTERSPEECH.",
      "citeRegEx" : "Mikolov et al\\.,? 2010",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2010
    }, {
      "title" : "Modeling semantic expectation: Using script knowledge for referent prediction",
      "author" : [ "Ashutosh Modi", "Ivan Titov", "Vera Demberg", "Asad Sayeed", "Manfred Pinkal." ],
      "venue" : "Transactions of the Association of Computational Linguistics, 5:31–44.",
      "citeRegEx" : "Modi et al\\.,? 2017",
      "shortCiteRegEx" : "Modi et al\\.",
      "year" : 2017
    }, {
      "title" : "A corpus and evaluation framework for deeper understanding of commonsense stories",
      "author" : [ "Nasrin Mostafazadeh", "Nathanael Chambers", "Xiaodong He", "Devi Parikh", "Dhruv Batra", "Lucy Vanderwende", "Pushmeet Kohli", "James Allen." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Mostafazadeh et al\\.,? 2016",
      "shortCiteRegEx" : "Mostafazadeh et al\\.",
      "year" : 2016
    }, {
      "title" : "Dynet: The dynamic neural network toolkit",
      "author" : [ "Graham Neubig", "Chris Dyer", "Yoav Goldberg", "Austin Matthews", "Waleed Ammar", "Antonios Anastasopoulos", "Miguel Ballesteros", "David Chiang", "Daniel Clothiaux", "Trevor Cohn" ],
      "venue" : null,
      "citeRegEx" : "Neubig et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Neubig et al\\.",
      "year" : 2017
    }, {
      "title" : "Machine learning for coreference resolution: From local classification to global ranking",
      "author" : [ "Vincent Ng." ],
      "venue" : "ACL.",
      "citeRegEx" : "Ng.,? 2005",
      "shortCiteRegEx" : "Ng.",
      "year" : 2005
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D. Manning." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Exchangeable and partially exchangeable random partitions",
      "author" : [ "Jim Pitman." ],
      "venue" : "Probability Theory and Related Fields, 102(2):145–158.",
      "citeRegEx" : "Pitman.,? 1995",
      "shortCiteRegEx" : "Pitman.",
      "year" : 1995
    }, {
      "title" : "Scoring coreference partitions of predicted mentions: A reference implementation",
      "author" : [ "Sameer Pradhan", "Xiaoqiang Luo", "Marta Recasens", "Eduard Hovy", "Vincent Ng", "Michael Strube." ],
      "venue" : "ACL.",
      "citeRegEx" : "Pradhan et al\\.,? 2014",
      "shortCiteRegEx" : "Pradhan et al\\.",
      "year" : 2014
    }, {
      "title" : "CoNLL2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes",
      "author" : [ "Sameer Pradhan", "Alessandro Moschitti", "Nianwen Xue", "Olga Uryupina", "Yuchen Zhang." ],
      "venue" : "EMNLPCoNLL.",
      "citeRegEx" : "Pradhan et al\\.,? 2012",
      "shortCiteRegEx" : "Pradhan et al\\.",
      "year" : 2012
    }, {
      "title" : "Discriminative reranking for machine translation",
      "author" : [ "Libin Shen", "Anoop Sarkar", "Franz Josef Och." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Shen et al\\.,? 2004",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2004
    }, {
      "title" : "Dropout: A simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov." ],
      "venue" : "Journal of Machine Learning Research, 15(1):1929–1958.",
      "citeRegEx" : "Srivastava et al\\.,? 2014",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2014
    }, {
      "title" : "Recurrent memory networks for languagemodeling",
      "author" : [ "Ke Tran", "Arianna Bisazza", "Christof Monz." ],
      "venue" : "NAACL-HLT.",
      "citeRegEx" : "Tran et al\\.,? 2016",
      "shortCiteRegEx" : "Tran et al\\.",
      "year" : 2016
    }, {
      "title" : "A modeltheoretic coreference scoring scheme",
      "author" : [ "Marc Vilain", "John Burger", "John Aberdeen", "Dennis Connolly", "Lynette Hirschman." ],
      "venue" : "MUC.",
      "citeRegEx" : "Vilain et al\\.,? 1995",
      "shortCiteRegEx" : "Vilain et al\\.",
      "year" : 1995
    }, {
      "title" : "Learning global features for coreference resolution",
      "author" : [ "Sam Wiseman", "Alexander M. Rush", "Stuart M. Shieber." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Wiseman et al\\.,? 2016",
      "shortCiteRegEx" : "Wiseman et al\\.",
      "year" : 2016
    }, {
      "title" : "Reference-aware language models",
      "author" : [ "Zichao Yang", "Phil Blunsom", "Chris Dyer", "Wang Ling." ],
      "venue" : "arXiv:1611.01628.",
      "citeRegEx" : "Yang et al\\.,? 2016",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2016
    }, {
      "title" : "Recurrent neural network regularization",
      "author" : [ "Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Zaremba et al\\.,? 2015",
      "shortCiteRegEx" : "Zaremba et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 28,
      "context" : "1, we find that it outperforms both a strong n-gram language model and a strong recurrent neural network language model on the English test set of the CoNLL 2012 shared task on coreference evaluation (Pradhan et al., 2012).",
      "startOffset" : 200,
      "endOffset" : 222
    }, {
      "referenceID" : 21,
      "context" : "3, it achieves state-of-the-art performance on the InScript corpus (Modi et al., 2017).",
      "startOffset" : 67,
      "endOffset" : 86
    }, {
      "referenceID" : 20,
      "context" : "Our starting point for language modeling is a recurrent neural network (Mikolov et al., 2010), which defines",
      "startOffset" : 71,
      "endOffset" : 93
    }, {
      "referenceID" : 13,
      "context" : "where Wh and b are parameters of the model (along with word embeddings xt), LSTM is the widely used recurrent function known as “long short-term memory” (Hochreiter and Schmidhuber, 1997), and ht is a LSTM hidden state encoding the history of the sequence up to the tth word.",
      "startOffset" : 153,
      "endOffset" : 187
    }, {
      "referenceID" : 35,
      "context" : "Great success has been reported for this model (Zaremba et al., 2015), which posits nothing explicitly about the words appearing in the text sequence.",
      "startOffset" : 47,
      "endOffset" : 69
    }, {
      "referenceID" : 10,
      "context" : "A generative model with a similar hierarchical structure was used by Haghighi and Klein (2010) for coreference resolution.",
      "startOffset" : 69,
      "endOffset" : 95
    }, {
      "referenceID" : 19,
      "context" : "This term can also be extended to include other surface-form features for coreference resolution (Martschat and Strube, 2015; Clark and Manning, 2016b).",
      "startOffset" : 97,
      "endOffset" : 151
    }, {
      "referenceID" : 5,
      "context" : "This term can also be extended to include other surface-form features for coreference resolution (Martschat and Strube, 2015; Clark and Manning, 2016b).",
      "startOffset" : 97,
      "endOffset" : 151
    }, {
      "referenceID" : 9,
      "context" : "CFSM is a class factorized softmax function (Goodman, 2001; Baltescu and Blunsom, 2015).",
      "startOffset" : 44,
      "endOffset" : 87
    }, {
      "referenceID" : 1,
      "context" : "CFSM is a class factorized softmax function (Goodman, 2001; Baltescu and Blunsom, 2015).",
      "startOffset" : 44,
      "endOffset" : 87
    }, {
      "referenceID" : 12,
      "context" : "A similar updating scheme has been used by Henaff et al. (2016) for the “memory blocks” in their recurrent entity network models.",
      "startOffset" : 43,
      "endOffset" : 64
    }, {
      "referenceID" : 23,
      "context" : "Our model is implemented with DyNet (Neubig et al., 2017) and available at https://github.",
      "startOffset" : 36,
      "endOffset" : 57
    }, {
      "referenceID" : 6,
      "context" : "We use AdaGrad (Duchi et al., 2011) with learning rate λ = 0.",
      "startOffset" : 15,
      "endOffset" : 35
    }, {
      "referenceID" : 16,
      "context" : "1 and ADAM (Kingma and Ba, 2014) with default learning rate λ = 0.",
      "startOffset" : 11,
      "endOffset" : 32
    }, {
      "referenceID" : 30,
      "context" : "To avoid overfitting, we also employ dropout (Srivastava et al., 2014) with the candidate rates as {0.",
      "startOffset" : 45,
      "endOffset" : 70
    }, {
      "referenceID" : 25,
      "context" : "We also experiment with the option to either use the pretrained GloVe word embeddings (Pennington et al., 2014) or randomly initialized word embeddings (then updated during training).",
      "startOffset" : 86,
      "endOffset" : 111
    }, {
      "referenceID" : 6,
      "context" : "We use AdaGrad (Duchi et al., 2011) with learning rate λ = 0.1 and ADAM (Kingma and Ba, 2014) with default learning rate λ = 0.001 as the candidate optimizers of our model. For all the parameters, we use the initialization tricks recommended by Glorot and Bengio (2010). To avoid overfitting, we also employ dropout (Srivastava et al.",
      "startOffset" : 16,
      "endOffset" : 270
    }, {
      "referenceID" : 28,
      "context" : "For language modeling and coreference resolution, we use the English benchmark data from the CoNLL 2012 shared task on coreference resolution (Pradhan et al., 2012).",
      "startOffset" : 142,
      "endOffset" : 164
    }, {
      "referenceID" : 21,
      "context" : "It includes 910 crowdsourced simple narrative texts in total and 18 stories were ignored due to labeling problems (Modi et al., 2017).",
      "startOffset" : 114,
      "endOffset" : 133
    }, {
      "referenceID" : 21,
      "context" : "We use the same training/development/test split as in (Modi et al., 2017), which includes 619, 91, 182 texts, respectively.",
      "startOffset" : 54,
      "endOffset" : 73
    }, {
      "referenceID" : 21,
      "context" : "For entity prediction, we employ the InScript corpus created by Modi et al. (2017). It consists of 10 scenarios, including grocery shopping, taking a flight, etc.",
      "startOffset" : 64,
      "endOffset" : 83
    }, {
      "referenceID" : 7,
      "context" : "A similar idea of using importance sampling for language modeling evaluation has been used by Dyer et al. (2016). For language modeling evaluation, we train our model on the training set from the CoNLL 2012 dataset with coreference annotation.",
      "startOffset" : 94,
      "endOffset" : 113
    }, {
      "referenceID" : 11,
      "context" : "We compare the language modeling performance with two competitive baselines: 5gram language model implemented in KenLM (Heafield et al., 2013) and RNNLM with LSTM units implemented in DyNet (Neubig et al.",
      "startOffset" : 119,
      "endOffset" : 142
    }, {
      "referenceID" : 23,
      "context" : ", 2013) and RNNLM with LSTM units implemented in DyNet (Neubig et al., 2017).",
      "startOffset" : 55,
      "endOffset" : 76
    }, {
      "referenceID" : 29,
      "context" : "This task is analogous to the reranking approach used in machine translation (Shen et al., 2004).",
      "startOffset" : 77,
      "endOffset" : 96
    }, {
      "referenceID" : 19,
      "context" : "This is the dominant approach for coreference resolution (Martschat and Strube, 2015; Clark and Manning, 2016a).",
      "startOffset" : 57,
      "endOffset" : 111
    }, {
      "referenceID" : 4,
      "context" : "This is the dominant approach for coreference resolution (Martschat and Strube, 2015; Clark and Manning, 2016a).",
      "startOffset" : 57,
      "endOffset" : 111
    }, {
      "referenceID" : 27,
      "context" : "For coreference resolution evaluation, we employ the CoNLL scorer (Pradhan et al., 2014).",
      "startOffset" : 66,
      "endOffset" : 88
    }, {
      "referenceID" : 32,
      "context" : "It computes three commonly used evaluation measures MUC (Vilain et al., 1995), B (Bagga and Baldwin, 1998), and CEAFe (Luo, 2005).",
      "startOffset" : 56,
      "endOffset" : 77
    }, {
      "referenceID" : 0,
      "context" : ", 1995), B (Bagga and Baldwin, 1998), and CEAFe (Luo, 2005).",
      "startOffset" : 11,
      "endOffset" : 36
    }, {
      "referenceID" : 17,
      "context" : ", 1995), B (Bagga and Baldwin, 1998), and CEAFe (Luo, 2005).",
      "startOffset" : 48,
      "endOffset" : 59
    }, {
      "referenceID" : 19,
      "context" : "We employed CORT (Martschat and Strube, 2015) as our baseline coreference resolution system.",
      "startOffset" : 17,
      "endOffset" : 45
    }, {
      "referenceID" : 19,
      "context" : "The numbers of the baseline are higher than the results reported in Martschat and Strube (2015) since the feature set of CORT was subsequently extended.",
      "startOffset" : 68,
      "endOffset" : 96
    }, {
      "referenceID" : 21,
      "context" : "Figure 3: A short story on bicycles from the InScript corpus (Modi et al., 2017).",
      "startOffset" : 61,
      "endOffset" : 80
    }, {
      "referenceID" : 21,
      "context" : "Figure 3: A short story on bicycles from the InScript corpus (Modi et al., 2017). The entity prediction task requires predicting xxxx given the preceding text either by choosing a previously mentioned entity or deciding that this is a “new entity”. In this example, the ground-truth prediction is [tire]4. For training, ENTITYNLM attempts to predict every entity. While, for testing, it predicts a maximum of 30 entities after the first three sentences, which is consistent with the experimental setup suggested by Modi et al. (2017).",
      "startOffset" : 62,
      "endOffset" : 534
    }, {
      "referenceID" : 21,
      "context" : "Based on Modi et al. (2017), we introduce a novel entity prediction task that tries to predict the next entity given the preceding text.",
      "startOffset" : 9,
      "endOffset" : 28
    }, {
      "referenceID" : 21,
      "context" : "We introduce two baselines in this task: (i) the always-new baseline that always predicts “new entity”; (ii) a linear classification model using shallow features from Modi et al. (2017), including the recency of an entity’s last mention and the frequency.",
      "startOffset" : 167,
      "endOffset" : 186
    }, {
      "referenceID" : 21,
      "context" : "We introduce two baselines in this task: (i) the always-new baseline that always predicts “new entity”; (ii) a linear classification model using shallow features from Modi et al. (2017), including the recency of an entity’s last mention and the frequency. We also compare with the model proposed by Modi et al. (2017). Their work assumes that the model has prior knowledge of all the participant types, which are specific to each scenario and fine-grained, e.",
      "startOffset" : 167,
      "endOffset" : 318
    }, {
      "referenceID" : 19,
      "context" : "CORT is the best-performing model of Martschat and Strube (2015) with greedy decoding.",
      "startOffset" : 37,
      "endOffset" : 65
    }, {
      "referenceID" : 21,
      "context" : "Modi et al. (2017) 62.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 21,
      "context" : "More details about human predictions are discussed in (Modi et al., 2017).",
      "startOffset" : 54,
      "endOffset" : 73
    }, {
      "referenceID" : 21,
      "context" : "For each entity slot, Modi et al. (2017) acquired 20 human predictions, and the majority vote was selected.",
      "startOffset" : 22,
      "endOffset" : 41
    }, {
      "referenceID" : 14,
      "context" : "Previous work focuses on contextual information from previous sentences (Ji et al., 2016a) or discourse relations between adjacent sentences (Ji et al.",
      "startOffset" : 72,
      "endOffset" : 90
    }, {
      "referenceID" : 15,
      "context" : ", 2016a) or discourse relations between adjacent sentences (Ji et al., 2016b), showing improvements to language modeling and related tasks like coherence evaluation and discourse relation prediction.",
      "startOffset" : 59,
      "endOffset" : 77
    }, {
      "referenceID" : 14,
      "context" : "Previous work focuses on contextual information from previous sentences (Ji et al., 2016a) or discourse relations between adjacent sentences (Ji et al., 2016b), showing improvements to language modeling and related tasks like coherence evaluation and discourse relation prediction. In this work, ENTITYNLM adds explicit entity information to the language model, which is another way of adding a memory network for language modeling. Unlike the work by Tran et al. (2016), where memory blocks are used to store general contextual information for language modeling, ENTITYNLM assigns each memory block specifically to only one entity.",
      "startOffset" : 73,
      "endOffset" : 471
    }, {
      "referenceID" : 23,
      "context" : "Two recent approaches to modeling entities in text are closely related to our model. The first is the “reference-aware” language models proposed by Yang et al. (2016), where the referred entities are from either a predefined item list, an external database, or the context from the same document.",
      "startOffset" : 31,
      "endOffset" : 167
    }, {
      "referenceID" : 23,
      "context" : "Two recent approaches to modeling entities in text are closely related to our model. The first is the “reference-aware” language models proposed by Yang et al. (2016), where the referred entities are from either a predefined item list, an external database, or the context from the same document. Yang et al. (2016) present three models, one for each case.",
      "startOffset" : 31,
      "endOffset" : 316
    }, {
      "referenceID" : 12,
      "context" : "Our entity updating scheme is similar to the “dynamic memory” method used by Henaff et al. (2016). Our entity representations are dynamically allocated and updated only when an entity appears up, while the EntNet from Henaff et al.",
      "startOffset" : 77,
      "endOffset" : 98
    }, {
      "referenceID" : 12,
      "context" : "Our entity updating scheme is similar to the “dynamic memory” method used by Henaff et al. (2016). Our entity representations are dynamically allocated and updated only when an entity appears up, while the EntNet from Henaff et al. (2016) does not model entities and their relationships explicitly.",
      "startOffset" : 77,
      "endOffset" : 239
    }, {
      "referenceID" : 10,
      "context" : "The hierarchical structure of our entity generation model is inspired by Haghighi and Klein (2010). They implemented this idea as a probabillistic graphical model with",
      "startOffset" : 73,
      "endOffset" : 99
    }, {
      "referenceID" : 26,
      "context" : "the distance-dependent Chinese Restaurant Process (Pitman, 1995) for entity assignment, while our model is built on a recurrent neural network architecture.",
      "startOffset" : 50,
      "endOffset" : 64
    }, {
      "referenceID" : 24,
      "context" : "The reranking method considered in our coreference resolution evaluation could also be extended with samples from additional coreference resolution systems, to produce more variety (Ng, 2005).",
      "startOffset" : 181,
      "endOffset" : 191
    }, {
      "referenceID" : 18,
      "context" : "In previous work, such information has been added as features (Luo et al., 2004; Björkelund and Kuhn, 2014) or by computing distributed entity representations (Wiseman et al.",
      "startOffset" : 62,
      "endOffset" : 107
    }, {
      "referenceID" : 2,
      "context" : "In previous work, such information has been added as features (Luo et al., 2004; Björkelund and Kuhn, 2014) or by computing distributed entity representations (Wiseman et al.",
      "startOffset" : 62,
      "endOffset" : 107
    }, {
      "referenceID" : 33,
      "context" : ", 2004; Björkelund and Kuhn, 2014) or by computing distributed entity representations (Wiseman et al., 2016; Clark and Manning, 2016b).",
      "startOffset" : 86,
      "endOffset" : 134
    }, {
      "referenceID" : 5,
      "context" : ", 2004; Björkelund and Kuhn, 2014) or by computing distributed entity representations (Wiseman et al., 2016; Clark and Manning, 2016b).",
      "startOffset" : 86,
      "endOffset" : 134
    }, {
      "referenceID" : 21,
      "context" : "3 is based on work by Modi et al. (2017). The main difference is that we do not assume that all entities belong to a previously known set of entity types specified for each narrative scenario.",
      "startOffset" : 22,
      "endOffset" : 41
    } ],
    "year" : 2017,
    "abstractText" : "Understanding a long document requires tracking how entities are introduced and evolve over time. We present a new type of language model, ENTITYNLM, that can explicitly model entities, dynamically update their representations, and contextually generate their mentions. Our model is generative and flexible; it can model an arbitrary number of entities in context while generating each entity mention at an arbitrary length. In addition, it can be used for several different tasks such as language modeling, coreference resolution, and entity prediction. Experimental results with all these tasks demonstrate that our model consistently outperforms strong baselines and prior work.",
    "creator" : "LaTeX with hyperref package"
  }
}