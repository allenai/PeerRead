{
  "name" : "1206.3264.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Sampling First Order Logical Particles",
    "authors" : [ "Hannaneh Hajishirzi" ],
    "emails" : [ "eyal}@uiuc.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Approximate inference in dynamic systems is the problem of estimating the state of the system given a sequence of actions and partial observations. High precision estimation is fundamental in many applications like diagnosis, natural language processing, tracking, planning, and robotics. In this paper we present an algorithm that samples possible deterministic executions of a probabilistic sequence. The algorithm takes advantage of a compact representation (using first order logic) for actions and world states to improve the precision of its estimation. Theoretical and empirical results show that the algorithm’s expected error is smaller than propositional sampling and Sequential Monte Carlo (SMC) sampling techniques."
    }, {
      "heading" : "1 Introduction",
      "text" : "Important AI applications like natural language processing, program verification, tracking, and robotics involve stochastic dynamic systems. The current state of such systems is changed by executing actions. Reasoning is the task of computing the posterior probability over the state of such dynamic systems given past actions and observations. Reasoning is difficult because the system’s exact initial state or the effects of its actions are uncertain (e.g., there may be some noise in the system or its actions may fail).\nExact reasoning (e.g., [11; 1]) is not tractable for long sequence of actions in complex systems. This is because domain features become correlated after some steps, even if the domain has much conditional-independence structure [4]. Therefore, approximate reasoning is of much interest. One of the most commonly used classes of techniques for approximate reasoning is SMC sampling [5]. These methods are efficient, but they require many samples (exponential in the dimensionality of the domain) to yield a\nlower error rate. Recently, [8] introduced a new sampling approach which achieves higher precision than SMC techniques given a fixed number of samples. Still, it requires a large number of samples in complex domains because in their method the domains are represented using propositional logic.\nIn this paper we present a sampling-based filtering algorithm in a first order dynamic system called Probabilistic Relational Action Model (PRAM). We show that our new algorithm takes fewer samples and yields better accuracy than previous sampling techniques. Such improvement is possible because of the underlying deterministic structure of the transition system, the compact representation of the domains using first order logic (FOL), and efficient subroutines for first order logical regression (e.g., [17]) and first order logical filtering [19; 14].\nWe model a PRAM (Section 2) using probabilistic situation calculus [17], extended with a first order probabilistic prior that combines FOL and probabilities in a single framework (e.g., [18; 3; 12; 15; 7; 20]). Transitions in a PRAM are modeled with probability distributions over possible deterministic executions of probabilistic actions (every transition model can be represented this way).\nOur algorithm (Section 3) first samples sequences of deterministic actions, called first order (FO) particles, that are possible executions of the given probabilistic action sequence and are consistent with the observations. It also updates the current state of the system given each FO particle. Next, the algorithm computes the probability of the query given the updated current state. To do so, it applies logical regression to the query for each FO particle, computes a FOL formula that represents all the possible initial states, and computes the prior probability of that formula. Finally, the algorithm computes the posterior probability as the weighted sum of these derived prior probabilities.\nThis algorithm achieves superior precision with fewer samples than SMC sampling techniques [5]. The intuition behind this improvement is that each FO particle corresponds to exponentially many state sequences (particles) generated\nby earlier techniques. The algorithm is computationally efficient when FOL regression and filtering with the deterministic actions are efficient. We prove our claim about precision and verify the results empirically by several experiments (Section 4).\nOur representation for PRAM differs from Dynamic Bayesian Networks (DBNs) (e.g., [13]). DBNs emphasize conditional independence among random variables. In contrast, our model applies a representation for the transition model as a distribution over deterministic actions. Also, PRAM uses a different model for observations. The observations are received asynchronously without prediction of what will be observed. Both frameworks are universal and can represent each other, but they are more compact and natural in different scenarios.\nThe closest work to ours is [8] which also samples deterministic executions of the given probabilistic sequence. However, that work assumes that the deterministic sequences are always executable. In this paper we relax this assumption and avoid sampling the sequences that are not executable by keeping track of the current state of the system. Also, our representation is more compact (uses FOL), so it achieves higher precision with less samples.\nRecently, [10; 22] explore reasoning algorithms in relational HMMs. They use a different representation than ours for their observations. Moreover, they do not use a compact representation for their prior distribution. Earlier algorithm trades efficiency of computation for precision as it is an exact method. Latter, introduced a sampling algorithm where each sample represents a set of states and is generated from the probability distribution over disjoint sets of the states. Therefore, to update this distribution they build new disjoint sets at each time step which leads to a large number of disjoint sets (exponential in domain features) in long sequences. Our algorithm is different in a sense that we do not sample states; we sample deterministic sequences which correspond to state sequences that are derived by regressing the query through the deterministic sequence."
    }, {
      "heading" : "2 Probabilistic Relational Action Models",
      "text" : "In this section we present our framework, called Probabilistic Relational Action Model (PRAM), for representing the dynamic system. The system is dynamic in a sense that its state changes by executing actions. Actions have probabilistic effects that are represented with a probability distribution over possible deterministic executions.\nA PRAM consists of two main parts: (1) A prior knowledge representing a probability distribution P0 over initial world states in a relational model. (2) A probability distribution PA over deterministic executions of probabilistic actions. In what follows we define the basic building blocks of a PRAM. We first define the language L of a PRAM for\nrepresenting the probability distributions:\nDefinition 1. [PRAM language] The language L of a PRAM is a tuple (F,C, V,A, DA) consisting of:\n• F a finite set of predicate variables (called fluents) whose values change over time. • C a finite set of constants representing objects in the\ndomain. • V a finite set of variables. • A, DA finite sets of probabilistic and deterministic\naction names, respectively.\nWe define a fluent atom as a formula of the form f(x1, . . . , xk) (also represented by f(~x)), where x1, . . . , xk ∈ V ∪ C are either variables or constants. In PRAM grounding of a fluent f(~x) is defined as replacing each variable in ~x with a constant c ∈ C. Accordingly, a world state s ∈ S is defined as a full assignment of {true, false} to all the groundings of all the fluents in F . Example 1 (Briefcase). Briefcase is a domain consisting of objects, locations, and a briefcase B. The variables ?o and ?l denote objects and locations, respectively. The fluents are: In(?o),At(B, ?l),At(?o, ?l). An agent interacts with the system by executing some probabilistic actions: putting objects in, PutIn(?o,B), taking objects out of the briefcase, TakeOut(?o,B), and moving the briefcase with objects inside, Move(B, ?l1, ?l2). Some deterministic actions in the domain are: MvWithObj, PutInSucc, and PutInFail.\nIn PRAM each deterministic action da(~x) ∈ DA is specified by precondition and successor state axioms [17].\nDefinition 2. [Deterministic action axioms] Precondition axioms show the conditions under which the deterministic action da is executable in a given state; Precond is a special predicate denoting the executability of a deterministic action: Precondda(~x)⇔ Φ(~x) where Φ is a FOL formula.\nSuccessor state axioms enumerate all the ways that the value of a particular fluent can be changed; A successor state axiom for a fluent f is defined as:\nPrecondtda(~x)⇒ (Succtf,da(~x)⇔ f t+1(~x))\nwhere Succtf,da(~x) is a FOL formula at time t. One can easily derive the effects of actions by the above axioms.\nFor example, the precondition of the deterministic action MvWithObj(B, ?l1, ?l2) is At(B, ?l1) ∧ ¬At(B, ?l2), and its effect is ¬At(B, ?l1) ∧ At(B, ?l2) ∧ (∀?o In(?o) ⇒ At(?o, ?l2)).\nThe grounding of a deterministic action is defined as the grounding of all the fluent atoms appearing in the precondition and effect logical formulas Precondtda(~x) and Succtf,da(~x). Therefore, a grounded deterministic action is a transition function T : S ×DA → S . One can see from the example that for grounding the action MvWithObj, one\nneeds to permute all the possible combinations of objects inside the briefcase. This increases the dimensionality of the domain and increases the required number of samples to yield low error. Hereinafter for simplicity, we represent fluents and actions without their arguments whenever it is not necessary to mention the variables or domain objects. Definition 3. [Probability distribution for probabilistic actions] Let ψ1 . . . ψk be FOL formulas (called partitions) that divide the world states into mutually disjoint sets. Then, PA(da|a, s) is defined as a probability distribution over possible deterministic executions da of the probabilistic action a in the state swhich satisfies one of the partitions ψi (i ≤ k). More formally, when some state s satisfies partition ψi then PA(da|a, s) = PAi(da), where PAi is a probability distribution over different deterministic executions da of action a corresponding to the partition ψi.\nPA(da|a, s) =  PA1(da) s |= ψ1PA2(da) s |= ψ2 . . .\n(1)\nWe assume that replacing variables in a(~x) with constants does not change PA(da(~x)|a(~x), s).\nFor example, for probabilistic action TakeOut(?o), deterministic action TakeOutSucc(?o), and s |= ψ1 = In(?o): PA(da|a, s) = PA1(TakeOutSucc(?o)) = 0.9.\nIn PRAM we assume a prior distribution over world states at time 0. Models like [18] are used to represent probabilities in relational models. A knowledge engineer can define the semantics of the prior distribution by using each of these models. Then, we use the inference algorithm defined in that model’s semantics to compute probability of formulas at time 0. Our filtering algorithm does not depend on the way the initial knowledge has been represented. We discuss more about this in Section 3.1.3.\nIn conclusion, we define a PRAM formally as follows: Definition 4. A PRAM is a tuple (L,AX,PA,P0) as:\n• Language L = (F,C, V,A, DA) representing the language of PRAM (Definition 1) • A set of deterministic action axioms AX (Definition 2) • A probability distribution PA for each probabilistic ac-\ntion (Definition 3) • A prior distribution P0 over initial world states\nBased on the aforementioned dynamic system, PRAM, we define our stochastic filtering problem as computing probability of a query given a sequence of probabilistic actions and observations. The query is defined as a FOL formula ϕT at the final time step, T . Note that throughout the paper superscripts represent time. Each at in the probabilistic action sequence 〈a1, . . . , aT 〉 represents the probabilistic action that has been executed at time t.\nThe observations 〈o0, . . . , oT 〉 are given asynchronously in time without prediction of what we will observe (thus,\nthis is different from HMMs [16], where a sensor model is given). Each observation ot is represented with a FOL formula over fluents. When ot is observed at time t, the FOL formula ot is true about the state of the world at time t. We assume that a deterministic execution da of the probabilistic action at in the sequence does not depend on the future observations."
    }, {
      "heading" : "3 Filtering Algorithm",
      "text" : "In this section we present our filtering algorithm for computing probability of a query ϕT given a sequence of probabilistic actions a1:T = 〈a1, . . . , aT 〉 and observations o0:T = 〈o0, . . . , oT 〉 in a PRAM. The algorithm approximates this probability by generating samples among possible deterministic executions of the given probabilistic action sequence. Then, it places those samples instead of the enumeration of deterministic executions and marginalizes over those samples. The following equation shows the exact computation.\nP (ϕT |a1:T , o0:T ) = (2)∑ i P (ϕT | ~DAi, o0:T )P ( ~DAi|a1:T , o0:T )\nwhere ~DAi is a possible execution of the sequence a1:T .\nThe first step of our approximate algorithm is generating N samples (called FO particles) from all the possible deterministic executions of the given probabilistic sequence. Two different sampling algorithms are introduced in Section 3.2. The algorithms (illustrated in Figure 1) generate a weighted FO particle ~DAi with weight wi given the sequence a1:T and the observations o0:T from the probability distribution P ( ~DAi|a1:T , o0:T ).\nThe next step of the algorithm is computing P (ϕT | ~DAi, o0:T ) for each FO particle ~DAi. It does\nso (Section 3.1) by updating the current state of the system and then computing the posterior probability conditioning on the updated current state.\nFinally, the algorithm uses generated samples in place of ~DAi in Equation (2) and computes P̃N (ϕT |a1:T , o0:T ) as an approximation for the posterior probability of the query ϕT given the sequence a1:T and the observations o0:T by using the Monte Carlo integration [5]:\nP̃N (ϕT |a1:T , o0:T ) = ∑ i wiP (ϕT | ~DAi, o0:T ) (3)\nDetails of each step of our first order filtering algorithm (FOFA, Figure 2) are explained next. We first present the step of computing P (ϕT | ~DAi, o0:T ) because it is used as a subroutine in the sampling algorithms."
    }, {
      "heading" : "3.1 Probability of a FOL Formula at time t",
      "text" : "In this section we show how to compute the probability P (ϕt| ~DA, o0:t) of the formula ϕt given a FO particle ~DA and observations o0:t. Executing the FO particle ~DA with observations o0:t updates the current state of the system. The current state is derived by applying a FO progression subroutine (described below) at each time step. Afterwards, procedure PFOF (Figure 3) computes the probability of the query given the current state of the system for each FO particle. Its first step applies a FO regression subroutine to the query and the current state formula and as output returns FOL formulas at time 0. This can be done since the actions are deterministic. The algorithm’s second step computes the prior probability of the regression of the query conditioned on the current state formula regressed by the FO particle; Recall that a FO particle is a sampled sequence of deterministic actions."
    }, {
      "heading" : "3.1.1 Progress Current State Formula",
      "text" : "We define the current state formula as a FOL formula representing the set of states that are true after executing a sequence of deterministic actions and receiving observations. In this section we present algorithm Progress that updates the current state formula given a deterministic action and an observation. In general, progressing a FOL for-\nmula δ with a deterministic action da, Progress(δ, da), results in a set of FOL formulas ∆(p1:n) where p1, . . . , pn are atomic subformulas of ∆ and δ ∧ Precondda |= ∆(Succp1,da, . . . , Succpn,da) (see [19] for more details). Also, filtering a FOL formula δ with an observation o is the FOL formula δ ∧ o.\nOverall, generating all ∆s is impossible because there are infinitely many such ∆s. In this paper we assume that progression of the current state formula curF with a deterministic action da, Progress(curF, da, o), is representable with a FOL formula. Hence, it is equal to∨ i ∆i(p1:n) ∧ o. Furthermore, [19; 14] provide some special conditions that the progression algorithm is polynomial and the representation of progressing a formula is compact. For example, progressing In(O) through deterministic action MvWithObj(L1, L2) results in the formula At(B,L2)∧ In(O) ∧ At(O,L2) ∧ (∀?o In(?o)⇒ At(?o, L2))."
    }, {
      "heading" : "3.1.2 Regressing a FOL Formula",
      "text" : "Procedure RegSeq takes a FOL formula ϕt and a FO particle ~DA and returns as output another FOL formula ϕ0. ϕ0 represents the set of possible initial states, given that the fi-\nnal state satisfies ϕt, and the FO particle ~DA occurs. Thus, every state that satisfies ϕ0 leads to a state satisfying ϕt after ~DA occurs.\nFor a deterministic action dat and a FOL formula ϕt, the regression Regress(ϕt, dat) of ϕt through dat is a FO formula ϕt−1 such that state st−1 satisfies ϕt−1 iff the result of the transition function T (st−1, dat) satisfies ϕt. The computation of the regression RegSeq(ϕt, ~DA) of ϕt through the FO particle ~DA is done recursively.\nRegSeq(ϕt, 〈da1, ..., dat〉) = RegSeq(Regress(ϕt, dat), 〈da1, ..., dat−1〉).\nRegression of the current state formula curF is defined similar to the regression of formula ϕt since the current state is also represented with a fist order formula.\nThe algorithm for regression Regress(ϕt, da) of formula ϕt with deterministic action da works as follows (see [17]). Suppose that ϕt is an atomic fluent f t(~x) with the successor state axiom Succt−1f,da(~x). Then, we derive the regression of ϕt by replacing f t(~x) with Succt−1f,da(~x). The regression of non-atomic formulas are derived inductively as follows:\n• Regress(¬ϕt) = ¬Regress(ϕt) • Regress(ϕt1 ∧ ϕt2) = Regress(ϕt) ∧ Regress(ϕt2) • Regress((∃ν)ϕt) = ∃νRegress(ϕt)\nThe efficiency of the regression yields from the fact that we regress non-grounded fluents f(~x). Besides, one can employ some approximations at each step (like removing unnecessary clauses) to maintain the compactness of the regressed formulas.\nWe now summarize and show how to compute the probability distribution P (ϕt| ~DA, o0:t) by applying progression and regression. The algorithm first computes curF by progressing through the FO particle and observations. Then, it computes ϕ0 and curF0 by regressing ϕt and curF. Finally, it computes P 0(ϕ0|curF0) as the evaluation of P (ϕt| ~DA, curF) as shown by the following lemma. Lemma 1. Let ϕt be the query and o0:t the observations. If curF is the current state formula, ϕ0 = RegSeq(ϕt, ~DA), and curF0 = RegSeq(curF, ~DA), then P (ϕt| ~DA, o0:t) = P 0(ϕ0|curF0).\nProof. Probability of a formula is a marginalization over states: P (ϕt, o0:t| ~DA) = ∑ s P (ϕ\nt, curF|s, ~DA)P (s| ~DA). For s, P (ϕt, curF|s, ~DA) = 1 if s |= ϕ0∧ curF 0, o.w. it is 0. The reason is that executing the deterministic sequence ~DA in state s results at time t that models ϕt and is consistent with observations o0:t iff s |= ϕ0 ∧ curF 0. Therefore, P (ϕt, o0:t| ~DA) = ∑ s|=ϕ0,curF0 P (s) = P 0(ϕ0, curF0). The same computations exist for P (o0:t| ~DA). Therefore,\nP (ϕt| ~DA, o0:t) = P (ϕ t, o0:t| ~DA) P (o0:t| ~DA) = P 0(ϕ0, curF0) P 0(curF0) .\nFigure 4 shows an example of computing P (ϕt| ~DA, o0:t) using procedure PFOF. The next section shows how to compute P 0(ϕ0|curF0) using the prior P 0. Note that ϕ0 and curF0 are FOL formulas over fluents at time 0."
    }, {
      "heading" : "3.1.3 Prior Probability of a FOL Formula",
      "text" : "In this section we describe procedure Prior-FOF to compute the probability of a FOL formula at time 0. We assume that the prior distribution P 0 over world states of a PRAM with language L = (F,C, V,A, DA) is represented by a Markov Logic Network (MLN) [18]. We choose to represent our prior probabilistic logic with an MLN (called prior MLN) because it keeps the expressive power of FOL for a fixed domain.\nOur prior MLN consists of a set of weighted FOL formulas over the fluents in language L of the PRAM. The semantics of the MLN is that of a Markov network MMLN [9] whose cliques correspond to groundings of the formulas given the universe of objects C ∈ L. The potential Φ of a clique Cl is defined as the exponential of the weight of the corresponding formula in case the grounding is true.\nWe introduce Prior-FOF (Figure 3) to compute the probability of a FOL formula ϕ0 by slightly changing the inference algorithm expressed for MLNs. Procedure Prior-FOF first converts ϕ0 to a clausal form; Note that like MLNs existentially quantified formulas are replaced by disjunction of their groundings. Then, Prior-FOF assigns an indicator function, Ig(Ci), to every grounding g of a clause Ci.\nIg(Ci)(~fg(Ci)) = { 1 ~fg(Ci) |= g(Ci) 0 otherwise\n(4)\nThe next step is to construct a Markov network M as the minimal subset of the original network MMLN required to compute P 0(ϕ0). The nodes in M consist of all the ground fluents fg(Ci) in clauses Ci of ϕ\n0 and all the nodes in the original Markov network MMLN with a path to the fluent fg(Ci). The final step is to perform inference on this network. It can be computed exactly by performing variable elimination (e.g., [9]) in P 0(ϕ0) ∝∑ f∈M ∏ ~fj∈Clj ,~fi∈Ci Φj(\n~fj)I(~fi). Also, one can employ any approximate inference algorithm like Gibbs sampling.\nWe are not restricted to use MLNs as a framework for representing prior probability distribution. We can use any other probabilistic logical frameworks ([3; 12; 15; 7; 20]) provided that the expressed inference algorithm in that framework can compute probability of the query. Note that the query ϕ0 for that framework is derived from the regression of the original query ϕt given an FO particle. Also, our algorithms would work for unbounded domains just by using a framework (e.g., [21]) for representing prior distribution over infinite states."
    }, {
      "heading" : "3.2 Sampling Algorithms",
      "text" : "In this section we describe procedures S/R-Actions (Figure 5) and S-Actions (Figure 6) which generate N samples (called FO particles) given a sequence of probabilistic actions and observations. Each FO particle is a possible deterministic execution of the given probabilistic sequence. Both algorithms incrementally build every FO particle by sampling a deterministic action at a time. Later, we will discuss about the deficiencies of S/R-Actions algorithm.\nBoth S/R-Actions and S/Actions generate a FO particle ~DA = 〈da1, . . . , daT 〉 given a sequence a1:T and observations o0:T from the distribution P ( ~DA|a1:T , o0:T ). We compute this probability distribution iteratively:\nP ( ~DA|a1:T , o0:T ) = P (da1|a1, o0:1) ∏ t P (dat|at, da1:t−1, o0:t)\nThe above derivation allows iterative sampling of deterministic actions from the distribution P (dat|at, da1:t−1, o0:t). Recall that a FO particle is a sequence of deterministic actions. Thus, at each time the algorithms sample a deterministic action dat given the probabilistic action at, the previous deterministic actions da1:t−1, and the previous obser-\nvations o0:t; Note that current deterministic action is independent of the future observations. Then, the algorithms update the current state formula curF (Section 3.1.1) given the current deterministic action and the observation.\nAlgorithms S/R-Actions, S-Actions use different approaches for incremental sampling of each deterministic action. In S/R-Actions, we adopt a sequential importance sampling approach. At each time step, S/R-Actions samples deterministic actions based on a normalized importance function, assigns weights to the particles, and resamples if the weights have high variance. The importance function π(dat|at, da1:t−1) approximates P (dat|at, da1:t−1, o0:t) by ignoring the effect of the current state curF in the PRAM = (L,AX,PA, P 0).\nπ(dat|at, da1:t−1) = ∑ i PAi(dat)PFOF(ψt−1i,at , da 1:t−1)\nwhere ψi,at is the ith partition of action at (Definition 3).\nAt time step t, S/R-Actions samplesN deterministic actions dat1:N from the importance function: π(da\nt|at, da1:t−1). It then restructures the nth particle ~DA t−1 n by attaching the deterministic action datn to that particle, i.e. ~DA t\nn = 〈da1n, . . . , dat−1n , datn〉. The importance weight w∗n of the nth particle is derived as: w∗n = P (datn|a t,da1:t−1n ,o 0:t)\nπ(datn|at,da 1:t−1 n )\n.\nAccordingly, the normalized importance weight wn of the nth particle is: wn =\nw∗n∑N i=1 w ∗ i .\nThe exact value for P (dat|at, da1:t−1, o0:t) is computed using PFOF subroutine (Figure 3).\nP (dat|at, da1:t−1, o0:t) (5) = ∑ i PAi(dat) · PFOF(ψt−1i,at , da 1:t−1, curF)\nS/R-Actions resamples the particles if the variance of the normalized importance weights becomes too high. The basic idea of resampling is to avoid those particles with very low weight and concentrate on particles that have higher normalized weight. An estimation (see [5]) for measuring high variance among the weights is estimating the effective number of particles as N̂eff = 1∑N i=1 wi\n. If N̂eff is smaller than a threshold then procedure Resample generates a new deterministic action dat from the probability distribution over the normalized weights w1:N of the particles. It then assigns equal weights, 1N , to all the particles.\nThe second sampling algorithm, S-Actions (Figure 6), is derived by simplifying the above sampling algorithm. This algorithm at each time step generates samples among executable deterministic actions. It samples every deterministic action in a FO particle from the exact computation for P (dat|at, da1:t−1, o0:t−1) (Equation 5) instead of sampling from the importance function and assigning weights. Therefore, all the weights are equal to 1N .\nS/R-Actions, samples deterministic actions even when they are not executable and assigns weight zero to those particles. Therefore, it decreases the effective number of samples. Resampling step does not help that much because it resamples the latest deterministic action in the FO particle. However, resampling earlier deterministic actions may result in a more effective set of FO particles. Note that this new type of resampling is not tractable. Therefore, it makes more sense to maintain the current state formula curF as in S-Actions and just sample executable deterministic actions (as in S-Actions) unless there exists a better resampling procedure. In the empirical results we examine the effects of using S/R-Actions and S-Actions sampling algorithms in the FOFA filtering algorithm (Figure 2)."
    }, {
      "heading" : "3.3 Correctness, Complexity, and Accuracy",
      "text" : "The following theorem shows how FOFA with S-Actions and S/R-Actions compute the approximate posterior distribution P̃N (ϕT |a1:T , o0:T ). Theorem 1. Let ϕT be the query, a1:T be the given probabilistic sequence, ~DAi be the FO particles, and o0:T be the observations. If curFi is the current formula given the ith FO particle, ϕ0i = RegSeq(ϕ\nT , ~DAi), and curF0i = RegSeq(curFi, ~DAi), then\nP̃N (ϕT |a1:T , o0:T ) = ∑ i wiP 0(ϕ0i |curF0i ) (6)\nwhere wi = 1N for S-Actions. Besides, for S-Actions:\nP̃N (ϕT |a1:T , o0:T )→N→∞ P (ϕT |a1:T , o0:T ) (7)\nThe proof follows from using MC integration in Equation 2 and using Lemma 1.\nThe running timeRFOFA of our filtering algorithm (Figure 2) is O(N · T · (RRegSeq +RProgress +RPrior-FOF)), where N is the number of samples and T is the length of the given sequence of probabilistic actions. Efficiency of FOFA results from efficiency of the underlying algorithms for RegSeq, Progress, and Prior-FOF.\nWe evaluate the accuracy of our sampling algorithm FOFA by computing expected KL-distance 1 as the expected value of all the KL-distances between the exact distribution P and the approximation P̃ derived by FOFA. Our algorithm, FOFA, has higher accuracy than SMC for a fixed number of samples. The intuition is that each FO particle generated by FOFA covers many particles generated by SMC.\nTheorem 2. If FOFA(S-Actions) and SMC approximate posterior distribution P (ϕT |a1:T , o0:T ) with N samples. Then, Expected-KLFOFA(S-Actions) ≤ Expected-KLSMC.\n1KL(P, P̃ ) = ∑ x Pxlog(Px/P̃x),KL(P, P̃ ) = 0 if P = P̃\nThe proof is similar to the proof of Theorem 3.3 in [8]. Intuitively, we define a mapping f to map each set of FO particles of S-Actions to sets of particles of SMC. The mapping f is defined such that it covers all the possible sets of particles of SMC, and for two separate sets of particles zi 6= zj , f(zi) ∩ f(zj) = ∅, and PrFOFA(z) = PrSMC(f(z)). Furthermore, we prove that ∀y ∈ f(z),KL(P, P̃ z1 ) ≤ KL(P, P̃ y2 ) where P̃1 and P̃2 are approximations returned by FOFA and SMC, respectively."
    }, {
      "heading" : "4 Empirical Results",
      "text" : "We implemented our algorithm FOFA (Figure 2) with both S/R-Actions (Figure 5) and S-Actions (Figure 6). Our algorithms take advantage of a different structure than that available in DBNs. Hence, we focused on planning-type domains: briefcase and depots taken from International Planning Competition at AIPS-98 and AIPS-02. 2 We randomly assigned deterministic executions and a probability distribution over them for each action. For example, for action PutIn we considered two executions PutInSucc and PutInFail with probabilities 0.9 and 0.1. Note that DBN representations (transitions between states) for the above frameworks are not compact because the independence assumptions among the state variables are not known.\nWe compared the accuracy of our FOFA(S/R-Actions) and FOFA(S-Actions) with SCAI [8] and SMC algorithms. Note that we grounded the domains for running SCAI and SMC. We ran sampling algorithms (50 times) for a fixed number of samples and computed the KL-distance between their approximation and the exact posterior. We calculated the average over these derived KL-distances to approximate the expected KL-distance. SCAI assumes that deterministic actions are always executable. We include this assumption in the depots domain in which we compared the results with SCAI. In the briefcase domain, we just compared our algorithms with the SMC techniques.\nFigures 7 and 8 show the expected KL-distances (in logarithmic scale) vs. number of samples in the depots and briefcase, respectively. As we expected, the average KLdistance for FOFA(S-Actions) is always the lowest. We expect to get more improvement when there are more objects in the domain (here, we just use 5 constants to be able to compute the exact distribution). For the briefcase domain (Figure 8) SMC has higher accuracy than FOFA(S/R-Actions) for 1000 samples. The reason is that the posterior distribution converges to the stationary distribution in this domain (4 constants, 256 states). Even grounded domain is not too big, but for cases involving longer sequences and more states we did not have the exact posterior to compare with since the implementation for the exact algorithm crashes.\n2Also available from: ftp://ftp.cs.yale.edu/pub/ mcdermott/domains/."
    }, {
      "heading" : "5 Conclusions and Future Work",
      "text" : "In this paper we presented a sampling algorithm to compute the posterior probability of a query given a sequence of actions and observations in a FO dynamic system. Our algorithm takes advantage of a compact representation and achieves higher accuracy than SMC sampling and earlier propositional sampling techniques.\nThere are several directions that we can continue this work: (1) Apply sampling in FO Markov Decision Processes(MDP)s [2](2) Learn the transition model in PRAM. (3) Apply the algorithm in text understanding. (4) Generalize the representation to continuous domains (e.g., by discretizing the real value variables or by combining with Rao-Blackwellised Particle Filtering [6])."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We would like to thank the anonymous reviewers for their helpful comments. This work was supported by DARPA SRI 27-001253 (PLATO project), NSF CAREER 05-46663, and UIUC/NCSA AESIS 251024 grants."
    } ],
    "references" : [ {
      "title" : "Reasoning about noisy sensors and effectors in the situation",
      "author" : [ "F. Bacchus", "J.Y. Halpern", "H.J. Levesque" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1999
    }, {
      "title" : "Symbolic dynamic programming for first-order MDPs",
      "author" : [ "C. Boutilier", "R. Reiter", "B. Price" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2001
    }, {
      "title" : "Clp(bn): Constraint logic programming for probabilistic knowledge",
      "author" : [ "V.S. Costa", "D. Page", "M. Qazi", "J. Cussens" ],
      "venue" : "In UAI,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2003
    }, {
      "title" : "Probabilistic temporal reasoning",
      "author" : [ "T. Dean", "K. Kanazawa" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1988
    }, {
      "title" : "Sequential Monte Carlo",
      "author" : [ "A. Doucet", "N. de Freitas", "N. Gordon" ],
      "venue" : "Methods in Practice. Springer,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2001
    }, {
      "title" : "Raoblackwellised particle filtering for dynamic bayesian networks",
      "author" : [ "A. Doucet", "N. de Freitas", "K. Murphy", "S. Russell" ],
      "venue" : "In UAI,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2000
    }, {
      "title" : "Learning probabilistic relational models",
      "author" : [ "N. Friedman", "L. Getoor", "D. Koller", "A. Pfeffer" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1999
    }, {
      "title" : "Stochastic filtering in a probabilistic action model",
      "author" : [ "H. Hajishirzi", "E. Amir" ],
      "venue" : "In AAAI’07,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2007
    }, {
      "title" : "Introduction to probabilistic graphical models",
      "author" : [ "Michael Jordan" ],
      "venue" : "Forthcoming",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2007
    }, {
      "title" : "Logical hidden markov models",
      "author" : [ "K. Kersting", "L.D. Raedt", "T. Raiko" ],
      "venue" : "Artificial Intelligence Research,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2006
    }, {
      "title" : "A computational scheme for reasoning in dynamic probabilistic networks",
      "author" : [ "U. Kjaerulff" ],
      "venue" : "In UAI,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1992
    }, {
      "title" : "Stochastic logic programs",
      "author" : [ "S. Muggleton" ],
      "venue" : "In Workshop on ILP,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1995
    }, {
      "title" : "Dynamic Bayesian Networks: Representation, Inference and Learning",
      "author" : [ "Kevin Murphy" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2002
    }, {
      "title" : "Reasoning about partially observed actions",
      "author" : [ "M. Nance", "A. Vogel", "E. Amir" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2006
    }, {
      "title" : "Probabilistic horn abduction and bayesian networks",
      "author" : [ "D. Pool" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1993
    }, {
      "title" : "A tutorial on hidden Markov models and selected applications in speech recognition",
      "author" : [ "L.R. Rabiner" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1989
    }, {
      "title" : "Knowledge In Action",
      "author" : [ "R. Reiter" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2001
    }, {
      "title" : "First order logical filtering",
      "author" : [ "A. Shirazi", "E. Amir" ],
      "venue" : "In IJ- CAI,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2005
    }, {
      "title" : "Discriminative probabilistic models for relational data",
      "author" : [ "B. Taskar", "P. Abbeel", "D. Koller" ],
      "venue" : "In UAI’02,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2002
    }, {
      "title" : "Infinite state bayesian networks",
      "author" : [ "M. Welling", "I. Porteous", "E. Bart" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2007
    }, {
      "title" : "Logical particle filtering",
      "author" : [ "L.S. Zettlemoyer", "H.M. Pasula", "L.P. Kaelbling" ],
      "venue" : "In Dagstuhl Seminar on Probabilistic, Logical, and Relational Learning,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "This is because domain features become correlated after some steps, even if the domain has much conditional-independence structure [4].",
      "startOffset" : 131,
      "endOffset" : 134
    }, {
      "referenceID" : 4,
      "context" : "One of the most commonly used classes of techniques for approximate reasoning is SMC sampling [5].",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 7,
      "context" : "Recently, [8] introduced a new sampling approach which achieves higher precision than SMC techniques given a fixed number of samples.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 16,
      "context" : ", [17]) and first order logical filtering [19; 14].",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 16,
      "context" : "We model a PRAM (Section 2) using probabilistic situation calculus [17], extended with a first order probabilistic prior that combines FOL and probabilities in a single framework (e.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 4,
      "context" : "This algorithm achieves superior precision with fewer samples than SMC sampling techniques [5].",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 12,
      "context" : ", [13]).",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 7,
      "context" : "The closest work to ours is [8] which also samples deterministic executions of the given probabilistic sequence.",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 16,
      "context" : "In PRAM each deterministic action da(~x) ∈ DA is specified by precondition and successor state axioms [17].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 15,
      "context" : "this is different from HMMs [16], where a sensor model is given).",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 4,
      "context" : "Finally, the algorithm uses generated samples in place of ~ DAi in Equation (2) and computes P̃N (φ |a , o ) as an approximation for the posterior probability of the query φ given the sequence a and the observations o by using the Monte Carlo integration [5]:",
      "startOffset" : 255,
      "endOffset" : 258
    }, {
      "referenceID" : 17,
      "context" : ", Succpn,da) (see [19] for more details).",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 16,
      "context" : "The algorithm for regression Regress(φ, da) of formula φ with deterministic action da works as follows (see [17]).",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 8,
      "context" : "The semantics of the MLN is that of a Markov network MMLN [9] whose cliques correspond to groundings of the formulas given the universe of objects C ∈ L.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 8,
      "context" : ", [9]) in P (φ) ∝ ∑ f∈M ∏",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 19,
      "context" : ", [21]) for representing prior distribution over infinite states.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 4,
      "context" : "An estimation (see [5]) for measuring high variance among the weights is estimating the effective number of particles as N̂eff = 1 ∑N i=1 wi .",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 7,
      "context" : "3 in [8].",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 7,
      "context" : "We compared the accuracy of our FOFA(S/R-Actions) and FOFA(S-Actions) with SCAI [8] and SMC algorithms.",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 1,
      "context" : "There are several directions that we can continue this work: (1) Apply sampling in FO Markov Decision Processes(MDP)s [2](2) Learn the transition model in PRAM.",
      "startOffset" : 118,
      "endOffset" : 121
    }, {
      "referenceID" : 5,
      "context" : ", by discretizing the real value variables or by combining with Rao-Blackwellised Particle Filtering [6]).",
      "startOffset" : 101,
      "endOffset" : 104
    } ],
    "year" : 2008,
    "abstractText" : "Approximate inference in dynamic systems is the problem of estimating the state of the system given a sequence of actions and partial observations. High precision estimation is fundamental in many applications like diagnosis, natural language processing, tracking, planning, and robotics. In this paper we present an algorithm that samples possible deterministic executions of a probabilistic sequence. The algorithm takes advantage of a compact representation (using first order logic) for actions and world states to improve the precision of its estimation. Theoretical and empirical results show that the algorithm’s expected error is smaller than propositional sampling and Sequential Monte Carlo (SMC) sampling techniques.",
    "creator" : "TeX"
  }
}