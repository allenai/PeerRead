{
  "name" : "1511.01666.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Comparing Writing Styles using Word Embedding and Dynamic Time Warping Comparing Writing Styles using Word Embedding and Dynamic Time Warping",
    "authors" : [ "Abhinav Tushar", "Abhinav Dahiya" ],
    "emails" : [ "a)abhinav.tushar.vs@gmail.com", "b)iit.abhinav.dahiya@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Comparing Writing Styles using Word Embedding and Dynamic Time Warping\nAbhinav Tushara) and Abhinav Dahiyab) Department of Electrical Engineering Indian Institute of Technology, Roorkee (Both authors contributed equally)\nThe development of plot or story in novels is reflected in the content and the words used. The flow of sentiments, which is one aspect of writing style, can be quantified by analyzing the flow of words. This study explores literary works as signals in word embedding space and tries to compare writing styles of popular classic novels using dynamic time warping.\nI. INTRODUCTION\nWriting style of a novel is based on many factors including the personal style of the author and the type of the novel itself. For example, thriller novels by the same author tend to be similar in the way the story develops. Apart from the specific factual details, literary works tend to have a definite flow of emotions that define the overall subjective feel of the work as a whole.\nThis flow can be quantified and compared by analyzing the text using natural language processing techniques. This study uses word embedding models to generate time series for novels and then compare the resulting series using dynamic time warping to find similarities. Considering time series analysis rather than a pure statistical one can capture the flow of the works and thus the similarities generated will provide a metric for comparing the novels based on the progression.\nThe following section explains the word embedding models and dynamic time warping. Section III describes the approach followed. Results on few classic literary works are described in Section IV. Conclusions are drawn in Section V."
    }, {
      "heading" : "II. BACKGROUND",
      "text" : ""
    }, {
      "heading" : "A. Word Embedding",
      "text" : "Word embedding refers to techniques which allow representing words to vectors of real numbers in a continuous space. The vector representation can be learned using many techniques and relies on using the information based on the context, a word is present in.\nWe use a model proposed by Mikolov et al. (2013a,b) that uses Continuous Bag-of-Words and Skip-Gram model to learn these continuous representations. This approach uses the following two models.\na)abhinav.tushar.vs@gmail.com b)iit.abhinav.dahiya@gmail.com\n1. Continuous Bag-of-Words model\nThis model (figure 1) samples a context of size defined by c from text around a word and tries to learn a projection to vector space that correctly predicts the middle word. Assuming words represented as one hot vectors w(i), where i denotes the position of the word in text, the model takes w(t−c), w(t−(c−1)), · · · , w(t−1), w(t+ 1), · · · , w(t + (c − 1)), w(t + c) and predicts the middle word w(t).\n2. Skip-Gram model\nThe skip-gram model (figure 2) does the opposite of previous model. It tries to predict the context, given the middle word. Using the previous notation, this model takes w(t) as input and outputs w(t − c), w(t − (c − 1)), · · · , w(t− 1), w(t+ 1), · · · , w(t+ (c− 1)), w(t+ c).\nThe projection layers are of desired vector dimension. The weights from one hot encoding of words to this layer\nar X\niv :1\n51 1.\n01 66\n6v 1\n[ cs\n.C L\n] 5\nN ov\n2 01\n5\nprovide, after training, the final transformation matrix for continuous word representation.\nOnce trained, words in the vector space are arranged according to semantic connections. This allows the vectors to have properties as shown below.\nW (king) −W (queen) ≈W (boy) −W (girl)\nW is the function from vocabulary to the vector space."
    }, {
      "heading" : "B. Dynamic Time Warping",
      "text" : "Dynamic Time Warping (DTW) is a technique to measure similarity between two time series. DTW handles the difference in speed and time between signals and has been used in applications like speech recognition and signature matching where the difference of signal speed should not affect the final result. It is based on optimal matching and the algorithm outputs a value corresponding to the separation of the two time series. One considerable advantage of DTW is that the two series need not to be of same length, which is the case here, as two different books will have different word count and consequently different length.\nThe objective of this algorithm is to calculate a distance measure for a given pair of temporal series, that can represent the similarity / dissimilarity between those two series. This is done by determining a path W which minimizes the cumulative euclidean distance between elements of the two series. Let the two temporal series, which need to be compared, are\nA = a1, a2, a3, ..., am\nB = b1, b2, b3, ..., bn\nFirst, all the euclidean distances are calculated between each possible set of elements from the two series, which results in total of m × n values. Let the matrix depicting these distances be\nD =  d1,1 d1,2 · · · d1,n d2,1 d2,2 · · · d2,n ... ... . . . ...\ndm,1 dm,2 · · · dm,n  where the element di,j represents the euclidean distance between ai and bj . Now, using dynamic programming, the optimal path W is determined from point (1, 1) to (m,n) along which the cumulative sum of the euclidean distances (i.e. the sum of di,j) is minimum. This path is continuous, which means that the indices of two consecutive elements of W do not differ by more than one in either series. The path is determined using the following recursive function:\nγ(i, j) = d(i, j)+min(γ(i−1, j−1), γ(i−1, j), γ(i, j−1))\nwhere γ(i, j) represents the cumulative sum up to elements ai, and bj . Figure 3 shows an example using two sinusoidal series. The optimal path and the series are shown."
    }, {
      "heading" : "III. APPROACH",
      "text" : ""
    }, {
      "heading" : "A. Novels to Signal",
      "text" : "Our approach for converting a novel to signal uses sentence vectors by taking mean of word vectors in a sentence. A word embedding model is trained using 1000 free e-books gathered from Project Gutenberg1. The model is trained using Word2Vec2.\nTraining is done using feature (word vector) size of 100 and context window size of 10 words. Once trained, a novel with Nw words results in a 100 dimensional time series of length Nw.\nTo reduce the dimension of data word vectors in a sentence are averaged, resulting in a time series matrix of shape Ns × 100, where Ns is the number of sentences in the novel. To reduce the row dimensions, we find cosine similarity of each row with few anchor points in the word space. This provides a measure of the movement of the signal in the whole vocabulary space.\nFor selecting appropriate anchor points to better cover the space in its entirety, we perform k-means clustering on the vocabulary of the embedding model and find 4\n1 Project Gutenberg https://www.gutenberg.org 2 Python adaptation here https://radimrehurek.com/gensim/ models/word2vec.html\ncluster centers. These 4 points act as the anchors, resulting in a final matrix of size Ns × 4 for each book.\nA plot of distances from these 4 points for The Sign of the Four by Arthur Conan Doyle is shown in figure 4. Due to the fickle nature of text a large amount of noise is present. After smoothing with a gaussian filter (window size = 200, σ = 60), the essential rising and falling trend of curve is preserved. Filtered lines are shown with deeper color in the plot."
    }, {
      "heading" : "B. Comparing Signals",
      "text" : "Once generated, the signals are compared using FastDTW (Salvador and Chan, 2004) which implements a faster version of DTW algorithm. Vanilla DTW requires O(n2) computation, while FastDTW computes in linear time.\nWe use 24 classic novels collected from Project Gutenberg (listed in table I) and generate distance matrix for the items.\nA scatter plot of the books using multi dimensional scaling is shown in figure 5. Although the similarity measure itself is a useful metric for comparison, the scatter\nplot also clusters the authors according to the writing styles, affirming the hypothesis that a comparison of flow in the time series of word vectors can have subjective projections."
    }, {
      "heading" : "V. CONCLUSIONS AND FUTURE WORKS",
      "text" : "This study provides a way to explore literary works as signals in word embedding space. The clusters of authors formed as the result of analysis provides encouraging support for this method as a high level text analysis technique.\nA more rigorous study can be done by building on present method to identify the principle components involved in shaping the overall picture of a book while at the same time, being agnostic of factual details. The text representation can be made better using paragraph (or\nsentence) vectors instead of word vectors. Anchor points can be improved based on the effects and the number of points. A frequency based analysis of signals can provide a better insight."
    } ],
    "references" : [ {
      "title" : "Distributed Representations of Words and Phrases and their Compositionality",
      "author" : [ "T. Mikolov", "K. Chen", "G. Corrado", "J. Dean" ],
      "venue" : "NIPS, Pp. 1–9.",
      "citeRegEx" : "Mikolov et al\\.,? 2013a",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Efficient Estimation of Word Representations in Vector Space",
      "author" : [ "T. Mikolov", "G. Corrado", "K. Chen", "J. Dean" ],
      "venue" : "Proceedings of the International Conference on Learning Representations (ICLR 2013), Pp. 1–12.",
      "citeRegEx" : "Mikolov et al\\.,? 2013b",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "FastDTW: Toward accurate dynamic time warping in linear time and space",
      "author" : [ "S. Salvador", "P. Chan" ],
      "venue" : "KDD Workshop on Mining Temporal and Sequential Data, Pp. 70–80.",
      "citeRegEx" : "Salvador and Chan,? 2004",
      "shortCiteRegEx" : "Salvador and Chan",
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Once generated, the signals are compared using FastDTW (Salvador and Chan, 2004) which implements a faster version of DTW algorithm.",
      "startOffset" : 55,
      "endOffset" : 80
    } ],
    "year" : 2015,
    "abstractText" : "This flow can be quantified and compared by analyzing the text using natural language processing techniques. This study uses word embedding models to generate time series for novels and then compare the resulting series using dynamic time warping to find similarities. Considering time series analysis rather than a pure statistical one can capture the flow of the works and thus the similarities generated will provide a metric for comparing the novels based on the progression.",
    "creator" : "LaTeX with hyperref package"
  }
}