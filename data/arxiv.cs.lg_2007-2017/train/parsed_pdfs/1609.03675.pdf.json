{
  "name" : "1609.03675.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Recurrent Coevolutionary Feature Embedding Processes for Recommendation",
    "authors" : [ "Hanjun Dai", "Yichen Wang", "Rakshit Trivedi", "Le Song" ],
    "emails" : [ "rstrivedi}@gatech.edu,", "lsong@cc.gatech.edu" ],
    "sections" : [ {
      "heading" : "1. INTRODUCTION",
      "text" : "E-commerce platforms and social service websites, such as Reddit, Amazon, and Netflix, attracts thousands of users every second. Effectively recommending the appropriate service items to users is a fundamentally important task for these online services. It can significantly boost the user activities on these sites and leads to increased product purchases and advertisement clicks.\n“You are what you eat and you think what you read.” The interactions between users and items play a critical role in driving the evolution of user interests and item features. For example, for music streaming services, a long-time fan of Rock music listens to an interesting Blues one day, and starts to listen to more Blues in stead of Rock music. Similarly, a single music may also serve different audiences at different times. For example, a music initially targeted for an older generation may become popular among the young, and the features of this music need to be updated.\n∗Authors have equal contributions.\nRecsys Workshop on Deep Learning for Recommendation Systems (DLRS ’16), September 15 2016, Boston, MA, USA\nACM ISBN 978-1-4503-2138-9.\nDOI: 10.1145/1235\nFurther more, as users interact with different items, users’ interests and items’ features can also co-evolve over time, i.e., their features are intertwined and can influence each other:\n• user → item. For instance, in online discussion forums, such as Reddit, although a group (item) is initially created for statistics topics, users with very different interest profiles can join this group. Therefore, the participants can shape the features of the group through their postings and responses. It is likely that this group can eventually become one about deep learning simply because most users here concern about deep learning. • item → user. As the group is evolving towards topics on\ndeep learning, some users may become more interested in deep learning topics, and they may participate in other specialized groups on deep learning. On the opposite side, some users may gradually gain interests in pure math groups, lose interests in statistics and become inactive in this group.\nSuch co-evolutionary nature of user-item interactions raises very important questions on how to model them and how to learn them from observed data. Further more, nowadays large amount of user-item interaction data are becoming increasingly available online. In addition to the precise time-stamps of the interactions, many datasets also contain additional context such as text, image, and video. There is urgent need to design new models, and learning and inference algorithms to leverage the huge potential of such data.\nHowever, existing methods either treat the temporal useritem interactions data as a static graph or use epoch based methods such as tensor factorization to learn the latent features [?]. These methods are not able to capture the fine grained temporal dynamics of user-item interactions. Recent point process based models treat time as a random variable and improves over the traditional methods significantly [1]. However, point process based methods typically make strong assumptions about the function form of the generative processes, which may not reflect the reality or may not be accurate enough to capture the complex and nonlinear user-item influence in real world. Moreover, it is not easy to incorporate the observed context features in such point process model.\nHow can we obtain a more expressive model to capture the co-evolution features of user-item interactions, and learn such a model from large volume of data? To tackle this challenge, in this paper, we combine recurrent neural network (RNN) with multivariate point process models [2], and propose a recurrent coevolutionary feature embedding process framework. In particular, our work makes the following contributions:\nar X\niv :1\n60 9.\n03 67\n5v 1\n[ cs\n.L G\n] 1\n3 Se\np 20\n16\n• We propose a novel model that captures the nonlinear co-evolution nature of users’ and items’ latent features. Our model assigns an evolving feature embedding process for each user and item, and the co-evolution of these latent feature processes is considered using two parallel components: (i) item → user component, a user’s latent feature is determined by the nonlinear embedding of latent features of the items he interacted with; and (ii) user → item component, conversely, an item’s latent features are also determined by the latent features of the users who interact with the item.\n• We use recurrent neural network to parametrize the nonlinear embedding and it can also take into account the presence of potentially high dimensional observed context features.\n• We evaluate our method over multiple datasets, verifying that our method can lead to significant improvements in user behavior prediction compared to previous state-of-the-arts. Precise time prediction is especially novel and not possible by most prior work."
    }, {
      "heading" : "2. RELATED WORK",
      "text" : "Recent work predominantly fix the latent features assigned to each user and item [3, 4, 5, 6, 7, 8, 9, 10]. In more sophisticated methods, the time is divided into epochs, and static latent feature models are applied to each epoch to capture some temporal aspects of the data [11, 12, 13, 12, 13, 14, 15, 16, 17, 18, 19, 20]. For such methods, it is not clear how to choose the epoch length parameter. First, different users may have very different timescale when they interact with those service items, making it difficult to choose a unified epoch length. Second, it is not easy for these methods to answer time-sensitive queries such as when a user will return to the service item. The predictions are only in the resolution of the chosen epoch length. Recently, [1] proposed a low-rank point process based model for time- sensitive recommendations from recurrent user activities. However, it fails to capture the heterogeneous coevolutionary properties of user-item interactions.\nIn the deep learning community, [21] proposed collaborative deep learning, a hierarchical Bayesian model that jointly performs learning for the content features and collaborative filtering for the ratings matrix. This method considers interaction data as static graph and also does not capture latent coevolutionary properties of user-item interactions. [22] applied recurrent neural network based approach to recommender systems. Specifically, they adopt item-to-item recommendation approach but use session based data with temporal ordering to capture influences of past interactions in particular session. However, it does not consider evolving and co-evolving features of users and items interacting with each other, partly because it is designed for the scenario where user information is not available. Finally, our work is inspired from newly proposed recurrent marked temporal point process framework [23] that builds a connection between RNN and Point Processes. However, [23] focuses on the task of next event prediction given a sequence of past events for an entity and is only designed for one-dimension point process. Significant generations and extensions are needed for the recommendation system setting with feature coevolution."
    }, {
      "heading" : "3. BACKGROUND ON TEMPORAL POINT PROCESSES",
      "text" : "A temporal point process [24, 25] is a random process whose realization consists of a list of discrete events localized in time, {ti} with ti ∈ R+ and i ∈ Z+. Equivalently, a given temporal point process can be represented as a counting process, N(t), which records the number of events before time t. An important way to characterize temporal point processes is via the conditional intensity function λ(t), a stochastic model for the time of the next event given all the previous events. Formally, λ(t)dt is the conditional probability of observing an event in a small window [t, t+ dt) given the history H(t) up to t and that the event has not happen before t, i.e.,\nλ(t)dt := P {event in [t, t+ dt)|H(t)} = E[dN(t)|H(t)]\n, where one typically assumes that only one event can happen in a small window of size dt, i.e., dN(t) ∈ {0, 1}.\nThen, given a time t > 0, we can also characterize the conditional probability that no event happens during [0, t) as [26]:\nS(t) = exp(− ∫ t 0 λ(τ) dτ)\nand the conditional density that an event occurs at time t is defined as\nf(t) = λ(t)S(t) (1)\nThe function form of the intensity λ(t) is often designed to capture the phenomena of interests. Commonly used form includes:\n• Hawkes processes [27, 28], whose intensity models the excitation between events, i.e., λ(t) = µ+α ∑ ti∈H(t) κω(t−\nti), where κω(t) := exp(−ωt)I[t > 0] is an exponential triggering kernel, µ > 0 is a baseline intensity independent of the history. Here, the occurrence of each historical event increases the intensity by a certain amount determined by the kernel κω and the weight α > 0, making the intensity history dependent and a stochastic process by itself.\n• Rayleigh process, whose intensity function is\nλ(t) = αt (2)\nwhere α > 0 is the weight parameter."
    }, {
      "heading" : "4. RECURRENT COEVOLUTIONARY FEATURE EMBEDDING PROCESSES",
      "text" : "In this section, we present the generative framework for modeling the temporal dynamics of user-item interactions. We first explicitly capture the co-evolving nature of users’ and items’ latent feature. Then, based on the compatibility between the users’ and items’ latent feature, we model the user-item interactions by a temporal point process and parametrize the intensity function by the compatibility."
    }, {
      "heading" : "4.1 Event representation",
      "text" : "Given m users and n items, we denote the ordered list of N observed events as O = {ej = (uj , ij , tj , qj)}Nj=1 on time window [0, T ], where t1 6 . . . 6 N . Each event is modeled as the tuple (uj , ij , tj , qj), where uj ∈ {1, . . . ,m},\nij ∈ {1, . . . , n}, tj ∈ R+, which means that the interaction between user uj , item ij at time tj , with the interaction context qj ∈ Rd. Here qj can be a high dimension vector such as the text review, or simply the embedding of static user/item features such as user’s profile and item’s categorical features. For notation simplicity, we define\n• Ou = {euj = (iuj , tuj , quj )} |Ou| j=1 as the ordered listed of all\nevents related to user u. • Similarly we have Oi = {eij = (uij , tij , qij)} |Oi| j=1 as the\nordered list of all events related to item i. We also set ti0 = t u 0 = 0 for all the users and items. We will also use tk− to denote the time point just before time tk."
    }, {
      "heading" : "4.2 Recurrent feature embedding processes",
      "text" : "We associate latent features Uu(t) ∈ Rk with each user u and Ii(t) ∈ Rk with each item i. These features represent the subtle properties which cannot be directly observed, such as the interests of a user and the semantic topics of an item. Specifically, we model the drift, evolution, and co-evolution of Uu(t) and Ii(t) as follows a piecewise constant function of time and has jumps only at event times. Specifically, we have define: • User embedding process. For each user u, the\ncorresponding embedding after user u’s k-th event euk = (i u k , t u k , q u k ) can be formulated as:\nUu(t u k) = σ ( W1(t\nu k − tuk−1)︸ ︷︷ ︸\ntemporal drift\n+W2Uu(t u k−1)︸ ︷︷ ︸\nself evolution\n(3)\n+ W3Iik (t u k−)︸ ︷︷ ︸\nco-evolution: item feature\n+ W4q u,ik k︸ ︷︷ ︸\ninteraction feature ) • Item embedding process. For each item i, we spec-\nify Ii(t) at time t i k as:\nIi(t i k) = σ ( V1(t\ni k − tik−1)︸ ︷︷ ︸\ntemporal drift\n+V2Ii(t i k−1)︸ ︷︷ ︸\nself evolution\n(4)\n+ V3Uuk (t i k−)︸ ︷︷ ︸\nco-evolution: item feature\n+ V4q i,uk k︸ ︷︷ ︸\ninteraction feature\n)\nwhere t− means the time point just before time t, W4,V4 ∈ Rk×d are the embedding matrices mapping from the explicit high-dimensional feature space into the low-rank latent feature space and Wi,Vi ∈ RK×K , i = 1, 2, 3 are weights parameters. σ(·) is the nonlinear activation function, such as commonly used ReLU, Tanh, or Sigmoid. For simplicity, we use basic recurrent neural network to formulate the recurrence, but it is also straightforward to extend it using GRU or LSTM to gain more expressive power. Figure 1 summarizes the basic setting of our model.\nHere both the user and item’s feature embedding processes are piecewise constant functions of time and only updated if an interaction event happens. A user’s attribute changes only when he had a new interaction with some item. For example, a user’s taste for music would change only when he listened to some new or old musics. Also, an item’s attribute would change only when some user interacts with it. Hence, the key idea is we only need to model the points when the embedding needs to evolve. Next we discuss the rationale of each term in detail:\n• Temporal drift. The first term is defined based on the time difference between consecutive events of specific user or item. It allows the basic features of users (e.g., a user’s self-crafted interests) and items (e.g., textual categories and descriptions) to smoothly drift through time. Such changes of basic features normally are caused by external influences.\n• Self evolution. The current user feature should also be influenced by its feature at the earlier time. This captures the intrinsic evolution of user/item features. For example, a user’s current taste should be more or less similar to his/her tastes two days ago.\n• Evolution with interaction features. Users’ and items’ features can evolve and be influenced by the characteristics of their interactions. For instance, the genre changes of movies indicate the changing tastes of users. The theme of a chatting-group can be easily\nshifted to certain topics of the involved discussions. In consequence, this term captures the influence of the current interaction features to the changes of the latent user (item) features.\n• User-item coevolution. Users’ and items’ latent features can mutually influence each other. This term captures the two parallel processes. First, a user’s latent feature is determined by the latent features of the items he interacted with. At each time tk, the latent item feature is Iik (t u k). In our model, we capture\nboth the temporal influence and feature of each history item as a latent process. Conversely, an item’s latent features are determined by the latent features of the user who just interacts with the item.\n• Interaction feature. The interaction feature is the additional information/data happened in the user-item interactions. For example, in online discussion forums such as Reddit, the interaction feature is the posts and comments made by the user. In the online review sites such as Yelp, it is the reviews of the businesses.\nTo summarize, each feature embedding process evolves according to the respective base temporal user (item) features and also are mutually dependent on each other due to the endogenous influences from the interaction features and the entangled latent features."
    }, {
      "heading" : "4.3 User-item interactions as temporal point processes",
      "text" : "For each user, we model the recurrent occurrences of user u’s interaction with all items as a multi-dimensional temporal point process, with each item as one dimension. In particular, the intensity in the i-th dimension (item i) is modeled as a Rayleigh process:\nλu,i(t− t0) = exp ( Uu(t−)>Ii(t−) ) ︸ ︷︷ ︸ user-item compatibility ∗ (t− t0)︸ ︷︷ ︸ time lapse\n(5)\nwhere t > t0, and t− means the time point just before time t. The rationale behind this formulation is three fold: • Time as a random variable. Instead of discretizing\nthe time into epochs as in traditional mehtods [16, 17, 18, 19, 20], we explicitly model the timing of each interaction event as a random variable, which naturally captures the heterogeneity of the temporal interactions between users and items. • Short term preference. The probability for user u to\ninteract with item i at time t depends on the compatibility of their instantaneous latent features. Such compatibility is evaluated through the inner product of their latent features at the last time t0. It serves as α in (2). • Rayleigh time distribution. The user and item feature\nembeddings are piecewise constant, and we use this Rayleigh term to make the intensity function to be piecewise linear. This formulation assumes a Rayleigh distribution for the time intervals between consecutive events in each dimension [29]. It is well-adapted to modeling fads, where the infection likelihood f in (1) rises to a peak and then drops extremely rapidly. Furthermore, it is computationally easy to compute integration and get analytic form of f . One can then\nuse f to make item recommendation by finding the dimension that reaches the peak.\nBecause Uu(t) and Ii(t) co-evolve through time, their innerproduct measures a general representation of the cumulative influence from the past interactions to the occurrence of the current event. When the product is positive, it indicates a self-exciting behavior that most recent activities will trigger more events in the near future. For instance, one may repeatedly listen to a newly bought album within a short-time window. When the product becomes negative, it represents a self-correcting behavior that most recent interactions will decrease the chance of more future events. For example, after one keeps listening to the same album for a long time, he may become bored and thus changes interests to other items.\nGiven a collection of events recorded within a time window [0, T ), we can further estimate the parameters using maximum likelihood estimation of all events. The joint negative log-likelihood is [30]:\n` = − N∑ j=1 log ( λuj ,ij (tj) ) − m∑ u=1 n∑ i=1 ∫ T 0 λu,i(τ) dτ (6)\nwhere each event from O will have one term in the the first summation, and the each pair of potential item-user interaction will have one term in the second double summation. One advantage of point process formulation is that the nonpresence of an interaction at particular point in time is nicely taken into account in survival terms in the second double summation."
    }, {
      "heading" : "5. PARAMETER LEARNING",
      "text" : "Having presented the model, in this section, we propose an efficient algortihm to learn the parameters. Though we presented batch objective function in Equation 6, we seek to use stochastic methods to learn the embedding parameters {Vi}4i=1 and {Wi} 4 i=1. The Adam Optimizer [31] is used in our experiment, since it has shown good performance in training RNNs., and use gradient clip to avoid gradient explosion.\nThe Back Propagation Through Time (BPTT) is the standard way to train a RNN. To make the back propagation tractable, one typically needs to do truncation during training. Different from traditional sequential data where one can easily break the sequences into multiple segments to make the BPTT trackable, here all the events are related to each other by the user-item bipartite graph, which makes it hard to decompose.\nTo do this, we first order all the events globally and then do mini-batch training in a sliding window fashion. Each time when conducting feed forward and back propagation, we take the consecutive events within current sliding window to build the computational graph. In our case the truncation is on the global timeline, instead over individual independent sequencs. Another benefit of ordering events globally is that it allows us to keep the user and item latent features that could be used for the future mini-batch training. Figure 2 illustrates our training method.\nSince the user-item interactions vary a lot across minibatches, the corresponding computational graph also changes greatly. To make the learning efficient, we use the graph embedding framework [32] which allows training deep learning models where each term in the objective has a different computational graphs but with shared parameters.\nNext, we discuss in details on gradient computation. First, note that the intensity function λu,i(t) is piecewise linear, hence the integration in (6) can also be computed in a piecewise fashion with closed form, where the number of pieces equals to the total number of events happened to user u and item i separately.\nComputing gradient For illustration purpose, we here use Sigmoid as the nonlinear activation function σ. In order to get gradient with respect to parameter W s, we first compute gradients with respect to each varying points of embeddings. For user u’s embedding after his k-th event, the corresponding partial derivatives are computed by:\n∂`\n∂Uu(tuk) = −Iiu k︸ ︷︷ ︸ from intensity + n∑ i=1\n∂ ∫ tuk+1 tu k λu,i(τ)dτ\n∂Uu(tuk)︸ ︷︷ ︸ from survival\n+ (7)\n∂`\n∂Uu(tuk+1) (1− Uu(tuk+1)) Uu(tuk+1)W2︸ ︷︷ ︸\nfrom user u’s next embedding\n+ ∂`\n∂Iiu k+1 (tuk+1) (1− Iiu k+1 (tuk+1)) Iiuk+1(t u k+1)︸ ︷︷ ︸\nfrom user u’s next item embedding\nwhere denotes element-wise multiplication. The gradient coming from the second term (i.e., the survival term) is also easy to compute, since the Rayleigh distribution has closed form of survival function. For a certain item i, if its feature doesn’t changed between time interval [tuk , t u k+1], then we have\n∂ ∫ tuk+1 tu k λu,i(τ)dτ\n∂Uu(tuk) =\n(tuk+1 − tuk)2\n2 exp\n( Uu(t u k) >Ii(t u k)Ii(t u k) )\n(8) On the other hand, if the embedding of item i changes during this time interval, then we should break this interval into segments and compute the summation of gradients in each segment in a way similar to (8). Thus, we are able to compute the gradients with respect to Wi, i ∈ {1, 2, 3, 4} as follows.\n∂`\n∂W1 = m∑ u=1 ∑ k ∂` ∂Uu(tuk) (1− Uu(tuk)) Uu(tuk)(tuk − tuk−1)\n∂`\n∂W2 = m∑ u=1 ∑ k ( ∂` ∂Uu(tuk) (1− Uu(tuk)) Uu(tuk) ) Uu(t u k−1) >\n∂`\n∂W3 = m∑ u=1 ∑ k ( ∂` ∂Uu(tuk) (1− Uu(tuk)) Uu(tuk) ) Iik (t u k−)>\n∂`\n∂W4 = m∑ u=1 ∑ k ( ∂` ∂Uu(tuk) (1− Uu(tuk)) Uu(tuk) ) q u,ik k\nSince the items are treated symmetrically as users, the corresponding derivatives can be obtained in a similar way."
    }, {
      "heading" : "6. EXPERIMENTS",
      "text" : "We evaluate our model on real-world datasets. For each sequence of user activities, we use all the events up to time T · p as the training data, and the rest events as the testing data, where T is the observation window. We report the results on two tasks: • Item prediction. At each test time, we predict the\nitem that the user will interact with. We rank all the\nJacob\n1:45pm\nSophie\nJacob\nSophie\n3:45pm 5:00pm 9:00pm 10:30pm\n3:30pm\n3:15pm\n2:30pm 4:25pm\n9:25pm\n9:45pm\n10:00pm8:15pm\nMini-batch 1 Mini-batch 2\n(user, forum)"
    }, {
      "heading" : "6.1 Competitors",
      "text" : ""
    }, {
      "heading" : "6.2 Datasets",
      "text" : "We use three real world datasets. IPTV. It contains 7,100 users’ watching history of 385 TV programs in 11 months (Jan 1 - Nov 30 2012), with around 2M events, and 1,420 movie features (including 1,073 actors, 312 directors, 22 278 genres, 8 countries and 5 years).\nYelp. This data was available in Yelp Dataset challenge Round 7. It contains reviews for various businesses from October, 2004 to December, 2015. Out of available 552K users, we used users with more than 100 posts for our experiments. We cleaned the review text by removing stop words\nand punctuation marks and only included words of length > 3 and frequency > 10. After this pre-processing, the dataset comprised of 1,503 users, 47,924 groups (businesses) and 34,508 text features with a total of 2,92,000 reviews. To be able to compare with baselines, we further decreased the size of this processed dataset and used a total of 95,000 reviews between randomly selected 95 users and 17,205 businesses.\nReddit. We collected discussion related data on different subreddits (groups) for the month of January 2014. We filtered all bot users’ and their posts from this dataset. Similar to Yelp dataset, we cleaned the text of posts to remove stop words and punctuation marks and only include words of length > 3 and frequency > 10. Furthermore, we only considered top 10,000 users sorted according to the frequency of posts and randomly selected 1,000 users out of it to create smaller dataset. After all pre-processing, the dataset consists of 1,000 users, 1,403 groups and 82,389 text features. This dataset contains a total of 10,000 discussion events."
    }, {
      "heading" : "6.3 Results",
      "text" : "Item Recommendation From Figure 3 we can see, our method significantly outperforms epoch-based baselines in terms of item prediction on all the datasets. While the best possible MAR one can achieve is 1, both our method and LowRankHawkes got quite accurate results. Regarding the MAR metric, the performance is also slightly better compared with LowRankHawkes. Since one only need the rank of conditional density f to conduct item prediction, LowRankHawkes may still be good at differentiating f , but could not learn the actual value of f accurately, as shown in the time prediction task where the value of f is needed for precise prediction.\nTime Prediction On time prediction, R-coevolve significantly outperforms other methods. For example, compared with LowRankHawkes, it has 2× time improvement on Yelp, 6× improvement on Reddit, and 30× improvement on IPTV. The time unit is hour. Hence it has 2 weeks accuracy improvement on IPTV and 2 days on Reddit. This is important for online merchants to make time sensitive recommendations. An intuitive explanation is that our method\naccurately captures the nonlinear pattern between user and item interactions. The competitor LowRankHawkes assumes specific parametric forms of the user-item interaction process, hence may not be accurate or expressive enough to capture real world temporal patterns. Furthermore, the LowRankHawkes modeled each user-item interaction dimension independently, which may lose the important affection from user’s interaction with other items while predicting the current item’s reoccurance time."
    }, {
      "heading" : "7. CONCLUSION",
      "text" : "We have proposed an efficient framework for modeling the co-evolution nature of users’ and items’ latent features. It is a generative model designed for modeling and understanding user’s online behaviors, which is different from prior work that only focuses on the prediction task in the recommender system. Moreover, the user and item’s evolving and co-evolving processes are captured by the RNN. We demonstrate the superior performance of our method on the time prediction task, which is not possible by most prior work. Future work includes extending to other applications such as modeling dynamics of social message groups, and understanding peoples’ behaviors on Q&A sites.\nAcknowledge This project was supported in part by NSF/NIH BIGDATA 1R01GM108341, ONR N00014-15-1- 2340, NSF IIS-1639792, NSF IIS-1218749, NSF CAREER IIS-1350983, Intel and NVIDIA."
    }, {
      "heading" : "8. REFERENCES",
      "text" : "[1] Nan Du, Yichen Wang, Niao He, and Le Song. Time\nsensitive recommendation from recurrent user activities. In NIPS, 2015.\n[2] Thomas Josef Liniger. Multivariate Hawkes Processes. PhD thesis, Swiss Federal Institute of Technology Zurich, 2009.\n[3] R. Salakhutdinov and A. Mnih. Bayesian probabilistic matrix factorization using markov chain monte carlo.\nIn W.W. Cohen, A. McCallum, and S.T. Roweis, editors, ICML, volume 307, pages 880–887. ACM, 2008.\n[4] Y. Chen, D. Pavlov, and J.F. Canny. Large-scale behavioral targeting. In J.F. Elder, F. Fogelman-Soulié, P.A. Flach, and M. J. Zaki, editors, KDD, pages 209–218. ACM, 2009.\n[5] D. Agarwal and B.-C. Chen. Regression-based latent factor models. In J.F. Elder, F. Fogelman-Soulié, P.A. Flach, and M.J. Zaki, editors, KDD, pages 19–28. ACM, 2009.\n[6] Michael D Ekstrand, John T Riedl, and Joseph A Konstan. Collaborative filtering recommender systems. Foundations and Trends in Human-Computer Interaction, 4(2):81–173, 2011.\n[7] Yehuda Koren and Joe Sill. Ordrec: an ordinal model for predicting personalized item rating distributions. In RecSys, 2011.\n[8] Shuang-Hong Yang, Bo Long, Alex Smola, Narayanan Sadagopan, Zhaohui Zheng, and Hongyuan Zha. Like like alike: joint friendship and interest propagation in social networks. In WWW, 2011.\n[9] Xing Yi, Liangjie Hong, Erheng Zhong, Nanthan Nan Liu, and Suju Rajan. Beyond clicks: Dwell time for personalization. In RecSys, 2014.\n[10] Yichen Wang and Aditya Pal. Detecting emotions in social media: A constrained optimization approach. In IJCAI, 2015.\n[11] Y. Koren. Collaborative filtering with temporal dynamics. In KDD, 2009.\n[12] Alexandros Karatzoglou, Xavier Amatriain, Linas Baltrunas, and Nuria Oliver. Multiverse recommendation: n-dimensional tensor factorization for context-aware collaborative filtering. In Recsys, pages 79–86. ACM, 2010.\n[13] Liang Xiong, Xi Chen, Tzu-Kuo Huang, Jeff G. Schneider, and Jaime G. Carbonell. Temporal collaborative filtering with bayesian probabilistic tensor factorization. In SDM, pages 211–222. SIAM, 2010.\n[14] Eric C Chi and Tamara G Kolda. On tensors, sparsity, and nonnegative factorizations. SIAM Journal on Matrix Analysis and Applications, 33(4):1272–1299, 2012.\n[15] San Gultekin and John Paisley. A collaborative kalman filter for time-evolving dyadic processes. In ICDM, pages 140–149, 2014.\n[16] Laurent Charlin, Rajesh Ranganath, James McInerney, and David M Blei. Dynamic poisson factorization. In RecSys, 2015.\n[17] Jiayu Zhou Juhan Lee Preeti Bhargava, Thomas Phan. Who, what, when, and where: Multi-dimensional collaborative recommendations using tensor factorization on sparse user-generated data. In WWW, 2015.\n[18] Prem Gopalan, Jake M Hofman, and David M Blei. Scalable recommendation with hierarchical poisson factorization. UAI, 2015.\n[19] Balázs Hidasi and Domonkos Tikk. General factorization framework for context-aware recommendations. Data Mining and Knowledge Discovery, pages 1–30, 2015.\n[20] Xin Wang, Roger Donaldson, Christopher Nell, Peter\nGorniak, Martin Ester, and Jiajun Bu. Recommending groups to users using user-group engagement and time-dependent matrix factorization. In AAAI, 2016.\n[21] Hao Wang, Naiyan Wang, and Dit-Yan Yeung. Collaborative deep learning for recommender systems. In KDD. ACM, 2015.\n[22] Balazs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. Session-based recommendations with recurrent neural networks. In Proceedings of the International Conference on Learning Representations (ICLR), 2016.\n[23] Nan Du, Hanjun Dai, Rakshit Trivedi, Utkarsh Upadhyay, Manuel Gomez-Rodriguez, and Le Song. Recurrent marked temporal point processes: Embedding event history to vector. In KDD. ACM, 2016.\n[24] D.R. Cox and V. Isham. Point processes, volume 12. Chapman & Hall/CRC, 1980.\n[25] D.R. Cox and P.A.W. Lewis. Multivariate point processes. Selected Statistical Papers of Sir David Cox: Volume 1, Design of Investigations, Statistical Methods and Applications, 1:159, 2006.\n[26] Odd Aalen, Ornulf Borgan, and Hakon Gjessing. Survival and event history analysis: a process point of view. Springer, 2008.\n[27] Alan G Hawkes. Spectra of some self-exciting and mutually exciting point processes. Biometrika, 58(1):83–90, 1971.\n[28] Yichen Wang, Bo Xie, Nan Du, and Le Song. Isotonic hawkes processes. In ICML, 2016.\n[29] Manuel Gomez-Rodriguez, David Balduzzi, and Bernhard Schölkopf. Uncovering the temporal dynamics of diffusion networks. In Proceedings of the International Conference on Machine Learning, 2011.\n[30] D.J. Daley and D. Vere-Jones. An introduction to the theory of point processes: volume II: general theory and structure, volume 2. Springer, 2007.\n[31] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n[32] Hanjun Dai, Bo Dai, and Le Song. Discriminative embeddings of latent variable models for structured data. In ICML, 2016.\n[33] Yichen Wang, Robert Chen, Joydeep Ghosh, Joshua C Denny, Abel Kho, You Chen, Bradley A Malin, and Jimeng Sun. Rubik: Knowledge guided tensor factorization and completion for health data analytics. In KDD, 2015.\n[34] Komal Kapoor, Karthik Subbian, Jaideep Srivastava, and Paul Schrater. Just in time recommendations: Modeling the dynamics of boredom in activity streams. In WSDM, 2015."
    } ],
    "references" : [ {
      "title" : "Time sensitive recommendation from recurrent user activities",
      "author" : [ "Nan Du", "Yichen Wang", "Niao He", "Le Song" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2015
    }, {
      "title" : "Multivariate Hawkes Processes",
      "author" : [ "Thomas Josef Liniger" ],
      "venue" : "PhD thesis, Swiss Federal Institute of Technology Zurich,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2009
    }, {
      "title" : "Bayesian probabilistic matrix factorization using markov chain monte carlo",
      "author" : [ "R. Salakhutdinov", "A. Mnih" ],
      "venue" : " In W.W. Cohen, A. McCallum, and S.T. Roweis, editors, ICML, volume 307, pages 880–887. ACM",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Large-scale behavioral targeting",
      "author" : [ "Y. Chen", "D. Pavlov", "J.F. Canny" ],
      "venue" : "J.F. Elder, F. Fogelman-Soulié, P.A. Flach, and M. J. Zaki, editors, KDD, pages 209–218. ACM",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Regression-based latent factor models",
      "author" : [ "D. Agarwal", "B.-C. Chen" ],
      "venue" : "J.F. Elder, F. Fogelman-Soulié, P.A. Flach, and M.J. Zaki, editors, KDD, pages 19–28. ACM",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Collaborative filtering recommender systems",
      "author" : [ "Michael D Ekstrand", "John T Riedl", "Joseph A Konstan" ],
      "venue" : "Foundations and Trends in Human-Computer Interaction,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2011
    }, {
      "title" : "Ordrec: an ordinal model for predicting personalized item rating distributions",
      "author" : [ "Yehuda Koren", "Joe Sill" ],
      "venue" : "In RecSys,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2011
    }, {
      "title" : "Like like alike: joint friendship and interest propagation in social networks",
      "author" : [ "Shuang-Hong Yang", "Bo Long", "Alex Smola", "Narayanan Sadagopan", "Zhaohui Zheng", "Hongyuan Zha" ],
      "venue" : "In WWW,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "Beyond clicks: Dwell time for personalization",
      "author" : [ "Xing Yi", "Liangjie Hong", "Erheng Zhong", "Nanthan Nan Liu", "Suju Rajan" ],
      "venue" : "In RecSys,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    }, {
      "title" : "Detecting emotions in social media: A constrained optimization approach",
      "author" : [ "Yichen Wang", "Aditya Pal" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "Collaborative filtering with temporal dynamics",
      "author" : [ "Y. Koren" ],
      "venue" : "KDD",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Multiverse recommendation: n-dimensional tensor factorization for context-aware collaborative filtering",
      "author" : [ "Alexandros Karatzoglou", "Xavier Amatriain", "Linas Baltrunas", "Nuria Oliver" ],
      "venue" : "In Recsys,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2010
    }, {
      "title" : "Temporal collaborative filtering with bayesian probabilistic tensor factorization",
      "author" : [ "Liang Xiong", "Xi Chen", "Tzu-Kuo Huang", "Jeff G. Schneider", "Jaime G. Carbonell" ],
      "venue" : "In SDM,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2010
    }, {
      "title" : "On tensors, sparsity, and nonnegative factorizations",
      "author" : [ "Eric C Chi", "Tamara G Kolda" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2012
    }, {
      "title" : "A collaborative kalman filter for time-evolving dyadic processes",
      "author" : [ "San Gultekin", "John Paisley" ],
      "venue" : "In ICDM,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2014
    }, {
      "title" : "Dynamic poisson factorization",
      "author" : [ "Laurent Charlin", "Rajesh Ranganath", "James McInerney", "David M Blei" ],
      "venue" : "In RecSys,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2015
    }, {
      "title" : "Who, what, when, and where: Multi-dimensional collaborative recommendations using tensor factorization on sparse user-generated data",
      "author" : [ "Thomas Phan" ],
      "venue" : "In WWW,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2015
    }, {
      "title" : "Scalable recommendation with hierarchical poisson factorization",
      "author" : [ "Prem Gopalan", "Jake M Hofman", "David M Blei" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2015
    }, {
      "title" : "General factorization framework for context-aware recommendations",
      "author" : [ "Balázs Hidasi", "Domonkos Tikk" ],
      "venue" : "Data Mining and Knowledge Discovery,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "Recommending groups to users using user-group engagement and time-dependent matrix factorization",
      "author" : [ "Xin Wang", "Roger Donaldson", "Christopher Nell", "Peter  Gorniak", "Martin Ester", "Jiajun Bu" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2016
    }, {
      "title" : "Collaborative deep learning for recommender systems",
      "author" : [ "Hao Wang", "Naiyan Wang", "Dit-Yan Yeung" ],
      "venue" : "In KDD. ACM,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2015
    }, {
      "title" : "Session-based recommendations with recurrent neural networks",
      "author" : [ "Balazs Hidasi", "Alexandros Karatzoglou", "Linas Baltrunas", "Domonkos Tikk" ],
      "venue" : "In Proceedings of the International Conference on Learning Representations (ICLR),",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2016
    }, {
      "title" : "Recurrent marked temporal point processes: Embedding event history to vector",
      "author" : [ "Nan Du", "Hanjun Dai", "Rakshit Trivedi", "Utkarsh Upadhyay", "Manuel Gomez-Rodriguez", "Le Song" ],
      "venue" : "In KDD",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2016
    }, {
      "title" : "Point processes",
      "author" : [ "D.R. Cox", "V. Isham" ],
      "venue" : "volume 12. Chapman & Hall/CRC",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 1980
    }, {
      "title" : "Multivariate point processes",
      "author" : [ "D.R. Cox", "P.A.W. Lewis" ],
      "venue" : "Selected Statistical Papers of Sir David Cox: Volume 1, Design of Investigations, Statistical Methods and Applications, 1:159",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Survival and event history analysis: a process point of view",
      "author" : [ "Odd Aalen", "Ornulf Borgan", "Hakon Gjessing" ],
      "venue" : null,
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2008
    }, {
      "title" : "Spectra of some self-exciting and mutually exciting point processes",
      "author" : [ "Alan G Hawkes" ],
      "venue" : null,
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 1971
    }, {
      "title" : "Isotonic hawkes processes",
      "author" : [ "Yichen Wang", "Bo Xie", "Nan Du", "Le Song" ],
      "venue" : "In ICML,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2016
    }, {
      "title" : "Uncovering the temporal dynamics of diffusion networks",
      "author" : [ "Manuel Gomez-Rodriguez", "David Balduzzi", "Bernhard Schölkopf" ],
      "venue" : "In Proceedings of the International Conference on Machine Learning,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2011
    }, {
      "title" : "An introduction to the theory of point processes: volume II: general theory and structure",
      "author" : [ "D.J. Daley", "D. Vere-Jones" ],
      "venue" : "volume 2. Springer",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2014
    }, {
      "title" : "Discriminative embeddings of latent variable models for structured data",
      "author" : [ "Hanjun Dai", "Bo Dai", "Le Song" ],
      "venue" : "In ICML,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2016
    }, {
      "title" : "Rubik: Knowledge guided tensor factorization and completion for health data analytics",
      "author" : [ "Yichen Wang", "Robert Chen", "Joydeep Ghosh", "Joshua C Denny", "Abel Kho", "You Chen", "Bradley A Malin", "Jimeng Sun" ],
      "venue" : "In KDD,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2015
    }, {
      "title" : "Just in time recommendations: Modeling the dynamics of boredom in activity streams",
      "author" : [ "Komal Kapoor", "Karthik Subbian", "Jaideep Srivastava", "Paul Schrater" ],
      "venue" : "In WSDM,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Recent point process based models treat time as a random variable and improves over the traditional methods significantly [1].",
      "startOffset" : 122,
      "endOffset" : 125
    }, {
      "referenceID" : 1,
      "context" : "How can we obtain a more expressive model to capture the co-evolution features of user-item interactions, and learn such a model from large volume of data? To tackle this challenge, in this paper, we combine recurrent neural network (RNN) with multivariate point process models [2], and propose a recurrent coevolutionary feature embedding process framework.",
      "startOffset" : 278,
      "endOffset" : 281
    }, {
      "referenceID" : 2,
      "context" : "Recent work predominantly fix the latent features assigned to each user and item [3, 4, 5, 6, 7, 8, 9, 10].",
      "startOffset" : 81,
      "endOffset" : 106
    }, {
      "referenceID" : 3,
      "context" : "Recent work predominantly fix the latent features assigned to each user and item [3, 4, 5, 6, 7, 8, 9, 10].",
      "startOffset" : 81,
      "endOffset" : 106
    }, {
      "referenceID" : 4,
      "context" : "Recent work predominantly fix the latent features assigned to each user and item [3, 4, 5, 6, 7, 8, 9, 10].",
      "startOffset" : 81,
      "endOffset" : 106
    }, {
      "referenceID" : 5,
      "context" : "Recent work predominantly fix the latent features assigned to each user and item [3, 4, 5, 6, 7, 8, 9, 10].",
      "startOffset" : 81,
      "endOffset" : 106
    }, {
      "referenceID" : 6,
      "context" : "Recent work predominantly fix the latent features assigned to each user and item [3, 4, 5, 6, 7, 8, 9, 10].",
      "startOffset" : 81,
      "endOffset" : 106
    }, {
      "referenceID" : 7,
      "context" : "Recent work predominantly fix the latent features assigned to each user and item [3, 4, 5, 6, 7, 8, 9, 10].",
      "startOffset" : 81,
      "endOffset" : 106
    }, {
      "referenceID" : 8,
      "context" : "Recent work predominantly fix the latent features assigned to each user and item [3, 4, 5, 6, 7, 8, 9, 10].",
      "startOffset" : 81,
      "endOffset" : 106
    }, {
      "referenceID" : 9,
      "context" : "Recent work predominantly fix the latent features assigned to each user and item [3, 4, 5, 6, 7, 8, 9, 10].",
      "startOffset" : 81,
      "endOffset" : 106
    }, {
      "referenceID" : 10,
      "context" : "In more sophisticated methods, the time is divided into epochs, and static latent feature models are applied to each epoch to capture some temporal aspects of the data [11, 12, 13, 12, 13, 14, 15, 16, 17, 18, 19, 20].",
      "startOffset" : 168,
      "endOffset" : 216
    }, {
      "referenceID" : 11,
      "context" : "In more sophisticated methods, the time is divided into epochs, and static latent feature models are applied to each epoch to capture some temporal aspects of the data [11, 12, 13, 12, 13, 14, 15, 16, 17, 18, 19, 20].",
      "startOffset" : 168,
      "endOffset" : 216
    }, {
      "referenceID" : 12,
      "context" : "In more sophisticated methods, the time is divided into epochs, and static latent feature models are applied to each epoch to capture some temporal aspects of the data [11, 12, 13, 12, 13, 14, 15, 16, 17, 18, 19, 20].",
      "startOffset" : 168,
      "endOffset" : 216
    }, {
      "referenceID" : 11,
      "context" : "In more sophisticated methods, the time is divided into epochs, and static latent feature models are applied to each epoch to capture some temporal aspects of the data [11, 12, 13, 12, 13, 14, 15, 16, 17, 18, 19, 20].",
      "startOffset" : 168,
      "endOffset" : 216
    }, {
      "referenceID" : 12,
      "context" : "In more sophisticated methods, the time is divided into epochs, and static latent feature models are applied to each epoch to capture some temporal aspects of the data [11, 12, 13, 12, 13, 14, 15, 16, 17, 18, 19, 20].",
      "startOffset" : 168,
      "endOffset" : 216
    }, {
      "referenceID" : 13,
      "context" : "In more sophisticated methods, the time is divided into epochs, and static latent feature models are applied to each epoch to capture some temporal aspects of the data [11, 12, 13, 12, 13, 14, 15, 16, 17, 18, 19, 20].",
      "startOffset" : 168,
      "endOffset" : 216
    }, {
      "referenceID" : 14,
      "context" : "In more sophisticated methods, the time is divided into epochs, and static latent feature models are applied to each epoch to capture some temporal aspects of the data [11, 12, 13, 12, 13, 14, 15, 16, 17, 18, 19, 20].",
      "startOffset" : 168,
      "endOffset" : 216
    }, {
      "referenceID" : 15,
      "context" : "In more sophisticated methods, the time is divided into epochs, and static latent feature models are applied to each epoch to capture some temporal aspects of the data [11, 12, 13, 12, 13, 14, 15, 16, 17, 18, 19, 20].",
      "startOffset" : 168,
      "endOffset" : 216
    }, {
      "referenceID" : 16,
      "context" : "In more sophisticated methods, the time is divided into epochs, and static latent feature models are applied to each epoch to capture some temporal aspects of the data [11, 12, 13, 12, 13, 14, 15, 16, 17, 18, 19, 20].",
      "startOffset" : 168,
      "endOffset" : 216
    }, {
      "referenceID" : 17,
      "context" : "In more sophisticated methods, the time is divided into epochs, and static latent feature models are applied to each epoch to capture some temporal aspects of the data [11, 12, 13, 12, 13, 14, 15, 16, 17, 18, 19, 20].",
      "startOffset" : 168,
      "endOffset" : 216
    }, {
      "referenceID" : 18,
      "context" : "In more sophisticated methods, the time is divided into epochs, and static latent feature models are applied to each epoch to capture some temporal aspects of the data [11, 12, 13, 12, 13, 14, 15, 16, 17, 18, 19, 20].",
      "startOffset" : 168,
      "endOffset" : 216
    }, {
      "referenceID" : 19,
      "context" : "In more sophisticated methods, the time is divided into epochs, and static latent feature models are applied to each epoch to capture some temporal aspects of the data [11, 12, 13, 12, 13, 14, 15, 16, 17, 18, 19, 20].",
      "startOffset" : 168,
      "endOffset" : 216
    }, {
      "referenceID" : 0,
      "context" : "Recently, [1] proposed a low-rank point process based model for time- sensitive recommendations from recurrent user activities.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 20,
      "context" : "In the deep learning community, [21] proposed collaborative deep learning, a hierarchical Bayesian model that jointly performs learning for the content features and collaborative filtering for the ratings matrix.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 21,
      "context" : "[22] applied recurrent neural network based approach to recommender systems.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "Finally, our work is inspired from newly proposed recurrent marked temporal point process framework [23] that builds a connection between RNN and Point Processes.",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 22,
      "context" : "However, [23] focuses on the task of next event prediction given a sequence of past events for an entity and is only designed for one-dimension point process.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 23,
      "context" : "A temporal point process [24, 25] is a random process whose realization consists of a list of discrete events localized in time, {ti} with ti ∈ R and i ∈ Z.",
      "startOffset" : 25,
      "endOffset" : 33
    }, {
      "referenceID" : 24,
      "context" : "A temporal point process [24, 25] is a random process whose realization consists of a list of discrete events localized in time, {ti} with ti ∈ R and i ∈ Z.",
      "startOffset" : 25,
      "endOffset" : 33
    }, {
      "referenceID" : 25,
      "context" : "Then, given a time t > 0, we can also characterize the conditional probability that no event happens during [0, t) as [26]:",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 26,
      "context" : "• Hawkes processes [27, 28], whose intensity models the excitation between events, i.",
      "startOffset" : 19,
      "endOffset" : 27
    }, {
      "referenceID" : 27,
      "context" : "• Hawkes processes [27, 28], whose intensity models the excitation between events, i.",
      "startOffset" : 19,
      "endOffset" : 27
    }, {
      "referenceID" : 15,
      "context" : "Instead of discretizing the time into epochs as in traditional mehtods [16, 17, 18, 19, 20], we explicitly model the timing of each interaction event as a random variable, which naturally captures the heterogeneity of the temporal interactions between users and items.",
      "startOffset" : 71,
      "endOffset" : 91
    }, {
      "referenceID" : 16,
      "context" : "Instead of discretizing the time into epochs as in traditional mehtods [16, 17, 18, 19, 20], we explicitly model the timing of each interaction event as a random variable, which naturally captures the heterogeneity of the temporal interactions between users and items.",
      "startOffset" : 71,
      "endOffset" : 91
    }, {
      "referenceID" : 17,
      "context" : "Instead of discretizing the time into epochs as in traditional mehtods [16, 17, 18, 19, 20], we explicitly model the timing of each interaction event as a random variable, which naturally captures the heterogeneity of the temporal interactions between users and items.",
      "startOffset" : 71,
      "endOffset" : 91
    }, {
      "referenceID" : 18,
      "context" : "Instead of discretizing the time into epochs as in traditional mehtods [16, 17, 18, 19, 20], we explicitly model the timing of each interaction event as a random variable, which naturally captures the heterogeneity of the temporal interactions between users and items.",
      "startOffset" : 71,
      "endOffset" : 91
    }, {
      "referenceID" : 19,
      "context" : "Instead of discretizing the time into epochs as in traditional mehtods [16, 17, 18, 19, 20], we explicitly model the timing of each interaction event as a random variable, which naturally captures the heterogeneity of the temporal interactions between users and items.",
      "startOffset" : 71,
      "endOffset" : 91
    }, {
      "referenceID" : 28,
      "context" : "This formulation assumes a Rayleigh distribution for the time intervals between consecutive events in each dimension [29].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 29,
      "context" : "The joint negative log-likelihood is [30]:",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 30,
      "context" : "The Adam Optimizer [31] is used in our experiment, since it has shown good performance in training RNNs.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 31,
      "context" : "To make the learning efficient, we use the graph embedding framework [32] which allows training deep learning models where each term in the objective has a different computational graphs but with shared parameters.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 13,
      "context" : "We compared our method to the following algorithms: • PoissonTensor [14]: Poisson Tensor Factorization has been shown to perform better than factorization methods based on squared loss [12, 13, 33] on recommendation tasks.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 11,
      "context" : "We compared our method to the following algorithms: • PoissonTensor [14]: Poisson Tensor Factorization has been shown to perform better than factorization methods based on squared loss [12, 13, 33] on recommendation tasks.",
      "startOffset" : 185,
      "endOffset" : 197
    }, {
      "referenceID" : 12,
      "context" : "We compared our method to the following algorithms: • PoissonTensor [14]: Poisson Tensor Factorization has been shown to perform better than factorization methods based on squared loss [12, 13, 33] on recommendation tasks.",
      "startOffset" : 185,
      "endOffset" : 197
    }, {
      "referenceID" : 32,
      "context" : "We compared our method to the following algorithms: • PoissonTensor [14]: Poisson Tensor Factorization has been shown to perform better than factorization methods based on squared loss [12, 13, 33] on recommendation tasks.",
      "startOffset" : 185,
      "endOffset" : 197
    }, {
      "referenceID" : 0,
      "context" : "• LowRankHawkes [1]: This is a low rank point process based model which assumes user-item interactions to be independent of each other and does not capture the co-evolution of user and item features.",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 33,
      "context" : "• STIC [34]: it fits a semi-hidden markov model to each observed user-item pair and is only designed for time prediction.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 10,
      "context" : "• TimeSVD++ [11] and FIP [8]: These two methods are only designed for explicit ratings, the implicit user feedbacks (in the form of a series of interaction events) are converted into the explicit ratings by the respective frequency of interactions with users.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 7,
      "context" : "• TimeSVD++ [11] and FIP [8]: These two methods are only designed for explicit ratings, the implicit user feedbacks (in the form of a series of interaction events) are converted into the explicit ratings by the respective frequency of interactions with users.",
      "startOffset" : 25,
      "endOffset" : 28
    } ],
    "year" : 2016,
    "abstractText" : "Recommender systems often use latent features to explain the behaviors of users and capture the properties of items. As users interact with different items over time, user and item features can influence each other, evolve and co-evolve over time. To accurately capture the fine grained nonlinear coevolution of these features, we propose a recurrent coevolutionary feature embedding process model, which combines recurrent neural network (RNN) with a multidimensional point process model. The RNN learns a nonlinear representation of user and item features which take into account mutual influence between user and item features, and the feature evolution over time. We also develop an efficient stochastic gradient algorithm for learning the model parameters, which can readily scale up to millions of events. Experiments on diverse real-world datasets demonstrate significant improvements in user behavior prediction compared to state-of-the-arts.",
    "creator" : "LaTeX with hyperref package"
  }
}