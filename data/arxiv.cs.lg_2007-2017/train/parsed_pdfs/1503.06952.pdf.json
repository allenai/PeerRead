{
  "name" : "1503.06952.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Everton A. Cherman", "Maria C. Monard" ],
    "emails" : [ "jeanmetz@utfpr.edu.br", "newtonsp@icmc.usp.br", "echerman@icmc.usp.br", "mcmonard@icmc.usp.br" ],
    "sections" : [ {
      "heading" : null,
      "text" : "? Acknowledgements: This research was supported by the São Paulo Research Foundation (FAPESP), grants 2008/06739-0, 2010/15992-0 and 2011/02393-4.\nJean Metz Federal Technological University of Paraná (UTFPR) P.O. Box 271, Zip Code 85884-000, Medianeira, PR, Brazil Tel.: +55-45-32408119 Fax: +55-45-32408101 E-mail: jeanmetz@utfpr.edu.br\nNewton Spolaôr · Everton A. Cherman · Maria C. Monard Institute of Mathematics and Computer Science (ICMC) University of São Paulo (USP) P.O. Box 668, Zip Code 13561-970, São Carlos, SP, Brazil Tel.: +55-16-33739700 Fax: +55-16-33712238 E-mail: {newtonsp,echerman,mcmonard}@icmc.usp.br ar X\niv :1\n50 3.\n06 95\n2v 1\n[ cs\n.L G\n] 2\n4 M\nar 2\nusing a simple baseline classifier, such that further explanations are provided when a classifiers performs worse than a baseline.\nKeywords machine learning · multi-label classification · multi-label baseline classifier · systematic review"
    }, {
      "heading" : "1 Introduction",
      "text" : "Given a set of examples (instances) characterized by the value of attributes and the class of the example, the aim of supervised learning algorithms is to construct a classifier which is able to assign new examples to the class they belong to. To this end, a great deal of learning algorithms have been proposed. The constructed classifiers are usually compared over a variety of datasets using various evaluation measures proposed in the literature. Most results are averages over a number of runs, where each run involves splitting the dataset into disjoint training and test sets, and the test set is used to estimate several evaluation measures of the classifier generated using the corresponding training set. Afterwards, it is important to statistically verify the hypothesis of improved performance (or not) of the learning algorithm (Demsar 2006). However, we consider that the evaluation measures of the classifier constructed by a learning algorithm should also be compared with the ones obtained by a simple baseline classifier, as it is actually done by most of the single-label learning community. This way, case any of these measures are worse, it would encourage the community to provide additional explanation of this fact.\nIn single-label learning, each example in the dataset is associated with only one class, which can assume several values. The task is called binary classification if there are only two possible class values (Yes/No), and multi-class classification when the number of class values is greater than two (Alpaydin 2010).\nFor single-label learning, a simple baseline classifier is the one constructed by only taking into account the class values, i.e., it does not consider the attributes that describe the examples in the dataset. Having only this information, and due to the fact that the classification of a new instance has only two possible outcomes, correct or incorrect, the best it can do is to output a classifier that always predicts the most frequently occurring class value in the dataset. Before the single-label learning community started to pay attention to this very simple baseline classifier, many evaluation measures worse than or equal to this baseline classifier had been published in the scientific literature, without special explanations.\nIn (Holte 1993) an experimental comparison involving 16 commonly used singlelabel datasets is carried out, where the error rate of the proposed algorithms are compared to the error rate of several learning systems reported in the literature. However, although the datasets used are not highly skewed, some of these reported results fail to improve the error rate of the simple baseline classifier. For example, considering two of these datasets, Breast Cancer and Hepatitis, from the collection distributed by the University of California at Irvine (Bache and Lichman 2013), 33 and 75 error rates are compiled respectively. For the dataset Hepatitis 8 out of 33 (more than 24%) reported error rates are worse than or equal to the simple baseline classifier, while for the dataset Breast Cancer the same happens for 29 out of 75 (more than 38%) reported error rates. As the simple single-label baseline\nclassifier is constructed by only looking at the class values, any learning algorithm which learns from non-skewed domains, and also takes into account the dataset attribute values should be able to construct a classifier with smaller error rates.\nDifferent to single-label learning, in multi-label learning an example can belong to several classes simultaneously. The main difference between multi-label learning and single-label learning is that classes in multi-label learning are often correlated, and the class values in single-label learning are mutually exclusive. Due to the increasing number of applications where examples are annotated with more than one class, multi-label learning has received increasing attention from the machine learning community (Tsoumakas et al. 2010).\nHowever, finding a simple multi-label baseline classifier by only looking at the multi-labels is not as straightforward as in single-label, where the classification of a new instance has only two possible outcomes, correct or incorrect, and the error rate is often considered an important single objective to be achieved. This is not the case in multi-label, as the evaluation measures of a multi-label classifier should also take into account partially correct classifications. To this end, many criteria are proposed to evaluate the classification performance from different perspectives. In (Dembczynski et al. 2012), the connection among these criteria are established, showing that some of these criteria are uncorrelated or even negatively correlated. In other words, some loss functions are essentially conflicting. Thus, several multilabel evaluation measures have been proposed, highlighting different aspects of this important characteristic of multi-label learning.\nMotivated by the lack of simple multi-label baseline classifiers, in (Metz et al. 2012) we propose a simple way to construct multi-label baseline classifiers for specific multi-label evaluation measures. Nevertheless, as a multi-label classifier which focuses on minimizing/maximizing one of these measures does not necessarily minimize/maximize the others, we also proposed a unique simple baseline classifier, called GeneralB , which does not focus on any one of these specific measures and can be used to determine all the multi-label evaluation measure baseline values of a classifier.\nAlthough we do not claim that the proposed GeneralB multi-label baseline classifier should be the one to be used by the community whenever classifiers evaluation measures are published, as other baseline classifiers could be proposed in the future, we believe that it is time to start a discussion related to this subject. Aiming to motivate the community, in this work we consider published experimental results which show that, similar to the single-label research primordium, some of the published results fail to improve on the ones obtained by our simple multi-label baseline classifier.\nHowever, unlike (Holte 1993), in which results reported on a dataset could also refer to the classifier generated by a learning algorithm using a slightly different dataset due to pre-processing, such as filter feature selection or other transformation, in this work we only used the results published in papers reporting experimental results of classifiers which have been constructed using publicy available identical datasets. Unfortunately, this constraint leaves out a great deal of papers, such as many related to text categorization, a typical multi-label problem, as most of the publicly available text datasets are modified by the authors in different ways to obtain the final dataset from which the classifier is generated and, in most cases, this final dataset is not publicly available. On the other hand, this constraint enables anyone to reproduce the experiments described in these papers.\nAs there is a lack of reviews focusing on pieces of work which report experimental results for multi-label learning, and the systematic review process can be useful to identify related publications in a wide, rigorous and replicable way (Kitchenham et al. 2010), we used this process to identify publications which report experimental results for multi-label learning. We have gathered the data used in this work from the selected publications which answer the systematic review research question and do not fulfill any of the exclusion criteria.\nMore specifically, in this work we report on several statistics of various evaluation measure values, which were published and obtained using the 10 datasets most frequently used in the selected papers. These statistics show that 12.8% of these published results are worse than or equal to the ones obtained by our simple multi-label baseline classifier GeneralB . Moreover, this percentage is unevenly distributed among the datasets. In the “worst” dataset, 43.0% of such results were reported, and in the “best” one only 0.6%. However, although a simple baseline classifier was not considered in these publications, it was observed that even for very poor results no special explanations were provided in most of these publications.\nThe remainder of this paper is organized as follows: Section 2 briefly describes multi-label learning and the evaluation measures used in this work. Section 3 explains the simple baseline classifier GeneralB . The systematic review carried out to select the papers from which we have gathered the data used in this work is described in Section 4, and statistics of these published evaluation measure values are reported in Section 5. Section 6 presents the conclusions and future work."
    }, {
      "heading" : "2 Multi-label Classification and Evaluation Measures",
      "text" : "Let D be a training set composed of N examples Ei = (xi, Yi), i = 1..N . Each example Ei is associated with a feature vector xi = (xi1, xi2, . . . , xiM ) described by M features Xj , j = 1..M , and a subset of labels Yi ⊆ L, where L = {y1, y2, . . . yq} is the set of q labels. Table 1 shows this representation. In this scenario, the multilabel classification task consists of generating a classifier H, which given an unseen instance E = (x, ?), is capable of accurately predicting its subset of labels Y , i.e., H(E)→ Y .\nThe predominant approaches of multi-label learning methods are: algorithm adaptation and problem transformation (Tsoumakas et al. 2010). The first one consists of methods which extend specific learning algorithms in order to handle multi-label data directly. The second approach is algorithm independent, allowing\nthe use of any state-of-the-art single-label learning algorithm to carry out multilabel learning. It consists of methods which transform the multi-label classification problem into either several binary classification problems, such as the Binary Relevance (BR) approach, or one multi-class classification problem, such as the Label Powerset (LP ) approach.\nThe BR approach decomposes the multi-label learning task into q independent binary classification problems, one for each label in L. To this end, the multi-label dataset D is first decomposed into q binary datasets Dyj , j = 1..q which are used to construct q independent binary classifiers. In each binary classification problem, examples associated with the corresponding label are regarded as positive and the other examples as negative. Finally, to classify a new multi-label instance BR outputs the aggregation of the labels positively predicted by the q independent binary classifiers. As BR scales linearly with size q of the label set L, it is appropriate for not a very large q. Although in its simple form it experiences the deficiency in which correlation among the labels is not taken into account, successful attempts have been made to model correlation using binary classifiers (Read et al. 2009; Tsoumakas et al. 2009; Cherman et al. 2012). On the other hand, the LP approach transforms the multi-label learning task into a multi-class learning task considering every unique combination of labels in a multi-label dataset as one class value of the corresponding multi-class dataset. Unlike BR, LP takes into account correlation among the labels.\nEvaluating the performance of multi-label classifiers is difficult mostly because multi-label prediction has an additional notion of being partially correct. To this end, several measures have been proposed for the evaluation of bipartitions and rankings with respect to the ground truth of multi-label data. A complete discussion on these performance measures is out of the scope of this paper, and can be found in (Tsoumakas et al. 2010).\nMeasures that evaluate bipartitions are further divided into example-based and label-based. The former are calculated based on the average differences of the classifier predicted multi-label of all examples in the test set, while the latter decompose the evaluation process into separate evaluations of each of the q labels, which are afterwards averaged on all labels. In what follows, we briefly describe the measures used in this work to evaluate bipartitions.\n2.1 Example-based\nLet Yi be the set of true labels (true multi-label) and Zi be the set of predicted labels (predicted multi-label). Hamming-Loss is defined by Equation 1, where ∆ represents the symmetric difference between two sets.\nHamming-Loss(H,D) = 1\nN N∑ i=1 |Yi∆Zi| |L| (1)\nHamming-Loss evaluates the frequency that labels in the multi-label are misclassified, i.e., the example is associated to a wrong label or a label belonging to the true instance which is not predicted.\nSubset-Accuracy is defined by Equation 2, where I(true) = 1 and I(false) = 0.\nSubset-Accuracy(H,D) = 1\nN N∑ i=1 I(Zi = Yi) (2)\nSubset-Accuracy is a very strict evaluation measure as it requires an exact match of the predicted and the true set of labels.\nIn (Godbole and Sarawagi 2004), the following definitions for Accuracy, Precision and Recall, defined by Equations 3, 4 and 5 respectively, are proposed.\nAccuracy(H,D) = 1\nN N∑ i=1 |Yi ∩ Zi| |Yi ∪ Zi|\n(3)\nPrecision(H,D) = 1\nN N∑ i=1 |Yi ∩ Zi| |Zi|\n(4)\nRecall(H,D) = 1\nN N∑ i=1 |Yi ∩ Zi| |Yi|\n(5)\nAccuracy is the proportion of the correctly predicted labels to the total number of labels in the predicted and the truth label set of an instance. Precision is the proportion of correctly predicted labels to the total number of predicted labels, and Recall is the proportion of correctly predicted labels to the total number of true labels.\nF-Measure, frequently used as performance measure for information retrieval systems, is the harmonic mean of Precision and Recall, defined by Equation 6.\nF -Measure(H,D) = 1\nN N∑ i=1 2× |Yi ∩ Zi| |Yi|+ |Zi|\n(6)\nAll these performance measures have values in the interval [0..1]. For HammingLoss, the smaller the value, the better the multi-label classifier performance is, while for the other measures, greater values indicate better performance.\n2.2 Label-based\nIn this case, for each single label yi ∈ L, the q binary classifiers are initially evaluated using any one of the binary evaluation measures proposed in the literature, such as Accuracy, F-Measure, ROC and others, which are afterwards averaged over all labels. Two operations, macro-averaging and micro-averaging, can be used to average over all labels.\nLet B ( TPyi , FPyi , TNyi , FNyi ) be a binary evaluation measure calculated for a label yi based on the number of true positive (TP ), false positive (FP ), true negative (TN ) and false negative (FN ). The macro-average version of B is defined by Equation 7 and the micro-average by Equation 8.\nBmacro = 1\nq q∑ i=1 B ( TPyi , FPyi , TNyi , FNyi ) (7)\nBmicro = B ( q∑ i=1 TPyi , q∑ i=1 FPyi , q∑ i=1 TNyi , q∑ i=1 FNyi ) (8)\nThus, the binary evaluation measure used is computed on individual labels first and then averaged for all labels by the macro-averaging operation, while it is computed globally for all instances and all labels by the micro-averaging operation. This means that macro-averaging would be more affected by labels that participate in fewer multi-labels, i.e., fewer examples, which is appropriate in the study of unbalanced datasets (Dendamrongvit et al. 2011). Furthermore, it should be observed that for some binary evaluation measures, such as Accuracy, macro-average and micro-average yield the same result.\n3 The Simple Multi-label Baseline Classifier GeneralB\nIn supervised learning, simple baseline classifiers can be constructed by only looking at the class, i.e., ignoring any other information from the dataset. The singlelabel learning community frequently uses as a reference the one which always predicts the majority class (the most frequent class value). Although a classifier might perform worse than this simple baseline classifier, as could be the case when learning from highly skewed domains, this behaviour requires a special explanation. In multi-label learning, as the LP transformation maps each distinct multi-label into a single-label, transforming the multi-label dataset into a multi-class (single-label) dataset, one could argue why not use the one which always predicts the majority multi-label as the most simple multi-label baseline classifier?. Although it is a possible baseline, which focuses on maximizing Subset-Accuracy, this strategy does not take into account the individual label distribution in the multi-labels, which provides additional information.\nMoreover, due to the fact that multi-label prediction has the notion of being partially correct, several multi-label evaluation measures have been proposed to evaluate the classification performance from different perspectives.\nIn (Metz et al. 2012), we propose specific simple baseline classifiers which are tailored to maximize/minimize one specific multi-label measure at a time. However, a specific baseline classifier tailored to maximize/minimize one measure does not necessarily maximize/minimize the other measures. Nevertheless, having different baseline classifiers to consider would be a cumbersome task, due to the number of different multi-label evaluation measures proposed in the literature, as well as multi-label learning algorithms which are tailored to maximize/minimize more than one measure (multi-objective). In this work we also proposed GeneralB , a simple baseline classifier which does not focus in maximizing/minimizing any specific measure, and which can be used to find general baselines for any bipartite multi-label evaluation measure.\nThe rationale behind GeneralB to find the predicted multi-label Z is very simple. It consists of ranking the single-labels in L according to their individual relative frequencies in the multi-labels, and then, the σ most frequent single-labels are included in Z. We are then left with the problem of choosing σ such that Z is representative, i.e., with a reasonable number of single-labels and at the same time avoiding being too strict (including too few single-labels) or too flexible (including\ntoo many single-labels). As we are interested in finding Z that represents the singlelabel distribution in the multi-labels well, we defined σ as the closest integer value of the label cardinality. The label cardinality, defined by Equation 9, represents the average size of the multi-labels in a dataset D composed of N examples, i.e., the average number of single-labels associated to each instance.\nCR(D) = 1\nN N∑ i=1 |Yi| (9)\nIn case of ties (single-labels with the same frequency), we consider the label cooccurrence measure, choosing the one which maximizes its co-occurrence with the better ranked labels. It should be observed that, as every other learner, GeneralB has a particular bias. For instance, it will work well for Subset-Accuracy whenever there is a positive correlation among the most frequent labels. However, it will not work well when the correlation is negative. In other words, it will work better whenever its bias fits the dataset well.\nThe specific baseline classifiers, as well as GeneralB , were implemented using the Mulan framework (Tsoumakas et al. 2011), a Java package for multi-label classification based on Weka1, which is commonly used by the multi-label learning community.\nAn analysis of several multi-label bipartition evaluation measure baselines obtained by the specifics, as well as by the GeneralB baseline classifier showed that, as expected, the specific ones perform better on the measure they try to maximize/minimize, although they degrade on the other measures. On the other hand, GeneralB shows a reasonable performance for all the considered bipartite measures. Ranking the results obtained by the specific baseline classifiers and by GeneralB , it was observed that GeneralB is ranked “in the middle”, as shown in (Metz et al. 2012), making it suitable to be used as a general baseline multi-label classifier."
    }, {
      "heading" : "4 Systematic Review",
      "text" : "Although multi-label classification has drawn increasing attention from the machine learning and data mining communities in the past decade, there are few extensive reviews researching publications on this topic. Moreover, to the best of our knowledge, there is no extensive review which explicitly focused on papers reporting experimental results for multi-label learning.\nTo this end, as we need published experimental results on evaluation measures of multi-label classifiers to compare with our proposed multi-label baseline classifier GeneralB , we have carried out the Systematic Review (SR) process, a method to search for relevant papers in a wide, rigorous and replicable way (Kitchenham et al. 2010). The SR process is able to answer Research Questions (RQ) about a subject by using a protocol of planned activities to identify, select and summarize relevant pieces of work.\nThe aim of our systematic review, which is reported in (Spolaôr et al. 2013), is to answer the following RQ: what are the publications which report experimental results for multi-label learning research?. To this end, we used nine world wide\n1 http://www.cs.waitako.ac.nz/ml/weka\nonline bibliographic database search engines as listed in Table 2, in which 1,543 publications were identified.\nSome retrieved pieces of work can be duplicated, as some sources, i.e., journals, proceedings and others, are indexed by more than one bibliographic database. Thus, cases with duplicated titles were automatically or manually (mistyped titles) removed, keeping only one copy of the publication. From the 1,543 publications, 847 (55%) were automatically removed and 79 (5%) were manually removed. Thus, we were left with 617 (40%) publications which were divided among the authors of this paper, such that each one of the 617 publications was manually analyzed using 16 exclusion criteria as a guide. Whenever a publication fulfilled one or more exclusion criteria, it was removed. If there were doubts about removing a publication, a second reviewer verified the doubtful publication.\nThe 16 exclusion criteria include: publications which do not consider examplebased or label-based evaluation measures; restricted access to the dataset; preprocessed datasets where the final attribute-value table used by the learning algorithm is not publicly available, and others. Recall that we only collected evaluation measures of classifiers that were obtained by multi-label learning algorithms using identical attribute-value datasets. At this stage, we were left with 64 (4%) publications which do not fulfill any of the 16 exclusion criteria. Figure 1 shows a summary of the selection procedures.\nNevertheless, similar to known systematic reviews (Kitchenham et al. 2010), results are bound to the electronic databases searched for publications, nine in our case. Thus, papers potentially relevant to the research question might not have been identified.\nFrom these 64 papers, we recorded information extracted manually on an electronic spreadsheet with 42 columns, described in detail in (Spolaôr et al. 2013). As most of the information extraction has to be carried out manually, this process was double checked. A relational database was set up to appropriately record the 42 columns, modelling each sheet as a database table. In this database, each sheet column is a table attribute and each sheet line is an instance. The corresponding entity-relationship model consists of four tables: main, dataset, measure and paper, as well as some relationships between them. The main table records the experimental settings and results published in the papers which are able to answer the research question, as well as some foreign keys which link results to a paper and a dataset. Furthermore, the dataset table records usual statistics from multi-label datasets, such as: the domain; number of examples; features and labels; as well as\nthe number of different multi-labels, label cardinality and label density; the URL where the dataset is publicly available is also kept in this table. The measure table manages the name and type of the recorded multi-label evaluation measures. The paper table records the selected publications.\nFigure 2 shows the distribution per year of the 64 papers considered in this work, where 21 (33%) of them were published in journals and the remaining in congresses and workshops. Moreover, besides 7 (10%) papers published in the Machine Learning Journal, at most 2 papers were published in the same source.\nFigure 3 shows the number of papers in which each dataset was used. As already mentioned, we do not consider experimental results from pre-processed datasets, such as the very frequently used Reuters, unless the final attribute-value table from which the classifier is generated is reported. Thus, few datasets whose original domain is text were considered in this study due to this restriction. As can be observed in this figure, the Yeast dataset is used in almost 80% of the 64 papers considered in this work.\n5 Comparing GeneralB to Published Evaluation Measure Values\nIn this section, some statistics of published experimental evaluation measure values of multi-label classifiers and the ones obtained by GeneralB are discussed.\n5.1 Datasets\nFrom the 25 datasets used in the 64 papers selected by the systematic review process shown in Figure 3, the 10 most frequently used are the ones considered.\nTable 3 presents the selected datasets and associated statistics. It shows: the application domain (Domain); number of instances (#E); number of features (#F); the total number of labels (|L|); and the percentage of the 64 publications which use the dataset (Usage). Moreover, Table 4 shows some statistics associated with the datasets labels: label cardinality (CR(D)), defined by Equation 9; label density (DS(D)), defined by Equation 10; number of distinct multi-labels (#Dist); the lowest (Min) and the highest (Max) single-label frequencies, as well as the first (1Q), second (median Med) and the third quartiles (3Q), as suggested by Tsoumakas (2013).\nDS(D) = 1\nN N∑ i=1 |Yi| |L| (10)\nObserve that these 10 datasets from five different domains have different characteristics. The number of instances vary from 593 up to 43,907; the number of features from 72 up to 1,836 and the number of single-labels (|L|) from 6 up to 374. Furthermore, the label cardinality varies from 1.074 up to 4.376; the label density from 0.009 up to 0.311 and the number of distinct multi-labels from 15 up to 6,555. It is worth noting that some datasets present labels with very low or\nzero frequency (Min). Although it could be a good practice to remove these sort of labels, the original versions of the datasets were kept in this work. More detail about these datasets can be found in the site where they are publicly available.\n5.2 Evaluation measures\nAs explained in Section 4, all example-based and label-based measure values reported in the 64 papers were manually collected. Similar to the dataset selection, we chose the 8 most frequently used out of the 17 different recorded evaluation measures. Moreover, the 9 measures not considered here are used in very few (at most 5) of the 64 papers. Figure 4 shows the number of papers in which the 8 evaluation measures considered were used.\nAs can be observed, at least in the papers considered in this work, examplebased measures are much more frequently used than the label-based ones. Furthermore, among the example-based measures, Hamming-Loss is the most frequently used (55 papers), while Subset-Accuracy is used in fewer papers. As already mentioned, Hamming-Loss evaluates partially correct classification, while Subset-Accuracy evaluates exact matching between the ground truth and the predicted multilabel.\nThe results reported on these measures come from different experimental setup and validation processes, such as cross-validation and hold-out. Considering all the 64 papers, it was observed that 49.6% were obtained using hold-out validation,\n43.0% using some type of k-fold cross-validation, such as 10-fold, 3-fold or 5 × 2- fold, and the rest using other validation process or the validation process is not explicitly mentioned in the respective publication. However, it is interesting to note that from all the measure values which we have manually extracted and recorded, a total of 6,989, the standard deviation is reported for only 1,435 (20.5%) of them.\nNext, statistics related to the published measure values considered in this work, which were obtained from the classifiers generated by a variety of multi-label learning algorithms, and the ones obtained by GeneralB , are presented.\n5.3 Results and discussion\nTable 5 shows the GeneralB baseline values for each evaluation measure and dataset considered in this work. The eight measures are denoted as: Example-based Accuracy (Acc); Example-based F-Measure (F1 ); Example-based Hamming-Loss (HL); Example-based Precision (Pr); Example-based Recall (Re); Example-based Subset-Accuracy (SAcc); Label-based Macro-averaged F-Measure (F1M ); and Label-based Microaveraged F-Measure (F1µ).\nThese values can be directly used in other publications, as they are the same for a given dataset and evaluation measure.\nTable 6 shows, for each dataset, the number of times that a published measure value underperforms or it is equal to the corresponding GeneralB baseline value. Summary information is shown in light gray cells. Column #Ud shows the total number of measures fulfilling this condition on a total of #Md measure values recorded for each dataset, and column % shows the percentage. Similar results are shown in rows #Um, #Mm and % for each measure considered. This information is shown graphically in Figures 5 and 6.\nAs can be observed, from a total of 5,342 measure values on the 10 datasets considered in this work, 12.8% are worse than or equal to the ones provided by GeneralB . Moreover, these worse results are concentrated in some datasets, such as Corel5k , Mediamill and Enron, as shown in Figure 6. On the other hand, only 4\nout of 716 (0.6%) of the measure values published for the Emotions dataset fulfill this condition. Figure 7 shows information of these four datasets.\nNevertheless, this kind of information does not show the degree of disagreement between the evaluation measure values published and the ones provided by GeneralB . To this end, we have extracted statistics from these values, as shown\n7.1%\n7.8%\n8.9%\n11.7%\n13.7%\n14.8%\n15.9%\n17.1%\n0% 5% 10% 15% 20%\nSubset-Accurary\nMacro F-measure\nMicro F-measure\nHamming-Loss\nAccuracy\nPrecision\nF-measure\nRecall\nPercentage of experiments\n43.0%\n37.9%\n24.9%\n18.8%\n14.2%\n11.0%\n6.1% 4.8%\n1.6% 0.6%\n0%\n10%\n20%\n30%\n40%\n50%\nDatasets\nFig. 6 Percentage of values per measure and dataset which underperform or are equal to the corresponding GeneralB baseline value\nin Figure 8 for the datasets Corel5k , Mediamill , Enron and Emotions considering the distribution of Accuracy, F-Measure and Hamming-Loss measure values. It also shows, in brackets, the worst and the best value found in the publications. Recall that for Hamming-Loss, the smaller the value, the better the multi-label classifier performance is, while for the others, greater values indicate better performance.\nIn fact, this sort of statistics extraction and organization was carried out for all datasets and measures considered, and can be found at http://www.labic.icmc. usp.br/pub/mcmonard/ExperimentalResults/Metz-GeneralB-SupplementaryMaterial.\nFigure 8 shows that, in some cases, there is a considerable gap between the worst and the best published measure values. Although this gap could be justified because different multi-label algorithms minimize different loss-functions, which in turn favors specific evaluation measures, it should be expected that special explanations are provided case these measures are worse than the ones from the simple baseline classifier GeneralB .\nFurthermore, considering in Figure 8 the measures which are better than or equal to the ones from GeneralB , it can be observed that there is little improvement in those measures for Corel5k , Mediamill and Enron datasets. On the other hand, the improvement is considerable for Emotions.\nTable 7 shows, for the 10 datasets, the highest (↑) and the lowest (↓) measure values published in the 64 papers for the 8 evaluation measures considered in this work, as well as the ones from GeneralB (GB). Light gray cells indicate that the difference between the highest and the lowest measure values is greater than or equal to 0.5. In most cases, it can be observed that there is a very high discrepancy between the highest and the lowest published measure values.\nRegarding the multi-label algorithms used in the 64 papers, most of them follow the problem transformation approach, using state-of-the-art single-label learning algorithms as a base learner. Binary Relevance is the most frequently used approach.\nAt this point, it is worth observing that we are quite confident about the correctness (with respect to the published results) of the collected measure values from the 64 papers. As stated earlier in Section 4, these values were initially double checked. After making the graphs for all datasets and measures considered in this work, we checked, once more, the worst and the best published values.\nFrom this third inspection of the gathered data, it was observed that few papers explain and justify very poor results. However, similar to single-label learning, case the multi-label community decides to adopt a simple baseline classifier such as GeneralB , or any other, we think that it will encourage the authors to provide special explanations on very poor results."
    }, {
      "heading" : "6 Conclusions and Future Work",
      "text" : "The single-label community expects that in non skewed domains a simple baseline classifier, which always predicts the majority class, should do worse than classifiers constructed by a learning algorithms. However, to the best of our knowledge, the multi-label community still does not have a consolidated idea of a simple multilabel baseline classifier.\nAiming to raise awareness of considering a simple multi-label baseline classifier, we have carried out a systematic review of the multi-label learning literature in\norder to collect experimental results to contrast with the proposed simple multilabel baseline classifier GeneralB .\nIt was found that an important number of published results (12.8%) are worse than or equal to the ones obtained by GeneralB . In fact, for all the 10 most frequently used datasets presented in the work, results worse than or equal to the ones obtained by GeneralB were found. In the extreme case, 43% of the published results for one dataset are worse than or equal to the GeneralB results.\nAlthough we do not claim that the proposed GeneralB multi-label baseline classifier should be the one to be used by the community, we hope that this work would encourage the multi-label community to consider the idea of using a simple baseline classifier as an initial reference related to the learning power of multi-label algorithms. With the use of a baseline, built by only taking into account the label distribution information, it would be possible to identify cases where the obtained results are not reasonable enough, and give support for better explanations about these results.\nAs future work, we plan to increase the number of electronic databases to search for publications which answer our research question and do not fulfil any of the exclusion criteria. As the organization of the information extracted allows to answer several useful questions, such as Which publications use algorithm A on dataset B using 10-fold cross-validation and what are the results obtained? Are there publications reporting results on datasets with cardinality greater than C and a distinct number of multi-labels greater than W?, we plan to increment and further structure the gathered information making it available to the community on a Web page."
    } ],
    "references" : [ {
      "title" : "Introduction to Machine Learning, 3rd Edition. MITP",
      "author" : [ "E. Alpaydin" ],
      "venue" : null,
      "citeRegEx" : "Alpaydin,? \\Q2010\\E",
      "shortCiteRegEx" : "Alpaydin",
      "year" : 2010
    }, {
      "title" : "Incorporating label dependency into the binary relevance framework for multi-label classification",
      "author" : [ "E.A. Cherman", "J. Metz", "M.C. Monard" ],
      "venue" : "Expert Syst. Appl",
      "citeRegEx" : "Cherman et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Cherman et al\\.",
      "year" : 2012
    }, {
      "title" : "On label dependence and loss minimization in multi-label classification",
      "author" : [ "K. Dembczynski", "W. Waegeman", "W. Cheng", "E. Hüllermeier" ],
      "venue" : "Machine Learning",
      "citeRegEx" : "Dembczynski et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Dembczynski et al\\.",
      "year" : 2012
    }, {
      "title" : "Statistical comparisons of classifiers over multiple data sets",
      "author" : [ "J. Demsar" ],
      "venue" : "Journal of Machine Learning Research",
      "citeRegEx" : "Demsar,? \\Q2006\\E",
      "shortCiteRegEx" : "Demsar",
      "year" : 2006
    }, {
      "title" : "Irrelevant attributes and imbalanced classes in multi-label text-categorization domains",
      "author" : [ "S. Dendamrongvit", "P. Vateekul", "M. Kubat" ],
      "venue" : "Intell. Data Anal",
      "citeRegEx" : "Dendamrongvit et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Dendamrongvit et al\\.",
      "year" : 2011
    }, {
      "title" : "Discriminative methods for multi-labeled classification",
      "author" : [ "S. Godbole", "S. Sarawagi" ],
      "venue" : "PAKDD. Vol. 3056 of Lecture Notes in Computer Science",
      "citeRegEx" : "Godbole and Sarawagi,? \\Q2004\\E",
      "shortCiteRegEx" : "Godbole and Sarawagi",
      "year" : 2004
    }, {
      "title" : "Very simple classification rules perform well on most commonly used datasets",
      "author" : [ "R.C. Holte" ],
      "venue" : "Machine Learning",
      "citeRegEx" : "Holte,? \\Q1993\\E",
      "shortCiteRegEx" : "Holte",
      "year" : 1993
    }, {
      "title" : "Systematic literature reviews in software engineering - a tertiary study",
      "author" : [ "B. Kitchenham", "R. Pretorius", "D. Budgen", "P. Brereton", "M. Turner", "M. Niazi", "S.G. Linkman" ],
      "venue" : "Information & Software Technology",
      "citeRegEx" : "Kitchenham et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Kitchenham et al\\.",
      "year" : 2010
    }, {
      "title" : "On the estimation of predictive evaluation measure baselines for multi-label learning",
      "author" : [ "J. Metz", "L.F.D. de Abreu", "E.A. Cherman", "M.C. Monard" ],
      "venue" : "IBERAMIA. Vol. 7637 of Lecture Notes in Computer Science",
      "citeRegEx" : "Metz et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Metz et al\\.",
      "year" : 2012
    }, {
      "title" : "Classifier chains for multi-label classification",
      "author" : [ "J. Read", "B. Pfahringer", "G. Holmes", "E. Frank" ],
      "venue" : "ECML/PKDD (2)",
      "citeRegEx" : "Read et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Read et al\\.",
      "year" : 2009
    }, {
      "title" : "A systematic review on experimental multi-label learning",
      "author" : [ "N. Spolaôr", "E.A. Cherman", "J. Metz", "M.C. Monard" ],
      "venue" : "Technical Report ICMC-USP No. 392,",
      "citeRegEx" : "Spolaôr et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Spolaôr et al\\.",
      "year" : 2013
    }, {
      "title" : "Correlation-based pruning of stacked binary relevance models for multi-label learning",
      "author" : [ "G. Tsoumakas", "A. Dimou", "E. Spyromitros", "V. Mezaris", "I. Kompatsiaris", "I. Vlahavas" ],
      "venue" : "Proc. ECML/PKDD 2009 Workshop on Learning from Multi-Label Data",
      "citeRegEx" : "Tsoumakas et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Tsoumakas et al\\.",
      "year" : 2009
    }, {
      "title" : "Mining multi-label data. In: Data Mining and Knowledge Discovery Handbook",
      "author" : [ "G. Tsoumakas", "I. Katakis", "I. Vlahavas" ],
      "venue" : null,
      "citeRegEx" : "Tsoumakas et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Tsoumakas et al\\.",
      "year" : 2010
    }, {
      "title" : "Mulan: A Java library for multi-label learning",
      "author" : [ "G. Tsoumakas", "E. Spyromitros-Xioufis", "J. Vilcek", "I. Vlahavas" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Tsoumakas et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Tsoumakas et al\\.",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Afterwards, it is important to statistically verify the hypothesis of improved performance (or not) of the learning algorithm (Demsar 2006).",
      "startOffset" : 126,
      "endOffset" : 139
    }, {
      "referenceID" : 0,
      "context" : "The task is called binary classification if there are only two possible class values (Yes/No), and multi-class classification when the number of class values is greater than two (Alpaydin 2010).",
      "startOffset" : 178,
      "endOffset" : 193
    }, {
      "referenceID" : 6,
      "context" : "In (Holte 1993) an experimental comparison involving 16 commonly used singlelabel datasets is carried out, where the error rate of the proposed algorithms are compared to the error rate of several learning systems reported in the literature.",
      "startOffset" : 3,
      "endOffset" : 15
    }, {
      "referenceID" : 12,
      "context" : "Due to the increasing number of applications where examples are annotated with more than one class, multi-label learning has received increasing attention from the machine learning community (Tsoumakas et al. 2010).",
      "startOffset" : 191,
      "endOffset" : 214
    }, {
      "referenceID" : 2,
      "context" : "In (Dembczynski et al. 2012), the connection among these criteria are established, showing that some of these criteria are uncorrelated or even negatively correlated.",
      "startOffset" : 3,
      "endOffset" : 28
    }, {
      "referenceID" : 8,
      "context" : "Motivated by the lack of simple multi-label baseline classifiers, in (Metz et al. 2012) we propose a simple way to construct multi-label baseline classifiers for specific multi-label evaluation measures.",
      "startOffset" : 69,
      "endOffset" : 87
    }, {
      "referenceID" : 6,
      "context" : "However, unlike (Holte 1993), in which results reported on a dataset could also refer to the classifier generated by a learning algorithm using a slightly different dataset due to pre-processing, such as filter feature selection or other transformation, in this work we only used the results published in papers reporting experimental results of classifiers which have been constructed using publicy available identical datasets.",
      "startOffset" : 16,
      "endOffset" : 28
    }, {
      "referenceID" : 7,
      "context" : "As there is a lack of reviews focusing on pieces of work which report experimental results for multi-label learning, and the systematic review process can be useful to identify related publications in a wide, rigorous and replicable way (Kitchenham et al. 2010), we used this process to identify publications which report experimental results for multi-label learning.",
      "startOffset" : 237,
      "endOffset" : 261
    }, {
      "referenceID" : 12,
      "context" : "The predominant approaches of multi-label learning methods are: algorithm adaptation and problem transformation (Tsoumakas et al. 2010).",
      "startOffset" : 112,
      "endOffset" : 135
    }, {
      "referenceID" : 9,
      "context" : "Although in its simple form it experiences the deficiency in which correlation among the labels is not taken into account, successful attempts have been made to model correlation using binary classifiers (Read et al. 2009; Tsoumakas et al. 2009; Cherman et al. 2012).",
      "startOffset" : 204,
      "endOffset" : 266
    }, {
      "referenceID" : 11,
      "context" : "Although in its simple form it experiences the deficiency in which correlation among the labels is not taken into account, successful attempts have been made to model correlation using binary classifiers (Read et al. 2009; Tsoumakas et al. 2009; Cherman et al. 2012).",
      "startOffset" : 204,
      "endOffset" : 266
    }, {
      "referenceID" : 1,
      "context" : "Although in its simple form it experiences the deficiency in which correlation among the labels is not taken into account, successful attempts have been made to model correlation using binary classifiers (Read et al. 2009; Tsoumakas et al. 2009; Cherman et al. 2012).",
      "startOffset" : 204,
      "endOffset" : 266
    }, {
      "referenceID" : 12,
      "context" : "A complete discussion on these performance measures is out of the scope of this paper, and can be found in (Tsoumakas et al. 2010).",
      "startOffset" : 107,
      "endOffset" : 130
    }, {
      "referenceID" : 5,
      "context" : "In (Godbole and Sarawagi 2004), the following definitions for Accuracy, Precision and Recall, defined by Equations 3, 4 and 5 respectively, are proposed.",
      "startOffset" : 3,
      "endOffset" : 30
    }, {
      "referenceID" : 4,
      "context" : ", fewer examples, which is appropriate in the study of unbalanced datasets (Dendamrongvit et al. 2011).",
      "startOffset" : 75,
      "endOffset" : 102
    }, {
      "referenceID" : 8,
      "context" : "In (Metz et al. 2012), we propose specific simple baseline classifiers which are tailored to maximize/minimize one specific multi-label measure at a time.",
      "startOffset" : 3,
      "endOffset" : 21
    }, {
      "referenceID" : 13,
      "context" : "The specific baseline classifiers, as well as GeneralB , were implemented using the Mulan framework (Tsoumakas et al. 2011), a Java package for multi-label classification based on Weka, which is commonly used by the multi-label learning community.",
      "startOffset" : 100,
      "endOffset" : 123
    }, {
      "referenceID" : 8,
      "context" : "Ranking the results obtained by the specific baseline classifiers and by GeneralB , it was observed that GeneralB is ranked “in the middle”, as shown in (Metz et al. 2012), making it suitable to be used as a general baseline multi-label classifier.",
      "startOffset" : 153,
      "endOffset" : 171
    }, {
      "referenceID" : 7,
      "context" : "To this end, as we need published experimental results on evaluation measures of multi-label classifiers to compare with our proposed multi-label baseline classifier GeneralB , we have carried out the Systematic Review (SR) process, a method to search for relevant papers in a wide, rigorous and replicable way (Kitchenham et al. 2010).",
      "startOffset" : 311,
      "endOffset" : 335
    }, {
      "referenceID" : 10,
      "context" : "The aim of our systematic review, which is reported in (Spolaôr et al. 2013), is to answer the following RQ: what are the publications which report experimental results for multi-label learning research?.",
      "startOffset" : 55,
      "endOffset" : 76
    }, {
      "referenceID" : 7,
      "context" : "Nevertheless, similar to known systematic reviews (Kitchenham et al. 2010), results are bound to the electronic databases searched for publications, nine in our case.",
      "startOffset" : 50,
      "endOffset" : 74
    }, {
      "referenceID" : 10,
      "context" : "From these 64 papers, we recorded information extracted manually on an electronic spreadsheet with 42 columns, described in detail in (Spolaôr et al. 2013).",
      "startOffset" : 134,
      "endOffset" : 155
    } ],
    "year" : 2015,
    "abstractText" : "In supervised learning, simple baseline classifiers can be constructed by only looking at the class, i.e., ignoring any other information from the dataset. The single-label learning community frequently uses as a reference the one which always predicts the majority class. Although a classifier might perform worse than this simple baseline classifier, this behaviour requires a special explanation. Aiming to motivate the community to compare experimental results with the ones provided by a multi-label baseline classifier, calling the attention about the need of special explanations related to classifiers which perform worse than the baseline, in this work we propose the use of GeneralB , a multi-label baseline classifier. GeneralB was evaluated in contrast to results published in the literature which were carefully selected using a systematic review process. It was found that a considerable number of published results on 10 frequently used datasets are worse than or equal to the ones obtained by GeneralB , and for one dataset it reaches up to 43% of the dataset published results. Moreover, although a simple baseline classifier was not considered in these publications, it was observed that even for very poor results no special explanations were provided in most of them. We hope that the findings of this work would encourage the multi-label community to consider the idea of ? Acknowledgements: This research was supported by the São Paulo Research Foundation (FAPESP), grants 2008/06739-0, 2010/15992-0 and 2011/02393-4. Jean Metz Federal Technological University of Paraná (UTFPR) P.O. Box 271, Zip Code 85884-000, Medianeira, PR, Brazil Tel.: +55-45-32408119 Fax: +55-45-32408101 E-mail: jeanmetz@utfpr.edu.br Newton Spolaôr · Everton A. Cherman · Maria C. Monard Institute of Mathematics and Computer Science (ICMC) University of São Paulo (USP) P.O. Box 668, Zip Code 13561-970, São Carlos, SP, Brazil Tel.: +55-16-33739700 Fax: +55-16-33712238 E-mail: {newtonsp,echerman,mcmonard}@icmc.usp.br ar X iv :1 50 3. 06 95 2v 1 [ cs .L G ] 2 4 M ar 2 01 5",
    "creator" : "LaTeX with hyperref package"
  }
}