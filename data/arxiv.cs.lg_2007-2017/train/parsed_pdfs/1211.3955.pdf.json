{
  "name" : "1211.3955.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On Calibrated Predictions for Auction Selection Mechanisms",
    "authors" : [ "H. Brendan McMahan", "Omkar Muralidharan" ],
    "emails" : [ "mcmahan@google.com", "omuralidharan@google.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n21 1.\n39 55\nv1 [\ncs .G\nT ]\n1 6\nWe show that certain natural notions of calibration can be impossible to achieve, depending on the details of the auction. We also show that it can be impossible to maximize auction efficiency while using calibrated predictions. Finally, we give conditions under which calibration is achievable and simultaneously maximizes auction efficiency: roughly speaking, bids and queries must not contain information about CTRs that is not already captured by the predictions."
    }, {
      "heading" : "1 Introduction",
      "text" : "Calibration is a fundamental measure of accuracy in prediction problems: if we group all the events a predictor says happen with probability p, about a p fraction should occur. This property has been extensively studied in the stochastic and online settings.\nWe study problems where the predictions themselves partially determine which events occur. Our general approach applies to many problems where predictions are used to make decisions, but we are motivated in particular by the application to search engine advertising. Over the past decade, this business has grown to tens of billions of dollars, and prediction systems play a fundamental role.\nIn a typical interaction, first a user does a query (say “flowers”) on a search engine. Then, the search engine selects a set of candidate ads that can be shown on the given query, based on keywords provided by advertisers. These\ncomponents can be reasonably approximated by an IID process. A prediction is made for each candidate ad, and an auction ranks the ads based on the prediction and the bid of the advertiser. Typically, the bid indicates the value of a click to the advertiser, and the score is simply the product of the bid and the prediction, giving an estimate of the value generated by showing the ad. Finally, some of these ads are shown to the user (we consider two models: the single top-ranked ad is shown, or all the ads with scores above a certain threshold are shown). This auction selection mechanism has been extensively studied, and has many nice properties [19, 8].\nIn this setting, an important measure of the quality of the predictions is how much value the auction generates (equivalently, how efficient are the allocations produced by the auction). The auction mechanisms we consider are in fact designed to maximize the combined value to the search engine and advertiser if bids accurately reflect value and the true click-through-rates (CTRs) are known.\nThe algorithm used to predict CTRs for such a system faces many constraints already, for example, the need to process enormous volumes of data quickly and produce predictions with extremely low latency (e.g., [13]). Thus, rather than advocating new algorithms, we focus on applying a post-correction via a prediction map to the outputs of an existing system in order to improve the quality of the predictions.\nWe consider two main questions. Informally stated: 1) Do efficiency-maximizing prediction maps with calibration properties exist, and can they can be found computationally efficiently? 2) If we iteratively calibrate our predictions so they match observed CTRs, does the process converge? And if so, is this prediction map efficiency maximizing?\nOutline and Summary of Results We formalize our model and questions in Section 2, where we introduce two primary variants of the selection mechanism that lead to different properties; Section 3 and 4 investigate these mechanisms in the general case. We demonstrate that without further assumptions, in both our models it may be impossible for a deterministic prediction map to produce calibrated predictions on the ads it serves, and iterative calibration procedures can fail badly. Since some deterministic map always maximizes value, this is unfortunate. When all ads above a certain threshold are shown, we give an algorithm for finding this value-maximizing map in polynomial time, but when the single highest-rated ad is shown, we prove finding the value-maximizing map is NP-hard (even if we knew the true CTRs).\nIn Section 5 we introduce additional assumptions that are sufficient to guarantee calibration procedures are well-behaved. While these assumptions are fairly strong, they are not unreasonable for real systems. Our strongest assumption is essentially that in all cases bid and query provide no more information than the raw prediction about average CTRs; under this assumption, we can show in both selection models a value-maximizing and calibrated prediction map exists. Under threshold selection, somewhat weaker conditions are in fact sufficient.\nRelated Work Calibration has been extensively studied. Much of the earliest work is in the probabilistic forecasting literature [1, 6, 18]. Calibration is particularly important when comparing predictors, since two sets of calibrated predictions can be fairly evaluated by how concentrated they are on observed outcomes [7, 12, 11]. Calibration also makes it easier to use predictions. For example, it is easier to threshold the output of a calibrated classifier to minimize weighted classification error [5].\nNot all prediction systems are naturally calibrated. However, when examples are drawn IID, if we have a good but uncalibrated predictor, we can calibrate it by applying a prediction map. For example, boosted trees are uncalibrated, but become excellent probability estimators after calibration [16, 2]. The two most common methods for calibration are Platt scaling, which is equivalent to logistic regression, and isotonic regression [17, 20, 15, 3].\nCalibration is also studied in the online setting, where no stochastic assumptions are made on the sequence of examples; in the worst case, they could be chosen by an adversary that sees our predictions. It is easy to see that in this setting, no deterministic classifier (or prediction map) can produce calibrated predictions for all sequences. However, if the system is allowed to use randomness (that is, predict a distribution), then calibration can be achieved ([9, 10] and [4, Sec 4.5])."
    }, {
      "heading" : "2 Problem Formalization",
      "text" : "The interaction of calibration and selection has received little direct attention in the literature, so constructing a suitable model requires some care: we require a formulation that is theoretically tractable but still captures the key characteristics of the real-world problems of interest.\nWe begin by defining our units of prediction (queries and ads) and the mechanism used to select them (auctions). We assume a fixed, existing prediction system provides a raw prediction for each ad; our study will then concern prediction maps, functions that attempt to map these raw predictions to calibrated probabilities. Once this framework is established, we can formally state the questions we study.\nWe model the interaction between a search engine’s users and advertising system. There is a fixed finite set of queries Q (strings like “flowers” or “car insurance” typed into the search engine), which are chosen according to distribution PrQ(q) for q ∈ Q. There is also a fixed finite set of ads C which can be shown alongside queries. Each ad i ∈ C is defined by tuple (pi, bi, zi, qi) where qi ∈ Q is the (only) query for which ad i can show,\n1 pi is the true probability of a click, bi is the bid (the maximum amount the advertiser is willing to pay for a click), and zi ∈ {1, . . . ,K} is a bucketed estimate of pi (we call zi the raw prediction). That is, we assume the predictions of the underlying prediction system have been discretized into K buckets. We drop the q (and sometimes z)\n1This is without loss of generality, as we can always replicate ads for each query to which the advertiser has targeted the ad.\nfrom the ad tuples when those values are clear from context. Each ad can show for a single query q, so we define C(q) ≡ {i |qi = q}, the indexes of the candidate ads for query q.\nOur goal is to find good prediction maps f : {1, . . . ,K} → [0, 1]. The prediction map will be used in the auction selection mechanism: First, a query is sampled from PrQ, and then the candidate ads for that query are ranked by b · f(z) (we drop the subscripts when we mean an arbitrary ad). We consider two models for which ads show:\nONE: We only show a single ad. If multiple ads achieve the highest value of b · f(z), we pick one uniformly at random.\nALL: We show all ads where b · f(z)− 1 > 0.\nMechanism ONE models the case of an oversold auction, where ads with different raw predictions z must compete for a single position. Mechanism ALL models the case where all eligible ads with positive predicted value can be shown. In general, mechanism ALL is much easier to work with theoretically, because for z1 6= z2, changing f(z1) does not change which ads with prediction z2 are shown. In either case, we assume any candidate (p, b, z) which is shown is clicked with probability p.2\nDistributions on Ads Other than the distribution PrQ, all probabilities and expectations will be with respect to some distribution on the set of candidate ads C. Two distributions will be of particular importance: PrC , the uniform distribution over candidate ads, and Prf , the distribution of ads shown by a prediction map f . We formalize these as follows:\nPrC is the distribution on ads where PrC(i) is proportional to Pr Q(qi). That\nis, letting C ≡ ∑\ni∈C Pr Q(qi), we have PrC(i) = PrQ(qi) C . This is not the same as\nchoosing a random query q from PrQ and then choosing a random candidate. For example, suppose there are two queries q1 and q2, with Pr Q(q1) = 1 2 and PrQ(q2) = 1 2 . There is one candidate a1 for query q1, and two candidates, a2 and a3 for query q2. Then, PrC(ai) = 1/3 for each ad, which means the marginal probability PrC(q1) = 1 3 and PrC(q2) = 2 3 . One can think of PrC as the distribution on ads shown if we showed all the eligible candidates for each query that occurs.\nPrf for a prediction map f is the distribution on ads where Prf (i) is proportional to wi ≡ Pr Q(qi)Pr(ad i shows | qi, f). The second term is actually only random in the case of selection mechanism ONE, when randomness is used to break ties. The distribution Prf is thus the distribution on ads shown when serving using prediction map f . Using this notation, Prf (i |q) = Pr(ad i shows | qi, f).\nWe use EC [·] and Ef [·] for the corresponding expectations.\n2This ignores the well-known issue of position normalization; this aspect of the problem is largely orthogonal to our work.\nCalibration We say a prediction map f is calibrated on a distribution on ads D if\n∀z, E(p,b,z,q)∼D[p |z] ︸ ︷︷ ︸\nAverage CTR given z\n= f(z). ︸ ︷︷ ︸\nPredicted CTR given z\nThe choice of the distribution D in the above definition is critical; a single f will in general not be able to achieve calibration for multiple D. For the auction selection problem, the natural distribution to consider is Prf . Thus, we will be particularly concerned with finding self-calibrated prediction maps f , which satisfy\n∀z, Ef [p |z] = f(z).\nIn general one may not be able to estimate Ef [ p | z ] exactly, and so calibration will only be approximately achievable. This issue is orthogonal to our results, so we assume that the necessary expected quantities can be estimated exactly. Thus, we emphasize that our negative results are a fundamental limitation, rather than a byproduct of insufficient data.\nAuction Efficiency In addition to calibration, we are concerned with how the choice of f impacts the auction mechanism. The expected value of showing ad (p, b) is p · b− cost, where we take cost = 1 for selection mechanism ALL, and cost = 0 for ONE. We assume the bid b reflects the true value to the advertiser of a click, which is justified by the incentives of the auction under a suitable pricing scheme [19]. The cost can be viewed as the cost per impression of showing the ad (either a cost incurred by the user doing the query or incurred by the search engine itself). In practice such costs might be different for clicked versus unclicked ad impressions, and might vary depending on the ad and query. Extending our results to such a models would add a significant notational burden, so we focus on the simplest interesting cost models.\nFor a given query q, the expected value generated is\n∑\ni∈C(q)\nPr(ad i shows |f, q)(pibi − cost).\nThe expected value per query is just\nEV(f) = ∑\nq∈Q\nPrQ(q) ∑\ni∈C(q)\nPrf (i |q)(pibi − cost)\n= ∑\ni∈C\nwi(pibi − cost).\nWe say an f∗ ∈ argmaxf EV(f) is efficiency maximizing. Our goal is to find an f that transforms the z into the best possible predictions in terms of efficiency. Note that if it was possible to predict exactly pi for ad i, these predictions would maximize efficiency.\nQuestions Ideally, we would like to use prediction maps that are self-calibrated and efficiency-maximizing; we say such prediction maps are nice, and say a problem instance is nice if such a map exists.\nFirst, we consider questions relating to the offline problem where we have access to all the problem data. Note that there must exist an efficiency-maximizing prediction map.3\nQ1 Are all problem instances nice? That is, do self-calibrated efficiencymaximizing prediction maps always exist?\nQ2 Can an efficiency-maximizing prediction map, even one that is not selfcalibrated, be found in polynomial time?\nIn practice, we are further concerned with learning a good prediction map from observed data. Suppose we start with some f0, for example the function that gives the predictions of the underlying system. Then, we serve some large number of queries with this f0, and observe the results. We would like to then train an improved f1 from this data, serve another large batch of queries ranked using f1, then train an f2, etc.\nA natural procedure is to choose ft so that the predictions on the ads shown in batch t − 1 would have been calibrated under ft. Of course, when we then select ads using ft on the next batch, we may show different ads. Formally, define T : [0, 1]K → [0, 1]K (a function from prediction maps to prediction maps) by T (f) = f ′ where\nf ′(z) =\n{\nEf [p |z] when Prf (z) > 0\nf(z) otherwise.\nWe assume we have enough data in each batch so that we can calculate Eft−1 [p | z] exactly. Then, we ask:\nQ3 Does T always have at most a small (polynomial) number of fixed points?\nQ4 Does T always have at least one fixed point where ads are shown?\nQ3 is important, because with an affirmative answer we could potentially enumerate the fixed points and find the best one from an efficiency perspective. A negative answer to Q4 implies the iterative calibration procedure will cycle. To see this, note that for a given starting point f0, subsequent ft(z) can only take on finitely many values: E[p |z] for some distribution of ads that show (finitely many values), or f0(z). That means that T maps some finite set of calibration maps into itself. Since it has no fixed points, T is a permutation and so must cycle.\nIn the next two sections, we address these questions in the general case (putting no additional restrictions on the problem instances).\n3Note EV depends only on the ordering of the ads for each query induced by f , and so over all possible f , EV takes on only a finite number of distinct values.\n3 Mechanism ALL: Threshold Selection\nIn this section, we consider the case where we select ads by mechanism ALL, that is, we show all ads where b · f(z)− 1 ≥ 0.\nWe will show that an efficiency-maximizing prediction map can be found efficiently (Q2), but without further assumptions, Q1, Q3, and Q4 are answered in the negative. We prove the negative results first; for this purpose, it is sufficient to construct counter-examples.\nIn this section, the examples we construct all require only a single query where all of the candidates have the same raw prediction z. Thus, choosing prediction map reduces to choosing a single value p̂ ∈ [0, 1]. The selection rule simply shows all candidates where b · f(z) = b · p̂ ≥ 1.\nQ1: All fixed points can have bad efficiency Consider an example with 2n+ 1 candidate ads, divided into three classes, with ads given as (p, b) tuples:\nA) 1 ad is (0.5, 2.0), shown if p̂ ≥ 0.5\nB) n ads are (1, 1.9), shown if p̂ ≥ 1/1.9 ≈ 0.53\nC) n ads are (0, 1.8), shown if p̂ ≥ 1/1.8 ≈ 0.56\nWe either show no ads, A, A+B, or A+B+C. Choosing p̂ = 0.5 is a fixed point (it only shows the first ad) which generates value 0.5 · 2 − 1 = 0. Using p̂ = 0.54 shows A+B, and generates value 0.9n. But, this is not a fixed point: the observed CTR is near one (for large n). Showing all the ads (which occurs for any p̂ > 1/1.8) is not a fixed point, and generates negative value, since ads from class C generate value −n.\nQ3: An example with exponentially many fixed points Suppose there are n candidates (pi, bi) where the pi are distinct, and we have indexed by i so that pi is strictly increasing. Further, suppose bi =\ni p1:i , a decreasing\nsequence (using the shorthand p1:i ≡ ∑i j=1 pj). Pick any i ∈ {1, . . . , n}, and let p̂ = 1 bi . We show candidate j if bj p̂ = bj bi ≥ 1. Since the bids are decreasing, we show candidate j if and only if j ≤ i. Thus, serving with p̂ = 1 bi = p1:i i we\nshow candidates 1, . . . , i, and so the average CTR is in fact p̂. Thus, for any i ∈ {1, . . . , n}, there is a fixed-point p̂ that shows ads {1, . . . , i}. Figure 1 shows an example of this construction. If we have m queries each with a distinct fixed raw prediction z and n candidates constructed in this manner, we can choose a per-query fixed point independently for each query, for nm distinct fixed points.\nQ4: An example with no fixed points Consider a single query with two candidates, (p1 = 0.7, b1 = 4, z) and (p2 = 0.1, b2 = 2, z). For any p̂ ≥ 0.5, both ads show and we observe a click-through-rate of 0.4, so no such p̂ can be self-calibrated. For any p̂ ∈ [0.25, 0.5), only ad 1 shows, and we observe a click-through rate of 0.7. For p̂ ∈ [0, 0.25), we don’t show any ads. Thus, there is no non-trivial fixed point; assuming we start with p̂ ≥ 0.25, the calibration procedure will cycle between 0.7 and 0.4.\nQ2: Calculating the efficiency-maximizing f The above examples show that self-calibrated prediction maps may not exist, and that even if they do, they need not maximize efficiency.\nNevertheless, given access to the full problem data (including true clickthrough rates) one might be interested in calculating an efficiency maximizing prediction map. The following algorithm accomplishes this in polynomial time.\nWe define f∗ by considering each z′ ∈ {1, 2, . . . ,K} independently:\n1. Consider the set of candidates (p, b, z, q) where z = z′, and sort these candidates in decreasing order of bid, for j = 1, . . . , nj . We must show some prefix of this list. In particular, if we set p̂ = 1/bj and bj+1 < bj , then we will show exactly ads 1, . . . , j.\n2. For each j where bj+1 < bj , compute the expected value per query of using p̂j = 1/bj (which shows ads 1, . . . , j). This can be computed as\nEV(p̂j) =\nj ∑\ni=1\nPrQ(qi)(pi · bi − 1).\n3. Let f(z) = p̂j∗ where p̂j∗ is the value that maximizes EV(p̂j).\nWhile this result is interesting theoretically (especially in contrast to results in the next section), we note it is not likely to be useful in practice: if it was possible to estimate pi accurately for each ad, then one could simply throw out the coarser-grained predictions zi and use these estimates.\n4 Mechanism ONE: Selecting One Ad\nIn this section, we consider results for selection mechanism ONE. When there is only a single query, or only a single raw prediction, selection mechanism ONE can be quickly analyzed, and our questions are in fact answered in the affirmative, except for Q3. But in non-trivial cases, we again show negative answers to all four questions.\nSingle query, multiple raw predictions Selection mechanism ONE becomes rather degenerate under a single query. We show how to construct a nice f , answering Q1 and Q2, and Q4 in the affirmative.\nFor each raw prediction z′ ∈ {1, . . . ,K}, observe that if an ad with zi = z ′ shows, it must be an ad that has bid b(z′) ≡ maxj:zj=z′ bj. Thus, if an ad with z′ shows, the expected value generated is b(z′) · EC [p | z\n′, b(z′)], where EC [p | z\n′, b(z′)] is the average click-through-rate of ads with z = z′, b = b(z′). We can guarantee we obtain this value by simply setting f(z′) = EC [p |z\n′, b(z′)] and f(z) = 0 for all z 6= z′. Note that this f is self-calibrated because ties are broken uniformly at random under selection mechanism ONE, answering Q4 in the affirmative. We obtain maximum efficiency by using the f that only shows ads with raw prediction\nz∗ = argmax z b(z) · EC [p |z, b(z)].\nLet fz be the f function that only shows candidates with the given z value. Thus, fz∗ is nice. However, we can define a more satisfying f ∗ by\nf∗(z) = Efz [p |z].\nWe only show ads (b, z) where b·f∗(z) achieves the argmax value over candidates, and in fact\nb · f∗(z) = b(z) · Efz [p |z],\nand so we still maximize efficiency. The answer to Q3 is negative: iterative calibration can have exponentially many fixed points. Suppose each ad i has a distinct zi, and pi = b −1 i . Let I be any subset of the ads and define fI : fI(zi) = pi for i ∈ I, fI(zi) = 0 for i 6∈ I. Then, under fI all ads in I tie, so we show them randomly. Each of the 2 |C| subsets of C thus corresponds to a self-calibrated prediction map that shows a different set of ads.\nMultiple queries, single raw prediction Under mechanism ONE, if there is a single raw prediction z made for all candidates (on all queries), then the ads that show are in fact independent of the value p̂ = f(z) > 0: for each query, we always randomly pick one of the candidates with the highest bid. Thus, any p̂ > 0 is efficiency-maximizing, and we can choose p̂ equal to the average observed CTR to obtain self-calibration. Thus, in this case we answer Q1- Q4 in the affirmative.\nQ2: NP-hardness in general In general (with at least two distinct raw predictions and at least two queries), under selection mechanism ONE, the offline problem of finding the efficiency-maximizing prediction map f is NP-hard, even if all bids are 1. We show this using a reduction from the minimum feedback arc set (MFAS) problem on tournaments (see, for example, Kleinberg et al. [14]).\nIn this problem, there are n players, {1, . . . , n}, that have just completed a tournament where every pair of players has played. The MFAS for this problem\nis a ranking of the players that minimizes the number of upsets; that is, if µi is the rank of player i, we want a ranking µ that minimizes the number of times µi > µj , but player j beat player i.\nWe encode this problem as an auction efficiency maximization problem as follows: There are n distinct z values, 1, . . . , n, one for each player, and there are 12n(n − 1) queries (each equally likely), one for each (i, j) pair with i < j. The query for the pair (i, j) (where i beat j without loss of generality) has two candidates (p, z), namely (1, i) and (0, j). Thus, if we show the ad corresponding to the winner (with z = i), we have p = 1, and the bid is 1, so we get value 1; if we show ad with z = j, we have p = 0, we get no value. It is then clear that the efficiency-maximizing ranking of the raw predictions z exactly corresponds to the solution to the MFAS problem.\nNegative results for Q1, Q3, and Q4 in general We also show negative results for Q1, Q3, and Q4 in general.\nFor Q1, observe that in the NP-hardness construction when there is a perfect ranking, we observe a CTR of 1.0, and so the efficiency-maximizing prediction map cannot be self-calibrated. We can illustrate this directly with the following example. There are four ads, each given as (p, b, z) tuples:\nq1 q2 A (1.0, 2, z1) C (1.0, 2, z2) B (0.0, 2, z2) D (0.0, 1, z1)\nWe need f(z1) > f(z2) in order to guarantee we show Ad A on q1; we also need f(z2) > 1 2f(z1) in order to show Ad C on q2. We will observe a 1.0 CTR on both z1 and z2 on any such efficiency maximizing f , but we are constrained to pick f(z2) < f(z1) ≤ 1, and so no such f can be self-calibrated.\nFor Q3, we have already shown multiple fixed points in the single-query case. If we consider multiple queries, where each query has a single distinct raw prediction, we immediately arrive at a problem with exponentially many fixed points.\nFor Q4, it is straightforward to construct an example with cycles, but constructing one with no fixed point is a bit trickier. In particular, any time there is some prediction z where each query has at least one ad with prediction z, we can always find a fixed point by setting f(z′) = 0 for z′ 6= z and f(z) > 0. The set of ads shown will be independent of the non-zero value f(z), so we can set it equal to the observed CTR, achieving self-calibration (except in the degenerate case where all the ads with prediction z have zero CTR).\nHowever, it is still possible to construct problems with no fixed points without resorting to such degeneracy, as the following example illustrates. Each query is equally likely, all the bids are 1, and the (p, z) ad tuples are:\nq1 q2 q1 q2 A (0.5, z1) B (0.6, z2) C (0.5, z1) E (0.2, z2)\nD (0.6, z2) F (0.3, z1)\nIf f(z1) > f(z2), then we show ads A,B, C, and F. In this case, we observe a CTR of (0.5 + 0.5 + 0.3)/3 = 0.433 for z1, and 0.6 for z2, so we cannot be selfcalibrated. If f(z1) < f(z2), we show ads A, B, D, and E, and observe a CTR of (0.6 + 0.6 + 0.2)/3 = 0.467 for z2, and 0.5 for z1, and so again we cannot be self-calibrated. Finally, if f(z1) = f(z2), we always show A and B, and show the other ads half of the time. Thus, we observe a CTR of (3/4)0.5+(1/4)0.3 = 0.45 for z1, and a CTR of (3/4)0.6 + (1/4)0.2 = 0.5 for z2, and so again we cannot be well-calibrated. Thus, no self-calibrated f exists for this problem."
    }, {
      "heading" : "5 Sufficient Conditions",
      "text" : "As the previous two sections show, without additional assumptions significant problems arise if one tries to achieve both calibration and auction efficiency. In this section, we introduce additional assumptions that are sufficient to guarantee nice prediction maps exist. Table 1 summarizes our results.\nThe intuition behind our results is a basic property of conditional probability. Calibration depends on the conditional expectation E[p |z]. In general, selection changes the distribution this expectation is with respect to. But if selection is only a function of z, it does not change the conditional distribution of p given z, since the latter is already conditioned on z.\nFor example, suppose we have a single query, and that all bids are 1, so all selection decisions are functions of z. This means that E[p | z] does not change under selection, and thus defines an efficiency-maximizing self-calibrated prediction map. To extend this intuition to more realistic auctions, we need to make sure that the query and the bid do not add any information about p, so that selection does not change E[p | z] and the different E[p | z] for each query can be reconciled. We now state these properties formally:\nProp E1 For each z there exists a value p̄(z) such that for each query q with PrC(q |z) > 0, and for each b with PrC(b |q, z) > 0,\nEC [p |z, b, q] = EC [p |z, q] = EC [p |z] ≡ p̄(z). (1)\nThat is, in all cases the bid and query provide no more information than the raw prediction about average click-through rates.4 For this assumption, the natural prediction map to consider is f(z) = p̄(z).\nProp E2 A weaker assumption is that\nEC [p |z, b] = EC [p |z] (2)\nwhenever both expectations are defined. This essentially marginalizes over queries, rather than holding simultaneously for all q.\nProp SI A problem instance is selection-invariant if for all f, f ′, for any z where both Ef [p |z] and Ef ′ [p |z] are defined, we have\nEf [p |z] = Ef ′ [p |z]. (3)\nSelection invariance says that the observed CTR for a given raw prediction z is independent of the prediction map used for selection. Under this assumption, the natural calibration map to consider is f∗(z) = Efz [p | z], where fz is any prediction map that shows some ads with raw prediction z.\nIt is easy to show that Prop E1 implies Prop E2. A weak per-query variant of Prop E1 is that, for all z, b, and q (when defined), EC [p | z, b, q] = EC [p | z, q]. We can dismiss this assumption as insufficient, as we can take the negative examples of Section 3 and re-state them where each candidate occurs on a distinct query, each equally likely. Thus, the above property holds trivially, but the pathological behaviors still occur."
    }, {
      "heading" : "5.1 Properties that Imply Nice Maps Exist",
      "text" : "First, we show that under mechanism ALL, Prop E2 and Prop SI are equivalent; we then show that Prop E2 (and hence also Prop SI) imply a nice problem.\nTheorem 1. Under selection mechanism ALL, Prop E2 is equivalent to Prop SI (selection invariance).\nProof sketch. Suppose Prop E2 holds. Selection mechanism ALL must show either all of the candidates with a given (z, b) combination, or none of them. Thus, for any f where Prf (z, b) > 0, we must have\nEf [p |z, b] = EC [p |z, b]. (4)\nThen, for any f , assuming Ef [p |z] is defined,\nEf [p |z] = Ef [Ef [p |z, b]]\n= Ef [EC [p |z, b]] Eq. (4)\n= Ef [EC [p |z]] Prop E2\n= EC [p |z].\n4Note that this does not hold under the NP-Hardness reduction for ONE in the previous section, as EC [p |z, q] 6= EC [p |z].\nFor the other direction, suppose we have selection invariance (Prop SI). It is sufficient to consider a fixed raw prediction z (if there are multiple z, we can consider them independently). Also, we can assume candidates have distinct bids - if multiple candidates have the same bid and raw prediction, mechanism ALL treats them all the same, so we can just average over them.\nIndex the bids (b1, b2, . . . ) in decreasing order. Then, depending on the chosen p̂ = f(z), we either show (when the appropriate queries occur) ad 1, or ads 1 and 2, etc. Prop SI says that no matter what p̂ is, the average CTR of the ads we show is the same. Suppose that all the ads are on the same query. Then Prop SI implies p1 = 1 2p1+ 1 2p2, so p1 = p2; 1 2 (p1 + p2) = 1 3 (p1 + p2+ p3), so p1 = p2 = p3; and so on. When the ads are on different queries, the weights in the above equalities change to reflect the query distribution, but are still all positive and sum to 1, so the same inductive reasoning holds.\nThis result implies that under selection mechanism ALL, when Prop E2 holds the prediction map f∗(z) = EC [p |z] is self-calibrated. Next, we show this map is in fact also efficiency-maximizing:\nTheorem 2. Under selection mechanism ALL, Prop E2 implies f∗ is efficiency maximizing, where f∗(z) = EC [p |z].\nProof. Recall we need to show f∗ maximizes\nEV (f) = ∑\ni∈C\nPrQ(qi)Pr(i |qi, f)(pibi − 1).\nSince selection decisions for one z value do not impact others, it suffices to consider a single z value. We can decompose the sum over C over the partition that associates all the ads that share a common bid and raw prediction. Let B = {i |bi = b, zi = z} ⊆ C be the element of this partition for (b, z). For a given f(z) = p̂, either all the ads in B show (when their respective queries occur), or none of them do; thus, if we can show that f∗ shows these ads if and only if they increase EV, we are done. The expected value per query of showing these ads is: ∑\ni∈B\nPrQ(qi)Pr(i |qi, f)(pibi − 1). (5)\nSince Pr(i | qi, f) ∈ {0, 1} must be the same for all these ads, this quantity is non-negative if and only if ∑\ni∈B Pr Q(qi)(pibi − 1) ≥ 0.\nRecall PrC(i) = Pr Q(qi)/C where C =\n∑\ni∈C Pr Q(qi). We have PrC(i ∧ b ∧\nz) = PrC(i) if i ∈ B, and 0 otherwise. Letting CB = ∑ i∈B Pr Q(qi), then PrC(b ∧ z) = CB C , and so\nPrC(i |b, z) = PrC(i)\nCB/C =\nPrQ(qi)/C\nCB/C =\nPrQ(qi)\nCB (6)\nfor i ∈ B, and 0 otherwise. Then,\nEC [p |z] = EC [p |b, z] Prop E2\n= ∑\ni∈C\nPrC(i |b, z)pi\n= 1\nCB\n∑\ni∈B\nPrQ(qi)pi. Eq. (6)\nUsing this result, we have\n∑\ni∈B\nPrQ(qi)(pibi − 1) = b\n( ∑\ni∈B\nPrQ(qi)pi\n)\n− CB)\n= CB(bEC [p |z]− 1).\nThis quantity is non-negative if and only if bf∗(z)− 1 ≥ 0; since this is exactly the condition we use to decide whether or not to show the ads in B, we are done.\nIt is not hard to directly prove that under selection mechanism ALL, Prop SI implies f∗ is efficiency-maximizing: the idea is to consider again a single z, sort the ads by bid into blocks, and show by induction that each block has average CTR f∗(z).\nIn Section 4 we saw that the problem of finding an efficiency-maximizing f is NP-hard under mechanism ONE, even under the assumption of a single bid. Under Prop E1, fortunately the situation is much easier:\nTheorem 3. Under selection mechanism ONE, if Prop E1 holds then the prediction map f∗ where f∗(z) = EC [p |z] is efficiency-maximizing and self-calibrated.\nProof. For a query q, consider a partition Bq of C(q) into sets of ads that share a common b and z, so the elements of the partition are\nBqb,z = {i |bi = b, zi = z, qi = q} ⊆ C(q)\nfor each (b, z) pair. All i ∈ B for some B must share a common value Prf (i |q). We also use B as the event that some i ∈ B shows; so for example Prf (B |q) is the probability that some ad from B shows. Under selection mechanism ONE, for each i ∈ B, we have Prf (i |B, q) = 1 |B| (since ties are broken at random). Also,\nEC [p |b, z, q] = 1\n|Bqb,z|\n∑\ni∈Bq b,z\npi. (7)\nRecalling cost is zero under ONE, for any f ,\nEV(f)\n= ∑\nq∈Q\nPrQ(q) ∑\ni∈C(q)\nPrf (i |q)pibi\n= ∑\nq∈Q\nPrQ(q) ∑\nB q\nb,z ∈Bq\n∑\ni∈Bq b,z\nPrf (i |q)pibi\n= ∑\nq∈Q\nPrQ(q) ∑\nB q\nb,z ∈Bq\nPrf (B q b,z |q)\n1\n|Bqb,z |\n∑\ni∈Bq b,z\npib\nand using Eq. (7),\n= ∑\nq∈Q\nPrQ(q) ∑\nB q\nb,z ∈Bq\nPrf (B q b,z |q)bEC [p |b, z, q]\n≤ ∑\nq∈Q\nPrQ(q) max B q\nb,z ∈Bq\nbEC [p |b, z, q].\nThus, it is sufficient to show that selecting ads using f∗ produces the expected value in the last line of the above inequality. For each query, we rank the ads using b · f∗(z) = bEC[p |b, z, q], and so this is exactly the expected value that f ∗ obtains. To see that f∗ is self-calibrated, observe that when Prf (z, b, q) > 0,\nEf [p |z, b, q] = EC [p |z, b, q] = f ∗(z),\nand so Ef [p |z] = ∑\nb,q\nPrf (b, q |z)Ef [p |z, b, q] = f ∗(z)."
    }, {
      "heading" : "5.2 Negative Results",
      "text" : "We show several negative results relating to the assumptions considered in the previous section.\nONE and ALL: Prop SI does not imply Prop E1 Consider an example with two queries, each equally likely. Each query has two candidates, given as the following (p, b) tuples (they all share a common z):\nq1 q2 A (0.1, 1) C (0.1, 2) B (0.2, 2) D (0.2, 1)\nBecause of the symmetry between these queries, under any f (and either selection mechanism), ad A must show with the same probability as ad D, as must ads B and C. Thus, for any f , Ef [p | b = 1, z] = 0.15, and similarly Ef [p | b = 2, z] = 0.15. Thus, selection invariance holds, as does Prop E2. However, EC [p |z, b = 1, q1] = 0.1 6= EC [p |z, q1] = 0.15.\nONE: Prop E2 does not imply Prop SI Consider the example, with two equally likely queries, and two distinct raw predictions:\nq1 q2 A (0.2, 2, z1) C (0.1, 2, z1) B (0.1, 1, z1) D (0.2, 1, z1) E (1.0, 9, z2)\nNote that EC [p | z1, b = 1] = EC [p | z1, b = 2] = 0.15. However, if we consider two prediction maps f(z1) = 0.5, f(z2) = 1 and f ′(z1) = 1, f ′(z2) = 0, under selection mechanism ONE, we have Ef [p |z1] = 0.1, but Ef ′ [p |z1] = 0.15.\nONE: Prop SI does not imply a nice problem We have four queries, each equally likely; the bids for the ads on q3 and q4 are defined in terms of some small ǫ > 0, with (p, b, z) tuples:\nq1 q2 q3 q4 A (1, 2, z1) C (1, 2, z2) A’ (0, 2ǫ, z1) C’ (0, 2ǫ, z2) B (0, 2, z2) D (0, 1, z1) B’ (1, 2ǫ, z2) D’ (1, 1ǫ, z1)\nNote that q3 and q4 mirror q1 and q2, except that the bids are scaled by ǫ, and the CTRs are reversed. Under any f , ads A and A′ show with the same probability, as do B and B′, and the other two pairs. Thus, under selection by any f , we have Ef [p |z1] = Ef [p |z2] = 0.5 whenever the expectation is defined, and so Prop SI holds. However, as ǫ → 0, only q1 and q2 have any impact on efficiency. Thus, as before we have constraints on the optimal solution that f(z1) > f(z2) > 1 2f(z1). Thus, the prediction map f\n∗ with f∗(z1) = 0.5 and f∗(z2) = 0.5 is not efficiency-maximizing, as it only shows ad A on q1 only half the time."
    }, {
      "heading" : "6 Discussion and Future Work",
      "text" : "Our sufficient conditions are quite strong, but not unrealistic. They require that the bid and query not add any information about the CTR, conditional on the raw prediction. CTR estimation systems normally use queries as features (e.g., [13]), so it is reasonable to hope that the query does not add extra information. Bids are set by advertisers for query-ad pairs, which are already used by CTR estimation systems, so any systematic patterns in bids are likely to be accounted for. Since advertisers have much less information than the auctioneer, it seems unlikely that they can add extra information about CTRs through fine-grained\nbid manipulation. We can test if our sufficient conditions hold by running randomization experiments that change the mix of ads shown.\nSince randomized predictions cannot in general lead to maximum efficiency, it is natural to first consider deterministic prediction maps. Nevertheless, given the negative results in the current work, it would be interesting to also study randomized calibration strategies that provide calibration guarantees without needing IID assumptions. Then the natural question becomes: how much efficiency is lost by using a randomized calibration strategy, versus using a deterministic efficiency-maximizing prediction map that is not self-calibrated."
    } ],
    "references" : [ {
      "title" : "Verification of forecasts expressed in terms of probability",
      "author" : [ "Glenn W. Brier" ],
      "venue" : "Monthly Weather Review,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1950
    }, {
      "title" : "An empirical comparison of supervised learning algorithms",
      "author" : [ "Rich Caruana", "Alexandru Niculescu-Mizil" ],
      "venue" : "In Proceedings of the 23rd international conference on Machine learning,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2006
    }, {
      "title" : "An empirical comparison of supervised learning algorithms",
      "author" : [ "Rich Caruana", "Alexandru Niculescu-Mizil" ],
      "venue" : "In Proceedings of the 23rd international conference on Machine learning,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2006
    }, {
      "title" : "Prediction, Learning, and Games",
      "author" : [ "Nicolò Cesa-Bianchi", "Gabor Lugosi" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2006
    }, {
      "title" : "Properties and benefits of calibrated classifiers",
      "author" : [ "Ira Cohen", "Moises Goldszmidt" ],
      "venue" : "European Conference on Principles and Practice of Knowledge Discovery in Databases (PKDD). Springer,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2004
    }, {
      "title" : "The well-calibrated Bayesian",
      "author" : [ "A. Dawid" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1982
    }, {
      "title" : "The comparison and evaluation of forecasters",
      "author" : [ "Morris H. DeGroot", "Stephen E. Fienberg" ],
      "venue" : "The Statistician,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1983
    }, {
      "title" : "Internet advertising and the generalized second-price auction: Selling billions of dollars worth of keywords",
      "author" : [ "Benjamin Edelman", "Michael Ostrovsky", "Michael Schwarz" ],
      "venue" : "American Economic Review,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2007
    }, {
      "title" : "A proof of calibration via blackwell’s approachability theorem",
      "author" : [ "Dean P. Foster" ],
      "venue" : "Games and Economic Behavior,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1999
    }, {
      "title" : "Strictly proper scoring rules, prediction, and estimation",
      "author" : [ "Tilmann Gneiting", "Adrian Es Raftery" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2007
    }, {
      "title" : "Probabilistic forecasts, calibration and sharpness",
      "author" : [ "Tilmann Gneiting", "Fadoua Balabdaoui", "Adrian E. Raftery" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2007
    }, {
      "title" : "Web-scale bayesian click-through rate prediction for sponsored search advertising in microsoft’s bing search engine",
      "author" : [ "Thore Graepel", "Joaquin Quiñonero Candela", "Thomas Borchert", "Ralf Herbrich" ],
      "venue" : "In ICML,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2010
    }, {
      "title" : "Predicting good probabilities with supervised learning",
      "author" : [ "Niculescu-Mizil", "Alexandru", "Rich Caruana" ],
      "venue" : "In Proceedings of the 22nd international conference on Machine learning,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2005
    }, {
      "title" : "Predicting good probabilities with supervised learning",
      "author" : [ "Alexandru Niculescu-Mizil", "Rich Caruana" ],
      "venue" : "In Proceedings of the 22nd international conference on Machine learning,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2005
    }, {
      "title" : "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. In Advances in Large Margin Classifiers",
      "author" : [ "John C. Platt" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1999
    }, {
      "title" : "Combining probability forecasts",
      "author" : [ "Roopesh Ranjan", "Tilmann Gneiting" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2010
    }, {
      "title" : "Transforming classifier scores into accurate multiclass probability estimates",
      "author" : [ "Bianca Zadrozny", "Charles Elkan" ],
      "venue" : "In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2002
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "This auction selection mechanism has been extensively studied, and has many nice properties [19, 8].",
      "startOffset" : 92,
      "endOffset" : 99
    }, {
      "referenceID" : 11,
      "context" : ", [13]).",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 0,
      "context" : "Much of the earliest work is in the probabilistic forecasting literature [1, 6, 18].",
      "startOffset" : 73,
      "endOffset" : 83
    }, {
      "referenceID" : 5,
      "context" : "Much of the earliest work is in the probabilistic forecasting literature [1, 6, 18].",
      "startOffset" : 73,
      "endOffset" : 83
    }, {
      "referenceID" : 15,
      "context" : "Much of the earliest work is in the probabilistic forecasting literature [1, 6, 18].",
      "startOffset" : 73,
      "endOffset" : 83
    }, {
      "referenceID" : 6,
      "context" : "Calibration is particularly important when comparing predictors, since two sets of calibrated predictions can be fairly evaluated by how concentrated they are on observed outcomes [7, 12, 11].",
      "startOffset" : 180,
      "endOffset" : 191
    }, {
      "referenceID" : 10,
      "context" : "Calibration is particularly important when comparing predictors, since two sets of calibrated predictions can be fairly evaluated by how concentrated they are on observed outcomes [7, 12, 11].",
      "startOffset" : 180,
      "endOffset" : 191
    }, {
      "referenceID" : 9,
      "context" : "Calibration is particularly important when comparing predictors, since two sets of calibrated predictions can be fairly evaluated by how concentrated they are on observed outcomes [7, 12, 11].",
      "startOffset" : 180,
      "endOffset" : 191
    }, {
      "referenceID" : 4,
      "context" : "For example, it is easier to threshold the output of a calibrated classifier to minimize weighted classification error [5].",
      "startOffset" : 119,
      "endOffset" : 122
    }, {
      "referenceID" : 13,
      "context" : "For example, boosted trees are uncalibrated, but become excellent probability estimators after calibration [16, 2].",
      "startOffset" : 107,
      "endOffset" : 114
    }, {
      "referenceID" : 1,
      "context" : "For example, boosted trees are uncalibrated, but become excellent probability estimators after calibration [16, 2].",
      "startOffset" : 107,
      "endOffset" : 114
    }, {
      "referenceID" : 14,
      "context" : "The two most common methods for calibration are Platt scaling, which is equivalent to logistic regression, and isotonic regression [17, 20, 15, 3].",
      "startOffset" : 131,
      "endOffset" : 146
    }, {
      "referenceID" : 16,
      "context" : "The two most common methods for calibration are Platt scaling, which is equivalent to logistic regression, and isotonic regression [17, 20, 15, 3].",
      "startOffset" : 131,
      "endOffset" : 146
    }, {
      "referenceID" : 12,
      "context" : "The two most common methods for calibration are Platt scaling, which is equivalent to logistic regression, and isotonic regression [17, 20, 15, 3].",
      "startOffset" : 131,
      "endOffset" : 146
    }, {
      "referenceID" : 2,
      "context" : "The two most common methods for calibration are Platt scaling, which is equivalent to logistic regression, and isotonic regression [17, 20, 15, 3].",
      "startOffset" : 131,
      "endOffset" : 146
    }, {
      "referenceID" : 8,
      "context" : "However, if the system is allowed to use randomness (that is, predict a distribution), then calibration can be achieved ([9, 10] and [4, Sec 4.",
      "startOffset" : 121,
      "endOffset" : 128
    }, {
      "referenceID" : 0,
      "context" : ",K} → [0, 1].",
      "startOffset" : 6,
      "endOffset" : 12
    }, {
      "referenceID" : 0,
      "context" : "Formally, define T : [0, 1] → [0, 1] (a function from prediction maps to prediction maps) by T (f) = f ′ where",
      "startOffset" : 21,
      "endOffset" : 27
    }, {
      "referenceID" : 0,
      "context" : "Formally, define T : [0, 1] → [0, 1] (a function from prediction maps to prediction maps) by T (f) = f ′ where",
      "startOffset" : 30,
      "endOffset" : 36
    }, {
      "referenceID" : 0,
      "context" : "Thus, choosing prediction map reduces to choosing a single value p̂ ∈ [0, 1].",
      "startOffset" : 70,
      "endOffset" : 76
    }, {
      "referenceID" : 11,
      "context" : ", [13]), so it is reasonable to hope that the query does not add extra information.",
      "startOffset" : 2,
      "endOffset" : 6
    } ],
    "year" : 2012,
    "abstractText" : "Calibration is a basic property for prediction systems, and algorithms for achieving it are well-studied in both statistics and machine learning. In many applications, however, the predictions are used to make decisions that select which observations are made. This makes calibration difficult, as adjusting predictions to achieve calibration changes future data. We focus on click-through-rate (CTR) prediction for search ad auctions. Here, CTR predictions are used by an auction that determines which ads are shown, and we want to maximize the value generated by the auction. We show that certain natural notions of calibration can be impossible to achieve, depending on the details of the auction. We also show that it can be impossible to maximize auction efficiency while using calibrated predictions. Finally, we give conditions under which calibration is achievable and simultaneously maximizes auction efficiency: roughly speaking, bids and queries must not contain information about CTRs that is not already captured by the predictions.",
    "creator" : "LaTeX with hyperref package"
  }
}