{
  "name" : "1704.02789.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Parsimonious Random Vector Functional Link Network for Data Streams",
    "authors" : [ "Mahardhika Pratama", "Plamen P. Angelov", "Edwin Lughofer", "Meng Joo Er" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "the design of neural networks (NNs) since it conveys solid theoretical justification of randomized learning. Existing works in RVFLN are hardly scalable for data stream analytics because they are inherent to the issue of complexity as a result of the absence of structural learning scenarios. A novel class of RVLFN, namely parsimonious random vector functional link network (pRVFLN), is proposed in this paper. pRVFLN features an open structure paradigm where its network structure can be built from scratch and can be automatically generated in accordance with degree of nonlinearity and time-varying property of system being modelled. pRVFLN is equipped with complexity reduction scenarios where inconsequential hidden nodes can be pruned and input features can be dynamically selected. pRVFLN puts into perspective an online active learning mechanism which expedites the training process and relieves operator’s labelling efforts. In addition, pRVFLN introduces a non-parametric type of hidden node, developed using an interval-valued data cloud. The hidden node completely reflects the real data distribution and is not constrained by a specific shape of the cluster. All learning procedures of pRVFLN follow a strictly single-pass learning mode, which is applicable for an online real-time deployment. The efficacy of pRVFLN was rigorously validated through numerous simulations and comparisons with stateof-the art algorithms where it produced the most encouraging numerical results. Furthermore, the robustness of pRVFLN was investigated and a new conclusion is made to the scope of random parameters where it plays vital role to the success of randomized learning."
    }, {
      "heading" : "Keywords – Random Vector Functional Link, Evolving Intelligent System, Online Learning, Randomized",
      "text" : "Neural Networks\nI. Introduction\nFor decades, research in artificial neural networks has mainly investigated the best way to determine network free parameters, which reduces error as close as possible to zero [1]. Various approaches [2],[3] were proposed, but a large volume of work is based on a first or second-order derivative approach in respect to the loss function [4], [5]. Due to the rapid technological progress in data storage, capture, and transmission [6], the machine learning community has encountered an information explosion, which calls for scalable data analytics. Significant growth of the problem space has led to a scalability issue for conventional machine learning approaches, which require iterating entire batches of data over multiple epochs. This phenomenon results in a strong demand for a simple, fast machine learning algorithm to be well-suited for deployment in numerous data-rich applications [7]. This provides a strong case for research in the area of randomized neural networks (RNNs) [10], which was very popular in late 80’s and early 90’s. RNNs offer an algorithmic framework, which allows them to generate most of the network\nparameters randomly while still retaining reasonable performance [8], [9]. One of the most prominent RNNs in the literature is the random vector functional link (RVFL) network, which has been analytically proven to be a universal approximator for continuous functions on bounded finite-dimensional sets and a\nuniversal approximator with the rate of approximation error convergence to zero in the order of )( n\nCO\n, where n is the number of basis functions with C independent of n [11].\nDue to its simple but sound working principle, the RNN has regained its popularity in the current literature [12]-[22]. Nonetheless, vast majority of RNNs in the literature suffer from the issue of complexity which make their computational complexity and memory burden prohibitive for data stream analytics since their complexities are manually determined and rely heavily on expert domain knowledge [14]. They presents a fixed-size structure which lacks of adaptive mechanism to encounter changing training patterns in the data streams [15]. Random selection of network parameters often leads the network complexity to go beyond what necessary due to existence of superfluous hidden nodes which contribute little to the generalization performance [16]. Although the universal approximation capability of RNN is assured only when the number of hidden nodes is sufficiently large, the suitable complexity for given problems is an open problems and problem-dependent.\nA novel RVFLN, namely parsimonious random vector functional link network (pRVFLN), is proposed in this paper. pRVFLN combines simple and fast working principles of RFVLN where all network parameters but the output weights are randomly generated with no tuning mechanism of hidden nodes. It characterises the online and adaptive nature of evolving intelligent systems where network components are automatically generated on the fly. pRVFLN is capable of coping with any variations of data streams no matter how slow, rapid, gradual, sudden or temporal drifts in data streams are because it can initiate its learning structure from scratch with no initial structure and its structure is self-evolved from data streams in the one-pass learning mode by automatically adding, pruning and recall its hidden node [16]. Furthermore, it is compatible for online real-time deployment because data streams are handled without revising previously seen samples. pRVFLN is equipped with the hidden node pruning mechanism which guarantees a compact and parsimonious network structure and the rule recall mechanism which aims to overcome the cyclic concept drift. pRVFLN incorporates an online feature selection scenario which is capable of activating and deactivating input attributes on demands and an online active learning scenario which reduces the number of training samples to be seen during the training process. Moreover, pRVFLN is a plug-and-play algorithm where all learning modules are integrated in a single training process and a sample-wise manner without pre-and/or post-processing steps. Novelties of pRVFLN are elaborated:\n• Network Architecture: Unlike existing RVFLNs usually constructed under a single hidden layer feedforward network [26], pRVFLN is structured as a locally recurrent neural network with a selffeedback loop at the hidden node to capture temporal system dynamic. The recurrent network architecture puts into perspective a spatio-temporal activation degree which takes into account compatibility of both\nprevious and current data points without compromising the local learning property because of its local recurrent connection. pRVFLN introduces a new type of hidden node, namely an interval-valued data cloud inspired by the notion of recursive density estimation and AnYa by Angelov [23], [27]. This hidden node is parameter-free and requires no parameterization per scalar variable. It does not follow a specific shape and its firing strength is defined as a local density calculated as accumulated distances between a local mean and all data points in the cloud seen thus far. Our version in this paper distinguishes itself from its predecessors in [23], [27] in the fact that an interval uncertainty is incorporated per local region which targets inaccurate, inexact and uncertain natures of real-world data streams. It is worth mentioning that the proposed hidden node still holds the universal approximation property of the RVFLN since it is derived using the Cauchy kernel which is asymptotically a Gaussian-like function. The Cauchy function forms a first order Taylor series approximation of the Gaussian function. Moreover, the output layer consists of a set of functional-link output nodes created by the second order Chebyshev polynomial which expands the degree of freedom of the output weight to improve its approximation power [28].\n• Online Active Learning Mechanism: pRVFLN possesses an online active learning mechanism which is meant to extract data samples for training process. This mechanism is capable of finding important data streams for training process, while discarding inconsequential samples [25]. This strategy expedites the training mechanism and improves model’s generalization since it prevents redundant samples to be learned. This scenario is underpinned by sequential entropy method (SEM) which forms a modified version of neighbourhood probability method [29] for data streams. The SEM quantifies the entropy of neighbourhood probability recursively and differs from that of [30] because it integrates the concept of data cloud. The concept of data cloud further simplifies the working principle of SEM since its activation degree already abstracts a density of a local region – a key attribute of neighbourhood probability. In realm of classification problems, the SEM does not always call for ground truth which leads to significant reduction of operator annotation’s efforts.\n• Hidden Node Growing Mechanism: pRVFLN is capable of automatically generating its hidden nodes on the fly with the help of type-2 self-constructing clustering (T2SCC) mechanism which was originally designed for an incremental feature clustering mechanism [32]. This method relies on the correlation measure in the input space and target space, which can be carried out in the single-pass learning mode with ease. The original version, cannot be directly implemented here because the original version is not yet designed for the interval-valued data cloud. This rule growing process also differs from those of other RNNs with a dynamic structure [14], because the hidden nodes are not randomly generated, rather they are created from the rule growing condition, which considers the locations of data samples in the input space. We argue that randomly choosing centers or focal points of hidden nodes independently from the original training data risks on performance deterioration [33], because it may not hold the completeness principle. pRVFLN relies on the data cloud-based hidden node, which does not require any parameterization, thus offering a simple and fast working framework as other RNNs.\n• Hidden Node Pruning and Recall Mechanism: a rule pruning scenario is integrated in pRVFLN. The rule pruning scenario plays vital role to assure the compactness and parsimony of the network structure since it is capable of detecting superfluous neurons to be removed during the training process [34]. pRVFLN employs the so-called type-2 relative mutual information (T2RMI) method which extends the RMI method [31] to the working principle of the interval-valued hidden node. The T2RMI method examines relevance of hidden nodes to target concept and thus captures outdated hidden nodes which is no longer compatible to portray the current target concept. In addition, the T2RMI method is applied to perform the rule recall mechanism which aims to cope with the cyclic drift. The cyclic drift refers to a situation where old concept re-emerges again in the future. The absence of rule recall scenario risks on the catastrophic forgetting of previously valid knowledge because the cyclic drift imposes an introduction of a new rule without any memorization of learning history. It differs from that [34] because the rule recall mechanism is separated from the rule growing scenario.\n• Online Feature Selection Mechanism: pRVFLN is capable of carrying out an online feature selection process, borrowing several concepts of online feature selection (OFS) [35]. Note that although feature selection is well-established for an offline situation, online feature selection remains a challenging and unsolved problem. Notwithstanding some online feature reduction scenarios in the literature, the issue of stability is still open, because an input feature once discarded cannot be recovered in the future. The OFS delivers a flexible online feature selection scenario, because it makes possible to select or deselect input attributes in every training observation by assigning crisp weights (0 or 1) to input features. Another prominent characteristic of the OFS method lies in its capability to deal with partial input features, because the cost of extracting all input features may be expensive in certain applications. The convergence of the OFS for both full and partial input attributes have been also proven. Nevertheless, the original version [35] is originally devised for linear regression and calls for some modification to be perfectly fit to pRVLFN. The generalized OFS (GOFS) is proposed here and incorporates some refinements to adapt to the working principle of the pRVFLN.\nMajor contributions of this paper are detailed as follows: 1) this paper presents a novel random vector functional link network termed pRVFLN which is designed to handle the problem of data streams efficiently; 2) It puts into perspective the interval-valued data cloud which is not shape-specific and nonparametric. It requires no parameterization per scalar variable and makes use of local density information of a data cloud; 3) four learning components, namely SEM, T2SCC, T2RMI, GOFS, are proposed. The efficacy of pRVFLN was thoroughly evaluated using numerous real-world data streams and benchmark with prominent algorithms recently published in the literature where pRVFLN demonstrated the most state-of-the art performance in achieving a tradeoff between accuracy and simplicity. Moreover, analysis of robustness of random intervals was performed. It is concluded that random regions should be carefully selected and should be chosen close enough to true operating regions of a system being modelled.\nThe remainder of this paper is structured as follows: Section 2 outlines related works; Section 3 details network architecture of pRVFLN; Section 4 discusses learning policy of pRVFLN; Section 5 elaborates proof of concepts and comparisons against state-of-the art algorithms; Concluding remarks are drawn in the last section of this paper.\nII. Network Architecture of pRVFLN\nThis section elaborates the network architecture of pRVFLN, which outputs its prediction. pRVFLN features a generalized recurrent network structure, possessing a local recurrent link at the hidden node. In the literature, there exist at least three types of recurrent network structures referring to its recurrent connections: global [36], interactive [37], and local [26], but the local recurrent connection is deemed to be the most compatible recurrent type to our case because it retains the local property, which assures stability when adding, pruning and fine-tuning hidden nodes. pRVFLN utilizes the notion of the functional-link neural network, having a direct connection from the input layer to the output layer [38]. Furthermore, the hidden layer of pRVFLN is built upon an interval-valued data cloud, which does not require any parametrization and granulation [23]. This hidden node does not have any pre-specified shape and fully adapts to the real data distribution because it is derived from recursive local density estimation. We bring the idea of data cloud further with an interval-valued case.\nSuppose that a data tuple streams at t-the time instant, where n tX  is an input vector, while m tT  is a target vector. n and m are respectively the numbers of input and output dimensions. Because pRVFLN works in a strictly online learning environment, it has no access to previously seen samples, and a data point is simply discarded after being learned. Due to the nature of data streams, the total number of data N is assumed to be unknown. The output of pRVFLN is defined as follows:\n],[ ~ ,)( ~\n1\n, GGGBXAGy temporal\nR\ni ttttemporaliio  \n (1)\nwhere i stands for an output node of the i-th component, produced by weighting the weight vector with an extended input vector i T ei wx , where 1)12(  nex is an extended input vector resulting from the functional link neural network based on the Chebyshev function and 1)12(  niw is a connective weight of the i-th output node. RBA t n t ,, are respectively the input weights, bias and the number of hidden nodes. For simplicity, the bias is set as 0 here. temporaliG , ~ is an interval-valued activation function of the i-\nth hidden node, triggered by the upper and lower activation function temporalitemporali GG ,, , . (1) can be further\nexpanded as follows:\n   \n R\ni\ntemporalii\nR\ni otemporaliioo GqGqy 1 , 1\n,)1(  (2)\nwhere mq  is a design factor to reduce an interval-valued function to a crisp one [38]. It is worth noting\nthat the upper and lower activation functions temporalitemporali GG ,, , deliver spatiotemporal characteristics as\na result of a local recurrent connection at the i-th hidden node, which combines the spatial and temporal firing strength of the i-th hidden node. These temporal activation functions output the following.\n1 ,,, )1(   t temporalii t spatialii t temporali GGG \n1 ,,, )1(   t\ntemporalii\nt\nspatialii\nt\ntemporali GGG \nwhere R is a weight vector of the recurrent link. The self-feedback loop here feeds the spatiotemporal firing strength at the previous time step 1\n, ~ t temporaliG back to itself and is consistent with the\nlocal learning principle. This trait happens to be very useful in coping with the temporal system dynamic because it functions as an internal memory component which memorizes a previously generated spatiotemporal activation function at t-1. Also, the recurrent network is capable of overcoming overdependency on time-delayed input features, which plays a critical role in the feedforward network to cope with strong temporal dependencies of subsequent patterns [36]. The feedforward network assumes a problem as a function of current and past input and outputs. This trait may lower input dimension, thereby alleviating computational complexity.\nThe hidden node of the RT2McRVFLN is an extension of the cloud-based hidden node, where it embeds an interval-valued concept to address the problem of uncertainty [39]. Instead of computing an activation degree of a hidden node to a sample, the cloud-based hidden node enumerates the activation degree of a sample to all intervals in a local region on-the-fly. This results in local density information, which fully reflects real data distributions. This concept was defined in AnYa [23], [40] and was patented in [27]. This concept is also the underlying component of AutoClass and TEDA-Class [41]. All of which come from Angelov’s sound work of RDE [40]. This paper aims to modify these prominent works to the\ninterval-valued case. Suppose that iN is the number of intervals associated with i-th data cloud, an activation degree of i-th cloud-based hidden node refers to its local density estimated recursively using the Cauchy function as follows:\n \n \n iN\nk i\ntk\nspatiali\nN\nxx G\n1\n2\n,\n) ~ (1\n1~ (3)\n],[ ~\n],,[~ ,,,,,, spatialispatialispatialiikikik GGGxxx \nwhere kx ~ is k-th interval in the i-th data cloud and tx is t-th data sample. It is observed that Eq. (3) requires the presence of all data points seen so far, which is impossible when dealing with data streams. Its recursive form is formalised in [40] and is generalized here to the interval-valued problem:\n2 ,, 2 ,\n,\n2\n,,\n2\n,\n,\n1\n1\n, 1\n1\ni i i\ni i i\nNiNiNit\nT t\nspatiali\nNiNiNit\nT\nt\nspatiali\nxA\nG\nxA G\n\n\n\n\n \n(4)\nwhere ii  , are upper and lower local means of the i-th cloud:\niii i\niNi\nNi i\ni\nNi x\nN\nx\nN\nN i\nii\n   \n  1,1,\n,\n1,, ,)\n1 (  ,\niii\nki\niki\nNi\ni\ni Ni x\nN\nx\nN\nN ii\n   \n  1,1, ,\n, 1,, ,) 1 (  (5)\nwhere i is an uncertainty factor of the i-th cloud, which determines a footprint of the uncertainty of the i-th cloud-based hidden node. The uncertainty factor creates an interval of the data cloud, which controls the degree of tolerance for uncertainty. It is worth noting that a data sample is considered as a population\nof the i-th cloud when resulting in the highest density. Moreover, ii Ni Ni ,, , are the upper and lower mean square lengths of the data vector in the i-th cloud as follows:\niii\ni\niNi Ni\nki\nki Ni\niii\ni\niNi\nNi\ni\ni Ni\nx N\nx\nN\nN\nx N\nx\nN\nN\ni\nii\ni\nii\n    \n    \n\n\n2\n1,1,\n2\n, 1,\n,\n, ,\n2\n1,1,\n2\n,\n1,,\n,) 1 (\n,) 1\n( (6)\nIt can be observed from Eq. (3) that the cloud-based hidden node does not have any specific shape and evolves naturally according to its supports. Furthermore, it is parameter-free, where no parameters – centroid, width, etc. as encountered in the conventional hidden node need to be assigned. Although the concept of the cloud-based hidden node was generalized in TeDaClass [42] by introducing the eccentricity and typicality criteria, the interval-valued idea is uncharted in [42]. Note that the Cauchy function is a first order approximation of the Gaussian function [43] and is asymptotically a Gaussian-like function, complying with the integreable requirement of the RVFLN to be a universal approximator.\nUnlike conventional RNNs, pRVFLN adopts the notion of the functional link neural network using the Chebyshev polynomial. This concept offers a direct connection from the input layer to the output layer with an enhancement layer, consistent with the original concept of the RVFLN. Note that recently developed RVFLNs in the literature mostly neglect the direct connection because they are designed with a zero-order output node [12]-[22]. The direct connection expands the output node to a higher degree of freedom, which aims to improve the local mapping aptitude of the output node. The direct connection\nproduces the extended input vector ex and is defined as the Chebyshev polynomial up to the second order:\n)()(2)( 11 jnjnjn xxxx    (7)\nwhere 12)(,)(,1)( 2\n210  jjjjj xxxxx  . Suppose that we deal with a two-dimensional input space\n],[ 21 xxX  , the extended input vector is expressed as the Chebyshev polynomial up to the second order )](,),(,,1[ 222121 xxxxxe  . Note that the term 1 here represents an intercept of the output node. This avoids\nan output node from going through origin, which may risk an untypical gradient. There exist other functions as well in functional-link neural networks: trigonometric and polynomial, power [68]. The Chebyshev function, however, scatters fewer parameters to be stored into memory than the trigonometric function, while the Chebyshev function has better mapping capability than other polynomial functions of the same order [68]. In addition, the polynomial power function is not robust against an extrapolation case. pRVFLN implements the random learning concept of the RVFLN, in which all parameters, namely the input weight A , design factor q , recurrent link weight  , and uncertainty factor , are randomly\ngenerated. Only the weight vector iw needs to be adjusted. Since the hidden node is parameter-free, no randomization takes place for hidden node parameters. The network structure of pRVFLN and the interval-valued data cloud are depicted in Fig. 1 and 2 respectively.\nIII. Learning Policy of pRVFLN\nThis section discusses the learning policy of pRVFLN. Section 3.1 outlines the online active learning strategy, which deletes inconsequential samples. Samples, selected in the sample selection mechanism, triggers the learning process to update the network architecture. Section 3.2 deliberates the hidden node growing strategy of pRVFLN. Section 3.3 elaborates the hidden node pruning and recall strategy, while Section 3.4 concerns the online feature selection mechanism. Section 3.5 explains the parameter learning scenario of pRVFLN. Algorithm 1 shows the pRVFLN learning procedure."
    }, {
      "heading" : "3.1 Online Active Learning Strategy",
      "text" : "The active learning component of the pRVFLN is built on the extended sequential entropy (ESEM) method, which is derived from the SEM method [29]. The ESEM method makes use of the entropy of the neighborhood probability to estimate the sample contribution. There exist at least three salient facets that distinguish the ESEM from its predecessor [29]:1) it forms an online version of the SEM; 2) it is combined with the concept of the data cloud, which accurately represents the local density; and 3) it handles regression as well as classification because it is free from the presence of true class label. It is worth noting that the vast majority of existing metacognitive learning variants are designed for classification problems only, and measure sample contribution on the decision surface. To the best of our knowledge, only Das et al. [44] address the regression problem, but it still shares the same principle as its predecessors, exploiting the hinge cost function to evaluate sample contribution [25]. The concept of neighborhood probability refers to the probability of an incoming data stream sitting in the existing data clouds, which\ncan be defined as follows:\n\n\n \n R\ni\nN\nk i\nkt\nN\nk i\nkt\nit i\ni\nN\nxXM\nN\nxXM\nNXP\n1 1\n1\n),(\n),(\n)( (8)\nwhere XT is a newly arriving data point and xn is a data sample, associated with the i-th rule. M(XT,xk) stands for a similarity measure, which can be defined as any similarity measure. The bottleneck problem of (8) is however caused by its requirement to revisit already seen samples. This issue can be tackled by formulating the recursive expression of (8). Instead of formulating the recursive definition of (8), the spatial firing strength of the data cloud suffices to be an alternative because it is derived from the idea of local density and is computed based on the local mean [23] which summarizes the characteristic of data streams. (8) is then written as follows:\n  \n \nR\ni\ni\ni it NXP\n1\n)( (9)\nwhere i is a type-reduced activation degree spatialispatialii GqGq ,,)1(  . Once finding the neighbourhood probability, its entropy can be calculated as follows:\n \n R\ni ititt NXPNXPXH 1\n)(log)()( (10)\nThe entropy of the neighbourhood probability measures the uncertainty of the existing network structure in addressing a training pattern. A sample with high uncertainty should be admitted for the model update, because it cannot be well-covered by an existing network structure. Learning such a sample is beneficial, because it minimises uncertainty. A sample is to be accepted for model updates, provided that the following condition is met:\nH (11)\nwhere  is an uncertainty threshold. This parameter is not fixed during the training process, rather it is dynamically adjusted to suit the learning context. This strategy is necessary to compensate for the potential increase in the number of data points to be accepted in the presence of concept drift. The\nthreshold is set as )1(1 sTT   , where it augments )1(1 sTT   when a sample is admitted for the training process, whereas it decreases )1(1 sTT   when a sample is ruled out for the training process. s here is a step size, set at s=0.01. This follows its default setting in [68]. 3.2 Hidden Node Growing Strategy\npRVFLN relies on the T2SCC method to generate its interval-valued data clouds on demands, which extends the so-called SCC method [32], [68] to be well-suited with the type-2 hidden node working framework. The significance of the hidden nodes in pRVFLN is evaluated by checking its input and output coherence through analysis of its correlation to existing data clouds and the target concept. Let\nn iii  1],[~  is a local mean of the i-th interval-valued data cloud (5), ntX  is an input vector and m tT  is a target vector, the input and output coherence are written as follows:\n),(),()1(),~( tititiC XqXqXI   (12)\n),(),()1(),~(\n)),,~(),((),~(\ntititi\ntitttiC\nTqTqT\nTTXXO     (13)\nwhere () is the correlation measure. Both linear and non-linear correlation measures are applicable here.\nHowever, the non-linear correlation measure is rather hard to deploy in the online environment, because it is usually executed through the Discretization or Parzen Window method [45]. This often leads to an assumption of uniform data distribution as implemented in the differential entropy [46]. The Pearson correlation measure is a widely used correlation measure but it is invariant to the scaling and translation of variables as well as being sensitive to rotation. The maximal information compression index (MCI) is one of attempts to tackle these problems and is used in the T2SCC to perform the correlation measure () [47]. It is defined as follows:\n))),(1)(var()var(4))var()(var(\n)var()(var( 2\n1 ),(\n22 BABABA\nBABA\n\n\n\n (14)\n)var()var(\n),cov( ),(\nBA\nBA BA  (15)\nwhere (A,B) are to be substituted by ),(),,(),,(),,(),,(),,( tttititititi TXXTTXX  .\n),(),,cov(),var( BABAA  respectively stand for the variance of A, covariance of A and B, and Pearson\ncorrelation index of A and B. The local mean of the interval-valued data cloud is used to represent a data cloud because it captures the influence of all intervals of a data cloud. In essence, the MCI method studies the eigenvalue of the normal direction to the principle component of (A,B), which indicates the amount of information compression. The principal component direction is referred to here, because it signifies the maximum information compression, resulting in maximum cost to be imposed when ignoring a datum. The MCI method features the following properties: 1) ))var()(var(5.0),(0 BABA   , 2) a maximum\ncorrelation is given by 0),( BA , 3) a symmetric property ),(),( ABBA   , 4) the mean expression is\ndiscounted, making it invariant against the translation of the dataset, and 5) it is also robust against rotation, which is verifiable from the perpendicular distance of a point to a line, and is unaffected by the rotation of the input features.\nThe input coherence explores the similarity between new data and existing data clouds directly, while the output coherence focusses on their dissimilarity indirectly through a target vector as a reference. The input and output coherence defines a test that determines the degree of confidence in the current’s hypothesis:\n21 ), ~(,),~(   tiCtiC XOXI (16)\nwhere ]1.0,01.0[],01.0,001.0[ 21   are predefined thresholds. If a hypothesis meets both conditions, a new training sample is associated with a data cloud with the highest input coherence i*. Accordingly, the number of intervals Ni*, local mean and square length ** ~ ,~ ii  are updated respectively with (5) and (6) as well as Ni*=Ni*+1. A new data cloud is introduced, provided that none of the existing hypotheses pass the test (16) – one of the conditions is violated. This situation reflects the fact that a new training pattern conveys significant novelty, which has to be incorporated to enrich the scope of the current hypotheses. Note that when a larger α1 is specified, fewer data clouds are generated and vice versa, whereas when a larger α2 is specified, larger data clouds are added and vice versa. The sensitivity of these two parameters are studied in the other section of this paper. Because a data cloud is a non-parametric model, no parameterization is committed for a new data cloud. On the other hand, the output node of a new data cloud is initialised:\nIWW RiR   1*1 , (17)\nwhere 510 is a large positive constant and 1R is the output covariance matrix of a new data cloud. The output node is set as the data cloud with the highest input coherence because this data cloud is the closest data cloud to the new one. Furthermore, the output covariance matrix is assigned as a large positive definite matrix, because this setting can produce a close approximation of the global minimum solution of batched learning, as proven mathematically in [48]."
    }, {
      "heading" : "3.3 Hidden Node Pruning and Recall Strategy",
      "text" : "pRVFLN is equipped with a data cloud pruning scenario, termed the type-2 relative mutual information (T2RMI) method. This method was firstly developed in [31] and extended in [16] to adapt to the intervalvalued working principle. This method is convenient to use in pRVFLN because it estimates mutual information between a data cloud and a target concept by analysing their correlation. Hence, the MCI method (14) - (15) can be applied to measure the correlation. Although this method has been wellestablished for the type-1 fuzzy system [31], and even type-2 fuzzy system [16], its effectiveness in handling data clouds and a recurrent structure as implemented in pRVFLN is to date an open question. Unlike both the RMI method [36] and T2RMI method [16] that apply the classic symmetrical uncertainty method, the T2RMI method is formulated using the MCI method as follows:\n),()1(),(), ~\n( ,,, ttempittempittempi TGqTGqTG   (18)\nwhere tempiG , is a lower temporal activation function of the i-th rule. The temporal activation function is\nincluded in (17) rather than the spatial activation function in order for the inter-temporal dependency of the recurrent structure to be considered. The MCI method is chosen here, because it offers a good tradeoff between accuracy and simplicity. It possesses significantly lower computational burden than the\nsymmetrical uncertainty method even when implemented with the differential entropy [46] but is more robust than a widely used Pearson correlation index. A data cloud is deemed inconsequential and thus is able to be pruned without significant loss of accuracy, provided that the following condition is met:\n)(2)( iii stdmean   (19)\nwhere )(),( ii stdmean  stand for the mean and standard deviation of the MCI during its lifespan. This criterion aims to capture an obsolete data cloud, which is no longer relevant to current data distribution due to possible concept drift, because it computes the downtrend of the MCI during its lifespan. It is worth mentioning that mutual information between hidden nodes and the target variable is a reliable indicator for changing data distributions, because it is in line with the definition of concept drift. Concept drift\nrefers to a situation, where the posterior probability changes overtime )()( 11  tttt XTPXTP .\nThe T2RMI method also functions as a rule recall mechanism, which is capable of coping with cyclic concept drift. Cyclic concept drifts frequently happen in the weather, customer preference, electricity power consumption data streams, etc. and is often related to seasonal change. This points to a situation where previous data distribution reappears again in the current training step. Once pruned by the T2RMI, a data cloud is not forgotten permanently and is inserted into a list of pruned data clouds R*=R*+1, where R* is the number of pruned data clouds. In this case, its local mean, square length, population, an output node, and output covariance matrix ***** ,,, ~ ,~ RRRRR N   are retained in the memory. Such data clouds can be reactivated again in the future, whenever its validity is confirmed by an up-to-date data trend. It is worth noting that adding a completely new data cloud when observing previously learned concept violates the notion of an evolving learner and catastrophically erases learning history. A data cloud is recalled subject to the following condition:\n)(max)(max ,..,1 * *,..,1* i Ri i Ri    (20)\nThis situation reveals that a previously pruned data cloud is more relevant than any existing ones. This condition pinpoints that a previously learned concept may reappear again. A previously pruned data cloud is then regenerated as follows:\n*1*1*1*1*1 ,,, ~~ ,~~ RRRRRRRRRR NN    (21)\nNote that although previously pruned data clouds are retained in the memory, the data cloud pruning module still contributes to lower the computational load, because all previously pruned data clouds are excluded from any training scenarios except (17). Unlike its predecessors [34], this rule recall scenario is independent of the data cloud growing process (please refer to Algorithm 1). It plays a role as another data cloud generation mechanism. This mechanism is also developed from the T2RMI method, which can represent the change of posterior probability – concept drift - more accurately than the density concept. 3.4 Online Feature Selection Strategy\nAlthough feature selection and extraction problems have attracted considerable research attention; little effort has been paid toward online feature selection. Two common approaches to tackling this issue are through soft or hard dimensionality reduction techniques [30],[34]. Soft dimensionality reduction minimizes the effect of inconsequential features by assigning low weights but still retains a complete set of input attributes in the memory, whereas hard dimensionality reduction lowers the input dimension by cutting off spurious input features. Nonetheless, the hard dimensionality reduction method compromises stability, because an input feature cannot be retrieved once pruned [43]. To the best of our knowledge, existing work always starts the input selection process from a full set of input attributes and gradually reduces the number as more observation are come across. A prominent work, namely online feature selection (OFS), was developed in [40] and covers both partial and full input conditions. The appealing trait of OFS lies in its aptitude for flexible feature selection, because it enables the provision of different combinations of input attributes in each episode by activating or deactivating input features (1 or 0), which adapts to up-to-date data trends. Furthermore, this technique is also capable of handling partial input attributes which, to the best of our knowledge, is uncharted by any existing work. OFS was originally devised for the LR and is generalized here to suit the working principle of pRVFLN.\nWe start our discussion from a condition where a learner is provided with full input variables. Suppose that B input attributes are to be selected in the training process and B<n, the simplest approach is to\ndiscard the input features with marginal accumulated output weights   \nR\ni j\nji\n1\n2\n1\n, and maintain only B input\nfeatures with the largest output weights. Note that the second term \n2\n1j\nis required, because pRVFLN\nemploys the Chebyshev polynomial up to the second order. The rule consequent informs a tendency of a rule which can be used as an alternate of gradient information which changes in each point [35]. Although it is straightforward to use, it cannot ensure the stability of the pruning process due to a lack of sensitivity analysis of the feature contribution. To correct this problem, a sparsity property of the L1 norm can be analyzed to examine whether the values of n input features are concentrated in the L1 ball. This allows the distribution of the input values to be checked to determine whether they are concentrated in the largest elements and that pruning the smallest elements won’t harm the model’s accuracy. This concept is actualized by first inspecting the accuracy of pRVFLN. The input pruning process is carried out when the\nsystem error is large enough  tt yT or, in the realm of the classification problem, misclassification is made. Nevertheless, the system error is not only large in the case of underfitting, but also in the case of overfitting. We modify this using the global mean and standard deviation of the system error and set the\nfollowing condition 11   tttt ee  . The constant  is a predefined parameter and fixed at 1.1 for\nsimplicity. The output nodes are updated using the gradient descent approach and then projected to the L2 ball to guarantee a bounded norm. Algorithm 2 details the algorithmic development of pRVFLN.\nAlgorithm 2. GOFS using full input attributes\nInput : α learning rate, χ regularization factor, B the number of features to be retained\nOutput: selected input features B selectedtX  1,\nFor t=1,…., T\nMake a prediction ty\nIF 111.1   tttt ee  // for regression or\nmo to Tyo ,..,1 )max(ˆ \n // for classification\ni\niii\nE\n  \n  , i\ni\ni  \n  )\n1\n,1min(\n2\n\nPrune input attributes tX except those of B largest  \nR i j ji 1 2 1 ,\nElse\niii  \nEnd IF"
    }, {
      "heading" : "End FOR",
      "text" : "where  , are respectively the learning rate and regularization factor. We assign 01.0,2.0  \nfollowing the same setting as [40]. The standard mean square error (MSE) is deployed as the cost function, which leads to the following gradient term:\n            \nR\ni\nR\ni\ntemporalitemporalitt\ni\nGqGqyT E\n1 1\n,,)1()( \nFurthermore, the system error has been theoretically proven to be bounded in [35] and the upper bound is also found. One can also notice that the GOFS enables different feature subsets to arrive at each training observation t.\nA relatively unexplored area of existing online feature selection is a situation where a limited number of features is accessible for the training process. To actualise this scenario, we assume that at most B input variables can be extracted during the training process. This strategy, however, cannot be done by simply acquiring any B input features, because this results in the same subset of input features during the training process. This problem is addressed using the Bernaoulli distribution with confidence level to sample B input attributes from n input attributes B<n. The detailed steps of the proposed feature selection are illustrated in Algorithm 3.\nAlgorithm 3. GOFS using partial input attributes\nInput : α learning rate, χ regularization factor, B the number of features to be retained,  confidence level\nOutput: selected input features B selectedtX  1,\nFor t=1,…., T\nSample  from Bernaoulli distribution with confidence level\nIF 1t\nRandomly select B out of n input attributes B tX  1\n~\nEnd IF\nMake a prediction ty\nIF 111.1   tttt ee  // for regression or\nmo to Tyo ,..,1 )max(ˆ \n // for classification\n)1()/( ~ˆ   nBXX tt\ni\nt iii\nXE\n  \n  )ˆ( , i\ni\ni  \n  )\n1\n,1min(\n2\n\nPrune input attributes tX except those of B largest  \nR\ni j\nji\n1\n2\n1\n,"
    }, {
      "heading" : "Else",
      "text" : "1,,  titi \nEnd IF"
    }, {
      "heading" : "End FOR",
      "text" : "As with Algorithm 2, the convergence of this scenario has been theoretically proven and the upper bound is found in [40]. One must bear in mind that pruning process in Algorithm 1, 2 are carried out by assigning crisp weights (0 or 1), which fully reflects the importance of input features. 3.5 Random Learning Strategy\npRVFLN adopts the random parameter learning scenario of the RVFLN leaving only the output nodes\nW to be analytically tuned with an online learning scenario, whereas others, namely ,,, qAt , can be randomly generated without any tuning process. To begin the discussion, we recall the output expression of pRVFLN as follows:\n \n R\ni tttemporaliio qAXGy 1\n, ),,,;( ~  (22)\nNote that the pRVFLN possesses a direct link from an input layer to the output layer iei wx as with the original RVFLN [49]. Referring to the RVFLN theory, the hidden node spatialiG , ~ and its derivative must be\nintegrable:\n  RR\ndxxGordxxG 22 )]([,)( (23)\nFurthermore, the number of hidden nodes R should be large enough to guarantee sufficient coverage of data samples. This strategy is to anticipate that the hidden nodes are not placed in the dense region of the\ninput space because they are chosen at random [50]. Nevertheless, this condition can be relaxed in the pRVFLN, because the data cloud growing mechanism, namely the T2SCC method, partitions the input region in respect to real data distributions. The concept of data cloud-based neurons comes into the picture with the concept of local density, which adapts to any variation of data streams. Furthermore, this concept is parameter-free and thus does not require any parameterization, which avoids deploying hidden nodes\nat random. Other parameters, namely ,,, qAt , are randomly chosen, and their region of randomisation should be carefully selected. Referring to Ingelnik and Pao [51], the scope of random parameters can be determined as follows:\n  \n  \n\n\n\n\n\n\n\n\n\nwb\nnn\nw\nd\nd\n]2,2[\n]1,0[\n],[],0[\n(24)\nwhere α,Ω,µ are probability measures. Nevertheless, this strategy is impossible to implement in the online real-time learning scenario, because it often entails a rigorous trial-error process to determine these parameters. Most RVFL works simply follow Schmidt et al.’s strategy [52], setting the region of random parameters in the range of [-1,1].\nAssuming that a complete dataset )(],[ mnNTXD  is accessible, a closed-form solution of (22)\ncan be defined to determine the output weightsW as follows:\nTGW temporal \n~ (25)\nwhere NRntemporaletemporal GxG   )12(\n~~ is a Moore-Penrose generalized inverse matrix. Although the\noriginal RVFLN adjusts the output weight with the conjugate gradient (CG) method, the closed-form solution is applicable, provided that matrix inversion with the use of a pseudo-inverse is feasible [49]. Note that the regularization technique needs to be undertaken if the Moore-Penrose inverse matrix is illconditioned. Although it is easy to use and ensures a globally optimum solution, this parameter learning scenario is however intractable for the online learning scenario, because it imposes revisiting preceding training patterns. The pRVFLN makes use of the fuzzily weighted generalized recursive least square (FWGRLS) method [35], which forms a local learning version of the GRLS [34]. The salient property of this method over its predecessor, namely the FWRLS method [43], is a generalized weight decay function, which aims to improve the model’s generalization. Because the FWGRLS approach has been detailed in [34], we do not recount it here."
    }, {
      "heading" : "3.6 Robustness of RVFL Network",
      "text" : "It is a common practice in the literature to randomly sample the network parameters from a predefined distribution. A uniform distribution within a range of [-1,1] is generally adopted. A new finding of Li and Wang in [65] shows that the process of randomly generating network parameters with a fixed scope [-α,\n1) https://www.dropbox.com/s/2p5vtpx0t0a8uwx/supplemental_document.pdf?dl=0 2) http://homepage.cs.latrobe.edu.au/dwang/html/DNNEweb/index.html 3) http://ispac.ing.uniroma1.it/scardapane/software/lynx/.\nα] does not ensure a theoretically feasible network or often the hidden node matrix is not full rank. Surprisingly, the hidden node matrix was not invertible in all their cases when randomly sampling network parameters in the range of [-1,1] and far better numerical results were achieved by choosing the scope [-200,200]. This trend was consistent with different numbers of hidden nodes. How to properly select scopes of random parameters and its corresponding distribution still require in-depth investigation [66]. In practice, many trials and cross-validation are required to arrive at a good projection space. Note that the range of random parameters by Ingelnik and Pao [51] is still at the theoretical level and does not touch implementation issue. In section IV.C, we study how the ranges of random regions affect the learning performance of the pRVFLN and furthermore answer a question how the region of random parameters should be determined.\nThis section elaborates the proof of concepts of our proposed algorithm through simulations using five case studies and comparisons with prominent algorithms in the literature. Two numerical examples, namely modelling of Nox emissions from a car engine, tool wear prediction of a high-speed milling process, are presented in this section, and the other three numerical examples, namely the prediction of the S&P 500 index time series, the appraisal of residential premise price and the prediction of household electricity power consumption, are provided as supplemental materials1) to keep the paper compact. Our numerical studies were carried out under two scenarios: the time-series scenario and the cross-validation scenario. The time-series procedure orderly executes data streams according to their arrival time and simulates the training and testing process in the online real-time mode. In the time-series mode, pRVFLN was compared against 10 state-of-the-art evolving algorithms: eT2Class [54], eTS [55], simp_eTS [56], PANFIS [57], GENEFIS [58], DFNN [59], GDFNN [60], FAOSPFNN [61], BARTFIS [62], ANFIS [63], eTS+ [43]. The CV scenarios were taken place in our experiment in order to follow the same\nsimulation environment of other RVFLNs in the literature. The RT2McRVFLN was benchmarked against 8 prominent RVFLNs in the literature: decorelated neural network ensemble (DNNE) [21], centralized RVFL (A) [22], RLS-consensus-RVFL [22] (B), RLS-local-RVFL (C) [22], LMS-concensus-RVFL (D) [22]. We run both online and batch versions of (A), (B), (C), whereas (D) has its batch version only. The MATLAB codes of these algorithms are available online 2,3). Comparisons were performed against five evaluation criteria: accuracy, the number of hidden nodes, the number of training samples, the number of network parameters, the number of input attributes, and execution time. The scope of random parameters followed Schmidt's suggestion [52] where the scope of random parameters was [-1,1] but we insert analysis of robustness in part C which provides additional results with different random regions. The contribution of each learning component to the overall learning performance and the sensitivity against predefined thresholds are also analysed in Section D and E respectively to verify impact of each learning component and to show that predefined thresholds in pRVFLN are not troublesome to set. To allow a fair comparison, all consolidated algorithms were run under the same computing environment: Microsoft Surface Book with an Intel (R) Core (TM) i7–2600 CPU, 3.4 GHz processor, and 8 GB memory.\nA. Modeling of Nox Emissions from a Car Engine: this section demonstrates the efficacy of the pRVFLN in modeling Nox emissions from a car engine [53]. This real-world problem is relevant to validate the learning performance, not only because it features noisy and uncertain characteristics as the nature of a car engine, but also it characterizes high dimensionality, containing 170 input attributes. That is, 17 physical variables were sampled in 10 consecutive measurements. Furthermore, this problem also contains changing system behaviors, because two important attributes of engine control, namely rotation speed and torque, were varied to represent different driving behaviors under different road conditions: normal, country, hilly. In the time-series procedure, 826 data points were streamed to consolidated algorithms, where 667 samples were used to evolve the model, and the remainder were fed for testing purposes. In the CV procedure, the experiment was run under the 10-fold CV scheme, and each fold was repeated five times similar to the scenario adopted in [22]. This strategy checked the consistency of the RVFLN’s learning performance because it adopts the random learning scenario and avoids data order dependency. The results are reported in Table 1 and 2.\nIt is evident that pRVFLN outperformed its counterparts in all the evaluation criteria except GENEFIS for the number of input attributes and network parameters. It is worth mentioning however that in the other three criteria: predictive accuracy, execution time, and a number of training samples, the GENEFIS was more inferior than ours. pRVFLN is equipped with the what-to-learn strategy, which was capable of lowering the number of training samples. This learning module had a significant effect of predictive accuracy. Furthermore, pRVFLN has the GOFS method, which was capable of coping with the curse of dimensionality. Note that the unique feature of the GOFS method is that it allows different feature subsets to arrive in every training episode. This case avoids the catastrophic forgetting of obsolete input attributes,\ntemporarily inactive due to changing data distributions. The GOFS can handle partial input attributes during the training process and resulted in the same level of accuracy as that of the full input attributes. The use of full input attributes slowed down the execution time because it needed to deal with 170 input variables first, before reducing the input dimension. In this case study, we selected five input attributes to be kept for the training process. Our experiment showed that it is not problem-dependent and can be set as any desirable number. There was no significant performance difference when using either the full input mode or partial input mode. On the other hand, consistent numerical results were achieved by pRVFLN, although the pRVFLN is built on the random vector functional link algorithm, as observed in the CV experimental scenario. In addition, pRVFLN produced the most encouraging performance in five evaluation criteria: NRMSE, the number of hidden nodes, the number of input attributes, the number of parameters and number of training samples. pRVFLN was slower in computational speed than the other RVFLNs because the other RVFLNs, (A)-(D), implement a simpler training procedure than pRVFLN by only fine-tuning output weights without any structural learning and feature selection mechanisms.\nB. Tool Wear Prediction of High-Speed Milling Process: this section presents a real-world problem, taken from a complex manufacturing problem [64]. The objective of this case study is to monitor the degradation of a ball nose end milling cutter during the dry machining of hardened tool steel with a hardness of 52-54 HRc (Courtesy of Dr. Li Xiang, Singapore). It is worth mentioning that tool wear prediction of a complex manufacturing plant remains a very complex issue for both academia and industry because of the multi-point cutting tools operating at high speeds, non-stationary machining parameters, and the inconsistency and variability of cutter geometry/ dimensions. Our experiment was performed using a CNC milling process (Rőders Tech RFM760) with a spindle rate up to 42000 RPM. Raw data were recorded with three sensors, namely the dynamometer, accelerometer, and acoustic emission (AE) sensor, measuring force signal, vibration signal, and AE signal respectively in three-dimensional cutting\naxes. These sensors were mounted onto a seven channels DAQ connected to the main computer in the station room, where the data pre-processing step took place. In total, 12 features were extracted from the force signal and data were collected using two different cutter profiles. The force signal was used because it is widely known to convey the most informative indicator of the tool wear, while the tool wear itself was evaluated through visual inspection with an Olympus SZX16 microscope. Concept drift in this case study was induced by changing surface integrity as well as tool wear degradation. Furthermore, we utilized two different cutter profiles for the experiment, inducing different machining environments. A total of 630 data samples were generated and the 12 input features are time-domain features: kurtosis, absolute mean amplitude, crest factor, etc. For the time-series experimental procedure, the consolidated algorithms were trained using data from cutter A, while the testing phase exploited data from cutter B. For the CV experimental procedure, all data were put together, and then the 10-fold CV procedure was undertaken. Each fold was repeated five times, and the numerical results were calculated from the average numerical results across all folds. Tables 3 and 4 report the numerical results of the consolidated algorithms. Fig. 3 depicts the frequency of the input attributes selected in the training process.\nIt is observed from Table 2 and 3 that pRVFLN evolved the most compact and parsimonious network structure while retaining a very high level of accuracy. It is worth noting that although the DNNE exceeded pRVFLN in accuracy, it imposed considerable complexity because it is an offline algorithm revisiting previously seen data samples and adopts an ensemble learning paradigm. The efficacy of the online sample selection strategy was seen, as it reduced the number of training samples to half of the total data streams. Using partial input information led to subtle differences to those with the full input information. It is seen in Fig. 3 that the GOFS selected different feature subsets in every training episode and it reflected the importance of input variables to the training process. Additional numerical examples can be found in 1).\nC. Analysis of Robustness: this section aims to numerically validate our claim in section III. E that a range [-1,1] does not always assure a reliable model [65],[66]. Additional numerical results with different\nintervals of random parameters are presented. Four intervals, namely [0,0.1],[0,0.5] [0,0.8] and [0,5] were tried out for two case studies in section IV.A and IV.B. Our experiments were undertaken in the 10-folds CV procedure where each fold was repeated five times to prevent the effect of random sampling. Our experiment made use of the feature selection scenario of a full number of input attributes. Numerical results are tabulated in Table 5.\nIt is seen in Table 5 that the best-performing model was generated by the range [0,0.1]. The higher the range of the model the more inferior the model was. It went up to the point where a model was no longer stable under the range [0,5]. On the other side, the range [0,0.5] induced the best-performing model with the highest accuracy while evolving comparable network complexity for the Nox emission case study. The higher the scope led to the deterioration of numerical results. Moreover, the range [0,0.1] did not deliver a better accuracy than the range [0,0.5] either since this range did not generate diverse enough random values. These numerical results are interpreted from the nature of pRVFLN – a clustering-based algorithm. The success of pRVFLN is determined from the compatibility of zone of influence of hidden nodes to a real data distribution, and its performance worsens when the scope is remote from the true data distribution. This finding is complementary to Li and Wang [65] where it relies on a sigmoid-based RVFL network, and the scope of random parameters can be fully independent of the training data. Its predictive performance is set by its approximation capability in the output space. pRVFLN is regarded as a special case of RVFL network and is derived from the theory of basis function.\nD. Contributions of Learning Components: this section demonstrates the efficacy of each learning module of the pRVFLN and analyses to what extent this learning module contributes to the resultant learning performance. The experiment was undertaken using the Mackey-Glass time series problem, a control model of the production of white blood cells. This problem features the chaotic characteristic,\nwhose nonlinear oscillations are well-accepted as a model of most psychological processes. This problem is described by the following mathematical model:\n)( )(1\n)()( 10\ntbx tx\ntax\ndt\ntdx \n\n \n\n (26)\nwhere 1.0,2.0  ba and 85 . The chaotic characteristic is attributed by 17 . The nonlinear dependence\nof this problem is built upon the series-parallel identification model as follows:\n))18(),12(),6(),(()85(  txtxtxtxftx (27)\n3000 training data from the range of [201,3200] and 500 testing data from the range of [5001,5500] were generated using the fourth order Range-Kutta method. The effect of each learning component was investigated by studying the learning performance of pRVFLN under five learning configurations: A) this configuration refers to pRVFLN with a feedforward network architecture; B) the what-to-learn part is deactivated; C) we switch off the online feature selection; D) pRVFLN is executed with the absence of hidden node pruning and recall mechanisms; E) this part depicts pRVFLN with full input attributes. The numerical results are summarised in Table 6. As with previous case studies, the learning performance was examined against five learning criteria: NDEI, hidden nodes, execution time, training samples, and input attributes.\nIt is obvious from Table 6 that each learning module played a critical role in the learning performance of pRVFLN. Without the recurrent connection, the predictive accuracy of pRVFLN slightly deteriorated and this also increased the number of training samples to be seen in the training process. The difference in performance was negligible and imposed the algorithm to see all samples when the what-to-learn scenario was shelved for the training process. This fact substantiates the efficacy of the what-to-learn scenario in extracting important data points for the training process. The absence of the hidden node pruning mechanism triggered the increase of hidden nodes to be evolved during the training process but only affected little the predictive accuracy. Moreover, pRVFLN was capable of learning data streams with partial input information as well as with full input information.\nE. Sensitivity Analysis of Predefined Thresholds: This section examines the impact of two predefined\nthresholds, namely 21, , on the overall learning performance of pRVFLN. Intuitively, one can envisage that the higher the value of α1, the fewer the data clouds are added during the training process and vice versa, whereas the higher the value of α2, the higher the number of data clouds are generated. To further confirm this aspect, the sensitivity of these parameters is analysed using the box Jenkins (BJ) gas furnace problem. The BJ gas furnace problem is a popular benchmark problem in the literature, where the goal is\nto model the CO2 level in off gas based on two input attributes: the methane flow rate )(nu , and its\nprevious one-step output )1( nt . From the literature, the best input and output relationship of the regression\nmodel is known as ))1(),4(()(ˆ  ntnufny . 290 data points were generated from the gas furnace, 200 of which\nwere assigned as the training samples, and the remainder were utilised to validate the model. 1 was varied in the range of [0.002,0.004,0.006,0.008], while 2 was assigned these values [0.02,0.04,0.06,0.08]. Two tests were carried out to test their sensitivity. That is, α1was fixed at 0.002, while setting different values of α2, whereas α2 was set at 0.02, while varying α1. The learning performance of pRVFLN was evaluated against four criteria: non-dimensional error index (NDEI), number of hidden nodes, execution time, number of training samples, and number of network parameters. The results are reported in Table 7.\nReferring to Table 7, it can be observed that pRVFLN can achieve satisfactory learning performance while demanding very low network, computational, and sample complexities. Allocating different values\nof 21, did not cause significant performance deterioration, where the NDEI, runtime and the number of samples were stable in the range of [0.27,0.38], [0.5,0.79], and [10,30] respectively. Note that the slight variation in these learning performances was attributed to the random learning algorithm of pRVFLN. On the other hand, the number of hidden nodes and parameters remained constant at 2 and 10 respectively and were not influenced by a variation of the two predefined thresholds. It is worth mentioning that the data cloud-based hidden node of pRVFLN incurred modest network complexity, because it did not have\nany parameters to be memorised and adapted. In all the simulations in this paper, 21, were fixed at 0.006 and 0.04 respectively to attain a fair comparison with its counterparts and to avoid laborious pretraining step in finding suitable values for these two parameters.\nV. Conclusions This paper presents a novel random vector functional link network, namely Parsimonious Random Vector Functional Link Network (pRVFLN), inspired by the three issues of metacognition of human learning, namely what-to-learn, how-to-learn, and when-to-learn. pRVFLN addresses the four issues not addressed by its predecessors: uncertainty, concept drift, temporal system dynamics, and the curse of dimensionality. To this end, pRVFLN extends the scope of RVFL under a local recurrent network architecture and interval-valued data cloud. Unlike conventional neurons, the concept of interval-valued data clouds does not require any parameterization or any pre-specified shape. It features non-crisp values,\nwhich provides the degree of tolerance for uncertainty. pRVFLN also put forwards some new learning scenarios: T2SCC method, the rule recall method and the GOFS method. Rigorous case studies were carried out to numerically validate the efficacy of pRVFLN, showing that it outperformed the state-ofthe-art algorithms in both complexity and accuracy. Future work will be devoted to developing the ensemble version of pRVFLN to further improve the accuracy of the algorithms.\nVI. ACKNOWLEDGEMENT\nThe third author acknowledges the support of the Austrian COMET-K2 program of the Linz Center of Mechatronics (LCM), funded by the Austrian federal government and the federal state of Upper Austria. We thank Dr. D. Wang for his suggestion\npertaining to robustness of RVFLN.\nVII. REFERENCES [1] Haykin, S. and Network, N. (2004). Neural Networks: A comprehensive foundation. Neural Networks, 2(2004) [2] R. Setiono and L. C. K. Hui, “Use of quasi-Newton method in a feedforward neural network construction algorithm,” IEEE Transactions on Neural Networks, vol. 6, pp. 273–277, 1995 [3] S. Singhal and L. Wu, “Training feed-forward networks with the extended Kalman filter,” in Advances in Neural Information Processing, D. S. Tourezky, Ed. San Mateo, CA: Morgan Kaufman, 1989, pp. 133–140 [4] M. T. Hagan and M. B. Manhaj, “Training feedforward networks with the Marquardt algorithm,” IEEE Transactions on Neural Networks, vol. 5, pp. 989– 993, 1994 [5] D. Mackay, “A practical Bayesian framework for backpropagation networks,” Neural Computing., vol. 4, pp. 448–472, 1992. [6] T.Martin, “Fuzzy sets in the fight against digital obesity,” Fuzzy Sets and Systems, vol. 156, no. 3, pp. 411–417, 2005 [7] D. Wang, “ Editorial: Randomized algorithms for training neural networks”, Information Sciences, Vol. 364-365, pp. 126-128, 2016 [8] L. Zhang, P.N. Suganthan, “ A Survey of Randomized Algorithms for Training Neural Networks”, Information Sciences, Vol.364-365, pp. 146-155, 2016 [9] Pao, Y.-H. and Phillips, S. M, ” The functional link net and learning optimal control”, Neurocomputing, Vol.9(2), pp. 149–164, (1995) [10] Zhang, L. and Suganthan, P. N. A comprehensive evaluation of random vector functional link networks. Information Sciences, doi: http://dx.doi.org/10.1016/j.ins.2015.09.025 [11] Bakırcıo˘glu, H. and Koc¸ak, T, “Survey of random neural network applications. European Journal of Operational Research”, Vol. 126(2), pp. 319–330, 2000 [12] Barlett, P. L. and Downs, T, “ Using random weights to train multilayer networks of hard-limiting units”, IEEE Transactions on Neural Networks, Vol. 3(2), pp. 202–210, 1992 [13] A. Rubio-Solis, G. Panoutsos, “Interval Type-2 Radial Basis Function Neural Network: A Modeling Framework”, IEEE Transactions on Neural Networks and Learning Systems, Vol. 23(2), pp.457-473, 2015 [14] Berry, H. and Quoy,” Structure and dynamics of random recurrent neural networks”, Adaptive Behavior, Vol 14(2), pp. 129–137, 2006 [15] L. Xie, Y. Yang, Z. Zhou, M. Tao, Z. Man, “Dynamic neural modelling of fatigue crack growth process in ductile alloys”, Information Sciences, Vol.364365, pp.167-183, 2016 [16] M. Pratama, J.Lu, E. Lughofer, G. Zhang, S. Anavatti, \" Scaffolding Type-2 Classifier for Incremental Learning under Concept Drifts\", Neurocomputing, Vol.191, pp. 304-329, (2016) [17] Elissee, A. and Paugam-Moisy, H, ” JNN, a randomized algorithm for training multilayer networks in polynomial time”, Neurocomputing, Vol. 29(1), pp. 3–24, 1999 [18] F. Cao, Y. Tan, M. Cai, “ Sparse algorithms of Random Weight Networks and applications”, Expert Systems with Applications, Vol. 41, pp. 2457-2462, 2014 [19]F. Cao, H. Ye, D. Wang, “ A probabilistic learning algorithm for robust modeling using neural networks with random weights”, Information Sciences, Vol.313, pp. 62-78, 2015 [20] J. Zhao, Z. Wang, F. Cao, D. Wang, “ A local learning algorithm for random weights networks”, Knowledge-based Systems, Vol.74, pp. 159-166, 2015 [21] M. Alhandoosh, D. Wang, “ Fast decorrelated neural network ensembles with random weights”, Information Sciences, Vol.264, pp. 104-117, 2014 [22] S. Scardapane, D. Wang, M. Panella, A. Uncini, “ Distributed learning for Random Vector Functional-Link networks”, Information Sciences, Vol. 301, pp. 271-284, 2015 [23]P. Angelov, R. Yager, “ A new type of simplified fuzzy rule-based system”, pp. 1-21, 2011 [24] C.F. Juang and C.T. Lin, “A recurrent self-organizing neural fuzzy inference network,” IEEE Transactions on Neural Networks, vol.10, pp.828-845, 1999 [25] K. Subramanian, S. Suresh, N. Sundararajan, “A Meta-Cognitive Neuro-Fuzzy Inference System (McFIS) for sequential classification systems”, IEEE Transactions on Fuzzy Systems, vol.21, no.6, pp.1080-1095,(2013) [26] C. F. Juang, Y. Y. Lin, and C. C. Tu, “A recurrent self-evolving fuzzy neural network with local feedbacks and its application to dynamic system processing,” Fuzzy Sets and Systems, vol.161,no.19, (2010) [27] P.Angelov,Anomalous System State Identification, Patentgb1208542.9, Priority Date15May2012. [28] J.C. Patra, A.C. Kot, “ Nonlinear dynamic system identification using Chebyshev functional link artificial neural networks“, IEEE Transactions on Systems, Man and Cybernetics-Part B: Cybernetics, Vol.32,no.4, pp.505-511, (2002) [29] Xiong,S., Azimi, J., Fern, X.Z,” Active Learning of Constraints for Semi-Supervised Clustering”,IEEE Transactions on Knowledge and Data Engineering, Vol.26(1), pp. 43-54, 2014 [30] M. Pratama, J. Lu, E. Lughofer, G. Zhang, S. Anavatti, “Scaffolding type-2classifier for incremental learning under concept drifts”, Neurocomputing, Vol. 191, pp. 304-329, 2016 [31] H. Gan, et al., “Nonlinear Systems Modeling Based on Self-Organizing Fuzzy-Neural-Network with Adaptive Computation Algorithm “, IEEE Transactions on Cybernetics, Vol. 44(4), pp.554-564, (2014) [32] J-Y. Jiang, R-J. Liao, S-J. Lee, “ A Fuzzy Self-Constructing Feature Clustering Algorithm for Text Classification”, IEEE Transactions on Knowledge and Data Engineering, Vol. 23(3), pp. 335-349, 2011 [33] I.Y. Tyukin , D.V. Prokhorov , Feasibility of random basis function approximators for modeling and control, in: Proceedings of IEEE Conference on Control Applications & Intelligent Control, 2009, pp. 1391–1396 [34] M.Pratama, et al., “ pClass:An Effective Classifier to Streaming Examples”, IEEE Transactions on Fuzzy Systems, Vol.23(2), pp.369-386, 2014\n[35] J. Wang, P. Zhao, S. Hoi, R. Jin, “ Online Feature Selection and Its Applications”, IEEE Transactions on Knowledge and Data Engineering, Vol. 26(3), pp. 698-710, 2014 [36] C.F. Juang, “A TSK-type recurrent fuzzy network for dynamic systems processing by neural network and genetic algorithms,” IEEE Transactions on Fuzzy Systems, vol.10,no.2, pp.155-170, (2002) [37] Yang-Yin Lin, Jyh-Yeong Chang, Chin-Teng Lin,” Identification and Prediction of Dynamic Systems Using an Interactively Recurrent Self-Evolving Fuzzy Neural Network”, IEEE Transactions on Neural Networks and Learning Systems, Vol.24,no.2,pp.310-321, (2013) [38] R. H. Abiyev and O. Kaynak, “Type-2 fuzzy neural structure for identification and control of time-varying plants,” IEEE Transactions on Industrial Electronics , vol. 57, no. 12, pp. 4147–4159, 2010 [39]Q. Liang and J. M. Mendel, “Interval type-2 fuzzy logic systems: Theory and design,” IEEE Transactions on Fuzzy Systems, vol. 8, no. 5, pp. 535–550, 2000 [40] P.P. Angelov , Autonomous Learning Systems: From Data Streams to Knowledge in Real-time, John Wiley and Sons Ltd., 2012 [41] P.P. Angelov , D. Kangin , X. Zhou , D. Kolev ,Symbol recognition with a new autonomously evolving classifier autoclass, in: Proceedings of 2014 IEEE Conference on Evolving and Adaptive Intelligent Systems, EAIS, 2014, pp. 1–7 . [42] D. Kangin, P. Angelov, J. A. Iglesias, “ Autonomously evolving classifier TEDAClass”, Information Sciences, Vol. 366, pp. 1-11, 2016 [43] P. Angelov, Evolving Takagi-Sugeno Fuzzy Systems from Data Streams (eTS+), In Evolving Intelligent Systems: Methodology and Applications (Angelov P., D. Filev, N. Kasabov Eds.), John Willey and Sons, IEEE Press Series on Computational Intelligence, pp. 21-50, ISBN: 978-0-470-28719-4, April 2010 [44] Das, A.K., Subramanian, K., Suresh, S., “ An Evolving Interval Type-2 Neurofuzzy Inference System and Its Metacognitive Sequential Learning Algorithm. IEEE Transactions on Fuzzy Systems, 23(6), pp. 2080-2093, (2015) [45] E. Parzen,” On estimation of a probability density function and mode”, The Annals of Mathematical Statistics, Vol.33, 1065–1076, (1962) [46] A. Lazo, et al.,” On the entropy of continuous probability distributions”, IEEE Transactions on Information Theory, 24(1), 120–122, (1978) [47] P. Mitra, C.A. Murthy, S.K. Pal,” Unsupervised feature selection using feature similarity”, IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol.24(3), pp.301-312, (2002) [48] E. Lughofer, Evolving Fuzzy Systems --- Methodologies, Advanced Concepts and Applications, Springer, Heidelberg, 2011 [49] Y.H. Pao, G. H. Park, and D. J. Sobajic, “Learning and generalization characteristics of random vector functional-link net,” Neurocomputing, vol. 6, pp. 163-180, 1994 [50] I.Y. Tyukin , D.V. Prokhorov , Feasibility of random basis function approximators for modeling and control, in: Proceedings of IEEE Conference on Control Applications & Intelligent Control, 2009, pp. 1391–1396 [51] B. Igelnik, Y.H. Pao, “Stochastic choice of basis functions in adaptive function approximation and the functional-link net”, IEEE Transactions on Neural Networks, Vol.6(6), pp.1320-1329, 1995 [52] W. Schmidt, M. Kraaijveld, R. Duin, Feedforward neural networks with random weights, in Proceedings of 11th IAPR International Conference on Pattern Recognition Methodology and Systems, 1992, pp. 1-4 [53] E. Lughofer, V. Macian, C. Guardiola and E.P Klement, “ Identifying Static and Dynamic Prediction Models for NOx Emissions with Evolving Fuzzy Systems”, Applied Soft Computing, vol. 11(2), pp. 2487-2500, 2011 [54] M. Pratama, J. Lu, G.Zhang, “ Evolving Type-2 Fuzzy Classifier”, online and in press, IEEE Transactions on Fuzzy Systems, on line and in press, (2015) [55] P.Angelov and D. Filev, \"An approach to online identification of Takagi-Sugeno fuzzy models,\" IEEE Transactions on Systems, Man, and Cybernetics, Part B.vol. 34, pp. 484-498.(2004) [56] P.Angelov and D. Filev, \"Simpl_eTS: A simplified method for learning evolving Takagi-Sugeno fuzzy models,\" in IEEE International Conference on Fuzzy Systems (FUZZ), pp. 1068-1073.(2005) [57] M. Pratama, S. Anavatti, P. Angelov, E. Lughofer, PANFIS: A Novel Incremental Learning, IEEE Transactions on Neural Networks and Learning Systems, Vol.25, no.1, pp.55-68,(2014) [58] M.Pratama, S.Anavatti, E.Lughofer, “ GENEFIS:Towards An Effective Localist Network”, IEEE Transactions on Fuzzy Systems, Vol.25, no.3, pp.547562, (2014) [59] S. Wu, and M-J. Er, Dynamic fuzzy neural networks—a novel approach to function approximation, IEEE Transaction on Systems Man Cybernetics, part b: Cybernetics,vol.30,pp. 358–364,(2000) [60] S.-Q. Wu, M-J. Er, and Y. Gao, A fast approach for automatic generation dynamicof fuzzy rules by generalized dynamic fuzzy neural networks, IEEE Transaction on Fuzzy System,Vol.9(4),pp.578–594,(2003). [61] N.Wang, M.J.Er, and M.X, Fast and Accurate Self Organizing Scheme for Parsimonious Fuzzy Neural Network, Neurocomputing, Vol 72,(2009) [62] R. J. Oentaryo, et al., \"Online probabilistic learning for fuzzy inference systems,\" Expert Systems with Applications, vol. 41, no. 11, pp. 5082-5096, (2014) [63] J.-S. R. Jang, “ANFIS: Adaptive-network-based fuzzy inference system”, IEEE Transaction on System. Man. Cybernetic, part b: cybernetics, vol. 23, pp. 665–684,(1993) [64] M. Pratama, et al., \"Data driven modeling based on dynamic parsimonious fuzzy neural network,\" Neurocomputing, vol. 110, pp. 18-28, 2013 [65] M. Li, D. Wang, “ Insights into randomized algorithms for neural networks: Practical issues and common pitfalls”, Information Sciences, Vol. 382, pp. 170-178, (2017) [66] S. Scardapane, D. Wang, “ Randomness in Neural Networks: An Overview”, WIRE’s Data Mining and Knowledge Discovery, (2017) [67] D. S. Broomhead and D. Lowe, “Multivariable functional interpolation and adaptive networks,” Complex Systems, vol. 2, pp. 321–355, (1988) [68] R-F. Xu, S-J. Lee, “ Dimensionality Reduction by Feature Clustering for Regression Problems”, Information Sciences, Vol. 299, pp. 42-57, (2015)"
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "the theory of random vector functional link network (RVFLN) has provided a breakthrough in<lb>the design of neural networks (NNs) since it conveys solid theoretical justification of randomized<lb>learning. Existing works in RVFLN are hardly scalable for data stream analytics because they are inherent<lb>to the issue of complexity as a result of the absence of structural learning scenarios. A novel class of<lb>RVLFN, namely parsimonious random vector functional link network (pRVFLN), is proposed in this<lb>paper. pRVFLN features an open structure paradigm where its network structure can be built from scratch<lb>and can be automatically generated in accordance with degree of nonlinearity and time-varying property<lb>of system being modelled. pRVFLN is equipped with complexity reduction scenarios where<lb>inconsequential hidden nodes can be pruned and input features can be dynamically selected. pRVFLN<lb>puts into perspective an online active learning mechanism which expedites the training process and<lb>relieves operator’s labelling efforts. In addition, pRVFLN introduces a non-parametric type of hidden<lb>node, developed using an interval-valued data cloud. The hidden node completely reflects the real data<lb>distribution and is not constrained by a specific shape of the cluster. All learning procedures of pRVFLN<lb>follow a strictly single-pass learning mode, which is applicable for an online real-time deployment. The<lb>efficacy of pRVFLN was rigorously validated through numerous simulations and comparisons with state-<lb>of-the art algorithms where it produced the most encouraging numerical results. Furthermore, the<lb>robustness of pRVFLN was investigated and a new conclusion is made to the scope of random parameters<lb>where it plays vital role to the success of randomized learning.",
    "creator" : "Microsoft® Word 2016"
  }
}