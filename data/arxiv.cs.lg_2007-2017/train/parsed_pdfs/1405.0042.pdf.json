{
  "name" : "1405.0042.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "REGULARIZATION BY EARLY STOPPING FOR ONLINE LEARNING ALGORITHMS",
    "authors" : [ "LORENZO ROSASCO", "SILVIA VILLA" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 5.\n00 42\nv1 [\nst at\n.M L\n] 3\n0 A\npr 2\n01 4\nKey words. Online learning, incremental gradient descent, consistency\n1. Introduction. Early stopping is a widely used heuristic to achieve regularization in online algorithms for supervised learning, see e.g. [22]. However, its theoretical foundation is still poorly understood. While early stopping has been recently studied for classical gradient descent learning and related algorithms [41, 37, 3, 6, 8, 29], we are not aware of similar studies for online learning. Here, with online learning algorithms, we refer broadly to iterative procedures that have access, at each iteration, to the gradient of a loss function at a single input-output pair.\nWe consider the statistical learning theory framework and are interested in online learning algorithms for expected risk minimization. The situation typically analyzed in this context is the one where each iteration of the algorithm corresponds to a new input-output pair [20]. In this paper, we are interested in allowing the algorithm to do multiple passes over the data (epochs). Indeed, the effect of the number of epochs to the generalization performance of the algorithm is the subject of our study. The procedure we consider can be seen as the incremental gradient descent (IGD) defined by the empirical risk. However, unlike typical analyses of this method, see e.g. [25], we are not interested in convergence of the algorithm to the minimum of the empirical risk, but rather to the one of the expected risk. More precisely, we consider the least squares loss and a hypothesis space which is a (reproducing kernel) Hilbert space. In this setting the minimization of the expected risk is a convex, but not strongly convex, problem and the domain where we minimize is neither compact nor bounded. In particular, the gradients are not bounded, unlike in most studies in stochastic optimization [26]. The instance of IGD we consider is defined by an unpenalized empirical risk and the step size is fixed to a universal constant, so that the only free parameter in the algorithm is effectively the number of epochs. Indeed, we show that better generalization is achieved if multiple, although finitely many, passes over the data are considered. Our main results show that: 1) provided with a suitable stopping rule, the IGD algorithm is universally consistent, 2) finite sample bounds can be derived under a suitable smoothness assumption, and 3) model selection via hold-out cross validation, which is typically used in practice, can adaptively achieve the same generalization guarantees. A detailed discussion of previous works is deferred to Section 3.2. Here we note that, the results more closely related to the analysis we present are those in [2, 34, 38] where consistency and finite sample bounds are derived. In [2] a single pass over the data is shown to suffice, however the analysis is restricted to a finite dimensional setting, and the generalization performance is empirically shown to be still increasing after the first epoch. In [34] and [38] the step size and/or a penalization parameter need to be chosen in a distribution dependent way (or by cross validation) and multiple passes over\n∗ DIBRIS, Università di Genova, Via Dodecaneso, 35, 16146, Genova, Italy, (lrosasco@mit.edu) † LCSL, Istituto Italiano di Tecnologia and Massachusetts Institute of Technology, Bldg. 46-5155, 77 Mas-\nsachusetts Avenue, Cambridge, MA 02139, USA, (atacchett@mit.edu, Silvia.Villa@iit.it)\nthe data are not allowed. When compared to these methods, we note that the algorithm we consider naturally computes the whole regularization path. While it is possible to use a penalization parameter, or even the step size γ, as a regularization parameter, with these choices computing the regularization path incurs in an extra computational cost. This cost is avoided when considering early stopping by exploiting a built-in warm-restart property.\nThe rest of the paper is organized as follows. In Section 2, we discuss the setting we consider and in Section 3 we introduce and discuss the IGD algorithm. The main results are presented in Section 4, while their proofs are discussed in Section 6. Section 5 discusses future work. Preliminary results and the rest of the proofs are given in the Appendices A, B, and C. Finally, Appendix D studies finite sample bounds in the hypotheses space norm.\nNotation We denote by R+ = [0,+∞[ and R++ = ]0,+∞[ . Given a normed space B and linear operators (Ai)1≤i≤m, Ai : B → B for every i, their composition Am ◦ · · · ◦A1 will be denoted as\n∏m i=1 Ai. By convention, we set ∏m i=m+1 Ai = I , where I is the identity\nof B. The operator norm will be denoted by ‖ · ‖ and the Hilbert-Schmidt norm by ‖ · ‖HS .\n2. Set-Up: Supervised Learning. We study the supervised learning problem. Consider an input space X , which is a measurable space, and an output space1 Y = R. Let Z = X ×Y be endowed with a probability measure ρ, ρX denote the marginal measure on X , and ρ(·|x) the conditional measure on Y given x ∈ X . The conditional probability is well defined (see e.g. [32] Lemma A.3.16) and we denote by fρ = ∫ Y ydρ(y|·) the conditional expectation, namely the regression function. We assume the distribution to be fixed, but known only through a set z = {(x1, y1), . . . , (xm, ym)} of m independent samples identically distributed according to ρ. The set z is called a training set and, for technical reasons, we assume m ≥ 2. With an abuse of notation, (∀m ∈ N), the measure ρm will be denoted by P.\nFurther, consider a separable reproducing kernel Hilbert space (RKHS) H [1], with inner product (norm) denoted by 〈·, ·〉H (‖·‖H), and a measurable reproducing kernelK : X×X → R. The function t 7→ K(x, t) ∈ H is denoted by Kx. Throughout the paper we make the following boundedness assumptions.\nASSUMPTION 1. Assume that there exist M > 0 and κ > 0 such that supp{ρ(·|x)} ⊆ [−M,M ], and √\nK(x, x) < κ, for almost all x ∈ X . In the above setting, we are interested in solving the expected risk minimization problem,\ninf f∈H\nE(f), E(f) = ∫ (y − f(x))2dρ(x, y) . (2.1)\nNote that the corresponding optimization problem is convex (but in general not strongly convex), the domain of optimization is neither compact nor bounded, and, especially, the gradient of the objective function is unbounded. Moreover, we do not assume the infimum to be achieved.\nWe are interested in approximate solutions f̂ which satisfy the following requirement\n(∀ρ), (∀ǫ > 0), lim m→∞ P\n(\nE(f̂)− inf f∈H\nE(f) > ǫ ) = 0. (2.2)\nWhen H is universal [32] this is exactly universal consistency, see for example [15]. In the following, with a slight abuse of terminology, we will still refer to the condition in equation (2.2) as to universal consistency, even when the kernel is not universal. Moreover, we are interested in strengthening the above requirement to replace convergence in probability with\n1Most results extend naturally to Y being a separable Hilbert space, e.g. RT .\nalmost sure convergence, as well as to prove convergence in probability uniformly with respect to ρ. As it is known, the latter result can only be derived under suitable assumptions on ρ and it is equivalent to considering learning rates [16, 31].\nREMARK 2.1. When the set of minimizers of E is not empty, one could also consider consistency in the RKHS norm, that is\n(∀ρ), (∀ǫ > 0), lim m→∞ P\n( ‖f̂ − fH‖H > ǫ ) = 0, (2.3)\nwhere fH is the minimum norm minimizer of E on H. The analysis of this case is analogous to the one required to prove (2.2), therefore we leave the results and the corresponding proofs in Appendix D.\nEXAMPLE 2.2 (The Linear Case). A particular case of the above setting is the one where X is a Euclidean space and K is taken to be the associated inner product. If H is the corresponding RKHS, we have that f ∈ H if and only if (∀x ∈ X ) f(x) = 〈w, x〉, for some w ∈ X and ‖f‖H = ‖w‖. Assumption 1 is satisfied if the marginal distribution ρX is supported in a ball of radius κ.\nREMARK 2.3 (Stochastic Optimization (SO)). In SO given a probability space (Z, ρ), a separable Hilbert space B, and a loss function L : Z × B → [0,∞), we are interested in solving\ninf h∈B\n∫\nL(z, h)dρ(z).\nIn this view, supervised learning corresponds to setting Z = X × Y , B = H and L(z, f) = (y − f(x))2. Note however that due to the lack of strong convexity, unboundedness of X and unboundedness of the gradients, the classical assumptions required to apply stochastic gradient descent are not satisfied [26].\n3. Early Stopping for Online Learning Algorithms. We consider the estimator obtained applying the incremental gradient descent (IGD) algorithm [5, 4] to empirical risk minimization,\ninf f∈H Ê(f), Ê(f) = 1 m\nm ∑\ni=1\n(yi − f(xi))2. (3.1)\nGiven f̂t ∈ H an iteration of IGD generates f̂t+1 according to the recursion,\nf̂t+1 = ψ̂ m t , (3.2)\nwhere ψ̂mt is obtained at the end of one cycle, namely as the last step of the recursion\nψ̂0t = f̂t; ψ̂ i t = ψ̂ i−1 t − γt m (ψ̂i−1t (xi)− yi)Kxi , i = 1, . . . ,m (3.3)\nfor a suitable sequence of stepsizes {γt}t∈N, γt ∈ R++. In this paper, we consider a sequential approach, where each point of the training set is selected exactly once within each cycle. Each iteration, called epoch, corresponds to one pass over data. In practice, other approaches, e.g. stochastic [25], can be considered and might lead to different behaviors, but we leave the analysis of these latter cases for future study.\nThe iteration in (3.2)-(3.3) can be readily implemented if we consider a linear kernel, see Example 2.2, or any kernel K(x, x′) = 〈Φ(x),Φ(x′)〉\nRD defined by a finite dimensional\nfeature map Φ : X → RD. In the latter case we can simply identify the function f̂t+1 : x 7→\n〈ŵt+1,Φ(x)〉RD with ŵt+1 ∈ RD. For a general kernel, it can be easily seen that, for f̂0 = 0, the solution after t epochs can be written as f̂t(·) = ∑m k=1(αt)kKxk , for suitable coefficients αt = ((αt)1, . . . , (αt)m) ∈ Rm, given by the following recursion, αt+1 = c m t\nc0t = αt, (c i t)k =\n{ (ci−1t )k − γtm ( ∑m j=1 K(xi, xj)(c i−1 t )j − yi ) , k = i\n(ci−1t )k, k 6= i The above algorithm is closely related, yet different, to the one discussed in [19], where an explicit regularization is considered (choose λk > 0 in the following Equation (3.4)). More precisely, consider the following online learning algorithm\nĥk+1 = ĥk − γk m (ĥk(xik )− yik)Kxki + λkĥk, k ∈ N (3.4)\nwhere {λk}k∈N and {γk}k∈N are suitable sequences such that (∀k ∈ N) λk ∈ R+ and γk ∈ R++, and ik ∈ {1, . . . ,m}. We note that different kind of analyses can be carried out when considering the iteration (3.4). The incremental gradient descent algorithm (3.2)-(3.3) is an instance of the general iterative procedure (3.4). Indeed, it corresponds to the case where the gradient descent step is taken cyclically more than once with respect to the same point (ik = k mod m), we do not impose any explicit regularization setting λk = 0, and we keep γk constant on each pass over the training set. With this choice, for a given common initialization, we have f̂t = ĥmt, for all t. In the following we are specifically interested to study the effect of multiple passes over the data, when the goal is to minimize the expected, rather than the empirical, risk (see discussion in Section 3.2). Towards this end, we precisely consider the situation where the step size is chosen a priori in a distribution independent way and investigate the regularization effect of the number of epochs. Indeed, we show that early stopping, that is considering multiple – yet finitely many– epochs, is useful. Our results extend to incremental gradient the analysis of early stopping carried out in [41, 37, 3, 6, 29] for the classical (batch) gradient descent and shed light on the effect of considering multiple passes over the data in online learning algorithms.\n3.1. Early stopping, Learning and Computations. Recent interest in providing a better understanding of regularization by early stopping is largely motivated by its practical use. Indeed, early stopping has long been applied as a heuristic to achieve regularization, especially in the context of neural networks [22]. In particular, it is natural to consider early stopping when dealing with big data-sets, since in this context algorithms should ideally have computational requirements tailored to the generalization properties allowed by the data. This is exactly the main feature of early stopping regularization. Figure 1 gives an illustration of this fact in a numerical simulation. Here, we added an explicit regularization parameter λ as in [19], see also (3.4), and plot the error of the estimator on a hold-out set, against the number of epochs for a few values of λ. At least two interesting observations can be made. First, the smaller is λ the fewer epochs are needed to achieve the best generalization. Among all choices, fixing λ = 0 achieves comparable generalization with the minimal amount of computation (iterations). Second, the algorithm we consider naturally computes the whole regularization path. While it is possible to use λ, or even the step size γ, as a regularization parameter (see below), computing the regularization path incurs in an extra cost. This cost is avoided by the iterative procedure in (3.2),(3.3) by virtue of the natural warm-restart property which is built-in.\nBefore discussing our main results, we briefly compare IGD with other online learning schemes.\n3.2. Previous Work. Online learning algorithms have been analyzed within different settings. Since our perspective is perhaps somewhat different (we use IGD to solve expected, rather than empirical, risk minimization), we provide a brief overview of previous theoretical studies.\nMinimization of the expected risk.. The question of the consistency of online learning algorithms when H can be infinite dimensional has been considered in [34, 38] (see also references therein). Both papers deal with an infinite sequence of i.i.d. training points (m = +∞) and fix ik = k: no epochs are considered and the data points are seen only once (each iteration correspond to a point). In [34] it is shown that if the step size γk and the regularization parameter λk are chosen as suitable functions of the number of points, then the corresponding algorithm can be universally consistent with probability one. Moreover, under suitable smoothness assumption, see (4.2), learning rates are derived if both (γk)k∈N and (λk)k∈N are chosen depending on smoothness. In [38] it is shown that indeed similar results can be derived for λk = 0 for all k ∈ N, but an horizon, that is the total number of points to be considered, needs to be known a priori to appropriately choose the step-size. Recently, a similar analysis, λk = 0 for all k ∈ N, is developed in [2] in a finite dimensional setting. In this case it is shown that a suitable fixed step-size suffices to ensure convergence (as well as convergence rates) only with one pass over the data, however this is a by-product of considering a finite dimensional setting. Finally we note that another possibility is to consider the case where λ is kept fixed (and different from zero). In this case, convergence of algorithm (3.4) to the solution of the regularized expected risk minimization problem,\nmin f∈H\nE(f) + λ‖f‖2H.\nis derived. These latter results could be coupled with an analysis of the (approximation) error minf∈H E(f) + λ‖f‖2H − infH E(f) to derive convergence for the expected risk.\nMinimization of the empirical risk.. In optimization theory, the IGD iteration is used to minimize objective functions which are the sum of m functions. In our case, by definition, we have\nÊ(f) = m ∑\ni=1\nVi(f), Vi(f) = 1\nm (yi − f(xi))2.\ntherefore the IGD could be applied to minimize the empirical risk, i.e. problem (3.1). Results in this context focus on showing that Ê(f t) − inff∈H Ê(f) converges to zero when t\nincreases, see [5] and references therein. Related results consider the case where the empirical risk is replaced by a regularized empirical risk, see e.g. [21]. In this paper, we propose a novel perspective showing how IGD can be used to approximately solve the expected risk minimization, i.e. problem (2.1) rather than the empirical risk.\nSequential Prediction. Finally, we note that iterative schemes as in (3.4) have been recently extensively studied in online learning (a.k.a. sequential prediction problems), see e.g. [10]. In this context, the data are not stochastic, and the goal is to control the so called regret. If the data are indeed generated by a stochastic process, then there is a classic approach, sometimes called online-to-batch conversion [9], to convert regret bounds to expected risk bounds. However, this latter approach does not seem to be appropriate for studying the effect of multiple passes over the data.\n4. Main Results. We begin by stating the universal consistency of (cyclic) IGD with early stopping.\nTHEOREM 4.1. Let θ ∈ [0, 1[ , and (∀t ∈ N), γt = κ−2(t + 1)−θ. Assume that (f̂t)t∈N is defined in (3.2)-(3.3). If we choose a stopping rule t∗ = t∗(m) so that\nlim m→+∞ t∗(m) = +∞ and lim m→+∞ t∗(m)4(1−θ)/m = 0 (4.1)\nthen\nlim m→∞ E(f̂t∗) = inf f∈H E(f) almost surely.\nThe above result shows that consistency is achieved computing a suitable number t∗(m) of iterations of IGD given m points. The number of required iterations tends to infinity as the number of available training points increases. In particular, this excludes the choice t∗(m) = 1 for all m, namely considering only one pass over the data. Condition (4.1) can be interpreted as an early stopping rule, since it requires the number of epochs not to grow too fast. Below, we further discuss how a suitable stopping rule can be derived if the problem satisfies certain prior assumptions. We note that several choices of the step-size are allowed in Theorem 4.1. In particular it is possible to choose a constant step-size, see also the discussion below. We next consider finite sample bounds that can be derived if the regression function satisfies the following smoothness assumption,\n∥ ∥L−rfρ ∥ ∥ ρ ≤ R, for some r > 0. (4.2)\nwhere L : L2(X , ρX ) → L2(X , ρX ) is such that Lf(x) = ∫ X f(x)K(x, x ′)dρX and ‖ · ‖ρ denotes the norm in L2(X , ρX ) = {f : X → R : ∫\ndρX |f |2 < ∞}. Assumption (4.2) is fairly standard (see [13], and [12, Section 4] for a discussion).\nTHEOREM 4.2. Let (∀t ∈ N), γt = κ−2(t + 1)−θ, for some θ ∈ [0, 1[ . Assume that (4.2) holds, and fix δ ∈ ]0, 1[. Then, with probability at least 1− δ,\n(∀t ∈ N) E(f̂t)− inf H\nE ≤ Cθ log2( 2 δ ) (t+ 1)4(1−θ) m + Crt −2r(1−θ) (4.3)\nwhere Cθ = C2/2κ6(1− θ)4 for some C ∈ R++ which does not depend on m, t, δ, and Cr = R 2(2r/e)4r/κ4(1− θ)2. Moreover, if we choose the stopping rule\nt∗(m) = ⌈m 1(4+2r)(1−θ) ⌉ (4.4)\nthen, with probability at least 1− δ,\nE(f̂t∗(m))− inf H\nE ≤ D ( log 2\nδ\n)2\nm− r r+2 , (4.5)\nwhere the constant D ∈ R++ can be explicitly given. Equation (4.3) arises from a form of bias-variance (sample-approximation) decomposition of the error. Choosing the number of epochs that optimize the bound (4.3), we derive an a priori stopping rule (4.4) and a corresponding bound (4.5). Again, this result confirms that the number of epochs acts as a regularization parameter and the best choice following from Equation (4.3) suggests multiple passes over the data to be beneficial. Interestingly, the best distribution independent choice of the step-size is a constant with respect to t, that is θ = 0. Such a bound can be compared to known lower bounds as well as previous results for least squares algorithms that can be studied under the same prior. Lower bounds are known [7, 33] under assumption (4.2) and further assuming that the eigenvalues of L have a polynomial decay, that is\n(σi)i∈N ∼ i−b, for some b ∈ [1,∞[ . (4.6)\nThis latter property can be interpreted as a measure of the effective dimensionality of the hypotheses space [40, 7]. There are a few special regimes of interest; assuming r ≥ 1/2 in (4.2) implies that the infimum of the expected risk over H is achieved [11], and it is known that sharp bounds are harder to get if r < 1/2 (see discussion in [33]). For b = 1, the condition (σi)i∈N = O(i−b) always holds. This is the situation we consider in this paper, and it is sometimes called the capacity independent setting. The lower bound, under assumptions (4.2), (4.6), are of order O(n− 2rb 2rb+1 ), and O(n− 2r 2r+1 ) in the capacity independent setting. The bound in (4.3) is not optimal as a consequence of a bad dependence of a suitably defined sample error estimate on the number of epochs. Deriving this bound is the main technical contribution of the paper and the proof is rather involved. While the dependence of the bound on the number of points is optimal, the dependence on the number of epochs can be improved. A comparison with the analysis of batch gradient descent suggests that the optimal dependence should be ∼ t(1−θ). The bound is proved generalizing classical results in inverse problems and is known to be essentially sharp [17]. We also note that several least squares estimator have been considered in this setting. Regularized least squares (a.k.a. kernel ridge regression) has been considered in [40, 7, 33, 24, 18], and iterative methods, as well as a larger class of so called spectral filtering methods [23] have been analyzed in [41, 37, 3, 6, 8, 29]. As we mentioned before, online learning algorithms are considered in [34, 38] where optimal bounds are derived in the capacity independent setting. The bound in the finite dimensional setting derived in [2] are also optimal. An important point here is that, while different least squares methods are based on the same class of priors and likely to have essentially the same statistical guarantees, their computational properties are different. In this view it is interesting to develop efficient least square estimators and early stopping provides a natural approach (see [42] for an interesting idea based on a divide and conquer approach).\nFinally, we present an adaptive early stopping rule. The stopping rule (4.4) depends on the smoothness parameter r which is typically unknown, and hold-out cross validation is often used in practice. Here we show that this procedure allows to adaptively achieve the same convergence rate as in (4.5). Our analysis follows the one in [8].\nGiven a training set z of cardinality m, we define\nz1 = {(x1, y1, . . . , (xn, yn)}, z2 = {(xn+1, yn+1), . . . , (xm, ym)}, (4.7)\nwhere n = ⌊m/2⌋ (note that other splits are possible). Fix tmax ∈ N, and use the training set z1 to compute the sequence (f̂t)t∈{1,...,tmax}. Then, set Ê2 to be the empirical error corresponding to the training set z2, and\nt̂ = argmin{Ê2(TM f̂t) : t ∈ {1, . . . tmax}} . (4.8)\nwhere TM : H → H : TMf(x) = min{|f(x)|,M}f(x)/|f(x)| is the truncation operator. THEOREM 4.3. Let (∀t ∈ N) γt = κ−2(t + 1)−θ for some θ ∈ [0, 1[. If tmax > m1/(4(1−θ)), with probability greater than 1− 2δ,\nE(TM f̂t̂)− inf H\nE ≤ 2D ( log 2\nδ\n)2\nm− r r+2 + 40M2 3(1− θ) ( log m 2δ ) m−1 (4.9)\nwhere D is the constant appearing in equation (4.5). Noting that for every r > 0, (log(m/2δ))m−1 goes to zero faster than m−r/(r+2), we see that the estimator obtained with hold-out cross-validation achieves the same learning rate as the one corresponding to the optimal choice of the number of epochs. We end noting that, while an hold-out procedure requires splitting the data, it does not worsen the computational complexity of the algorithm, unlike other model selection criterions [29].\n5. Some Future Directions. The analysis presented in the paper is a first step towards exploring the properties of early stopping for online learning algorithms. We mention a few directions for future work.\n• Sharpen the bounds. In particular incorporating kernel dependent assumption such as (4.6), quantifying the effective dimension of the hypotheses space. This latter point would be particularly interesting to bridge the analysis in the finite and infinite dimensional setting. • Consider other iterative procedures. Particularly, stochastic variants of the IGD algorithms, where the indices in each cycle are randomly selected [5], and also accelerated version as proposed in [27]. Ideas related to this latter point have been considered in [3, 6, 8] for batch gradient techniques (the gradient of the empirical risk is considered in each iteration). In particular in [3] it is shown that a variant of gradient descent, sometimes called the ν-method, can obtain the same generalization guarantees of non accelerated gradient descent learning, but using much fewer iterations. • Generalize the analysis. In particular, consider loss functions other than the least squares loss, and especially priors other than (4.2),(4.6). This latter question seems particularly interesting and possibly challenging.\n6. Proofs of the Main Results. In this section we the sketch the proofs of the main results. The error analysis is based on the following decomposition into sample error and approximation error\nE(f̂t)− inf H E ≤ 2κ2‖f̂t − ft‖2H + 2(E(ft)− inf H E) (6.1)\nwhere ft are the iterates of the IGD on the expected error defined in equation (B.9). Theorems 4.1 and 4.2. will follow from a probabilistic upper bound for the sample error and an upper bound for the approximation error.\nTHEOREM 6.1 (Sample error). Let f̂0 = f0 = 0, (γt)t∈N be such that (∀t ∈ N) γt ∈]0,mκ−2], and f̂t and ft be defined in (3.2) and (B.9), respectively. There exists C > 0\nsuch that, for all t ∈ N and δ ∈]0, 1[ (sufficiently small),\nP\n(\n‖f̂t − ft‖H ≤ C√ m\n(\nlog 2\nδ\n) t−1 ∑\nk=1\nγk\nk−1 ∑\ni=0\nγi\n)\n≥ 1− δ .\nIn particular, if γi = κ−2(i+ 1)−θ with θ ∈ [0, 1[ , with probability at least 1− δ,\n‖f̂t − ft‖H ≤ C\n2κ4(1− θ)2√m\n(\nlog 2\nδ\n)\n(t+ 1)2(1−θ)\nThe proof of Theorem 6.1 is postponed to Appendix C. We next show that the approximation error E(ft) − infH E go to zero as t goes to infinity, and give a rate under assumption (4.2). For similar results with respect to the norm in H see Appendix D.\nTHEOREM 6.2 (Approximation error). Let f0 = 0, (γt)t∈N be such that (∀t ∈ N) γt ∈]0,mκ−2[, ∑ t∈N γt = +∞ and (ft)t∈N be defined as in (B.9). Then\nE(ft)− inf H E → 0. (6.2)\nMoreover, if fρ satisfies (4.2), then\nE(ft)− inf H\nE ≤ R2(r/e)2r( t−1 ∑\ni=0\nγi) −2r .\nIn particular, if γi = κ−2(i+ 1)−θ with θ ∈ [0, 1[,\nE(ft)− inf H\nE ≤ R 2(r/e)2r\nκ2(1− θ) t −2r(1−θ) .\nCombining the bounds for the sample and approximation errors, we get a proof of Theorem 4.2 and Theorem 4.1, solving a bias-variance trade-off. Proof. of Theorem 4.2. Recalling equation (6.1), we have\nE(f̂t)− inf H E ≤ 2κ2‖f̂t − ft‖2H + 2(E(ft)− inf H E) .\nApplying Theorem 6.1 and Theorem 6.2 we get that with probability at least 1− δ\nE(f̂t)− inf H\nE ≤ C 2\n2κ6(1− θ)2m\n(\nlog 2\nδ\n)2\nt4(1−θ) + R(r/e)2r\nκ2(1− θ) t −2r(1−θ), (6.3)\nand (4.3) follows by suitably chosing the constant D. Next let t∗(m) = ⌈mα⌉. Substituting this choice of t into the right hand side of (6.3), and minimizing over α, we get the linear equation\n4α(1− θ)− 1 = −2αr(1− θ), (6.4)\nwhose solution is α = 1/((1− θ)(4 + 2r)). Proof. of Theorem 4.1. Combining Theorem 6.1 and Theorem 6.2 we immediately get convergence in probability of E(f̂t) − inff∈H E . Almost sure convergence follows applying the Borel-Cantelli lemma. Proof. of Theorem 4.3 We define\ntρ = argmin{E(TM f̂t) : t ∈ {1, . . . , T }} . (6.5)\nThe latter definition is equivalent to\ntρ = argmint∈{1,...,tmax}\n∥ ∥ ∥S(TM f̂t)− fρ ∥ ∥ ∥ 2\nρ (6.6)\nwhere S is the operator defined in Appendix B. Since by assumption the support of ρ(y|x) is contained in [−M,M ], it follows that fρ(x) ∈ [−M,M ] almost surely. Therefore, from the definition of tρ,\n∥ ∥ ∥S(TM f̂tρ)− fρ ∥ ∥ ∥\nρ ≤\n∥ ∥ ∥S(TM f̂t∗(m))− fρ ∥ ∥ ∥\nρ ≤\n∥ ∥ ∥Sf̂t∗(m) − fρ ∥ ∥ ∥\nρ . (6.7)\nIf we denote by Pfρ the projection onto the closure of H in L2(X , ρX ), from the chain of inequalities in (6.7), noting that (∀g ∈ H) ‖Sg − Pfρ‖2ρ = ‖Sg − fρ‖ 2 ρ − ‖Pfρ − fρ‖ 2 ρ we get\nE(TM f̂tρ)− inf H\nE = ∥ ∥ ∥S(TM f̂tρ)− Pfρ ∥ ∥ ∥ 2\nρ ≤\n∥ ∥ ∥Sf̂t∗(m) − Pfρ ∥ ∥ ∥ 2\nρ = E(f̂t∗(m))− inf H E . (6.8)\nFinally, we claim that with probability greater than 1− δ, if tmax = m1/(4(1−θ)),\n‖S(TM f̂t̂)− Pfρ‖2ρ ≤ 2‖S(TM f̂tρ)− Pfρ‖2ρ + 640M2(1− θ)\nm log\n2m\nδ (6.9)\nFirst note that adding to both sides ‖Pfρ − fρ‖2ρ (6.8) is equivalent to\n‖S(TM f̂t̂)− fρ‖2ρ ≤ 2‖S(TM f̂tρ)− fρ‖2ρ + 640M2(1− θ)\nm log\n2m\nδ (6.10)\nThe statement then follows combining (6.8) and (6.9), and applying Theorem 4.2. To prove (6.10), let z2 = {(xn+1, yn+1, . . . , (xm, ym)}, and define the random variables for i = n+ 1, . . . ,m:\nξti = (S(TM f̂t)(xi)− yi)2 − (fρ(xi)− yi)2 .\nWe have for every t ∈ {1, . . . , tmax}\n|ξti | ≤ 4M2 (6.11)\nE[(ξti )] =\n∫\nX×Y\n( (TM f̂t(x)− y)2 − (fρ(x)− y)2 ) dρ = ‖S(TM f̂t)− fρ‖2ρ (6.12)\nE[(ξti ) 2] =\n∫\nX×Y\n(TM f̂t(x)− fρ(x))2(TM f̂t(x) + fρ(x)− 2y)2 dρ (6.13)\n≤ 16M2E[(ξti )] (6.14)\nApplying Proposition A.2 with Xi = ξti , µ = E[(ξ t i )], B = 4M 2, σ2 = E[(ξti ) 2] ≤\n16M2E[(ξti )], we obtain for all t ∈ {1, . . . , tmax} with probability greater than 1− δ\n1\nm− n\nm−n ∑\ni=1\nξti ≤ (1 + 16αM2)µ+ ǫ (6.15)\nand\n(1− 16αM2)E[ξti ] ≤ 1\nm− n\nm−n ∑\ni=1\nξti + ǫ, (6.16)\nwith ǫ = (3+16αM 2) 24(1−θ)(m−n)α log (m−n) δ . Therefore, since tmax ≥ t∗(m)\n‖Sf̂t̂ − fρ‖2ρ = E[ξt̂i ] ≤ 1\n1− 16αM2\n(\n1\nm− n\nm−n ∑\ni=1\nξt̂i\n)\n+ ǫ\n1− 16αM2\n≤ 1 1− 16αM2\n(\n1\nm− n\nm−n ∑\ni=1\nξ t∗(m) i\n)\n+ ǫ\n1− 16αM2\n≤ 1 + 16αM 2\n1− 16αM2E[ξ t∗(m) i ] +\n2ǫ\n1− 16αM2 .\nIf we choose α = 1/(48M2) we get 16αM2 = 1/3 and\n‖Sf̂t̂ − fρ‖2ρ ≤ 2(E(TM f̂t∗(m))− E(fρ)) + 20M2 3(1− θ)(m− n) log (m− n) δ . (6.17)\nRecalling that m− n ≥ m/2 we obtain the statement. Acknowledgments. This material is based upon work supported by the FIRB project RBFR12M3AC “Learning meets time: a new computational approach for learning in dynamic systems” and the Center for Minds, Brains and Machines (CBMM), funded by NSF STC award CCF-1231216. S. V. is member of the Gruppo Nazionale per l’Analisi Matematica, la Probabilità e le loro Applicazioni (GNAMPA) of the Istituto Nazionale di Alta Matematica (INdAM). The authors are grateful to Francesco Orabona for fruitful discussions.\nREFERENCES\n[1] N. Aronszajn. Theory of reproducing kernels. Trans. Amer. Math. Soc, 68(3):337–404, 1950. [2] F. Bach and E. Moulines. Non-strongly-convex smooth stochastic approximation with convergence rate o(1/n). In Advances in Neural Information Processing Systems (NIPS), 2013. [3] Frank Bauer, Sergei Pereverzev, and Lorenzo Rosasco. On regularization algorithms in learning theory. Journal of complexity, 23(1):52–72, 2007. [4] Dimitri P. Bertsekas. A new class of incremental gradient methods for least squares problems. SIAM J. Optim., 7(4):913–926, 1997. [5] Dimitri P Bertsekas and John N Tsitsiklis. Gradient convergence in gradient methods with errors. SIAM Journal on Optimization, 10(3):627–642, 2000. [6] G. Blanchard and N. Krämer. Optimal learning rates for kernel conjugate gradient regression. In Advances in Neural Inf. Proc. Systems (NIPS), pages 226–234, 2010. [7] A. Caponnetto and E. De Vito. Optimal rates for regularized least-squares algorithm. Found. Comput. Math., 2006. [8] A. Caponnetto and Yuan Yao. Adaptive rates for regularization operators in learning theory. Analysis and Applications, 08, 2010. [9] N. Cesa-Bianchi, A. Conconi, and C. Gentile. On the generalization ability of on-line learning algorithms.\nIEEE Transactions on Information Theory, 50(9):2050–2057, 2004. [10] Nicolò Cesa-Bianchi and Gábor Lugosi. Prediction, learning, and games. Cambridge University Press, 2006. [11] Felipe Cucker and Steve Smale. On the mathematical foundations of learning. Bulletin of the American Mathematical Society, 39:1–49, 2002. [12] Felipe Cucker and Ding Xuan Zhou. Learning Theory: An Approximation Theory Viewpoint. Cambridge University Press, 2007. [13] E. De Vito, A. Caponnetto, and L. Rosasco. Model selection for regularized least-squares algorithm in learning theory. Found. Comput. Math., 5(1):59–85, 2005. [14] E. De Vito, L. Rosasco, A. Caponnetto, U. De Giovannini, and F. Odone. Learning from examples as an inverse problem. Journal of Machine Learning Research, 6:883–904, 2005. [15] L. Devroye, L. Györfi, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Number 31 in Applications of mathematics. Springer, New York, 1996. [16] Luc Devroye. Sample-based non-uniform random variate generation. In Proceedings of the 18th conference\non Winter simulation, pages 260–265. ACM, 1986.\n[17] H. W. Engl, M. Hanke, and A. Neubauer. Regularization of inverse problems. Kluwer, 1996. [18] Daniel Hsu, Sham M Kakade, and Tong Zhang. Random design analysis of ridge regression. Journal of Machine Learning Research-Proceedings Track, 23:9–1, 2012. [19] Jyrki Kivinen, Alexander J Smola, and Robert C Williamson. Online learning with kernels. Signal Processing, IEEE Transactions on, 52(8):2165–2176, 2004. [20] Harold J. Kushner and G. George Yin. Stochastic approximation algorithms and applications, volume 35 of Applications of Mathematics (New York). Springer-Verlag, New York, 1997. [21] Nicolas Le Roux, Mark Schmidt, and Francis Bach. A stochastic gradient method with an exponential con-\nvergence rate for strongly-convex optimization with finite training sets. arXiv preprint arXiv:1202.6258, 2012.\n[22] Y. LeCun, L. Bottou, G. Orr, and K. Muller. Efficient backprop. In G. Orr and Muller K., editors, Neural Networks: Tricks of the trade. Springer, 1998. [23] Laura Lo Gerfo, Lorenzo Rosasco, Francesca Odone, Ernesto De Vito, and Alessandro Verri. Spectral algorithms for supervised learning. Neural Computation, 20(7):1873–1897, 2008. [24] Shahar Mendelson and Joseph Neeman. Regularization in kernel learning. The Annals of Statistics, 38(1):526– 565, 2010. [25] Angelia Nedic and Dimitri P Bertsekas. Incremental subgradient methods for nondifferentiable optimization. SIAM Journal on Optimization, 12(1):109–138, 2001. [26] A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM J. Optim., 19(4):1574–1609, 2008. [27] Y. Nesterov. A method for unconstrained convex minimization problem with the rate of convergence o(1/k2). Doklady AN SSSR, 269(3):543–547, 1983. [28] Iosif Pinelis. Optimum bounds for the distributions of martingales in Banach spaces. Ann. Probab., 22(4):1679–1706, 1994. [29] Garvesh Raskutti, Martin J Wainwright, and Bin Yu. Early stopping for non-parametric regression: An optimal data-dependent stopping rule. In Communication, Control, and Computing (Allerton), 2011 49th Annual Allerton Conference on, pages 1318–1325. IEEE, 2011. [30] Michael Reed and Barry Simon. Methods of modern mathematical physics. I. Functional analysis. Academic Press, New York, 1972. [31] Ingo Steinwart. On the influence of the kernel on the consistency of support vector machines. The Journal of Machine Learning Research, 2:67–93, 2002. [32] Ingo Steinwart and Andreas Christmann. Support Vector Machines. Springer Publishing Company, Incorporated, 1st edition, 2008. [33] Ingo Steinwart, Don R. Hush, and Clint Scovel. Optimal rates for regularized least squares regression. In COLT, 2009. [34] Pierre Tarrès and Yuan Yao. Online learning as stochastic approximation of regularization paths. arXiv preprint arXiv:1103.5538, 2011. [35] Angus E. Taylor and David C. Lay. Introduction to functional analysis. 2nd ed. (Reprint of the orig. 1980, publ. by John Wiley &amp; Sons, Inc., New York etc.). Malabar, Florida: Robert E. Krieger Publishing Company. XI, 1986. [36] Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computational Mathematics, 12(4):389–434, 2012. [37] Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto. On early stopping in gradient descent learning. Constructive Approximation, 26(2):289–315, 2007. [38] Yiming Ying and Massimiliano Pontil. Online gradient descent learning algorithms. Foundations of Computational Mathematics, 8(5):561–596, 2008. [39] Vadim Yurinsky. Sums and Gaussian vectors, volume 1617 of Lecture Notes in Mathematics. Springer-Verlag, Berlin, 1995. [40] Tong Zhang. Learning bounds for kernel regression using effective data dimensionality. Neural Computation, 17:2077–2098, 2005. [41] Tong Zhang and Bin Yu. Boosting with early stopping: Convergence and consistency. Annals of Statistics, pages 1538–1579, 2005. [42] Yuchen Zhang, John C. Duchi, and Martin J. Wainwright. Divide and conquer kernel ridge regression. In COLT, pages 592–617, 2013.\nAppendix A. Useful Results. In the various proofs we repeatedly use the following wellknown equation. Let (Xr)r∈N be a sequence in H, defined recursively by\nXr+1 = ArXr +Br, (A.1)\nwith Ar : H → H a linear operator for all r ∈ N, and {Br}r∈N ⊆ H . Then, for every s ∈ N,\ns < r,\nXr =\n(\nr−1 ∏\ni=s\nAi\n)\nXs + r−1 ∑\nk=s\n(\nr−1 ∏\ni=k+1\nAi\n)\nBk . (A.2)\nThe following concentration inequality due to [28] (see also [39] and [34, Proposition A.3]) and is the useful to obtain a probabilistic upper bound for the sample error.\nTHEOREM A.1 (Bernstein-Pinelis). Let ξi be a martingale difference sequence taking values in a Hilbert space. Suppose that ‖ξi‖ ≤ M . Then for every δ ∈ ]0, 1[ the following holds\nP\n({\n∥ ∥ ∥ 1\nm\nm ∑\ni=1\nξi\n∥ ∥ ∥ ≤ 8M 3 √ m log 2 δ\n})\n≥ 1− δ .\nWhile more refined concentration inequality could be considered [36], this would affect only the constants in the bound which are not the main focus of this paper. In the proof of Theorem 4.3, we will also use a variant of the previous Bernstein concentration inequality, which is taken from [8].\nPROPOSITION A.2. Let (ξi)1≤i≤m be real valued i.i.d. random variables with mean µ, |ξi| ≤ B and E[(ξi − µ)2] ≤ σ2, for all i ∈ {1, . . . ,m}. Then for arbitrary α ∈ R++, δ ∈]0, 1],\nP\n[∣\n∣ ∣ ∣ ∣ 1 m\nm ∑\ni=1\nξi − µ ∣ ∣ ∣ ∣\n∣ ≥ ασ2 + 3 + 4αB 6mα log 2δ\n]\n≤ δ . (A.3)\nAppendix B. Recursive Expressions and Error Decomposition. We introduce some linear operators that will be useful in the following. Let S : H → L2(X , ρX ) be the embedding operator. S is well defined and ‖S‖ ≤ κ. If we set T = S∗S andL = SS∗, then T : H → H, L : L2(X , ρX ) → L2(X , ρX ) : Lf(x) = ∫\nX f(x)K(x, x′)dρX\nwith ‖T ‖ ≤ κ2 and ‖L‖ ≤ κ2. Fix x ∈ X . Then Sx : H → R : f 7→ f(x) is well defined and ‖Sx‖ ≤ κ. Its adjoint is S∗x : R → H : a 7→ aKx. The operator S∗xSx is denoted by Tx. Clearly ‖Tx‖ ≤ κ2. Finally, given the training set z, the operator\n∑m i=1 Txi/m is denoted by T̂ . Note that, using\nthese linear operators, we can write the empirical error and the risk as\nÊ(f) = 1 m\nm ∑\ni=1\n(Sxif − yi)2, E(f) = ‖Sf − fρ‖2ρ + E(fρ) , (B.1)\nfor alll f ∈ H. We describe some useful recursive expressions for the IGD and the gradient descent (GD) iteration. These will be used in the error analysis and provide some useful comparison between these two methods.\nLEMMA B.1. Let f̂0 ∈ H and for every t ∈ N, let f̂t be defined as in equations (3.2) and (3.3). Then\nf̂t+1 = m ∏\ni=1\n( I − γt m Txi ) f̂t + γt m\nm ∑\ni=1\nm ∏\nk=i+1\n( I − γt m Txk ) S∗xiyi (B.2)\nProof. The update of ψ̂it in (3.3) is of the form (A.1), with Ar = I − γtmTxr+1 , and Br = γt mS\n∗ xr+1yr+1, and X0 = f̂t. Equation (B.2) follows by writing (A.2) for r = m. PROPOSITION B.2. Assume that m ≥ 2. The iteration of the incremental gradient\ndescent can be written as\nf̂t+1 =\n\nI − γt m\nm ∑\nj=1\nTxj\n\n f̂t + γt\n(\n1\nm\nm ∑\nj=1\nS∗xjyj\n)\n+ γ2t\n( Âtf̂t − b̂t )\n(B.3)\nwith\nÂt = 1\nm2\nm ∑\nk=2\nm ∏\ni=k+1\n( I − γt m Txi ) Txk\nk−1 ∑\nj=1\nTxj , b̂t = 1\nm2\nm ∑\nk=2\nm ∏\ni=k+1\n( I − γt m Txi ) Txk\nk−1 ∑\nj=1\nS∗xjyj\n(B.4)\nProof. In order to prove the equality in (B.3), we first derive a equation for the quantity hi = ∑i j=1(Txj ψ̂ j−1 t − S∗xjyj). We show by induction that for all i = 1, . . . ,m\nhi = i ∑\nj=1\nTxj f̂t − i ∑\nj=1\nS∗xjyj + γt m R̂itf̂t + γt m r̂it (B.5)\nfor a linear operator R̂it : H → H and an element r̂it ∈ H. Since\nh1 = Tx1ψ̂ 0 t − S∗x1y1 = Tx1 f̂t − S ∗ x1y1 ,\nclearly (B.5) holds for i = 1 with R̂1t = 0 and r̂ 1 t = 0. Now we suppose by inductive hypothesis that (B.5) holds for i, and we prove it for i+1. First note that ψ̂it = f̂t−(γt/m)hi, and then by inductive hypothesis,\nhi+1 = hi + Txi+1ψ̂ i t − S∗xi+1yi+1\n= hi + Txi+1(f̂t − γt m hi)− S∗xi+1yi+1 = (I − γt m Txi+1)hi + Txi+1 f̂t − S∗xi+1yi+1\n= (I − γt m Txi+1)(\ni ∑\nj=1\nTxj f̂t − i ∑\nj=1\nS∗xjyj + γt m R̂itf̂t + γt m r̂it) + Txi+1 f̂t − S∗xi+1yi+1\n=\ni+1 ∑\nj=1\nTxj f̂t − i+1 ∑\nj=1\nS∗xjyj + γt m\n\n\n( I − γt m Txi+1 )\nR̂it − Txi+1 i ∑\nj=1\nTxj\n\n f̂t (B.6)\n+ γt m\n(\n( 1− γt m Txi+1 ) r̂it + Txi+1\nj ∑\ni=1\nS∗xjyj\n)\n.\nBy setting for i = 1, . . . ,m− 1\nR̂i+1t = ( 1− γt m Txi+1 )\nR̂it−Txi+1 i ∑\nj=1\nTxj , r̂ i+1 t =\n( I − γt m Txi+1 ) r̂it+Txi+1\ni ∑\nj=1\nS∗xjyj\nwe get (B.5) for i+ 1. From (B.5) we derive\nf̂t+1 = f̂t− γt m hm =\n\nI − γt m\nm ∑\nj=1\nTxj − (γt m )2 R̂mt\n\n f̂t+ γt m\nm ∑\nj=1\nS∗xjyj− (γt m )2 r̂it (B.7)\nEquation (B.3) then follows by defining Ât = −R̂mt /m2 and b̂t = r̂mt /m2 and recalling (3.3). Moreover, applying (A.2), we have\nÂt = 1\nm2\nm ∑\nk=2\nm ∏\ni=k+1\n( I − γt m Txi ) Txk\nk−1 ∑\nj=1\nTxj , b̂t = 1\nm2\nm ∑\nk=2\nm ∏\ni=k+1\n( I − γt m Txi ) Txk\nk−1 ∑\nj=1\nS∗xjyj\nEquation (B.3) allows to compare the resulting update of one epoch of the incremental gradient descent with the one of a standard gradient descent on the empirical error with stepsize γt, which is given by\nϕ̂t+1 =\n\nI − γt m\nm ∑\nj=1\nTxj\n\n ϕ̂t + γt m\nm ∑\nj=1\nS∗xjyj (B.8)\nfor an arbitrary ϕ̂0 ∈ H. As can be seen directly comparing (B.3) and (B.8), the incremental gradient descent can be interpreted as a perturbed gradient descent step, with perturbation\nêt = γ 2 t\n( Âtf̂t − b̂t ) ,\nwhich is proportional to γ2t . To analyze consistency properties of IGD applied to the empirical risk, we need to introduce an auxiliary iteration, obtained by applying the incremental gradient descent algorithm to the expected loss, which clearly, for fixed m, can be written as\nf 7→ m ∑\ni=1\nE(f) m .\nReasoning as in Proposition B.2, simply replacing Txi by T and Sxi by S, we get\nft+1 = (I − γtT ) ft + γtS∗fρ + γ2t (Atft − bt) (B.9)\nwith\nAt = 1\nm2\nm ∑\nk=2\n[\nm ∏\ni=k+1\n( I − γt m T )\n]\nT\nk−1 ∑\nj=1\nTft, bt = 1\nm2\nm ∑\nk=2\n[\nm ∏\ni=k+1\n( I − γt m T )\n]\nT\nk−1 ∑\nj=1\nS∗fρ\n(B.10)\nREMARK B.3. First note that, although not explicitly specified, the operator At and the element bt ∈ H depend on m. Moreover, since in this case the gradient of each summand coincides with the gradient of the function E , one epoch of the incremental gradient method in (B.9) corresponds to m steps of a gradient descent with stepsize γt/m (where the stepsize is fixed across m iterations).\nAppendix C. Proof of Theorems 6.1 and 6.2. To prove Theorem 6.1 we need some auxiliary results. The proof will be given at the end of the section.\nLEMMA C.1. For all t ∈ N,\nf̂t − ft = [ t−1 ∏\nk=0\n(I − γkT + γ2kAk) ] (f̂0 − f0) + t−1 ∑\nk=0\nγk\n\n\nt−1 ∏\nj=k+1\n(I − γjT + γ2jAj)\n\nχk\n(C.1) with\nχk = (T − T̂ )f̂k + γk(Âk −Ak)f̂k + ( 1\nm\nm ∑\ni=1\nŜ∗xiyi − S ∗fρ) + γk(bk − b̂k), (C.2)\nwhere Âk and b̂k are defined in (B.4), and Ak and bk in (B.10). Proof. Fix t ∈ N. We have\nf̂t+1 = (I − γtT̂ + γ2t Ât)f̂t + γt m ∑\ni=1\nŜ∗xiyi − γ 2 t b̂t .\nAdding and subtracting (−γtT + γ2tAt)f̂t we get\nf̂t+1 = (I − γtT + γ2tAt)f̂t + γt(T − T̂ )f̂t + γ2t (Ât −At)f̂t + γt 1\nm\nm ∑\ni=1\nŜ∗xiyi − γ 2 t b̂t .\nTherefore\nf̂t+1 − ft+1 = (I − γtT + γ2tAt)(f̂t − ft) + γt(T − T̂ )f̂t + γ2t (Ât −At)f̂t\n+ γt\n( 1\nm\nm ∑\ni=1\nŜ∗xiyi − S ∗fρ\n)\n+ γ2t (bt − b̂t)\nRelying on equation (A.1) we get (C.1). We next state other lemmas which are needed to bound the various terms appearing in the decomposition derived in Lemma C.1. First we bound the norm of the operator which is applied to the random variable χk.\nLEMMA C.2. Fix m ∈ N, m ≥ 1, j ≥ 1, and let γj ∈ ]0,mκ−2]. Then\n‖I − γjT + γ2jAj‖ ≤ 1 . (C.3)\nProof. By definition of Aj\nI − γjT + γ2jAj = ( I − γj m T )m . (C.4)\nSince ‖T ‖ ≤ κ2 and by assumption γj/m ≤ κ−2, ∥ ∥I − γjmT ∥ ∥ ≤ 1 and the statement follows.\nThe longest part of the proof is the one required to get a bound on the norm of the random variable χk. We will proceed bounding each summand separately.\nLEMMA C.3. Assume (γk)k∈N to be a sequence in ]0,+∞[, and f̂0 = 0. Then, for all t ∈ N, ‖f̂t‖K ≤ κM ∑t−1 k=0 γk. The following lemma is a direct consequence of BernsteinPinelis inequality A.1, taking into account that ‖T ‖HS ≤ κ2 and ‖Txi‖HS ≤ κ2 (see also [14])\nLEMMA C.4. For every δ ∈ ]0, 1[\nP\n(\n‖ 1 m\nm ∑\ni=1\nTxi − T ‖HS ≤ 16κ2\n3 √ m\nlog 2\nδ\n)\n≥ 1− δ (C.5)\nand\nP\n(\n‖ 1 m\nm ∑\ni=1\nS∗xiyi − S ∗fρ‖H ≤\n16κM\n3 √ m\nlog 2\nδ\n)\n≥ 1− δ (C.6)\nLEMMA C.5. Let (γk)k∈N be a sequence in ]0,mκ−2[. For any δ ∈ ]0, 1[ and k ∈ N,\nP\n(\n‖Âk −Ak‖HS ≤ 2κ4\nm2 +\n64κ4 9 √ m log 2 δ\n)\n≥ 1− δ (C.7)\nProof. We first show a useful decomposition. Recall that\nÂk = 1\nm\nm ∑\nj=2\n1\nm\nm ∏\ni=j+1\n( I − γk m Txi ) Txj\nj−1 ∑\nl=1\nTxj Ak = 1\nm\nm ∑\nj=2\nm ∏\ni=j+1\n( I − γk m T ) T 1 m\nj−1 ∑\nl=1\nT .\nIf we set for j ∈ {2, . . . ,m}\nB̂k,j =\n\n\nm ∏\ni=j+1\n( I − γk m Txi )\n\n Txj , Bk,j =\n\n\nm ∏\ni=j+1\n( I − γk m T )\n\nT\nwe have\nÂk −Ak = 1\nm\nm ∑\nj=2\nj − 1 m B̂k,j\n(\n1\nj − 1\nj−1 ∑\nl=1\nTxl\n)\n− 1 m\nm ∑\nj=2\nBk,j 1\nm\nj−1 ∑\nl=1\nT (C.8)\n= 1\nm\n\n\nm ∑\nj=2\nj − 1 m B̂k,j\n(\n1\nj − 1\nj−1 ∑\nl=1\nTxl − T ) +QmT\n\n ,\nwith\nQm =\nm ∑\nj=2\nj − 1 m (B̂k,j −Bk,j) . (C.9)\nWe next bound each term appearing in the decomposition in equation (C.8). Let (γk)k∈N be a sequence in ]0,mκ−2[. By Lemma C.4, with probability greater than 1− δ,\n∥ ∥ ∥ 1\nj − 1\nj−1 ∑\nl=1\nTxl − T ∥ ∥ ∥ HS ≤ 8κ\n2\n3 √ j − 1 log\n2 δ . (C.10)\nOn the other hand\n‖B̂k,j‖ ≤ m ∏\ni=j+1\n∥ ∥ ∥I − γk m Txi ∥ ∥ ∥‖Txj‖ ≤ κ2. (C.11)\nNote that ∑m j=2 j−1 m B̂k,j\n(\n1 j−1 ∑j−1 l=1 Txl − T\n)\nis Hilbert-Schmidt, for Txl and T are Hilbert-\nSchmidt operators, with ‖Txl‖ ≤ κ2 and ‖T ‖HS ≤ κ2, and the family of Hilbert-Schmidt\noperators is an ideal with respect to the composition in L(H) (see TheoremVI.22 and Exercise 28 in [30]). Therefore, combining (C.10) and (C.11), and by noting that\n∑m j=1\n√ j − 1 ≤\n2m3/2/3\n1\nm\n∥ ∥ ∥\nm ∑\nj=2\nj − 1 m B̂k,j ( 1 j − 1\nj−1 ∑\nl=1\nTxl − T )∥ ∥ ∥ HS ≤ 8κ\n4\n3m2 log\n2\nδ\nm ∑\nj=2\n√ j − 1 ≤ 16κ 4\n9 1√ m log 2 δ\n(C.12) holds with probability greater than 1− δ, for any δ ∈ ]0, 1].\nNext we write the quantity ∑m j=2 j−1 m (B̂k,j − Bk,j) appearing in the second term in (C.8) as the sum of a martingale and a bounded term. For short, we set γ = γkm and for all j ∈ {2, . . . ,m} we denote\nΠ̂mj+1 = l − 1 m\nm ∏\ni=j+1\n(I − γTxi) , Πmj+1 = j − 1 m\nm−1 ∏\ni=j+1\n(I − γT ) ,\nso that from the definition of Qm in (C.9),\nQm =\nm ∑\nj=2\n(Π̂mj+1Txj −Πmj+1T ),\nfor all m > 1. We can derive a recursive update for the quantity Qm as follows\nQm+1 =\nm+1 ∑\nj=2\n(Π̂m+1j+1 Txj −Πm+1j+1 T )\n= m\nm+ 1\n[ (Txm+1 − T ) + m ∑\nj=2\n(Π̂m+1j+1 Txj −Πm+1j+1 T ) ]\n= m\nm+ 1\n[ (Txm+1 − T ) + m ∑\nj=2\n((I − γTxm+1)Π̂mj+1Txj − (I − γT )Πmj+1T ) ]\n= m\nm+ 1\n[ (Txm+1 − T ) + (I − γTxm+1) m ∑\nj=2\n(Π̂mj+1Txj −Πmj+1T )\n+ γ(T − Txm+1) m ∑\nj=2\nΠmj+1T ]\n= m\nm+ 1 (I − γTxm+1)Qm +\nm\nm+ 1 (Txm+1 − T )\n( I − γ m ∑\nj=2\nΠmj+1T ) .\nApplying equation (A.1), we get\nQm =\nm ∏\nl=3\nl − 1 l ( I − γk m Txl ) 1 2 (Tx2 − T ) + ξk,l (C.13)\nwhere\nξk,l =\nm ∑\nl=3\nm ∏\ni=l+1\ni− 1 i ( I − γk m Txi ) l− 1 l (Txl − T )\n\nI − γk m\nl−1 ∑\nj=2\nj − 1 l − 1\nl−1 ∏\ni=j+1\n( I − γk m T ) T\n\n .\nFor all l = 3, . . . ,m\nE[ξk,l] = 0,\nbeing Tx3 , . . . , Txm independent and E[(Txl − T )] = 0 for all l = 3, . . . ,m. Moreover the conditional expectation\nE[ξk,l | ξk,l+1, . . . , ξk,m] = 0,\nsince Txl is independent fromTxl+1, . . . , Txm . Therefore the sequence (ξk,l) for l = 3, . . . ,m is a martingale difference sequence for all k.\nThe operator ξk,l is Hilbert-Schmidt, since it is the composition of a Hilbert-Schmidt operator with a continuous one. Next, since the operator T is compact and self-adjoint and 0 ≤ γk/m ≤ 1/‖T ‖, from the spectral mapping theorem, for every l ∈ {3, . . . ,m}\n‖I − γk m\nl−1 ∑\nj=2\nj − 1 l − 1\nl−1 ∏\ni=j+1\n( I − γk m T ) T ‖ = sup x∈[0,1]\n∣ ∣ ∣1− 1 l − 1\nl−1 ∑\nj=2\n(j − 1) (1− x)l−j−1 x ∣ ∣ ∣ .\nWe have\n0 ≤ 1 l − 1\nl−1 ∑\nj=2\n(j − 1) (1− x)l−j−1 x ≤ l−1 ∑\nj=2\n(1− x)l−j−1 x\n=\nl−1 ∑\nj=2\n(1− x)l−j−1 − (1− x)l−j = 1− (1− x)l−2 ≤ 1\nTherefore\n‖I − γk m\nl−1 ∑\nj=2\nj − 1 l − 1\nl−1 ∏\ni=j+1\n( I − γk m T ) T ‖ ≤ 1 .\nUsing the last inequality, we derive\n‖ξk,l‖HS ≤ l − 1 l ∥ ∥ ∥\nm ∏\ni=l+1\ni− 1 i ( I − γk m Txi )∥ ∥ ∥‖Txl − T ‖HS‖I − γk m\nl−1 ∑\nj=2\nj − 1 l − 1\nl−1 ∏\ni=j+1\n( I − γk m T ) T ‖\n≤ l − 1 l ‖Txl − T ‖HS m ∏\ni=l+1\ni− 1 i\n≤ l − 1 m (‖Txj‖HS + ‖T ‖HS) ≤ 2κ2 .\nThen applying Theorem A.1 to ξk,l, with probability greater than 1− δ\n‖ 1 m\nm ∑\nl=3\nξk,l‖ ≤ 16κ2\n3 √ m\nlog 2\nδ . (C.14)\nOn the other hand, ∥\n∥ ∥ ∥ ∥\nm ∏\nl=3\nl − 1 l ( I − γk m Txl ) Tx2 − T 2\n∥ ∥ ∥ ∥ ∥ ≤ 2κ2/m, (C.15)\ntherefore, combining (C.14) with (C.15), and recalling (C.13), with probability greater than 1− δ\n1\nm\n∥ ∥ ∥QmT ∥ ∥ ∥\nHS ≤\n(\n2κ2 m2 + 16κ2 3 √ m log 2 δ\n)\nκ2 (C.16)\nThe statement then follows recalling the decomposition in (C.8), and summing (C.16) with (C.12).\nLEMMA C.6. Let (γk)k∈N be a sequence in ]0,mκ−2[. For any δ ∈ ]0, 1[ and k ∈ N,\nP\n(\n‖b̂k − bk‖H ≤ 2κ2M2\nm2 +\n64κ2M2\n9 √ m\nlog 2\nδ\n)\n≥ 1− δ (C.17)\nProof. Using the same notation as in Lemma C.5, from the definition of b̂k and bk in equations (B.3) and (B.10) respectively, we have\nb̂k − bk = 1\nm2\nm ∑\nj=1\nB̂k,j\nj−1 ∑\nl=1\nS∗xlyl − 1\nm2\nm ∑\nj=1\nBk,j\nj−1 ∑\nl=1\nS∗fρ . (C.18)\nStarting from this decomposition, the proof follows the same line as the one of Lemma C.5.\nWe are now ready to prove the probabilistic upper bound on the sample error. Proof. of Theorem 6.1 By Lemma C.1 we get\nf̂t − ft = t−1 ∑\nk=0\nγk\n\n\nt−1 ∏\nj=k+1\n(I − γjT + γ2jAj)\n\nχk (C.19)\nwith χk defined as in (C.2), for f̂0 = f0 = 0. Hence, applying Lemma C.2,\n‖f̂t − ft‖H ≤ t−1 ∑\nk=1\nγk‖χk‖H. (C.20)\nRecalling that\nχk = (T − T̂ )f̂k + γk(Âk −Ak)f̂k + ( 1\nm\nm ∑\ni=1\nŜ∗xiyi − S ∗fρ) + γk(bk − b̂k) .\nthen, by Lemmas C.3, C.4, C.6, and C.5, we get\n‖χk‖H ≤ ( 16κ2\n3 √ m\nlog 2\nδ + γk\n(2κ4\nm2 +\n64κ4 9 √ m log 2 δ )\n)\nκM\nk−1 ∑\ni=0\nγi\n+ 16κM\n3 √ m\nlog 2\nδ + γk\n(2κ2M2\nm2 +\n64κ2M2\n9 √ m\nlog 2\nδ\n)\nThe previous bound implies that, (if δ is sufficiently small) there exists a constant C > 0 such that\n‖χk‖H ≤ C√ m\n(\nlog 2\nδ\n) k−1 ∑\ni=0\nγi . (C.21)\nTherefore, from (C.20) we get\n‖f̂t − ft‖H ≤ C√ m\n(\nlog 2\nδ\n) t−1 ∑\nk=0\nγk\nk−1 ∑\ni=0\nγi .\nNext, if (∀i ∈ N), γi = κ−2(i+ 1)−θ, then ∑k−1 i=0 γi ≤ 1κ2 (1 − θ)k1−θ and thus t−1 ∑\nk=0\nγk\nk−1 ∑\ni=0\nγi ≤ 1\nκ4(1− θ)\nt−1 ∑\nk=0\n(k + 1)1−2θ .\nAn analogous computation shows that\n1\nκ4(1− θ)\nt−1 ∑\nk=0\n(k + 1)1−2θ ≤ 1 2κ4(1− θ)2 (t+ 1) 2(1−θ),\nwhich gives the statement. Proof. of Theorem 6.2 Fix m ∈ N, m ≥ 2. Let Pfρ be the projection onto the closure (in L2(X , ρX )) of the range of S. By Remark B.3, we know that the t-th epoch of the incremental gradient descent iteration in (B.9) coincides with m steps of gradient descent with stepsize γt/m. More precisely, (∀t ∈ N), ft = hmt, where h0 = 0 and\nhk+1 = (I − ηkT )hk + ηkS∗Pfρ, ηk = γt m , for k ∈ [mt, (m+ 1)t− 1] .\nFrom equation (A.1), it follows\nhk+1 =\nk ∑\nj=0\nk ∏\ni=j+1\n( I − ηiT ) ηjS ∗Pfρ .\nHence\n∥ ∥Shk+1 − Pfρ ∥ ∥\nρ =\n∥ ∥(S\nk ∑\nj=0\nηj\nk ∏\ni=j+1\n( I − ηiT ) S∗ − I)Pfρ‖ρ (C.22)\nBy the spectral theorem, see e.g. [17, equation (2.43)],\n∥ ∥Shk+1 − Pfρ ∥ ∥\nρ =\n∥ ∥(L\nk ∑\nj=0\nηj\nk ∏\ni=j+1\n( I − ηiL ) − I)Pfρ‖ρ . (C.23)\nDefine the spectral function rt :]0, ‖L‖[→ R\nrt(λ) = λ k ∑\nj=0\nηj\nk ∏\ni=j+1\n( I − ηiλ ) − 1 . (C.24)\nIt follows from the definition that Pfρ ∈ R(L), for R(S) = R(L) by [35, Theorem 11.2(b)]. Thus, Pfρ ∈ N(L)⊥. Then, by (C.23) and definition of rt, if (σn)n∈N are the strictly positive eigenvalues of L, and (vn)n∈N is the corresponding family of eigenvectors in L2(X , ρX ), ∥\n∥Shk+1 − Pfρ ∥ ∥ 2 ρ = ‖rt(L)Pfρ‖2ρ =\n+∞ ∑\nn=1\n(\nσn\nk ∑\nj=0\nηj\nk ∏\ni=j+1\n( 1− ηiσn ) − 1 )2 |〈Pfρ, vn〉|2\n=\n+∞ ∑\nn=1\n( k ∏\ni=0\n( 1− ηiσn )\n)2\n|〈Pfρ, vn〉|2 (C.25)\n(C.26)\nNote that (∀n ∈ N), ηiσn ∈ ]0, 1[, and\n0 ≤ k ∏\ni=0\n( 1− ηiσn ) ≤ e−σn ∑k i=0 ηi .\nSince σn > 0 and limk→∞ ∑k i=0 ηi = +∞, it follows that limk→+∞ e−σn ∑k\ni=0 ηi = 0. On the other hand,\n0 ≤ +∞ ∑\nn=1\n( k ∏\ni=0\n( 1− ηiσn )\n)2\n|〈Pfρ, vn〉|2 ≤ ‖Pfρ‖2ρ. (C.27)\nHence, from equation (C.25)\nlim k→+∞\n‖Shk+1 − Pfρ ∥ ∥ 2\nρ = lim\nk→+∞\n+∞ ∑\nn=1\n( k ∏\ni=0\n( 1− ηiσn )\n)2\n|〈Pfρ, vn〉|2 = 0\nwhere the last equality follows by applying the dominated convergence theorem, which can be used thanks to (C.27). Equation (6.2) then follows recalling that ft = hmt.\nIf condition (4.2) holds, we can apply [37, Theorem 2.10] to (hk)k∈N. We have\nE(ft)− inf f∈H E = E(hmt− inf f∈H\nE) ≤ (r/e)2r (\nt−1 ∑\nj=0\n( (j+1)m−1 ∑\ni=jm\nγj m\n))−2r\n= (r/e)2r(\nt−1 ∑\nj=0\nγj) −2r .\n(C.28)\nAppendix D. Consistency with respect to the norm in H. In this section we collect two main theorems, which are the analogous of Theorems 4.1 and 4.2, respectively.\nTHEOREM D.1. Under the same assumptions as in Theorem 4.1, if E has minimizers on H, and fH is the one of minimal norm,\nlim m→∞\n‖f̂t∗ − fH‖H = 0 almost surely.\nTHEOREM D.2. Under the same assumptions as in Theorem 4.2, suppose additionally that r ≥ 1/2. Then E has minimizers, so that fH exists. Moreover, for some constant C > 0,\n‖f̂t − fH‖H ≤ C 2κ4(1 − θ)2 ( log 2 δ ) (t+ 1)2(1−θ) m + R((r − 1/2)/e)(r−1/2) κ2(1− θ) t (−r+1/2)(1−θ) . (D.1) If we choose the stopping rule t∗(m) = ⌈m 1 (3+2r)(1−θ) ⌉ then there exists a constant D such that, with probability at least 1− δ,\n‖f̂t∗(m))− fH‖H ≤ D log 2\nδ m−\n2r−1 3+2r . (D.2)\nThe proof of the previous theorems rely on the following decomposition into sample ad approximation error\n‖f̂t − fH‖H ≤ ‖f̂t − ft‖H + ‖ft − fH‖H. (D.3)\nTo estimate the sample error, we can rely on Theorem 6.1. An upper bound of the approximation error is given in the following theorem\nTHEOREM D.3 (Approximation error in H). Let f0 = 0, (γt)t∈N be such that (∀t ∈ N) γt ∈]0,mκ−2[, ∑\nt∈N γt = +∞ and (ft)t∈N be defined as in (B.9). Assume that (4.2) is satisfied for some r ≥ 1/2. Then fH exists and\n‖ft − fH‖H ≤ R((r − 1/2)/e)(r−1/2)( t−1 ∑\ni=0\nγi) −r+1/2 .\nIn addition, if γt = κ−2(t+ 1)−θ for some θ ∈ [0, 1[\n‖ft − fH‖H ≤ R((r − 1/2)/e)(r−1/2)\nκ2(1− θ) t (−r+1/2)(1−θ) .\nThe theorem follows from [37, Theorem 2.10]. Proof. of Theorem D.1 Equation (D.1) immediately follows by applying Theorems 6.1 and D.3. Next let t∗(m) = ⌈mα⌉. substituting this choice of t into the right hand side of (D.1), and minimizing over α, we get the linear equation\n2α(1− θ)− 1 = α(−r + 1/2)(1− θ), (D.4)\nwhose solution is α = 1/((3 + 2r)(1 − θ))."
    } ],
    "references" : [ {
      "title" : "Theory of reproducing kernels",
      "author" : [ "N. Aronszajn" ],
      "venue" : "Trans. Amer. Math. Soc,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1950
    }, {
      "title" : "Non-strongly-convex smooth stochastic approximation with convergence rate o(1/n)",
      "author" : [ "F. Bach", "E. Moulines" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2013
    }, {
      "title" : "On regularization algorithms in learning theory",
      "author" : [ "Frank Bauer", "Sergei Pereverzev", "Lorenzo Rosasco" ],
      "venue" : "Journal of complexity,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2007
    }, {
      "title" : "A new class of incremental gradient methods for least squares problems",
      "author" : [ "Dimitri P. Bertsekas" ],
      "venue" : "SIAM J. Optim.,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1997
    }, {
      "title" : "Gradient convergence in gradient methods with errors",
      "author" : [ "Dimitri P Bertsekas", "John N Tsitsiklis" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2000
    }, {
      "title" : "Optimal learning rates for kernel conjugate gradient regression",
      "author" : [ "G. Blanchard", "N. Krämer" ],
      "venue" : "In Advances in Neural Inf. Proc. Systems (NIPS),",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2010
    }, {
      "title" : "Optimal rates for regularized least-squares algorithm",
      "author" : [ "A. Caponnetto", "E. De Vito" ],
      "venue" : "Found. Comput. Math.,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2006
    }, {
      "title" : "Adaptive rates for regularization operators in learning theory",
      "author" : [ "A. Caponnetto", "Yuan Yao" ],
      "venue" : "Analysis and Applications,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2010
    }, {
      "title" : "On the generalization ability of on-line learning algorithms",
      "author" : [ "N. Cesa-Bianchi", "A. Conconi", "C. Gentile" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2004
    }, {
      "title" : "Prediction, learning, and games",
      "author" : [ "Nicolò Cesa-Bianchi", "Gábor Lugosi" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2006
    }, {
      "title" : "On the mathematical foundations of learning",
      "author" : [ "Felipe Cucker", "Steve Smale" ],
      "venue" : "Bulletin of the American Mathematical Society,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2002
    }, {
      "title" : "Learning Theory: An Approximation Theory Viewpoint",
      "author" : [ "Felipe Cucker", "Ding Xuan Zhou" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2007
    }, {
      "title" : "Model selection for regularized least-squares algorithm in learning theory",
      "author" : [ "E. De Vito", "A. Caponnetto", "L. Rosasco" ],
      "venue" : "Found. Comput. Math.,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2005
    }, {
      "title" : "Learning from examples as an inverse problem",
      "author" : [ "E. De Vito", "L. Rosasco", "A. Caponnetto", "U. De Giovannini", "F. Odone" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2005
    }, {
      "title" : "A Probabilistic Theory of Pattern Recognition",
      "author" : [ "L. Devroye", "L. Györfi", "G. Lugosi" ],
      "venue" : "Number 31 in Applications of mathematics",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1996
    }, {
      "title" : "Sample-based non-uniform random variate generation",
      "author" : [ "Luc Devroye" ],
      "venue" : "In Proceedings of the 18th conference on Winter simulation,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1986
    }, {
      "title" : "Regularization of inverse problems",
      "author" : [ "H.W. Engl", "M. Hanke", "A. Neubauer" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1996
    }, {
      "title" : "Random design analysis of ridge regression",
      "author" : [ "Daniel Hsu", "Sham M Kakade", "Tong Zhang" ],
      "venue" : "Journal of Machine Learning Research-Proceedings Track,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2012
    }, {
      "title" : "Online learning with kernels",
      "author" : [ "Jyrki Kivinen", "Alexander J Smola", "Robert C Williamson" ],
      "venue" : "Signal Processing, IEEE Transactions on,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2004
    }, {
      "title" : "Stochastic approximation algorithms and applications, volume 35 of Applications of Mathematics (New York)",
      "author" : [ "Harold J. Kushner", "G. George Yin" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1997
    }, {
      "title" : "A stochastic gradient method with an exponential convergence rate for strongly-convex optimization with finite training sets",
      "author" : [ "Nicolas Le Roux", "Mark Schmidt", "Francis Bach" ],
      "venue" : "arXiv preprint arXiv:1202.6258,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2012
    }, {
      "title" : "Efficient backprop",
      "author" : [ "Y. LeCun", "L. Bottou", "G. Orr", "K. Muller" ],
      "venue" : "Neural Networks: Tricks of the trade. Springer,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1998
    }, {
      "title" : "Spectral algorithms for supervised learning",
      "author" : [ "Laura Lo Gerfo", "Lorenzo Rosasco", "Francesca Odone", "Ernesto De Vito", "Alessandro Verri" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2008
    }, {
      "title" : "Regularization in kernel learning",
      "author" : [ "Shahar Mendelson", "Joseph Neeman" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2010
    }, {
      "title" : "Incremental subgradient methods for nondifferentiable optimization",
      "author" : [ "Angelia Nedic", "Dimitri P Bertsekas" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2001
    }, {
      "title" : "Robust stochastic approximation approach to stochastic programming",
      "author" : [ "A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro" ],
      "venue" : "SIAM J. Optim.,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2008
    }, {
      "title" : "A method for unconstrained convex minimization problem with the rate of convergence o(1/k)",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Doklady AN SSSR,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 1983
    }, {
      "title" : "Optimum bounds for the distributions of martingales in Banach spaces",
      "author" : [ "Iosif Pinelis" ],
      "venue" : "Ann. Probab.,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 1994
    }, {
      "title" : "Early stopping for non-parametric regression: An optimal data-dependent stopping rule",
      "author" : [ "Garvesh Raskutti", "Martin J Wainwright", "Bin Yu" ],
      "venue" : "In Communication, Control, and Computing (Allerton),",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2011
    }, {
      "title" : "Methods of modern mathematical physics. I. Functional analysis",
      "author" : [ "Michael Reed", "Barry Simon" ],
      "venue" : null,
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 1972
    }, {
      "title" : "On the influence of the kernel on the consistency of support vector machines",
      "author" : [ "Ingo Steinwart" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2002
    }, {
      "title" : "Support Vector Machines",
      "author" : [ "Ingo Steinwart", "Andreas Christmann" ],
      "venue" : null,
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2008
    }, {
      "title" : "Optimal rates for regularized least squares regression",
      "author" : [ "Ingo Steinwart", "Don R. Hush", "Clint Scovel" ],
      "venue" : "In COLT,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2009
    }, {
      "title" : "Online learning as stochastic approximation of regularization paths",
      "author" : [ "Pierre Tarrès", "Yuan Yao" ],
      "venue" : "arXiv preprint arXiv:1103.5538,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2011
    }, {
      "title" : "Introduction to functional analysis",
      "author" : [ "Angus E. Taylor", "David C. Lay" ],
      "venue" : "2nd ed. (Reprint of the orig",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 1980
    }, {
      "title" : "User-friendly tail bounds for sums of random matrices",
      "author" : [ "Joel A. Tropp" ],
      "venue" : "Foundations of Computational Mathematics,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2012
    }, {
      "title" : "On early stopping in gradient descent learning",
      "author" : [ "Yuan Yao", "Lorenzo Rosasco", "Andrea Caponnetto" ],
      "venue" : "Constructive Approximation,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2007
    }, {
      "title" : "Online gradient descent learning algorithms",
      "author" : [ "Yiming Ying", "Massimiliano Pontil" ],
      "venue" : "Foundations of Computational Mathematics,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2008
    }, {
      "title" : "Sums and Gaussian vectors, volume 1617 of Lecture Notes in Mathematics",
      "author" : [ "Vadim Yurinsky" ],
      "venue" : null,
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 1995
    }, {
      "title" : "Learning bounds for kernel regression using effective data dimensionality",
      "author" : [ "Tong Zhang" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2005
    }, {
      "title" : "Boosting with early stopping: Convergence and consistency",
      "author" : [ "Tong Zhang", "Bin Yu" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 21,
      "context" : "[22].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 40,
      "context" : "While early stopping has been recently studied for classical gradient descent learning and related algorithms [41, 37, 3, 6, 8, 29], we are not aware of similar studies for online learning.",
      "startOffset" : 110,
      "endOffset" : 131
    }, {
      "referenceID" : 36,
      "context" : "While early stopping has been recently studied for classical gradient descent learning and related algorithms [41, 37, 3, 6, 8, 29], we are not aware of similar studies for online learning.",
      "startOffset" : 110,
      "endOffset" : 131
    }, {
      "referenceID" : 2,
      "context" : "While early stopping has been recently studied for classical gradient descent learning and related algorithms [41, 37, 3, 6, 8, 29], we are not aware of similar studies for online learning.",
      "startOffset" : 110,
      "endOffset" : 131
    }, {
      "referenceID" : 5,
      "context" : "While early stopping has been recently studied for classical gradient descent learning and related algorithms [41, 37, 3, 6, 8, 29], we are not aware of similar studies for online learning.",
      "startOffset" : 110,
      "endOffset" : 131
    }, {
      "referenceID" : 7,
      "context" : "While early stopping has been recently studied for classical gradient descent learning and related algorithms [41, 37, 3, 6, 8, 29], we are not aware of similar studies for online learning.",
      "startOffset" : 110,
      "endOffset" : 131
    }, {
      "referenceID" : 28,
      "context" : "While early stopping has been recently studied for classical gradient descent learning and related algorithms [41, 37, 3, 6, 8, 29], we are not aware of similar studies for online learning.",
      "startOffset" : 110,
      "endOffset" : 131
    }, {
      "referenceID" : 19,
      "context" : "The situation typically analyzed in this context is the one where each iteration of the algorithm corresponds to a new input-output pair [20].",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 24,
      "context" : "[25], we are not interested in convergence of the algorithm to the minimum of the empirical risk, but rather to the one of the expected risk.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 25,
      "context" : "In particular, the gradients are not bounded, unlike in most studies in stochastic optimization [26].",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 1,
      "context" : "Here we note that, the results more closely related to the analysis we present are those in [2, 34, 38] where consistency and finite sample bounds are derived.",
      "startOffset" : 92,
      "endOffset" : 103
    }, {
      "referenceID" : 33,
      "context" : "Here we note that, the results more closely related to the analysis we present are those in [2, 34, 38] where consistency and finite sample bounds are derived.",
      "startOffset" : 92,
      "endOffset" : 103
    }, {
      "referenceID" : 37,
      "context" : "Here we note that, the results more closely related to the analysis we present are those in [2, 34, 38] where consistency and finite sample bounds are derived.",
      "startOffset" : 92,
      "endOffset" : 103
    }, {
      "referenceID" : 1,
      "context" : "In [2] a single pass over the data is shown to suffice, however the analysis is restricted to a finite dimensional setting, and the generalization performance is empirically shown to be still increasing after the first epoch.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 33,
      "context" : "In [34] and [38] the step size and/or a penalization parameter need to be chosen in a distribution dependent way (or by cross validation) and multiple passes over ∗ DIBRIS, Università di Genova, Via Dodecaneso, 35, 16146, Genova, Italy, (lrosasco@mit.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 37,
      "context" : "In [34] and [38] the step size and/or a penalization parameter need to be chosen in a distribution dependent way (or by cross validation) and multiple passes over ∗ DIBRIS, Università di Genova, Via Dodecaneso, 35, 16146, Genova, Italy, (lrosasco@mit.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 31,
      "context" : "[32] Lemma A.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "Further, consider a separable reproducing kernel Hilbert space (RKHS) H [1], with inner product (norm) denoted by 〈·, ·〉H (‖·‖H), and a measurable reproducing kernelK : X×X → R.",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 31,
      "context" : "2) When H is universal [32] this is exactly universal consistency, see for example [15].",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 14,
      "context" : "2) When H is universal [32] this is exactly universal consistency, see for example [15].",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 15,
      "context" : "As it is known, the latter result can only be derived under suitable assumptions on ρ and it is equivalent to considering learning rates [16, 31].",
      "startOffset" : 137,
      "endOffset" : 145
    }, {
      "referenceID" : 30,
      "context" : "As it is known, the latter result can only be derived under suitable assumptions on ρ and it is equivalent to considering learning rates [16, 31].",
      "startOffset" : 137,
      "endOffset" : 145
    }, {
      "referenceID" : 25,
      "context" : "Note however that due to the lack of strong convexity, unboundedness of X and unboundedness of the gradients, the classical assumptions required to apply stochastic gradient descent are not satisfied [26].",
      "startOffset" : 200,
      "endOffset" : 204
    }, {
      "referenceID" : 4,
      "context" : "We consider the estimator obtained applying the incremental gradient descent (IGD) algorithm [5, 4] to empirical risk minimization, inf f∈H Ê(f), Ê(f) = 1 m m ∑",
      "startOffset" : 93,
      "endOffset" : 99
    }, {
      "referenceID" : 3,
      "context" : "We consider the estimator obtained applying the incremental gradient descent (IGD) algorithm [5, 4] to empirical risk minimization, inf f∈H Ê(f), Ê(f) = 1 m m ∑",
      "startOffset" : 93,
      "endOffset" : 99
    }, {
      "referenceID" : 24,
      "context" : "stochastic [25], can be considered and might lead to different behaviors, but we leave the analysis of these latter cases for future study.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 18,
      "context" : ", k = i (c t )k, k 6= i The above algorithm is closely related, yet different, to the one discussed in [19], where an explicit regularization is considered (choose λk > 0 in the following Equation (3.",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 40,
      "context" : "Our results extend to incremental gradient the analysis of early stopping carried out in [41, 37, 3, 6, 29] for the classical (batch) gradient descent and shed light on the effect of considering multiple passes over the data in online learning algorithms.",
      "startOffset" : 89,
      "endOffset" : 107
    }, {
      "referenceID" : 36,
      "context" : "Our results extend to incremental gradient the analysis of early stopping carried out in [41, 37, 3, 6, 29] for the classical (batch) gradient descent and shed light on the effect of considering multiple passes over the data in online learning algorithms.",
      "startOffset" : 89,
      "endOffset" : 107
    }, {
      "referenceID" : 2,
      "context" : "Our results extend to incremental gradient the analysis of early stopping carried out in [41, 37, 3, 6, 29] for the classical (batch) gradient descent and shed light on the effect of considering multiple passes over the data in online learning algorithms.",
      "startOffset" : 89,
      "endOffset" : 107
    }, {
      "referenceID" : 5,
      "context" : "Our results extend to incremental gradient the analysis of early stopping carried out in [41, 37, 3, 6, 29] for the classical (batch) gradient descent and shed light on the effect of considering multiple passes over the data in online learning algorithms.",
      "startOffset" : 89,
      "endOffset" : 107
    }, {
      "referenceID" : 28,
      "context" : "Our results extend to incremental gradient the analysis of early stopping carried out in [41, 37, 3, 6, 29] for the classical (batch) gradient descent and shed light on the effect of considering multiple passes over the data in online learning algorithms.",
      "startOffset" : 89,
      "endOffset" : 107
    }, {
      "referenceID" : 21,
      "context" : "Indeed, early stopping has long been applied as a heuristic to achieve regularization, especially in the context of neural networks [22].",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 18,
      "context" : "Here, we added an explicit regularization parameter λ as in [19], see also (3.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 33,
      "context" : "The question of the consistency of online learning algorithms when H can be infinite dimensional has been considered in [34, 38] (see also references therein).",
      "startOffset" : 120,
      "endOffset" : 128
    }, {
      "referenceID" : 37,
      "context" : "The question of the consistency of online learning algorithms when H can be infinite dimensional has been considered in [34, 38] (see also references therein).",
      "startOffset" : 120,
      "endOffset" : 128
    }, {
      "referenceID" : 33,
      "context" : "In [34] it is shown that if the step size γk and the regularization parameter λk are chosen as suitable functions of the number of points, then the corresponding algorithm can be universally consistent with probability one.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 37,
      "context" : "In [38] it is shown that indeed similar results can be derived for λk = 0 for all k ∈ N, but an horizon, that is the total number of points to be considered, needs to be known a priori to appropriately choose the step-size.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 1,
      "context" : "Recently, a similar analysis, λk = 0 for all k ∈ N, is developed in [2] in a finite dimensional setting.",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 4,
      "context" : "increases, see [5] and references therein.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 20,
      "context" : "[21].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 9,
      "context" : "[10].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 8,
      "context" : "If the data are indeed generated by a stochastic process, then there is a classic approach, sometimes called online-to-batch conversion [9], to convert regret bounds to expected risk bounds.",
      "startOffset" : 136,
      "endOffset" : 139
    }, {
      "referenceID" : 12,
      "context" : "2) is fairly standard (see [13], and [12, Section 4] for a discussion).",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 6,
      "context" : "Lower bounds are known [7, 33] under assumption (4.",
      "startOffset" : 23,
      "endOffset" : 30
    }, {
      "referenceID" : 32,
      "context" : "Lower bounds are known [7, 33] under assumption (4.",
      "startOffset" : 23,
      "endOffset" : 30
    }, {
      "referenceID" : 39,
      "context" : "6) This latter property can be interpreted as a measure of the effective dimensionality of the hypotheses space [40, 7].",
      "startOffset" : 112,
      "endOffset" : 119
    }, {
      "referenceID" : 6,
      "context" : "6) This latter property can be interpreted as a measure of the effective dimensionality of the hypotheses space [40, 7].",
      "startOffset" : 112,
      "endOffset" : 119
    }, {
      "referenceID" : 10,
      "context" : "2) implies that the infimum of the expected risk over H is achieved [11], and it is known that sharp bounds are harder to get if r < 1/2 (see discussion in [33]).",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 32,
      "context" : "2) implies that the infimum of the expected risk over H is achieved [11], and it is known that sharp bounds are harder to get if r < 1/2 (see discussion in [33]).",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 16,
      "context" : "The bound is proved generalizing classical results in inverse problems and is known to be essentially sharp [17].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 39,
      "context" : "kernel ridge regression) has been considered in [40, 7, 33, 24, 18], and iterative methods, as well as a larger class of so called spectral filtering methods [23] have been analyzed in [41, 37, 3, 6, 8, 29].",
      "startOffset" : 48,
      "endOffset" : 67
    }, {
      "referenceID" : 6,
      "context" : "kernel ridge regression) has been considered in [40, 7, 33, 24, 18], and iterative methods, as well as a larger class of so called spectral filtering methods [23] have been analyzed in [41, 37, 3, 6, 8, 29].",
      "startOffset" : 48,
      "endOffset" : 67
    }, {
      "referenceID" : 32,
      "context" : "kernel ridge regression) has been considered in [40, 7, 33, 24, 18], and iterative methods, as well as a larger class of so called spectral filtering methods [23] have been analyzed in [41, 37, 3, 6, 8, 29].",
      "startOffset" : 48,
      "endOffset" : 67
    }, {
      "referenceID" : 23,
      "context" : "kernel ridge regression) has been considered in [40, 7, 33, 24, 18], and iterative methods, as well as a larger class of so called spectral filtering methods [23] have been analyzed in [41, 37, 3, 6, 8, 29].",
      "startOffset" : 48,
      "endOffset" : 67
    }, {
      "referenceID" : 17,
      "context" : "kernel ridge regression) has been considered in [40, 7, 33, 24, 18], and iterative methods, as well as a larger class of so called spectral filtering methods [23] have been analyzed in [41, 37, 3, 6, 8, 29].",
      "startOffset" : 48,
      "endOffset" : 67
    }, {
      "referenceID" : 22,
      "context" : "kernel ridge regression) has been considered in [40, 7, 33, 24, 18], and iterative methods, as well as a larger class of so called spectral filtering methods [23] have been analyzed in [41, 37, 3, 6, 8, 29].",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 40,
      "context" : "kernel ridge regression) has been considered in [40, 7, 33, 24, 18], and iterative methods, as well as a larger class of so called spectral filtering methods [23] have been analyzed in [41, 37, 3, 6, 8, 29].",
      "startOffset" : 185,
      "endOffset" : 206
    }, {
      "referenceID" : 36,
      "context" : "kernel ridge regression) has been considered in [40, 7, 33, 24, 18], and iterative methods, as well as a larger class of so called spectral filtering methods [23] have been analyzed in [41, 37, 3, 6, 8, 29].",
      "startOffset" : 185,
      "endOffset" : 206
    }, {
      "referenceID" : 2,
      "context" : "kernel ridge regression) has been considered in [40, 7, 33, 24, 18], and iterative methods, as well as a larger class of so called spectral filtering methods [23] have been analyzed in [41, 37, 3, 6, 8, 29].",
      "startOffset" : 185,
      "endOffset" : 206
    }, {
      "referenceID" : 5,
      "context" : "kernel ridge regression) has been considered in [40, 7, 33, 24, 18], and iterative methods, as well as a larger class of so called spectral filtering methods [23] have been analyzed in [41, 37, 3, 6, 8, 29].",
      "startOffset" : 185,
      "endOffset" : 206
    }, {
      "referenceID" : 7,
      "context" : "kernel ridge regression) has been considered in [40, 7, 33, 24, 18], and iterative methods, as well as a larger class of so called spectral filtering methods [23] have been analyzed in [41, 37, 3, 6, 8, 29].",
      "startOffset" : 185,
      "endOffset" : 206
    }, {
      "referenceID" : 28,
      "context" : "kernel ridge regression) has been considered in [40, 7, 33, 24, 18], and iterative methods, as well as a larger class of so called spectral filtering methods [23] have been analyzed in [41, 37, 3, 6, 8, 29].",
      "startOffset" : 185,
      "endOffset" : 206
    }, {
      "referenceID" : 33,
      "context" : "As we mentioned before, online learning algorithms are considered in [34, 38] where optimal bounds are derived in the capacity independent setting.",
      "startOffset" : 69,
      "endOffset" : 77
    }, {
      "referenceID" : 37,
      "context" : "As we mentioned before, online learning algorithms are considered in [34, 38] where optimal bounds are derived in the capacity independent setting.",
      "startOffset" : 69,
      "endOffset" : 77
    }, {
      "referenceID" : 1,
      "context" : "The bound in the finite dimensional setting derived in [2] are also optimal.",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 7,
      "context" : "Our analysis follows the one in [8].",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 28,
      "context" : "We end noting that, while an hold-out procedure requires splitting the data, it does not worsen the computational complexity of the algorithm, unlike other model selection criterions [29].",
      "startOffset" : 183,
      "endOffset" : 187
    }, {
      "referenceID" : 4,
      "context" : "Particularly, stochastic variants of the IGD algorithms, where the indices in each cycle are randomly selected [5], and also accelerated version as proposed in [27].",
      "startOffset" : 111,
      "endOffset" : 114
    }, {
      "referenceID" : 26,
      "context" : "Particularly, stochastic variants of the IGD algorithms, where the indices in each cycle are randomly selected [5], and also accelerated version as proposed in [27].",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 2,
      "context" : "Ideas related to this latter point have been considered in [3, 6, 8] for batch gradient techniques (the gradient of the empirical risk is considered in each iteration).",
      "startOffset" : 59,
      "endOffset" : 68
    }, {
      "referenceID" : 5,
      "context" : "Ideas related to this latter point have been considered in [3, 6, 8] for batch gradient techniques (the gradient of the empirical risk is considered in each iteration).",
      "startOffset" : 59,
      "endOffset" : 68
    }, {
      "referenceID" : 7,
      "context" : "Ideas related to this latter point have been considered in [3, 6, 8] for batch gradient techniques (the gradient of the empirical risk is considered in each iteration).",
      "startOffset" : 59,
      "endOffset" : 68
    }, {
      "referenceID" : 2,
      "context" : "In particular in [3] it is shown that a variant of gradient descent, sometimes called the ν-method, can obtain the same generalization guarantees of non accelerated gradient descent learning, but using much fewer iterations.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 0,
      "context" : "REFERENCES [1] N.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 1,
      "context" : "[2] F.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[3] Frank Bauer, Sergei Pereverzev, and Lorenzo Rosasco.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[4] Dimitri P.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[5] Dimitri P Bertsekas and John N Tsitsiklis.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[6] G.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[7] A.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[8] A.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "[9] N.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "[10] Nicolò Cesa-Bianchi and Gábor Lugosi.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[11] Felipe Cucker and Steve Smale.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[12] Felipe Cucker and Ding Xuan Zhou.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[13] E.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[14] E.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[15] L.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[16] Luc Devroye.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[17] H.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[18] Daniel Hsu, Sham M Kakade, and Tong Zhang.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "[19] Jyrki Kivinen, Alexander J Smola, and Robert C Williamson.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[20] Harold J.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "[21] Nicolas Le Roux, Mark Schmidt, and Francis Bach.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "[22] Y.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "[23] Laura Lo Gerfo, Lorenzo Rosasco, Francesca Odone, Ernesto De Vito, and Alessandro Verri.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "[24] Shahar Mendelson and Joseph Neeman.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "[25] Angelia Nedic and Dimitri P Bertsekas.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 25,
      "context" : "[26] A.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 26,
      "context" : "[27] Y.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 27,
      "context" : "[28] Iosif Pinelis.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 28,
      "context" : "[29] Garvesh Raskutti, Martin J Wainwright, and Bin Yu.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 29,
      "context" : "[30] Michael Reed and Barry Simon.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 30,
      "context" : "[31] Ingo Steinwart.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 31,
      "context" : "[32] Ingo Steinwart and Andreas Christmann.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 32,
      "context" : "[33] Ingo Steinwart, Don R.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 33,
      "context" : "[34] Pierre Tarrès and Yuan Yao.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 34,
      "context" : "[35] Angus E.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 35,
      "context" : "[36] Joel A.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 36,
      "context" : "[37] Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 37,
      "context" : "[38] Yiming Ying and Massimiliano Pontil.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 38,
      "context" : "[39] Vadim Yurinsky.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 39,
      "context" : "[40] Tong Zhang.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 40,
      "context" : "[41] Tong Zhang and Bin Yu.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 27,
      "context" : "2) The following concentration inequality due to [28] (see also [39] and [34, Proposition A.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 38,
      "context" : "2) The following concentration inequality due to [28] (see also [39] and [34, Proposition A.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 35,
      "context" : "While more refined concentration inequality could be considered [36], this would affect only the constants in the bound which are not the main focus of this paper.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 7,
      "context" : "3, we will also use a variant of the previous Bernstein concentration inequality, which is taken from [8].",
      "startOffset" : 102,
      "endOffset" : 105
    }, {
      "referenceID" : 13,
      "context" : "1, taking into account that ‖T ‖HS ≤ κ and ‖Txi‖HS ≤ κ (see also [14]) LEMMA C.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 29,
      "context" : "22 and Exercise 28 in [30]).",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 0,
      "context" : "T ‖ = sup x∈[0,1] ∣",
      "startOffset" : 12,
      "endOffset" : 17
    } ],
    "year" : 2017,
    "abstractText" : "Abstract. We study the learning algorithm corresponding to the incremental gradient descent defined by the empirical risk over an infinite dimensional hypotheses space. We consider a statistical learning setting and show that, provided with a universal step-size and a suitable early stopping rule, the learning algorithm thus obtained is universally consistent and derive finite sample bounds. Our results provide a theoretical foundation for considering early stopping in online learning algorithms and shed light on the effect of allowing for multiple passes over the data.",
    "creator" : "LaTeX with hyperref package"
  }
}