{
  "name" : "1606.01568.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Active-Labelling by Adaptive Huber Loss Regression",
    "authors" : [ "Jacopo Cavazza", "Vittorio Murino" ],
    "emails" : [ "jacopo.cavazza@iit.it." ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms—Robust Scalar Regression, Label Active Selection, Huber Loss with Adaptive Threshold, Convex Optimization, Learning from Noisy Labels, Crowd Counting.\nI. INTRODUCTION\nRegression is one of the most widely studied problems in different research disciplines such as statistics, econometrics, computational biology and physics to name a few, and is a pillar topic in machine learning. Formally, it addresses the problem of inferring a functional relationship between input and output spaces. Under a classical machine learning perspective, this is learnt by means of (training) examples and, to this aim, two mainstream approaches pop up: optimizationbased and Bayesian frameworks. In the former, once a suitable space of functions is fixed, the goal is minimizing an objective functional, where a loss measures how good the learned function reproduces input-output relationships inside the training set. Instead, in the Bayesian formalism, a prior distribution constrains the solution upon some a priori knowledge, while the final regression map maximizes the posterior/likelihood probability distribution.\nA problem which affects both paradigms lies in the size of available training data, whatever regression technique is used. Actually, the scarcity of annotations can seriously impact on the performance, making the adopted method ineffective for those applications where labelled data are difficult to obtain. In this respect, semi-supervised approaches play a substantial role in exploiting unlabelled samples to support the search for the solution. Among the others, multi-view learning [1], [2] is a class of algorithms that considers the structure of the input data as composed by several “views” which are associated to different hypothesis spaces employed to construct as many\nJacopo Cavazza and Vittorio Murino are with Pattern Analysis & Computer Vision, Istituto Italiano di Tecnologia (IIT), Via Morego, 30, 16163, Genova, Italy.\nJacopo Cavazza is also with Dipartimento di Ingegneria Navale, Elettrica, Elettronica e delle Telecomunicazioni (DITEN), University of Genoa, Via Opera Pia 11, 16145 Genova, Italy.\nVittorio Murino is also with Computer Science Department, University of Verona, Strada Le Grazie 15, 37134, Verona, Italy.\nPrimary email contact: jacopo.cavazza@iit.it.\ndecision functions, finally fusing them together. Manifold regularization constitutes another family of semi-supervised methods which use the geometry of the unknown probability distribution underlying the data to shape the structure of the solution space [3], typically imposing regularization by means of the graph Laplacian operator [4].\nHowever, even in the small labelling regime, since annotations are usually provided by human operators, they are frequently prone to errors and noisy in general, making them rather misleading. Hence, for both theoretical and practical aspects, it is of utmost importance to devise algorithms which are able to automatically analyse the data as to guarantee robustness towards outliers. As an efficient tool to tackle the latter issue, [5] effectively take advantage of the Huber loss\nHξ : R→ [0,+∞), Hξ(y) =\n{ y2\n2 if |y| ≤ ξ ξ|y| − ξ 2 2 otherwise, (1)\nwhere ξ > 0. However, as the major drawback of such a loss, there is no closed-form solution to optimize it and, as a consequence, iterative schemes (such as quadratic programming [6] or self-dual minimization [7]) were previously exploited for either the original Huber loss [8], [7] or its spurious versions (hinge-Huber [6] or the huberized Laplacian [9]). Moreover, in all cases, additional computational efforts have to be spent in order to fix the threshold ξ, such as statistical efficiency analysis [8].\nIn this work we face all the aforementioned issues trough the following main contributions.\nI . We derive a theoretical solution to exactly optimize the Huber loss in a semi-supervised setup, where we unified multi-view learning and manifold regularization, as to guarantee the maximal generalization of our pipeline1. II . We devise the novel Huber Loss Regression (HLR) algorithm to efficiently implement the proposed solution as to avoid classical iterative schemes [6], [7]. With respect other regression approaches, the following two aspects of HLR are remarkable. Active-Labelling. While taking advantage of the entire labelled and unlabelled component of the sample set, the former is inspected so that HLR automatically removes those annotations which violate a specific numerical check, whenever recognized as either noisy or inconsistent for the training process. Adaptive threshold. Differently from all [8], [6], [7], [9], ξ is automatically learnt in a data-driven fashion by\n1In semi-superivision, manifold regularization and multi-view learning are two of the most effective approaches which have been theoretically unified by [10] also demonstrating the effectiveness of their combination for computer vision applications.\nar X\niv :1\n60 6.\n01 56\n8v 1\n[ cs\n.L G\n] 5\nJ un\n2 01\n6\n2 our algorithm, with no additional analysis required and without increasing HLR computational complexity.\nIII . With an extensive empirical experimentation, we validate the proposed technique, which allows to score competitive results in curve fitting, learning with noisy labels, classical regression problems and crowd counting application. Despite using variegate types of data and considering diverse problems, HLR is able to outperform state-of-the-art regression algorithms.\nThe paper is outlined as follows. In Sections II we present our general semi-supervised setting where, in Section III, we propose our solution for optimizing the Huber loss. Section IV broadly discusses our HLR algorithm. As to prove the versatility of HLR, once benchmarked with a state-of-theart convex solver in Section V-A, we registered a strong performance when applying it to curve fitting and learning from noisy labels (Section V-B), classical regression problems (Section V-C) and crowd counting application (Section V-D). Finally, Section VI draws the conclusions."
    }, {
      "heading" : "II. MULTI-VIEW SCALAR REGRESSION",
      "text" : "In this Section, we introduce the formalism to model data points sampled from a composite input space X which is divided into multiple substructures. Precisely, we assume that for any x ∈ X , we have x = [x1, . . . , xm] and xα belongs to the subspace Xα, for any α = 1, . . . ,m. This is a very natural way to model high dimensional data: x is the concatenation of x1, . . . , xm, each one representing a particular class of features, that is one out of multiple views [1], [2], [10] in which data may be structured.\nIn order to find the regression map, we assume that it belongs to an hypothesis space H of functions h : X → R whose construction is investigated below. For any α = 1, . . . ,m, let κα : Xα × Xα → R a Mercer kernel [11], that is a symmetric and positive semi-definite function. Let us define K(x, z) = diag(κ1(x1, z1), . . . , κm(xm, zm)) ∈ Rm×m, where x = [x1, . . . , xm], z = [z1, . . . , zm] ∈ X . Consider S0 the space of functions z 7→ f(z) = ∑n i=1K(z,xi)ui with x1, . . . ,xn ∈ X and u1, . . . , un column vectors in Rm. Define the norm ‖f‖2K = ∑n i,j=1 u > iK(xi,xj)uj . The reproducing kernel Hilbert space (RKHS) SK related to K is the completion of S0 with the limits of Cauchy sequences converging with respect to ‖·‖K [12]. Finally, c ∈ Rm induces a sampling operator c>: SK → H whose image is our final hypothesis space. Consequently,\nh(z) = c>f(z) = n∑ i=1 c>K(z,xi)ui (2)\nis the general expression for every h ∈ H with f ∈ S0. a) Remark.: Differently from other previous multi-view learning paradigms [2], [1], [10], the kernel matrix K is fixed to be diagonal. Empirically, it allows to dedicate an independent kernel to each view, gathering the subspace information codified by our data. In addition, such choice provides some theoretical advantages. Indeed, as pointed out in [10], it is not trivial to prove that the kernel adopted by [2] is positive semi-definite, while such property is trivially\nfulfilled in our setting. Also, thanks to (2), we can recover h(z) = [h1(z1), . . . , hm(zm)] for every m by means of the analytical expression hα(zα) = ∑ i c ακα(zα, xαi )u α i , for α = 1, . . . ,m. Differently, in [1] an analogous result only holds for m = 2."
    }, {
      "heading" : "III. HUBER LOSS-BASED REGRESSION",
      "text" : "Once the hypothesis spaceH is defined according to Section II, we can perform an optimization based learning for the regression map. To this aim, we consider a training set D made of ` labelled instances (x1, y1), . . . , (x`, y`) ∈ X × R and u additional unlabelled inputs x`+1, . . . ,x`+u ∈ X . Among the class of functions (2), similarly to [3], [10], the regression map is found by minimizing the following objective functional\nJλ,γ(f) = 1\n` ∑̀ i=1 Hξ(yi − c>f(xi)) + λ‖f‖2K + γ‖f‖2M . (3)\nIn equation (3), 1` ∑` i=1Hξ(yi − c>f(xi)) represents the empirical risk and is defined by means of the Huber loss (1), ξ > 0: it measures how well c>f(xi) predicts the ground truth output yi. To avoid either under-fitting or over-fitting, ‖f‖2K is a Tichonov regularizer which controls the complexity of the solution, it is scaled by λ > 0. Instead, γ > 0 weighs\n‖f‖2M = m∑ α=1 u+∑̀ i,j=1 fα(xαi )M α ijf α(xαj ), (4)\nwhich infers the geometrical information of the feature space. The family {M1, . . . ,Mm} of symmetric and positive semidefinite matrices Mα ∈ R(u+`)×(u+`) specifies ‖f‖2M and ensures its non-negativeness. This setting generalizes [3], [1], [2] where M1 = · · · = Mm = L, being L is the graph Laplacian related to the (u+ `)× (u+ `) adjacency matrix W. The latter captures the interdependencies between f(x1), . . . , f(xu+`) by measuring their mutual similarity. Then, in this simplified setting, the regularizer rewrites ∑u+` i,j=1 wij‖f(xi)− f(xj)‖22, if wij denotes the (i, j)-th entry of W. In this particular case, we retrieve the idea of manifold regularization [3] to enforce nearby patterns in the high-dimensional RKHS to share similar labels, so imposing further regularity on the solution. By means of our generalization, we can consider, for instance, m graph Laplacians L1, . . . , Lm of the view-specific adjacency matrices W 1, . . . ,Wm. Thus,\n‖f‖2M = m∑ α=1 u+∑̀ i,j=1 wαij ∣∣fα(xαi )− f(xαj )∣∣2 (5)\nforces the smoothness of f on any subspace Xα instead of doing it globally on X as in [3].\nUsual single-viewed (m = 1), fully supervised (u = 0) and classically regularized optimization problems (γ = 0) can be retrieve as particular cases, making our theoretical contribution applicable to a broad field of optimization problems. Precisely, in such a unified framework, we will now provide a general solution to guarantee the exact optimization for the convex and differentiable Huber loss function Hξ [5], [13]. Actually, the Huber loss generalizes both the quadratic loss and the absolute value, which can be recovered in the extremal cases"
    }, {
      "heading" : "JACOPO CAVAZZA AND VITTORIO MURINO: ACTIVE-LABELLING BY ADAPTIVE HUBER LOSS REGRESSION 3",
      "text" : "ξ → +∞ and ξ → 0 respectively. Hξ has been shown to be robust against outliers [5] since, in a neighbourhood of the origin, it penalizes small errors in a more smoother way than absolute value, whereas, when |y| ≥ ξ, the linear growth plays an intermediate role between over-penalization (quadratic loss) and under-penalization (absolute value) of bigger errors. Globally, it resumes the positive aspect of the two losses, while remarkably mitigating their weaknesses. Of course, Hξ suffers the issue of setting the threshold ξ and, to this end, we devise an adaptive pipeline to learn it from data (see Section IV). For all these reasons, we focus on a Huber-loss based regression framework.\nIn order to optimize our objective (3), we can exploit the theory of the RKHS to optimize Jλ,γ over SK by using the Representer Theorem [4]. Precisely, in addition to ensuring existence and uniqueness of the minimizer f? = arg minf∈Sk Jλ,γ(f), it provides the following expansion valid for any z ∈ X :\nf?(z) = u+∑̀ j=1 c>K(z,xj)wj , (6)\nwhich involves the data inputs x1, . . . ,xu+` and some w1, . . . , wu+` ∈ Rm. Equation (6) is the main tool exploited to deduce our general solution\nTheorem 1 (General solution for Huber loss multi-view manifold regularization regression). For any ξ > 0, the coefficients w = [w1, . . . , wu+`]\n> defining the solution (6) of problem (3) are given by\n2`λwi + 2`γ u+∑̀ j,h=1 MijK(xj ,xh)wh =\n=  −ξc if i ∈ L+[D,w, ξ]yi − u+∑̀ j=1 c>K(xi,xj)wj  c if i ∈ L0[D,w, ξ] +ξc if i ∈ L−[D,w, ξ] 0 otherwise, (7)\nwhere λ, γ > 0 and we set Mij = diag(M1ij , . . . ,M m ij ),\nL+[D,w, ξ] = i ≤ ` : u+∑̀ j=1 c>K(xi,xj)wj ≥ yi + ξ , L0[D,w, ξ] =\ni ≤ ` : ∣∣∣∣∣∣ u+∑̀ j=1 c>K(xi,xj)wj − yi ∣∣∣∣∣∣ < ξ ,\nL−[D,w, ξ] = i ≤ ` : u+∑̀ j=1 c>K(xi,xj)wj ≤ yi − ξ . Proof. To not fragment the dissertation, the proof was moved to the APPENDIX.\nLet us interpret numerically the sets L−[D,w, ξ], L0[D,w, ξ], and L+[D,w, ξ]. First of all, they provide a partition of {1, . . . , `}. Define εi = ∣∣∣∑u+`j=1 c>K(xi,xj)wj − yi∣∣∣ ,\nthe absolute in-sample error involving the i-th labelled element (xi, yi) ∈ D. Call ε∞ = maxi≤` εi their maximum. Thus, the set L0[D,w, ξ] = {i ≤ ` : εi < ξ} collects all the indexes of those instances which are more effective in strictly reducing absolute errors εi below threshold ξ. Instead, in L+[D,w, ξ] and L−[D,w, ξ], the target variables are over and under-estimated, respectively. Furthermore, when L0[D,w, ξ] = {1, . . . , `}, since εi < ξ for any i, we obtain ε∞ < ξ. So, if L+[D,w, ξ] and L−[D,w, ξ] are empty, ξ is an upper bound for the absolute error of the labelled training set in D. In spite of the nice numerical interpretation, the sets L+[D,w, ξ], L0[D,w, ξ], L−[D,w, ξ] can be computed only if w is given. Thus, apparently, we can not exploit the general solution (7) directly implementing it. Actually, such issue is solved in Section IV."
    }, {
      "heading" : "IV. THE HLR ALGORITHM",
      "text" : "As a preliminary step to introduce the algorithm to minimize the objective (3), we rephrase (7) into the matrix formulation (C?CQ[D,w, ξ] + 2`λMK[x])w + 2`γw = C?b[D,w, ξ], exploiting the notation reported below. • w = [w1, . . . , wu+`]\n> collects wj ∈ Rm for any j. • Let y = [y1, . . . , y`]> and denote with y0 the result of\nappending to y a u× 1 vector of zeros. • M is the block matrix collecting Mij for any i, j. • Consider K[x], the Gram matrix defined by Kij [x] = K(xi,xj) for i, j = 1, . . . , u+ `. • Denoting In the n × n identity matrix and ⊗ the Kronecker tensor product, let C = Iu+` ⊗ c> and C? = Iu+` ⊗ c.2 • Let Q[D,w, ξ] ∈ Rm(u+`)×m(u+`) a block matrix,\nQij [D,w, ξ] = { Kij [x] if i ∈ L0[D,w, ξ] 0 otherwise.\n• b[D,w, ξ] = [b1[D,w, ξ], . . . , bu+`[D,w, ξ]] >,\nbi[D,w, ξ] =  −ξ if i ∈ L+[D,w, ξ] yi if i ∈ L0[D,w, ξ] +ξ if i ∈ L−[D,w, ξ] 0 if i = `+ 1, . . . , u+ `.\nAs observed in Section III, the main difficulty pertains to the sets L+[D,w, ξ], L0[D,w, ξ], and L−[D,w, ξ], which are not computable if w is unknown, compromising the numerical implementability of (7). As a very naive approach, if a certain initialization for the solution is provided, we can therefore set a scheme in which, at each iteration, L0, L+ and L− are firstly computed for an initial approximation w = wold of the solution. Then, w is updated to wnew solving the linear system (\nC?CQ[D,wold, ξ] + 2`λMK[x] ) wnew+\n+ 2`γwnew = C?b[D,wold, ξ] (8)\nfor a fixed ξ > 0. Such approach suffers from several troubles. Indeed, an initialization is required for the scheme, whose\n2In terms of linear maps, matrices C and C? represent two operators, where one is the adjoint of the other.\n4 Algorithm 1. HLR algorithm pseudo-code Input: D dataset, M1, . . . ,Mm positive definite m × m matrices, λ > 0 Tichonov regularizing parameter, γ > 0 manifold regularization parameter, ∆ξ > 0 updating rate, maximum number T of refinements. Output: Coefficients vector w? = [w?1 , . . . , w?u+`]>. Find w(0) solving (C?C + `γM)K[x]w(0) + `λw(0) = C?y0. D(0) := D and compute ξ(0) as in (9). for τ = 1, . . . , T do ξ̃(τ) := ξ(τ−1) −∆ξ. Solve (8) in the unknown wnew := w̃(τ), with wold := w(τ−1), D := D(τ−1) and ξ := ξ̃(τ). Compute the sets L0, L+ and L− on [D(τ−1), w̃(τ), ξ̃(τ)]. if L0[D(τ−1), w̃(τ), ξ̃(τ)] is empty then\nReturn w? := w(τ−1). else\nObtain D(τ) from D(τ) by removing the labels yi of the data points (xi, yi) ∈ D(τ−1) such that i ∈ L+[D\n(τ−1), w̃(τ), ξ̃(τ)] ∪ L−[D(τ−1), w̃(τ), ξ̃(τ)]. Sort w(τ) permuting w(τ−1) in a way that the elements w\n(τ−1) j with j ∈ L0[D(τ−1), w̃(τ), ξ̃(τ)] occupy the\nfirst entries. Compute ξ(τ) using relation (9).\nend if end for Return w? := w(τ)\nconvergence is eventually not guaranteed. Also, the Huber loss threshold ξ has to be manually selected.\nAs a main contribution of the paper, we introduce the Huber loss regression algorithm (HLR) to perform a principled optimization and compute the exact solution of the problem (7), while also learning the best value for ξ via a sequential refinements ξ(0), ξ(1), . . . , ξ(T ) and automatically scanning the labelled data exploited. The pseudo-code for our method is described in Algorithm 1 and it is discussed in Section IV-A together with the following results, consisting in the theoretical foundation of HLR algorithm.\nProposition 1. For any τ = 0, 1, . . . , T, the coefficients w(τ) satisfy (7) with ξ = ξ(τ), where\nξ(τ) = max i≤` ∣∣∣∣∣∣ u+∑̀ j=1 c>K(xi,xj)w (τ) j − yi ∣∣∣∣∣∣ . (9) Proof. It is enough to show that, for any τ = 0, 1, . . . , T, we get L0[D(τ),w(τ), ξ(τ)] = {1, . . . , `}. Let’s go by induction.\nFor τ = 0, as mentioned before, w(0) solves (7) for ξ = +∞ since, for any dataset D and any vector w of coefficients w1, . . . , wu+`, we have L0[D,w,+∞] = {1, . . . , `}. Since ξ(0) represents the maximum absolute error inside the training set D(0) = D when the solution of the optimization problem is specified by w(0), we have L0[z,w(0), ξ(0)] = {1, . . . , `}. So the thesis is proved for τ = 0.\nNow, we assume that the thesis holds for the (τ − 1)-th refinement and we prove it for the τ -th one. As a consequence, we have\nL0[D (τ−1),w(τ−1), ξ(τ−1)] = {1, . . . , `} (10)\nand we must show that the same relation is valid also for τ. Once computed w̃(τ), we do not discard yi from the training data D(τ−1) if and only if ∣∣∣∑u+`j=1 c>K(xi,xj)w̃(τ)j − yi∣∣∣ ≤ ξ̃(τ). Since the algorithm requires to compute w(τ) by permuting w(τ−1) in a way that the elements w(τ−1)j with j ∈ L0[D(τ−1), w̃(τ), ξ̃(τ)] occupy the first entries, we have∣∣∣∑u+`j=1 c>K(xi,xj)w(τ)j − yi∣∣∣ ≤ ξ̃(τ) thanks to the assumption (10). Since ξ(τ) is defined as the maximum of a finite set of elements all bounded by ξ̃(τ), we conclude\nξ(τ) = max i=1,...,` ∣∣∣∣∣∣ u+∑̀ j=1 c>K(xi,xj)w (τ) j − yi ∣∣∣∣∣∣ ≤ ξ̃(τ). (11) From the previous relation and from the definition of the set L0, we obtain L0[D(τ),w(τ), ξ(τ)] = {1, . . . , `}.\nProposition 2. The sequence ξ(0), ξ(1), . . . , ξ(T ) is monotonically strictly decreasing.\nProof. In formulæ, we want to show that ξ(0) > ξ(1) > · · · > ξ(T ). In order to prove monotonicity, we fix an arbitrary refinement τ = 1, . . . , T and our goal is to show ξ(τ) < ξ(τ−1). Directly using (11), we have\nξ(τ) ≤ ξ̃(τ). (12)\nBy definition of ξ̃(τ),\nξ̃(τ) = ξ(τ−1) −∆ξ, (13)\nand, since ∆ξ > 0, then\nξ(τ−1) −∆ξ < ξ(τ−1). (14)\nCombining the equations (12), (13) and (14) we get\nξ(τ) < ξ(τ−1). (15)\nThe thesis follow after the generality of τ in (15).\nA. Insights about the HLR pseudo-code\nEssentially, Algorithm 1 solves the burden related to the computability of the sets L0, L+ and L−, by adapting the threshold ξ of the Huber loss. Indeed, w0 is easily computable in the setting ξ = +∞ since L0[D,w,+∞] = {1, . . . , `} for any w and D and (7) reduces to a ordinary linear system. However, thanks to the definition of ξ(τ) in (9), we can ensure that w(0) solves the problem (7) for the value ξ = ξ(0) of the Huber loss threshold. The refinements for ξ start with a fixed reduction of ξ(τ−1) by ∆ξ > 0, while, once (8) is solved, the final value ξ(τ) is updated: this is the key passage to ensure that Proposition 1 holds for any τ . Let us stress again that, for any refinement τ = 0, . . . , T, the vector w(τ) gives the exact solution (7) for our robust scalar regression framework (3), where the Huber loss threshold ξ equals to ξ(τ)."
    }, {
      "heading" : "JACOPO CAVAZZA AND VITTORIO MURINO: ACTIVE-LABELLING BY ADAPTIVE HUBER LOSS REGRESSION 5",
      "text" : "Additionally, HLR is able to automatically select which output variables y1, . . . , y` do not provide sufficient information for learning the regression map. Indeed, at each refinement, HLR scans the labelled training set {(x1, y1), . . . , (x`, y`)} ∈ D(τ−1) and check whether∣∣∣∣∣∣ u+∑̀ j=1 c>K(xi,xj)w̃j (τ) − yi\n∣∣∣∣∣∣ ≥ ξ̃(τ). (16) Equation (16) means that, for xi, the prediction of HLR is suboptimal, since differing from the actual value yi for more than ξ̃(τ). In such case, the algorithm automatically remove yi from the dataset, assigning xi to be unlabelled in D(τ) and actually trying to recover a better prediction by looking for the output value in the RKHS which is closest to f(xi) in the sense of the norm ‖ · ‖M . For the sake of clarity, notice that the computational cost does not change when some scalar yi is removed from the training set. Indeed, any unlabelled input xi still counts one equation, as it is when (xi, yi) ∈ D. We will refer to such automatic balance of labelled data as the active-labelling component of HLR (see Section IV-B).\nFurthermore, thanks to Proposition 2, the algorithms perform an automatic threshold adaptation for ξ. Indeed, ξ(0) > ξ(1) > · · · > ξ(T ) is a decreasing sequence whose latter element represents the data-driven selection performed by HLR for the optimal Huber loss threshold ξ, after T refinements. Precisely, according to (9), such optimality is measured in terms of T successive decreasing reductions of the absolute error paid inside the labelled part of our training set.\nFinally, let us conclude with a couple of details. First, the computational cost of HLR is O((T +1)m2(u+`)2). Second, once coefficients w?1 , . . . , w ? u+` are computed by Algorithm\n1, f?(v) = ∑u+` j=1 c >K(v,xj)w ? j is the predicted output associated to the multi-view test instance v ∈ X .\nB. Interpreting the active-labelling component\nThe active-labelling component of HLR has a natural interpretation in machine learning. Indeed, consider xi to represent a collection of different types (i.e., views) of features which are computed on the raw input data by enhancing separated discriminative characteristics, while possibly reducing the noise impact. Conversely, all the labels yi are directly obtained from ground truth annotations, which are usually acquired by humans and, consequently, more prone to noise and errors. Also, during the learning phase, semantic ambiguities may result in descriptors which are at the same time dissimilar in terms of corresponding labels and similar while compared in the feature space. Clearly, such condition violates the classical assumption in semi-supervision where the conditional probability distribution p(y|x) smoothly changes over the geodesics induced by pX (x) [3]. Thus, despite classical frameworks leveraging on unlabelled examples can be easily damaged in such condition, differently, HLR shows a unique robustness towards corrupted/misleading annotations, being able to totally and automatically discard them."
    }, {
      "heading" : "V. EMPIRICAL VALIDATION OF HLR",
      "text" : "This Section presents our empirical analysis of HLR. In Section V-A, we check the effectiveness of the proposed solution for the Huber loss and the adaptation criterion to automatically learn ξ from data. In Section V-B, the activelabelling component of HLR are benchmarked on curve fitting and learning from noisy labels problems. Section V-C compares HLR with state-of-the-art regression methods on classical machine learning datasets. Finally, in Section V-D, we consider the crowd counting application, comparing our method against the most effective ones in the literature trough several experiments."
    }, {
      "heading" : "A. Comparison with state-of-the-art convex solver",
      "text" : "As stressed in the Introduction and proved in Section IV, HLR leverages on an exact solution for optimizing the Huber loss, as a diametrical opposed perspective to iterative solving. In order to experimentally check the powerfulness of such aspect, we compare against CVX [14] which is a state-ofthe-art optimization tool for convex problems. Precisely, by either exploiting HLR or CVX, we are able to optimize the same objective functional (3) as to investigate which of the two methods is more efficient in terms of both performance and running time. At the same time, we are able to inspect the automatic pipeline to adapt ξ in a data-driven fashion while comparing with a classical cross-validation.\nTo these aims, we face a classical linear regression problem ruled by the underlying model y = β>x, where x ∈ R10, y ∈ R and β = [1/10, . . . , 1/10]>. We randomly generate n = 50, 100, 500 and 1000 samples x from a uniform distribution over the unit 10-dimensional hypercube [0, 1]×· · ·× [0, 1]. As a further experiment, we introduce some outliers to the model which becomes y = β>x + , where the additive noise ∼ N (0, 0.1) is Gaussian distributed. For HLR, T = 1 and ∆ξ = 0.1, λ = 10−2 and γ = 10−3 are fixed. The performance of CVX and HLR are measured via the reconstruction error between the ground truth values and the predictions. Also, we monitor the computational running time of both.\nThe analysis of Figure 1 yields to the following comments. − Numerically, our closed form solution shows a comparable performance with respect to classical iterative schemes in terms of reconstruction error. − For both algorithms, noise addiction does not remarkably\n6 influence the reconstruction error: this is due to the robustness provided by the Huber loss. − Undoubtedly, the reconstruction error of CVX greatly fluctuates when ξ varies in {10−3, 10−2, . . . , 103}.\nIn addition, Table I clarifies how HLR is much more computationally efficient: the runtime3 of HLR is about a few seconds even if n grows while, for CVX, it sharply raises in the cases n = 500 and n = 1000. Also, it is worth nothing that the computational running time for CVX only accounts one optimization for a given ξ which has to be repeated within the cross validation pipeline, leading to the mean and standard deviation values reported in Table I.\nGlobally, HLR is performing on par with respect to a stateof-the-art convex solver, being much more quicker in the computation, while automatically learning ξ."
    }, {
      "heading" : "B. Evaluation of the active-labelling component",
      "text" : "In this Section, we evaluate the robustness provided by the HLR active-labelling component thanks to the usage of the Huber loss. For this purpose we either considered a curve fitting example and we also faced the problem of binary classification in a corrupted data regime.\nCurve fitting. As in Section V-A, starting from the same linear model y = β>x, we severely corrupted a random percentage of target data points by inverting their sign. It is a quite sensible change since each entry of x is uniformly distributed in [0, 1], being thus non-negative. Consequently, our algorithm should be able to recognize the negative data as clear outlier and automatically remove them from the training set. Such evaluation is performed through Table II where, for different noise rates, we report the reconstruction error while measuring whether the labels removed by HLR actually refers to corrupted inputs. To do the latter, we employ the SørensenDice index s [15] to measure the amount of corrupted data effectively removed by the HLR. In formulæ, s = 2|C∩R||C|+|R| where the sets C and R collects the corrupted and removed data, respectively: s ∈ [0, 1] and spans from the worst overlap case (s = 0 since C ∩ R = ∅) to the perfect one (s = 1 if C = R).\n3For all experiments, we used MATLAB R2015b on a Intel(R) Xeon(R) CPU X5650 @2.67 GHz ×2 cores and 12 GB RAM.\nIn Table II, despite the increasing noise level, the reconstruction error is quite stable and only degrades at the highest noise levels. Additionally, when the noise level has a minor impact (1% and 10%), we get s = 1: the removal process is perfect and exactly all the corrupted labels are effectively removed. When percentages of noise increases (25%, 50%), we still have good overlapping measures. The final drop at 75% is coherent with the huge amount of noise (only 1 target over 4 is not corrupted).\nLearning with noisy labels. We want to benchmark our method in facing noisy annotations during the learning phase. Thus, we compare HLR against several approaches for the same problem in the setting of [16]. Therein, binary classification is performed in the presence of random label noise so that, in training, instead of exploiting the true labels, some of them have been randomly flipped with a given probability. Precisely, following [16], we denote with ρ+ the probability that the label of a positive sample is flipped from +1 to −1. In a similar manner, ρ− quantifies the negative training instances whose label is wrongly assigned to be −1. In [16], such problem is stated under a theoretical perspective, formulating some bounds for the generalization error and the empirical risk, as to guarantee the feasibility of the learning task even in such an extreme situation. Although interesting per se, such arguments are out of the scope of our work, where, instead, we compared with the two methods proposed by [16]: a surrogate logarithmic loss function (˜̀log) and a variant of support vector machine algorithm, where the cost parameter is adapted depending on the labels (C-SVM). In [16], ˜̀log and CSVM were shown to outperform many state-of-the-art methods for the same task: the max margin perceptron algorithm (PAM) [17], Gaussian herding (NHERD) [18] and random projection classifier (RP) [19]. All the aforementioned methods are compared with HLR algorithm actually specialized for this task: as usually done for binary decision boundaries, we exploit the sign of the learnt function f? (see Section IV-A) to perform classification. To ensure a fair comparison, we reproduce the same experimental protocol in [16] by both choosing all the free parameters via a cross validation pipeline and computing the accuracy on the test set with respect to the clean distribution of labels. Also, as in[16], we considered the same datasets, training/testing splits and data normalization4.\nFrom the experimental results reported in Table III, HLR scored a strong performance. Indeed, despite some modest classification results on Thyroid an Diabetes datasets, HLR is able to beat the considered competitors, scoring the best classification accuracy in the remaining 4 out of 6 ones (Breast Cancer, German, Heary and Image). Interestingly, this happens in both low and high noise levels: for instance, when ρ+ = ρ− = 0.2 on Breast Cancer and when ρ+ = ρ− = 0.4 on Image.\nThe results presented in this Section attest the activelabelling HLR component to be able to effectively detect the presence of outlier data while, at the same time, guaranteeing an effective learning of the regression model.\n4http://theoval.cmp.uea.ac.uk/matlab"
    }, {
      "heading" : "JACOPO CAVAZZA AND VITTORIO MURINO: ACTIVE-LABELLING BY ADAPTIVE HUBER LOSS REGRESSION 7",
      "text" : ""
    }, {
      "heading" : "C. HLR for classical regression applications",
      "text" : "To compare the effectiveness of HLR in learning the regressor map, in this Section, we benchmark on four datasets from the UCI Machine Learning Repository5, we will focus on house pricing estimation (Boston Housing – House), physical simulations (AirFoil Self-Noise – Air and Yatch Hydrodynamics – Idro) and agronomic quality control (Wine). We will briefly discribe each of them.\nHouse datasets predicts housing values in Boston suburbs. The dataset consists in 506 examples and 13 feature components which are either discrete (average number of rooms), binary (whether or not tracting bounds of Charles river) or continuous (pupil-teacher ratio by town). Air datasets address the problem of physical simulations. It is provided by NASA and shows 1503 aerodynamic and acoustic acquisitions of two and three-dimensional air foil blade sections. A 6-dimensional feature vector encodes different size and thickness for blades, various wind tunnel speeds and angles of attack. The output variable is the sound pressure level measured in decibel. Idro predicts the resistance of sailing yachts at the initial design stage, estimating the required propulsive power. Inputs provides hull dimensions an boat velocity (6 dimensional features, 308 instances). The output variable is the residuary resistance per unitary displacement. Wine dataset consists in 11-dimensional 1599 input instances (we only focused on red wine). The goal is predicting the grades, given by a crew of sommeliers, using pH, alcohol and sulphates concentrations as data.\nOver the aforementioned datasets, we compare Huber loss regression (HLR) against Gaussian process regression (GPR), ridge regression (RR), K nearest neighbours (K-nn), one-\n5 https://archive.ics.uci.edu/ml/datasets\nhidden-layer neural network (NN) and linear support vector machine for regression (SVR). For each method, the parameters setting are obtained after cross validation (see Table IV). For a fair comparison, we split each dataset in five equispaced folds: on each, we used 20% of samples as training set, while test on the remaining ones. To give a comprehensive results on each datasets, we averaged the errors on each fold using one out of those following metrics\nmean absolute error MAE = 1\nn n∑ i=1 |yi − ŷi|, (17)\nmean squared error MSE = 1\nn n∑ i=1 (yi − ŷi)2, (18)\nmean relative error MRE = 1\nn n∑ i=1 |yi − ŷi| yi , (19)\nwhere y1, . . . , yn are the (true) testing target variables and ŷ1, . . . , ŷn the corresponding prediction.\nQualitative and quantitative analysis has been reported in Table V and Figure 2, respectively. Globally, HLR shows remarkable performances since outstanding the other methods in 5 cases out of 12. Those performances are also remarkable since they have been obtained with a fixed set of parameters, confirming the ductility of HLR (see Table IV). Indeed, despite GPR and RR scored comparable performance with respect to HLR, the regularizing parameter of RR has to be tuned and the parameters of the GPR has to be learnt in a maximum likelihood sense (mean function plus covariance kernel).\nFrom this analysis, the low errors scored by of HLR a joint manner with the fixed configuration of parameters we adopted, maker HLR outperforming many state-of-the-art approaches for scalar regression tasks.\n8 Method House Air Idro Wine GPR Affine mean, mixture covariance type (linear + squared exponential)\nRR α = 0.3 α = 0.01 α = 0.5 α = 1 K-nn K = 3 K = 3 K = 3 K = 5\nNN nh = 3 nh = 5 nh = 7 nh = 6 SVR = 0.003, C = 10 = 0.005, C = 10 ν = 0.003, C = 1 ν = 0.01, C = 10 HLR λ = 0.001, γ = 0.0001,∆ξ = 0.01, T = 3\nTABLE IV IN ADDITION TO THE PARAMETERS λ, γ,Dξ AND T OF HLR, WE REPORT THE PARAMETERS/SETTINGS OF OTHER METHODS BENCHMARKED ON THE UCI MACHINE LEARNING REPOSITORY EXPERIMENTS: THE MEAN AND COVARIANCE FUNCTIONS USED FOR GPR, THE REGULARIZING PARAMETER α FOR RR, THE VALUE OF K NEIGHBOURS CONSIDERED, THE NUMBER OF NEURONS nh IN THE HIDDEN LAYER FOR NN AND THE /ν CHOICES FOR SVR AS WELL AS THE COST FUNCTION C USED.\nHouse Air Idro Wine Methods MAE MSE MRE MAE MSE MRE MAE MSE MAE MSE MRE\nGPR 4.21(3) 41.00(3) 0.20(3) 4.47(2) 33.84(2) 0.03(1) 7.10(2) 118.3(3) 0.59(1) 0.72(1) 0.107(2) RR 3.79(1) 28.73(1) 16.03(2) 4.76(3) 37.61(3) 3.87(3) 7.28(3) 113.9(2) 0.59(1) 0.72(1) 0.106(1) K-nn 5.91(6) 64.96(6) 22.64(6) 6.01(5) 65.57(6) 4.89(5) 9.08(6) 267.0(6) 0.61(5) 0.78(5) 0.108(4) NN 5.49(5) 56.97(5) 20.94(5) 6.56(6) 64.69(5) 5.32(6) 8.32(5) 183.2(5) 0.86(6) 1.35(6) 0.161(6)\nSVR 4.88(4) 51.55(4) 20.72(4) 4.93(4) 38.64(4) 3.99(4) 7.41(4) 143.8(4) 0.59(1) 0.73(3) 0.109(5) HLR 4.13(2) 36.78(2) 0.15(1) 4.16(1) 30.20(1) 0.04(2) 6.91(1) 110.8(1) 0.61(4) 0.77(4) 0.107(2)\nTABLE V COMPARISON OF HLR AGAINST GAUSSIAN PROCESS REGRESSION, RIDGE REGRESSION, K NEAREST NEIGHBORS, NEURAL NETS AND SUPPORT VECTOR MACHINE FOR REGRESSION. IN BOLD, TOP THREE PERFORMING METHODS. IN BRACKETS, THE RELATIVE RANKING. FOR Idro, SINCE THE TARGET VARIABLE IS SOMETIMES (CLOSE TO) ZERO, MRE METRICS DIVERGE AND WAS THEREFORE NOT REPORTED."
    }, {
      "heading" : "D. HLR for crowd counting application",
      "text" : "As a final test bed of our proposed framework, we address the crowd counting application. It consists in estimating the number of people in a real world environment using video data. Crowd counting can be rephrased in learning a regression map from frame-specific features to the amount of people whereby [20]. Three benchmark datasets have been used to test the performances of our Huber loss regression method. They are MALL [21], UCSD [22] and PETS 2009 [23].\nMALL – From a surveillance camera in a shopping centre, 2000 RGB images were extracted (resolution 320 × 240). In each image, crowd density varies from 13 to 53. The main challenges are related to shadows and reflections. Following the literature [21], our system is trained with the first 800 frames, and the remaining ones are left for testing.\nUCSD – A hand-held camera recorded a campus outdoor scene composed by 2000 gray-scale 238-by-158 frames. The\ndensity grows from 11 to 46. Environment changes are less severe, while geometric distorsion is sometimes a burden. As usually done [22], we used the frames 601÷1400 for training.\nPETS 2009 – Within the Eleventh Performance Evaluation of Tracking and Surveillance workshop, a new dataset has been recorded from a British campus. Crowd counting experiments are carried out on sequences 13-57,13-59,14-03,14- 06 from camera 1 [23], and three regions of interests have"
    }, {
      "heading" : "JACOPO CAVAZZA AND VITTORIO MURINO: ACTIVE-LABELLING BY ADAPTIVE HUBER LOSS REGRESSION 9",
      "text" : "been introduced (R0, R1 and R2 in Fig. 3). Crowd density ranges between 0 and 42, shadows and the appearance of both walking and running people are the main challenges. We adopted the training/testing split of [23] (see Table VI).\nIn addition to replicating training/testing split, for a fair comparison, we employed publicly available6 ground truth annotations and pre-computed features: precisely, we employed size, edges and texture features [24]. Size descriptors refer to the magnitude of any interesting segments or patches from an image which are deemed to be relevant (e.g., the foreground pixels [25]). Edges pertain to the relative changes in graylevel values and binary detectors (Canny algorithm [26]) are used for extraction. The texture class includes several statistics, like energy or entropy, which are computed from gray-level co-occurrence matrix [27] or local binary pattern [20]. Before extracting these descriptors, a region of interest is detected, perspective is normalized as in [28] and, sometimes, an intermediate motion segmentation phase is performed (hence, crowd can be subdivided according to motion directions).\nIn our framework, we set m = 3 and each category of features is thus encoded with a separate kernel (we adopt either quadratic-polynomial or linear ones). We fix c = [1/3, 1/3, 1/3]> and Mα is the sum of between-view operator from [10] and normalized graph laplacian Lα related to the α-th view. The model parameters (refinement number T, the regularizing parameters λ, γ, and ∆ξ) are chosen via cross validation on the training set (see Table VII). For instance, results from Table XI all comes from this default setting.\nFigures 4 and 5 show the qualitative results, overlapping the profile of the ground truth crowd density with our predicted evaluations for all the considered experiments: in all cases the ground truth crowd size is estimated in a satisfactory way. Quantitative performance evaluation usually uses the three\n6http://personal.ie.cuhk.edu.hk/∼ccloy/downloads mall dataset.html for MALL; http://visal.cs.cityu.edu.hk/downloads/ for UCSD and PETS 2009.\nerror measures MAE, MSE and MRE already defined in Section V-C.\nFollowing the protocol of [24], in Table VIII we integrate Huber Loss Regression (HLR) in the comparison between Gaussian Process Regression (GPR), regularized linear regression (Lin), K-nearest neighbours (K-nn) with K = 4 and neural networks (NN) methods with a unique hidden layer composed by 8 artificial neurons. Using same training/testing splits as in [24], we scored top three error on MALL, while we set state-of-the art in UCSD considerably reducing MRE.\nWe also tested UCSD and MALL datasets in the same conditions as [21] and [31] (see Table X). Therein, the methods adopted in the comparison are least square support vector regression LSSVR [32], kernel ridge regression KRR [33], random forest regression RFR [34], Gaussian process regression GPR [22], ridge regression RR [35] with its cumulative attribute CA-RR [31] variant. Additionally, we also compare with multiple localised regression MLR [36] and multiple output regression MORR [21], a class of local approaches for crowd counting which rely on a preliminary fine tessellation of the video frames. Also in this demanding comparison HLR is able to set the lowest and second lowest MAE,MSE and MRE in all of the comparisons, respectively.\nAdditionally, we benchmarked HLR with other methods which leverage on semi-supervision. Precisely we benchmarked against the baseline one-viewed manifold regularization (MR) [3] while also considering semi-supervisedregression (SSR) [29] and elastic net (EN) [30]. For the former, SSR optimize a similar functional as (3) where the quadratic loss is used in a multi-view setting (m = 2) as to impose a spatial regularization within-frames and a temporal regularization across consecutive frames. Finally, EN [30] implements a sparsity principle while adopting a L1-based semi-supervised variation of Lasso. In Table IX we report the MSE quantitative results where, HLR is able to outperform\n10\nother semi-supervised methods. Moving to PETS 2009, we reproduce the same experimental conditions as in [23]. Motion segmentation allows to divide the right-moving pedestrians from the others moving in the opposite direction. Total crowd density has been obtained summing the partial results. Table XI shows a comparison of Huber loss vs. Gaussian Process Regression (GPR) in this setting reported in Table VI. Performances are sometimes substantially improved, see sequence 13− 57, regions R1 and R2. Again, HLR scored a sound performance, setting in 46 cases out of 54 the lowest MAE or MSE error metric.\nDiscussion. • While comparing with other semi-supervised methods (Table IX), multi-view learning and manifold regularization are effectively combined by HLR in a framework where the final boost of accuracy is ensured by the usage of the Huber loss which attested to be superior to both the quadratic (MR and SSR) and the L1 losses (EN). • The active-labelling is able to proficiently rule the amount of supervision. Indeed, on UCSD only 1% of the labels is not exploited by HLR: evidently, the preprocessing step perspective correction [22] is enough effective to make almost all the data exploitable in a supervised fashion. Differently, on MALL 11% of labelled instances are discarded: this happens when some pedestrians are partially occluded by some static elements of the scene and, sometimes, there are some sitting people whose appearance greatly differs from the walking ones. Finally, on PETS 2009, HLR outperforms GPR even if using, on average, about 100 less annotations: this is due the visual ambiguities generated by the cross-setting where training and testing scenes are different. • Similarly to Section V-C, HLR performance does not requires any burdensome parameter tuning (Table VII): a good rule-of-the-thumb is T = 3, λ = 10−4 and γ = 10−5; ∆ξ = 0.01: in general, only minor corrections are required. • In terms of time complexity, the HLR is a fast method: indeed, in the setup of Table X, training and testing on MALL last 6.50 and 0.36 seconds respectively. Similarly, on UCSD, training requires 5.6 and testing 0.5 seconds. • In synthesis, the crowd counting application showed that HLR is able to fully take advantage of the most effective techniques in semi-supervision and to improve state-of-the-art methods for crowd counting, while being robust to noisy annotations, ensuring a fast computation and skipping annoying parameter tuning processes."
    }, {
      "heading" : "VI. CONCLUSION, LIMITATIONS & FUTURE WORK",
      "text" : "In this paper we deduce a solution to exactly minimize the Huber loss in a general optimization framework which unified the most effective approaches in semi-supervision (multi-view learning and manifold regularization). Differently from previous approaches [8], [6], [7], [9], leveraging on our solutions, the proposed HLR algorithm 1) avoids burdensome iterative solving and, at the same time, 2) automatically learns the threshold ξ and 3) actively selects the most beneficial annotations for the learning phase.\nSuch unique aspects resulted in a remarkable performance on different tasks where HLR scored always on par and often\nsuperior to state-of-the-art algorithms for learning from noisy annotations, classical regression problems and crowd counting application. Moreover, low errors were registered by HLR at low computational cost which, guaranteeing fast computation without any expensive parameter tuning.\nFuture works will essentially focus on specializing the framework for classification tasks. Also, to face the main drawback of the method, which consists in the not-learnable weights c, the connections with M-Estimators theory, Multiple Kernel Learning and Radial Basis Function Networks could be investigated."
    }, {
      "heading" : "JACOPO CAVAZZA AND VITTORIO MURINO: ACTIVE-LABELLING BY ADAPTIVE HUBER LOSS REGRESSION 11",
      "text" : "[23] A. Chan, M. Morrow, and N. Vasconcelos, “Analysis of crowded scenes using holistic properties,” in Workshop on Performance Evaluation of Tracking and Surveillance, CVPR, 2009. [24] D. Ryan, S. Denman, S. Sridharan, and C. Fookes, “An evaluation of crowd counting methods, features and regression models,” Computer Vision and Image Understanding, vol. 130, pp. 1–17, 2015. [25] A. C. Davies, J. H. Yin, and S. A. Velastin, “Crowd monitoring using image processing,” Electron Commun Eng, vol. 7, pp. 37–47, 1995. [26] J. Canny, “A computational approach to edge detection,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 8, no. 6, pp. 679–698, 1986. [27] R. Haralick, K. Shanmugam, and I. Dinstein, “Textural features for image classification,” Trans SMC, vol. 3, no. 6, pp. 610–621, 1973. [28] R. Ma, L. Li, W. Huang, and Q. Tian, “On pixel count based crowd density estimation for visual surveillance,” in IEEE Conference on CIS, 2004. [29] C. C. Loy, S. Gong, and T. Xiang, “From semi-supervised to transfer counting of crowds,” in ICCV, 2013. [30] B. Tan, J. Zhang, and L. Wang, “Semi-supervised elastic net for pedestrian counting,” Pattern Recognition, vol. 44, pp. 2297–2304, 2011. [31] K. Chen, S. Gong, T. Xiang, and C. Change Loy, “Cumulative attribute space for age and crowd density estimation,” in CVPR, 2013. [32] T. V. Gestel, J. A. K. Suykens, B. D. Moor, and J. Vandewalle, “Automatic relevance determination for least squares support vector machines classifiers.” in ESANN, 2001. [33] S. An, W. Liu, and S. Venkatesh, “Face recognition using kernel ridge regression.” in CVPR, 2007. [34] A. Liaw and M. Wiener, “Classification and regression by randomforest,” R Journal, vol. 2, no. 3, pp. 18–22, 2002. [35] C. Saunders, A. Gammerman, and V. Vovk, “Ridge regression learning algorithm in dual variables,” in ICML, 1998. [36] X. Wu, G. Liang, K. K. Lee, and Y. Xu, “Crowd density estimation using texture analysis and learning,” in ROBIO, 2006."
    }, {
      "heading" : "APPENDIX",
      "text" : "In this Section we report the proof of Theorem 1 while applying the Representer Theorem [4] to cast the optimization\n12\nproblem into a minimizing on the coefficients w defining f? in (6). Precisely, implementing it in (3) yields\nJλ,γ(w) = 1\n` ∑̀ i=1 Hξ yi − u+∑̀ j=1 c>K(xi,xj)wj + + λ\nu+∑̀ j,k=1 w>j K(xj ,xk)wk+ (20)\n+ γ u+∑̀ i,j=1 u+∑̀ h,k=1 w>hK(xh,xi)MijK(xj ,xk)wj .\nTheoretically, minimizing Jλ,γ over the RKHS SK is fully equivalent to minimizing Jλ,γ with respect to w, being the latter approach computationally convenient because, for this purpose, the optimization domain Rm(u+`) is preferable to an infinite-dimensional functional space. Notice that each addend of Jλ,γ is differentiable with respect to w. Indeed, one has\nH ′ξ(y) =  −ξ if y ≤ −ξ y if |y| ≤ ξ +ξ if y ≥ ξ,\n(21)\nand ‖f‖2K and ‖f‖2M in (20) are also differentiable since polynomials in w1, . . . , wu+`. Thus, for any p = 1, . . . , u+ ` and η = 1, . . . ,m, we compute ∂Jλ,γ ∂wηp , differentiating with respect to wηp , the η-th view of wp. Then,\n∂Jλ,γ ∂wηp = −1 ` ∑̀ i=1 H ′ξ yi − u+∑̀ j=1 c>K(xi,xj)wj  · · u+∑̀ h=1 m∑ α=1 cακα(xαi , x α h) ∂wαh ∂wηp +\nλ u+∑̀ j,k=1 m∑ α=1 ∂wαj ∂wηp κα(xαj , x α k )w α k+\nλ u+∑̀ j,k=1 m∑ α=1 wαj κ α(xαj , x α k ) ∂wαk ∂wηp +\nγ u+∑̀ i,j=1 u+∑̀ h,k=1 m∑ α=1 ∂wαh ∂wηp κα(xαi , x α h)M α ijκ α(xαj , x α k )w α k+\nγ u+∑̀ i,j=1 u+∑̀ h,k=1 m∑ α=1 κα(xαi , x α h)w α hM α ijκ α(xαj , x α k ) ∂wαk ∂wηp . (22)\nIn order to simplify (22), we can apply the relation-\nship ∂wαi ∂wηj = δαηδij , valid for any α, β = 1, . . . ,m and i, j = 1, . . . , u + `, where, for any integers m,n, δmn is the Kronecker delta and δmn = 1 if m = n, while, otherwise, δmn = 0 if m 6= n. Thus,\n∂Jλ,γ ∂wηp = −1 ` ∑̀ i=1 H ′ξ yi − u+∑̀ j=1 c>K(xi,xj)wj  · · u+∑̀ h=1 m∑ α=1 cακα(xαi , x α h)δαηδhp+\nλ u+∑̀ j,k=1 m∑ α=1 δαηδjpκ α(xαj , x α k )w α k+\nλ u+∑̀ j,k=1 m∑ α=1 wαj κ α(xαj , x α k )δαηδkp+\nγ u+∑̀ i,j=1 u+∑̀ h,k=1 m∑ α=1 δαηδhpκ α(xαi , x α h)M α ijκ α(xαj , x α k )w α k+\nγ u+∑̀ i,j=1 u+∑̀ h,k=1 m∑ α=1 κα(xαi , x α h)w α hM α ijκ α(xαj , x α k )δαηδkp.\n(23)\nBy exploiting the properties of Kronecker delta, inside a summation over the index i, δij discards all the addends except to j. Then, we can rewrite equation (23) obtaining\n∂Jλ,γ ∂wηp = −1 ` ∑̀ i=1 H ′ξ yi − u+∑̀ j=1 c>K(xi,xj)wj  · ·cηκη(xηi , x η p)+\nλ u+∑̀ k=1 κη(xηp, x η k)w η k + λ u+∑̀ j=1 wηj κ η(xηj , x η p)+\nγ u+∑̀ i,j=1 u+∑̀ k=1 κη(xηi , x η p)M η ijκ η(xηj , x η k)w η k+\nγ u+∑̀ i,j=1 u+∑̀ h=1 κη(xηi , x η h)w η hM η ijκ η(xηj , x η p). (24)\nTo rearrange equation (24), we can exploit the functional symmetry of both Mercer kernels κ1, . . . , κm and linear operators M1, . . . ,Mm. Then, one sees\n∂Jλ,γ ∂wηp = −1 ` ∑̀ i=1 H ′ξ yi − u+∑̀ j=1 c>K(xi,xj)wj  · ·cηκη(xαi , xαp )+\n2λ u+∑̀ k=1 wηkκ η(xαp , x α k )+\n2γ u+∑̀ i,j=1 u+∑̀ k=1 κη(xαi , x α p )M η ijκ η(xαj , x α k )w η k . (25)\nAfter vectorizing with respect to η = 1, . . . ,m, the deriva-\nJACOPO CAVAZZA AND VITTORIO MURINO: ACTIVE-LABELLING BY ADAPTIVE HUBER LOSS REGRESSION 13\ntive ∂Jλ,γ ∂wp equals to\n−1 ` ∑̀ i=1 H ′ξ yi − u+∑̀ j=1 c>K(xi,xj)wj K(xp,xi)c+ 2λ\nu+∑̀ k=1 K(xp,xk)ωk+\n2γ u+∑̀ i,j=1 u+∑̀ k=1 K(xp,xi)MijK(xj ,xk)wk. (26)\nExpression (26) rewrites ∑u+` i=1 K(xp,xi)ψi, once defined, for any i = 1, . . . , u+ `,\nψi = −1(i ≤ `) 1\n` H ′ξ yi − u+∑̀ j=1 c>K(xi,xj)wj  c+ 2λωi + 2γ\nu+∑̀ j,h=1 MijK(xj ,xh)wh, (27)\nwhere the indicator function 1 is conditionally defined to be 1(i ≤ `) = 1 if i ≤ ` and 1(i ≤ `) = 0 if i > `.\nIf we set ψ1 = · · · = ψu+` = 0, then, from equation (25), ∂Jλ,γ ∂w1 = · · · = ∂Jλ,γ ∂wu+` = 0 and this leads to a solution of (3). But, this is the only solution we have since, as motivated in the paper, the optimization problem (3) has unique solution thanks to Representer Theorem [4]. Then, the previous discussion ensures that, globally, the two systems of equations are totally equivalent since, for every i = 1, . . . , u+ `,\nψi = 0 if and only if ∂Jλ,γ ∂wi = 0. (28)\nHence, the optimization of (3) can be done by solving\n2`λwi + 2`γ u+∑̀ j,h=1 MijK(xj ,xh)wh =\nH ′ξ yi − u+∑̀ j=1 c>K(xi,xj)wj  c (29) for i = 1, . . . , `; and, when i = `+ 1, . . . , u+ `,\n2λwi + 2γ u+∑̀ j,h=1 MijK(xj ,xh)wh = 0. (30)\nSubstitute equation (21) into (29). Then, for i = 1, . . . , `,\n2`λwi + 2`γ u+∑̀ j,h=1\nMijK(xj ,xh)wh = −ξc if yi − ∑u+` j=1 c >K(xi,xj)wj ≤ −ξyi − u+∑̀ j=1 c>K(xi,xj)wj  c if ∣∣∣yi −∑u+`j=1 c>K(xi,xj)wj∣∣∣ ≤ ξ +ξc\nif yi − ∑u+` j=1 c >K(xi,xj)wj ≥ ξ.\n(31)\nIf one defines the following set of indices\nL+[D,w, ξ] = i ≤ ` : u+∑̀ j=1 c>K(xi,xj)wj ≥ yi + ξ , L0[D,w, ξ] =\ni ≤ ` : ∣∣∣∣∣∣ u+∑̀ j=1 c>K(xi,xj)wj − yi ∣∣∣∣∣∣ < ξ ,\nL−[D,w, ξ] = i ≤ ` : u+∑̀ j=1 c>K(xi,xj)wj ≤ yi − ξ , equation (31) therefore becomes\n2`λwi + 2`γ u+∑̀ j,h=1\nMijK(xi,xh)wh = −ξc if i ∈ L+[D,w, ξ]yi − u+∑̀ j=1 c>K(xi,xj)wj  c if i ∈ L0[D,w, ξ] +ξc if i ∈ L−[D,w, ξ]. (32)\nThe thesis follows as the straightforward combination of equations (32) and (30).\n14"
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "This paper addresses the scalar regression problem<lb>presenting a solution for optimizing the Huber loss in a general<lb>semi-supervised setting, which combines multi-view learning and<lb>manifold regularization. To this aim, we propose a principled<lb>algorithm to 1) avoid computationally expensive iterative solu-<lb>tions while 2) adapting the Huber loss threshold in a data-driven<lb>fashion and 3) actively balancing the use of labelled data to<lb>remove noisy or inconsistent annotations from the training stage.<lb>In a wide experimental evaluation, dealing with diverse applica-<lb>tions, we assess the superiority of our paradigm which is able<lb>to combine strong performance and robustness to noise at a low<lb>computational cost.",
    "creator" : "LaTeX with hyperref package"
  }
}