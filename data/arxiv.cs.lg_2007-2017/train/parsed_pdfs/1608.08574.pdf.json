{
  "name" : "1608.08574.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Applying Naı̈ve Bayes Classification to Google Play Apps Categorization",
    "authors" : [ "Babatunde Olabenjo" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "In this project, we built two variations of the Naı̈ve Bayes classifier using open metadata from top developer apps on Google Play Store in other to classify new apps on the store. These classifiers are then evaluated using various evaluation methods and their results compared against each other. The results show that the Naı̈ve Bayes algorithm performs well for our classification problem and can potentially automate app categorization for Android app publishers on Google Play Store.\nI. INTRODUCTION Machine Learning has been widely used in various domains to study and learn from patterns in data to make accurate predictions. The use of machine learning can be seen in our daily lives, especially in email providers for detecting spam messages. With the increase in data in recent years, understanding and making appropriate decisions can be challenging due to the vast amount of data to be analyzed. Furthermore, because the data is in different forms, making accurate decisions can be overwhelming. Machine Learning can allow us to make proper decisions from this vast amount of data by allowing the computer to learn statistically from the large data set and make predictions for new instances based on what has been learnt previously. Also, machines can be trained to learn by feeding it with examples where it then makes decisions based on the carefully selected examples provided.\nTo understand how machine learning systems work, we can classify them into three broad categories based on the nature of the learning system. These learning systems as described by [1] are:\n1) Supervised Learning: In supervised learning, the machine or computer is given a set of examples to learn\nfrom. The machine learns from the inputs (examples) and makes predictions based on the examples provided. 2) Unsupervised Learning: In unsupervised learning, the machine is left to learn from the data provided to it in order to discover patterns that can be used to make predictions eventually. 3) Reinforcement Learning: Reinforcement learning, for example, a computer learning to play a game, is the process whereby the computer learns in a dynamic environment to perform a certain task without explicitly being told if it is close to achieving the goal. This way, the computer learns from the mistakes it has previously made and from the reward it gets from achieving a particular goal [1].\nIn recent years, mobile phones have proven useful in our daily lives especially with the increased availability and reduction in cost. Furthermore, with the advent of App Stores for hosting mobile applications thus providing a variety of tools useful in our daily lives, these mobile devices become more and more integrated into our lives. Google Play Store is one of the biggest App Stores with millions of applications and the official App Store for hosting Android applications for the Android operating system. Finding useful applications can be challenging to app downloaders because many applications are being placed in the wrong category by developers. At the same time, this affects the number of downloads an app will receive ultimately affecting the earnings of app publishers."
    }, {
      "heading" : "A. Motivation",
      "text" : "Making accurate predictions for developers about what category an app should be uploaded to on Google Play Store will potentially improve the discovery of their applications and revenue at the long run. Classification of apps on the store is a useful application of machine learning. There is an increasing number of research using machine learning to classify text, sentiment analysis and documents with Naı̈ve Bayes such as [2]–[5]. With the increasing amount of data available online, providing useful information from this data creates new knowledge and improves the overall success of businesses [5]. Since most machine learning classification algorithms are time-consuming and complicated, using Naı̈ve Bayes classification provides a fast and simple way to classify data [6].\nar X\niv :1\n60 8.\n08 57\n4v 1\n[ cs\n.L G\n] 3\n0 A\nug 2\n01 6\nAndroid app users visiting Google Play Store often find top apps listed whenever they search for an app in the store than non-top apps that may be useful to them. In some situations, these users are willing to look into app categories for apps related to a category they want. Looking for an app via categories in the store can become frustrating for users if there are many wrongly categorised apps; this can be a problem for developers as well, as app users will find it difficult to discover their apps. For example, The Sun Daily 1 reported a case on Health apps where more than 50% of apps were miscategorized leading to fewer downloads.\nDeciding what category to upload an app to can be challenging for developers and Machine learning can be used to suggest suitable categories to developers based on the details they provide. In this research, we find out how machine learning, using supervised learning can suggest appropriate categories with data extracted from successful developers on Google Play Store. The success of an app on the store varies based on the description of the app, the category, whether the app is free or not including other factors. Using data from successful developers on Google Play Store, we can provide a supervised learning training set for efficient classification of apps on the store.\nThe aim of this research is to categorize apps on Google Play Store, using existing data from apps developed by top publishers on the store in order to suggest the best category for a new app. We use Term Frequency–Inverse Document Frequency (TF-IDF) statistics to extract useful information that can be used to build our classifier with Naive Bayes. The effectiveness of the classifier is measured using various validation methods, and the results are presented using a confusion matrix, f1-scores and other statistical variances. These validation methods include the k-fold cross-validation [7], shuffle-split cross-validation used to generate a learning curve that determines the training and test scores for various training data size [8] and the recursive feature elimination for testing the number of features that produces the best results [9]."
    }, {
      "heading" : "II. LITERATURE REVIEW",
      "text" : "There have been numerous application of machine learning in the industry; Amazon store, IBM e-commerce and others have employed machine learning in product classification as well as product recommendation. Advert placement and ad content design have been improved greatly by Google with machine learning. Machine learning has also been used extensively in image processing by Google for their image search. Although there are several machine learning algorithms available for various tasks, classification problems have become most predominant in this space. As described by [10] classification is an example of supervised learning, where a training set of observations correctly identified are fed into the machine learning algorithm to train the system. The process allows the machine learning algorithm to identify correctly\n1TheSunDaily: http://www.thesundaily.my/news/871252\nnew data provided, based on the knowledge acquired from the training set. In unsupervised learning, this process is known as clustering, and it involves data being grouped into categories based on some measure of similarity in the data.\nNumerous research has been done to improve classification in various domain; an example is a research done by Schnack et al. [11] using machine learning to classify patients with schizophrenia, bipolar disorder and healthy subjects with their structural MRI scans. In their research, they used the Support Vector Machine (SVM) machine learning algorithm to create models from gray matter density images. There has also been similar research in product classification using SVM, for example [12] shows that SVM adds value to the classification of fashion brands in their research thereby making it easy for users to narrow down their searches when looking for a particular product. Other research such as sentiment analysis done by [13] used Naı̈ve Bayes algorithm to classify the most identified features in an unstructured review and determine polarity distribution in terms of being positive, negative and neutral. Although little research has been published using Naı̈ve Bayes for product classification, there are so many other classification problems in which the Naı̈ve Bayes algorithm has been very effective."
    }, {
      "heading" : "A. Applying Naı̈ve Bayes to Classification Problems",
      "text" : "The Naive Bayes machine learning model [14] is a popular statistical learning system that has been successful in many applications where features are independent of each other. An example of this model is found in the bag-of-words representation of text where the ordering of words is ignored. One of the earliest application of this model to Information Extraction was done by [15]. Information extraction involving extracting specific facts from text has also played a massive role in simplifying large dataset for users to understand. Zhenmei et al. [16] proposed a smoothing strategy using Naı̈ve Bayes for Information Extraction. The authors show that a well designed smoothing method will improve the performance of a Naı̈ve Bayes Information Extraction learning system. [17] compared Naı̈ve Bayes with other classification algorithms for a medical dataset. Their results show that Naı̈ve Bayes performed better than other algorithms in classifying medical datasets and can be applied to medical data mining; this is due to its simplicity and computational speed. Although Naı̈ve Bayes have been criticized for its independence assumptions, the combination of Naı̈ve Bayes and other classification algorithms can eventually improve its overall performance.\nOther variations of Naı̈ve Bayes algorithm have been proposed to improve the performance of the algorithm for various purposes. An example of this is the modified Naı̈ve Bayes algorithm proposed by [18] to improve the classification of Nepali texts. Since the Nepali text is non-English and lacks basic linguistic components such as the stop words list, the stemmer, which involves removing morphological and inflexional endings from English words, and the Part-Of-Speech Tagger; the authors improved the performance of the classifier using lexicon domain pooling, and because the algorithm is\nflexible, it can be extended to other non-English languages like the Chinese or Japanese language. Another example of improving the Naive Bayes algorithm is the improved Naive Bayes probabilistic model-Multinomial Event Model for text classification by [19]. The model works by pushing down larger counts of word frequency because the Multinomial Naı̈ve Bayes treats the occurrence of a word in a document independently even though multiple occurrences of the same word in a document are not necessarily independent [19] and Multinomial Naı̈ve Bayes does not account for this occurrence. Other weighting schemes in text classification involve the use of N-grams, which is a sequence of n-items from a given document or text and (Term Frequency-Inverse Document Frequency) TF-IDF, which shows how important a word is in a document or a given set of documents [20].\nOne major application of the Naı̈ve Bayes algorithm is in spam detection. [21] proposed a Naı̈ve Bayes spam detection method based on decision trees, they also presented an improved method based on classifier error weight. Their experimentation shows that the implementation is valid, but there are not many solutions for valid incremental decision tree induction algorithm as they described [21]. Another use of Naı̈ve Bayes is that proposed by [22] for ranking. The authors used a weighed Naı̈ve Bayes for ranking in which each attribute has a different weight. Their results show that the weighted Naı̈ve Bayes outperforms the standard Naı̈ve Bayes, and both the weighted Naı̈ve Bayes and the standard Naı̈ve Bayes are better in performance than the decision tree algorithm [22].\nThere has been a few research done on Google Play Store using sentiment analysis applied to customer reviews on mobile apps to determine their polarity such as [23], where app reviews were automatically classified and result compared with other classification methods. From their results, the authors show that natural language processing with metadata from apps can improve classification precision significantly. This system can improve the design of review analytics tools for developers and app users when a large number of reviews is involved. Additionally, [24] used sentiment analysis on customer reviews on the store. Although the authors did not use Naı̈ve Bayes for analyzing the reviews, research such as [19], [21] and [23] show that machine learning using Naı̈ve Bayes can also be applied in classifying customer reviews on Google Play Store. Most research show that machine learning using the Naı̈ve Bayes algorithm can be implemented in numerous domains for classification problems. On Google Play Store, various research focus on sentiment analysis in customer reviews. Little research has been done using machine learning to suggest appropriate categories for app developers with well known categorized applications on the store or to detect spam apps based on app metadata on the app store."
    }, {
      "heading" : "III. ALGORITHM DESCRIPTION",
      "text" : "Naı̈ve Bayes classifier used in this research is a simplified probabilistic classifier that is based on the Bayes theorem. Bayes theorem describes the probability of an event based\non the conditions relating to the event. Bayes rule is defined mathematically as (1)\nP (A|B) = P (B|A)P (A) P (B)\n(1)\nwhere A and B are two events such that P (A) which is the prior probability, and P (B) are the probabilities of A and B independent of each other 2.\nP (A|B), is the posterior probability described as the conditional probability of observing event A given that B is true.\nP (B|A), is the likelihood described as the probability of observing event B given that A is true.\nAs discussed earlier in previous sections, the Naı̈ve Bayes classifier has been used in various applications such as document classification, email spam detection and sentiment analysis. The classifier is based on an assumption that all attributes are independent of each other. Although this assumption makes other advanced classifier perform better in some scenarios, the Naı̈ve Bayes classifier is known for its speed and less training set required to solve a classification problem. Since the classification of apps will be done on a regular computer, Naı̈ve Bayes is most efficient in terms of CPU and memory consumption as described in [25]."
    }, {
      "heading" : "A. Theoretical Background",
      "text" : "From [26] we can see that even though the probability estimates of Naı̈ve Bayes is sometimes of low quality, the classification decisions can provide good results. In text classification as described by [26], words are represented as tokens and classified into a particular class and by using the Maximum a Posterior (MAP) we can generate the classifier as described by [26] as (2).\ncMAP = argmax c∈C\n(P (c|d))\n= argmax c∈C P (c) ∏ 1≤k≤nd P (tk|c)  (2) where P (c|d) represents conditional probability of class c\ngiven document d\ntk represent the tokens of the document, C represents the set of classes used in the classification.\nP (c) represents the prior probability of class c and P (tk|c) represents the conditional probability of token tk given the class c.\nHere, we estimate the likelihood, multiplied by the probability of a particular class prior for each class and select\n2Bayes Theorem: https://en.wikipedia.org/wiki/Bayes theorem\nthe class with the highest probability represented as cmap. In order to prevent underflow when calculating the product of the probabilities, we maximize the sum of their logarithms as described by [26] using (3), thereby choosing classes with the highest log score represented as cmap.\ncmap = argmax c∈C logP (c) + ∑ 1≤k≤nd logP (tk|c)  (3) Furthermore, if a word does not occur in a particular class, then the conditional probability is 0 giving us log(0) which will eventually throw an error. To resolve this, we use Laplace smoothing by adding 1 to each count, giving us (4).\nP (t|c) = Tct + 1∑ t′∈V (Tct′ + 1)\n= Tct + 1( ∑\nt′∈V Tct′\n) +B′\n(4)"
    }, {
      "heading" : "B. Application in Google Play Store App Categorization",
      "text" : "To build a Naı̈ve Bayes classifier that would classify words for our dataset as features for a particular category, we would use two variations; the Multinomial and Bernoulli Naı̈ve Bayes classifiers. The Multinomial Naı̈ve Bayes classifier is used when the number of occurrence of a word matter in our prediction which seem to be relevant in classifying apps into categories. For example, if the word “fun” occurs multiple times in the Games category and a very few times in the Education category, it shows that the word “fun” is more important for the Game category and less important for the Education category. Alternatively, the Bernoulli Naı̈ve Bayes classifier is used when the absence of a particular word matters. For example, if the word “fun” does not occur in the Business category it is assumed that “fun” cannot be used to classify an app into the Business category because it does not exist there. The Bernoulli Naı̈ve Bayes classifier is commonly used in classifying Spam and Adult contents.\nIn this research, we lay emphasis on the Multinomial Naı̈ve Bayes classifier because the number of occurrence of a word in an app detail is important in classifying the app into a category. The Multinomial Naı̈ve Bayes classification as described by [26] is represented as (5).\nP (t|c) = Tct∑ t′∈V Tct′ (5)\nThis estimates the probability of a term t given the category c as the relative frequency of term t in apps belonging to category c. The algorithm to be used as described in [26] is shown in Figure 1\nThe Bernoulli Naı̈ve Bayes classifier differs from Multinomial Naive Bayes classifier as it does not take into account\n3Source: http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-textclassification-1.html\nthe number of occurrence of the word. Bernoulli Naı̈ve Bayes provides a Boolean indicator for the occurrence of a word as 1 and 0 if the word does not exist. The algorithm that will be used in this project as described in [26] is shown in Figure 2."
    }, {
      "heading" : "IV. DATASET OVERVIEW",
      "text" : "The dataset used in this project is the metadata of 1,197,995 of 1,390,545 apps after filtering out bad data. This dataset is a CSV file containing apps extracted from Google Play Store as at June 2015 with GooglePlayStoreCrawler5. This data contains the following attributes:\n4Source: http://nlp.stanford.edu/IR-book/html/htmledition/the-bernoullimodel-1.html\n5GooglePlayStoreCrawler: https://github.com/MarcelloLins/GooglePlayAppsCrawler\nAppName, Developer, IsTopDeveloper, DeveloperURL, DeveloperNormalizedDomain, Category, IsFree, Price, Reviewers, Score.Total, Score.Count, Score.FiveStars, Score.FourStars, Score.ThreeStars,\nScore.TwoStars, Score.OneStars, Instalations, CurrentVersion, MinimumOSVersion, ContentRating,\nHaveInAppPurchases, DeveloperEmail, DeveloperWebsite, PhysicalAddress, LastUpdateDate,\nDescription\nThe data contains 41 categories out of the current 43 categories available on Google Play, leaving out “wallpapers” and “widget” which were formally classified as “personalization” before being separated later. To ensure the correctness of our data, we extracted all apps uploaded by top developers, leaving us with 10,369 apps, labelled by their category. These apps will be used to train and test our classifier because they provide a verified source for classification. Providing good data for the classifier will improve its accuracy. Furthermore, it is very likely that successful developers will take much time to properly categorise their apps, provide eloquent descriptions for their apps and ensure their details are complete."
    }, {
      "heading" : "A. Feature Processing",
      "text" : "To create an accurate predictive model, we selected five attributes from all applications; this will be used to extract our features for each category later. These attributes include: • AppName • ContentRating • IsFree • HaveInAppPurchases • Description\nUsing the bag-of-words model, we extracted tokens from our 10,369 apps after combining these attributes together, removing stop words such as “the, as, is, who, on, . . . ”, removing numbers, punctuation and setting all words to lowercase. At this stage, we have 113,463 features that were extracted from 10,369 app details using Term Frequency-Inverse Document Frequency (TF-IDF) statistics. TF-IDF allows us to reduce the impact of tokens that occurs frequently, so that they do not affect features that occur in small amounts. The formula used to calculate TF-IDF is given as (6) as described in [20].\nwn = TFn × log(IDFn) (6)\nWhere wn represents a word in the vector (w0, w1, w2, ..., wn) for each app, represented as a document D.\nTFn is the Term Frequency of the n-th word in document D.\nIDFn is represented as the Inverse Document Frequency of the n-th term in document D represented as:\n#documents\n#documents containing the n− th term\nThe TF-IDF method is more preferable than a regular frequency count of tokens [27]; this is because specific words are\nused to describe specific categories in the store, such as “fun” in “Games” category and “money” in “Business” category. The number of occurrence of these words determine how important they are for all apps. Table I shows the aggregate distributions for each app category in our 10,369 app dataset."
    }, {
      "heading" : "B. Further Processing",
      "text" : "Further processing was done to the 10,369 app dataset in order to remove attributes or features with little significance. In this process, the TF-IDF method was used again to reduce the number of features from 113,463 to 14,571, by removing words that occur in more than 70% and words that occur in less than 0.05% of all 10,369 apps. Furthermore, apps were grouped into two filters as shown below:\n1) All Apps: All apps contain all 10,369 apps with 14,571 optimal features after preprocessing. 2) Filtered Apps: Filtered apps contain 8,366 apps after apps with low description (less than a simple paragraph)\nwere removed. This ensures we remove apps that are not descriptive enough. A descriptive app is about four to six sentences which is approximately 100 words 6. Furthermore, categories will little support, i.e. categories with very few apps were removed. These categories are (COMICS, LIBRARIES AND DEMO, GAME MUSIC, GAME WORD) and they were removed because when the dataset is split into test and training data, the categories will have very few support and thus affect the performance of the classifier.\nFor each filter above, apps were further grouped into four categories:\n1) OnlyGameApps: These are apps with the “GAME ” category. In “Filtered Apps” there are 4,374 games with 15 categories and in “All apps” there are 5,174 games with 17 categories. 2) GroupedGameApps: These apps are all 10,369 apps where all “GAME ” categories are grouped together, in this group, all games are grouped as “GAMES” giving us 25 categories other than the 41 initial categories. 3) OnlyOtherCategories: This contains all apps in other categories except games. With this, we have 24 categories and 5,195 apps in the “All apps” filter. 4) AllCategories: TThese are all 41 categories from the 10,369 apps and 37 categories from 8,366 filtered apps.\nTable II shows the distribution of the grouped apps used in evaluating the performance of the classifier."
    }, {
      "heading" : "C. Prior and Likelihood Formation",
      "text" : "Given our dataset, we can calculate our prior and likelihood for each category using Naı̈ve Bayes as\ncMAP = argmax c∈C\nP (app|c)P (c)\n= argmax c∈C\nP (w0, w1, w2, ..., wn|c)P (c)\nFor example, the prior for the ”BUSINESS” category is calculated as:\nP (b) = Nb N = 158 10369 = 0.015\n6Paragraph Length: https://strainindex.wordpress.com/2010/10/25/plainparagraph-length/\nThe likelihood using multinomial Naı̈ve Bayes can be calculated as:\nP (w|b) = count(w, b) count(b) ,∀w ∈W\nFrom this we can estimate the probability of a category given an app using Naı̈ve Bayes as (7).\ncNB = argmax c∈C\nP (cj) ∏\nw∈W P (w|c) (7)"
    }, {
      "heading" : "V. RESULTS",
      "text" : "The algorithm was implemented using Python and the “sklearn” 7 machine learning library. Sklearn is a machine learning library in Python built on NumPy, SciPy, and matplotlib 8. The library allows us to perform data mining and data analysis such as clustering, regression analysis, classification and preprocessing of data."
    }, {
      "heading" : "A. Evaluating the model",
      "text" : "Evaluating the model used for classifying apps on Google Play involved using various cross-validation methods to test its performance. Other evaluation strategies involved testing the model with the training and testing data, drawing a confusion matrix to determine true positives/negatives and false positive/negatives. Furthermore, a learning curve was plotted to see how the model performs as the training and testing set increases and a Recursive Feature Elimination with cross-validation applied to see how the model performs when eliminating features for each iteration. The following highlights how this classifier performs using these methods with the AllCategories apps and GroupedGameApps apps as described in the previous section. AllCategories represents 41 categories from 10,369 apps, and the GroupedGameApps represents 25 categories from 10,369 applications when all games are grouped together as a single category.\n1) Training and Test Data evaluation: The data was split into training and testing dataset of approximately 80% to 20%. The training dataset was used to train the classifier for both Multinomial and Bernoulli classifiers and then used to predict the outcome of the test dataset. Figure 3 shows the performance of the classifier for all 10,369 apps.\nFrom the results above, it is seen that the Multinomial Naı̈ve Bayes classifier performs better than the Bernoulli Naı̈ve Bayes classifier with about 70% accuracy. The classifier is also faster in terms of computation speed than the Bernoulli Naı̈ve Bayes classifier. To improve accuracy, all “games” were grouped into a “GAMES” category, reducing the number of categories to 25 and tested with the classifier. Figure 4 shows a significant improvement in the results with 85.6% accuracy in the Multinomial Naı̈ve Bayes and 82.4% accuracy in the Bernoulli Naı̈ve Bayes.\n7Sklearn: http://scikit-learn.org 8SciPy Libraries: http://scipy.org\n2) K-Fold Cross Validation: Using K-Fold cross-validation, a 2-Fold and 10-Fold cross-validation for both Multinomial Naı̈ve Bayes and the Bernoulli Naı̈ve Bayes classifiers was computed to determine how each classifier performs overall. Table III shows the average score for both the 2-Fold and the 10-Fold cross-validation.\nFrom the results above, it can be seen that the Multinomial Naı̈ve Bayes classifier still performs better than the Bernoulli. Also, the average classifier score improves as the number of\nfolds’ increases.\n3) Recursive Feature Elimination using StratifiedKFold Cross Validation: Recursive Feature Elimination involves selecting features recursively and then reducing the number of features. Here, the classifier is trained with the initial set of features and scored. Then, features are gradually pruned from the current set of features in each iteration and scored. In this test, the StratifiedKFold cross-validation involving splitting the data n-times while shuffling the dataset was used with a Recursive Feature Elimination. Stratification involves rearranging the data as to ensure each fold is a good representative of the whole. In this test, 20% of the features were removed for each iteration to determine the number of features that performs best with the model. Figure 5 shows a Recursive Feature Elimination (RFE) with cross-validation for the Multinomial Naı̈ve Bayes Classifier for all categories.\nIn Figure 6, we can see a significant improvement in the result when all games are grouped together. The classifier performs well initially with about 20% of the features used resulting in a 50% accuracy compared to the 10% accuracy observed when all 41 categories are used in Figure 5.\n4) Learning Curve: A learning curve is a graphical representation of the increase in learning with experience. The experience of the classifier increases as the number of training dataset increases thus the precision or accuracy of the algorithm is improved. Figure 7 shows the learning curves for Multinomial Naı̈ve Bayes for the 41 categories (AllCategories).\nSignificant improvement is seen in Figure 8 when all games are grouped together.\n5) Classification Report: Table IV shows the general statistics of the Multinomial Naı̈ve Bayes classifier for AllCategories. This classifier is selected because it performs better\nthan the Bernoulli Naı̈ve Bayes classifier. In Table IV, we show the precision (accuracy), recall, and f1-score which is the harmonic mean of precision and recall. False Positives (FP), False Negatives (FN), True Positive (TP), True Negative (TN), True Positive Ratio (TPR), False Negative Ratio (FNR) and support for the classifier derived from the confusion matrix for the 2,074 testing dataset is also displayed.\nFrom the results obtained, a confidence level of around 70% shows that the algorithm performs well in determining the category of a set of applications. However, because the ”Game” category has several sub-categories, the algorithm might misclassify some gaming apps. For example, an “Action Game” might be misclassified as an “Arcade Game”, and this will eventually affect the performance of the model because they both have similar words used to describe them. However, grouping the ”Games” category significantly improved the performance of the model to about 85%. We can further\ndetermine how gaming apps perform by training the algorithm on the “games” category only and further compare that with other categories apart from games.\nB. Improving Results\nThe dataset was further filtered to remove categories with few applications and remove apps with few words describing them in order to improve accuracy. The resulting filtered dataset contains 8,366 apps and 37 categories compared to the 10,369 apps used previously. The results show that further optimization of the dataset can significantly improve performance. Figure 9 reveals the improvement over the previous result in both Multinomial and Bernoulli Naı̈ve Bayes algorithms for all 37 categories.\nFurthermore, grouping all games in a single category, significantly improved the overall performance of the classifier. The Multinomial Naı̈ve Bayes classifier improved to about 87% accuracy from 85% accuracy observed in the previous results. We can determine the effect of game categories by plotting a learning curve on all 4,374 games to discover how the classifier performs on all 15 game categories. Figure 10 shows the learning curve for all filtered Game categories using 20% testing dataset from the 4,374 games.\nFrom the results, the classifier was 67% accurate in categorizing games. This result can be compared to all other categories apart from games. Figure 11 shows that the classifier performed better when classifying apps in other categories than classifying games. The result observed might be as a result of close similarity in the words chosen to describe a game; it is also possible that games with similar features will be described with similar words even if they are in different categories resulting in false negatives.\nWe can observe the misclassification of games from the confusion matrix in Table V. The results show 875 games tested against 3,499 games used to train the classifier. From the result, several misclassification is seen in GAME ACTION and GAME ARCADE, GAME CASUAL and GAME ARCADE, GAME PUZZLE and GAME CASUAL, GAME ADVENTURE and GAME CASUAL. It can also be observed that many apps in these categories are described with similar words from the word tokens thereby resulting in misclassification of those apps. Overall, the algorithm was successful in classifying 67% of games and 72.7% of all applications in all categories. Further improvement is seen when all games are grouped and one with 87% accuracy.\nTable VI shows the overall performance of both the Multinomial Naı̈ve Bayes classifier and the Bernoulli Naive Bayes classifier in categorizing Android applications with various\nfilters in our dataset. The table shows the classifier precision on a training and testing dataset of 80% and 20% respectively."
    }, {
      "heading" : "VI. DISCUSSION",
      "text" : "The results show that Multinomial Naı̈ve Bayes performs better than the Bernoulli Naı̈ve Bayes algorithm; this because the number of occurrence of a word matters a lot in our classification problem. In Bernoulli Naı̈ve Bayes, the absence of a word matters, and although the results are not as efficient as the Multinomial Naı̈ve Bayes, it can play a role in classifying other problems such as spam detection in emails. The overall result shows that the algorithm is weak at classifying games, because of the similarity in words used to describe most gaming applications in different categories. It can also be observed that the classifier performs better when classifying gaming apps in entirely different categories than others. For example, TRANSPORTATION, MEDICAL and WEATHER categories all have a precision of 100%. Words such as “raining” in WEATHER will most likely not occur in many other categories, same as “doctor” or “pregnancy” in the MEDICAL category. Furthermore, the dataset used do not necessarily mean that the applications were classified accurately. Top developers on Google Play might misclassify some of their applications and eventually affect the performance of the classifier. Alternatively, misclassified applications, especially in the GAMES category may not necessarily mean the app was classified wrongly; this can be observed in “Arcade” and “Action” games as they are sometimes misclassified.\nIn general, Naı̈ve Bayes is useful when it comes to text classification because of its speed and performance even with a limited training set. Additionally, this makes Naı̈ve Bayes useful as a baseline for machine learning algorithms in various research. Machine learning plays a significant role in data mining because the computer can learn from past experiences (training), making it useful in analyzing large dataset difficult for humans to comprehend. From data acquisition to data optimization and algorithm selection, we can see that proper data collection and optimization can significantly improve the overall performance of the classifier. Furthermore, selecting the right algorithm for a classification problem is also important, just as the Multinomial Naive Bayes algorithm performed better than the Bernoulli Naı̈ve Bayes algorithm in our classification problem. Additionally, overfitting can be reduced as seen from the learning curve when more data is used to train the classifier."
    }, {
      "heading" : "VII. FUTURE WORK",
      "text" : "Naı̈ve Bayes assumes features are independent, and this is likely not the case in many scenarios. There are still several aspects that require improvement, and it is important to note that Naı̈ve Bayes is relatively efficient in document classification as applied in our app categorization, but this can further be improved by using more advanced algorithms. Further enhancements can be made to our classifier as described below."
    }, {
      "heading" : "A. N-gram usage",
      "text" : "Using 2-grams for example, “alarm clock”, “angry birds” and “music box”, may improve results as these phrases specifically describe a feature. Several n-grams can be combined such as a unigram, 2-grams or 3-grams, to see how precision can be improved with phrases and single words using natural language processing techniques."
    }, {
      "heading" : "B. Other Machine Learning algorithms",
      "text" : "Advanced classification algorithms such as Support Vector Machines (SVM), Hidden Markov Models (HMM) and Decision Trees can further improve categorization accuracy. Although computationally intensive, these algorithms may perform better than the Naı̈ve Bayes classifier."
    }, {
      "heading" : "VIII. SUMMARY",
      "text" : "Similar words used to describe various applications in different categories can affect the performance of the classifier. This situation is observed in the gaming categories. Although the error rates are higher than classifying apps in other categories, it can be seen that most misclassification that occur in the “games” category, does not necessarily mean those apps do not belong to the category predicted. For example, an app published in an “Arcade Game” category may also be suitable in an “Action Game” category depending on the type of game. Furthermore, the training data used is not fundamentally a full proof of how apps are classified. Additionally, it can be observed that Multinomial Naı̈ve Bayes performed better than the Bernoulli Naive Bayes classifier in text classification where\nthe number of occurrence of a word is important. In general, proper data collection, optimization and large training set can significantly improve the performance of a machine learning classification algorithm."
    } ],
    "references" : [ {
      "title" : "Artificial Intelligence: A Modern Approach, 3rd edition",
      "author" : [ "S. Russell", "P. Norvig" ],
      "venue" : "Pearson Education Limited,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2009
    }, {
      "title" : "On The Naive Bayes Model for Text Categorization",
      "author" : [ "S. Eyheramendy", "D.D. Lewis", "D. Madigan" ],
      "venue" : "Proceedings Artificial Intelligence & Statistics, 2003, pp. 3–10.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Naive Bayes for Text Classification with Unbalanced Classes",
      "author" : [ "E. Frank", "R.R. Bouckaert" ],
      "venue" : "PKDD’06 Proceedings of the 10th European conference on Principle and Practice of Knowledge Discovery in Databases, vol. 4213, Berlin, Germany, 2006, pp. 503–510.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Text Classification for Authorship Attribution Using Naive Bayes Classifier with Limited Training Data",
      "author" : [ "F. Howedi", "M. Mohd" ],
      "venue" : "Computer Engineering and Intelligent Systems, vol. 5, no. 14, pp. 48–56, 2014.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Is Naı̈ve bayes a good classifier for document classification?",
      "author" : [ "S.L. Ting", "W.H. Ip", "A.H.C. Tsang" ],
      "venue" : "International Journal of Software Engineering and its Applications,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2011
    }, {
      "title" : "Fast and Accurate Sentiment Classification Using an Enhanced Naive Bayes Model",
      "author" : [ "V. Narayanan", "I. Arora", "A. Bhatia" ],
      "venue" : "Intelligent Data Engineering and Automated Learning – IDEAL 2013. Springer Berlin Heidelberg, 2013, vol. 8206, pp. 194–201.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Estimation of prediction error by using K-fold crossvalidation",
      "author" : [ "T. Fushiki" ],
      "venue" : "Statistics and Computing, vol. 21, no. 2, pp. 137–146, 2011.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "RANDOM PERMUTATION TESTING IN MULTIPLE LINEAR REGRESSION",
      "author" : [ "M.-H. Huh", "M. Jhun" ],
      "venue" : "Communications in Statistics - Theory and Methods, vol. 30, no. 10, pp. 2023–2032, aug 2001.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2023
    }, {
      "title" : "Recursive feature elimination with random forest for PTR-MS analysis of agroindustrial products",
      "author" : [ "P.M. Granitto", "C. Furlanello", "F. Biasioli", "F. Gasperi" ],
      "venue" : "Chemometrics and Intelligent Laboratory Systems, vol. 83, no. 2, pp. 83–90, 2006.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Introduction to Machine Learning",
      "author" : [ "E. Alpaydin" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2010
    }, {
      "title" : "Can structural MRI aid in clinical classification? A machine learning study in two independent samples of patients with schizophrenia, bipolar disorder and healthy subjects",
      "author" : [ "H.G. Schnack", "M. Nieuwenhuis", "N.E. van Haren", "L. Abramovic", "T.W. Scheewe", "R.M. Brouwer", "H.E. Hulshoff Pol", "R.S. Kahn" ],
      "venue" : "NeuroImage, vol. 84, pp. 299–306, jan 2014.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Using supervised learning to classify clothing brand styles",
      "author" : [ "C. David Kreyenhagen", "T.I. Aleshin", "J.E. Bouchard", "A.M.I. Wise", "R.K. Zalegowski" ],
      "venue" : "2014 Systems and Information Engineering Design Symposium (SIEDS). Charlottesville, VA, USA: IEEE, apr 2014, pp. 239–243.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Sentiment Analysis on Unstructured Review",
      "author" : [ "R. Nithya", "D. Maheswari" ],
      "venue" : "2014 International Conference on Intelligent Computing Applications. Coimbatore, India: IEEE, mar 2014, pp. 367–371.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Automatic Indexing: An Experimental Inquiry",
      "author" : [ "M.E. Maron" ],
      "venue" : "Journal of the ACM, vol. 8, no. 3, pp. 404–417, jul 1961.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1961
    }, {
      "title" : "Machine learning for information extraction in informal domains",
      "author" : [ "D. Freitag" ],
      "venue" : "Machine learning, vol. 39, no. 2-3, pp. 169–202, 2000.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Naive Bayes Modeling with Proper Smoothing for Information Extraction",
      "author" : [ "Zhenmei Gu", "N. Cercone" ],
      "venue" : "2006 IEEE International Conference on Fuzzy Systems. Vancouver, BC, Canada: IEEE, 2006, pp. 393–400.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Medical Data Classification with Naive Bayes Approach",
      "author" : [ "K. Al-Aidaroo", "A. Bakar", "Z. Othman" ],
      "venue" : "Information Technology Journal, vol. 11, no. 9, pp. 1166–1174, sep 2012.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A lexicon pool augmented Naive Bayes Classifier for Nepali Text",
      "author" : [ "S.K. Thakur", "V.K. Singh" ],
      "venue" : "2014 Seventh International Conference on Contemporary Computing (IC3). Noida, India: IEEE, aug 2014, pp. 542–546.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "An Effective Algorithm for Improving the Performance of Naive Bayes for Text Classification",
      "author" : [ "Guo Qiang" ],
      "venue" : "2010 Second International Conference on Computer Research and Development, no. 1. Kuala Lumpur, Malaysia: IEEE, 2010, pp. 699–701.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Low Cost Portability for Statistical Machine Translation based on N-gram Frequency and TF-IDF",
      "author" : [ "M. Eck", "S. Vogel", "A. Waibel" ],
      "venue" : "IInternational Workshop on Spoken Language Translation, IWSLT 2005, Pittsburgh, PA, USA, 2005, pp. 61–67.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "An Approach to Spam Detection by Naive Bayes Ensemble Based on Decision Induction",
      "author" : [ "Z. Yang", "X. Nie", "W. Xu", "J. Guo" ],
      "venue" : "Sixth International Conference on Intelligent Systems Design and Applications, vol. 2. Jinan, China: IEEE, oct 2006, pp. 861–866.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Learning Weighted Naive Bayes with Accurate Ranking",
      "author" : [ "H. Zhang", "Shengli Sheng" ],
      "venue" : "Fourth IEEE International Conference on Data Mining (ICDM’04). IEEE, 2004, pp. 567–570.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Bug report, feature request, or simply praise? On automatically classifying app reviews",
      "author" : [ "W. Maalej", "H. Nabil" ],
      "venue" : "2015 IEEE 23rd International Requirements Engineering Conference (RE). Ottawa, ON: IEEE, aug 2015, pp. 116–125.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Numeric rating of Apps on Google Play Store by sentiment analysis on user reviews",
      "author" : [ "M.R. Islam" ],
      "venue" : "2014 International Conference on Electrical Engineering and Information & Communication Technology. Dhaka, Bangladesh: IEEE, apr 2014, pp. 1–4.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Comparing naive Bayes, decision trees, and SVM with AUC and accuracy",
      "author" : [ "J. Huang", "J. Lu", "C. Ling" ],
      "venue" : "Third IEEE International Conference on Data Mining. IEEE Comput. Soc, 2003, pp. 553–556.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "These learning systems as described by [1] are:",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 0,
      "context" : "This way, the computer learns from the mistakes it has previously made and from the reward it gets from achieving a particular goal [1].",
      "startOffset" : 132,
      "endOffset" : 135
    }, {
      "referenceID" : 1,
      "context" : "There is an increasing number of research using machine learning to classify text, sentiment analysis and documents with Naı̈ve Bayes such as [2]–[5].",
      "startOffset" : 142,
      "endOffset" : 145
    }, {
      "referenceID" : 4,
      "context" : "There is an increasing number of research using machine learning to classify text, sentiment analysis and documents with Naı̈ve Bayes such as [2]–[5].",
      "startOffset" : 146,
      "endOffset" : 149
    }, {
      "referenceID" : 4,
      "context" : "businesses [5].",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 5,
      "context" : "Since most machine learning classification algorithms are time-consuming and complicated, using Naı̈ve Bayes classification provides a fast and simple way to classify data [6].",
      "startOffset" : 172,
      "endOffset" : 175
    }, {
      "referenceID" : 6,
      "context" : "These validation methods include the k-fold cross-validation [7], shuffle-split cross-validation used to generate a learning curve that determines the training and test scores for various training data size [8] and the recursive feature elimination for testing the number of features that produces the best results [9].",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 7,
      "context" : "These validation methods include the k-fold cross-validation [7], shuffle-split cross-validation used to generate a learning curve that determines the training and test scores for various training data size [8] and the recursive feature elimination for testing the number of features that produces the best results [9].",
      "startOffset" : 207,
      "endOffset" : 210
    }, {
      "referenceID" : 8,
      "context" : "These validation methods include the k-fold cross-validation [7], shuffle-split cross-validation used to generate a learning curve that determines the training and test scores for various training data size [8] and the recursive feature elimination for testing the number of features that produces the best results [9].",
      "startOffset" : 315,
      "endOffset" : 318
    }, {
      "referenceID" : 9,
      "context" : "As described by [10] classification is an example of supervised learning, where a",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 10,
      "context" : "[11] using machine learning to classify patients with schizophrenia, bipolar disorder and healthy subjects with their structural MRI scans.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "There has also been similar research in product classification using SVM, for example [12] shows that SVM adds value to the classification of fashion brands in their research thereby making it easy for users to narrow down their searches when looking for a particular product.",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 12,
      "context" : "Other research such as sentiment analysis done by [13] used Naı̈ve Bayes algorithm to classify the most identified features in an unstructured review and determine polarity distribution in terms of being positive, negative and neutral.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 13,
      "context" : "The Naive Bayes machine learning model [14] is a popular statistical learning system that has been successful in many applications where features are independent of each other.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 14,
      "context" : "One of the earliest application of this model to Information Extraction was done by [15].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 15,
      "context" : "[16] proposed a smoothing strategy using Naı̈ve Bayes for Information Extraction.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[17] compared Naı̈ve Bayes with other classification algorithms for a medical dataset.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "An example of this is the modified Naı̈ve Bayes algorithm proposed by [18] to improve the classification of Nepali texts.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 18,
      "context" : "Another example of improving the Naive Bayes algorithm is the improved Naive Bayes probabilistic model-Multinomial Event Model for text classification by [19].",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 18,
      "context" : "independently even though multiple occurrences of the same word in a document are not necessarily independent [19] and Multinomial Naı̈ve Bayes does not account for this occurrence.",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 19,
      "context" : "Other weighting schemes in text classification involve the use of N-grams, which is a sequence of n-items from a given document or text and (Term Frequency-Inverse Document Frequency) TF-IDF, which shows how important a word is in a document or a given set of documents [20].",
      "startOffset" : 270,
      "endOffset" : 274
    }, {
      "referenceID" : 20,
      "context" : "[21] proposed a Naı̈ve Bayes spam detection method based on decision trees, they also presented an improved method based on classifier error weight.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "Their experimentation shows that the implementation is valid, but there are not many solutions for valid incremental decision tree induction algorithm as they described [21].",
      "startOffset" : 169,
      "endOffset" : 173
    }, {
      "referenceID" : 21,
      "context" : "Another use of Naı̈ve Bayes is that proposed by [22] for ranking.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 21,
      "context" : "Their results show that the weighted Naı̈ve Bayes outperforms the standard Naı̈ve Bayes, and both the weighted Naı̈ve Bayes and the standard Naı̈ve Bayes are better in performance than the decision tree algorithm [22].",
      "startOffset" : 213,
      "endOffset" : 217
    }, {
      "referenceID" : 22,
      "context" : "There has been a few research done on Google Play Store using sentiment analysis applied to customer reviews on mobile apps to determine their polarity such as [23], where app reviews were automatically classified and result compared with other classification methods.",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 23,
      "context" : "Additionally, [24] used sentiment analysis on customer reviews on the store.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 18,
      "context" : "Although the authors did not use Naı̈ve Bayes for analyzing the reviews, research such as [19], [21] and [23] show that machine learning using Naı̈ve Bayes can also be applied in classifying customer reviews on Google Play Store.",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 20,
      "context" : "Although the authors did not use Naı̈ve Bayes for analyzing the reviews, research such as [19], [21] and [23] show that machine learning using Naı̈ve Bayes can also be applied in classifying customer reviews on Google Play Store.",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 22,
      "context" : "Although the authors did not use Naı̈ve Bayes for analyzing the reviews, research such as [19], [21] and [23] show that machine learning using Naı̈ve Bayes can also be applied in classifying customer reviews on Google Play Store.",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 24,
      "context" : "Since the classification of apps will be done on a regular computer, Naı̈ve Bayes is most efficient in terms of CPU and memory consumption as described in [25].",
      "startOffset" : 155,
      "endOffset" : 159
    }, {
      "referenceID" : 19,
      "context" : "The formula used to calculate TF-IDF is given as (6) as described in [20].",
      "startOffset" : 69,
      "endOffset" : 73
    } ],
    "year" : 2016,
    "abstractText" : "There are over one million apps on Google Play Store and over half a million publishers. Having such a huge number of apps and developers can pose a challenge to app users and new publishers on the store. Discovering apps can be challenging if apps are not correctly published in the right category, and, in turn, reduce earnings for app developers. Additionally, with over 41 categories on Google Play Store, deciding on the right category to publish an app can be challenging for developers due to the number of categories they have to choose from. Machine Learning has been very useful, especially in classification problems such sentiment analysis, document classification and spam detection. These strategies can also be applied to app categorization on Google Play Store to suggest appropriate categories for app publishers using details from their application. In this project, we built two variations of the Naı̈ve Bayes classifier using open metadata from top developer apps on Google Play Store in other to classify new apps on the store. These classifiers are then evaluated using various evaluation methods and their results compared against each other. The results show that the Naı̈ve Bayes algorithm performs well for our classification problem and can potentially automate app categorization for Android app publishers on Google Play Store.",
    "creator" : "LaTeX with hyperref package"
  }
}