{
  "name" : "1611.09904.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "C-RNN-GAN: Continuous recurrent neural networks with adversarial training",
    "authors" : [ "Olof Mogren" ],
    "emails" : [ "olof@mogren.one" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Generative adversarial networks (GANs) are a class of neural network architectures designed with the aim of generating realistic data [Goodfellow et al., 2014]. The approach involves training two neural models with conflicting objectives, one generator (G), and one discriminator (D), forcing each other to improve. The generator tries to produce samples that looks real, and the discriminator tries to discriminate between generated samples and real data. Using this framework makes it possible to train deep generative models without expensive normalizing constants, and the technique has proven to produce highly realistic samples of data [Denton et al., 2015, Alec Radford, 2016, Im et al., 2016].\nIn this work, we investigate the feasibility of using adversarial training for a sequential model with continuous data, and evaluate it using classical music in freely available midi files.\nRecurrent neural networks (RNNs) are often used to model sequences of data. These models are usually trained using a maximum likelihood criterion. E.g. for language modelling, they are trained to predict the next token at any point in the sequence, i.e. to model the conditional probability of the next token given the sequence of preceding tokens. By sampling from this conditional distribution, one can generate reasonably realistic sequences [Graves, 2013]. The sampling is non-trivial and you usually resort to beam-search to generate good sequences in tasks such as machine translation [Sutskever et al., 2014].\nRNNs have been used to model music [Eck and Schmidhuber, 2002, Nicolas Boulanger-Lewandowski, 2012, Yu et al., 2016], but to our knowledge they always use a symbolic representation. In contrast, our work demonstrates how one can train a highly flexible and expressive model with fully continuous sequence data for tone lengths, frequencies, intensities, and timing.\nWe propose a recurrent neural network architecture, C-RNN-GAN (Continuous RNN-GAN), that is trained with adversarial training to model the whole joint probability of a sequence, and to be able to generate sequences of data. Our system is demonstrated by training it on sequences of classical music in midi-format, and evaluated using metrics such as scale consistency and tone range.\nWe conclude that generative adversarial training is a viable way of training networks that model a distribution over sequences of continuous data, and see potential for also modelling many other types of sequential continuous data.\nConstructive Machine Learning Workshop (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n61 1.\n09 90\n4v 1\n[ cs\n.A I]\n2 9\nN ov\nWork using RNNs for music generation includes [Eck and Schmidhuber, 2002], modelling blues songs with 25 discrete tone values, and [Nicolas Boulanger-Lewandowski, 2012], combining the RNN with restricted Boltzmann machines, representing 88 distinct tones. Yu et al. [2016] trained an RNN with adversarial training, applying policy gradient methods to cope with the discrete nature of the symbolic representation they employed. In contrast to this, our work represents tones using real valued continuous quadruplets of frequency, length, intensity, and timing. This allows us to use standard backpropagation to train the whole model end-to-end. Im et al. [2016] presented a recurrent model with adversarial training to generate images. The LAPGAN model [Denton et al., 2015] is another sequential model with adversarial training, that generates images in a coarse-to-fine fashion."
    }, {
      "heading" : "2 C-RNN-GAN: A continuous recurrent network with adversarial training",
      "text" : "The proposed model is a recurrent neural network with adversarial training. The adversaries are two different deep recurrent neural models, a generator (G) and a discriminator (D). The generator is trained to generate data that is indistinguishable from real data, while the discriminator is trained to identify the generated data. The training becomes a zero-sum game for which the Nash equilibrium is when the generator produces data that the discriminator cannot tell from real data. We define the following loss functions LD and LG:\nLG = 1\nm m∑ i=1 log(1−D(G(z(i))))\nLD = 1\nm m∑ i=1 [ − logD(x(i))− (log(1−D(G(z(i))))) ]\n(where z(i) is a sequence of uniform random vectors in [0, 1]k, and x(i) is a sequence from the training data. k is the dimensionality of the data in the random sequence.)\nThe input to each cell in G is a random vector, concatenated with the output of previous cell. Feeding the output from the previous cell is common practice when training RNNs as language models [Mikolov et al., 2010], and has also been used in music composition [Eck and Schmidhuber, 2002].\nThe discriminator consists of a bidirectional recurrent net, allowing it to take context in both directions into account for its decisions. In this work, the recurrent network used is the Long short-term memory (LSTM) [Schmidhuber and Hochreiter, 1997]. It has an internal structure with gates that help with the vanishing gradient problem, and to learn longer dependencies [Hochreiter, 1998, Bengio et al., 1994]."
    }, {
      "heading" : "2.1 Modelling music",
      "text" : "In this work, we set out to evaluate the viability of using generative adversarial models to learn the generating distribution behind classical music. Inspired by the venerable MIDI format for communicating signals between digital musical instruments, we model the signal with four realvalued scalars at every data point: tone length, frequency, intensity, and time spent since the previous tone. Modelling the data in this way allows the network to represent polyphonous chords (with zero time between two tones). To evaluate its effect on polyphony, we have also experimented with having up to three tones represented as output from each LSTM cell in G (with corresponding modifications to D). Each tone is then represented with its own quadruplet of values as described above. We refer to this version as C-RNN-GAN-3. Similarly to the MIDI format, the absence of a tone is represented by zero intensity output."
    }, {
      "heading" : "3 Experimental setup",
      "text" : "Model layout details: The LSTM network in both G and D has depth 2, each LSTM cell has 350 internal (hidden) units. D has a bidirectional layout, while G is unidirectional. The output from each LSTM cell in D are fed into a fully connected layer with weights shared across time steps, and one sigmoid output per cell is then averaged to the final decision for the sequence.\nBaseline: Our baseline is a recurrent network similar to our generator, but trained entirely to predict the next tone event at each point in the recurrence.\nDataset: Training data was collected from the web in the form of music files in midi format, containing well-known works of classical music. Each midi event of the type “note on” was loaded and saved together with its duration, tone, intensity (velocity), and time since beginning of last tone. The tone data is internally represented with the corresponding sound frequency. Internally, all data is normalized to a tick resolution of 384 per quarter note. The data contains 3697 midi files from 160 different composers of classical music.\nThe source code is available on Github1, including a utility to download all data used in this study from different websites.\nTraining: Backpropagation through time (BPTT) and mini-batch stochastic gradient descent was used. Learning rate was set to 0.1, and we apply L2 regularization to the weights both in G and D. The model was pretrained for 6 epochs with a squared error loss for predicting the next event in the training sequence. Just as in the adversarial setting, the input to each LSTM cell is a random vector v, concatenated with the output at previous time step. v is uniformly distributed in [0, 1]k, and k was chosen to be the number of features in each tone, 4. During pretraining, we used a schema for sequence length, beginning with short sequences, randomly sampled from the training data, eventually training the model with increasingly long sequences. We consider this a form of curriculum learning, where we begin by learning short passages, and the relations between points that are near in time.\nDuring adversarial training, we noticed that D can become too strong, resulting in a gradient that cannot be used to improve G. This effect is particularly clear when the network is initialized without pretraining. For this reason, we apply freezing [Salimans et al., 2016], which means stopping the updates of D whenever its training loss is less than 70% of the training loss of G. We do the corresponding thing when G become too strong.\nWe also employ feature matching [Salimans et al., 2016], an approach to encourage greater variance in G, and avoid overfitting to the current discriminator by replacing the standard generator loss, LG. Normally, the objective for the generator is to maximize the error that the discriminator makes, but with feature matching, the objective is instead to produce an internal representation at some level in the discriminator that matches that of real data. We choose the representations R from the last layer before the final logistic classification layer in D, and define the new objective L̂G for G.\nL̂G = 1\nm m∑ i=1 (R(x(i))−R(G(z(i))))2\n1https://github.com/olofmogren/c-rnn-gan"
    }, {
      "heading" : "3.1 Evaluation",
      "text" : "Evaluation of C-RNN-GAN was done using a number of measurements on generated output.\nPolyphony, measuring how often (at least) two tones are played simultaneously (their start time is exactly the same). Note that this is a rather restricted metric, as it can give a low score to music that has simultaneous tones that does not start at exactly the same time.\nScale consistency were computed by counting the fraction of tones that were part of a standard scale, and reporting the number for the best matching such scale.\nRepetitions of short subsequences were counted, giving a score on how much recurrence there is in a sample. This metric takes only the tones and their order into account, not their timing.\nTone span is the number of half-tone steps between the lowest and the highest tone in a sample.\nA tool was implemented to compute these estimates, which is available on Github2 together with all source code used in this work."
    }, {
      "heading" : "4 Results",
      "text" : "The results of the experimental study is presented in Figure 3. Adversarial training helps the model learn patterns with more variability, larger tone span, and larger intensity span. Allowing the model to output more than one tone per LSTM cell helps to generate music with a higher polyphony score."
    }, {
      "heading" : "4.1 Listening impressions",
      "text" : "While we have not yet performed a thorough listening study of this work, the impressions of the author and co-workers is that the model trained with feature matching gets a better trade-off between structure and surprise than the other variants. The versions with only maximum-likelihood pretraining tends to not give enough surprise, and the versions with mini-batch features tended to sample music with too little structure to be interesting to a real listener. This is a bit surprising and something that we intend to look more into.\nMusic files generated with C-RNN-GAN can be downloaded from http://mogren.one/publications/2016/c-rnn-gan/."
    }, {
      "heading" : "5 Discussion and conclusions",
      "text" : "In this paper, we have proposed a recurrent neural model for continuous data, trained using an approach based on generative adversarial networks. While more experimentation needs to be done, we believe that the results are promising. We have noted that adversarial training helps the recurrent neural network generate music that varies more in both the number of tones used, and the span of intensities of played tones. The generated music can not yet compare to the music in the training data, by human judgement. The reasons for this remain to be explored. However, in the evaluation (see Figure 3) one can see that the scores of music generated using C-RNN-GAN show more resemblance to scores of real music, than do those of music generated using the baseline. The generated music is polyphonous, but in the polyphony score in our experimental evaluation, measuring how often two tones are played at exactly the same time, C-RNN-GAN scored low. Allowing each LSTM cell to output up to three tones at the same time resulted in a model that scored much better with polyphony. One can hear in the generated samples, that while timing can vary quite a bit from sample to sample, it is generally rather consistent within one track, giving a feeling of tempo in the generated songs."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work has been done within the project “Data-driven secure business intelligence”, grant IIS110089 from the Swedish Foundation for Strategic Research (SSF).\n2https://github.com/olofmogren/c-rnn-gan"
    } ],
    "references" : [ {
      "title" : "Unsupervised representation learning with deep convolutional generative adversarial networks",
      "author" : [ "Soumith Chintala Alec Radford", "Luke Metz" ],
      "venue" : "In International Conference on Learning Representations,",
      "citeRegEx" : "Radford and Metz.,? \\Q2016\\E",
      "shortCiteRegEx" : "Radford and Metz.",
      "year" : 2016
    }, {
      "title" : "Learning long-term dependencies with gradient descent is difficult",
      "author" : [ "Yoshua Bengio", "Patrice Simard", "Paolo Frasconi" ],
      "venue" : "Neural Networks, IEEE Transactions on,",
      "citeRegEx" : "Bengio et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 1994
    }, {
      "title" : "Deep generative image models using a laplacian pyramid of adversarial networks",
      "author" : [ "Emily L Denton", "Soumith Chintala", "Rob Fergus" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Denton et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Denton et al\\.",
      "year" : 2015
    }, {
      "title" : "Finding temporal structure in music: Blues improvisation with lstm recurrent networks",
      "author" : [ "Douglas Eck", "Juergen Schmidhuber" ],
      "venue" : "In Neural Networks for Signal Processing,",
      "citeRegEx" : "Eck and Schmidhuber.,? \\Q2002\\E",
      "shortCiteRegEx" : "Eck and Schmidhuber.",
      "year" : 2002
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Generating sequences with recurrent neural networks",
      "author" : [ "Alex Graves" ],
      "venue" : "arXiv preprint arXiv:1308.0850,",
      "citeRegEx" : "Graves.,? \\Q2013\\E",
      "shortCiteRegEx" : "Graves.",
      "year" : 2013
    }, {
      "title" : "The vanishing gradient problem during learning recurrent neural nets and problem solutions",
      "author" : [ "Sepp Hochreiter" ],
      "venue" : "International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems,",
      "citeRegEx" : "Hochreiter.,? \\Q1998\\E",
      "shortCiteRegEx" : "Hochreiter.",
      "year" : 1998
    }, {
      "title" : "Generating images with recurrent adversarial networks",
      "author" : [ "Daniel Jiwoong Im", "Chris Dongjoo Kim", "Hui Jiang", "Roland Memisevic" ],
      "venue" : "arXiv preprint arXiv:1602.05110,",
      "citeRegEx" : "Im et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Im et al\\.",
      "year" : 2016
    }, {
      "title" : "Recurrent neural network based language model",
      "author" : [ "Tomas Mikolov", "Martin Karafiát", "Lukas Burget", "Jan Cernockỳ", "Sanjeev Khudanpur" ],
      "venue" : "In Interspeech,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2010
    }, {
      "title" : "Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription",
      "author" : [ "Yoshua Bengio" ],
      "venue" : "In Proceedings of the 29th International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Boulanger.Lewandowski and Bengio.,? \\Q2012\\E",
      "shortCiteRegEx" : "Boulanger.Lewandowski and Bengio.",
      "year" : 2012
    }, {
      "title" : "Improved techniques for training gans",
      "author" : [ "Tim Salimans", "Ian Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Salimans et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Salimans et al\\.",
      "year" : 2016
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Jürgen Schmidhuber", "Sepp Hochreiter" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Schmidhuber and Hochreiter.,? \\Q1997\\E",
      "shortCiteRegEx" : "Schmidhuber and Hochreiter.",
      "year" : 1997
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Sutskever et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Seqgan: Sequence generative adversarial nets with policy gradient",
      "author" : [ "Lantao Yu", "Weinan Zhang", "Jun Wang", "Yong Yu" ],
      "venue" : "arXiv preprint arXiv:1609.05473,",
      "citeRegEx" : "Yu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Generative adversarial networks (GANs) are a class of neural network architectures designed with the aim of generating realistic data [Goodfellow et al., 2014].",
      "startOffset" : 134,
      "endOffset" : 159
    }, {
      "referenceID" : 5,
      "context" : "By sampling from this conditional distribution, one can generate reasonably realistic sequences [Graves, 2013].",
      "startOffset" : 96,
      "endOffset" : 110
    }, {
      "referenceID" : 12,
      "context" : "The sampling is non-trivial and you usually resort to beam-search to generate good sequences in tasks such as machine translation [Sutskever et al., 2014].",
      "startOffset" : 130,
      "endOffset" : 154
    }, {
      "referenceID" : 3,
      "context" : "Work using RNNs for music generation includes [Eck and Schmidhuber, 2002], modelling blues songs with 25 discrete tone values, and [Nicolas Boulanger-Lewandowski, 2012], combining the RNN with restricted Boltzmann machines, representing 88 distinct tones.",
      "startOffset" : 46,
      "endOffset" : 73
    }, {
      "referenceID" : 2,
      "context" : "The LAPGAN model [Denton et al., 2015] is another sequential model with adversarial training, that generates images in a coarse-to-fine fashion.",
      "startOffset" : 17,
      "endOffset" : 38
    }, {
      "referenceID" : 2,
      "context" : "Work using RNNs for music generation includes [Eck and Schmidhuber, 2002], modelling blues songs with 25 discrete tone values, and [Nicolas Boulanger-Lewandowski, 2012], combining the RNN with restricted Boltzmann machines, representing 88 distinct tones. Yu et al. [2016] trained an RNN with adversarial training, applying policy gradient methods to cope with the discrete nature of the symbolic representation they employed.",
      "startOffset" : 47,
      "endOffset" : 273
    }, {
      "referenceID" : 2,
      "context" : "Work using RNNs for music generation includes [Eck and Schmidhuber, 2002], modelling blues songs with 25 discrete tone values, and [Nicolas Boulanger-Lewandowski, 2012], combining the RNN with restricted Boltzmann machines, representing 88 distinct tones. Yu et al. [2016] trained an RNN with adversarial training, applying policy gradient methods to cope with the discrete nature of the symbolic representation they employed. In contrast to this, our work represents tones using real valued continuous quadruplets of frequency, length, intensity, and timing. This allows us to use standard backpropagation to train the whole model end-to-end. Im et al. [2016] presented a recurrent model with adversarial training to generate images.",
      "startOffset" : 47,
      "endOffset" : 661
    }, {
      "referenceID" : 8,
      "context" : "Feeding the output from the previous cell is common practice when training RNNs as language models [Mikolov et al., 2010], and has also been used in music composition [Eck and Schmidhuber, 2002].",
      "startOffset" : 99,
      "endOffset" : 121
    }, {
      "referenceID" : 3,
      "context" : ", 2010], and has also been used in music composition [Eck and Schmidhuber, 2002].",
      "startOffset" : 53,
      "endOffset" : 80
    }, {
      "referenceID" : 11,
      "context" : "In this work, the recurrent network used is the Long short-term memory (LSTM) [Schmidhuber and Hochreiter, 1997].",
      "startOffset" : 78,
      "endOffset" : 112
    }, {
      "referenceID" : 10,
      "context" : "For this reason, we apply freezing [Salimans et al., 2016], which means stopping the updates of D whenever its training loss is less than 70% of the training loss of G.",
      "startOffset" : 35,
      "endOffset" : 58
    }, {
      "referenceID" : 10,
      "context" : "We also employ feature matching [Salimans et al., 2016], an approach to encourage greater variance in G, and avoid overfitting to the current discriminator by replacing the standard generator loss, LG.",
      "startOffset" : 32,
      "endOffset" : 55
    } ],
    "year" : 2016,
    "abstractText" : "Generative adversarial networks have been proposed as a way of efficiently training deep generative neural networks. We propose a generative adversarial model that works on continuous sequential data, and apply it by training it on a collection of classical music. We conclude that it generates music that sounds better and better as the model is trained, report statistics on generated music, and let the reader judge the quality by downloading the generated songs.",
    "creator" : "LaTeX with hyperref package"
  }
}