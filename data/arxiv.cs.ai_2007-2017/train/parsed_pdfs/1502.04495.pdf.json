{
  "name" : "1502.04495.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Generalization of Gustafson-Kessel Algorithm Using a New Constraint Parameter",
    "authors" : [ "Vasile Patrascu" ],
    "emails" : [ "vpatrascu@tarom.ro,", "vpatrascu@caramail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "In this paper one presents a new fuzzy clustering algorithm based on a dissimilarity function determined by three parameters. This algorithm can be considered a generalization of the Gustafson-Kessel algorithm for fuzzy clustering.\nKeywords: fuzzy clustering, GustafsonKessel algorithm, dissimilarity function, cluster density, cluster volume.\n1 Introduction\nThe Gustafson-Kessel [4] algorithm is an important algorithm for fuzzy clustering. Although there has been developed a more efficient algorithm (GathGeva [3]), the Gustafson-Kessel algorithm remains the most utilized, because it does not need the utilization of the exponential function [3]. One of the limits of the Gustafson-Kessel is the fact that it supplies unsatisfactory results when clusters having very different volumes are to be separated [5], [6], and [7].\nIn this paper one presents an algorithm that can eliminate this insufficiency. The algorithm presented in this paper can be considered as a generalization of the Gustafson-Kessel algorithm.\nFurther the paper has the following structure: section 2 does a short presentation of the GustafsonKessel algorithm; section 3 presents the new algorithm; section 4 presents experimental results; section 5 presents some conclusions.\n2 The Gustafson-Kessel Algorithm\nThe Gustafson-Kessel algorithm was proposed in [4] as an improvement of the fuzzy C-means clustering algorithm [1, 2]. Let there be the objective function:\n)()(),( 11 ijij\nc\nj\nN\ni xDwMWJ α == ∑∑= (1)\nwhere { } ,1[| NiRxX ki ∈∈= ] is a set containing unlabelled vectors; N\n{ } cjRm M kj ],1[| ∈∈= is the set of centers of clusters; is the ][ ijwW = cN × fuzzy c-partition matrix, containing the membership values of all in all clusters; is a dissimilarity measure between the vector and the center of a specific cluster defined by: ix )( ij xD ix\njm j\n( ) 2 1\n)det()( ijkjjij dCxD ⋅⋅= λ (2)\nwhere is the fuzzy covariance matrix of the cluster defined by: jC j\n( ) ( )\nα\nα\n)(\n)(\n1\n1\nij\nN\ni\nN\ni\nT jijiij\nj\nw\nmxmxw C\n∑\n∑\n=\n= −⋅−⋅\n= (3)\nijd is the Mahalanobis distance\n)()( 12 jij T jiij mxCmxd −⋅⋅−= − (4)\nand cλλλ ,...,, 21 are positive constants. Usually c 1=jλ ; ),1( ∞∈α is a control parameter of\nfuzziness. Usually 2=α . The clustering problem can be defined as the minimization of under the following constraint: J\n1 1 =∑ =\nc\nj ijw (5)\nThe Gustafson-Kessel algorithm consists in the iteration of the following formulae:\nα\nα\n)(\n)(\n1\n1\nij\nN\ni\niij\nN\ni j\nw\nxw m\n∑\n∑\n=\n= ⋅\n= (6)\nand\n∑ =\n−\n⎟⎟ ⎠ ⎞ ⎜⎜ ⎝ ⎛\n= c\nt it\nij\nij\nxD xD\nw\n1\n1 1\n)( )(\n1\nα\n(7)\nThe major drawback is that Gustafson-Kessel algorithm is restricted to constant volume clusters due to the fixed a priori values jλ .\n3 The New Fuzzy Clustering Algorithm\n3.1 The 3-parameter Dissimilarity Function\nLet there be the vector set where and let c be the number of clusters that\nmust be obtained. The cluster will be described by the parameters: the cluster center, the fuzzy covariance matrix, a parameter that shows how large or how small is the cluster in comparison to the other clusters, the fuzzy membership degree of the element . One denotes:\n{ }NxxxX ,...,, 21= k\ni Rx ∈ j\njm jC\njf j\nijw\nix\n∑ =\nα= N\ni ijj wn 1 )( (8)\n)det( jj CV = (9)\nj\nj j V\nn =ρ (10)\nj is very close to the cluster cardinality, can be considered as a measure for cluster volume and n jV\njρ a measure for cluster density. In the framework of this algorithm the dissimilarity function of the element\nto the cluster is defined by the relation: ix j\n2\n2 k⎞\nij j ij df D ⋅⎟ ⎟ ⎠ ⎜ ⎜ ⎝\n= (11) j V⎛\nwhere the distance is defined by relation (4).\nolve the problem there will be used the following objective function:\nNc\nijd\nIn order to s\n)()(),,( 1 ij i ij j xDwFMWJ ∑∑ ==\n⋅= α (12)\nwhere\n1\n{ } cjf F j ],1[|)1,0( ∈∈= and M , W have by the Gustafson-Kess\nrs ve constraint:\n=j (13)\nIn addition here ar constraints for the membership degrees\nthe same definition used el algorithm. The paramete rify the following jf\n1=∑ j c\nf 1\nto this, t e the well-known ijw .\nNi ,...,2,1=∀ 1 1 =∑ =\nter , due\ning formulae that are similar to those\nO ective Function J\nIn this section we will prove the following equivalent form for the objective function\nc\nj ijw (14)\nIn this paper there will be presented only the determination of formulae for the parame jf to the fact that the parameters jm and ijw are computed us used by the Gustafson-Kessel algorithm.\n3.2 An Equivalent Form for the bj\nJ .\nj kj c nk V J ⋅⋅⎟ ⎞ ⎜ ⎛ = ∑ 2\n(15) jj f ⎟⎠⎜⎝=1\nThis form will be used in the s this it is necessary to demonstrate the following\n=1\nection 3.3. To prove\nrelation:\njijij N nkdw ⋅=⋅∑ 2)( α (16) i\nLet there be the or diagonalizes the covariance\nthe eigenvalues of the ma .\n⋅⋅= −− 11 (18)\njj CHS ⋅⋅= (19)\nis the following diagonal matrix:\njH thogonal matrix that matrix jC and\n22 2 2 1 ,...,, jkjj σσσ trix jC\nThere will be: T jj HH = −1 (17)\njj T jj HSHC\nT jj H\nwhere jS\n( )22221 ,...,, jkjjj diagS σσσ=\n(20)\nDenoting:\niji xHy ⋅= (21)\njjj mH ⋅=µ (22)\nit results:\n)( jijji mxHy −⋅=− µ (23)\nFrom (4), (18) and (23) it results:\n(24)\nor\n)()( 12 jij T jiij ySyd µµ −⋅⋅−= −\n( ) 2 2\n1 jtt σ 2 jtit\nk ij y d µ−\n= ∑ =\n(25)\nFrom (19) and (3) it results:\nα\nα\n)(\n))(()(\n1i=\n1\nij\nN\nN\ni\nT j T jijijij\nj\nw\nHmxmxHw S\n∑\n∑ = −−⋅ = (26)\nUsing (23) the relation (26) becomes:\nα\nα µµ\n)(\n)Tj()()( 1\nij\nN\nN\ni ijiij\nj\nw\nyyw S\n∑\n∑ = −⋅−⋅ = (27)\nFrom (27) and (20) it results for\n1i=\nkt ,...,2,1=\nα\nα µ σ\n)(\n)()(\n1\n2\n12\nij\nN\ni\njtitij N w∑\ni jt\nw\ny\n∑ =\n= −⋅\n= (28)\nUsing (8) relation (28) becomes:\nj jt n=2σ (29)\njtitij\nN\ni yw −⋅∑ =\n2 1 )()( µα\nFrom (25) it results:\n∑ ∑∑ == ⎟\n⎟ ⎞\n⎜ ⎜ ⎝\n⎛ −⋅ ⋅ k\nt\nN jtitij N\ni ij\nyw dw\n1 2\n2\n1\n2 )()()( σ µαα (30) = ⎠ = i jt ij 1\nUsing (29) relation (30) becomes:\nt demonstrated.\n3.3 The Calculus of the Parameters f\nIn this section it i parameters . There will be considered the\nj\nN\ni ijij nkdw ⋅=⋅∑ =1\n2)( α (31)\nnamely he relation (16) that might be\ns presented the determination of the jf\nobjective function:\n⎟ ⎟ ⎠ ⎞ ⎜ ⎜ ⎝ ⎛ −+⋅⋅⎟ ⎟ ⎠ ⎞ ⎜ ⎜ ⎝ ⎛ = ∑∑ == 1 1\n2\n1 j\nc\nj j\nk\nj\nj c\nj fnk f V J ω (32)\nwhere ω is the Lagrange multiplier.\nWe must solve the equation:\n0= ∂ ∂ jf J\n2) and (33) it results:\n(33)\nFrom (3\n012 2\n21 =+⋅⋅⋅− + ωkjj\nk j\nVkn\nf k\n(34)\nFrom (34) it results:\n( ) 2 1\n222 ++ ⋅⋅⎟ ⎠ ⎞ ⎜ ⎝ ⎛= kj k j\nk k\nj Vnf ω (35)\naking into account the relations (13) and (35) it results: T\n( ) 2 1 2\n1\n2 12 + =⎟ ⎞ ⎜ ⎛ k\nk\n+\n= ⋅\n⎠⎝ ∑ ktkt c\nt Vn\nω (36)\nand\n( )\n( ) 2 1 2 +\n= k c jf\n1= ⋅∑ ktt t Vn\nFrom (10) and (37) it results the follo equivalent formula f\n2 1\n2 +⋅ kj k j Vn\n(37)\nwing or the parameter : jf\ntt\nc\nt\njj j\nV\nV f\nk k\nk k\n⋅\n⋅ =\n+\n+\n∑ =\n2\n2\n1 ρ\nρ (38)\nTh\nalgorithm, namely:\n3.4 The Calculus of the Parameters m\ne calculus formulae for the cluster centers jm are similar to those used by the Gustafson-Kessel\nα\nα\n)(\n)(\n1i ∑ =\n1\nij\nN\niij\nN\ni j\nw\nxw m ∑ = ⋅ = (39)\n3.5 The Calculus of the Parameter w\nThe calculus formulae for the membership degrees the Gustafson-\ns\nijw are similar to those used by Kessel algorithm, namely:\n∑ =\n−α\n⎟ ⎟ ⎠ ⎞ ⎜ ⎜ ⎝ ⎛\n= c\nt it\nij\nij\nxD\nxD w\n1\n1 1\n)(\n)(\n1 (40)\nBut from (38) it results:\nt k\nk\nj\nt c\ntj\nj V f V ⋅⎟\n⎟ ⎠ ⎞ ⎜ ⎜ ⎝ ⎛ = + = ∑ 2 1 ρ ρ\n2\n2\n1\n2 ij\nk c\nt t\nk k\nj\nt ij dVD ⋅ ⎟⎟ ⎟ ⎟\n⎠\n⎞\n⎜⎜ ⎜ ⎜ ⎝\n⎛\n⋅⎟ ⎟ ⎠ ⎞ ⎜ ⎜ ⎝ ⎛ = ∑ = + ρ ρ (42)\nFrom (40) and (42) one can obtain the following equivalent formulae for ijw .\n1\n1\n2 1\n12 2 −\n= ⎟ ⎟ ⎟\n⎠\n⎞\n⎜⎜ ⎜ ⎝\n⎛ ⋅\n⎠⎝=\n+∑ α\nρ it t\nc\nt\nij ij\nd\nw\nk\n(43)\n4 Experimental Results\nOne presents the using of the new algorithm for the two test sets shown in the figures 1 and 5. The first set was obtained by a random generation of some points inside two ellipses (figure 1). The second set was obtained by a random generating of some points inside three ellipses (figure 5). The obtained results using the new algorithm can be seen in the figures 3 and 7 respectively, those obtained using the Gustafson-Kessel algorithm in the figures 2 and 6, and those obtained using Gath-Geva alg\n1 1\n2\n2\n⎟⎜ d\norithm in figures 4 and 8. One can see that while the lgorithm has generated clusters having approximately the same size ( figures 2 and\non-Kessel algorithm. There has been used a dissimilarity function having three parameters and a new constraint. In this way there it was eliminated one of the main insufficiency of the Gustafson-Kessel algorithm regarding to the obtaining of some clusters having very different sizes. The experimental results stand as proof of the new algorithm superiority in comparison to the Gustafson-Kessel one.\n12 − ⎟ ⎟ ⎞ ⎜ ⎜ ⎛ ⋅ + α ρ j k\nGustafson-Kessel a\n6), the new algorithm (figures 3 and 7) has generated clusters that are very close to the ideal clustering (figures 1 and 5). The results obtained using the proposed algorithm are very close to those obtained using the Gath-Geva algorithm.\n5 Conclusions\nThis paper has presented an algorithm that can be considered a generalization of Gustafs\n(41)\nUsing (41) relation (11) becomes:\nclustering.\nclustering.\nReferences\n[1] J. C. Bezdek, Fuzzy mathematics in pattern classification, Ph.D. dissertation, Cornell Univ., Ithaca, NY, 1973. [2] J. C. Bezdek, Pattern recognition with fuzzy objective function algorithms, New York: Plenum Press, 1981. [3] I. I. Gath and A. B. Geva, “Unsupervised optimal fuzzy clustering”, IEEE Trans. Pattern And Machine Intell., Vol. 2, no. 7, pp. 773-781, 1989. [4] E. E. Gustafson and W. C. Kessel, “Fuzzy clustering with a fuzzy covariance matrix”, Proc. IEEE CDC, San-Diego, CA, pp. 761-766, 1979.\n[5] R. A. Keller and F. Klawonn, “Adaptation of\ncluster sizes in objective function based fuzzy clustering”, In T.Leondes, editor. Intelligent Systems: Techniques and Applications – Database and Learning Systems, vol. 4, pp. 181-191. CRC Press, 2002. [6] R. Krishnapuram and J. Kim, “Clustering algorithms based on volume criteria”. IEEE Transactions on Fuzzy Systems, 8(2), pp. 228- 236, 2000. [7] M. Setnes and U. Kaymak, “Extended fuzzy cmeans with volume prototypes and cluster merging”, In Proc. Of 6th European Congress on Intelligent Techniques and Soft Computing, pp. 1360-1364, 1998."
    } ],
    "references" : [ {
      "title" : "Fuzzy mathematics in pattern classification, Ph.D",
      "author" : [ "J.C. Bezdek" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1973
    }, {
      "title" : "Pattern recognition with fuzzy objective function algorithms",
      "author" : [ "J.C. Bezdek" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1981
    }, {
      "title" : "Unsupervised optimal fuzzy clustering",
      "author" : [ "I.I. Gath", "A.B. Geva" ],
      "venue" : "IEEE Trans. Pattern And Machine Intell.,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1989
    }, {
      "title" : "Fuzzy clustering with a fuzzy covariance matrix",
      "author" : [ "E.E. Gustafson", "W.C. Kessel" ],
      "venue" : "Proc. IEEE CDC, San-Diego,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1979
    }, {
      "title" : "Adaptation of cluster sizes in objective function based fuzzy clustering",
      "author" : [ "R.A. Keller", "F. Klawonn" ],
      "venue" : "In T.Leondes, editor. Intelligent Systems: Techniques and Applications – Database and Learning Systems,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2002
    }, {
      "title" : "Clustering algorithms based on volume criteria",
      "author" : [ "R. Krishnapuram", "J. Kim" ],
      "venue" : "IEEE Transactions on Fuzzy Systems,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2000
    }, {
      "title" : "Extended fuzzy cmeans with volume prototypes and cluster merging",
      "author" : [ "M. Setnes", "U. Kaymak" ],
      "venue" : "In Proc. Of 6 European Congress on Intelligent Techniques and Soft Computing,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1998
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "1 Introduction The Gustafson-Kessel [4] algorithm is an important algorithm for fuzzy clustering.",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 2,
      "context" : "Although there has been developed a more efficient algorithm (GathGeva [3]), the Gustafson-Kessel algorithm remains the most utilized, because it does not need the utilization of the exponential function [3].",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 2,
      "context" : "Although there has been developed a more efficient algorithm (GathGeva [3]), the Gustafson-Kessel algorithm remains the most utilized, because it does not need the utilization of the exponential function [3].",
      "startOffset" : 204,
      "endOffset" : 207
    }, {
      "referenceID" : 4,
      "context" : "One of the limits of the Gustafson-Kessel is the fact that it supplies unsatisfactory results when clusters having very different volumes are to be separated [5], [6], and [7].",
      "startOffset" : 158,
      "endOffset" : 161
    }, {
      "referenceID" : 5,
      "context" : "One of the limits of the Gustafson-Kessel is the fact that it supplies unsatisfactory results when clusters having very different volumes are to be separated [5], [6], and [7].",
      "startOffset" : 163,
      "endOffset" : 166
    }, {
      "referenceID" : 6,
      "context" : "One of the limits of the Gustafson-Kessel is the fact that it supplies unsatisfactory results when clusters having very different volumes are to be separated [5], [6], and [7].",
      "startOffset" : 172,
      "endOffset" : 175
    }, {
      "referenceID" : 3,
      "context" : "2 The Gustafson-Kessel Algorithm The Gustafson-Kessel algorithm was proposed in [4] as an improvement of the fuzzy C-means clustering algorithm [1, 2].",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 0,
      "context" : "2 The Gustafson-Kessel Algorithm The Gustafson-Kessel algorithm was proposed in [4] as an improvement of the fuzzy C-means clustering algorithm [1, 2].",
      "startOffset" : 144,
      "endOffset" : 150
    }, {
      "referenceID" : 1,
      "context" : "2 The Gustafson-Kessel Algorithm The Gustafson-Kessel algorithm was proposed in [4] as an improvement of the fuzzy C-means clustering algorithm [1, 2].",
      "startOffset" : 144,
      "endOffset" : 150
    } ],
    "year" : 2005,
    "abstractText" : "In this paper one presents a new fuzzy clustering algorithm based on a dissimilarity function determined by three parameters. This algorithm can be considered a generalization of the Gustafson-Kessel algorithm for fuzzy clustering.",
    "creator" : "Acrobat PDFMaker 6.0 for Word"
  }
}