{
  "name" : "1207.5536.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "MCTS Based on Simple Regret",
    "authors" : [ "David Tolpin", "Solomon Eyal Shimony" ],
    "emails" : [ "tolpin@cs.bgu.ac.il", "shimony@cs.bgu.ac.il" ],
    "sections" : [ {
      "heading" : "Introduction",
      "text" : "Monte-Carlo tree search, and especially a version based on the UCT formula (Kocsis and Szepesvári 2006) appears in numerous search applications, such as (Gelly and Wang 2006; Eyerich, Keller, and Helmert 2010). Although these methods are shown to be successful empirically, most authors appear to be using the UCT formula “because it has been shown to be successful in the past”, and “because it does a good job of trading off exploration and exploitation”. While the latter statement may be correct for the multi-armed bandit and for the UCB method (Auer, CesaBianchi, and Fischer 2002), we argue that it is inappropriate for search. The problem is not that UCT does not work; rather, a simple reconsideration from basic principles can result in schemes that outperform UCT.\nThe core issue is that in adversarial search and search in “games against nature” — optimizing behavior under uncertainty, the goal is typically to either find a good (or optimal) strategy, or even just to find the best first action of\nCopyright c© 2012, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nsuch a policy. Once such an action is discovered, it is usually not beneficial to further sample that action, “exploitation” is thus meaningless for search problems. Finding a good first action is closer to the pure exploration variant, as seen in the selection problem (Bubeck, Munos, and Stoltz 2011; Tolpin and Shimony 2012). In the selection problem, it is much better to minimize the simple regret. However, the simple and the cumulative regret cannot be minimized simultaneously; moreover, (Bubeck, Munos, and Stoltz 2011) shows that in many cases the smaller the cumulative regret, the greater the simple regret.\nWe begin with background definitions and related work. Some sampling schemes are introduced, and shown to have better bounds for the simple regret on sets than UCB, the first contribution of this paper. The results are applied to sampling in trees by combining the proposed sampling schemes on the first step of a rollout with UCT for the rest of the rollout. An additional sampling scheme based on metareasoning principles is also suggested, another contribution of this paper. Finally, the performance of the proposed sampling schemes is evaluated on sets of Bernoulli arms, in randomly generated 2-level trees, and on the sailing domain, showing where the proposed schemes have improved performance."
    }, {
      "heading" : "Background and Related Work",
      "text" : "Monte-Carlo tree search was initially suggested as a scheme for finding approximately optimal policies for Markov Decision Processes (MDP). An MDP is defined by the set of states S, the set of actions A (also called moves in this paper), the transition distribution T (s, a, s′), the reward function R(s, a, s′), the initial state s and an optional goal state t: (S,A, T,R, s, t) (Russell and Norvig 2003). Several MCTS schemes explore an MDP by performing rollouts— trajectories from the current state to a state in which a termination condition is satisfied (either the goal state, or a cutoff state for which the reward is evaluated approximately)."
    }, {
      "heading" : "Multi-armed bandits and UCT",
      "text" : "In the Multi-armed Bandit problem (Vermorel and Mohri 2005) we have a set of K arms (see Figure 1.a). Each arm can be pulled multiple times. Sometimes a cost is associated with each pulling action. When the ith arm is pulled,\nar X\niv :1\n20 7.\n55 36\nv1 [\ncs .A\nI] 2\n3 Ju\nl 2 01\n2\na random reward Xi from an unknown stationary distribution is encountered. The reward is usually bounded between 0 and 1. In the cumulative setting (the focus of much of the research literature on Multi-armed bandits), all encountered rewards are collected by the agent. The UCB scheme was shown to be near-optimal in this respect (Auer, CesaBianchi, and Fischer 2002): Definition 1. Scheme UCB(c) pulls arm i that maximizes upper confidence bound bi on the reward:\nbi = Xi +\n√ c log(n)\nni (1)\nwhere Xi is the average sample reward obtained from arm i, ni is the number of times arm i was pulled, and n is the total number of pulls so far.\nThe UCT algorithm, an extension of UCB to Monte-Carlo Tree Search is described in (Kocsis and Szepesvári 2006), and shown to outperform many state of the art search algorithms in both MDP and adversarial games (Eyerich, Keller, and Helmert 2010; Gelly and Wang 2006).\nIn the simple regret (selection) setting, the agent gets to collect only the reward of the last pull. Definition 2. The simple regret Er of a sampling policy for the Multi-armed Bandit Problem is the expected difference between the best true expected reward µ∗ and the true expected reward µj of the arm with the greatest sample mean, j = arg maxiXi:\nEr = K∑ j=1 ∆j Pr(j = arg max i Xi) (2)\nwhere ∆j = µ∗ − µj . Strategies that minimize the simple regret are called pure exploration strategies (Bubeck, Munos, and Stoltz 2011). An upper bound on the simple regret of uniform sampling is exponentially decreasing in the number of samples (see (Bubeck, Munos, and Stoltz 2011), Proposition 1). For UCB(c) the best known respective upper bound on the simple regret of UCB(c) is only polynomially decreasing in the number of samples (see (Bubeck, Munos, and Stoltz 2011), Theorems 2,3). However, empirically UCB(c) appears to yield a lower simple regret than uniform sampling."
    }, {
      "heading" : "Metareasoning",
      "text" : "A completely different scheme for control of sampling can use the principles of bounded rationality (Horvitz 1987) and metareasoning — (Russell and Wefald 1991) provided a formal description of rational metareasoning and case studies of applications in several problem domains. In search, under myopic and sub-tree independence assumptions, one maintains a current best move α at the root, and finds the expected gain from finding another move β to be better than the current best (Russell and Wefald 1991). The “cost” of search actions can also be factored in. Ideally, an “optimal” sampling scheme, to be used for selecting what to sample, both at the root node (Hay and Russell 2011) and elsewhere, can be developed using metareasoning. However, this task is daunting for the following reasons:\n• The method is in general intractable, necessitating simplifying assumptions. However, using the standard metareasoning myopic assumption, where samples would be selected as though at most one sample can be taken before an action is chosen, we run into serious problems. Even the basic selection problem (Tolpin and Shimony 2012) exhibits a non-concave utility function and results in premature stopping of the standard myopic algorithms. This is due to the fact that the value of information of a single measurement (analogous to a sample in MCTS) is frequently less than its time-cost, even though this is not true for multiple measurements. When applying the selection problem to MCTS, the situation is exacerbated. The utility of an action is usually bounded, and thus in many cases a single sample may be insufficient to change the current best action, regardless of its outcome. As a result, we frequently get a zero “myopic” value of information for a single sample.\n• Rational metareasoning requires a known distribution model, which may be difficult to obtain.\n• Defining the time-cost of a sample is not trivial. As the above ultimate goal is extremely difficult to achieve, we introduce in this paper simple schemes more amenable to analysis, loosely based on the metareasoning concept of value of information, and compare them to UCB (on sets) and UCT (in trees)."
    }, {
      "heading" : "Sampling Based on Simple Regret",
      "text" : ""
    }, {
      "heading" : "Analysis of Sampling on Sets",
      "text" : "We examine two sampling schemes with superpolynomially decreasing upper bounds on the simple regret. The bounds suggest that these schemes achieve a lower simple regret than uniform sampling; indeed, this is confirmed by experiments.\nWe first consider ε-greedy sampling as a straightforward generalization of uniform sampling:\nDefinition 3. The ε-greedy sampling scheme pulls the arm that currently has the greatst sample mean, with probability 0 < ε < 1, and any other arm with probability 1−εK−1 .\nThis sampling scheme exhibits an exponentially decreasing simple regret:\nTheorem 1. For every 0 < η < 1 and γ > 1 there exists N such that for any number of samples n > N the simple regret of the ε-greedy sampling scheme is bounded from above as\nErε-greedy ≤ 2γ K∑ i=1 ∆i exp  −2∆2jnε( 1 + √ (K−1)ε\n1−ε\n)2  (3)\nwith probability at least 1− η.\nProof outline: Bound the probability Pi that a non-optimal arm i is selected. Split the interval [µi, µ∗] at µi + δi. Apply\nthe Chernoff-Hoeffding bound to get:\nPi ≤ Pr[Xi > µi + δi] + Pr[X∗ < µ∗ − (∆i − δi)] ≤ exp ( −2δ2i ni ) + exp ( −2(∆i − δi)2n∗ ) (4)\nObserve that, in probability, Xi → µi as n → ∞, therefore n∗ → nε, ni → n(1−ε)K−1 as n→∞. Conclude that for every 0 < η < 1, γ > 1 there exists N such that for every n > N and all non-optimal arms i:\nPi ≤ γ ( exp ( −2δ2i n(1− ε)\nK − 1\n) + exp ( −2(∆i − δi)2nε )) (5)\nRequire\nexp ( −2δ\n2 i n(1− ε) K − 1\n) = exp ( −2(∆i − δi)2nε ) δi\n∆i − δi =\n√ (K − 1)ε\n1− ε (6)\nSubstitute (5) together with (6) into (2) and obtain\nErε-greedy ≤ 2γ K∑ i=1 ∆i exp  −2∆2inε( 1 + √ (K−1)ε\n1−ε\n)2  (7)\nIn particular, as the number of arms K grows, the bound for 12 -greedy sampling (ε = 1 2 ) becomes considerably tighter than for uniform random sampling (ε = 1K ):\nCorollary 1. For uniform random sampling,\nEruniform ≤ 2γ K∑ i=1 ∆i exp ( −∆ 2 in K ) (8)\nFor 12 -greedy sampling,\nEr 1 2 -greedy ≤ 2γ K∑ i=1 ∆i exp\n( −2∆2in(\n1 + √ K − 1\n)2 ) (9)\n≈ 2γ K∑ i=1 ∆i exp ( −2∆2in K ) for K 1\nε-greedy is based solely on sampling the arm that has the greatest sample mean (henceforth called the “current best” arm) with a higher probability then the rest of the arms, and ignores information about sample means of other arms. On the other hand, UCB distributes samples in accordance with sample means, but, in order to minimize cumulative regret, chooses the current best arm too often. Intuitively, a better scheme for simple regret minimization would distribute samples in a way similar to UCB, but would sample the current best arm less often. This can be achieved by replacing log(·) in Equation 1 with a faster growing sublinear function, for example, √ ·.\nDefinition 4. Scheme UCB√·(c) pulls arm i that maximizes bi, where:\nbi = Xi +\n√ c √ n\nni (10)\nwhere, as before, Xi is the average reward obtained from arm i, ni is the number of times arm i was pulled, and n is the total number of pulls so far.\nThis scheme also exhibits a super-polynomially decreasing simple regret: Theorem 2. For every 0 < η < 1 and γ > 1 there exists N such that for any number of samples n > N the simple regret of the UCB√·(c) sampling scheme is bounded from above as\nErucb√· ≤ 2γ K∑ i=1 ∆i exp ( −c √ n 2 ) (11)\nwith probability at least 1− η.\nProof outline: Bound the probability Pi that a non-optimal arm i is chosen. Split the interval [µi, µ∗] at µi + ∆i2 . Apply the Chernoff-Hoeffding bound to get:\nPi ≤ Pr [ Xi > µi +\n∆i 2\n] + Pr [ X∗ < µ∗ −\n∆i 2 ] ≤ exp ( −∆\n2 ini 2\n) + exp ( −∆\n2 in∗ 2\n) (12)\nObserve that, in probability, ni → c √ n\n∆2i , ni ≤ n∗ as n→∞.\nConclude that for every 0 < η < 1, γ > 1 there exists N such that for every n > N and all non-optimal arms i:\nPi ≤ 2γ exp ( −c √ n\n2\n) (13)\nSubstitute (13) into (2) and obtain\nErucb√· ≤ 2γ K∑ i=1 ∆i exp ( −c √ n 2 ) (14)"
    }, {
      "heading" : "Sampling in Trees",
      "text" : "As mentioned above, UCT (Kocsis and Szepesvári 2006) is an extension of UCB for MCTS, that applies UCB(c) at each step of a rollout. At the root node, the sampling in MCTS is usually aimed at finding the first move to perform. Search is re-started, either from scratch or using some previously collected information, after observing the actual outcome (in MDPs) or the opponent’s move (in adversarial games). Once one move is shown to be the best choice with high confidence, the value of information of additional samples of the best move (or, in fact, of any other samples) is low. Therefore, one should be able to do better than UCT by optimizing simple regret, rather than cumulative regret, at the root node.\nNodes deeper in the search tree are a different matter. In order to support an optimal move choice at the root, it\nis beneficial in many cases to find a more precise estimate of the value of the state in these search tree nodes. For these internal nodes, optimizing simple regret is not the answer, and cumulative regret optimization is not so far off the mark. Lacking a complete metareasoning for sampling, which would indicate the optimal way to sample both root nodes and internal nodes, our suggested improvement to UCT thus combines different sampling schemes on the first step and during the rest of each rollout: Definition 5. The SR+CR MCTS sampling scheme selects an action at the current root node according to a scheme suitable for minimizing the simple regret (SR), such as 12 - greedy or UCB√·, and (at non-root nodes) then selects actions according to UCB, which approximately minimizes the cumulative regret (CR).\nThe pseudocode of this two-stage rollout for an undiscounted MDP is in Algorithm 1: FIRSTACTION selects the first step of a rollout (line 5), and NEXTACTION (line 6) selects steps during the rest of the rollout (usually using UCB). The reward statistic for the selected action is updated (line 10), and the sample reward is back-propagated (line 11) towards the current root.\nWe denote such two-step realizations of SR+CR as Alg+UCT, where Alg is the sampling scheme employed at the first step of a rollout (e.g. 12 -greedy+UCT).\nAlgorithm 1 Two-stage Monte-Carlo tree search sampling 1: procedure ROLLOUT(node, depth=1) 2: if ISLEAF(node, depth) then 3: return 0 4: else 5: if depth=1 then action← FIRSTACTION(node) 6: else action← NEXTACTION(node) 7: next-node← NEXTSTATE(node, action) 8: reward← REWARD(node, action, next-node) 9: + ROLLOUT(next-node, depth+1) 10: UPDATESTATS(node, action, reward) 11: return reward 12: end if 13: end procedure\nWe expect such two-stage sampling schemes to outperform UCT and be significantly less sensitive to the tuning of the exploration factor c of UCB(c). That is since the contradiction between the need for a larger value of c on the first step (simple regret) and a smaller value for the rest of the rollout (cumulative regret) (Bubeck, Munos, and Stoltz 2011) is resolved. In fact, a sampling scheme that uses UCB(c) at all steps but a larger value of c for the first step than for the rest of the steps, should also outperform UCT."
    }, {
      "heading" : "VOI-aware Sampling",
      "text" : "Further improvement can be achieved by computing or estimating the value of information (VOI) of the rollouts and choosing rollouts that maximize the VOI. However, as indicated above, actually computing the VOI is infeasible. Instead we suggest the following scheme based on the following features of value of information:\n1. An estimate of the probability that one or more rollouts will make another action appear better than the current best α.\n2. An estimate of the gain that may be incurred if such a change occurs. If the distribution of results generated by the rollouts were\nknown, the above features could be easily computed. However, this is not the case for most MCTS applications. Therefore, we estimate bounds on the feature values from the current set of samples, based on the myopic assumption that the algorithm will only sample one of the actions, and use these bounds as the feature values, to get:\nV OIα ≈ Xβ\nnα + 1 exp\n( −2(Xα −Xβ)2nα ) (15)\nV OIi ≈ 1−Xα ni + 1\nexp ( −2(Xα −Xi)2ni ) , i 6= α\nwhere α = arg max i Xi, β = arg max i, i 6=α Xi\nwith V OIα being the (approximate) value for sampling the current best action, and V OIi is the (approximate) value for sampling some other action i.\nThese equations were derived as follows. The gain from switching from the current best action α to another action can be bounded: by the current expectation of the value the current second-best action for the case where we sample only α, and by 1 (the maximum reward) minus the current expectation of α when sampling any other action. The probability that another action be found best can be bounded by an exponential function of the difference in expectations when the true value of the actions becomes known. But the effect of each individual sample on the sample mean is inversely proportional to the current number of samples, hence the current number of samples (plus one in order to handle the initial case of no previous samples) in the denominator.\nThese VOI estimates are used in the “VOI-aware” sampling scheme as follows: sample the action that has maximum estimated VOI. We judged these estimates to be too crude to be used as “stopping criteria” that can be used to cut off sampling, leaving this issue for future research. Although this scheme appears too complicated to be amenable to a formal analysis, early experiments (Section ) with this approach demonstrate a significantly lower simple regret."
    }, {
      "heading" : "Empirical Evaluation",
      "text" : "The results were empirically verified on Multi-armed Bandit instances, on search trees, and on the sailing domain, as defined in (Kocsis and Szepesvári 2006). In most cases, the experiments showed a lower average simple regret for 12 - greedy an UCB√· than for UCB on sets, and for the SR+CR scheme than for UCT in trees.\nSimple regret in multi-armed bandits Figure 1 presents a comparison of MCTS sampling schemes on Multi-armed bandits. Figure 1.a shows the search tree corresponding to a problem instance. Each arm returns a random reward drawn from a Bernoulli distribution. The search\nselects an arm and compares the expected reward, unknown to the algorithm during the sampling, to the expected reward of the best arm.\nFigure 1.b shows the regret vs. the number of samples, averaged over 10000 experiments for randomly generated instances of 32 arms. Either 12 -greedy or UCB √ · dominate UCB over the whole range. For larger number of samples, the advantage of UCB√· over 1 2 -greedy becomes more significant.\nMonte Carlo tree search The second set of experiments was performed on randomly generated 2-level max-max trees crafted so as to deliberately deceive uniform sampling (Figure 2.a), necessitating an adaptive sampling scheme, such as UCT. That is due to the switch nodes, each with 2 children with anti-symmetric values, which would cause a uniform sampling scheme to incorrectly give them all a value of 0.5.\nSimple regret vs. the number of samples are shown for trees with root degree 16 (Figure 2.b) and 64 (Figure 2.c). The exploration factor c is set to 2, the default value for rewards in the range [0, 1]. The algorithms exhibit a similar relative performance: either 12 -greedy+UCT or UCB√·+UCT result in the lowest regret, UCB√·+UCT dominates UCT everywhere except when the number of samples is small. The advantage of both 12 -greedy+UCT and UCB√·+UCT grows with the number of arms."
    }, {
      "heading" : "The sailing domain",
      "text" : "Figures 3–5 show results of experiments on the sailing domain. Figure 3 shows the regret vs. the number of samples, computed for a range of values of c. Figure 3.a shows the median cost, and Figure 3.b — the minimum costs. UCT is always worse than either 12 -greedy+UCT or UCB√·+UCT, and is sensitive to the value of c: the median cost is much higher than the minimum cost for UCT.\nFor both 12 -greedy+UCT and UCB √ ·+UCT, the difference is significantly less prominent. Figure 4 shows the regret vs. the exploration factor for different numbers of samples. UCB√·+UCT is always better than UCT, and 12 -greedy+UCT is better than UCT expect for a small range of values of the exploration factor.\nFigure 5 shows the cost vs. the exploration factor for lakes of different sizes. The relative difference between the sampling schemes becomes more prominent when the lake size increases."
    }, {
      "heading" : "VOI-aware MCTS",
      "text" : "Finally, the VOI-aware sampling scheme was empirically compared to other sampling schemes (UCT, 12 -greedy+UCT, UCT√·+UCT). Again, the experiments were performed on randomly generated trees with structure shown in Figure 2.a. Figure 6 shows the results for 32 arms. VOI+UCT, the\nscheme based on a VOI estimate, outperforms all other sampling schemes in this example. Similar performance improvements (not shown) also occur for the sailing domain."
    }, {
      "heading" : "Conclusion and Future Work",
      "text" : "UCT-based Monte-Carlo tree search has been shown to be very effective for finding good actions in both MDPs and adversarial games. Further improvement of the sampling scheme is thus of interest in numerous search applications. We argue that although UCT is already very efficient, one can do better if the sampling scheme is considered from a metareasoning perspective of value of information (VOI).\nThe MCTS SR+CR scheme presented in the paper differs from UCT mainly in the first step of the rollout, when we attempt to minimize the ‘simple’ selection regret rather than the cumulative regret. Both the theoretical analysis and the empirical evaluation provide evidence for better general performance of the proposed scheme.\nAlthough SR+CR is inspired by the notion of VOI, the VOI is used there implicitly in the analysis of the algorithm, rather than computed or learned explicitly in order\nto plan the rollouts. Ideally, using VOI to control sampling ab-initio should do even better, but the theory for doing that is still not up to speed. Instead we suggest a “VOI-aware” sampling scheme based on crude probability and value estimates, which despite its simplicity already shows a marked improvement in minimizing regret. However, application of the theory of rational metareasoning to Monte Carlo Tree Search is an open problem (Hay and Russell 2011), and both a solid theoretical model and empirically efficient VOI estimates need to be developed.\nFinding a better sampling scheme for non-root nodes, as well as the root node, should also be possible. Although cumulative regret does reasonably well there, it is far from optimal, as meta-reasoning principles imply that an optimal scheme for these nodes must be asymmetrical (e.g. it is not helpful to find out that the value of the current best action is even better than previously believed).\nFinally, applying VOI methods in complex deployed applications that already use MCTS is a challenge that should be addressed in future work. In particular, UCT is extremely successful in Computer Go (Gelly and Wang 2006; Braudiš and Loup Gailly 2011; Enzenberger and Müller 2009), and the proposed scheme should be evaluated on this domain. This is non-trivial, since Go programs typically use “non-pure” versions of UCT, extended with domain-specific knowledge. For example, Pachi (Braudiš and Loup Gailly 2011) typically re-uses information from rollouts generated for earlier moves, thereby violating our underlying assumption that information is only used for selecting the current move. In early experiments not shown here (disallowing reuse of samples, admittedly not really a fair comparison) the VOI-aware scheme apears to dominate UCT. Nevertheless, it should also be possible to adapt the VOI-aware schemes to take into account expected re-use of samples, another topic for future research."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The research is partially supported by Israel Science Foundation grant 305/09, by the Lynne and William Frankel Center for Computer Sciences, and by the Paul Ivanier Center for Robotics Research and Production Management."
    } ],
    "references" : [ {
      "title" : "Finite-time analysis of the multiarmed bandit problem",
      "author" : [ "Cesa-Bianchi Auer", "P. Fischer 2002] Auer", "N. CesaBianchi", "P. Fischer" ],
      "venue" : null,
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "and Loup Gailly",
      "author" : [ "P. Braudiš" ],
      "venue" : "J.",
      "citeRegEx" : "Braudiš and Loup Gailly 2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Pure exploration in finitelyarmed and continuous-armed bandits",
      "author" : [ "Munos Bubeck", "S. Stoltz 2011] Bubeck", "R. Munos", "G. Stoltz" ],
      "venue" : "Theor. Comput. Sci",
      "citeRegEx" : "Bubeck et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Bubeck et al\\.",
      "year" : 2011
    }, {
      "title" : "and Müller",
      "author" : [ "M. Enzenberger" ],
      "venue" : "M.",
      "citeRegEx" : "Enzenberger and Müller 2009",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "High-quality policies for the Canadian traveler’s problem",
      "author" : [ "Keller Eyerich", "P. Helmert 2010] Eyerich", "T. Keller", "M. Helmert" ],
      "venue" : "In In Proc. AAAI",
      "citeRegEx" : "Eyerich et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Eyerich et al\\.",
      "year" : 2010
    }, {
      "title" : "and Wang",
      "author" : [ "S. Gelly" ],
      "venue" : "Y.",
      "citeRegEx" : "Gelly and Wang 2006",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "S",
      "author" : [ "N. Hay", "Russell" ],
      "venue" : "J.",
      "citeRegEx" : "Hay and Russell 2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "E",
      "author" : [ "Horvitz" ],
      "venue" : "J.",
      "citeRegEx" : "Horvitz 1987",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "and Szepesvári",
      "author" : [ "L. Kocsis" ],
      "venue" : "C.",
      "citeRegEx" : "Kocsis and Szepesvári 2006",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "and Norvig",
      "author" : [ "S.J. Russell" ],
      "venue" : "P.",
      "citeRegEx" : "Russell and Norvig 2003",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "and Wefald",
      "author" : [ "S. Russell" ],
      "venue" : "E.",
      "citeRegEx" : "Russell and Wefald 1991",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "S",
      "author" : [ "D. Tolpin", "Shimony" ],
      "venue" : "E.",
      "citeRegEx" : "Tolpin and Shimony 2012",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "and Mohri",
      "author" : [ "J. Vermorel" ],
      "venue" : "M.",
      "citeRegEx" : "Vermorel and Mohri 2005",
      "shortCiteRegEx" : null,
      "year" : 2005
    } ],
    "referenceMentions" : [ ],
    "year" : 2012,
    "abstractText" : "UCT, a state-of-the art algorithm for Monte Carlo tree search (MCTS) in games and Markov decision processes, is based on UCB, a sampling policy for the Multi-armed Bandit problem (MAB) that minimizes the cumulative regret. However, search differs from MAB in that in MCTS it is usually only the final “arm pull” (the actual move selection) that collects a reward, rather than all “arm pulls”. Therefore, it makes more sense to minimize the simple regret, as opposed to the cumulative regret. We begin by introducing policies for multiarmed bandits with lower finite-time and asymptotic simple regret than UCB, using it to develop a two-stage scheme (SR+CR) for MCTS which outperforms UCT empirically. Optimizing the sampling process is itself a metareasoning problem, a solution of which can use value of information (VOI) techniques. Although the theory of VOI for search exists, applying it to MCTS is non-trivial, as typical myopic assumptions fail. Lacking a complete working VOI theory for MCTS, we nevertheless propose a sampling scheme that is “aware” of VOI, achieving an algorithm that in empirical evaluation outperforms both UCT and the other proposed algorithms. Introduction Monte-Carlo tree search, and especially a version based on the UCT formula (Kocsis and Szepesvári 2006) appears in numerous search applications, such as (Gelly and Wang 2006; Eyerich, Keller, and Helmert 2010). Although these methods are shown to be successful empirically, most authors appear to be using the UCT formula “because it has been shown to be successful in the past”, and “because it does a good job of trading off exploration and exploitation”. While the latter statement may be correct for the multi-armed bandit and for the UCB method (Auer, CesaBianchi, and Fischer 2002), we argue that it is inappropriate for search. The problem is not that UCT does not work; rather, a simple reconsideration from basic principles can result in schemes that outperform UCT. The core issue is that in adversarial search and search in “games against nature” — optimizing behavior under uncertainty, the goal is typically to either find a good (or optimal) strategy, or even just to find the best first action of Copyright c © 2012, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. such a policy. Once such an action is discovered, it is usually not beneficial to further sample that action, “exploitation” is thus meaningless for search problems. Finding a good first action is closer to the pure exploration variant, as seen in the selection problem (Bubeck, Munos, and Stoltz 2011; Tolpin and Shimony 2012). In the selection problem, it is much better to minimize the simple regret. However, the simple and the cumulative regret cannot be minimized simultaneously; moreover, (Bubeck, Munos, and Stoltz 2011) shows that in many cases the smaller the cumulative regret, the greater the simple regret. We begin with background definitions and related work. Some sampling schemes are introduced, and shown to have better bounds for the simple regret on sets than UCB, the first contribution of this paper. The results are applied to sampling in trees by combining the proposed sampling schemes on the first step of a rollout with UCT for the rest of the rollout. An additional sampling scheme based on metareasoning principles is also suggested, another contribution of this paper. Finally, the performance of the proposed sampling schemes is evaluated on sets of Bernoulli arms, in randomly generated 2-level trees, and on the sailing domain, showing where the proposed schemes have improved performance. Background and Related Work Monte-Carlo tree search was initially suggested as a scheme for finding approximately optimal policies for Markov Decision Processes (MDP). An MDP is defined by the set of states S, the set of actions A (also called moves in this paper), the transition distribution T (s, a, s′), the reward function R(s, a, s′), the initial state s and an optional goal state t: (S,A, T,R, s, t) (Russell and Norvig 2003). Several MCTS schemes explore an MDP by performing rollouts— trajectories from the current state to a state in which a termination condition is satisfied (either the goal state, or a cutoff state for which the reward is evaluated approximately). Multi-armed bandits and UCT In the Multi-armed Bandit problem (Vermorel and Mohri 2005) we have a set of K arms (see Figure 1.a). Each arm can be pulled multiple times. Sometimes a cost is associated with each pulling action. When the ith arm is pulled, ar X iv :1 20 7. 55 36 v1 [ cs .A I] 2 3 Ju l 2 01 2 a random reward Xi from an unknown stationary distribution is encountered. The reward is usually bounded between 0 and 1. In the cumulative setting (the focus of much of the research literature on Multi-armed bandits), all encountered rewards are collected by the agent. The UCB scheme was shown to be near-optimal in this respect (Auer, CesaBianchi, and Fischer 2002): Definition 1. Scheme UCB(c) pulls arm i that maximizes upper confidence bound bi on the reward: bi = Xi + √ c log(n)",
    "creator" : "LaTeX with hyperref package"
  }
}