{
  "name" : "1505.06798.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Accelerating Very Deep Convolutional Networks for Classification and Detection",
    "authors" : [ "Xiangyu Zhang", "Jianhua Zou", "Kaiming He", "Jian Sun" ],
    "emails" : [ "kahe@microsoft.com", "jiansun@microsoft.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms—Convolutional Neural Networks, Acceleration, Image Classification, Object Detection\nF"
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "The accuracy of convolutional neural networks (CNNs) [3], [4] has been continuously improving [5], [6], [7], [1], [8], but the computational cost of these networks also increases significantly. For example, the very deep VGG models [1], which have witnessed great success in a wide range of recognition tasks [9], [2], [10], [11], [12], [13], are substantially slower than earlier models [4], [5]. Real-world systems may suffer from the low speed of these networks. For example, a cloud service needs to process thousands of new requests per seconds; portable devices such as phones and tablets may not afford slow models; some recognition tasks like object detection [7], [2], [10] and semantic segmentation [11], [12], [13] need to apply these models on higher-resolution images. It is thus of practical importance to accelerate test-time performance of CNNs.\nThere have been a series of studies on accelerating deep CNNs [14], [15], [16], [17]. A common focus of these methods is on the decomposition of one or a few layers. These methods have shown promising speedup ratios and accuracy on one or two layers and whole (but shallower) models. However, few results are available for accelerating very deep models (e.g., ≥ 10 layers). Experiments on complex datasets such as ImageNet [18] are also limited - e.g., the results in [15], [16], [17] are about accelerating a single layer of the shallower AlexNet [4]. Moreover, performance of\n† Correspondence author. • X. Zhang and J. Zou are with Xi’an Jiaotong University, Xi’an, China.\nThis work was done when X. Zhang was an intern at Microsoft Research. • K. He and J. Sun are with Microsoft Research, Beijing, China. E-mail: {kahe,jiansun}@microsoft.com\nthe accelerated networks as generic feature extractors for other recognition tasks [2], [11] remain unclear.\nIt is nontrivial to speed up whole, very deep models for complex tasks like ImageNet classification. Acceleration algorithms involve not only the decomposition of layers, but also the optimization solutions to the decomposition. Data (response) reconstruction solvers [16] based on stochastic gradient descent (SGD) and backpropagation work well for simpler tasks such as character classification [16], but are less effective for complex ImageNet models (as we will discussed in Sec. 4). These SGD-based solvers are sensitive to initialization and learning rates, and might be trapped into poorer local optima for regressing responses. Moreover, even when a solver manages to accelerate a single layer, the accumulated error of approximating multiple layers grow rapidly, especially for very deep models. Besides, the layers of a very deep model may exhibit a great diversity in filter numbers, feature map sizes, sparsity, and redundancy. It may not be beneficial to uniformly accelerate all layers.\nIn this paper, we present an accelerating method that is effective for very deep models. We first propose a response reconstruction method that takes into account the nonlinear neurons and a low-rank constraint. A solution based on Generalized Singular Vector Decomposition (GSVD) is developed for this nonlinear problem, without the need of SGD. Our explicit treatment of the nonlinearity better models a nonlinear layer, and more importantly, enables an asymmetric reconstruction that accounts for the error from previous approximated layers. This method effectively reduces the accumulated error when multiple layers are approximated sequentially. We also present a rank selection method for adaptively determine the acceleration of each layer for a whole model,\nar X\niv :1\n50 5.\n06 79\n8v 1\n[ cs\n.C V\n] 2\n6 M\nay 2\n01 5\n2 based on their redundancy. In experiments, we demonstrate the effects of the nonlinear solution, asymmetric reconstruction, and whole-model acceleration by controlled experiments of a 10-layer model on ImageNet classification [18]. Furthermore, we apply our method on the publicly available VGG-16 model [1], and achieve a 4× speedup with merely a 0.3% increase of top-5 centerview error.\nThe impact of the ImageNet dataset [18] is not merely on the specific 1000-class classification task; deep models pre-trained on ImageNet have been actively used to replace hand-engineered features, and have showcased excellent accuracy for challenging tasks such as object detection [9], [2], [10] and semantic segmentation [11], [12], [13]. We exploit our method to accelerate the very deep VGG-16 model for Fast R-CNN [2] object detection. With a 4× speedup of all convolutions, our method has a graceful degradation of 0.8% mAP (from 66.9% to 66.1%) on the PASCAL VOC 2007 detection benchmark [19].\nA preliminary version of this manuscript has been presented in a conference [20]. This manuscript extends the initial version from several aspects to strengthen our method. (1) We demonstrate compelling acceleration results on very deep VGG models, and are among the first few works accelerating very deep models. (2) We investigate transfer learning results of accelerated models through object detection [2], which is one of the most important applications of ImageNet pre-trained networks. (3) We provide evidence showing that a model trained from scratch and sharing the same structure as the accelerated model is inferior. This discovery suggests that a very deep model can be accelerated not simply because the decomposed network architecture is more powerful, but because the acceleration optimization algorithm is able to digest information."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "Methods [14], [15], [16], [17] for accelerating testtime computation of CNNs in general have two components: (i) a layer decomposition design that reduces time complexity, and (ii) an optimization scheme for the decomposition design. Although the former (“decomposition”) attracts more attention because it directly addresses the time complexity, the latter (“optimization”) is also essential because not all decompositions are equally easy to fine good local optima.\nThe method of Denton et al. [15] is one of the first to exploit low-rank decompositions of filters. Several decomposition designs along different dimensions have been investigated. This method does not consider the nonlinearity of the neurons, which is influential to the accuracy as we will show. This method presents experiments of accelerating a single layer of an OverFeat network [6], but no whole-model results are available.\nJaderberg et al. [16] present efficient decompositions by separating k × k filters into k × 1 and 1 × k filters, which was earlier developed for accelerating generic image filters [21]. Channel-wise dimension reduction is also considered. Two optimization schemes are proposed: (i) “filter reconstruction” that minimizes the error of filter weights, and (ii) “data reconstruction” that minimizes the error of responses. The paper adopts conjugate gradient descent to solve filter reconstruction, and SGD with backpropagation to solve data reconstruction. In their paper, “data reconstruction” demonstrates excellent performance on a character classification task using a 4-layer network. For ImageNet classification, their paper evaluates a single layer of an OverFeat network by “filter reconstruction”. But the performance of whole, very deep models in ImageNet remains unclear.\nConcurrent with our work, Lebedev et al. [17] adopt “CP-decomposition” to decompose a layer into five layers of lower complexity. But for ImageNet classification, only a single-layer acceleration of AlexNet is reported in [17]. Moreover, Lebedev et al. report that they “failed to find a good SGD learning rate” in their fine-tuning, suggesting that it is nontrivial to optimize the factorization for even a single layer in ImageNet models.\nDespite some promising preliminary results have been obtained in the above works [15], [16], [17], the whole-model acceleration of very deep networks for ImageNet is still an open problem.\nBesides the research on decomposing layers, there have been other streams on improving train/test-time performance of CNNs. FFT-based algorithms [22], [23] are applicable for both training and testing, and are particularly effective for large spatial kernels. On the other hand, it is also proposed to train “thin” and deep networks [24], [25] for good trade-off between speed and accuracy. Besides reducing running time, a related issue involving memory conservation [26] is also studied."
    }, {
      "heading" : "3 APPROACHES",
      "text" : "Our method exploits a low-rank assumption for decomposition, following the stream of [15], [16]. We show that this decomposition has a closed-form solution (SVD) for linear neurons, and a slightly more complicated solution based on Generalized SVD (GSVD) [27], [28], [29] for nonlinear neurons. The simplicity of our solver enables an asymmetric reconstruction method for reducing accumulated error of very deep models."
    }, {
      "heading" : "3.1 Low-rank Approximation of Responses",
      "text" : "Our assumption is that the filter response at a pixel of a layer approximately lies on a low-rank subspace. A\n3 \uD835\uDC50 channels W′ P \uD835\uDC51′ channels \uD835\uDC51 channels W (a) (b)\nFigure 1: Illustration of the decomposition. (a) An original layer with complexity O(dk2c). (b) An approximated layer with complexity reduced to O(d′k2c) +O(dd′).\nresulting low-rank decomposition reduces time complexity. To find the approximate low-rank subspace, we minimize the reconstruction error of the responses.\nMore formally, we consider a convolutional layer with a filter size of k×k×c, where k is the spatial size of the filter and c is the number of input channels of this layer. To compute a response, this filter is applied on a k × k × c volume of the layer input. We use x ∈ Rk2c+1 to denote a vector that reshapes this volume, where we append one as the last entry for the sake of the bias. A response y ∈ Rd at a position of a layer is computed as:\ny = Wx. (1)\nwhere W is a d-by-(k2c+1) matrix, and d is the number of filters. Each row of W denotes the reshaped form of a k × k × c filter with the bias appended.\nUnder the assumption that the vector y is on a lowrank subspace, we can write y = M(y− ȳ) + ȳ, where M is a d-by-d matrix of a rank d′ < d and ȳ is the mean vector of responses. Expanding this equation, we can compute a response by:\ny = MWx + b, (2)\nwhere b = ȳ−Mȳ is a new bias. The rank-d′ matrix M can be decomposed into two d-by-d′ matrices P and Q such that M = PQ>. We denote W′ = Q>W as a d′-by-(k2c+1) matrix, which is essentially a new set of d′ filters. Then we can compute (2) by:\ny = PW′x + b. (3)\nThe complexity of using Eqn.(3) is O(d′k2c) +O(dd′), while the complexity of using Eqn.(1) is O(dk2c). For many typical models/layers, we usually have O(dd′) O(d′k2c), so the computation in Eqn.(3) will reduce the complexity to about d′/d.\nFig. 1 illustrates how to use Eqn.(3) in a network. We replace the original layer (given by W) by two\nlayers (given by W′ and P). The matrix W′ is actually d′ filters whose sizes are k × k × c. These filters produce a d′-dimensional feature map. On this feature map, the d-by-d′ matrix P can be implemented as d filters whose sizes are 1 × 1 × d′. So P corresponds to a convolutional layer with a 1×1 spatial support, which maps the d′-dimensional feature map to a ddimensional one.\nNote that the decomposition of M = PQ> can be arbitrary. It does not impact the value of y computed in Eqn.(3). A simple decomposition is the Singular Vector Decomposition (SVD) [30]: M = Ud′Sd′Vd′>, where Ud′ and Vd′ are d-by-d′ column-orthogonal matrices and Sd′ is a d′-by-d′ diagonal matrix. Then we can obtain P = Ud′S 1/2 d′ and Q = Vd′S 1/2 d′ .\nIn practice the low-rank assumption does not strictly hold, and the computation in Eqn.(3) is approximate. To find an approximate low-rank subspace, we optimize the following problem:\nmin M ∑ i ‖(yi − ȳ)−M(yi − ȳ)‖22, (4)\ns.t. rank(M) ≤ d′.\nHere yi is a response sampled from the feature maps in the training set. This problem can be solved by SVD [30] or actually Principal Component Analysis (PCA): let Y be the d-by-n matrix concatenating n responses with the mean subtracted, compute the eigen-decomposition of the covariance matrix YY> = USU> where U is an orthogonal matrix and S is diagonal, and M = Ud′Ud′> where Ud′ are the first d′ eigenvectors. With the matrix M computed, we can find P = Q = Ud′ .\nHow good is the low-rank assumption? We sample the responses from a CNN model (with 7 convolutional layers, detailed in Sec. 4) trained on ImageNet. For the responses of each layer, we compute the eigenvalues of their covariance matrix and then plot the sum of the largest eigenvalues (Fig. 2). We see that substantial energy is in a small portion of the largest eigenvectors. For example, in the Conv2 layer (d = 256) the first 128 eigenvectors contribute over 99.9% energy; in the Conv7 layer (d = 512), the first 256 eigenvectors contribute over 95% energy. This indicates that we can use a fraction of the filters to precisely approximate the original filters.\nThe low-rank behavior of the responses y is because of the low-rank behaviors of the filter weights W and the inputs x. Although the low-rank assumptions about filter weights W have been adopted in recent work [15], [16], we further adopt the low-rank assumptions about the filter inputs x, which are local volumes and have correlations. The responses y will have lower rank than W and x, so the approximation can be more precise. In our optimization (4), we directly address the low-rank subspace of y."
    }, {
      "heading" : "3.2 Nonlinear Case",
      "text" : "Next we investigate the case of using nonlinear units. We use r(·) to denote the nonlinear operator. In this paper we focus on the Rectified Linear Unit (ReLU) [31]: r(·) = max(·, 0).\nDriven by Eqn.(4), we minimize the reconstruction error of the nonlinear responses:\nmin M,b ∑ i ‖r(yi)− r(Myi + b)‖22, (5)\ns.t. rank(M) ≤ d′.\nHere b is a new bias to be optimized, and r(My+b) = r(MWx + b) is the nonlinear response computed by the approximated filters.\nThe above optimization problem is challenging due to the nonlinearity and the low-rank constraint. To find a feasible solution, we relax it as:\nmin M,b,{zi} ∑ i ‖r(yi)− r(zi)‖22 + λ‖zi − (Myi + b)‖22\ns.t. rank(M) ≤ d′. (6)\nHere {zi} is a set of auxiliary variables of the same size as {yi}. λ is a penalty parameter. If λ → ∞, the solution to (6) will converge to the solution to (5) [32]. We adopt an alternating solver, fixing {zi} and solving for M, b and vice versa.\n(i) The subproblem of M, b. In this case, {zi} are fixed. It is easy to show that b is solved by b = z̄ − Mȳ where z̄ is the mean vector of {zi}. Substituting b into the objective function, we obtain the problem involving M:\nmin M ∑ i ‖(zi − z̄)−M(yi − ȳ)‖22, (7)\ns.t. rank(M) ≤ d′.\nThis problem appears similar to Eqn.(4) but the variables are “asymmetric”.\nThis optimization problem also has a closed-form solution by Generalized SVD (GSVD) [27], [28], [29]. Let Z be the d-by-n matrix concatenating the vectors of {zi − z̄}. We rewrite the above problem as:\nmin M ‖Z−MY‖2F, (8)\ns.t. rank(M) ≤ d′.\nHere ‖ · ‖F is the Frobenius norm. A problem in this form is known as Reduced Rank Regression [27], [28], [29]. This problem belongs to a broader category of procrustes problems [27] that have been adopted for various data reconstruction problems [33], [34], [35]. The solution is as follows. Let M̂ = ZY>(YY>)−1. GSVD [27], [28], [29] is applied on M̂: M̂ = USV>, such that U is a d-by-d orthogonal matrix satisfying U>U = Id where Id is a d-by-d identity matrix, and V is a d-by-d matrix satisfying V>YY>V = Id (called generalized orthogonality). Then the solution M to (8) is given by M = Ud′Sd′Vd′> where Ud′ and Vd′ are the first d′ columns of U and V and Sd′ are the largest d′ singular values. One can show that if Z = Y (so the problem in (7) becomes (4)), this GSVD solution becomes the eigen-decomposition of YY>.\n(ii) The subproblem of {zi}. In this case, M and b are fixed. Then in this subproblem each element zij of each vector zi is independent of any other. So we solve a 1-dimensional optimization problem as follows:\nmin zij\n(r(yij)− r(zij))2 + λ(zij − y′ij)2, (9)\nwhere y′ij is the j-th entry of Myi + b. By separately considering zij ≥ 0 and zij < 0, we obtain the solution as follows: let\nz0 = min(0, y ′ ij) (10)\nz1 = max(0, λ · y′ij + r(yij)\nλ+ 1 ) (11)\n5 then zij = arg minz0,z1(r(yij)− r(zij))2 + λ(zij − y′ij)2. Our method is also applicable for other types of nonlinearities. The subproblem in (9) is a 1-dimensional nonlinear least squares problem, so can be solved by gradient descent for other r(·).\nWe alternatively solve (i) and (ii). The initialization is given by the solution to the linear case (4). We warm up the solver by setting the penalty parameter λ = 0.01 and run 25 iterations. Then we increase the value of λ. In theory, λ should be gradually increased to infinity [32]. But we find that it is difficult for the iterative solver to make progress if λ is too large. So we increase λ to 1, run 25 more iterations, and use the resulting M as our solution. As before, we obtain P and Q by SVD on M.\nIn experiments, we find it is sufficient to randomly sample 3,000 images to solve Eqn.(5). It only takes our method 2-5 minutes in MATLAB solving a layer. This is much faster than SGD-based solvers."
    }, {
      "heading" : "3.3 Asymmetric Reconstruction for Multi-Layer",
      "text" : "When each layer is approximated independently, the error of shallower layers will be rapidly accumulated and affect deeper layers. We propose an asymmetric reconstruction method to alleviate this problem.\nWe apply our method sequentially on each layer, from the shallower layers to the deeper ones. Let us consider a layer whose input feature map is not precise due to the approximation of the previous layer/layers. We denote the approximate input to the current layer as x̂. For the training data, we can still compute its non-approximate responses as y = Wx. So we can optimize an “asymmetric” version of (5):\nmin M,b ∑ i ‖r(Wxi)− r(MWx̂i + b)‖22, (12)\ns.t. rank(M) ≤ d′.\nIn the first term r(Wx) = r(y) is the non-approximate output of this layer. In the second term, x̂i is the approximated input to this layer, and r(MWx̂i + b) is the approximated output of this layer. In contrast to using x (or x̂) for both terms, this asymmetric formulation faithfully incorporates the two actual terms before/after the approximation of this layer. The optimization problem in (12) can be solved using the same algorithm as for (5)."
    }, {
      "heading" : "3.4 Rank Selection for Whole-Model Acceleration",
      "text" : "In the above, the optimization is based on a target d′ of each layer. d′ is the only parameter that determines the complexity of an accelerated layer. But given a desired speedup ratio of the whole model, we need to determine the proper rank d′ used for each layer. One may adopt a uniform speedup ratio for each layer. But this is not an optimal solution, because the layers are not equally redundant.\n9596979899100\n−0.5\n−0.4\n−0.3\n−0.2\n−0.1\n0\nPCA Accumulative Energy (%)\n∆ A\ncc ur\nac y\n(% )\nConv2 Conv3 Conv4 Conv5 Conv6 Conv7\nFigure 3: PCA accumulative energy and the accuracy rates (top-5). Here the accuracy is evaluated using the linear solution (the nonlinear solution has a similar trend). Each layer is evaluated independently, with other layers not approximated. The accuracy is shown as the difference to no approximation.\nWe empirically observe that the PCA energy after approximations is roughly related to the classification accuracy. To verify this observation, in Fig. 3 we show the classification accuracy (represented as the difference to no approximation) vs. the PCA energy. Each point in this figure is empirically evaluated using a reduced rank d′. 100% energy means no approximation and thus no degradation of classification accuracy. Fig. 3 shows that the classification accuracy is roughly linear on the PCA energy.\nTo simultaneously determine the reduced ranks of all layers, we further assume that the whole-model classification accuracy is roughly related to the product of the PCA energy of all layers. More formally, we consider this objective function:\nE = ∏ l d′l∑ a=1 σl,a (13)\nHere σl,a is the a-th largest eigenvalue of the layer l, and ∑d′l a=1 σl,a is the PCA energy of the largest d ′ l\neigenvalues in the layer l. The product ∏\nl is over all layers to be approximated. The objective E is assumed to be related to the accuracy of the approximated whole network. Then we optimize this problem:\nmax {d′l} E , s.t. ∑ l d′l dl Cl ≤ C. (14)\nHere dl is the original number of filters in the layer l, and Cl is the original time complexity of the layer l. So d ′ l\ndl Cl is the complexity after the approximation. C is the total complexity after the approximation, which\nis given by the desired speedup ratio. This optimization problem means that we want to maximize the accumulated energy subject to the time complexity constraint.\nThe problem in (14) is a combinatorial problem [36]. So we adopt a greedy strategy to solve it. We initialize d′l as dl, and consider the set {σl,a}. In each step we remove an eigenvalue σl,d′l from this set, chosen from a certain layer l. The relative reduction of the objective is 4E/E = σl,d′/ ∑d′l a=1 σl,a, and the reduction of complexity is 4C = 1dlCl. Then we define a measure as 4E/E4C . The eigenvalue σl,d′l that has the smallest value of this measure is removed. Intuitively, this measure favors a small reduction of 4E/E and a large reduction of complexity 4C. This step is greedily iterated, until the constraint of the total complexity is achieved."
    }, {
      "heading" : "3.5 Higher-Dimensional Decomposition",
      "text" : "In our formulation, we focus on reducing the channels (from d to d′). There are algorithmic advantages of operating on the channel dimension. Firstly, this dimension can be easily controlled by the rank constraint rank(M) ≤ d′. This constraint enables closed-form solutions, e.g., SVD or GSVD. Secondly, the optimized low-rank projection M can be exactly decomposed into low-dimensional filters (P and Q). These simple and closed-form solutions can produce good results using a very small subset of training images (3,000 out of one million).\nOn the other hand, compared with decomposition methods that operate on multiple dimensions (spatial and channel) [16], our method has to use a smaller d′ to approach a given speedup ratio, which might limit the accuracy of our method. To avoid d′ being too small, we further propose to combine our solver with Jaderberg et al.’s spatial decomposition. Thanks\nto our asymmetric reconstruction, our method can effectively alleviate the accumulated error for the multi-decomposition.\nTo determined the decomposed architecture (but not yet the weights), we first use our method to decompose all conv layers of a model. This involves the rank selection of d′ for all layers. Then we apply Jaderberg et al.’s method to further decompose the resulting k×k layers (k > 1) into k×1 and 1×k filters. The first k×1 layer has d′′ output channels depending on the speedup ratio. In this way, an original layer of (k × k, d) is decomposed into three layers of (k × 1, d′′), (1 × k, d′), and (1 × 1, d). For a speedup ratio r, we let each method contribute a speedup of √ r.\nWith the decomposed architecture determined, we solve for the weights of the decomposed layers. Given their order as above, we first optimize the (k × 1, d′′) and (1 × k, d) layers using “filter reconstruction” [16] (we will discuss “data reconstruction” later). Then we adopt our solution on the (1 × k, d) layer and optimize for the (1 × k, d′) and (1 × 1, d) layers. We use our asymmetric reconstruction in Eqn.(12). In the r(MWx̂+b) term, x̂ is the approximated input to this 1 × k layer, and the r(Wx) = r(y) term is still the true response of the original k × k layer without any decomposition. The approximation error of the spatial decomposition will also be addressed by our asymmetric reconstruction, which is important to alleviate accumulated error. We term this as “asymmetric (3d)” in the following."
    }, {
      "heading" : "3.6 Fine-tuning",
      "text" : "With any approximated whole model, we may “finetune” this model end-to-end in the ImageNet training data. This process is similar to training a classification network with the approximated model as the initialization.\nHowever, we empirically find that fine-tuning is very sensitive to the initialization (given by the approximated model) and the learning rate. If the initialization is poor and the learning rate is small, the finetuning is easily trapped in a poor local optimum and makes little progress. If the learning rate is large, the fine-tuning process behaves very similar to training the decomposed architecture “from scratch” (as we will discuss later). A large learning rate may jump out of the initialized local optimum, and the initialization appears to be “forgotten”.\nFortunately, our method has achieved very good accuracy even without fine-tuning as we will show by experiments. With our approximated model as the initialization, the fine-tuning with a sufficiently small learning rate is able to further improve the results. In our experiments, we use a learning rate of 1e-5 and a mini-batch size of 128, and fine-tune the models for 5 epochs in the ImageNet training data.\nWe note that in the following the results are without fine-tuning unless specified."
    }, {
      "heading" : "4 EXPERIMENTS",
      "text" : "We comprehensively evaluate our method on two models. The first model is a 10-layer model of “SPPnet (OverFeat-7)” in [7], which we denote as “SPP-10”. This model (detailed in Table 1) has a similar architecture to the OverFeat model [6] but is deeper. It has 7 conv layers and 3 fc layers. The second model is the publicly available VGG-16 model [1]1 that has 13\n1. www.robots.ox.ac.uk/∼vgg/research/very deep/\nconv layers and 3 fc layers. SPP-10 won the 3-rd place and VGG-16 won the 2-nd place in ILSVRC 2014 [18].\nWe evaluate the “top-5 error” using single-view testing. The view is the center 224×224 region cropped from the resized image whose shorter side is 256. The single-view error rate of SPP-10 is 12.51% on the ImageNet validation set, and VGG-16 is 10.09% in our testing (which is consistent with the number reported by [1]2). These numbers serve as the references for the increased error rates of our approximated models."
    }, {
      "heading" : "4.1 Experiments with SPP-10",
      "text" : "We first evaluate the effect of our each step on the SPP-10 model by a series of controlled experiments. Unless specified, we do not use the 3-d decomposition.\nSingle-Layer: Linear vs. Nonlinear In this subsection we evaluate the single-layer performance. When evaluating a single approximated layer, the rest layers are unchanged and not approximated. The speedup ratio (involving that single layer only) is shown as the theoretical ratio computed by the complexity.\nIn Fig. 4 we compare the performance of our linear solution (4) and nonlinear solution (6). The performance is displayed as increase of error rates (decrease of accuracy) vs. the speedup ratio of that layer. Fig. 4 shows that the nonlinear solution consistently performs better than the linear solution. In Table 1, we\n2. http://www.vlfeat.org/matconvnet/pretrained/\nshow the sparsity (the portion of zero activations after ReLU) of each layer. A zero activation is due to the truncation of ReLU. The sparsity is over 60% for Conv2-7, indicating that the ReLU takes effect on a substantial portion of activations. This explains the discrepancy between the linear and nonlinear solutions. Especially, the Conv7 layer has a sparsity of 95%, so the advantage of the nonlinear solution is more obvious.\nFig. 4 also shows that when accelerating only a single layer by 2×, the increased error rates of our solutions are rather marginal or ignorable. For the Conv2 layer, the error rate is increased by < 0.1%; for the Conv3-7 layers, the error rate is increased by ≈ 0.2%.\nWe also notice that for Conv1, the degradation is ignorable near 2× speedup (1.8× corresponds to d′ = 32). This can be explained by Fig. 2(a): the PCA energy has little loss when d′ ≥ 32. But the degradation can grow quickly for larger speedup ratios, because in this layer the channel number c = 3 is small and d′ needs to be reduced drastically to achieve the speedup ratio. So in the following whole-model experiments of SPP10, we will use d′ = 32 for Conv1.\nMulti-Layer: Symmetric vs. Asymmetric Next we evaluate the performance of asymmetric reconstruction as in the problem (12). We demonstrate approximating 2 layers or 3 layers. In the case of 2 layers, we show the results of approximating Conv6 and 7; and in the case of 3 layers, we show the results of approximating Conv5-7 or Conv2-4. The comparisons are consistently observed for other cases of multi-layer.\nWe sequentially approximate the layers involved, from a shallower one to a deeper one. In the asymmetric version (12), x̂ is from the output of the previous approximated layer (if any), and x is from the output of the previous non-approximate layer. In the\nsymmetric version (5), we use x for both terms. We have also tried another symmetric version of using x̂ for both terms, and found this symmetric version is even worse.\nFig. 5 shows the comparisons between the symmetric and asymmetric versions. The asymmetric solution has significant improvement over the symmetric solution. For example, when only 3 layers are approximated simultaneously (like Fig. 5 (c)), the improvement is over 1.0% when the speedup is 4×. This indicates that the accumulative error rate due to multi-layer approximation can be effectively reduced by the asymmetric version.\nWhen more and all layers are approximated simultaneously (as below), if without the asymmetric solution, the error rates will increase more drastically.\nWhole-Model: with/without Rank Selection In Table 2 we show the results of whole-model acceleration. The solver is the asymmetric version. For Conv1, we fix d′ = 32. For other layers, when the rank selection is not used, we adopt the same speedup ratio on each layer and determine its desired rank d′ accordingly. When the rank selection is used, we apply it to select d′ for Conv2-7. Table 2 shows that the rank selection consistently outperforms the counterpart without rank selection. The advantage of rank selection is observed in both linear and nonlinear solutions.\nIn Table 2 we notice that the rank selection often chooses a higher rank d′ (than the no rank selection) in Conv5-7. For example, when the speedup is 3×, the rank selection assigns d′ = 167 to Conv7, while this layer only requires d′ = 153 to achieve 3× singlelayer speedup of itself. This can be explained by Fig. 2(c). The energy of Conv5-7 is less concentrated, so these layers require higher ranks to achieve good approximations.\nAs we will show, the rank selection is more promi-\nnent for VGG-16 because of its diversity of layers.\nComparisons with Jaderberg et al.’s method [16] We compare with Jaderberg et al.’s method [16], which is a recent state-of-the-art solution to efficient evaluation. Although our decomposition shares some high-level motivations as [16], we point out that our optimization strategy is different with [16] and is important for accuracy, especially for very deep models that previous acceleration methods rarely addressed.\nJaderberg et al.’s method [16] decomposes a k × k spatial support into a cascade of k × 1 and 1 × k spatial supports. A channel-dimension reduction is\nalso considered. Their optimization method focuses on the linear reconstruction error. In the paper of [16], their method is only evaluated on a single layer of an OverFeat network [6] for ImageNet.\nOur comparisons are based on our implementation of [16]. We use the Scheme 2 decomposition in [16] and its “filter reconstruction” version (as we explain below), which is used for ImageNet as in [16]. Our reproduction of the filter reconstruction in [16] gives a 2× single-layer speedup on Conv2 of SPP-10 with 0.2% increase of error. As a reference, in [16] it reports 0.5% increase of error on Conv2 under a 2× single-layer speedup, evaluated on another OverFeat network [6] similar to SPP-10.\nIt is worth discussing our implementation of Jaderberg et al.’s [16] “data reconstruction” scheme, which was suggested to use SGD and backpropagation for optimization. In our reproduction, we find that data reconstruction works well for the character classification task as studied in [16]. However, we find it nontrivial to make data reconstruction work for large models trained for ImageNet. We observe that the learning rate needs to be carefully chosen for the SGD-based data reconstruction to converge (as also reported independently in [17] for another decomposition), and when the training starts to converge, the results are still sensitive to the initialization (for which we have tried Gaussian distributions of a wide range of variances). We conjecture that this is because the ImageNet dataset and models are more complicated, and using SGD to regress a single layer may be sensitive to multiple local optima. In fact, Jaderberg et al.’s [16] only report “filter reconstruction” results of a single layer on ImageNet. For these reasons, our implementation of Jaderberg et al.’s method on ImageNet models is based on filter reconstruction. We believe that these issues have not be settled and\n10\nneed to be investigated further, and accelerating deep networks does not just involve decomposition but also the way of optimization.\nIn Fig. 6 we compare our method with Jaderberg et al.’s [16] for whole-model speedup. For whole-model speedup of [16], we implement their method sequentially on Conv2-7 using the same speedup ratio.3 The speedup ratios are the theoretical complexity ratios involving all convolutional layers. Our method is the asymmetric version and with rank selection. Fig. 6 shows that when the speedup ratios are large (4× and 5×), our method outperforms Jaderberg et al.’s method significantly. For example, when the speedup ratio is 4×, the increased error rate of our method is 4.2%, while Jaderberg et al.’s is 6.0%. Jaderberg et al.’s result degrades quickly when the speedup ratio is getting large, while ours degrades slowly. This suggests the effects of our method for reducing accumulative error.\nWe further compare with our asymmetric version using 3d decomposition (Sec. 3.5). In Fig. 6 we show the results “asymmetric (3d)”. Fig. 6 shows that this strategy leads to significantly smaller increase of error. For example, when the speedup is 5×, the error is increased by only 2.5%. Our asymmetric solver effectively controls the accumulative error even if the multiple layers are decomposed extensively, and the 3d decomposition is easier to achieve a certain speedup ratio.\nFor completeness, we also evaluate our approximation method on the character classification model released by [16]. Our asymmetric (3d) solution achieves 4.5× speedup with only a drop of 0.7% in classification accuracy, which is better than the 1% drop for the same speedup reported by [16].\nComparisons with Training from Scratch The architecture of the approximated model can\n3. We do not apply Jaderberg et al.’s method [16] on Conv1, because this layer has a small number of input channels (3), and the first k×1 decomposed layer can only have a very small number of filters (e.g., 5) to approach a speedup ratio (e.g., 4×). Also note that the speedup ratio is about all conv layers, and because Conv1 is not accelerated, other layers will have a slightly larger speedup.\nalso be trained “from scratch” on the ImageNet dataset. One hypothesis is that the underlying architecture is sufficiently powerful, and the acceleration algorithm might be not necessary. We show that this hypothesis is premature.\nWe directly train the model of the same architecture as the decomposed model. The decomposed model is much deeper than the original model (each layer replaced by three layers), so we adopt the initialization method in [37] otherwise it is not easy to converge. We train the model for 100 epochs. We follow the common practice in [38], [7] of training ImageNet models.\nThe comparisons are in Table 4. The accuracy of the model trained from scratch is worse than that of our accelerated model by a considerable margin (2.8%). These results indicate that the accelerating algorithms can effectively digest information from the trained models. They also suggest that the models trained from scratch have much redundancy.\nComparisons of Absolute Performance Table 3 shows the comparisons of the absolute performance of the accelerated models. We also evaluate the AlexNet [4] which is similarly fast as our accelerated 4× models. The comparison is based on our re-implementation of AlexNet. Our AlexNet is the same as in [4] except that the GPU splitting is ignored. Our re-implementation of this model has top-5 singleview error rate as 18.8% (10-view top-5 16.0% and top1 37.6%). This is better than the one reported in [4]4.\nThe models accelerated by our asymmetric (3d) version have 14.1% and 13.8 top-5 error, without\n4. In [4] the 10-view error is top-5 18.2% and top-1 40.7%.\n11\nand with fine-tuning. This means that the accelerated model has 5.0% lower error than AlexNet, while its speed is nearly the same as AlexNet.\nTable 3 also shows the actual running time per view, on a C++ implementation and Intel i7 CPU (2.9GHz) or Nvidia K40 GPU. In our CPU version, our method has actual speedup ratios (3.5×) close to theoretical speedup ratios (4.0×). This overhead mainly comes from the fc and other layers. In our GPU version, the actual speedup ratio is about 3.3×. An accelerated model is less easy for parallelism in a GPU, so the actual ratio is lower."
    }, {
      "heading" : "4.2 Experiments with VGG-16",
      "text" : "The very deep VGG models [1] have substantially improved a wide range of visual recognition tasks,\nincluding object detection [9], [2], [10], semantic segmentation [11], [12], [13], [39], [40], image captioning [41], [42], [43], video/action recognition [44], image question answering [45], texture recognition [46], etc. Considering the big impact yet slow speed of this model, we believe it is of practical significance to accelerate this model.\nAccelerating VGG-16 for ImageNet Classification\nFirstly we discover that our whole-model rank selection is particularly important for accelerating VGG16. In Table 6 we show the results without/with rank selection. No 3d decomposition is used in this comparison. For a 4× speedup, the rank selection reduces the increased error from 6.38% to 3.84%. This is because of the greater diversity of layers in VGG-\n12\n16 (Table 5). Unlike SPP-10 (or other shallower models [4], [5]) that repeatedly applies 3×3 filters on the same feature map size, the VGG-16 model applies them more evenly on five feature map sizes (224, 112, 56, 28, and 14). Besides, as the filter numbers in Conv51-53 are not increased, the time complexity of Conv51-53 is smaller than others. The selected ranks d′ in Table 6 show their adaptivity - e.g., the layers Conv51 to Conv53 keep more filters, because they have small time complexity and it is not a good tradeoff to compactly reduce them. The whole-model rank selection is a key to maintain a high accuracy for accelerating VGG-16.\nIn Table 7 we evaluate our method on VGG-16 for ImageNet classification. Here we evaluate our asymmetric 3d version (without or with fine-tuning). We evaluate challenging speedup ratios of 3×, 4× and 5×. The ratios are those of the theoretical speedups of all 13 conv layers. Table 8 further shows the absolute performance of accuracy and speed.\nSomewhat surprisingly, our method has demonstrated compelling results for this very deep model, even without fine-tuning. Our no-fine-tuning model has a 0.9% increase of 1-view top-5 error for a speedup ratio of 4×. On the contrary, the previous method [16] suffers greatly from the increased depth because of the rapidly accumulated error of multiple approximated layers. After fine-tuning, our model has a 0.3% increase of 1-view top-5 error for a 4× speedup. This degradation is even lower than that of the shallower model of SPP-10. This suggests that the information in the very deep VGG-16 model is highly redundant, and our method is able to effectively digest it.\nLebedev et al.’s work [17] is one of few existing works that present results of accelerating the whole\nmodel of VGG-16. They report increased top-5 1- view error rates of 3.4% and 7.1% for actual CPU speedups of 3× and 4× (for 4× theoretical speedup they report a 3.8× actual CPU speedup). Thus our method is substantially more accurate than theirs. Note that Lebedev et al.’s results are after fine-tuning. This suggests that fine-tuning is not sufficient for whole-model acceleration; a good optimization solver for the decomposition is needed.\nAccelerating VGG-16 for Object Detection Current state-of-the-art object detection methods [9], [2], [10] all exploit the VGG-16 model. We evaluate our accelerated VGG-16 models for object detection. Our method is based on the latest Fast R-CNN [2].\nWe evaluate on the PASCAL VOC 2007 object detection benchmark [19]. This dataset contains 5k trainval images and 5k test images. We follow the default setting of Fast R-CNN using the publicly released code5. We train Fast R-CNN on the trainval set and evaluate on the test set. The accuracy is evaluated by mean Average Precision (mAP).\nIn our experiments, we first approximate the VGG16 model on the ImageNet classification task. Then we use the approximated model as the pre-trained model for Fast R-CNN. We use our asymmetric 3d version with fine-tuning. Note that unlike image classification where the conv layers dominate running time, for Fast R-CNN detection the conv layers consume about 70% actual running time [2]. The reported speedup ratios are the theoretical speedups about the conv layers only.\nTable 9 shows the results of the accelerated models in PASCAL VOC 2007 detection. Our method with a\n5. https://github.com/rbgirshick/fast-rcnn\n13\n4× convolution speedup has a graceful degradation of 0.8% in mAP. We believe this trade-off between accuracy and speed is of practical importance, because even with the recent advance of fast object detection [7], [9], the feature extraction running time is still considerable."
    }, {
      "heading" : "5 CONCLUSION",
      "text" : "We have presented an acceleration method for very deep networks. Our method is evaluated under whole-model speedup ratios. Our method can effectively reduce the accumulated error of multiple layers thanks to the nonlinear asymmetric reconstruction. Competitive speedups and accuracy are demonstrated in the complex ImageNet classification task and PASCAL VOC object detection task."
    } ],
    "references" : [ {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "K. Simonyan", "A. Zisserman" ],
      "venue" : "International Conference on Learning Representations (ICLR), 2015.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Fast R-CNN",
      "author" : [ "R. Girshick" ],
      "venue" : "arXiv:1504.08083, 2015.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Backpropagation applied to handwritten zip code recognition",
      "author" : [ "Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel" ],
      "venue" : "Neural computation, 1989.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G. Hinton" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS), 2012.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Visualizing and understanding convolutional neural networks",
      "author" : [ "M.D. Zeiler", "R. Fergus" ],
      "venue" : "European Conference on Computer Vision (ECCV), 2014.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Overfeat: Integrated recognition, localization and detection using convolutional networks",
      "author" : [ "P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun" ],
      "venue" : "International Conference on Learning Representations (ICLR), 2014.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Spatial pyramid pooling in deep convolutional networks for visual recognition",
      "author" : [ "K. He", "X. Zhang", "S. Ren", "J. Sun" ],
      "venue" : "European Conference on Computer Vision (ECCV), 2014.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Going deeper with convolutions",
      "author" : [ "C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich" ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Rich feature hierarchies for accurate object detection and semantic segmentation",
      "author" : [ "R. Girshick", "J. Donahue", "T. Darrell", "J. Malik" ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Object detection networks on convolutional feature maps",
      "author" : [ "S. Ren", "K. He", "R. Girshick", "X. Zhang", "J. Sun" ],
      "venue" : "arXiv:1504.06066, 2015.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Fully convolutional networks for semantic segmentation",
      "author" : [ "J. Long", "E. Shelhamer", "T. Darrell" ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Convolutional feature masking for joint object and stuff segmentation",
      "author" : [ "J. Dai", "K. He", "J. Sun" ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Hypercolumns for object segmentation and fine-grained localization",
      "author" : [ "B. Hariharan", "P. Arbeláez", "R. Girshick", "J. Malik" ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Improving the speed of neural networks on CPUs",
      "author" : [ "V. Vanhoucke", "A. Senior", "M.Z. Mao" ],
      "venue" : "Deep Learning and Unsupervised Feature Learning Workshop, NIPS 2011, 2011.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Exploiting linear structure within convolutional networks for efficient evaluation",
      "author" : [ "E. Denton", "W. Zaremba", "J. Bruna", "Y. LeCun", "R. Fergus" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS), 2014.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Speeding up convolutional neural networks with low rank expansions",
      "author" : [ "M. Jaderberg", "A. Vedaldi", "A. Zisserman" ],
      "venue" : "British Machine Vision Conference (BMVC), 2014.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Speeding-up convolutional neural networks using fine-tuned cp-decomposition",
      "author" : [ "V. Lebedev", "Y. Ganin", "M. Rakhuba", "I. Oseledets", "V. Lempitsky" ],
      "venue" : "International Conference on Learning Representations (ICLR), 2015.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Imagenet large scale visual recognition challenge",
      "author" : [ "O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein" ],
      "venue" : "arXiv:1409.0575, 2014.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results",
      "author" : [ "M. Everingham", "L. Van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman" ],
      "venue" : "2007.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Efficient and accurate approximations of nonlinear convolutional networks",
      "author" : [ "X. Zhang", "J. Zou", "X. Ming", "K. He", "J. Sun" ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Learning separable filters",
      "author" : [ "R. Rigamonti", "A. Sironi", "V. Lepetit", "P. Fua" ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2013.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Fast convolutional nets with fbfft: A gpu performance evaluation",
      "author" : [ "N. Vasilache", "J. Johnson", "M. Mathieu", "S. Chintala", "S. Piantino", "Y. LeCun" ],
      "venue" : "International Conference on Learning Representations (ICLR), 2015.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Fast training of convolutional networks through ffts",
      "author" : [ "M. Mathieu", "M. Henaff", "Y. LeCun" ],
      "venue" : "arXiv:1312.5851, 2013.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Convolutional neural networks at constrained time cost",
      "author" : [ "K. He", "J. Sun" ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Fitnets: Hints for thin deep nets",
      "author" : [ "A. Romero", "N. Ballas", "S.E. Kahou", "A. Chassang", "C. Gatta", "Y. Bengio" ],
      "venue" : "International Conference on Learning Representations (ICLR), 2015.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Memory bounded deep convolutional networks",
      "author" : [ "M.D. Collins", "P. Kohli" ],
      "venue" : "arXiv:1412.1442, 2014.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Generalized constrained redundancy analysis",
      "author" : [ "Y. Takane", "S. Jung" ],
      "venue" : "Behaviormetrika, pp. 179–192, 2006.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Regularized linear and kernel redundancy analysis",
      "author" : [ "Y. Takane", "H. Hwang" ],
      "venue" : "Computational Statistics & Data Analysis, pp. 394–405, 2007.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Matrix computations",
      "author" : [ "G.H. Golub", "C.F. van Van Loan" ],
      "venue" : "1996.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Rectified linear units improve restricted boltzmann machines",
      "author" : [ "V. Nair", "G.E. Hinton" ],
      "venue" : "International Conference on Machine Learning (ICML), 2010, pp. 807–814.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A new alternating minimization algorithm for total variation image reconstruction",
      "author" : [ "Y. Wang", "J. Yang", "W. Yin", "Y. Zhang" ],
      "venue" : "SIAM Journal on Imaging Sciences, 2008.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Iterative quantization: A procrustean approach to learning binary codes",
      "author" : [ "Y. Gong", "S. Lazebnik" ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2011.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Optimized product quantization",
      "author" : [ "T. Ge", "K. He", "Q. Ke", "J. Sun" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2014.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Sparse projections for highdimensional binary codes",
      "author" : [ "Y. Xia", "K. He", "P. Kohli", "J. Sun" ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Modern heuristic techniques for combinatorial problems",
      "author" : [ "C.R. Reeves" ],
      "venue" : null,
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 1993
    }, {
      "title" : "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
      "author" : [ "K. He", "X. Zhang", "S. Ren", "J. Sun" ],
      "venue" : "arXiv:1502.01852, 2015.  14",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 1852
    }, {
      "title" : "Return of the devil in the details: Delving deep into convolutional nets",
      "author" : [ "K. Chatfield", "K. Simonyan", "A. Vedaldi", "A. Zisserman" ],
      "venue" : "British Machine Vision Conference (BMVC), 2014.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Semantic image segmentation with deep convolutional nets and fully connected crfs",
      "author" : [ "L.-C. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille" ],
      "venue" : "ICLR, 2015.",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation",
      "author" : [ "J. Dai", "K. He", "J. Sun" ],
      "venue" : "arXiv:1503.01640, 2015.",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "From captions to visual concepts and back",
      "author" : [ "H. Fang", "S. Gupta", "F. Iandola", "R. Srivastava", "L. Deng", "P. Dollár", "J. Gao", "X. He", "M. Mitchell", "J. Platt" ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Deep visual-semantic alignments for generating image descriptions",
      "author" : [ "A. Karpathy", "L. Fei-Fei" ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Learning a recurrent visual representation for image caption generation",
      "author" : [ "X. Chen", "C.L. Zitnick" ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Unsupervised learning of video representations using lstms",
      "author" : [ "N. Srivastava", "E. Mansimov", "R. Salakhutdinov" ],
      "venue" : "International Conference on Machine Learning (ICML), 2015.",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Image question answering: A visual semantic embedding model and a new dataset",
      "author" : [ "M. Ren", "R. Kiros", "R. Zemel" ],
      "venue" : "ICML 2015 Deep Learning Workshop, 2015.",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Deep convolutional filter banks for texture recognition and segmentation",
      "author" : [ "M. Cimpoi", "S. Maji", "A. Vedaldi" ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.",
      "citeRegEx" : "46",
      "shortCiteRegEx" : null,
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Abstract—This paper aims to accelerate the test-time computation of convolutional neural networks (CNNs), especially very deep CNNs [1] that have substantially impacted the computer vision community.",
      "startOffset" : 132,
      "endOffset" : 135
    }, {
      "referenceID" : 0,
      "context" : "For the widely used very deep VGG-16 model [1], our method achieves a whole-model speedup of 4× with merely a 0.",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 1,
      "context" : "Our 4× accelerated VGG-16 model also shows a graceful accuracy degradation for object detection when plugged into the latest Fast R-CNN detector [2].",
      "startOffset" : 145,
      "endOffset" : 148
    }, {
      "referenceID" : 2,
      "context" : "The accuracy of convolutional neural networks (CNNs) [3], [4] has been continuously improving [5], [6], [7], [1], [8], but the computational cost of these networks also increases significantly.",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 3,
      "context" : "The accuracy of convolutional neural networks (CNNs) [3], [4] has been continuously improving [5], [6], [7], [1], [8], but the computational cost of these networks also increases significantly.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 4,
      "context" : "The accuracy of convolutional neural networks (CNNs) [3], [4] has been continuously improving [5], [6], [7], [1], [8], but the computational cost of these networks also increases significantly.",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 5,
      "context" : "The accuracy of convolutional neural networks (CNNs) [3], [4] has been continuously improving [5], [6], [7], [1], [8], but the computational cost of these networks also increases significantly.",
      "startOffset" : 99,
      "endOffset" : 102
    }, {
      "referenceID" : 6,
      "context" : "The accuracy of convolutional neural networks (CNNs) [3], [4] has been continuously improving [5], [6], [7], [1], [8], but the computational cost of these networks also increases significantly.",
      "startOffset" : 104,
      "endOffset" : 107
    }, {
      "referenceID" : 0,
      "context" : "The accuracy of convolutional neural networks (CNNs) [3], [4] has been continuously improving [5], [6], [7], [1], [8], but the computational cost of these networks also increases significantly.",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 7,
      "context" : "The accuracy of convolutional neural networks (CNNs) [3], [4] has been continuously improving [5], [6], [7], [1], [8], but the computational cost of these networks also increases significantly.",
      "startOffset" : 114,
      "endOffset" : 117
    }, {
      "referenceID" : 0,
      "context" : "For example, the very deep VGG models [1], which have witnessed great success in a wide range of recognition tasks [9], [2], [10], [11], [12], [13], are substantially slower than earlier models [4], [5].",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 8,
      "context" : "For example, the very deep VGG models [1], which have witnessed great success in a wide range of recognition tasks [9], [2], [10], [11], [12], [13], are substantially slower than earlier models [4], [5].",
      "startOffset" : 115,
      "endOffset" : 118
    }, {
      "referenceID" : 1,
      "context" : "For example, the very deep VGG models [1], which have witnessed great success in a wide range of recognition tasks [9], [2], [10], [11], [12], [13], are substantially slower than earlier models [4], [5].",
      "startOffset" : 120,
      "endOffset" : 123
    }, {
      "referenceID" : 9,
      "context" : "For example, the very deep VGG models [1], which have witnessed great success in a wide range of recognition tasks [9], [2], [10], [11], [12], [13], are substantially slower than earlier models [4], [5].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 10,
      "context" : "For example, the very deep VGG models [1], which have witnessed great success in a wide range of recognition tasks [9], [2], [10], [11], [12], [13], are substantially slower than earlier models [4], [5].",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 11,
      "context" : "For example, the very deep VGG models [1], which have witnessed great success in a wide range of recognition tasks [9], [2], [10], [11], [12], [13], are substantially slower than earlier models [4], [5].",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 12,
      "context" : "For example, the very deep VGG models [1], which have witnessed great success in a wide range of recognition tasks [9], [2], [10], [11], [12], [13], are substantially slower than earlier models [4], [5].",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 3,
      "context" : "For example, the very deep VGG models [1], which have witnessed great success in a wide range of recognition tasks [9], [2], [10], [11], [12], [13], are substantially slower than earlier models [4], [5].",
      "startOffset" : 194,
      "endOffset" : 197
    }, {
      "referenceID" : 4,
      "context" : "For example, the very deep VGG models [1], which have witnessed great success in a wide range of recognition tasks [9], [2], [10], [11], [12], [13], are substantially slower than earlier models [4], [5].",
      "startOffset" : 199,
      "endOffset" : 202
    }, {
      "referenceID" : 6,
      "context" : "For example, a cloud service needs to process thousands of new requests per seconds; portable devices such as phones and tablets may not afford slow models; some recognition tasks like object detection [7], [2], [10] and semantic segmentation [11], [12], [13] need to apply these models on higher-resolution images.",
      "startOffset" : 202,
      "endOffset" : 205
    }, {
      "referenceID" : 1,
      "context" : "For example, a cloud service needs to process thousands of new requests per seconds; portable devices such as phones and tablets may not afford slow models; some recognition tasks like object detection [7], [2], [10] and semantic segmentation [11], [12], [13] need to apply these models on higher-resolution images.",
      "startOffset" : 207,
      "endOffset" : 210
    }, {
      "referenceID" : 9,
      "context" : "For example, a cloud service needs to process thousands of new requests per seconds; portable devices such as phones and tablets may not afford slow models; some recognition tasks like object detection [7], [2], [10] and semantic segmentation [11], [12], [13] need to apply these models on higher-resolution images.",
      "startOffset" : 212,
      "endOffset" : 216
    }, {
      "referenceID" : 10,
      "context" : "For example, a cloud service needs to process thousands of new requests per seconds; portable devices such as phones and tablets may not afford slow models; some recognition tasks like object detection [7], [2], [10] and semantic segmentation [11], [12], [13] need to apply these models on higher-resolution images.",
      "startOffset" : 243,
      "endOffset" : 247
    }, {
      "referenceID" : 11,
      "context" : "For example, a cloud service needs to process thousands of new requests per seconds; portable devices such as phones and tablets may not afford slow models; some recognition tasks like object detection [7], [2], [10] and semantic segmentation [11], [12], [13] need to apply these models on higher-resolution images.",
      "startOffset" : 249,
      "endOffset" : 253
    }, {
      "referenceID" : 12,
      "context" : "For example, a cloud service needs to process thousands of new requests per seconds; portable devices such as phones and tablets may not afford slow models; some recognition tasks like object detection [7], [2], [10] and semantic segmentation [11], [12], [13] need to apply these models on higher-resolution images.",
      "startOffset" : 255,
      "endOffset" : 259
    }, {
      "referenceID" : 13,
      "context" : "There have been a series of studies on accelerating deep CNNs [14], [15], [16], [17].",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 14,
      "context" : "There have been a series of studies on accelerating deep CNNs [14], [15], [16], [17].",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 15,
      "context" : "There have been a series of studies on accelerating deep CNNs [14], [15], [16], [17].",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 16,
      "context" : "There have been a series of studies on accelerating deep CNNs [14], [15], [16], [17].",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 17,
      "context" : "Experiments on complex datasets such as ImageNet [18] are also limited - e.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 14,
      "context" : ", the results in [15], [16], [17] are about accelerating a single layer of the shallower AlexNet [4].",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 15,
      "context" : ", the results in [15], [16], [17] are about accelerating a single layer of the shallower AlexNet [4].",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 16,
      "context" : ", the results in [15], [16], [17] are about accelerating a single layer of the shallower AlexNet [4].",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 3,
      "context" : ", the results in [15], [16], [17] are about accelerating a single layer of the shallower AlexNet [4].",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 1,
      "context" : "com the accelerated networks as generic feature extractors for other recognition tasks [2], [11] remain unclear.",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 10,
      "context" : "com the accelerated networks as generic feature extractors for other recognition tasks [2], [11] remain unclear.",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 15,
      "context" : "Data (response) reconstruction solvers [16] based on stochastic gradient descent (SGD) and backpropagation work well for simpler tasks such as character classification [16], but are less effective for complex ImageNet models (as we will discussed in Sec.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 15,
      "context" : "Data (response) reconstruction solvers [16] based on stochastic gradient descent (SGD) and backpropagation work well for simpler tasks such as character classification [16], but are less effective for complex ImageNet models (as we will discussed in Sec.",
      "startOffset" : 168,
      "endOffset" : 172
    }, {
      "referenceID" : 17,
      "context" : "In experiments, we demonstrate the effects of the nonlinear solution, asymmetric reconstruction, and whole-model acceleration by controlled experiments of a 10-layer model on ImageNet classification [18].",
      "startOffset" : 199,
      "endOffset" : 203
    }, {
      "referenceID" : 0,
      "context" : "Furthermore, we apply our method on the publicly available VGG-16 model [1], and achieve a 4× speedup with merely a 0.",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 17,
      "context" : "The impact of the ImageNet dataset [18] is not merely on the specific 1000-class classification task; deep models pre-trained on ImageNet have been actively used to replace hand-engineered features, and have showcased excellent accuracy for challenging tasks such as object detection [9], [2], [10] and semantic segmentation [11], [12], [13].",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 8,
      "context" : "The impact of the ImageNet dataset [18] is not merely on the specific 1000-class classification task; deep models pre-trained on ImageNet have been actively used to replace hand-engineered features, and have showcased excellent accuracy for challenging tasks such as object detection [9], [2], [10] and semantic segmentation [11], [12], [13].",
      "startOffset" : 284,
      "endOffset" : 287
    }, {
      "referenceID" : 1,
      "context" : "The impact of the ImageNet dataset [18] is not merely on the specific 1000-class classification task; deep models pre-trained on ImageNet have been actively used to replace hand-engineered features, and have showcased excellent accuracy for challenging tasks such as object detection [9], [2], [10] and semantic segmentation [11], [12], [13].",
      "startOffset" : 289,
      "endOffset" : 292
    }, {
      "referenceID" : 9,
      "context" : "The impact of the ImageNet dataset [18] is not merely on the specific 1000-class classification task; deep models pre-trained on ImageNet have been actively used to replace hand-engineered features, and have showcased excellent accuracy for challenging tasks such as object detection [9], [2], [10] and semantic segmentation [11], [12], [13].",
      "startOffset" : 294,
      "endOffset" : 298
    }, {
      "referenceID" : 10,
      "context" : "The impact of the ImageNet dataset [18] is not merely on the specific 1000-class classification task; deep models pre-trained on ImageNet have been actively used to replace hand-engineered features, and have showcased excellent accuracy for challenging tasks such as object detection [9], [2], [10] and semantic segmentation [11], [12], [13].",
      "startOffset" : 325,
      "endOffset" : 329
    }, {
      "referenceID" : 11,
      "context" : "The impact of the ImageNet dataset [18] is not merely on the specific 1000-class classification task; deep models pre-trained on ImageNet have been actively used to replace hand-engineered features, and have showcased excellent accuracy for challenging tasks such as object detection [9], [2], [10] and semantic segmentation [11], [12], [13].",
      "startOffset" : 331,
      "endOffset" : 335
    }, {
      "referenceID" : 12,
      "context" : "The impact of the ImageNet dataset [18] is not merely on the specific 1000-class classification task; deep models pre-trained on ImageNet have been actively used to replace hand-engineered features, and have showcased excellent accuracy for challenging tasks such as object detection [9], [2], [10] and semantic segmentation [11], [12], [13].",
      "startOffset" : 337,
      "endOffset" : 341
    }, {
      "referenceID" : 1,
      "context" : "We exploit our method to accelerate the very deep VGG-16 model for Fast R-CNN [2] object detection.",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 18,
      "context" : "1%) on the PASCAL VOC 2007 detection benchmark [19].",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 19,
      "context" : "A preliminary version of this manuscript has been presented in a conference [20].",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 1,
      "context" : "(2) We investigate transfer learning results of accelerated models through object detection [2], which is one of the most important applications of ImageNet pre-trained networks.",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 13,
      "context" : "Methods [14], [15], [16], [17] for accelerating testtime computation of CNNs in general have two components: (i) a layer decomposition design that reduces time complexity, and (ii) an optimization scheme for the decomposition design.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 14,
      "context" : "Methods [14], [15], [16], [17] for accelerating testtime computation of CNNs in general have two components: (i) a layer decomposition design that reduces time complexity, and (ii) an optimization scheme for the decomposition design.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 15,
      "context" : "Methods [14], [15], [16], [17] for accelerating testtime computation of CNNs in general have two components: (i) a layer decomposition design that reduces time complexity, and (ii) an optimization scheme for the decomposition design.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 16,
      "context" : "Methods [14], [15], [16], [17] for accelerating testtime computation of CNNs in general have two components: (i) a layer decomposition design that reduces time complexity, and (ii) an optimization scheme for the decomposition design.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 14,
      "context" : "[15] is one of the first to exploit low-rank decompositions of filters.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 5,
      "context" : "This method presents experiments of accelerating a single layer of an OverFeat network [6], but no whole-model results are available.",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 15,
      "context" : "[16] present efficient decompositions by separating k × k filters into k × 1 and 1 × k filters, which was earlier developed for accelerating generic image filters [21].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "[16] present efficient decompositions by separating k × k filters into k × 1 and 1 × k filters, which was earlier developed for accelerating generic image filters [21].",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 16,
      "context" : "[17] adopt “CP-decomposition” to decompose a layer into five layers of lower complexity.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "But for ImageNet classification, only a single-layer acceleration of AlexNet is reported in [17].",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 14,
      "context" : "Despite some promising preliminary results have been obtained in the above works [15], [16], [17], the whole-model acceleration of very deep networks for ImageNet is still an open problem.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 15,
      "context" : "Despite some promising preliminary results have been obtained in the above works [15], [16], [17], the whole-model acceleration of very deep networks for ImageNet is still an open problem.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 16,
      "context" : "Despite some promising preliminary results have been obtained in the above works [15], [16], [17], the whole-model acceleration of very deep networks for ImageNet is still an open problem.",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 21,
      "context" : "FFT-based algorithms [22], [23] are applicable for both training and testing, and are particularly effective for large spatial kernels.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 22,
      "context" : "FFT-based algorithms [22], [23] are applicable for both training and testing, and are particularly effective for large spatial kernels.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 23,
      "context" : "On the other hand, it is also proposed to train “thin” and deep networks [24], [25] for good trade-off between speed and accuracy.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 24,
      "context" : "On the other hand, it is also proposed to train “thin” and deep networks [24], [25] for good trade-off between speed and accuracy.",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 25,
      "context" : "Besides reducing running time, a related issue involving memory conservation [26] is also studied.",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 14,
      "context" : "Our method exploits a low-rank assumption for decomposition, following the stream of [15], [16].",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 15,
      "context" : "Our method exploits a low-rank assumption for decomposition, following the stream of [15], [16].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 26,
      "context" : "We show that this decomposition has a closed-form solution (SVD) for linear neurons, and a slightly more complicated solution based on Generalized SVD (GSVD) [27], [28], [29] for nonlinear neurons.",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 27,
      "context" : "We show that this decomposition has a closed-form solution (SVD) for linear neurons, and a slightly more complicated solution based on Generalized SVD (GSVD) [27], [28], [29] for nonlinear neurons.",
      "startOffset" : 170,
      "endOffset" : 174
    }, {
      "referenceID" : 28,
      "context" : "A simple decomposition is the Singular Vector Decomposition (SVD) [30]: M = Ud′Sd′Vd′, where Ud′ and Vd′ are d-by-d′ column-orthogonal matrices and Sd′ is a d′-by-d′ diagonal matrix.",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 28,
      "context" : "This problem can be solved by SVD [30] or actually Principal Component Analysis (PCA): let Y be the d-by-n matrix concatenating n responses with the mean subtracted, compute the eigen-decomposition of the covariance matrix YY> = USU> where U is an orthogonal matrix and S is diagonal, and M = Ud′Ud′ where Ud′ are the first d′ eigenvectors.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 14,
      "context" : "Although the low-rank assumptions about filter weights W have been adopted in recent work [15], [16], we further adopt the low-rank assumptions about the filter inputs x, which are local volumes and have correlations.",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 15,
      "context" : "Although the low-rank assumptions about filter weights W have been adopted in recent work [15], [16], we further adopt the low-rank assumptions about the filter inputs x, which are local volumes and have correlations.",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 29,
      "context" : "In this paper we focus on the Rectified Linear Unit (ReLU) [31]: r(·) = max(·, 0).",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 30,
      "context" : "If λ → ∞, the solution to (6) will converge to the solution to (5) [32].",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 26,
      "context" : "This optimization problem also has a closed-form solution by Generalized SVD (GSVD) [27], [28], [29].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 27,
      "context" : "This optimization problem also has a closed-form solution by Generalized SVD (GSVD) [27], [28], [29].",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 26,
      "context" : "A problem in this form is known as Reduced Rank Regression [27], [28], [29].",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 27,
      "context" : "A problem in this form is known as Reduced Rank Regression [27], [28], [29].",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 31,
      "context" : "This problem belongs to a broader category of procrustes problems [27] that have been adopted for various data reconstruction problems [33], [34], [35].",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 32,
      "context" : "This problem belongs to a broader category of procrustes problems [27] that have been adopted for various data reconstruction problems [33], [34], [35].",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 33,
      "context" : "This problem belongs to a broader category of procrustes problems [27] that have been adopted for various data reconstruction problems [33], [34], [35].",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 26,
      "context" : "GSVD [27], [28], [29] is applied on M̂: M̂ = USV>, such that U is a d-by-d orthogonal matrix satisfying U>U = Id where Id is a d-by-d identity matrix, and V is a d-by-d matrix satisfying V>YY>V = Id (called generalized orthogonality).",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 27,
      "context" : "GSVD [27], [28], [29] is applied on M̂: M̂ = USV>, such that U is a d-by-d orthogonal matrix satisfying U>U = Id where Id is a d-by-d identity matrix, and V is a d-by-d matrix satisfying V>YY>V = Id (called generalized orthogonality).",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 30,
      "context" : "In theory, λ should be gradually increased to infinity [32].",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 6,
      "context" : "Table 1: The architecture of the SPP-10 model [7].",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 6,
      "context" : "The final conv layer is followed by a spatial pyramid pooling layer [7] that have 4 levels ({6 × 6, 3 × 3, 2 × 2, 1 × 1}, totally 50 bins).",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 34,
      "context" : "The problem in (14) is a combinatorial problem [36].",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 15,
      "context" : "On the other hand, compared with decomposition methods that operate on multiple dimensions (spatial and channel) [16], our method has to use a smaller d′ to approach a given speedup ratio, which might limit the accuracy of our method.",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 15,
      "context" : "Given their order as above, we first optimize the (k × 1, d′′) and (1 × k, d) layers using “filter reconstruction” [16] (we will discuss “data reconstruction” later).",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 6,
      "context" : "The first model is a 10-layer model of “SPPnet (OverFeat-7)” in [7], which we denote as “SPP-10”.",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 5,
      "context" : "This model (detailed in Table 1) has a similar architecture to the OverFeat model [6] but is deeper.",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 0,
      "context" : "The second model is the publicly available VGG-16 model [1]1 that has 13",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 17,
      "context" : "SPP-10 won the 3-rd place and VGG-16 won the 2-nd place in ILSVRC 2014 [18].",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 0,
      "context" : "09% in our testing (which is consistent with the number reported by [1]2).",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 15,
      "context" : "’s method [16]",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 15,
      "context" : "’s method [16], which is a recent state-of-the-art solution to efficient evaluation.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 15,
      "context" : "Although our decomposition shares some high-level motivations as [16], we point out that our optimization strategy is different with [16] and is important for accuracy, especially for very deep models that previous acceleration methods rarely addressed.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 15,
      "context" : "Although our decomposition shares some high-level motivations as [16], we point out that our optimization strategy is different with [16] and is important for accuracy, especially for very deep models that previous acceleration methods rarely addressed.",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 15,
      "context" : "’s method [16] decomposes a k × k spatial support into a cascade of k × 1 and 1 × k spatial supports.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 15,
      "context" : "’s spatial decomposition method [16] for SPP-10.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 15,
      "context" : "In the paper of [16], their method is only evaluated on a single layer of an OverFeat network [6] for ImageNet.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 5,
      "context" : "In the paper of [16], their method is only evaluated on a single layer of an OverFeat network [6] for ImageNet.",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 15,
      "context" : "Our comparisons are based on our implementation of [16].",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 15,
      "context" : "We use the Scheme 2 decomposition in [16] and its “filter reconstruction” version (as we explain below), which is used for ImageNet as in [16].",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 15,
      "context" : "We use the Scheme 2 decomposition in [16] and its “filter reconstruction” version (as we explain below), which is used for ImageNet as in [16].",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 15,
      "context" : "Our reproduction of the filter reconstruction in [16] gives a 2× single-layer speedup on Conv2 of SPP-10 with 0.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 15,
      "context" : "As a reference, in [16] it reports 0.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 5,
      "context" : "5% increase of error on Conv2 under a 2× single-layer speedup, evaluated on another OverFeat network [6] similar to SPP-10.",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 15,
      "context" : "’s [16] “data reconstruction” scheme, which was suggested to use SGD and backpropagation for optimization.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 15,
      "context" : "In our reproduction, we find that data reconstruction works well for the character classification task as studied in [16].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 16,
      "context" : "We observe that the learning rate needs to be carefully chosen for the SGD-based data reconstruction to converge (as also reported independently in [17] for another decomposition), and when the training starts to converge, the results are still sensitive to the initialization (for which we have tried Gaussian distributions of a wide range of variances).",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 15,
      "context" : "’s [16] only report “filter reconstruction” results of a single layer on ImageNet.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 6,
      "context" : "SPP-10 [7] 12.",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 15,
      "context" : "[16] (our impl.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 3,
      "context" : "AlexNet [4] 18.",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 15,
      "context" : "’s [16] for whole-model speedup.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 15,
      "context" : "For whole-model speedup of [16], we implement their method sequentially on Conv2-7 using the same speedup ratio.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 15,
      "context" : "For completeness, we also evaluate our approximation method on the character classification model released by [16].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 15,
      "context" : "7% in classification accuracy, which is better than the 1% drop for the same speedup reported by [16].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 15,
      "context" : "’s method [16] on Conv1, because this layer has a small number of input channels (3), and the first k×1 decomposed layer can only have a very small number of filters (e.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 35,
      "context" : "The decomposed model is much deeper than the original model (each layer replaced by three layers), so we adopt the initialization method in [37] otherwise it is not easy to converge.",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 36,
      "context" : "We follow the common practice in [38], [7] of training ImageNet models.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 6,
      "context" : "We follow the common practice in [38], [7] of training ImageNet models.",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 6,
      "context" : "SPP-10 [7] 12.",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 3,
      "context" : "We also evaluate the AlexNet [4] which is similarly fast as our accelerated 4× models.",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 3,
      "context" : "Our AlexNet is the same as in [4] except that the GPU splitting is ignored.",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 3,
      "context" : "This is better than the one reported in [4]4.",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 3,
      "context" : "In [4] the 10-view error is top-5 18.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 0,
      "context" : "Table 5: The architecture of the VGG-16 model [1].",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 0,
      "context" : "The very deep VGG models [1] have substantially improved a wide range of visual recognition tasks, including object detection [9], [2], [10], semantic segmentation [11], [12], [13], [39], [40], image captioning [41], [42], [43], video/action recognition [44], image question answering [45], texture recognition [46], etc.",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 8,
      "context" : "The very deep VGG models [1] have substantially improved a wide range of visual recognition tasks, including object detection [9], [2], [10], semantic segmentation [11], [12], [13], [39], [40], image captioning [41], [42], [43], video/action recognition [44], image question answering [45], texture recognition [46], etc.",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 1,
      "context" : "The very deep VGG models [1] have substantially improved a wide range of visual recognition tasks, including object detection [9], [2], [10], semantic segmentation [11], [12], [13], [39], [40], image captioning [41], [42], [43], video/action recognition [44], image question answering [45], texture recognition [46], etc.",
      "startOffset" : 131,
      "endOffset" : 134
    }, {
      "referenceID" : 9,
      "context" : "The very deep VGG models [1] have substantially improved a wide range of visual recognition tasks, including object detection [9], [2], [10], semantic segmentation [11], [12], [13], [39], [40], image captioning [41], [42], [43], video/action recognition [44], image question answering [45], texture recognition [46], etc.",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 10,
      "context" : "The very deep VGG models [1] have substantially improved a wide range of visual recognition tasks, including object detection [9], [2], [10], semantic segmentation [11], [12], [13], [39], [40], image captioning [41], [42], [43], video/action recognition [44], image question answering [45], texture recognition [46], etc.",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 11,
      "context" : "The very deep VGG models [1] have substantially improved a wide range of visual recognition tasks, including object detection [9], [2], [10], semantic segmentation [11], [12], [13], [39], [40], image captioning [41], [42], [43], video/action recognition [44], image question answering [45], texture recognition [46], etc.",
      "startOffset" : 170,
      "endOffset" : 174
    }, {
      "referenceID" : 12,
      "context" : "The very deep VGG models [1] have substantially improved a wide range of visual recognition tasks, including object detection [9], [2], [10], semantic segmentation [11], [12], [13], [39], [40], image captioning [41], [42], [43], video/action recognition [44], image question answering [45], texture recognition [46], etc.",
      "startOffset" : 176,
      "endOffset" : 180
    }, {
      "referenceID" : 37,
      "context" : "The very deep VGG models [1] have substantially improved a wide range of visual recognition tasks, including object detection [9], [2], [10], semantic segmentation [11], [12], [13], [39], [40], image captioning [41], [42], [43], video/action recognition [44], image question answering [45], texture recognition [46], etc.",
      "startOffset" : 182,
      "endOffset" : 186
    }, {
      "referenceID" : 38,
      "context" : "The very deep VGG models [1] have substantially improved a wide range of visual recognition tasks, including object detection [9], [2], [10], semantic segmentation [11], [12], [13], [39], [40], image captioning [41], [42], [43], video/action recognition [44], image question answering [45], texture recognition [46], etc.",
      "startOffset" : 188,
      "endOffset" : 192
    }, {
      "referenceID" : 39,
      "context" : "The very deep VGG models [1] have substantially improved a wide range of visual recognition tasks, including object detection [9], [2], [10], semantic segmentation [11], [12], [13], [39], [40], image captioning [41], [42], [43], video/action recognition [44], image question answering [45], texture recognition [46], etc.",
      "startOffset" : 211,
      "endOffset" : 215
    }, {
      "referenceID" : 40,
      "context" : "The very deep VGG models [1] have substantially improved a wide range of visual recognition tasks, including object detection [9], [2], [10], semantic segmentation [11], [12], [13], [39], [40], image captioning [41], [42], [43], video/action recognition [44], image question answering [45], texture recognition [46], etc.",
      "startOffset" : 217,
      "endOffset" : 221
    }, {
      "referenceID" : 41,
      "context" : "The very deep VGG models [1] have substantially improved a wide range of visual recognition tasks, including object detection [9], [2], [10], semantic segmentation [11], [12], [13], [39], [40], image captioning [41], [42], [43], video/action recognition [44], image question answering [45], texture recognition [46], etc.",
      "startOffset" : 223,
      "endOffset" : 227
    }, {
      "referenceID" : 42,
      "context" : "The very deep VGG models [1] have substantially improved a wide range of visual recognition tasks, including object detection [9], [2], [10], semantic segmentation [11], [12], [13], [39], [40], image captioning [41], [42], [43], video/action recognition [44], image question answering [45], texture recognition [46], etc.",
      "startOffset" : 254,
      "endOffset" : 258
    }, {
      "referenceID" : 43,
      "context" : "The very deep VGG models [1] have substantially improved a wide range of visual recognition tasks, including object detection [9], [2], [10], semantic segmentation [11], [12], [13], [39], [40], image captioning [41], [42], [43], video/action recognition [44], image question answering [45], texture recognition [46], etc.",
      "startOffset" : 285,
      "endOffset" : 289
    }, {
      "referenceID" : 44,
      "context" : "The very deep VGG models [1] have substantially improved a wide range of visual recognition tasks, including object detection [9], [2], [10], semantic segmentation [11], [12], [13], [39], [40], image captioning [41], [42], [43], video/action recognition [44], image question answering [45], texture recognition [46], etc.",
      "startOffset" : 311,
      "endOffset" : 315
    }, {
      "referenceID" : 15,
      "context" : "[16] (our impl.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "Table 7: Accelerating the VGG-16 model [1] using a speedup ratio of 3×, 4×, or 5×.",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 0,
      "context" : "VGG-16 [1] 10.",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 15,
      "context" : "[16] (our impl.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "Table 8: Absolute performance of accelerating the VGG-16 model [1].",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 3,
      "context" : "Unlike SPP-10 (or other shallower models [4], [5]) that repeatedly applies 3×3 filters on the same feature map size, the VGG-16 model applies them more evenly on five feature map sizes (224, 112, 56, 28, and 14).",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 4,
      "context" : "Unlike SPP-10 (or other shallower models [4], [5]) that repeatedly applies 3×3 filters on the same feature map size, the VGG-16 model applies them more evenly on five feature map sizes (224, 112, 56, 28, and 14).",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 15,
      "context" : "On the contrary, the previous method [16] suffers greatly from the increased depth because of the rapidly accumulated error of multiple approximated layers.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 16,
      "context" : "’s work [17] is one of few existing works that present results of accelerating the whole model of VGG-16.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 8,
      "context" : "Accelerating VGG-16 for Object Detection Current state-of-the-art object detection methods [9], [2], [10] all exploit the VGG-16 model.",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 1,
      "context" : "Accelerating VGG-16 for Object Detection Current state-of-the-art object detection methods [9], [2], [10] all exploit the VGG-16 model.",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 9,
      "context" : "Accelerating VGG-16 for Object Detection Current state-of-the-art object detection methods [9], [2], [10] all exploit the VGG-16 model.",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 1,
      "context" : "Our method is based on the latest Fast R-CNN [2].",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 18,
      "context" : "We evaluate on the PASCAL VOC 2007 object detection benchmark [19].",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 1,
      "context" : "Note that unlike image classification where the conv layers dominate running time, for Fast R-CNN detection the conv layers consume about 70% actual running time [2].",
      "startOffset" : 162,
      "endOffset" : 165
    }, {
      "referenceID" : 1,
      "context" : "The detector is Fast R-CNN [2] using the pre-trained VGG-16 model.",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 6,
      "context" : "We believe this trade-off between accuracy and speed is of practical importance, because even with the recent advance of fast object detection [7], [9], the feature extraction running time is still considerable.",
      "startOffset" : 143,
      "endOffset" : 146
    }, {
      "referenceID" : 8,
      "context" : "We believe this trade-off between accuracy and speed is of practical importance, because even with the recent advance of fast object detection [7], [9], the feature extraction running time is still considerable.",
      "startOffset" : 148,
      "endOffset" : 151
    } ],
    "year" : 2015,
    "abstractText" : "This paper aims to accelerate the test-time computation of convolutional neural networks (CNNs), especially very deep CNNs [1] that have substantially impacted the computer vision community. Unlike existing methods that are designed for approximating linear filters or linear responses, our method takes the nonlinear units into account. We develop an effective solution to the resulting nonlinear optimization problem without the need of stochastic gradient descent (SGD). More importantly, while current methods mainly focus on optimizing one or two layers, our nonlinear method enables an asymmetric reconstruction that reduces the rapidly accumulated error when multiple (e.g., ≥10) layers are approximated. For the widely used very deep VGG-16 model [1], our method achieves a whole-model speedup of 4× with merely a 0.3% increase of top-5 error in ImageNet classification. Our 4× accelerated VGG-16 model also shows a graceful accuracy degradation for object detection when plugged into the latest Fast R-CNN detector [2].",
    "creator" : "LaTeX with hyperref package"
  }
}