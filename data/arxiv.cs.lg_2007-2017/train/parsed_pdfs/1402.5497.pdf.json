{
  "name" : "1402.5497.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Efficient Semidefinite Spectral Clustering via Lagrange Duality",
    "authors" : [ "Yan Yan", "Chunhua Shen", "Hanzi Wang" ],
    "emails" : [ "yanyan@xmu.edu.cn)", "chunhua.shen@adelaide.edu.au)", "hanzi.wang@xmu.edu.cn)" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms—Spectral clustering, Doubly stochastic normalization, Semidefinite programming, Lagrange duality.\nI. INTRODUCTION\nCLUSTERING is one of the most popular techniques forstatistical data analysis with various applications, including image analysis, pattern recognition, machine learning, and information retrieval [1]. The objective of clustering is to partition a data set into groups (called clusters) such that the data points in the same cluster are more similar than those in other clusters. Numerous clustering algorithms have been developed in the literature [1], such as k-means, single linkage, and fuzzy clustering.\nIn recent years, spectral clustering [2]–[16], a class of clustering algorithms based on the spectrum analysis of the affinity matrix, has emerged as an effective clustering technique. Compared with the traditional algorithms [1], such as k-means or single linkage, spectral clustering has many fundamental advantages. For example, it is easy to implement and reasonably fast, especially for large sparse matrices [17].\nSpectral clustering formulates clustering as a graph partitioning problem without estimating an explicit model of the data distribution. In general, a graph partitioning approach starts with a pairwise affinity matrix, which measures the\nY. Yan is with the School of Information Science and Technology, Xiamen University, Xiamen, 361005, China. (e-mail: yanyan@xmu.edu.cn)\nC. Shen is with the Australian Center for Visual Technologies, and School of Computer Science at The University of Adelaide, SA 5005, Australia. (email: chunhua.shen@adelaide.edu.au)\nH. Wang is with the School of Information Science and Technology, Xiamen University, Xiamen, 361005, China. (e-mail: hanzi.wang@xmu.edu.cn)\ndegree of similarity between data points, followed by a normalization step. Then, the leading eigenvectors of the normalized affinity matrix are extracted to perform dimensionality reduction for effective clustering in the lower dimensional subspace. Therefore, the three critical factors that affect the final performance of spectral clustering are [18], [19]: 1) the construction of the affinity matrix, 2) the normalization of the affinity matrix, and 3) the simple clustering algorithm, as shown in Fig. 1.\nThe purpose of the construction of the affinity matrix is to model the neighborhood relationship between data points. There are several popular ways [12] to construct the affinity matrix, such as the k-nearest neighbor graph and the fully connected graph. The normalization of the affinity matrix is achieved by finding the closest doubly stochastic matrix to the affinity matrix under a certain error measure [18]–[20], while the simple clustering algorithm (e.g. k-means) is used to partition an embedded coordinate system (formed by the principal k eigenvectors of the normalized affinity matrix) in an easier and simpler way. Empirical studies [19] indicate that the first two critical factors have a greater impact on the final clustering performance compared with the third critical factor (i.e., the simple clustering algorithm). In this paper, we mainly investigate the second critical factor − the normalization of the affinity matrix for effective spectral clustering.\nWe briefly review some related work [3]–[7] before presenting our work. Given a similarity graph with the affinity matrix, the simplest way to construct a partition of the graph is to solve the mincut problem [3], which aims to minimize the weights of edges (i.e., the summation of the similarity) between subgraphs. The mincut, however, usually leads to unsatisfactory clustering results due to an inexplicit limit for the size of the subgraph. To circumvent this problem,\nar X\niv :1\n40 2.\n54 97\nv1 [\ncs .L\nG ]\n2 2\nFe b\n20 14\nRatio-cut [4], [5] and Normalized-cut [7] are the two most common algorithms. In Ratio-cut [4], [5], the size of the subgraph is measured by the number of vertices, whereas, the size is measured by the weight of the edges attached to a subgraph in Normalized-cut. In essence, what Normalized-cut and Ratio-cut try to achieve is to balance the cuts between clusters. Unfortunately, the optimal solution to the above graph partitioning problems is NP hard. An effective approach is to consider the continuous relaxation versions of these problems [2], [7]–[10]. Minmax-cut was proposed in [8] and showed more balanced partitions than Normalized-cut and Ratio-cut. Nie et al. [13] applied an additional nonnegative constraint into Minmax-cut to obtain more accurate clustering results. Recently, a spectral embedding clustering framework [14] was developed to incorporate the linear property of the cluster assignment matrix.\nIn [18], [19], it has been shown that the key difference between Ratio-cut and Normalized-cut is the error measure used to find the closest doubly stochastic approximation of the input affinity matrix during the normalization step. When repeated, the Normalized-cut process converges to the global optimal solution under the relative entropy measure (also called the Kullback-Leibler divergence), while the L1 normalization leads to the Ratio-cut clustering. Zass et al. [19] developed a scheme for finding the optimal doubly stochastic matrix under the Frobenius norm. Experimental results [19] have demonstrated that the Frobenius normalization based spectral clustering achieves better performance on various standard data sets than the traditional normalization based algorithms, such as the L1 normalization and the relative entropy normalization based spectral clustering methods.\nThe main problem with the Frobenius normalization is that the positive semidefinite (p.s.d.) constraint is neglected during the normalization step, which makes the approximation to the affinity matrix less accurate. On the other hand, the Frobenius normalization with the p.s.d. constraint leads to a semidefinite programming (SDP) problem. Standard interior points based SDP solvers, however, have a complexity of approximately O(n6.5) for problems involving a matrix variable of size n × n and O(n) linear constraints. In the case of clustering, n is the number of data points and we can only solve the problem with a limited number of samples (a few hundred in our experiments). In other words, the complexity of the SDP solver limits the applications of the Frobenius normalization with the p.s.d. constraint. Therefore, in this paper, we focus on developing efficient algorithms to solve the Frobenius normalization with the p.s.d. constraint for spectral clustering, termed Semidefinite Spectral Clustering (SSC) hereafter.\nIn this paper, we present an efficient and effective algorithm, called LD-SSC, which exploits the Lagrange dual structure to solve the SSC problem. The proposed algorithm seeks a matrix that satisfies both the doubly stochastic and positive semidefinite constraints as closely as possible to the affinity matrix. The formulated optimization refers to the SDP problem which aims to optimize a convex function over the convex cone of symmetric and positive semidefinite matrices [21]. Therefore, the global optimal solution can be approached in the polynomial time. What is more, by exploring the\nLagrange dual form, we are able to apply off-the-shelf eigendecomposition and gradient descent methods (e.g., L-BFGSB [22]) to solve the SSC problem in a simple manner. Two versions of the proposed algorithm are given in this paper: one with less memory usage (LD-SSC1) and the other with faster convergence rate (LD-SSC2).\nOne of the main advantages of our algorithm is that, with the proposed formulation, we can solve the SSC problem in the Lagrange dual form very efficiently. Because the strong duality holds, we can recover the primal variable (i.e., the normalized affinity matrix) from the dual solution. Moreover, the computational complexity of the proposed algorithm is O(t ·n3), which is much lower than the traditional SDP solver O(n6.5). Here, t is the number of iterations for convergence. Typically t is around 300 (LD-SSC1) or t ≈ 10 ∼ 20 (LDSSC2) in our experiments. In summary, the main contributions of our work are summarized as follows:\n1) The proposed LD-SSC algorithm introduces the p.s.d. constraint into the normalization of the affinity matrix. Semidefinite spectral clustering finds a doubly stochastic and p.s.d. matrix that is the closest to the affinity matrix under the Frobenius norm. Improved clustering accuracy is achieved on various standard data sets. 2) We propose an efficient algorithm to semidefinite spectral clustering via Lagrange duality. Our algorithm is much more scalable than the standard SDP solvers. The importance of this development is that it allows us to apply semidefinite spectral clustering to large scale data clustering. Compared with the traditional SDP solvers, our proposed algorithm is significantly more efficient, especially when the number of data points is large.\nThe rest of the paper is organized as follows: Section II introduces the normalization of the affinity matrix. In Section III, the details of the proposed LD-SSC algorithm are presented. The performance of our algorithm is demonstrated and compared with other state-of-the-art algorithms in Section IV. Finally, we conclude our work in Section V."
    }, {
      "heading" : "II. NORMALIZATION OF THE AFFINITY MATRIX",
      "text" : "In this section, we briefly introduce the connection between kernel k-means and spectral clustering [18], [19], [23] and reveal the role of the normalization of the affinity matrix for spectral clustering. We begin by introducing the notation used in this paper.\nA matrix is denoted by a bold upper-case letter (X) and a column vector is denoted by a bold lower-case letter (x). The set of real M ×N matrices is denoted as RM×N . Let us denote the space of real matrices as S. Similarly, we denote the space of M ×M symmetric matrices by SM and positive semidefinite matrices by SM+ . For a matrix X ∈ SM , the following statements are equivalent: (1) X < 0 (X ∈ SM+ ); (2) All the eigenvalues of X are non-negative (λi(X) ≥ 0, i = 1, . . . ,M); and (3) ∀µ ∈ RM , µ>Xµ ≥ 0. The inner product defined on the SM is 〈A,B〉 = Tr(A>B). Here, Tr(·) is the trace of a symmetric matrix. ‖·‖F denotes the Frobenius\nnorm, which is defined as\n‖X‖2F = Tr(X>X) = M∑\ni, j=1\nx2i,j.\nGiven a symmetric matrix X ∈ SM , its eigen-decomposition is X = UΣU>. Here, U = [u1, . . . ,uM ] is an orthonormal matrix, and Σ = diag(λ1, . . . , λM ) is a diagonal matrix whose entries are the eigenvalues of U. We can explicitly express the positive part of X as:\nX+ = ∑ λi>0 (λiuiu > i ),\nand the negative part of X as: X− = ∑ λi<0 (λiuiu > i ).\nClearly, X = X+ + X− holds. Given a set of points {(a1, . . . ,an)|ai ∈ RM , i = 1, . . . , n}, we attempt to partition the n observations into k sets {C1, . . . , Ck} with nr points in Cr (r = 1, . . . , k). Let Ki,j = κ(ai,aj) be a symmetric positive semidefinite affinity function. Here the affinity function transforms the pairwise similarity or the pairwise distance into a graph. Thus, the clustering problem is converted to find a partition based on the affinity matrix as K = {Ki,j}. k-means is a standard clustering algorithm that partitions a data set into k clusters. However, a major disadvantage of the k-means algorithm is that it can only find linearly-separable clusters in the input space [23]. To overcome this disadvantage, the kernel k-means algorithm uses a function φ(x) to map the input vector into a possibly higher-dimensional feature space so that the clusters are linearly separable in the new space. The kernel k-means algorithm seeks to find the clusters so as to minimize the following objective function:\nk∑ r=1 ∑ ai∈Cr ‖φ(ai)−mr‖2, (1)\nwhere the function φ(ai) maps the input vector ai into a higher-dimensional feature space and mr = (1/|Cr|) ∑ ai∈Cr φ(ai) is the center of the r-th cluster with |Cr| = nr. After some algebraic manipulations, we can derive that minimizing (1) is equivalent to solve the following problem:\nmax C1,...,Ck k∑ r=1 1 nr ∑ (ai,aj)∈Cr κ(ai,aj), (2)\nwhere κ(ai,aj) = φ(ai)>φ(aj). Since Ki,j = κ(ai,aj), (2) can be converted into the following matrix form [18]:\nmax W\nTr(W>KW)\ns.t. W ≥ 0,WW>1 = 1,W>W = I, (3)\nwhere W ∈ Rn×k is the desired assignment matrix with wi,j = 1/ √ nj if i ∈ Cj , and wi,j = 0 otherwise. 1 is a column vector, all of whose components are ones. I is an\nidentity matrix, whose dimension is clear from the context. Hence, if we obtain a matrix W that maximizes Tr(W>KW) under the above constraints, we can find the solution to kernel k-means.\nOn the other hand, spectral clustering defines the twostage approach to the above problem (3). First, the normalized matrix K̂ of the input affinity matrix K is computed. Then, the spectral decomposition is used to find the solution as follows:\nmax W\nTr(W>K̂W)\ns.t. W>W = I, (4)\nwhose optimal solution is composed by the principal k eigenvectors of K̂. Typically, the eigenvectors form a new coordinate system in a k-dimensional subspace where the popular clustering approaches, such as k-means, are readily applicable. We refer to the process of transforming K to the normalized matrix K̂ as the normalization step.\nNext, we show that the normalization step in the spectral clustering algorithms, such as Normalized-cut and Ratio-cut, is to find a doubly stochastic matrix as closely as possible to the input affinity matrix under different error measures.\nLet F ∈ Sn be a square matrix with fi,j = 1/nr if (ai,aj) ∈ Cr, and fi,j = 0 otherwise. Here, if we arrange the data points according to the cluster membership, then F is a block diagonal matrix with the diagonal blocks F1, . . . ,Fk, where Fr = (1/nr)11>. Obviously, we have F = WW>. Based on (3), F satisfies the following constraints [24]:\nF ≥ 0,F1 = 1,F = F>. (5)\nHere F is called as the doubly stochastic matrix, which is a square matrix of nonnegative real numbers and the elements in whose rows and columns add up to 1.\nThe normalization with the form K̂ = D−1/2KD−1/2 where D = diag(K1) is used by Normalized-cut. In [18], it has been proved that for any non-negative symmetric matrix K(0), the iterative process K(t+1) = D−1/2K(t)D−1/2 with D = diag(K(t)1) converges to a doubly stochastic matrix under the relative entropy measure (using the symmetric version of the iterative proportional fitting procedure [24]). Alternatively, the closest doubly stochastic matrix under the L1 norm is K̂ = K −D + I which leads to Ratio-cut. Zass et al. [19] have shown that it is more natural to find the doubly stochastic matrix under the Frobenius error norm. The Frobenius normalization can be formulated as the following quadratic linear programming (QLP) problem:\nK̂ = arg min F ‖K− F‖2F\ns.t. F ≥ 0,F1 = 1,F = F>. (6)\nIn conclusion, kernel k-means and spectral clustering are closely connected where the normalization of the affinity matrix is related to the doubly stochastic constraint induced by kernel k-means."
    }, {
      "heading" : "III. SEMIDEFINITE SPECTRAL CLUSTERING",
      "text" : "In this section, we present an efficient algorithm with two versions, which aims at the effective normalization of the affinity matrix for semidefinite spectral clustering."
    }, {
      "heading" : "A. Frobenius Normalization with the P.S.D. Constraint",
      "text" : "Empirical studies [18], [19] have shown that the normalization of the affinity matrix K has significant effects on the final clustering results. Compared with the L1 normalization and the relative entropy normalization, the Frobenius normalization has been proved to be very practical and can significantly boost the clustering performance. In fact, it is natural to find a doubly stochastic approximation that satisfies the constraints (5) to K under the Frobenius norm, which is the extension of the common Euclidean vector norm ‖ · ‖2 for the matrix.\nA simple derivation yields that F is a p.s.d. matrix (i.e., F < 0) since F = WW> and ∀µ ∈ Rn, µ>Fµ = µ>WW>µ ≥ 0. However, the p.s.d. constraint is neglected during the normalization step in [19] due to the simplification of the computational complexity. Taking the p.s.d. constraint into consideration will make the doubly stochastic approximation to the affinity matrix more accurate. In other words, the doubly stochastic approximation should satisfy the p.s.d constraint. Therefore, it is desirable to find a doubly stochastic and p.s.d. matrix that approximates the affinity matrix as closely as possible under the error measure.\nThe proposed algorithm aims to seek a doubly stochastic and p.s.d. matrix under the Frobenius norm. The optimization problem can be written as follows:\nK̂ = arg min F ‖K− F‖2F\ns.t. F ≥ 0,F1 = 1,F = F>,F < 0, (7)\nwhere the first three constraints make the optimal solution be doubly stochastic while the last constraint forces the final matrix to be p.s.d..\nThe optimization problem (7) can be converted into an instance of semidefinite programming (SDP) where the matrix is required to be p.s.d., and then be solved by the standard solver packages directly. However, as we discussed earlier, general purpose SDP solvers [21] are computationally expensive and only small scale problems is applicable in a reasonable time. Thus, it is necessary to design an alternative algorithm that can greatly reduce the computational complexity while at the same time, achieving comparable performance. In the next subsection, an efficient algorithm to solve the above problem by exploiting the Lagrange dual problem of (7) is presented."
    }, {
      "heading" : "B. Semidefinite Spectral Clustering via Lagrange Duality",
      "text" : "The Lagrange duality takes the constraints in the primal form into consideration by augmenting the objective function with a weighted sum of the constraint functions. To derive the Lagrange dual of (7), we introduce the symmetric matrix Z and Q to associate with the p.s.d. constraint F < 0 and the non-negative constraint F ≥ 0, respectively. The two variables u1 and u2 associate with the equality constraints in the primal form. The Lagrangian of (7) is then written as:\n`( F︸︷︷︸ primal ,Z,Q,u1,u2︸ ︷︷ ︸ dual ) = 1 2 ‖K− F‖2F − 〈F,Q〉 − (F1− 1)>u1\n− (F>1− 1)>u2 − 〈F,Z〉 , (8)\nwith Z < 0 and Q ≥ 0.\nBecause F = F>, we have u1 = u2 = u. Based on the Karush-Kuhn-Tucker (KKT) optimality conditions [21], we minimize the Lagrangian over F which means that its gradient is set to zero and then we have ∂`(F,Z,Q,u)\n∂F = F−K−Q− u1> − 1u> − Z = 0. (9)\nTherefore, the connection between the primal and dual variables is given by\nF∗ = K + Q∗ + u∗1> + 1u∗> + Z∗. (10)\nBased on the above expression for F, the dual function is\ng(Z,Q,u) = inf F `(F,Z,Q,u)\n= 1\n2 ‖K− F‖2F − 〈F,Q〉 − 〈F,Z〉\n= 1\n2 ‖Z + Q + M‖2F − 〈F,Z + Q + M〉+ 21>u\n= −1 2 ‖Z + Q + M + K‖2F + 1 2 ‖K‖2F + 21>u, (11)\nwhere M = u1> + 1u>. The above equation is derived by using the fact that 〈 F,u1> 〉 = Tr(F1u>) = u>1 and〈\nF,1u> 〉\n= Tr(F1u>) = u>1. So, the dual formulation becomes\nmax Z,Q,u − 1 2 ‖Z + Q + M + K‖2F + 1 2 ‖K‖2F + 21>u\ns.t. Z < 0,Q ≥ 0. (12)\nBoth the primal and Lagrange dual problems are convex. Under mild conditions, the Slater’s condition holds, which means the strong duality between the primal and dual problems. It also implies that the duality gap is zero. As a result, we are able to indirectly solve the primal by solving the dual problem. In addition, the KKT optimality conditions (which are necessary and sufficient conditions for any pair of primal and dual optimal points of a convex problem) enable us to recover the primal variable from the dual solution in our case, as shown in (10).\nSince ‖K‖2F is a constant, (12) can be further simplified as\nmin Z,Q,u\n1 2 ‖Z + Q + M + K‖2F − 21>u\ns.t. Z < 0,Q ≥ 0. (13)\nProblem (13) still has the p.s.d. constraint and it is not immediately clear about how to solve the problem efficiently other than using off-the-shelf SDP solvers. One solution is coordinate ascent. By taking the idea of the cyclic coordinate ascent technique [25] (which seeks for the optimum of the objective function by repeatedly optimizing each of the coordinate directions), we can efficiently solve (13).\nIn particular, if we fix Q and u, the dual problem (13) becomes\nmin Z\n1 2 ‖Z + Q + M + K‖2F\ns.t. Z < 0. (14)\nIf we define a symbol P = − (Q + M + K) which is a function of Q and u, then (14) is to minimize ‖Z − P‖2F\nsuch that Z satisfies the p.s.d. constraint. It has a closed-form solution Z∗ = P+, where P+ is the positive part of P.\nHence, (13) is simplified into\nmin Q,u\n1 2 ‖P−‖2F − 21>u\ns.t. Q ≥ 0, (15)\nwhere P− = P−P+. Given a fixed Q, we can write the above optimization problem as follows:\nmin u\n1 2 ‖P−‖2F − 21>u, (16)\nwhere the objective function can be proved to be differentiable (see [26] for details). So, (16) can be easily solved by using a gradient descent method (e.g. L-BFGS-B [22]) since it does not have the matrix variables. L-BFGS-B is a limited-memory quasi-Newton algorithm for solving bound-constrained nonlinear optimization problems.\nOn the other hand, given fixed u and Z, (13) becomes\nmin Q\n1 2 ‖Q + Z + M + K‖2F\ns.t. Q ≥ 0. (17)\nProblem (17) has a closed-form solution which is Q∗ = thr≥0(−(Z + M + K)), Here, thr≥0(X) is an operator that zeros out all the negative entries of X.\nTo use L-BFGS-B, we need to implement the callback function of L-BFGS-B, which computes the gradient of the objective function of (16). The gradient of the dual problem (16) can be calculated as:\ng(ui) = −2− 〈 P−, T̂i 〉 , i = 1, . . . , n. (18)\nHere T̂i = Ti+T>i , where Ti is an n×n zero matrix except that all the elements in the i-th row are ones.\nIn summary, problem (13) can be solved by alternatively optimizing Z, Q and u, where in each iteration one variable is optimized while fixing all other variables. One version of the proposed algorithm is given in Algorithm 1 (denoted as LD-SSC1).\nIn (18), to obtain the gradient for each variable ui (i = 1, · · · , n), we need to compute the inner product between P− and T̂i at each iteration. Here, P− is a function of the variable u. Note that, the computation of P− involves the eigendecomposition and P− needs to be calculated to evaluate all the gradients at each iteration of L-BFGS-B. When the number of constraints is not far more than the number of data points, the eigen-decomposition dominates the computational complexity at each iteration. Therefore, the overall complexity of LD-SSC1 is O(t·n3). Here, t is the number of iterations for convergence (typically t ≈ 250 ∼ 1, 000 in our experiments); n is the number of data points. To be specific, the number of iterations for the inner loop (L-BFGS-B [22] is employed in our case in Step 2) is 5∼10, while the number of iterations for the outer loop (Step 2 to Step 4) is 50∼100.\nThe above optimization takes the dual variables into consideration individually, but (15) can also be directly solved by L-BFGS-B with the variables Q and u altogether, since we\nAlgorithm 1 LD-SSC1. Input: Given a set of points {(a1, . . . ,an)|ai ∈ RM , i =\n1, . . . , n} and the number k of clusters. Output: Clusters {C1, . . . , Ck} with nr points in Cr.\n1: Construct a similarity graph (e.g., k-nearest neighbor graph or fully connected graph) based on the given set; Initialize Q = I; 2: Optimize (16) to get u using L-BFGS-B with the gradient (18) of the objective function; 3: Calculate Z by using Z = P+, and Q by using Q = th≥0(X); 4: Go to Step 2 until the algorithm converges; 5: Obtain the final K̂ according to (10); 6: Compute the first k eigenvectors of K̂; 7: Cluster the points in the k-dimensional subspace using the\nsimple clustering algorithm.\nAlgorithm 2 LD-SSC2. Input: Given a set of points {(a1, . . . ,an)|ai ∈ RM , i =\n1, . . . , n} and the number k of clusters. Output: Clusters {C1, . . . , Ck} with nr points in Cr.\n1: Construct a similarity graph (e.g. k-nearest neighbor graph or fully connected graph) based on the given set; 2: Optimize (15) to get Q and u using L-BFGS-B with the gradients calculated by (18) and (19); 3: Calculate Z∗ by using Z∗ = P+ based on the outputs (i.e., Q∗ and u∗) of the optimization step in Step 2; 4: Obtain the final K̂ according to (10); 5: Compute the first k eigenvectors of K̂; 6: Cluster the points in the k-dimensional subspace using the\nsimple clustering algorithm.\ncan easily obtain the gradients of the objective function of (15) over Q and u, which are\ng(Q) = −P− (19)\nand (18), respectively. Thus, we have another efficient version to solve the SSC problem, which is given in Algorithm 2 (denoted as LD-SSC2).\nCompared with LD-SSC1, the main difference between LDSSC2 and LD-SSC1 is in that LD-SSC2 is more efficient since the outer loop is removed during the optimization. More specifically, when evaluating the gradients over Q and u at each iteration, both gradients need to compute the P− that involves the eigen-decomposition. Similar to LD-SSC1, the eigen-decomposition dominates the computational complexity of LD-SSC2. Therefore, the overall complexity of LD-SSC2 is O(t′ ·n3). Here, t′ is the number of iterations for convergence and typically t′ = 10 ∼ 20 in our experiments. Compared with LD-SSC1, the computational complexity of LD-SSC2 is lower. However, because the variables Q and u are jointly optimized, LD-SSC2 requires more memory usage than LDSSC1 during each iteration in the optimization step (cf. (15) and (16))."
    }, {
      "heading" : "C. Discussions",
      "text" : "There are a few important issues on the proposed LD-SSC1 and LD-SSC2 algorithms. • First, LD-SSC and the originial Frobenius normalization\nbased spectral clustering [19] are intrinsically different in the formulated optimization problems and hence the solutions are different. On one hand, the optimization problem in [19] is formulated as a quadratic programming problem. In contrast, our formulation is an SDP problem. Compared with the work in [19] that only tries to find a closet doubly stochastic matrix, the proposed LD-SSC emphasizes the importance of the p.s.d. property of the normalization matrix, which makes the approximation to the input affinity matrix more accurate. On the other hand, in [19], the Von-Neumann’s successive projections lemma [27] is applied to solve the quadratic problem. Our proposed LD-SSC, however, solves the SDP problem by exploiting the Lagrange dual form. • Second, Xing et al. [28] proposed the semidefinite relaxation for k Normalized-cut. Our purposed LD-SSC method is different from Xing et al. The SDP relaxation to k Normalized-cut gives only a tighter lower bound on the cut weight compared to the traditional spectral relaxation. In contrast, LD-SSC mainly focuses on the normalization step for solving the SSC problem. • Third, to deal with the large-scale SDP problem, LDSSC exploits the duality property. Several methods [29], [30] have also been proposed to solve large-scale SDP problems. For example, in [29], matrix factorization is used to approximate a large-scale SDP problem with a smaller one. Note that in our case, we exactly solve the original SDP problem. • Lastly, Luo et al. [16] developed a graph learning algorithm by solving a convex optimization problem with the low rank and p.s.d. constraints. Our algorithm and Luo et al.’s work present two different approaches to obtain a good normalization. An efficient algorithm based on augmented Lagrangian multiplier was proposed to attain the global optimum in [16], while LD-SSC takes advantages of the Lagrange duality property.\nNext, we test the proposed methods on various data sets."
    }, {
      "heading" : "IV. EXPERIMENTS",
      "text" : "In order to evaluate the proposed LD-SSC algorithm (two versions: i.e., LD-SSC1 and LD-SSC2), we conduct a set of clustering experiments across the popular data sets. The following subsections describe the details of the experiments and results."
    }, {
      "heading" : "A. Data Sets",
      "text" : "We use several well-studied data sets from the UCI machine learning repository1 (including SPECTF heart, Wine, Pima, Hayes-Roth, Iris, and BUPA), the cancer data sets2 (including Leukemia and Lung), two public face data sets (including\n1UCI Repository: http://www.ics.uci.edu/∼mlearn/MLRepository.html 2Cancer Data Sets: http://datam.i2r.a-star.edu.sg/datasets/krbd/\nORL face database3 and Yale face database4), and two object image data sets (including COIL-20 [32]5 and the handwritten binary Alphadigits data set6), respectively. The UCI repository is well established and widely used for benchmarking different clustering algorithms. The cancer data sets are the challenging benchmark in the cancer community. The last four data sets are the commonly used real-world image data sets. ORL exhibits the variations in facial expressions and poses while Yale shows various lighting conditions; COIL-20 contains the variations in the viewpoint of objects and the Alphadigits data set exhibits the variations in the shape of handwritten digits and letters. Sample images from the last four image data sets are shown in Fig. 2. Table I summarizes the detailed information and kernel settings for all the data sets."
    }, {
      "heading" : "B. Parameter Settings and Evaluation Metric",
      "text" : "In this section, we evaluate the multi-class spectral clustering described in [31] which iteratively solves a discrete solution by using an alternating optimization procedure taking the k principal eigenvectors. Other methods (such as [2]) can also be used, but these methods give similar results [19]. Hence, we employ the framework of [31] while replacing different normalization algorithms in our experiments. Note that the results of all the clustering algorithms depend on the initialization. To reduce statistical variation, we repeat all the clustering algorithms for 10 times with random initialization, and report the results corresponding to the best objective values (similar to [31]).\nTwo types of kernels used to construct the affinity matrix are the Gaussian kernel and the polynomial kernel. The similarity function for the Gaussian kernel can be written as Ki,j = exp−‖ai−aj‖\n2/δ2 , where the parameter δ controls the width of the neighborhoods. The similarity function for the polynomial kernel is Ki,j = (a>i aj+1)\nd, where the parameter d represents the degree of the polynomial that affects the shape of the curve. In this paper, in order to achieve the best performance for clustering, the kernel type and kernel parameter are manually chosen for each data set.\nFor the UCI repository and the cancer data sets, the extracted features are available in the data sets. In contrast, the face data sets, ORL and Yale, are only provided with the raw images. ORL has 40 subjects, where each subject contains 10 images with the size 92×112. Yale has 11 images for each of\n3ORL: http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html 4Yale: http://cvc.yale.edu/projects/yalefaces/yalefaces.html 5COIL-20: http://www.cs.columbia.edu/CAVE/software/softlib/coil-20.php 6Alphadigits: http://cs.nyu.edu/∼roweis/data/binaryalphadigs.mat\n15 subjects. All the images in Yale are normalized to 128×128 [35]. For simplicity, we extract the feature vector of each face image by lexicographic ordering of the pixel elements of the image. Note that the feature vectors from the cancer data sets and the face data sets are both high-dimensional data, as shown in Table I. To effectively perform spectral clustering, the dimensionality reduction technique is used for preprocessing. In this paper, we use the principal component analysis (PCA) [34] to perform dimensionality reduction. Other sophisticated dimensionality reduction algorithms can also be applied.\nThe COIL-20 data set [32] has 1, 440 images of 20 object categories. Each category contains 72 images. All the images are normalized to 32×32 pixel arrays with 256 gray levels per pixel and then transformed to a 1,024-D feature vector. The binary handwritten Alphadigits data set contains the binary handwritten digits and capital letters. There are 36 classes (including digits of ’0’ through ’9’ and capital letters of ’A’ through ’Z’). Each class has 39 samples, and each binary image is 20×16 in resolution, which results in a 320-D feature vector.\nWe implement the proposed algorithm in Matlab and the L-BFGS-B part is in Fortran and Matlab interface. All the\ncomputational time is reported on a desktop with Intel i7 (2.20GHz) CPUs and 4.00 GB RAM.\nThe clustering performance is evaluated by the error rate. Given that ri and si are the obtained cluster label and the ground truth label, respectively, the error rate is defined as follows [33]:\nError Rate = 1− ∑n i δ(si,map(ri))\nn ,\nwhere n is the total number of data points; δ(a, b) is the delta function that equals to one if a = b and zero otherwise. The function map(ri) is the permutation mapping function that maps the cluster label ri to the ground truth label. We choose the lowest error rate [19] and the mean error rate across different kernel parameters (δ in the Gaussian kernel and d in the polynomial kernel) as the evaluation metric in our experiments."
    }, {
      "heading" : "C. Comparisons with State-of-the-art Algorithms",
      "text" : "We perform a comparison between the proposed LD-SSC1, LD-SSC2 and spectral clustering with three different stateof-the-art normalization algorithms, including: 1) Normalizedcut (NC) [7], [18] which is based on the relative entropy\nnormalization; 2) Ratio-cut (RC) [4] which is based on the L1 normalization; and 3) the Frobenius normalization based spectral clustering (FSC) [19]. To show the importance of the normalization step, we also give the clustering results when no normalization (NO) is applied.\nTables II and III show the comparison results (including the lowest error rate and mean error rate) obtained by the competing algorithms on various data sets. The proposed LDSSC1 and LD-SSC2 give better or comparable results against the state-of-the-art algorithms like NO, RC, NC, and FSC in terms of the lowest error rate and mean error rate. LD-SSC1 and LD-SSC2 have achieved similar performance since both algorithms optimizes the same objective function. It is worth noting that the algorithms, such as NC or RC, can worsen the performance of clustering compared with that without the normalization step for some data sets (e.g. Iris, BUPA, Yale, COIL-20, and Alphadigits). The reason may be that the L1 norm or the relative entropy measure are not good error measures for the normalization in these data sets.\nFig. 3 shows the clustering performance of different algorithms on all the data sets under varying kernel paramters (δ or d). From the experimental results, we can see that LD-SSC1 and LD-SSC2 consistently outperform the other algorithms for most data sets on the UCI repository. FSC, LD-SSC1, and LD-SSC2 obtain the similar error rate in Iris and BUPA. Some algorithms, such as NO and RC, show great variations in the error rate when the kernel parameter changes, especially for Iris and BUPA. Improved clustering results are achieved by LD-SSC1 and LD-SSC2 for Leukemia and Lung. FSC, however, has achieved a high error rate with some parameters for Lung. LD-SSC1 and LD-SSC2 outperform the other algorithms on the ORL database while FSC, LD-SSC1 and LD-SSC2 achieve similar performance on the Yale database. For the COIL-20 and Alphadigits data sets, both LD-SSC1 and LD-SSC2 obtain the lowest error rate and mean error rate by about 10% ∼ 15% less compared with the other algorithms. Again, we observe that the L1 normalization or the relative entropy normalization degrade the performance of the algorithms when it is compared with that without normalization for some data sets. However, the Frobenius normalization based algorithms (FSC, LD-SSC1, and LD-SSC2) can boost the performance in most data sets. LD-SS1 and LD-SS2 achieve very close performance in most cases.\nIn Fig. 4, we plot the embedded results (formed by the two principal eigenvectors of the normalized affinity matrix) obtained by the different clustering algorithms for the cancer data sets (i.e., Leukemia and Lung) given the fixed kernel parameter (d = 2). The first two principal components (PCs) of the cancer data after the PCA preprocessing step are also given for comparison. The ground truth distribution of the two classes are plotted with different colors. Ideally, after the normalization step, the points in the low-dimensional subspace with higher similarity form closer cluster and the points with lower similarity are far apart from each other. Thus, the data points can be partitioned in an easier and simpler way. For example, most algorithms show improved data distribution results after the normalization step for the\nlung data set. However, we find that RC (for Leukemia and Lung) or FSC (for Leukemia) have some outliers after normalization, which makes the following clustering more difficult. NC mixes the two classes together after the normalization step for the Leukemia data set. LD-SSC1 and LD-SSC2 have better clustering results for the Leukemia data set, while the performance of these two methods for the Lung data set seems to be not very separable. This is due to the use of only the first two PCs. However, from Fig. 4, it shows that LD-SSC1 and LD-SSC2 try to align all the points in a line so that the two classes are well separated, which is similar to the idea of the linear discriminant analysis (LDA) [35] for the two-classes case in the supervised learning.\nClustering results show that in most cases, the Frobenius normalization with the p.s.d. constraint (i.e., LD-SSC1 and LD-SSC2) achieves better results than that without the p.s.d. constraint (i.e. FSC) on various data sets. To further demonstrate that the p.s.d. constraint is necessary for more accurate doubly stochastic approximation, we show the comparison of the obtained normalized affinity matrix between FSC and LD-SSC1 on COIL-20 and Alphadigits (given the fixed kernel parameter) in Fig. 5. For convenience, we only show four classes from the two data sets. The original affinity matrix has four dense clusters with overlaps between them. However, after the normalization step, the overlap between clusters are greatly reduced (see Fig. 5(c) and Fig. 5(f)). The connections between different clusters are suppressed while the connections within the same clusters are enhanced, which has a similar effect as the aggregation network [36]. LD-SSC1 (LD-SSC2) obtains a better normalized affinity matrix than FSC. Therefore, by taking the p.s.d. constraint into account, the doubly stochastic approximation to the affinity matrix is more accurate for the Frobenius normalization, which results in better clustering performance.\nIn summary, experimental results on various data sets show that a good error measure during the normalization can influence the final clustering results. The Frobenius norm is more natural than other error measures, and the proposed LDSSC1 and LD-SSC2 achieve better clustering results compared to the state-of-the-art algorithms in most cases. The error rate varies with different kernel parameters which controls the intrinsic affinity relationships between data points. In short, LD-SSC1 and LD-SSC2 are more stable than the other competing algorithms in terms of the error rate when the kernel parameter (δ in the Gaussian kernel or d in the polynomial kernel) changes."
    }, {
      "heading" : "D. Computational Complexity",
      "text" : "The optimization problem of SSC is a semidefinite programming (SDP) problem, which allows us to use off-the-shelf SDP solvers. To show the efficiency of the proposed algorithm, we also compare the computational time between our scalable LD-SSC algorithm (LD-SSC1 and LD-SSC2) and the SSC using CVX (CVX-SSC) [37], which is a standard package for convex optimization. There are two solvers provided in CVX - SeDumi and SDPT3. We find that the SDPT3 solver is faster than the SeDumi solver for the SDP problem. Therefore, we use CVX with the SDPT3 solver for comparison. Note\nthat, the running time of FSC is not reported. In general, it is much faster than our methods. Because our methods solve much more complicated optimization problems due to the introduction of the p.s.d. constraint.\nFirst, we show the obtained results on synthetic toy data. We randomly generate two classes according to different Gaussian distributions (different means and covariance matrices). Fig. 6(a) shows the computational time with different numbers of samples (from 50 to 1,000). Note that when the number of samples exceeds 200, CVX-SSC halts due to the memory limits in Matlab. Therefore, we only report the computational time when the number of samples is smaller than 200 for CVX-SSC. In contrast, the proposed scalable LD-SSC1 and LD-SSC2 can deal with more than 1,000 samples for cluster-\ning. For the large scale clustering, the general purpose SDP solvers are not viable, but our scalable algorithm is applicable by exploiting the dual form of the SSC problem.\nThen, we show the clustering results on a real data set. Fig.\n6(b) gives the computational time on the Yale face database with different numbers of samples for clustering, using CVXSSC and the proposed algorithm. It is obvious to observe that the proposed LD-SSC1 and LD-SSC2 are much faster than CVX-SSC. LD-SSC1 and LD-SSC2, which use the special structure in the dual form, achieve a higher efficiency than the general-purpose SDP solver.\nFinally, Table IV gives a comparison of the lowest error rate and the corresponding computational time of CVX-SSC for all the data sets given the fixed kernel parameters. Note that all the computational time of CVX-SSC for SPECTF, Pima, BUPA, ORL, COIL-20, and Alphadigits data sets is not shown, because the number of samples in these data sets exceeds 200, which makes CVX-SSC not applicable. CVXSSC, LD-SSC1 and LD-SSC2 have similar error rate for most data sets. However, the proposed LD-SSC1 and LD-SSC2 are much more efficient than CVX-SSC. In Table V, we also report the memory usage of all the parameters for L-BFGS-B which consumes the majority of the computational time in LD-SSC1 and LD-SSC2. LD-SSC2 has a faster convergence rate, but requires more memory than LD-SSC1 at each iteration in L-BFGS-B. This is due to the fact that LD-SSC2 finds the solution of Q and u, but LD-SSC1 only obtains the solution of u in L-BFGS-B.\nE. Image Segmentation Results\nIn this subsection, we explore the application of the proposed algorithm and show real image segmentation with our and other competing clustering algorithms. The framework of [31] is used to perform different clustering algorithms for real image segmentation. Images are convolved with oriented filter pairs to extract the magnitude of edge responses. The pixel affinity matrix K is measured based on the maximum magnitude of edges across the line between two pixels [38]. For convenience, we resize all the images to the size of 40×40. In all the segmentation experiments, the number of classes k is manually chosen (k = 4 in our experiments). Similar to the previous experiments, the only difference between these image segmentation approaches is the normalization algorithm used.\nFig. 7 shows the image segmentation results on a set of face images. We observe that LD-SSC1 and LD-SSC2 outperform NO, NC, RC, and FSC in most cases. Both LD-SSC1 and LDSSC2 are able to accurately locate the boundary of the object and remove small segmentation patches. FSC, however, has the over-segmention problem at the area inside the object. In constrast, semidefinite spectral clustering via Lagrange duality yields more accurate segmentation results than the traditional spectral clustering algorithms."
    }, {
      "heading" : "V. CONCLUSION AND DISCUSSION",
      "text" : "Normalization of the affinity matrix is a crucial factor for spectral clustering. Existing normalization algorithms can be considered as the doubly stochastic approximation to the affinity matrix under different error measures. In this paper, an efficient and scalable normalization algorithm with two versions (i.e., LD-SSC1 and LD-SSC2) for semidefinite spectral clustering is presented. The two versions are equivalent for the SSC problem but differ only in their optimization step, where LD-SSC1 requires less memory usage while LDSSC2 has faster convergence rate. We show that it is more desirable to have the doubly stochastic constraint as well as the p.s.d. constraint during the normalization step.\nThe proposed LD-SSC1 and LD-SSC2 are simpler and much more scalable than the standard interior-point based SDP solvers. The key to our algorithm is to exploit the Lagrange dual form by using the structure of the optimization problem. Experimental results on various data sets have shown the importance of the normalization and the p.s.d. constraint to the final performance of clustering. The proposed algorithm achieves better performance than the state-of-the-art algorithms in most data sets. We also observe that the L1 normalization (Ratio-cut) or the Relative entropy normalization (Normalized-cut) can sometimes degrade the clustering performance in some data sets compared with the case without\nnormalization. On the contrary, the proposed LD-SSC1 and LD-SSC2 improve the clustering performance in most cases."
    } ],
    "references" : [ {
      "title" : "Data clustering: a review",
      "author" : [ "A.K. Jain", "M. Murty", "P.J. Flynn" ],
      "venue" : "ACM Comput. Surv., vol. 31, no. 3, pp. 264–323, 1999.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "On spectral clustering: analysis and algorithm",
      "author" : [ "A.Y. Ng", "M.I. Jordan", "Y. Weiss" ],
      "venue" : "Proc. Adv. Neural Inf. Process. Syst., Vancouver, B. C., Canada, 2001, pp. 849–856.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Partitioning sparse matrices with eigenvectors of graph",
      "author" : [ "A. Pothen", "H.D. Simon", "K.P. Liou" ],
      "venue" : "SIAM Journal of Matrix Anal. Appl., vol. 11, pp. 430–452, 1990.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "New spectral methods for ratio cut partioning and clustering",
      "author" : [ "L. Hagen", "A. Kahng" ],
      "venue" : "IEEE Trans. Comput.-Aided Des. Integr. Circuits Syst., vol. 11, no. 9. pp. 1074–1085, 1992.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Spectral k-way ratio-cut partitioning and clustering",
      "author" : [ "P.K. Chan", "M.D.F. Schlag", "J.Y. Zien" ],
      "venue" : "IEEE Trans. Comput.-Aided Des. Integr. Circuits Syst., vol. 13, no. 9. pp. 1088–1096, 1994.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Spectral graph theory",
      "author" : [ "F. Chung" ],
      "venue" : "AMS publication, 1997.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Normalized cuts and image segmentation",
      "author" : [ "J. Shi", "J. Malik" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell., vol. 22, no. 8, pp. 888–905, 2000.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "A min-max cut algorithm for graph partitioning and data clustering",
      "author" : [ "C. Ding", "X. He", "H. Zha", "M. Gu", "H. Simon" ],
      "venue" : "Proc. IEEE Int. Conf. Data Mining, Washington, D.C., USA, 2001, pp. 107–114.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Learning spectral clustering with application to speech separation",
      "author" : [ "F.R. Bach", "M.I. Jordan" ],
      "venue" : "J. Mach. Learn. Research, vol. 7, pp. 1963–2001, 2006.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1963
    }, {
      "title" : "Laplacian eigenmaps and spectral techniques for embedding and clustering",
      "author" : [ "M. Belkin", "P. Niyogi" ],
      "venue" : "Proc. Adv. Neural Inf. Process. Syst., Vancouver, B. C., Canada, 2002, pp. 585–591.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Image clustering using local discriminant models and global integration",
      "author" : [ "Y. Yang", "D. Xu", "F.P. Nie", "S.C. Yan", "Y.T. Zhuang" ],
      "venue" : "IEEE Trans. Image Process., vol. 19, no. 10, pp. 2761–2772, 2010.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A tutorial on spectral clustering",
      "author" : [ "C. Ding" ],
      "venue" : "Talk presented at ICML 2004. (slides available at http://ranger.uta.edu/ chqding/Spectral/)",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Improved minmax cut graph clustering with nonnegative relaxation",
      "author" : [ "F. Nie", "C. Ding", "D. Luo", "H. Huang" ],
      "venue" : "Proc. European Conf. on Mach. Learn. / Principles and Pract. of Know. Discov. in Databases, Barcelona, Spain, 2010, pp. 451–466",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Spectral embedded clustering: a framework for in-sample and out-of-sample spectral clustering",
      "author" : [ "F. Nie", "Z. Zeng", "I.W. Tsang", "D. Xu", "C. Zhang" ],
      "venue" : "IEEE Trans. Neural Netw., vol. 22, no. 11, pp. 1796-1808, 2011.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1808
    }, {
      "title" : "Consensus spectral clustering in nearlinear time",
      "author" : [ "D. Luo", "C. Ding", "H. Huang" ],
      "venue" : "Proc. IEEE Conf. Data Eng., Hannover, Germany, pp. 1079-1090, 2011.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Forging the graphs: a low rank and positive semidefinite graph learning approach",
      "author" : [ "D. Luo", "C. Ding", "H. Huang", "F. Nie" ],
      "venue" : "Proc. Adv. Neural Inf. Process. Syst., Lake Tahoe, Nevada, USA, 2012, pp. 2969–2977,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Parallel spectral clustering in distributed systems",
      "author" : [ "W.Y. Chen", "Y. Song", "H. Bai", "C.J. Lin", "E.Y. Chang" ],
      "venue" : "IEEE Pattern Anal. Mach. Intell., vol. 33, no. 3, pp. 568–586, 2011.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A unifying approach to hard and probabilistic clustering",
      "author" : [ "R. Zass", "A. Shashua" ],
      "venue" : "Proc. IEEE Int. Conf. Comp. Vis., Beijing, China, 2005, pp. 294–301.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Doubly stochastic normalization for spectral clustering",
      "author" : [ "R. Zass", "A. Shashua" ],
      "venue" : "Proc. Adv. Neural Inf. Process. Syst., Vancouver, B. C., Canada, 2006, pp. 1569–1576.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Robust multi-class transductive learning with graphs",
      "author" : [ "W. Liu", "S. Chang" ],
      "venue" : "Proc. IEEE Conf. Comp. Vis. Patt. Recogn.,Miami, Florida, USA, 2009, pp. 381–388.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "L-BFGS-B: Algorithm 778: L-BFGS-B, FORTRAN routines for large scale bound constrained optimization",
      "author" : [ "C. Zhu", "R.H. Byrd", "J. Nocedal" ],
      "venue" : "ACM Trans. Mathematical Software, vol. 23, no. 4, pp. 550–560, 1997.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Kernel k-means, spectral clustering and normalized cuts",
      "author" : [ "I.S. Dhillon", "Y. Guan", "B. Kulis" ],
      "venue" : "Proc. ACM Int. Conf. Knowledge & Discovery Data Mining, Seattle, USA, 2004, pp. 555–556.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Concerning non-negative matrices and doubly stochastic matrices",
      "author" : [ "R. Sinkhorn", "P. Knopp" ],
      "venue" : "Pacific J. Math., vol. 21, pp. 343–348, 1967.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 1967
    }, {
      "title" : "A convex programming procedure",
      "author" : [ "D D’Esopo." ],
      "venue" : "Naval Logistics Quarterly, vol. 6, no. 1, pp. 33–42, 1959.  YAN et al.: EFFICIENT SEMIDEFINITE SPECTRAL CLUSTERING VIA LAGRANGE DUALITY  13",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 1959
    }, {
      "title" : "Functional Operators Vol. II",
      "author" : [ "J. Von Neumann" ],
      "venue" : "Princeton University Press, 1950.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 1950
    }, {
      "title" : "On semidefinite relaxation for normalized k-cut and connections to spectral clustering",
      "author" : [ "E.P. Xing", "M.I. Jordan" ],
      "venue" : "Division of Computer Science, University of California, Berkeley, Technical Report CSD-03- 1265, 2003.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Graph Laplacian regularization for large-scale semidefinite programming",
      "author" : [ "K.Q. Weinberger", "F. Sha", "Q. Zhu", "L. Saul" ],
      "venue" : "Proc. Adv. Neural Inf. Process. Syst., Vancouver, B. C., Canada, pp. 1489–1496, 2007.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "A distributed SDP approach for largescale noisy anchor-free graph realization with applications to molecular conformation",
      "author" : [ "P. Biswas", "K. Toh", "Y. Ye" ],
      "venue" : "SIAM Journal on Scien. Comput., vol. 30, no. 3, pp. 1251–1277, 2008.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Multiclass spectral clsutering",
      "author" : [ "S.X. Yu", "J. Shi" ],
      "venue" : "Proc. IEEE Int. Conf. Comp. Vis., Pittsburgh, PA, USA, 2003, pp. 313–319.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Columbia object image library (COIL-20)",
      "author" : [ "S.A. Nene", "S.K. Nayar", "H. Murase" ],
      "venue" : "Columbia Univ., Technical Report CUCS-006-96, Feb. 1996.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Document clustering using locality preserving indexing",
      "author" : [ "D. Cai", "X.F. He", "J. Han" ],
      "venue" : "IEEE Trans. Knowl. Data Eng., vol. 17, no. 12, pp. 1624–1637, 2005.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Eigenfaces for recognition",
      "author" : [ "M. Turk", "A. Pentland" ],
      "venue" : "J. Cogn. Neurosci., vol. 3, no. 1, pp. 71–86, 1991.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Eigenfaces vs. Fisherfaces: recognition using class specific linear projection",
      "author" : [ "P.N. Belhumeur", "J.P. Hepanha", "D.J. Kriegman" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell., vol. 19, no. 7, pp. 711–720, 1997.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Document retrieval and clustering: from principal component analysis to self-aggregation networks",
      "author" : [ "C. Ding" ],
      "venue" : "Proc. Int. Workshop on Artificial Intelligence and Statistics, Key West, Florida, 2003.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "CVX: MATLAB software for disciplined convex programming. Version 1.22",
      "author" : [ "M. Grant", "S. Boyd" ],
      "venue" : "http://cvxr.com/cvx/",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 0
    }, {
      "title" : "Computational models of perceptual organization",
      "author" : [ "S.X. Yu" ],
      "venue" : "Ph.D. thesis, Carnegie Mellon University, May 2003.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "INTRODUCTION CLUSTERING is one of the most popular techniques for statistical data analysis with various applications, including image analysis, pattern recognition, machine learning, and information retrieval [1].",
      "startOffset" : 210,
      "endOffset" : 213
    }, {
      "referenceID" : 0,
      "context" : "Numerous clustering algorithms have been developed in the literature [1], such as k-means, single linkage, and fuzzy clustering.",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 1,
      "context" : "In recent years, spectral clustering [2]–[16], a class of clustering algorithms based on the spectrum analysis of the affinity matrix, has emerged as an effective clustering technique.",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 15,
      "context" : "In recent years, spectral clustering [2]–[16], a class of clustering algorithms based on the spectrum analysis of the affinity matrix, has emerged as an effective clustering technique.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 0,
      "context" : "Compared with the traditional algorithms [1], such as k-means or single linkage, spectral clustering has many fundamental advantages.",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 16,
      "context" : "For example, it is easy to implement and reasonably fast, especially for large sparse matrices [17].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 17,
      "context" : "Therefore, the three critical factors that affect the final performance of spectral clustering are [18], [19]: 1) the construction of the affinity matrix, 2) the normalization of the affinity matrix, and 3) the simple clustering algorithm, as shown in Fig.",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 18,
      "context" : "Therefore, the three critical factors that affect the final performance of spectral clustering are [18], [19]: 1) the construction of the affinity matrix, 2) the normalization of the affinity matrix, and 3) the simple clustering algorithm, as shown in Fig.",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 11,
      "context" : "There are several popular ways [12] to construct the affinity matrix, such as the k-nearest neighbor graph and the fully connected graph.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 17,
      "context" : "The normalization of the affinity matrix is achieved by finding the closest doubly stochastic matrix to the affinity matrix under a certain error measure [18]–[20], while the simple clustering algorithm (e.",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 19,
      "context" : "The normalization of the affinity matrix is achieved by finding the closest doubly stochastic matrix to the affinity matrix under a certain error measure [18]–[20], while the simple clustering algorithm (e.",
      "startOffset" : 159,
      "endOffset" : 163
    }, {
      "referenceID" : 18,
      "context" : "Empirical studies [19] indicate that the first two critical factors have a greater impact on the final clustering performance compared with the third critical factor (i.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 2,
      "context" : "We briefly review some related work [3]–[7] before presenting our work.",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 6,
      "context" : "We briefly review some related work [3]–[7] before presenting our work.",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 2,
      "context" : "Given a similarity graph with the affinity matrix, the simplest way to construct a partition of the graph is to solve the mincut problem [3], which aims to minimize the weights of edges (i.",
      "startOffset" : 137,
      "endOffset" : 140
    }, {
      "referenceID" : 3,
      "context" : "Ratio-cut [4], [5] and Normalized-cut [7] are the two most common algorithms.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 4,
      "context" : "Ratio-cut [4], [5] and Normalized-cut [7] are the two most common algorithms.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 6,
      "context" : "Ratio-cut [4], [5] and Normalized-cut [7] are the two most common algorithms.",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 3,
      "context" : "In Ratio-cut [4], [5], the size of the subgraph is measured by the number of vertices, whereas, the size is measured by the weight of the edges attached to a subgraph in Normalized-cut.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 4,
      "context" : "In Ratio-cut [4], [5], the size of the subgraph is measured by the number of vertices, whereas, the size is measured by the weight of the edges attached to a subgraph in Normalized-cut.",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 1,
      "context" : "An effective approach is to consider the continuous relaxation versions of these problems [2], [7]–[10].",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 6,
      "context" : "An effective approach is to consider the continuous relaxation versions of these problems [2], [7]–[10].",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 9,
      "context" : "An effective approach is to consider the continuous relaxation versions of these problems [2], [7]–[10].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 7,
      "context" : "Minmax-cut was proposed in [8] and showed more balanced partitions than Normalized-cut and Ratio-cut.",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 12,
      "context" : "[13] applied an additional nonnegative constraint into Minmax-cut to obtain more accurate clustering results.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "Recently, a spectral embedding clustering framework [14] was developed to incorporate the linear property of the cluster assignment matrix.",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 17,
      "context" : "In [18], [19], it has been shown that the key difference between Ratio-cut and Normalized-cut is the error measure used to find the closest doubly stochastic approximation of the input affinity matrix during the normalization step.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 18,
      "context" : "In [18], [19], it has been shown that the key difference between Ratio-cut and Normalized-cut is the error measure used to find the closest doubly stochastic approximation of the input affinity matrix during the normalization step.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 18,
      "context" : "[19] developed a scheme for finding the optimal doubly stochastic matrix under the Frobenius norm.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "Experimental results [19] have demonstrated that the Frobenius normalization based spectral clustering achieves better performance on various standard data sets than the traditional normalization based algorithms, such as the L1 normalization and the relative entropy normalization based spectral clustering methods.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 20,
      "context" : ", L-BFGSB [22]) to solve the SSC problem in a simple manner.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 17,
      "context" : "In this section, we briefly introduce the connection between kernel k-means and spectral clustering [18], [19], [23] and reveal the role of the normalization of the affinity matrix for spectral clustering.",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 18,
      "context" : "In this section, we briefly introduce the connection between kernel k-means and spectral clustering [18], [19], [23] and reveal the role of the normalization of the affinity matrix for spectral clustering.",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 21,
      "context" : "In this section, we briefly introduce the connection between kernel k-means and spectral clustering [18], [19], [23] and reveal the role of the normalization of the affinity matrix for spectral clustering.",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 21,
      "context" : "However, a major disadvantage of the k-means algorithm is that it can only find linearly-separable clusters in the input space [23].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 17,
      "context" : "Since Ki,j = κ(ai,aj), (2) can be converted into the following matrix form [18]:",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 22,
      "context" : "Based on (3), F satisfies the following constraints [24]: F ≥ 0,F1 = 1,F = F>.",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 17,
      "context" : "In [18], it has been proved that for any non-negative symmetric matrix K, the iterative process K = D−1/2K(t)D−1/2 with D = diag(K1) converges to a doubly stochastic matrix under the relative entropy measure (using the symmetric version of the iterative proportional fitting procedure [24]).",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 22,
      "context" : "In [18], it has been proved that for any non-negative symmetric matrix K, the iterative process K = D−1/2K(t)D−1/2 with D = diag(K1) converges to a doubly stochastic matrix under the relative entropy measure (using the symmetric version of the iterative proportional fitting procedure [24]).",
      "startOffset" : 285,
      "endOffset" : 289
    }, {
      "referenceID" : 18,
      "context" : "[19] have shown that it is more natural to find the doubly stochastic matrix under the Frobenius error norm.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "Constraint Empirical studies [18], [19] have shown that the normalization of the affinity matrix K has significant effects on the final clustering results.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 18,
      "context" : "Constraint Empirical studies [18], [19] have shown that the normalization of the affinity matrix K has significant effects on the final clustering results.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 18,
      "context" : "constraint is neglected during the normalization step in [19] due to the simplification of the computational complexity.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 23,
      "context" : "By taking the idea of the cyclic coordinate ascent technique [25] (which seeks for the optimum of the objective function by repeatedly optimizing each of the coordinate directions), we can efficiently solve (13).",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 20,
      "context" : "L-BFGS-B [22]) since it does not have the matrix variables.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 20,
      "context" : "To be specific, the number of iterations for the inner loop (L-BFGS-B [22] is employed in our case in Step 2) is 5∼10, while the number of iterations for the outer loop (Step 2 to Step 4) is 50∼100.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 18,
      "context" : "• First, LD-SSC and the originial Frobenius normalization based spectral clustering [19] are intrinsically different in the formulated optimization problems and hence the solutions are different.",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 18,
      "context" : "On one hand, the optimization problem in [19] is formulated as a quadratic programming problem.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 18,
      "context" : "Compared with the work in [19] that only tries to find a closet doubly stochastic matrix, the proposed LD-SSC emphasizes the importance of the p.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 18,
      "context" : "On the other hand, in [19], the Von-Neumann’s successive projections lemma [27] is applied to solve the quadratic problem.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 24,
      "context" : "On the other hand, in [19], the Von-Neumann’s successive projections lemma [27] is applied to solve the quadratic problem.",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 25,
      "context" : "[28] proposed the semidefinite relaxation for k Normalized-cut.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 26,
      "context" : "Several methods [29], [30] have also been proposed to solve large-scale SDP problems.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 27,
      "context" : "Several methods [29], [30] have also been proposed to solve large-scale SDP problems.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 26,
      "context" : "For example, in [29], matrix factorization is used to approximate a large-scale SDP problem with a smaller one.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 15,
      "context" : "[16] developed a graph learning algorithm by solving a convex optimization problem with the low rank and p.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "An efficient algorithm based on augmented Lagrangian multiplier was proposed to attain the global optimum in [16], while LD-SSC takes advantages of the Lagrange duality property.",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 29,
      "context" : "ORL face database3 and Yale face database4), and two object image data sets (including COIL-20 [32]5 and the handwritten binary Alphadigits data set6), respectively.",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 28,
      "context" : "Parameter Settings and Evaluation Metric In this section, we evaluate the multi-class spectral clustering described in [31] which iteratively solves a discrete solution by using an alternating optimization procedure taking the k principal eigenvectors.",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 1,
      "context" : "Other methods (such as [2]) can also be used, but these methods give similar results [19].",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 18,
      "context" : "Other methods (such as [2]) can also be used, but these methods give similar results [19].",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 28,
      "context" : "Hence, we employ the framework of [31] while replacing different normalization algorithms in our experiments.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 28,
      "context" : "To reduce statistical variation, we repeat all the clustering algorithms for 10 times with random initialization, and report the results corresponding to the best objective values (similar to [31]).",
      "startOffset" : 192,
      "endOffset" : 196
    }, {
      "referenceID" : 32,
      "context" : "All the images in Yale are normalized to 128×128 [35].",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 31,
      "context" : "In this paper, we use the principal component analysis (PCA) [34] to perform dimensionality reduction.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 29,
      "context" : "The COIL-20 data set [32] has 1, 440 images of 20 object categories.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 30,
      "context" : "Given that ri and si are the obtained cluster label and the ground truth label, respectively, the error rate is defined as follows [33]:",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 18,
      "context" : "We choose the lowest error rate [19] and the mean error rate across different kernel parameters (δ in the Gaussian kernel and d in the polynomial kernel) as the evaluation metric in our experiments.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 6,
      "context" : "Comparisons with State-of-the-art Algorithms We perform a comparison between the proposed LD-SSC1, LD-SSC2 and spectral clustering with three different stateof-the-art normalization algorithms, including: 1) Normalizedcut (NC) [7], [18] which is based on the relative entropy",
      "startOffset" : 227,
      "endOffset" : 230
    }, {
      "referenceID" : 17,
      "context" : "Comparisons with State-of-the-art Algorithms We perform a comparison between the proposed LD-SSC1, LD-SSC2 and spectral clustering with three different stateof-the-art normalization algorithms, including: 1) Normalizedcut (NC) [7], [18] which is based on the relative entropy",
      "startOffset" : 232,
      "endOffset" : 236
    }, {
      "referenceID" : 3,
      "context" : "normalization; 2) Ratio-cut (RC) [4] which is based on the L1 normalization; and 3) the Frobenius normalization based spectral clustering (FSC) [19].",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 18,
      "context" : "normalization; 2) Ratio-cut (RC) [4] which is based on the L1 normalization; and 3) the Frobenius normalization based spectral clustering (FSC) [19].",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 32,
      "context" : "4, it shows that LD-SSC1 and LD-SSC2 try to align all the points in a line so that the two classes are well separated, which is similar to the idea of the linear discriminant analysis (LDA) [35] for the two-classes case in the supervised learning.",
      "startOffset" : 190,
      "endOffset" : 194
    }, {
      "referenceID" : 33,
      "context" : "The connections between different clusters are suppressed while the connections within the same clusters are enhanced, which has a similar effect as the aggregation network [36].",
      "startOffset" : 173,
      "endOffset" : 177
    }, {
      "referenceID" : 34,
      "context" : "To show the efficiency of the proposed algorithm, we also compare the computational time between our scalable LD-SSC algorithm (LD-SSC1 and LD-SSC2) and the SSC using CVX (CVX-SSC) [37], which is a standard package for convex optimization.",
      "startOffset" : 181,
      "endOffset" : 185
    }, {
      "referenceID" : 28,
      "context" : "The framework of [31] is used to perform different clustering algorithms for real image segmentation.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 35,
      "context" : "The pixel affinity matrix K is measured based on the maximum magnitude of edges across the line between two pixels [38].",
      "startOffset" : 115,
      "endOffset" : 119
    } ],
    "year" : 2014,
    "abstractText" : "We propose an efficient approach to semidefinite spectral clustering (SSC), which addresses the Frobenius normalization with the positive semidefinite (p.s.d.) constraint for spectral clustering. Compared with the original Frobenius norm approximation based algorithm, the proposed algorithm can more accurately find the closest doubly stochastic approximation to the affinity matrix by considering the p.s.d. constraint. In this paper, SSC is formulated as a semidefinite programming (SDP) problem. In order to solve the high computational complexity of SDP, we present a dual algorithm based on the Lagrange dual formalization. Two versions of the proposed algorithm are proffered: one with less memory usage and the other with faster convergence rate. The proposed algorithm has much lower time complexity than that of the standard interior-point based SDP solvers. Experimental results on both UCI data sets and real-world image data sets demonstrate that 1) compared with the state-of-the-art spectral clustering methods, the proposed algorithm achieves better clustering performance; and 2) our algorithm is much more efficient and can solve larger-scale SSC problems than those standard interior-point SDP solvers.",
    "creator" : "TeX"
  }
}