{
  "name" : "1605.05172.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Siamese convolutional networks based on phonetic features for cognate identification",
    "authors" : [ "Taraka Rama" ],
    "emails" : [ "taraka-rama.kasicheyanula@uni-tuebingen.de" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Cognates are words that are known to have descended from a common ancestral language. In historical linguistics, identification of cognates is an important step for positing relationships between languages. Historical linguists apply the comparative method (Trask, 1996) for positing relationships between languages.\nIn NLP, automatic identification of cognates is associated with the task of determining if two words are descended from a common ancestor or not. There are at least two ways to achieve automatic identification of cognates.\nOne way is to modify a well-known string alignment technique such as Longest Common Subsequence or Needleman-Wunsch algorithm (Needleman and Wunsch, 1970) to weigh the alignments differentially (Kondrak, 2001; List, 2012). The weights are determined through the linguistic knowledge of the sound changes that occurred in the language family.\nThe second approach employs a machine learning perspective that is widely employed in NLP. The cognate identification is achieved by training a linear classifier or a sequence labeler on a set of labeled positive and negative examples; and then employ the trained classifier to classify new word\npairs. The features for a classifier consist of word similarity measures based on number of shared bigrams, edit distance, and longest common subsequence (Hauer and Kondrak, 2011; Inkpen et al., 2005).\nThe above procedures provide an estimate of the similarity between a pair of words and cannot directly be used to infer a phylogeny based on models of trait evolution. The pairwise judgments have to be converted into multiple cognate judgments so that the multiple judgments can be supplied to a automatic tree building program for inferring a phylogeny for the languages under study.\nIt has to be noted that the Indo-European dating studies (Bouckaert et al., 2012; Chang et al., 2015) employ human expert cognacy judgments for inferring phylogeny and dates of a very well-studied language family. Hence, there is a need for developing automated cognate identification methods that can be applied to under-studied languages of the world."
    }, {
      "heading" : "2 Related work",
      "text" : "The earlier computational effort of (Jäger, 2013; Rama et al., 2013) employs Pointwise Mutual Information (PMI) to compute transition matrices between sounds. Both Jäger (2013) and Rama et al. (2013) employ undirectional sound correspondence based scorer to compute word similarity. The general approach is to align word pairs using vanilla edit distance and impose a cutoff to extract potential cognate pairs. The aligned sound symbols are then employed to compute the PMI scoring matrix that is used to realign the pairs. The PMI scoring matrix is recounted from the realigned pairs. This procedure is repeated until convergence.\nJäger (2013) imposes an additional cutoff based on the PMI scoring matrix. Further, Jäger (2013)\nar X\niv :1\n60 5.\n05 17\n2v 2\n[ cs\n.C L\n] 2\nJ ul\n2 01\n6\nalso employs the PMI scoring matrix to infer family trees for new language families and compares those trees with the expert trees given in Glottolog (Nordhoff and Hammarström, 2011). Rama et al. (2013) take a slightly different approach, in that, the authors compute a PMI matrix independently for each language family and evaluate its performance at the task of pair-wise cognate identification. In this work, we also compare the convolutional networks against PMI based binary classifier.\nPrevious works of cognate identification such as (Bergsma and Kondrak, 2007; Inkpen et al., 2005) supply string similarity measures as features for training different classifiers such as decision trees, maximum-entropy, and SVMs for the purpose of determining if a given word pair is cognate or not.\nIn another line of work, List (2012) employs a transition matrix derived from historical linguistic knowledge to align and score word pairs. This approach is algorithmically similar to that of Kondrak (2000) who employs articulation motivated weights to score a sound transition matrix. The weighted sound transition matrix is used to score a word pair.\nThe work of List (2012) known as Sound-Class Phonetic Alignment (SCA) approach reduces the phonemes to historical linguistic motivated sound classes such that transitions between some classes are less penalized than transitions between the rest of the classes. For example, the probability of velars transitioning to palatals is a wellattested sound change across the world. The SCA approach employs a weighted directed graph to model directionality and proportionality of sound changes between sound classes. For example, a direct change between velars and dentals is unattested and would get a zero weight. Both Kondrak (2000) and List (2012) set the weights and directions in the sound transition graph to suit the reality of sound change.\nAll the above outlined approaches employ a scoring matrix that is derived automatically or manually; or, employ a SVM to train form similarity based features for the purpose of cognate identification."
    }, {
      "heading" : "3 Convolutional networks",
      "text" : "This article is the first to apply convolutional networks (ConvNets) to phonemes by treating each phoneme as a vector of binary valued phonetic\nfeatures. This approach has the advantage that it does not require explicit feature engineering, alignments, and a sound transition matrix. The approach requires cognacy statements and phonetic descriptions of sounds used to transcribe the words. The cognacy statements can be obtained from etymological dictionaries and the quality of the phonemes can be obtained from Ladefoged and Maddieson (1998).\nCollobert et al. (2011) proposed ConvNets for NLP tasks in 2011 and were since applied for sentence classification (Kim, 2014; Johnson and Zhang, 2015; Kalchbrenner et al., 2014; Zhang et al., 2015), part-of-speech tagging (Santos and Zadrozny, 2014), and information retrieval (Shen et al., 2014).\nKim (2014) applied convolutional networks to pre-trained word embeddings in a sentence for the task of sentence classification. Johnson and Zhang (2015) train their convolutional network from scratch by using a one-hot vector for each word. The authors show that their convolutional network performs better than a SVM classifier trained on bag-of-words features. Santos and Zadrozny (2014) use character embeddings to train their POS-tagger. The authors find that the POS-tagger performs better than the accuracies reported in (Manning, 2011).\nIn a recent work, Zhang et al. (2015) treat documents as a sequence of characters and transform each document into a sequence of one-hot character vectors. The authors designed and trained two 9-layer convolutional networks for the purpose of sentiment classification. The authors report competitive or state-of-the art performance on a wide range of benchmark sentiment classification datasets."
    }, {
      "heading" : "4 Character convolutional networks",
      "text" : "Chopra et al. (2005) extended the traditional ConvNets to classify if two images belong to the same person. These ConvNets are known as Siamese Networks (inspired from Siamese twins) and share weights for independent but identical layers of convolutional networks. Siamese networks and their variants have been employed for identifying if two images are from the same person or different persons (Zagoruyko and Komodakis, 2015); and for recognizing if two speech segments belong to the same word class (Kamper et al., 2015)."
    }, {
      "heading" : "4.1 Word as image",
      "text" : "Historical linguists perform cognate identification based on regular correspondences which are described as changes in phonetic features of phonemes. For instance, Grimm’s law bh ∼ b is described as loss of aspiration; p ∼ f is described as change from plosives to fricatives; and devoicing d ∼ t in English ten ∼ Latin decem.\nLearning criteria for cognacy through phonetic features from a set of training examples implies that there is no need for explicit alignment and design/learning of sound scoring matrices. In this article, we represent each phoneme as a binaryvalued vector of phonetic features and then perform convolution on the two-dimensional matrix."
    }, {
      "heading" : "4.2 Siamese network",
      "text" : "Intuitively, a network should learn a similarity function such that words that diverged due to accountable sound shifts are placed close to one another than two words that are not cognates. And, Siamese networks are suitable for this task since, they learn a similarity function that has a higher similarity between cognates as compared to noncognates. The weight tying ensures that two cognate words sharing similar phonetic features in a local context tend to be get higher weights than words that are not cognate."
    }, {
      "heading" : "4.3 Phoneme vectorization",
      "text" : "In this article, we work with the ASJP alphabet (Brown et al., 2013). The ASJP alphabet is coarser than IPA but is designed with the aim to capture highly frequent sounds in the world’s languages. The ASJP database has word lists for 60% of the world’s languages but only has cognate judgments for some selected families (Wichmann and Holman, 2013).\nWe composed a binary vector for each phoneme based on the description given in table 1. In total, there are 16 binary valued features. We also reduced all vowels to a single vowel that has a value of 1 for voicing feature and 0 for the rest of the features. The main motivation for such decision is that vowels are diachronically unstable than consonants (Kessler, 2007).\nA word such as “fat” would be represented as 3×16 matrix where each column provides a binary value for the phonetic feature (cf. table 2)."
    }, {
      "heading" : "4.4 ConvNet Models",
      "text" : "In this subsection, we describe the ConvNet models used in our experiments.\nSiamese ConvNet Siamese networks takes a pair of inputs and minimizes the distance between the output representations. Each branch of the Siamese network is composed of a convolutional network. The Euclidean distance D between the representations of each branch is then used to train a contrastive-loss function yD+(1− y)max{0,m − D} where m is the margin and y is the true label. We only describe the architecture since this forms the basis for the rest of our experiments with Siamese architectures.1\nManhattan Siamese ConvNet The second ConvNet is also a Siamese network where the Euclidean distance is replaced by a element-wise absolute difference layer followed by a fully con-\n1The results were slightly better than a majority class classifier and were not reported in the article\nnected layer (cf. figure 1). To the best of our knowledge, only Zagoruyko and Komodakis (2015) added two fully connected layers to the concatenated outputs of the Siamese network and trained a system that predicts if two image patches belong to the same image or different images. We refer this architecture as a Manhattan Siamese ConvNet due to the difference layer’s similarity to Manhattan distance.\n2-channel Convnet Until now, each word is treated as a separate input. Zagoruyko and Komodakis (2015) introduced a 2-channel architecture which treats a pair of image patches as a 2-channel image. This can also be applied to words. The 2-channel ConvNet has two convolutional layers, a maxpooling layer, and a fully connected layer with 8 units.\nThe number of feature maps in each convolutional layer is fixed at 10 with a kernel size of 2 × 3. The max-pooling layer halves the output of the previous convolutional layer. We also inserted a dropout layer with 0.5 probability (Srivastava et al., 2014) after a fully-connected layer to avoid over-fitting. The convolutional layers were trained with ReLU non-linearity.\nWe zero-padded each word to obtain a length of 10 for all the words to apply the filter equally about a word. We used adadelta optimizer (Zeiler, 2012) with learning rate of 1.0, ρ = 0.95, and = 10−6. We fixed the mini-batch size to 128 in all our experiments. We experimented with different batch sizes ([32, 64, 128, 256]) and did not observe any significant deviation in the validation loss. Both, Manhattan and 2-stream ConvNets were trained using the log-loss function. Both our architectures are relatively shallow (3) as compared to the text classification architecture of Zhang et al. (2015). We trained all our networks using Keras (Chollet, 2015) and Theano (Bergstra et al., 2010)."
    }, {
      "heading" : "5 Comparison methods",
      "text" : "We compare the ConvNet architectures with SVM classifiers trained with different string similarities as features.\nOther sound classes/alphabets Apart from ASJP alphabet, there are two other alphabets that have been designed by historical linguists for the purpose of modeling sound change. As mentioned before, the main idea behind the design of sound classes is to discourage transitions between partic-\nular classes of sounds but allow transitions within a sound class. Dolgopolsky (1986) proposed a ten sound class system based on the empirical data of 140 languages. SCA alphabet (List, 2012) has a size of 25 and attempts to address some issues with the ASJP alphabet (lack of tones) and also extend Dolgopolsky’s sound classes based on evidence from more number of languages.\nOrthographic measures as features We converted all the datasets into all the three sound classes and computed the following string similarity scores: • Edit distance. • Common number of bigrams. • Length of the longest common subsequence. • Length of longest common prefix. • Common number of trigrams. • Global alignment based on Needlman-\nWunch algorithm (Needleman and Wunsch, 1970). • Local alignment score based on Smith-\nWaterman algorithm (Smith and Waterman, 1981). • Semi-global alignment score is a compromise\nbetween global and local alignments (Durbin et al., 2002).2 • Common number of skipped bigrams (XDICE). • A positional extension of XDICE known as\nXXDICE (Brew and McKelvie, 1996). Pair-wise Mutual Information (PMI) We also computed a PMI score for a pair of ASJP transcribed words using the PMI scoring matrix developed by Jäger (2013). This system is referred to as PMI system.\nWe included length of each word and the absolute difference in length between the words as features for both the Orthographic and PMI systems. The sound class orthographic scores system attempts to combine the previous cognate identification systems developed by (Inkpen et al., 2005; Hauer and Kondrak, 2011) and the insights from applying string similarities to sound classes for language comparison (Kessler, 2007)."
    }, {
      "heading" : "6 Datasets",
      "text" : "In this section, we describe the datasets used in our experiments.\nIELex database The Indo-European Lexical 2The global, local, and alignment scores were computed\nusing LingPy library (List and Moran, 2013).\ndatabase is created by Dyen et al. (1992) and curated by Michael Dunn.3 The transcription in IELex database is not uniformly IPA and retains many forms transcribed in the Romanized IPA format of Dyen et al. (1992). We cleaned the IELex database of any non-IPA-like transcriptions and converted part of the database into ASJP format.\nAustronesian vocabulary database The Austronesian Vocabulary Database (Greenhill and Gray, 2009) has word lists for 210 Swadesh concepts and 378 languages.4 The database does not have transcriptions in a uniform IPA format. We removed all symbols that do not appear in the standard IPA and converted the lexical items to ASJP format.5\nShort word lists with cognacy judgments Wichmann and Holman (2013) and List (2014) compiled cognacy wordlists for subsets of families from various scholarly sources such as comparative handbooks and historical linguistics’ articles. The details of this compilation is given below. For each dataset, we give the number of languages/the number of concepts in parantheses. This dataset is henceforth referred to as “Mixed dataset”.\n• Wichmann and Holman (2013): Afrasian (21/40), Kadai (12/40), Kamasau (8/36), Lolo-Burmese (15/40), Mayan (30/100), Miao-Yao (6/36), Mixe-Zoque (10/100), Mon-Khmer (16/100).\n• List (2014): Bai dialects (9/110), Chinese dialects (18/180), Huon (14/84), Japanese (10/200), ObUgrian (21/110; Hungarian excluded from Ugric sub-family), Tujia (5/107; Sino-Tibetan).\n3ielex.mpi.nl 4http://language.psy.auckland.ac.nz/\naustronesian/ 5For computational reasons, we work with a subset of 100 languages.\nWe performed two experiments with these datasets. In the first experiment, we randomly selected 70% of concepts from IELex, ABVD, and Mayan datasets for training and the rest of the 30% concepts for testing. The motivation behind this experiment is to test if ConvNets can learn phonetic feature patterns across concepts. In the second experiment, we trained on the Mixed dataset but tested on the Indo-European and Austronesian datasets. The motivation behind this experiment is to test if ConvNets can learn general patterns of sound change across language families. The number of training and testing examples in each dataset is given in table 3."
    }, {
      "heading" : "7 Results",
      "text" : "In this section, we report the results of our crossconcepts and cross-family experiments.\nSVM training and evaluation metrics We used a linear kernel and optimized the SVM hyperparameter (C) through ten-fold cross-validation and grid search on the training data. We report accuracies, class-wise F-scores (positive and negative), combined F-score, and average precision score for each system on concepts dataset in table 4. The average precision score corresponds to the area under the precision-recall curve and is an indicator of the robustness of the model to thresholds."
    }, {
      "heading" : "7.1 Cross-Concept experiments",
      "text" : "Effect of size and width of fully connected layers We observed that both the depth and width of the fully connected layers do not affect the performance of the ConvNet models. We used a fully connected network of size 8 in all our experiments. We increased the number of neurons from 8 to 64 in multiples of two and observed that increasing the number of neurons hurts the performance of the system.\nEffect of filter size Zhang and Wallace (2015) observed that the size of the filter patch can affect the performance of the system. We experimented with different filter sizes of dimensions m × k where, m ∈ [1, 2] and k ∈ [1, 3]. We did not find any change in the performance in concepts experiments. We report the results for m = 2, k = 3 filter size for cross-concept experiments."
    }, {
      "heading" : "7.2 Cross-Family experiments",
      "text" : "Effect of filter size Unlike the previous experiment, the filter size has a effect on the performance of the ConvNet system. We observed that the best results were obtained with a filter size of 1× 3.\nWe did not include the results of the 2-channel ConvNet because of its worse performance at the task of cross-family cognate identification. The results of our experiments are given in table 5."
    }, {
      "heading" : "8 Discussion",
      "text" : "The Manhattan ConvNet competes with PMI and orthographic models at cross-concept cognate identification task. The Manhattan ConvNet performs better than PMI and orthographic models in terms of overall accuracy in all the three language families. In terms of averaged F-scores, Manhattan ConvNet performs slightly better than orthographic model and only performs worse than the other models at Austronesian language family.\nThe Manhattan ConvNet shows mixed performance at the task of cross-family cognate identification. The Manhattan ConvNet does not turn up as the best system across all the evaluation metrics in a single language family. The ConvNet performs better than PMI but is not as good as Orthographic measures at Indo-European language family. In terms of accuracies, the ConvNet comes closer to PMI than the orthographic system.\nThese experiments suggest that ConvNets can compete with a classifier trained on different orthographic measures and different sound classes. ConvNets can also compete with a data driven method like PMI which was trained in an EM-like fashion on millions of word pairs. ConvNets can certainly perform better than a classifier trained\non word similarity scores at cross-concept experiments.\nThe Orthographic system and PMI system show similar performance at the Austronesian crossconcept task. However, ConvNets do not perform as well as orthographic and PMI systems. The reason for this could be due to the differential transcriptions in the database."
    }, {
      "heading" : "9 Conclusion",
      "text" : "In this article, we explored the use of phonetic feature convolutional networks for the task of pairwise cognate identification. Our experiments with convolutional networks show that phonetic features can be directly used for classifying if two words are related or not.\nIn the future, we intend to work directly with speech recordings and include language relatedness information into ConvNets to improve the performance. We are currently working towards building a larger database of word lists in IPA transcription."
    }, {
      "heading" : "Acknowledgments",
      "text" : "I thank Aparna Subhakari, Vijayaditya Peddinti, Johann-Mattis List, Johannes Dellert, Armin Buch, Çağrı Çöltekin, Gerhard Jäger, and Daniël de Kok for all the useful comments. The data for\nthe experiments was processed by Johann-Mattis List and Pavel Sofroniev."
    } ],
    "references" : [ {
      "title" : "Alignment-based discriminative string similarity",
      "author" : [ "Bergsma", "Kondrak2007] Shane Bergsma", "Grzegorz Kondrak" ],
      "venue" : "In Proceedings of the 45th annual meeting of the association of computational linguistics,",
      "citeRegEx" : "Bergsma et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Bergsma et al\\.",
      "year" : 2007
    }, {
      "title" : "Theano: a cpu and gpu math expression compiler",
      "author" : [ "Olivier Breuleux", "Frédéric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David WardeFarley", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Bergstra et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Bergstra et al\\.",
      "year" : 2010
    }, {
      "title" : "Mapping the origins and expansion",
      "author" : [ "Philippe Lemey", "Michael Dunn", "Simon J. Greenhill", "Alexander V. Alekseyenko", "Alexei J. Drummond", "Russell D. Gray", "Marc A. Suchard", "Quentin D. Atkinson" ],
      "venue" : null,
      "citeRegEx" : "Bouckaert et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bouckaert et al\\.",
      "year" : 2012
    }, {
      "title" : "Word-pair extraction for lexicography",
      "author" : [ "Brew", "McKelvie1996] Chris Brew", "David McKelvie" ],
      "venue" : "In Proceedings of the Second International Conference on New Methods in Language Processing,",
      "citeRegEx" : "Brew et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Brew et al\\.",
      "year" : 1996
    }, {
      "title" : "Ancestryconstrained phylogenetic analysis supports the IndoEuropean steppe",
      "author" : [ "Chang et al.2015] Will Chang", "Chundra Cathcart", "David Hall", "Andrew Garrett" ],
      "venue" : "hypothesis. Language,",
      "citeRegEx" : "Chang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chang et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning a similarity metric discriminatively, with application to face verification",
      "author" : [ "Chopra et al.2005] Sumit Chopra", "Raia Hadsell", "Yann LeCun" ],
      "venue" : "In Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Chopra et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Chopra et al\\.",
      "year" : 2005
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Collobert et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "A probabilistic hypothesis concerning the oldest relationships among the language families of northern Eurasia",
      "author" : [ "Aron B. Dolgopolsky" ],
      "venue" : null,
      "citeRegEx" : "Dolgopolsky.,? \\Q1986\\E",
      "shortCiteRegEx" : "Dolgopolsky.",
      "year" : 1986
    }, {
      "title" : "Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids",
      "author" : [ "Sean R. Eddy", "Anders Krogh", "Graeme Mitchison" ],
      "venue" : null,
      "citeRegEx" : "Durbin et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Durbin et al\\.",
      "year" : 2002
    }, {
      "title" : "An Indo-European classification: A lexicostatistical experiment. Transactions of the American Philosophical Society, 82(5):1–132",
      "author" : [ "Dyen et al.1992] Isidore Dyen", "Joseph B. Kruskal", "Paul Black" ],
      "venue" : null,
      "citeRegEx" : "Dyen et al\\.,? \\Q1992\\E",
      "shortCiteRegEx" : "Dyen et al\\.",
      "year" : 1992
    }, {
      "title" : "Austronesian language phylogenies: Myths and misconceptions about Bayesian computational methods. Austronesian Historical Linguistics and Culture History: A Festschrift",
      "author" : [ "Greenhill", "Gray2009] Simon J. Greenhill", "Russell D. Gray" ],
      "venue" : null,
      "citeRegEx" : "Greenhill et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Greenhill et al\\.",
      "year" : 2009
    }, {
      "title" : "Clustering semantically equivalent words into cognate sets in multilingual lists",
      "author" : [ "Hauer", "Kondrak2011] Bradley Hauer", "Grzegorz Kondrak" ],
      "venue" : "In Proceedings of 5th International Joint Conference on Natural Language Processing,",
      "citeRegEx" : "Hauer et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Hauer et al\\.",
      "year" : 2011
    }, {
      "title" : "Automatic identification of cognates and false friends in French and English",
      "author" : [ "Inkpen et al.2005] Diana Inkpen", "Oana Frunza", "Grzegorz Kondrak" ],
      "venue" : "In Proceedings of the International Conference Recent Advances in Natural Language Pro-",
      "citeRegEx" : "Inkpen et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Inkpen et al\\.",
      "year" : 2005
    }, {
      "title" : "Phylogenetic inference from word lists using weighted alignment with empirically determined weights",
      "author" : [ "Gerhard Jäger" ],
      "venue" : "Language Dynamics and Change,",
      "citeRegEx" : "Jäger.,? \\Q2013\\E",
      "shortCiteRegEx" : "Jäger.",
      "year" : 2013
    }, {
      "title" : "Effective use of word order for text categorization with convolutional neural networks",
      "author" : [ "Johnson", "Zhang2015] Rie Johnson", "Tong Zhang" ],
      "venue" : "In NAACL HLT 2015, The 2015 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Johnson et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2015
    }, {
      "title" : "A convolutional neural network for modelling sentences",
      "author" : [ "Edward Grefenstette", "Phil Blunsom" ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association",
      "citeRegEx" : "Kalchbrenner et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kalchbrenner et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep convolutional acoustic word embeddings using word-pair side information. CoRR, abs/1510.01032",
      "author" : [ "Kamper et al.2015] Herman Kamper", "Weiran Wang", "Karen Livescu" ],
      "venue" : null,
      "citeRegEx" : "Kamper et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kamper et al\\.",
      "year" : 2015
    }, {
      "title" : "Word similarity metrics and multilateral comparison",
      "author" : [ "Brett Kessler" ],
      "venue" : "In Proceedings of the Ninth Meeting of the ACL Special Interest Group in Computational Morphology and Phonology,",
      "citeRegEx" : "Kessler.,? \\Q2007\\E",
      "shortCiteRegEx" : "Kessler.",
      "year" : 2007
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim" ],
      "venue" : "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Kim.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "A new algorithm for the alignment of phonetic sequences",
      "author" : [ "Grzegorz Kondrak" ],
      "venue" : "In Proceedings of the First Meeting of the North American Chapter of the Association for Computational Linguistics,",
      "citeRegEx" : "Kondrak.,? \\Q2000\\E",
      "shortCiteRegEx" : "Kondrak.",
      "year" : 2000
    }, {
      "title" : "Identifying cognates by phonetic and semantic similarity",
      "author" : [ "Grzegorz Kondrak" ],
      "venue" : "In Proceedings of the second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies,",
      "citeRegEx" : "Kondrak.,? \\Q2001\\E",
      "shortCiteRegEx" : "Kondrak.",
      "year" : 2001
    }, {
      "title" : "An open source toolkit for quantitative historical linguistics",
      "author" : [ "List", "Moran2013] Johann-Mattis List", "Steven Moran" ],
      "venue" : "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations,",
      "citeRegEx" : "List et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "List et al\\.",
      "year" : 2013
    }, {
      "title" : "SCA: phonetic alignment based on sound classes",
      "author" : [ "Johann-Mattis List" ],
      "venue" : "In New Directions in Logic, Language and Computation,",
      "citeRegEx" : "List.,? \\Q2012\\E",
      "shortCiteRegEx" : "List.",
      "year" : 2012
    }, {
      "title" : "Sequence comparison in historical linguistics. Düsseldorf University Press, Düsseldorf",
      "author" : [ "J.-M. List" ],
      "venue" : null,
      "citeRegEx" : "List.,? \\Q2014\\E",
      "shortCiteRegEx" : "List.",
      "year" : 2014
    }, {
      "title" : "Partof-speech tagging from 97% to 100%: is it time for some linguistics",
      "author" : [ "Christopher D Manning" ],
      "venue" : "In Computational Linguistics and Intelligent Text Processing,",
      "citeRegEx" : "Manning.,? \\Q2011\\E",
      "shortCiteRegEx" : "Manning.",
      "year" : 2011
    }, {
      "title" : "A general method applicable to the search for similarities in the amino acid sequence of two proteins",
      "author" : [ "Needleman", "Wunsch1970] Saul B. Needleman", "Christian D. Wunsch" ],
      "venue" : "Journal of Molecular Biology,",
      "citeRegEx" : "Needleman et al\\.,? \\Q1970\\E",
      "shortCiteRegEx" : "Needleman et al\\.",
      "year" : 1970
    }, {
      "title" : "Glottolog/Langdoc: Defining dialects, languages, and language families as collections of resources",
      "author" : [ "Nordhoff", "Harald Hammarström" ],
      "venue" : "In Proceedings of the First International Workshop",
      "citeRegEx" : "Nordhoff et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Nordhoff et al\\.",
      "year" : 2011
    }, {
      "title" : "Two methods for automatic identification of cognates",
      "author" : [ "Rama et al.2013] Taraka Rama", "Prasant Kolachina", "Sudheer Kolachina" ],
      "venue" : "QITL,",
      "citeRegEx" : "Rama et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Rama et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning character-level representations for part-of-speech tagging",
      "author" : [ "Santos", "Zadrozny2014] Cicero D Santos", "Bianca Zadrozny" ],
      "venue" : "In Proceedings of the 31st International Conference on Machine Learning",
      "citeRegEx" : "Santos et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Santos et al\\.",
      "year" : 2014
    }, {
      "title" : "A latent semantic model with convolutional-pooling structure for information retrieval",
      "author" : [ "Shen et al.2014] Yelong Shen", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Grégoire Mesnil" ],
      "venue" : "In Proceedings of the 23rd ACM International Conference on Conference",
      "citeRegEx" : "Shen et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2014
    }, {
      "title" : "Identification of common molecular subsequences",
      "author" : [ "Smith", "Waterman1981] Temple F. Smith", "Michael S. Waterman" ],
      "venue" : "Journal of molecular biology,",
      "citeRegEx" : "Smith et al\\.,? \\Q1981\\E",
      "shortCiteRegEx" : "Smith et al\\.",
      "year" : 1981
    }, {
      "title" : "Dropout: A simple way to prevent neural networks from overfitting",
      "author" : [ "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Srivastava et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2014
    }, {
      "title" : "Languages with longer words have more lexical change. In Approaches to Measuring Linguistic Differences, pages 249–281",
      "author" : [ "Wichmann", "Holman2013] Søren Wichmann", "Eric W Holman" ],
      "venue" : null,
      "citeRegEx" : "Wichmann et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Wichmann et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning to compare image patches via convolutional neural networks",
      "author" : [ "Zagoruyko", "Komodakis2015] Sergey Zagoruyko", "Nikos Komodakis" ],
      "venue" : "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June",
      "citeRegEx" : "Zagoruyko et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zagoruyko et al\\.",
      "year" : 2015
    }, {
      "title" : "Adadelta: an adaptive learning rate method",
      "author" : [ "Matthew D Zeiler" ],
      "venue" : "arXiv preprint arXiv:1212.5701",
      "citeRegEx" : "Zeiler.,? \\Q2012\\E",
      "shortCiteRegEx" : "Zeiler.",
      "year" : 2012
    }, {
      "title" : "A sensitivity analysis of (and practitioners’ guide to) convolutional neural networks for sentence classification",
      "author" : [ "Zhang", "Wallace2015] Ye Zhang", "Byron Wallace" ],
      "venue" : "arXiv preprint arXiv:1510.03820",
      "citeRegEx" : "Zhang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    }, {
      "title" : "Character-level convolutional networks for text classification",
      "author" : [ "Xiang Zhang", "Junbo Zhao", "Yann LeCun" ],
      "venue" : "Advances in Neural Information Processing",
      "citeRegEx" : "Zhang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 20,
      "context" : "One way is to modify a well-known string alignment technique such as Longest Common Subsequence or Needleman-Wunsch algorithm (Needleman and Wunsch, 1970) to weigh the alignments differentially (Kondrak, 2001; List, 2012).",
      "startOffset" : 194,
      "endOffset" : 221
    }, {
      "referenceID" : 22,
      "context" : "One way is to modify a well-known string alignment technique such as Longest Common Subsequence or Needleman-Wunsch algorithm (Needleman and Wunsch, 1970) to weigh the alignments differentially (Kondrak, 2001; List, 2012).",
      "startOffset" : 194,
      "endOffset" : 221
    }, {
      "referenceID" : 12,
      "context" : "The features for a classifier consist of word similarity measures based on number of shared bigrams, edit distance, and longest common subsequence (Hauer and Kondrak, 2011; Inkpen et al., 2005).",
      "startOffset" : 147,
      "endOffset" : 193
    }, {
      "referenceID" : 2,
      "context" : "It has to be noted that the Indo-European dating studies (Bouckaert et al., 2012; Chang et al., 2015)",
      "startOffset" : 57,
      "endOffset" : 101
    }, {
      "referenceID" : 4,
      "context" : "It has to be noted that the Indo-European dating studies (Bouckaert et al., 2012; Chang et al., 2015)",
      "startOffset" : 57,
      "endOffset" : 101
    }, {
      "referenceID" : 13,
      "context" : "The earlier computational effort of (Jäger, 2013; Rama et al., 2013) employs Pointwise Mutual Information (PMI) to compute transition matrices between sounds.",
      "startOffset" : 36,
      "endOffset" : 68
    }, {
      "referenceID" : 27,
      "context" : "The earlier computational effort of (Jäger, 2013; Rama et al., 2013) employs Pointwise Mutual Information (PMI) to compute transition matrices between sounds.",
      "startOffset" : 36,
      "endOffset" : 68
    }, {
      "referenceID" : 13,
      "context" : "The earlier computational effort of (Jäger, 2013; Rama et al., 2013) employs Pointwise Mutual Information (PMI) to compute transition matrices between sounds. Both Jäger (2013) and Rama et al.",
      "startOffset" : 37,
      "endOffset" : 177
    }, {
      "referenceID" : 13,
      "context" : "The earlier computational effort of (Jäger, 2013; Rama et al., 2013) employs Pointwise Mutual Information (PMI) to compute transition matrices between sounds. Both Jäger (2013) and Rama et al. (2013) employ undirectional sound correspondence based scorer to compute word similarity.",
      "startOffset" : 37,
      "endOffset" : 200
    }, {
      "referenceID" : 27,
      "context" : "Rama et al. (2013) take a slightly different approach, in that, the authors compute a PMI matrix independently for each language family and evaluate its performance at the task of pair-wise cognate identification.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 12,
      "context" : "Previous works of cognate identification such as (Bergsma and Kondrak, 2007; Inkpen et al., 2005) supply string similarity measures as features for training different classifiers such as decision trees, maximum-entropy, and SVMs for the purpose of",
      "startOffset" : 49,
      "endOffset" : 97
    }, {
      "referenceID" : 22,
      "context" : "In another line of work, List (2012) employs a transition matrix derived from historical linguistic knowledge to align and score word pairs.",
      "startOffset" : 25,
      "endOffset" : 37
    }, {
      "referenceID" : 19,
      "context" : "approach is algorithmically similar to that of Kondrak (2000) who employs articulation motivated weights to score a sound transition matrix.",
      "startOffset" : 47,
      "endOffset" : 62
    }, {
      "referenceID" : 22,
      "context" : "The work of List (2012) known as Sound-Class Phonetic Alignment (SCA) approach reduces the phonemes to historical linguistic motivated sound classes such that transitions between some classes",
      "startOffset" : 12,
      "endOffset" : 24
    }, {
      "referenceID" : 19,
      "context" : "Both Kondrak (2000) and List (2012) set the weights and directions in the sound transition graph to suit the reality of sound change.",
      "startOffset" : 5,
      "endOffset" : 20
    }, {
      "referenceID" : 19,
      "context" : "Both Kondrak (2000) and List (2012) set the weights and directions in the sound transition graph to suit the reality of sound change.",
      "startOffset" : 5,
      "endOffset" : 36
    }, {
      "referenceID" : 18,
      "context" : "(2011) proposed ConvNets for NLP tasks in 2011 and were since applied for sentence classification (Kim, 2014; Johnson and Zhang, 2015; Kalchbrenner et al., 2014; Zhang et al., 2015), part-of-speech tagging (Santos and Zadrozny, 2014), and information retrieval (Shen et al.",
      "startOffset" : 98,
      "endOffset" : 181
    }, {
      "referenceID" : 15,
      "context" : "(2011) proposed ConvNets for NLP tasks in 2011 and were since applied for sentence classification (Kim, 2014; Johnson and Zhang, 2015; Kalchbrenner et al., 2014; Zhang et al., 2015), part-of-speech tagging (Santos and Zadrozny, 2014), and information retrieval (Shen et al.",
      "startOffset" : 98,
      "endOffset" : 181
    }, {
      "referenceID" : 35,
      "context" : "(2011) proposed ConvNets for NLP tasks in 2011 and were since applied for sentence classification (Kim, 2014; Johnson and Zhang, 2015; Kalchbrenner et al., 2014; Zhang et al., 2015), part-of-speech tagging (Santos and Zadrozny, 2014), and information retrieval (Shen et al.",
      "startOffset" : 98,
      "endOffset" : 181
    }, {
      "referenceID" : 29,
      "context" : ", 2015), part-of-speech tagging (Santos and Zadrozny, 2014), and information retrieval (Shen et al., 2014).",
      "startOffset" : 87,
      "endOffset" : 106
    }, {
      "referenceID" : 24,
      "context" : "ported in (Manning, 2011).",
      "startOffset" : 10,
      "endOffset" : 25
    }, {
      "referenceID" : 35,
      "context" : "In a recent work, Zhang et al. (2015) treat documents as a sequence of characters and transform each document into a sequence of one-hot char-",
      "startOffset" : 18,
      "endOffset" : 38
    }, {
      "referenceID" : 16,
      "context" : "Siamese networks and their variants have been employed for identifying if two images are from the same person or different persons (Zagoruyko and Komodakis, 2015); and for recognizing if two speech segments belong to the same word class (Kamper et al., 2015).",
      "startOffset" : 237,
      "endOffset" : 258
    }, {
      "referenceID" : 17,
      "context" : "The main motivation for such decision is that vowels are diachronically unstable than consonants (Kessler, 2007).",
      "startOffset" : 97,
      "endOffset" : 112
    }, {
      "referenceID" : 31,
      "context" : "5 probability (Srivastava et al., 2014) after a fully-connected layer to avoid over-fitting.",
      "startOffset" : 14,
      "endOffset" : 39
    }, {
      "referenceID" : 34,
      "context" : "We used adadelta optimizer (Zeiler, 2012) with learning rate of 1.",
      "startOffset" : 27,
      "endOffset" : 41
    }, {
      "referenceID" : 1,
      "context" : "We trained all our networks using Keras (Chollet, 2015) and Theano (Bergstra et al., 2010).",
      "startOffset" : 67,
      "endOffset" : 90
    }, {
      "referenceID" : 34,
      "context" : "Both our architectures are relatively shallow (3) as compared to the text classification architecture of Zhang et al. (2015). We trained all our networks using Keras (Chollet, 2015) and Theano (Bergstra et al.",
      "startOffset" : 105,
      "endOffset" : 125
    }, {
      "referenceID" : 22,
      "context" : "SCA alphabet (List, 2012) has a size of 25 and attempts to address some issues with the ASJP alphabet (lack of tones) and also extend Dolgopolsky’s sound classes based on evidence from more number of languages.",
      "startOffset" : 13,
      "endOffset" : 25
    }, {
      "referenceID" : 7,
      "context" : "Dolgopolsky (1986) proposed a ten sound class system based on the empirical data of 140 languages.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 8,
      "context" : "• Semi-global alignment score is a compromise between global and local alignments (Durbin et al., 2002).",
      "startOffset" : 82,
      "endOffset" : 103
    }, {
      "referenceID" : 13,
      "context" : "computed a PMI score for a pair of ASJP transcribed words using the PMI scoring matrix developed by Jäger (2013). This system is referred to as PMI system.",
      "startOffset" : 100,
      "endOffset" : 113
    }, {
      "referenceID" : 12,
      "context" : "The sound class orthographic scores system attempts to combine the previous cognate identification systems developed by (Inkpen et al., 2005; Hauer and Kondrak, 2011) and the insights from applying string similarities to sound classes for",
      "startOffset" : 120,
      "endOffset" : 166
    }, {
      "referenceID" : 17,
      "context" : "language comparison (Kessler, 2007).",
      "startOffset" : 20,
      "endOffset" : 35
    }, {
      "referenceID" : 9,
      "context" : "database is created by Dyen et al. (1992) and curated by Michael Dunn.",
      "startOffset" : 23,
      "endOffset" : 42
    }, {
      "referenceID" : 9,
      "context" : "database is created by Dyen et al. (1992) and curated by Michael Dunn.3 The transcription in IELex database is not uniformly IPA and retains many forms transcribed in the Romanized IPA format of Dyen et al. (1992). We cleaned the IELex database of any non-IPA-like transcriptions and converted part of the database into ASJP format.",
      "startOffset" : 23,
      "endOffset" : 214
    }, {
      "referenceID" : 22,
      "context" : "Short word lists with cognacy judgments Wichmann and Holman (2013) and List (2014) compiled cognacy wordlists for subsets of families",
      "startOffset" : 71,
      "endOffset" : 83
    }, {
      "referenceID" : 22,
      "context" : "• List (2014): Bai dialects (9/110), Chinese dialects (18/180), Huon (14/84), Japanese (10/200), ObUgrian (21/110; Hungarian excluded from Ugric sub-family), Tujia (5/107; Sino-Tibetan).",
      "startOffset" : 2,
      "endOffset" : 14
    } ],
    "year" : 2016,
    "abstractText" : "In this paper, we explore the use of convolutional networks (ConvNets) for the purpose of cognate identification. We compare our architecture with binary classifiers based on string similarity measures on different language families. Our experiments show that convolutional networks achieve competitive results across concepts and across language families at the task of cognate identification.",
    "creator" : "LaTeX with hyperref package"
  }
}