{
  "name" : "1608.08905.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Novel Online Real-time Classifier for Multi-label Data Streams",
    "authors" : [ "Rajasekar Venkatesan", "Meng Joo Er", "Shiqian Wu", "Mahardhika Pratama" ],
    "emails" : [ "RAJA0046@e.ntu.edu.sg", "EMJER@ntu.edu.sg", "shiqian.wu@wust.edu.cn", "m.pratama@latrobe.edu.au" ],
    "sections" : [ {
      "heading" : null,
      "text" : "based online multi-label classifier for real-time data streams is proposed. Multi-label classification is one of the actively researched machine learning paradigm that has gained much attention in the recent years due to its rapidly increasing real world applications. In contrast to traditional binary and multiclass classification, multi-label classification involves association of each of the input samples with a set of target labels simultaneously. There are no real-time online neural network based multi-label classifier available in the literature. In this paper, we exploit the inherent nature of high speed exhibited by the extreme learning machines to develop a novel online realtime classifier for multi-label data streams. The developed classifier is experimented with datasets from different application domains for consistency, performance and speed. The experimental studies show that the proposed method outperforms the existing state-of-the-art techniques in terms of speed and accuracy and can classify multi-label data streams in real-time.\nKeywords—Real-time, Classification, Multi-label, Online,\nExtreme learning machines, High speed.\nI. INTRODUCTION\nClassification in machine learning is the problem of identifying the function f(x) that maps each attribute vector xi to its associated target label yi, i = 1,2,….,n, where n is the total number of training samples [1]. Traditional classification problems in machine learning involve associating each of the sample instance with a single target label. i.e. unique target association. This type of classification is called single label classification. On the contrary, several real world classification problems involve data samples which correspond to a subset of target labels. This results in the emergence of a new category of machine learning classification called the multi-label classification. The multi-label classification problems are gaining much importance and attention in the recent years due to the rapidly increasing real world application areas. Some of the real world application domains that require multi-label classification are medical diagnosis [2, 3], text categorization [4-8], genomics, bioinformatics [9, 10], multimedia, emotion, music categorization, scene and video categorization [11-13], map labeling [14], marketing etc. Due to the omnipresence of\nmulti-label problems in a wide range of real world scenarios, multi-label classification is an emerging field in machine learning classification [15].\nThe traditional single label classification problems maps each of the input samples to a unique target label from the pool of available target labels. The single label classification problems can be categorized into binary and multi-class classification. When the number of available target labels is two, it is called binary classification. Binary classification is the most fundamental classification problem in which the input sample belongs to either of the two target class labels. Examples of binary classification problems include biometric security, medical diagnosis, etc. When the number of available target labels is greater than two, the classification problem is called multi-class classification. Biometric identification, character recognition and other similar classification problems are examples of multi-class classification. Binary classification is a special case of multi-class classification in which the number of target labels is two.\nThere are several real world applications in which the target labels are not mutually exclusive and requires the need for multi-label classification. Multi-label classification involves associating each of the input samples with a set of target labels. Therefore, multi-label classification forms the superset of binary and multi-class classification problems. When compared to single label classification, multi-label classification is more difficult and more complex due to the increased generality of the multi-label problems [16].\nSeveral machine learning techniques is available in the literature for multi-label classification problems. The existing multi-label classifiers available in the literature are based on Support Vector Machines (SVM), Decision Trees (DT), Extreme Learning Machines (ELM) etc. The machine learning techniques available can be broadly categorized into two categories: Batch learning and Online learning. Batch learning techniques involve collection of all the data samples in prior and estimating the system parameters by processing all the data concurrently. Batch learning techniques require all the training data beforehand and cannot learn from streaming data. This poses a major limitation to the applications of batch learning\ntechniques as several real world applications require learning from sequentially streaming data samples. Online learning is a family of machine learning techniques in which the learning is achieved by incrementally updating the system parameters from the data that arrives sequentially using single-pass learning procedure [17, 18]. Therefore, online learning techniques are preferred over batch learning techniques for real world applications [19, 20].\nIt is to be noted that there is very limited research on multilabel classification for streaming data applications [21]. Online techniques for multi-label classification are much to be explored. An ELM based online multi-label classifier is proposed for streaming data applications. The proposed ELM based online multi-label classifier outperforms the existing classifiers in speed and performance and also scalable for large scale streaming data applications.\nThe organization of the rest of the paper is as follows. Section 2 describes the preliminary discussion on multi-label classifiers and extreme learning machines. The details of the proposed method are elaborated in Section 3. The benchmark evaluation metrics and the experimentation specifications used for analyzing the proposed method are described in Section 4. Section 5 summarizes the experimentation results and performance comparison with state-of-the-art techniques and concluding remarks are given in Section 6.\nII. BACKGROUND AND PRELIMINARIES"
    }, {
      "heading" : "A. Multi-label Classifiers",
      "text" : "The summary of multi-label learning problem is given as,\n The input space X is of feature dimension D\nxi ϵ X, xi = (xi1,xi2,….xiD)\n The label space L is of dimension M\nL = {ζ1, ζ2,…., ζM}\n Each of the N training samples can be represented by a pair of tuples (input space and label space)\n{(xi,yi) | xi ϵ X, yi ϵ Y, Y ⊆ L, 1≤i≤N}\n A training model that maps the input tuple to output tuple.\nSorower [22] defines multi-label classification as, “Given a training set, S = (xi, yi), 1 ≤ i ≤ n, consisting of n training instances, (xi ϵ X, yi ϵ Y) drawn from an unknown distribution D, the goal of multi-label learning is to produce a multi-label classifier h:X→Y that optimizes some specific evaluation function or loss function”.\nIf there are M target class labels, and pi denotes the probability that the input sample is assigned to ith class, as opposed to the single label classification in which each of the input samples belongs to only one target label and the set of target labels are mutually exclusive, multi-label classification enables association of multiple labels to the input sample. Therefore, the following inequality holds true for multi-label classification.\n(1)\nThe existing multi-label classification techniques available in literature can be classified into two major categories: Batch learning techniques and online learning techniques. In literature, only a very limited number of application specific online multi-label techniques are available. An overview of exiting techniques is shown in Fig. 1.\n1) Batch Learning Methods\nThe multi-label classification techniques available in the literature are largely batch learning based methods. Tsoumakas et. al [23] categorized the existing batch learning based multilabel classification algorithm available in literature into two categories: Problem Transformation (PT) methods and Algorithm Adaptation (AA) methods. Madjarov et. al [24] extended the classification to include Ensemble (EN) based multi-label classification methods.\nPT methods transform the multi-label classification problems into multiple single-label classification problems. Existing single label classifiers are then used to perform the classification and finally the results from the multiple single label classifiers are combined together to provide the multilabel classification result. There are three sub-categories to the PT methods: Binary relevance methods, Pairwise methods and Label powerset method. AA methods extend the base classification algorithm to adapt multi-label problems. AA methods are algorithm-dependent methods. EN methods use an ensemble of PT and AA methods to achieve multi-label classification.\n2) Online Learning Methods\nThere are limited number of techniques available in the literature on multi-label classification for data streams [21]. A simpler approach is to use batch learning classifiers that trains on new batches of data streams by replacing the classifiers of previous batches. This type of learning is called batchincremental learning. The first work on multi-label classifier for data streams is based on ensemble of classifiers which are trained on successive data chunks [25]. The paper by Read et. al [26] proposes multi-label stream classification by extending the heoffding tree [27] by using batch multi-label classifier in each node. Spyromitros-Xioufis [28] proposes binary relevance and kNN based multi-label classifier for data streams. Microsoft [29] developed an Active Learning framework for multi-label classification as the result of the increase in demand for the need of multi-label classification in real world multimedia datasets. A Passive-Aggressive method is proposed by Crammer et. al [30] for multi-label classification. A Bayesian Online Multi-label Classification (BOMC) method is developed by Zhang et. al [31] for online multi-label classification. The Passive Aggressive and the Bayesian Online Multi-label Classification techniques are application specific and are implemented only for text categorization datasets."
    }, {
      "heading" : "B. Extreme Learning Machines",
      "text" : "ELM is a single-hidden layer feedforward neural network based learning technique. The special feature of ELM is that the initial weights and the hidden layer bias can be selected at random. This results in high speed training and small number of tunable parameters thus enabling ELM to have fast learning speed and generalization of performance. The universal approximation capability and generalization ability [32] are the key distinguishing factors of ELM. Several variants of ELM has been developed [33-36]. A condensed overview of ELM algorithm as adapted from [32, 37] is discussed.\nConsider there are N training samples in the multi-label dataset. Let L be the target label space represented as L = {ζ1, ζ2,…., ζM}. Consider the input is of the form, xi = [xi1,xi2,…,xin]T ϵ Rn and the corresponding output is represented as yi = [yi1,yi2,…yim]T and Y⊆L. Let be the number of hidden layer neurons, the output ‘o’ of the single hidden layer feedforward neural network is given by\n(2)\nwhere, wi = [wi1,wi2,…win]T is the input weight, g(x) is the activation function, and bi is the hidden layer bias and βi = [βi1,βi2,…βim]T is the output weight.\nThe network should be trained such that the error difference between the actual output and the predicted output is 0.\n(3)\nThus, the output of the ELM classifier is given as,\n(4)\nThe equation (4) written in matrix form is represented as,\nHβ = Y (5)\nwhere,\n(6)\n(7)\n(8)\nThe output weights of the ELM network can be estimated using the equation\nβ = H+Y (9)\nwhere H+ is the Moore-Penrose inverse of the hidden layer output matrix H, and it can be calculated as follows:\nH+ = (HTH)-1HT (10)\nThere are several papers [32, 37, 38] available in literature that elaborates on the theory and the mathematical background behind the ELM and hence are not discussed here. There are other similar neural network based techniques [39, 40] which did not gain popularity and are largely forgotten.\nIII. PROPOSED APPROACH\nThis paper exploits the inherent high speed nature of the ELM and OS-ELM to develop an online sequential multi-label classifier for real-time streaming data applications. The key novelty of the proposed approach is that, there are no online techniques available thus far in literature to perform real-time multi-label classification.\nIn single label classification problems such as binary and multi-class classification, each input sample corresponds to a single target label. Therefore the classifier is required to identify the single target label corresponding to the input sample. On the contrary, in multi-label classification, each of the input samples belongs to a subset of target labels. Therefore, the multi-label classifier is required to identify both the number of labels and the identity of the labels in order to perform multi-label classification. This results in the increased complexity of the multi-label classification problems. Another key challenge in implementing a generic multi-label classifier is that, not all datasets are equally multi-labelled. The degree of multi-labelness varies for every dataset. The increased complexity and the varying degree of multi-labelness are the two major challenges in developing a multi-label classifier.\nThe proposed method falls under the algorithm adaptation techniques category as the base algorithm is extended to adapt to the multi-label problem. The various steps involved in the proposed online sequential multi-label ELM (OSML-ELM) approach are\n Initialization\n Pre-processing\n ELM Training\n ELM Testing\n Multi-label identification\nPre-processing and post-processing of data are of prime importance in extending the ELM based technique for multilabel classification.\nInitialization: The fundamental parameters of the ELM network such as the number of hidden layer neurons and the activation function are initialized. The number of hidden layer neurons is selected for each dataset so as to avoid the overfitting problem. The input weights and the bias value of the network are randomly initialized.\nPre-processing: In single label classification, each of the input samples corresponds to only one target class. Therefore, the dimension of the target output label is always fixed at 1. On the contrary, in multi-label classification, each input is associated with an M-tuple output label with each element of the set as 0 or 1 representing the belongingness of the input corresponding to the target labels. Therefore the dimension of the target output label is ‘M’. The label set denoting the belongingness for each of the labels is converted from unipolar representation to bipolar representation.\nELM Training: The processed input is then supplied sequentially to the online sequential variant of the ELM\ntechnique. Let N0 be the number of samples in the initial block, from equations (9) and (10), the initial output weight β0 is calculated as β0 = M0H0TY where M0 = (H0TH0)-1.\nUpon calculating the initial output weight β0, for each sequentially arriving data/block of data, the output weights of the network are updated based on recursive least square using the equations,\n(11)\n(12)\nwhere k = 0,1,2…. N-N0-1.\nThe theory and the mathematics behind using the recursive least square for online sequential ELM is discussed in several papers [41, 42] in the literature.\nAlgorithm: Proposed OSML-ELM algorithm for multi-\nlabel classification\n1. Initialization: The fundamental parameters of the network are initialized\n2. Pre-processing: The raw input data is processed for classification\n3. ELM Training:\n– Initial phase\nProcessing of initial block of data\nM0 = (H0TH0)-1 β0 = M0H0TY0\n– Sequential phase\nOnline processing of sequential data\n– Threshold identification\n4. ELM Testing:\nEstimation of raw output values using Y = Hβ\n5. Post-processing and multi-label identification\nThe raw output values is compared with the threshold\nvalue.\nSeparation into two categories of labels (Labels that the data sample belong to and labels the data sample\ndoes not belong to)\nIdentifying the number of labels corresponding to input data sample\nIdentifying the target class labels for the input data\nsample\nELM Testing: During the testing phase, the network computes the predicted raw output value Y using the formula Y = Hβ where β is output weight obtained during the training phase.\nMulti-label Identification: The multi-label identification step is the key step in extending the ELM based technique for multi-label classification. As foreshadowed, multi-label classifiers are required to predict both the number of target labels and the identity of the target labels corresponding to\neach of the input samples. Since the number of labels corresponding to each input is completely unknown and dynamic, a thresholding based technique is used. The threshold value is selected during the training phase such that it maximizes the separation between the family of labels the input belongs to and the family of labels the input does not belong to. Setting up of the threshold value is of prime importance as it directly affects the performance of the classifier. The raw output values Y obtained from the previous step is then compared to the threshold value. The number of raw output values that are greater than the threshold determines the number of target labels corresponding the input sample and the index of the corresponding values determines the identity of the target labels. The overview of the proposed algorithm is summarized.\nIV. EXPERIMENTATION\nThis section elaborates the experimental design and the dataset specifications used to evaluate the performance of the proposed technique. Multi-label problems have a unique feature called the degree of multi-labelness. In other words, not all datasets are equally multi-labelled. The multi-label nature of datasets varies widely from each other. The differences in the number of labels, the number of samples having multiple labels and the average number of target labels for each sample result in the varying degree of multi-labelness to each dataset. Two metrics, label cardinality (LC) and label density (LD) are used to quantitatively measure the degree of multi-labelness of the datasets. Label cardinality is the average number of labels corresponding to each sample in the dataset. Label density also factors the number of labels in the dataset in addition to the average number of labels [23]. The LC and LD can be calculated using the following equations.\n(13)\n(14)\nwhere, N is the number of training samples, L is the label set and Yi gives the multi-label belongingness to target labels corresponding to each input sample. The impact of differences in LC and LD and its influence on the performance of the classifier is discussed by Bernardini et. al in [43]. Two datasets have same LC and different LD or vice versa can result in significant variation in the performance of the classifier [16].\nThe proposed technique is experimented with five datasets from different application domains and wide range of LC and LD. The specifications of the dataset used are tabulated in Table 1. The performance metrics such as hamming loss, accuracy, F1 measure, training time and testing time are evaluated for the five datasets and the results are compared with five state-of-the-art techniques. The details of the state-ofthe-art techniques are given in Table 2.\nThe proposed technique is evaluated in terms of consistency, performance and compatibility for large scale streaming data applications."
    }, {
      "heading" : "A. Performance Metrics",
      "text" : "The complex nature of multi-label problems results in a unique feature of multi-label classifiers called partial\ncorrectness of results. Since both the number of labels and the\nlabel identities are to be predicted by the multi-label classifier,\nthe problem of partial correctness arises. The classifier can\nwrongly predict either the number of labels corresponding to\nan input sample is the identity of corresponding target labels. Therefore, the hamming loss performance metric is used to\nquantitatively evaluate the performance of the classifier along\nwith accuracy, precision, recall and F1 measure. The hamming\nloss is calculated as the fraction of wrong labels to the total\nnumber of labels. Hamming loss for an ideal classifier is zero. Also, the training time and the testing time of the proposed\ntechnique is evaluated and compared with the state-of-the-art\ntechniques. The results for the performance metrics by the proposed technique is tabulated in Table 3. The proposed technique is compared with five different state-of-the-art techniques and the results are given in Tables 4-5 and Fig. 2-3.\nFrom the results it can be clearly seen that the proposed technique outperforms all the existing techniques in terms of speed and performance."
    }, {
      "heading" : "B. Consistency",
      "text" : "Consistency is the key virtue that is essential for any new technique. Since the initial weights of the networks are\nrandomly chosen for an ELM based technique, evaluation of\nconsistency is of critical importance. Therefore the proposed\ntechnique is evaluated for consistency using 5-fold and 10-\nfold cross validation. The results obtained are tabulated in\nTable 6."
    }, {
      "heading" : "C. Streaming Data Classification",
      "text" : "The proposed OSML-ELM technique is a real-time online multi-label classifier for streaming data applications. The\nperformance of the proposed method is experimented on five\ndatasets of different domains with a wide range of LC and LD.\nFrom the results it is evident that the proposed method is\nconsistent and outperforms the existing state-of-the-art\ntechniques in terms of speed and remains one of the top methods in terms of performance. The high-speed nature of\nthe OSML-ELM supports scalability of the proposed\ntechnique for real-time data streams."
    }, {
      "heading" : "ACKNOWLEDGEMENT",
      "text" : "The authors would like to acknowledge the funding support from the Ministry of Education, Singapore (Tier 1 AcRF,\nRG30/14), Rajasekar Venkatesan is supported by NTU\nResearch Student Scholarship.\nREFERENCES\n[1] A. P. L. F. de Carvalho and A. Freitas, \"A Tutorial on Multi-label\nClassification Techniques,\" in Foundations of Computational Intelligence Volume 5, Springer Berlin Heidelberg, 2009, pp. 177-195. [2] A. Karali and V. Pirnat, \"Significance level based multiple tree classification,\" Informatica, vol. 15, no. 5, 1991. [3] H. Shao, G. Li, G. Liu, and Y. Wang, \"Symptom selection for multi-\nlabel data of inquiry diagnosis in traditional Chinese medicine,\" Science China Information Sciences, vol. 56, pp. 1-13, 2013. [4] T. Gonçalves and P. Quaresma, \"A Preliminary Approach to the\nMultilabel Classification Problem of Portuguese Juridical Documents,\" Progress in Artificial Intelligence. vol. 2902, pp. 435-444, 2003. [5] T. Joachims, \"Text categorization with Support Vector Machines:\nLearning with many relevant features,\" in Machine Learning: ECML-98. vol. 1398, C. Nédellec and C. Rouveirol, Eds., ed: Springer Berlin Heidelberg, 1998, pp. 137-142. [6] X. Luo and A. N. Zincir-Heywood, \"Evaluation of Two Systems on Multi-class Multi-label Document Classification,\" in Foundations of\nIntelligent Systems. vol. 3488, M.-S. Hacid, N. Murray, Z. Raś, and S.\nTsumoto, Eds., ed: Springer Berlin Heidelberg, 2005, pp. 161-169. [7] D. Tikk and G. Biró, \"Experiments with multi-label text classifier on the\nReuters collection,\" in Proceedings of the international conference on\ncomputational cybernetics (ICCC 03), 2003, pp. 33-38. [8] K. Yu, S. Yu, and V. Tresp, \"Multi-label informed latent semantic\nindexing,\" presented at the Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, Salvador, Brazil, 2005. [9] A. Elisseeff and J. Weston, \"A kernel method for multi-labelled classification,\" in Advances in neural information processing systems,\n2001, pp. 681-687.\n[10] X. Wang, W. Zhang, Q. Zhang, and G.-Z. Li, \"MultiP-SChlo: multilabel protein subchloroplast localization prediction with Chou’s pseudo\namino acid composition and a novel multi-label classifier,\"\nBioinformatics, vol. 31, pp. 2639-2645, 2015. [11] M. Boutell, X. Shen, J. Luo, and C. Brown, \"Multi-label semantic scene\nclassification,\" technical report, dept. comp. sci. u. rochester2003.\n[12] X. Shen, M. Boutell, J. Luo, and C. Brown, \"Multilabel machine learning and its application to semantic scene classification,\" in\nElectronic Imaging, 2003, pp. 188-199.\n[13] W. Indyk, T. Kajdanowicz, and P. Kazienko, \"Relational large scale multi-label classification method for video categorization,\" Multimedia\nTools and Applications, vol. 65, pp. 63-74, 2013.\n[14] B. Zhu and C. K. Poon, \"Efficient Approximation Algorithms for Multilabel Map Labeling,\" in Algorithms and Computation. vol. 1741, ed:\nSpringer Berlin Heidelberg, 1999, pp. 143-152.\n[15] G. Tsoumakas, I. Katakis, and I. Vlahavas, \"Mining Multi-label Data,\" in Data Mining and Knowledge Discovery Handbook, O. Maimon and\nL. Rokach, Eds., ed: Springer US, 2010, pp. 667-685.\n[16] M.-L. Zhang and Z.-H. Zhou, \"ML-KNN: A lazy learning approach to multi-label learning,\" Pattern Recognition, vol. 40, pp. 2038-2048,\n2007.\n[17] M. Pratama, S. Anavatti, and J. Lu, \"Recurrent Classifier based on An Incremental Meta-Cognitive-based Scaffolding Algorithm,\" Fuzzy\nSystems, IEEE Transactions on, vol. 23, no. 6, pp. 2048-2066, 2015.\n[18] M. Pratama, J. Lu, and G. Zhang, \"Evolving Type-2 Fuzzy Classifier,\" Fuzzy Systems, IEEE Transactions on, 2015. [19] M. Pratama, S. G. Anavatti, J. Meng, and E. D. Lughofer, \"pClass: An\nEffective Classifier for Streaming Examples,\" Fuzzy Systems, IEEE Transactions on, vol. 23, pp. 369-386, 2015. [20] M. Pratama, J. Lu, S. Anavatti, E. Lughofer, and C.-P. Lim, \"An\nincremental meta-cognitive-based scaffolding fuzzy neural network,\" Neurocomputing, vol. In Press, 2015. [21] J. R. Sato, M. Q. Hoexter, X. F. Castellanos, and L. A. Rohde,\n\"Abnormal brain connectivity patterns in adults with ADHD: a coherence study,\" BioMed Research International, 2014. [22] M. S. Sorower, \"A literature survey on algorithms for multi-label\nlearning,\" Oregon State University, Corvallis, 2010. [23] G. Tsoumakas and I. Katakis, \"Multi-label classification: An overview,\"\nDept. of Informatics, Aristotle University of Thessaloniki, Greece, 2006.\n[24] G. Madjarov, D. Kocev, D. Gjorgjevikj, and S. Džeroski, \"An extensive\nexperimental comparison of methods for multi-label learning,\" Pattern Recognition, vol. 45, pp. 3084-3104, 2012. [25] H. Abdulsalam, D. B. Skillicorn, and P. Martin, \"Classifying evolving\ndata streams using dynamic streaming random forests,\" in Database and Expert Systems Applications, 2008, pp. 643-651. [26] J. Read, A. Bifet, G. Holmes, and B. Pfahringer, \"Efficient multi-label\nclassification for evolving data streams,\" 2010. [27] P. Domingos and G. Hulten, \"Mining high-speed data streams,\" in\nProceedings of the sixth ACM SIGKDD international conference on\nKnowledge discovery and data mining, 2000, pp. 71-80. [28] E. Spyromitros-Xioufis, \"Dealing with concept drift and class imbalance\nin multi-label stream classification,\" Department of Computer Science,\nAristotle University of Thessaloniki, 2011. [29] X.-S. Hua and G.-J. Qi, \"Online multi-label active learning for large-\nscale multimedia annotation,\" TechReport MSR-TR-2008-1032008.\n[30] Y. S. Crammer, \"Online learning of complex categorical problems,\" Hebrew University of Jerusalem, 2004. [31] X. Zhang, T. Graepel, and R. Herbrich, \"Bayesian online learning for\nmulti-label and multi-variate performance measures,\" in International Conference on Artificial Intelligence and Statistics, 2010, pp. 956-963. [32] G.-B. Huang, D. Wang, and Y. Lan, \"Extreme learning machines: a\nsurvey,\" International Journal of Machine Learning and Cybernetics, vol. 2, pp. 107-122, 2011. [33] W. Zong, G.-B. Huang, and Y. Chen, \"Weighted extreme learning\nmachine for imbalance learning,\" Neurocomputing, vol. 101, pp. 229- 242, 2013. [34] L. Jiahua, V. Chi-Man, and W. Pak-Kin, \"Sparse Bayesian Extreme Learning Machine for Multi-classification,\" Neural Networks and\nLearning Systems, IEEE Transactions on, vol. 25, pp. 836-843, 2014.\n[35] W. Ning, E. Meng Joo, and H. Min, \"Parsimonious Extreme Learning Machine Using Recursive Orthogonal Least Squares,\" Neural Networks\nand Learning Systems, IEEE Transactions on, vol. 25, pp. 1828-1841,\n2014. [36] Y. Lan, Y. C. Soh, and G.-B. Huang, \"Ensemble of online sequential\nextreme learning machine,\" Neurocomputing, vol. 72, pp. 3391-3395,\n2009. [37] G.-B. Huang, Q.-Y. Zhu, and C.-K. Siew, \"Extreme learning machine:\nTheory and applications,\" Neurocomputing, vol. 70, pp. 489-501, 2006.\n[38] S. Ding, H. Zhao, Y. Zhang, X. Xu, and R. Nie, \"Extreme learning machine: algorithm, theory and applications,\" Artificial Intelligence\nReview, vol. 44, pp. 103-115, 2015.\n[39] W. F. Schmidt, M. A. Kraaijveld, and R. P. Duin, \"Feedforward neural networks with random weights,\" in Pattern Recognition, 1992. Vol. II.\nConference B: Pattern Recognition Methodology and Systems,\nProceedings., 11th IAPR International Conference on, 1992, pp. 1-4. [40] Y. H. Pao and Y. Takefuji, \"Functional-link net computing: theory,\nsystem architecture, and functionalities,\" Computer, vol. 25, pp. 76-79,\n1992. [41] B. Li, J. Wang, Y. Li, and Y. Song, \"An improved on-line sequential\nlearning algorithm for extreme learning machine,\" Advances in Neural\nNetworks–ISNN 2007, pp. 1087-1093, 2007. [42] N.-Y. Liang, G.-B. Huang, P. Saratchandran, and N. Sundararajan, \"A\nfast and accurate online sequential learning algorithm for feedforward networks,\" Neural Networks, IEEE Transactions on, vol. 17, pp. 1411- 1423, 2006. [43] F. C. Bernardini, R. B. da Silva, R. M. Rodovalho, and E. B. M. Meza,\n\"Cardinality and density measures and their influence to multi-label learning methods,\" Submitted to Learning and Nonlinear Models, 2014."
    } ],
    "references" : [ {
      "title" : "A Tutorial on Multi-label Classification Techniques",
      "author" : [ "A.P.L.F. de Carvalho", "A. Freitas" ],
      "venue" : "Foundations of Computational Intelligence Volume 5, Springer Berlin Heidelberg, 2009, pp. 177-195.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Significance level based multiple tree classification",
      "author" : [ "A. Karali", "V. Pirnat" ],
      "venue" : "Informatica, vol. 15, no. 5, 1991.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Symptom selection for multilabel data of inquiry diagnosis in traditional Chinese medicine",
      "author" : [ "H. Shao", "G. Li", "G. Liu", "Y. Wang" ],
      "venue" : "Science China Information Sciences, vol. 56, pp. 1-13, 2013.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A Preliminary Approach to the Multilabel Classification Problem of Portuguese Juridical Documents",
      "author" : [ "T. Gonçalves", "P. Quaresma" ],
      "venue" : "Progress in Artificial Intelligence. vol. 2902, pp. 435-444, 2003.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Text categorization with Support Vector Machines: Learning with many relevant features",
      "author" : [ "T. Joachims" ],
      "venue" : "Machine Learning: ECML-98. vol. 1398, C. Nédellec and C. Rouveirol, Eds., ed: Springer Berlin Heidelberg, 1998, pp. 137-142.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Evaluation of Two Systems on Multi-class Multi-label Document Classification",
      "author" : [ "X. Luo", "A.N. Zincir-Heywood" ],
      "venue" : "Foundations of Intelligent Systems. vol. 3488, M.-S. Hacid, N. Murray, Z. Raś, and S. Tsumoto, Eds., ed: Springer Berlin Heidelberg, 2005, pp. 161-169.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Experiments with multi-label text classifier on the Reuters collection",
      "author" : [ "D. Tikk", "G. Biró" ],
      "venue" : "Proceedings of the international conference on computational cybernetics (ICCC 03), 2003, pp. 33-38.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Multi-label informed latent semantic indexing",
      "author" : [ "K. Yu", "S. Yu", "V. Tresp" ],
      "venue" : "presented at the Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, Salvador, Brazil, 2005.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "A kernel method for multi-labelled classification",
      "author" : [ "A. Elisseeff", "J. Weston" ],
      "venue" : "Advances in neural information processing systems, 2001, pp. 681-687.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "MultiP-SChlo: multilabel protein subchloroplast localization prediction with Chou’s pseudo amino acid composition and a novel multi-label classifier",
      "author" : [ "X. Wang", "W. Zhang", "Q. Zhang", "G.-Z. Li" ],
      "venue" : "Bioinformatics, vol. 31, pp. 2639-2645, 2015.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Multi-label semantic scene classification",
      "author" : [ "M. Boutell", "X. Shen", "J. Luo", "C. Brown" ],
      "venue" : "technical report, dept. comp. sci. u. rochester2003.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Multilabel machine learning and its application to semantic scene classification",
      "author" : [ "X. Shen", "M. Boutell", "J. Luo", "C. Brown" ],
      "venue" : "Electronic Imaging, 2003, pp. 188-199.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Relational large scale multi-label classification method for video categorization",
      "author" : [ "W. Indyk", "T. Kajdanowicz", "P. Kazienko" ],
      "venue" : "Multimedia Tools and Applications, vol. 65, pp. 63-74, 2013.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Efficient Approximation Algorithms for Multilabel Map Labeling",
      "author" : [ "B. Zhu", "C.K. Poon" ],
      "venue" : "Algorithms and Computation. vol. 1741, ed: Springer Berlin Heidelberg, 1999, pp. 143-152.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Mining Multi-label Data",
      "author" : [ "G. Tsoumakas", "I. Katakis", "I. Vlahavas" ],
      "venue" : "Data Mining and Knowledge Discovery Handbook, O. Maimon and L. Rokach, Eds., ed: Springer US, 2010, pp. 667-685.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "ML-KNN: A lazy learning approach to multi-label learning",
      "author" : [ "M.-L. Zhang", "Z.-H. Zhou" ],
      "venue" : "Pattern Recognition, vol. 40, pp. 2038-2048, 2007.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Recurrent Classifier based on An Incremental Meta-Cognitive-based Scaffolding Algorithm",
      "author" : [ "M. Pratama", "S. Anavatti", "J. Lu" ],
      "venue" : "Fuzzy Systems, IEEE Transactions on, vol. 23, no. 6, pp. 2048-2066, 2015.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Evolving Type-2 Fuzzy Classifier",
      "author" : [ "M. Pratama", "J. Lu", "G. Zhang" ],
      "venue" : "Fuzzy Systems, IEEE Transactions on, 2015.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "pClass: An Effective Classifier for Streaming Examples",
      "author" : [ "M. Pratama", "S.G. Anavatti", "J. Meng", "E.D. Lughofer" ],
      "venue" : "Fuzzy Systems, IEEE Transactions on, vol. 23, pp. 369-386, 2015.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "An incremental meta-cognitive-based scaffolding fuzzy neural network",
      "author" : [ "M. Pratama", "J. Lu", "S. Anavatti", "E. Lughofer", "C.-P. Lim" ],
      "venue" : "Neurocomputing, vol. In Press, 2015.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Abnormal brain connectivity patterns in adults with ADHD: a coherence study",
      "author" : [ "J.R. Sato", "M.Q. Hoexter", "X.F. Castellanos", "L.A. Rohde" ],
      "venue" : "BioMed Research International, 2014.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A literature survey on algorithms for multi-label learning",
      "author" : [ "M.S. Sorower" ],
      "venue" : "Oregon State University, Corvallis, 2010.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Multi-label classification: An overview",
      "author" : [ "G. Tsoumakas", "I. Katakis" ],
      "venue" : "Dept. of Informatics, Aristotle University of Thessaloniki, Greece, 2006.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "An extensive experimental comparison of methods for multi-label learning",
      "author" : [ "G. Madjarov", "D. Kocev", "D. Gjorgjevikj", "S. Džeroski" ],
      "venue" : "Pattern Recognition, vol. 45, pp. 3084-3104, 2012.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Classifying evolving data streams using dynamic streaming random forests",
      "author" : [ "H. Abdulsalam", "D.B. Skillicorn", "P. Martin" ],
      "venue" : "Database and Expert Systems Applications, 2008, pp. 643-651.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Efficient multi-label classification for evolving data streams",
      "author" : [ "J. Read", "A. Bifet", "G. Holmes", "B. Pfahringer" ],
      "venue" : "2010.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Mining high-speed data streams",
      "author" : [ "P. Domingos", "G. Hulten" ],
      "venue" : "Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining, 2000, pp. 71-80.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Dealing with concept drift and class imbalance in multi-label stream classification",
      "author" : [ "E. Spyromitros-Xioufis" ],
      "venue" : "Department of Computer Science, Aristotle University of Thessaloniki, 2011.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Online multi-label active learning for largescale multimedia annotation",
      "author" : [ "X.-S. Hua", "G.-J. Qi" ],
      "venue" : "TechReport MSR-TR-2008-1032008.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Online learning of complex categorical problems",
      "author" : [ "Y.S. Crammer" ],
      "venue" : "Hebrew University of Jerusalem, 2004.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Bayesian online learning for multi-label and multi-variate performance measures",
      "author" : [ "X. Zhang", "T. Graepel", "R. Herbrich" ],
      "venue" : "International Conference on Artificial Intelligence and Statistics, 2010, pp. 956-963.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Extreme learning machines: a survey",
      "author" : [ "G.-B. Huang", "D. Wang", "Y. Lan" ],
      "venue" : "International Journal of Machine Learning and Cybernetics, vol. 2, pp. 107-122, 2011.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Weighted extreme learning machine for imbalance learning",
      "author" : [ "W. Zong", "G.-B. Huang", "Y. Chen" ],
      "venue" : "Neurocomputing, vol. 101, pp. 229- 242, 2013.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Sparse Bayesian Extreme Learning Machine for Multi-classification",
      "author" : [ "L. Jiahua", "V. Chi-Man", "W. Pak-Kin" ],
      "venue" : "Neural Networks and Learning Systems, IEEE Transactions on, vol. 25, pp. 836-843, 2014.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Parsimonious Extreme Learning Machine Using Recursive Orthogonal Least Squares",
      "author" : [ "W. Ning", "E. Meng Joo", "H. Min" ],
      "venue" : "Neural Networks and Learning Systems, IEEE Transactions on, vol. 25, pp. 1828-1841, 2014.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 1828
    }, {
      "title" : "Ensemble of online sequential extreme learning machine",
      "author" : [ "Y. Lan", "Y.C. Soh", "G.-B. Huang" ],
      "venue" : "Neurocomputing, vol. 72, pp. 3391-3395, 2009.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Extreme learning machine: Theory and applications",
      "author" : [ "G.-B. Huang", "Q.-Y. Zhu", "C.-K. Siew" ],
      "venue" : "Neurocomputing, vol. 70, pp. 489-501, 2006.",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Extreme learning machine: algorithm, theory and applications",
      "author" : [ "S. Ding", "H. Zhao", "Y. Zhang", "X. Xu", "R. Nie" ],
      "venue" : "Artificial Intelligence Review, vol. 44, pp. 103-115, 2015.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Feedforward neural networks with random weights",
      "author" : [ "W.F. Schmidt", "M.A. Kraaijveld", "R.P. Duin" ],
      "venue" : "Pattern Recognition, 1992. Vol. II. Conference B: Pattern Recognition Methodology and Systems, Proceedings., 11th IAPR International Conference on, 1992, pp. 1-4.",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Functional-link net computing: theory, system architecture, and functionalities",
      "author" : [ "Y.H. Pao", "Y. Takefuji" ],
      "venue" : "Computer, vol. 25, pp. 76-79, 1992.",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "An improved on-line sequential learning algorithm for extreme learning machine",
      "author" : [ "B. Li", "J. Wang", "Y. Li", "Y. Song" ],
      "venue" : "Advances in Neural Networks–ISNN 2007, pp. 1087-1093, 2007.",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "A fast and accurate online sequential learning algorithm for feedforward networks",
      "author" : [ "N.-Y. Liang", "G.-B. Huang", "P. Saratchandran", "N. Sundararajan" ],
      "venue" : "Neural Networks, IEEE Transactions on, vol. 17, pp. 1411- 1423, 2006.",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Cardinality and density measures and their influence to multi-label learning methods",
      "author" : [ "F.C. Bernardini", "R.B. da Silva", "R.M. Rodovalho", "E.B.M. Meza" ],
      "venue" : "Submitted to Learning and Nonlinear Models, 2014.",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : ",n, where n is the total number of training samples [1].",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 1,
      "context" : "Some of the real world application domains that require multi-label classification are medical diagnosis [2, 3], text categorization [4-8], genomics, bioinformatics [9, 10], multimedia, emotion, music categorization, scene and video categorization [11-13], map labeling [14], marketing etc.",
      "startOffset" : 105,
      "endOffset" : 111
    }, {
      "referenceID" : 2,
      "context" : "Some of the real world application domains that require multi-label classification are medical diagnosis [2, 3], text categorization [4-8], genomics, bioinformatics [9, 10], multimedia, emotion, music categorization, scene and video categorization [11-13], map labeling [14], marketing etc.",
      "startOffset" : 105,
      "endOffset" : 111
    }, {
      "referenceID" : 3,
      "context" : "Some of the real world application domains that require multi-label classification are medical diagnosis [2, 3], text categorization [4-8], genomics, bioinformatics [9, 10], multimedia, emotion, music categorization, scene and video categorization [11-13], map labeling [14], marketing etc.",
      "startOffset" : 133,
      "endOffset" : 138
    }, {
      "referenceID" : 4,
      "context" : "Some of the real world application domains that require multi-label classification are medical diagnosis [2, 3], text categorization [4-8], genomics, bioinformatics [9, 10], multimedia, emotion, music categorization, scene and video categorization [11-13], map labeling [14], marketing etc.",
      "startOffset" : 133,
      "endOffset" : 138
    }, {
      "referenceID" : 5,
      "context" : "Some of the real world application domains that require multi-label classification are medical diagnosis [2, 3], text categorization [4-8], genomics, bioinformatics [9, 10], multimedia, emotion, music categorization, scene and video categorization [11-13], map labeling [14], marketing etc.",
      "startOffset" : 133,
      "endOffset" : 138
    }, {
      "referenceID" : 6,
      "context" : "Some of the real world application domains that require multi-label classification are medical diagnosis [2, 3], text categorization [4-8], genomics, bioinformatics [9, 10], multimedia, emotion, music categorization, scene and video categorization [11-13], map labeling [14], marketing etc.",
      "startOffset" : 133,
      "endOffset" : 138
    }, {
      "referenceID" : 7,
      "context" : "Some of the real world application domains that require multi-label classification are medical diagnosis [2, 3], text categorization [4-8], genomics, bioinformatics [9, 10], multimedia, emotion, music categorization, scene and video categorization [11-13], map labeling [14], marketing etc.",
      "startOffset" : 133,
      "endOffset" : 138
    }, {
      "referenceID" : 8,
      "context" : "Some of the real world application domains that require multi-label classification are medical diagnosis [2, 3], text categorization [4-8], genomics, bioinformatics [9, 10], multimedia, emotion, music categorization, scene and video categorization [11-13], map labeling [14], marketing etc.",
      "startOffset" : 165,
      "endOffset" : 172
    }, {
      "referenceID" : 9,
      "context" : "Some of the real world application domains that require multi-label classification are medical diagnosis [2, 3], text categorization [4-8], genomics, bioinformatics [9, 10], multimedia, emotion, music categorization, scene and video categorization [11-13], map labeling [14], marketing etc.",
      "startOffset" : 165,
      "endOffset" : 172
    }, {
      "referenceID" : 10,
      "context" : "Some of the real world application domains that require multi-label classification are medical diagnosis [2, 3], text categorization [4-8], genomics, bioinformatics [9, 10], multimedia, emotion, music categorization, scene and video categorization [11-13], map labeling [14], marketing etc.",
      "startOffset" : 248,
      "endOffset" : 255
    }, {
      "referenceID" : 11,
      "context" : "Some of the real world application domains that require multi-label classification are medical diagnosis [2, 3], text categorization [4-8], genomics, bioinformatics [9, 10], multimedia, emotion, music categorization, scene and video categorization [11-13], map labeling [14], marketing etc.",
      "startOffset" : 248,
      "endOffset" : 255
    }, {
      "referenceID" : 12,
      "context" : "Some of the real world application domains that require multi-label classification are medical diagnosis [2, 3], text categorization [4-8], genomics, bioinformatics [9, 10], multimedia, emotion, music categorization, scene and video categorization [11-13], map labeling [14], marketing etc.",
      "startOffset" : 248,
      "endOffset" : 255
    }, {
      "referenceID" : 13,
      "context" : "Some of the real world application domains that require multi-label classification are medical diagnosis [2, 3], text categorization [4-8], genomics, bioinformatics [9, 10], multimedia, emotion, music categorization, scene and video categorization [11-13], map labeling [14], marketing etc.",
      "startOffset" : 270,
      "endOffset" : 274
    }, {
      "referenceID" : 14,
      "context" : "Due to the omnipresence of multi-label problems in a wide range of real world scenarios, multi-label classification is an emerging field in machine learning classification [15].",
      "startOffset" : 172,
      "endOffset" : 176
    }, {
      "referenceID" : 15,
      "context" : "When compared to single label classification, multi-label classification is more difficult and more complex due to the increased generality of the multi-label problems [16].",
      "startOffset" : 168,
      "endOffset" : 172
    }, {
      "referenceID" : 16,
      "context" : "Online learning is a family of machine learning techniques in which the learning is achieved by incrementally updating the system parameters from the data that arrives sequentially using single-pass learning procedure [17, 18].",
      "startOffset" : 218,
      "endOffset" : 226
    }, {
      "referenceID" : 17,
      "context" : "Online learning is a family of machine learning techniques in which the learning is achieved by incrementally updating the system parameters from the data that arrives sequentially using single-pass learning procedure [17, 18].",
      "startOffset" : 218,
      "endOffset" : 226
    }, {
      "referenceID" : 18,
      "context" : "Therefore, online learning techniques are preferred over batch learning techniques for real world applications [19, 20].",
      "startOffset" : 111,
      "endOffset" : 119
    }, {
      "referenceID" : 19,
      "context" : "Therefore, online learning techniques are preferred over batch learning techniques for real world applications [19, 20].",
      "startOffset" : 111,
      "endOffset" : 119
    }, {
      "referenceID" : 20,
      "context" : "It is to be noted that there is very limited research on multilabel classification for streaming data applications [21].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 21,
      "context" : "Sorower [22] defines multi-label classification as, “Given a training set, S = (xi, yi), 1 ≤ i ≤ n, consisting of n training instances, (xi ε X, yi ε Y) drawn from an unknown distribution D, the goal of multi-label learning is to produce a multi-label classifier h:X→Y that optimizes some specific evaluation function or loss function”.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 22,
      "context" : "al [23] categorized the existing batch learning based multilabel classification algorithm available in literature into two categories: Problem Transformation (PT) methods and Algorithm Adaptation (AA) methods.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 23,
      "context" : "al [24] extended the classification to include Ensemble (EN) based multi-label classification methods.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 20,
      "context" : "There are limited number of techniques available in the literature on multi-label classification for data streams [21].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 24,
      "context" : "The first work on multi-label classifier for data streams is based on ensemble of classifiers which are trained on successive data chunks [25].",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 25,
      "context" : "al [26] proposes multi-label stream classification by extending the heoffding tree [27] by using batch multi-label classifier in each node.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 26,
      "context" : "al [26] proposes multi-label stream classification by extending the heoffding tree [27] by using batch multi-label classifier in each node.",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 27,
      "context" : "Spyromitros-Xioufis [28] proposes binary relevance and kNN based multi-label classifier for data streams.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 28,
      "context" : "Microsoft [29] developed an Active Learning framework for multi-label classification as the result of the increase in demand for the need of multi-label classification in real world multimedia datasets.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 29,
      "context" : "al [30] for multi-label classification.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 30,
      "context" : "al [31] for online multi-label classification.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 31,
      "context" : "The universal approximation capability and generalization ability [32] are the key distinguishing factors of ELM.",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 32,
      "context" : "Several variants of ELM has been developed [33-36].",
      "startOffset" : 43,
      "endOffset" : 50
    }, {
      "referenceID" : 33,
      "context" : "Several variants of ELM has been developed [33-36].",
      "startOffset" : 43,
      "endOffset" : 50
    }, {
      "referenceID" : 34,
      "context" : "Several variants of ELM has been developed [33-36].",
      "startOffset" : 43,
      "endOffset" : 50
    }, {
      "referenceID" : 35,
      "context" : "Several variants of ELM has been developed [33-36].",
      "startOffset" : 43,
      "endOffset" : 50
    }, {
      "referenceID" : 31,
      "context" : "A condensed overview of ELM algorithm as adapted from [32, 37] is discussed.",
      "startOffset" : 54,
      "endOffset" : 62
    }, {
      "referenceID" : 36,
      "context" : "A condensed overview of ELM algorithm as adapted from [32, 37] is discussed.",
      "startOffset" : 54,
      "endOffset" : 62
    }, {
      "referenceID" : 31,
      "context" : "There are several papers [32, 37, 38] available in literature that elaborates on the theory and the mathematical background behind the ELM and hence are not discussed here.",
      "startOffset" : 25,
      "endOffset" : 37
    }, {
      "referenceID" : 36,
      "context" : "There are several papers [32, 37, 38] available in literature that elaborates on the theory and the mathematical background behind the ELM and hence are not discussed here.",
      "startOffset" : 25,
      "endOffset" : 37
    }, {
      "referenceID" : 37,
      "context" : "There are several papers [32, 37, 38] available in literature that elaborates on the theory and the mathematical background behind the ELM and hence are not discussed here.",
      "startOffset" : 25,
      "endOffset" : 37
    }, {
      "referenceID" : 38,
      "context" : "There are other similar neural network based techniques [39, 40] which did not gain popularity and are largely forgotten.",
      "startOffset" : 56,
      "endOffset" : 64
    }, {
      "referenceID" : 39,
      "context" : "There are other similar neural network based techniques [39, 40] which did not gain popularity and are largely forgotten.",
      "startOffset" : 56,
      "endOffset" : 64
    }, {
      "referenceID" : 40,
      "context" : "The theory and the mathematics behind using the recursive least square for online sequential ELM is discussed in several papers [41, 42] in the literature.",
      "startOffset" : 128,
      "endOffset" : 136
    }, {
      "referenceID" : 41,
      "context" : "The theory and the mathematics behind using the recursive least square for online sequential ELM is discussed in several papers [41, 42] in the literature.",
      "startOffset" : 128,
      "endOffset" : 136
    }, {
      "referenceID" : 22,
      "context" : "Label density also factors the number of labels in the dataset in addition to the average number of labels [23].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 42,
      "context" : "al in [43].",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 15,
      "context" : "Two datasets have same LC and different LD or vice versa can result in significant variation in the performance of the classifier [16].",
      "startOffset" : 130,
      "endOffset" : 134
    } ],
    "year" : 2016,
    "abstractText" : "In this paper, a novel extreme learning machine based online multi-label classifier for real-time data streams is proposed. Multi-label classification is one of the actively researched machine learning paradigm that has gained much attention in the recent years due to its rapidly increasing real world applications. In contrast to traditional binary and multiclass classification, multi-label classification involves association of each of the input samples with a set of target labels simultaneously. There are no real-time online neural network based multi-label classifier available in the literature. In this paper, we exploit the inherent nature of high speed exhibited by the extreme learning machines to develop a novel online realtime classifier for multi-label data streams. The developed classifier is experimented with datasets from different application domains for consistency, performance and speed. The experimental studies show that the proposed method outperforms the existing state-of-the-art techniques in terms of speed and accuracy and can classify multi-label data streams in real-time. Keywords—Real-time, Classification, Multi-label, Online, Extreme learning machines, High speed.",
    "creator" : "'Certified by IEEE PDFeXpress at 04/14/2016 4:19:50 AM'"
  }
}