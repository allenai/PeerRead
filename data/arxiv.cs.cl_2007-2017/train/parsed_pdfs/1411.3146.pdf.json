{
  "name" : "1411.3146.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Distributed Representations for Compositional Semantics",
    "authors" : [ "Karl Moritz Hermann" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Distributed Representations for\nCompositional Semantics\nKarl Moritz Hermann New College\nUniversity of Oxford\nA thesis submitted for the degree of\nDoctor of Philosophy\nHilary 2014\nar X\niv :1\n41 1.\n31 46\nv1 [\ncs .C\nL ]\n1 2\nN ov\n2 01\n4"
    }, {
      "heading" : "Acknowledgements",
      "text" : "Many thanks are due at this point. I am deeply grateful to my supervisors, Stephen Pulman and Phil Blunsom, for their guidance and advice throughout my studies. Stephen’s encouragement was vital to getting me started again in research after my long detour away from computer science. Over the course of the past four years my research focus gradually shifted towards more statistical and machine learning related approaches. Phil was a key driver behind this shift and he has taught me most of what I know about these fields today. This thesis is a direct consequence of the many discussions I have had with him. It was a great pleasure working with him, even if up until this day I walk away from most of our conversations feeling enlightened and ignorant at the same time. Chris Dyer also deserves thanks: during his stay at Oxford he convinced me to look at distributed representations and this is what I ended up doing. During my studies I had the opportunity to collaborate with a large number of people. I am grateful to Kevin Knight and David Chiang for inviting me to spend the summer of 2012 at the ISI/USC. In 2013 I spent some time at Google Research in New York, working with Kuzman Ganchev as well as with Dipanjan Das and Jason Weston. I have very fond memories of that internship, and the work I did at Google ended up featuring in this thesis, making for a productive summer. I want to thank my colleagues at the CLG group at Oxford and in particular Ed Grefenstette who became a good friend and collaborator. Having moved to London in 2012 I am very thankful to Sebastian Riedel for hosting me at his MR group at UCL, as well as for the stimulating conversations with him and his students. Ed and his various flatmates deserve additional thanks for putting up with me during my many overnight stays in Oxford in those years. On a personal note, thanks for my friends in Oxford and London, and in particular the Trinity Arms pub quiz team for helping me maintain a certain degree of sanity throughout the whole experience. I am grateful to my parents for being a constant source of support and encouragement throughout all my (sometimes seemingly random) education and career choices, past and present. Finally and most of all, I would like to thank my wife Clémence for being there.\nAbstract\nThe mathematical representation of semantics is a key issue for Natural Language Processing (NLP). A lot of research has been devoted to finding ways of representing the semantics of individual words in vector spaces. Distributional approaches—meaning distributed representations that exploit co-occurrence statistics of large corpora—have proved popular and successful across a number of tasks. However, natural language usually comes in structures beyond the word level, with meaning arising not only from the individual words but also the structure they are contained in at the phrasal or sentential level. Modelling the compositional process by which the meaning of an utterance arises from the meaning of its parts is an equally fundamental task of NLP. This dissertation explores methods for learning distributed semantic representations and models for composing these into representations for larger linguistic units. Our underlying hypothesis is that neural models are a suitable vehicle for learning semantically rich representations and that such representations in turn are suitable vehicles for solving important tasks in natural language processing. The contribution of this thesis is a thorough evaluation of our hypothesis, as part of which we introduce several new approaches to representation learning and compositional semantics, as well as multiple state-of-the-art models which apply distributed semantic representations to various tasks in NLP. Part I focuses on distributed representations and their application. In particular, in Chapter 3 we explore the semantic usefulness of distributed representations by evaluating their use in the task of semantic frame identification. Part II describes the transition from semantic representations for words to compositional semantics. Chapter 4 covers the relevant literature in this field. Following this, Chapter 5 investigates the role of syntax in semantic composition. For this, we discuss a series of neural network-based models and learning mechanisms, and demonstrate how syntactic information can be incorporated into semantic composition. This study allows us to establish the effectiveness of syntactic information as a guiding parameter for semantic composition, and answer questions about the link between syntax and semantics. Following these discoveries regarding the role of syntax, Chapter 6 investigates whether it is possible to further reduce the impact of monolingual surface forms and syntax when attempting to capture semantics. Asking how machines can best approximate human signals of semantics, we propose multilingual information as one method for grounding semantics, and develop an extension to the distributional hypothesis for multilingual representations. Finally, Part III summarizes our findings and discusses future work.\nContents"
    }, {
      "heading" : "1 Introduction 1",
      "text" : "1.1 Aims of this thesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.2 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.3 Thesis Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4"
    }, {
      "heading" : "I Distributed Semantics 7",
      "text" : ""
    }, {
      "heading" : "2 Distributed Semantic Representations 8",
      "text" : "2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.2 Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n2.2.1 Feature-Based Representations . . . . . . . . . . . . . . . . . . . . 9 2.2.2 Semantic Networks . . . . . . . . . . . . . . . . . . . . . . . . . . 10 2.2.3 Semantic Space Representations . . . . . . . . . . . . . . . . . . . 11\n2.3 Distributional Representations . . . . . . . . . . . . . . . . . . . . . . . . 12 2.3.1 Learning distributional representations . . . . . . . . . . . . . . . . 13 2.3.2 Weighting Techniques . . . . . . . . . . . . . . . . . . . . . . . . 14 2.4 Neural Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 2.5 Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.5.1 LSA, LSI and LDA . . . . . . . . . . . . . . . . . . . . . . . . . . 15 2.5.2 Dimensionality Reduction Techniques . . . . . . . . . . . . . . . . 17 2.5.3 Similarity Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n2.6 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 2.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20"
    }, {
      "heading" : "3 Frame Semantic Parsing with Distributed Representations 21",
      "text" : "3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 3.2 Frame-Semantic Parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n3.2.1 FrameNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 3.2.2 PropBank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n3.3 Model Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 3.4 Frame Identification with Embeddings . . . . . . . . . . . . . . . . . . . . 27\n3.4.1 Context Representation Extraction . . . . . . . . . . . . . . . . . . 30 3.4.2 Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n3.5 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\ni\n3.5.1 Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 3.5.2 Frame Identification Baselines . . . . . . . . . . . . . . . . . . . . 33 3.5.3 Common Experimental Setup . . . . . . . . . . . . . . . . . . . . 34 3.5.4 Experimental Setup for FrameNet . . . . . . . . . . . . . . . . . . 35 3.5.5 Experimental Setup for PropBank . . . . . . . . . . . . . . . . . . 37 3.5.6 FrameNet Results . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 3.5.7 PropBank Results . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n3.6 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 3.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43"
    }, {
      "heading" : "II Compositional Semantics 46",
      "text" : ""
    }, {
      "heading" : "4 Compositional Distributed Representations 47",
      "text" : "4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 4.2 Theoretical Foundations . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 4.3 Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n4.3.1 Algebraic Composition . . . . . . . . . . . . . . . . . . . . . . . . 52 4.3.2 Lexical Function Models . . . . . . . . . . . . . . . . . . . . . . . 54 4.3.3 Recursive Composition . . . . . . . . . . . . . . . . . . . . . . . . 56\n4.4 Learning Signals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60 4.4.1 Autoencoders . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60 4.4.2 Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 4.4.3 Bilingual Constraints . . . . . . . . . . . . . . . . . . . . . . . . . 66 4.4.4 Signal Combination . . . . . . . . . . . . . . . . . . . . . . . . . 66 4.5 Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 4.5.1 Signal Propagation in Recursive Structures . . . . . . . . . . . . . 67 4.5.2 Gradient Update Functions . . . . . . . . . . . . . . . . . . . . . . 69 4.6 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72 4.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72"
    }, {
      "heading" : "5 The Role of Syntax in Compositional Semantics 74",
      "text" : "5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74 5.2 Formal Accounts of Semantic Composition . . . . . . . . . . . . . . . . . 76\n5.2.1 Combinatory Categorial Grammar . . . . . . . . . . . . . . . . . . 77 5.3 Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78 5.4 Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81 5.4.1 Supervised Learning . . . . . . . . . . . . . . . . . . . . . . . . . 82 5.5 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\n5.5.1 Sentiment Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . 83 5.5.2 Qualitative Analysis . . . . . . . . . . . . . . . . . . . . . . . . . 85\n5.6 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86 5.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88\nii"
    }, {
      "heading" : "6 Multilingual Approaches for Learning Semantics 90",
      "text" : "6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91 6.2 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92 6.3 Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93\n6.3.1 Two Composition Models . . . . . . . . . . . . . . . . . . . . . . 94 6.3.2 Document-level Semantics . . . . . . . . . . . . . . . . . . . . . . 95\n6.4 Corpora . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96 6.5 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n6.5.1 Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97 6.5.2 RCV1/RCV2 Document Classification . . . . . . . . . . . . . . . 98 6.5.3 TED Corpus Experiments . . . . . . . . . . . . . . . . . . . . . . 99 6.5.4 Linguistic Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . 103\n6.6 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104 6.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106"
    }, {
      "heading" : "III Conclusions and Further Work 107",
      "text" : ""
    }, {
      "heading" : "7 Further Research 108",
      "text" : ""
    }, {
      "heading" : "8 Conclusions 110",
      "text" : ""
    }, {
      "heading" : "A Semantic-Frame Parsing: Argument Identification 113",
      "text" : "A.1 Learning and Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114"
    }, {
      "heading" : "B FrameNet Development Data 116",
      "text" : ""
    }, {
      "heading" : "C CCG Categories for CCAE Models 117",
      "text" : "References 117\niii\nList of Figures\n2.1 A hypothetical distributed semantic space . . . . . . . . . . . . . . . . . . 11 2.2 Comparison of LDA and pLSA models . . . . . . . . . . . . . . . . . . . 16\n3.1 Example sentences with frame-semantic analyses . . . . . . . . . . . . . . 25 3.2 Context representation extraction for the embedding model . . . . . . . . . 28\n4.1 Main types of models for semantic composition . . . . . . . . . . . . . . . 51 4.2 Sentences with shared vocabulary but different meaning . . . . . . . . . . . 52 4.3 A simple three-input recursive neural network . . . . . . . . . . . . . . . . 58 4.4 Two ConvNN with different receptive widths . . . . . . . . . . . . . . . . 59 4.5 A simple three-layer autoencoder . . . . . . . . . . . . . . . . . . . . . . . 61 4.6 Recursive Autoencoder with three inputs . . . . . . . . . . . . . . . . . . . 62 4.7 Unfolding autoencoder with three inputs . . . . . . . . . . . . . . . . . . . 63 4.8 Extract of a recursive autoencoder . . . . . . . . . . . . . . . . . . . . . . 68\n5.1 CCG derivation example . . . . . . . . . . . . . . . . . . . . . . . . . . . 77 5.2 Forward application in CCG and as an autoencoder rule . . . . . . . . . . . 80 5.3 CCAE-B applied to Tina likes tigers . . . . . . . . . . . . . . . . . . . . . 80\n6.1 A parallel composition vector model . . . . . . . . . . . . . . . . . . . . . 94 6.2 A parallel document-level compositional vector model . . . . . . . . . . . 95 6.3 Classification accuracies over training data . . . . . . . . . . . . . . . . . . 100 6.4 t-SNE proejctions for BI+ word representations . . . . . . . . . . . . . . . 104 6.5 t-SNE projections for BI+ phrase representations . . . . . . . . . . . . . . 105\niv\nList of Tables\n3.1 Hyperparameter search space for the FrameNet and PropBank experiments 35 3.2 Frame identification results on FrameNet development data . . . . . . . . . 38 3.3 Frame identification results on FrameNet test data . . . . . . . . . . . . . . 38 3.4 Full structure prediction results for FrameNet development data . . . . . . 38 3.5 Full structure prediction results for FrameNet test data . . . . . . . . . . . 39 3.6 Frame identification results on PropBank development data . . . . . . . . . 40 3.7 Frame identification results on PropBank test data . . . . . . . . . . . . . . 40 3.8 Full structure prediction results on PropBank development data . . . . . . . 40 3.9 Full structure prediction results on PropBank test data . . . . . . . . . . . . 40 3.10 CoNLL 2005 argument evaluation results on PropBank development data . 40 3.11 CoNLL 2005 argument evaluation results on PropBank test data . . . . . . 41\n4.1 Word and sentence statistics for Europarl v7 . . . . . . . . . . . . . . . . . 48 4.2 Algebraic operators frequently used for semantic composition . . . . . . . 53 4.3 Partial derivatives for backpropagation . . . . . . . . . . . . . . . . . . . . 68\n5.1 Aspects of the CCG formalism used by the models . . . . . . . . . . . . . 78 5.2 Encoding functions of the four CCAE models . . . . . . . . . . . . . . . . 78 5.3 CCG combinatory rules considered in our models. Combinators are based\non those implemented in the C&C parser (Curran et al., 2007). Frequency indicates the rounded number of observations on the SP dataset. . . . . . . 79\n5.4 Accuracy on sentiment classification tasks . . . . . . . . . . . . . . . . . . 84 5.5 Effect of pretraining on model performance on the SP dataset . . . . . . . . 85 5.6 Phrases and their semantically closes match according to CCAE-D . . . . . 86 5.7 Comparison of model complexity . . . . . . . . . . . . . . . . . . . . . . 87\n6.1 Classification accuracy on the RCV corpus . . . . . . . . . . . . . . . . . . 99 6.2 F1-scores for TED cross-lingual document classification . . . . . . . . . . 101 6.3 F1-scores for pivoted TED cross-lingual document classification . . . . . . 102 6.4 F1-scores for TED monolingual document classification . . . . . . . . . . 103\nA.1 SRL argument identification features . . . . . . . . . . . . . . . . . . . . . 114\nB.1 Development data from the FrameNet 1.5 corpus . . . . . . . . . . . . . . 116\nv\nC.1 CCG categories considered in the CCAE models. Frequency denotes the frequency of the labels on the British National Corpus dataset used for pretraining those models in §5.5.1. . . . . . . . . . . . . . . . . . . . . . . . . 117\nvi\nChapter 1\nIntroduction\nThis thesis investigates the application of distributed representations to semantic models in natural language processing (NLP). NLP is the discipline concerned with the interpretation and manipulation of human (natural) language with computational means. This includes all forms of interaction between computers and natural language, as well as the development of tools and resources for working with natural language text. Tasks within NLP include the annotation of (large-scale) corpora for subsequent linguistic analysis, algorithms for extracting information from text, models for translating text across languages, and models for generating text based on structured data. A lot of recent progress on these problems stems from the development of statistical approaches to NLP, which deploy machine learning algorithms that attempt to solve such problems by exploiting patterns found in large corpora.\nMachine learning and statistical NLP have mostly focused on tasks related to syntax such as part-of-speech (POS) tagging and parsing, as well as larger tasks such as statistical machine translation, which largely rely on syntactic and frequency-based effects, too. More recently, semantics—that is the study of meaning—has again become a focus of research in NLP. While semantics has enjoyed considerable attention in linguistics and Computational Linguistics, this was primarily from the perspective of symbolic-reasoning, with the exception of early NLP pioneers such as Karen Spärck Jones and Margaret Masterman (Spärck Jones, 1988; Masterman, 2005, inter alia). This thesis is part of this line of work, which investigates semantics within the realm of statistical NLP and machine learning. Precisely, we focus on the study of representing meaning with continuous, distributed objects and explore how to learn and manipulate these objects in such a fashion that the information contained therein can be exploited for various NLP-related tasks.\n1"
    }, {
      "heading" : "1.1 Aims of this thesis",
      "text" : "The primary aim of this thesis is to investigate the use of distributed representations for capturing semantics, and to evaluate their efficacy in solving tasks in NLP for which a degree of semantic understanding would be beneficial. Our hypothesis is that distributed representations are a highly suitable mechanism for capturing and manipulating semantics, and further, that meaning both at the word level and beyond can be encoded distributionally.\nThroughout this thesis we evaluate this hypothesis in a number of ways. In order to establish the suitability and efficacy of distributed representations for capturing semantics we apply such representations to a number of popular and important tasks in NLP. We evaluate the performance of models supported by distributed semantic representations relative to the performance of alternative, state-of-the-art solutions. As we end up outperforming the prior state of the art on a number of such problems, using relatively simple models in conjunction with distributed representations, these experiments strongly support the first part of this thesis’ hypothesis. The second aspect of our hypothesis concerns the question whether distributed representations can be used to represent semantics beyond the word level. This question is investigated throughout Part II of this thesis, which focuses on distributed representations for compositional semantics. We attempt to verify this hypothesis two-fold. First, we again develop systems for semantic vector composition that learn to represent a sentence or a document in a distributed fashion, and then pit these representations against other approaches on several tasks. Second, we analyse a number of popular methods for learning and composing distributed representations and evaluate to what extend these methods are capable of learning to encode actual semantics.\nIn the following section we discuss the main contributions of this thesis. Subsequently, §1.3 explains the structure of the remainder of this thesis and summarises the content of each chapter."
    }, {
      "heading" : "1.2 Contributions",
      "text" : "Here, we summarise the major contributions of this thesis.\nThe task considered in Chapter 3—frame-semantic parsing—is a very popular and important task within NLP. The chapter contributes to the thesis two-fold. First, by using\n2\ndistributed semantic representations to solve the task, we determine the feasibility of using distributed representations for capturing semantics and furthermore discover a number of important factors to be considered when using distributed representations. Second, we present a full frame-semantic parsing pipeline as part of our experiments, and contribute to the field by setting a new state of the art on this task. Thus, we have not only validated our thesis about the use of distributed semantic representations, but further have demonstrated the superior performance of this approach over all prior work on the semantic frame-identification and frame-parsing tasks.\nFollowing a background chapter on compositional semantics (Chapter 4), this thesis continues by exploring the effect of syntax in guiding semantic composition. Here (Chapter 5), we present a novel method for composing and learning distributed semantic representations given syntactic information. We focus on combinatory categorial grammars in this chapter, and show that our system, which integrates syntactic information with semantics, outperforms comparable work that does not exploit syntactic information. We fulfil the aim of this chapter by thus establishing a link between syntax and semantics. As an additional contribution, we provide a novel model for sentiment analysis, which outperformed the state-of-the-art system at that point in time.\nMost prior work on semantic representation learning focuses on task-specific learning, which inevitably will lead to representations exhibiting only certain aspects of semantics as required by the objective function of a given task. In Chapter 6 we explore the use of multilingual data to learn representations further abstracted away from monolingual surface forms and task-specific biases, thereby extending the distributional hypothesis for multilingual joint-space representations. We demonstrate how multilingual data can be used to learn semantic distributed representations and develop a novel algorithm for doing so efficiently. We apply representations learned under this framework to a document classification task to verify their efficacy. The success of our models in the document classification experiments give further support to the initial hypothesis of this thesis concerning the usefulness of distributed semantic representations. As the models learn through semantic transfer at the sentence level or beyond, we further gain additional insight into the second part of our hypothesis, namely that such distributed representation are capable of encoding semantics beyond the word level.\n3"
    }, {
      "heading" : "1.3 Thesis Structure",
      "text" : "This thesis is organised into two distinct parts. Part I focuses on distributed representations and their application, with Chapter 2 introducing distributed representations and their application to natural language semantics. Chapter 3 explores the efficacy of semantic distributed representations by evaluating their use in the task of semantic frame identification.\nPart II describes the transition from semantic representations for words to compositional semantics. Chapter 4 covers the relevant literature in this field. Following this, we continue our investigation of semantics in distributed representations. Chapter 5 investigates the role of syntax in semantic composition. For this, we discuss a series of neural network-based models and learning mechanisms, and demonstrate how syntactic information can be incorporated into semantic composition. This study allows us to establish the effectiveness of syntactic information as a guiding parameter for semantic composition, and answer questions about the link between syntax and semantics. Following the discoveries made regarding the role of syntax, Chapter 6 investigates whether is it possible to further reduce the impact of monolingual surface forms and syntax when attempting to capture semantics. Asking the question of how machines can best approximate human signals of semantics, we propose multilingual information as one proxy for machines’ lack of shared embodiment and bodily experience and describe mechanisms for extracting semantic representations from parallel corpora. We conclude with Part III, which summarises our findings and discusses future work.\nThis thesis contains material that has been previously published. The bulk of the work in this thesis is based on three papers presented at the Annual Meeting of the Association for Computational Linguistics (ACL), with smaller aspects of the thesis being based on further publications as follows. The work contained in these publications and presented in this thesis is principally mine, except when stated otherwise in the relevant chapters. Where co-authors have contributed significantly or where aspects of the work published are solely the responsibility of a co-author, this is accredited accordingly in the respective chapters.\nBelow we summarise each chapter of this thesis and elaborate on the material contained\ntherein.\nChapter 2: Distributed Semantic Representations\nWe discuss semantics in the context of NLP, and provide an overview of popular\n4\nattempts to capture, express, and reason with semantics in the literature. As part of this we motivate distributed semantic representations. We then go on to discuss how such representations can be learned and cover a number of underlying principles necessary for understanding the remainder of this thesis.\nChapter 3: Frame Semantic Parsing with Distributed Representations\nHaving motivated the use of distributed representations for semantics, we underline this argument with an extensive empirical evaluation. We focus on the task of semantic frame identification, for which we propose a new solution relying on distributed semantic representations. We describe our novel approach as well as relevant work in the literature, and subsequently evaluate our new model. For a full comparison we also make use of a semantic role-labelling system, which allows us to test the semantic frame identification model within a full frame-semantic parsing pipeline, where our model sets a new state of the art. The work presented in this chapter is based on the following publication:\nKarl Moritz Hermann, Dipanjan Das, Jason Weston and Kuzman Ganchev. 2014. Semantic Frame Identification with Distributed Word Representations. In Proceedings of ACL.\nChapter 4: Compositional Distributed Representations\nHaving established the efficacy of distributed representations in conveying semantic information in the previous chapter, we now go on to focus on compositional semantics, that is the representation of meaning of larger, composed linguistic units such as phrases or sentences. Here, we survey prior work in that field, as well as the theoretical foundations on which this thesis builds. Further, we attempt to structure prior efforts on tasks in this area by discriminating between lexical-function and algebraic, as well as between distributional and distributed approaches to compositional semantics.\nChapter 5: The Role of Syntax in Compositional Semantics\nHaving already made extensive use of syntactic information in Chapter 3, we now investigate the role of syntax in compositional (distributional) semantics in more detail. We do this by extending existing work on compositional semantics with various\n5\ntypes of syntactic information based on combinatory categorial grammar, and evaluate the effects derived from this additional information. This chapter is based on work first published in:\nKarl Moritz Hermann and Phil Blunsom. 2013. The Role of Syntax in Vector Space Models of Compositional Semantics. In Proceedings of ACL.\nChapter 6: Multilingual Approaches for Learning Semantics\nHaving so far in this thesis focused on task-specific problems which we enhanced with semantic information, we now attempt to learn more general semantic representations by reducing task-specific bias when learning representations, and further, by abstracting away from monolingual surface forms through the use of multilingual data. We develop a novel objective function for word representation learning that can be applied to multilingual data and that—as a further novelty—does not rely on word alignment across languages. Multiple evaluations validate our approach, with our model setting a new state of the art on a crosslingual document classification task. The work presented in this chapter is based on the following two publications:\nKarl Moritz Hermann and Phil Blunsom. 2014. Multilingual Distributed Representations without Word Alignment. In Proceedings of ICLR.\nKarl Moritz Hermann and Phil Blunsom. 2014. Multilingual Models for Compositional Distributed Semantics. In Proceedings of ACL.\nChapter 8: Conclusions\nThe final chapter of this thesis summarises our findings and proposes future work based on the work presented here.\n6\nPart I\nDistributed Semantics\n7\nChapter 2\nDistributed Semantic Representations\nChapter Abstract\nThis chapter presents an overview of key concepts, formalisms and background literature related to distributed semantics. This review begins by introducing the distributional account of semantics according to Firth (1957) and various dimensionality reducing techniques typically combined with extracting distributional representations. Subsequently, we will explore alternative methods for learning distributed representations for words and their applications."
    }, {
      "heading" : "2.1 Introduction",
      "text" : "In this chapter we provide an overview of popular methods for learning distributed word representations. We begin in §2.2 by discussing the role of semantics in natural language processing. §2.3 describes the distributional account of semantics and how it can be exploited for learning distributed representations in an unsupervised setting. Subsequently, §2.4 covers alternative methods for learning distributed representations, going beyond a purely distributional approach. Finally, we survey prior work on, and applications of, distributed representations for words in §2.6. In the literature distributed word representations are frequently referred to as word embeddings; please note that we will use these two terms interchangeably throughout this thesis.\nWords can be represented as discrete units by mapping a string of characters to integers by looking up words in a dictionary. Frequently, however, it is better to represent words by\n8\ngoing beyond their surface form and attempting to capture syntactic and semantic aspects in their representation. This would be useful for establishing similarities and relationships among different words. Within language modelling for instance, part of speech (POS) tags have proved a useful method for clustering words and determining likely word sequences in a given language. Related ideas include augmenting word representations with grammatical information such as their conjugated or declined form, their infinitive or stem and other morpho-syntactic information. Such grammatical information can be used to learn relationships between morphemes of the same base word.\nWhile syntactic information can be useful for a number of tasks such as language modelling or word reordering in generative models, these problems, as well as a large number of others, would also benefit from semantic information included in a word’s representation. In the case of language modelling it is easy to see how a measure for semantic similarity between words would allow such a model to better generalise for rare words, as the semantic similarity score could be used to make predictions based on the statistics of semantically similar, more frequent terms."
    }, {
      "heading" : "2.2 Semantics",
      "text" : "While there is little doubt concerning the usefulness of semantic information, the question of how such knowledge can be “acquired, organized and ultimately used in language processing and understanding has been a topic for great debate in cognitive science” (Mitchell and Lapata, 2010). Semantics have been represented in a number of ways throughout the literature. Broadly, such accounts of semantics can be categorised into feature-based models and semantics spaces. The related concept of semantic networks also deserves a mention, and will also briefly be discussed together with the other two accounts below."
    }, {
      "heading" : "2.2.1 Feature-Based Representations",
      "text" : "Feature-based models attempt to capture specific aspects of semantics, either through a list of pre-defined features or by learning attributes that are considered relevant to the meaning of a word by human annotators (Andrews et al., 2009; McRae et al., 1997, inter alia).\nThesauri and other lexicographical resources such as the WordNet project (Fellbaum, 1998) can provide some such semantic features by providing relational information for words such as hypernomy and hyponymy, meronymy or synonymy and antonymy.\n9\nRelated lines of work include super-tagging (Bangalore and Joshi, 1999) and subsequently supersense-tagging (Ciaramita and Johnson, 2003; Curran, 2005). Super-tagging provides richer syntactic information about words by capturing the localised syntactic context in which they appear. The similarly named supersense-tagging, on the other hand, attempts to learn “supersenses” as used by the WordNet lexicographers for words outside of the WordNet lexicon.\nAll of these approaches, however, are limiting in that they can only capture specific aspects of syntactic or semantic information, and further, in that they typically rely on syntactic and semantic categories as defined by hand. Unsupervised clustering methods can partially overcome the second issue, but the first remains."
    }, {
      "heading" : "2.2.2 Semantic Networks",
      "text" : "Semantic networks describe semantic relations between entities or concepts. Conventionally such networks are represented as directed or undirected graphs, with nodes representing concepts and vertices (edges) between nodes representing relations. The idea was first proposed by Peirce (1931), with the application to semantics proposed in Richens (1956) and Richens (1958) and developed by Collins and Quillian (1969).\nWordNet, introduced in §2.2.1 is an example for such a semantic network, where words—concepts—are linked by relations such as synonymy or meronymy. Alternative networks use more explicit relations, such as IS-A and SIBLING-OF relations. Semantic similarity tends to be measured by the path length between two concepts.\nSemantic networks are popular for certain tasks. For instance, in joint work prior to this thesis, we studied the use of such semantic networks as a form of interlingua for machine translation (Jones et al., 2012; Chiang et al., 2013). Similarly, they are popular for tasks in relation extraction and identification (e.g. Riedel et al., 2013).\nHowever, as semantic networks are typically hand crafted with a predetermined set of features, their application is limited to domains with the necessary resources or availability of annotators. Further, path length as a similarity measure is vague and cannot be applied globally: For instance, for cat, one could envisage relations “cat IS-A mammal” and “cat HAS whiskers”, which would insinuate an equal degree of similarity between these terms.\n10"
    }, {
      "heading" : "2.2.3 Semantic Space Representations",
      "text" : "An alternative approach for representing words, which we explore in this thesis, are distributed representations or semantic space representations. Here, words are represented by mathematical objects, frequently vectors.\nConventional dictionary-based methods for representing words as indices can be used to represent words as vectors. In that case, word vectors would have the size of the dictionary and each word would be captured by a vector containing zeros in all positions except for a one in the position of their index. This is known as a one-hot representation. Obvious shortcomings of one-hot representations include their high dimensionality, their inability to deal with out of vocabulary (OOV) words, and furthermore their lack of robustness with regard to sparsity, as no information is shared across words.\nBetter results can be achieved by representing words as continuous vectors, where each dimension represents some latent category (e.g. a semantic or syntactic feature). See Figure 2.1 for an example. Key benefits of such a representation are that it does not require hand crafted features, and that distance measures can be applied to evaluate semantic proximity between words given their distributed representation.\nSuch distributed representations stem from the idea that the meaning of a word can be captured from its linguistic environment. While not all models of distributed semantics make explicit use of this distributional hypothesis, it directly or indirectly forms the basis\n11\nof most work in this field. Later on in this thesis, when introducing multilingual models in Chapter 6, we generalise this concept by using a different form of context informing the semantic learning process. In the next section §2.3 we introduce the distributional hypothesis in greater detail before surveying other popular methods for learning continuous distributed representations of word-level semantics from §2.4 onwards."
    }, {
      "heading" : "2.3 Distributional Representations",
      "text" : "Distributional representations encode an expression by its environment, assuming the contextdependent nature of meaning according to which one “shall know a word by the company it keeps” (Firth, 1957). The underlying idea of this distributional account of semantics— concisely captured by the quote above—is that the meaning of words can be inferred from their usage and the context they appear in. By implication, this also means that words with similar distributions over the contexts they appear in have similar meaning. For instance, we assume that the words bicycle and bike would occur in similar contexts, whereas the contexts of bicycle and oranges would be rather different.\nThis distributional hypothesis provides the basis for statistical semantics, allowing the inference of semantics from distributional information extracted from sufficiently large corpora. Distributional models of semantics thus characterize the meanings of words as a function of the words they co-occur with.\nEffectively this is usually achieved by considering the co-occurrence with other words in large corpora and mapping this co-occurrence information onto a matrix. Thus, distributional representations are a special form of distributed representations, where the distributed information equals distributional information. Distributional representations can be learned through a number of approaches and are not limited to using words as the basis of their co-occurrence matrix. Examples for other bases include larger linguistic units such as n-grams (Jones and Mewhort, 2007), documents (Landauer and Dumais, 1997) or predicate-argument slots (Grefenstette, 1994; Padó and Lapata, 2007). In their simplest form, statistical information from large corpora can be used to learn distributed word representations. Usually, the information used to compute word embeddings are words occurring very close to the target word, typically in a five word window. This is related to topic-modelling techniques such as LSA (Dumais et al., 1988), LSI, and LDA (Blei et al.,\n12\n2003) (see §2.5.1), but these methods use a document-level context, and tend to capture the topics a word is used in rather than its more immediate syntactic context.\nAs already stated in §2.2.3, an advantage of this statistical approach to semantics is that word meaning can now be quantified. The semantic similarity between two words can be measured by the distance between their representation in such a space (or the cosine of the angle between them). See again Figure 2.1 for an illustration of this.\nThese models, mathematically instantiated as sets of vectors in high dimensional vector spaces, have been applied to tasks such as thesaurus extraction (Grefenstette, 1994; Curran, 2004), word-sense discrimination (Schütze, 1998), automated essay marking (Landauer and Dumais, 1997), word-word similarity (Mitchell and Lapata, 2008) and so on. We provide an overview of such applications and methods in §2.6. We describe the collocational approach for learning distributional representations in §2.3.1, followed by expanding on a number of strategies for improving these representations using dimensionality reduction and smoothing techniques."
    }, {
      "heading" : "2.3.1 Learning distributional representations",
      "text" : "The vectors for distributional semantic models are generally produced from corpus data via the following procedure:\n1. For each word in a lexicon, its contexts of appearance are collected from a corpus,\nbased on some context-selection criterion (e.g. tokens within k words of the target word, or being linked by a dependency or other syntactic relation).\n2. These contexts are processed to reshape or filter the information they contain (e.g. only\nconsidering context words from the most frequent n words in a corpus, or those of specific syntactic classes).\n3. These contexts of occurrence are encoded in a vector where each vector component\ncorresponds to a possible context, and every component weight corresponds to how frequently the target word occurs in that context.\n4. Optionally, the vector component weights are reweighted by some function (e.g. term-\nfrequency-inverse-document-frequency, ratio of probabilities, pointwise mutual information).\n13\n5. Optionally, the vectors are subsequently projected onto a lower dimensional space\nusing some dimensionality reduction technique (see §2.5.2).\nThe semantic similarity of words is then determined by computing the distance between their thus-constructed vector representations, using geometric similarity metrics such as the cosine of the angle between the vectors (see §2.5.3). While typically only the co-occurrence with some nmost frequent words is considered, this can still lead to fairly high-dimensional vector representations. As this dimensionality will influence the size of the many other model parameters, it can be useful to reduce word representations learned through distributional means. Such dimensionality reduction further allows words to have similar representations regardless of the particular documents from which their co-occurrence statistics have been extracted. In §2.5.2 we provide an overview of commonly used methods for this purpose.\nFor a comprehensive overview of the different parametric options typically used in the production of such semantic vectors and of their comparison, we refer to the surveys found in Curran (2004) and Mitchell (2011)."
    }, {
      "heading" : "2.3.2 Weighting Techniques",
      "text" : "One frequently employed mechanism for improving the quality of the extracted distributional vectors is to apply some form of normalisation. The purpose of normalising vectors is to maximise their information content and/or to pre-process such vectors for subsequent use in composition models or other functions where inputs are expected to be bound by a certain range or matching a certain distribution. Similarly, vectors are frequently normalized to form a probability distribution.\nTF-IDF The term frequency-inverse document frequency (TF-IDF) is a statistical measure frequently employed for this purpose (Spärck Jones, 1988). TF-IDF stems from information retrieval and text mining, and is used to weight words by their semantic content. In its simplest form, TF-IDF provides a weight for each word, computed by considering two statistics.\nFirst, the term frequency is a measure of how frequently a word appears in a given document. Second, the inverse document frequency is the total number of documents in a corpus, divided by the number of documents containing the word in question at least once.\n14\nThus, common words may obtain a high term frequency but a low inverse document frequency. On the other hand, words appearing in a particular document but rarely throughout the overall corpus may offset their low term frequency by a high idf -score.\nFor distributional representations, TF-IDF can be used in several ways. It provides a simple heuristic for identifying and removing stop-words. Similarly, TF-IDF weights can be used to scale distributional counts. It was shown that this technique can alleviate sparsity-induced problems in distributed representation learning. Conventionally, this is achieved by treating the context words of each word type as a document from which TFIDF weights can be learned. Thereby, greater weight is given “to words with more idiosyncratic distributions and may improve the informativeness of a distributional representation” (Huang and Yates, 2009)."
    }, {
      "heading" : "2.4 Neural Language Models",
      "text" : "Neural language models are another popular approach for inducing distributed word representations, first developed by Y. Bengio and coauthors (Bengio et al., 2003).\nThey have subsequently been explored by others (Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2010) and have achieved good performance across various tasks. The neural language model described by Mikolov et al. (2010) for instance learns word embeddings together with additional transformation matrices which are used to predict the next word given a context vector created by the previous words. Collobert et al. (2011) further popularized neural network architectures for learning word embeddings from large amounts of largely unlabeled data by showing the embeddings can then be used to improve standard supervised tasks, i.e. in a semi-supervised setup. Unsupervised word representations can easily be plugged into a variety of NLP tasks."
    }, {
      "heading" : "2.5 Methods",
      "text" : ""
    }, {
      "heading" : "2.5.1 LSA, LSI and LDA",
      "text" : "Latent Semantic Analysis (LSA, henceforth) (Dumais et al., 1988) describes a mechanism for extracting latent semantic information from words in context. In the context of information retrieval, LSA is also known as Latent Semantic Indexing (LSI).\n15\nLSA uses a term-document matrix which describes the occurrences of terms in documents. For this matrix X a lower rank approximation is found, using the k largest singular values from X = UΣV T where U and V are orthogonal matrices and Σ a diagonal matrix containing the singular values in question. Using the decomposition UΣV allows one to find the best k rank approximation for X .\nWhile LSA is typically used in connection with bag of word models focussed on learning topic representations for documents, it can also be applied to distributional representation learning. Similar to TF-IDF (§2.3.2 above), LSA can be applied to context vectors of a given word (Huang and Yates, 2009, inter alia).\nLatent Dirichlet Allocation (LDA, henceforth) also deserves mention here. Similar to LSA, LDA was initially developed with a focus on document-level analysis and topic modelling in particular (Blei et al., 2003). LDA uses a generative latent variable model that models documents as mixtures over topics with each of these latent topics using a probability distribution over a vocabulary to generate words. This is comparable to a probabilistic variant of LSA (pLSA), with the topics of LDA being equivalent to the latent class structure of pLSA. See Figure 2.2 for a comparison of the two models.\nA crucial difference between topic models such as LDA and other models presented in this chapter is that LDA represents words as a probability distribution rather than as points in a high-dimensional (semantic) space. This probability distribution, however, is equivalent to points on a simplex in a high-dimensional space. As generative models, they learn probabilities for words given a topic. Thus, both LDA and pLSA can easily be used to\n16\nlearn low dimensional representations of observed variables by expressing their distribution over the latent topic variables as a probabilistic distributed representation. In prior work, not included in this thesis, we applied variations of LDA to learn semantic representations for noun compounds and adjective noun pairs (Hermann et al., 2012a; Hermann et al., 2012b)."
    }, {
      "heading" : "2.5.2 Dimensionality Reduction Techniques",
      "text" : "Beyond specific methods such as LSA there are a number of general, statistical methods for reducing the rank of vectors, which can easily be applied to both distributional and distributed word representations. As some of the methods introduced in this chapter, particularly those extracting distributional representations, can lead to very large vectors, these methods are very useful both to alleviate sparsity via smoothing as well as to improve the efficiency of subsequent models making use of such representations.\nPrincipal Component Analysis (PCA, henceforth) is similar to LSA (above) in that it performs rank reduction on a matrix using decomposition and orthogonal vectors to take correlation between individual vector elements (matrix rows) into account. The key difference is that instead of using a term-document matrix, PCA uses a term covariance matrix, processed to have a zero mean. Both LSA and PCA rely on singular value decomposition (SVD) for the actual rank reduction operation.\nFactor Analysis is another statistical method for discovering latent variables (factors) that can represent higher-dimensional data through a lower-rank approximation. Here, variables are first shifted to have zero mean, and subsequently a factor matrix L is learned, such that X−µ = LF + , where F denotes the low rank approximation of X−µ and denotes some error term. On the surface, this approach is comparable to PCA. The main difference between the two methods is that PCA is a descriptive technique, while factor analysis uses a latent modelling technique to learn its factors."
    }, {
      "heading" : "2.5.3 Similarity Metrics",
      "text" : "For many tasks it is necessary to evaluate the similarity between several distributed representations. Examples for this include word-word similarity tasks (similarity between two\n17\nrepresentations), unsupervised clustering (cluster a number of entities with distributed representations) or annotation tasks (find the closest label to a representation in a given space).\nDepending on the model settings and normalization options, cosine similarities (Eq.\n2.1) or Euclidean distances (Eq. 2.2) can be used to evaluate such tasks.\nCos(~x, ~y) = cos(θ) = ~x · ~y ‖~x‖‖~y‖\n(2.1)\nEucl(~x, ~y) = |~x− ~y| = √√√√ d∑ i=0 (xi − yi)2 (2.2)\nThe key difference between the two measures is that the cosine distance (or similarity) takes into account the difference between two vectors in terms of their angle, while the Euclidean distance accounts for the metric distance between two points. Space is treated as an inner product space for the cosine distance, and the cosine distance can be derived from the inner product (or dot product):\n~x · ~y = ‖~x‖‖~y‖cos(θ), (2.3)\nwhich can also be used as a similarity metric in its own right.\nAnother metric that is interesting to consider for distributed representations is the Mahalanobis distance. The Mahalanobis distance can be viewed as a scale-invariant extension of the Euclidean distance that further accounts for correlations within a dataset. In its general formulation (Eq. 2.4), it uses a covariance matrix S to effectively scale and account for interactions among the different elements within the vectors under comparison.\nd(~x, ~y) = √ (~x− ~y)TS−1(~x− ~y) (2.4)\nNote that this describes the quadratic form of a Gaussian distribution, as used in the exponentiated part of a Gaussian mixture model (Equations 2.5 and 2.6, where ~µi is the mean and Si the covariance in a d-variate mixture model).\np(~x|θ) = M∑ i=1 wig(~x|~µi, Si) (2.5)\ng(~x|~µi, Si) = 1√\n(2π)d|Si| e− 1 2 (~x−~µi)TS−1i (~x−~µi) (2.6)\nWeinberger and Saul (2009) propose a pseudometric based on the squared Mahalanobis distanceDM (confusingly termed Mahalanobis metric), depicted in Eq. 2.7. The difference\n18\nhere is that rather than using the covariance matrix, a custom positive semidefinite matrix M can be used.\nDM(~x, ~y) = (~x− ~y)TM(~x− ~y) (2.7)\nThis metric has been shown to be useful for tasks such as kNN classification and related annotation tasks due to the its ability to adjust for variance in scale in multidimensional data. The key difference between the general form (Equation 2.4) and the pseudometric in Equation 2.7 is that the latter can learn these scaling factors on supervised data by adjusting matrix M, whereas the former uses a default scale adjustment based on the covariance matrix S. In Chapter 3 we make use of a derivation of this metric for our frame semantic parsing task.\nCha (2007) provides a comprehensive survey of these distance metrics and others. Also, we refer the interested reader to Curran (2004) and Mitchell (2011) who provide further insight into this subject."
    }, {
      "heading" : "2.6 Applications",
      "text" : "Semantic space representations can easily be plugged into a variety of NLP related tasks. By providing richer representations of meaning than what can be encompassed in a discrete model, such distributed representations have demonstrated improvements on a wide range of tasks. The list below is by no means exhaustive, but is intended to demonstrate the usefulness of such representations across a broad range of areas in NLP.\nTopic modelling has been explored using distributed representations, particularly in the context of LDA (Blei et al., 2003; Steyvers and Griffiths, 2005). Other fields include thesaurus extraction (Grefenstette, 1994; Curran, 2004), word-sense discrimination (Schütze, 1998), automated essay marking (Landauer and Dumais, 1997), synonymy or word-word similarity (McDonald, 2000; Griffiths et al., 2007; Mitchell and Lapata, 2008), named entity recognition (Collobert et al., 2011; Turian et al., 2010), cross-lingual document classification (Klementiev et al., 2012), bilingual lexicon induction (Haghighi et al., 2008), semantic priming (Steyvers and Griffiths, 2005; Landauer and Dumais, 1997), discourse analysis (Foltz et al., 1998; Kalchbrenner and Blunsom, 2013) or selectional preference acquisition (Pereira et al., 1993; Lin, 1999).\nWhen considering representations of larger syntactic units this list will expand even further. In this chapter and the next, we focus on word-level representations only. From\n19\nChapter 4 onwards we discuss models for learning representations for higher linguistic units such as phrases, sentences or documents."
    }, {
      "heading" : "2.7 Summary",
      "text" : "In this chapter, we have surveyed the field of semantics within the context of natural language processing. Following a brief exposition of the various strands of semantic frameworks popular in the field, this chapter has subsequently focused on distributed semantic representations in particular. Having explained how such distributed representations can be learned, we are now able to begin evaluating the hypothesis that this thesis sets out to solve.\nThe following chapter (Chapter 3) begins this evaluation, by describing a relatively simple model which employs distributed representations to solve the frame-identification step of the semantic frame-parsing task. As this is both an important and a popular task within the NLP community, outperforming a series of prior models on this task with this simple approach supports our hypothesis concerning the efficacy of distributed representations and their use in solving challenging tasks in NLP.\n20\nChapter 3\nFrame Semantic Parsing with Distributed Representations\nChapter Abstract\nThis chapter investigates the use of distributed semantic representations for semantically complex tasks. We focus on the task of semantic frame identification and present a novel technique using distributed representations of predicates and their syntactic context for identifying semantic frames. This technique leverages automatic syntactic parses and a generic set of word embeddings. In order to evaluate this approach against the state-of-the-art, we combine our method with a standard argument identification system. In this combination, we outperform the former best model on FrameNet-style frame-semantic analysis, while reporting competitive results on the PropBank related task. These results strongly indicate the value of distributed representations for capturing semantics and for tackling tasks that benefit from semantic representations.\nThe material in this chapter was originally presented in Hermann et al. (2014). The aspects of the model related to distributed representations in the context of frame identification are primarily the first author’s own work. The argument identification system used in the experimental part of this chapter is a standard system with some modifications by my co-authors and should not be counted towards the original work presented in this thesis.\n21"
    }, {
      "heading" : "3.1 Introduction",
      "text" : "Having introduced distributed representations as a way to encode semantic information in NLP, we put this concept to the test. This chapter investigates the use of distributed representations for semantic frame identification—a task that would clearly benefit from a degree of semantic understanding.\nAs pointed out in Chapter 2, there exists a large body of literature on learning distributional representations. However, much less work has been done on establishing whether such representations truly capture semantics and whether they can be used for explicitly semantic tasks. Here, we address this question, which directly relates to the hypothesis stated at the outset of this thesis. For this purpose, we develop a novel technique for semantic frame identification that leverages distributed word representations, and compare its performance against similar models that do not rely on distributed representations in various experimental settings. A benefit of the semantic frame identification task is that the usefulness of semantic information in solving this task is almost self-evident. Further, two popular formalisms and related test sets exist for this task together with a suitably large body of prior work which allows for fair comparison of our approach. The empirical results in this chapter and our analysis of the various experimental settings support our initial hypothesis and provide us with further insight into the use and usefulness of distributed representations.\nIn our experiments we assume the presence of generic word embeddings constructed independently of the task at hand, such as e.g. the distributions learned and provided by Collobert et al. (2011), Turian et al. (2010) or Al-Rfou’ et al. (2013). Given a predicate in a sentence, we extract its syntactic context via an automatic parse; we learn to project the collection of the word embeddings for each context word into a low-dimensional space. Simultaneously, we learn an embedding for the semantic frames in our domain. Both projections are learnt jointly to perform well on the supervised task. At prediction time, the context representation of a predicate is projected to the low dimensional space and the nearest frame is chosen as our prediction. We perform the learning within WSABIE (Weston et al., 2011), a generic framework for embedding instances and their labels in a shared lowdimensional space.\nWe apply our approach to frame-semantic parsing tasks on two frame-semantic formalisms. First, we evaluate on the FrameNet corpus (Baker et al., 1998; Fillmore et al.,\n22\n2003), and show that we outperform the prior state-of-the-art system (Das et al., 2014) on semantic frame identification. When combined with a standard argument identification method (Appendix A), we also report the best results on this task to date. Second, we present results on PropBank-style data (Palmer et al., 2005; Meyers et al., 2004; Màrquez et al., 2008), where we achieve results on a par with the prior state of the art (Punyakanok et al., 2008).\nThis remainder of this chapter is structured as follows. §3.2 provides the necessary background on semantic-frame parsing and the various corpora and formalisms used in this chapter. §3.3 provides a high-level overview of our model and §3.4 then describes our frame identification model and in particular the context-extraction method developed to incorporate distributed representations into the frame identification process. Finally, we describe and discuss the empirical evaluation of our approach (§3.5 and §3.7), as well as their implications for our further study into the nature and application of distributed representations for semantics."
    }, {
      "heading" : "3.2 Frame-Semantic Parsing",
      "text" : "According to the theory of frame semantics (Fillmore, 1982), a semantic frame represents an event or scenario, and possesses frame elements (or semantic roles) that participate in the event. From the perspective of Natural Language Processing, frame semantics provide the formal basis for a parsing task—not unlike phrase structure grammars for constituencybased parse trees and dependency grammars for dependency-based parsing.\nFrame-semantic analysis as a task in NLP was pioneered by Gildea and Jurafsky (2002), who proposed a system for identifying semantic roles given a sentence and frame-annotated frame-inducing word. Gildea and Jurafsky (2002) based their work on the FrameNet formalism, with subsequent work in this area focusing on either the FrameNet or PropBank framework. Among the two frameworks PropBank has proved somewhat more popular since. Supervised approaches typically use the corpora developed by the two projects of the same name.\nMost work on frame-semantic parsing has divided the task into two major subtasks: frame identification, namely the disambiguation of a given predicate to a frame, and argument identification (or semantic role labeling), the analysis of words and phrases in the sentential context that satisfy the frame’s semantic roles (Das et al., 2010; Das et al., 2014).\n23\nNaturally, there are some exceptions, wherein the task has been modelled using a pipeline of three classifiers that perform frame identification, a binary stage that classifies candidate arguments, and argument identification on the filtered candidates (Baker et al., 2007; Johansson and Nugues, 2007).\nWe focus on the first subtask, frame identification for given predicates, which we tackle using distributed representations as a key input. Subsequently, we combine our distributed approach to this problem with a standard argument identification method (Appendix A). This allows us to compare our approach with the state-of-the-art on the full frame-semantic parsing task.\nFrame-semantic analysis has received a significant boost in attention owing to the CoNLL 2004 and 2005 shared tasks (Carreras and Màrquez, 2004; Carreras and Màrquez, 2005) on PropBank semantic role labeling (SRL). At least since then, it has been treated as an important problem in NLP. That said, research has mostly focused on argument identification, the second of the two subtasks described above, entirely skipping the frame disambiguation step and its potential interaction with argument analysis.\nFrame-semantic parsing is closely related to SRL and describes the full process of resolving a predicate sense into a frame and the subsequent analysis of the frame’s arguments. Therefore it could be viewed as a strict extension of SRL for situations where sentences may contain multiple frames and moreover where those frames require labeling on top of argument identification. Due to the differences between PropBank and FrameNet, as detailed below, work in this area focuses on the FrameNet full text annotations of the SemEval’07 data (Baker et al., 2007). The original FrameNet corpus is unsuitable for this task as it consists of exemplar sentences with a single annotated frame each.\nNotable work on frame-semantic parsing includes Johansson and Nugues (2007), the best performing system at SemEval’07 and Das et al. (2010) who significantly improved performance before presenting the current state-of-the-art system in Das et al. (2014). Matsubayashi et al. (2009) provide an overview of the efficacy of various argument identification features exploiting different types of taxonomic relations to generalize over roles."
    }, {
      "heading" : "3.2.1 FrameNet",
      "text" : "The FrameNet project (Baker et al., 1998) is a lexical database that contains information about words and phrases (represented as lemmas conjoined with a coarse part-of-speech\n24\nJohn bought a car .\nCOMMERCE_BUY buy.V\nBuyer Goods\nJohn bought a car .\nbuy.01 buy.V\nA0 A1\nMary sold a car .\nCOMMERCE_BUY sell.V\nMary sold a car .\nsell.01 sell.V\ntag) termed as lexical units, with a set of semantic frames that they could evoke. For each frame, there is a list of associated frame elements (or roles, henceforth), that are also distinguished as core or non-core.1 Sentences are annotated using this universal frame inventory. For example, consider the pair of sentences in Figure 3.1(a). COMMERCE BUY is a frame that can be evoked by morphological variants of the two example lexical units buy.V and sell.V. Buyer, Seller and Goods are some example roles for this frame."
    }, {
      "heading" : "3.2.2 PropBank",
      "text" : "The PropBank project (Palmer et al., 2005) is another popular resource related to semantic role labeling. The PropBank corpus has verbs annotated with sense frames and their arguments. Like FrameNet, it also has a lexical database that stores type information about verbs, in the form of sense frames and the possible semantic roles each frame could take. There are modifier roles that are shared across verb frames, somewhat similar to the noncore roles in FrameNet. Figure 3.1(b) shows annotations for two verbs “bought” and “sold”, with their lemmas (akin to the lexical units in FrameNet) and their verb frames buy.01 and 1Additional information such as finer distinction of the coreness properties of roles, the relationship between frames, and that of roles are also present, but we do not leverage that information in this work.\n25\nsell.01. Generic core role labels (of which there are seven, namely A0-A5 and AA) for the verb frames are marked in the figure.2 A key difference between the two annotation systems is that PropBank uses a local frame inventory, where frames are predicate-specific. Moreover, role labels, although few in number, take specific meaning for each verb frame. Figure 3.1 highlights this difference: while both sell.V and buy.V are members of the same frame in FrameNet, they evoke different frames in PropBank. In spite of this difference, nearly identical statistical models could be employed for both frameworks."
    }, {
      "heading" : "3.3 Model Overview",
      "text" : "We model the frame-semantic parsing problem in two stages: frame identification and argument identification. As mentioned in §3.1, these correspond to a frame disambiguation stage and a stage that finds the various arguments that fulfil the frame’s semantic roles within the sentence, respectively. We are particularly interested in the frame disambiguation or identification stage. To exemplify this stage, consider PropBank, where the predicate buy (or the lexical unit3 buy.V) has three verb frames. We want to learn to disambiguate between these frames given a sentential context.\nThis framework is similar to that of Das et al. (2014), with the difference that Das et al. (2014) solely focus on FrameNet corpora. The main novelty of our approach lies in the frame identification stage (§3.4). Note that this two-stage approach is unusual for the PropBank corpora when compared to prior work, where the vast majority of published papers have not focused on the verb frame disambiguation problem at all, only focusing on the role labelling stage. We refer the interested reader to the overview paper of Màrquez et al. (2008) for more information on this.\nWe approach the frame identification stage from a distributed perspective and present a model that takes word embeddings (distributed representations in Rn) as input and learns to identify semantic frames given these embeddings. Specifically, we use word representations to capture the syntactic context of a given predicate instance, and to represent this context as a vector. This can be exemplified using a short sentence such as “He runs the\n2NomBank (Meyers et al., 2004) is a similar resource for nominal predicates, but we do not consider it in our experiments.\n3PropBank never formally uses the term lexical unit, whose usage we adopt from the frame semantics literature.\n26\ncompany”. Here, the predicate runs has two syntactic dependents—a subject and direct object (but no prepositional phrases or clausal complements).\nMaking use of these syntactic dependencies, we could represent the context of runs as a structured vector with slots for all possible types of dependents warranted by a syntactic parser. With a structured vector, we refer to a vector where certain spans over indices are reserved for certain content. So, in this case, this hypothetical structured vector might contain a slot for the subject dependency at 0 . . . n, a second slot corresponding to the direct object complement in n+1 . . . 2n, a third slot corresponding to a clausal complement dependent in 2n+1 . . . 3n, and so forth. Overall, this results in a vector of Rkn, where k is the number of dependency types available in a given parsing formalism.\nGiven our example sentence “He runs the company.”, this would result in a vector representation where the subject dependent slot contains the embedding of he and the direct object dependent slot contains the embedding for company, with all other slots empty. For the purposes of our model, we define empty slots as being equal to zero.\nNow, given such input vectors for our training data, we learn a mapping from this highdimensional space Rkn into a lower dimensional space Rm. Simultaneously, the model learns an embedding for all the possible labels (i.e. the frames in a given lexicon). At inference time, the predicate-context is mapped to the low dimensional space, and we choose the nearest frame label as our classification. There are multiple reasons for taking this approach. First, learning a mapping into a lower dimensional space Rm makes the model more efficient at run time, as the nearest neighbour classification takes place in a smaller search space. More importantly, the two mappings of training data and labels into the joint space describe the actual model parameter space, with our update function being able to learn which dimensions of the larger input space Rkn are relevant; to what extent; and in which combination. Third, by placing the labels in this joint space—rather than learning a multi-class classifier from that space to the set of labels—joint inference of the model is simplified. Related experiments within the context of image annotation have highlighted the effectiveness of this approach (Weston et al., 2011)."
    }, {
      "heading" : "3.4 Frame Identification with Embeddings",
      "text" : "We continue using the example sentence from §3.3: “He runs the company”, for which we want to disambiguate the frame of runs in context. First, we extract the words in the\n27\nsyntactic context of runs and concatenate their word embeddings as described in §3.3 to create an initial vector space representation. Using this vector representation, which will be high dimensional, we learn a mapping into a lower dimensional space. In this lowdimensional space we also learn representations for each possible frame label. This enables us to posit the task of frame identification as a distance measure, where each frame instance is resolved to the closest suitable label in the low-dimensional space. In order to accomplish this, we use an objective function that ensures that the correct frame label is as close as possible to the mapped context representations, while competing frame labels are farther away.\nFormally, let x represent the actual sentence with a marked predicate, along with the associated syntactic parse tree; let our initial representation of the predicate context be g(x). Suppose that the word embeddings we start with are of dimension n. Then g is a function from a parsed sentence x to Rkn, where k is the number of syntactic context types considered by g and will vary depending on g. For instance, assume gz to only consider clausal complements and direct objects. Then gz : X → R2n, with 0 . . . n reserved for the clausal complement and n+1 . . . 2n reserved for direct objects. In the case of our example sentence, the resultant vector would have zeros in positions 0 . . . n and the embedding of\n28\nthe word company in positions n+1 . . . 2n:\ngz(x) = [0, . . . , 0, embedding of company]\nThe actual context representation extraction function g we use in our experiments is somewhat more complex than this. We describe this function in §3.4.1. Next, we describe the mapping from Rkn to the low dimensional joint space. Let the low dimensional space we map to be Rm and the learned mapping be M : Rkn → Rm. The mappingM is a linear transformation, and we learn it using the WSABIE algorithm (Weston et al., 2011). WSABIE also learns an embedding for each frame label (y, henceforth). In our setting, this means that each frame corresponds to a point in Rm. Given F possible frames we can store those parameters in an F × m matrix, one m-dimensional point for each frame, which we will refer to as the linear mapping Y . Thus, we have two mappings into Rm: M : Rkn → Rm\nY : {1 . . . F} → Rm (3.1)\nAssume a frame lexicon which stores all available frames, corresponding semantic roles and the lexical units associated with the frame. Let the lexical unit (the lemma conjoined with a coarse POS tag) for the marked predicate be `. We denote the frames that associate with ` in the frame lexicon and our training corpus as F`.\nAs discussed in §3.3, we follow the learning method for joint embedding spaces as introduced in Weston et al. (2011), following their success in using this model to learn distributed representations for annotating images. WSABIE performs gradient-based updates on an objective that tries to minimize the distance between M(g(x)) and the embedding of the correct label Y (y), while maintaining a large distance between M(g(x)) and the other possible labels Y (ȳ) in the confusion set F`. At disambiguation time, we use the dot product as our distance metric, meaning that the model chooses a label by computing the argmaxys(x, y) where s(x, y) = M(g(x)) · Y (y), where the argmax iterates over the possible frames y ∈ F` if ` was seen in the lexicon or the training data, or y ∈ F , if it was unseen. This disambiguation scheme is similar to the one adopted by Das et al. (2014), but they use unlemmatised words to define their confusion set. Model learning is performed using the margin ranking loss function as described in Weston et al. (2011), and in more detail in §3.4.2.\n29\nSince WSABIE learns a single mapping M from g(x) to Rm, parameters are shared between different words and different frames. So for example “He runs the company” could help the model disambiguate “He owns the company.” Moreover, since g(x) relies on word embeddings rather than word identities, information is shared between words. For example “He runs the company” could help us to learn about “She runs a corporation”. It is due to this kind of information sharing that we believe that distributed representations are a suitable vehicle for encoding semantics and by extension the continuous and multidimensional degrees of semantic similarity that could be useful for learning from examples such as above. These types of semantically motivated inference problems are also one of the reasons why we selected semantic frame-identification as a good problem to evaluate the efficacy of semantic distributed representations."
    }, {
      "heading" : "3.4.1 Context Representation Extraction",
      "text" : "In principle g(x) could be any feature function. For the purposes of our investigation we focus on a particular variant, where our representation is a block vector where each block corresponds to a syntactic position relative to the frame inducing token (e.g. the predicate). Each block’s value of course corresponds to the embedding of the word at that syntactic position, as we described using the example of gz(x) above. Therefore we have g(x) ∈ Rkn, where n is the dimension of the input word embeddings and k is the number of positions we are modelling, and hence k is the number of blocks. We parse the input sentence using a dependency parser, and consider positions relative to the frame inducing element in terms of the parse tree.\nWe consider two types of relative positions. First, we consider syntactic dependents in a fashion corresponding to the example provided in §3.4. To elaborate, the positions of interest are the labels of the direct dependents of the predicate, so k is the number of labels that the dependency parser can produce. For example, if the label on the edge between runs and He is nsubj, we would put the embedding of He in the block corresponding to nsubj. If a label occurs multiple times, then the embeddings of the words below this label are averaged.\nUnfortunately, using only the direct dependents can miss a lot of useful information. For example, topicalisation can place discriminating information farther from the predicate.\n30\nConsider “He runs the company.” vs. “It was the company that he runs.” In the second sentence, the discriminating word, company, dominates the predicate runs.\nSimilarly, predicates in embedded clauses may have a distant agent which cannot be captured using direct dependents. For example, consider the sentences “The athlete ran the marathon.” vs. “The athlete prepared himself for three months to run the marathon.” In the second example, for the predicate run, the agent The athlete is not a direct dependent, but is connected via a longer dependency path.\nFor these reasons, and in order to capture more relevant context, we include a second group of syntactic dependents defined as follows: We scanned the training data for a given task (either the PropBank or the FrameNet domains) for the dependency paths that connected the gold predicates to the gold semantic arguments. This set of dependency paths were deemed as possible positions in the initial vector space representation.\nThus, the final cardinality of k is the sum of the number of scanned gold dependency path types plus the number of dependency labels in our parser. Given a predicate in its sentential context, we therefore extract only those context words that appear in positions warranted by the accordingly defined set. See Figure 3.2 for an illustration of this process. Note that in this figure we posit the linear mapping M : Rkn → Rm as a fully weighted additive function of embeddings, with each input block i ∈ {1..k} being associated with a submatrix Mi ∈ Rn×m. Note that this reformulation results in exactly the same representations as the original formulation where empty blocks are represented with zeros."
    }, {
      "heading" : "3.4.2 Learning",
      "text" : "We model our objective function following Weston et al. (2011), by using a weighted approximate-rank pairwise loss, learned with stochastic gradient descent. The mapping from g(x) to the low dimensional space Rm is a linear transformation, so the model parameters to be learnt are the matrix M ∈ Rkn×m as well as the embedding of each possible frame label, represented as another matrix Y ∈ RF×m when there are F frames in total. The training objective function minimizes∑\nx ∑ ȳ L ( ranky(x) ) [γ + s(x, y)− s(x, ȳ)]+\nwhere x, y are the training inputs and their corresponding correct frames, and ȳ are negative frames (that do not correspond with x), γ is the margin and s(x, y) is the score between an\n31\ninput and a frame. Further, [x]+ = max(0, x) denotes the standard hinge loss and ranky(x) is the rank of the positive frame y relative to all the negative frames:\nranky(x) = ∑ ȳ I(s(x, y) ≤ γ + s(x, ȳ)),\nand L(η) converts the rank to a weight, e.g. L(η) = ∑η\ni=1 1/i.\nThe purpose of L is to convert the rank to a weighting of the given pairwise constraint comparing d and d̄. Choosing L(η) = Cη for any positive constant C optimizes the mean\nrank, whereas a weighting such as L(η) = ∑η\ni=1 1/i (adopted here) optimizes the top of\nthe ranked list, as described in Usunier et al. (2009). To train with such an objective, we can employ stochastic gradient descent.\nFor speed the computation of ranky(x) is then replaced with a sampled approximation: sample N items ȳ until a violation is found, i.e. max(0, γ + s(x, ȳ) − s(x, y)) > 0 and then approximate the rank with (F − 1)/N ; see Weston et al. (2011) for more details on this procedure. For the choices of the stochastic gradient learning rate, margin (γ) and dimensionality (m), please refer to §3.5.4-§3.5.5. Note that an alternative approach could learn only the matrix M , and then use a knearest neighbour classifier in Rm, as in Weinberger and Saul (2009). The advantage of learning an embedding for the frame labels is that at inference time we need to consider only the set of labels for classification rather than all training examples. Additionally, since we use a frame lexicon that gives us the possible frames for a given predicate, we usually only consider a handful of candidate labels. If we used all training examples for a given predicate for finding a nearest-neighbour match at inference time, we would have to consider many more candidates, making the process very slow."
    }, {
      "heading" : "3.5 Experiments",
      "text" : "In this section, we present our experiments in frame-semantic parsing. As already motivated in §3.1, we first evaluate our frame identification system in isolation to evaluate whether distributed representations can be a suitable choice for a problem. Subsequently, we combine our system with a standard argument identification system (Appendix A), which allows us to compare performance with the previous state of the art.\nWhile we used a standard approach for the argument identification step, we used a custom implementation and a number of new features. Therefore, in order to evaluate the\n32\neffects of the distributed model fairly, we also combine our argument identification model with a standard log-linear frame identification model for comparison (see 3.5.2)."
    }, {
      "heading" : "3.5.1 Data",
      "text" : "We run experiments on both FrameNet- and PropBank-style structures. For FrameNet, we use the full-text annotations in the FrameNet 1.5 release4 which was used by Das et al. (2014). We used the same test set as Das et al. containing 23 documents with 4,458 predicates. Of the remaining 55 documents, 16 documents were randomly chosen for development (Appendix B), resulting in a development set with a simlar number of predicates.\nFor experiments with PropBank, we used the OntoNotes corpus (Hovy et al., 2006), version 4.0, and only made use of the Wall Street Journal documents. Following convention, we used sections 2-21 for training, section 24 for development and section 23 for testing. This split also resembles the setup used by Punyakanok et al. (2008). All the verb frame files in OntoNotes were used for creating our frame lexicon."
    }, {
      "heading" : "3.5.2 Frame Identification Baselines",
      "text" : "As briefly rationalised above, we also implemented a set of baseline models with the purpose of isolating the effect of the distributed model within the full frame semantic parsing setup. Our baselines are log-linear models with varying feature configurations. At training time, the baseline models use the following probability:\np(y|x, `) = e ψ·f(y,x,`)∑\nȳ∈F` e ψ·f(ȳ,x,`) (3.2)\nAt test time, this model chooses the best frame as argmaxyψ · f(y, x, `) where argmax iterates over the possible frames y ∈ F` if ` was seen in the lexicon or the training data, or y ∈ F , if it was unseen, like the disambiguation scheme of §3.4. We train this model by maximizing the L2 regularized log-likelihood. We use L-BFGS for training, with the regularisation constant set to 0.1 in all experiments.\nFor comparison with our model from §3.4, which we call WSABIE EMBEDDING, we implemented two baselines with the log-linear model. Both of these baseline models rely on a variation of the context extraction mechanism developed for the distributed model and described in §3.4.1. 4https://framenet.icsi.berkeley.edu.\n33\nThe first baseline reuses the context extraction system, but uses word identities rather than word embeddings for its representation. The model first conjoins the word identities with their respective dependency paths, and second uses the words themselves as backoff features. This model could be viewed as a standard NLP approach for the frame identification problem. However, despite its simplicity, we find that it performs competitively with the state of the art. We refer to this baseline model as LOG-LINEAR WORDS.\nOur second baseline decouples the concept of using embeddings from the specific training method applied here. Specifically, it decouples the WSABIE training from the embedding input, and trains a log-linear model using the embeddings as input. Thus, this second baseline model (LOG-LINEAR EMBEDDING) has exactly the same input representation as the WSABIE EMBEDDING model. Unlike the WSABIE EMBEDDING model, however, the LOG-LINEAR EMBEDDING model does not learn distributed representations for labels in a joint space but instead simple probabilities given the high-dimensional distributed input."
    }, {
      "heading" : "3.5.3 Common Experimental Setup",
      "text" : "We process our PropBank and FrameNet training, development and test corpora with a shift-reduce dependency parser that uses the Stanford conventions (de Marneffe and Manning, 2013) and which relies on an arc-eager transition system with beam size of 8; the parser and its features are described by Zhang and Nivre (2011).\nBefore parsing the data, it is tagged with a POS tagger trained using a conditional random field (Lafferty et al., 2001) with the following emission features: the word, the word cluster, word suffixes of length 1, 2 and 3, capitalization, whether it has a hyphen, digit and punctuation. Beyond the bias transition feature, we have two cluster features for the left and right words in the transition. We use Brown clusters learned using the algorithm of Uszkoreit and Brants (2008) on a large English newswire corpus for cluster features. We use the same word clusters for the argument identification features in Table A.1.\nWe learn the initial embedding representations for our frame identification model (§3.4) using a deep neural language model similar to the one proposed by Bengio et al. (2003). We use 3 hidden layers each with 1024 neurons and learn a 128-dimensional embedding from a large corpus containing over 100 billion tokens. In order to speed up learning, we use an unnormalised output layer and a hinge-loss objective. The objective tries to ensure that\n34\nthe correct word scores higher than a random incorrect word, and we train with minibatch stochastic gradient descent."
    }, {
      "heading" : "3.5.4 Experimental Setup for FrameNet",
      "text" : ""
    }, {
      "heading" : "3.5.4.1 Hyperparameters",
      "text" : "We optimise the hyperparameters for the WSABIE training on our development data. There are three relevant parameters: the stochastic gradient descent learning rate, which we set to 0.0001, the margin γ which we set to 0.01 and the dimensionality of the joint space Rm, where we set m to 256. See Table 3.1 for an overview of the parameters we evaluated in the hyperparameter search. Our hyperparameter sweep optimises for ambiguous frame identification accuracy, that is the performance of the model with respect to identifying the frames of lexical units with more than one possible semantic frame."
    }, {
      "heading" : "3.5.4.2 Argument Candidates",
      "text" : "The candidate argument extraction method used for the FrameNet data (as described in Appendix A) was adapted from the algorithm of Xue and Palmer (2004) applied to dependency trees. Since the original algorithm was designed for verbs, we augmented the set of rules to also handle non-verbal predicates as follows:\n1. We added the predicate itself as a candidate argument.\n2. We added the span ranging from the sentence position to the right of the predicate to\nthe rightmost index of the subtree headed by the predicate’s head. This helps capture cases like “a few months”, where few is the predicate and months is the argument.\n35\n3. We added the span ranging from the leftmost index of the subtree headed by the pred-\nicate’s head to the position immediately before the predicate, for cases like “your gift to Goodwill”, where to is the predicate and your gift is the argument.\nNote that Das et al. (2014) describe the state of the art in FrameNet-based analysis, but their argument identification strategy considered all possible dependency subtrees in a parse, resulting in a much larger search space."
    }, {
      "heading" : "3.5.4.3 Frame Lexicon",
      "text" : "In our experimental setup, we scanned the XML files in the “frames” directory of the FrameNet 1.5 release, which lists all the frames, the corresponding roles and the associated lexical units, and created a frame lexicon to be used in our frame and argument identification models.\nA frame lexicon extracted using such a process encompasses all relevant units for our test case. Therefore, at frame disambiguation time, we only have to score the frames in F` for each predicate `. In essence this means that for all instances we only had to choose a semantic frame from a small list of candidates rather than from the global set of available frames in FrameNet. Please refer back to §3.4 and §3.5.2 for more details on this. We refer to this setup as FULL LEXICON.\nWhile attempting to compare our system with the prior state of the art (Das et al., 2014), we noted that they found several unseen predicates at test time. This is due to their frame lexicon creation method: Instead of scanning the frame files, Das et al. (2014) constructed a frame lexicon by scanning FrameNet’s exemplars and the training corpus. For fair comparison, we replicated their lexicon and removed all instances containing their unseen predicates from our training data, thereby having to select semantic frames from F rather than F` for those predicates during test time. We refer to this setup as SEMAFOR LEXICON, and further report results on the set of unseen instances used in that paper."
    }, {
      "heading" : "3.5.4.4 ILP Constraints",
      "text" : "For FrameNet, we used three integer linear programming (ILP) constraints during argument identification (Appendix A). First, each span could have only one role; second, each core role could be present only once; and third, all overt arguments had to be non-overlapping.\n36"
    }, {
      "heading" : "3.5.5 Experimental Setup for PropBank",
      "text" : "From the point of view of frame identification, the PropBank and FrameNet annotation schemes differ in an important respect. PropBank frames are specific to a predicate, so for example run.01 is a sense of run, which means to operate a machine or company, and is very similar in meaning to operate.01, but when the system sees the word run in a sentence, the sense run.01 is available while operate.01 is not available. In contrast, FrameNet frames are shared between verbs, and both operate and run can evoke the frame OPERATING A SYSTEM. Because of this difference, the set of experiments possible for PropBank and FrameNet differ in a number of aspects."
    }, {
      "heading" : "3.5.5.1 Hyperparameters",
      "text" : "We report the hyperparameters in Table 3.1, which we chose by searching over the same space as described for the FrameNet case above. Again, we optimised for ambiguous lexical units in the hyperparameter search."
    }, {
      "heading" : "3.5.5.2 Argument Candidates",
      "text" : "For PropBank we use the algorithm of Xue and Palmer (2004) applied to dependency trees. As PropBank only considers predicates, there was no need to adjust the algorithm."
    }, {
      "heading" : "3.5.5.3 Frame Lexicon",
      "text" : "For the PropBank experiments we scanned the frame files for propositions in OntoNotes 4.0, and stored possible core roles for each verb frame. The lexical units were simply the verb associating with the verb frames. There were no unseen verbs at test time."
    }, {
      "heading" : "3.5.5.4 ILP Constraints",
      "text" : "For PropBank, we used five ILP constraints. The first three were the same as the ones for FrameNet (§3.5.4.4). In addition, we also made sure that: 4) continuation arguments of the form C-A* could appear only after the corresponding overt A* argument, and 5) relative arguments of the form R-A* could appear only if the corresponding A* argument is present. These constraints are identical to the ones proposed and used in Punyakanok et al. (2008).\n37\n38"
    }, {
      "heading" : "3.5.6 FrameNet Results",
      "text" : "Tables 3.2 and 3.3 present accuracy results on frame identification. We present results on all predicates, ambiguous predicates seen in the lexicon or the training data, and rare ambiguous predicates that appear≤ 11 times in the training data. The WSABIE EMBEDDING model performs significantly better than the LOG-LINEAR WORDS baseline, while LOGLINEAR EMBEDDING underperforms in every metric.\nFor the SEMAFOR LEXICON setup, we compare with the state of the art from Das et al. (2014), who used a semi-supervised learning method to improve upon a supervised latent-variable log-linear model. We outperform their system on every metric, including the unseen predicates setting. When removing the artificial restrictions on our lexicon— introduced for a fair comparison with Das et al. (2014)—the absolute accuracy numbers of the WSABIE EMBEDDING model increase further to 88.73%.\nAs discussed previously, we next evaluate our model on the full frame-semantic parsing task, by combining the frame identification model with an argument identification model (as described in Appendix A). We evaluate this task adhering to the SemEval 2007 shared task evaluation setup. Again, the WSABIE EMBEDDING outperforms the previously published best results, setting a new state of the art. The results on the development data are in Table 3.4, with the actual results on the test data in Table 3.5."
    }, {
      "heading" : "3.5.7 PropBank Results",
      "text" : "Tables 3.6 and 3.7 show frame identification results on the PropBank development- and test-data, respectively. On the development set, our best model performs with the highest accuracy on all and ambiguous predicates, but performs worse on rare ambiguous predi-\n39\n40\ncates. On the test set, the LOG-LINEAR WORDS baseline performs best by a very narrow margin. We analyse this in the discussion section §3.6. Next, following the FrameNet-structure set of experiments, we provide results on the full frame-semantic parsing task in Tables 3.8 and 3.9. The results follow the same trend as in the frame identification task.\nFinally, in Tables 3.10 and 3.11, we present SRL results that measure the argument performance only, irrespective of the choice of frame. This task, for which we use the evaluation script from CoNLL 2005 (Carreras and Màrquez, 2005), allows us to compare the PropBank results to the state-of-the-art results on that frame-semantic formalism.\nWe note that with a better frame identification model, our performance on SRL improves in general. Here too, the embedding model barely misses the performance of the best baseline, but we are at par and sometimes better than the single parser setting of a state-of-the-art SRL system (Punyakanok et al., 2008). Note that the Combined results refer to a system which uses the combination of two syntactic parsers as input."
    }, {
      "heading" : "3.6 Discussion",
      "text" : "With the models and experiments in this chapter we wanted to establish whether distributed representations are a suitable mechanism for addressing semantic tasks. The experimental evaluation of our WSABIE EMBEDDING model strongly supports this hypothesis. In this section we will analyse the experimental evaluation in greater detail, before drawing more general conclusions as to the role of distributed representations for semantics in §3.7, the concluding section of this chapter.\n41\nLet us first consider the FrameNet experiments. Here, the WSABIE EMBEDDING model strongly outperforms all baselines as well as the prior state of the art on all metrics. Setting a new state of the art in its own right already strongly suggests that our distributional approach is suitable for this task; however, in order to determine this more clearly, we further isolated the performance of the distributed approach from all other aspects of the model.\nWhen comparing the WSABIE EMBEDDING model to our two baselines, the LOG-LINEAR EMBEDDING and the LOG-LINEAR WORDS models, we discover a number of interesting results. First, we notice that WSABIE EMBEDDING performs better than the LOG-LINEAR WORDS model, which could be seen as the discrete counterpart to our distributed model. This result, combined with the fact that the LOG-LINEAR WORDS model still outperforms the prior state of the art (Das et al., 2014) supports the hypothesis of this thesis, namely that distributed representations can bring an advantage to semantically challenging tasks. We believe this performance gain stems from the fact that the WSABIE EMBEDDING model allows examples with different labels and confusion sets to share information, as all labels live in a single space and as all examples are mapped into this space using a single projection matrix.\nThe second result is more interesting yet: While the WSABIE EMBEDDING model outperforms the LOG-LINEAR WORDS model, that model in turn performs consistently better than the LOG-LINEAR EMBEDDING model. This could be due to a number of reasons. One particular advantage of the WSABIE EMBEDDING approach over the LOG-LINEAR EMBEDDING model is that the former learns a transformation which maps all instances and labels into a single joint space, while the latter learns to classify for each label independently. Therefore the LOG-LINEAR EMBEDDING model is less able to share information across examples and labels, and consequently more likely to suffer from sparsity-related effects.\nWhy the LOG-LINEAR WORDS model outperforms the LOG-LINEAR EMBEDDING model, is a more difficult question to answer. One likely explanation is related to the above analysis: the failure of the log-linear approaches to share information across multiple labels and confusion sets may be more pronounced in the embedded case, where the input representations will have significantly higher dimensionalities than in the LOG-LINEAR WORDS case. An important realisation here is that while distributed input representations can be highly advantageous (as demonstrated with WSABIE EMBEDDING), they are not guaranteed\n42\nto bring a benefit, particularly when applied in a fairly brute-force fashion as we have with the LOG-LINEAR EMBEDDING model.\nOn the PropBank data, we see that the LOG-LINEAR WORDS baseline has roughly the same performance as our model on most metrics: slightly better on the test data and slightly worse on the development data. This can partially be explained with the significantly larger training set size for PropBank, making features based on words more useful. This cannot be the only explanation, however, since the LOG-LINEAR EMBEDDING baseline underperforms on the FrameNet data sets as well.\nAnother important distinction between PropBank and FrameNet is that the latter formalism shares frames between multiple lexical units. We see this particularly when looking at the “Rare” column in Table 3.7. WSABIE EMBEDDING performs poorly in this setting while LOG-LINEAR EMBEDDING performs well. Part of the explanation has to do with the particulars of WSABIE training. Recall that the WSABIE EMBEDDING model needs to estimate the label location in Rm for each frame. In other words, it must estimate 512 parameters based on at most 10 training examples. However, since the input representation is shared across all frames, every other training example from all the lexical units affects the optimal estimate, since they all modify the joint parameter matrix M . By contrast, in the log-linear models each label has its own set of parameters, and they interact only via the normalization constant. The LOG-LINEAR WORDS model does not have this entanglement, but cannot share information between words. For PropBank, these drawbacks and benefits balance out and we see similar performance for LOG-LINEAR WORDS and LOG-LINEAR EMBEDDING. In the FrameNet setting, estimating the label embedding is not as much of a problem because even if a lexical unit is rare, the potential frames can be frequent. For example, we might have seen the SENDING frame many times, even though telex.V is a rare lexical unit."
    }, {
      "heading" : "3.7 Summary",
      "text" : "In this chapter we have presented a simple model that outperforms the previous state of the art on FrameNet-style frame-semantic parsing, and performs on par with one of the best single-parser systems on PropBank semantic role labelling.\nImportantly, this model utilises distributed semantic representations as its input. Unlike the prior state of the art, Das et al. (2014), our model does not rely on heuristics to\n43\nconstruct a similarity graph and leverage WordNet; hence, in principle it is generalisable to varying domains, and to other languages. This again highlights the extreme usefulness of distributed representations—as previously pointed out in Chapter 2—for solving a wide variety of tasks without relying on task-specific supervised data or annotations.\nAs for the question asked at the outset of this chapter, our results clearly indicate the efficacy of distributed representations in conveying semantic information and thus being able to solve semantic tasks. The relative performance of the two models relying on distributed representations—with WSABIE EMBEDDING performing very strongly and setting the new state of the art on FrameNet and LOG-LINEAR EMBEDDING performing poorly on most metrics—further lead us to the conclusion that the performance of models relying on distributed representations strongly depends on a number of factors beyond the quality of the input representations alone.\nThe analysis of sparsity-related issues allowed us to highlight some of these factors. In the FrameNet case, where there was less training data in absolute terms, but more data sharing possible between representations, the WSABIE based model performed best, whereas in the PropBank case, sparsity became more an issue for exactly that model (which needed to learn representations in Rm for a large number of labels) than for the LOG-LINEAR WORDS approach, as also reflected by the results on that task. This was highlighted especially by the experiments on rare lexical units. On the FrameNet data—with the possibility of full information sharing—the relative rarity of a lexical unit did not strongly affect accuracy. This result gives further credence to the hypothesis that we can use distributed representations to encode and share semantics, assuming that the strong performance on rare words was caused by semantically motivated information sharing from more frequent lexical units of the same label during training. That this is in case so, is supported by the results on the PropBank rare data. Here, no information is shared between labels, and consequently the LOG-LINEAR WORDS model outperforms the WSABIE EMBEDDING setting.\nIn combination, these results indicate two important factors to consider when using distributed representations as inputs in an NLP task. First, sparsity still remains an issue and needs to be considered. Second, the key benefit of distributed representations lies in their ability to share information. In order to successfully make use of distributed representations, models are required that use their inputs in a manner that maximises the possibility for information sharing and minimises the impact of sparsity.\n44\nAnother key difference between the log-linear and the WSABIE approach is that the latter learns a transformation of its input into a smaller space. This could be seen as a form of composition of individual representations into a shared, dense representation of a complex object (see Figure 3.2 for that interpretation), where this smaller-space representation benefits from increased information sharing across variables. While the complex object in this case is a frame instance represented by a single vector, it is feasible to consider generalizing such methods for representing other complex linguistic structures by a composed distributed representation. We will study such systems in the remainder of this thesis.\n45\nPart II\nCompositional Semantics\n46\nChapter 4\nCompositional Distributed Representations\nChapter Abstract\nThis chapter surveys prior work on compositional semantics and gives an overview of key principles and methods in that field. We motivate compositional semantics by demonstrating the limitations of distributional (collocational) representations beyond the word level. Having established the need for composition in distributed semantics, we attempt to categorise prior work in this area along two axes. We introduce the distinction between distributional and distributed representations and finally introduce a number of standard techniques for representation learning in recursive and recurrent systems which are essential tools for all models of compositional distributed semantics."
    }, {
      "heading" : "4.1 Introduction",
      "text" : "So far in this thesis, we have covered distributed models of semantics at the word level. However, for a number of important problems, semantic representations of individual words do not suffice, but instead a semantic representation of a larger structure—e.g. a phrase or a sentence—is required. As elaborated in §3.7, the WSABIE EMBEDDING model of Chapter 3 was a borderline case, as the model learned to predict labels given a structured input consisting of multiple words, with those words however being disconnected. In other cases, for\n47\ninstance when wanting to establish the sentiment or the veracity of a sentence, such an approach is likely to be insufficient and instead a complete representation of the full sentence is more likely to lead to success. Therefore, during the past few years, research has shifted from using distributional methods for modelling the semantics of words to using them for modelling the semantics of larger linguistic units such as phrases or entire sentences.\nSelf-evidently, sparsity prevents the learning of such higher representations using the same collocational methods as applied to the word level. This can be illustrated by considering corpus statistics. For instance, when considering the statistics of the Europarl Corpus v7 (Koehn, 2005) in Table 4.1, the problem with learning distributed representations at the sentence-level becomes apparent. While out of 304, 786 words, 58, 552 appear ten or more times, this is only the case for 579 sentences, roughly 0.0003% of the total number of unique sentences found in that corpus. Hence, in order to learn meaningful statistics at the sentence level, exponentially more training data is required compared to the word level.\nMost literature instead focuses on learning composition functions that represent the semantics of a larger structure as a function of the representations of its parts. Simple algebraic composition functions have been shown to suffice for tasks such as judging bigram semantic similarity (Mitchell and Lapata, 2008). More complex models, learning distributed representations for sentences or documents, have proved useful in tasks such as sentiment analysis (Socher et al., 2011b; Hermann and Blunsom, 2013), relational similarity (Turney, 2012) or dialogue analysis (Kalchbrenner and Blunsom, 2013).\nIn this chapter, we first discuss the theoretical foundations of semantic compositionality and their implications for modelling distributed compositional models (§4.2), before surveying a number of architectures for such composition functions in §4.3. Next, we provide an overview over commonly used objective functions and error signals employed for\n48\nlearning both composition functions and semantic representations (§4.4) and finally, §4.6 provides a survey of applications for such models in the literature."
    }, {
      "heading" : "4.2 Theoretical Foundations",
      "text" : "The meaning of an utterance is a function of the meanings of its parts and their composition rules. (G. Frege, 1892)\nSince Frege stated his ‘Principle of Semantic Compositionality’ in 1892 researchers have pondered both how the meaning of a complex expression is determined by the meanings of its parts, and how those parts are combined (Frege, 1892; Pelletier, 1994). Over a hundred years on, the choice of representational unit for this process of compositional semantics, and how these units combine, remain open questions.\nWithin Natural Language Processing, work on semantic representations can roughly be grouped into two distinct categories. On one side, Montagovian approaches are concerned with symbolic entities composed into logical representations of meaning. Here, meaning is typically associated with sentences, which are expressed as logical formulae over abstract types. On the other hand, distributional approaches such as discussed in this thesis, learn continuous representations of individual words. Frequently, the distributional approach to semantics stops at precisely that point, namely the word level. However, there has been some work on applying distributional (and by extension distributed) approaches to semantic composition, in order to learn such distributed representations for higher linguistic units beyond the word level.\nIn this thesis we focus on semantic composition on the basis of distributed representations. With that we mean the derivation of distributed representations of phrases or other grammatical units based on models of semantic composition and representations for the parts which these larger structures are composed of. Other aspects of compositionality, such as questions of compositionality versus lexicality of compounds—‘gravy sauce’ vs. ‘gravy train’ (Bannard et al., 2003; Biemann and Giesbrecht, 2011; Hermann et al., 2012a, inter alia))—are not considered here.\nFollowing Frege’s principle, we assume that the meaning of a sentence is composed of the meanings of the individual words or phrases it contains, and likewise, that the meaning of a phrase can be composed of the meanings of its words. As we assume distributed\n49\nrepresentations as the basis of our work, we can define a general composition function, following Mitchell and Lapata (2008), as\np = f(u, v, R,K), (4.1)\nwhere p represents the composed meaning of inputs u and v given some background knowledge K and some syntactic relation R between u and v. While Mitchell and Lapata (2008) assume u and v to be vectors representing the semantics of the underlying words, we remove any assumptions about the shape of the semantic representation of words and larger structures.\nThis formulation of semantic composition captures “a wide class of composition functions,” (Mitchell and Lapata, 2010) sharing this underlying principle of semantic composition. Typically, a number of constraints are placed on f , such as u, v,p ∈ Rn, which ensures that f can be applied recursively. Conventionally the background knowledge K is dropped from most composition models. In §4.3 we survey the main classes of composition models in the literature implementing such functions f .\nThe notion of compositionality we consider here is comparable to that found in Montagovian formal semantics (Montague, 1970; Montague, 1974) (in spirit, at least). In Montague’s system, grammatical analysis rules are paired with higher-order logical interpretations, allowing the derivation of the logical interpretation of a sentence to be obtained deterministically from its syntactic structure. In effect, syntax guides semantic composition.\nUnlike the distributed representations discussed in this thesis, such frameworks typically represent meaning symbolically (hence symbolic logic), which does not make them well suited for quantitative modelling. Consider the semantic space as represented by a symbolic logic. Here, similarity is well defined—two atoms are similar if equal and likewise for compound types—but this definition has no tolerance for noisy representation or vague semantics. In fact, semantic similarity in symbolic logic collapses onto type identity, which is undesirable in instances where semantic ambiguity needs to be accounted for. We expand on the discussion of these types of grammars in Chapter 5, where we empirically evaluate the role of syntax in learning models for compositional distributed semantics.\nThe basic composition formula (Equation 4.1) places no constraints on the form of its inputs. While we generally assume u and v to be vectors, they could just as well be\n50\nsymbols (see previous paragraph) or more complex representations such as tensors or tuples of several types of representations.\nTensor spaces in particular have attracted a lot of attention as a possible format for representing compositional semantics. Tensor representations have the nice property that predicate-argument completion can be posited as tensor contraction with tensors of suitable rank (Clark et al., 2008; Coecke et al., 2010; Grefenstette, 2013b, inter alia). We will describe this idea in more detail in §4.3.2."
    }, {
      "heading" : "4.3 Architectures",
      "text" : "Various architectures exist for composing distributed representations. We provide a survey over a number of relevant approaches as proposed in the literature. We use the function defined above (Eq. 4.1) as the basis of this survey.\nWe attempt to structure prior work in this field along two dimensions (see Figure 4.1). First, we differentiate between the type of composition function, which can roughly be divided between algebraic composition models and lexical function models. On the sec-\n51\nond axis, we differentiate between distributional and distributed approaches, where distributional approaches rely on collocational or other distributional information for model learning and word representations."
    }, {
      "heading" : "4.3.1 Algebraic Composition",
      "text" : "Algebraic composition has been proposed in the context of distributional semantics as a simple mechanism for obtaining distributed representations for composed words. Algebraic composition functions simplify Equation 4.1 by removing the background knowledge K, and frequently also the relational information R, effectively reducing the function signature to\np = f(u, v) (4.2)\nAddition can be seen as the simplest form of algebraic composition. Assume representations for words red, apple as ~vred, ~vapple. Then, under the additive model, we would represent red apple as\n~vred apple = ~vred + ~vapple\nAddition (or averaging) has successfully been applied to some problems such as essay grading (Landauer and Dumais, 1997) or selectional preference (Kintsch, 2001). However, addition as a composition function makes no use of syntactic information or for that matter word ordering. Therefore, two sentences with matching words but different meaning would be represented by the same vector. An example for this problem is given in Mitchell and Lapata (2010), who compare the two sentences in Figure 4.2. While the two sentences share the same set of words, clearly their meaning is entirely different.\nOther algebraic functions that have been explored in the literature include pointwise multiplication, weighted addition, dilation, and tensor products (outer product or Kronecker\n52\nproduct) — see Table 4.2 (Mitchell and Lapata, 2008; Mitchell and Lapata, 2009; Guevara, 2010; Zanzotto et al., 2010, inter alia). Related research also includes holographic reduced representations, random indexing and convolution products (Widdows, 2004; Widdows, 2008). An extensive survey of these algebraic operators applied to semantic composition can be found in Mitchell and Lapata (2010) together with comparative results across a number of tasks. When applied recursively, such models are frequently augmented with a non-linearity such as a hyperbolic tangent or sigmoid function. Among other effects, this ensures that word ordering is accounted for in the model, resulting in different representations depending on the composition order. One popular model that uses such a composition function are recursive autoencoders as presented in Socher et al. (2011b).\nLimitations While again some success could be shown for specific tasks such as similarity ratings of adjective-noun, noun-noun and verb-object pairs, all of these algebraic operators face similar limitations as the additive model. Many algebraic operators ignore word ordering, thereby essentially turning these composition models into bag of word models. Even those algebraic operators that take word order into account, such as tensor products or dilation, ignore all syntactic information. This limitation contradicts evidence from the literature which suggests that such syntactic information is not only useful but necessary for semantic composition. Chapter 5 investigates this question by empirically studying the role of syntax in compositional semantics.\nBeyond this, some algebraic models suffer from tendencies caused by their underlying algebraic functions. For instance, multiplicative models will tend to zero with increasing sentence length, while additive models will tend to the average word representation in\n53\na corpus. Chapter 2 in Grefenstette (2013a) provides a comprehensive analysis of these effects."
    }, {
      "heading" : "4.3.2 Lexical Function Models",
      "text" : "Lexical function models could be seen as an extension of the algebraic composition methods described in §4.3.1. While algebraic composition is largely parameter-free, lexical function models rely on some parametrization in their composition function. Further, making use of syntactic information, composition can be governed by parse trees or similar syntactic structures.\nAugmented Algebraic Models One approach to incorporate lexical information in the composition function is to augment algebraic composition models with syntactic information. For instance, Zanzotto et al. (2010) propose an enhanced additive model (BAM-SP) for estimating selectional preference that incorporates the syntactic relation between two words into its composition function\np = u Rv(r) (4.3)\nwith u and v reversed if v is the semantic head of the two words, and Rv(r) returns a selectional preference vector for word v and relation r. Thereby, the composition function is now a function not only of the two input words, but also of the specific relation r connecting the two words. They demonstrated that this addition of syntactic information substantially improved results on a number of tasks compared with standard algebraic composition models. This model is essentially a parametrised version of a simple multiplicative model, where syntactic relations govern one of the vector choices.\nGuevara proposes a partial least squares regression model to learn a similar composition function, which extends the general additive model from Table 4.2 (Guevara, 2010; Guevara, 2011). This approach, as that of Zanzotto et al. (2010), parametrizes its composition function on the syntactic relationship between the two words that are to be composed (here limited to adjective-noun and verb-noun pairs).\nA key difference of these models compared to the algebraic composition models above is that the composition parameters are learned by considering the distributional representations of both unigrams and bigrams, thereby enabling the regression learning. The idea\n54\nbehind this is to directly learn bigram representations using some distributional representation learning method. These bigram representations are then used together with unigram representations to learn the model parameters.\nSyntactic Functions A second group of lexical function models separates the functional information from the lexical elements more strictly. An example for this, also described in Zanzotto et al. (2010), is the general additive composition model\np = Au +Bv, (4.4)\nwhere A,B are square matrices “capturing the relation R and the background knowledge K” of Equation 4.1. This means that the word representations u and v are learnt independently of their syntactic function.\nApplied to sets of adjective-noun, noun-noun and verb-noun sequences, the authors demonstrated that this type of composition function can be trained to successfully separate synonymous pairs from other data using a cosine similarity measure to determine semantic similarity.\nImplicit Lexical Functions A number of variations and extensions to syntactic function models as briefly outlined above are easily imaginable. An alternative approach would be to encode the composition function directly in the semantic representation of a linguistic unit. Technically the BAM-SP model by Zanzotto et al. (2010) could be counted as such an approach, with linguistic units being represented by sets of vectors, and the composition step choosing the appropriate vector depending on the composition context.\nA more principled approach is the idea to use representations that fully incorporate the composition function, removing the need for additional variables such as in the fully additive approach. Building on pregroup grammars (Clark et al., 2008; Coecke et al., 2010) introduced in §4.2, tensor-based composition methods have been proposed as one such alternative mechanism for capturing semantic interaction between words.\nThe original idea of using tensor products for semantic composition is attributed to\nSmolensky (1990), who proposed such a model in cognitive science1:\nMeaning is represented by a set of structure roles {ri}, which may be occupied by fillers fi. Then, s is a set of constituents, each a filler/role binding fi/ri.\n1The following description is based on that in Clark et al. (2008).\n55\nLinguistic structures such as parse trees or predicate-argument systems can be thought of as such sets of constituents. In order to represent a set of constituents, Smolensky and Legendre (2006) then propose a tensor product formulation:\ns = ∑ i fi ⊗ ri (4.5)\nThe proposal in Clark et al. (2008) extends this concept under the idea of unifying distributional and symbolic models of meaning. As we described earlier, symbolic approaches to semantics have predominately concerned themselves with the composition of atomic types into larger logical structures, whereas distributed approaches frequently failed to go beyond the word level. Picking up the idea of using tensor products for composition, they propose to use distributional representations at the word level in combination with some form of symbolic representation at the sentence level, giving sentence representations such as (from Clark et al. (2008)):\ndrinks⊗ subj ⊗ John⊗ obj ⊗ (beer ⊗ adj ⊗ strong)⊗ adv ⊗ quickly\nA more recent line of research has focused on tensor contraction as opposed to tensor products for semantic composition (Coecke et al., 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Grefenstette, 2013a, inter alia). An important consideration here is to condition the shape of each representation by its role, which allows the removal of the role representations {ri}. One example for this approach is Baroni and Zamparelli (2010), which focuses on the case of adjective-noun composition. Here, adjectives are represented as matrices and nouns as (distributional) vectors. Thus, adjective-noun composition becomes a simple matrix-vector product with adjectives being a linear map over nouns. It is easy to see how this approach naturally extends to other linguistic units, such as intransitive (matrices) and transitive verbs (order 3 tensors)."
    }, {
      "heading" : "4.3.3 Recursive Composition",
      "text" : "Most of the work described so far is concerned with either theoretical approaches to semantic composition or with semantic composition of small units such as adjective-noun pairs or noun compounds, where distributional methods can be combined with regression analysis to learn a composition function.2\n2Of course, the implicit lexical models are an exception here as these models provide a natural mechanism for composition of arbitrary depth given sufficiently complex implicit representations.\n56\nIn terms of the matrix of composition models in §4.3 (Figure 4.1), these models primarily cover the left half of the model space. As the curse of dimensionality precludes the application of the distributional hypothesis to linguistic structures significantly beyond the size of unigrams, alternative approaches are required for capturing compositional semantics of larger structures such as sentences. The implicit approaches combining distributional and symbolic forms of composition have similar limitations: While technically only unigram representations need to be learned, it remains unclear how such higher-order representations (e.g. order-3 tensors for transitive verbs) can be learned. Further, for more complex linguistic units which can take several arguments, the order of the tensor to be learned would become infeasibly large.\nFor the sake of argument, if one used CCG types to condition the shape of representations as has been proposed (Clark et al., 2008), a simple word such as for could require an order 5 tensor with its type signature ((S\\NP )\\(S\\NP ))/NP . Even with a modest 50 dimensional embedding, this would result in a 312,500,000 dimensional representation for this word alone, calling into question the practicability of such an approach. There exist a number of approximate learning algorithms and other simplifications such as low-rank tensor approximations that can help to address this issue (e.g. Grefenstette et al., 2013; Baroni and Zamparelli, 2010), and hence tensor-based models remain a very plausible space to be explored. For the purposes of this thesis, we focus on approaches to compositional semantics that do not rely on implicit functional representations, but instead use composition functions in a more conventional matrix-vector setup. With implicit functional representation we mean the idea of encoding all functional information within the word representation. This is opposed to approaches that use a combination of word representations and word-independent functional features for semantic composition.\nNow the key question is how to solve the learning problem without making use of distributional representations for composed entities. Resolving this issue would allow us to take any of the composition functions proposed so far and to recursively apply these to learn distributed representations of sentences and other higher order linguistic structures. Indeed, many of the models introduced in the previous two sections have trivial recursive extensions, meaning that they could be used to compute a representation not only for a pair of words, but recursively for a phrase, a sentence or a document and so on.\nThe generalised composition models presented in this section can be viewed as the\n57\ndistributed extension of compositional semantics. The composition models discussed in the previous two subsections all rely on distributional embeddings as the basis of their composition. Generalised vector space composition models can still be instantiated using distributional embeddings, but no longer rely on them. Instead, word representations and composition functions are jointly learned given some objective function.\nHere, we describe structures for producing distributed representations at the sentence or other higher level. The remainder of this background chapter will then outline a number of objective functions that can be used for training such systems.\nRecursive Neural Networks (RecNN, henceforth) are one popular mechanism for combining fully additive models recursively. A single composition step can be described with the following template:\np = g(Ax+By + d) (4.6)\nwhere x, y are input vectors, A,B weight matrices and d a bias term. g represents an element-wise activation function such as a sigmoid or hyperbolic tangent function. This non-linearity is required to turn this composition step into a single layer neural network. Subsequently, by enforcing A and B to be quadratic, we ensure that p, x, y, d ∈ Rn for some n. This of course allows the composition function to be applied recursively along some binary-branching structure such as binarised parse trees or simple left-to-right trees (see Figure 4.3). Recursive Neural Networks in that fashion were first applied to language in Socher et al. (2011b), who used such a model for predicting sentiment (see §4.6).\n58\nConvolutional Neural Networks (ConvNN, henceforth) provide a different fashion for composing multiple vectors, which is based on research on visual cortices in animals (Hubel and Wiesel, 1968). The underlying idea is to connect cells in such a fashion that while each outer cell may cover only a small region of the visual field, cells on subsequent layers indirectly cover increasingly large regions of the visual field. This enables animals, humans and machines to exploit local correlations. Convolutional Neural Networks have proved a popular tool in visual recognition tasks such as document recognition and OCR (LeCun et al., 1998a).\nHowever, the underlying idea can easily be ported to natural language processing when replacing the visual input with text (see Figure 4.4 for a schematic depiction of two such ConvNN). See (e.g. Kalchbrenner et al., 2014) for the application of ConvNN to modelling language. Unlike a RecNN, a ConvNN does not require any syntactic or other parse. The composition step of a ConvNN with receptive width 2 is equivalent to that of a RecNN (Equation 4.6), with additional weight matrices required for larger receptive widths.\nConvNN can be structured in a number of fashions. Typically, weight matrices and biases are either shared across all composition steps or alternatively across all composition steps within a given layer of the network. This weight sharing is important, as it allows the network to detect features irrespective of their position in the input.\nMatrix-Vector Neural Networks (MV-RNN, henceforth) are an extension of RecNN or ConvNN with a more complex input representation (Socher et al., 2012b). Here, instead of representing words by a vector, words and other units are represented by a tuple consisting\n59\nof a vector a ∈ Rn and a matrix A ∈ Rn×n. Composition is achieved using the template:\np = g(V Ab+WBa+ d) (4.7)\nP = MA+NB (4.8)\nwhere V,W,M,N are matrices in Rn×n and d, g as before. This extends the matrix-vector multiplication approaches presented earlier (Mitchell and Lapata, 2010; Zanzotto et al., 2010) and (Baroni and Zamparelli, 2010) by adding a non-linearity as well as a mechanism for propagating weight matrices in a recursive setup."
    }, {
      "heading" : "4.4 Learning Signals",
      "text" : "The models discussed in §4.3.3 provide an architecture for producing composed representations given a lexicon of input (word) embeddings, as well as a set of model parameters θ, while being agnostic about how these parameters and input embeddings are learned. In this section, we survey the most popular types of learning signals used in conjunction with compositional vector space models."
    }, {
      "heading" : "4.4.1 Autoencoders",
      "text" : "Autoencoders provide a popular method for learning embeddings from unsupervised data or in conjunction with other, supervised signals. They are a useful tool to compress information. One can think of an autoencoder as a funnel through which information has to pass (see Figure 4.5). By forcing the autoencoder to reconstruct an input given only the reduced amount of information available inside the funnel it serves as a compression tool, representing high-dimensional objects in a lower-dimensional space.\nTypically a given autoencoder, that is the functions for encoding and reconstructing data, is used on multiple inputs. By optimizing the two functions to minimize the difference between all inputs and their respective reconstructions, this autoencoder will effectively discover some hidden structures within the data that can be exploited to represent it more efficiently.\nAs a simple example, assume input vectors xi ∈ Rn, i ∈ (0..N), weight matrices W enc ∈ R(m×n),W rec ∈ R(n×m), biases benc ∈ Rm, brec ∈ Rn and some non-linearity g.\n60\nThe encoding matrix and bias are used to create an encoding ei from xi:\nei = g (f enc(xi) = W encxi + b enc) (4.9)\nSubsequently e ∈ Rm is used to reconstruct x as x′ using the reconstruction matrix and bias:\nx′i = (f rec(ei) = W recei + b rec) (4.10)\nθ = (W enc,W rec, benc, brec) can then be learned by minimizing the error function describing the difference between x′ and x:\nE = 1\n2 N∑ i ‖x′i − xi‖ 2 (4.11)\nNow, if m < n, this will intuitively lead to ei encoding a latent structure contained in xi and shared across all xj, j ∈ (0..N), with θ encoding and decoding to and from that hidden structure."
    }, {
      "heading" : "4.4.1.1 Recursive Autoencoders",
      "text" : "In §4.4.1, we introduced autoencoders as a simple mechanism to extract latent structure by enforcing data to learn a joint compression and reconstruction regime.\nIt is possible to apply multiple autoencoders on top of each other, creating a deep autoencoder (Bengio et al., 2007; Hinton et al., 2006). For such a multi-layered model to\n61\nlearn anything beyond what a single layer could learn, a non-linear transformation g needs to be applied at each layer. Usually, a variant of the logistic (σ) or hyperbolic tangent (tanh) function is used for g (LeCun et al., 1998b).\nf enc(xi) = g (W encxi + b enc) (4.12) f rec(ei) = g (W recei + b rec)\nFurthermore, autoencoders can easily be used as a composition function by concatenat-\ning two input vectors, such that:\ne = f(x1, x2) = g (W (x1‖x2) + b) (4.13)\n(x′1‖x′2) = g (W ′e+ b′)\nExtending this idea, recursive autoencoders (RAE) allow the modelling of data of variable size. By setting the n = 2m, it is possible to recursively combine a structure into an autoencoder tree. See Figure 4.6 for an example, where x1, x2, x3 are recursively encoded into y2. Thus, an RAE is equivalent to a RecNN (§4.3.3) with an additional decoding layer for each composition step.\nThe recursive application of autoencoders was first introduced in Pollack (1990), whose recursive auto-associative memories learn vector representations over pre-specified recursive data structures. More recently this idea was extended and applied to dynamic structures (Socher et al., 2011b). These types of models have become increasingly prominent since\n62\ndevelopments within the field of Deep Learning have made the training of such hierarchical structures more effective and tractable (LeCun et al., 1998b; Hinton and Salakhutdinov, 2006).\nIntuitively the top layer of an RAE will encode aspects of the information stored in all of the input vectors. Previously, RAEs have successfully been applied to a number of tasks including sentiment analysis, paraphrase detection, relation extraction and 3D object identification (Blacoe and Lapata, 2012; Socher et al., 2011b; Socher et al., 2012b).\nDepending on the layout and structure of an RAE setup, one has to be careful to prevent degeneration. If an objective function jointly optimises the error at each layer of an autoencoder, e.g. in a convolution setup, a degenerate solution is to minimise the magnitude of all encoding and reconstruction weights, as this would cause a large error on the first level but minimal error on all subsequent levels. There are various strategies for avoiding such behaviour, such as normalising embeddings at each level in the autoencoder. Another strategy is to use unfolding autoencoders which only consider the reconstruction error of original input elements. We introduce unfolding autoencoders in the next subsection.\n63"
    }, {
      "heading" : "4.4.1.2 Unfolding Autoencoders",
      "text" : "Unfolding autoencoders are an extension of recursive autoencoders, where each reconstruction step is applied recursively until an original input is reconstructed (Socher et al., 2011a). Figure 4.7 demonstrates this: where a standard RAE would stop the reconstruction step from y2 at y′1, the unfolding autoenocoder continues its recursive reconstruction by unfolding y′1 into x ′′ 1 and x ′′ 2, thereby reconstructing data from the input layer.\nUnfolding autoencoders have a number of nice properties. Particularly, the unfolding prevents an RAE from degenerating, as standard recursive autoencoders are incentivised to learn small weights for all internal layers, thereby shrinking the overall reconstruction error. As the unfolding autoencoder measures its error function always by comparing with input weights, this strategy becomes void. Of course, if input weights are updated as part of the learning process, the issue of degenerating all weights to zero still persists, and needs to be addressed separately."
    }, {
      "heading" : "4.4.1.3 Denoising Autoencoders",
      "text" : "Another variant of autoencoders are denoising autoencoders. Here, the idea is to force the hidden layer to discover structures in the data by making it reconstruct the input data from a corrupted version of itself, with the idea that this process will improve the robustness of the discovered representations (Vincent et al., 2008).\nDenoising autoencoders are effectively a stochastic version of regular autoencoders, which corrupt the input layer before feeding it into the encoding function. A number of possibilities exist for this corruption function. In Vincent et al. (2008), it consists of randomly setting a number of inputs to zero. Alternative corruption processes could introduce a random amount of Gaussian noise for instance.\nThe denoising autoencoder thus works as follows (using the RecNN notation):\nx̃1 ∼ q(x̃1|x1) (4.14) x̃2 ∼ q(x̃2|x2)\ne = g(Ax̃1 +Bx̃2 + d)\nr1 = g(A ′ 1e+ d ′ 1) r2 = g(A ′ 2e+ d ′ 2) E = ‖x1 − r1‖2 + ‖x2 − r2‖2\n64\nHere, x̃1 is the corrupted version of x1, which is used for the encoding e and reconstruction step r1. Finally, however, the error is calculated by comparing the uncorrupted input with the reconstructed vector.\nThe noise added by corrupting inputs acts in a similar to a regularization term in this setup. For a detailed account of denoising autoencoders and their underlying mathematics, please refer to Vincent et al. (2008) and Bengio (2009). Also of interest is Wager et al. (2013), who provide an account of how denoising acts as a regularisation mechanism in neural network."
    }, {
      "heading" : "4.4.2 Classification",
      "text" : "While autoencoders simply learn to compress data efficiently by making use of latent structures within the data, frequently we want to train models for a specific task. For instance for a paraphrase detection task we are interested in a robust semantic representation that allows us to identify phrases with similar meaning, and hence autoencoders may be a suitable mechanism for learning such a system. On the other hand, if we want to train a system to discover sentiment in text, it might make more sense to make use of some training data to help the composition function on the aspects of the representation that are relevant for sentiment.\nFor this purpose, classification systems and errors can be used in conjunction with a semantic composition process. A supervised classification layer can be applied to the root node of a recursive compositional model, or indeed to any tree nodes of the model. A simple binary classifier could be a sigmoid layer such as this:\npredictor(l=1|v, θ) = sigmoid(Wlabelv + blabel) (4.15)\nwhere v is the vector to be classified, and Wlabel, blabel are the classifier weights and bias, respectively. θ here represents the set of all model data.\nGiven a label and encoding pair (l, e), the classifier error can be formulated as:\nElabel(l, e, θ) = 1\n2 ‖l − o‖2 (4.16)\nwith o = sigmoid(Wlabele+ blabel) (4.17)\nSet in the context of a recursive composition model, such a classifier (and the corresponding classification error learning signal) can be applied to any number of nodes in\n65\nthe composition tree. If we assume that only the top node in a tree will be used for classification, we get the following objective function. Assume a corpus of input label pairs (x, l) ∈ D where x = 〈x0, x1, . . . , xj〉 and j = |x|. Further assume a recursive composition function such that e = recfunc(x) returns the representation on top of the composition tree. Then we have\nJ = 1\n|D| D∑ (x,l) E(x, l, θ) + λ 2 ||θ||2 (4.18)\nE(x, l, θ) = 1\n2 ‖l − recfunc(x)‖2 (4.19)\nwhere the second term of the objective function is a standard L2 regularization parameter."
    }, {
      "heading" : "4.4.3 Bilingual Constraints",
      "text" : "Beyond the autoencoding and classification signals discussed in the previous subsections, we can also use specificities in the data for training composition models. For instance, when training a model on data where we know that two inputs capture the same information, we can make use of that knowledge to force the model to assign similar representations to both inputs. This is particularly of interest when dealing with inputs that are different on the surface level but equivalent on a higher (i.e. composed) level. In computer vision an example for this would be multiple photos of the same object, in compositional semantics two sentences with the same underlying meaning.\nWhen considering semantic representations an obvious source for such data are paraphrases and multilingual corpora. In particular, multilingual aligned data has nice theoretical properties that could allow models to learn representations further removed from mono-lingual surface realisations. As this is part of the novel work presented in this thesis, we defer this discussion to the separate chapter on multilingual signals (Chapter 6)."
    }, {
      "heading" : "4.4.4 Signal Combination",
      "text" : "A nice property of the space of recursive functions we are investigating here is that the various learning signals proposed in this chapter are easily combinable. For instance, in the following chapter we will combine recursive autoencoders with a classification signal. Due to the type of gradient learning employed in training such models, additional signals can be added at a cost linear in the number of training instances.\n66"
    }, {
      "heading" : "4.5 Learning",
      "text" : "Here we discuss strategies for efficiently learning word embeddings and model parameters for compositional vector space models. First, we investigate how to calculate gradients in recursive and dynamic structures. Second, we provide an overview of gradient descent algorithms used for updating weights."
    }, {
      "heading" : "4.5.1 Signal Propagation in Recursive Structures",
      "text" : "Given some error signal E, we need to compute the gradients ∂E/∂w for all w ∈ θ. In neural networks and other deep, recursive or multi-layered setups, backpropagation can be used to efficiently calculate these gradients (Goller and Küchler, 1996).\nBackpropagation works by calculating the partial derivatives with respect to all temporary and internal nodes in a network, and by using the chain rule to efficiently calculate partial derivatives of nodes lower in the network based on those of nodes higher up (closer to the output / error signal). For a simple example, assume a two layer neural network with input i, intermediate encoding e and output o all in Rn:\nz = W ai+ ba (4.20)\ne = σ(z)\nk = W be+ bb\no = σ(k)\nE = ‖o− y‖2,\nwhere W a ∈ Rm×n and W b ∈ Rm×n are encoding and reconstruction matrices and ba ∈ Rm and bb ∈ Rn are bias vectors. Further, σ represents the sigmoid function, and y the expected output of the network. Using the intermediate values at z and k, the derivatives with respect to the error function E can efficiently be calculated for all model weights and inputs. For instance, the partial derivatives for the reconstruction weight matrix can be calculated as in Table 4.3. Using the chain rule, we get\n∂E ∂W b = ∂E ∂o ∂o ∂k ∂k ∂W b (4.21) ∂E\n∂i = ∂E ∂o ∂o ∂k ∂k ∂e ∂e ∂z ∂z ∂i (4.22)\n67\nand so forth. This allows us to avoid the repetitive calculation of intermediate values such as ∂o ∂k , which are shared by several gradients relevant for the model update."
    }, {
      "heading" : "4.5.1.1 Backpropagation Through Structure",
      "text" : "For deep, recursive networks, backpropagation through structure (BPTS) provides an efficient mechanism for learning gradients (Goller and Küchler, 1996). BPTS is essentially the extension of simple backpropagation to general structured models.\nUnlike the example in §4.5.1, node x receives multiple signals from the structure, as the objective function will include terms covering both reconstruction layers as well as\n68\npropagated signals from higher up in the model. We know that\n∂E ∂x = ∑ γ∈Γ ∂E ∂γ ∂γ ∂x (4.23) Γ = Successors of x\nThis allows us to efficiently calculate all partial derivatives with respect to E.\nA slight modification of the previous example (Equation 4.20) can serve to demonstrate\nthis. Assume an equivalent network with a new error function\nE = ‖o− î‖2 (4.24)\nwhere î = i (this helper variable will keep the notation clean). This is the formulation for a simple autoencoder setup with a non-linear encoding function, two weight matrices and bias terms. Now, term i has two successors in the network: first the intermediate value z, and second the error function E via î. Thus, we get\n∂E ∂i = ∑\nγ∈{z,̂i}\n∂E ∂γ ∂γ ∂i (4.25)\n= ∂E\n∂o\n∂o ∂k ∂k ∂e ∂e ∂z ∂z ∂i + ∂E\n∂î\n∂î ∂i\nBackpropagation through structure is then the process of optimizing this derivative calculation by computing intermediate derivatives in such an order that no partial derivative needs to be computed repeatedly."
    }, {
      "heading" : "4.5.2 Gradient Update Functions",
      "text" : "Using the backpropagation strategy as described above, we can represent a model during training by a vector over its parameters θt and an equally sized vector of gradients ∇θt = ∇J(θt) at time t given some objective function J . In order to minimize J , a number of strategies are available to us for updating weights."
    }, {
      "heading" : "4.5.2.1 Standard Gradient Descent",
      "text" : "A standard (or batch) gradient descent method updates weights using a uniformly weighted gradient subtraction\nθt+1 = θt − α∇θt (4.26)\n69\nwhere α is the step size or learning rate (these two terms can be used interchangeably). Thus, given some differentiable objective function, all model parameters are iteratively updated based on their gradient with respect to this error function. For convex problems, batch gradient descent is guaranteed to converge as long as the learning rate α is chosen appropriately.\nFor large amounts of training data, it can be prohibitively expensive and inexpedient to perform a full batch gradient update. An alternative is to use mini-batch or on-line stochastic gradient descent. Here, gradients are calculated for a random subset (in on-line gradient descent a subset of size one) of the training data. Assume a mini-batch size of n and a corpus ofm training examples. Then each mini-batch contains m n instances, and the gradient at each step is calculated with respect to the objective function given those instances. While this gradient is unlikely to match the true gradient (the objective function differentiated with respect to all inputs), such a strategy can lead to faster convergence as each individual iteration will be much cheaper to calculate. The assumption for this is that while the gradient update steps will be slightly incorrect, on average they would still lead to the correct direction. For convex problems, stochastic gradient descent is also almost guaranteed to converge under the Robbins-Siegmund theorem (e.g. Bottou, 2010), provided some mild conditions are met such as the learning rate α decreasing at an appropriate rate. Unfortunately, most composition models have non-convex objective functions, which means that typically only a local minimum will be found."
    }, {
      "heading" : "4.5.2.2 L-BFGS",
      "text" : "The Broyden-Fletcher-Goldfarb-Shanno algorithm (BFGS, henceforth), and particularly the limited memory version of this algorithm (L-BFGS), provides an alternative to the default gradient descent mechanism. BFGS is a quasi-Newtonian method, which estimates the Hessian matrix Bt (the matrix of second derivatives) of J and uses this to determine the search direction pt of its gradient update step.\nBtpt = −∇J(θt) (4.27)\nBFGS uses this search direction in combination with a line search method to perform its gradient update, while simultaneously updating its estimation of the Hessian.\nWhile BFGS is proven to find a global optimum for convex optimization problems, it performs well also on non-convex problems. Particularly, when used in conjunction with\n70\nWolfe-type or Armijo-type line search, BFGS is globally convergent for functions with Lipschitz continuous gradients (Li and Fukushima, 2000). This is an important result as the types of non-linearities typically employed in the recursive frameworks described in this thesis (sigmoid and hyperbolic tangent functions) have Lipschitz continuous gradients, thereby guaranteeing that BFGS will converge to some stationary point.\nFor more details about the properties of BFGS and various line search methods that can be used in conjunction with BFGS or other gradient descent algorithms, the reader is referred to Chapter 3 of Nocedal and Wright (2006)."
    }, {
      "heading" : "4.5.2.3 Adaptive Gradient Descent",
      "text" : "Adaptive (sub)gradient descent (AdaGrad, henceforth) is another popular gradient descent algorithm (Duchi et al., 2011) which adjusts the learning rate on a feature-by-feature basis. A general class of constrained optimisation update functions for an objective function J can be described by the following function:\nθt+1 = Π diag(Gθ)\n1/2\nθ\n( θt − ηdiag(Gt)−1/2∇J(θt) ) , (4.28)\nwhere Gt = ∑t\nτ=1∇J(θτ )∇J(θτ )> is the outer product of the subgradients, diag(x) extracts the diagonal of a matrix x, and ΠAX (y) = argminx∈X (x−y) · A(x−y) denotes the projection of a point y onto X according to A. η denotes some fixed step size.\nThis generalised gradient update function can be simplified. While the projection ΠAX(y) is necessary for constrained problems where it is used to project parameters onto the feasible set, we can remove this projection for unconstrained problems. Further, if θ is represented as a vector, equation 4.28 can further be reduced to\nθt+1 = θt − ηG−1/2t ∇J(θt), (4.29)\nwhere Gt = ∑t\nτ=1∇J(θτ )2. This algorithm is similar to second-order gradient descent, as the G−1/2t term approximates the Hessian of the objective function J . AdaGrad has proved a very popular algorithm for performing gradient descent. In practice it converges much faster than L-BFGS on problems where calculating the objective function is expensive, which makes the line search in L-BFGS costly. Further, AdaGrad is essentially parameter-free, with the step size being controlled by the proximal function (the Hessian approximation). For a detailed\n71\nanalysis and description of the AdaGrad algorithm, the reader is referred to Duchi et al. (2011)."
    }, {
      "heading" : "4.6 Applications",
      "text" : "The move from word representations to capturing the semantics of sentences and other composed linguistic units has yielded models applied to a large variety of NLP-related tasks. Here we provide a short (and by no means exhaustive) overview of some such applications. A number of publications targeted paraphrase detection (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011; Blacoe and Lapata, 2012, inter alia), sentiment analysis (Socher et al., 2011b; Socher et al., 2012b; Socher et al., 2013) , and semantic relation classification (ibid.). Other tasks include relational similarity (Turney, 2012) and discourse analysis (Kalchbrenner and Blunsom, 2013).\nConcerning the types of composition models discussed in this chapter, most efforts so far approach the problem of modelling phrase meaning through vector composition using linear algebraic vector operations (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Zanzotto et al., 2010), matrix or tensor-based approaches (Baroni and Zamparelli, 2010; Coecke et al., 2010; Kartsaklis et al., 2012; Grefenstette et al., 2013; Clark, 2013; Maillard et al., 2014, inter alia), or through the use of recursive auto-encoding (Socher et al., 2011a) or neural-networks (Socher et al., 2012b). Some alternative approaches avoid composition. For instance, Erk and Padó (2008) keep word vectors separate, using syntactic information from sentences to disambiguate words in context; likewise Turney (2012) treats the compositional aspect of phrases and sentences as a matter of similarity measure composition rather than vector composition."
    }, {
      "heading" : "4.7 Summary",
      "text" : "In this chapter we have surveyed the field of compositional semantics within the context of distributed representations. Having demonstrated in Chapter 3 that distributed representations are useful for solving semantically challenging tasks, we subsequently pointed out that for a large number of such tasks representations beyond the word level would be more beneficial.\n72\nWe explored and rejected the idea of extending the distributional hypothesis introduced in Chapter 2 to sentence-level representations in §4.1. Following that realisation, we investigated compositional semantics from the perspective of composing distributed representations into higher level representations. For this we first proposed the distinction between distributional and distributed representations and subsequently developed a model for categorising existing and new methods for semantic composition.\nThis chapter concluded with an overview of some of these models together with a brief introduction to various important mathematical concepts and learning algorithms that are essential for further work in this field. In the following two chapters we will build on the analysis and background knowledge presented here. With a view to validating the hypothesis stated in the beginning of this thesis, Chapter 5 attempts to evaluate the role of syntax in compositional semantics and, given a novel model for semantic vector composition, investigates our hypothesis that distributed representations are also suitable for encoding semantics at the sentence-level.\nBuilding on this, Chapter 6 goes one step further and investigates representation learning with the aim to minimise the impact of monolingual surface forms and other biases. For this, we propose a novel algorithm for capturing semantics from multilingual corpora. Using the insights gained in this introductory chapter as well as Chapter 5, this algorithm relies on semantic transfer at the sentence-level, thereby enabling us to jointly investigate both aspects of this thesis’ hypothesis.\n73\nChapter 5\nThe Role of Syntax in Compositional Semantics\nChapter Abstract\nModelling the compositional process by which the meaning of an utterance arises from the meaning of its parts is a fundamental task of Natural Language Processing. In this chapter we draw upon recent advances in the learning of vector space representations of sentential semantics and the transparent interface between syntax and semantics provided by Combinatory Categorial Grammar (CCG) to introduce Combinatory Categorial Autoencoders. This model leverages the CCG combinatory operators to guide a non-linear transformation of meaning within a sentence. We use this model to learn high dimensional embeddings for sentences and evaluate them in a range of tasks, demonstrating that the incorporation of syntax allows a concise model to learn representations that are both effective and general."
    }, {
      "heading" : "5.1 Introduction",
      "text" : "In this chapter we investigate compositional semantics from a syntactic perspective. This investigation is based on Frege’s principle (see §4.2), which claims that composed meaning\nThe material in this chapter was originally presented in Hermann and Blunsom (2013). Aspects of the material were also presented in Hermann et al. (2013).\n74\ncan be derived from the meaning of its parts and some composition rules. Frege’s principle may be debatable from a linguistic and philosophical standpoint, but it has provided a basis for a range of formal approaches to semantics which attempt to capture meaning in logical models. Montague grammar (Montague, 1970) is a prime example for this, building a model of composition based on lambda-calculus and formal logic. More recent work in this field includes Combinatory Categorial Grammar (CCG), which also places increased emphasis on syntactic coverage (Szabolcsi, 1989).\nFurther to Frege’s principle, empirical evidence in the literature suggests that such composition rules can benefit from syntactic information. Examples for this include Duffy et al. (1989), who investigated the effect of context for naming target words. In particular, the authors showed that such priming depended on a combination of multiple preceding words rather than just the individual preceding unigram. Subsequently, the third author of that paper showed in Morris (1994) that the exploitation of syntactic dependencies for priming improved experimental results. Morris and Harris (2002) discuss how early report bias also conforms to syntax, further supporting this argument, with other work reporting a loss of priming effects when scrambling words in a sentence (see Mitchell (2011) for a more in-depth survey of cognitive science research into this problem). These early results motivate a further investigation into the role of syntax in compositional semantics.\nAs discussed so far in this thesis, those searching for the right representation for compositional semantics have recently drawn inspiration from the success of distributional models of lexical semantics. This approach represents single words as distributional vectors, implying that a word’s meaning is a function of the environment it appears in, be that its syntactic role or co-occurrences with other words (Pereira et al., 1993; Schütze, 1998).\nThese vectors implicitly encode semantics by clustering words with similar environments within the vector space. While distributional semantics is easily applied to single words, sparsity implies that attempts to directly extract distributional representations for larger expressions are doomed to fail (see the discussion in §4.1). While in the past few years, attempts have been made at extending distributed representations to semantic composition (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2011b, inter alia), these approaches make minimal use of linguistic information beyond the word level.\n75\nHere, we attempt to bridge the gap between recent advances in machine learning and more traditional approaches within computational linguistics. We achieve this goal by employing the CCG formalism to consider compositional structures at any point in a parse tree. CCG is attractive both for its transparent interface between syntax and semantics, and a small but powerful set of combinatory operators with which we can parametrise our non-linear transformations of compositional meaning.\nWe present a novel class of recursive models, the Combinatory Categorial Autoencoders (CCAE), which marry a semantic process provided by a recursive autoencoder with the syntactic representations of the CCG formalism. Through this model we seek to answer two questions: Can recursive vector space models be reconciled with a more formal notion of compositionality; and is there a role for syntax in guiding semantics in these types of models? CCAEs make use of CCG combinators and types by conditioning each composition function on its equivalent step in a CCG proof. In terms of learning complexity and space requirements, our models strike a balance between simpler greedy approaches (Socher et al., 2011b) and the larger recursive vector-matrix models (Socher et al., 2012a).\nWe show that this combination of state of the art machine learning and an advanced linguistic formalism translates into concise models with competitive performance on a variety of tasks. In the experimental evaluation of our models we show that our CCAE models match or better comparable recursive autoencoder models."
    }, {
      "heading" : "5.2 Formal Accounts of Semantic Composition",
      "text" : "There exist a number of formal approaches to language that provide mechanisms for compositionality. Generative Grammars (Jackendoff, 1972) treat semantics, and thus compositionality, essentially as an extension of syntax, with the generative (syntactic) process yielding a structure that can be interpreted semantically. By contrast Montague grammar achieves greater separation between the semantic and the syntactic by using lambda calculus to express meaning. However, this greater separation between surface form and meaning comes at a price in the form of reduced computability. While this is outside the scope of this thesis, see e.g. Kracht (2008) for a detailed analysis of compositionality in these formalisms.\n76"
    }, {
      "heading" : "5.2.1 Combinatory Categorial Grammar",
      "text" : "In this chapter we focus on CCG, a linguistically expressive yet computationally efficient grammar formalism. It uses a constituency-based structure with complex syntactic types (categories) from which sentences can be deduced using a small number of combinators. CCG relies on combinatory logic (as opposed to lambda calculus) to build its expressions. For a detailed introduction and analysis vis-à-vis other grammar formalisms see e.g. Steedman and Baldridge (2011).\nCCG has been described as having a transparent surface between the syntactic and the semantic. It is this property which makes it attractive for our purposes of providing a conditioning structure for semantic operators. A second benefit of the formalism is that it is designed with computational efficiency in mind. While one could debate the relative merits of various linguistic formalisms, the existence of mature tools and resources, such as the CCGBank (Hockenmaier and Steedman, 2007), the Groningen Meaning Bank (Basile et al., 2012) and the C&C Tools (Curran et al., 2007) is another big advantage for CCG.\nCCG’s transparent surface stems from its categorial property: Each point in a derivation corresponds directly to an interpretable category. These categories (or types) associated with each term in a CCG govern how this term can be combined with other terms in a larger structure, implicitly making them semantically expressive.\nFor instance in Figure 5.1, the word likes has type (S[dcl]\\NP)/NP, which means that it first looks for a type NP to its right hand side. Subsequently the expression likes tigers (as type S[dcl]\\NP) requires a second NP on its left. The final type of the phrase S[dcl] indicates a sentence and hence a complete CCG proof.\nThus at each point in a CCG parse we can deduce the possible next steps in the deriva-\ntion by considering the available types and combinatory rules.\n77\nConsidering only the types of each word in a sentence, we can derive a parse tree consisting of a set of rules applied in a certain order in order to generate that sentence. Any given sentence in CCG is thus parametrised by its categories and the combinatory rules with which these categories are composed together to form a sentence. For our models in the rest of this chapter, we will exploit such parse trees and the combinatory rules used within, to govern the structure and combination mechanisms of our RAE."
    }, {
      "heading" : "5.3 Model",
      "text" : "The models in this chapter combine the power of recursive, vector-based models with the linguistic intuition of the CCG formalism. Their purpose is to learn semantically meaningful vector representations for sentences and phrases of variable size, while the purpose of this work is to investigate the use of syntax and linguistic formalisms in such vector-based compositional models.\nWe assume a CCG parse to be given. Let C denote the set of combinatory rules, and T the set of categories used, respectively. We assume a finite set of combinatory rules C (Table 5.3) and categories S.\nWe use the parse tree to structure an RAE, so that each combinatory step is represented\n78\nby an autoencoder function. We refer to these models as Combinatory Categorial Autoencoders (CCAE). In total we describe four models, each making increased use of the CCG formalism compared with the previous one (Table 5.1).\nAs an internal baseline we use model CCAE-A, which is an RAE structured along a CCG parse tree. CCAE-A uses a single weight matrix each for the encoding and reconstruction step (Table 5.2). This model is similar to Socher et al. (2011b), except that we use a fixed structure in place of the greedy tree building approach. As CCAE-A uses only minimal syntactic guidance, this should allow us to better ascertain to what degree the use of syntax helps our semantic models.\nOur second model (CCAE-B) uses the composition function in equation (5.1), with\nc ∈ C.\nf enc(x, y, c) = g (W cenc(x‖y) + bcenc) (5.1) f rec(e, c) = g (W crece+ b c rec)\nThis means that for every combinatory rule we define an equivalent autoencoder composition function by parametrizing both the weight matrix and bias on the combinatory rule (e.g. Figure 5.2).\n79\nIn this model, as in the following ones, we assume a reconstruction step symmetric with the composition step. For the remainder of this chapter we will focus on the composition step and drop the use of enc and rec in variable names where it isn’t explicitly required. Figure 5.3 shows model CCAE-B applied to our previous example sentence.\nWhile CCAE-B uses only the combinatory rules, we want to make fuller use of the linguistic information available in CCG. For this purpose, we build another model CCAEC, which parametrizes on both the combinatory rule c ∈ C and the CCG category t ∈ T at every step (see Figure 5.2). This model provides an additional degree of insight, as the categories T are semantically and syntactically more expressive than the CCG combinatory rules by themselves. Summing over weights parametrised on c and t respectively, adds an additional degree of freedom and also allows for some model smoothing.\nAn alternative approach is encoded in model CCAE-D. Here we consider the categories not of the element represented, but of the elements it is generated from together with the combinatory rule applied to them. The intuition is that in the first step we transform two expressions based on their syntax. Subsequently we combine these two conditioned on\n80\ntheir joint combinatory rule.\nAn important aspect of all of these models is the use of a non-linearity in the composition function. This non-linearity causes an interdependence between the model inputs. As we discussed in Chapter 4, there are a number of possibilities for modelling functionlike behaviour in vector composition, with matrix-vector- and tensor-composition being an obvious choice. However, as pointed out earlier, such tensor-based models would require an excessively large number of model parameters. The use of non-linearities can thus be seen as a cheaper alternative (in terms of model size) to tensor-composition for modelling functional dependence between words or larger linguistic units. See e.g. Bengio (2009) for a more detailed account of this. The interdependence caused by the non-linearity can be observed by considering the derivative of a non-linear function—here, the sigmoid function—compared with that of a linear function:\nlin(x, y) = W1x+W2y (5.2) ∂lin(x, y)\n∂x = W1 (5.3)\nnonlin(x, y) = sigmoid(x+ y) (5.4) ∂nonlin(x, y)\n∂x = sigmoid(x+ y) (1− sigmoid(x+ y)) (5.5)\nThe important difference here is that the gradient of the non-linear function with respect to x depends on y. In the linear case this interdependency does not exist. This gradient interdependence caused by the non-linear function thus enables a model to capture functional relationships between inputs beyond the modelling capabilities of a linear model."
    }, {
      "heading" : "5.4 Learning",
      "text" : "Here we briefly discuss unsupervised learning for our models. Subsequently we describe how these models can be extended to allow for semi-supervised training and evaluation.\nLet θ = (W ,B, L) be our model parameters and λ a vector with regularization parameters for all model parameters. W represents the set of all weight matrices, B the set of all biases and L the set of all word vectors. Let N be the set of training data consisting of tree-nodes tn with inputs xn, yn and reconstruction rn. The error at tn given θ is:\nE(tn|θ) = 1\n2 ∥∥∥rn − (xn‖yn)∥∥∥2 (5.6) 81\nThe gradient of the regularised objective function then becomes:\n∂J ∂θ = 1 N N∑ n ∂E(tn|θ) ∂θ + λθ (5.7)\nWe learn the gradient using backpropagation through structure (Goller and Küchler, 1996), and minimize the objective function using L-BFGS. See §4.5.1.1 for details on the backpropagation algorithm."
    }, {
      "heading" : "5.4.1 Supervised Learning",
      "text" : "The unsupervised method described so far learns a vector representation for each sentence. Such a representation can be useful for some tasks such as paraphrase detection, but is not sufficient for other tasks such as sentiment classification, which we are considering in this chapter.\nIn order to extract sentiment from our models, we extend them by adding a supervised classifier on top, using the learned representations v as input for a binary classification model:\npred(l=1|v, θ) = sigmoid(Wlabel v + blabel) (5.8)\nGiven our corpus of CCG parses with label pairs (N, l), the new objective function becomes:\nJ = 1\nN ∑ (N,l) E(N, l, θ) + λ 2 ||θ||2 (5.9)\nAssuming each node n ∈ N contains children xn, yn, encoding en and reconstruction rn, so that n = {x, y, e, r} this breaks down into:\nE(N, l, θ) = ∑ n∈N αErec (n, θ) + (1−α)Elbl(en, l, θ) (5.10) Erec(n, θ) = 1\n2 ∥∥∥[xn‖yn]− rn∥∥∥2 (5.11) Elbl(e, l, θ) = 1\n2 ‖l − e‖2 (5.12)\nThis is the combination of the two training signals described in §4.4.1.1 and §4.4.2.\n82"
    }, {
      "heading" : "5.5 Experiments",
      "text" : "Here, we describe the evaluations used to determine the performance of our model. First, we use two corpora for sentiment analysis, which allow us to compare model performance with a number of related approaches. Subsequently, we perform a small qualitative analysis of the model to get a better understanding of whether the combination of CCG parse structures and RAE can learn semantically expressive embeddings.\nIn our experiments we use the hyperbolic tangent as non-linearity g. Unless stated otherwise we use word-vectors of size 50, initialized using the embeddings provided by Turian et al. (2010) based on the model of Collobert and Weston (2008).1\nWe use the C&C parser (Clark and Curran, 2007) to generate CCG parse trees for the data used in our experiments. For models CCAE-C and CCAE-D we use the 25 most frequent CCG categories (as extracted from the British National Corpus) with an additional general weight matrix in order to catch all remaining types (Appendix C)."
    }, {
      "heading" : "5.5.1 Sentiment Analysis",
      "text" : "We evaluate our model on the MPQA opinion corpus (Wiebe et al., 2005), which annotates expressions for sentiment.2 The corpus consists of 10,624 instances with approximately 70 percent describing a negative sentiment. We apply the same pre-processing as Nakagawa et al. (2010) and Socher et al. (2011b) by using an additional sentiment lexicon (Wilson et al., 2005) during the model training for this experiment.\nAs a second corpus we make use of the sentence polarity (SP) dataset v1.0 (Pang and Lee, 2005).3 This dataset consists of 10662 sentences extracted from movie reviews which are manually labelled with positive or negative sentiment and equally distributed across sentiment.\nExperiment 1: Semi-Supervised Training In the first experiment, we use a semi-supervised training strategy combining an autoencoder signal with a classification signal such as described in Chapter 4. We initialise our models with the embeddings provided by Turian et\n1http://www.metaoptimize.com/projects/wordreprs/ 2http://mpqa.cs.pitt.edu/ 3http://www.cs.cornell.edu/people/pabo/movie-review-data/\n83\nal. (2010). The results of this evaluation are in Table 5.4. While we achieve the best performance on the MPQA corpus, the results on the SP corpus are less convincing. Perhaps surprisingly, the simplest model CCAE-A outperforms the other models on this dataset.\nWhen considering the two datasets, sparsity seems a likely explanation for this difference in results: In the MPQA experiment most instances are very short with an average length of 3 words, while the average sentence length in the SP corpus is 21 words. The MPQA task is further simplified through the use of an additional sentiment lexicon. Considering dictionary size, the SP corpus has a dictionary of 22k words, more than three times the size of the MPQA dictionary.\nThis issue of sparsity is exacerbated in the more complex CCAE models, where the training points are spread across different CCG types and rules. While the initialization of the word vectors with previously learned embeddings (as was previously shown by Socher et al. (2011b)) helps the models, all other model variables such as composition weights and biases are still initialised randomly and thus highly dependent on the amount of training data available.\nExperiment 2: Pretraining Due to our analysis of the results of the initial experiment, we ran a second series of experiments on the SP corpus. We follow Scheible and Schütze (2013) for this second series of experiments, which are carried out on a random 90/10 training-testing split, with some data reserved for development.\n84\nInstead of initialising the model with external word embeddings, we first train it on a large amount of data with the aim of overcoming the sparsity issues encountered in the previous experiment. Learning is thus divided into two steps:\nThe first, unsupervised training phase, uses the British National Corpus together with the SP corpus. In this phase only the reconstruction signal is used to learn word embeddings and transformation matrices. Subsequently, in the second phase, only the SP corpus is used, this time with both the reconstruction and the label error.\nBy learning word embeddings and composition matrices on more data, the model is likely to generalise better. Particularly for the more complex models, where the composition functions are conditioned on various CCG parameters, this should help to overcome issues of sparsity.\nIf we consider the results of the pre-trained experiments in Table 5.5, this seems to be the case. In fact, the trend of the previous results has been reversed, with the more complex models now performing best, whereas in the previous experiments the simpler models performed better. Using the Turian embeddings instead of random initialisation did not improve results in this setup."
    }, {
      "heading" : "5.5.2 Qualitative Analysis",
      "text" : "To get better insight into our models we also perform a small qualitative analysis. Using one of the models trained on the MPQA corpus, we generate word-level representations of all phrases in this corpus and subsequently identify the most related expressions by using the cosine distance measure. We perform this experiment on all expressions of length 5, considering all expressions with a word length between 3 and 7 as potential matches.\n85\nAs can be seen in Table 5.6, this works with varying success. Linking expressions such as conveying the message of peace and safeguard(ing) peace and security suggests that the model does learn some form of semantics.\nOn the other hand, the connection between expressed their satisfaction and support and expressed their admiration and surprise suggests that the pure word level content still has an impact on the model analysis. Likewise, the expressions is a story of success and is a staunch supporter have some lexical but little semantic overlap. Further reducing this link between the lexical and the semantic representation is an issue that should be addressed in future work in this area."
    }, {
      "heading" : "5.6 Discussion",
      "text" : "Overall, our models compare favourably with the state of the art. On the MPQA corpus model CCAE-D achieves the best published results we are aware of, whereas on the SP corpus we achieve competitive results. With an additional, unsupervised training step we achieved results beyond the current state of the art on this task, too.\nSemantics The qualitative analysis as well as the results on the sentiment analysis task suggest that the CCAE models are capable of learning semantics. An advantage of our approach—and of autoencoders generally—is their ability to learn in an unsupervised setting. The pre-training step for the sentiment task was essentially the same training step as\n86\nused for the qualitative analysis. While other models such as the MV-RNN (Socher et al., 2012a) achieve good results on a particular task, they do not allow unsupervised training. This prevents the possiblity of pretraining, which we showed to have a big impact on results, and further prevents the training of general models: The CCAE models can be used for multiple tasks without the need to re-train the main model.\nComplexity Previously in this thesis we argued that our models combined the strengths of other approaches. By using a grammar formalism we increase the expressive power of the model while the complexity remains low. For the complexity analysis see Table 5.7. We strike a balance between the greedy approaches (e.g. Socher et al. (2011b)), where learning is quadratic in the length of each sentence and existing syntax-driven approaches such as the MV-RNN of Socher et al. (2012a), where the size of the model, that is the number of variables that needs to be learned, is quadratic in the size of the word-embeddings.\nSparsity Parametrizing on CCG types and rules increases the size of the model compared to a greedy RAE (Socher et al., 2011b). The effect of this was highlighted by the sentiment analysis task, with the more complex models performing worse in comparison with the simpler ones. We were able to overcome this issue by using additional training data. Beyond this, it would also be interesting to investigate the relationships between different types and to derive functions to incorporate this into the learning procedure. For instance model learning could be adjusted to enforce some mirroring effects between the weight matrices of forward and backward application, or to support similarities between those of forward application and composition.\n87\nCCG-Vector Interface Exactly how the information contained in a CCG derivation is best applied to a vector space model of compositionality remains an open question: our investigation of this matter by exploring different model setups has proved somewhat inconclusive. While CCAE-D incorporated the deepest conditioning on the CCG structure, it did not decisively outperform the simpler CCAE-B which just conditioned on the combinatory operators. Compared with prior work on related models and the same experiments, however, demonstrated that any conditioning is better than none. As pointed out above, sparsity—which we found to have a big impact in our experiments on pre-training— favours simpler models with less model parameters. By extension the results suggest that more complex conditioning on the CCG parse is beneficial, but only if sufficient data is available to appropriately train such a model.\nAs discussed in §5.3, we approximated the functional aspect of CCG composition using non-linearities in our model. The relative performance of our models compared with the prior state of the art suggests that this approximation works. However, an interesting avenue of future research would be to improve this approximation using low-rank tensor factorisations or similar approaches together with composition functions such as suggested by Clark (2013), Grefenstette (2013b) or Maillard et al. (2014)."
    }, {
      "heading" : "5.7 Summary",
      "text" : "In this chapter we have brought a more formal notion of semantic compositionality to vector space models based on recursive autoencoders. This was achieved through the use of the CCG formalism to provide a conditioning structure for the matrix vector products that define the RAE.\nWe have explored a number of models, each of which conditions the compositional operations on different aspects of the CCG derivation. Our experimental findings indicate a clear advantage for a deeper integration of syntax over models that use only the bracketing structure of the parse tree. Further, we demonstrated that the functional aspect of CCG composition can be approximated in a cheap yet sensible fashion through the use of non-linearities extending a fully-additive composition model such as the ones discussed in Chapter 4.\nThe most effective way to condition the compositional operators on the syntax remains unclear. Once the issue of sparsity had been addressed, the complex models outperformed\n88\nthe simpler ones. Among the complex models, however, we could not establish significant or consistent differences to convincingly argue for a particular approach. This uncertainty could also be linked to sparsity, considering the relatively small amounts of training data used in our experiments.\nWhile the connections between formal linguistics and vector space approaches to NLP may not be immediately obvious, we believe that there is a case for the continued investigation of ways to best combine these two schools of thought. The approach presented here is one step towards the reconciliation of traditional formal approaches to compositional semantics with modern machine learning.\nThe system proposed in this chapter relies heavily on grammatical annotation (here, CCG parses), both for learning distributed word representations as well as for learning a composition model based on these word level representations. Our results clearly indicate that this is a suitable mechanism when applied to the English language and when considering a specific task such as sentiment analysis where data is available for supervised training. However, two questions remain. First, it is unclear how this approach can be extended to less resource fortunate languages, and second, it is difficult to see how such a semi-supervised training setup can be extended to learn more generic semantic representations rather than representations primed for sentiment analysis or a similar such task.\nWith these questions in mind, we next investigate unsupervised approaches for representation learning. We focus on such models with three objectives in mind. First, we want to investigate approaches that can easily be applied to a multitude of languages, which effectively forces us to abandon the use of syntax-driven composition and learning functions. Second, words from these multiple languages should be projected into joint-space embeddings, thereby enabling us to use the representations independent of their respective source languages. Finally, we specifically want to design a representation learning method that results in semantically grounded representations, similar to e.g. representations learned based on the distributional hypothesis (see §2.3) in a monolingual scenario. The outcome of this investigation is presented in Chapter 6.\n89\nChapter 6\nMultilingual Approaches for Learning Semantics\nChapter Abstract\nThis chapter continues our investigation into both compositional and word level semantic representations. Building on the conclusions drawn in Chapters 3 and 5, we devise an extension to the distributional hypothesis §2.3 for multilingual data and joint-space embeddings. The models presented in this chapter leverage parallel data and learn to strongly align the embeddings of semantically equivalent sentences, while maintaining sufficient distance between those of dissimilar sentences. The models do not rely on word alignments or any syntactic information and are successfully applied to a number of diverse languages. Instead of employing word alignment we achieve semantic transfer across languages using compositional representations at the sentence level. We extend our approach to learn semantic representations at the document level, too. We evaluate these models on two cross-lingual document classification tasks, outperforming the prior state of the art. Through qualitative analysis and the study of pivoting effects we demonstrate that our representations are semantically plausible and can capture semantic relationships across languages without parallel data.\nThe material in this chapter was originally presented in Hermann and Blunsom (2014a) and Hermann and Blunsom (2014b).\n90"
    }, {
      "heading" : "6.1 Introduction",
      "text" : "As we have established in this thesis so far, distributed representations provide a highly suitable mechanism for encoding semantics both at the word level and at higher levels, such as demonstrated with the sentiment analysis task in Chapter 5. That said, as pointed out both in the previous chapter as well as in the analysis of compositional semantics learning signals in Chapter 4, it is difficult to distinguish between semantic and task-specific representations when considering a task such as sentiment analysis.\nIn order to remove such task-specific biases from the representation learning process, a different approach is required. Within a monolingual context, the distributional hypothesis (Firth, 1957) forms the basis of most approaches for learning word representations. This hypothesis is attractive because it offers an approach to learn distributed representations independent of a particular task or signal and therefore carries the promise of learning task-independent semantic representations.\nHere we extend this distributional hypothesis to multilingual data and joint-space embeddings. We present a novel unsupervised technique for learning semantic representations that leverages parallel corpora and employs semantic transfer through compositional representations. Unlike most methods for learning word representations, which are restricted to a single language, our approach learns to represent meaning across languages in a shared multilingual semantic space. Furthermore, by employing a multilingual variation of the distributional hypothesis, we learn representations that are not only multilingual, but also task-independent and semantically grounded to a degree not available to most other approaches for learning distributed representations.\nTo show the efficacy of our model, we present experiments on two corpora. First, we show that for cross-lingual document classification on the Reuters RCV1/RCV2 corpora (Lewis et al., 2004), we outperform the prior state of the art (Klementiev et al., 2012). Second, we also present classification results on a massively multilingual corpus which we derive from the TED corpus (Cettolo et al., 2012). The results on this task, in comparison with a number of strong baselines, further demonstrate the relevance of our approach and the success of our method in learning multilingual semantic representations over a wide range of languages.\n91"
    }, {
      "heading" : "6.2 Overview",
      "text" : "Distributed representation learning describes the task of learning continuous representations for discrete objects. Here, we focus on learning semantic representations and investigate how the use of multilingual data can improve learning such representations at the word and higher level. We present a model that learns to represent each word in a lexicon by a continuous vector in Rd. Such distributed representations allow a model to share meaning between similar words, and have been used to capture semantic, syntactic and morphological content (Collobert and Weston, 2008; Turian et al., 2010, inter alia).\nWe describe a multilingual objective function that uses a noise-contrastive update between semantic representations of different languages to learn these word embeddings. As part of this, we use a compositional vector model (CVM, henceforth) to compute semantic representations of sentences and documents. A CVM learns semantic representations of larger syntactic units given the semantic representations of their constituents (Clark and Pulman, 2007; Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012a; Hermann and Blunsom, 2013, inter alia).\nA key difference between our approach and those listed above is that we only require sentence-aligned parallel data in our otherwise unsupervised learning function. This removes a number of constraints that normally come with CVM models, such as the need for syntactic parse trees, word alignment or annotated data as a training signal. At the same time, by using multiple CVMs to transfer information between languages, we enable our models to capture a broader semantic context than would otherwise be possible.\nThe idea of extracting semantics from multilingual data stems from prior work in the field of semantic grounding. Language acquisition in humans is widely seen as grounded in sensory-motor experience (Bloom, 2001; Roy, 2003). Based on this idea, there have been some attempts at using multi-modal data for learning better vector representations of words (e.g. Srivastava and Salakhutdinov (2012)). Such methods, however, are not easily scalable across languages or to large amounts of data for which no secondary or tertiary representation might exist.\nParallel data in multiple languages provides an alternative to such secondary representations, as parallel texts share their semantics, and thus one language can be used to ground the other. Some work has exploited this idea for transferring linguistic knowledge into lowresource languages or to learn distributed representations at the word level (Klementiev et\n92\nal., 2012; Zou et al., 2013; Lauly et al., 2013, inter alia). So far almost all of this work has been focused on learning multilingual representations at the word level. As distributed representations of larger expressions have been shown to be highly useful for a number of tasks, it seems to be a natural next step to attempt to induce these, too, cross-lingually."
    }, {
      "heading" : "6.3 Approach",
      "text" : "Most prior work on learning compositional semantic representations employs parse trees on their training data to structure their composition functions (Socher et al., 2012a; Hermann and Blunsom, 2013, inter alia). Further, these approaches typically depend on specific semantic signals such as sentiment- or topic-labels for their objective functions. While these methods have been shown to work in some cases, the need for parse trees and annotated data limits such approaches to resource-fortunate languages. Our novel method for learning compositional vectors removes these requirements, and as such can more easily be applied to low-resource languages.\nSpecifically, we attempt to learn semantics from multilingual data. The idea is that, given enough parallel data, a shared representation of two parallel sentences would be forced to capture the common elements between these two sentences. What parallel sentences share, of course, are their semantics. Naturally, different languages express meaning in different ways. We utilise this diversity to abstract further from mono-lingual surface realisations to deeper semantic representations. We exploit this semantic similarity across languages by defining a bilingual (and trivially multilingual) energy as follows.\nAssume two functions f : X → Rd and g : Y → Rd, which map sentences from languages x and y onto distributed semantic representations in Rd (see Figure 6.1). Given a parallel corpus C, we then define the energy of the model given two sentences (a, b) ∈ C as:\nEbi(a, b) = ‖f(a)− g(b)‖2 (6.1)\nWe want to minimize Ebi for all semantically equivalent sentences in the corpus. In order to prevent the model from degenerating, we further introduce a noise-constrastive largemargin update which ensures that the representations of non-aligned sentences observe a certain margin from each other. For every pair of parallel sentences (a, b) we sample a number of additional sentence pairs (·, n) ∈ C, where n—with high probability—is not\n93\nsemantically equivalent to a. We use these noise samples as follows:\nEhl(a, b, n) = [m+ Ebi(a, b)− Ebi(a, n)]+\nwhere [x]+ = max(x, 0) denotes the standard hinge loss and m is the margin. This results in the following objective function:\nJ(θ) = ∑\n(a,b)∈C ( k∑ i=1 Ehl(a, b, ni) + λ 2 ‖θ‖2 ) (6.2)\nwhere θ is the set of all model variables."
    }, {
      "heading" : "6.3.1 Two Composition Models",
      "text" : "The objective function in Equation 6.2 could be coupled with any two given vector composition functions f, g from the literature. As we aim to apply our approach to a wide range of languages, we focus on composition functions that do not require any syntactic information. We evaluate the following two composition functions.\nThe first model, ADD, represents a sentence by the sum of its word vectors. This is a distributed bag-of-words approach as sentence ordering is not taken into account by the model.\n94\nSecond, the BI model is designed to capture bigram information, using a non-linearity\nover bigram pairs in its composition function:\nf(x) = n∑ i=1 tanh (xi−1 + xi) (6.3)\nThe use of a non-linearity enables the model to learn interesting interactions between words in a document, which the bag-of-words approach of ADD is not capable of learning. We use the hyperbolic tangent as activation function. See the discussion in Chapter 5 (at the end of §5.3) for more detail on the motivation for non-linearities in such models."
    }, {
      "heading" : "6.3.2 Document-level Semantics",
      "text" : "For a number of tasks, such as topic modelling, representations of objects beyond the sentence level are required. While most approaches to compositional distributed semantics end at the sentence level, our model extends to document-level learning quite naturally, by recursively applying the composition and objective function (Equation 6.2) to compose sentences into documents. This is achieved by first computing semantic representations for\n95\neach sentence in a document. Next, these representations are used as inputs in a higher-level CVM, computing a semantic representation of a document (Figure 6.2).\nThis recursive approach integrates document-level representations into the learning process. We can thus use corpora of parallel documents—regardless of whether they are sentence aligned or not—to propagate a semantic signal back to the individual words. If sentence alignment is available, of course, the document-signal can simply be combined with the sentence-signal, as we did with the experiments described in §6.5.3. This concept of learning compositional representations for documents contrasts with prior work (Socher et al., 2011b; Klementiev et al., 2012, inter alia) who rely on summing or averaging sentence-vectors if representations beyond the sentence-level are required for a particular task.\nWe evaluate the models presented in this chapter both with and without the documentlevel signal. We refer to the individual models used as ADD and BI if used without, and as DOC/ADD and DOC/BI is used with the additional document composition function and error signal."
    }, {
      "heading" : "6.4 Corpora",
      "text" : "We use two corpora for learning semantic representations and performing the experiments described in the following section.\nThe Europarl corpus v71 (Koehn, 2005) was used during initial development and testing of our approach, as well as to learn the representations used for the Cross-Lingual Document Classification task described in §6.5.2. We considered the English-German and English-French language pairs from this corpus. From each pair the final 100,000 sentences were reserved for development.\nSecond, we developed a massively multilingual corpus based on the TED corpus2 for IWSLT 2013 (Cettolo et al., 2012). This corpus contains English transcriptions and multilingual, sentence-aligned translations of talks from the TED conference. While the corpus is aimed at machine translation tasks, we use the keywords associated with each talk to build a subsidiary corpus for multilingual document classification as follows.3\n1http://www.statmt.org/europarl/ 2https://wit3.fbk.eu/ 3http://www.clg.ox.ac.uk/tedcldc/\n96\nThe development sections provided with the IWSLT 2013 corpus were again reserved for development. We removed approximately 10 percent of the training data in each language to create a test corpus (all talks with id ≥ 1,400). The new training corpus consists of a total of 12,078 parallel documents distributed across 12 language pairs4. In total, this amounts to 1,678,219 non-English sentences (the number of unique English sentences is smaller as many documents are translated into multiple languages and thus appear repeatedly in the corpus). Each document (talk) contains one or several keywords. We used the 15 most frequent keywords for the topic classification experiments described in section §6.5.3. Both corpora were pre-processed using the set of tools provided by cdec5 for tokenizing and lowercasing the data. Further, all empty sentences and their translations were removed from the corpus."
    }, {
      "heading" : "6.5 Experiments",
      "text" : "We report results on two experiments. First, we replicate the cross-lingual document classification task of Klementiev et al. (2012), learning distributed representations on the Europarl corpus and evaluating on documents from the Reuters RCV1/RCV2 corpora. Subsequently, we design a multi-label classification task using the TED corpus, both for training and evaluating. The use of a wider range of languages in the second experiments allows us to better evaluate our models’ capabilities in learning a shared multilingual semantic representation. We also investigate the learned embeddings from a qualitative perspective in §6.5.4."
    }, {
      "heading" : "6.5.1 Learning",
      "text" : "All model weights were randomly initialised using a Gaussian distribution (µ=0, σ2=0.1). We used the available development data to set our model parameters. For each positive sample we used a number of noise samples (k ∈ {1, 10, 50}), randomly drawn from the corpus at each training epoch. All our embeddings have dimensionality d=128, with the margin\n4English to Arabic, German, French, Spanish, Italian, Dutch, Polish, Brazilian Portuguese, Romanian, Russian and Turkish. Chinese, Farsi and Slovenian were removed due to the small size of those datasets.\n5http://cdec-decoder.org/\n97\nset to m=d.6 Further, we use L2 regularization with λ=1 and step-size in {0.01, 0.05}. We use 100 iterations for the RCV task, 500 for the TED single and 5 for the joint corpora. We use the adaptive gradient method, AdaGrad (Duchi et al., 2011), for updating the weights of our models, in a mini-batch setting (b ∈ {10, 50})."
    }, {
      "heading" : "6.5.2 RCV1/RCV2 Document Classification",
      "text" : "We evaluate our models on the cross-lingual document classification (CLDC, henceforth) task first described in Klementiev et al. (2012). This task involves learning language independent embeddings which are then used for document classification across the EnglishGerman language pair. For this, CLDC employs a particular kind of supervision, namely using supervised training data in one language and evaluating without further supervision in another. Thus, CLDC can be used to establish whether our learned representations are semantically useful across multiple languages.\nWe follow the experimental setup described in Klementiev et al. (2012), with the exception that we learn our embeddings using solely the Europarl data and use the Reuters corpora only during for classifier training and testing. Each document in the classification task is represented by the average of the d-dimensional representations of all its sentences. We train the multiclass classifier using an averaged perceptron (Collins, 2002) with the same settings as in Klementiev et al. (2012).\nWe present results from four models. The ADD model is trained on 500k sentence pairs of the English-German parallel section of the Europarl corpus. The ADD+ model uses an additional 500k parallel sentences from the English-French corpus, resulting in one million English sentences, each paired up with either a German or a French sentence, with BI and BI+ trained accordingly. The motivation behind ADD+ and BI+ is to investigate whether we can learn better embeddings by introducing additional data from other languages. A similar idea exists in machine translation where English is frequently used to pivot between other languages (Cohn and Lapata, 2007).\nThe actual CLDC experiments are performed by training on English and testing on German documents and vice versa. Following prior work, we use varying sizes between 100 and 10,000 documents when training the multiclass classifier. The results of this task across training sizes are in Figure 6.3. Table 6.1 shows the results for training on 1,000\n6On the RCV task we also report results for d=40 which matches the dimensionality of Klementiev et al. (2012).\n98\ndocuments compared with the results published in Klementiev et al. (2012). Our models outperform the prior state of the art, with the BI models performing slightly better than the ADD models. As the relative results indicate, the addition of a second language improves model performance. It it interesting to note that results improve in both directions of the task, even though no additional German data was used for the ‘+‘ models."
    }, {
      "heading" : "6.5.3 TED Corpus Experiments",
      "text" : "Here we describe our experiments on the TED corpus, which enables us to scale up to multilingual learning. Consisting of a large number of relatively short and parallel documents, this corpus allows us to evaluate the performance of the DOC model described in §6.3.2. We use the training data of the corpus to learn distributed representations across 12 languages. Training is performed in two settings. In the single mode, vectors are learnt from a single language pair (en-X), while in the joint mode vector-learning is performed on all parallel sub-corpora simultaneously. This setting causes words from all languages to be embedded in a single semantic space.\n99\nFirst, we evaluate the effect of the document-level error signal (DOC, described in §6.3.2), as well as whether our multilingual learning method can extend to a larger variety of languages. We train DOC models, using both ADD and BI as CVM (DOC/ADD, DOC/BI), both in the single and joint mode. For comparison, we also train ADD and DOC models without the document-level error signal. The resulting document-level representations are used to train classifiers (system and settings as in §6.5.2) for each language, which are then evaluated in the paired language. In the English case we train twelve individual classifiers, each using the training data of a single language pair only. As described in §6.4, we use 15 keywords for the classification task. We report cumulative results in the form of F1-scores, which are more insightful than individual scores on a keyword by keyword basis.\nMT System We develop a machine translation baseline as follows. We train a machine translation tool on the parallel training data, using the development data of each language pair to optimize the translation system. We use the cdec decoder (Dyer et al., 2010) with default settings for this purpose. With this system we translate the test data, and then use a Naı̈ve Bayes classifier7 for the actual experiments. To exemplify, this means the de→ar result is produced by training a translation system from Arabic to German. The Arabic test\n7We use the implementation in Mallet (McCallum, 2002)\n100\nset is translated into German. A classifier is then trained on the German training data and evaluated on the translated Arabic. While we developed this system as a baseline, it must be noted that the classifier of this system has access to significantly more information (all words in the document) as opposed to our models (one embedding per document), and we do not expect to necessarily beat this system.\nThe results of this experiment are in Table 6.2. When comparing the results between the ADD model and the models trained using the document-level error signal, the benefit of this additional signal becomes clear. The joint training mode leads to a relative improvement when training on English data and evaluating in a second language. This suggests that the joint mode improves the quality of the English embeddings more than it affects the L2embeddings. More surprising, perhaps, is the relative performance between the ADD and BI composition functions, especially when compared to the results in §6.5.2, where the BI models relatively consistently performed better. We suspect that the better performance of the additive composition function on this task is related to the smaller amount of training data available which could cause sparsity issues for the bigram model.\nAs expected, the MT system slightly outperforms our models on most language pairs. However, the overall performance of the models is comparable to that of the MT sys-\n101\ntem. Considering the relative amount of information available during the classifier training phase, this indicates that our learned representations are semantically useful, capturing almost the same amount of information as available to the Naı̈ve Bayes classifier.\nWe next investigate linguistic transfer across languages. We re-use the embeddings learned with the DOC/ADD joint model from the previous experiment for this purpose, and train classifiers on all non-English languages using those embeddings. Subsequently, we evaluate their performance in classifying documents in the remaining languages. Results for this task are in Table 6.3. While the results across language-pairs might not be very insightful, the overall good performance compared with the results in Table 6.2 implies that we learnt semantically meaningful vectors and in fact a joint embedding space across thirteen languages.\nIn a third evaluation (Table 6.4), we apply the embeddings learnt with out models to a monolingual classification task, enabling us to compare with prior work on distributed representation learning. In this experiment a classifier is trained in one language and then evaluated in the same. We again use a Naı̈ve Bayes classifier on the raw data to establish a reasonable upper bound.\nWe compare our embeddings with the SENNA embeddings, which achieve state of the art performance on a number of tasks (Collobert et al., 2011). Additionally, we use the Polyglot embeddings of Al-Rfou’ et al. (2013), who published word embeddings across 100\n102\nlanguages, including all languages considered in our work. We represent each document by the mean of its word vectors and then apply the same classifier training and testing regime as with our models. Even though both of these sets of embeddings were trained on much larger datasets than ours, our models outperform these baselines on all languages—even outperforming the Naı̈ve Bayes system on several languages. While this may partly be attributed to the fact that our vectors were learned on in-domain data, this is still a very positive outcome."
    }, {
      "heading" : "6.5.4 Linguistic Analysis",
      "text" : "While the classification experiments focused on establishing the semantic content of the sentence level representations, we also want to briefly investigate the induced word embeddings. We use the BI+ model trained on the Europarl corpus for this purpose. Figure 6.4 shows the t-SNE projections for a number of English, French and German words. Even though the model did not use any parallel French-German data during training, it still managed to learn semantic word-word similarity across these two languages.\nGoing one step further, Figure 6.5 shows t-SNE projections for a number of short phrases in these three languages. We use the English the president and gender-specific expressions Mr President and Madam President as well as gender-specific equivalents in French and German. The projection demonstrates a number of interesting results: First, the model correctly clusters the words into three groups, corresponding to the three En-\n103\nglish forms and their associated translations. Second, a separation between genders can be observed, with male forms on the bottom half of the chart and female forms on the top, with the neutral the president in the vertical middle. Finally, if we assume a horizontal line going through the president, this line could be interpreted as a “gender divide”, with male and female versions of one expression mirroring each other on that line. In the case of the president and its translations, this effect becomes even clearer, with the neutral English expression being projected close to the mid-point between each other language’s gender-specific versions.\nThese results further support our hypothesis that the bilingual contrastive error function can learn semantically plausible embeddings and furthermore, that it can abstract away from mono-lingual surface realisations into a shared semantic space across languages."
    }, {
      "heading" : "6.6 Related Work",
      "text" : "As introduced in Chapters 2 and 4, most research on distributed representation induction has focused on single languages. English, with its large number of annotated resources, has enjoyed most attention. However, there exists a body of prior work on learning multilingual embeddings or on using parallel data to transfer linguistic information across languages.\n104\nOne has to differentiate between approaches such as Al-Rfou’ et al. (2013), that learn embeddings across a large variety of languages, and models such as ours, that learn joint embeddings, that is a projection into a shared semantic space across multiple languages.\nRelated to our work, Yih et al. (2011) proposed S2Nets to learn joint embeddings of tf-idf vectors for comparable documents. Their architecture optimises the cosine similarity of documents, using relative semantic similarity scores during learning. More recently, Lauly et al. (2013) proposed a bag-of-words autoencoder model, where the bag-of-words representation in one language is used to train the embeddings in another. By placing their vocabulary in a binary branching tree, the probabilistic setup of this model is similar to that of Mnih and Hinton (2009). Similarly, Sarath Chandar et al. (2013) train a cross-lingual encoder, where an autoencoder is used to recreate words in two languages in parallel. This is effectively the linguistic extension of Ngiam et al. (2011), who used a similar method for audio and video data.\nKlementiev et al. (2012), our baseline in §6.5.2, use a form of multi-agent learning on word-aligned parallel data to transfer embeddings from one language to another. Earlier work, Haghighi et al. (2008), proposed a method for inducing bilingual lexica using monolingual feature representations and a small initial lexicon to bootstrap with. This approach has recently been extended by Mikolov et al. (2013a), Mikolov et al. (2013b), who de-\n105\nveloped a method for learning transformation matrices to convert semantic vectors of one language into those of another. Is was demonstrated that this approach can be applied to improve tasks related to machine translation. Their CBOW model is also worth noting for its similarities to the ADD composition function used here. Using a slightly different approach, Zou et al. (2013), also learned bilingual embeddings for machine translation."
    }, {
      "heading" : "6.7 Summary",
      "text" : "In this chapter we have presented a novel method for learning multilingual word embeddings using parallel data in conjunction with a multilingual objective function for compositional vector models. This approach extends the distributional hypothesis to multilingual joint-space representations. Coupled with very simple composition functions, vectors learned with this method outperform the state of the art on the task of cross-lingual document classification. Further experiments and analysis support our hypothesis that bilingual signals are a useful tool for learning distributed representations by enabling models to abstract away from mono-lingual surface realisations into a deeper semantic space.\nAfter extending our approach to include multilingual training data, we were able to demonstrate that adding additional languages further improves the model. Furthermore, using some qualitative experiments and visualizations, we showed that our approach also allows us to learn semantically related embeddings across languages without any direct training data.\nOverall, the work presented in this chapter adds further support to the hypothesis described in the introduction to this thesis. In conjunction with the work presented in Chapter 5, we demonstrated that distributed semantic representations are useful beyond the word level and have proposed multiple approaches for learning such representations. The multilingual models discussed in this chapter are of particular interest, as they highlight the possibility for learning semantically plausible representations both at the word and sentence level while at the same time minimising the impact of language or syntax specific biases.\n106\nPart III\nConclusions and Further Work\n107\nChapter 7\nFurther Research\nThere are many avenues for future work related to the research presented in this thesis, both related to the application of distributed representations to new tasks and to the further development of models for semantic composition. These research opportunities include:\n1. In this thesis we have demonstrated the efficacy of distributed representations in solv-\ning semantically ambitious tasks such as semantic frame identification. Within the frame-semantic parsing pipeline, an obvious extension would be to attempt to also solve the semantic role labelling task using distributed information.\n2. Another extension to the experimental support of our thesis in Chapter 3 is to at-\ntempt to integrate this method with the fully compositional approaches presented in the subsequent chapters of this thesis—that is to use a more complex syntactic composition function to learn the context instance representations required for the frame identification stage.\n3. In Chapter 5 we investigated a number of composition models that exploit the CCG\nframework to guide their composition steps. While we relied on vector-based representations in this approach and solely used non-linearities to enforce interdependence between arguments, there are alternative proposals that could be evaluated in this direction. A direction of particular interest here are tensor-models, where the categorial type of a linguistic unit would guide its shape. While there are some issues with the scalability and complexity of such models (see §4.3.3), recent work has made some advances in this direction by proposing approximations that might overcome these issues (Grefenstette, 2013b; Clark, 2013; Maillard et al., 2014).\n108\n4. The approach presented in Chapter 6 for learning semantically motivated multilin-\ngual distributed representations also invites further research. Several avenues of future work are available here. One possibility is to attempt to reconcile this multilingual objective function with more complex composition functions. While some initial research into this question has not borne any fruit, this deserves further attention. A likely explanation for the current lack of success regarding this problem is in the limited amount of training data available, and thus this problem should be revisited when we have access to larger parallel corpora.\n5. Another, related idea is to attempt to inverse the process described in most of the\nmodels presented in this thesis. This thesis focuses on learning representations given words. Similarly to the work in language modelling (Mikolov et al., 2010; Mnih and Hinton, 2009, inter alia), it would be possible to imagine reversing the process and to generate words from compositional representations. Initial work in this direction was discussed in (Kalchbrenner and Blunsom, 2013; Blunsom et al., 2013)\n109\nChapter 8\nConclusions\nThroughout this thesis we have analysed the construction, the use, capabilities and limitations and other properties of distributed representations for semantics. The primary aim of this thesis was to investigate the use of distributed representations for capturing semantics, and to evaluate their efficacy in solving tasks in NLP for which a degree of semantic understanding could be beneficial. At the outset in Chapter 1 we stated the underlying hypothesis of this thesis.\nOur hypothesis is that distributed representations are a highly suitable mechanism for capturing and manipulating semantics, and further, that meaning both at the word level and beyond can be encoded distributionally. (§1.1)\nThe analysis and empirical evidence collected throughout this thesis strongly supports our original hypothesis. Considering the first aspect on whether distributed representations are suitable for capturing and manipulating semantic information, we showed in Chapter 3 that generic distributional representations can be used to outperform the state of the art on semantically challenging tasks such as semantic frame identification. Throughout the following chapters we repeatedly introduced simple models that learn or exploit distributed representations and that with high consistency outperformed more complex models reliant on symbolic reasoning or purely frequency-based or syntactic methods.\nConcerning the second aspect of our hypothesis, namely that distributed representations are not restricted to encoding semantics at the word level, we also evaluated and supported this claim through thorough investigation, particularly in the second part of this thesis. In Chapter 4 we provided an overview of various theoretical foundations for the composition of distributed representations into higher order structures. Subsequently in Chapter 5 we\n110\ninvestigated the role of syntax in semantic vector composition. Here we supported our hypothesis by highlighting prior work on learning compositional semantic representations and further by developing several new composition models that learned semantically plausible embeddings for sentences as highlighted by their performance on various sentiment analysis tasks. Not content purely with the analysis of semantic representations learned through task-specific objective functions, we investigated how to learn semantic representations with task-independent distributions. The multilingual objective function developed in Chapter 6 removes task-specific biases and minimises the impact of monolingual surface forms on learning distributed representations.\nCombining the conclusions drawn in the earlier chapters of this thesis, the model developed in Chapter 6 unifies both aspects of our hypothesis into a single model. The models learn representations both for words and for sentences. Furthermore, these representations are learned using a multilingual extension of Firth’s distributional hypothesis, which we argued should result in convincing semantic representations. Put to the test on multiple cross-lingual document classification tasks our models—again simpler compared with a number of our baseline models—outperformed the prior state of the art once more.\nOn the basis of the analysis and the experimental support provided in this thesis, we argue that the work presented here strongly supports the hypothesis we set out to investigate.\nLooking forward, the work presented in this thesis opens up new questions. Having established the usefulness of distributed representations for capturing semantics, such new questions are how far this approach can be pushed, what limitations there are to the use of distributed representations, and what problems can or cannot be solved with this mechanism. So far most work on distributed semantic representations focuses on learning representations given natural language input, as well as various classification and ranking tasks based on these representations. One of the big challenges in the future will be to reverse this process, that is to develop algorithms for generating natural language given composed distributed representations. Work on neural language modelling is only a first step in that direction. Finding efficient and effective models for this task is a key requirement for the application of distributed representations to complex NLP tasks such as question answering and machine translation. When considering such tasks, additional questions come to mind that require further research. One such question is whether distributed representations are sufficiently expressive to encode complex meaning as required for logical or semantic\n111\ninference. A related question is whether distributed representations will suffice on their own, or whether dual approaches that combine distributed and symbolic representations of meaning will be more suitable for such tasks. There has been some initial work on these questions, but this will remain an exciting area of research for many years to come.\n112"
    }, {
      "heading" : "Appendix A",
      "text" : "Semantic-Frame Parsing: Argument Identification\nWe briefly describe the argument identification model used in our frame-semantic parsing experiments in Chapter 3. This model is used after the frame identification step to find arguments to fill the semantic roles of a given frame. The model is based on existing work in this field (Xue and Palmer, 2004; Das et al., 2010) and contains only a small number of modifications, as its primary purpose is to allow us to evaluate our frame identification model in the context of a full-scale frame-semantic parsing task.\nThe implementation of the argument identification system, as well as the modifications explained here and later in §3.5.4.2 are the work of my co-authors in Hermann et al. (2014). They are included in this thesis as they are essential for understanding the experiments discussed in this chapter, but should not be considered part of the contribution of this thesis.\nGiven x, the sentence with a marked predicate, the argument identification model assumes that the predicate frame y has been disambiguated. From a frame lexicon, we look up the set of semantic rolesRy that associate with y. This set also contains the null role r∅. From x, a rule-based candidate argument extraction algorithm then extracts a set of spans A that could potentially serve as the overt arguments Ay for y. By overtness, we mean the non-null instantiation of a semantic role in a frame-semantic parse. Details of the specific candidate argument extraction algorithms used vary a little between the FrameNet and the PropBank case, and are specified in §3.5.4-§3.5.5.\n113\nA.1 Learning and Inference\nGiven training data of the form 〈〈x(i), y(i),M(i)〉〉Ni=1, where,\nM = {(r, a) : r ∈ Ry, a ∈ A ∪Ay}, (A.1)\nis a set of tuples that associates each role r in Ry with a span a according to the gold data. Note that this mapping associates spans with the null role r∅ as well. We optimize the following log-likelihood to train our model:\nmax θ N∑ i=1 |M(i)|∑ j=1 log pθ ( (r, a)j|x, y,Ry ) − C‖θ‖22\nwhere pθ is a log-linear model normalized over the set Ry, with the features described in Table A.1, and the following resultant formulation:\npθ ( (r, a)|x, y,Ry ) =\nexpθ · h(r, a, y, x)∑ r̄∈Ry expθ · h(r̄, a, y, x) .\n114\nAbove, θ are the model parameters and h is a feature function that uses the features from Table A.1; we describe the nature of the discrete word clusters that are used in the feature set in §3.5.3. We train this model using L-BFGS (Liu and Nocedal, 1989) and set C to 1.0. Although our learning mechanism uses a local log-linear model, we perform inference globally on a per-frame basis by applying hard structural constraints. Following Das et al. (2014) and Punyakanok et al. (2008) we use the log-probability of the local classifiers as a score in an integer linear program (ILP) to assign roles subject to hard constraints described in §3.5.4 and §3.5.5. We use an off-the-shelf ILP solver for inference.1\n1http://scip.zib.de/\n115"
    }, {
      "heading" : "Appendix B",
      "text" : "FrameNet Development Data\nTable B.1 features a list of the 16 randomly selected documents from the FrameNet 1.5 corpus, which we used for development in the frame-semantic parsing task described in Chapter 3. The resultant development set consists of roughly 4,500 predicates. We used the same test set as in Das et al. (2014), containing 23 documents and 4,458 predicates.\n116"
    }, {
      "heading" : "Appendix C",
      "text" : "CCG Categories for CCAE Models"
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2014,
    "abstractText" : "The mathematical representation of semantics is a key issue for Natural Language Processing (NLP). A lot of research has been devoted to finding ways of representing the semantics of individual words in vector spaces. Distributional approaches—meaning distributed representations that exploit co-occurrence statistics of large corpora—have proved popular and successful across a number of tasks. However, natural language usually comes in structures beyond the word level, with meaning arising not only from the individual words but also the structure they are contained in at the phrasal or sentential level. Modelling the compositional process by which the meaning of an utterance arises from the meaning of its parts is an equally fundamental task of NLP. This dissertation explores methods for learning distributed semantic representations and models for composing these into representations for larger linguistic units. Our underlying hypothesis is that neural models are a suitable vehicle for learning semantically rich representations and that such representations in turn are suitable vehicles for solving important tasks in natural language processing. The contribution of this thesis is a thorough evaluation of our hypothesis, as part of which we introduce several new approaches to representation learning and compositional semantics, as well as multiple state-of-the-art models which apply distributed semantic representations to various tasks in NLP. Part I focuses on distributed representations and their application. In particular, in Chapter 3 we explore the semantic usefulness of distributed representations by evaluating their use in the task of semantic frame identification. Part II describes the transition from semantic representations for words to compositional semantics. Chapter 4 covers the relevant literature in this field. Following this, Chapter 5 investigates the role of syntax in semantic composition. For this, we discuss a series of neural network-based models and learning mechanisms, and demonstrate how syntactic information can be incorporated into semantic composition. This study allows us to establish the effectiveness of syntactic information as a guiding parameter for semantic composition, and answer questions about the link between syntax and semantics. Following these discoveries regarding the role of syntax, Chapter 6 investigates whether it is possible to further reduce the impact of monolingual surface forms and syntax when attempting to capture semantics. Asking how machines can best approximate human signals of semantics, we propose multilingual information as one method for grounding semantics, and develop an extension to the distributional hypothesis for multilingual representations. Finally, Part III summarizes our findings and discusses future work.",
    "creator" : "LaTeX with hyperref package"
  }
}