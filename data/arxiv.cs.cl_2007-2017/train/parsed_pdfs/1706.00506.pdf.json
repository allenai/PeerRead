{
  "name" : "1706.00506.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Morphological Embeddings for Named Entity Recognition in Morphologically Rich Languages",
    "authors" : [ "Onur Güngör", "Eray Yıldız" ],
    "emails" : [ "onurgu@boun.edu.tr", "suzan.uskudarli@boun.edu.tr", "eray.yildiz@huawei.com", "gungort@boun.edu.tr" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Named Entity Recognition (NER) is an important task in Natural Language Processing (NLP) that aims to discover references to entities in some text. Identified entities are classified into predefined categories like person, location and organization. NER is mostly utilized prior to complex natural language understanding tasks such as relation extraction, knowledge base generation, and question answering (Liu and Ren, 2011; Lee et al., 2007). Additionally, NER systems are often part of search engines (Guo et al., 2009b) and machine translation systems (Babych and Hartley, 2003).\nEarly studies regarding NER propose using hand crafted rules and lists of names of people, places and organizations (Humphreys et al., 1998; Appelt et al., 1995). Traditional approaches typically use several hand-crafted features such as capitalization, word length, gazetteer related features, and syntactic features (part-ofspeech tags, chunk tags, etc.) (McCallum and Li, 2003; Finkel et al., 2005). A wide range of machine learning-based methods have been proposed to address named entity recognition. Some\nof the well known approaches are conditional random fields (CRF) (McCallum and Li, 2003; Finkel et al., 2005), maximum entropy (Borthwick, 1999), bootstrapping (Jiang and Zhai, 2007; Wu et al., 2009), latent semantic association (Guo et al., 2009a), and decision trees (Szarvas et al., 2006).\nRecently, deep learning models have been instrumental in deciding how the parts of the input should be composed to allow the most beneficial features to form leading to state-of-theart results (Collobert et al., 2011). Likewise, researchers have found that representing words with fixed length vectors in a dense space helps improving the overall performance of many tasks: sentiment analysis (Socher et al., 2013), syntactic parsing (Collobert and Weston, 2008), language modeling (Mikolov et al., 2010), part-of-speech tagging and NER (Collobert et al., 2011). These word representations or embeddings are automatically learned both during or before the training using various methods such as Word2Vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014).\nBuilding upon these findings, there are recent studies which treat the NER task as a sequence labeling problem which employ LSTM or GRU components (Lample et al., 2016; Huang et al., 2015; Ma and Hovy, 2016; Yang et al., 2016) to capture the syntactic and semantic relations between the units that make up a natural language sentence. However, these approaches are not well studied for morphologically rich languages. Unlike other languages, morphologically rich languages such as Turkish may retain important information in the morphology of the surface form of the word while the same information may be contained in the syntax of other languages. For example, the word “İstanbul’daydı” means ‘he/she was in Istanbul’ in English. The morphological analysis of the word ar X iv :1\n70 6.\n00 50\n6v 1\n[ cs\n.C L\n] 1\nJ un\n2 01\n7\nis “İstanbul+Noun+Prop+A3sg+Pnon+LocˆDB+ Verb+Zero+Past+A3sg” where ‘Prop’ indicates a proper noun, ‘A3sg’ signifies the third singular person aggreement whereas ‘Pnon’ signifies no possesive agreement is active. ‘DB’ indicates a transition of Part-Of-Speech type usually induced by a derivative suffix (Oflazer, 1994). In this case, the derivation is triggered by the ‘-di’ suffix which was decoded as ‘Past’ tag which indicates past tense. As seen from the example, morphological tags may help in capturing syntactic and semantic information. In order to address this, character based embeddings in word representations (Lample et al., 2016) and entities tagged at character level (Kuru et al., 2016) were proposed for NER. Embedding based frameworks for representing morphology were also proposed in other contexts such as language modeling (Luong et al., 2013; dos Santos and Zadrozny, 2014; Xu and Liu, 2017; Bhatia et al., 2016; Lankinen et al., 2016) and morphological tagging and segmentation (Shen et al., 2016; Cotterell and Schütze, 2017). However, even though morphological tags have been employed in the past (Tür et al., 2003; Yeniterzi, 2011), our work is the first to propose an embedding based framework for representing the morphological analysis in the context of NER.\nWe build upon a state-of-the-art NER tagger (Lample et al., 2016) based on a sequential neural model with extensible word representations in Section 2. We show that augmenting the word representation with morphological embeddings based on Bi-LSTMs (Section 2.1) improves the performance of the base model, which uses only pretrained word embeddings. We contribute by investigating various configurations of the morphological analysis of the surface form of a word (Section 2.2). In Section 3.3, we compare the performance of several experiment setups which employ character and morphological embeddings in various combinations. We report F1-measures of 93.59% and 79.59% for Turkish and Czech respectively. These results are the state-of-the-art results compared to previous work (Demir and Özgür, 2014; Seker and Eryiğit, 2012) which rely on a regularized averaged perceptron and CRF respectively both with hand crafted features."
    }, {
      "heading" : "2 Model",
      "text" : "We formally define an input sentence as X = (x1, x2, . . . , xn) where each xi is a fixed length\nvector of size d, consisting of embeddings that represent the ith word (See Section 2.1). xi are then fed to a Bi-LSTM which is composed of two LSTMs (Hochreiter and Schmidhuber, 1997) treating the input forwards and backwards respectively. Thus we obtain these forward and backward components’ cell matrices −→ H and ←− H which are both of size n × p, where p is the number of dimensions of one component of the Bi-LSTM. Thus, −→ H i,j is the value of jth dimension of ith output vector of the right component which corresponds to the ith word in the sentence. We then feed the concatenation of these matricesH = [ −→ H, ←− H ] to a fully connected hidden layer ofK output neurons. To model the dependencies between the corresponding labels of consecutive input units, we follow a conditional random field (CRF) (Lafferty et al., 2001) based approach. This dependency is clearly indicated by labels in IOB tagging scheme, i.e. B-PERSON, I-PERSON, etc.\nTo do this, we obtain a score vector at each position i and aim to minimize the following objective function for a single sample sentence X:\ns(X, y) = ∑ i Ayi,yi+1 + ∑ i ξi,yi\nwhereAi,j represents the score of a transition from tag i to j and ξi are the tag scores at position i output by the uppermost fully connected layer. Using this model, we decode the most probable tagging sequence y∗ as argmaxỹ s(X, ỹ)."
    }, {
      "heading" : "2.1 Embeddings",
      "text" : "It has been shown that modeling units of information in a natural language input as fixed length vectors is more effective at encoding semantic properties of the words compared to deciding on the features apriori (Collobert et al., 2011; Turian et al., 2010). Therefore we represent the input words, xi, as a combination of three embeddings: word, character, and morphological. Thus the size d of xi is dw + 2dm + 2dc. We describe these embeddings below and illustrate in Figure 1.\nWord embeddings. A vector of size dw which is learned by the global objective function. However, we never learn this component from scratch, instead we load pretrained vectors.\nCharacter embeddings. We learn another fixed length vector of size 2dc for each word. However, in contrast with a word embedding, we\nwant to capture the covert relationships in the sequence of characters of the word. To achieve this, we have a separate Bi-LSTM component for this embedding type with a cell dimension of dc. We feed it with the characters of the surface form of xi and concatenate the cell output of the forward and backward LSTMs to obtain the character embedding of the word.\nMorphological embeddings. These are constructed similar to character embeddings. In this case, the individual tags of the morphological analysis are treated as a sequence and fed into the separate Bi-LSTM component for morphological embeddings to obtain a vector of length 2dm. We devised several different combinations of morphological tags which is explained in Section 2.2. To illustrate, we use the word ‘evlerinde’ which can both mean ‘in their house’ or ‘in their houses’ or ‘in his/her houses’ in Turkish. In Figure 1, we assume that the correct morphological analysis is ‘ev+Noun+A3pl+P3sg+Loc’, where ‘A3pl’ indicates 3rd person plural, ‘P3sg’ is the possessive marker for 3rd person singular, and ‘Loc’ is the locative case marker, thus can be translated as ‘in his/her houses’."
    }, {
      "heading" : "2.2 Embedding configurations",
      "text" : "We experimented with different combinations of morphological tags to discover an effective configuration for extracting the syntactic and semantic information in the morphological analysis of a token.\nA simple embedding configuration is to use all morphological tags in the analysis along with or without the root (we call this WR and WOR re-\nspectively). Secondly we tried to remove the tags between the root and the derivation boundary (DB) based on the information they carry may not be relevant in some aspects because of the transformation into a new word with partly different lexical and syntactic properties. We call this version WR ADB, i.e. “İstanbul+ˆDB+Verb+ Zero+Past+A3sg”. Lastly, we devised a scheme in which we treat the string of morphological analysis as a surface form and process each character of this surface form as we did for character embeddings and call this scheme as CHAR."
    }, {
      "heading" : "3 Experiments",
      "text" : ""
    }, {
      "heading" : "3.1 Training",
      "text" : "The parameters to be learned by the training algorithm is the parameters of the Bi-LSTM in Section 2, the parameters of the Bi-LSTMs for the character and morphological embeddings and the word embeddings for each unique word. We experimented with several different choices for the number of dimensions for these parameters and observed that a choice of 100 for word embeddings, 200 for character embeddings and 200 for morphological embeddings. We trained the models by calculating the gradients using backpropagation algorithm and updating with stochastic gradient descent algorithm with a learning rate of 0.01. We also employed gradient clipping to handle gradients diverging from zero. We additionally used dropout on the inputs with probability 0.5."
    }, {
      "heading" : "3.2 Dataset",
      "text" : "We train and evaluate our model with a corpus which is widely used in previous work on Turkish NER (Tür et al., 2003). In addition to the entity tags and tokens, the corpus also contains the disambiguated morphological tags of input words. We observed many morphological analysis errors and incorrect entity taggings in the corpus (Tür et al., 2003), probably as a result of automated analysis and labeling.\nWe obtained word embeddings1 of Turkish words as vectors of length 100 using the skipgram algorithm (Mikolov et al., 2013) on a corpus of 951M words (Yildiz et al., 2016), 2,045,040 of which are unique. This corpus consists of Turkish text extracted from several national newspapers, news sites, and book transcripts.\n1We will make these word embeddings available at (we refrain from sharing the url during the review process)."
    }, {
      "heading" : "3.3 Results",
      "text" : "We observed that using pretrained word embeddings gave the best results compared to learning word embeddings while training the model. Therefore we only include the results of experiment setups with pretrained word embeddings in Table 1.\nWe start with comparing Setup 1 and 4 (and 5) and suggest that the morphological analysis does indeed contribute to higher performance with CHAR and WOR. However, Setup 2 and 3 did not reach the performance level of Setup 1. We suspect that the reason is the relatively high number of parameters when we include the 20030 roots into the model. This effect is also seen in Setup 6 and 7 which have a lower performance compared to Setups 8 and 9. However, we have to note that using character embeddings alone also improved the performance in Setup 10. Nevertheless, we see that the best performance is achieved in Setup 9 when both of them are employed. However, when we performed the McNemar’s test (Dietterich, 1998), we observed that the difference between them is not significant at 95% confidence level. We explain the difference in performance between Setup 4 and 5 with the errors in the morphological analysis which are mostly due to unknown or mispelled words. In those cases, the analysis become usually the same nominal case with 3rd person singular. We suspect that the fact that ME(CHAR) can process the root even if it is faulty allows it to capture useful information into the embedding.\nDespite CE caused a large improvement gener-\nally, it provides a relatively small increase in Setup 8 compared to Setup 4. We believe that the reason behind this is that CE and ME(CHAR) competes with each other in representing the morphological information of the word. The reason that Setup 9 achieved higher performance compared to Setup 8 is probably because the missing roots in Setup 9 can be covered by CE combined with relatively lower complexity of ME(WOR).\nWe have also evaluated our model on text in Czech which is another morphologically rich language. To be able to compare our results, we used the CNEC 2.0 corpus in CoNLL format as other studies did (Konkol and Konopı́k, 2013). We chose to include only the ME(CHAR) setup for Czech because it gave good results both with and without character embeddings.\nLastly, we compare our best results with previous state-of-the-art in Table 2. The performance of (Seker and Eryiğit, 2012) without gazetteers is 89.55%, (Kuru et al., 2016) does not employ any external data and (Demir and Özgür, 2014) still relies on hand-crafted features despite exploiting word embeddings trained externally."
    }, {
      "heading" : "4 Conclusions",
      "text" : "In this work, we demonstrated a new state-of-theart system for Turkish and Czech named entity recognition using the model of (Lample et al., 2016). We introduced embedding configurations to understand the affect of different combinations of the morphological tags. Using these configurations, we showed that augmenting word representations with morphological embeddings improves the performance. However, the contribution of morphological embeddings seems to be subsumed by character embeddings in some of these configurations. Thus a thorough examination and comparison of character and morphological embeddings learned in this sense is required for further discussion."
    } ],
    "references" : [ {
      "title" : "SRI International FASTUS system: MUC-6 test results and analysis",
      "author" : [ "Douglas E Appelt", "Jerry R Hobbs", "John Bear", "David Israel", "Megumi Kameyama", "David Martin", "Karen Myers", "Mabry Tyson." ],
      "venue" : "Proceedings of the 6th Conference on Message Un-",
      "citeRegEx" : "Appelt et al\\.,? 1995",
      "shortCiteRegEx" : "Appelt et al\\.",
      "year" : 1995
    }, {
      "title" : "Improving machine translation quality with automatic named entity recognition",
      "author" : [ "Bogdan Babych", "Anthony Hartley." ],
      "venue" : "Proceedings of the 7th International EAMT Workshop on MT and Other Language Technology Tools, Improving MT through",
      "citeRegEx" : "Babych and Hartley.,? 2003",
      "shortCiteRegEx" : "Babych and Hartley.",
      "year" : 2003
    }, {
      "title" : "Morphological priors for probabilistic neural word embeddings",
      "author" : [ "Parminder Bhatia", "Robert Guthrie", "Jacob Eisenstein." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Bhatia et al\\.,? 2016",
      "shortCiteRegEx" : "Bhatia et al\\.",
      "year" : 2016
    }, {
      "title" : "A Maximum Entropy Approach to Named Entity Recognition",
      "author" : [ "Andrew Eliot Borthwick." ],
      "venue" : "Ph.D. thesis, New York University, New York, NY, USA.",
      "citeRegEx" : "Borthwick.,? 1999",
      "shortCiteRegEx" : "Borthwick.",
      "year" : 1999
    }, {
      "title" : "A unified architecture for natural language processing",
      "author" : [ "Ronan Collobert", "Jason Weston." ],
      "venue" : "Proceedings of the 25th International Conference on Machine Learning - ICML 08. ACM. https://doi.org/10.1145/1390156.1390177.",
      "citeRegEx" : "Collobert and Weston.,? 2008",
      "shortCiteRegEx" : "Collobert and Weston.",
      "year" : 2008
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa." ],
      "venue" : "Journal of Machine Learning Research 12(Aug):2493–2537.",
      "citeRegEx" : "Collobert et al\\.,? 2011",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "Joint semantic synthesis and morphological analysis of the derived word",
      "author" : [ "Ryan Cotterell", "Hinrich Schütze." ],
      "venue" : "CoRR abs/1701.00946.",
      "citeRegEx" : "Cotterell and Schütze.,? 2017",
      "shortCiteRegEx" : "Cotterell and Schütze.",
      "year" : 2017
    }, {
      "title" : "Improving named entity recognition for morphologically rich languages using word embeddings",
      "author" : [ "Hakan Demir", "Arzucan Özgür." ],
      "venue" : "Machine Learning and Applications (ICMLA), 2014 13th International Conference on. IEEE, pages 117–122.",
      "citeRegEx" : "Demir and Özgür.,? 2014",
      "shortCiteRegEx" : "Demir and Özgür.",
      "year" : 2014
    }, {
      "title" : "Approximate statistical tests for comparing supervised classification learning algorithms",
      "author" : [ "Thomas G Dietterich." ],
      "venue" : "Neural Computation 10(7):1895– 1923.",
      "citeRegEx" : "Dietterich.,? 1998",
      "shortCiteRegEx" : "Dietterich.",
      "year" : 1998
    }, {
      "title" : "Learning character-level representations for part-of-speech tagging",
      "author" : [ "Cı́cero Nogueira dos Santos", "Bianca Zadrozny" ],
      "venue" : null,
      "citeRegEx" : "Santos and Zadrozny.,? \\Q2014\\E",
      "shortCiteRegEx" : "Santos and Zadrozny.",
      "year" : 2014
    }, {
      "title" : "Incorporating non-local information into information extraction systems by Gibbs sampling",
      "author" : [ "Jenny Rose Finkel", "Trond Grenager", "Christopher Manning." ],
      "venue" : "Proceedings of the 43rd Annual Meeting of ACL. ACL, pages 363–370.",
      "citeRegEx" : "Finkel et al\\.,? 2005",
      "shortCiteRegEx" : "Finkel et al\\.",
      "year" : 2005
    }, {
      "title" : "Domain adaptation with latent semantic association for named entity recognition",
      "author" : [ "Honglei Guo", "Huijia Zhu", "Zhili Guo", "Xiaoxun Zhang", "Xian Wu", "Zhong Su." ],
      "venue" : "Proceedings of Human Language",
      "citeRegEx" : "Guo et al\\.,? 2009a",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2009
    }, {
      "title" : "Named entity recognition in query",
      "author" : [ "Jiafeng Guo", "Gu Xu", "Xueqi Cheng", "Hang Li." ],
      "venue" : "Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, pages 267–274.",
      "citeRegEx" : "Guo et al\\.,? 2009b",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2009
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural Computation 9(8).",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Bidirectional LSTM-CRF models for sequence tagging",
      "author" : [ "Zhiheng Huang", "Wei Xu", "Kai Yu." ],
      "venue" : "arXiv preprint arXiv:1508.01991 .",
      "citeRegEx" : "Huang et al\\.,? 2015",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2015
    }, {
      "title" : "University of Sheffield: Description of the LaSIE-II system as used for MUC-7",
      "author" : [ "Kevin Humphreys", "Robert Gaizauskas", "Saliha Azzam", "Chris Huyck", "Brian Mitchell", "Hamish Cunningham", "Yorick Wilks." ],
      "venue" : "Proceedings of the Seventh Message Un-",
      "citeRegEx" : "Humphreys et al\\.,? 1998",
      "shortCiteRegEx" : "Humphreys et al\\.",
      "year" : 1998
    }, {
      "title" : "Instance weighting for domain adaptation in NLP",
      "author" : [ "Jing Jiang", "Cheng Xiang Zhai." ],
      "venue" : "Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics. ACL, Prague, volume 7, pages 264–271.",
      "citeRegEx" : "Jiang and Zhai.,? 2007",
      "shortCiteRegEx" : "Jiang and Zhai.",
      "year" : 2007
    }, {
      "title" : "Crfbased czech named entity recognizer and consolidation of czech ner research",
      "author" : [ "Michal Konkol", "Miloslav Konopı́k" ],
      "venue" : "In Ivan Habernal and Václav Matoušek, editors, Text, Speech and Dialogue,",
      "citeRegEx" : "Konkol and Konopı́k.,? \\Q2013\\E",
      "shortCiteRegEx" : "Konkol and Konopı́k.",
      "year" : 2013
    }, {
      "title" : "Charner: Character-level named entity recognition",
      "author" : [ "Onur Kuru", "Ozan Arkan Can", "Deniz Yuret." ],
      "venue" : "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Orga-",
      "citeRegEx" : "Kuru et al\\.,? 2016",
      "shortCiteRegEx" : "Kuru et al\\.",
      "year" : 2016
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "John Lafferty", "Andrew McCallum", "Fernando Pereira." ],
      "venue" : "Proceedings of the eighteenth international conference on machine learning, ICML.",
      "citeRegEx" : "Lafferty et al\\.,? 2001",
      "shortCiteRegEx" : "Lafferty et al\\.",
      "year" : 2001
    }, {
      "title" : "Neural architectures for named entity recognition",
      "author" : [ "Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer." ],
      "venue" : "Proceedings of NAACL-HLT 2016. pages 260– 270.",
      "citeRegEx" : "Lample et al\\.,? 2016",
      "shortCiteRegEx" : "Lample et al\\.",
      "year" : 2016
    }, {
      "title" : "A character-word compositional neural language model for finnish",
      "author" : [ "Matti Lankinen", "Hannes Heikinheimo", "Pyry Takala", "Tapani Raiko", "Juha Karhunen." ],
      "venue" : "CoRR abs/1612.03266.",
      "citeRegEx" : "Lankinen et al\\.,? 2016",
      "shortCiteRegEx" : "Lankinen et al\\.",
      "year" : 2016
    }, {
      "title" : "Fine-grained named entity recognition and relation extraction for question answering",
      "author" : [ "Changki Lee", "Yi-Gyu Hwang", "Myung-Gil Jang." ],
      "venue" : "Proceedings of the 30th Annual International",
      "citeRegEx" : "Lee et al\\.,? 2007",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2007
    }, {
      "title" : "Japanese named entity recognition for question answering system",
      "author" : [ "Ye Liu", "Fuji Ren." ],
      "venue" : "2011 IEEE International Conference on Cloud Computing and Intelligence Systems. IEEE. https://doi.org/10.1109/ccis.2011.6045098.",
      "citeRegEx" : "Liu and Ren.,? 2011",
      "shortCiteRegEx" : "Liu and Ren.",
      "year" : 2011
    }, {
      "title" : "Better word representations with recursive neural networks for morphology",
      "author" : [ "Thang Luong", "Richard Socher", "Christopher D Manning." ],
      "venue" : "CoNLL. pages 104–113.",
      "citeRegEx" : "Luong et al\\.,? 2013",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2013
    }, {
      "title" : "End-to-end sequence labeling via bi-directional LSTM-CNNsCRF",
      "author" : [ "Xuezhe Ma", "Eduard Hovy." ],
      "venue" : "arXiv preprint arXiv:1603.01354 .",
      "citeRegEx" : "Ma and Hovy.,? 2016",
      "shortCiteRegEx" : "Ma and Hovy.",
      "year" : 2016
    }, {
      "title" : "Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons",
      "author" : [ "Andrew McCallum", "Wei Li." ],
      "venue" : "Proceedings of the 7th Conference on Natural Language Learning at HLT-NAACL. Associa-",
      "citeRegEx" : "McCallum and Li.,? 2003",
      "shortCiteRegEx" : "McCallum and Li.",
      "year" : 2003
    }, {
      "title" : "Recurrent neural network based language model",
      "author" : [ "Tomas Mikolov", "Martin Karafiát", "Lukas Burget", "Jan Cernockỳ", "Sanjeev Khudanpur." ],
      "venue" : "Interspeech. volume 2, page 3.",
      "citeRegEx" : "Mikolov et al\\.,? 2010",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2010
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean." ],
      "venue" : "Advances in Neural Information Processing Systems. pages 3111–3119.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Two-level description of Turkish morphology",
      "author" : [ "Kemal Oflazer." ],
      "venue" : "Literary and Linguistic Computing 9(2):137–148.",
      "citeRegEx" : "Oflazer.,? 1994",
      "shortCiteRegEx" : "Oflazer.",
      "year" : 1994
    }, {
      "title" : "GloVe: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D. Manning." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). pages 1532–1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Initial explorations on using CRFs for Turkish named entity recognition",
      "author" : [ "Gökhan Akın Seker", "Gülşen Eryiğit." ],
      "venue" : "Proceedings of COLING 2012. The COLING 2012 Organizing Committee, Mumbai, India, pages 2459–2474.",
      "citeRegEx" : "Seker and Eryiğit.,? 2012",
      "shortCiteRegEx" : "Seker and Eryiğit.",
      "year" : 2012
    }, {
      "title" : "The role of context in neural morphological disambiguation",
      "author" : [ "Qinlan Shen", "Daniel Clothiaux", "Emily Tagtow", "Patrick Littell", "Chris Dyer." ],
      "venue" : "Proceedings of COLING 2016, the 26th International Conference on Computational Linguis-",
      "citeRegEx" : "Shen et al\\.,? 2016",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2016
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 2013 Conference on",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "A multilingual named entity recognition system using boosting and C4.5 decision tree learning algorithms",
      "author" : [ "György Szarvas", "Richárd Farkas", "András Kocsor" ],
      "venue" : "In International Conference on Discovery",
      "citeRegEx" : "Szarvas et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Szarvas et al\\.",
      "year" : 2006
    }, {
      "title" : "A statistical information extraction system for turkish",
      "author" : [ "Gökhan Tür", "Dilek Hakkani-Tür", "Kemal Oflazer." ],
      "venue" : "Natural Language Engineering 9(2):181– 210.",
      "citeRegEx" : "Tür et al\\.,? 2003",
      "shortCiteRegEx" : "Tür et al\\.",
      "year" : 2003
    }, {
      "title" : "Word representations: A simple and general method for semi-supervised learning",
      "author" : [ "Joseph Turian", "Lev-Arie Ratinov", "Yoshua Bengio." ],
      "venue" : "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Association for",
      "citeRegEx" : "Turian et al\\.,? 2010",
      "shortCiteRegEx" : "Turian et al\\.",
      "year" : 2010
    }, {
      "title" : "Domain adaptive bootstrapping for named entity recognition",
      "author" : [ "Dan Wu", "Wee Sun Lee", "Nan Ye", "Hai Leong Chieu." ],
      "venue" : "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP). ACL, volume 3, pages 1523–",
      "citeRegEx" : "Wu et al\\.,? 2009",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2009
    }, {
      "title" : "Implicitly incorporating morphological information into word embedding",
      "author" : [ "Yang Xu", "Jiawei Liu." ],
      "venue" : "CoRR abs/1701.02481.",
      "citeRegEx" : "Xu and Liu.,? 2017",
      "shortCiteRegEx" : "Xu and Liu.",
      "year" : 2017
    }, {
      "title" : "Multi-task cross-lingual sequence tagging from scratch",
      "author" : [ "Zhilin Yang", "Ruslan Salakhutdinov", "William W. Cohen." ],
      "venue" : "CoRR abs/1603.06270.",
      "citeRegEx" : "Yang et al\\.,? 2016",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2016
    }, {
      "title" : "Exploiting morphology in turkish named entity recognition system",
      "author" : [ "Reyyan Yeniterzi." ],
      "venue" : "Proceedings of the ACL 2011 Student Session. Association for Computational Linguistics, Stroudsburg, PA, USA, HLT-SS ’11, pages 105–110.",
      "citeRegEx" : "Yeniterzi.,? 2011",
      "shortCiteRegEx" : "Yeniterzi.",
      "year" : 2011
    }, {
      "title" : "A morphology-aware network for morphological disambiguation",
      "author" : [ "Eray Yildiz", "Caglar Tirkaz", "H Bahadir Sahin", "Mustafa Tolga Eren", "Omer Ozan Sonmez." ],
      "venue" : "30th AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Yildiz et al\\.,? 2016",
      "shortCiteRegEx" : "Yildiz et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "Additionally, NER systems are often part of search engines (Guo et al., 2009b) and machine translation systems (Babych and Hartley, 2003).",
      "startOffset" : 59,
      "endOffset" : 78
    }, {
      "referenceID" : 1,
      "context" : ", 2009b) and machine translation systems (Babych and Hartley, 2003).",
      "startOffset" : 41,
      "endOffset" : 67
    }, {
      "referenceID" : 15,
      "context" : "Early studies regarding NER propose using hand crafted rules and lists of names of people, places and organizations (Humphreys et al., 1998; Appelt et al., 1995).",
      "startOffset" : 116,
      "endOffset" : 161
    }, {
      "referenceID" : 0,
      "context" : "Early studies regarding NER propose using hand crafted rules and lists of names of people, places and organizations (Humphreys et al., 1998; Appelt et al., 1995).",
      "startOffset" : 116,
      "endOffset" : 161
    }, {
      "referenceID" : 26,
      "context" : ") (McCallum and Li, 2003; Finkel et al., 2005).",
      "startOffset" : 2,
      "endOffset" : 46
    }, {
      "referenceID" : 10,
      "context" : ") (McCallum and Li, 2003; Finkel et al., 2005).",
      "startOffset" : 2,
      "endOffset" : 46
    }, {
      "referenceID" : 26,
      "context" : "Some of the well known approaches are conditional random fields (CRF) (McCallum and Li, 2003; Finkel et al., 2005), maximum entropy (Borthwick, 1999), bootstrapping (Jiang and Zhai, 2007; Wu et al.",
      "startOffset" : 70,
      "endOffset" : 114
    }, {
      "referenceID" : 10,
      "context" : "Some of the well known approaches are conditional random fields (CRF) (McCallum and Li, 2003; Finkel et al., 2005), maximum entropy (Borthwick, 1999), bootstrapping (Jiang and Zhai, 2007; Wu et al.",
      "startOffset" : 70,
      "endOffset" : 114
    }, {
      "referenceID" : 3,
      "context" : ", 2005), maximum entropy (Borthwick, 1999), bootstrapping (Jiang and Zhai, 2007; Wu et al.",
      "startOffset" : 25,
      "endOffset" : 42
    }, {
      "referenceID" : 16,
      "context" : ", 2005), maximum entropy (Borthwick, 1999), bootstrapping (Jiang and Zhai, 2007; Wu et al., 2009), latent semantic association (Guo",
      "startOffset" : 58,
      "endOffset" : 97
    }, {
      "referenceID" : 37,
      "context" : ", 2005), maximum entropy (Borthwick, 1999), bootstrapping (Jiang and Zhai, 2007; Wu et al., 2009), latent semantic association (Guo",
      "startOffset" : 58,
      "endOffset" : 97
    }, {
      "referenceID" : 34,
      "context" : ", 2009a), and decision trees (Szarvas et al., 2006).",
      "startOffset" : 29,
      "endOffset" : 51
    }, {
      "referenceID" : 5,
      "context" : "Recently, deep learning models have been instrumental in deciding how the parts of the input should be composed to allow the most beneficial features to form leading to state-of-theart results (Collobert et al., 2011).",
      "startOffset" : 193,
      "endOffset" : 217
    }, {
      "referenceID" : 33,
      "context" : "searchers have found that representing words with fixed length vectors in a dense space helps improving the overall performance of many tasks: sentiment analysis (Socher et al., 2013), syntactic parsing (Collobert and Weston, 2008), language",
      "startOffset" : 162,
      "endOffset" : 183
    }, {
      "referenceID" : 4,
      "context" : ", 2013), syntactic parsing (Collobert and Weston, 2008), language",
      "startOffset" : 27,
      "endOffset" : 55
    }, {
      "referenceID" : 27,
      "context" : "modeling (Mikolov et al., 2010), part-of-speech tagging and NER (Collobert et al.",
      "startOffset" : 9,
      "endOffset" : 31
    }, {
      "referenceID" : 5,
      "context" : ", 2010), part-of-speech tagging and NER (Collobert et al., 2011).",
      "startOffset" : 40,
      "endOffset" : 64
    }, {
      "referenceID" : 30,
      "context" : ", 2013) and GloVe (Pennington et al., 2014).",
      "startOffset" : 18,
      "endOffset" : 43
    }, {
      "referenceID" : 29,
      "context" : "‘DB’ indicates a transition of Part-Of-Speech type usually induced by a derivative suffix (Oflazer, 1994).",
      "startOffset" : 90,
      "endOffset" : 105
    }, {
      "referenceID" : 20,
      "context" : "In order to address this, character based embeddings in word representations (Lample et al., 2016) and entities tagged at character level (Kuru et al.",
      "startOffset" : 77,
      "endOffset" : 98
    }, {
      "referenceID" : 18,
      "context" : ", 2016) and entities tagged at character level (Kuru et al., 2016) were proposed for NER.",
      "startOffset" : 47,
      "endOffset" : 66
    }, {
      "referenceID" : 24,
      "context" : "texts such as language modeling (Luong et al., 2013; dos Santos and Zadrozny, 2014; Xu and Liu, 2017; Bhatia et al., 2016; Lankinen et al., 2016) and morphological tagging and segmentation (Shen et al.",
      "startOffset" : 32,
      "endOffset" : 145
    }, {
      "referenceID" : 38,
      "context" : "texts such as language modeling (Luong et al., 2013; dos Santos and Zadrozny, 2014; Xu and Liu, 2017; Bhatia et al., 2016; Lankinen et al., 2016) and morphological tagging and segmentation (Shen et al.",
      "startOffset" : 32,
      "endOffset" : 145
    }, {
      "referenceID" : 2,
      "context" : "texts such as language modeling (Luong et al., 2013; dos Santos and Zadrozny, 2014; Xu and Liu, 2017; Bhatia et al., 2016; Lankinen et al., 2016) and morphological tagging and segmentation (Shen et al.",
      "startOffset" : 32,
      "endOffset" : 145
    }, {
      "referenceID" : 21,
      "context" : "texts such as language modeling (Luong et al., 2013; dos Santos and Zadrozny, 2014; Xu and Liu, 2017; Bhatia et al., 2016; Lankinen et al., 2016) and morphological tagging and segmentation (Shen et al.",
      "startOffset" : 32,
      "endOffset" : 145
    }, {
      "referenceID" : 35,
      "context" : "However, even though morphological tags have been employed in the past (Tür et al., 2003; Yeniterzi, 2011), our work is the first to propose an embedding based framework for representing the morphological analysis in the context of NER.",
      "startOffset" : 71,
      "endOffset" : 106
    }, {
      "referenceID" : 40,
      "context" : "However, even though morphological tags have been employed in the past (Tür et al., 2003; Yeniterzi, 2011), our work is the first to propose an embedding based framework for representing the morphological analysis in the context of NER.",
      "startOffset" : 71,
      "endOffset" : 106
    }, {
      "referenceID" : 20,
      "context" : "We build upon a state-of-the-art NER tagger (Lample et al., 2016) based on a sequential neural model with extensible word representations in Section 2.",
      "startOffset" : 44,
      "endOffset" : 65
    }, {
      "referenceID" : 7,
      "context" : "These results are the state-of-the-art results compared to previous work (Demir and Özgür, 2014; Seker and Eryiğit, 2012) which rely on a regularized averaged perceptron and CRF respectively both with hand crafted features.",
      "startOffset" : 73,
      "endOffset" : 121
    }, {
      "referenceID" : 31,
      "context" : "These results are the state-of-the-art results compared to previous work (Demir and Özgür, 2014; Seker and Eryiğit, 2012) which rely on a regularized averaged perceptron and CRF respectively both with hand crafted features.",
      "startOffset" : 73,
      "endOffset" : 121
    }, {
      "referenceID" : 13,
      "context" : "xi are then fed to a Bi-LSTM which is composed of two LSTMs (Hochreiter and Schmidhuber, 1997) treating the input forwards and backwards respectively.",
      "startOffset" : 60,
      "endOffset" : 94
    }, {
      "referenceID" : 19,
      "context" : "To model the dependencies between the corresponding labels of consecutive input units, we follow a conditional random field (CRF) (Lafferty et al., 2001) based approach.",
      "startOffset" : 130,
      "endOffset" : 153
    }, {
      "referenceID" : 5,
      "context" : "It has been shown that modeling units of information in a natural language input as fixed length vectors is more effective at encoding semantic properties of the words compared to deciding on the features apriori (Collobert et al., 2011; Turian et al., 2010).",
      "startOffset" : 213,
      "endOffset" : 258
    }, {
      "referenceID" : 36,
      "context" : "It has been shown that modeling units of information in a natural language input as fixed length vectors is more effective at encoding semantic properties of the words compared to deciding on the features apriori (Collobert et al., 2011; Turian et al., 2010).",
      "startOffset" : 213,
      "endOffset" : 258
    }, {
      "referenceID" : 35,
      "context" : "We train and evaluate our model with a corpus which is widely used in previous work on Turkish NER (Tür et al., 2003).",
      "startOffset" : 99,
      "endOffset" : 117
    }, {
      "referenceID" : 35,
      "context" : "We observed many morphological analysis errors and incorrect entity taggings in the corpus (Tür et al., 2003), probably as a result of automated analysis and labeling.",
      "startOffset" : 91,
      "endOffset" : 109
    }, {
      "referenceID" : 28,
      "context" : "We obtained word embeddings1 of Turkish words as vectors of length 100 using the skipgram algorithm (Mikolov et al., 2013) on a corpus of 951M words (Yildiz et al.",
      "startOffset" : 100,
      "endOffset" : 122
    }, {
      "referenceID" : 41,
      "context" : ", 2013) on a corpus of 951M words (Yildiz et al., 2016), 2,045,040 of which are unique.",
      "startOffset" : 34,
      "endOffset" : 55
    }, {
      "referenceID" : 8,
      "context" : "However, when we performed the McNemar’s test (Dietterich, 1998), we observed that the difference between them is not significant at 95% confidence level.",
      "startOffset" : 46,
      "endOffset" : 64
    }, {
      "referenceID" : 17,
      "context" : "0 corpus in CoNLL format as other studies did (Konkol and Konopı́k, 2013).",
      "startOffset" : 46,
      "endOffset" : 73
    }, {
      "referenceID" : 31,
      "context" : "The performance of (Seker and Eryiğit, 2012) without gazetteers is 89.",
      "startOffset" : 19,
      "endOffset" : 44
    }, {
      "referenceID" : 18,
      "context" : "55%, (Kuru et al., 2016) does not employ any external data and (Demir and Özgür, 2014) still relies on hand-crafted features despite exploiting",
      "startOffset" : 5,
      "endOffset" : 24
    }, {
      "referenceID" : 7,
      "context" : ", 2016) does not employ any external data and (Demir and Özgür, 2014) still relies on hand-crafted features despite exploiting",
      "startOffset" : 46,
      "endOffset" : 69
    }, {
      "referenceID" : 18,
      "context" : "(Kuru et al., 2016) 91.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 7,
      "context" : "(Demir and Özgür, 2014) 91.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 31,
      "context" : "(Seker and Eryiğit, 2012) 91.",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 20,
      "context" : "In this work, we demonstrated a new state-of-theart system for Turkish and Czech named entity recognition using the model of (Lample et al., 2016).",
      "startOffset" : 125,
      "endOffset" : 146
    } ],
    "year" : 2017,
    "abstractText" : "In this work, we present new state-ofthe-art results of 93.59% and 79.59% for Turkish and Czech named entity recognition based on the model of (Lample et al., 2016). We contribute by proposing several schemes for representing the morphological analysis of a word in the context of named entity recognition. We show that a concatenation of this representation with the word and character embeddings improves the performance. The effect of these representation schemes on the tagging performance is also investigated.",
    "creator" : "LaTeX with hyperref package"
  }
}