{
  "name" : "1510.07303.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "A Framework for Distributed Deep Learning Layer Design in Python",
    "authors" : [ "Clay McLeod" ],
    "emails" : [ "clmcleod@go.olemiss.edu" ],
    "sections" : [ {
      "heading" : "Introduction",
      "text" : "In recent years, the amount of data that is produced annually has increased dramatically [1], [2]. The availability of this data has prompted researchers to publish large amounts of literature that study how we can make sense of this data, and, in turn, how can we handle this data in a computationally efficient manner [3], [4], [5], [6]. The best performing methods involve a statistical technique called Machine Learning (ML), wherein we try to create a model of the data by taking advantage of a machine’s high processing power [7], [8]. For a complete overview of ML and the different algorithms involved, please refer to [9]. One particularly good ML algorithm for working with large amounts of data is a specific subset of Artificial Neural Networks (ANNs) [10], [11], called Deep Neural Networks (DNNs) [12], [13], [14]. DNNs work well with large sets of data because of their peculiar quality of being a universal approximator [15], [16], which means that theoretically speaking, they can model any real function. This is extremely helpful in the case of big data because DNNs can flush out relationships within large amounts of data that humans would miss, due to our lower computational capacity. Lately, Python has become a language of choice for statisticians to model DNNs, partially because it abstracts away difficult language semantics, allowing researchers to focus mainly on the design of the algorithms rather than syntactical errors. Another key factor in Python’s success in the DNN research space is due to it’s portability and the large amount of scientific libraries that are actively being developed, such as Numpy [17], Scipy [18], Pandas [19], and Theano [20], [21]. This ecosystem allows for high performance APIs with GPU acceleration to be run on a variety of different operating systems. In this paper, we will examine the benefits and detriments of using Python to model DNNs. Next, we will examine best practices for implementing a scalable infrastructure within which adding workers to the cluster is\ntrivial. Lastly, some experimental results will be presented to show the effectiveness of this approach."
    }, {
      "heading" : "Machine Learning in Python",
      "text" : ""
    }, {
      "heading" : "Benefits",
      "text" : "Simplicity: First, Python is a good language because it is an easily language to understand and program with. Because the barrier to begin programming in Python is so low, it attracts a large audience of people - especially experts from other academic backgrounds who have little programming experience but would like to take advantage of advanced computational processes.\nStrong, Open Source Community: Partially because of the simplicity referenced earlier, a strong community of academics has formed around the open source community. This allows libraries to evolve and improve at a rapid pace that is not reasonable when a small group of programmers is working on a closed source project. Thus, many Python libraries utilize cutting edge techniques and strong documentation pages. Many Python libraries also use nonrestrictive licenses, which also prompts businesses to use them and contribute back to the community.\nMature scientific libraries: Furthermore, many experts in the scientific fields (wishing to reach the largest possible audience) focus exclusively on developing libraries in Python. This has lead to a mature, production tested ecosystem for developing applications in Python (see “Dependencies”)."
    }, {
      "heading" : "Detriments",
      "text" : "Global Interpreter Lock: One of the major roadblocks for implementing massively parallel systems in Python in the existence of the Global Interpreter Lock (GIL) [22]. The GIL is necessary in the default Python implementation because Python’s memory management system is not thread safe. Concretely, this means that means that no two Python threads can access a python object at the same time, which allows for concurrency, but not parallelism [23]. This restriction greatly reduces Python’s ability to utilize multiple CPUs [24]. Furthermore, since the GIL has been implemented in Python since its inception, many other subsystems within Python’s ecosystem have grown to rely on it’s existence. Figure 1 shows the some core Python scientific libraries and their relationship with the\nar X\niv :1\n51 0.\n07 30\n3v 1\n[ cs\n.L G\n] 2\n5 O\nct 2\n01 5\n2 GIL. Although several transpilers, such as IronPython and Jython, have overcome Python’s reliance on the GIL, these transpilers have not been widely adopted due to their lack of full API support. No one has successfully implemented a solution for the GIL, so the problem still persists today.1"
    }, {
      "heading" : "Objectives",
      "text" : "Before system design is discussed, the objectives of this paper and the system should first be discussed. Many key goals were taken into consideration during the design of this system, namely:\n• Best DNN layer design: Although some literature exists on the practical design of deep neural networks [25], [26], [27], designing deep neural networks remains a mix of following these guidelines and mastering the “black art” of designing deep neural networks. This system was designed to discover what empirical design rules can be discovered about designing deep neural networks by systemically evaluating performance of (1000-50000) DNNs on the same dataset while varying the hyper-parameters and layer design. • Simplicity: When designing large systems such as this, one might lose sight of the forest for the trees. In this implementation, simplicity is the stressed wherever possible. This allows the user to focus solely on the design of the DNN rather than fix system errors. • Fullstack Python: For this implementation, a fullstack Python solution will be presented. This is not necessarily for performance benefits so much as it is for completeness: this objective is important because the reader is presumably familiar with Python, but not necessarily familiar with any other programming languages. Therefore, to appeal to as wide of an audience as possible, only Python will be used in this system. Optimization of different parts of this system are left as an exercise to the reader. • Dataset: Briefly, we are focusing on a very particular type of dataset. Namely, datasets under 1TB that\n1Note that a Python interpreter is just a C level program. Whenever this C level program accesses a Python object, the GIL is locked for that process. However, different processes on a UNIX system have exclusive memory spaces, so each subprocess of Python has it’s own GIL. This alleviates some of the problems introduced by the GIL, but objects still have serialized and deserialized for objects to communicate, which can be costly. Credit: email correspondence with Matthew Rocklin\ncontain numerical features and are trying to predict a certain class for the label. Generally speaking, this system could potentially work for other variants of data (> 1TB, regression problems). Due to the difficulty of addressing every type of problem that DNNs can solve exhaustively, those are left as an exercise to the reader. Note: For datasets larger than 1TB, this system will probably note work well for two reasons. First, uploading of a CSV bigger than 1TB is extremely cumbersome through a web api, usually causing modern web browsers to crash. Second, the system design proposed assumes that your dataset will fit trivially in memory. If your dataset is larger than 1TB, the reader is encouraged to use a different system, such as Hadoop [5]. • Easy deployment: Another key goal of this system design was to ensure ease/quickness of deployment across the system when testing rules and other code changes were applied to the code base. Since most researchers studying DNN design are not expert system administrators, this step is crucial to ensuring efficient research on DNNs. • GPU acceleration: Although not true for every machine, some machines are equipped with graphic processing units (GPUs), which can be utilized to greatly decrease running time for training DNNs [28]. Therefore, this system ensures that each machine that has a GPU available will take advantage of this asset."
    }, {
      "heading" : "Dependencies",
      "text" : "In the following sections, the host machine refers to a single machine that coordinates the cluster of worker machines. This includes distributed jobs through the message queue (see “RabbitMQ”), the database (see “MongoDB”), and the HTTP reporting server (see “Reporting Server”). A worker machine is a dispensable machine that runs tasks as it receives it from the host machine. This system has only been tested on Python 2.7, but porting to Python 3.X should be trivial."
    }, {
      "heading" : "Ecosystem",
      "text" : "Docker: Docker is a lightweight Linux container system that allows you to compile a specific Linux image once and deploy that image to multiple destinations with ease [29]. Concretely, we can compile a Ubuntu Linux image with the correct Python libraries installed then deploy our image to each of our worker servers with ease. Docker runs on top of most (if not all) major operating systems and takes up little overhead. For more information on configuring Docker or its capabilities, visit [30].\nCoreOS: CoreOS is a lightweight, flexible operating system that runs containers on top of it [31]. CoreOS has autoclustering and job deployment (called services) baked in. In addition, the CoreOS team has built a painless distributed key-value store called etcd [32] that comes\n3 with the operating system. Although many of these features are not applicable to this particular paper, utilization of these features could further improve upon the proposed implementation.\nRabbitMQ: RabbitMQ is a robust message queue implementation that is easy, portable, and supports a number of messaging protocols, including the popular AMPQ protocol [33]. Message queues [34] are crucial when building a distributed computing platform utilizing Python, because it provides a centralized pool of tasks for each of your worker machines to pull from. The decision to use this particular messaging queue to store future parameters is based on the reputation of RabbitMQ being a battle-tested, production ready product. However, other similar task storing systems, such as Redis [35], are worth mentioning.\nMongoDB: MongoDB is a NoSQL database that offers easy storage and installation [36]. This is one of the few components of the proposed system that is chosen purely by preference instead of some performance based metric. As previously stated, one of the major goals of this project is to keep the solution as simple as possible. MongoDB helps achieve this goal because of the vast amount of documentation available, the ease of installation/management, and the availability of well-documented libraries to interact with it. However, since MongoDB is only used for results storage, any database you like could be used here. A comprehensive comparison of databases and their trade offs can be found in [37]."
    }, {
      "heading" : "Reporting Server",
      "text" : "The reporting server was built on a web platform, utilizing HTML5, CSS3, and JavaScript. Notable libraries and their functions include:\n• NGINX (http://www.nginx.com/): Serve static web pages and reverse proxy reporting server. • Bootstrap (http://getbootstrap.com/): CSS Styling library provided by Twitter. • jQuery (http://jquery.com): DOM manipulation and communication with backend REST server. • plot.ly (http://plot.ly): Plotting results, manipulating graphs, and exporting results. • Papa Parse (http://papaparse.com/): JS library for easily parsing CSV files."
    }, {
      "heading" : "Python Libraries",
      "text" : "Flask: Flask is a microframework for building web services uses Python [38]. In this system, Flask acts as the middleman between the website and the database by exposing a REST api [39] (see “System Design”). Although not suitable for larger project because of its long-blocking operations, Flask was used in this system because of its simplicity and the commitment to write as much of the system as possible using fullstack Python.\nCelery: Celery is an “asynchronous task queue/job queue based on distributed message passing” [40]. Celery is crucial for the system presented as it abstracts away all the intricacies of building a distributed worker system, exposing an easy to use API for writing distributed Python applications. Furthermore, there are several niceties built in, such as compatibility with many major message queues/databases (namely RabbitMQ, Redis, MongoDB) and web interfaces for monitoring performance.\nNumPy, SciPy, Pandas: NumPy [17], SciPy [18], and Pandas [19] are the foundational scientific libraries upon which most other major Python libraries are built. These libraries provide low level optimizations for scientific computation across platforms. Furthermore, adoption of a common set of libraries allows for easy interaction between different libraries. Concretely, NumPy provides basic array functionality, SciPy provides a suit of scientific functions and calculations (such as optimization and statistical operations), and Pandas provides a common interface for formatting and manipulating data. Of course, each of these does much more than these functions, and the reader is encouraged to read their respective documentations to gain a comprehensive list of the benefits for using each.\nTheano: Theano is a symbolic Python library that allows you to “define, optimize, and evaluate mathematical expressions with multi-dimensional arrays efficiently” [20], [21]. Developed at the Université de Montréal by leading DNN researchers, Theano plays a crucial role in the DNN development stack. Theano allows easy GPU integration, provides several key deep neural network functions (such as activation functions), and, because of its symbolic natural, greatly simplifies the math for the user by automatically computing complicated gradient functions, as well as other complex mathematic equations.\nPyBrain: PyBrain aims to offer a high performance neural network library without the complicated entry barrier that most users experience when trying to enter the field [41]. Developed independently at Technische Universität München, PyBrain does not rely on Theano, and therefore, does not take advantage of the GPU acceleration or optimized mathematical functions Theano provides. However, it has been selected as one of the DNN libraries because of its flexibility and ease of use.\nKeras: Keras lives at the other end of the neural network library spectrum [42]. Keras is built on top of Theano, and is closely integrated with Theano under the hood. It takes full advantage of Theano’s optimizations wherever possible, and encourages its users to use best practices and the latest DNN techniques. The barrier to learn Keras and realize its full potential is slightly higher than that of PyBrain. However, given its expressive and powerful nature, the tradeoff is well worth it.\n4"
    }, {
      "heading" : "System Design",
      "text" : "For the purposes of this discussion, there are four atomic units of functionality, all working asynchronously with each other. These units of functionality include uploading the data (see “Data Upload), initializing tasks to run for that data (see”Task Management“), executing the tasks (see”Task Execution“), and storing/visualizing the results (see”Results Storage and Visualization“). Because these units operate independently of each other, failure in the pipeline can occur in one stage of the pipeline without affecting the performance of another stage. Each stage”fails forward“, meaning that if a failure occurs, the system merely presents the appropriate error and keeps executing its next task rather than crashing. These are all principles the reader is encouraged to follow when creating their own systems."
    }, {
      "heading" : "Data Upload",
      "text" : "First, a static website was created for uploading data to the server to be processed. This server used a standard Bootstrap template and exposed an interface to upload a CSV file, since most datasets are easily represented as this type of file. Next, the Papa Parse library was used to parse the CSV file to a multidimensional JavaScript array. If Papa Parse encountered an error in parsing the file (such as incorrect format), it would through an error, which would be communicated to the user and the process would be aborted. Note: missing data was not considered an error, due to the desired compatibility with sparse datasets. If the dataset was properly formatted, the user was prompted to choose a “label” (what value of the data we would like to predict), and Papa Parse would then hand off the data the jQuery, which would attempt to upload the data to the Flask REST api. Upon success, jQuery received a “Session ID” from the server, which was a unique identifier that could be used to check on the progress/results of any given session. At this point, the function of the web interface is mostly done - if the user chooses to stay on the page, jQuery continuously checks back with the server on the progress of that session and updates a progress bar accordingly. When the current session completes, the user is presented a link to review the results visualization. Figure 2 shows the flowchart for the web interface.\nAlthough we have discussed the function of the static website, deployment of this website has not yet been discussed. This is where Docker plays a crucial role in flexible deployment. First, a Docker image was compiled from the base NGINX image. Concretely, Docker has several precompiled containers available at [43] that you can base your image off of. In this particular image, NGINX is already preconfigured and running on top of the Ubuntu operating system. Simply copy your static website files into the location specified on the NGINX container page and you have a fully portable image ready to serve your static website.\nIn this system, the Docker image serving our website is run on top of the host machine for low latency communications with the database and REST api. Port 80 of the host machine is mapped to the port that NGINX is serving the website on inside of the container, meaning anyone who visited the host machine’s public IP address would get served the website."
    }, {
      "heading" : "Tasks Management",
      "text" : "It has been hinted that the relationship between RabbitMQ and Celery is an integral part of this system. As discussed earlier, setting up a RabbitMQ server is very easy using Docker - simply run the default RabbitMQ container provided at [43] with the web interface enabled. In this system, the RabbitMQ container was run on the host machine for low latency communication with the Flask server. For the Celery instance, a separate Docker container was created on top of the default Python 2.7 container [43] that handles the running of the Flask/Celery Python microframework. Because all of these Docker containers are run on the same machine and the ports are mapped to the host machine, communication between Docker containers is seamless.\nOnce this JavaScript array has been passed off to the Flask server, the user can preprocess the data as the like to prepare it for training. This unit of functionality (as well as “Task Execution”) is where most of the “black magic” of designing DNNs comes into play. In this system, a few\n5 simple experiments were designed to test different hyperparameters and layer designs across different datasets. Although the specific of that design will be discussed here, the reader is encouraged to come up with their own rules for designing their DNNs.\nFirst, missing values were filled with zeroes (necessary for processing in neural networks). Next, some best practices for working with DNNs were implemented:\n1. All of the features (every value in the dataset except for the label) were scaled from [0..1] [25]. 2. Label data was split into categories and encoded using a technique called “One Hot Encoding”. This creates a different output node in the DNN for each categorical answer and allows the DNN to output a probability that any given categorical answer is the correct one [25]. 3. The data was split into two distinct sets: 80% training and 20% testing. Holding out some data for testing a DNN helps to alleviate overfitting [44].\nAfter the data has been preprocessed, it can be passed off to different subsystems in the “Task Execution” unit for processing by Keras and PyBrain. This is easily accomplished by creating two distinct celery tasks for training a DNN with PyBrain and Keras respectively. Depending on which library you would like to train with, you can submit jobs with varying hyper-parameters and layers for processing in this stage of the pipeline. Celery will serialize of this data into RabbitMQ and execute these training tasks as worker machines become available. Figure 3 shows the flowchart for the Task Management pipeline."
    }, {
      "heading" : "Task Execution",
      "text" : "As discussed in “Task Management”, this unit of operation simply implements a training function for either Keras/PyBrain that trains the DNN based on the parameters provided by Celery. This is a simple as writing a single method and letting Celery run these training methods as the workers become available. What is not trivial, however, is the rapid deployment on new workers into the system.\nA Docker container was built on top of the default Python 2.7 image [43] that ran an instance of a Celery worker. In this way, anytime you deploy this Docker container to a worker machine, another worker was made available in the Celery worker cluster. The key to optimizing efficiency is to take advantage of:\n1. Worker machines with GPU acceleration. 2. Worker machines with multiple cores.\nTo take advantage of multiple CPU cores, simply set the “THEANO_FLAGS” environment variable in the Docker as follows:\nTHEANO_FLAGS=device=gpu,floatX=float32\nFig. 3. Flowchart showing task management process\nYou can specific this when you are deploying on a machine you know has a GPU, or you can just always set the environment variable, as Theano recognizing when the machine you are running is not configured with a GPU.\nOne of the nice features about using a task scheduler like Celery is that is has built in multi-process support. Meaning that you can simply pass in a command line argument indicating the number of worker processes you would like when starting a Celery worker is started, and Celery will handle the logistics for you. For most use cases, a number of workers equal to the number of cores on your worker machine is recommended for optimal performance. Another alternative is to spawn multiple Docker containers with a lower amount of workers, although this approach is not quite as memory efficient due to the overhead of running multiple Ubuntu instances. Figure 4 shows the flowchart for the Task Execution process."
    }, {
      "heading" : "Results Storage and Visualization",
      "text" : "After processing by the Task Execution subsystem, results are stored in the MongoDB instance. A default MongoDB container [43] was instantiated on the host machine,\n6\nallowing for low latency communication with the Flask server and a centralized datastore. A RESTful API was used to access the MongoDB results from the client. Some information stored in the database includes the session id, the training time, the model accuracy, and the parameters used to train the model. Other niceties of this setup include a web interface for monitoring both Celery and RabbitMQ.\nFigures 5-7 show these reporting features.\nFig. 5. Example of plot.ly showing training time vs. hidden layers\nDiscussion"
    }, {
      "heading" : "Results",
      "text" : "Discussion of results is not the main focus of this paper, and research on what a systematic approach to studying DNNs is still ongoing. However, a few preliminary observations stand out for those interested:\n• Every DNN tested, on small datasets and large datasets, seem to reach a “critical mass” of information storage around 500-700 hidden layers, wherein the DNNs performance will flatline for any number of layers greater than the critical mass point. Quite probably, this is due to overfitting of the deep neural network to the training data and is not a significant finding, except to encourage designers to use as few layers as possible to avoid overfitting. Note: this premise was not exhaustively tested against all kinds of layers and activation functions, but a significant number of combinations were tried. • Training of DNN increases roughly linearly with the amount of layers added to the network for a significant number of layer combinations tried (see Figure 5). • Granular control over parameters greatly increases performance in DNNs. For instance, testing several different combinations of activation functions will produce some interesting results."
    }, {
      "heading" : "Improvements/Future Work",
      "text" : "Several improvements are possible, notably:\n• Asynchronous REST API : This focus of this experiment was to build a full stack Python system to\n7 train DNNs. However, this is generally not optimal for asynchronous applications such as a REST API. The reader is encouraged to look for alternatives outside of the Python programming language, such as NodeJS [45] or golang [46], to build their RESTful API, as Python’s blocking nature presents major speed issues.\n• Granular web interface: Rather than programming the rules for choosing training parameters directly, it is conceivable that you could build this functionality into the web user interface, allowing DNN designers not familiar with a specific library to set these training rules through a GUI."
    }, {
      "heading" : "Conclusion",
      "text" : "To conclude, a distributed, fullstack Python implementation was described in detail for examining performance results for different parameters in DNN training. This is potentially useful because the design of DNNs is more of an art than a science, so the more we can find out about how different hyper-parameters/layers affect the accuracy of a deep neural network, the better. By utilizing several deployment tools (Docker [29], CoreOS [31]), production grade distributed solutions (RabbitMQ [33], MongoDB [36]), and the Python scientific ecosystem (NumPy [17], SciPy [18], Theano [20], [21], Keras [42], PyBrain [41]), such a system is easily implemented in a flexible way that prioritizes simplicity and modularity. Some preliminary discoveries are discussed, and work to build off of the current implementation is also presented."
    } ],
    "references" : [ {
      "title" : "Big data: The next frontier for innovation, competition, and productivity",
      "author" : [ "J. Manyika", "M. Chui", "B. Brown", "J. Bughin", "R. Dobbs", "C. Roxburgh", "A.H. Byers" ],
      "venue" : "2011.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Big data: A revolution that will transform how we live, work, and think",
      "author" : [ "V. Mayer-Schönberger", "K. Cukier" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2013
    }, {
      "title" : "Big data analytics",
      "author" : [ "P. Russom", "others" ],
      "venue" : "TDWI Best Practices Report, Fourth Quarter, 2011.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "The hadoop distributed file system",
      "author" : [ "K. Shvachko", "H. Kuang", "S. Radia", "R. Chansler" ],
      "venue" : "Mass storage systems and technologies (mSST), 2010 iEEE 26th symposium on, 2010, pp. 1–10.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Spark: Cluster computing with working sets",
      "author" : [ "M. Zaharia", "M. Chowdhury", "M.J. Franklin", "S. Shenker", "I. Stoica" ],
      "venue" : "Proceedings of the 2nd uSENIX conference on hot topics in cloud computing, 2010, vol. 10, p. 10.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Genetic algorithms and machine learning",
      "author" : [ "D.E. Goldberg", "J.H. Holland" ],
      "venue" : "Machine learning, vol. 3, no. 2, pp. 95–99, 1988.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "An overview of machine learning",
      "author" : [ "J.G. Carbonell", "R.S. Michalski", "T.M. Mitchell" ],
      "venue" : "Machine learning, Springer, 1983, pp. 3–23.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1983
    }, {
      "title" : "Mitchell,Machine learning: An artificial intelligence approach",
      "author" : [ "R.S. Michalski", "J.G. Carbonell", "T. M" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2013
    }, {
      "title" : "A comprehensive foundation",
      "author" : [ "S. Haykin", "N. Network" ],
      "venue" : "Neural Networks, vol. 2, no. 2004, 2004.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Deep learning in neural networks: An overview",
      "author" : [ "J. Schmidhuber" ],
      "venue" : "CoRR, vol. abs/1404.7828, 2014.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Unsupervised feature learning and deep learning: A review and new perspectives",
      "author" : [ "Y. Bengio", "A.C. Courville", "P. Vincent" ],
      "venue" : "CoRR, vol. abs/1206.5538, 2012.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Deep learning",
      "author" : [ "Y. Bengio", "I.J. Goodfellow", "A. Courville" ],
      "venue" : "2015.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Multilayer feedforward networks are universal approximators",
      "author" : [ "K. Hornik", "M. Stinchcombe", "H. White" ],
      "venue" : "Neural networks, vol. 2, no. 5, pp. 359–366, 1989.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Multilayer feedforward networks with a nonpolynomial activation function can approximate any function",
      "author" : [ "M. Leshno", "V.Y. Lin", "A. Pinkus", "S. Schocken" ],
      "venue" : "Neural networks, vol. 6, no. 6, pp. 861–867, 1993.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "NumPy GitHub repository",
      "author" : [ "N. team" ],
      "venue" : "GitHub repository. GitHub, 2015.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Data structures for statistical computing in python",
      "author" : [ "W. McKinney" ],
      "venue" : "Proceedings of the 9th python in science conference, 2010, pp. 51–56.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Theano: New features and speed improvements.",
      "author" : [ "F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I.J. Goodfellow", "A. Bergeron", "N. Bouchard", "Y. Bengio" ],
      "venue" : "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2012
    }, {
      "title" : "Theano: A CPU and GPU math expression compiler",
      "author" : [ "J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio" ],
      "venue" : "Proceedings of the python for scientific computing conference (SciPy), 2010.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Global interpreter lock documentation",
      "author" : [ "P. Foundation" ],
      "venue" : "2015. [Online]. Available: https://wiki.python.org/ moin/GlobalInterpreterLock. [Accessed: 23-Oct-2015].",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "PyData and the gIL",
      "author" : [ "M. Rocklin" ],
      "venue" : "2015.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Understanding the python gIL.",
      "author" : [ "D. Beazley" ],
      "venue" : null,
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2010
    }, {
      "title" : "A practical guide to training restricted boltzmann machines",
      "author" : [ "G. Hinton" ],
      "venue" : "Momentum, vol. 9, no. 1, p. 926, 2010.  8",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Practical recommendations for gradientbased training of deep architectures",
      "author" : [ "Y. Bengio" ],
      "venue" : "Neural networks: Tricks of the trade, Springer, 2012, pp. 437–478.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Improving neural networks by preventing co-adaptation of feature detectors",
      "author" : [ "G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov" ],
      "venue" : "arXiv preprint arXiv:1207.0580, 2012.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "GPU computing",
      "author" : [ "J.D. Owens", "M. Houston", "D. Luebke", "S. Green", "J.E. Stone", "J.C. Phillips" ],
      "venue" : "Proceedings of the IEEE, vol. 96, no. 5, pp. 879–899, 2008.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Docker: Lightweight linux containers for consistent development and deployment",
      "author" : [ "D. Merkel" ],
      "venue" : "Linux J., vol. 2014, no. 239, Mar. 2014.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Docker documentation",
      "author" : [ "D. Team" ],
      "venue" : "2015. [Online]. Available: http://docs.docker.com. [Accessed: 24-Oct-2015].",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "CoreOS website",
      "author" : [ "C. Team" ],
      "venue" : "2015. [Online]. Available: https://coreos.com/. [Accessed: 24-Oct-2015].",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "CoreOS GitHub repository",
      "author" : [ "C. team" ],
      "venue" : "GitHub repository. https://github.com/coreos/etcd; GitHub, 2015.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "RabbitMQ website",
      "author" : [ "R. Team" ],
      "venue" : "2015. [Online]. Available: https://www.rabbitmq.com/. [Accessed: 24-Oct- 2015].",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Remote queues: Exposing message queues for optimization and atomicity",
      "author" : [ "E.A. Brewer", "F.T. Chong", "L.T. Liu", "S.D. Sharma", "J.D. Kubiatowicz" ],
      "venue" : "Proceedings of the seventh annual aCM symposium on parallel algorithms and architectures, 1995, pp. 42–53.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Redis website",
      "author" : [ "R. Team" ],
      "venue" : "2015. [Online]. Available: https://www.redis.io/. [Accessed: 24-Oct-2015].",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "MongoDB website",
      "author" : [ "M. Team" ],
      "venue" : "2015. [Online]. Available: https://www.mongodb.com/. [Accessed: 24-Oct- 2015].",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Cassandra vs mongoDB vs couchDB vs redis vs riak vs hBase vs couchbase vs orientDB vs aerospike vs neo4j vs hypertable vs elasticSearch vs accumulo vs voltDB vs scalaris vs rethinkDB comparison",
      "author" : [ "K. Kovacs" ],
      "venue" : "2015. [Online]. Available: http://kkovacs.eu/ cassandra-vs-mongodb-vs-couchdb-vs-redis. [Accessed: 24- Oct-2015].",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "REST aPI design rulebook",
      "author" : [ "M. Masse" ],
      "venue" : null,
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2011
    }, {
      "title" : "PyBrain",
      "author" : [ "T. Schaul", "J. Bayer", "D. Wierstra", "Y. Sun", "M. Felder", "F. Sehnke", "T. Rückstieß", "J. Schmidhuber" ],
      "venue" : "Journal of Machine Learning Research, vol. 11, pp. 743–746, 2010.",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Keras website",
      "author" : [ "F. et a. Chollet" ],
      "venue" : "2015. [Online]. Available: http://keras.io. [Accessed: 24-Oct-2015].",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Docker hub",
      "author" : [ "D. Team" ],
      "venue" : "2015. [Online]. Available: http://hub.docker.com. [Accessed: 25-Oct-2015].",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Neural network studies. 1. comparison of overfitting and overtraining",
      "author" : [ "I.V. Tetko", "D.J. Livingstone", "A.I. Luik" ],
      "venue" : "Journal of chemical information and computer sciences, vol. 35, no. 5, pp. 826–833, 1995.",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "NodeJS website",
      "author" : [ "N. team" ],
      "venue" : "2015. [Online]. Available: http://nodejs.org/. [Accessed: 25-Oct-2015].",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Golang website",
      "author" : [ "Google" ],
      "venue" : "2015. [Online]. Available: http://golang.org/. [Accessed: 25-Oct-2015].",
      "citeRegEx" : "46",
      "shortCiteRegEx" : null,
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "In recent years, the amount of data that is produced annually has increased dramatically [1], [2].",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 1,
      "context" : "In recent years, the amount of data that is produced annually has increased dramatically [1], [2].",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 2,
      "context" : "The availability of this data has prompted researchers to publish large amounts of literature that study how we can make sense of this data, and, in turn, how can we handle this data in a computationally efficient manner [3], [4], [5], [6].",
      "startOffset" : 221,
      "endOffset" : 224
    }, {
      "referenceID" : 3,
      "context" : "The availability of this data has prompted researchers to publish large amounts of literature that study how we can make sense of this data, and, in turn, how can we handle this data in a computationally efficient manner [3], [4], [5], [6].",
      "startOffset" : 231,
      "endOffset" : 234
    }, {
      "referenceID" : 4,
      "context" : "The availability of this data has prompted researchers to publish large amounts of literature that study how we can make sense of this data, and, in turn, how can we handle this data in a computationally efficient manner [3], [4], [5], [6].",
      "startOffset" : 236,
      "endOffset" : 239
    }, {
      "referenceID" : 5,
      "context" : "high processing power [7], [8].",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 6,
      "context" : "high processing power [7], [8].",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 7,
      "context" : "For a complete overview of ML and the different algorithms involved, please refer to [9].",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 8,
      "context" : "One particularly good ML algorithm for working with large amounts of data is a specific subset of Artificial Neural Networks (ANNs) [10], [11], called Deep Neural Networks (DNNs) [12], [13], [14].",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 9,
      "context" : "One particularly good ML algorithm for working with large amounts of data is a specific subset of Artificial Neural Networks (ANNs) [10], [11], called Deep Neural Networks (DNNs) [12], [13], [14].",
      "startOffset" : 179,
      "endOffset" : 183
    }, {
      "referenceID" : 10,
      "context" : "One particularly good ML algorithm for working with large amounts of data is a specific subset of Artificial Neural Networks (ANNs) [10], [11], called Deep Neural Networks (DNNs) [12], [13], [14].",
      "startOffset" : 185,
      "endOffset" : 189
    }, {
      "referenceID" : 11,
      "context" : "One particularly good ML algorithm for working with large amounts of data is a specific subset of Artificial Neural Networks (ANNs) [10], [11], called Deep Neural Networks (DNNs) [12], [13], [14].",
      "startOffset" : 191,
      "endOffset" : 195
    }, {
      "referenceID" : 12,
      "context" : "DNNs work well with large sets of data because of their peculiar quality of being a universal approximator [15], [16], which means that theoretically speaking, they can model any real function.",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 13,
      "context" : "DNNs work well with large sets of data because of their peculiar quality of being a universal approximator [15], [16], which means that theoretically speaking, they can model any real function.",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 14,
      "context" : "Another key factor in Python’s success in the DNN research space is due to it’s portability and the large amount of scientific libraries that are actively being developed, such as Numpy [17], Scipy [18], Pandas [19], and Theano [20], [21].",
      "startOffset" : 186,
      "endOffset" : 190
    }, {
      "referenceID" : 15,
      "context" : "Another key factor in Python’s success in the DNN research space is due to it’s portability and the large amount of scientific libraries that are actively being developed, such as Numpy [17], Scipy [18], Pandas [19], and Theano [20], [21].",
      "startOffset" : 211,
      "endOffset" : 215
    }, {
      "referenceID" : 16,
      "context" : "Another key factor in Python’s success in the DNN research space is due to it’s portability and the large amount of scientific libraries that are actively being developed, such as Numpy [17], Scipy [18], Pandas [19], and Theano [20], [21].",
      "startOffset" : 228,
      "endOffset" : 232
    }, {
      "referenceID" : 17,
      "context" : "Another key factor in Python’s success in the DNN research space is due to it’s portability and the large amount of scientific libraries that are actively being developed, such as Numpy [17], Scipy [18], Pandas [19], and Theano [20], [21].",
      "startOffset" : 234,
      "endOffset" : 238
    }, {
      "referenceID" : 18,
      "context" : "Global Interpreter Lock: One of the major roadblocks for implementing massively parallel systems in Python in the existence of the Global Interpreter Lock (GIL) [22].",
      "startOffset" : 161,
      "endOffset" : 165
    }, {
      "referenceID" : 19,
      "context" : "Concretely, this means that means that no two Python threads can access a python object at the same time, which allows for concurrency, but not parallelism [23].",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 20,
      "context" : "This restriction greatly reduces Python’s ability to utilize multiple CPUs [24].",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 19,
      "context" : "Python scientific libraries and their reliance on the GIL [23]",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 21,
      "context" : "• Best DNN layer design: Although some literature exists on the practical design of deep neural networks [25], [26], [27], designing deep neural networks remains a mix of following these guidelines and mastering the “black art” of designing deep neural networks.",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 22,
      "context" : "• Best DNN layer design: Although some literature exists on the practical design of deep neural networks [25], [26], [27], designing deep neural networks remains a mix of following these guidelines and mastering the “black art” of designing deep neural networks.",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 23,
      "context" : "• Best DNN layer design: Although some literature exists on the practical design of deep neural networks [25], [26], [27], designing deep neural networks remains a mix of following these guidelines and mastering the “black art” of designing deep neural networks.",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 3,
      "context" : "If your dataset is larger than 1TB, the reader is encouraged to use a different system, such as Hadoop [5].",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 24,
      "context" : "• GPU acceleration: Although not true for every machine, some machines are equipped with graphic processing units (GPUs), which can be utilized to greatly decrease running time for training DNNs [28].",
      "startOffset" : 195,
      "endOffset" : 199
    }, {
      "referenceID" : 25,
      "context" : "Docker: Docker is a lightweight Linux container system that allows you to compile a specific Linux image once and deploy that image to multiple destinations with ease [29].",
      "startOffset" : 167,
      "endOffset" : 171
    }, {
      "referenceID" : 26,
      "context" : "For more information on configuring Docker or its capabilities, visit [30].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 27,
      "context" : "CoreOS: CoreOS is a lightweight, flexible operating system that runs containers on top of it [31].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 28,
      "context" : "In addition, the CoreOS team has built a painless distributed key-value store called etcd [32] that comes",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 29,
      "context" : "RabbitMQ: RabbitMQ is a robust message queue implementation that is easy, portable, and supports a number of messaging protocols, including the popular AMPQ protocol [33].",
      "startOffset" : 166,
      "endOffset" : 170
    }, {
      "referenceID" : 30,
      "context" : "Message queues [34] are crucial when building a distributed computing platform utilizing Python, because it provides a centralized pool of tasks for each of your worker machines to pull from.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 31,
      "context" : "However, other similar task storing systems, such as Redis [35], are worth mentioning.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 32,
      "context" : "MongoDB: MongoDB is a NoSQL database that offers easy storage and installation [36].",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 33,
      "context" : "A comprehensive comparison of databases and their trade offs can be found in [37].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 34,
      "context" : "In this system, Flask acts as the middleman between the website and the database by exposing a REST api [39] (see “System Design”).",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 14,
      "context" : "NumPy, SciPy, Pandas: NumPy [17], SciPy [18], and Pandas [19] are the foundational scientific libraries upon which most other major Python libraries are built.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 15,
      "context" : "NumPy, SciPy, Pandas: NumPy [17], SciPy [18], and Pandas [19] are the foundational scientific libraries upon which most other major Python libraries are built.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 16,
      "context" : "Theano: Theano is a symbolic Python library that allows you to “define, optimize, and evaluate mathematical expressions with multi-dimensional arrays efficiently” [20], [21].",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 17,
      "context" : "Theano: Theano is a symbolic Python library that allows you to “define, optimize, and evaluate mathematical expressions with multi-dimensional arrays efficiently” [20], [21].",
      "startOffset" : 169,
      "endOffset" : 173
    }, {
      "referenceID" : 35,
      "context" : "PyBrain: PyBrain aims to offer a high performance neural network library without the complicated entry barrier that most users experience when trying to enter the field [41].",
      "startOffset" : 169,
      "endOffset" : 173
    }, {
      "referenceID" : 36,
      "context" : "Keras: Keras lives at the other end of the neural network library spectrum [42].",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 37,
      "context" : "Concretely, Docker has several precompiled containers available at [43] that you can base your image off of.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 37,
      "context" : "As discussed earlier, setting up a RabbitMQ server is very easy using Docker - simply run the default RabbitMQ container provided at [43] with the web interface enabled.",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 37,
      "context" : "[43] that handles the running of the Flask/Celery Python microframework.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "1] [25].",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 21,
      "context" : "This creates a different output node in the DNN for each categorical answer and allows the DNN to output a probability that any given categorical answer is the correct one [25].",
      "startOffset" : 172,
      "endOffset" : 176
    }, {
      "referenceID" : 38,
      "context" : "Holding out some data for testing a DNN helps to alleviate overfitting [44].",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 37,
      "context" : "7 image [43] that ran an instance of a Celery worker.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 37,
      "context" : "A default MongoDB container [43] was instantiated on the host machine,",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 39,
      "context" : "The reader is encouraged to look for alternatives outside of the Python programming language, such as NodeJS [45] or golang [46], to build their RESTful API, as Python’s blocking nature presents major speed issues.",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 40,
      "context" : "The reader is encouraged to look for alternatives outside of the Python programming language, such as NodeJS [45] or golang [46], to build their RESTful API, as Python’s blocking nature presents major speed issues.",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 25,
      "context" : "By utilizing several deployment tools (Docker [29], CoreOS [31]), production grade distributed solutions (RabbitMQ [33], MongoDB [36]), and the Python scientific ecosystem (NumPy [17], SciPy [18], Theano [20], [21], Keras [42], PyBrain [41]), such a system is easily implemented in a flexible way that prioritizes simplicity and modularity.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 27,
      "context" : "By utilizing several deployment tools (Docker [29], CoreOS [31]), production grade distributed solutions (RabbitMQ [33], MongoDB [36]), and the Python scientific ecosystem (NumPy [17], SciPy [18], Theano [20], [21], Keras [42], PyBrain [41]), such a system is easily implemented in a flexible way that prioritizes simplicity and modularity.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 29,
      "context" : "By utilizing several deployment tools (Docker [29], CoreOS [31]), production grade distributed solutions (RabbitMQ [33], MongoDB [36]), and the Python scientific ecosystem (NumPy [17], SciPy [18], Theano [20], [21], Keras [42], PyBrain [41]), such a system is easily implemented in a flexible way that prioritizes simplicity and modularity.",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 32,
      "context" : "By utilizing several deployment tools (Docker [29], CoreOS [31]), production grade distributed solutions (RabbitMQ [33], MongoDB [36]), and the Python scientific ecosystem (NumPy [17], SciPy [18], Theano [20], [21], Keras [42], PyBrain [41]), such a system is easily implemented in a flexible way that prioritizes simplicity and modularity.",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 14,
      "context" : "By utilizing several deployment tools (Docker [29], CoreOS [31]), production grade distributed solutions (RabbitMQ [33], MongoDB [36]), and the Python scientific ecosystem (NumPy [17], SciPy [18], Theano [20], [21], Keras [42], PyBrain [41]), such a system is easily implemented in a flexible way that prioritizes simplicity and modularity.",
      "startOffset" : 179,
      "endOffset" : 183
    }, {
      "referenceID" : 16,
      "context" : "By utilizing several deployment tools (Docker [29], CoreOS [31]), production grade distributed solutions (RabbitMQ [33], MongoDB [36]), and the Python scientific ecosystem (NumPy [17], SciPy [18], Theano [20], [21], Keras [42], PyBrain [41]), such a system is easily implemented in a flexible way that prioritizes simplicity and modularity.",
      "startOffset" : 204,
      "endOffset" : 208
    }, {
      "referenceID" : 17,
      "context" : "By utilizing several deployment tools (Docker [29], CoreOS [31]), production grade distributed solutions (RabbitMQ [33], MongoDB [36]), and the Python scientific ecosystem (NumPy [17], SciPy [18], Theano [20], [21], Keras [42], PyBrain [41]), such a system is easily implemented in a flexible way that prioritizes simplicity and modularity.",
      "startOffset" : 210,
      "endOffset" : 214
    }, {
      "referenceID" : 36,
      "context" : "By utilizing several deployment tools (Docker [29], CoreOS [31]), production grade distributed solutions (RabbitMQ [33], MongoDB [36]), and the Python scientific ecosystem (NumPy [17], SciPy [18], Theano [20], [21], Keras [42], PyBrain [41]), such a system is easily implemented in a flexible way that prioritizes simplicity and modularity.",
      "startOffset" : 222,
      "endOffset" : 226
    }, {
      "referenceID" : 35,
      "context" : "By utilizing several deployment tools (Docker [29], CoreOS [31]), production grade distributed solutions (RabbitMQ [33], MongoDB [36]), and the Python scientific ecosystem (NumPy [17], SciPy [18], Theano [20], [21], Keras [42], PyBrain [41]), such a system is easily implemented in a flexible way that prioritizes simplicity and modularity.",
      "startOffset" : 236,
      "endOffset" : 240
    } ],
    "year" : 2015,
    "abstractText" : "In this paper, a framework for testing Deep Neural Network (DNN) design in Python is presented. First, big data, machine learning (ML), and Artificial Neural Networks (ANNs) are discussed to familiarize the reader with the importance of such a system. Next, the benefits and detriments of implementing such a system in Python are presented. Lastly, the specifics of the system are explained, and some experimental results are presented to prove the effectiveness of the system.",
    "creator" : "LaTeX with hyperref package"
  }
}