{
  "name" : "1612.00913.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "END-TO-END JOINT LEARNING OF NATURAL LANGUAGE UNDERSTANDING AND DIALOGUE MANAGER",
    "authors" : [ "Xuesong Yang", "Yun-Nung Chen", "Dilek Hakkani-Tür", "Paul Crook", "Xiujun Li", "Jianfeng Gao", "Li Deng" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n61 2.\n00 91\n3v 1\n[ cs\n.C L\n] 3\nD ec\nIndex Terms— language understanding, spoken dialogue systems, end-to-end, dialogue manager, deep learning"
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "Recent progress of designing conversational agents for commercial purposes, e.g. Microsoft’s Cortana, Apple’s Siri, and Amazon’s Echo, has attracted more attention from both academia and industry. The essential components of these conversational agents are natural language understanding (NLU) and dialog manager (DM). NLU typically detects dialog domains by parsing user utterances followed by user intent classification and filling associated slots according to a domain-specific semantic template [1]; DM keeps monitoring the belief distribution over all possible user states underlying current user behaviors and predicts system actions as responses [2, 3]. For example, the intent request movie and slots genre and date are predicted by NLU given an user utterance “any action movies recommended this weekend?”. The system action request location is predicted by DM.\nTraditional approaches for NLU usually model tasks of domain/intent classification and slot filling separately.\nSequential labeling methods, e.g. hidden Markov models (HMMs) and conditional random field (CRF) [4–6] are widely used in slot tagging tasks; maximum entropy and support vector machines with linear kernel (LinearSVM) [7, 8] are applied to user intent prediction. These models highly rely on careful feature engineering that is laborious and timeconsuming. Deep learning techniques making incredible progress on learning expressive feature representations have achieved better solutions to NLU modeling in ATIS domain [9–12]. The performance was improved significantly by incorporating recurrent neural networks (RNN) and CRF model [13–15]. Convolutional neural networks are also used for domain/intent classification [16, 17].\nSlot tags and intents, as semantics representations of user behaviors, may share knowledge with each other such that separate modeling of these two tasks is constrained to take full advantage of all supervised signals. Flexible architectures of neural networks provide a way of jointly training with intent classification and slot filling [18, 19]. Contextual information of previous queries and domain/intent prediction was also incorporated into RNN structures [20, 21].\nInformation flows from NLU to DM, and noisy outputs of NLU are apt to transfer errors to the following DM, so that it brings in challenges for monitoring the belief distribution and predicting system actions. Most successful approaches cast the dialog manager as a partially observable Markov decision process [2], which uses hand-crafted features to represent the state and action space, and requires a large number of annotated conversations [22] or human interactions [23, 24]. Converting these methods into practice is far from trivial, and exact policy learning is computational intractable. Therefore, they are constrained in narrow domains.\nIn order to address the above problems, we propose an end-to-end deep RNN with limited contextual dialog memory that can be jointly trained by three supervised signals– user slot tagging, intent prediction and system action prediction (SAP). Our model expresses superb advantages in natural language understanding and dialog manager. Highly expressive feature representations beyond conventional aggregation\nof slot tags and intents are expected to be captured in our joint model, such that the affects of noisy output from NLU can be mitigated. Extra supervised signal from system actions is capable of refining NLU model by back-propagating the associated error gradients."
    }, {
      "heading" : "2. END-TO-END JOINT MODEL",
      "text" : "The joint model can be considered as a SAP model stacked on top of a history of NLU models in Figure 3. NLU model is designed by sharing bi-directional long short-term memory (biLSTM) layers with slot tagging and intent prediction."
    }, {
      "heading" : "2.1. Sequence to Sequence Model with biLSTM Cells",
      "text" : "Given a sequence of input vectors x={ xt } T\n1 , a recurrent unit H computes a sequence of hidden vectorsh={ ht } T\n1 , and output sequence y={ yt } T 1 by iterating the following equations,\nht = H (xt, ht−1) = σ (Wxhxt + Uhhht−1)\nŷt = argmax (softmax (Whyht))\nwhere softmax (zm) = ezm/ ∑ i e zi , σ is a activation function, and Wxh, Uhh and Why are weight matrices. The goal\nLSTM\nutterj-3\nLSTM\nNLU\nLSTM\nutterj-2\nLSTM\nNLU\nLSTM\nutterj-1\nLSTM\nNLU\nLSTM\nutterj\nLSTM\nNLU\nSigmoid\nSystem Actions at j+1\nIntents\nSlot Tagging\nIntents\nSlot Tagging Slot Tagging Slot Tagging\nIntents Intents\nFig. 3: Proposed End-to-End Joint Model\nof sequence to sequence model (Seq2Seq) is to estimate the conditional probability p (y|x) = ∏T\nt=1 p (yt|x) such that the distance between predicted distribution p (ŷt|x) and target distribution q (yt|x) is minimized, namely,\nloss = −\nT ∑\nt=1\nM ∑\nz=1\nq (yt = z|x) log p (ŷt = z|x)\nwhere M denotes the size of output label set. This sequential model can be trained using backpropagation. LSTM cells are used as recurrent units because LSTM can mitigate problems of vanishing or exploding gradients in long-term dependencies via self-regularization [25]. The recurrent unit H can be further expanded as,\nht = H (xt, ht−1) = ot ◦ tanh (ct)\nct = ft ◦ ct−1 + it ◦ gt\not = sigm (Wxoxt + Uhoht−1) , it = sigm (Wxixt + Uhiht−1)\nft = sigm (Wxfxt + Uhfht−1) , gt = tanh (Wxgxt + Uhght−1)\nwhere the sigmoid functions sigm and tanh are applied element-wise, and ◦ denotes element-wise product. Since preceding and following lexical contexts are important in analysis of user utterances, bi-directional LSTM cells (biLSTM) [26] is used. The idea is that sequence x and its reverse sequence go through LSTM layer individually, and their forward hidden output −→ h and backward output ←− h are concatenated together, namely,\n−→ h t = H ( xt, −→ h t−1 ) , ←− h t = H ( xt, ←− h t+1 )\nŷt = argmax ( softmax (−→ Why −→ h t + ←− Why ←− h t ))\nwhere −→ Why and ←− Why are bi-directional weight matrices."
    }, {
      "heading" : "2.2. Joint Modeling",
      "text" : "The proposed joint model is a RNN classifier that utilizes bidirectional LSTM cells H, which takes as inputs I − 1 his-\ntory of current hidden outputs h(nlu)j = { h (nlu) i }\nj j−I+1 from\nNLU units and performs one-vs-all binary classifications for SAP at the output layer (see Figure 3), in other word, −→ h\n(act) i = H\n(\nh (nlu) i ,\n−→ h\n(act) i−1\n) , ←− h\n(act) i = H\n(\nh (nlu) i ,\n←− h\n(act) i+1\n)\np(act) = sigm (−→ W\n(act) hy\n−→ h\n(act) j +\n←− W\n(act) hy\n←− h\n(act) j\n)\nŷ (act) k =\n{\n1, p (act) k ≥ threshold 0, otherwise\nwhere k ∈ [1,K] denotes the index of system action labels. NLU model at the i-th history is considered as a multi-task joint model with shared biLSTM layers for two tasks, where it takes as inputs a sequence of word vectors w = {wt } T\n1 , and performs Seq2Seq for slot tagging and one-vs-all binary classifications for intent prediction (see Figure 1). The biLSTM architecture mentioned in Section 2.1 can be directly applied to slot tagging task with the size M of user slot tags, −→ h\n1(i) t = H\n( wt, −→ h 1(i) t−1 ) , ←− h 1(i) t = H ( wt, ←− h 1(i) t+1 )\nŷ (tagi) t = argmax\n(\nsoftmax (−→ W\n(tag) hy\n−→ h\n1(i) t +\n←− W\n(tag) hy\n←− h\n1(i) t\n))\nwhere −→ h\n1(i) t and\n←− h\n1(i) t denotes hidden outputs of the shared\nforward and backward layers. As for intent prediction task in NLU, we add one more recurrent LSTM layer on top of biLSTM layers and only consider last hidden vector h2(inti)T as the output of this second recurrent layer. In human-human dialogs, there exists various number of intents in correspondence to a single user utterance. We therefore design a set of one-vs-all binary classifiers in the output layer where each neuron is activated using a sigmoid function. The positive label is predicted if the probability is no less than the threshold,\nh 2(inti) t = H\n(\nh 2(inti) t−1 ,\n−→ h\n1(i) t ,\n←− h\n1(i) t\n)\np(inti) = sigm (\nW 2(int) hy h 2(inti) T\n)\nŷ(inti)n =\n{\n1, p (inti) n ≥ threshold 0, otherwise\nwhere n ∈ [1, N ] denotes the index of intent labels. Calculating hidden vectors h(nlu)i chooses similar architecture with intent model except the size of hidden vectors,\nh (nlu) i = h 2(nlui) T , h 2(nlui) t = H\n(\nh 2(nlui) t−1 ,\n−→ h\n1(i) t ,\n←− h\n1(i) t\n)\nThe goal of end-to-end joint training is to estimate the conditional probability given a history of word vectors w = {w(i) } I\n1, such that loss = l (act) + l(tag) + l(int)\nis minimized, where l(act) = − K ∑\nk=1\n1 ∑\nz=0\nq (\ny (act) k\n= z | w ) log p (\nŷ (act) k\n= z | w )\nl(tag) = − I ∑\ni=1\nT ∑\nt=1\nM ∑\nz=1\nq (\ny (tagi) t = z|w\n(i) ) log p (\nŷ (tagi) t = z|w\n(i) )\nl(int) = − I ∑\ni=1\nN ∑\nn=1\n1 ∑\nz=0\nq ( y (inti) n = z | w (i) ) log p ( ŷ (inti) n = z | w (i) )\nTable 1: Statistics of data set used in experiments.\n#utters #words #tags #intents #actions\ntrain 5,648 2,252 87 68 66 dev 1,939 1,367 79 54 53 test 3,178 1,752 75 58 58"
    }, {
      "heading" : "3. EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "3.1. Training Configurations",
      "text" : "For the training procedure, we choose a mini-batch stochastic gradient descent method Adam [27] with the batch size of 32 examples. The size of each hidden recurrent layer is 256, and the size of output vector of NLU units is M+N . We assume the joint model can only get access to previous history with I = 5. The dimension of word embeddings is 512. Dropout rate is 0.5. We apply 300 training epochs and do not use any early stop strategy. The model is evaluated on dev set every epoch. Best models for three tasks are selected individually under different metrics. Token-level micro-average F1 score is used for slot filling; frame-level accuracy (it counts only when the whole frame parse is correct) is used for both user intent prediction and system action prediction, of which the decision thresholds are tuned on dev set as well."
    }, {
      "heading" : "3.2. Corpus",
      "text" : "We choose DSTC4 corpus1 for experiments. This corpus collected the human-human dialogs of tourist information for Singapore from Skype calls that spanned five domains– accommodation, attraction, food, shopping, and transportation. Each tourist and guide tend to be expressed in a series of multiple turns. The guide is defined as the system in this paper. We transform raw data into examples that fit our experiments. Each example includes an user utterance and its associated slot tags in IOB format [28], user intents, and system actions as the responses. Labels of system actions are defined as the concatenation of categories and attributes of speech acts, e.g. QST WHEN. We also add NULL as a waiting response from guides when they are expressed in multiple turns. The consecutive guide actions in response to a single tourist utterance is merged as multiple labels. The whole corpus is split into train/dev/test as shown in Table 1. Unseen words, user intents, slot tags, and system actions in the dev/test set are categorized as UNK."
    }, {
      "heading" : "3.3. Evaluation Results",
      "text" : "We compare our proposed joint model with following models for three tasks: slot filling, intent prediction and system action prediction.\n1http://www.colips.org/workshop/dstc4/data.html\nEvaluation results of end-to-end models are illustrated in Table 3. Our proposed joint model outperforms all other endto-end models in frame-level accuracy by a large margin. The joint model and biLSTMs pipeline achieved absolute increase over baseline with 15.03% and 4.25%, respectively. Both models beat the SVMs oracle scores. The biLSTMs pipeline model get worse than biLSTM oracle as expected since it transfer the errors from NLU to the SAP model. Nevertheless, the joint model obtains 10.88% increase than pipeline model and 3.17% than biLSTM oracle. These promising improvements indicate that joint training can mitigate the downside\nof pipeline model in that the hidden outputs from a history of NLU units capture highly expressive feature representations than the conventional aggregation of user intents and slot tags. In comparison of these two oracle models, the large improvement (12.02%) for biLSTM model indicates that the contextual user turns make significant contribution to system action prediction. In real human interaction scenarios, frame-level metrics are far more important than token-level ones especially for these multi-label classification tasks since predicting precise number of labels is more challenging.\nEvaluation results of NLU models that are frozen as independent models are illustrated in Table 2. Baseline using CRF and SVMs still maintains a strong frame-level accuracy with 33.13%, however, biLSTM models taken from pipeline and joint model achieve better increase 3.25% and 4.25%, respectively. This observation indicates that joint training with two tasks of slot filling and intent prediction captures implicit knowledge underlying the shared user utterances, while another supervised signal from system actions is capable of refining the biLSTM based model by back-propagating the associated error gradients. Best accuracy at frame-level for slot filling task is obtained by traditional CRF baseline with only lexical features of words, and our biLSTM models fall behind with absolute decrease 0.47% and 0.82%. Best frame accuracy for intent prediction task is achieved by our proposed model with 5.21% improvement."
    }, {
      "heading" : "4. CONCLUSION",
      "text" : "We proposed an end-to-end deep recurrent neural network with limited contextual dialog memory that can be jointly trained by three supervised signals of user slot filling, intent prediction and system action prediction. Experiments on multi-domain human-human dialogs demonstrated that our proposed model expressed superb advantages in natural language understanding and dialog manager. It achieved better frame-level accuracy significantly than the state of the art that pipelines separate models of NLU and SAP together. The promising performance illustrated that contextual dialog memory made significant contribution to dialog manager, and highly expressive feature representations beyond conventional aggregation of slot tags and intents could be captured in our joint model such that the affects of noisy output from NLU were mitigated. Extra supervised signal from system actions is capable of refining NLU model by back-propagating."
    } ],
    "references" : [ {
      "title" : "Spoken language understanding: Systems for extracting semantic information from speech",
      "author" : [ "Gokhan Tür", "Renato De Mori" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2011
    }, {
      "title" : "Pomdp-based statistical spoken dialog systems: A review",
      "author" : [ "Steve Young", "Milica Gašić", "Blaise Thomson", "Jason D Williams" ],
      "venue" : "the IEEE, vol. 101, no. 5, pp. 1160–1179, 2013.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "End-toend reinforcement learning of dialogue agents for information access",
      "author" : [ "Bhuwan Dhingra", "Lihong Li", "Xiujun Li", "Jianfeng Gao", "Yun-Nung Chen", "Faisal Ahmed", "Li Deng" ],
      "venue" : "arXiv preprint arXiv:1609.00777, 2016.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Spoken language understanding",
      "author" : [ "Ye-Yi Wang", "Li Deng", "Alex Acero" ],
      "venue" : "IEEE Signal Processing Magazine, vol. 22, no. 5, pp. 16–31, 2005.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Generative and discriminative algorithms for spoken language understanding",
      "author" : [ "Christian Raymond", "Giuseppe Riccardi" ],
      "venue" : "INTERSPEECH, 2007.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "John Lafferty", "Andrew McCallum", "Fernando Pereira" ],
      "venue" : "ICML, 2001, vol. 1, pp. 282–289.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Optimizing SVMs for complex call classification",
      "author" : [ "Patrick Haffner", "Gokhan Tur", "Jerry H Wright" ],
      "venue" : "ICASSP. IEEE, 2003, vol. 1, pp. I–632.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Speech utterance classification",
      "author" : [ "Ciprian Chelba", "Milind Mahajan", "Alex Acero" ],
      "venue" : "ICASSP, 2003.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Deep belief nets for natural language callrouting",
      "author" : [ "Ruhi Sarikaya", "Geoffrey E Hinton", "Bhuvana Ramabhadran" ],
      "venue" : "ICASSP, 2011.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Application of deep belief networks for natural language understanding",
      "author" : [ "Ruhi Sarikaya", "Geoffrey E Hinton", "Anoop Deoras" ],
      "venue" : "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 22, no. 4, pp. 778–784, 2014.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Knowledge as a teacher: Knowledge-guided structural attention networks",
      "author" : [ "Yun-Nung Chen", "Dilek Hakkani-Tur", "Gokhan Tur", "Asli Celikyilmaz", "Jianfeng Gao", "Li Deng" ],
      "venue" : "arXiv preprint arXiv:1609.03286, 2016.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Syntax or semantics? knowledge-guided joint semantic frame parsing",
      "author" : [ "Yun-Nung Chen", "Dilek Hakkani-Tür", "Gokan Tur", "Asli Celikyilmaz", "Jianfeng Gao", "Li Deng" ],
      "venue" : "SLT, 2016.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Spoken language understanding using long short-term memory neural networks",
      "author" : [ "Kaisheng Yao", "Baolin Peng", "Yu Zhang", "Dong Yu", "Geoffrey Zweig", "Yangyang Shi" ],
      "venue" : "SLT, 2014.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Using recurrent neural networks for slot filling in spoken language understanding",
      "author" : [ "Grégoire Mesnil", "Yann Dauphin", "Kaisheng Yao", "Yoshua Bengio", "Li Deng", "Dilek Hakkani-Tur", "Xiaodong He", "Larry Heck", "Gokhan Tur", "Dong Yu" ],
      "venue" : "IEEE/ACM Transactions on Au-  dio, Speech, and Language Processing, vol. 23, no. 3, pp. 530–539, 2015.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Recurrent conditional random field for language understanding",
      "author" : [ "Kaisheng Yao", "Baolin Peng", "Geoffrey Zweig", "Dong Yu", "Xiaolong Li", "Feng Gao" ],
      "venue" : "ICASSP, 2014.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Contextual domain classification in spoken language understanding systems using recurrent neural network",
      "author" : [ "Puyang Xu", "Ruhi Sarikaya" ],
      "venue" : "ICASSP, 2014.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Towards deeper understanding: Deep convex networks for semantic utterance classification",
      "author" : [ "Gokhan Tur", "Li Deng", "Dilek Hakkani-Tür", "Xiaodong He" ],
      "venue" : "ICASSP, 2012.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Easy contextual intent prediction and slot detection",
      "author" : [ "Aditya Bhargava", "Asli Celikyilmaz", "Dilek Hakkani-Tür", "Ruhi Sarikaya" ],
      "venue" : "ICASSP, 2013.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Convolutional neural network based triangular CRF for joint intent detection and slot filling",
      "author" : [ "Puyang Xu", "Ruhi Sarikaya" ],
      "venue" : "ASRU, 2013.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Contextual spoken language understanding using recurrent neural networks",
      "author" : [ "Yangyang Shi", "Kaisheng Yao", "Hu Chen", "Yi-Cheng Pan", "Mei-Yuh Hwang", "Baolin Peng" ],
      "venue" : "ICASSP, 2015.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "End-to-end memory networks with knowledge carryover for multi-turn spoken language understanding",
      "author" : [ "Yun-Nung Chen", "Dilek Hakkani-Tür", "Gokhan Tur", "Jianfeng Gao", "Li Deng" ],
      "venue" : "INTERSPEECH, 2016.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Word-based dialog state tracking with recurrent neural networks",
      "author" : [ "Matthew Henderson", "Blaise Thomson", "Steve Young" ],
      "venue" : "SIGDIAL, 2014, pp. 292–299.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "On-line policy optimisation of Bayesian spoken dialogue systems via human interaction",
      "author" : [ "M Gašić", "Catherine Breslin", "Matthew Henderson", "Dongho Kim", "Martin Szummer", "Blaise Thomson", "Pirros Tsiakoulis", "Steve Young" ],
      "venue" : "ICASSP. IEEE, 2013, pp. 8367–8371.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A network-based endto-end trainable task-oriented dialogue system",
      "author" : [ "Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "Lina M Rojas-Barahona", "Pei-Hao Su", "Stefan Ultes", "David Vandyke", "Steve Young" ],
      "venue" : "arXiv preprint arXiv:1604.04562, 2016.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Long shortterm memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural computation, vol. 9, no. 8, pp. 1735–1780, 1997.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Bidirectional recurrent neural networks",
      "author" : [ "Mike Schuster", "Kuldip K Paliwal" ],
      "venue" : "IEEE Transactions on Signal Processing, vol. 45, no. 11, pp. 2673–2681, 1997.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980, 2014.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Multi-domain joint semantic frame parsing using bidirectional RNN-LSTM",
      "author" : [ "Dilek Hakkani-Tür", "Gokhan Tur", "Asli Celikyilmaz", "Yun- Nung Chen", "Jianfeng Gao", "Li Deng", "Ye-Yi Wang" ],
      "venue" : "INTERSPEECH, 2016.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "NLU typically detects dialog domains by parsing user utterances followed by user intent classification and filling associated slots according to a domain-specific semantic template [1]; DM keeps monitoring the belief distribution over all possible user states underlying current user behaviors and predicts system actions as responses [2, 3].",
      "startOffset" : 181,
      "endOffset" : 184
    }, {
      "referenceID" : 1,
      "context" : "NLU typically detects dialog domains by parsing user utterances followed by user intent classification and filling associated slots according to a domain-specific semantic template [1]; DM keeps monitoring the belief distribution over all possible user states underlying current user behaviors and predicts system actions as responses [2, 3].",
      "startOffset" : 335,
      "endOffset" : 341
    }, {
      "referenceID" : 2,
      "context" : "NLU typically detects dialog domains by parsing user utterances followed by user intent classification and filling associated slots according to a domain-specific semantic template [1]; DM keeps monitoring the belief distribution over all possible user states underlying current user behaviors and predicts system actions as responses [2, 3].",
      "startOffset" : 335,
      "endOffset" : 341
    }, {
      "referenceID" : 3,
      "context" : "hidden Markov models (HMMs) and conditional random field (CRF) [4–6] are widely used in slot tagging tasks; maximum entropy and support vector machines with linear kernel (LinearSVM) [7, 8] are applied to user intent prediction.",
      "startOffset" : 63,
      "endOffset" : 68
    }, {
      "referenceID" : 4,
      "context" : "hidden Markov models (HMMs) and conditional random field (CRF) [4–6] are widely used in slot tagging tasks; maximum entropy and support vector machines with linear kernel (LinearSVM) [7, 8] are applied to user intent prediction.",
      "startOffset" : 63,
      "endOffset" : 68
    }, {
      "referenceID" : 5,
      "context" : "hidden Markov models (HMMs) and conditional random field (CRF) [4–6] are widely used in slot tagging tasks; maximum entropy and support vector machines with linear kernel (LinearSVM) [7, 8] are applied to user intent prediction.",
      "startOffset" : 63,
      "endOffset" : 68
    }, {
      "referenceID" : 6,
      "context" : "hidden Markov models (HMMs) and conditional random field (CRF) [4–6] are widely used in slot tagging tasks; maximum entropy and support vector machines with linear kernel (LinearSVM) [7, 8] are applied to user intent prediction.",
      "startOffset" : 183,
      "endOffset" : 189
    }, {
      "referenceID" : 7,
      "context" : "hidden Markov models (HMMs) and conditional random field (CRF) [4–6] are widely used in slot tagging tasks; maximum entropy and support vector machines with linear kernel (LinearSVM) [7, 8] are applied to user intent prediction.",
      "startOffset" : 183,
      "endOffset" : 189
    }, {
      "referenceID" : 8,
      "context" : "Deep learning techniques making incredible progress on learning expressive feature representations have achieved better solutions to NLU modeling in ATIS domain [9–12].",
      "startOffset" : 161,
      "endOffset" : 167
    }, {
      "referenceID" : 9,
      "context" : "Deep learning techniques making incredible progress on learning expressive feature representations have achieved better solutions to NLU modeling in ATIS domain [9–12].",
      "startOffset" : 161,
      "endOffset" : 167
    }, {
      "referenceID" : 10,
      "context" : "Deep learning techniques making incredible progress on learning expressive feature representations have achieved better solutions to NLU modeling in ATIS domain [9–12].",
      "startOffset" : 161,
      "endOffset" : 167
    }, {
      "referenceID" : 11,
      "context" : "Deep learning techniques making incredible progress on learning expressive feature representations have achieved better solutions to NLU modeling in ATIS domain [9–12].",
      "startOffset" : 161,
      "endOffset" : 167
    }, {
      "referenceID" : 12,
      "context" : "The performance was improved significantly by incorporating recurrent neural networks (RNN) and CRF model [13–15].",
      "startOffset" : 106,
      "endOffset" : 113
    }, {
      "referenceID" : 13,
      "context" : "The performance was improved significantly by incorporating recurrent neural networks (RNN) and CRF model [13–15].",
      "startOffset" : 106,
      "endOffset" : 113
    }, {
      "referenceID" : 14,
      "context" : "The performance was improved significantly by incorporating recurrent neural networks (RNN) and CRF model [13–15].",
      "startOffset" : 106,
      "endOffset" : 113
    }, {
      "referenceID" : 15,
      "context" : "Convolutional neural networks are also used for domain/intent classification [16, 17].",
      "startOffset" : 77,
      "endOffset" : 85
    }, {
      "referenceID" : 16,
      "context" : "Convolutional neural networks are also used for domain/intent classification [16, 17].",
      "startOffset" : 77,
      "endOffset" : 85
    }, {
      "referenceID" : 17,
      "context" : "Flexible architectures of neural networks provide a way of jointly training with intent classification and slot filling [18, 19].",
      "startOffset" : 120,
      "endOffset" : 128
    }, {
      "referenceID" : 18,
      "context" : "Flexible architectures of neural networks provide a way of jointly training with intent classification and slot filling [18, 19].",
      "startOffset" : 120,
      "endOffset" : 128
    }, {
      "referenceID" : 19,
      "context" : "Contextual information of previous queries and domain/intent prediction was also incorporated into RNN structures [20, 21].",
      "startOffset" : 114,
      "endOffset" : 122
    }, {
      "referenceID" : 20,
      "context" : "Contextual information of previous queries and domain/intent prediction was also incorporated into RNN structures [20, 21].",
      "startOffset" : 114,
      "endOffset" : 122
    }, {
      "referenceID" : 1,
      "context" : "Most successful approaches cast the dialog manager as a partially observable Markov decision process [2], which uses hand-crafted features to represent the state and action space, and requires a large number of annotated conversations [22] or human interactions [23, 24].",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 21,
      "context" : "Most successful approaches cast the dialog manager as a partially observable Markov decision process [2], which uses hand-crafted features to represent the state and action space, and requires a large number of annotated conversations [22] or human interactions [23, 24].",
      "startOffset" : 235,
      "endOffset" : 239
    }, {
      "referenceID" : 22,
      "context" : "Most successful approaches cast the dialog manager as a partially observable Markov decision process [2], which uses hand-crafted features to represent the state and action space, and requires a large number of annotated conversations [22] or human interactions [23, 24].",
      "startOffset" : 262,
      "endOffset" : 270
    }, {
      "referenceID" : 23,
      "context" : "Most successful approaches cast the dialog manager as a partially observable Markov decision process [2], which uses hand-crafted features to represent the state and action space, and requires a large number of annotated conversations [22] or human interactions [23, 24].",
      "startOffset" : 262,
      "endOffset" : 270
    }, {
      "referenceID" : 24,
      "context" : "LSTM cells are used as recurrent units because LSTM can mitigate problems of vanishing or exploding gradients in long-term dependencies via self-regularization [25].",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 25,
      "context" : "Since preceding and following lexical contexts are important in analysis of user utterances, bi-directional LSTM cells (biLSTM) [26] is used.",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 26,
      "context" : "For the training procedure, we choose a mini-batch stochastic gradient descent method Adam [27] with the batch size of 32 examples.",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 27,
      "context" : "Each example includes an user utterance and its associated slot tags in IOB format [28], user intents, and system actions as the responses.",
      "startOffset" : 83,
      "endOffset" : 87
    } ],
    "year" : 2016,
    "abstractText" : "Natural language understanding and dialogue policy learning are both essential in conversational systems that predict the next system actions in response to a current user utterance. Conventional approaches aggregate separate models of natural language understanding (NLU) and system action prediction (SAP) as a pipeline that is sensitive to noisy outputs of error-prone NLU. To address the issues, we propose an end-to-end deep recurrent neural network with limited contextual dialogue memory by jointly training NLU and SAP on DSTC4 multi-domain human-human dialogues. Experiments show that our proposed model significantly outperforms the state-of-the-art pipeline models for both NLU and SAP, which indicates that our joint model is capable of mitigating the affects of noisy NLU outputs, and NLU model can be refined by error flows backpropagating from the extra supervised signals of system actions.",
    "creator" : "LaTeX with hyperref package"
  }
}