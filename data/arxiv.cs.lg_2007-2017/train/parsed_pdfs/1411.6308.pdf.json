{
  "name" : "1411.6308.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Convex Formulation for Spectral Shrunk Clustering",
    "authors" : [ "Xiaojun Chang", "Feiping Nie", "Zhigang Ma", "Yi Yang", "Xiaofang Zhou" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n41 1.\n63 08\nv1 [\ncs .L\nG ]\n2 3\nN ov\n2 01"
    }, {
      "heading" : "Introduction",
      "text" : "Clustering has been widely used in many real-world applications (Jain and Dubes 1988; Wang, Nie, and Huang 2014). The objective of clustering is to cluster the original data points into various clusters, so that data points within the same cluster are dense while those in different clusters are far away from each other (Filippone et al. 2008). Researchers have proposed a variety of clustering algorithms, such as Kmeans clustering and mixture models (Wang et al. 2014; Nie, Wang, and Huang 2014; Nie et al. 2011b), etc.\nThe existing clustering algorithms, however, mostly work well when the samples’ dimensionality is low. When partitioning high-dimensional data, the performance of these algorithms is not guaranteed. For example, K-means clustering iteratively assigns each data point to the cluster\nThis paper was partially supported by the ARC DECRA project DE130101311, UQ ECR (2013002401) and Tianjin Key Laboratory of Cognitive Computing and Application. Copyright c© 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nwith the closest center based on specific distance/similarity measurement and updates the center of each cluster. But the distance/similarity measurements may be inaccurate on high-dimensional data, which tends to limit the clustering performance. As suggested by some researchers, many high-dimensional data may exhibit dense grouping in a low-dimensional subspace (Nie et al. 2009). Hence, researchers have proposed to first project the original data into a low-dimensional subspace via some dimensionality reduction techniques and then cluster the computed lowdimensional embedding for high-dimensional data clustering. For instance, a popular approach is to use Principle component analysis (PCA) to reduce the dimensionality of the original data followed by Kmeans for clustering (PcaKm) (Xu and Wunsch 2005). Ding et al. present a clustering algorithm based on Linear discriminant analysis (LDA) method (Ding and Li 2007). Ye et al propose discriminative K-means (DisKmeans) clustering which unifies the iterative procedure of dimensionality reduction and K-means clustering into a trace maximization problem (Ye, Zhao, and Wu 2007).\nAnother genre of clustering, i.e., spectral clustering (Shi and Malik 2000) integrates dimensionality reduction into its clustering process. The basic idea of spectral clustering is to find a clustering assignment of the data points by adopting the spectrum of similarity matrix that leverages the nonlinear manifold structure of original data. Spectral clustering has been shown to be easy to implement and oftentimes it outperforms traditional clustering methods because of its capacity of mining intrinsic geometric structures, which facilitates partitioning data with more complicated structures. The benefit of utilizing manifold information has been demonstrated in many applications, such as image segmentation and web mining. Due to the advantage of spectral clustering, different variants of spectral clustering algorithms have been proposed these years (Li et al. 2015). For example, local learning-based clustering (LLC) (Wu and Schlkopf 2006) utilizes a kernel regression model for label prediction based on the assumption that the class label of a data point can be determined by its neighbors. Self-tuning SC (Zelnik-Manor and Perona 2004) is able to tune parameters automatically in an unsupervised scenario. Normalized cuts is capable of balancing the volume of clusters for the usage of data density information\n(Shi and Malik 2000). Spectral clustering is essentially a two-stage approach, i.e., manifold learning based in the original highdimensional space and dimensionality reduction. To achieve proper clustering, spectral clustering assumes that two nearby data points in the high density region of the reduceddimensional space have the same cluster label. However, this assumption does not always hold. More possibly, these nearest neighbors may be far away from each other in the original high-dimensional space due to the curse of dimensionality. That being said, the distance measurement of the original data could not precisely reflect the low-dimensional manifold structure, thus leading to suboptimal clustering performance.\nIntuitively, if the manifold structure in the lowdimensional space is precisely captured, the clustering performance could be enhanced when applied to highdimensional data clustering. Aiming to achieve this goal, we propose a novel clustering algorithm that is able to mine the inherent manifold structure of the low-dimensional space for clustering. Moreover, compared to traditional spectral clustering algorithms, the shrunk pattern learned by the proposed algorithm does not have an orthogonal constraint, giving it more flexibility to fit the manifold structure. It is worthwhile to highlight the following merits of our work:\n• The proposed algorithm is more capable of uncovering the manifold structure. Particularly, the shrunk pattern does not have the orthogonal constraint, making it more flexible to fit the manifold structure.\n• The integration of manifold learning and clustering makes the former particularly tailored for the latter. This is intrinsically different from most state-of-the-art clustering algorithms.\n• The proposed algorithm is convex and converges to global optimum, which indicates that the proposed algorithm does not rely on the initialization.\nThe rest of this paper is organized as follows. After reviewing related work on spectral clustering in section 2, we detail the proposed algorithm in section 3. Extensive experimental results are given in section 4 and section 5 concludes this paper."
    }, {
      "heading" : "Related Work",
      "text" : "Our work is inspired by spectral clustering. Therefore, we review the related work on spectral clustering in this section."
    }, {
      "heading" : "Basics of Spectral Clustering",
      "text" : "To facilitate the presentation, we first summarize the notations that will be frequently used in this paper. Given a dataset X = {x1, . . . , xn}, xi ∈ Rd(1 ≤ i ≤ n) is the i-th datum and n is the total number of data points. The objective of clustering is to partition χ into c clusters. Denote the cluster assignment matrix by Y = {y1, . . . , yn} ∈ Rn×c, where yi ∈ {0, 1}c×1 (1 ≤ i ≤ n) is the cluster indicator vector for the datum xi. The j-th element of yi is 1 if xi is clustered to the j-th cluster, and 0 otherwise.\nExisting spectral clustering algorithms adopt a weighted graph to partition the data. Let us denote G = {X , A} as a weighted graph with a vertex set X and an affinity matrix A ∈ Rn×n. Aij is the affinity of a pair of vertexes of the weighted graph. Aij is commonly defined as:\nAij =\n{ exp(− ‖xi−xj‖ 2\nδ2 ), if xi and xj are k nearest neighbors.\n0, otherwise.\nwhere δ is the parameter to control the spread of neighbors. The Laplacian matrix L is computed according to L = D − A, where D is a diagonal matrix with the diagonal elements as Dii = ∑ j Aij , ∀i. Following the work in (Ye, Zhao, and Wu 2007), we denote the scaled cluster indicator matrix F as follows:\nF = [F1, F2, . . . , Fn] T = Y (Y TY )− 1 2 , (1)\nwhere Fi is the scaled cluster indicator of xi. The j-th column of F is defined as follows by (Ye, Zhao, and Wu 2007):\nfj =  0, . . . , 0,︸ ︷︷ ︸\n∑j−1 i=1 ni\n1 √ nj , . . . , 1 √ nj , ︸ ︷︷ ︸ nj 0, . . . , 0︸ ︷︷ ︸∑ c i=j+1 nk\n  , (2)\nwhich indicates which data points are partitioned into the jth cluster Cj . nj is the number of data points in cluster Cj .\nThe objective function of spectral clustering algorithm is generally formulated as follows:\nmin F\nTr(FTLF )\ns.t. F = Y (Y TY )− 1 2\n(3)\nwhere Tr(·) denotes the trace operator. By denoting I as an identity matrix, we can define the normalized Laplacian matrix Ln as:\nLn = I −D− 1 2AD− 1 2 . (4)\nBy replacing L in Eq. (3) with the normalized Laplacian matrix, the objective function becomes the well-known SC algorithm normalized cut (Shi and Malik 2000). In the same manner, if we replace L in Eq. (3) by the Laplacian matrix obtained by local learning (Yang et al. 2010)(Wu and Schlkopf 2006), the objective function converts to Local Learning Clustering (LLC)."
    }, {
      "heading" : "Progress on Spectral Clustering",
      "text" : "Being easy to implement and promising for many applications, spectral clustering has been widely studied for different problems. Chen et al. propose a Landmark-based Spectral Clustering (LSC) for large scale clustering problems (Chen and Cai 2011). Specifically, a few representative data points are first selected as the landmarks and the original data points are then represented as the linear combinations of these landmarks. The spectral clustering is performed on the landmark-based representation.\nYang et al. propose to utilize a nonnegative constraint to relax the elements of cluster indicator matrix for spectral clustering (Yang et al. 2011). Liu et al. propose to compress the original graph used for spectral clustering into a sparse bipartite graph. The clustering is then performed on the bipartite graph instead, which improved the efficiency for large-scale data (Liu et al. 2013). Xia et al. propose a multi-view spectral clustering method based on low-rank and sparse decomposition (Xia et al. 2014). Yang et al. propose to use Laplacian Regularized L1-Graph for clustering (Yang et al. 2014). Tian et al. recently propose to adopt deep learning in spectral clustering (Tian et al. 2014).\nIn spite of the encouraging progress, few of the existing spectral clustering methods have considered learn the manifold in the low-dimensional subspace more precisely, not to mention integrating such manifold learning and clustering into a unified framework. This issue shall be addressed in this paper for boosted clustering performance."
    }, {
      "heading" : "The Proposed Algorithm",
      "text" : "In this section, we present the details of the proposed algorithm. A fast iterative method is also proposed to solve the objective function."
    }, {
      "heading" : "Problem Formulation",
      "text" : "Our algorithms is built atop the aim of uncovering the utmost manifold structure in the low-dimensional subspace of original data. Inspired by (Hou et al. 2013), we adopt the pattern shrinking during the manifold learning and the shrunk patterns are exploited for clustering simultaneously.\nTo begin with, we have the following notations. Denote the shrunk patterns of n data samples as {g1, · · · , gn}, where gi ∈ Rc. We first obtain spectral embedding F of the original samples by minimizing the traditional spectral clustering algorithm min Tr(FTLnF ), where Ln is a normalized Laplacian matrix.\nNext, the shrunk patterns are computed by satisfying the following requirements. (1) The shrunk patterns should keep consistency with the spectral embedding. To be more specific, the shrunk patterns should not be far away from the spectral clustering. (2) Note that nearby points are more likely to belong to the same cluster. We thus design a similarity matrix to measure pair similarity of any two spectral embedding, which the shrunk patters should follow.\nTo characterize the manifold structure of the spectral embedding {f1, · · · , fn}, a k-nearest neighbor graph is constructed by connecting each point to its k nearest neighbors. The similarity matrix, W , is computed by Wij = exp(− ‖fi−fj‖ 2\nδ2 ).\nFrom this similarity matrix, we can observe that if two spectral embeddings are nearby, they should belong to the same cluster and the corresponding weight should be large, which satisfies the first requirement (Nie et al. 2011a).\nTo keep the local similarity of spectral embedding, we propose to optimize the following objective function.\nmin G\n∑\nij\nWij‖gi − gj‖2 (5)\nWe also aim to keep the consistency between spectral embedding and shrunk patterns. Hence, we propose to minimize the following loss function directly.\nmin G\n‖G− F‖22 (6)\nTo this end, we formulate the objection function as follows:\nmin G\n‖G− F‖22 + γ ∑\ni,j\nWij‖gi − gj‖2 (7)\nwhere γ is a balance parameter. It can be easily proved that our formulation is convex. Due to the space limit, we omit the proof here. Since our method exploits shrunk patterns as the input for clustering, we name it Spectral Shrunk Clustering (SSC).\nAs indicated in (Ma et al. 2012; Kong, Ding, and Huang 2011), the least square loss function is not robust to outliers. To make our method even more effective, we follow (Ma et al. 2012; Nie et al. 2010) and employ l2,1-norm to handle the outliers. The objective function is rewritten as follows:\nmin G\n‖G− F‖2,1 + γ ∑\ni,j\nWij‖gi − gj‖2 (8)"
    }, {
      "heading" : "Optimization",
      "text" : "The proposed function involves the l2,1-norm, which is difficult to solve in a closed form. We propose to solve this problem in the following steps. Denote H = G − F and H = [h1, · · · , hd], where d is the dimension of spectral embedding. The objective function can be rewritten as follows:\nmin G\nTr((G− F )TS(G− F )) + γ ∑\nij\nwij‖gi − gj‖2 (9)\nwhere\nS =   1 2‖h1‖2\n. . . 1\n2‖hd‖2\n  . (10)\nDenote a Laplacian matrix L̃ = D̃ − W̃ , where W̃ is a re-weighted weight matrix defined by\nW̃ij = Wij\n2‖gi − gj‖2 (11)\nD̃ is a diagonal matrix with the i-th diagonal element as∑ j W̃ij . By simple mathematical deduction, the objective function arrives at:\nmin G\nTr((G− F )TS(G− F )) + γT r(GT L̃G). (12)\nBy setting the derivative of Eq. (12) to G to 0, we have:\nG = (S + γL̃)−1SF. (13)\nBased on the above mathematical deduction, we propose an iterative algorithm to optimize the objective function in Eq. (3), which is summarized in Algorithm 1. Once the shrunk patterns G are obtained, we perform K-means clustering on it to get the final clustering result.\nAlgorithm 1: Optimization Algorithm for SSC\nData: Data X ∈ Rd×n, Parameter γ and the number of clusters c Result: The discrete cluster assignment Y ∈ Rn×c\n1 Compute the normalized Laplacian matrix Ln ; 2 Obtain the spectral embedding F by using the\ntraditional spectral clustering ; 3 Compute the similarity matrix W using the spectral\nembedding F ; 4 Obtain the Laplacian matrix with the reweighted weight\nmatrix according to Eq. (11) ; 5 Set t = 0 ; 6 Initialize G0 ∈ Rn×c; 7 repeat 8 Compute Ht = Gt − F ; 9 Compute the diagonal matrix St according to (10) ;\n10 Compute Gt+1 according to\nGt+1 = (St + γW̃ ) −1StX ;\n11 t = t + 1 ; 12 until Convergence; 13 Based on G∗, compute the discrete cluster assignment\nmatrix Y by using K-means clustering; 14 Return the discrete cluster assignment matrix Y ."
    }, {
      "heading" : "Convergence Analysis",
      "text" : "To prove the convergence of the Algorithm 1, we need the following lemma (Nie et al. 2010).\nLemma 1. For any nonzero vectors g, gt ∈ Rc, the following inequality holds:\n‖g‖2 − ‖g‖22/2‖gt‖2 ≤ ‖gt‖2 − ‖gt‖22/2‖gt‖2 (14)\nThe following theorem guarantees that the problem in Eq. (8)converges to the global optimum by Algorithm 1 .\nTheorem 1. The Algorithm 1 monotonically decreases the objective function value of the problem in Eq. (8) in each iteration, thus making it converge to the global optimum.\nProof. Define f(G) = Tr((G−F )TS(G−F ). According to Algorithm 1, we know that\nGt+1 = argmin G\nf(G) + γ ∑\ni,j\n(W̃ )ij‖gi − gj‖22 (15)\nNote that (W̃t)ij = Wij\n2‖gti−g t j‖2\n, so we have\nf(Gt+1) + γ ∑\nij\nWij‖gt+1i − gt+1j ‖22 2‖gti − gtj‖2\n≤f(Gt) + γ ∑\nij\nWij‖gti − gtj‖22 2‖gti − gtj‖2\n(16)\nAccording to Lemma 1, we have\n∑\nij\nWij(‖gt+1i − gt+1j ‖2 − ‖gt+1i − gt+1j ‖22 2‖gti − gtj‖2 )\n≤ ∑\nij\nWij(‖gti − gtj‖2 − ‖gti − gtj‖22 2‖gti − gtj‖2 )\n(17)\nBy summing Eq. (16) and Eq. (17), we arrive at:\nf(Gt+1) + γ ∑\nij\nWij‖gt+1i − gt+1j ‖2\n≤f(Gt) + γ ∑\nij\nWij‖gti − gtj‖2 (18)\nThus, Algorithm 1 monotonically decreases the objective function value of the problem in Eq. (8) in each iteration t. When converged,Gt and L̃t satisfy Eq. (13). As the problem in Eq. (8) is convex, satisfying Eq. (13) indicates that Gt is the global optimum solution of the problem in Eq. (8). Therefore, using Algorithm 1 makes the problem in Eq. (8) converge to the global optimum.\nExperiment In this section, we perform extensive experiments on a variety of applications to test the performance of our method SSC. We compare SSC to several clustering algorithms including the classical K-means, the classical spectral clustering (SC), PCA Kmeans (Xu and Wunsch 2005), PCA spectral clustering (PCA SC), LDA Kmeans (Ding and Li 2007), LDA spectral clustering (LDA SC), Local Learning Clustering (LLC) (Wu and Schlkopf 2006) and SPLS (Hou et al. 2013)."
    }, {
      "heading" : "Datasets",
      "text" : "A variety of datasets are used in our experiments which are described as follows. The AR dataset (Martinez and Benavente 1998) contains 840 faces of 120 different people. We utilize the pixel value as the feature representations. The JAFFE dataset (Lyons et al. 1997) consists of 213 images of different facial expressions from 10 different Japanese female models. The images are resized to 26×26 and represented by pixel values. The ORL dataset (Samaria and Harter 1994) consists of 40 different subjects with 10 images each. We also resize each image to 32 × 32 and use pixel values to represent the images. The UMIST face dataset (Graham and M 1998) consists of 564 images of 20 individuals with mixed race, gender and appearance. Each individual is shown in a range of poses from profile\nto frontal views. The pixel value is used as the feature representation. The BinAlpha dataset contains 26 binary hand-written alphabets and we randomly select 30 images for every alphabet. The MSRA50 dataset contains 1799 images from 12 different classes. We resize each image to 32 × 32 and use the pixel values as the features. The YaleB dataset (Georghiades, Belhumeur, and Kriegman 2001) contains 2414 near frontal images from 38 persons under different illuminations. Each image is resized to 32 × 32 and the pixel value is used as feature representation. We additionally use the USPS dataset to validate the performance on handwritten digit recognition. The dataset consists of 9298 gray-scale handwritten digit images. We resize the images to 16× 16 and use pixel values as the features."
    }, {
      "heading" : "Setup",
      "text" : "The size of neighborhood, k is set to 5 for all the spectral clustering algorithms. For parameters in all the comparison algorithms, we tune them in the range of {10−6, 10−3, 100, 103, 106} and report the best results. Note that the results of all the clustering algorithms vary on different initialization. To reduce the influence of statistical variation, we repeat each clustering 50 times with random initialization and report the results corresponding to the best objective function values. For all the dimensionality reduction based K-means and Spectral clustering, we project the original data into a low dimensional subspace of 10 to 150 and report the best results."
    }, {
      "heading" : "Evaluation Metrics",
      "text" : "Following most work on clustering, we use clustering accuracy (ACC) and normalized mutual information (NMI) as our evaluation metrics in our experiments.\nLet qi represent the clustering label result from a clustering algorithm and pi represent the corresponding ground truth label of an arbitrary data point xi. Then ACC is defined as follows:\nACC =\n∑n i=1 δ(pi,map(qi))\nn , (19)\nwhere δ(x, y) = 1 if x = y and δ(x, y) = 0 otherwise. map(qi) is the best mapping function that permutes clustering labels to match the ground truth labels using the KuhnMunkres algorithm. A larger ACC indicates better clustering performance.\nFor any two arbitrary variables P and Q, NMI is defined as follows (Strehl and Ghosh 2003):\nNMI = I(P,Q)√ H(P )H(Q) , (20)\nwhere I(P,Q) computes the mutual information between P and Q, and H(P ) and H(Q) are the entropies of P and Q. Let tl represent the number of data in the cluster Cl(1 ≤ l ≤ c) generated by a clustering algorithm and t̃h represent the number of data points from the h-th ground truth class. NMI metric is then computed as follows (Strehl and Ghosh 2003):\nNMI =\n∑c l=1 ∑c h=1 tl,hlog( n×tl,h tl t̃h\n) √ ( ∑c l=1 tl log tl n )( ∑c h=1 t̃h log t̃h n ) , (21)\nwhere tl,h is the number of data samples that lie in the intersection between Cl and h-th ground truth class. Similarly, a larger NMI indicates better clustering performance."
    }, {
      "heading" : "Experimental Results",
      "text" : "The experimental results on listed in Table 1 and Table 2. We can see from the two tables that our method is consistently the best algorithm using both evaluation metrics. We also observe that:\n1. The spectral clustering algorithm and its variants achieve better performance than the classical k-means and its variants. This observation suggests that it is beneficial to utilize the pairwise similarities between all data points from a weighted graph adjacency matrix that contains helpful information for clustering.\n2. PCA Kmeans and LDA Kmeans are better than K-means whereas PCA SC and LDA SC are better than SC. This demonstrates that dimensionality reduction is helpful for improving the cluster performance.\n3. LDA Kmeans outperforms PCA Kmeans while LDA SC outperforms PCA SC. This indicates that LDA is more capable of keeping the structural information than PCA when doing dimensionality reduction.\n4. Among various spectral clustering variants, LLC is the most robust algorithm. This means using a more sophisticated graph Laplacian is beneficial for better exploitation of manifold structure.\n5. SPLS is the second best clustering algorithm. This is because it incorporates both the linear and nonlinear structures of original data.\n6. Our proposed Spectral Shrunk Clustering (SSC) consistently outperforms the other K-means based and spectral clustering based algorithms. This advantage is attributed to the optimal manifold learning in the low-dimensional subspace and it being tightly coupled with the clustering optimization."
    }, {
      "heading" : "Parameter Sensitivity",
      "text" : "In this section, we study the sensitivity of our algorithm w.r.t. the parameter γ in Eq. (3). Fig 1 shows the accuracy (y-axis) of SSC for different γ values (x-axis) on all the experimental datasets. It can be seen from the figure that the performance varies when different values of γ are used. However, except on MSRA50 and USPS datasets, our method attains the best/respectable performance when γ = 1. This indicates that our method has a consistent preference on parameter setting, which makes it uncomplicated to get optimal parameter value in practice."
    }, {
      "heading" : "Convergence Study",
      "text" : "As mentioned before, the proposed iterative approach in Algorithm 1 monotonically decreases the objective function value in Eq. (3). In this experiment, we show the convergence curves of the iterative approach on different datasets in Figure 2. The parameter γ is fixed at 1, which is the median value of the tuned range of the parameters.\nIt can be observed that the objective function value converges quickly. The convergence experiment demonstrates the efficiency of our algorithm."
    }, {
      "heading" : "Conclusion",
      "text" : "In this paper, we have proposed a novel convex formulation of spectral shrunk clustering. The advantage of our method is three-fold. First, it is able to learn the manifold structure in the low-dimensional subspace rather than the original space. This feature contributes to more precise structural information for clustering based on the low-dimensional space. Second, our method is more capable of uncovering the manifold structure. Particularly, the shrunk pattern learned by the\nproposed algorithm does not have the orthogonal constraint, which makes it more flexible to fit the manifold structure. The learned manifold knowledge is particularly helpful for achieving better clustering result. Third, our algorithm is convex, which makes it easy to implement and very suitable for real-world applications. Extensive experiments on a variety of applications are given to show the effectiveness of the proposed algorithm. By comparing it to several state-ofthe-art clustering approaches, we validate the advantage of our method.\nReferences [Chen and Cai 2011] Chen, X., and Cai, D. 2011. Large scale spec-\ntral clustering with landmark-based representation. In Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2011, San Francisco, California, USA, August 7-11, 2011.\n[Ding and Li 2007] Ding, C. H. Q., and Li, T. 2007. Adaptive dimension reduction using discriminant analysis and K-means clustering. In Machine Learning, Proceedings of the Twenty-Fourth International Conference (ICML 2007), Corvallis, Oregon, USA, June 20-24, 2007, 521–528.\n0 2 4 6 8 10 12 14 16 18 20 50\n100\n150\n200\n250 300 O bj ec tiv e Fu nc tio n V al ue\nNumber of Iterations\n(a) AR\n0 2 4 6 8 10 12\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0.035\nO bj\nec tiv\ne Fu\nnc tio\nn V\nal ue\nNumber of Iterations\n(b) JAFFE\n0 5 10 15 20 25\n0\n10\n20\n30\n40\n50\n60\nO bj\nec tiv\ne Fu\nnc tio\nn V\nal ue\nNumber of Iterations\n(c) ORL\n1 2 3 4 5 16\n17\n18\n19\n20\n21\n22\n23\nO bj\nec tiv\ne Fu\nnc tio\nn V\nal ue\nNumber of Iterations\n(d) UMIST\n1 2 3 4 5 6 28\n30\n32\n34\n36\n38\n40\n42\nO be\nct iv\ne Fu\nnc tio\nn V\nal ue\nNumber of Iterations\n(e) binalpha\n0 2 4 6 8 10\n1.90\n1.95\n2.00\n2.05\n2.10\n2.15\n2.20\nO bj\nec tiv\ne Fu\nnc tio\nn V\nal ue\nNumber of Iterations\n(f) MSRA50\n0 2 4 6 8 10 12 14 16 -10 0\n10 20 30 40 50 60 70 80\nO bj\nec tiv\ne Fu\nnc tio\nn V\nal ue\nNumber of Iterations\n(g) YaleB\n1 2 3 4 5 6 7 8.0 8.5 9.0 9.5\n10.0 10.5 11.0 11.5 12.0\nO bj\nec tiv\ne Fu\nnc tio\nn V\nal ue\nNumber of Iterations\n(h) USPS\nArtificial Intelligence, AAAI 2011, San Francisco, California, USA, August 7-11, 2011.\n[Yang et al. 2014] Yang, Y.; Wang, Z.; Yang, J.; Wang, J.; Chang, S.; and Huang, T. S. 2014. Data clustering by laplacian regularized l1-graph. In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, July 27 -31, 2014, Québec City, Québec, Canada., 3148–3149.\n[Ye, Zhao, and Wu 2007] Ye, J.; Zhao, Z.; and Wu, M. 2007. Discriminative k-means for clustering. In Proc. NIPS, 1649–1656.\n[Zelnik-Manor and Perona 2004] Zelnik-Manor, L., and Perona, P. 2004. Self-tuning spectral clustering. In Proc. NIPS, 1601–1608."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2014,
    "abstractText" : "Spectral clustering is a fundamental technique in the<lb>field of data mining and information processing. Most<lb>existing spectral clustering algorithms integrate dimen-<lb>sionality reduction into the clustering process assisted<lb>by manifold learning in the original space. However,<lb>the manifold in reduced-dimensional subspace is likely<lb>to exhibit altered properties in contrast with the orig-<lb>inal space. Thus, applying manifold information ob-<lb>tained from the original space to the clustering process<lb>in a low-dimensional subspace is prone to inferior per-<lb>formance. Aiming to address this issue, we propose a<lb>novel convex algorithm that mines the manifold struc-<lb>ture in the low-dimensional subspace. In addition, our<lb>unified learning process makes the manifold learning<lb>particularly tailored for the clustering. Compared with<lb>other related methods, the proposed algorithm results in<lb>more structured clustering result. To validate the effi-<lb>cacy of the proposed algorithm, we perform extensive<lb>experiments on several benchmark datasets in compar-<lb>ison with some state-of-the-art clustering approaches.<lb>The experimental results demonstrate that the proposed<lb>algorithm has quite promising clustering performance. Introduction<lb>Clustering has been widely used in many<lb>real-world applications (Jain and Dubes 1988;<lb>Wang, Nie, and Huang 2014). The objective of cluster-<lb>ing is to cluster the original data points into various<lb>clusters, so that data points within the same cluster<lb>are dense while those in different clusters are far away<lb>from each other (Filippone et al. 2008). Researchers have<lb>proposed a variety of clustering algorithms, such as K-<lb>means clustering and mixture models (Wang et al. 2014;<lb>Nie, Wang, and Huang 2014; Nie et al. 2011b), etc.<lb>The existing clustering algorithms, however, mostly work<lb>well when the samples’ dimensionality is low. When par-<lb>titioning high-dimensional data, the performance of these<lb>algorithms is not guaranteed. For example, K-means clus-<lb>tering iteratively assigns each data point to the cluster This paper was partially supported by the ARC DECRA project<lb>DE130101311, UQ ECR (2013002401) and Tianjin Key Labo-<lb>ratory of Cognitive Computing and Application. Copyright c<lb>©<lb>2015, Association for the Advancement of Artificial Intelligence<lb>(www.aaai.org). All rights reserved.<lb>with the closest center based on specific distance/similarity<lb>measurement and updates the center of each cluster. But<lb>the distance/similarity measurements may be inaccurate<lb>on high-dimensional data, which tends to limit the clus-<lb>tering performance. As suggested by some researchers,<lb>many high-dimensional data may exhibit dense grouping<lb>in a low-dimensional subspace (Nie et al. 2009). Hence, re-<lb>searchers have proposed to first project the original data<lb>into a low-dimensional subspace via some dimensionality<lb>reduction techniques and then cluster the computed low-<lb>dimensional embedding for high-dimensional data cluster-<lb>ing. For instance, a popular approach is to use Princi-<lb>ple component analysis (PCA) to reduce the dimensional-<lb>ity of the original data followed by Kmeans for cluster-<lb>ing (PcaKm) (Xu and Wunsch 2005). Ding et al. present a<lb>clustering algorithm based on Linear discriminant analysis<lb>(LDA) method (Ding and Li 2007). Ye et al propose dis-<lb>criminative K-means (DisKmeans) clustering which uni-<lb>fies the iterative procedure of dimensionality reduction and<lb>K-means clustering into a trace maximization problem<lb>(Ye, Zhao, and Wu 2007). Another genre of clustering, i.e., spectral clustering<lb>(Shi and Malik 2000) integrates dimensionality reduction<lb>into its clustering process. The basic idea of spectral clus-<lb>tering is to find a clustering assignment of the data points<lb>by adopting the spectrum of similarity matrix that leverages<lb>the nonlinear manifold structure of original data. Spectral<lb>clustering has been shown to be easy to implement and of-<lb>tentimes it outperforms traditional clustering methods be-<lb>cause of its capacity of mining intrinsic geometric struc-<lb>tures, which facilitates partitioning data with more com-<lb>plicated structures. The benefit of utilizing manifold infor-<lb>mation has been demonstrated in many applications, such<lb>as image segmentation and web mining. Due to the ad-<lb>vantage of spectral clustering, different variants of spec-<lb>tral clustering algorithms have been proposed these years<lb>(Li et al. 2015). For example, local learning-based cluster-<lb>ing (LLC) (Wu and Schlkopf 2006) utilizes a kernel regres-<lb>sion model for label prediction based on the assumption<lb>that the class label of a data point can be determined by its<lb>neighbors. Self-tuning SC (Zelnik-Manor and Perona 2004)<lb>is able to tune parameters automatically in an unsupervised<lb>scenario. Normalized cuts is capable of balancing the vol-<lb>ume of clusters for the usage of data density information (Shi and Malik 2000).<lb>Spectral clustering is essentially a two-stage approach,<lb>i.e., manifold learning based in the original high-<lb>dimensional space and dimensionality reduction. To achieve<lb>proper clustering, spectral clustering assumes that two<lb>nearby data points in the high density region of the reduced-<lb>dimensional space have the same cluster label. However, this<lb>assumption does not always hold. More possibly, these near-<lb>est neighbors may be far away from each other in the original<lb>high-dimensional space due to the curse of dimensionality.<lb>That being said, the distance measurement of the original<lb>data could not precisely reflect the low-dimensional mani-<lb>fold structure, thus leading to suboptimal clustering perfor-<lb>mance.<lb>Intuitively, if the manifold structure in the low-<lb>dimensional space is precisely captured, the clustering<lb>performance could be enhanced when applied to high-<lb>dimensional data clustering. Aiming to achieve this goal, we<lb>propose a novel clustering algorithm that is able to mine the<lb>inherent manifold structure of the low-dimensional space for<lb>clustering. Moreover, compared to traditional spectral clus-<lb>tering algorithms, the shrunk pattern learned by the proposed<lb>algorithm does not have an orthogonal constraint, giving it<lb>more flexibility to fit the manifold structure. It is worthwhile<lb>to highlight the following merits of our work: • The proposed algorithm is more capable of uncovering the<lb>manifold structure. Particularly, the shrunk pattern does<lb>not have the orthogonal constraint, making it more flexi-<lb>ble to fit the manifold structure.<lb>• The integration of manifold learning and clustering makes<lb>the former particularly tailored for the latter. This is in-<lb>trinsically different from most state-of-the-art clustering<lb>algorithms.<lb>• The proposed algorithm is convex and converges to global<lb>optimum, which indicates that the proposed algorithm<lb>does not rely on the initialization. The rest of this paper is organized as follows. After re-<lb>viewing related work on spectral clustering in section 2, we<lb>detail the proposed algorithm in section 3. Extensive experi-<lb>mental results are given in section 4 and section 5 concludes<lb>this paper.",
    "creator" : "LaTeX with hyperref package"
  }
}