{
  "name" : "1603.04068.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Signaling Game Approach to Databases Querying and Interaction",
    "authors" : [ "Ben McCamish", "Arash Termehchy", "Behrouz Touri" ],
    "emails" : [ "mccamis@oregonstate.edu", "termehca@oregonstate.edu", "touri@colorado.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 3.\n04 06\n8v 4\n[ cs\n.D B\n] 2\n2 Ju\nn 20\n17\nAs most users cannot precisely express their information needs in form of queries, it is challenging for database management systems to understand the true information needs behind users’ queries. Query interfaces leverage user’s feedback on the returned answers for a query to improve their understanding of the true information need behind the query. Current query interfaces generally assume that a user follows a fixed strategy of expressing her information needs, that is, the likelihood by which a user submits a query to express a certain information need remains unchanged over a potentially long period of time. Nevertheless, users may learn from their interactions with the database system and gradually choose more precise queries to express their intents. In this paper, we introduce a novel formal framework that models database querying as a collaboration between two active and potentially rational agents: the user and the database management system. These agents follow the identical interest of establishing amutual language for representing information needs. We formalize this collaboration as a signaling game, where each mutual language is an equilibrium for the game. A query interface is more effective if it establishes a less ambiguous mutual language faster. We explore some important characteristics of the equilibria of the game. Using an extensive empirical analysis over a real-world query workload, we show that users follow a reinforcement learning method to improve the articulation of their information needs. We also propose and analyze a reinforcement learning mechanism for the database query interface. We prove that this adaptation mechanism for the query interface improves the effectiveness of answering queries stochastically speaking, and converges almost surely, for both the cases where users follows a fixed and reinforcement learning strategy for expressing their information needs."
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "Most users do not know the structure and/or content of databases and cannot precisely express their queries [35, 13, 21, 34]. Hence, it is challenging for a database management system (DBMS), to understand and satisfy users’ information needs. Database researchers have proposed models, methods and systems to help users specify their queries more precisely and DBMS understand users’ intents more accurately [44, 34, 13, 46, 35, 25, 8, 22, 37, 21, 2, 9, 52]. Current models mainly focus on improving user satisfaction for a single information need. Given a user’s information need e, the DBMS estimates e by various methods including showing potential results to the user and collecting her feedback [40, 17, 9, 52, 60], asking questions from her [2], or suggesting potential queries to her [36].\nNevertheless, many users may explore a database to find answers for various queries and information needs over a rather long period of time. For these users, database querying is an inherently interactive and continuing process. This setting extends the problem of answering a single information need in two aspects. First, the DBMS can improve its understanding of how the user expresses her intents progressively over the course of many queries. The DBMS may learn the desired answers for a submitted query q from the user’s explicit or implicit feedback on the returned results. Types of the user’s feedbackmay include clicking on the relevant answers [30, 65], the amount of time the user spends on reading the results [29], or user’s eye movements [33]. Second, the user may leverage her experience from previous interactions with the DBMS to formulate her future queries. As the user submits queries and inspect or consume their results, she may gain a better understanding of the database content, which may impact how she formulates queries in the future. For example, to find the answers for a particular information need, the user may submit some initial and underspecified query, observes its results, and reformulate it according to her observations. Ideally, we would like the user and DBMS to gradually develop some degree of mutual understanding over the course of several queries and interactions: the DBMS should better understand how the user expresses her intents and the user may get more familiar with the content of the database. Of course, the user and DBMS would like to establish a perfect or near-perfect mutual understanding, where the DBMS returns all or a majority of the desired answers to all or most user queries. An important question is if there are inherent limits to establishing these mutual understandings. In other words, one would like to know the degrees to which these mutual understandings are possible. One would also like explore the characteristics of information needs, queries, and behavior of the DBMS in such limits. Subsequently, one may leverage these results to design methods and strategies for DBMS to help establishing more effective mutual understandings in rather small amounts of time. Unfortunately, current models assume that a user follows a fixed strategy of expressing her information needs in form of queries. In other words, they maintain that a user submits a query with a fixed probability to express an intent throughout her interaction with the database system. Hence, one cannot use current models and techniques to answer the aforementioned questions and help the database system to\nunderstand the users’ information need in rather long periods of interaction. Furthermore, it has been shown that certain learning strategies that may be useful in a static setting do not converge to a desired outcome in a setting where both agents modify their strategies [24]. Thus, current query interfaces may not be able to effectively satisfy users’ information needs in a long-term interaction. In this paper, we propose a novel framework that formalizes the collaboration between user and the DBMS over the course of many interactions. Our framework models this collaboration as a game between two active and potentially rational agents: the user and DBMS. The common goal of the user and DBMS is to reach a mutual understanding on expressing information needs in the form of queries. The user informs the DBMS of her intents by submitting queries. The DBMS returns some results for the query. Both players receive some reward based on the degree by which the returned answers satisfy the information need behind the query. The equilibria and stable states of the game model the inherent limits in establishing a mutual understanding between the user and DBMS. We further explore the strategies that a DBMS may adapt to improve user satisfaction. We believe that this framework naturally models the long term interactions between the user and DBMS. More specifically, we make the following contributions:\n• We model the long term interaction between the user and DBMS as a particular type of game called a signaling game in Section 2 [15, 49]. The user’s strategy in the game is a stochastic mapping from her intents to queries, which reflects the user’s decision in choosing a query to express an information need. The DBMS strategy is a stochastic mapping from queries to results. After each interaction, the DBMS may update its strategy based user feedback and/or the user may update its strategy based on the returned results.\n• We analyze the equilibria of the game and show that the game has some Nash and strict Nash equilibria in Section 3. We find the number of Nash and strict Nash equilibria in some interesting settings. We also show that the Nash and strict Nash equilibria of the game may not provide a desired level of mutual understanding between the user and DBMS. Hence, the gamemay stabilize in an undesirable state.\n• Using extensive empirical studies over a real-world query workload, we show that users’ learning can be accurately modeled by Roth and Erv’s reinforcement learning algorithm [51] in Section 4.\n• We propose a reinforcement learning algorithm that updates the DBMS strategy in Section 5 and prove that it improves the effectiveness of answering queries stochastically speaking, and converges almost surely. Our proof shows that the proposed learning rule is robust to the choice of effectiveness measure which is a highly desirable property. We show that our results hold for both the cases where the user adapts her strategy using Roth and Erv’s reinforcement learning algorithm a the case where she follows a fixed strategy."
    }, {
      "heading" : "2. FORMAL FRAMEWORK",
      "text" : "Users andDBMSs typically achieve a common understanding gradually and using a querying/feedback paradigm. After submitting each query, the user may revise her strategy of expressing intents based on the returned result. If the returned answers satisfy her intent to a large extent, she may keep using the same query to articulate her intent. Otherwise, she may revise her strategy and choose another query to express her intent in the hope that the new query will provide her with more relevant answers. The user may also inform the database system about the degree by which the returned answers satisfy the intent behind the query using explicit or implicit feedback, e.g., click-through information [29]. The database system may update its interpretation of the query according to the user’s feedback. Intuitively, one may model this interaction as a game between two agents with identical interests in which the agents communicate via sharing queries, results and feedback on the results. In each interaction, both agents will receive some reward according to the degree by which the returned result for a query matches the intent behind the query. The user will receive her rewards in form of answers relevant to her intent and database system will receive its reward through getting positive feedback on the returned results. The final goal of both agents is to maximize the amount of reward they receive during the course of their interaction. They will get the maximum reward if they establish a common mapping between intents and queries."
    }, {
      "heading" : "2.1 The Data Interaction Game",
      "text" : "Next, we describe the components and structure of the data interaction game for relational databases. A schema S is a set of relation symbols. Each relation symbol R has a corresponding arity. Let dom be an infinite set constants. A database instance I over S associates with every n-ary relation symbol R ∈ S a finite subset IR of dom n.\n2.1.1 Intent\nAn intent represents an information need sought after by the user. Current usable query interfaces over databases usually assume that each intent is a query in a sufficiently expressive query language in the domain of interest, e.g., SQL, over the underlying database instance [14, 35, 21]. Our framework and results are orthogonal to the language that precisely describes the users’ intents. In our examples, we consider the intent language to be conjunctive queries [1]. Table 1 illustrates a database instance with schemaUniv(Name, Abbreviation, State, Type, Ranking) that contains information about university rankings. A user may want to find the ranking for university MSU in Michigan, which is precisely represented by the intent e2 in Table 2(a): ans(z) ← Univ(x, ‘MSU ’, ‘MI’, y, z).\n2.1.2 Query\nUsers’ articulations of their intents are queries. Users do not often know relatively complex languages, such as SQL or Datalog, that precisely describe their intents. Thus, they prefer to articulate their intents in languages that are easy-touse and relatively less complex, such as keyword query language [35, 14, 21]. In this instantiation of signaling games\nfor database querying, we assume that the user expresses her intents as keyword queries. More formally, we fix a countably infinite set of terms T . A keyword query (query for short) is a nonempty (finite) set of terms in T . Consider the database instance in Table 1. Table 2 depicts a set of queries and intents over this database. Suppose the user wants to find the ranking for Michigan State University in Michigan, i.e. the intent e2. Because the user may not know how to write conjunctive queries and not be sufficiently familiar with the content of the database, she may express intent e2 using query q2 : ‘MSU’. Some users may know and submit their queries in the language that is sufficiently expressive to represent their intents. Nevertheless, because they may not know precisely the content and structure of the data, their submitted queries may not always be the same as their intents [12, 36]. For example, a user may know how to write a union of conjunctive query. But, since she may not know the state abbreviationMI, she may articulate intent e2 as ans(t) ← Univ(x, ‘MSU ’, y, z, t), which is different from e2. Our framework can be extended for these scenarios.\n2.1.3 User Strategy\nThe user strategy indicates the likelihood by which the user submits query q given that her intent is e. In practice, a user has finitely many intents and submits finitely many queries over a database in a finite period of time. Hence, we assume the sets of the user’s intents and queries are finite. We index each user’s intent and each user’s query by 1 ≤ i ≤ m and 1 ≤ j ≤ n, respectively. A user strategy, denoted as U , is am× n row-stochastic matrix from her set of intents to her set of queries. Consider again the intents and queries in Table 2. The matrix on the left hand side of Table 3(a) depicts a user strategy in which the user submits query q1 to express intents e1, e2, and e3.\nThe DBMS interprets the submitted queries to find the intents behind them. A DBMS may not exactly know the set\nof user’s intents and the language in which those intents are precisely expressed. The DBMS usually selects a query language for the interpreted intents according to the information in the domain of interest [21, 31, 14]. For example, some keyword query interfaces over relational databases interpret keyword queries as Select-Project-Join SQL queries with only conjunctions in the where clause [31, 42]. We denote the language used by the DBMS for the interpreted intents over the underlying database instance asE. The DBMS observes the set of queries user submits during the interactionQu. The interpretation of DBMS on queries that are not in the set of submitted queries does not impact its interaction with the user. Furthermore, given a query, the DBMS explores finitely many alternative intents to interpret the query efficiently [21, 31, 14]. Thus, the DBMS strategy, denoted as D, is a mapping fromQu to a finite subset of E, denoted as Ed. We index each intent in Ed by 1 ≤ ℓ ≤ o. Consider the intents and queries in Table 2. The matrix on the right hand side of Table 3(a) illustrates a DBMS strategy in which the DBMS interprets both q1 and q2 as e2. The intent language used by the DBMS in this example is the set of conjunctive queries. DBMSs often implement their strategies using a real-valued function called scoring function over pairs of queries and intents [45, 14, 21]. Given a query, the DBMS selects the intent with the maximum score, executes the intent on the database, and returns the resulting tuples. It is shown, however, that such a deterministic mapping may limit the effectiveness of interpreting queries in long-term interactions [30, 65, 62, 5]. Because the DBMS shows only the result of the intent(s) with the highest score(s) to the user, its learning remains largely biased toward the highly scored interpreted intents. To learn more about the user’s intents, the DBMS has to show and get feedback on a sufficiently diverse set of intents. Of course, the DBMS should ensure that this set of intents are relatively relevant to the query, otherwise the user may become discouraged and give up querying. Because such a set of intents may be relatively large, it may not be useful to show all of them to the user during a single interaction. Hence, the DBMS may adopt a stochastic strategy:it may randomly select and show the results of intents such that the ones with higher scores are more often chosen [30, 65, 62, 5]. Researchers have shown that it is possible to find the stochastic strategy that results in returning sufficiently many answers to keep users engaged and sufficiently diverse results to collect useful feedback and improve significantly the effectiveness of query answering [62, 30]. Hence, we redefine the DBMS strategy as an n × o row-stochastic matrix from\nthe set of queries in Qu to the set of interpreted intents Ed. Consider again the intents and queries in Table 2. The matrix on the right hand side of Table 3(b) depicts a DBMS strategy in which the DBMS interprets query q2 as intent e1 and e2 each with probability 0.5.\n2.1.5 Interaction & Reward\nThe data interaction game is a repeated game with identical interest between two players, the user and the database system. At each round of the game, i.e., a single interaction, the user selects the intent eu according to the prior probability distribution π. She then picks the query q according to her strategy and submits it to the DBMS. The DBMS observes q and interprets q as intent ed based on its strategy, executes ed on the underlying database, and returns its result to the user. The user provides some feedback on the returned tuples and informs the DBMS how relevant the result is to her intent eu. The goal of both the user and the DBMS is to have as many relevant tuples as possible in the returned result. Hence, both the user and the DBMS receive some reward according to the degree by which the returned result matches the user’s intent eu. This reward is measured based on the user feedback and using standard effectiveness metrics [45]. One example of such metrics is precision, which is the ratio of the number of relevant and returned tuples to the total number of returned tuples. In cases where the user examines only the top-k returned answers, one may use precision at k, p@k, that measure the precision of the topk returned tuples. At the end of each round, both the user and the DBMS receive a reward equal to the value of the selected effectiveness metric for the returned result. Each pair of the user and the DBMS strategy, (U ,D), is a strategy profile. Thus, the expected payoff for both players with strategy profile (U ,D) in each interaction is\nur(U,D) = m ∑\ni=1\nπi\nn ∑\nj=1\nUij\no ∑\nℓ=1\nDjℓ r(ei, eℓ) (1)\nwhere r is the effectivenessmetric computed using the user’s feedback on the result of the interpreted intent eℓ over the underlying database.\n2.1.6 Information\nNone of the players know the other player’s strategy. Let the user and DBMS strategy at round t ∈ N of the game be U(t) and D(t), respectively. With a slight abuse of notation, let ur(t) :=ur(U(t), D(t)). In round t ∈ N of the game, the user has access to her sequence of intents (eu(i)), queries (q(i)), and rewards (ur(i)) and the DBMS knows the sequence of submitted queries (q(i)), interpreted intents (ed(i)), and rewards (ur(i)), 1 ≤ i ≤ t, up to round t. We note than whether and how the user and the DBMS will use the aforementioned prices of information depend on their degrees of rationality, which we discuss later in the paper. The prior probability distribution on intents, π is common knowledge between the players.\n2.1.7 Strategy Adaptation\nGive the information available to each player, the player may modify its strategy at the end of each round. For example, the DBMS may reduce the probability of returning cer-\ntain intent that has not received any positive feedback from the user in the last round of the game. Data Interaction Game: The data interaction game, G, is represented as an 8-tuple:\n(S,U,D, π, (eu(i)), (q(i)), (ed(i)), (ur(i))) (2)\nwhere 1 ≤ i ≤ t, in which S is the schema of the underlying database. Due to their lack of information, users may sometimes submit queries that may not return any tuple over the underlying database [4]. For example, they may submit overspecified SQL queries whose execution over the underlying database produces an empty set. The DBMS typically map these queries to relatively broad and relaxed intents with non-empty result over the database [4, 67]. Thus, the DBMS may be still able to solicit feedback for these types of queries."
    }, {
      "heading" : "2.2 Open Problems",
      "text" : "Traditionally, usable query interfaces, e.g., keyword query interfaces, aim at improving users’ satisfaction by optimizing some effectiveness metrics, e.g., p@k, for their input queries [14]. In our game-theoretic formalization, however, the goal of the DBMS should be to guide the interaction to a desired and stable state, i.e., equilibrium, in which, roughly speaking, both players do not have any motivation to change their strategies and they both get the maximum possible reward. There are three important questions regarding this game.\n• What are the desired and undesired equilibria of the game? It is important to identify the non-optimal equilibria of the game as the interaction may stabilize in these equilibria.\n• What are the reasonable assumptions on the behavior and the degree of rationality of the user?\n• Given the answers to the previous two questions, what strategy adaptation mechanism(s) the DBMS should use to guide and converge the interaction to a desired equilibrium fast? At the first glance, it may seem that if the DBMS adapts a reasonable learning mechanism, the user’s adaptation can only help further the DBMS to reach an optimal state as both players have identical interest. Nevertheless, in some collaborative two-player games in which both players adapt their strategies to improve their payoff, the learning may not converge to any (desired) equilibrium and cycle among several unstable states [54, 64]. More importantly, the DBMS should use an adaptation strategy that keeps users engaged [30]. In other words, the adaptation mechanism may not significantly reduce the payoff of the user for too many subsequent sessions.\nWe present some preliminary results on the aforementioned questions in the following sections."
    }, {
      "heading" : "3. EQUILIBRIUM ANALYSIS",
      "text" : ""
    }, {
      "heading" : "3.1 Fixed User Strategy",
      "text" : "In some settings, the strategy of a user may change in a much slower time scale than that of the DBMS. In these\ncases, it is reasonable to assume that the user’s strategy is fixed. Hence, the game will reach a desirable state where the DBMS adapts a strategy that maximizes the expected payoff. Let a strategy profile be a pair of user and DBMS strategies.\nDEFINITION 3.1. Given a strategy profile (U ,D), D is a best response to U w.r.t. effectiveness measure r if we have ur(U,D) ≥ ur(U,D ′) for all the database strategiesD′.\nA DBMS strategy D is a strict best response to U if the inequality in Definition 3.1 becomes strict for all D′ 6= D.\nEXAMPLE 3.1. Consider the database instance about universities that is shown in Table 1 and the intents, queries, and the strategy profiles in Tables 2(a), 2(b), 3(a), and 3(b), respectively. Given a uniform prior over the intents, the DBMS strategy is a best response the user strategy w.r.t. w.r.t precision and p@k in both strategy profiles 3(a) and 3(b).\nDEFINITION 3.2. Given a strategy profile (U,D), an intent ei, and a query qj , the payoff of ei using qj is\nur(ei, qj) =\no ∑\nℓ=1\nDj,ℓr(ei, sℓ).\nDEFINITION 3.3. The pool of intents for query qj in user strategy U is the set of intents ei such that Ui,j > 0.\nWe denote the pool of intents of qj as PL(qj). Our definition of pool of intent resembles the notion of pool of state in signaling games [15, 18]. Each result sℓ such that Dj,ℓ > 0 may be returned in response to query qj . We call the set of these results the reply to query qj .\nDEFINITION 3.4. A best reply to query qj w.r.t. effectiveness measure r is a reply that maximizes ∑\nei∈PL(qj) πiUi,j\nur(ei, qj).\nThe following characterizes the best response to a strategy.\nLEMMA 3.5. Given a strategy profile (U,D),D is a best response to U w.r.t. effectiveness measure r if and only if D maps every query to one of its best replies.\nPROOF. If each query is assigned to its best reply in D, no improvement in the expected payoff is possible, thus D is a best response for U . Let D be a best response for U such that some query q is not mapped to its best reply in D. Let rmax be a best reply for q. We create a DBMS strategy D′ 6= D such that all queries q′ 6= q in D′ have the same reply as they have in D and the reply of q is rmax. Clearly, D′ has higher payoff than D for U . Thus, D is not a best response.\nThe following corollary directly results from Lemma 3.5.\nCOROLLARY 3.6. Given a strategy profile (U,D), D is a strict best response to U w.r.t. effectiveness measure r if and only if every query has one and only one best reply and D maps each query to its best reply.\nGiven an intent e over database instance I , some effectiveness measures, such as precision, take their maximum for other results in addition to e(I). For example, given intent e, the precision of every non-empty result s ⊂ e(I) is equal to the precision of e(I) for e. Hence, there are more than one best reply for an intent w.r.t. precision. Thus, according to Corollary 3.6, there is not any strict best response w.r.t. precision."
    }, {
      "heading" : "3.2 Nash Equilibrium",
      "text" : "In this section and Section 3.3, we analyze the equilibria of the game where both user and DBMS may modify their strategies. A Nash equilibrium for a game is a strategy profile where the DBMS and user will not do better by unilaterally deviating from their strategies.\nDEFINITION 3.7. A strategy profile (U,D) is a Nash equilibriumw.r.t. a satisfaction function r if ur(U,D)≥ ur(U\n′, D) for all user strategy U ′ and ur(U,D) ≥ ur(U,D\n′) for all database strategyD′.\nEXAMPLE 3.2. Consider again the database about universities that is shown in Table 1 and the intents, queries, and the strategy profiles in Tables 2(a), 2(b), 3(a), and 3(b), respectively. Both strategy profiles 3(a) and 3(b) are Nash equilibria w.r.t precision and p@k. User and DBMS cannot unilaterally change their strategies and receive a better payoff. If one modifies the strategy of the database in strategy profile 3(b) and replaces the probability of executing and returning e1 and e3 given query q2 to ǫ and 1 − ǫ, 0 ≤ ǫ ≤ 1, the resulting strategy profiles are all Nash equilibria.\nIntuitively, the concept of Nash equilibrium captures the fact that users may explore different ways of articulating and interpreting intents, but they may not be able to look ahead beyond the payoff of a single interaction when adapting their strategies. Some users may be willing to lose some payoff in the short-term to gain more payoff in the long run, therefore, an interesting direction is to define and analyze less myopic equilibria for the game [26]. If the interaction between user and DBMS reaches a Nash equilibrium, they user do not have a strong incentive to change her strategy. As a result the strategy of the DBMS and the expected payoff of the game will likely to remain unchanged. Hence, in a Nash equilibrium the strategies of user andDBMS are likely to be stable. Also, the payoff at a Nash equilibrium reflects a potential eventual payoff for the user and DBMS in their interaction. Query qj is a best query for intent ei if qj ∈ argmaxqk ur(ei, qk). The following lemma characterizes the Nash equilibrium of the game.\nLEMMA 3.8. A strategy profile (U,D) is a Nash equilibrium w.r.t. effectiveness measure r if and only if\n• for every query q, q is a best query for every intent e ∈ PL(q), and\n• D is a best response to U .\nPROOF. Assume that (U,D) is a Nash equilibrium. Also, assume qj is not a best query for ei ∈ PL(qj). Let qj′ be a best query for ei. We first consider the case where ur(ei, qj′) > 0. We build strategy U ′ where U ′k,ℓ = Uk,ℓ for all entries (k, ℓ) 6= (i, j) and (k, ℓ) 6= (i, j′), U ′i,j = 0, and U ′i,j′ = Ui,j . We have U ′ 6= U and ur(U,D) < ur(U ′, D). Hence, (U,D) is not a Nash equilibrium. Thus, we have Ui,j = 0 and the first condition of the theorem holds. Now, consider the case where ur(ei, qj′) = 0. In this case, we will also have ur(ei, qj) = 0, which makes qj a best query for ei. We prove the necessity of the second condition of the theorem similarly. This concludes the proof for the necessity part of the theorem. Now, assume that both conditions\nof the theorem hold for strategies U and D. We can prove that it is not possible to have strategies U ′′ andD′′ such that ur(U,D) < ur(U ′′, D) or ur(U,D) < ur(U,D ′′) using a similar method."
    }, {
      "heading" : "3.3 Strict Nash Equilibrium",
      "text" : "A strict Nash equilibrium is a strategy profile in which the DBMS and user will do worse by unilaterally changing their equilibrium strategy.\nDEFINITION 3.9. A strategy profile (U,D) is a strict Nash equilibriumw.r.t. effectiveness measure r if we haveur(U,D) > ur(U,D\n′) for all DBMS strategiesD′ 6= D and ur(U,D) > ur(U ′, D) for all user strategies U ′ 6= U .\nEXAMPLE 3.3. Consider the intents, queries, strategy profile, and database instance in Tables 4(a), 4(b), 5, and 1. The strategy profile is a strict Nash equilibrium w.r.t precision. However, the strategy profile in Example 3.2 is not a strict Nash equilibrium as one may modify the value of ǫ without changing the payoff of the players.\nNext, we investigate the characteristics of strategies in a strict Nash equilibria profile. Recall that a strategy is pure iff it has only 1 or 0 values. A user strategy is onto if there is not any query qj such that Ui,j = 0 for all intend i. A DBMS strategy is one-to-one if it does not map two queries to the same result. In other words, there is not any result sell such that Djℓ > 0 andDj′ℓ > 0 where j 6= j ′.\nTHEOREM 3.10. If (U,D) is a strict Nash equilibrium w.r.t. satisfaction function r, we have\n• U is pure and onto.\n• D is pure and one-to-one.\nPROOF. Let us assume that there is an intent ei and a query qj such that 0 < Ui,j < 1. Since U is row stochastic, there is a query qj′ where 0 < Ui,j′ < 1. Let ur(Ui,j , D) = ∑o\nℓ=1 Dj,ℓr(ei, sℓ). If ur(Ui,j , D) = ur(Ui,j′ , D), we can create a new user strategy U ′ where U ′i,j = 1 and U ′ i,j′ = 0 and the values of other entries in U ′ is the same as U . Note that the payoff of (U ,D) and (U ′,D) are equal and hence, (U ,D) is not a strict Nash equilibrium. If ur(Ui,j , D) 6= ur(Ui,j′ , D), without loss of generality one can assume that ur(Ui,j , D) > ur(Ui,j′ , D). We construct a new user strategy U ′′ whose values for all entries except (i, j) and (i, j′) are equal to U and U ′′i,j = 1, U ′′i,j′ = 0. Because ur(U,D) < ur(U ′′, D), (U ,D) is not a strict Nash equilibrium. Hence, U must be a pure strategy. Similarly, it can be shown thatD should be a pure strategy. If U is not onto, there is a query qj that is not mapped to any intent in U . Hence, one may change the value in row j ofD without changing the payoff of (U,D). Assume thatD is not one-to-one. Hence, there are queries qi and qj and a result sℓ such thatDi,ℓ =Dj,ℓ = 1. Because (U,D) is a strict Nash, U is pure and we have either Ui,ℓ = 1 or Uj,ℓ = 1. Assume that Ui,ℓ = 1. We can construct strategy U ′ that have the same values as U for all entries except for (i, ℓ) and (j, ℓ) and U ′i,ℓ = 0, U ′ j,ℓ = 1. Since the payoffs of (U,D) and (U ′, D) are equal, (U,D) is not a strict Nash equilibrium.\nTheorem 3.10 extends the Theorem 1 in [18] for our model. In some settings, the user may knows and use fewer queries than intents, i.e., m > n. Because the DBMS strategy in a strict Nash equilibrium is one-to-one, the DBMS strategy does not map some of the results to any query. Hence, the DBMS will never return some results in a strict Nash equilibrium no matter what query is submitted. Interestingly, as Example 3.1 suggests some of these results may be the results that perfectly satisfy some user’s intents. That is, given intent ei over database instance I , the DBMS may never return ei(I) in a strict Nash equilibrium. Using a proof similar to the one of Lemma 3.8, we have the following properties of strict Nash equilibria of a game. A strategy profile (U,D) is a strict Nash equilibrium w.r.t. effectiveness measure r if and only if:\n• Every intent e has a unique best query and the user strategy maps e to its best query, i.e., e ∈ PL(qi).\n• D is the strict best response to U ."
    }, {
      "heading" : "3.4 Number of Equilibria",
      "text" : "A natural question is how many (strict) Nash equilibria exist in a game. Theorem 3.10 guarantees that both user and DBMS strategies in a strict Nash equilibrium are pure. Thus, given that the sets of intents and queries are finite, there are finitely many strict Nash equilibria in the game. We note that each set of results is always finite. However, we will show that if the sets of intents and queries in a game are finite, the game has infinite Nash equilibria.\nLEMMA 3.11. If a game has a non-strict Nash equilibrium. Then there is an infinitely many Nash equilibria.\nPROOF. The result follows from the fact that the payoff function (1) is a bilinear form of U and D, i.e. it is a\nlinear of D when U is fixed and a linear function of U , when D is fixed. If for D 6= D′, (U,D) and (U,D′) are Nash-equilibria, then ur(U,D) = ur(U,D\n′). Therefore ur(U, αD + (1 − α)D\n′) = ur(U,D) for any α ∈ R. In particular, for α ∈ [0, 1], if D,D′ are stochastic matrices, αD + (1 − α)D′ will be a stochastic matrix and hence, (U, αD + (1 − α)D′) is a Nash equilibrium as well. Similarly, if (U ′, D) and (U,D) are Nash equilibria for U 6= U ′, then ur(αU + (1− α)U\n′, D) = ur(U,D) and (αU + (1− α)U ′, D) is a Nash-equilibrium for any α ∈ [0, 1].\nTHEOREM 3.12. Given a game with finitely many intents and queries, if the game has a non-strict Nash equilibrium, it has an infinite number of Nash equilibria.\nPROOF. Every finite game has always a mixed Nash equilibrium [57]. According to Theorem 3.10, a mixed Nash is not a strict Nash equilibrium. Therefore, using Lemma 3.11, the game will have infinitely many Nash equilibria."
    }, {
      "heading" : "3.5 Efficiency",
      "text" : "In this section we discuss the efficiency of different equilibria. We refer to the value of the utility (payoff) in formula (1) at a strategy profile to the efficiency of the strategy. Therefore, the most efficient strategy profile is naturally the one that maximizes (1). We refer to an equilibria with maximum efficiency as an efficient equilibrium. Thus far we have discussed two types of equilibria, Nash and strict Nash, that once reached it is unlikely that either player will deviate from its current strategy. In some cases it may be possible to enter a state of equilibria where neither player has any incentive to deviate, but that equilibria may not be an efficient equilibrium. The strategy profile in Table 3(b) provides the highest payoff for the user and DBMS given the intents and queries in Tables 2(a) and 2(b) over the database in Table 1. However, some Nash equilibria may not provide high payoffs. For instance, Table 3(a) depicts another strategy profile for the set of intents and queries in Tables 2(a) and 2(b) over the database in Table 1. In this strategy profile, the user has little knowledge about the database content and expresses all of her intents using a single query q2, which asks for the ranking of universities whose abbreviations areMSU. Given query q2, the DBMS always returns the ranking of Michigan State University. Obviously, the DBMS always returns the non-relevant answers for the intents of finding the rankings of Mississippi State University and Missouri State University. If all intents have equal prior probabilities, this strategy profile is a Nash equilibrium. For example, the user will not get a higher payoff by increasing their knowledge about the database and using query q1 to express intent e2. Clearly, the payoff of this strategy profile is less than the strategy profile in Table 3(b). Nevertheless, the user and the DBMS do not have any incentive to leave this undesirable stable state once reached and will likely stay in this state.\nDEFINITION 3.13. A strategy profile (U ,D) is optimal w.r.t. an effectiveness measure r if we have ur(U,D) ≥ u(U\n′, D′) for all DBMS strategies D′ and U ′\nSince, the games discussed in this paper are games of identical interest, i.e. the payoff of the user and the DBMS are the\nsame, therefore, an optimal strategy (U,D) (w.r.t. an effectiveness measure r) is a Nash equilibrium.\nLEMMA 3.14. A strategy (U ,D) is optimal if and only if it is an efficient equilibrium.\nPROOF. Note that if (U,D) is optimal, then none of the two players (i.e. the user and the DBMS) has a unilateral incentive to deviate. Therefore (U,D) is a Nash equilibrium. On the other hand, since the payoff function (1) is a continuous function of U andD and the domain of row-stochastic matrices is a compact space, therefore a maximizer (U,D) of (1) exists and by the previous part it is a Nash equilibrium. Note that the efficiency of all strategies are bounded by the efficiency of an optimal strategy and hence, any efficient equilibrium is optimal.\nSimilar to the analysis on efficiency of a Nash equilibria, there are strict Nash equilibria that are less efficient than others. Strict Nash equilibria strategy profiles are unlikely to deviate from the current strategy profile, since any unilateral deviation will result in a lower payoff. From this we can say that strict Nash equilibria are also more stable than Nash equilibria since unilateral deviation will always have a lower payoff.\nAs an example of a strict Nash equilibrium that is not efficient, consider both strategy profiles illustrated in Tables 6 and 7. Note that the intents. queries, and results in this example are different from the ones in the previous examples. For this illustration, we set the rewards to r(e1, s1) = 1, r(e2, s2) = 2, r(e2, s3) = 0.1, and r(e3, s3) = 3 where all other rewards are 0. Using our payoff function in Equation 1 we can calculate the total payoff for the strategy profile in Table 6 as u(U,D) = 4.1. This strategy profile is a strict Nash since any unilateral deviation by either player will result in a strictly worse payoff. Consider the strategy profile in Table 7 with payoff u(U,D) = 5. This payoff is higher than the payoff the strategy profile in Table 6 receives. It is also not likely for the strategy profile with less payoff to change either strategy to the ones in the strategy profile with higher payoff as both are strict Nash."
    }, {
      "heading" : "4. USER ADAPTATIONMECHANISM",
      "text" : "It is well established that humans show reinforcement behavior in learning [56, 47, 48]. Many lab studies with human subjects conclude that one can model human learning using reinforcement learningmodels [56, 47]. The exact reinforcement learning method used by a person, however, may vary based on her capabilities and the task at hand. We have performed an empirical study of a real-world query log to figure out the reinforcement learning model(s) that best explain the learning mechanism by which users adapt their strategies during interaction with a data management system."
    }, {
      "heading" : "4.1 Query Workload",
      "text" : "We have used a subsample of a Yahoo! query log for our empirical study [63]. The Yahoo! query log consists of queries submitted to a Yahoo! search engine over a period of time in July 2010. Each record in the query log consists of a time stamp, user cookie, query submitted, the 10 results displayed to the user, and the positions of the user clicks. All the record logs are anonymized such that each time stamp, query, and returned result are saved as a unique identifier. Table 8 illustrates an example of what the log record looks like. Note, for the Results column, there are 10 results returned. Accompanying the query log is a set of relevance judgment scores for each query and result pair. The relevance judgment scores determine user satisfaction with that result. The score has the possible values of 0,1,2,3,4, with 0 meaning not relevant at all and 4 meaning the most relevant result. Table 9 illustrates an example of the relevance judgment scores dataset. For example, if the user entered QueryID = 00002efd and one of results returned was ResultID = 2722a07f , then the relevance of that result would be three.\nFor our analysis we sorted the query log by the time stamp attribute to simulate the time line of the users interaction with the Yahoo! search engine. We determine the intent behind each query by using the relevance judgment scores for the results for each query. We consider the intent behind each query to be the set of results, i.e., URLs, with non-zero relevance scores."
    }, {
      "heading" : "4.2 Reinforcement Learning Methods",
      "text" : "We have used and adapted six different reinforcement learning methods to model users’ strategy in interaction with data systems. These models mainly vary based on 1) the degree by which the user considers past interactions when computing future strategies, 2) how they update the user strategy,\nand 3) the rate by which they update the user strategy. Some models assume that the user leverages outcomes of her past interactions when she updates her current strategy. Other models allow the user to forget older interactions. These methods also differ in how they use the value of reward to update the user’s strategy. Some reinforce a behavior, e.g., using a certain query to convey an intent, after a successful attempt by a fixed value independent of the amount of reward. Others use the value of received reward to reinforce a behavior. Finally, a model may have some discounting factors to control the rate by which a behavior is reinforced. In this study, we have used the value of NDCG for the returned results of a query as the reward in each interaction. Since NDCG models different levels of relevance, it provides a more exact estimate of the true reward in an interaction than other metrics that measure the quality of a ranked list, such as precision at k.\n4.2.1 Bush and Mosteller’s Model\nBush and Mosteller’s model increases the probability that a user will choose a given query when searching for a specific intent by an amount proportional based on the reward of using that query and the current probability of using this query for the intent in the strategy [10]. It also decreases the probabilities of queries not used in a successful interaction. This method updates the probabilities of using queries for the intent ei after an interaction using the following formulas.\nUij(t+1) =\n{\nUij(t) + α BM · (1− Uij(t)) qj = q(t) ∧ r ≥ 0 Uij(t)− β BM · Uij(t) qj = q(t) ∧ r < 0\n(3)\nUij(t+1) =\n{\nUij(t)− α BM · Uij(t) qj 6= q(t) ∧ r ≥ 0 Uij(t) + β BM · (1− Uij(t) qj 6= q(t) ∧ r < 0\n(4) In the aforementioned formulas,αBM ∈ [0, 1] and βBM ∈ [0, 1] are parameters of the model, q(t) denotes the query picked by the user at time t, and r is the reward of the interaction. If query qj is equal to the query chosen by the user to represent intent ei, then Equation 3 is used. For all other queries qj for the intent ei at time t, Equation 4 is used. The probabilities of using queries for intents other than ei remains unchanged. Since the value of NDCG is always greater than zero, i.e., r > 0, the parameter βBM is never used nor trained.\n4.2.2 Cross’s Model\nCross’s model modifies the user’s strategy similar to Bush and Mosteller’s model [16]. However, it uses the amount of the received reward to update the user strategy. There are two parameters in this model, αC and βC that influence the rate of reinforcement\nUij(t+ 1) =\n{\nUij(t) +R(r) · (1− Uij(t)) qj = q(t) Uij(t)−R(r) · Uij(t) qj 6= q(t) (5)\nR(r) = αC · r + βC (6)\nIn the above formulas, αC ∈ [0, 1] and βC ∈ [0, 1] are the parameters used compute the adjusted reward R(r) based\non the value of actual reward r. The parameter βC is a static increment of the adjusted reward. Similar to Bush and Mosteller’s model, the aforementioned formulas are used to update the probabilities of using queries for the intent ei in the current interaction. Other entries in the user’s strategy are remained unchanged.\n4.2.3 Roth and Erev’s Model\nRoth and Erev’s model reinforces the probabilities directly from the reward value r that is received when the user enters query q(t) [51]. Its most important difference with other models is that it explicitly accumulates all the rewards gained by using a query to express an intent. Sij(t) in matrix S(t) maintains the accumulated reward of using query qj to express intent ei over the course of the user and database system interactions up to round (time) t.\nSij(t+ 1) =\n{\nSij(t) + r qj = q(t) Sij(t) qj 6= q(t) (7)\nUij(t+ 1) = Sij(t+ 1)\nn ∑\nj′ Sij′ (t+ 1)\n(8)\nRoth and Erev’s model increases the probability of using a query to express an intent based on the accumulated rewards of using that query over the long-term interaction of the user and data management system. It does not explicitly penalize other queries. Of course, because the user strategy U is row-stochastic, each query not used in a successful interaction will be implicitly penalized as when the probability of a query increases, all others will decrease to keep U rowstochastic.\n4.2.4 Roth and Erev’s Modified Model\nRoth and Erev’s modified model is similar to the original Roth and Erev’s model, but it has an additional parameter that determines to what extent the user takes in to account the outcomes of her past interactions with the system [20]. It is reasonable to assume that the user may forget the results of her much earlier interactions with the system. User’s memory is imperfect which means that over time the strategy may change merely due to the forgetful nature of the user. This is accounted for by the forget parameter σ ∈ [0, 1]. Matrix S(t) has the same role it has for the Roth and Erev’s model.\nSij(t+ 1) = (1− σ) · Sij(t) + E(j, R(r)) (9)\nE(j, R(r)) =\n{\nR(r) · (1− ǫ) qj = q(t) R(r) · (ǫ) qj 6= q(t) (10)\nR(r) = r − rmin (11)\nUij(t+ 1) = Sij(t+ 1)\nn ∑\nj′ Sij′ (t+ 1)\n(12)\nIn the aforementioned formulas, ǫ ∈ [0, 1] is a parameter that weights the reward that the user receives, n is the maximum number of possible queries for a given intent ei, and rmin is the minimum expected reward that the user wants to receive. The intuition behind this parameter is that the user\noften assumes some minimum amount of reward is guaranteed when she queries the database. The model uses this minimum amount to discount the received reward. We set rmin to 0 in our analysis, representing that there is no expected reward in an interaction. Therefor the model uses the total received reward to reinforce a strategy.\n4.2.5 Win-Stay/Lose-Randomize\nTheWin-Stay/Lose-Randomizemethod uses only the most recent interaction for an intent to determine the queries used to express the intent in the future [7]. Assume that the user conveys an intent ei by a query qj . If the reward of using qj is above a specified threshold π the user will use qj to express ei in the future. Otherwise, if the the reward is below the threshold, the user will simply randomize her strategy, where for ei, all queries have an equal probability to be chosen.\n4.2.6 Latest Reward\nThe Latest-Rewardmethod reinforces the user strategy based on the previous reward that the user has seen when querying for an intent ei. All other queries have an equal probability to be chosen for a given intent. Let a user receive reward r ∈ [0, 1] by entering query qj to express intent ei. The Latest-Reward method sets the probability of using qj to convey ei in the user strategy, Uij to r and distribute the the remaining probability mass 1 − r evenly between other entries related to intent ei, in Uik, where k 6= j.\n4.2.7 Comparing the Methods\nNext, we compare the aforementioned models in terms of their use of past interaction and their update rules. Bush and Mosteller’s, Cross’s, and both Roth and Erev’s models use information from the past to compute the future strategies. The Roth and Erev’s models use the information about the past interactionsmore than others. Win-Stay/Lose-Randomize and Latest-Reward models do not rely as much as the first four methods on the outcomes of the previous interactions. Cross’s, both Roth and Erev’s, and the Latest-Reward models use the value of the reward that is received after entering a query to update the strategy. Bush and Mosteller’s and WinStay/Lose-Randomize models change their strategies based on a fixed amount independent of the reward."
    }, {
      "heading" : "4.3 Training and Testing",
      "text" : "Some models, e.g., Cross’s model, have some parameters that need to be trained. We have used 5,000 records in the query workload and found the optimal values for those parameters using a grid search and the sum of squared errors. Each strategy has been initialized with an uniform distribution of probabilities, so that all queries are likely to be used for a given intent at the initial strategy. Once we had the parameters estimated for each model, we let each model to run over 300,000 records that follow the initial 5,000 records in the query log to compute a user strategy. All models have used the same set of queries and results. During the training phase a total of 3,456 intents, 3,088 queries, and 253,952 users were covered. This means that queries and intents were reused frequently and there were many returning users over time. We have calculated the NDCG value for each interac-\ntion using the available relevance judgment scores that accompanies Yahoo! query workload. Interestingly, we have not observed any result with perfect or zero NDCG in the query workload. At the end of this round, we get six user strategies, one per reinforcement method. We have evaluated the accuracy of the trained user strategies in predicting the future strategies of the users using the interaction records for 2,000 unique intents in the query log that follow the 300,000 records used in training. For each intent, we have found its first log record that immediately follows the records used in training and compared the predication of the strategy with the query actually used in this log record to express the intent. During the testing phase a total of 2000 intents were used, 1806 queries, and 1999 users were covered. To compare the prediction accuracies of the strategies, we calculated the mean squared distance between what a given strategy predicted and what the user actually did."
    }, {
      "heading" : "4.4 Empirical Results",
      "text" : "Table 10 shows the results from the tests that we performed as well as the estimated parameters. A lower mean squared distance implies that the model more accurately represents the users’ learning method. Roth and Erev’s and Roth and Erev’s modified models both perform the best out of all the tested models. Since σ = 0 in Roth and Erev’s modified model, this model does not forget the outcomes of the previous interactions. Thus, it behaves more similarly to the original Roth and Erev’s model. Because Roth and Erev models update the users strategies using the information of the previous strategies and interactions, users use their previous strategies and the outcomes of their previous interactions with the system when they pick a query to express their current intent. This result also indicates that the value of received reward should be considered when reinforcing a strategy. Bush andMosteller’s, Cross’s, andWin-Stay/Lose-Randomize models perform worse than either of Roth and Erev’s models. Bush and Mosteller’s model has a relatively low value of α. Therefore, the rate of reinforcement is quite slow as the lower α is, the less a successful strategy is reinforced. With an α of 0 for example, there would be no reinforce-\nment at all. Bush and Mosteller’s model also does not consider the reward when reinforcing and therefor cannot reinforce queries that get effective results more than others that receive a smaller reward. Cross’s model suffers from the same lack of reinforcement rate as Bush and Mosteller’s but has an additional downfall. If the reward is extremely low, almost zero, the query will still be reinforced as β is a constant value independent of the reward. This means that queries with higher reward will be reinforced more, but also means that queries with an extremely low reward will still be reinforced when they probably should be left alone. WinStay/Lose-Randomize does not provide an accurate predication because it does not consider the entire history of strategies that the user has used. It also does not explore the space of possible queries to improve the effectiveness of the interaction. Hence, it seems that the users keep exploring possible queries to express an intent more effectively, although they may already know a query that conveys the intent quite successfully. Also, by only considering the previous reward, Win-Stay/Lose-Randomize cannot make robust adjustments and instead makes fixed changes in the model that are quite drastic. Finally, Latest-Reward performs the worst when compared to all models by an order of magnitude. This is because not only does this method have not memory like Win-Stay/Lose-Randomize, but it also reinforcing the strategy too drastically. From our analysis it appears that users show a substantially intelligent behavior when adopting andmodifying their strategies. They leverage all most of their past interactions and their outcomes, i.e., have an effective long-term memory. This is specially interesting as the results of previous lab studies have indicated that mostly only proficient subjects rely on the accumulated rewards of the past interactions and use Roth and Erev’s model for learning. Those studies show that non-proficient users tend to use models that do not leverage the information about the past interactions, such as Cross’s model [11]. Also, the reward they receive directly impacts how they reinforce their strategy and will dictate what queries are used to represent intents in the future."
    }, {
      "heading" : "5. ADAPTATIONMECHANISMSFORDBMS",
      "text" : "In many relevant applications, the user’s learning is happening in a much slower time-scale compared to the learning of the DBMS. So, one can assume that the user’s strategy is fixed compared to the time-scale of the DBMS adaptation. Therefore, in this section, first, we consider the case that the user is not adapting to the strategy of the DBMS and then, we consider the case that the user’s strategy is adapting to the DBMS’s strategy but perhaps on a slower time-scale. When dealing with the game introduced in the previous sections, many questions arise:\ni. How can a DBMS learn or adapt to a user’s strategy?\nii. Mathematically, is a given learning rule effective?\niii. What would be the limiting behavior of a given learning rule?\nHere, we address the first and the second questions above. Dealing with the third question is far beyond the page limits of this paper. For simplicity of notation, we refer to intent ei\nand result sℓ as intent i and ℓ, respectively, in the rest of the paper. Hence, we have:\nur(U,D) =\nm ∑\ni=1\nπi\nn ∑\nj=1\nUij\no ∑\nℓ=1\nDjℓriℓ,\nwhere r : [m] × [o] → R+ is the effectiveness measure between the intent i and the result, i.e., decoded intent ℓ."
    }, {
      "heading" : "5.1 Reinforcement Learning for an Arbitrary Similarity Measure",
      "text" : "As in [32], we consider Roth-Erev reinforcement learning mechanism for adaptation of the DBMS adaption. For the case that both the DBMS and the user adapt their strategies, one can use the results in [32]. Let us discuss the DBMS adaptation rule. The learning/adaptation rule happens over discrete time t = 0, 1, 2, 3, . . . instances where t denotes the tth interaction of the user and the DBMS. We refer to t simply as the iteration of the learning rule. With this, the reinforcement learningmechanism for the DBMS adaptation is as follows:\na. Let R(0) > 0 be an n × o initial reward matrix whose entries are strictly positive.\nb. Let D(0) be the initial DBMS strategy with Djℓ(0) = Rjℓ(0)∑ o ℓ=1 Rjℓ(0) > 0 for all j ∈ [n] and ℓ ∈ [o].\nc. For iterations t = 1, 2, . . ., do\ni. If the user’s query at time t is q(t), DBMS return a result E(t) ∈ E with probability:\nP (E(t) = i′ | q(t)) = Dq(t)i′(t).\nii. User gives a reward rii′ given that i is the intent of the user at time t. Note that the reward depends both on the intent i at time t and the result i′. Then, set\nRjℓ(t+ 1) =\n{\nRjℓ(t) + riℓ if j = q(t) and ℓ = i ′ Rjℓ(t) otherwise .\n(13)\niii. Update the DBMS strategy by\nDji(t+ 1) = Rji(t+ 1) ∑o\nℓ=1 Rjℓ(t+ 1) , (14)\nfor all j ∈ [n] and i ∈ [o].\nIn the above scheme R(t) is simply the reward matrix at time t. Few comments are in order regarding the above adaptation rule:\n- One can use available ranking functions, e.g. [12], for the initial reward condition R(0) which possibly leads to an intuitive initial point for the learning rule. One may normalize and convert the scores returned by these functions to probability values.\n- In step c.ii., if the DBMS has the knowledge of the user’s intent after the interactions (e.g. through a click), the DBMS setsRji+1 for the known intent i. The mathematical analysis of both cases will be similar.\n- In the initial step, as the DBMS uses a ranking function to compute the probabilities, it may not materialize R and D. As the game progresses, DBMS maintains the strategy and reward matrices with entries for only the observed queries, their underlying intents, and their returned results. Hence, the DBMS does not need to materialize R and D for the sets of possible intents, queries, and results. DBMS also does not need to know the set of user’s interns beforehand. Hence, the algorithm is practical for the cases where the sought-for intents, submitted queries, and returned results are not very large. Moreover,R and D are generally sparse. As queries and intents generally follow a power law distribution [45], one may use sampling techniques to use this algorithm in other settings. The rigorous theoretical and empirical analysis of applying such techniques are interesting subjects of future work."
    }, {
      "heading" : "5.2 Analysis of the Learning Rule",
      "text" : "In this section, we provide an analysis of the reinforcement mechanism provided above and will show that, statistically speaking, the adaptation rule leads to improvement of the efficiency of the interaction. Note that since the user gives feedback only on one tuple in the result, one can without the loss of generality assume that the cardinality of the list k is 1. For the analysis of the reinforcement learning mechanism in Section 5 and for simplification, denote\nu(t) := ur(U,D(t)) = ur(U,D(t)), (15)\nfor an effectiveness measure r as ur is defined in (1). We recall that a random process {X(t)} is a submartingale [19] if it is absolutely integrable (i.e. E(|X(t)|) < ∞ for all t) and\nE(X(t+ 1) | Ft) ≥ X(t),\nwhereFt is the history or σ-algebra generated byX1, . . . , Xt. In other words, a process {X(t)} is a sub-martingale if the expected value ofX(t+1) givenX(t), X(t−1), . . . , X(0), is not strictly less than the value ofXt. Note that submartingales are nothing but the stochastic counterparts of monotonically increasing sequences. As in the case of bounded (from above) monotonically increasing sequences, submartingales pose the same property, i.e. any submartingale {X(t)} with E(|X(t)|) < B for some B ∈ R+ and all t ≥ 0 is convergent almost surely. We refer the interested readers to [19] for further information on this result (martingale convergence theorem). The main result in this section is that the sequence of the utilities {u(t)} (which is indeed a stochastic process as {D(t)} is a stochastic process) defined by (15) is a submartignale when the reinforcement learning rule in Section 5 is utilized. As a result the proposed reinforcement learning rule stochastically improves the efficiency of communication between the DBMS and the user. More importantly, this holds for an arbitrary reward/effectiveness measure r. This is rather a very strong result as the algorithm is robust to the choice of the reward mechanism. To show this, we discuss an intermediate result. For simplicity of notation, we fix the time t and we use superscript+ to denote variables at time (t+1) and drop the dependencies\nat time t for variables depending on time t. Throughout the rest of our discussions, we let {Ft} be the natural filtration for the process {D(t)}, i.e. F is the σ-algebra generated by D(0), . . . , D(t).\nLEMMA 5.1. For any ℓ ∈ [m] and j ∈ [n], we have\nE(D+jℓ | Ft)−Djℓ\n= Djℓ · m ∑\ni=1\nπiUij\n(\nriℓ\nR̄j + ril −\no ∑\nℓ′=1\nDjℓ′ riℓ′\nR̄j + riℓ′\n)\n,\nwhere R̄j = ∑o\nℓ′=1 Rjℓ′ .\nPROOF. Fix ℓ ∈ [m] and j ∈ [n]. Let A be the event that at the t’th iteration, we reinforce a pair (j, ℓ′) for some ℓ′ ∈ [m]. Then on the complement Ac of A, D+jℓ(ω) = Djℓ(ω). Let Ai,ℓ′ ⊆ A be the subset of A such that the intent of the user is i and the pair (j, ℓ′) is reinforced. Note that the collection of sets {Ai,ℓ′} for i, ℓ\n′ ∈ [m], are pairwise mutually exclusive and their union constitute the set A. We note that\nD+jℓ =\nm ∑\ni=1\n\n   Rjℓ + ril R̄j + riℓ 1Ai,ℓ +\no ∑\nℓ′=1 ℓ′ 6=ℓ\nRjℓ\nR̄j + riℓ′ 1Ai,ℓ′\n\n  \n+Djℓ1Ac .\nTherefore, we have\nE(D+jℓ | Ft) = m ∑\ni=1\nπiUijDjℓ Rjℓ + riℓ R̄j + riℓ\n+\nm ∑\ni=1\nπiUij ∑\nℓ 6=ℓ′\nDjℓ′ Rjℓ\nR̄j + riℓ′ + (1− p)Djℓ,\nwhere p = P(A | F). Note thatDjℓ = Rji\nR̄j and hence,\nE(D+jℓ | Ft)−Djℓ = m ∑\ni=1\nπiUijDjℓ riℓR̄j −Rjℓ R̄j(R̄j + riℓ)\n−\nm ∑\ni=1\nπiUij ∑\nℓ 6=ℓ′\nDjℓ′ Rjℓriℓ′\nR̄j(R̄j + riℓ′ ) .\nReplacing Rjl R̄j with Djℓ and rearranging the terms in the above expression, we get the result.\nTo show the main result, we will utilize the following result in martingale theory.\nTHEOREM 5.2. [50] A random process {Xt} converges almost surely if Xt is bounded, i.e., E(|Xt|) < B for some B ∈ R+ and all t ≥ 0 and\nE(Xt+1|Ft) ≥ Xt − βt (16)\nwhere βt ≥ 0 is a summable sequence almost surely, i.e., ∑\nt βt < ∞ with probability 1.\nNote that this result is a weaker form of the Robins-Siegmund martingale convergence theorem in [50] but it will serve for the purpose of our discussion.\nUsing Lemma 5.1 and the above result, we show that up to a summable disturbance, the proposed learning mechanism is stochastically improving.\nTHEOREM 5.3. Let {u(t)} be the sequence given by (15). Then,\nE(u(t+ 1 | Ft) ≥ E(u(t) | Ft)− βt,\nfor some non-negative random process {βt} that is summable (i.e.\n∑∞ t=0 β < ∞ almost surely). As a result {u(t)} con-\nverges almost surely.\nPROOF. Let u+ := u(t+ 1), u := u(t),\nuj := uj(U(t), D(t)) =\nm ∑\ni=1\no ∑\nℓ=1\nπiUijDjℓriℓ(t),\nand also define R̄j := ∑m ℓ′=1 Rjℓ′ . Note that u j is the efficiency of the jth signal/query. Using the linearity of conditional expectation and Lemma 5.1, we have:\nE(u+ | Ft)− u =\nm ∑\ni=1\nn ∑\nj=1\nπiUij\no ∑\nℓ=1\nriℓ′ ( E(D+jℓ | Ft)−Djℓ )\n=\nm ∑\ni=1\nn ∑\nj=1\no ∑\nℓ=1\nπiUijDjℓriℓ\n(\nm ∑\ni′=1\nπ′iUi′j\n(\nri′ℓ\nR̄j + ri′ℓ\n−\no ∑\nℓ′=1\nDjℓ′ ri′ℓ′\nR̄j + ri′ℓ′\n))\n.\n(17)\nNow, let yjℓ = ∑m i=1 πiUijriℓ and zjℓ = ∑m i=1 πiUij riℓ\nR̄j+riℓ .\nThen, we get from the above expression that\nE(u+ | Ft)− u = n ∑\nj=1\n(\no ∑\nℓ=1\nDjℓyiℓzjℓ −\no ∑\nℓ=1\nDjℓyjℓ\no ∑\nℓ′=1\nDjℓ′zjℓ′\n)\n.(18)\nNow, we express the above expression as\nE(u+ | Ft)− u = Vt + Ṽt (19)\nwhere\nVt =\nn ∑\nj=1\n1\nR̄j\n\n\no ∑\nℓ=1\nDjℓy 2 jℓ −\n(\no ∑\nl=1\nDjℓyjℓ\n)2 \n ,\nand\nṼt =\nn ∑\nj=1\n(\no ∑\nℓ=1\nDjℓyjℓ\no ∑\nℓ′=1\nDjℓ′ z̃jℓ′ −\nm ∑\nℓ=1\nDjℓyjℓz̃jℓ\n)\n.\n(20)\nFurther, z̃jℓ = ∑ i=1 πiUij r2iℓ\nR̄j(R̄j+riℓ) .\nWe claim that Vt ≥ 0 for each t and {Ṽt} is a summable sequence almost surely. Then, from (19) and Theorem 5.2, we get that {ut} converges almost surely and it completes the proof. Next, we validate our claims. We first show that Vt ≥ 0, ∀t. Note that D is a rowstochastic matrix and hence, ∑o\nℓ=1 Djℓ = 1. Therefore, by\nthe Jensen’s inequality [19], we have:\no ∑\nℓ=1\nDjℓ(yjℓ) 2 ≥\no ∑\nℓ=1\n(Djℓyjℓ) 2.\nHence, V ≥ 0. We next claim that {Ṽt} is a summable sequence with probability one. It can be observed from (20) that\nVt ≤ o ∑\nj=1\no2n R̄2j . (21)\nsince yjℓ ≤ 1, z̃jℓ ≤ R̄ −2 j for each j ∈ [n], ℓ ∈ [m] and D is a row-stochastic matrix. To prove the claim, it suffices to show that for each j ∈ [m], the sequence { 1\nR2 j (t)\n}\nis summable. Note that for each j ∈ [m] and for each t, we have R̄j(t+1) = R̄j(t)+ǫt where ǫt ≥ ǫ > 0with probability pt ≥ p > 0. Therefore, using the Borel-Cantelli Lemma for adapted processes [19] we have { 1\nR2 j (t)\n} is summable\nwhich concludes the proof.\nThe above result implies that the effectiveness of the DBMS, stochastically speaking, increases as time progresses when the learning rule in Section 5 is utilized. Not only that, but this property does not depend on the choice of the effectiveness function (i.e. riℓ in this case). This is indeed a desirable property for any adapting scheme for DBMS adaptation."
    }, {
      "heading" : "5.3 User Adaptation",
      "text" : "Here, we consider the case that the user also adapts to the DBMS’s strategy. When the user submits a query q and the DBMS returns a result that fully satisfy the intent behind the query e, it is relatively more likely that the user will use the query q to express e again during her interaction with the DBMS. On the other hand, if the DBMS returns a result that does not contain any tuple that is relevant to the e, it less likely that the user expresses e using q in future. In fact, researchers have observed that users show reinforcement learning behavior when interacting with a DBMS over a period of time [11]. In particular, the authors in [11] have shown that some groups of users learned to formulate queries with a model similar to Roth-Erev reinforcement learning. We define the similarity measure as follows. For simplicity we assume that m = o and use the following similarity measure:\nriℓ =\n{\n1 if i = ℓ, 0 otherwise .\nIn this case, we assume that the user adapts to the DBMS strategy at time steps 0 < t1 < · · · < tk < · · · and in those time-steps the DBMS is not adapting as there is no reason to assume the synchronicity between the user and the DBMS. The reinforcement learning mechanism for the user is as follows:\na. Let S(0) > 0 be an m × n reward matrix whose entries are strictly positive.\nb. Let U(0) be the initial user’s strategy with\nUij(0) = Sij(0)\n∑n j′=1 Sij′ (0)\nfor all i ∈ [m] and j ∈ [n]. and let U(tk) = U(tk − 1) = · · · = U(tk−1 + 1) for all k.\nc. For all k ≥ 1, do the following:\ni. The nature picks a random intent t ∈ [m] with probability πi (independent of the earlier choices of the nature) and the user picks a query j ∈ [n] with probability\nP (q(tk) = j | i(tk) = i) = Uij(tk).\nii. The DBMS uses the current strategyD(tk) and interpret the query by the intent i′(t) = i′ with probability\nU(i′(tk) = i ′ | q(tk) = j) = Dji′(tk).\niii. User gives a reward 1 if i = i′ and otherwise, gives no rewards, i.e.\nS+ij =\n{\nSij(tk) + 1 if j = q(tk) and i(tk) = i ′(tk) Sij(tk) otherwise\nwhere S+ij = Sij(tk + 1).\niv. Update the user’s strategy by\nUij(tk + 1) = Uji(tk + 1) ∑n\nj′=1 Sij′ (tk + 1) , (22)\nfor all i ∈ [m] and j ∈ [n].\nIn the above scheme S(t) is the reward matrix at time t for the user."
    }, {
      "heading" : "5.4 Analysis of User and DBMS Adaptation",
      "text" : "In this section, we provide an analysis of the reinforcement mechanism provided above and will show that, statistically speaking, our proposed adaptation rule for DBMS, even when the user adapts, leads to improvement of the efficiency of the interaction. With a slight abuse of notation, let\nu(t) := ur(U,D(t)) = ur(U(t), D(t)), (23)\nfor an effectiveness measure r as ur is defined in (1).\nLEMMA 5.4. Let t = tk for some k ∈ N. Then, for any i ∈ [m] and j ∈ [n], we have\nE(U+ij | Ft)− Uij = πiUij ∑n\nℓ=1 Siℓ + 1 (Dji − u\ni(t)) (24)\nwhere\nui(t) =\nn ∑\nj=1\nUij(t)Dji(t).\nPROOF. Fix i ∈ [m], j ∈ [n] and k ∈ N. Let B be the event that at the tk’th iteration, user reinforces a pair (i, ℓ) for some ℓ ∈ [n]. Then, on the complement Bc of B, P+ij (ω) = Pij(ω). Let B1 ⊆ B be the subset of B such that\nthe pair (i, j) is reinforced and B2 = B \\ B1 be the event that some other pair (i, ℓ) is reinforced for ℓ 6= i. We note that\nU+ij = Sij + 1\n∑n ℓ=1 Siℓ + 1\n1B1 + Sij\n∑n ℓ=1 Siℓ + 1\n1B2 + Uij1Bc .\nTherefore, we have\nE(U+ij | Fkt) = πiUijDji Sij + 1\n∑n ℓ=1 Siℓ + 1\n+ ∑\nℓ 6=j\nπiUiℓDℓi Sij ∑n\nℓ′=1 Siℓ′ + 1 + (1− p)Uij ,\nwhere p = U(B | Fkt) = ∑\nℓ πiUijDji. Note that Uij = Sij∑ n ℓ=1 Siℓ and hence,\nE(U+ij | Ft)− Uij =\n1 ∑n\nℓ′=1 Siℓ′ + 1\n(\nπiUijDji − πiUij ∑\nℓ\nUiℓDℓi\n)\n.\nwhich can be rewritten as in (24).\nUsing Lemma 5.1, we show that the process {u(t)} is a sub-martingale.\nTHEOREM 5.5. Let t = tk for some k ∈ N. Then, we have\nE(u(t+ 1) | Ft)− u(t) ≥ 0 (25)\nwhere u(t) is given by (23).\nPROOF. Fix t = tk for some k ∈ N. Let u + := u(t+ 1), u := u(t), ui := ui(U(t), D(t)) and also define S̃i := ∑m\nℓ′=1 Siℓ′ + 1. Then, using the linearity of conditional expectation and Lemma 5.1, we have:\nE(u+ | Ft)− u =\nm ∑\ni=1\nn ∑\nj=1\nπiDji ( E(U+ij | Ft)− Uij )\n=\nm ∑\ni=1\nn ∑\nj=1\nπiDji πiUij ∑m\nℓ′=1 Sjℓ′ + 1\n( Dji − u i )\n=\nm ∑\ni=1\nπ2i S̃i\n\n\nn ∑\nj=1\nUij(Dji) 2 − (ui)2\n\n .\n(26)\nNote thatU is a row-stochasticmatrix and hence, ∑m\ni=1 Uij = 1. Therefore, by the Jensen’s inequality [19], we have:\nn ∑\nj=1\nUij(Dji) 2 ≥\n\n\nn ∑\nj=1\nDjiUij\n\n\n2\n= (ui)2.\nReplacing this in the right-hand-side of (26), we conclude that E(u+ | Ft)− u ≥ 0 and hence, the sequence {u(t)} is a submartingale.\nCOROLLARY 5.6. The sequence {u(t)} given by (15) converges almost surely.\nPROOF. Note from Theorem 5.3 and 5.5 that the sequence {u(t)} satisfies all the conditions of Theorem 5.2. Hence, proven."
    }, {
      "heading" : "5.5 Limiting Equilibria",
      "text" : "One of the most important immediate directions to be explored is to study the limiting behavior of the adaptation mechanism and identify its properties. Such questions are partially addressed in [32] for the case that both the players in a singling game adapt their strategy synchronously and identically. In this case, in the limit, only certain equilibria can emerge. In particular, eventually, either an object, intent in our model, will be assigned to many signals, queries in our model, (synonyms) or many intents can be assigned to one query (polysemy). But two intents will not be assigned to two queries (see Theorem 2.3 in [32] for more details). The authors in [32] follow a traditional language game approach as explained in Section 6. Our immediate future research directions on the study of the adaptation mechanism is to study the convergence properties of the proposed reinforcement algorithm in Section 5.1 and Section 5.3. In particular, when the user is not adapting, does the strategy of the DBMS converge to some limiting strategy such as is the best response to the user’s strategy? Also, when both the user and the DBMS adapt to each other, what equilibria will emerge? We believe that answers to these questions will significantly contribute to the proposed learning framework in the database systems and provide a novel theoretical perspective on the efficiency of a query interface."
    }, {
      "heading" : "6. RELATED WORK",
      "text" : "Signaling games model communication between two or more agents and have been widely used in economics, sociology, biology, and linguistics [39, 15, 49, 18]. Generally speaking, in a signaling game a player observes the current state of the world and informs the other player(s) by sending a signal. The other player interprets the signal and makes a decision and/or performs an action that affect the payoff of both players. A signaling game may not be cooperative in which the interests of players do not coincide [15]. Our framework extends a particular category of signaling games called language games [61, 49, 18] and is closely related to learning in signaling games [32, 59, 23]. These games have been used to model the evolution of a population’s language in a shared environment. In a language game, the strategy of each player is a stochastic mapping between a set of signals and a set of states. Each player observes its internal state, picks a signal according its strategy, and sends the signal to inform other player(s) about its state. If the other player(s) interpret the correct state from the sent signal, the communication is successful and both players will be rewarded. Our framework, however, differs from language games in several fundamental aspects. First, in a language game every player signals, but only one of our players, i.e., user, sends signals. Second, language games model states as an unstructured set of objects. However, each user’s intent in our framework is a set of tuples and different intents may intersect. Third, we use widely used similarity functions between the desired and returned answers for a query to measure the degree of success of answering a\nquery. Fourth, the signals in language games do not posses any particular meaning and can be assigned to every state. A database query, however, restricts its possible answers. Finally, there is not any work on language games on analyzing the dynamics of reinforcement learning where players learn in different time scales. Game theoretic approaches have been used in various areas of computer science, such as distributed systems, planning, security, and data mining [3, 38, 27, 55, 53, 28, 43]. Researchers have also leveraged economical models to build query interfaces that return desired results to the users using the fewest possible interactions [66]. In particular, researchers have recently applied game theoretic approaches to model the actions taken by users and document retrieval systems in a single session [41]. They propose a framework to find out whether the user likes to continue exploring the current topic or move to another topic. We, however, explore the development of common representations of intents between the user and DMBS. We also investigate the querying and interactions that may span over multiple queries and sessions. Moreover, we analyze equilibria of the game and convergence rates of some strategy adaptation methods for the user and DBMS. Finally, we focus on structured rather than unstructured data. Avestani et al. have used signaling games to create a shared lexicon between multiple autonomous systems [6]. We, however, focus on modeling users’ information needs and development of mutual understanding between users and the DBMS. Moreover, as opposed to the autonomous systems, a DBMS and user may update their information about the interaction in different time scales. We also provide a rigorous analysis of the game equilibria and propose novel strategy adaptation mechanisms for the DBMS. We have proposed the possibility of using signaling games to model the interaction between users and DBMS and provided an initial result in a short paper in [58]. The current paper precisely formalizes and significantly develops our framework, provides new results, and extends our previous result for practical settings. First, we precisely define the concepts in our framework, e.g., intent and query, and their relationships. Second, we define the equilibria of the game, analyze their properties, and show some equilibria are desirable. Third, our previous work models a database as a set of tuples and assumes that the desired answer for each query is a single tuple. These assumptions do not generally hold in practice. In this paper, we analyze both the equilibria and learning algorithms for the game where the answers to a query are a set or ranked lists of tuples. Usable query interfaces usually only partially satisfy the user’s information needs. However, authors in [58] assume that a result either fully satisfies the user’s information need or does not contain information relevant to the submitted intent. In this paper, we model the cases where results partially satisfy user’s intents. We show that the ability to return results that partially match the desired answers of a query significantly impacts the equilibria of the game and the algorithms for updating the strategy of the DBMS. Finally, we rigorously analyze our proposed algorithm for updating DBMS strategy where user also changes her strategy in a possibly different time scale than the DBMS."
    }, {
      "heading" : "7. CONCLUSION & FUTUREWORK",
      "text" : "Many users query and explore databases interactivity and over a relatively long period of time. We modeled the interaction between the user and the DBMS as a signaling game, where the players start with different mappings between queries and desired results, and would like to reach a mutual understanding. We formally analyzed different equilibria of this game. We also showed that some equilibria may not have an optimal payoff. We proposed an adaptation mechanism for the DBMS to learn the querying strategy of the user and proved that this mechanism increases the expected payoff for both user and the DBMS in average and converges almost surely. This case was extended for when the user changes her strategy and we provided an adaptation mechanism for the DBMS that converges almost surely. Interactive querying of databases has recently received a lot of attention [34]. We believe that our proposed framework will provide a basis for deep exploration of some interesting and important problems in this context. In particular, we plan to explore the properties of Nash and efficient Nash equilibria in this game. We also plan to analyze other types of equilibria ans stable states for the game. Moreover, we would like to design and explore other adaptation algorithms for the DBMS that converge to efficient equilibria and are robust to various user’s adaptation mechanisms. The equilibria discussed in this paper were defined for a database instance. We believe that one may find some equilibria that share important properties over all databases instances of the same schema. This will open the door to explore the properties of the game in database schema level."
    }, {
      "heading" : "8. REFERENCES",
      "text" : "[1] S. Abiteboul, R. Hull, and V. Vianu. Foundations of Databases: The Logical Level. Addison-Wesley, 1994.\n[2] A. Abouzied, D. Angluin, C. H. Papadimitriou, J. M. Hellerstein, and A. Silberschatz. Learning and verifying quantified boolean queries by example. In PODS, 2013.\n[3] I. Abraham, D. Dolev, R. Gonen, and J. Halpern. Distributed computing meets game theory: robust mechanisms for rational secret sharing and multiparty computation. In PODC, 2006.\n[4] S. Agrawal, S. Chaudhuri, G. Das, and A. Gionis. Automated ranking of database query results. In CIDR, 2003.\n[5] P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem.Machine learning, 47(2-3):235–256, 2002.\n[6] P. Avesani and M. Cova. Shared lexicon for distributed annotations on the Web. InWWW, 2005.\n[7] J. A. Barrett and K. Zollman. The role of forgetting in the evolution and learning of language. Journal of Experimental and Theoretical Artificial Intelligence, 21(4):293–309, 2008.\n[8] T. Beckers et al. Report on INEX 2009. SIGIR Forum, 44(1), 2010.\n[9] A. Bonifati, R. Ciucanu, and S. Staworko. Learning join queries from user examples. TODS, 40(4), 2015.\n[10] R. R. Bush and F. Mosteller. A stochastic model with\napplications to learning. The Annals of Mathematical Statistics, pages 559–585, 1953.\n[11] Y. Cen, L. Gan, and C. Bai. Reinforcement learning in information searching. Information Research: An International Electronic Journal, 18(1):n1, 2013.\n[12] S. Chaudhuri, G. Das, V. Hristidis, and G. Weikum. Probabilistic information retrieval approach for ranking of database query results. TODS, 31(3), 2006.\n[13] Y. Chen, W. Wang, Z. Liu, and X. Lin. Keyword search on structured and semi-structured data. In SIGMOD, 2009.\n[14] Y. Chen, W. Wang, Z. Liu, and X. Lin. Keyword search on structured and semi-structured data. In SIGMOD, 2009.\n[15] I. Cho and D. Kreps. Signaling games and stable equilibria. Quarterly Journal of Economics, 102, 1987.\n[16] J. G. Cross. A stochastic learning model of economic behavior. The Quarterly Journal of Economics, 87(2):239–266, 1973.\n[17] K. Dimitriadou, O. Papaemmanouil, and Y. Diao. Explore-by-example: An automatic query steering framework for interactive data exploration. In SIGMOD, 2014.\n[18] M. C. Donaldson, M. Lachmannb, and C. T. Bergstroma. The evolution of functionally referential meaning in a structured world. Journal of Mathematical Biology, 246, 2007.\n[19] R. Durrett. Probability: theory and examples. Cambridge university press, 2010.\n[20] I. Erev and A. E. Roth. On the Need for Low Rationality, Gognitive Game Theory: Reinforcement Learning in Experimental Games with Unique, Mixed Strategy Equilibria. 1995.\n[21] R. Fagin, B. Kimelfeld, Y. Li, S. Raghavan, and S. Vaithyanathan. Understanding Queries in a Search Database System. In PODS, 2010.\n[22] G. H. L. Fletcher, J. V. D. Bussche, D. V. Gucht, and S. Vansummeren. Towards a theory of search queries. In ICDT, 2009.\n[23] M. J. Fox, B. Touri, and J. S. Shamma. Dynamics in atomic signaling games. Journal of theoretical biology, 376, 2015.\n[24] D. Fudenberg and D. Levine. The Theory of Learning in Games. MIT Press, 1998.\n[25] N. Fuhr and T. Rolleke. A probabilistic relational algebra for the integration of information retrieval and database systems. TOIS, 15, 1997.\n[26] A. Ghosh and S. Sen. Learning toms: Towards non-myopic equilibria. In AAAI, 2004.\n[27] G. D. Giacomo, , A. D. Stasio, A. Murano, and S. Rubin. Imperfect information games and generalized planning. In IJCAI, 2016.\n[28] G. Gottlob, N. Leone, and F. Scarcello. Robbers, marshals, and guards: Game theoretic and logical characterizations of hypertree width. In PODS, 2001.\n[29] L. A. Granka, T. Joachims, and G. Gay. Eye-tracking analysis of user behavior in www search. In SIGIR,\n2004.\n[30] K. Hofmann, S. Whiteson, and M. de Rijke. Balancing exploration and exploitation in listwise and pairwise online learning to rank for information retrieval. Information Retrieval, 16(1), 2013.\n[31] V. Hristidis, L. Gravano, and Y. Papakonstantinou. Efficient IR-Style Keyword Search over Relational Databases. In VLDB 2003.\n[32] Y. Hu, B. Skyrms, and P. Tarrès. Reinforcement learning in signaling game. arXiv preprint arXiv:1103.5818, 2011.\n[33] J. Huang, R. White, and G. Buscher. User see, user point: Gaze and cursor alignment in web search. In CHI, 2012.\n[34] S. Idreos, O. Papaemmanouil, and S. Chaudhuri. Overview of data exploration techniques. In SIGMOD, 2015.\n[35] H. V. Jagadish, A. Chapman, A. Elkiss, M. Jayapandian, Y. Li, A. Nandi, and C. Yu. Making database systems usable. In SIGMOD, 2007.\n[36] N. Khoussainova, Y. Kwon, M. Balazinska, and D. Suciu. Snipsuggest: Context-aware autocompletion for sql. PVLDB, 4(1), 2010.\n[37] B. Kimelfeld and Y. Sagiv. Finding and Approximating Top-k Answers in Keyword Proximity Search. In PODS, 2005.\n[38] D. Koller and A. Pfeffer. Generating and solving imperfect information games. In IJCAI, 1995.\n[39] D. Lewis. Convention. Cambridge: Harvard University Press, 1969.\n[40] H. Li, C.-Y. Chan, and D. Maier. Query from examples: An iterative, data-driven approach to query construction. PVLDB, 8(13), 2015.\n[41] J. Luo, S. Zhang, and H. Yang. Win-win search: Dual-agent stochastic game in session search. In SIGIR, 2014.\n[42] Y. Luo, X. Lin, W. Wang, and X. Zhou. SPARK: Top-k Keyword Query in Relational Databases. In SIGMOD 2007.\n[43] Q. Ma, S. Muthukrishnan, B. Thompson, and G. Cormode. Modeling collaboration in academia: A game theoretic approach. In BigScholar, 2014.\n[44] D. Maier, D. Rozenshtein, S. Salveter, J. Stein, and D. S. Warren. Toward logical data independence: A relational query language without relations. In SIGMOD, 1982.\n[45] C. Manning, P. Raghavan, and H. Schutze. An Introduction to Information Retrieval. Cambridge University Press, 2008.\n[46] A. Nandi and H. V. Jagadish. Guided interaction: Rethinking the query-result paradigm. PVLDB, 102, 2011.\n[47] Y. Niv. The neuroscience of reinforcement learning. In ICML, 2009.\n[48] Y. Niv. Reinforcement learning in the brain. The Journal of Mathematical Psychology, 53(3):139–154, 2009.\n[49] M. A. Nowak and D. C. Krakauer. The evolution of\nlanguage. PNAS, 96(14), 1999.\n[50] H. Robbins and D. Siegmund. A convergence theorem for non negative almost supermartingales and some applications. In Herbert Robbins Selected Papers. Springer, 1985.\n[51] A. E. Roth and I. Erev. Learning in extensive-form games: Experimental data and simple dynamic models in the intermediate term. Games and economic behavior, 8(1):164–212, 1995.\n[52] A. D. Sarmaa, A. Parameswaran, H. Garcia-Molina, and J. Widom. Synthesizing view definitions from data. In ICDT, 2010.\n[53] M. Schuster and T. Schwentick. Games for active XML revisited. In ICDT, 2015.\n[54] L. S. Shapley et al. Some topics in two-person games. Advances in game theory, 52(1-29):1–2, 1964.\n[55] Y. Shoham. Computer science and game theory. Commun. ACM, 51(8), 2008.\n[56] H. Shteingart and Y. Loewenstein. Reinforcement learning and human behavior. Current Opinion in Neurobiology, 25:93–98, 04/2014 2014.\n[57] S. Tadelis. Game Theory: An Introduction. Princeton University Press, 2013.\n[58] A. Termehchy and B. Touri. A signaling game approach to database querying and interaction. In ICTIR, 2015.\n[59] B. Touri and C. Langbort. Language evolution in a noisy environment. In (ACC). IEEE, 2013.\n[60] Q. Tran, C. Chan, and S. Parthasarathy. Query by output. In SIGMOD, 2009.\n[61] P. Trapa and M. Nowak. Nash equilibria for an evolutionary language game. Journal of Mathematical Biology, 41, 2000.\n[62] A. Vorobev, D. Lefortier, G. Gusev, and P. Serdyukov. Gathering additional feedback on search results by multi-armed bandits with respect to production ranking. InWWW, 2015.\n[63] Yahoo! Yahoo! webscope dataset anonymized Yahoo! search logs with relevance judgments version 1.0. http://labs.yahoo.com/Academic_Relations, 2011. [Online; accessed 5-January-2017].\n[64] H. P. Young. Strategic learning and its limits. OUP Oxford, 2004.\n[65] Y. Yue, J. Broder, R. Kleinberg, and T. Joachims. The k-armed dueling bandits problem. J. Comput. Syst. Sci., 78(5), 2012.\n[66] C. Zhai. Towards a game-theoretic framework for information retrieval. In SIGIR, 2015.\n[67] X. Zhou, J. Gaugaz, W.-T. Balke, and W. Nejdl. Query relaxation using malleable schemas. In SIGMOD, 2007."
    } ],
    "references" : [ {
      "title" : "Foundations of Databases: The Logical Level",
      "author" : [ "S. Abiteboul", "R. Hull", "V. Vianu" ],
      "venue" : "Addison-Wesley,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Learning and verifying quantified boolean queries by example",
      "author" : [ "A. Abouzied", "D. Angluin", "C.H. Papadimitriou", "J.M. Hellerstein", "A. Silberschatz" ],
      "venue" : "PODS,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Distributed computing meets game theory: robust mechanisms for rational secret sharing and multiparty computation",
      "author" : [ "I. Abraham", "D. Dolev", "R. Gonen", "J. Halpern" ],
      "venue" : "PODC,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Automated ranking of database query results",
      "author" : [ "S. Agrawal", "S. Chaudhuri", "G. Das", "A. Gionis" ],
      "venue" : "CIDR,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Finite-time analysis of the multiarmed bandit problem.Machine learning",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "P. Fischer" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2002
    }, {
      "title" : "Shared lexicon for distributed annotations on the Web",
      "author" : [ "P. Avesani", "M. Cova" ],
      "venue" : "InWWW,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "The role of forgetting in the evolution and learning of language",
      "author" : [ "J.A. Barrett", "K. Zollman" ],
      "venue" : "Journal of Experimental and Theoretical Artificial Intelligence, 21(4):293–309,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Learning join queries from user examples",
      "author" : [ "A. Bonifati", "R. Ciucanu", "S. Staworko" ],
      "venue" : "TODS, 40(4),",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A stochastic model with  applications to learning",
      "author" : [ "R.R. Bush", "F. Mosteller" ],
      "venue" : "The Annals of Mathematical Statistics, pages 559–585,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1953
    }, {
      "title" : "Reinforcement learning in information searching",
      "author" : [ "Y. Cen", "L. Gan", "C. Bai" ],
      "venue" : "Information Research: An International Electronic Journal, 18(1):n1,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Probabilistic information retrieval approach for ranking of database query results",
      "author" : [ "S. Chaudhuri", "G. Das", "V. Hristidis", "G. Weikum" ],
      "venue" : "TODS, 31(3),",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Keyword search on structured and semi-structured data",
      "author" : [ "Y. Chen", "W. Wang", "Z. Liu", "X. Lin" ],
      "venue" : "SIGMOD,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Keyword search on structured and semi-structured data",
      "author" : [ "Y. Chen", "W. Wang", "Z. Liu", "X. Lin" ],
      "venue" : "SIGMOD,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Signaling games and stable equilibria",
      "author" : [ "I. Cho", "D. Kreps" ],
      "venue" : "Quarterly Journal of Economics, 102,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "A stochastic learning model of economic behavior",
      "author" : [ "J.G. Cross" ],
      "venue" : "The Quarterly Journal of Economics, 87(2):239–266,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1973
    }, {
      "title" : "Explore-by-example: An automatic query steering framework for interactive data exploration",
      "author" : [ "K. Dimitriadou", "O. Papaemmanouil", "Y. Diao" ],
      "venue" : "SIGMOD,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "The evolution of functionally referential meaning in a structured world",
      "author" : [ "M.C. Donaldson", "M. Lachmannb", "C.T. Bergstroma" ],
      "venue" : "Journal of Mathematical Biology, 246,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Probability: theory and examples",
      "author" : [ "R. Durrett" ],
      "venue" : "Cambridge university press,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "On the Need for Low Rationality, Gognitive Game Theory: Reinforcement Learning in Experimental Games with Unique",
      "author" : [ "I. Erev", "A.E. Roth" ],
      "venue" : "Mixed Strategy Equilibria",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1995
    }, {
      "title" : "Understanding Queries in a Search Database System",
      "author" : [ "R. Fagin", "B. Kimelfeld", "Y. Li", "S. Raghavan", "S. Vaithyanathan" ],
      "venue" : "PODS,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Towards a theory of search queries",
      "author" : [ "G.H.L. Fletcher", "J.V.D. Bussche", "D.V. Gucht", "S. Vansummeren" ],
      "venue" : "ICDT,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Dynamics in atomic signaling games",
      "author" : [ "M.J. Fox", "B. Touri", "J.S. Shamma" ],
      "venue" : "Journal of theoretical biology, 376,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "The Theory of Learning in Games",
      "author" : [ "D. Fudenberg", "D. Levine" ],
      "venue" : "MIT Press,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "A probabilistic relational algebra for the integration of information retrieval and database systems",
      "author" : [ "N. Fuhr", "T. Rolleke" ],
      "venue" : "TOIS, 15,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Learning toms: Towards non-myopic equilibria",
      "author" : [ "A. Ghosh", "S. Sen" ],
      "venue" : "AAAI,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Imperfect information games and generalized planning",
      "author" : [ "G.D. Giacomo", "A.D. Stasio", "A. Murano", "S. Rubin" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2016
    }, {
      "title" : "Robbers, marshals, and guards: Game theoretic and logical characterizations of hypertree width",
      "author" : [ "G. Gottlob", "N. Leone", "F. Scarcello" ],
      "venue" : "PODS,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Eye-tracking analysis of user behavior in www search",
      "author" : [ "L.A. Granka", "T. Joachims", "G. Gay" ],
      "venue" : "SIGIR, ",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Balancing exploration and exploitation in listwise and pairwise online learning to rank for information retrieval",
      "author" : [ "K. Hofmann", "S. Whiteson", "M. de Rijke" ],
      "venue" : "Information Retrieval,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2013
    }, {
      "title" : "Efficient IR-Style Keyword Search over Relational Databases",
      "author" : [ "V. Hristidis", "L. Gravano", "Y. Papakonstantinou" ],
      "venue" : "VLDB",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Reinforcement learning in signaling game",
      "author" : [ "Y. Hu", "B. Skyrms", "P. Tarrès" ],
      "venue" : "arXiv preprint arXiv:1103.5818,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "User see, user point: Gaze and cursor alignment in web search",
      "author" : [ "J. Huang", "R. White", "G. Buscher" ],
      "venue" : "CHI,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Overview of data exploration techniques",
      "author" : [ "S. Idreos", "O. Papaemmanouil", "S. Chaudhuri" ],
      "venue" : "SIGMOD,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Making database systems usable",
      "author" : [ "H.V. Jagadish", "A. Chapman", "A. Elkiss", "M. Jayapandian", "Y. Li", "A. Nandi", "C. Yu" ],
      "venue" : "SIGMOD,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Snipsuggest: Context-aware autocompletion for sql",
      "author" : [ "N. Khoussainova", "Y. Kwon", "M. Balazinska", "D. Suciu" ],
      "venue" : "PVLDB, 4(1),",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Finding and Approximating Top-k Answers in Keyword Proximity Search",
      "author" : [ "B. Kimelfeld", "Y. Sagiv" ],
      "venue" : "PODS,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Generating and solving imperfect information games",
      "author" : [ "D. Koller", "A. Pfeffer" ],
      "venue" : "IJCAI,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Convention",
      "author" : [ "D. Lewis" ],
      "venue" : "Cambridge: Harvard University Press,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 1969
    }, {
      "title" : "Query from examples: An iterative, data-driven approach to query construction",
      "author" : [ "H. Li", "C.-Y. Chan", "D. Maier" ],
      "venue" : "PVLDB, 8(13),",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Win-win search: Dual-agent stochastic game in session search",
      "author" : [ "J. Luo", "S. Zhang", "H. Yang" ],
      "venue" : "SIGIR,",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "SPARK: Top-k Keyword Query in Relational Databases",
      "author" : [ "Y. Luo", "X. Lin", "W. Wang", "X. Zhou" ],
      "venue" : "SIGMOD",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Modeling collaboration in academia: A game theoretic approach",
      "author" : [ "Q. Ma", "S. Muthukrishnan", "B. Thompson", "G. Cormode" ],
      "venue" : "BigScholar,",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Toward logical data independence: A relational query language without relations",
      "author" : [ "D. Maier", "D. Rozenshtein", "S. Salveter", "J. Stein", "D.S. Warren" ],
      "venue" : "SIGMOD,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 1982
    }, {
      "title" : "An Introduction to Information Retrieval",
      "author" : [ "C. Manning", "P. Raghavan", "H. Schutze" ],
      "venue" : "Cambridge University Press,",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Guided interaction: Rethinking the query-result paradigm",
      "author" : [ "A. Nandi", "H.V. Jagadish" ],
      "venue" : "PVLDB, 102,",
      "citeRegEx" : "46",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "The neuroscience of reinforcement learning",
      "author" : [ "Y. Niv" ],
      "venue" : "ICML,",
      "citeRegEx" : "47",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Reinforcement learning in the brain",
      "author" : [ "Y. Niv" ],
      "venue" : "The Journal of Mathematical Psychology, 53(3):139–154,",
      "citeRegEx" : "48",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "The evolution of  language",
      "author" : [ "M.A. Nowak", "D.C. Krakauer" ],
      "venue" : "PNAS, 96(14),",
      "citeRegEx" : "49",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "A convergence theorem for non negative almost supermartingales and some applications",
      "author" : [ "H. Robbins", "D. Siegmund" ],
      "venue" : "Herbert Robbins Selected Papers. Springer,",
      "citeRegEx" : "50",
      "shortCiteRegEx" : null,
      "year" : 1985
    }, {
      "title" : "Learning in extensive-form games: Experimental data and simple dynamic models in the intermediate term",
      "author" : [ "A.E. Roth", "I. Erev" ],
      "venue" : "Games and economic behavior, 8(1):164–212,",
      "citeRegEx" : "51",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Synthesizing view definitions from data",
      "author" : [ "A.D. Sarmaa", "A. Parameswaran", "H. Garcia-Molina", "J. Widom" ],
      "venue" : "ICDT,",
      "citeRegEx" : "52",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Games for active XML revisited",
      "author" : [ "M. Schuster", "T. Schwentick" ],
      "venue" : "ICDT,",
      "citeRegEx" : "53",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Some topics in two-person games",
      "author" : [ "L.S. Shapley" ],
      "venue" : "Advances in game theory,",
      "citeRegEx" : "54",
      "shortCiteRegEx" : "54",
      "year" : 1964
    }, {
      "title" : "Computer science and game theory",
      "author" : [ "Y. Shoham" ],
      "venue" : "Commun. ACM, 51(8),",
      "citeRegEx" : "55",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Reinforcement learning and human behavior",
      "author" : [ "H. Shteingart", "Y. Loewenstein" ],
      "venue" : "Current Opinion in Neurobiology, 25:93–98, 04/2014",
      "citeRegEx" : "56",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Game Theory: An Introduction",
      "author" : [ "S. Tadelis" ],
      "venue" : "Princeton University Press,",
      "citeRegEx" : "57",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A signaling game approach to database querying and interaction",
      "author" : [ "A. Termehchy", "B. Touri" ],
      "venue" : "ICTIR,",
      "citeRegEx" : "58",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Language evolution in a noisy environment",
      "author" : [ "B. Touri", "C. Langbort" ],
      "venue" : "(ACC). IEEE,",
      "citeRegEx" : "59",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Query by output",
      "author" : [ "Q. Tran", "C. Chan", "S. Parthasarathy" ],
      "venue" : "SIGMOD,",
      "citeRegEx" : "60",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Nash equilibria for an evolutionary language game",
      "author" : [ "P. Trapa", "M. Nowak" ],
      "venue" : "Journal of Mathematical Biology, 41,",
      "citeRegEx" : "61",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Gathering additional feedback on search results by multi-armed bandits with respect to production ranking",
      "author" : [ "A. Vorobev", "D. Lefortier", "G. Gusev", "P. Serdyukov" ],
      "venue" : "InWWW,",
      "citeRegEx" : "62",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Strategic learning and its limits",
      "author" : [ "H.P. Young" ],
      "venue" : "OUP Oxford,",
      "citeRegEx" : "64",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "The k-armed dueling bandits problem",
      "author" : [ "Y. Yue", "J. Broder", "R. Kleinberg", "T. Joachims" ],
      "venue" : "J. Comput. Syst. Sci., 78(5),",
      "citeRegEx" : "65",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Towards a game-theoretic framework for information retrieval",
      "author" : [ "C. Zhai" ],
      "venue" : "SIGIR,",
      "citeRegEx" : "66",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Query relaxation using malleable schemas",
      "author" : [ "X. Zhou", "J. Gaugaz", "W.-T. Balke", "W. Nejdl" ],
      "venue" : "SIGMOD,",
      "citeRegEx" : "67",
      "shortCiteRegEx" : null,
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 33,
      "context" : "Most users do not know the structure and/or content of databases and cannot precisely express their queries [35, 13, 21, 34].",
      "startOffset" : 108,
      "endOffset" : 124
    }, {
      "referenceID" : 11,
      "context" : "Most users do not know the structure and/or content of databases and cannot precisely express their queries [35, 13, 21, 34].",
      "startOffset" : 108,
      "endOffset" : 124
    }, {
      "referenceID" : 19,
      "context" : "Most users do not know the structure and/or content of databases and cannot precisely express their queries [35, 13, 21, 34].",
      "startOffset" : 108,
      "endOffset" : 124
    }, {
      "referenceID" : 32,
      "context" : "Most users do not know the structure and/or content of databases and cannot precisely express their queries [35, 13, 21, 34].",
      "startOffset" : 108,
      "endOffset" : 124
    }, {
      "referenceID" : 42,
      "context" : "Database researchers have proposed models, methods and systems to help users specify their queries more precisely and DBMS understand users’ intents more accurately [44, 34, 13, 46, 35, 25, 8, 22, 37, 21, 2, 9, 52].",
      "startOffset" : 165,
      "endOffset" : 214
    }, {
      "referenceID" : 32,
      "context" : "Database researchers have proposed models, methods and systems to help users specify their queries more precisely and DBMS understand users’ intents more accurately [44, 34, 13, 46, 35, 25, 8, 22, 37, 21, 2, 9, 52].",
      "startOffset" : 165,
      "endOffset" : 214
    }, {
      "referenceID" : 11,
      "context" : "Database researchers have proposed models, methods and systems to help users specify their queries more precisely and DBMS understand users’ intents more accurately [44, 34, 13, 46, 35, 25, 8, 22, 37, 21, 2, 9, 52].",
      "startOffset" : 165,
      "endOffset" : 214
    }, {
      "referenceID" : 44,
      "context" : "Database researchers have proposed models, methods and systems to help users specify their queries more precisely and DBMS understand users’ intents more accurately [44, 34, 13, 46, 35, 25, 8, 22, 37, 21, 2, 9, 52].",
      "startOffset" : 165,
      "endOffset" : 214
    }, {
      "referenceID" : 33,
      "context" : "Database researchers have proposed models, methods and systems to help users specify their queries more precisely and DBMS understand users’ intents more accurately [44, 34, 13, 46, 35, 25, 8, 22, 37, 21, 2, 9, 52].",
      "startOffset" : 165,
      "endOffset" : 214
    }, {
      "referenceID" : 23,
      "context" : "Database researchers have proposed models, methods and systems to help users specify their queries more precisely and DBMS understand users’ intents more accurately [44, 34, 13, 46, 35, 25, 8, 22, 37, 21, 2, 9, 52].",
      "startOffset" : 165,
      "endOffset" : 214
    }, {
      "referenceID" : 20,
      "context" : "Database researchers have proposed models, methods and systems to help users specify their queries more precisely and DBMS understand users’ intents more accurately [44, 34, 13, 46, 35, 25, 8, 22, 37, 21, 2, 9, 52].",
      "startOffset" : 165,
      "endOffset" : 214
    }, {
      "referenceID" : 35,
      "context" : "Database researchers have proposed models, methods and systems to help users specify their queries more precisely and DBMS understand users’ intents more accurately [44, 34, 13, 46, 35, 25, 8, 22, 37, 21, 2, 9, 52].",
      "startOffset" : 165,
      "endOffset" : 214
    }, {
      "referenceID" : 19,
      "context" : "Database researchers have proposed models, methods and systems to help users specify their queries more precisely and DBMS understand users’ intents more accurately [44, 34, 13, 46, 35, 25, 8, 22, 37, 21, 2, 9, 52].",
      "startOffset" : 165,
      "endOffset" : 214
    }, {
      "referenceID" : 1,
      "context" : "Database researchers have proposed models, methods and systems to help users specify their queries more precisely and DBMS understand users’ intents more accurately [44, 34, 13, 46, 35, 25, 8, 22, 37, 21, 2, 9, 52].",
      "startOffset" : 165,
      "endOffset" : 214
    }, {
      "referenceID" : 7,
      "context" : "Database researchers have proposed models, methods and systems to help users specify their queries more precisely and DBMS understand users’ intents more accurately [44, 34, 13, 46, 35, 25, 8, 22, 37, 21, 2, 9, 52].",
      "startOffset" : 165,
      "endOffset" : 214
    }, {
      "referenceID" : 50,
      "context" : "Database researchers have proposed models, methods and systems to help users specify their queries more precisely and DBMS understand users’ intents more accurately [44, 34, 13, 46, 35, 25, 8, 22, 37, 21, 2, 9, 52].",
      "startOffset" : 165,
      "endOffset" : 214
    }, {
      "referenceID" : 38,
      "context" : "Given a user’s information need e, the DBMS estimates e by various methods including showing potential results to the user and collecting her feedback [40, 17, 9, 52, 60], asking questions from her [2], or suggesting potential queries to her [36].",
      "startOffset" : 151,
      "endOffset" : 170
    }, {
      "referenceID" : 15,
      "context" : "Given a user’s information need e, the DBMS estimates e by various methods including showing potential results to the user and collecting her feedback [40, 17, 9, 52, 60], asking questions from her [2], or suggesting potential queries to her [36].",
      "startOffset" : 151,
      "endOffset" : 170
    }, {
      "referenceID" : 7,
      "context" : "Given a user’s information need e, the DBMS estimates e by various methods including showing potential results to the user and collecting her feedback [40, 17, 9, 52, 60], asking questions from her [2], or suggesting potential queries to her [36].",
      "startOffset" : 151,
      "endOffset" : 170
    }, {
      "referenceID" : 50,
      "context" : "Given a user’s information need e, the DBMS estimates e by various methods including showing potential results to the user and collecting her feedback [40, 17, 9, 52, 60], asking questions from her [2], or suggesting potential queries to her [36].",
      "startOffset" : 151,
      "endOffset" : 170
    }, {
      "referenceID" : 58,
      "context" : "Given a user’s information need e, the DBMS estimates e by various methods including showing potential results to the user and collecting her feedback [40, 17, 9, 52, 60], asking questions from her [2], or suggesting potential queries to her [36].",
      "startOffset" : 151,
      "endOffset" : 170
    }, {
      "referenceID" : 1,
      "context" : "Given a user’s information need e, the DBMS estimates e by various methods including showing potential results to the user and collecting her feedback [40, 17, 9, 52, 60], asking questions from her [2], or suggesting potential queries to her [36].",
      "startOffset" : 198,
      "endOffset" : 201
    }, {
      "referenceID" : 34,
      "context" : "Given a user’s information need e, the DBMS estimates e by various methods including showing potential results to the user and collecting her feedback [40, 17, 9, 52, 60], asking questions from her [2], or suggesting potential queries to her [36].",
      "startOffset" : 242,
      "endOffset" : 246
    }, {
      "referenceID" : 28,
      "context" : "Types of the user’s feedbackmay include clicking on the relevant answers [30, 65], the amount of time the user spends on reading the results [29], or user’s eye movements [33].",
      "startOffset" : 73,
      "endOffset" : 81
    }, {
      "referenceID" : 62,
      "context" : "Types of the user’s feedbackmay include clicking on the relevant answers [30, 65], the amount of time the user spends on reading the results [29], or user’s eye movements [33].",
      "startOffset" : 73,
      "endOffset" : 81
    }, {
      "referenceID" : 27,
      "context" : "Types of the user’s feedbackmay include clicking on the relevant answers [30, 65], the amount of time the user spends on reading the results [29], or user’s eye movements [33].",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 31,
      "context" : "Types of the user’s feedbackmay include clicking on the relevant answers [30, 65], the amount of time the user spends on reading the results [29], or user’s eye movements [33].",
      "startOffset" : 171,
      "endOffset" : 175
    }, {
      "referenceID" : 22,
      "context" : "Furthermore, it has been shown that certain learning strategies that may be useful in a static setting do not converge to a desired outcome in a setting where both agents modify their strategies [24].",
      "startOffset" : 195,
      "endOffset" : 199
    }, {
      "referenceID" : 13,
      "context" : "• We model the long term interaction between the user and DBMS as a particular type of game called a signaling game in Section 2 [15, 49].",
      "startOffset" : 129,
      "endOffset" : 137
    }, {
      "referenceID" : 47,
      "context" : "• We model the long term interaction between the user and DBMS as a particular type of game called a signaling game in Section 2 [15, 49].",
      "startOffset" : 129,
      "endOffset" : 137
    }, {
      "referenceID" : 49,
      "context" : "• Using extensive empirical studies over a real-world query workload, we show that users’ learning can be accurately modeled by Roth and Erv’s reinforcement learning algorithm [51] in Section 4.",
      "startOffset" : 176,
      "endOffset" : 180
    }, {
      "referenceID" : 27,
      "context" : ", click-through information [29].",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 12,
      "context" : ", SQL, over the underlying database instance [14, 35, 21].",
      "startOffset" : 45,
      "endOffset" : 57
    }, {
      "referenceID" : 33,
      "context" : ", SQL, over the underlying database instance [14, 35, 21].",
      "startOffset" : 45,
      "endOffset" : 57
    }, {
      "referenceID" : 19,
      "context" : ", SQL, over the underlying database instance [14, 35, 21].",
      "startOffset" : 45,
      "endOffset" : 57
    }, {
      "referenceID" : 0,
      "context" : "In our examples, we consider the intent language to be conjunctive queries [1].",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 33,
      "context" : "Thus, they prefer to articulate their intents in languages that are easy-touse and relatively less complex, such as keyword query language [35, 14, 21].",
      "startOffset" : 139,
      "endOffset" : 151
    }, {
      "referenceID" : 12,
      "context" : "Thus, they prefer to articulate their intents in languages that are easy-touse and relatively less complex, such as keyword query language [35, 14, 21].",
      "startOffset" : 139,
      "endOffset" : 151
    }, {
      "referenceID" : 19,
      "context" : "Thus, they prefer to articulate their intents in languages that are easy-touse and relatively less complex, such as keyword query language [35, 14, 21].",
      "startOffset" : 139,
      "endOffset" : 151
    }, {
      "referenceID" : 10,
      "context" : "Nevertheless, because they may not know precisely the content and structure of the data, their submitted queries may not always be the same as their intents [12, 36].",
      "startOffset" : 157,
      "endOffset" : 165
    }, {
      "referenceID" : 34,
      "context" : "Nevertheless, because they may not know precisely the content and structure of the data, their submitted queries may not always be the same as their intents [12, 36].",
      "startOffset" : 157,
      "endOffset" : 165
    }, {
      "referenceID" : 19,
      "context" : "The DBMS usually selects a query language for the interpreted intents according to the information in the domain of interest [21, 31, 14].",
      "startOffset" : 125,
      "endOffset" : 137
    }, {
      "referenceID" : 29,
      "context" : "The DBMS usually selects a query language for the interpreted intents according to the information in the domain of interest [21, 31, 14].",
      "startOffset" : 125,
      "endOffset" : 137
    }, {
      "referenceID" : 12,
      "context" : "The DBMS usually selects a query language for the interpreted intents according to the information in the domain of interest [21, 31, 14].",
      "startOffset" : 125,
      "endOffset" : 137
    }, {
      "referenceID" : 29,
      "context" : "For example, some keyword query interfaces over relational databases interpret keyword queries as Select-Project-Join SQL queries with only conjunctions in the where clause [31, 42].",
      "startOffset" : 173,
      "endOffset" : 181
    }, {
      "referenceID" : 40,
      "context" : "For example, some keyword query interfaces over relational databases interpret keyword queries as Select-Project-Join SQL queries with only conjunctions in the where clause [31, 42].",
      "startOffset" : 173,
      "endOffset" : 181
    }, {
      "referenceID" : 19,
      "context" : "Furthermore, given a query, the DBMS explores finitely many alternative intents to interpret the query efficiently [21, 31, 14].",
      "startOffset" : 115,
      "endOffset" : 127
    }, {
      "referenceID" : 29,
      "context" : "Furthermore, given a query, the DBMS explores finitely many alternative intents to interpret the query efficiently [21, 31, 14].",
      "startOffset" : 115,
      "endOffset" : 127
    }, {
      "referenceID" : 12,
      "context" : "Furthermore, given a query, the DBMS explores finitely many alternative intents to interpret the query efficiently [21, 31, 14].",
      "startOffset" : 115,
      "endOffset" : 127
    }, {
      "referenceID" : 43,
      "context" : "DBMSs often implement their strategies using a real-valued function called scoring function over pairs of queries and intents [45, 14, 21].",
      "startOffset" : 126,
      "endOffset" : 138
    }, {
      "referenceID" : 12,
      "context" : "DBMSs often implement their strategies using a real-valued function called scoring function over pairs of queries and intents [45, 14, 21].",
      "startOffset" : 126,
      "endOffset" : 138
    }, {
      "referenceID" : 19,
      "context" : "DBMSs often implement their strategies using a real-valued function called scoring function over pairs of queries and intents [45, 14, 21].",
      "startOffset" : 126,
      "endOffset" : 138
    }, {
      "referenceID" : 28,
      "context" : "It is shown, however, that such a deterministic mapping may limit the effectiveness of interpreting queries in long-term interactions [30, 65, 62, 5].",
      "startOffset" : 134,
      "endOffset" : 149
    }, {
      "referenceID" : 62,
      "context" : "It is shown, however, that such a deterministic mapping may limit the effectiveness of interpreting queries in long-term interactions [30, 65, 62, 5].",
      "startOffset" : 134,
      "endOffset" : 149
    }, {
      "referenceID" : 60,
      "context" : "It is shown, however, that such a deterministic mapping may limit the effectiveness of interpreting queries in long-term interactions [30, 65, 62, 5].",
      "startOffset" : 134,
      "endOffset" : 149
    }, {
      "referenceID" : 4,
      "context" : "It is shown, however, that such a deterministic mapping may limit the effectiveness of interpreting queries in long-term interactions [30, 65, 62, 5].",
      "startOffset" : 134,
      "endOffset" : 149
    }, {
      "referenceID" : 28,
      "context" : "Hence, the DBMS may adopt a stochastic strategy:it may randomly select and show the results of intents such that the ones with higher scores are more often chosen [30, 65, 62, 5].",
      "startOffset" : 163,
      "endOffset" : 178
    }, {
      "referenceID" : 62,
      "context" : "Hence, the DBMS may adopt a stochastic strategy:it may randomly select and show the results of intents such that the ones with higher scores are more often chosen [30, 65, 62, 5].",
      "startOffset" : 163,
      "endOffset" : 178
    }, {
      "referenceID" : 60,
      "context" : "Hence, the DBMS may adopt a stochastic strategy:it may randomly select and show the results of intents such that the ones with higher scores are more often chosen [30, 65, 62, 5].",
      "startOffset" : 163,
      "endOffset" : 178
    }, {
      "referenceID" : 4,
      "context" : "Hence, the DBMS may adopt a stochastic strategy:it may randomly select and show the results of intents such that the ones with higher scores are more often chosen [30, 65, 62, 5].",
      "startOffset" : 163,
      "endOffset" : 178
    }, {
      "referenceID" : 60,
      "context" : "Researchers have shown that it is possible to find the stochastic strategy that results in returning sufficiently many answers to keep users engaged and sufficiently diverse results to collect useful feedback and improve significantly the effectiveness of query answering [62, 30].",
      "startOffset" : 272,
      "endOffset" : 280
    }, {
      "referenceID" : 28,
      "context" : "Researchers have shown that it is possible to find the stochastic strategy that results in returning sufficiently many answers to keep users engaged and sufficiently diverse results to collect useful feedback and improve significantly the effectiveness of query answering [62, 30].",
      "startOffset" : 272,
      "endOffset" : 280
    }, {
      "referenceID" : 43,
      "context" : "This reward is measured based on the user feedback and using standard effectiveness metrics [45].",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 3,
      "context" : "Due to their lack of information, users may sometimes submit queries that may not return any tuple over the underlying database [4].",
      "startOffset" : 128,
      "endOffset" : 131
    }, {
      "referenceID" : 3,
      "context" : "The DBMS typically map these queries to relatively broad and relaxed intents with non-empty result over the database [4, 67].",
      "startOffset" : 117,
      "endOffset" : 124
    }, {
      "referenceID" : 64,
      "context" : "The DBMS typically map these queries to relatively broad and relaxed intents with non-empty result over the database [4, 67].",
      "startOffset" : 117,
      "endOffset" : 124
    }, {
      "referenceID" : 12,
      "context" : ", p@k, for their input queries [14].",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 52,
      "context" : "Nevertheless, in some collaborative two-player games in which both players adapt their strategies to improve their payoff, the learning may not converge to any (desired) equilibrium and cycle among several unstable states [54, 64].",
      "startOffset" : 222,
      "endOffset" : 230
    }, {
      "referenceID" : 61,
      "context" : "Nevertheless, in some collaborative two-player games in which both players adapt their strategies to improve their payoff, the learning may not converge to any (desired) equilibrium and cycle among several unstable states [54, 64].",
      "startOffset" : 222,
      "endOffset" : 230
    }, {
      "referenceID" : 28,
      "context" : "More importantly, the DBMS should use an adaptation strategy that keeps users engaged [30].",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 13,
      "context" : "Our definition of pool of intent resembles the notion of pool of state in signaling games [15, 18].",
      "startOffset" : 90,
      "endOffset" : 98
    }, {
      "referenceID" : 16,
      "context" : "Our definition of pool of intent resembles the notion of pool of state in signaling games [15, 18].",
      "startOffset" : 90,
      "endOffset" : 98
    }, {
      "referenceID" : 24,
      "context" : "Some users may be willing to lose some payoff in the short-term to gain more payoff in the long run, therefore, an interesting direction is to define and analyze less myopic equilibria for the game [26].",
      "startOffset" : 198,
      "endOffset" : 202
    }, {
      "referenceID" : 16,
      "context" : "10 extends the Theorem 1 in [18] for our model.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 0,
      "context" : "In particular, for α ∈ [0, 1], if D,D are stochastic matrices, αD + (1 − α)D will be a stochastic matrix and hence, (U, αD + (1 − α)D) is a Nash equilibrium as well.",
      "startOffset" : 23,
      "endOffset" : 29
    }, {
      "referenceID" : 0,
      "context" : "Similarly, if (U , D) and (U,D) are Nash equilibria for U 6= U , then ur(αU + (1− α)U , D) = ur(U,D) and (αU + (1− α)U , D) is a Nash-equilibrium for any α ∈ [0, 1].",
      "startOffset" : 158,
      "endOffset" : 164
    }, {
      "referenceID" : 55,
      "context" : "Every finite game has always a mixed Nash equilibrium [57].",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 54,
      "context" : "It is well established that humans show reinforcement behavior in learning [56, 47, 48].",
      "startOffset" : 75,
      "endOffset" : 87
    }, {
      "referenceID" : 45,
      "context" : "It is well established that humans show reinforcement behavior in learning [56, 47, 48].",
      "startOffset" : 75,
      "endOffset" : 87
    }, {
      "referenceID" : 46,
      "context" : "It is well established that humans show reinforcement behavior in learning [56, 47, 48].",
      "startOffset" : 75,
      "endOffset" : 87
    }, {
      "referenceID" : 54,
      "context" : "Many lab studies with human subjects conclude that one can model human learning using reinforcement learningmodels [56, 47].",
      "startOffset" : 115,
      "endOffset" : 123
    }, {
      "referenceID" : 45,
      "context" : "Many lab studies with human subjects conclude that one can model human learning using reinforcement learningmodels [56, 47].",
      "startOffset" : 115,
      "endOffset" : 123
    }, {
      "referenceID" : 8,
      "context" : "Bush and Mosteller’s model increases the probability that a user will choose a given query when searching for a specific intent by an amount proportional based on the reward of using that query and the current probability of using this query for the intent in the strategy [10].",
      "startOffset" : 273,
      "endOffset" : 277
    }, {
      "referenceID" : 0,
      "context" : "Uij(t)− α BM · Uij(t) qj 6= q(t) ∧ r ≥ 0 Uij(t) + β BM · (1− Uij(t) qj 6= q(t) ∧ r < 0 (4) In the aforementioned formulas,α ∈ [0, 1] and β ∈ [0, 1] are parameters of the model, q(t) denotes the query picked by the user at time t, and r is the reward of the interaction.",
      "startOffset" : 126,
      "endOffset" : 132
    }, {
      "referenceID" : 0,
      "context" : "Uij(t)− α BM · Uij(t) qj 6= q(t) ∧ r ≥ 0 Uij(t) + β BM · (1− Uij(t) qj 6= q(t) ∧ r < 0 (4) In the aforementioned formulas,α ∈ [0, 1] and β ∈ [0, 1] are parameters of the model, q(t) denotes the query picked by the user at time t, and r is the reward of the interaction.",
      "startOffset" : 141,
      "endOffset" : 147
    }, {
      "referenceID" : 14,
      "context" : "Cross’s model modifies the user’s strategy similar to Bush and Mosteller’s model [16].",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 0,
      "context" : "In the above formulas, α ∈ [0, 1] and β ∈ [0, 1] are the parameters used compute the adjusted reward R(r) based",
      "startOffset" : 27,
      "endOffset" : 33
    }, {
      "referenceID" : 0,
      "context" : "In the above formulas, α ∈ [0, 1] and β ∈ [0, 1] are the parameters used compute the adjusted reward R(r) based",
      "startOffset" : 42,
      "endOffset" : 48
    }, {
      "referenceID" : 49,
      "context" : "Roth and Erev’s model reinforces the probabilities directly from the reward value r that is received when the user enters query q(t) [51].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 18,
      "context" : "Roth and Erev’s modified model is similar to the original Roth and Erev’s model, but it has an additional parameter that determines to what extent the user takes in to account the outcomes of her past interactions with the system [20].",
      "startOffset" : 230,
      "endOffset" : 234
    }, {
      "referenceID" : 0,
      "context" : "This is accounted for by the forget parameter σ ∈ [0, 1].",
      "startOffset" : 50,
      "endOffset" : 56
    }, {
      "referenceID" : 0,
      "context" : "In the aforementioned formulas, ǫ ∈ [0, 1] is a parameter that weights the reward that the user receives, n is the maximum number of possible queries for a given intent ei, and rmin is the minimum expected reward that the user wants to receive.",
      "startOffset" : 36,
      "endOffset" : 42
    }, {
      "referenceID" : 6,
      "context" : "TheWin-Stay/Lose-Randomizemethod uses only the most recent interaction for an intent to determine the queries used to express the intent in the future [7].",
      "startOffset" : 151,
      "endOffset" : 154
    }, {
      "referenceID" : 0,
      "context" : "Let a user receive reward r ∈ [0, 1] by entering query qj to express intent ei.",
      "startOffset" : 30,
      "endOffset" : 36
    }, {
      "referenceID" : 9,
      "context" : "Those studies show that non-proficient users tend to use models that do not leverage the information about the past interactions, such as Cross’s model [11].",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 30,
      "context" : "As in [32], we consider Roth-Erev reinforcement learning mechanism for adaptation of the DBMS adaption.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 30,
      "context" : "For the case that both the DBMS and the user adapt their strategies, one can use the results in [32].",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 10,
      "context" : "[12], for the initial reward condition R(0) which possibly leads to an intuitive initial point for the learning rule.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 43,
      "context" : "As queries and intents generally follow a power law distribution [45], one may use sampling techniques to use this algorithm in other settings.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 17,
      "context" : "We recall that a random process {X(t)} is a submartingale [19] if it is absolutely integrable (i.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 17,
      "context" : "We refer the interested readers to [19] for further information on this result (martingale convergence theorem).",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 48,
      "context" : "[50] A random process {Xt} converges almost surely if Xt is bounded, i.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 48,
      "context" : "Note that this result is a weaker form of the Robins-Siegmund martingale convergence theorem in [50] but it will serve for the purpose of our discussion.",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 17,
      "context" : "the Jensen’s inequality [19], we have:",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 17,
      "context" : "Therefore, using the Borel-Cantelli Lemma for adapted processes [19] we have { 1 R j (t) } is summable",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 9,
      "context" : "In fact, researchers have observed that users show reinforcement learning behavior when interacting with a DBMS over a period of time [11].",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 9,
      "context" : "In particular, the authors in [11] have shown that some groups of users learned to formulate queries with a model similar to Roth-Erev reinforcement learning.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 17,
      "context" : "Therefore, by the Jensen’s inequality [19], we have:",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 30,
      "context" : "Such questions are partially addressed in [32] for the case that both the players in a singling game adapt their strategy synchronously and identically.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 30,
      "context" : "3 in [32] for more details).",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 30,
      "context" : "The authors in [32] follow a traditional language game approach as explained in Section 6.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 37,
      "context" : "Signaling games model communication between two or more agents and have been widely used in economics, sociology, biology, and linguistics [39, 15, 49, 18].",
      "startOffset" : 139,
      "endOffset" : 155
    }, {
      "referenceID" : 13,
      "context" : "Signaling games model communication between two or more agents and have been widely used in economics, sociology, biology, and linguistics [39, 15, 49, 18].",
      "startOffset" : 139,
      "endOffset" : 155
    }, {
      "referenceID" : 47,
      "context" : "Signaling games model communication between two or more agents and have been widely used in economics, sociology, biology, and linguistics [39, 15, 49, 18].",
      "startOffset" : 139,
      "endOffset" : 155
    }, {
      "referenceID" : 16,
      "context" : "Signaling games model communication between two or more agents and have been widely used in economics, sociology, biology, and linguistics [39, 15, 49, 18].",
      "startOffset" : 139,
      "endOffset" : 155
    }, {
      "referenceID" : 13,
      "context" : "A signaling game may not be cooperative in which the interests of players do not coincide [15].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 59,
      "context" : "Our framework extends a particular category of signaling games called language games [61, 49, 18] and is closely related to learning in signaling games [32, 59, 23].",
      "startOffset" : 85,
      "endOffset" : 97
    }, {
      "referenceID" : 47,
      "context" : "Our framework extends a particular category of signaling games called language games [61, 49, 18] and is closely related to learning in signaling games [32, 59, 23].",
      "startOffset" : 85,
      "endOffset" : 97
    }, {
      "referenceID" : 16,
      "context" : "Our framework extends a particular category of signaling games called language games [61, 49, 18] and is closely related to learning in signaling games [32, 59, 23].",
      "startOffset" : 85,
      "endOffset" : 97
    }, {
      "referenceID" : 30,
      "context" : "Our framework extends a particular category of signaling games called language games [61, 49, 18] and is closely related to learning in signaling games [32, 59, 23].",
      "startOffset" : 152,
      "endOffset" : 164
    }, {
      "referenceID" : 57,
      "context" : "Our framework extends a particular category of signaling games called language games [61, 49, 18] and is closely related to learning in signaling games [32, 59, 23].",
      "startOffset" : 152,
      "endOffset" : 164
    }, {
      "referenceID" : 21,
      "context" : "Our framework extends a particular category of signaling games called language games [61, 49, 18] and is closely related to learning in signaling games [32, 59, 23].",
      "startOffset" : 152,
      "endOffset" : 164
    }, {
      "referenceID" : 2,
      "context" : "Game theoretic approaches have been used in various areas of computer science, such as distributed systems, planning, security, and data mining [3, 38, 27, 55, 53, 28, 43].",
      "startOffset" : 144,
      "endOffset" : 171
    }, {
      "referenceID" : 36,
      "context" : "Game theoretic approaches have been used in various areas of computer science, such as distributed systems, planning, security, and data mining [3, 38, 27, 55, 53, 28, 43].",
      "startOffset" : 144,
      "endOffset" : 171
    }, {
      "referenceID" : 25,
      "context" : "Game theoretic approaches have been used in various areas of computer science, such as distributed systems, planning, security, and data mining [3, 38, 27, 55, 53, 28, 43].",
      "startOffset" : 144,
      "endOffset" : 171
    }, {
      "referenceID" : 53,
      "context" : "Game theoretic approaches have been used in various areas of computer science, such as distributed systems, planning, security, and data mining [3, 38, 27, 55, 53, 28, 43].",
      "startOffset" : 144,
      "endOffset" : 171
    }, {
      "referenceID" : 51,
      "context" : "Game theoretic approaches have been used in various areas of computer science, such as distributed systems, planning, security, and data mining [3, 38, 27, 55, 53, 28, 43].",
      "startOffset" : 144,
      "endOffset" : 171
    }, {
      "referenceID" : 26,
      "context" : "Game theoretic approaches have been used in various areas of computer science, such as distributed systems, planning, security, and data mining [3, 38, 27, 55, 53, 28, 43].",
      "startOffset" : 144,
      "endOffset" : 171
    }, {
      "referenceID" : 41,
      "context" : "Game theoretic approaches have been used in various areas of computer science, such as distributed systems, planning, security, and data mining [3, 38, 27, 55, 53, 28, 43].",
      "startOffset" : 144,
      "endOffset" : 171
    }, {
      "referenceID" : 63,
      "context" : "Researchers have also leveraged economical models to build query interfaces that return desired results to the users using the fewest possible interactions [66].",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 39,
      "context" : "In particular, researchers have recently applied game theoretic approaches to model the actions taken by users and document retrieval systems in a single session [41].",
      "startOffset" : 162,
      "endOffset" : 166
    }, {
      "referenceID" : 5,
      "context" : "have used signaling games to create a shared lexicon between multiple autonomous systems [6].",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 56,
      "context" : "We have proposed the possibility of using signaling games to model the interaction between users and DBMS and provided an initial result in a short paper in [58].",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 56,
      "context" : "However, authors in [58] assume that a result either fully satisfies the user’s information need or does not contain information relevant to the submitted intent.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 32,
      "context" : "Interactive querying of databases has recently received a lot of attention [34].",
      "startOffset" : 75,
      "endOffset" : 79
    } ],
    "year" : 2017,
    "abstractText" : "As most users cannot precisely express their information needs in form of queries, it is challenging for database management systems to understand the true information needs behind users’ queries. Query interfaces leverage user’s feedback on the returned answers for a query to improve their understanding of the true information need behind the query. Current query interfaces generally assume that a user follows a fixed strategy of expressing her information needs, that is, the likelihood by which a user submits a query to express a certain information need remains unchanged over a potentially long period of time. Nevertheless, users may learn from their interactions with the database system and gradually choose more precise queries to express their intents. In this paper, we introduce a novel formal framework that models database querying as a collaboration between two active and potentially rational agents: the user and the database management system. These agents follow the identical interest of establishing amutual language for representing information needs. We formalize this collaboration as a signaling game, where each mutual language is an equilibrium for the game. A query interface is more effective if it establishes a less ambiguous mutual language faster. We explore some important characteristics of the equilibria of the game. Using an extensive empirical analysis over a real-world query workload, we show that users follow a reinforcement learning method to improve the articulation of their information needs. We also propose and analyze a reinforcement learning mechanism for the database query interface. We prove that this adaptation mechanism for the query interface improves the effectiveness of answering queries stochastically speaking, and converges almost surely, for both the cases where users follows a fixed and reinforcement learning strategy for expressing their information needs.",
    "creator" : "LaTeX with hyperref package"
  }
}