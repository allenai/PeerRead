{
  "name" : "1606.08821.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Generation and Pruning of Pronunciation Variants to Improve ASR Accuracy",
    "authors" : [ "Zhenhao Ge", "Aravind Ganapathiraju", "Ananth N. Iyer", "Scott A. Randal", "Felix I. Wyss" ],
    "emails" : [ "felix.wyss}@inin.com" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Grammar-based Automatic Speech Recognition (ASR), can be challenging due to variation of pronunciations. These variations can be pronunciations of native words from non-natives, or pronunciations of imported non-native words from natives, or it may be caused by uncommon spelling of some special words. Many techniques have been tried to address this challenge, such as weighted speaker clustering, massive adaptation, and adaptive pronunciation modeling [1].\nWords specified in the grammar have their baseline pronunciations either covered in regular dictionaries, such as 1) prototype dictionaries for the most common words, and 2) linguist hand-crafted dictionaries for the less common words, or 3) generated using a spelling-to-pronunciation (STP)/graphemeto-phoneme (G2P) engine with a set of rules for special words. These baseline pronunciations sometimes have a big mismatch for the “difficult” words and may cause recognition errors, and ASR accuracy can be significantly improved if more suitable pronunciations can be learned. However, blindly generating variants of word pronunciation, though it can increase the recognition rate for that particular word, will reduce accuracy recognizing potential “similar” words, which are close to the target word in the pronunciation space.\nThere are various ways to learn pronunciations [2, 3] and here we propose a novel efficient algorithm. The goal of this algorithm is two-fold. For each target word: a) select the best set of alternate pronunciations from a candidate set originating from the baseline pronunciation; b) avoid any “side-effect” on neighboring words in pronunciation space. This is achieved by maximizing the overall recognition accuracy of a word set containing the target word and its neighboring words. A pronunciation variant generation and searching process is developed, which further performs sorting and pruning to limit the number of total accepted pronunciations for each word.\nBeaufays et al. used probability models to suggest alternative pronunciations by changing one phoneme at a time [4]. Reveil et al. adds pronunciation variants to a baseline lexicon using multiple phoneme-to-phoneme (P2P) converters with different features and rules [5]. Compared to these methods, the proposed technique is more efficient and allows searching in a much wider space without affecting accuracy.\nThe work was initiated during the first author’s internship at Interactive Intelligence (ININ) [6], and later improved in terms of accuracy, efficiency and flexibility. This paper is organized as follows: Sec. 2 describes the database, Sec. 3 provides an overview of the grammar-based name recognition framework; Sec. 4 introduces some preliminary knowledge which faciliates the explanation of the pronunciation learning algorithm in Sec. 5; Sec. 6 provides some heuristics to improve efficiency in implementing pronunciation learning, followed by the results and conclusions in Sec. 7."
    }, {
      "heading" : "2. Data",
      "text" : "This work used the ININ company directory database, which contains human names (concatenation of 2,3, or 4 words), intentionally collected from 2 phases for pronunciation learning (training) and accuracy improvement evaluation (testing) respectively. They share the same pool of 13875 names, and Tab. 1 lists the statistics and baseline accuracies. Names were pronounced in English by speakers from multiple regions and countries. They were asked to read a list of native and nonnative names with random repetitions. Then, the audio was segmented into recordings of individual names. The reduction in Name Error Rate (NER) from phase 1 to phase 2 was mainly because the latter were recorded in cleaner channels with less packet loss, and better corpus creation methods.\nRecognition is normally more challenging when the grammar size increases, since names are more dense in the pronunciation space and more easily confused with others. Here NER is evaluated with a growing grammar size G, and data in both phases were randomly segmented into subsets CG, with G = 1000, 2000, . . . , 13000, 13875, where the larger subset always includes the smaller one."
    }, {
      "heading" : "3. Overview of Grammar-based ASR",
      "text" : "Grammar-based ASR is used to recognize input speech as one of the entries specified in the grammar file. For example, if the grammar contains various names, the input speech will be\nar X\niv :1\n60 6.\n08 82\n1v 1\n[ cs\n.C L\n] 2\n8 Ju\nn 20\n16\nrecognized as one of the most likely name or the system will report “no match”, if it is not close to any name. This work used Interaction Speech Recognition®, a grammar-based ASR developed at ININ. Fig. 1 illustrates the main components with both acoustic and language resources. The acoustic information is modeled as Hidden Markov Model-Gaussian Mixture Model (HMM-GMM). The front end for this systems uses MelFrequency Cepstral Coefficients (MFCCs) transformed using Linear Discriminant Analysis (LDA). The language resource is provided as a name grammar according to the speech recognition grammar specification (SRGS) [7]. The linguistic information is encoded using a lexicon containing text normalizers, pronunciation dictionaries, and a decision-tree-based spellingto-pronunciation (STP) predictor. The work here updates the pronunciation dictionaries by adding learned pronunciations to build a better lexicon for recognition."
    }, {
      "heading" : "4. Preliminaries",
      "text" : "To better describe the pronunciation learning algorithm in Sec. 5, this section introduces three preliminary interrelated concepts, including a) confusion matrix, b) pronunciation space and distance, and c) generation of candidate pronunciations."
    }, {
      "heading" : "4.1. Confusion Matrix",
      "text" : "This work used the Arpabet phoneme set of 39 phonemes [8] to construct a 39× 39 confusion matrixM. The valueM(pi, pj) serves as a similarity measurement between phonemes pi and pj . The smaller the value, the more similar they are. It considers both acoustic and linguistic similarities and is formulated as:\nM(pi, pj) =Macoustic(pi, pj) · Mlinguistic(pi, pj) (1)\nTo construct Macoustic, phoneme alignment was performed on the Wall Street Journal (WSJ) corpus to find the average loglikelihood of recognizing phoneme pi as pj . These values were then sign-flipped and normalized, so the diagonal values inMacoustic are all zeros. Mlinguistic is a symmetric binary matrix where Mlinguistic(pi, pj) = 0 if pi and pj are in the same linguistic cluster. The confusion between pi and pj is linguistically likely even though they may acoustically sound very different, and vice versa. Tab. 2 shows the 16 clusters defined by in-house linguists based on linguistic similarities. Using Eq. (1), the combined confusion matrix M prioritizes the linguistic similarity, where the acoustic similarity is considered only when the phonemes are not in the same linguistic cluster."
    }, {
      "heading" : "4.2. Pronunciation Space and Distance Measurement",
      "text" : "Pronunciation space is spanned by all possible pronunciations (phoneme sequences). Sequences are considered as points in this space and the “distances” between them are computed using a confusion matrix M. The distance d(Pi,Pj) between two pronunciations Pi = [p1, p2, . . . , pM ] and Pj = [q1, q2, . . . , qN ], where M,N are the lengths of phoneme sequences, is measured using Levenshtein distance with Dynamic Programming [9]. It is then normalized by the maximum length of these two, i.e., d(Pi,Pj) = C(M,N)/max{M,N}. For a database with grammar size G, a G × G name distance matrix N is pre-computed before pronunciation learning, where N (s, t) indicates the distance between nameNs andNt."
    }, {
      "heading" : "4.3. Generation of Candidate Pronunciations",
      "text" : "pronunciation learning of a target word Wt requires generating a pool of candidate pronunciations PCan “around” the baseline pronunciation PBase in the pronunciation space to search from. Given PBase = [pM , pM−1, . . . , p1], where M is the length of PBase, by thresholding the mth phoneme pm in the confusion matrixM with search radius rm, m ∈ [1,M ], one can find Nm candidate phonemes (including pm itself, since M(pm, pm) = 0 < rm), which can be indexed in the range [0, 1, . . . , nm, . . . , Nm−1]. Note that the phoneme symbol pm in PBase are indexed in reverse order and the index of candidate phonemes nm for pm start from 0, rather than 1. This is intentional to make it easier to describe the candidate pronunciation indexing later in this section. Here we use the same search radius r0 to search potential replacements for each phoneme, i.e., r0 = r1 = · · · = rM , where r0 is experimentally determined by the workload (i.e. the number of misrecognized name instances) required for pronunciation learning.\nAfter finding Nm candidate phones for pm using search radius r, the total number of candidate pronunciations (PCan) X can be calculated by X = ∏M m=1 Nm. For example, given the word paine with PBase = [p ey n], here M = 3 and there are N3 = 2, N2 = 4, N1 = 2 candidate phonemes for pm,m ∈ [1,M ]. The phoneme candidates for substitution are listed in Tab. 3, and Tab. 4 shows all 16 PCan (X = 16) with repetition patterns underlined. Meanwhile, the distance from PBase ofWt to the farthest candidate pronunciation is defined as its outreach distance dt, which is later used to define the scope\nin findingWt’s neighboring words. It is formulated below:\ndt = 1\nM M∑ m=1 dt(pm)\n= 1\nM M∑ m=1 max nm∈[0,Nm−1] M(pm, pm(nm)),\n(2)\nand pm(nm) is the nthm candidate phoneme alternative for pm. After generating candidate pronunciation list PCan from PBase using this method, fast one-to-one mapping/indexing between phoneme indices (nM , nM−1, . . . , nm, . . . , n1) and pronunciation index x, x ∈ [0, X − 1] is essential for efficient candidate pronunciation lookup and pronunciation list segmentation based on the phoneme position index m during pronunciation learning. Therefore, a pair of bi-directional mapping functions is provided in Eq. (3) and Eq. (4). For example, [p iy ng] can be indexed by both PCan(x = 13) and PCan(n3 = 1, n2 = 2, n1 = 1).\nx = M∑ m=1 nm( m−1∏ i=0 Ni), nm ∈ [0, Nm−1], N0 = 1, (3)\nnm =\n[ x− x mod ( ∏m−1 i=0 Ni)∏m−1\ni=0 Ni\n] mod Nm, N0 = 1. (4)\nThe example shown above illustrates candidate pronunciation generation with phoneme replacement. This method can be easily extended to include more candidates with phoneme deletion, by introducing a special “void” phoneme. However, it does not handle phoneme insertion since it may include too many possible candidates."
    }, {
      "heading" : "5. Pronunciation Learning Algorithm",
      "text" : "Pronunciation learning aims to find better alternative pronunciation for misrecognized names through a pronunciation generation and pruning process, which maximizes the accuracy improvement on a regional nameset Drincluding the target name Nt and its nearby similar namesNc. The learning is performed for all misrecognized names. However, it is only applied on a word basis, to the misrecognized words in the misrecognized names. The following subsections first introduce the main word pronunciation learning algorithm and then elaborate on the key components."
    }, {
      "heading" : "5.1. Algorithm Outline",
      "text" : "1. Set phoneme search radius r0, and upper bounds on the number of total pronunciations per name K1 and per word K2.\n2. Perform baseline name recognition and collect all misrecognized name instances in De. 3. For each target nameNt with error instances in De:\na. Compute its PCan with r0 and outreach distance dt in Eq. (2) to find its corresponding regional nameset Dr . b. For each misrecognized word instance Wt(i), find the best pronunciationP∗(i) using hierarchical pronunciation determination and get the accuracy increment Ã(i) on Dr by adding P∗(i) into dictionary. c. Sort P∗(i) by Ã(i) and keep up to K1 pronunciations in PLearned dictionary.\n4. For each target wordWt with learned pronunciations:\na. Find all names containingWt to form a nameset Dw. b. Evaluate PLearned(Wt) significance by their accuracy\nboost Ãw on Dw and keep up to top K2 pronunciations.\n5. Combine all PLearned for error words We after pruning and replace PBase(We) with PLearned(We) in dictionary."
    }, {
      "heading" : "5.2. Hierarchical Pronunciation Determination",
      "text" : "Generally, given an input test nameNBase to the grammar-based ASR, it outputs a hypothesized name NHyp associated with the highest hypothesized score SHyp. However, if NHyp has multiple pronunciations, which one is actually used to yield SHyp is not provided for decoding efficiency (Fig. 2a). It is similar in the case of pronunciation learning (Fig. 2b). By providing massive number of PCan for an ASR with single grammar (grammar contains only one nameNt), only the highest hypothesized score S∗ is yielded and the associated best pronunciation P∗ is not provided. In order to find P∗ from PCan, hierarchical pronunciation determination with PCan segmentation is used, by determining its phoneme one at a time. For simplicity, an ex-\nwww.inin.com ©2012 Interactive Intelligence Group Inc.\nample to determine P∗ for the word paine is demonstrated. The same method applies to a name (concatenation of words) as well. In Fig. 3, the phonemes in P∗ are determined in the order of p∗3 = p→ p∗2 = ey→ p∗1 = ng, by tracking the PCan segmentation with highest confidence score SW = 0.6957.\nIn general, given Ni phoneme candidates for the ith phoneme of PBase, Lm = ∏m\ni=1 Ni is the number of pronunciations processed to determine the mth phoneme, and L =∑M\nm=1 Lm is the total number of pronunciations processed while learning the pronunciation for one word. In addition, the number of times running the recognizer is ∑M m=1 Nm. Given that the computational cost of running the recognizer once is T1 and processing each candidate pronunciation is T2, where T1 T2, the total computational cost of running hierarchical\nwww.inin.com ©2012 Interactive Intelligence Group Inc.\nHierarchical Pronunciation Learning ‐ Example\npaine\npronunciation determination T is approximately\nT ≈ TRun + TPron\n=\n( M∑\nm=1\nNm ) T1 + ( M∑\nm=1 ( m∏ i=1 Ni )) T2.\n(5)\nFor example, when determining phonemes in the order of p3 → p2 → p1 (natural order) in Figure 3.\nTpaine ≈ (2 + 4 + 2)T1 + [(2 · 4 · 2) + (4 · 2) + 2]T2 = 8T1 + 26T2. (6)\nSince T1 T2, T is mainly determined by TRun, i.e. the factor ∑M m=1 Nm. Comparing with the brute-force method of evaluating candidate pronunciations one-by-one, associated with the factor ∏M m=1 Nm, this algorithm is significantly faster."
    }, {
      "heading" : "6. Optimization in Implementation",
      "text" : ""
    }, {
      "heading" : "6.1. Search Radius Reduction",
      "text" : "In the step 3b of Sec. 5.1, if too many alternatives are generated for a particular word, due to the phoneme sequence length M > Mmax, search radius reduction is triggered to reduce the computational cost by decreasing the phoneme search radius form r0 to Mmax−1M−1 r0. For example, the word desjardins with PBase = [d eh s zh aa r d iy n z] is a long word with M = 10, and phonemes {eh, s, zh , aa, iy, z} have more than 5 phoneme candidates each. The total number of PCan is 4, 536, 000 which requires much longer learning time than regular words. There are less than 20% of words in DTrain that triggered this. However, the average word pronunciation length was reduced from 20,204 to 11,941. Both r0 and Mmax in are determined experimentally, here r0 = 3 and Mmax = 6. This method narrows pronunciation variants search to the more similar ones."
    }, {
      "heading" : "6.2. Phoneme Determination Order Optimization",
      "text" : "Given {NM , NM−1, . . . , N1} are the number of phoneme candidates for phonemes {pM , pM−1, . . . , p1} in PBase. Fig. 3 shows that the phonemes are determined in the natural order of Nm, such as p3 → p2 → p1, and the total number of PCan processed is LNatural = 26. However, if they are determined in the descending order of Nm, such as p2 → p3 → p1 (N2 = 4 ≥ N3 = 2 ≥ N1 = 2), then the number of PCan processed is minimized as LDescend = 22 < LNatural = 26 (Fig. 4). Generally, it can be mathematically proven that LDescend ≤ LNatural ≤ LAscend."
    }, {
      "heading" : "7. Results and Conclusions",
      "text" : "The improvement from baseline varies from the experimental settings, e.g. 1) how challenging the database is (percentage of uncommon words); 2) the dataset and grammar sizes; 3) the quality of audio recording and 4) the ASR acoustic modeling, etc. The namesets in Tab. 1 contain 13.4% uncommon words (4.6% native, 8.8% non-native). Tab. 5 and Fig. 5 show the baselines are already with competitive accuracies, since the dictionaries provide canonical pronunciations of 97%+ words, and the rest are generated from a well-trained STP with perfectmatch accuracy 55% on 5800 words reserved for testing. The pronunciations learned from phase 1 update the lexicon and are tested in phase 2. All NERs grow when G increases, and NER(2)learn is much lower but grows slightly faster than the other two.\nwww.inin.com\n©2012 Interactive Intelligence Group Inc.\nBeaufays et al. [4] achieved ERR 40% with 1600 names, compared to a baseline letter-to-phone pronunciation engine. We obtained similar ERR with a much larger grammar size (42.13% with 13000 names) with a much better baseline. Compared with Reveil et al. [5], whose ERR was close to 40% with 3540 names spoken by speakers from 5 different language origins, our dataset may not have such diversity but we achieved much higher ERR of around 58% for a similar grammar size.\nThis pronunciation learning algorithm is an essential complement to a) STP/G2P interpreters and b) content-adapted pronunciation dictionaries made by linguists. When these two are not sophisticated enough to cover pronunciation variations, it can help to build a better lexicon, and ASR accuracy can significantly be improved, especially when the grammar size is not too large. As indicated in Tab. 5, ERR of phase 2 tends to decrease when the grammar size increases, since there is not much room for learning when one name is surrounded by many other names with similar pronunciations. Similarly, the learned dictionary is also dependent on grammar size, i.e., one dictionary learned from a small database might not be a good fit for a much larger database, since it may be too aggressive in learning, while a larger database requires a more conservative approach to learn. In the future, learned pronunciations can be used to improve the STP interpreter by generating alternative spelling-topronunciation interpretation rules, so it can automatically output alternative pronunciations covering new names, and provide a baseline that is good enough even without learning."
    }, {
      "heading" : "8. References",
      "text" : "[1] Y. Gao, B. Ramabhadran, J. Chen, M. Picheny et al., “Innova-\ntive approaches for large vocabulary name recognition,” in ICASSP 2001, vol. 1. IEEE, 2001, pp. 53–56.\n[2] I. Badr, “Pronunciation learning for automatic speech recognition,” Ph.D. dissertation, Massachusetts Institute of Technology, 2011.\n[3] H. Y. Chan and R. Rosenfeld, “Discriminative pronunciation learning for speech recognition for resource scarce languages,” in Proceedings of the 2nd ACM Symposium on Computing for Development. ACM, 2012, p. 12.\n[4] F. Beaufays, A. Sankar, S. Williams, and M. Weintraub, “Learning name pronunciations in automatic speech recognition systems,” in Tools with Artificial Intelligence 2003. Proceedings. 15th IEEE International Conference, 2003, pp. 233–240.\n[5] B. Réveil, J. Martens, and H. Van Den Heuvel, “Improving proper name recognition by means of automatically learned pronunciation variants,” Speech Communication, vol. 54, no. 3, pp. 321–340, 2012.\n[6] Z. Ge, “Mispronunciation detection for language learning and speech recognition adaptation,” Ph.D. Dissertation, Purdue University West Lafayette, 2013.\n[7] “Speech recognition grammar specification version 1.0,” http:// www.w3.org/TR/speech-grammar/, accessed 2015-07-01.\n[8] R. Doe. (2013) CMU pronouncing dictionary @ONLINE. [Online]. Available: http://www.speech.cs.cmu.edu/cgi-bin/cmudict\n[9] W. J. Heeringa, “Measuring dialect pronunciation differences using levenshtein distance,” Ph.D. dissertation, Citeseer, 2004."
    } ],
    "references" : [ {
      "title" : "Innovative approaches for large vocabulary name recognition",
      "author" : [ "Y. Gao", "B. Ramabhadran", "J. Chen", "M. Picheny" ],
      "venue" : "ICASSP 2001, vol. 1. IEEE, 2001, pp. 53–56.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Pronunciation learning for automatic speech recognition",
      "author" : [ "I. Badr" ],
      "venue" : "Ph.D. dissertation, Massachusetts Institute of Technology, 2011.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Discriminative pronunciation learning for speech recognition for resource scarce languages",
      "author" : [ "H.Y. Chan", "R. Rosenfeld" ],
      "venue" : "Proceedings of the 2nd ACM Symposium on Computing for Development. ACM, 2012, p. 12.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Learning name pronunciations in automatic speech recognition systems",
      "author" : [ "F. Beaufays", "A. Sankar", "S. Williams", "M. Weintraub" ],
      "venue" : "Tools with Artificial Intelligence 2003. Proceedings. 15th IEEE International Conference, 2003, pp. 233–240.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Improving proper name recognition by means of automatically learned pronunciation variants",
      "author" : [ "B. Réveil", "J. Martens", "H. Van Den Heuvel" ],
      "venue" : "Speech Communication, vol. 54, no. 3, pp. 321–340, 2012.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Mispronunciation detection for language learning and speech recognition adaptation",
      "author" : [ "Z. Ge" ],
      "venue" : "Ph.D. Dissertation, Purdue University West Lafayette, 2013.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "CMU pronouncing dictionary @ONLINE",
      "author" : [ "R. Doe" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2013
    }, {
      "title" : "Measuring dialect pronunciation differences using levenshtein distance",
      "author" : [ "W.J. Heeringa" ],
      "venue" : "Ph.D. dissertation, Citeseer, 2004.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Many techniques have been tried to address this challenge, such as weighted speaker clustering, massive adaptation, and adaptive pronunciation modeling [1].",
      "startOffset" : 152,
      "endOffset" : 155
    }, {
      "referenceID" : 1,
      "context" : "There are various ways to learn pronunciations [2, 3] and here we propose a novel efficient algorithm.",
      "startOffset" : 47,
      "endOffset" : 53
    }, {
      "referenceID" : 2,
      "context" : "There are various ways to learn pronunciations [2, 3] and here we propose a novel efficient algorithm.",
      "startOffset" : 47,
      "endOffset" : 53
    }, {
      "referenceID" : 3,
      "context" : "used probability models to suggest alternative pronunciations by changing one phoneme at a time [4].",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 4,
      "context" : "adds pronunciation variants to a baseline lexicon using multiple phoneme-to-phoneme (P2P) converters with different features and rules [5].",
      "startOffset" : 135,
      "endOffset" : 138
    }, {
      "referenceID" : 5,
      "context" : "The work was initiated during the first author’s internship at Interactive Intelligence (ININ) [6], and later improved in terms of accuracy, efficiency and flexibility.",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 6,
      "context" : "This work used the Arpabet phoneme set of 39 phonemes [8] to construct a 39× 39 confusion matrixM.",
      "startOffset" : 54,
      "endOffset" : 57
    }, {
      "referenceID" : 0,
      "context" : "pm in PBase(M = 3) p3 = p p2 = ey p1 = n Number of candidates Nm N3 = 2 N2 = 4 N1 = 2 Candidate index nm n3 ∈ [0, 1] n2 ∈ [0, 3] n1 ∈ [0, 1]",
      "startOffset" : 110,
      "endOffset" : 116
    }, {
      "referenceID" : 2,
      "context" : "pm in PBase(M = 3) p3 = p p2 = ey p1 = n Number of candidates Nm N3 = 2 N2 = 4 N1 = 2 Candidate index nm n3 ∈ [0, 1] n2 ∈ [0, 3] n1 ∈ [0, 1]",
      "startOffset" : 122,
      "endOffset" : 128
    }, {
      "referenceID" : 0,
      "context" : "pm in PBase(M = 3) p3 = p p2 = ey p1 = n Number of candidates Nm N3 = 2 N2 = 4 N1 = 2 Candidate index nm n3 ∈ [0, 1] n2 ∈ [0, 3] n1 ∈ [0, 1]",
      "startOffset" : 134,
      "endOffset" : 140
    }, {
      "referenceID" : 7,
      "context" : ", qN ], where M,N are the lengths of phoneme sequences, is measured using Levenshtein distance with Dynamic Programming [9].",
      "startOffset" : 120,
      "endOffset" : 123
    }, {
      "referenceID" : 3,
      "context" : "[4] achieved ERR 40% with 1600 names, compared to a baseline letter-to-phone pronunciation engine.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[5], whose ERR was close to 40% with 3540 names spoken by speakers from 5 different language origins, our dataset may not have such diversity but we achieved much higher ERR of around 58% for a similar grammar size.",
      "startOffset" : 0,
      "endOffset" : 3
    } ],
    "year" : 2016,
    "abstractText" : "Speech recognition, especially name recognition, is widely used in phone services such as company directory dialers, stock quote providers or location finders. It is usually challenging due to pronunciation variations. This paper proposes an efficient and robust data-driven technique which automatically learns acceptable word pronunciations and updates the pronunciation dictionary to build a better lexicon without affecting recognition of other words similar to the target word. It generalizes well on datasets with various sizes, and reduces the error rate on a database with 13000+ human names by 42%, compared to a baseline with regular dictionaries already covering canonical pronunciations of 97%+ words in names, plus a well-trained spelling-to-pronunciation (STP) engine.",
    "creator" : "LaTeX with hyperref package"
  }
}