{"conference": "ICLR 2017 conference submission", "title": "Unsupervised Cross-Domain Image Generation", "abstract": "We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given representation function f, which accepts inputs in either domains, would remain unchanged. Other than f, the training data is unsupervised and consist of a set of samples from each domain, without any mapping between them. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f preserving component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.", "reviews": [{"is_meta_review": true, "comments": "Update: After reading the rebuttal comments and the revised paper, I'm leaving the rating as it was before.\n\nThis paper proposes an unsupervised algorithm for transferring samples from one domain to another (related) domain under the constraint that some predefined f returns same result for the input and the result.\n\nPros:\n1. The paper presents an interesting idea of comparing samples from different domains using a fixed perceptual function f.\n\n2. The proposed method produces visually appealing results on several datasets\n\n3. The authors demonstrate how their approach can be used for domain adaptation and obtain improved results on the SVHN->MNIST task\n\n4. The paper is well-written and easy to read\n\nCons:\n1. The novelty of the method is relatively minor (I consider f-constancy term as the main contribution)\n\n2. It feels like the proposed approach would break for more dissimilar domains. The method relies on a fixed f which is trained on the source domain. This f can potentially drop information important for obtaining 1) better reconstructions in the target domain  2) more tightly related x and g(f(x)). I think the authors should consider either training all the modules in the model end-to-end or incorporating target samples into the training of f.\n\n3. A single domain adaptation experiment is definitely not enough to consider the proposed method as a universal alternative to the existing DA approaches.\n\nI would also like to point out that using super-resolved outputs as opposed to the actual model\u2019s outputs can produce a false impression of the visual quality of the transferred samples. I\u2019d suggest moving original outputs from the appendix into the main part.", "IS_META_REVIEW": true}, {"TITLE": "Training set for SVHN domain?", "OTHER_KEYS": "(anonymous)", "comments": "In section 5.1, you said that you employ the extra training split of SVHN for two purposes: learning the function f and as a unsupervised training set s. So can I say that, in Table 2, your results are based on SVHN 'extra' training split as s domain traning set? But it is different from other methods since SVHN dataset includes train set, test set and extra training split. As far as I know, other methods are using train set as the s domain unsupervised tranining data. ", "IS_META_REVIEW": false, "DATE": "17 Apr 2017 (modified: 18 Apr 2017)", "is_meta_review": false}, {"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "The authors propose a application of GANs to map images to new domains with no labels. E.g., an MNIST 3 is used to generate a SVHN 3. Ablation analysis is given to help understand the model. The results are (subjectively) impressive and the approach could be used for cross-domain transfer, an important problem. All in all, a strong paper.", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"TITLE": "Interesting Work", "OTHER_KEYS": "yunjey choi", "comments": "This paper presents an unsupervised domain transfer from the image of domain S to the image of domain T. \nIt was really refreshing that this conversion was possible without any mapping data. \nFor example, in the paper, the model can transfer the SVHN image '3' to the MNIST image '3' without the mapping data.\nThe model can be roughly divided into GAN and Content Extractor (f in the paper).\n\n1. GAN\nDuring training, the discriminator sees the mnist image and learns to determine it as a real image. \nAnd with GAN loss, the generator learns to get the mnist image as output when it receives an svhn image as input to deceive the discriminator.\n\n2. Content Extractor\nIf the model use only GAN loss, the content in the image may not be retained even if the domain is changed.\nFor example, the generator may convert the svhn image '3' to the mnist image '2' to deceive the discriminator.\nIn this paper, authors introduce a new function called 'f' to maintain the content.\nThe generator includes f and generates a fake mnist image when it receives an svhn image as input.\nThe original svhn image and the generated fake mnist image are put back into f.\nThen additional loss function is set so that the resulting values \u200b\u200bare the same.\nHere, f is learning to extract content regardless of domain.\n\n\nI felt very fresh in this paper so i implemented this paper myself.\nHere is the code I implemented.\n\n", "IS_META_REVIEW": false, "DATE": "23 Jan 2017", "is_meta_review": false}, {"TITLE": "Authors' summary of discussion", "OTHER_KEYS": "Yaniv Taigman", "comments": "We thank the reviewers for their time and insights. All 3 reviewers seem to agree that the work is interesting, well-written and presents extensive experiments. R1 & R2 both note the f-constancy as a novelty of this work. Also, the fact that the method does not require training pairs for the two domains is noted by R3 as a major contribution, which \u201ccould be impactful in broad problem context\u201d. R2 & R3 both agree that the output generations are visually appealing. We have no factual dispute with the reviewers and replied to each individually below.\n\nThe open review discussion has been extremely beneficial to us and the paper has been revised in order to address all actionable items raised throughout the review period. We thank the reviewers for their thoughtful and constructive reviews and all other community members who shared their comments. During the review period, the work has already been cited several times, reimplemented on github, and drawn considerable attention.\n", "IS_META_REVIEW": false, "DATE": "16 Jan 2017", "is_meta_review": false}, {"TITLE": "Revision #2", "OTHER_KEYS": "Yaniv Taigman", "comments": "Following the request of the reviewing team, we have just uploaded a new version of our manuscript, which includes expression preserving experiments. \nThe new experiments are in Appendix B. \n\nIn order to provide a quick way to track the changes from the original submission, we color all modifications in red. \n\nThank you for the extremely useful feedback.\n", "IS_META_REVIEW": false, "DATE": "02 Jan 2017", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "An interesting work", "is_meta_review": false, "comments": "This paper presents an unsupervised image transformation method that maps a sample from source domain to target domain. The major contribution lies in that it does not require aligned training pairs from two domains. The model is based on GANs. To make it work in the unsupervised setting, this paper decomposes the generation function into two modules: an encoder that identify a common feature space between two domains and an decoder that generates samples in the target domain. To avoid trivial solutions, this paper proposed two additional losses that penalize 1) the feature difference between a source sample and its transformed sample and 2) the pixel difference between a target sample and its re-generated sample. This paper presents extensive experiments on transferring SVHN digit images to MNIST style and transferring face images to emoji style. \n+The proposed learning method enables unsupervised domain transfer that could be impactful in broad problem contexts. \n+This paper presents careful ablation studies to analyze the effects of different components of the system, which is helpful for understanding the paper. \n+The transferred images are visually impressive and quantitative results also show the image identities are preserved across domains to some degree. \n-It will be more interesting to show results in other domains such as texts and images. \n-In addition to the face identities, it is also of great interest to analyze how well the facial attributes are preserved when mapping to target domain. ", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "30 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Interesting work, needs more explanations ", "is_meta_review": false, "comments": "Update: thank you for running more experiments, and add more explanations in the manuscript. They addressed most of my concerns, so I updated the score accordingly. \n\n\nThe work aims at learning a generative function G that can maps input from source domain to the target domain, such that a given representation function f remain unchanged accepting inputs from either domain. The criteria is termed f-constancy. The proposed method is evaluated on two visual domain adaptation tasks. \n\nThe paper is relatively easy to follow, and the authors provided quite extensive experimental results on the two datasets. f-constancy is the main novelty of the work. \n\nIt seems counter-intuitive to force the function G to be of g o f, i.e., starting from a restricted function f which might have already lost information. As in the face dataset, f is learned to optimize the performance of certain task on some external dataset. It is not clear if an input from the source or target domain can be recovered from applying G as in equation (5) and (6). Also, the f function is learned with a particular task in mind. As in the two experiments, the representation function f is learned to identify the digits in the source SVHN dataset or the identity of some face dataset.  As a result, the procedure has to be repeated if we were to perform domain adaptation for the same domains but for different tasks, such as recognizing expressions instead of identity. \n\nDo the authors have insight on why the baseline method proposed in equation (1) and (2) perform so poorly?\n\nFigure 5 shows some visual comparison between style transfer and the proposed method. It is not clear though which method is better. Will it be possible to apply style transfer to generate emojis from photos and repeat the experiments shown in table 4?", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "20 Dec 2016 (modified: 13 Jan 2017)", "REVIEWER_CONFIDENCE": 3}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Final review", "is_meta_review": false, "comments": "Update: After reading the rebuttal comments and the revised paper, I'm leaving the rating as it was before.\n\nThis paper proposes an unsupervised algorithm for transferring samples from one domain to another (related) domain under the constraint that some predefined f returns same result for the input and the result.\n\nPros:\n1. The paper presents an interesting idea of comparing samples from different domains using a fixed perceptual function f.\n\n2. The proposed method produces visually appealing results on several datasets\n\n3. The authors demonstrate how their approach can be used for domain adaptation and obtain improved results on the SVHN->MNIST task\n\n4. The paper is well-written and easy to read\n\nCons:\n1. The novelty of the method is relatively minor (I consider f-constancy term as the main contribution)\n\n2. It feels like the proposed approach would break for more dissimilar domains. The method relies on a fixed f which is trained on the source domain. This f can potentially drop information important for obtaining 1) better reconstructions in the target domain  2) more tightly related x and g(f(x)). I think the authors should consider either training all the modules in the model end-to-end or incorporating target samples into the training of f.\n\n3. A single domain adaptation experiment is definitely not enough to consider the proposed method as a universal alternative to the existing DA approaches.\n\nI would also like to point out that using super-resolved outputs as opposed to the actual model\u2019s outputs can produce a false impression of the visual quality of the transferred samples. I\u2019d suggest moving original outputs from the appendix into the main part.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "19 Dec 2016 (modified: 23 Jan 2017)", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Revision", "OTHER_KEYS": "Yaniv Taigman", "comments": "Following the ongoing discussion with the reviewing team as well as with other readers, we have just uploaded a new version of our manuscript, which is aimed at improving clarity. \nIn order to provide a quick way to track the changes, we color all modifications in red. \nIn addition, as mentioned below, we will soon share our open implementation in Torch. \nThank you all for the extremely useful feedback.", "IS_META_REVIEW": false, "DATE": "19 Dec 2016", "is_meta_review": false}, {"TITLE": "Accuracy without reconstructing images in target domain", "OTHER_KEYS": "Xun Huang", "comments": "Given the goal of transferring samples from source domain to target domain, I am wondering whether it is necessary to simultaneously train the model to reconstruct images in the target domain. Any ablation studies on this?", "IS_META_REVIEW": false, "DATE": "19 Dec 2016", "is_meta_review": false}, {"TITLE": "Explanations", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "IS_META_REVIEW": false, "DATE": "10 Dec 2016", "is_meta_review": false}, {"TITLE": "Questions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "IS_META_REVIEW": false, "DATE": "02 Dec 2016 (modified: 03 Dec 2016)", "is_meta_review": false}, {"TITLE": "About the configuration of the generator ", "OTHER_KEYS": "(anonymous)", "comments": "Hi, it is really interesting and I want to re-implement this work. But the paper doesn't provide much information about the configuration of the generator. For example, do you use a fully connected layer as the first layer? If true, what is the dimension?  How many filters of each deconvolution layer do you use ?  Thank you very much.", "IS_META_REVIEW": false, "DATE": "12 Nov 2016", "is_meta_review": false}, {"TITLE": "One puzzle", "OTHER_KEYS": "(anonymous)", "comments": "In section 5.1, you map SVHN-trained representation to 32*32 grayscale images, but the encoder f's input is three-channel, how do you solve this ? Why don't you transform RGB SVHN images into grayscale images or replicate the grayscale MNIST images three times ? I mean why don't you use the same image format for training f and the adversarial network? ", "IS_META_REVIEW": false, "DATE": "11 Nov 2016", "is_meta_review": false}, {"TITLE": "How is this work different from style transfer?", "OTHER_KEYS": "(anonymous)", "comments": "Can the authors comment on what they are doing in terms of end-use/goal in addition to what we know from style transfer already?", "IS_META_REVIEW": false, "DATE": "08 Nov 2016", "is_meta_review": false}, {"TITLE": "About the reference and one minor error", "OTHER_KEYS": "WANG Zongwei", "comments": "This work is really interesting, but I find the network architecture and loss functions in this work are extremely similar to that in the work \"Generating Images with Perceptual Similarity Metrics based on Deep Networks\" by Alexey Dosoviskiy and Thomas Brox.  \n\nThe loss function in this work vs that of the latter: \nL_const is equal to L_feat;\nL_tid is equall to L_img;\nL_GANG is equal to L_adv.\n\nHowever, I do not see the later's name in your reference paper list.  Besides, have you forgotten to print a 'log' before D_3(x) in your L_D loss  on page 4?", "IS_META_REVIEW": false, "DATE": "06 Nov 2016", "is_meta_review": false}], "SCORE": 7, "authors": "Yaniv Taigman, Adam Polyak, Lior Wolf", "accepted": true, "id": ""}
