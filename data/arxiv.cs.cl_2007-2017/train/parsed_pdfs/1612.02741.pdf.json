{
  "name" : "1612.02741.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Coupling Distributed and Symbolic Execution for Natural Language Queries",
    "authors" : [ "Lili Mou", "Zhengdong Lu", "Hang Li", "Zhi Jin" ],
    "emails" : [ "doublepower.mou@gmail.com,", "luz@DeeplyCurious.ai", "HangLi.HL@huawei.com,", "zhijin@sei.pku.edu.cn" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Using natural language to query a knowledge base is an important task in NLP and has wide applications in question answering (QA) [Yin et al., 2016a], human-computer conversation [Wen et al., 2016], etc. Table 1 illustrates an example of a knowledge base (a table) and a query “How long is the game with the largest host country size?” To answer the question, we shall first find a row with the largest value in the column Area, and then select the value of the chosen row with the column being Duration.\nA typical approach is to convert a natural language sentence to an “executable” logic forms for table/knowledge base querying, known as semantic parsing [Long et al., 2016; Pasupat and Liang, 2016]. Such approaches require extensive human engineering. With the fast development of modern neural networks, Dong and Lapata [2016] and Xiao et al. [2016] apply sequenceto-sequence (seq2seq) neural models to generate a logic form conditioned on an input sentence. However, seq2seq models need strong supervision of groundtruth logic forms, which are\n∗Work done during internship at Noah’s Ark Lab, Huawei Technologies. The preprint version of this paper is formatted in a page-saving and thus environment-friendly style.\nQuery: How long is the game with the largest host country size? Knowledge base (table): Year City · · · Area · · · Duration\nlabor-intensive to obtain and ad hoc to a specific dataset; they cannot be trained in an end-to-end fashion with denotations1 only. An emerging research topic in neural semantic parsing is to query a knowledge base directly by neural networks [Yin et al., 2016b; Neelakantan et al., 2016; Liang et al., 2016]. Because queries can be composited in a highly complicated manner, neural enquirers necessitate multiple steps of execution. The difficulty then lies in the lack of step-by-step supervision. In other words, we only assume groundtruth denotations are available in realistic settings, and that we do not know the execution sequence or intermediate execution results.\nThere have seen several studies addressing the problem. Yin et al. [2016b] propose a fully distributed neural enquirer, comprising several neuralized execution layers of field attention, row annotation, etc. While the model is not efficient in execution because of intensive matrix/vector operation during neural information processing and lacks explicit interpretation of execution, it can be trained in an end-to-end fashion because all components in the neural enquirer are differentiable.\nNeelakantan et al. [2016], on the other hand, propose a neural programmer by defining a set of symbolic operations (e.g., argmax, greater than); at each step, all possible execution results are fused by a softmax layer, which predicts the probability of each operator at the current step. The step-bystep fusion is accomplished by weighted sum and the model is trained with mean square error. Hence, such approaches work with numeric tables, but may not be suited for other operations like string matching; it also suffers from the problem of “exponential numbers of combinatorial states.” Liang et al. [2016] train a symbolic executor by REINFORCE policy gradient, but it is known that the REINFORCE algorithm is sensitive to the\n1A denotation refers to the execution result.\nar X\niv :1\n61 2.\n02 74\n1v 1\n[ cs\n.L G\n] 8\nD ec\n2 01\n6\nNeural ExecutorIn\npu t Neural\nExecutor In te rm\ned ia te\n R es ul ts Neural Executor\nIn te rm\ned ia te\n R es ul ts\nO ut pu\nt\nOperator 1 In pu t\nIn te rm\ned ia te\n R es ul ts\nIn te rm\ned ia te\n R es ul ts\nO ut pu\nt\nOperator 2\nOperator n\n. . .\nOperator 1\nOperator 2\nOperator n\n. . .\nOperator 1\nOperator 2\nOperator n\n. . .\nOperator/Argument  Predictor Operator/Argument  Predictor Operator/Argument  Predictor\n(a)\n(b)\nDifferentiable\nImperfect  Stepbystep supervision\nNon differentiable\nFigure 1: An overview of the coupled distributed and symbolic neural enquirers.\ninitial policy. It could be very difficult to get started with a random initial policy; thus it only works with simple cases.\nIn this paper, we propose coupled distributed and symbolic execution for natural language queries. Our intuition rises from the observation that a fully distributed/neuralized executor also exhibits some (imperfect) symbolic interpretation. For example, the field attention gadget in Yin et al. [2016b] generally aligns with column selection. We therefore use the distributed model’s intermediate execution results as supervision signals to pretrain a symbolic executor. Guided by such imperfect step-by-step supervision, the symbolic executor learns a fairly meaningful initial policy, which largely alleviates the cold start problem of the REINFORCE algorithm.\nWe evaluate the proposed approach on the QA dataset in Yin et al. [2016b]. In our experiment, the REINFORCE algorithm alone takes long to get started; even if it does, it is stuck in a poor local optimum. Once pretrained by imperfect supervision signals, the symbolic executor can recover execution sequences with an accuracy of 83.8%. It also outperforms the distributed enquirer in terms of denotation accuracy. It should be mentioned that, in our experiment, neither the distributed enquirer nor the symbolic one is aware of groundtruth execution sequences, and that the entire model is trained with weak supervision of denotations.\nTo the best of our knowledge, we are the first to couple distributed and symbolic execution for semantic parsing. Our study also sheds light on neural sequence prediction in general."
    }, {
      "heading" : "2 Approach",
      "text" : "In this section, we first introduce the fully distributed (neuralized) enquirer proposed in Yin et al. [2016b]. Then we design a set of operators that are complete to the task at hand. At each execution step, a neural network predicts a particular operator and possibly arguments for symbolic execution.\nSubsection 2.3 provides a unified view of distributed and symbolic execution. (See also Figure 1). We explain how the symbolic executor is pretrained by the distributed model’s intermediate execution results, and then further trained with the REINFORCE algorithm."
    }, {
      "heading" : "2.1 Distributed Enquirer",
      "text" : "Yin et al. [2016b] propose a fully distributed neural enquirer to query a table. By “distributed,” we mean that all semantic units (including words in the query, entries in the table, and execution results) are represented as distributed, real-valued vectors and\nprocessed by neural networks. One of the most notable studies of distributed semantics is word embeddings, which map discrete words to vectors as anonymous meaning representations [Mikolov et al., 2013].\nThe distributed neural enquirer consists of the following main components • Query encoder. Words are mapped to word embeddings\nand a bi-directional recurrent neural network (RNN) aggregates information over the sentence. RNNs’ last states in both directions are concatenated as the query representation (denoated as q). • Table encoder. All table cells are also represented as embeddings. For a cell c (e.g., Beijing in Table 1) with column/field name being f (e.g., City), the cell vector is the concatenation of the embeddings of c and f , further processed by a feed-forward neural network (also known as multi-layer perceptron). We denote the representation of a cell as c. • Executor. As shown in Figure 1a, the neural enquirer comprises several layers of execution, which is further illustrated in Figure 2. In each execution step, the neural network selects a field and then one or a few rows based on the previous execution results. More specifically, the execution result is a distribution pf over columns and a scalar r in (0, 1) for each row.2 Finally, a softmax layer is applied to the entire table to select a cell as the answer. We further describe the executor as follows. Let p(t−1)f and r (t−1) be the previous step’s execution results. We represent the selected cell in each row as the sum of all cells in that row, weighted by soft field selection p(t−1)f . Formally, for row i,\nc (t−1) select [i] = ∑ j p (t−1) fj cij (1)\nThe row selection r(t−1) serves as a gate, indicating how much information cselect flows to subsequent processing. The i-th row’s information is thus\nr (t−1) i = r (t−1) i · c (t−1) select [i] (2)\nThen we summarize the global information of execution (denoted as g(t−1)) by max-pooling c(t−1)select over all rows:\ng(t−1) = MaxPooli { c (t−1) select [i] } (3)\nIn the current step, field selection p(t)f is based on the query q, the previous global information g(t−1), and the field name embeddings f .\np (t) fj\n= softmax ( MLP ( [q;fj ; g (t−1)] ))\n(4)\nwhere [·; ·; · · · ] denotes vector concatenation; MLP refers to a multi-layer perceptron.\n2Here, we do not strictly follow the original neural enquirer, where the row vector representation is computed directly a feed-forward neural network. In the current version, a neural layer predicts a gate for each row; then the row vector is computed by Equation 2. But this is not the main point of this paper.\nOperator Explanation select row Choose a row whose value of a particular column is mentioned in the query argmin Choose the row from previously selected candidate rows with the minimum value in a particular column argmin Choose the row from previously selected candidate rows with the maximum value in a particular column greater than Choose rows whose value in a particular column is greater than a previously selected row less than Choose rows whose value in a particular column is less than a previously selected row select value Choose the value of a particular column and of the previously selected row EOE Terminate, indicating the end of execution\nTable 2: Primitive operators for symbolic execution.\nNow we select one or a few rows based on the query q, previous global execution information g(t−1), previous row information r(t−1)i , and current cell selection c (t) i . In particular, we maintain a scalar for each row ri ∈ (0, 1) as\nr (t) i = sigmoid\n( MLP ( [q, g(t−1), r(t−1), c (t) i ] )) (5)\nIntuitively, if ri is close to one, the row is selected; if ri is close to zero, the row is turned off. Because some executions (e.g., greater than) may select multiple rows, we use the sigmoid function instead of softmax for row selection.\nAs said, a softmax layer over all cells is applied at the end of execution to select the answer. Please refer to Yin et al. [2016b] for details of the distributed neural enquirer."
    }, {
      "heading" : "2.2 Symbolic Executor",
      "text" : "The methodology of designing a symbolic executor is to define a set of primitive operators (Subsection 2.2.1) and then to use a machine learning model to predict the operator and its arguments (Subsection 2.2.2)."
    }, {
      "heading" : "2.2.1 Primitive Operators",
      "text" : "We design six operators for symbolic execution, which are complete as they cover all types of queries in our scenario. Similar to the distributed neural enquirer, the result of one-step symbolic execution is a 0-1 boolean scalar for each row; it takes previous execution results as input, with a column/field as the argument. Table 2 summarizes our symbolic operator set.\nIn Table 1, for example, the first step of execution is argmax over the column Area, with previous (initial) row selection being all ones. This step yields a single row 〈2008,Beijing, · · · , 350, · · · , 25〉. The second execution operator is select value, which an argument column=Duration, yielding the result 25. Then the executor terminates (EOE).\nStacked with multiple steps of primitive operators, the executor can answer faily complicated questions like “How long is the last game which has smaller country size than the game whose host country GDP is 250?” The execution sequence is\n1. select row: select the row where the column is GDP and the value is mentioned in the query. 2. less than: select rows whose country size is less than that of the previously selected row. 3. argmax: select the row whose year is the largest among previously selected rows. 4. select value: choose the value of the previously selected row with the column being Duration.\nThen the execution terminates. In our scenario, the execution is limited to four steps (EOE excluded) as such queries are touching the logic depth of common human intuition."
    }, {
      "heading" : "2.2.2 Operator and Argument Predictors",
      "text" : "We also leverage neural models, in particular recurrent neural networks (RNNs), to predict the operator and its argument (a selected field/column).\nLet h(t−1) be the previous state’s hidden vectors. The current vector is\nh(t−1)op = sigmoid(Wrech (t−1) op ) (6)\nwhere Wrec is weight parameters and a bias term is omitted in the equation for simplicity. The initial hidden state is the query embedding, i.e., h(0)op = q.\nThe predicted probability of an operator i is given by\np(t)opi = softmax { w>i h (t−1) op } (7)\nThe operator with the largest predicted probability is selected for execution. Our RNN here does not have input, because the execution sequence is not dependent on the result of the previous execution step. Such architecture is known as a Jordan-type RNN [Jordan, 1997; Mesnil et al., 2013].\nLikewise, we use another Jordan-type RNN to predict the field selection. The only difference lies in the weight of the output softmax, i.e., wi in Equation 8 is substituted with the embedding of a field/column name f , given by\np (t) fj = softmax { f>j h (t−1) field } (8)\nTraining a symbolic executor without step-by-step supervision signals is non-trivial. A typical training method is reinforcement learning in a trial-and-error fashion. However, for a random initial policy, the probability of recovering an accurate execution sequence is extremely low. For a 10 × 10 table, for example, the probability is 1/(64 · 104) ≈ 7.7 × 10−8; the probability of obtaining an accurate denotation is 1%, which is also very low. Therefore, symbolic executors are not efficient in learning."
    }, {
      "heading" : "2.3 A Unified View",
      "text" : "We now have two worlds of execution: • The distributed enquirer is end-to-end learnable, but it\nis of low execution efficiency because of intensive matrix/vector multiplication during neural information processing. The neural execution also lacks explicit interpretation. • The symbolic enquirer has high execution efficiency and explicit interpretation. However, it cannot be trained in an end-to-end manner, and suffers from the cold start problem of reinforcement learning.\nWe propose to combine the two worlds by using the distributed enquirer’s intermediate execution results to pretrain the\nDenotation Execution Query type SEMPRE Distributed Our Method Distributed Our Method SelectWhere 93.8 96.2 100.0 – 100.0 Superlative 97.8 98.9 98.8 – 98.7 WhereSuperlative 34.8 80.4 82.1 – 69.1 NestQuery 34.4 60.5 84.4 – 67.5 Overall 65.2 84.0 91.3 – 83.8\nTable 3: Accuracies (in percentage) of Sempre tookit (reported in Yin et al. [2016b]), the distributed neural enquirer, and our proposed coupled approach.\nsymbolic enquirer for an initial policy. We then use the REINFORCE algorithm to improve the policy for symbolic execution. As a result, we manage to make use of the advantages in both worlds: high learning efficiency, high execution efficiency, and high interpretability; we also obtain better performance in terms of accuracy.\nConcretely, we observe that the field attention in Equation 4 generally aligns with column selection in Equation 7. We therefore pretrain the column selector in the symbolic enquirer with labels predicted by a fully neuralized enquirer. Such pretraining can obtain up to 70% accurate field selection and largely reduce the search space during reinforcement learning.\nAfter obtaining a meaningful, albeit imperfect, initial policy, we apply REINFORCE [Sutton and Barto, 1998] to improve the policy. Formally, the operator predictor and argument predictor in each execution step are the actions (denoted as a) in reinforcement learning terminologies.\nWe define a binary reward R indicating whether the final result of symbolic execution matches with the groundtruth denotation. The loss function of a policy is the expected reward where actions are sampled from the current predicted probabilities\nJ = Ea1,a2,··· ,an∼θ[R(a1, a2, · · · , an)] (9)\nThe partial derivative for a particular sampled action is\n∂J ∂oi = R̃ · (pi − 1ai) (10)\nwhere pi is the predicted probability of all possible actions at the time step i, 1ai is a onehot representation of a sampled action ai, and oi is the input (also known as logit) of softmax. R̃ is the adjusted reward, which will be described shortly.\nTo help the training of REINFORCE, we have two tricks:\n• We balance exploration and exploitation with a small probability . In other words, we sample an action from the predicted action distribution with probability 1 − and from a uniform distribution over all possible actions with probability . The small fraction of uniform sampling helps the model to escape from poor local optima, as it continues to explore the entire action space during training. • We adjust the reward by subtracting the mean reward, av-\neraged over action samples for a certain data point. This is a common practice for REINFORCE [Ranzato et al., 2016]. We also truncate negative rewards as zero to prevent gradient from being messed up by incorrect executions."
    }, {
      "heading" : "3 Experiments",
      "text" : ""
    }, {
      "heading" : "3.1 Dataset",
      "text" : "We evaluated our approach on a QA dataset in Yin et al. [2016b].3 The dataset comprises 25k different tables and queries. The queries can be divided into four types: SelectWhere, Superlative, WhereSuperlative, and NestQuery, each requiring 2–4 execution steps (EOE excluded).\nWe have both groundtruth denotation and execution actions (including operators and fields), as the dataset is synthesized by complicated rules and templates for research purposes. However, only denotations are used as labels during training, which is a realistic setting. Execution sequences are used only during testing to better understand our approach; this is also a strong motivation of using synthesis datasets for research. For the sake of simplicity, we presume the number of execution steps is known a priori during training (but not during testing). Although we have such (little) knowledge of execution, it is not a limitation of our approach and out of scope of the focus of this paper. One can easily design a dummy operator to fill an unnecessary step or one can also train a discriminative sentence model to predict the number of execution steps if a small quantity of labels are available."
    }, {
      "heading" : "3.2 Settings",
      "text" : "All settings of distributed neural enquirers were derived from Yin et al. [2016b] so that we can have a fair comparison. The dimensions of all layers were in the range of 20–50 and the learning algorithm was AdaDelta with default hyperparameters.\nFor the pretraining of the symbolic enquirer, we applied maximum likelihood estimation for 40 epochs to column selection with labels predicted by the distributed enquirers. We then used the REINFORCE algorithm to improve the policy, where we generated 10 action samples for each data point with the exploration probability being 0.1."
    }, {
      "heading" : "3.3 Results",
      "text" : "Table 3 presents the experimental results of our coupled distributed and symbolic enquirers as well as the SUMPRE baseline.\nAs we see, both the fully distributed and our proposed approach outperform the SEMPRE system.4 The coupled distributed and symbolic enquirer also significant outperforms the fully distributed one in terms of almost all query types. Regarding execution accuracy, we find that our approach can recover 83.4% correct execution sequences even without intermediate\n3The dataset will be made available soon. 4http://nlp.stanford.edu/software/sempre/\nstep-by-step training signals. By contrast, a fully distributed neural enquirer does not have explicit interpretations.\nFigure 3 further plots the validation learning curves. We see that, if the symbolic enquirer is trained by REINFORCE alone, it takes 200 epochs to get started to escape from the poor initial plateau. Even if it achieves ∼50% execution accuracy (for simple query types) and ∼75% denotation accuracy, it is stuck in the poor local optima.\nRed lines plot the learning curves of the symbolic executor pretrained with intermediate field attention of the distributed enquirer. Because the operator predictors in the symbolic executor are still hanging after pretraining, the denotation accuracy is near 0 before reinforcement learning. However, after only a few epochs of REINFORCE training, the performance increases sharply until it reaches a plateau of ∼50% execution accuracy and ∼75% denotation accuracy. The model eventually escapes from the plateau at about 200-th epoch and achieves high accuracy gradually. The results show that our coupled distributed and symbolic execution has much higher learning efficiency than a purely symbolic executor."
    }, {
      "heading" : "4 Conclusion",
      "text" : "In this paper, we proposed a coupled view of distributed and symbolic execution for natural language queries. By pretraining with intermediate execution results of a distributed enquirer, we managed to accelerate the training of the symbolic model to a large extent. Our proposed approach takes advantage of both distributed and symbolic worlds, and achieves high accuracy, high execution efficiency, high learning efficiency, as well as high interpretability.\nIn future work, we would like to design interpretable operator selection in the distributed model to better couple the two worlds and to further ease the training with REINFORCE. We would also like to feed back the symbolic enquirer’s execution results to better train the distributed one. Such step-by-step supervision may also be iterated in a co-training fashion."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank Pengcheng Yin, Jiatao Gu, and Hao Zhou for helpful discussion."
    } ],
    "references" : [ {
      "title" : "Language to logical form with neural attention",
      "author" : [ "Li Dong", "Mirella Lapata" ],
      "venue" : "In ACL,",
      "citeRegEx" : "Dong and Lapata.,? \\Q2016\\E",
      "shortCiteRegEx" : "Dong and Lapata.",
      "year" : 2016
    }, {
      "title" : "Serial order: A parallel distributed processing approach",
      "author" : [ "Michael I Jordan" ],
      "venue" : "Advances in psychology,",
      "citeRegEx" : "Jordan.,? \\Q1997\\E",
      "shortCiteRegEx" : "Jordan.",
      "year" : 1997
    }, {
      "title" : "Neural symbolic machines: Learning semantic parsers on freebase with weak supervision",
      "author" : [ "Chen Liang", "Jonathan Berant", "Quoc Le", "Kenneth D Forbus", "Ni Lao" ],
      "venue" : "arXiv preprint arXiv:1611.00020,",
      "citeRegEx" : "Liang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2016
    }, {
      "title" : "Simpler context-dependent logical forms via model projections",
      "author" : [ "Reginald Long", "Panupong Pasupat", "Percy Liang" ],
      "venue" : "In ACL,",
      "citeRegEx" : "Long et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Long et al\\.",
      "year" : 2016
    }, {
      "title" : "Investigation of recurrent-neural-network architectures and learning methods for spoken language understanding",
      "author" : [ "Grégoire Mesnil", "Xiaodong He", "Li Deng", "Yoshua Bengio" ],
      "venue" : "In INTERSPEECH,",
      "citeRegEx" : "Mesnil et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mesnil et al\\.",
      "year" : 2013
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Neural programmer: Inducing latent programs with gradient descent",
      "author" : [ "Arvind Neelakantan", "Quoc V Le", "Ilya Sutskever" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Neelakantan et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Neelakantan et al\\.",
      "year" : 2016
    }, {
      "title" : "Inferring logical forms from denotations",
      "author" : [ "Panupong Pasupat", "Percy Liang" ],
      "venue" : "In ACL,",
      "citeRegEx" : "Pasupat and Liang.,? \\Q2016\\E",
      "shortCiteRegEx" : "Pasupat and Liang.",
      "year" : 2016
    }, {
      "title" : "Sequence level training with recurrent neural networks",
      "author" : [ "MarcAurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Ranzato et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ranzato et al\\.",
      "year" : 2016
    }, {
      "title" : "Reinforcement Learning: An Introduction, volume 1",
      "author" : [ "Richard S Sutton", "Andrew G Barto" ],
      "venue" : null,
      "citeRegEx" : "Sutton and Barto.,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton and Barto.",
      "year" : 1998
    }, {
      "title" : "A network-based end-to-end trainable taskoriented dialogue system",
      "author" : [ "Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "Lina M RojasBarahona", "Pei-Hao Su", "Stefan Ultes", "David Vandyke", "Steve Young" ],
      "venue" : "arXiv preprint arXiv:1604.04562,",
      "citeRegEx" : "Wen et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wen et al\\.",
      "year" : 2016
    }, {
      "title" : "Sequence-based structured prediction for semantic parsing",
      "author" : [ "Chunyang Xiao", "Marc Dymetman", "Claire Gardent" ],
      "venue" : "In ACL,",
      "citeRegEx" : "Xiao et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Xiao et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural generative question answering",
      "author" : [ "Jun Yin", "Xin Jiang", "Zhengdong Lu", "Lifeng Shang", "Hang Li", "Xiaoming Li" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "Yin et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Yin et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural enquirer: Learning to query tables with natural language",
      "author" : [ "Pengcheng Yin", "Zhengdong Lu", "Hang Li", "Ben Kao" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "Yin et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Yin et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : ", 2016a], human-computer conversation [Wen et al., 2016], etc.",
      "startOffset" : 38,
      "endOffset" : 56
    }, {
      "referenceID" : 3,
      "context" : "A typical approach is to convert a natural language sentence to an “executable” logic forms for table/knowledge base querying, known as semantic parsing [Long et al., 2016; Pasupat and Liang, 2016].",
      "startOffset" : 153,
      "endOffset" : 197
    }, {
      "referenceID" : 7,
      "context" : "A typical approach is to convert a natural language sentence to an “executable” logic forms for table/knowledge base querying, known as semantic parsing [Long et al., 2016; Pasupat and Liang, 2016].",
      "startOffset" : 153,
      "endOffset" : 197
    }, {
      "referenceID" : 0,
      "context" : "With the fast development of modern neural networks, Dong and Lapata [2016] and Xiao et al.",
      "startOffset" : 53,
      "endOffset" : 76
    }, {
      "referenceID" : 0,
      "context" : "With the fast development of modern neural networks, Dong and Lapata [2016] and Xiao et al. [2016] apply sequenceto-sequence (seq2seq) neural models to generate a logic form conditioned on an input sentence.",
      "startOffset" : 53,
      "endOffset" : 99
    }, {
      "referenceID" : 6,
      "context" : "An emerging research topic in neural semantic parsing is to query a knowledge base directly by neural networks [Yin et al., 2016b; Neelakantan et al., 2016; Liang et al., 2016].",
      "startOffset" : 111,
      "endOffset" : 176
    }, {
      "referenceID" : 2,
      "context" : "An emerging research topic in neural semantic parsing is to query a knowledge base directly by neural networks [Yin et al., 2016b; Neelakantan et al., 2016; Liang et al., 2016].",
      "startOffset" : 111,
      "endOffset" : 176
    }, {
      "referenceID" : 2,
      "context" : ", 2016; Liang et al., 2016]. Because queries can be composited in a highly complicated manner, neural enquirers necessitate multiple steps of execution. The difficulty then lies in the lack of step-by-step supervision. In other words, we only assume groundtruth denotations are available in realistic settings, and that we do not know the execution sequence or intermediate execution results. There have seen several studies addressing the problem. Yin et al. [2016b] propose a fully distributed neural enquirer, comprising several neuralized execution layers of field attention, row annotation, etc.",
      "startOffset" : 8,
      "endOffset" : 468
    }, {
      "referenceID" : 2,
      "context" : ", 2016; Liang et al., 2016]. Because queries can be composited in a highly complicated manner, neural enquirers necessitate multiple steps of execution. The difficulty then lies in the lack of step-by-step supervision. In other words, we only assume groundtruth denotations are available in realistic settings, and that we do not know the execution sequence or intermediate execution results. There have seen several studies addressing the problem. Yin et al. [2016b] propose a fully distributed neural enquirer, comprising several neuralized execution layers of field attention, row annotation, etc. While the model is not efficient in execution because of intensive matrix/vector operation during neural information processing and lacks explicit interpretation of execution, it can be trained in an end-to-end fashion because all components in the neural enquirer are differentiable. Neelakantan et al. [2016], on the other hand, propose a neural programmer by defining a set of symbolic operations (e.",
      "startOffset" : 8,
      "endOffset" : 912
    }, {
      "referenceID" : 2,
      "context" : ", 2016; Liang et al., 2016]. Because queries can be composited in a highly complicated manner, neural enquirers necessitate multiple steps of execution. The difficulty then lies in the lack of step-by-step supervision. In other words, we only assume groundtruth denotations are available in realistic settings, and that we do not know the execution sequence or intermediate execution results. There have seen several studies addressing the problem. Yin et al. [2016b] propose a fully distributed neural enquirer, comprising several neuralized execution layers of field attention, row annotation, etc. While the model is not efficient in execution because of intensive matrix/vector operation during neural information processing and lacks explicit interpretation of execution, it can be trained in an end-to-end fashion because all components in the neural enquirer are differentiable. Neelakantan et al. [2016], on the other hand, propose a neural programmer by defining a set of symbolic operations (e.g., argmax, greater than); at each step, all possible execution results are fused by a softmax layer, which predicts the probability of each operator at the current step. The step-bystep fusion is accomplished by weighted sum and the model is trained with mean square error. Hence, such approaches work with numeric tables, but may not be suited for other operations like string matching; it also suffers from the problem of “exponential numbers of combinatorial states.” Liang et al. [2016] train a symbolic executor by REINFORCE policy gradient, but it is known that the REINFORCE algorithm is sensitive to the",
      "startOffset" : 8,
      "endOffset" : 1496
    }, {
      "referenceID" : 12,
      "context" : "For example, the field attention gadget in Yin et al. [2016b] generally aligns with column selection.",
      "startOffset" : 43,
      "endOffset" : 62
    }, {
      "referenceID" : 12,
      "context" : "For example, the field attention gadget in Yin et al. [2016b] generally aligns with column selection. We therefore use the distributed model’s intermediate execution results as supervision signals to pretrain a symbolic executor. Guided by such imperfect step-by-step supervision, the symbolic executor learns a fairly meaningful initial policy, which largely alleviates the cold start problem of the REINFORCE algorithm. We evaluate the proposed approach on the QA dataset in Yin et al. [2016b]. In our experiment, the REINFORCE algorithm alone takes long to get started; even if it does, it is stuck in a poor local optimum.",
      "startOffset" : 43,
      "endOffset" : 496
    }, {
      "referenceID" : 12,
      "context" : "In this section, we first introduce the fully distributed (neuralized) enquirer proposed in Yin et al. [2016b]. Then we design a set of operators that are complete to the task at hand.",
      "startOffset" : 92,
      "endOffset" : 111
    }, {
      "referenceID" : 5,
      "context" : "One of the most notable studies of distributed semantics is word embeddings, which map discrete words to vectors as anonymous meaning representations [Mikolov et al., 2013].",
      "startOffset" : 150,
      "endOffset" : 172
    }, {
      "referenceID" : 12,
      "context" : "Please refer to Yin et al. [2016b] for details of the distributed neural enquirer.",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 1,
      "context" : "Such architecture is known as a Jordan-type RNN [Jordan, 1997; Mesnil et al., 2013].",
      "startOffset" : 48,
      "endOffset" : 83
    }, {
      "referenceID" : 4,
      "context" : "Such architecture is known as a Jordan-type RNN [Jordan, 1997; Mesnil et al., 2013].",
      "startOffset" : 48,
      "endOffset" : 83
    }, {
      "referenceID" : 12,
      "context" : "Table 3: Accuracies (in percentage) of Sempre tookit (reported in Yin et al. [2016b]), the distributed neural enquirer, and our proposed coupled approach.",
      "startOffset" : 66,
      "endOffset" : 85
    }, {
      "referenceID" : 9,
      "context" : "After obtaining a meaningful, albeit imperfect, initial policy, we apply REINFORCE [Sutton and Barto, 1998] to improve the policy.",
      "startOffset" : 83,
      "endOffset" : 107
    }, {
      "referenceID" : 8,
      "context" : "This is a common practice for REINFORCE [Ranzato et al., 2016].",
      "startOffset" : 40,
      "endOffset" : 62
    }, {
      "referenceID" : 12,
      "context" : "We evaluated our approach on a QA dataset in Yin et al. [2016b].3 The dataset comprises 25k different tables and queries.",
      "startOffset" : 45,
      "endOffset" : 64
    }, {
      "referenceID" : 12,
      "context" : "All settings of distributed neural enquirers were derived from Yin et al. [2016b] so that we can have a fair comparison.",
      "startOffset" : 63,
      "endOffset" : 82
    } ],
    "year" : 2016,
    "abstractText" : "Building neural networks to query a knowledge base (a table) with natural language is an emerging research topic in NLP. The neural enquirer typically necessitates multiple steps of execution because of the compositionality of queries. In previous studies, researchers have developed either distributed enquirers or symbolic ones for table querying. The distributed enquirer is end-to-end learnable, but is weak in terms of execution efficiency and explicit interpretability. The symbolic enqurier, on the contrary, is efficient during execution; but it is very difficult to train especially at initial stages. In this paper, we propose to couple distributed and symbolic execution for natural language queries. The observation is that a fully distributed executor also exhibits meaningful, albeit imperfect, interpretation. We can thus pretrain the symbolic executor with the distributed one’s intermediate execution results in a step-by-step fashion. Experiments show that our approach significantly outperforms either the distributed or symbolic executor; moreover, we have recovered more than 80% execution sequences with only groundtruth denotations during training. In summary, the coupled neural enquirer takes advantages of both distributed and symbolic executors, and has high performance, high learning efficiency, high execution efficiency, and high interpretability.",
    "creator" : "LaTeX with hyperref package"
  }
}