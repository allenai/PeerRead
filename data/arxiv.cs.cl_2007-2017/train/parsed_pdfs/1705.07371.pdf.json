{
  "name" : "1705.07371.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Spelling Correction as a Foreign Language",
    "authors" : [ "Yingbo Zhou", "Utkarsh Porwal", "Roberto Konow" ],
    "emails" : [ "yingbzhou@ebay.com", "uporwal@ebay.com", "rkonow@ebay.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Misspellings are so often that automatic spelling correction is a crucial component for any service that requires users to type in natural language, such as a search engine. A correct spelling correction not only reduces the user’s mental load for the task, but also improves the quality of the service as it attempts to predict user’s intention. From a probabilistic perspective, let x̃ be the misspelled text that we observe, spelling correction seeks to uncover the true text x∗ = argmaxx P (x|x̃). Traditionally, spelling correction problem has been mostly approached by using the noisy channel model Kernighan et al. (1990). The model consists of two parts: 1) a language model (or source model, i.e. P (x)) that represent the prior probability of the intended correct input text; and 2) an error model (or channel model, i.e. P (x̃|x)) that represent the process, in which the correct input text got corrupted to an incorrect misspelled text. The final correction is therefore obtained by using the Bayes rule, i.e. x∗ = argmaxx P (x)P (x̃|x). There are several problem with this approach: 1) we need two separate models and the error in estimating one model would affect the performance of the final output. 2) It is not easy to model the channel since there is a lot of sources for spelling mistakes, e.g. typing too fast, unintentional key stroke, phonetic ambiguity, etc. 3) In certain context (e.g. in a search engine) it is not easy to obtain clean training data for language model as the input does not follow what is typical in natural language.\nSince the goal is to get text that maximize P (x|x̃), can we directly model this conditional distribution instead? In this work, we explore this route, which by passes the need to have multiple models and avoid getting errors from multiple sources. We achieve this by applying the sequence to sequence learning framework using recurrent neural networks Sutskever et al. (2014) and reformulate the spelling correction problem as a neural machine translation problem, where the misspelled input is treated as a foreign language."
    }, {
      "heading" : "2 Background and Preliminaries",
      "text" : "The recurrent neural network (RNN) is a natural extension to feed-forward neural network for modeling sequential data. More formally, let (x1, x2, . . . , xT ), xt ∈ Rd be the input, an RNN update its internal recurrent hidden states by doing the following computation:\nht = ψ(ht−1, xt) (1)\nar X\niv :1\n70 5.\n07 37\n1v 1\n[ cs\n.C L\n] 2\n1 M\nay 2\n01 7\nwhere ψ is a nonlinear function. Traditionally, in a standard RNN the ψ is implemented as an affine transformation followed by a pointwise nonlinearity, such as\nht = ψ(ht−1, xt) = tanh(Wxt + Uht−1 + bh)\nIn addition, the RNN may also have outputs (y1, y2, . . . , yT ), yt ∈ Ro that can be calculated by using another nonlinear function φ\nyt = φ(ht, xt)\nFrom this recursion, the recurrent neural network naturally models the conditional probability P (yt|x1, . . . , xt). One problem with standard RNN is that it is difficult for them to learn long term dependencies Bengio et al. (1994); Hochreiter et al. (2001), and therefore in practice more sophisticated function ψ are often used to alleviate this problem. For example the long short term memory (LSTM) Hochreiter & Schmidhuber (1997) is one widely used recursive unit that is designed to learn long term dependencies. A layer LSTM consists of three gates and one memory cell, the computation of LSTM is as following1:\nit = σ(Wixt + Uiht−1 + bi) (2) ot = σ(Woxt + Uoht−1 + bo) (3) ft = σ(Wfxt + Ufht−1 + bf ) (4) ct = ft ct−1 + (1− ft) tanh(Wcxt + Ucht−1 + bc) (5) ht = ot tanh(ct) (6)\nwhere W , U , and b represents the corresponding input-to-hidden, hidden-to-hidden weights and biases respectively. σ(·) denotes the sigmoid function, and is the elementwise product. Another problem when using RNN to solve sequence to sequence learning problem is that it is not clear what strategy to apply when the input and output sequence does not share the same length (i.e. for outputs we have T ′ time steps, which may not equal to T ), which is the typical setting for this type of tasks. Sutskever et al. Sutskever et al. (2014) propose to use an auto-encoder type of strategy, where the input sequence is encoded to a fixed length vector by using the last hidden state of the recurrent neural network, and then decode the output sequence from the vector. In more detail, let input and output sequence have T and T ′ time steps, and fe, fd denote the encoding and decoding functions respectively, then the model tries to learn P (y1, . . . , yT ′ |x1, . . . , xT ) by\ns , fe(x1, . . . , xT ) = hT (7)\nyt , fd(s, y1, . . . , yt−1) (8)\nwhere fe and fd are implemented using multi-layer LSTMs."
    }, {
      "heading" : "3 Spelling Correction as a Foreign Language",
      "text" : "It is easy to see that spelling correction problem can be formulated as a sequence to sequence learning problem as mentioned in section 2. In this sense, it is very similar to a machine translation problem, where the input is the misspelled text and the output is the corresponding correct spellings. One challenge for this formulation is that unlike in machine translation problem, the vocabulary is large but still limited2. However, in spelling correction, the input vocabulary is potentially unbounded, which rules out the possibility of applying word based encoding for this problem. In addition, the large output vocabulary is a general challenge in neural network based machine translation models because of the large Softmax output matrix.\nThe input/output vocabulary problem can be solved by using a character based encoding scheme. Although it seems appropriate for encoding the input, this scheme puts unnecessary burden on the decoder, since for a correction the decoder need to learn the correct spelling of the word, word boundaries, and etc. We choose the byte pair encoding (BPE) scheme Sennrich et al. (2015) that strikes the balance between too large output vocabulary and too much learning burden for decoders.\n1Sometimes additional weight matrix and vector are added to generate output from ht for LSTM, we choose to stick with the original formulation for simplicity.\n2The vocabulary is limited in a sense that the number of words are upper bounded, in general\nIn this scheme, the vocabulary is built by recursively merging most frequent pairs of strings starting from character, and the vocabulary size is controlled by the number of merging iterations.\nAs shown in papers Bahdanau et al. (2014), encoding the whole input string to a single fixed length vector is not optimal, since it may not reserve all the information that is required for a successful decoding. Therefore, we introduce the attention mechanism from Bahdanau et al.Bahdanau et al. (2014) into this model. Formally, the attention model calculates a context vector ci from the encoding states h1, . . . , hT and decoder state si−1by\nci = T∑ j=1 λijhj (9)\nλij = exp{aij}∑T k=1 exp{aik}\n(10)\naij = tanh(Wssi−1 +Whhj + b) (11) where Ws, Wh are the weight vector for alignment model, and b denotes the bias.\nNow we are ready to introduce the full model for spelling correction. The model takes a sequence of input (characters or BPE encoded sub-words) x1, . . . , xT and outputs a sequence of BPE encoded sub-words y1, . . . , yT ′ . For each input token the encoder learns a function fe to map to its hidden representation ht\nht = fe(ht−1, xt; θe) (12) h0 = 0 (13)\nThe attentional decoder first obtain the context vector ct based on equation 10, and then learns a function fd that decodes yt from the context vector ct\np(yt|st) = softmax(Wst + bd) (14) st = fd(st−1, ct; θd) (15) s0 = UhT (16)\nwhere W and bd are the output matrix and bias, U is a matrix that make sure that the hidden states of encoder would be consistent with the decoder’s. In our implementation, both fe and fd are modeled using a multi-layer LSTM. As a whole, the end-to-end model is then trying to learn\np(y1, . . . , yT ′ |x1, . . . , xT ) = T ′∏ i=1 p(yi|x1, . . . , xT , yi−1) (17)\n= T ′∏ i=1 p(yi|fd(si−1, ci); θd) (18)\nnotice that in equation 18 the context vector ci is a function of the encoding function fe, so we are not left the encoder isolated. Since all components are smooth and differentiable, the model can be easily trained with gradient based method to maximize the likelihood on the dataset."
    }, {
      "heading" : "4 Experiments",
      "text" : "We test our model in the setting of correcting e-commerce queries. Unlike machine translation problem, there is no public datasets for e-commerce spelling correction, and therefore we collect both training and evaluation data internally. For training data, we use the event logs that tracks user behavior on an e-commerce website. Our heuristic for finding potential spelling related queries is based on consecutive user actions in one search session. The hypothesis is that users will try to modify the search query until the search result is desirable with the search intent, and from this sequence of action on queries we can potentially extract the misspelling and correct spelled query pair. Obviously, this includes a lot more diversity on query activities besides spelling mistakes, and thus additional filtering is required to obtain representative data for spelling correction. We use the same techniques as Hasan et al.Hasan et al. (2015). Filtering multiple months of data from our data warehouse, we got about 70 million misspelling and spell correction pairs as our training data. For testing, we use the same dataset as in paper Hasan et al. (2015), where it contains 4602 queries and the samples are labeled by human.\nWe use beam search to obtain the final result from the model. The result is illustrated in table 1, it is clear that our albeit much simpler, our RNN based model offers competitive performance as compare to the previous methods. It is interesting to note that, the BPE based encoder and decoder performs the best. The better performance may attribute to the shorter resultant sequence as compared to the character case, and possibly more semantic meaningful segments from the sub-words as compared to the characters. Surprisingly, the character based decoder performs quite well considering the complexity of the learning task. This demonstrated the benefit from end-to-end training and the robustness of the framework."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we reformulated the spelling correction problem as a machine translation task under the encoder-decoder framework. The reformulation allowed us to use a single model for solving the problem and can be trained from end-to-end. We demonstrate the effectiveness of this model using an internal dataset, where the training data is automatically obtained from user logs. Despite the simplicity of the model, it performed competitively as compared to the state of the art methods that require a lot of feature engineering and human intervention."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua" ],
      "venue" : "arXiv preprint arXiv:1409.0473",
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning long-term dependencies with gradient descent is difficult",
      "author" : [ "Bengio", "Yoshua", "Simard", "Patrice", "Frasconi", "Paolo" ],
      "venue" : "IEEE transactions on neural networks,",
      "citeRegEx" : "Bengio et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 1994
    }, {
      "title" : "Spelling Correction of User Search Queries through Statistical Machine Translation",
      "author" : [ "Hasan", "Sasa", "Heger", "Carmen", "Mansour", "Saab" ],
      "venue" : null,
      "citeRegEx" : "Hasan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hasan et al\\.",
      "year" : 2015
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Hochreiter", "Sepp", "Schmidhuber", "Jürgen" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Hochreiter et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter et al\\.",
      "year" : 1997
    }, {
      "title" : "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies",
      "author" : [ "Hochreiter", "Sepp", "Bengio", "Yoshua", "Frasconi", "Paolo", "Schmidhuber", "Jürgen" ],
      "venue" : null,
      "citeRegEx" : "Hochreiter et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Hochreiter et al\\.",
      "year" : 2001
    }, {
      "title" : "A spelling correction program based on a noisy channel model. Pages 205–210",
      "author" : [ "Kernighan", "Mark D", "Church", "Kenneth W", "Gale", "William A" ],
      "venue" : "of: Proceedings of the 13th conference on Computational linguistics-Volume",
      "citeRegEx" : "Kernighan et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Kernighan et al\\.",
      "year" : 1990
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Sennrich", "Rico", "Haddow", "Barry", "Birch", "Alexandra" ],
      "venue" : "arXiv preprint arXiv:1508.07909",
      "citeRegEx" : "Sennrich et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2015
    }, {
      "title" : "Sequence to sequence learning with neural networks. Pages 3104–3112 of: Advances in neural information processing systems",
      "author" : [ "Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V" ],
      "venue" : null,
      "citeRegEx" : "Sutskever et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "Traditionally, spelling correction problem has been mostly approached by using the noisy channel model Kernighan et al. (1990). The model consists of two parts: 1) a language model (or source model, i.",
      "startOffset" : 103,
      "endOffset" : 127
    }, {
      "referenceID" : 5,
      "context" : "Traditionally, spelling correction problem has been mostly approached by using the noisy channel model Kernighan et al. (1990). The model consists of two parts: 1) a language model (or source model, i.e. P (x)) that represent the prior probability of the intended correct input text; and 2) an error model (or channel model, i.e. P (x̃|x)) that represent the process, in which the correct input text got corrupted to an incorrect misspelled text. The final correction is therefore obtained by using the Bayes rule, i.e. x∗ = argmaxx P (x)P (x̃|x). There are several problem with this approach: 1) we need two separate models and the error in estimating one model would affect the performance of the final output. 2) It is not easy to model the channel since there is a lot of sources for spelling mistakes, e.g. typing too fast, unintentional key stroke, phonetic ambiguity, etc. 3) In certain context (e.g. in a search engine) it is not easy to obtain clean training data for language model as the input does not follow what is typical in natural language. Since the goal is to get text that maximize P (x|x̃), can we directly model this conditional distribution instead? In this work, we explore this route, which by passes the need to have multiple models and avoid getting errors from multiple sources. We achieve this by applying the sequence to sequence learning framework using recurrent neural networks Sutskever et al. (2014) and reformulate the spelling correction problem as a neural machine translation problem, where the misspelled input is treated as a foreign language.",
      "startOffset" : 103,
      "endOffset" : 1435
    }, {
      "referenceID" : 1,
      "context" : "One problem with standard RNN is that it is difficult for them to learn long term dependencies Bengio et al. (1994); Hochreiter et al.",
      "startOffset" : 95,
      "endOffset" : 116
    }, {
      "referenceID" : 1,
      "context" : "One problem with standard RNN is that it is difficult for them to learn long term dependencies Bengio et al. (1994); Hochreiter et al. (2001), and therefore in practice more sophisticated function ψ are often used to alleviate this problem.",
      "startOffset" : 95,
      "endOffset" : 142
    }, {
      "referenceID" : 1,
      "context" : "One problem with standard RNN is that it is difficult for them to learn long term dependencies Bengio et al. (1994); Hochreiter et al. (2001), and therefore in practice more sophisticated function ψ are often used to alleviate this problem. For example the long short term memory (LSTM) Hochreiter & Schmidhuber (1997) is one widely used recursive unit that is designed to learn long term dependencies.",
      "startOffset" : 95,
      "endOffset" : 319
    }, {
      "referenceID" : 1,
      "context" : "One problem with standard RNN is that it is difficult for them to learn long term dependencies Bengio et al. (1994); Hochreiter et al. (2001), and therefore in practice more sophisticated function ψ are often used to alleviate this problem. For example the long short term memory (LSTM) Hochreiter & Schmidhuber (1997) is one widely used recursive unit that is designed to learn long term dependencies. A layer LSTM consists of three gates and one memory cell, the computation of LSTM is as following1: it = σ(Wixt + Uiht−1 + bi) (2) ot = σ(Woxt + Uoht−1 + bo) (3) ft = σ(Wfxt + Ufht−1 + bf ) (4) ct = ft ct−1 + (1− ft) tanh(Wcxt + Ucht−1 + bc) (5) ht = ot tanh(ct) (6) where W , U , and b represents the corresponding input-to-hidden, hidden-to-hidden weights and biases respectively. σ(·) denotes the sigmoid function, and is the elementwise product. Another problem when using RNN to solve sequence to sequence learning problem is that it is not clear what strategy to apply when the input and output sequence does not share the same length (i.e. for outputs we have T ′ time steps, which may not equal to T ), which is the typical setting for this type of tasks. Sutskever et al. Sutskever et al. (2014) propose to use an auto-encoder type of strategy, where the input sequence is encoded to a fixed length vector by using the last hidden state of the recurrent neural network, and then decode the output sequence from the vector.",
      "startOffset" : 95,
      "endOffset" : 1208
    }, {
      "referenceID" : 6,
      "context" : "We choose the byte pair encoding (BPE) scheme Sennrich et al. (2015) that strikes the balance between too large output vocabulary and too much learning burden for decoders.",
      "startOffset" : 46,
      "endOffset" : 69
    }, {
      "referenceID" : 0,
      "context" : "As shown in papers Bahdanau et al. (2014), encoding the whole input string to a single fixed length vector is not optimal, since it may not reserve all the information that is required for a successful decoding.",
      "startOffset" : 19,
      "endOffset" : 42
    }, {
      "referenceID" : 0,
      "context" : "As shown in papers Bahdanau et al. (2014), encoding the whole input string to a single fixed length vector is not optimal, since it may not reserve all the information that is required for a successful decoding. Therefore, we introduce the attention mechanism from Bahdanau et al.Bahdanau et al. (2014) into this model.",
      "startOffset" : 19,
      "endOffset" : 303
    }, {
      "referenceID" : 2,
      "context" : "We use the same techniques as Hasan et al.Hasan et al. (2015). Filtering multiple months of data from our data warehouse, we got about 70 million misspelling and spell correction pairs as our training data.",
      "startOffset" : 30,
      "endOffset" : 62
    }, {
      "referenceID" : 2,
      "context" : "We use the same techniques as Hasan et al.Hasan et al. (2015). Filtering multiple months of data from our data warehouse, we got about 70 million misspelling and spell correction pairs as our training data. For testing, we use the same dataset as in paper Hasan et al. (2015), where it contains 4602 queries and the samples are labeled by human.",
      "startOffset" : 30,
      "endOffset" : 276
    }, {
      "referenceID" : 2,
      "context" : "Method Accuracy Hasan et al.Hasan et al. (2015) 62.",
      "startOffset" : 16,
      "endOffset" : 48
    } ],
    "year" : 2017,
    "abstractText" : "In this paper, we reformulated the spell correction problem as a machine translation task under the encoder-decoder framework. This reformulation enabled us to use a single model for solving the problem that is traditionally formulated as learning a language model and an error model. This model employs multi-layer recurrent neural networks as an encoder and a decoder. We demonstrate the effectiveness of this model using an internal dataset, where the training data is automatically obtained from user logs. The model offers competitive performance as compared to the state of the art methods but does not require any feature engineering nor hand tuning between models.",
    "creator" : "LaTeX with hyperref package"
  }
}