{
  "name" : "1603.06829.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Multi-velocity neural networks for gesture recognition in videos",
    "authors" : [ "Otkrist Gupta", "Dan Raviv", "Ramesh Raskar" ],
    "emails" : [ "otkrist@mit.edu", "raviv@mit.edu", "raskar@media.mit.edu" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Nonverbal communication is a key factor in the interaction between individuals, replacing or amplifying spoken words. Our body language, voice pitch, intonation and volume, movement of our pupils or our chronemics choices are just a few examples emphasizing the richness of human communication skills [32]. A special subset of nonverbal interactions, explored in this paper, is based on facial expressions. Their perception initiates rapid cognitive processes in the brain and have both communicative and reflexive components [12]. They assist in verbal communication by providing context to what we are saying, making their recognition important for studying social interactions.\nWe use computers on a daily basis, interacting with artificial agents in increasing number of tasks. Advances in speech and natural language processing have presented us with personalized smart agents [42, 11], while examining our facial gestures has provided a richer set of tools for improving human computer interaction [9]. In the last two decades we have seen several attempts to automate the way computers respond towards human emotions [23, 6, 2], where the ultimate goal is to create humanoid robots which can blend in the environment [53].\nUnfortunately this task is hard to solve, and current state-\nof-the-art results are far from satisfactory. Recent advances in machine learning have shown that if we provide a neural network with enough samples, it can learn very complex structures [15]. Today hard tasks in computer vision, such as labeling images, recognizing objects and faces or classifying videos have become a feasible task for computers which can now provide competitive results to humans and sometimes even outperform them [37, 45].\nIn this paper we focus on facial gesture recognition from videos using deep neural networks. We tackle the problem of a small size labeled dataset, and present a new layer which compensates for velocity changes in the time domain. We compare our methods to multiple techniques and datasets, as well as presenting our own collected data, and we report state-of-the-art results in almost every category.\nWe hope to empower researchers in this area by providing them with a huge dataset which can help them build even bigger and better deep neural networks, while eliminating the need to spend several months required to acquire a dataset of such proportions, and for some living under limited resources acquiring such a dataset can be prohibitively impossible. We summarize our contributions by:"
    }, {
      "heading" : "1.1. Contributions",
      "text" : "1. To the best of out knowledge, we have built the largest face video dataset to date comprising of 162 million face images with facial landmark labels (7.8 billion annotations) contained in 6.5 million video clips, and 2777 videos which have been labeled for seven emotions. The dataset will be made public for research purposes.\n2. We develop a multi-velocity autoencoder architecture using new multi-velocity layers for generating velocity-free deep motion features.\n3. We report state of the art results for video gesture recognition using spatio-temporal convolutional neural networks.\n4. We introduce a new topology and protocol for semisupervised learning, where the number of labeled data\n1\nar X\niv :1\n60 3.\n06 82\n9v 1\n[ cs\n.C V\n] 2\n2 M\nar 2\n01 6\npoints is only a fraction of the entire dataset."
    }, {
      "heading" : "2. Related Work",
      "text" : "Machine learning techniques such as Support Vector Machines have been used for facial expression recognition given the movement of facial fiducial points [24, 33, 41, 10] achieving real time performance [38]. Many of these techniques involve a pipeline with multiple phases - face detection and alignment, feature extraction/landmark localization and classification as the final step. Other interesting approaches [7, 50, 36, 48] we should mention are based on temporal features [28, 51], and multiple kernels [29], action units [54, 40], as well as emotion recognition from speech [34, 39]. We will compare our method against some of those approaches in section 5.\nRecently, deep neural nets have been shown to perform well on classification tasks on images and videos, outperforming most traditional learning systems. One of the most interesting results was presented three years back on a large scale dataset (LSVRC 2011), where a deep convolutional net outperformed all other methods by far [25]. With advances in convolutional neural nets, we have seen neural nets applied to video classification [21, 46] and even facial expression recognition [1, 13] but these networks were not deep enough or used other feature extraction techniques like PCA or Fisherface.\nTraining a neural net normally requires a large labeled dataset which is hard to obtain using reasonable resources. Providing high quality results when only a small part of the data is labeled is an interesting problem referred to as semi-supervised learning. In [26] the authors pre-trained the system using pseudo labels, while in [52, 22] they embedded the data in a low dimensional space. Very recently superior results have been shown [30, 20, 18, 14, 19] using deep neural nets to combine labels and un-labeled data in the same package. In this paper we follow those guidelines and train from start-to-end a hybrid system composed\nof autoencoders for unlabeled data and additional loss function for the classification tasks."
    }, {
      "heading" : "3. Method",
      "text" : "We propose a semi-supervised approach using a deep neural network, by combining an autoencoder with a classification loss function, and training both of them in parallel. The input for the first layer is a short sequence of facial gestures composed of 9 frames cropped to 145 × 145 pixels window. The loss function is evaluated by combining a predictive loss from 7 different pre-labeled gestures (for the labeled part of the dataset), and autoencoder Euclidean loss for the entire (labeled and un-labeled) collection. The weights of each layer are dynamically altered such that the importance of the autoencoder loss decreases with relation to the predictive loss as the training progresses. While generating the data, we use Viola and Jones face detection [49] for cropping the faces. We use slow fusion based convolutional neural network with convolutions in both space and time (see figure 4 for a detailed overview)."
    }, {
      "heading" : "3.1. Action autoencoder",
      "text" : "Our action autoencoder consists of convolutional autoencoder for learning deep features and reducing the dimensionality of the data. We use convolutional filters with weight sharing in the first 6 layers followed by 2 fully connected layers. This network is similar to Imagenet [25] but accepts inputs of size 145 × 145 × 9 as an input. Using shorthand notation the full architecture can be written as C(96, 11, 3) − N − C(256, 5, 2) − N − C(384, 3, 2) − N − FC(4096) − FC(4096) − DC(96, 11, 3) − N − DC(256, 5, 2) − N − DC(384, 3, 2), where C(n, f, s) stands for convolution layers with n filters of size f and stride s. DC(n, f, s) stands for deconvolution layers with n deconvolving filters of size f and stride s. FC(n) stands for fully connected layers with n nodes and N stands for local response normalization layers. We extend the convo-\nlution layers in time and use slow fusion model [21] which slowly combines temporal information in successive layers. The first convolution filters have size 3 and stride 2 in time domain, the next layer has size 2 and stride 2 and the third layer combines all temporal features. The deconvolution layers are extended in time as well and reverse the slow fusion generating temporal features successively (see figure 3)."
    }, {
      "heading" : "3.2. Multi-Velocity Encoders",
      "text" : "One of the main challenges in action recognition is related to assigning similar classification to objects at different velocities. In this work we propose to learn the velocity of the sequence in parallel to its classification by adaptive temporal interpolation. Our multi-velocity autoencoder consists of 3 action autoencoders combined together to access temporal features for different velocities. We achieve this by adding a convolution layer as the first layer which uses cubic b-spline interpolation to slow down the video and generate intermediate frames. Piece-wise cubic b-spline interpolation is preferred over polynomial techniques as it can minimize interpolation error for fewer points and lower degree polynomials [16]. For initialization a sampling factor of 1, 2/3 and 1/3 is chosen, which is later refined as a part of the learning.\nNext we show how to generate the required weights for interpolation and encode them as a neural network layer. Cubic b-splines are continuous piecewise-polynomial functions containing polynomials of degree 3 or less. A cubic b-spline spanning n + 1 points comprises of n cubic polynomials ( Sn(x) N n=1 ) which can be uniquely defined using 4n coefficients. These coefficients can be recovered by ap-\nplying linear constraints arising from continuity and differentiability of the function on the break points (or knots). We represent input video at each pixel as a function of time and use cubic b-splines to approximate intermediate values. We represent intermediate polynomials between n+1 frames as a coefficient vector p̄ containing coefficients for all n polynomials.\nLet x̄, ȳ be the frame numbers and pixel values known to us at the different frames, these frames are obvious choices for break points as we try approximating space between frames using b-spline curves. Cubic b-spline coefficients p̄ for each pixel can be generated by solving a linear equation Ap̄ = Tȳ as shown in the appendix. Here both A and T depend only on frame numbers (x̄) and are independent of pixel values (ȳ) or pixel coordinates.\nLet uo be one of new points in time where we want to interpolate a video frame, we can compute it now by selecting kth consecutive frames containing uo, choosing piecewise polynomial contained in-between these frames (Sk(x)) and evaluating it at uo. We can write this as a dot product between coefficients and input r̄ · p̄, where uo lies between kth consecutive frames and r̄ is as defined below in (1):\n¯[ri] = { ui−4ko if xbi/4c ≤ uo ≤ xbi/4c+1 0 o/w\n(1)\nExtending (1) to several temporal positions; Let ū = [ui] be locations in time where frames needs to be interpolated, we compute them as a matrix vector product Rp̄. Here each row of R = [rj,i] is computed using a shifted version of the\nequation given above, specifically:\n[rj,i] = { (uj − j)i−4k if xbi/4c < uj < xbi/4c+1 0 o/w (2)\nEquation (2) shows how interpolated pixel values F(ū) are linearly related to b-spline coefficients Rp̄. Solving for p̄ using p̄ = A−1Tȳ we infer that F(ū) = RA−1Tȳ. The new velocity layer weights are initialized by computing matrix RA−1T which is independent of pixel values ȳ and their spatial locations. We can represent this matrix as a caffe convolution layer with shared weights, which contains n filters of size 1× 1×n applied to all frames of video. We use algorithm 1 to create 3 different weight matrices which interpolate sampling factors of 1, 2/3 and 1/3.\nAlgorithm 1 Generate convolution layer spline weights. Input: Frame numbers x̄, new temporal locations ū Output: Caffe Weight Matrix W\n1: function SPLINEWEIGHTS(c) 2: nSplines← length(x̄)− 1 3: for (i← 0; i < nSplines; i+ +) do 4: p← 4i 5: Tp,i ← 1 6: Tp+1,i+1 ← 1 7: for (h← 0;h <= 1;h+ +) do 8: s← p− 4h 9: Ap+h,p:p+3 ← [h3, h2, h, 1]\n10: Ai+2,s+4:s+8 ← −1h+1[3h2, 2h, 1, 0] 11: Ai+3,s+4:s+8 ← −1h+1[6h, 2, 0, 0] 12: Ai+3,i−4:i+3 ← [6, 0, 0, 0,−6, 0, 0, 0] 13: Ai+4,0:7 ← [6, 0, 0, 0,−6, 0, 0, 0] 14: for (i← 0; i < length(ū); i+ +) do 15: p← find(x̄, bū(i)c) 16: for (h← 0;h < 4;h+ +) do 17: Ri,4p+h ← (ū(i)− x̄(p))h 18: W← RA−1T 19: return W"
    }, {
      "heading" : "3.3. Semi-Supervised Learner",
      "text" : "One of the main challenges we face today for training deep neural networks is the need for large labeled datasets. The richness of data is probably one of the main reasons why neural nets report such impressive predictive results in almost every field, but it is also extremely hard to collect and label such datasets. In Semi-supervised paradigm, we assume that only a part of the data is labeled, yet we wish to utilize the knowledge hidden within the entire set. Here we combine the action autoencoder convolution layers with a softmax loss function for the labeled set. The classifier neural net is inspired by Imagenet [25], with additional\nfully connected layers which are shared with the autoenoder, to generate deeper classification features from the latter. The full architecture of the predictor is C(96, 11, 3) − N −C(256, 5, 2)−N −C(384, 3, 2)−N −FC(4096)− FC(8192) − FC(4096) − FC(512) − FC(8) with softmax layers in the end for label classification. Please refer to section 3.1 for explanation of the architecture shorthand.\nThe protocol we suggest for training the net is as important as the topology itself. We begin by training the autoencoder as a sole learner from the outer layer to the inner ones. Meaning, we adaptively add layers to the autoencoder, train the neural net, and use the produced weights as initialization for the next step. This is one of the traditional approaches used to train autoencoders [15, 5]. Next, we use the weights for initialization of the semi-supervised net, allowing the entire net to fine tune. A key factor in training is the learning rate of the two matched learners. We begin the training using a higher learning rate for the autoencoder (with predictor layers staying fixed using zero learning rate) and end the process with increased importance to the labeled loss function. While training on the labeled data, ratio between the two varies from a factor of 103 to a factor of 105 favoring the loss layer."
    }, {
      "heading" : "3.3.1 Multi-Velocity Semi-Supervised Learner",
      "text" : "Finally we attach the new proposed Multi-Velocity layers as the first structure of the semi-supervised neural net. Each sub-structure (See Figure 4), has its own autoencoder, all of which are concatenated after the inner most convolution layer into a feature vector (size 12288), later used by the labeled loss function. The learner loss function can be expressed as a weighted sum of autoencoder and predictor loss given in equation 3 below.\nL = α ∑ v ||x̄− x̄v|| − β ∑ j yj log ( eoj∑ k e ok ) (3)\nHere x̄, x̄v are autoencoder inputs and outputs, yj are the input labels and oj is the outputs from predictor layer.∑\nv ||x̄− x̄v|| is the combined Euclidean loss across three multi-velocity encoders and − ∑ j yj log ( eoj∑ k e ok ) is softmax loss [4]. While training using labeled data, the loss coefficient β is selected to keep softmax loss an order of magnitude higher than the Euclidean loss. Loss coefficient α is adjusted as softmax loss goes down to continue training predictor layers, without overfitting autoencoder layers. Notice that we use two coefficients for the energy function and not just controlling the ratio between the two since the back-propagation algorithm has its own additional parameters."
    }, {
      "heading" : "4. Datasets",
      "text" : "In order to evaluate the proposed architecture we use two known datasets from literature as well as present two additional datasets collected by us; The first dataset contains more than 160 million images combined into 6.5 million short (25 frames) clips, used by us to train our autoencoders. The second dataset is comprised of 2777 short clips labeled for seven emotions. In the following section we elaborate on the four datasets."
    }, {
      "heading" : "4.1. Autoencoder dataset",
      "text" : "In order to train very deep neural nets we must obtain a huge collection of data. Here we collected 6.5 million video clips containing 25 frames each, adding up to more than 162 million face images. We used viola-jones face detector to find and segment out the faces. Next, we localized landmarks for each frame using a deformable model for the face [3] and detected the facial pose by fitting a 3D model to the landmarks. This process allowed us to restrict the dataset to videos which contain faces tilted less than 30 degrees and remove any faces looking sideways.\nIn order to extract only meaningful video clips we re-\nmoved clips with static gestures or those where the faces were rapidly altering, either due to some high speed movement or simply due to appearance of a different face. We achieved this by blurring the clips and calculating the difference between consecutive frames.\nThe raw videos were taken from public sources such as CNN, MSNBC, FOX and CSPAN. To our knowledge this is the biggest facial dataset reported in literature, and we plan to make it public."
    }, {
      "heading" : "4.2. Asevo dataset",
      "text" : "In order to collect and label our own gestures we developed a video recording and annotation tools. We developed the application using python based OpenCV and captured the clips using Logitech C920 HD camera. The database contains facial clips from 160 subjects both male and female, where gestures were artificially generated according to a specific request, or genuinely given due to a shown stimulus. We collected a total of 2777 clips out of which 1745 were captured after providing the stimulus while 1032 were generated artificially. To create natural facial expressions we selected a bank of YouTube videos for each facial expression and showed them to subjects, capturing their reaction to the visual stimulus. We quantitatively summarize this dataset in table 2, where posed clips refers to the artificially generated expressions and non-posed to the stimulus activation procedure."
    }, {
      "heading" : "4.3. Cohn Kanade Dataset",
      "text" : "The Cohn Kanade Dataset [31] is one of the most popular datasets used for facial expression recognition. The dataset contains 593 sequences out of which 327 are labeled for 7 emotions. Along with posed facial expressions, the dataset also contains non-posed smile expressions. However the dataset lacks depth in having other non-posed expressions and is not extensive as Asevo dataset in capturing naturally expressed emotions. Each video clip contains facial expres-\nsion going from baseline neutral to peak of expressed emotion."
    }, {
      "heading" : "4.4. MMI Dataset",
      "text" : "MMI facial expression dataset [35] is an ongoing effort for representing both posed and non posed facial expressions. The dataset has total 2894 video clips out of which 197 have been labeled for six basic emotions. MMI originally contained only posed facial expressions and recently was extended to contain induced happiness, disgust and surprise [47]. Each video clip in MMI contains people going from neutral to peak and then back to neutral facial expression."
    }, {
      "heading" : "5. Experiments and Results",
      "text" : ""
    }, {
      "heading" : "5.1. Video autoencoder",
      "text" : "Our first experiment shows qualitatively results of a single video autoencoder. We use 145 × 145 × 9 clips as input, where the spatial resolution received by downsampling all clips to that single size using bspline interpolation, and 9 frames are extracted from the clip by using every third frame. We use caffe [17] to train the system. In practice we convert each video clip into a image strip containing con-\nsecutive frames placed horizontally and use caffe ”imagedata”, ”split” and ”concat” layers for video data input.\nWe minimize contrastive divergence [5] to train autoencoder layers successively. We train the first 4 beginning and end layers by creating an intermediate neural network (C(96, 11, 3)−N −C(256, 5, 2)−N −DC(256, 5, 2)− N − DC(384, 3, 2)) and training it on facial video clips. We then train third convolution and deconvolution layer by initializing weights from previously trained neural net and fixing the weights for first 4 beginning and end layers. We fine tune all layers once the neural net weights have converged. We repeat the process for fourth fully connected layer to generate deep features.\nPlease refer to figure 2 to see results from neural net based reconstruction using different number of layers."
    }, {
      "heading" : "5.2. Multi-velocity video autoencoder",
      "text" : "Multi velocity semi-supervised learner comprises of an array of three independent autoencoders and a predictor net. We initialize the autoencoders using the weights from the video autoencoder and add a convolution layer as described in section 3.2. We fine tune the multi-velocity layers by creating 3 datasets containing video clips at different velocities. We achieve that by selecting every third frame to create set 1 (speed = 3x), selecting every second frame to generate set 2 (speed = 2x) and taking first 9 frames for set 3 (speed = 1x). The weights from this step are used for initialization of our multi-velocity predictor which described next."
    }, {
      "heading" : "5.3. Multi-velocity predictor",
      "text" : "For training, testing and validation we divide each dataset into 3 parts randomly. We select 50% inputs for training, 30% of dataset for testing and use 20% of dataset for validation. After the dataset was split, we further increased the size of the training dataset by shifting each video along both axes, rotating images and taking their mirror.\nWe train our proposed semi-supervised learner and the multi-velocity semi-supervised learner on the three datasets\n(MMI, CK and Asevo), and compare our results against multiple kernel methods [29] and expression-lets base approaches [28]. We used sources downloaded from Visual Information Processing and Learning Resources [43] as a reference to compare to our methods. Note that we made the same data partitioning scheme (train, validation, test) for all methods to show a fair comparison.\nWe outperform all the methods compared on all the datasets used, by a substantial gap, in almost all cases. We summarize our findings in Table 4, and show confusion matrices per facial expression in Tables 1 and 3. For baseline comparison against other deep neural architectures, we compare our methods against [25] and GoogleNet [44]. We further verified our results against prior state of the art methods discussed in [27] by performing 10 fold cross validation. On MMI we get 66.15 (vs 63.4) % and on CK+ we get 94.18 (vs 92.4) %, making our method state of the art for face expression recognition."
    }, {
      "heading" : "6. Discussion and Future Work",
      "text" : "This paper presents a learning strategy for large datasets with a dramatically lower number of labeled points, in addition to new layers carefully designed to improve recognition in multi-velocity setup. We currently trim the videos to the facial window using Viola and Jones face detection, and focus solely on frontal views. Recognition in-the-wild still remains a challenge with a known low success rate. We believe that given a large and rich dataset this problem would be feasible to solve in our system, and we plan to explore that in the future.\nWe introduced a new layer, which adaptively resamples the videos, achieving a multi-velocity invariant learning procedure. Inserting invariants into a learning process is a research direction that we must push forward. Today training of deep neural network is still time consuming, where huge clusters are being heavily used on reasonably\nlarge datasets. We are already reaching the time-space limit of this process, and better/smarter approaches need to be considered for advancement. Our multi-velocity setup is one approach for reducing the need for data in multiple velocities, while other invariants should be explored in future work."
    }, {
      "heading" : "7. Conclusions",
      "text" : "In this paper we introduced a new topology and learning protocol for semi-supervised convolutional neural networks on video sequences. We further developed a multi-velocity layer based on temporal resampling which was tuned as part of the learning procedure on an enormous collected facial dataset. We report state-of-the-art results on our own data and on public available datasets."
    }, {
      "heading" : "Appendix A: Equations for B spline Interpolation",
      "text" : "Let {xk, f(xk)}Nk=0 be N + 1 observations of a function f . Cubic spline is defined as a set of polynomials Sn(x) N−1 n=0 with coefficients pn,i which approximate f as follows\nS(x) = Sn(x) = pn,0 + pn,1(xn − xk) + pn,2(xn − xk)2 + pn,3(xn − xk)3, (4)\nwhere xk < xn < xk+1. We need at least 4N constraints to recover pn,i uniquely. We can generate 4N − 2 constraints by fixing the values of polynomials at the boundaries and assuming first and second derivatives of adjacent polynomials coincide at the boundaries as well. We add additional constraints assuming that the curve is natural [8] and has zero derivative at boundaries. The coefficients pk,n are constrained by:\nS(x) = f(xk) ∀k ∈ {0..N} (5a) Sk(x)− Sk+1(xk+1) = 0 ∀k ∈ {0..N − 2} (5b) S′k(x)− S′k+1(xk+1) = 0 ∀k ∈ {0..N − 2} (5c) S′′k(x)− S′′k+1(xk+1) = 0 ∀k ∈ {0..N − 2} (5d) S′′0(x) = S ′′ N−1(x) = 0 (5e)\nLet A denote matrix representing the constraints on spline polynomial coefficients and p̄ represent coefficients as described in equation 5. Let ȳ be the vector of function values f(xk), k ∈ {0..N} known to us. Right side of 5 can be written as a product of matrix T with vector ȳ, where T is a binary matrix. Then Ap̄ = Tȳ or p̄ = A−1Tȳ."
    } ],
    "references" : [ {
      "title" : "A neural network based facial expression recognition using fisherface",
      "author" : [ "Z. Abidin", "A. Harjoko" ],
      "venue" : "International Journal of Computer Applications,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2012
    }, {
      "title" : "Exploiting models of personality and emotions to control the behavior of animated interactive agents",
      "author" : [ "E. André", "M. Klesen", "P. Gebhard", "S. Allen", "T. Rist" ],
      "venue" : "In Workshop on Achieving Human-Like Behavior in Interactive Animated Agents,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2000
    }, {
      "title" : "Incremental face alignment in the wild",
      "author" : [ "A. Asthana", "S. Zafeiriou", "S. Cheng", "M. Pantic" ],
      "venue" : "In Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "Convex neural networks",
      "author" : [ "Y. Bengio", "N.L. Roux", "P. Vincent", "O. Delalleau", "P. Marcotte" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2005
    }, {
      "title" : "On contrastive divergence learning",
      "author" : [ "M.A. Carreira-Perpinan", "G.E. Hinton" ],
      "venue" : "In Proceedings of International Workshop on Artificial Intelligence and Statistics,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2005
    }, {
      "title" : "Interactive agents for multimodal emotional user interaction",
      "author" : [ "E. Cerezo", "S. Baldassarri", "F. Seron" ],
      "venue" : "Multi Conferences on Computer Science and Information Systems,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2007
    }, {
      "title" : "3d modelbased continuous emotion recognition",
      "author" : [ "H. Chen", "J. Li", "F. Zhang", "Y. Li", "H. Wang" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2015
    }, {
      "title" : "On spline finite element method",
      "author" : [ "S. Chung-Tze" ],
      "venue" : "Mathematica Numerica Sinica, (MNS),",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1979
    }, {
      "title" : "Emotion recognition in human-computer interaction",
      "author" : [ "R. Cowie", "E. Douglas-Cowie", "N. Tsapatsoulis", "G. Votsis", "S. Kollias", "W. Fellenz", "J.G. Taylor" ],
      "venue" : "Signal Processing Magazine, IEEE,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2001
    }, {
      "title" : "Emotion recognition using PHOG and LPQ features",
      "author" : [ "A. Dhall", "A. Asthana", "R. Goecke", "T. Gedeon" ],
      "venue" : "In International Conference on Automatic Face & Gesture Recognition,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2011
    }, {
      "title" : "Radar: A personal assistant that learns to reduce email overload",
      "author" : [ "M. Freed", "J.G. Carbonell", "G.J. Gordon", "J. Hayes", "B.A. Myers", "D.P. Siewiorek", "S.F. Smith", "A. Steinfeld", "A. Tomasic" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2008
    }, {
      "title" : "Role of facial expressions in social interactions",
      "author" : [ "C. Frith" ],
      "venue" : "Philosophical Transactions of the Royal Society B: Biological Sciences,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2009
    }, {
      "title" : "Facial expression recognition using artificial neural networks",
      "author" : [ "M. Gargesha", "P. Kuchi" ],
      "venue" : "Artificial Neural Computer Systems,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2002
    }, {
      "title" : "Multimodal affective dimension prediction using deep bidirectional long short-term memory recurrent neural networks",
      "author" : [ "L. He", "D. Jiang", "L. Yang", "E. Pei", "P. Wu", "H. Sahli" ],
      "venue" : "In Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2015
    }, {
      "title" : "A fast learning algorithm for deep belief nets",
      "author" : [ "G.E. Hinton", "S. Osindero", "Y.-W. Teh" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2006
    }, {
      "title" : "Cubic splines for image interpolation and digital filtering",
      "author" : [ "H.S. Hou", "H. Andrews" ],
      "venue" : "Transactions on Acoustics, Speech and Signal Processing,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1978
    }, {
      "title" : "Caffe: Convolutional architecture for fast feature embedding",
      "author" : [ "Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell" ],
      "venue" : "arXiv preprint arXiv:1408.5093,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2014
    }, {
      "title" : "Joint fine-tuning in deep neural networks for facial expression recognition",
      "author" : [ "H. Jung", "S. Lee", "J. Yim", "S. Park", "J. Kim" ],
      "venue" : "In Proceedings of the IEEE International Conference on Computer Vision,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2015
    }, {
      "title" : "Emonets: Multimodal deep learning approaches for emotion recognition in video",
      "author" : [ "S.E. Kahou", "X. Bouthillier", "P. Lamblin", "C. Gulcehre", "V. Michalski", "K. Konda", "S. Jean", "P. Froumenty", "Y. Dauphin", "N. Boulanger-Lewandowski" ],
      "venue" : "Journal on Multimodal User Interfaces,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "Combining modality specific deep neural networks for emotion recognition in video",
      "author" : [ "S.E. Kahou", "C. Pal", "X. Bouthillier", "P. Froumenty", "Ç. Gülçehre", "R. Memisevic", "P. Vincent", "A. Courville", "Y. Bengio", "R.C. Ferrari" ],
      "venue" : "In Proceedings of the 15th ACM on International conference on multimodal interaction,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2013
    }, {
      "title" : "Large-scale video classification with convolutional neural networks",
      "author" : [ "A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung", "R. Sukthankar", "L. Fei-Fei" ],
      "venue" : "In Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2014
    }, {
      "title" : "Semi-supervised learning with deep generative models",
      "author" : [ "D.P. Kingma", "S. Mohamed", "D.J. Rezende", "M. Welling" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2014
    }, {
      "title" : "Computer response to user frustration",
      "author" : [ "T. Klein", "W. Picard" ],
      "venue" : "MIT Media Laboratory Vision and Modelling Group Technical Reports, TR 480,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1999
    }, {
      "title" : "Facial expression recognition in image sequences using geometric deformation features and support vector machines",
      "author" : [ "I. Kotsia", "I. Pitas" ],
      "venue" : "Transactions on Image Processing,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2007
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2012
    }, {
      "title" : "Pseudo-label: The simple and efficient semisupervised learning method for deep neural networks",
      "author" : [ "D.-H. Lee" ],
      "venue" : "In Workshop on Challenges in Representation Learning, ICML,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2013
    }, {
      "title" : "Deeply learning deformable facial action parts model for dynamic expression analysis",
      "author" : [ "M. Liu", "S. Li", "S. Shan", "R. Wang", "X. Chen" ],
      "venue" : "In Computer Vision–ACCV",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2014
    }, {
      "title" : "Learning expressionlets on spatio-temporal manifold for dynamic facial expression recognition",
      "author" : [ "M. Liu", "S. Shan", "R. Wang", "X. Chen" ],
      "venue" : "In Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2014
    }, {
      "title" : "Combining multiple kernel methods on riemannian manifold for emotion recognition in the wild",
      "author" : [ "M. Liu", "R. Wang", "S. Li", "S. Shan", "Z. Huang", "X. Chen" ],
      "venue" : "In Proceedings of the 16th International Conference on Multimodal Interaction,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2014
    }, {
      "title" : "Facial expression recognition via a boosted deep belief network",
      "author" : [ "P. Liu", "S. Han", "Z. Meng", "Y. Tong" ],
      "venue" : "In Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2014
    }, {
      "title" : "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression",
      "author" : [ "P. Lucey", "J.F. Cohn", "T. Kanade", "J. Saragih", "Z. Ambadar", "I. Matthews" ],
      "venue" : "In Computer Vision and Pattern Recognition Workshops (CVPRW),",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2010
    }, {
      "title" : "An approach to environmental psychology",
      "author" : [ "A. Mehrabian", "J.A. Russell" ],
      "venue" : null,
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 1974
    }, {
      "title" : "Real time facial expression recognition in video using support vector machines",
      "author" : [ "P. Michel", "R. El Kaliouby" ],
      "venue" : "In Proceedings of the 5th international conference on Multimodal interfaces,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2003
    }, {
      "title" : "Speech emotion recognition using Hidden Markov Models",
      "author" : [ "T.L. Nwe", "S.W. Foo", "L.C. De Silva" ],
      "venue" : "Speech communication,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2003
    }, {
      "title" : "Webbased database for facial expression analysis",
      "author" : [ "M. Pantic", "M. Valstar", "R. Rademaker", "L. Maat" ],
      "venue" : "In International Conference on Multimedia and Expo, pages 5–pp. IEEE,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2005
    }, {
      "title" : "Using hankel matrices for dynamics-based facial emotion recognition and pain detection",
      "author" : [ "L. Presti", "M. Cascia" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2015
    }, {
      "title" : "On deep generative models with applications to recognition",
      "author" : [ "M.A. Ranzato", "J. Susskind", "V. Mnih", "G. Hinton" ],
      "venue" : "In Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2011
    }, {
      "title" : "Face alignment at 3000 fps via regressing local binary features",
      "author" : [ "S. Ren", "X. Cao", "Y. Wei", "J. Sun" ],
      "venue" : "In Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2014
    }, {
      "title" : "Speech emotion recognition combining acoustic features and linguistic information  in a hybrid support vector machine-belief network architecture",
      "author" : [ "B. Schuller", "G. Rigoll", "M. Lang" ],
      "venue" : "In International Conference on Acoustics, Speech, and Signal Processing,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2004
    }, {
      "title" : "Facial action unit detection using active learning and an efficient non-linear kernel approximation",
      "author" : [ "T. Senechal", "D. McDuff", "R. Kaliouby" ],
      "venue" : "In Proceedings of the IEEE International Conference on Computer Vision Workshops,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2015
    }, {
      "title" : "Robust facial expression recognition using local binary patterns",
      "author" : [ "C. Shan", "S. Gong", "P.W. McOwan" ],
      "venue" : "In International Conference on Image Processing,",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2005
    }, {
      "title" : "Application design for wearable computing",
      "author" : [ "D. Siewiorek", "A. Smailagic", "T. Starner" ],
      "venue" : "Synthesis Lectures on Mobile and Pervasive Computing,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2008
    }, {
      "title" : "Going deeper with convolutions",
      "author" : [ "C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 2015
    }, {
      "title" : "Deepface: Closing the gap to human-level performance in face verification",
      "author" : [ "Y. Taigman", "M. Yang", "M. Ranzato", "L. Wolf" ],
      "venue" : "In Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 2014
    }, {
      "title" : "Learning spatiotemporal features with 3d convolutional networks",
      "author" : [ "D. Tran", "L. Bourdev", "R. Fergus", "L. Torresani", "M. Paluri" ],
      "venue" : "arXiv preprint arXiv:1412.0767,",
      "citeRegEx" : "46",
      "shortCiteRegEx" : "46",
      "year" : 2014
    }, {
      "title" : "Induced disgust, happiness and surprise: an addition to the mmi facial expression database",
      "author" : [ "M. Valstar", "M. Pantic" ],
      "venue" : "In Workshop on EMOTION: Corpora for Research on Emotion and Affect,",
      "citeRegEx" : "47",
      "shortCiteRegEx" : "47",
      "year" : 2010
    }, {
      "title" : "Facial expression recognition under a wide range of head poses",
      "author" : [ "R.-L. Vieriu", "S. Tulyakov", "S. Semeniuta", "E. Sangineto", "N. Sebe" ],
      "venue" : "In Automatic Face and Gesture Recognition (FG),",
      "citeRegEx" : "48",
      "shortCiteRegEx" : "48",
      "year" : 2015
    }, {
      "title" : "Robust real-time face detection",
      "author" : [ "P. Viola", "M.J. Jones" ],
      "venue" : "International Journal of Computer Vision,",
      "citeRegEx" : "49",
      "shortCiteRegEx" : "49",
      "year" : 2004
    }, {
      "title" : "Variable-state latent conditional random fields for facial expression recognition and action unit detection",
      "author" : [ "R. Walecki", "O. Rudovic", "V. Pavlovic", "M. Pantic" ],
      "venue" : "In Automatic Face and Gesture Recognition (FG),",
      "citeRegEx" : "50",
      "shortCiteRegEx" : "50",
      "year" : 2015
    }, {
      "title" : "Capturing complex spatiotemporal relations among facial muscles for facial expression recognition",
      "author" : [ "Z. Wang", "S. Wang", "Q. Ji" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "51",
      "shortCiteRegEx" : "51",
      "year" : 2013
    }, {
      "title" : "Deep learning via semi-supervised embedding",
      "author" : [ "J. Weston", "F. Ratle", "H. Mobahi", "R. Collobert" ],
      "venue" : "In Neural Networks: Tricks of the Trade,",
      "citeRegEx" : "52",
      "shortCiteRegEx" : "52",
      "year" : 2012
    }, {
      "title" : "A survey on media interaction in social robotics",
      "author" : [ "L. Yang", "H. Cheng", "J. Hao", "Y. Ji", "Y. Kuang" ],
      "venue" : "In Advances in  Multimedia Information Processing (PCM),",
      "citeRegEx" : "53",
      "shortCiteRegEx" : "53",
      "year" : 2015
    }, {
      "title" : "Joint patch and multi-label learning for facial action unit detection",
      "author" : [ "K. Zhao", "W.-S. Chu", "F. De la Torre", "J.F. Cohn", "H. Zhang" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "54",
      "shortCiteRegEx" : "54",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 31,
      "context" : "Our body language, voice pitch, intonation and volume, movement of our pupils or our chronemics choices are just a few examples emphasizing the richness of human communication skills [32].",
      "startOffset" : 183,
      "endOffset" : 187
    }, {
      "referenceID" : 11,
      "context" : "Their perception initiates rapid cognitive processes in the brain and have both communicative and reflexive components [12].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 41,
      "context" : "Advances in speech and natural language processing have presented us with personalized smart agents [42, 11], while examining our facial gestures has provided a richer set of tools for improving human computer interaction [9].",
      "startOffset" : 100,
      "endOffset" : 108
    }, {
      "referenceID" : 10,
      "context" : "Advances in speech and natural language processing have presented us with personalized smart agents [42, 11], while examining our facial gestures has provided a richer set of tools for improving human computer interaction [9].",
      "startOffset" : 100,
      "endOffset" : 108
    }, {
      "referenceID" : 8,
      "context" : "Advances in speech and natural language processing have presented us with personalized smart agents [42, 11], while examining our facial gestures has provided a richer set of tools for improving human computer interaction [9].",
      "startOffset" : 222,
      "endOffset" : 225
    }, {
      "referenceID" : 22,
      "context" : "In the last two decades we have seen several attempts to automate the way computers respond towards human emotions [23, 6, 2], where the ultimate goal is to create humanoid robots which can blend in the environment [53].",
      "startOffset" : 115,
      "endOffset" : 125
    }, {
      "referenceID" : 5,
      "context" : "In the last two decades we have seen several attempts to automate the way computers respond towards human emotions [23, 6, 2], where the ultimate goal is to create humanoid robots which can blend in the environment [53].",
      "startOffset" : 115,
      "endOffset" : 125
    }, {
      "referenceID" : 1,
      "context" : "In the last two decades we have seen several attempts to automate the way computers respond towards human emotions [23, 6, 2], where the ultimate goal is to create humanoid robots which can blend in the environment [53].",
      "startOffset" : 115,
      "endOffset" : 125
    }, {
      "referenceID" : 51,
      "context" : "In the last two decades we have seen several attempts to automate the way computers respond towards human emotions [23, 6, 2], where the ultimate goal is to create humanoid robots which can blend in the environment [53].",
      "startOffset" : 215,
      "endOffset" : 219
    }, {
      "referenceID" : 14,
      "context" : "Recent advances in machine learning have shown that if we provide a neural network with enough samples, it can learn very complex structures [15].",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 36,
      "context" : "Today hard tasks in computer vision, such as labeling images, recognizing objects and faces or classifying videos have become a feasible task for computers which can now provide competitive results to humans and sometimes even outperform them [37, 45].",
      "startOffset" : 243,
      "endOffset" : 251
    }, {
      "referenceID" : 43,
      "context" : "Today hard tasks in computer vision, such as labeling images, recognizing objects and faces or classifying videos have become a feasible task for computers which can now provide competitive results to humans and sometimes even outperform them [37, 45].",
      "startOffset" : 243,
      "endOffset" : 251
    }, {
      "referenceID" : 23,
      "context" : "Machine learning techniques such as Support Vector Machines have been used for facial expression recognition given the movement of facial fiducial points [24, 33, 41, 10] achieving real time performance [38].",
      "startOffset" : 154,
      "endOffset" : 170
    }, {
      "referenceID" : 32,
      "context" : "Machine learning techniques such as Support Vector Machines have been used for facial expression recognition given the movement of facial fiducial points [24, 33, 41, 10] achieving real time performance [38].",
      "startOffset" : 154,
      "endOffset" : 170
    }, {
      "referenceID" : 40,
      "context" : "Machine learning techniques such as Support Vector Machines have been used for facial expression recognition given the movement of facial fiducial points [24, 33, 41, 10] achieving real time performance [38].",
      "startOffset" : 154,
      "endOffset" : 170
    }, {
      "referenceID" : 9,
      "context" : "Machine learning techniques such as Support Vector Machines have been used for facial expression recognition given the movement of facial fiducial points [24, 33, 41, 10] achieving real time performance [38].",
      "startOffset" : 154,
      "endOffset" : 170
    }, {
      "referenceID" : 37,
      "context" : "Machine learning techniques such as Support Vector Machines have been used for facial expression recognition given the movement of facial fiducial points [24, 33, 41, 10] achieving real time performance [38].",
      "startOffset" : 203,
      "endOffset" : 207
    }, {
      "referenceID" : 6,
      "context" : "Other interesting approaches [7, 50, 36, 48] we should mention are based on temporal features [28, 51], and multiple kernels [29], action units [54, 40], as well as emotion recognition from speech [34, 39].",
      "startOffset" : 29,
      "endOffset" : 44
    }, {
      "referenceID" : 48,
      "context" : "Other interesting approaches [7, 50, 36, 48] we should mention are based on temporal features [28, 51], and multiple kernels [29], action units [54, 40], as well as emotion recognition from speech [34, 39].",
      "startOffset" : 29,
      "endOffset" : 44
    }, {
      "referenceID" : 35,
      "context" : "Other interesting approaches [7, 50, 36, 48] we should mention are based on temporal features [28, 51], and multiple kernels [29], action units [54, 40], as well as emotion recognition from speech [34, 39].",
      "startOffset" : 29,
      "endOffset" : 44
    }, {
      "referenceID" : 46,
      "context" : "Other interesting approaches [7, 50, 36, 48] we should mention are based on temporal features [28, 51], and multiple kernels [29], action units [54, 40], as well as emotion recognition from speech [34, 39].",
      "startOffset" : 29,
      "endOffset" : 44
    }, {
      "referenceID" : 27,
      "context" : "Other interesting approaches [7, 50, 36, 48] we should mention are based on temporal features [28, 51], and multiple kernels [29], action units [54, 40], as well as emotion recognition from speech [34, 39].",
      "startOffset" : 94,
      "endOffset" : 102
    }, {
      "referenceID" : 49,
      "context" : "Other interesting approaches [7, 50, 36, 48] we should mention are based on temporal features [28, 51], and multiple kernels [29], action units [54, 40], as well as emotion recognition from speech [34, 39].",
      "startOffset" : 94,
      "endOffset" : 102
    }, {
      "referenceID" : 28,
      "context" : "Other interesting approaches [7, 50, 36, 48] we should mention are based on temporal features [28, 51], and multiple kernels [29], action units [54, 40], as well as emotion recognition from speech [34, 39].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 52,
      "context" : "Other interesting approaches [7, 50, 36, 48] we should mention are based on temporal features [28, 51], and multiple kernels [29], action units [54, 40], as well as emotion recognition from speech [34, 39].",
      "startOffset" : 144,
      "endOffset" : 152
    }, {
      "referenceID" : 39,
      "context" : "Other interesting approaches [7, 50, 36, 48] we should mention are based on temporal features [28, 51], and multiple kernels [29], action units [54, 40], as well as emotion recognition from speech [34, 39].",
      "startOffset" : 144,
      "endOffset" : 152
    }, {
      "referenceID" : 33,
      "context" : "Other interesting approaches [7, 50, 36, 48] we should mention are based on temporal features [28, 51], and multiple kernels [29], action units [54, 40], as well as emotion recognition from speech [34, 39].",
      "startOffset" : 197,
      "endOffset" : 205
    }, {
      "referenceID" : 38,
      "context" : "Other interesting approaches [7, 50, 36, 48] we should mention are based on temporal features [28, 51], and multiple kernels [29], action units [54, 40], as well as emotion recognition from speech [34, 39].",
      "startOffset" : 197,
      "endOffset" : 205
    }, {
      "referenceID" : 24,
      "context" : "One of the most interesting results was presented three years back on a large scale dataset (LSVRC 2011), where a deep convolutional net outperformed all other methods by far [25].",
      "startOffset" : 175,
      "endOffset" : 179
    }, {
      "referenceID" : 20,
      "context" : "With advances in convolutional neural nets, we have seen neural nets applied to video classification [21, 46] and even facial expression recognition [1, 13] but these networks were not deep enough or used other feature extraction techniques like PCA or Fisherface.",
      "startOffset" : 101,
      "endOffset" : 109
    }, {
      "referenceID" : 44,
      "context" : "With advances in convolutional neural nets, we have seen neural nets applied to video classification [21, 46] and even facial expression recognition [1, 13] but these networks were not deep enough or used other feature extraction techniques like PCA or Fisherface.",
      "startOffset" : 101,
      "endOffset" : 109
    }, {
      "referenceID" : 0,
      "context" : "With advances in convolutional neural nets, we have seen neural nets applied to video classification [21, 46] and even facial expression recognition [1, 13] but these networks were not deep enough or used other feature extraction techniques like PCA or Fisherface.",
      "startOffset" : 149,
      "endOffset" : 156
    }, {
      "referenceID" : 12,
      "context" : "With advances in convolutional neural nets, we have seen neural nets applied to video classification [21, 46] and even facial expression recognition [1, 13] but these networks were not deep enough or used other feature extraction techniques like PCA or Fisherface.",
      "startOffset" : 149,
      "endOffset" : 156
    }, {
      "referenceID" : 25,
      "context" : "In [26] the authors pre-trained the system using pseudo labels, while in [52, 22] they embedded the data in a low dimensional space.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 50,
      "context" : "In [26] the authors pre-trained the system using pseudo labels, while in [52, 22] they embedded the data in a low dimensional space.",
      "startOffset" : 73,
      "endOffset" : 81
    }, {
      "referenceID" : 21,
      "context" : "In [26] the authors pre-trained the system using pseudo labels, while in [52, 22] they embedded the data in a low dimensional space.",
      "startOffset" : 73,
      "endOffset" : 81
    }, {
      "referenceID" : 29,
      "context" : "Very recently superior results have been shown [30, 20, 18, 14, 19] using deep neural nets to combine labels and un-labeled data in the same package.",
      "startOffset" : 47,
      "endOffset" : 67
    }, {
      "referenceID" : 19,
      "context" : "Very recently superior results have been shown [30, 20, 18, 14, 19] using deep neural nets to combine labels and un-labeled data in the same package.",
      "startOffset" : 47,
      "endOffset" : 67
    }, {
      "referenceID" : 17,
      "context" : "Very recently superior results have been shown [30, 20, 18, 14, 19] using deep neural nets to combine labels and un-labeled data in the same package.",
      "startOffset" : 47,
      "endOffset" : 67
    }, {
      "referenceID" : 13,
      "context" : "Very recently superior results have been shown [30, 20, 18, 14, 19] using deep neural nets to combine labels and un-labeled data in the same package.",
      "startOffset" : 47,
      "endOffset" : 67
    }, {
      "referenceID" : 18,
      "context" : "Very recently superior results have been shown [30, 20, 18, 14, 19] using deep neural nets to combine labels and un-labeled data in the same package.",
      "startOffset" : 47,
      "endOffset" : 67
    }, {
      "referenceID" : 47,
      "context" : "While generating the data, we use Viola and Jones face detection [49] for cropping the faces.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 24,
      "context" : "This network is similar to Imagenet [25] but accepts inputs of size 145 × 145 × 9 as an input.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 20,
      "context" : "lution layers in time and use slow fusion model [21] which slowly combines temporal information in successive layers.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 15,
      "context" : "Piece-wise cubic b-spline interpolation is preferred over polynomial techniques as it can minimize interpolation error for fewer points and lower degree polynomials [16].",
      "startOffset" : 165,
      "endOffset" : 169
    }, {
      "referenceID" : 24,
      "context" : "The classifier neural net is inspired by Imagenet [25], with additional Figure 5.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 14,
      "context" : "This is one of the traditional approaches used to train autoencoders [15, 5].",
      "startOffset" : 69,
      "endOffset" : 76
    }, {
      "referenceID" : 4,
      "context" : "This is one of the traditional approaches used to train autoencoders [15, 5].",
      "startOffset" : 69,
      "endOffset" : 76
    }, {
      "referenceID" : 27,
      "context" : "Confusion matrices over test results for Cohn Kanade dataset using our methods and best performing external method which uses Expressionlets [28].",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 3,
      "context" : "max loss [4].",
      "startOffset" : 9,
      "endOffset" : 12
    }, {
      "referenceID" : 2,
      "context" : "Next, we localized landmarks for each frame using a deformable model for the face [3] and detected the facial pose by fitting a 3D model to the landmarks.",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 30,
      "context" : "The Cohn Kanade Dataset [31] is one of the most popular datasets used for facial expression recognition.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 28,
      "context" : "Confusion matrix over test results for Asevo dataset using the proposed multi-velocity semi-supervised learner (left) and external competing method using Covariance Riemann kernel [29].",
      "startOffset" : 180,
      "endOffset" : 184
    }, {
      "referenceID" : 34,
      "context" : "MMI facial expression dataset [35] is an ongoing effort for representing both posed and non posed facial expressions.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 45,
      "context" : "MMI originally contained only posed facial expressions and recently was extended to contain induced happiness, disgust and surprise [47].",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 16,
      "context" : "We use caffe [17] to train the system.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 4,
      "context" : "We minimize contrastive divergence [5] to train autoencoder layers successively.",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 28,
      "context" : "(MMI, CK and Asevo), and compare our results against multiple kernel methods [29] and expression-lets base approaches [28].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 27,
      "context" : "(MMI, CK and Asevo), and compare our results against multiple kernel methods [29] and expression-lets base approaches [28].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 24,
      "context" : "For baseline comparison against other deep neural architectures, we compare our methods against [25] and GoogleNet [44].",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 42,
      "context" : "For baseline comparison against other deep neural architectures, we compare our methods against [25] and GoogleNet [44].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 26,
      "context" : "We further verified our results against prior state of the art methods discussed in [27] by performing 10 fold cross validation.",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 7,
      "context" : "We add additional constraints assuming that the curve is natural [8] and has zero derivative at boundaries.",
      "startOffset" : 65,
      "endOffset" : 68
    } ],
    "year" : 2016,
    "abstractText" : "We present a new action recognition deep neural network which adaptively learns the best action velocities in addition to the classification. While deep neural networks have reached maturity for image understanding tasks, we are still exploring network topologies and features to handle the richer environment of video clips. Here, we tackle the problem of multiple velocities in action recognition, and provide state-of-the-art results for gesture recognition, on known and new collected datasets. We further provide the training steps for our semi-supervised network, suited to learn from huge unlabeled datasets with only a fraction of labeled examples.",
    "creator" : "LaTeX with hyperref package"
  }
}