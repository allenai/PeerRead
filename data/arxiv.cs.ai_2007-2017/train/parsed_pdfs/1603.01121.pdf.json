{
  "name" : "1603.01121.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Deep Reinforcement Learning from Self-Play in Imperfect-Information Games",
    "authors" : [ "Johannes Heinrich", "David Silver" ],
    "emails" : [ "J.HEINRICH@CS.UCL.AC.UK", "D.SILVER@CS.UCL.AC.UK" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Games have a tradition of encouraging advances in artificial intelligence and machine learning (Samuel, 1959; Tesauro, 1995; Campbell et al., 2002; Riedmiller et al., 2009; Gelly et al., 2012; Bowling et al., 2015). Game theory defines a game as a domain of conflict or cooperation between several entities (Myerson, 1991). One motivation of studying the simpler recreational games is to develop algorithms that will scale to more complex, realworld games such as airport and network security, financial and energy trading, traffic control and routing (Lambert III et al., 2005; Nevmyvaka et al., 2006; Bazzan, 2009; Tambe, 2011; Urieli & Stone, 2014; Durkota et al., 2015). Most of these real-world games involve decision making with imperfect information and high-dimensional information state spaces. Unfortunately, many machine learning methods, that have been applied to classical games, lack convergence guarantees for learning in imperfect-information games. On the other hand, many game-theoretic approaches lack the ability to extract relevant patterns and generalise from data. This results in limited scalability to large games, unless the domain is abstracted to a manageable size using\nhuman expert knowledge, heuristics or modelling. However, acquiring human expertise often requires expensive resources and time. In addition, humans can be easily fooled into irrational decisions or assumptions (Selten, 1990; Ariely & Jones, 2008). This motivates algorithms that learn useful strategies end-to-end.\nIn this paper we introduce NFSP, a deep reinforcement learning method for learning approximate Nash equilibria of imperfect-information games. NFSP agents learn by playing against themselves without explicit prior knowledge. Technically, NFSP extends and instantiates Fictitious Self-Play (FSP) (Heinrich et al., 2015) with neural network function approximation. An NFSP agent consists of two neural networks and two kinds of memory. Memorized experience of play against fellow agents is used by reinforcement learning to train a network that predicts the expected values of actions. Experience of the agent’s own behaviour is stored in a separate memory, which is used by supervised learning to train a network that predicts the agent’s own average behaviour. An NFSP agent acts cautiously by sampling its actions from a mixture of its average, routine strategy and its greedy strategy that maximizes its predicted expected value. NFSP approximates fictitious play, which is a popular game-theoretic model of learning in games that converges to Nash equilibria in some classes of games, e.g. two-player zero-sum and many-player potential games.\nWe empirically evaluate our method in two-player zerosum computer poker games. In this domain, current gametheoretic approaches use heuristics of card strength to abstract the game to a tractable size (Zinkevich et al., 2007; Gilpin et al., 2007; Johanson et al., 2013). While Limit Texas Hold’em (LHE), a poker game of real-world scale, has got within reach of being solved with current computational resources (Bowling et al., 2015), most other poker and real-world games remain far out of scope without abstraction. Our approach does not rely on engineering such abstractions or any other prior knowledge. NFSP agents leverage deep reinforcement learning to learn directly from their experience of interacting in the game. When applied to Leduc poker, NFSP approached a Nash equilibrium, whereas common reinforcement learning methods diar X\niv :1\n60 3.\n01 12\n1v 1\n[ cs\n.L G\n] 3\nM ar\n2 01\n6\nverged. We also applied NFSP to LHE, learning directly from the raw inputs. NFSP learnt a competitive strategy, approaching the performance of state-of-the-art methods based on handcrafted abstractions."
    }, {
      "heading" : "2. Background",
      "text" : "In this section we provide a brief overview of reinforcement learning, extensive-form games and fictitious self-play. For a more detailed exposition we refer the reader to (Sutton & Barto, 1998), (Myerson, 1991), (Fudenberg, 1998) and (Heinrich et al., 2015)."
    }, {
      "heading" : "2.1. Reinforcement Learning",
      "text" : "Reinforcement learning (Sutton & Barto, 1998) agents typically learn to maximize their expected future rewards from interaction with an environment. The environment is usually modelled as a Markov decision process (MDP). An agent behaves according to a policy that specifies a distribution over available actions at each state of the MDP. The agent’s goal is to improve its policy in order to maximize its gain, Gt = ∑T i=tRi+1, which is a random variable of the agent’s cumulative future rewards starting from time t.\nMany reinforcement learning algorithms learn from sequential experience in the form of transition tuples, (st, at, rt+1, st+1), where st is the state at time t, at is the action chosen in that state, rt+1 the reward received thereafter and st+1 the next state that the agent transitioned to. A common objective is to learn the action-value function, Q(s, a) = Eπ [Gt |St = s,At = a], defined as the expected gain of taking action a in state s and following policy π thereafter. An agent is learning on-policy if it learns about the policy that it is currently following. In the off-policy setting an agent learns from experience of another agent or another policy, e.g. a previous policy.\nQ-learning (Watkins & Dayan, 1992) is a popular offpolicy reinforcement learning method. It learns about the greedy policy, which at each state takes the action of the highest estimated value. Storing and replaying past experience by applying off-policy reinforcement learning to the respective transition tuples is known as experience replay (Lin, 1992). Fitted Q Iteration (FQI) (Ernst et al., 2005) is a batch reinforcement learning method that replays experience with Q-learning. Neural Fitted Q Iteration (NFQ) (Riedmiller, 2005) and Deep Q Network (DQN) (Mnih et al., 2015) are extensions of FQI that use neural network function approximation with batch and online updates respectively."
    }, {
      "heading" : "2.2. Extensive-Form Games",
      "text" : "Extensive-form games are a model of sequential interaction involving multiple players. Assuming rationality, each\nplayer’s goal is to maximize his payoff in the game. In imperfect-information games, each player only observes his respective information states, e.g. in a poker game a player only knows his own private cards but not those of other players. Each player chooses a behavioural strategy that maps information states to probability distributions over available actions. We assume games with perfect recall, i.e. each player’s current information state sit implies knowledge of the sequence of his information states and actions, si1, a i 1, s i 2, a i 2, ..., s i t, that led to this information state. The realization-probability (Von Stengel, 1996), xπi(sit) = ∏t−1 k=1 π i(sik, a i k), determines the probability that player i’s behavioural strategy, πi, contributes to realizing his information state sit. A strategy profile π = (π1, ... , πn) is a collection of strategies for all players. π−i refers to all strategies in π except πi. Given a fixed strategy profile π−i, any strategy of player i that achieves optimal payoff performance against π−i is a best response. An approximate or -best response is suboptimal by no more than . A Nash equilibrium is a strategy profile such that each player’s strategy in this profile is a best response to the other strategies. Similarly, an approximate or -Nash equilibrium is a profile of -best responses. In a Nash equilibrium no player can gain by deviating from his strategy. Therefore, a Nash equilibrium can be regarded as a fixed point of rational self-play learning. In fact, Nash equilibria are the only strategy profiles that rational agents can hope to converge on in self-play (Bowling & Veloso, 2001)."
    }, {
      "heading" : "2.3. Fictitious Self-Play",
      "text" : "Fictitious play (Brown, 1951) is a game-theoretic model of learning from self-play. Fictitious players choose best responses to their opponents’ average behaviour. The average strategies of fictitious players converge to Nash equilibria in certain classes of games, e.g. two-player zero-sum and many-player potential games (Robinson, 1951; Monderer & Shapley, 1996). Leslie & Collins (2006) introduced generalised weakened fictitious play. It has similar convergence guarantees as common fictitious play, but allows for approximate best responses and perturbed average strategy updates, making it particularly suitable for machine learning.\nFictitious play is commonly defined in normal form, which is exponentially less efficient for extensive-form games. Heinrich et al. (2015) introduce Full-Width ExtensiveForm Fictitious Play (XFP) that enables fictitious players to update their strategies in behavioural, extensive form, resulting in linear time and space complexity. A key insight is that for a convex combination of normal-form strategies, σ̂ = λ1π̂1 + λ2π̂2, we can achieve a realizationequivalent behavioural strategy σ, by setting it to be proportional to the respective convex combination of realization-\nprobabilities,\nσ(s, a) ∝ λ1xπ1(s)π1(s, a)+λ2xπ2(s)π2(s, a)∀s, a, (1)\nwhere λ1xπ1(s)+λ2xπ2(s) is the normalizing constant for the strategy at information state s. In addition to defining a full-width average strategy update of fictitious players in behavioural strategies, equation (1) prescribes a way to sample data sets of such convex combinations of strategies. Heinrich et al. (2015) introduce Fictitious Self-Play (FSP), a sample- and machine learning-based class of algorithms that approximate XFP. FSP replaces the best response computation and the average strategy updates with reinforcement and supervised learning respectively. In particular, FSP agents generate datasets of their experience in self-play. Each agent stores its experienced transition tuples, (st, at, rt+1, st+1), in a memory, MRL, designated for reinforcement learning. Experience of the agent’s own behaviour, (st, at), is stored in a separate memory,MSL, designated for supervised learning. Self-play sampling is set up in a way that an agent’s reinforcement learning memory approximates data of an MDP defined by the other players’ average strategy profile. Thus, an approximate solution of the MDP by reinforcement learning yields an approximate best response. Similarly, an agent’s supervised learning memory approximates data of the agent’s own average strategy, which can be learned by supervised classification."
    }, {
      "heading" : "3. Neural Fictitious Self-Play",
      "text" : "NFSP is an evolution of FSP, introducing multiple extensions such as neural network function approximation, reservoir sampling, anticipatory dynamics and a fully agent-based approach. An NFSP agent interacts with the other players in a game and memorizes its experience of game transitions and its own behaviour. NFSP treats these memories as two datasets suitable for deep reinforcement learning and supervised classification. In particular, the agent trains a neural network, FQ, to predict action values, Q(s, a), from the data inMRL using off-policy reinforcement learning. The resulting network defines the agent’s approximate best response strategy, β = -greedy(FQ), which selects a random action with probability and otherwise chooses the action that maximizes the predicted action values. The NFSP agent trains a separate neural network, FS , to imitate its own past behaviour using supervised classification on the data inMSL. This network maps states to action probabilities and defines the agent’s average strategy, π = FS . During play, the agent chooses its actions from a mixture of its two strategies, β and π.\nWhile fictitious players usually best respond to the average strategy of their opponents, in continuous-time dynamic fictitious play (Shamma & Arslan, 2005) players choose\nAlgorithm 1 Neural Fictitious Self-Play (NFSP) with DQN Require:\nΓ {Game} MRL,MSL {RL and SL memories} FQ, FS {Action value and policy networks} β = -GREEDY(FQ) {Best response policy} π = FS {Average policy} σ {Current policy}\nEnsure: π an approximate Nash equilibrium in self-play function STEP() st, rt, ct ← OBSERVE(Γ) at ← THINK(st, rt, ct) ACT(Γ, at)\nend function function THINK(st, rt, ct)\nif ct = 0 {episode terminated} then σ ← SAMPLEPOLICY(β, π) end if if st−1 6= nil then τt ← (st−1, at−1, rt, st, ct) UPDATERLMEMORY(MRL, τt) end if at ← SAMPLEACTION(σ) if σ = β then\nUPDATESLMEMORY(MSL, (st, at)) end if st−1 ← st at−1 ← at β ← REINFORCEMENTLEARNING(MRL) π ← SUPERVISEDLEARNING(MSL)\nend function function REINFORCEMENTLEARNINIG(MRL) FQ ← DQN(MRL) return -GREEDY(FQ) end function function SUPERVISEDLEARNING(MSL) FS ← Apply stochastic gradient descent to loss\nE(s,a)∼MSL [− log π(s, a)] return FS\nend function\nbest responses to a short-term prediction of their opponents’ average normal-form strategies, π̂−it + η d dt π̂ −i t . The authors show that for appropriate, game-dependent choice of η stability of fictitious play at equilibrium points can be improved. NFSP uses β̂it+1 − π̂it ≈ ddt π̂ i t as a discretetime approximation of the derivative that is used in these anticipatory dynamics. Note that ∆π̂it ∝ β̂it+1 − π̂it is the normal-form update direction of common discrete-time fictitious play. In order for an NFSP agent to compute an approximate best response, βi, to its opponents’ anticipated average strategy profile, σ−i ≡ π̂−i + η(β̂−i − π̂−i), the\nagent iteratively evaluates and maximizes its action values, Qi(s, a) ≈ Eβi,σ−i [ Git ∣∣St = s,At = a]. This can be achieved by off-policy reinforcement learning, e.g. Qlearning or DQN, on experience of play against the opponents’ anticipated strategy, σ−i. To ensure that the agents’ reinforcement learning memories,MRL, contain this kind of experience, NFSP requires all agents to choose their actions from σ ≡ (1 − η)π̂ + ηβ̂, where η ∈ R is called the anticipatory parameter.\nFictitious play usually keeps track of the average of normal-form best response strategies that players have chosen in the game, π̂iT = 1 T ∑T t=1 β̂ i t . Heinrich et al. (2015) propose to use sampling and machine learning to generate data on and learn convex combinations of normal-form strategies in extensive form. E.g. we can generate a set of extensive-form data of πiT by sampling whole episodes of the game, using βit , t = 1, ... , T , in proportion to their weight, 1T , in the convex combination. NFSP uses reservoir sampling (Vitter, 1985; Osborne et al., 2014) to memorize experience of its average best responses. The agent’s supervised learning memory,MSL, is a reservoir to which it only adds experience when following its approximate best response policy β. An NFSP agent regularly trains its average policy network, π = FS , to match its average behaviour stored in its supervised learning memory, e.g. by optimizing the log-probability of past actions taken. Algorithm 1 presents NFSP, using DQN for reinforcement learning."
    }, {
      "heading" : "4. Experiments",
      "text" : "We evaluate NFSP and related algorithms in Leduc (Southey et al., 2005) and Limit Texas Hold’em poker games. Most of our experiments measure the exploitability of learned strategy profiles. In a two-player zero-sum game, the exploitability of a strategy profile is defined as the expected average payoff that a best response profile achieves against it. An exploitability of 2δ yields at least a δ-Nash equilibrium."
    }, {
      "heading" : "4.1. Robustness of XFP",
      "text" : "To understand how function approximation interacts with FSP, we begin with some simple experiments that emulate approximation and sampling errors in the full-width algorithm XFP. Firstly, we explore what happens when the perfect averaging used in XFP is replaced by an incremental averaging process closer to gradient descent. Secondly, we explore what happens when the exact table lookup used in XFP is replaced by an approximation with epsilon error.\nFigure 1 shows the performance of XFP with default, 1/T , and constant stepsizes for its strategy updates. We see improved asymptotic but lower initial performance for smaller\nstepsizes. For constant stepsizes the performance seems to plateau rather than diverge. With reservoir sampling we can achieve an effective stepsize of 1/T . However, the results suggest that exponentially-averaged reservoir sampling can be a viable choice too, as exponential averaging of past memories would approximately correspond to using a constant stepsize.\nXFP with stepsize 1 is equivalent to a full-width iterated best response algorithm. While this algorithm converges to a Nash equilibrium in finite perfect-information two-player zero-sum games, the results suggest that with imperfect information this is not generally the case. The Poker-CNN algorithm introduced by Yakovenko et al. (2016) stores a small number of past strategies which it iteratively computes new strategies against. Replacing strategies in that set is similar to updating an average strategy with a large stepsize. This might lead to similar problems as shown in Figure 1.\nOur NFSP agents add random exploration to their policies and use noisy stochastic gradient updates to learn action values, which determine their approximate best responses. Therefore, we investigated the impact of random noise added to the best response computation, which XFP performs by dynamic programming. At each backward induction step, we pass back a uniform-random action’s value with probability and the best action’s value otherwise. Figure 2 shows monotonically decreasing performance with added noise. However, performance remains stable and keeps improving for all noise levels."
    }, {
      "heading" : "4.2. Convergence of NFSP",
      "text" : "We empirically investigate the convergence of NFSP to Nash equilibria in Leduc Hold’em. We also study whether removing or altering some of NFSP’s components breaks convergence.\nOne of our goals is to minimize reliance on prior knowledge. Therefore, we attempt to define an objective encoding of information states in poker games. Contrary to other work on computer poker (Zinkevich et al., 2007; Gilpin et al., 2007; Johanson et al., 2013), we do not engineer any higher-level features. Poker games usually consist of multiple rounds. At each round new cards are revealed to the players. We represent each rounds’ cards by a k-of-n encoding. E.g. LHE has a card deck of 52 cards and on the second round three cards are revealed. Thus, this round is encoded with a vector of length 52 and three elements set to 1 and the rest to 0. In Limit Hold’em poker games, players usually have three actions to choose from, namely {fold, call, raise}. Note that depending on context, calls and raises can be referred to as checks and bets respectively. Betting is capped at a fixed number of raises per round. Thus, we can represent the betting history as a tensor with 4 dimensions, namely {player, round, number of raises, action taken}. E.g. heads-up LHE contains 2 players, 4 rounds, 0 to 4 raises per round and 3 actions. Thus we can represent a LHE betting history as a 2× 4× 5× 3 tensor. In a heads-up game we do not need to encode the fold action, as a two-player game always ends if one players gives up. Thus, we can flatten the 4-dimensional tensor to a vector of length 80. Concatenating with the card inputs of 4 rounds, we encode an information state of LHE as a vector of length 288. Similarly, an information state of Leduc Hold’em can be encoded as a vector of length 30, as it contains 6 cards with 3 duplicates, 2 rounds, 0 to 2 raises per round and 3 actions.\nFor learning in Leduc Hold’em, we manually calibrated NFSP for a fully connected neural network with 1 hidden layer of 64 neurons and rectified linear activations. We then repeated the experiment for various network architectures with the same parameters. In particular, we set the sizes\nof memories to 200k and 2m forMRL andMSL respectively.MRL functioned as a circular buffer containing a recent window of experience. MSL was updated with reservoir sampling (Vitter, 1985). The reinforcement and supervised learning rates were set to 0.1 and 0.005, and both used vanilla Stochastic Gradient Descent (SGD) without momentum for stochastic optimization of the neural networks. Each agent performed 2 stochastic gradient updates of mini-batch size 128 per network for every 128 steps in the game. The target network of the DQN algorithm was refitted every 300 updates. NFSP’s anticipatory parameter was set to η = 0.1. The -greedy policies’ exploration started at 0.06 and decayed to 0, proportionally to the inverse square root of the number of iterations.\nFigure 3 shows NFSP approaching Nash equilibria for various network architectures. We observe a monotonic performance increase with size of the networks. NFSP achieved an exploitability of 0.06, which full-width XFP typically achieves after around 1000 full-width iterations.\nIn order to investigate the relevance of various components of NFSP, e.g. reservoir sampling and anticipatory dynamics, we conducted an experiment that isolated their effects. Figure 4 shows that these modifications led to decremental performance. In particular, using a fixed-size sliding window to store experience of the agents’ own behaviour led to divergence. For a high anticipatory parameter of 0.5 NFSP’s performance plateaued. Finally, using exponentially-averaged reservoir sampling for supervised learning memory updates led to noisy performance."
    }, {
      "heading" : "4.3. Comparison to DQN",
      "text" : "Several stable algorithms have previously been proposed for deep reinforcement learning, notably the DQN algorithm (Mnih et al., 2015). However, the empirical stability of these algorithms was only previously established in single-agent, perfect (or near-perfect) information MDPs. Here, we investigate the stability of DQN in multi-agent, imperfect-information games, in comparison to NFSP.\nDQN learns a deterministic, greedy strategy. This is sufficient to behave optimally in MDPs, which the algorithm is designed for. Imperfect-information games, on the other hand, generally require stochastic strategies for optimal behaviour. Thus, in addition to DQN’s -greedy strategy, we store its actions in a supervised learning memory, MSL, and learn its average behaviour. This average policy does not affect DQN’s runtime behaviour at all, as it is never played. We implement this variant of DQN by using NFSP with an anticipatory parameter of η = 1. We set most of DQN’s parameters to be equal to those found for NFSP in the previous section’s experiments. This is motivated by the supervised learning parameters not directly affecting DQN’s performance. We trained DQN with all combinations of the following parameters: Learning rate {0.2, 0.1, 0.05}, decaying exploration starting at {0.06, 0.12} and reinforcement learning mem-\nory {2m reservoir, 2m sliding window}. We then chose the best-performing result of DQN and compared to the performance of NFSP that was achieved in the previous section’s experiment. DQN achieved its best performing result with a learning rate of 0.1, exploration starting at 0.12 and a sliding window memory of size 2m.\nFigure 5 shows that DQN’s deterministic strategy is highly exploitable, which is expected as imperfect-information games usually require stochastic policies. DQN’s average behaviour does not approach a Nash equilibrium either. This is notable because DQN stores its experience in a replay memory and thus would effectively learn against the opponents’ average behaviour as long as its replay memory is big enough to keep track of it. This is quite similar to a fictitious play. However, because DQN agents use their -greedy strategies in self-play their experience is highly correlated over time and focussed on only a subset of states. We believe this is the main reason for NFSP’s superior performance in our experiments. NFSP agents use an ever more slowly changing average policy in self-play. Thus, their experience varies more slowly, resulting in a more stable data distribution contained in their memories. This might help their training of neural networks and adaptation to each other. Other common reinforcement learning methods have been shown to exhibit similarly stagnating performance in poker games (Ponsen et al., 2011; Heinrich & Silver, 2015)."
    }, {
      "heading" : "4.4. Limit Texas Hold’em",
      "text" : "We applied NFSP to LHE, a game that is popular with humans. Since in 2008 a computer program beat expert human LHE players for the first time in a public competition, modern computer agents are widely considered to have achieved super-human performance (Newall, 2013). The game was essentially solved by Bowling et al. (2015). We evaluated our agents against SmooCT, a Smooth UCT (Heinrich & Silver, 2015) agent which achieved 3 silver medals in the Annual Computer Poker Competition (ACPC) in 2014. Learning performance was measured in milli-big-blinds won per hand, mbb/h, i.e. one thousandth of a big blind that players post at the beginning of a hand.\nWe manually calibrated NFSP by trying 9 configurations. We achieved the best performance with the following parameters. The neural networks were fully connected with four hidden layers of 1024, 512, 1024 and 512 neurons with rectified linear activations. The memory sizes were set to 600k and 30m for MRL and MSL respectively. MRL functioned as a circular buffer containing a recent window of experience.MSL was updated with exponentiallyaveraged reservoir sampling (Osborne et al., 2014), replacing entries in MSL with minimum probability 0.25. We used vanilla SGD without momentum for both reinforce-\nment and supervised learning, with learning rates set to 0.1 and 0.01 respectively. Each agent performed 2 stochastic gradient updates of mini-batch size 256 per network for every 256 steps in the game. The target network of the DQN algorithm was refitted every 1000 updates. NFSP’s anticipatory parameter was set to η = 0.1. The -greedy policies’ exploration started at 0.08 and decayed to 0, more slowly than in Leduc Hold’em. In addition to NFSP’s main, average strategy profile we also evaluated the best response and greedy-average strategies, which deterministically choose actions that maximize the predicted action values or probabilities respectively.\nTo provide some intuition for win rates in heads-up LHE, a player that always folds will lose 750 mbb/h, and expert human players typically achieve expected win rates of 40- 60 mbb/h at online high-stakes games. Similarly, the top half of computer agents in the ACPC 2014 achieved up to 50 mbb/h between themselves. While training, we periodically evaluated NFSP’s performance against SmooCT from symmetric play for 25000 hands each. Figure 6 presents the learning performance of NFSP. NFSP’s average and greedy-average strategy profiles exhibit a stable and relatively monotonic performance improvement, and achieve win rates of around -50 and -20 mbb/h respectively. The best response strategy profile exhibited more noisy performance, mostly ranging between -50 and 0 mbb/h. We also evaluated the final greedy-average strategy against the other top 3 competitors of the ACPC 2014. Table 1 presents the results."
    }, {
      "heading" : "5. Related work",
      "text" : "Reliance on human expert knowledge can be expensive, prone to human biases and limiting if such knowledge is suboptimal. Yet many methods that have been ap-\nplied to games have relied on human expert knowledge. Deep Blue used a human-engineered evaluation function for chess (Campbell et al., 2002). In computer Go, Maddison et al. (2015) and Clark & Storkey (2015) trained deep neural networks from data of expert human play. In computer poker, current game-theoretic approaches use heuristics of card strength to abstract the game to a tractable size (Zinkevich et al., 2007; Gilpin et al., 2007; Johanson et al., 2013). Waugh et al. (2015) recently combined one of these methods with function approximation. However, their fullwidth algorithm has to implicitly reason about all information states at each iteration, which is prohibitively expensive in large domains. In contrast, NFSP focuses on the sample-based reinforcement learning setting where the game’s states need not be exhaustively enumerated and the learner may not even have a model of the game’s dynamics.\nMany successful applications in games have relied on local search (Campbell et al., 2002; Browne et al., 2012). Local search algorithms efficiently plan decisions in a game at runtime, e.g. via Monte Carlo simulation or limiteddepth backward induction. However, common simulationbased local search algorithms have been shown to diverge when applied to imperfect-information poker games (Ponsen et al., 2011; Heinrich & Silver, 2015). Furthermore, even game-theoretic methods do not generally achieve unexploitable behaviour when planning locally in imperfectinformation games (Burch et al., 2014; Ganzfried & Sandholm, 2015; Lisý et al., 2015). Another problem of local search is the potentially prohibitive cost at runtime if no prior knowledge is injected to guide the search. This poses the question of how to obtain this prior knowledge. Silver et al. (2016) trained convolutional neural networks on human expert data and then used a self-play reinforcement learning procedure to optimize these networks further. By using these neural networks to guide a high-performance local search, they beat a Go grandmaster 5 to 0. In this work, we evaluate our agents without any local search at runtime. If local search methods for imperfect-information games were developed, strategies trained by NFSP could be a promising choice for guiding the search.\nNash equilibria are the only strategy profiles that rational agents can hope to converge on in self-play (Bowling & Veloso, 2001). TD-Gammon (Tesauro, 1995) is a worldclass backgammon agent, whose main component is a neu-\nral network trained from self-play reinforcement learning. While its algorithm, based on temporal-difference learning, is sound in two-player zero-sum perfect-information games, it does not generally converge in games with imperfect information. DQN (Mnih et al., 2015) combines temporal-difference learning with experience replay and deep neural network function approximation. It achieved human-level performance in a majority of Atari games, learning from raw sensory inputs. However, these Atari games were set up as single-agent domains with potential opponents fixed and controlled by the Atari emulator. Our experiments showed that DQN agents were unable to approach a Nash equilibrium in Leduc Hold’em, where players were allowed to adapt dynamically. Yakovenko et al. (2016) trained deep neural networks in self-play in computer poker, including two poker games that are popular with humans. Their networks performed strongly against heuristic-based and simple computer programs. Expert human players were able to outperform their agent, albeit over a statistically insignificant sample size. It remains to be seen whether their approach converges in practice or theory. In contrast, we have empirically shown NFSP’s convergence to approximate Nash equilibria in Leduc Hold’em. Furthermore, the approach is principled and builds on the theory of fictitious play in extensive-form games."
    }, {
      "heading" : "6. Conclusion",
      "text" : "We have introduced NFSP, the first end-to-end deep reinforcement learning approach to learning approximate Nash equilibria of imperfect-information games from self-play. NFSP addresses three problems. Firstly, NFSP agents learn without prior knowledge. Secondly, they do not rely on local search at runtime. Thirdly, they converge to approximate Nash equilibria in self-play. Our empirical results provide the following insights. The performance of fictitious play degrades gracefully with various approximation errors. NFSP converges reliably to approximate Nash equilibria in a small poker game, whereas DQN’s greedy and average strategies do not. NFSP learned a competitive strategy in a real-world scale imperfect-information game from scratch without using explicit prior knowledge.\nIn this work, we focussed on imperfect-information twoplayer zero-sum games. Fictitious play, however, is also guaranteed to converge to Nash equilibria in cooperative, potential games. It is therefore conceivable that NFSP can be successfully applied to these games as well. Furthermore, recent developments in continuous-action reinforcement learning (Lillicrap et al., 2015) could enable NFSP to be applied to continuous-action games, which current game-theoretic methods cannot deal with directly."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We thank Peter Dayan, Marc Lanctot and Marc Bellemare for helpful discussions and feedback. This research was supported by the UK Centre for Doctoral Training in Financial Computing and by the NVIDIA Corporation."
    } ],
    "references" : [ {
      "title" : "Predictably irrational",
      "author" : [ "Ariely", "Dan", "Jones", "Simon" ],
      "venue" : "HarperCollins New York,",
      "citeRegEx" : "Ariely et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Ariely et al\\.",
      "year" : 2008
    }, {
      "title" : "Opportunities for multiagent systems and multiagent reinforcement learning in traffic control",
      "author" : [ "Bazzan", "Ana LC" ],
      "venue" : "Autonomous Agents and Multi-Agent Systems,",
      "citeRegEx" : "Bazzan and LC.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bazzan and LC.",
      "year" : 2009
    }, {
      "title" : "Rational and convergent learning in stochastic games",
      "author" : [ "Bowling", "Michael", "Veloso", "Manuela" ],
      "venue" : "In Proceedings of the 17th International Joint Conference on Artifical Intelligence,",
      "citeRegEx" : "Bowling et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Bowling et al\\.",
      "year" : 2001
    }, {
      "title" : "Heads-up limit holdem poker is solved",
      "author" : [ "Bowling", "Michael", "Burch", "Neil", "Johanson", "Tammelin", "Oskari" ],
      "venue" : null,
      "citeRegEx" : "Bowling et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bowling et al\\.",
      "year" : 2015
    }, {
      "title" : "Iterative solution of games by fictitious play",
      "author" : [ "Brown", "George W" ],
      "venue" : "Activity analysis of production and allocation,",
      "citeRegEx" : "Brown and W.,? \\Q1951\\E",
      "shortCiteRegEx" : "Brown and W.",
      "year" : 1951
    }, {
      "title" : "Solving imperfect information games using decomposition",
      "author" : [ "Burch", "Neil", "Johanson", "Michael", "Bowling" ],
      "venue" : "In 28th AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Burch et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Burch et al\\.",
      "year" : 2014
    }, {
      "title" : "Training deep convolutional neural networks to play go",
      "author" : [ "Clark", "Christopher", "Storkey", "Amos" ],
      "venue" : "In Proceedings of the 32nd International Conference on Machine Learning,",
      "citeRegEx" : "Clark et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2015
    }, {
      "title" : "Optimal network security hardening using attack graph games",
      "author" : [ "Durkota", "Karel", "Lisỳ", "Viliam", "Bošanskỳ", "Branislav", "Kiekintveld", "Christopher" ],
      "venue" : "In Proceedings of the 24th International Joint Conference on Artifical Intelligence,",
      "citeRegEx" : "Durkota et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Durkota et al\\.",
      "year" : 2015
    }, {
      "title" : "Treebased batch mode reinforcement learning",
      "author" : [ "Ernst", "Damien", "Geurts", "Pierre", "Wehenkel", "Louis" ],
      "venue" : "In Journal of Machine Learning Research,",
      "citeRegEx" : "Ernst et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Ernst et al\\.",
      "year" : 2005
    }, {
      "title" : "The theory of learning in games, volume 2",
      "author" : [ "Fudenberg", "Drew" ],
      "venue" : "MIT press,",
      "citeRegEx" : "Fudenberg and Drew.,? \\Q1998\\E",
      "shortCiteRegEx" : "Fudenberg and Drew.",
      "year" : 1998
    }, {
      "title" : "Endgame solving in large imperfect-information games",
      "author" : [ "Ganzfried", "Sam", "Sandholm", "Tuomas" ],
      "venue" : "In Proceedings of the 14th International Conference on Autonomous Agents and Multi-Agent Systems,",
      "citeRegEx" : "Ganzfried et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ganzfried et al\\.",
      "year" : 2015
    }, {
      "title" : "The grand challenge of computer go: Monte Carlo tree search and extensions",
      "author" : [ "Gelly", "Sylvain", "Kocsis", "Levente", "Schoenauer", "Marc", "Sebag", "Michèle", "Silver", "David", "Szepesvári", "Csaba", "Teytaud", "Olivier" ],
      "venue" : "Communications of the ACM,",
      "citeRegEx" : "Gelly et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Gelly et al\\.",
      "year" : 2012
    }, {
      "title" : "Gradient-based algorithms for finding Nash equilibria in extensive form games",
      "author" : [ "Gilpin", "Andrew", "Hoda", "Samid", "Pena", "Javier", "Sandholm", "Tuomas" ],
      "venue" : "In Internet and Network Economics,",
      "citeRegEx" : "Gilpin et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Gilpin et al\\.",
      "year" : 2007
    }, {
      "title" : "Smooth UCT search in computer poker",
      "author" : [ "Heinrich", "Johannes", "Silver", "David" ],
      "venue" : "In Proceedings of the 24th International Joint Conference on Artifical Intelligence,",
      "citeRegEx" : "Heinrich et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Heinrich et al\\.",
      "year" : 2015
    }, {
      "title" : "Fictitious self-play in extensive-form games",
      "author" : [ "Heinrich", "Johannes", "Lanctot", "Marc", "Silver", "David" ],
      "venue" : "In Proceedings of the 32nd International Conference on Machine Learning,",
      "citeRegEx" : "Heinrich et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Heinrich et al\\.",
      "year" : 2015
    }, {
      "title" : "Evaluating state-space abstractions in extensive-form games",
      "author" : [ "Johanson", "Michael", "Burch", "Neil", "Valenzano", "Richard", "Bowling" ],
      "venue" : "In Proceedings of the 12th International Conference on Autonomous Agents and MultiAgent Systems,",
      "citeRegEx" : "Johanson et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Johanson et al\\.",
      "year" : 2013
    }, {
      "title" : "A fictitious play approach to large-scale optimization",
      "author" : [ "III Lambert", "J Theodore", "Epelman", "A Marina", "Smith", "L. Robert" ],
      "venue" : "Operations Research,",
      "citeRegEx" : "Lambert et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Lambert et al\\.",
      "year" : 2005
    }, {
      "title" : "Generalised weakened fictitious play",
      "author" : [ "Leslie", "David S", "Collins", "Edmund J" ],
      "venue" : "Games and Economic Behavior,",
      "citeRegEx" : "Leslie et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Leslie et al\\.",
      "year" : 2006
    }, {
      "title" : "Continuous control with deep reinforcement learning",
      "author" : [ "Lillicrap", "Timothy P", "Hunt", "Jonathan J", "Pritzel", "Alexander", "Heess", "Nicolas", "Erez", "Tom", "Tassa", "Yuval", "Silver", "David", "Wierstra", "Daan" ],
      "venue" : "arXiv preprint arXiv:1509.02971,",
      "citeRegEx" : "Lillicrap et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lillicrap et al\\.",
      "year" : 2015
    }, {
      "title" : "Self-improving reactive agents based on reinforcement learning, planning and teaching",
      "author" : [ "Lin", "Long-Ji" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Lin and Long.Ji.,? \\Q1992\\E",
      "shortCiteRegEx" : "Lin and Long.Ji.",
      "year" : 1992
    }, {
      "title" : "Move evaluation in go using deep convolutional neural networks",
      "author" : [ "Maddison", "Chris J", "Huang", "Aja", "Sutskever", "Ilya", "Silver", "David" ],
      "venue" : "The International Conference on Learning Representations,",
      "citeRegEx" : "Maddison et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Maddison et al\\.",
      "year" : 2015
    }, {
      "title" : "Fictitious play property for games with identical interests",
      "author" : [ "Monderer", "Dov", "Shapley", "Lloyd S" ],
      "venue" : "Journal of economic theory,",
      "citeRegEx" : "Monderer et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Monderer et al\\.",
      "year" : 1996
    }, {
      "title" : "Game Theory: Analysis of Conflict",
      "author" : [ "Myerson", "Roger B" ],
      "venue" : null,
      "citeRegEx" : "Myerson and B.,? \\Q1991\\E",
      "shortCiteRegEx" : "Myerson and B.",
      "year" : 1991
    }, {
      "title" : "Reinforcement learning for optimized trade execution",
      "author" : [ "Nevmyvaka", "Yuriy", "Feng", "Yi", "Kearns", "Michael" ],
      "venue" : "In Proceedings of the 23rd International Conference on Machine Learning,",
      "citeRegEx" : "Nevmyvaka et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Nevmyvaka et al\\.",
      "year" : 2006
    }, {
      "title" : "Further Limit Hold ’em: Exploring the Model Poker Game",
      "author" : [ "P. Newall" ],
      "venue" : "Two Plus Two Publishing,",
      "citeRegEx" : "Newall,? \\Q2013\\E",
      "shortCiteRegEx" : "Newall",
      "year" : 2013
    }, {
      "title" : "Exponential reservoir sampling for streaming language models",
      "author" : [ "Osborne", "Miles", "Lall", "Ashwin", "Van Durme", "Benjamin" ],
      "venue" : "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Osborne et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Osborne et al\\.",
      "year" : 2014
    }, {
      "title" : "Computing approximate Nash equilibria and robust bestresponses using sampling",
      "author" : [ "Ponsen", "Marc", "de Jong", "Steven", "Lanctot" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Ponsen et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Ponsen et al\\.",
      "year" : 2011
    }, {
      "title" : "Neural fitted q iteration–first experiences with a data efficient neural reinforcement learning method",
      "author" : [ "Riedmiller", "Martin" ],
      "venue" : "In Machine Learning: ECML",
      "citeRegEx" : "Riedmiller and Martin.,? \\Q2005\\E",
      "shortCiteRegEx" : "Riedmiller and Martin.",
      "year" : 2005
    }, {
      "title" : "Reinforcement learning for robot soccer",
      "author" : [ "Riedmiller", "Martin", "Gabel", "Thomas", "Hafner", "Roland", "Lange", "Sascha" ],
      "venue" : "Autonomous Robots,",
      "citeRegEx" : "Riedmiller et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Riedmiller et al\\.",
      "year" : 2009
    }, {
      "title" : "An iterative method of solving a game",
      "author" : [ "Robinson", "Julia" ],
      "venue" : "Annals of Mathematics, pp",
      "citeRegEx" : "Robinson and Julia.,? \\Q1951\\E",
      "shortCiteRegEx" : "Robinson and Julia.",
      "year" : 1951
    }, {
      "title" : "Some studies in machine learning using the game of checkers",
      "author" : [ "Samuel", "Arthur L" ],
      "venue" : "IBM Journal of research and development,",
      "citeRegEx" : "Samuel and L.,? \\Q1959\\E",
      "shortCiteRegEx" : "Samuel and L.",
      "year" : 1959
    }, {
      "title" : "Bounded rationality",
      "author" : [ "Selten", "Reinhard" ],
      "venue" : "Journal of Institutional and Theoretical Economics, pp",
      "citeRegEx" : "Selten and Reinhard.,? \\Q1990\\E",
      "shortCiteRegEx" : "Selten and Reinhard.",
      "year" : 1990
    }, {
      "title" : "Dynamic fictitious play, dynamic gradient play, and distributed convergence to Nash equilibria",
      "author" : [ "Shamma", "Jeff S", "Arslan", "Gürdal" ],
      "venue" : "IEEE Transactions on Automatic Control,",
      "citeRegEx" : "Shamma et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Shamma et al\\.",
      "year" : 2005
    }, {
      "title" : "Mastering the game of go with deep neural networks and tree",
      "author" : [ "Sutskever", "Ilya", "Lillicrap", "Timothy", "Leach", "Madeleine", "Kavukcuoglu", "Koray", "Graepel", "Thore", "Hassabis", "Demis" ],
      "venue" : "search. Nature,",
      "citeRegEx" : "Sutskever et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2016
    }, {
      "title" : "Reinforcement learning: An introduction, volume 1",
      "author" : [ "Sutton", "Richard S", "Barto", "Andrew G" ],
      "venue" : null,
      "citeRegEx" : "Sutton et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 1998
    }, {
      "title" : "Security and game theory: algorithms, deployed systems, lessons learned",
      "author" : [ "Tambe", "Milind" ],
      "venue" : null,
      "citeRegEx" : "Tambe and Milind.,? \\Q2011\\E",
      "shortCiteRegEx" : "Tambe and Milind.",
      "year" : 2011
    }, {
      "title" : "Temporal difference learning and tdgammon",
      "author" : [ "Tesauro", "Gerald" ],
      "venue" : "Communications of the ACM,",
      "citeRegEx" : "Tesauro and Gerald.,? \\Q1995\\E",
      "shortCiteRegEx" : "Tesauro and Gerald.",
      "year" : 1995
    }, {
      "title" : "Tactex’13: a champion adaptive power trading agent",
      "author" : [ "Urieli", "Daniel", "Stone", "Peter" ],
      "venue" : "In Proceedings of the 13th International Conference on Autonomous Agents and Multi-Agent Systems,",
      "citeRegEx" : "Urieli et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Urieli et al\\.",
      "year" : 2014
    }, {
      "title" : "Random sampling with a reservoir",
      "author" : [ "Vitter", "Jeffrey S" ],
      "venue" : "ACM Transactions on Mathematical Software (TOMS),",
      "citeRegEx" : "Vitter and S.,? \\Q1985\\E",
      "shortCiteRegEx" : "Vitter and S.",
      "year" : 1985
    }, {
      "title" : "Efficient computation of behavior strategies",
      "author" : [ "Von Stengel", "Bernhard" ],
      "venue" : "Games and Economic Behavior,",
      "citeRegEx" : "Stengel and Bernhard.,? \\Q1996\\E",
      "shortCiteRegEx" : "Stengel and Bernhard.",
      "year" : 1996
    }, {
      "title" : "Solving games with functional regret estimation",
      "author" : [ "Waugh", "Kevin", "Morrill", "Dustin", "Bagnell", "J. Andrew", "Bowling", "Michael" ],
      "venue" : "In 29th AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Waugh et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Waugh et al\\.",
      "year" : 2015
    }, {
      "title" : "Poker-cnn: A pattern learning strategy for making draws and bets in poker games using convolutional networks",
      "author" : [ "Yakovenko", "Nikolai", "Cao", "Liangliang", "Raffel", "Colin", "Fan", "James" ],
      "venue" : "In 30th AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Yakovenko et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Yakovenko et al\\.",
      "year" : 2016
    }, {
      "title" : "Regret minimization in games with incomplete information",
      "author" : [ "Zinkevich", "Martin", "Johanson", "Michael", "Bowling", "Piccione", "Carmelo" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Zinkevich et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Zinkevich et al\\.",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 28,
      "context" : "Games have a tradition of encouraging advances in artificial intelligence and machine learning (Samuel, 1959; Tesauro, 1995; Campbell et al., 2002; Riedmiller et al., 2009; Gelly et al., 2012; Bowling et al., 2015).",
      "startOffset" : 95,
      "endOffset" : 214
    }, {
      "referenceID" : 11,
      "context" : "Games have a tradition of encouraging advances in artificial intelligence and machine learning (Samuel, 1959; Tesauro, 1995; Campbell et al., 2002; Riedmiller et al., 2009; Gelly et al., 2012; Bowling et al., 2015).",
      "startOffset" : 95,
      "endOffset" : 214
    }, {
      "referenceID" : 3,
      "context" : "Games have a tradition of encouraging advances in artificial intelligence and machine learning (Samuel, 1959; Tesauro, 1995; Campbell et al., 2002; Riedmiller et al., 2009; Gelly et al., 2012; Bowling et al., 2015).",
      "startOffset" : 95,
      "endOffset" : 214
    }, {
      "referenceID" : 23,
      "context" : "develop algorithms that will scale to more complex, realworld games such as airport and network security, financial and energy trading, traffic control and routing (Lambert III et al., 2005; Nevmyvaka et al., 2006; Bazzan, 2009; Tambe, 2011; Urieli & Stone, 2014; Durkota et al., 2015).",
      "startOffset" : 164,
      "endOffset" : 285
    }, {
      "referenceID" : 7,
      "context" : "develop algorithms that will scale to more complex, realworld games such as airport and network security, financial and energy trading, traffic control and routing (Lambert III et al., 2005; Nevmyvaka et al., 2006; Bazzan, 2009; Tambe, 2011; Urieli & Stone, 2014; Durkota et al., 2015).",
      "startOffset" : 164,
      "endOffset" : 285
    }, {
      "referenceID" : 13,
      "context" : "Technically, NFSP extends and instantiates Fictitious Self-Play (FSP) (Heinrich et al., 2015) with neural network function approximation.",
      "startOffset" : 70,
      "endOffset" : 93
    }, {
      "referenceID" : 42,
      "context" : "stract the game to a tractable size (Zinkevich et al., 2007; Gilpin et al., 2007; Johanson et al., 2013).",
      "startOffset" : 36,
      "endOffset" : 104
    }, {
      "referenceID" : 12,
      "context" : "stract the game to a tractable size (Zinkevich et al., 2007; Gilpin et al., 2007; Johanson et al., 2013).",
      "startOffset" : 36,
      "endOffset" : 104
    }, {
      "referenceID" : 15,
      "context" : "stract the game to a tractable size (Zinkevich et al., 2007; Gilpin et al., 2007; Johanson et al., 2013).",
      "startOffset" : 36,
      "endOffset" : 104
    }, {
      "referenceID" : 3,
      "context" : "While Limit Texas Hold’em (LHE), a poker game of real-world scale, has got within reach of being solved with current computational resources (Bowling et al., 2015), most other poker and real-world games remain far out of scope without ab-",
      "startOffset" : 141,
      "endOffset" : 163
    }, {
      "referenceID" : 13,
      "context" : "For a more detailed exposition we refer the reader to (Sutton & Barto, 1998), (Myerson, 1991), (Fudenberg, 1998) and (Heinrich et al., 2015).",
      "startOffset" : 117,
      "endOffset" : 140
    }, {
      "referenceID" : 8,
      "context" : "Fitted Q Iteration (FQI) (Ernst et al., 2005) is a batch reinforcement learning method that replays experience with Q-learning.",
      "startOffset" : 25,
      "endOffset" : 45
    }, {
      "referenceID" : 13,
      "context" : "Heinrich et al. (2015) introduce Full-Width ExtensiveForm Fictitious Play (XFP) that enables fictitious players to update their strategies in behavioural, extensive form, re-",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 13,
      "context" : "Heinrich et al. (2015) introduce Fictitious Self-Play (FSP), a sample- and machine learning-based class of algorithms that approximate XFP.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 25,
      "context" : "NFSP uses reservoir sampling (Vitter, 1985; Osborne et al., 2014) to memorize experience of its average best responses.",
      "startOffset" : 29,
      "endOffset" : 65
    }, {
      "referenceID" : 13,
      "context" : "Heinrich et al. (2015) propose to use sampling and machine learning to generate data on and learn convex combinations of normal-form strategies in extensive form.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 41,
      "context" : "The Poker-CNN algorithm introduced by Yakovenko et al. (2016) stores a small number of past strategies which it iteratively computes new strategies against.",
      "startOffset" : 38,
      "endOffset" : 62
    }, {
      "referenceID" : 42,
      "context" : "Contrary to other work on computer poker (Zinkevich et al., 2007; Gilpin et al., 2007; Johanson et al., 2013), we do not engineer any higher-level features.",
      "startOffset" : 41,
      "endOffset" : 109
    }, {
      "referenceID" : 12,
      "context" : "Contrary to other work on computer poker (Zinkevich et al., 2007; Gilpin et al., 2007; Johanson et al., 2013), we do not engineer any higher-level features.",
      "startOffset" : 41,
      "endOffset" : 109
    }, {
      "referenceID" : 15,
      "context" : "Contrary to other work on computer poker (Zinkevich et al., 2007; Gilpin et al., 2007; Johanson et al., 2013), we do not engineer any higher-level features.",
      "startOffset" : 41,
      "endOffset" : 109
    }, {
      "referenceID" : 26,
      "context" : "Other common reinforcement learning methods have been shown to exhibit similarly stagnating performance in poker games (Ponsen et al., 2011; Heinrich & Silver, 2015).",
      "startOffset" : 119,
      "endOffset" : 165
    }, {
      "referenceID" : 24,
      "context" : "Since in 2008 a computer program beat expert human LHE players for the first time in a public competition, modern computer agents are widely considered to have achieved super-human performance (Newall, 2013).",
      "startOffset" : 193,
      "endOffset" : 207
    }, {
      "referenceID" : 2,
      "context" : "The game was essentially solved by Bowling et al. (2015). We evaluated our agents against SmooCT, a Smooth UCT (Heinrich & Silver, 2015) agent which achieved 3 silver medals in the Annual Computer Poker Competition",
      "startOffset" : 35,
      "endOffset" : 57
    }, {
      "referenceID" : 25,
      "context" : "MSL was updated with exponentiallyaveraged reservoir sampling (Osborne et al., 2014), replacing entries in MSL with minimum probability 0.",
      "startOffset" : 62,
      "endOffset" : 84
    }, {
      "referenceID" : 42,
      "context" : "In computer poker, current game-theoretic approaches use heuristics of card strength to abstract the game to a tractable size (Zinkevich et al., 2007; Gilpin et al., 2007; Johanson et al., 2013).",
      "startOffset" : 126,
      "endOffset" : 194
    }, {
      "referenceID" : 12,
      "context" : "In computer poker, current game-theoretic approaches use heuristics of card strength to abstract the game to a tractable size (Zinkevich et al., 2007; Gilpin et al., 2007; Johanson et al., 2013).",
      "startOffset" : 126,
      "endOffset" : 194
    }, {
      "referenceID" : 15,
      "context" : "In computer poker, current game-theoretic approaches use heuristics of card strength to abstract the game to a tractable size (Zinkevich et al., 2007; Gilpin et al., 2007; Johanson et al., 2013).",
      "startOffset" : 126,
      "endOffset" : 194
    }, {
      "referenceID" : 18,
      "context" : "In computer Go, Maddison et al. (2015) and Clark & Storkey (2015) trained deep neural networks from data of expert human play.",
      "startOffset" : 16,
      "endOffset" : 39
    }, {
      "referenceID" : 18,
      "context" : "In computer Go, Maddison et al. (2015) and Clark & Storkey (2015) trained deep neural networks from data of expert human play.",
      "startOffset" : 16,
      "endOffset" : 66
    }, {
      "referenceID" : 12,
      "context" : ", 2007; Gilpin et al., 2007; Johanson et al., 2013). Waugh et al. (2015) recently combined one of these methods with function approximation.",
      "startOffset" : 8,
      "endOffset" : 73
    }, {
      "referenceID" : 26,
      "context" : "However, common simulationbased local search algorithms have been shown to diverge when applied to imperfect-information poker games (Ponsen et al., 2011; Heinrich & Silver, 2015).",
      "startOffset" : 133,
      "endOffset" : 179
    }, {
      "referenceID" : 5,
      "context" : "Furthermore, even game-theoretic methods do not generally achieve unexploitable behaviour when planning locally in imperfectinformation games (Burch et al., 2014; Ganzfried & Sandholm, 2015; Lisý et al., 2015).",
      "startOffset" : 142,
      "endOffset" : 209
    }, {
      "referenceID" : 5,
      "context" : "Furthermore, even game-theoretic methods do not generally achieve unexploitable behaviour when planning locally in imperfectinformation games (Burch et al., 2014; Ganzfried & Sandholm, 2015; Lisý et al., 2015). Another problem of local search is the potentially prohibitive cost at runtime if no prior knowledge is injected to guide the search. This poses the question of how to obtain this prior knowledge. Silver et al. (2016) trained convolutional neural networks on human expert data and then used a self-play reinforcement",
      "startOffset" : 143,
      "endOffset" : 429
    }, {
      "referenceID" : 41,
      "context" : "Yakovenko et al. (2016) trained deep neural networks in self-play in computer poker, including two poker games that are popular with humans.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 18,
      "context" : "more, recent developments in continuous-action reinforcement learning (Lillicrap et al., 2015) could enable NFSP to be applied to continuous-action games, which current game-theoretic methods cannot deal with directly.",
      "startOffset" : 70,
      "endOffset" : 94
    } ],
    "year" : 2016,
    "abstractText" : "Many real-world applications can be described as large-scale games of imperfect information. To deal with these challenging domains, prior work has focused on computing Nash equilibria in a handcrafted abstraction of the domain. In this paper we introduce the first scalable endto-end approach to learning approximate Nash equilibria without any prior knowledge. Our method combines fictitious self-play with deep reinforcement learning. When applied to Leduc poker, Neural Fictitious Self-Play (NFSP) approached a Nash equilibrium, whereas common reinforcement learning methods diverged. In Limit Texas Hold’em, a poker game of realworld scale, NFSP learnt a competitive strategy that approached the performance of human experts and state-of-the-art methods.",
    "creator" : "LaTeX with hyperref package"
  }
}