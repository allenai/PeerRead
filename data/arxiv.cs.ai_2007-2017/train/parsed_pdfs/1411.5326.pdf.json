{
  "name" : "1411.5326.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Compress and Control",
    "authors" : [ "Joel Veness", "Marc G. Bellemare", "Marcus Hutter", "Alvin Chua", "Guillaume Desjardins" ],
    "emails" : [ "veness@google.com", "bellemare@google.com", "alschua@google.com", "gdesjardins@google.com", "marcus.hutter@anu.edu.au" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Within recent years, a number of information-theoretic approaches have emerged as practical alternatives to traditional machine learning algorithms. Noteworthy examples include the compression-based approaches of Frank, Chui, and Witten (2000) and Bratko et al. (2006) to classification, and Cilibrasi and Vitányi (2005) to clustering. What differentiates these techniques from more traditional machine learning approaches is that they rely on the ability to compress the raw input, rather than combining or learning features relevant to the task at hand. Thus this family of techniques has proven most successful in situations where the nature of the data makes it somewhat unwieldy to specify or learn appropriate features. This class of methods can be formally justified by appealing to various notions within algorithmic information theory, such as Kolmogorov complexity (Li and Vitányi, 2008). In this paper we show how a similarly inspired approach can be applied to reinforcement learning, or more specifically, to the tasks of policy evaluation and on-policy control.\nPolicy evaluation refers to the task of estimating the value function associated with a given policy, for an arbitrary given environment. The performance of well-known reinforcement learning techniques such as policy iteration\nCopyright c© 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n(Howard, 1960), approximate dynamic programming (Bertsekas and Tsitsiklis, 1996; Powell, 2011) and actor-critic methods (Sutton and Barto, 1998), for example, all crucially depend on how well policy evaluation can be performed. In this paper we introduce a model-based approach to policy evaluation, which transforms the task of estimating a value function to that of learning a particular kind of probabilistic state model.\nTo better put our work into context, it is worth making the distinction between two fundamentally different classes of model based reinforcement learning methods. Simulation based techniques involve learning some kind of forward model of the environment from which future samples can be generated. Given access to such models, planning can be performed directly using search. Noteworthy recent examples include the work of Doshi-Velez (2009), Walsh, Goschin, and Littman (2010), Veness et al. (2010), Veness et al. (2011), Asmuth and Littman (2011), Guez, Silver, and Dayan (2012), Hamilton, Fard, and Pineau (2013) and Tziortziotis, Dimitrakakis, and Blekas (2014). Although the aforementioned works demonstrate quite impressive performance on small domains possessing complicated dynamics, scaling these methods to large state or observation spaces has proven challenging. The main difficulty that arises when using learnt forward models is that the modeling errors tend to compound when reasoning over long time horizons (Talvitie, 2014).\nIn contrast, another family of techniques, referred to in the literature as planning as inference, attempt to side-step the issue of needing to perform accurate simulations by reducing the planning task to one of probabilistic inference within a generative model of the system. These ideas have been recently explored in both the neuroscience (Botvinick and Toussaint, 2012; Solway and Botvinick, 2012) and machine learning (Attias, 2003; Poupart, Lang, and Toussaint, 2011) literature. The experimental results to date have been somewhat inconclusive, making it far from clear whether the transformed problem is any easier to solve in practice. Our main contribution in this paper is to show how to set up a particularly tractable form of inference problem by generalizing compression-based classification to reinforcement learning. The key novelty is to focus the modeling effort on learning the stationary distribution of a particular kind of augmented Markov chain describing the system, from which we ar X\niv :1\n41 1.\n53 26\nv1 [\ncs .A\nI] 1\n9 N\nov 2\n01 4\ncan approximate a type of dual representation (Wang, Bowling, and Schuurmans, 2007; Wang et al., 2008) of the value function. Using this technique, we were able to produce effective controllers on a problem domain orders of magnitude larger than what has previously been addressed with simulation based methods."
    }, {
      "heading" : "2 Background",
      "text" : "We start with a brief overview of the parts of reinforcement learning and information theory needed to describe our work, before reviewing compression-based classification."
    }, {
      "heading" : "2.1 Markov Decision Processes",
      "text" : "A Markov Decision Process (MDP) is a type of probabilistic model widely used within reinforcement learning (Sutton and Barto, 1998; Szepesvári, 2010) and control (Bertsekas and Tsitsiklis, 1996). In this work, we limit our attention to finite horizon, time homogenous MDPs whose action and state spaces are finite. Formally, an MDP is a triplet (S,A, µ), where S is a finite, non-empty set of states, A is a finite, non-empty set of actions and µ is the transition probability kernel that assigns to each state-action pair (s, a) ∈ S ×A a probability measure µ(· | s, a) over S ×R. S and A are known as the state space and action space respectively. The transition probability kernel gives rise to the state transition kernel P(s′|s, a) := µ({s′} × R | s, a), which gives the probability of transitioning from state s to state s′ if action a is taken in s.\nAn agent’s behavior is determined by a policy, that defines, for each state s ∈ S and time t ∈ N, a probability measure over A denoted by πt(· | s). A stationary policy is a policy which is independent of time, which we will denote by π(· | s) where appropriate. At each time t, the agent communicates an action At ∼ πt(· |St−1) to the system in state St−1 ∈ S. The system then responds with a statereward pair (St, Rt) ∼ µ(· |St−1, At). Here we will assume that each reward is bounded between [rmin, rmax] ⊂ R and that the system starts in a state s0 and executes for an infinite number of steps. Thus the execution of the system can be described by a sequence of random variables A1, S1, R1, A2, S2, R2, ...\nThe finite m-horizon return from time t is defined as Zt := ∑t+m−1 i=t Ri. The expected m-horizon return from time t, also known as the value function, is denoted by V π(st) := E[Zt+1 |St = st]. The return space Z is the set of all possible returns. The action-value function is defined by Qπ(st, at+1) := E[Zt+1 |St = st, At+1 = at+1]. An optimal policy, denoted by π∗, is a policy that maximizes the expected return E [Zt+1 |St] for all t; in our setting, a state-dependent deterministic optimal policy always exists."
    }, {
      "heading" : "2.2 Compression and Sequential Prediction",
      "text" : "We now review sequential probabilistic prediction in the context of statistical data compression. An alphabet X is a set of symbols. A string of data x1x2 . . . xn ∈ Xn of length n is denoted by x1:n. The prefix x1:j of x1:n, j ≤ n, is denoted by x≤j or x<j+1. The empty string is denoted by . The concatenation of two strings s and r is denoted by sr.\nA coding distribution ρ is a sequence of probability mass functions ρn : Xn → [0, 1], which for all n ∈ N satisfy the constraint that ρn(x1:n) = ∑ y∈X ρn+1(x1:ny) for all x1:n ∈ Xn, with the base case ρ0( ) := 1. From here onwards, whenever the meaning is clear from the argument to ρ, the subscript on ρ will be dropped. Under this definition, the conditional probability of a symbol xn given previous data x<n is defined as ρ(xn|x<n) := ρ(x1:n)/ρ(x<n) provided ρ(x<n) > 0, with the familiar chain rules ρ(x1:n) =∏n i=1 ρ(xi|x<i) and ρ(xj:k |x<j) = ∏k i=j ρ(xi|x<i) now following. A binary source code c : X ∗ → {0, 1}∗ assigns to each possible data sequence x1:n a binary codeword c(x1:n) of length `c(x1:n). The typical goal when constructing a source code is to minimize the lengths of each codeword while ensuring that the original data sequence x1:n is always recoverable from c(x1:n). A fundamental technique known as arithmetic encoding (Witten, Neal, and Cleary, 1987) makes explicit the connection between coding distributions and source codes. Given a coding distribution ρ and a data sequence x1:n, arithmetic encoding constructs a code aρ which produces a binary codeword whose length is essentially − log2 ρ(x1:n). We refer the reader to the standard text of Cover and Thomas (1991) for further information."
    }, {
      "heading" : "2.3 Compression-based classification",
      "text" : "Compression-based classification was introduced by Frank, Chui, and Witten (2000). Given a sequence of n labeled i.i.d. training examples D := (y1, c1), . . . , (yn, cn), where yi and ci are the input and class labels respectively, one can apply Bayes rule to express the probability of a new example Y being classified as class C ∈ C given the training examples D by\nP [ C | Y,D ] = P [ Y | C,D ] P [ C | D ]∑ c∈C P [ Y | c,D ] P [ c | D ] . (1)\nThe main idea behind compression-based classification is to model P [ Y | C,D ] using a coding distribution for the inputs that is trained on the subset of examples from D that match class C. Well known non-probabilistic compression methods such as LEMPEL-ZIV (Ziv and Lempel, 1977) can be used by forming their associated coding distribution 2−`z(x1:n), where `z(x1:n) is the length of the compressed data x1:n in bits under compression method z. The class probability P [ C | D] can be straightforwardly estimated from its empirical frequency or smoothed versions thereof. Thus the overall accuracy of the classifier essentially depends upon how well the inputs can be modeled by the class conditional coding distribution.\nCompression-based classification has both advantages and disadvantages. On one hand, it is straightforward to apply generic compression techniques (including those operating at the bit or character level) to complicated input types such as richly formatted text or DNA strings (Frank, Chui, and Witten, 2000; Bratko et al., 2006). On the other hand, learning a probabilistic model of the input may be significantly more difficult than directly applying standard dis-\ncriminative classification techniques. Our approach to policy evaluation, which we now describe, raises similar questions."
    }, {
      "heading" : "3 Compression and Control",
      "text" : "We now introduce Compress and Control (CNC), our new method for policy evaluation."
    }, {
      "heading" : "3.1 Overview",
      "text" : "Policy evaluation is concerned with the estimation of the state-action value function Qπ(s, a). Here we assume that the environment is a finite, time homogenous MDP M := (S,A, µ), and that the policy to be evaluated is a stationary Markov policy π. To simplify the exposition, we consider the finite m-horizon case, and assume that all rewards are drawn from a finite set R ⊂ R; later we will discuss how to remove these restrictions.\nAt a high level, CNC performs policy evaluation by learning a time-independent state-action conditional distribution P(Z |S,A); the main technical component of our work involves establishing that this time-independent conditional probability is well defined. Our technique involves constructing a particular kind of augmented Markov chain whose stationary distribution allows for the recovery of P(Z |S,A). Given this distribution, we can obtain\nQπ(s, a) = ∑ z∈Z z P(Z = z |S = s,A = a).\nIn the spirit of compression-based classification, CNC estimates this distribution by using Bayes rule to combine learnt density models of both P(S |Z,A) and P(Z |A). Although it might seem initially strange to learn a model that conditions on the future return, the next section shows how this counterintuitive idea can be made rigorous."
    }, {
      "heading" : "3.2 Transformation",
      "text" : "Our goal is to define a transformed chain whose stationary distribution can be marginalized to obtain a distribution over states, actions and the m-horizon return. We need two lemmas for this purpose. To make these statements precise, we will use some standard terminology from the Markov chain literature; for more detail, we recommend the textbook of Brémaud (1999). Definition 1. A Homogenous Markov Chain (HMC) given by {Xt}t≥0 over state space X is said to be: (AP) aperiodic iff gcd{n ≥ 1 : P[Xn = x|X0 = x] > 0} = 1,∀x ∈ X ; (PR) positive recurrent iff E[min{n ≥ 1 : Xn = x}|X0 = x] < ∞,∀x ∈ X ; (IR) irreducible iff ∀x, x′ ∃n ≥ 1 : P[Xn = x′|X0 = x] > 0; (EA) essentially aperiodic iff gcd{n ≥ 1 : P[Xn = x|X0 = x] > 0} ∈ {1,∞},∀x ∈ X . Note also that EA+IR implies AP. Although the term ergodic is sometimes used to describe particular combinations of these properties (e.g. AP+PR+IR), here we avoid it in favor of being more explicit. Lemma 1. Consider a stochastic process {Xt}t≥1 over state space X that is independent of a sequence of U-valued random variables {Ut}t≥1 in the sense that\nP(xt|x<t, u<t) = P(xt|x<t), and with Ut only depending on Xt−1 and Xt in the sense that P(ut|x1:t, u<t) = P(ut|xt−1, xt) and P(Ut = u|Xt−1 = x,Xt = x′) being independent of t. Then, if {Xt}t≥1 is an (IR/EA/PR) HMC over X , then {Yt}t≥1 := {(Xt, Ut)}t≥1 is an (IR/EA/PR) HMC over Y := {yt ∈ X × U : ∃xt−1 ∈ X : P(yt|xt−1) > 0}. Lemma 1 allows HMC {Xt := (At, St)}t≥1 to be augmented to obtain the HMC {Yt := (Xt, Rt)}t≥1, where At, St and Rt denote the action, state and reward at time t respectively; see Figure 1 for a graphical depiction of the dependence structure.\nThe second result allows the HMC {Yt}t≥1 to be further augmented to give the snake HMC {Yt:t+m}t≥1 (Brémaud, 1999). This construction ensures that there is sufficient information within each augmented state to be able to condition on the m-horizon return. Lemma 2. If {Yt}t≥1 is an (IR/EA/PR) HMC over state space Y , then for any m ∈ N, the stochastic process {Wt}t≥1, where Wt := (Yt, ..., Yt+m), is an (IR/EA/PR) HMC overW := {y0:m ∈ Ym+1 : P(y1:m|y0) > 0}.\nNow if we assume that the HMC defined by M and π is (IR+EA+PR), Lemmas 1 and 2 imply that there exists a unique stationary distribution ν′ over the augmented state space (A× S ×R)m+1.\nFurthermore, if we let (A′0, S ′ 0, R ′ 0, . . . , A ′ m, S ′ m, R ′ m) ∼ ν′ and define Z ′ := ∑m i=1R ′ i, it is clear that there exists a joint distribution ν over Z × (A × S × R)m+1 such that (Z ′, A′0, S ′ 0, R ′ 0, . . . , A ′ m, S ′ m, R ′ m) ∼ ν. Hence the νprobability P [Z ′ | S′0, A′1] is well defined, which allows us to express the action-value function Qπ as\nQπ(s, a) = Eν [Z ′ | S′0 = s,A′1 = a] . (2) Finally, by expanding the expectation and applying Bayes\nrule, Equation 2 can be further re-written as Qπ(s, a) = ∑ z∈Z z ν(z | s, a)\n= ∑ z∈Z z ν(s | z, a) ν(z | a)∑\nz′∈Z ν(s | z′, a) ν(z′ | a)\n. (3)\nThe CNC approach to policy evaluation involves directly learning the conditional distributions ν(s | z, a) and ν(z | a) in Equation 3 from data, and then using these learnt distributions to form a plug-in estimate of Qπ(s, a). Notice that\nν(s|z, a) conditions on the return, similar in spirit to prior work on planning as inference (Attias, 2003; Botvinick and Toussaint, 2012; Solway and Botvinick, 2012). The distinguishing property of CNC is that the conditioning is performed with respect to a stationary distribution that has been explicitly constructed to allow for efficient modeling and inference."
    }, {
      "heading" : "3.3 Online Policy Evaluation",
      "text" : "We now provide an online algorithm for compression-based policy evaluation. This will produce, for all times t ∈ N, an estimate Q̂πt (s, a) of the m-horizon expected return Qπ(s, a) as a function of the first t−m action-observationreward triples.\nConstructing our estimate involves modeling the νprobability terms in Equation 3 using two different coding distributions, ρS and ρZ respectively; ρS will encode states conditional on return-action pairs, and ρZ will encode returns conditional on actions. Sample states, actions and returns can be generated by directly executing the system (M, π); Provided the HMCM + π is (IR+EA+PR), Lemmas 1 and 2 ensure that the empirical distributions formed from a sufficiently large sample of action/state/return triples will be arbitrarily close to the required conditional ν-probabilities.\nNext we describe how the coding distributions are trained. Given a history ht := s0, a1, s1, r1 . . . , an+m, sn+m, rn+m with t = n +m, we define the m-lagged return at any time i ≤ n+ 1 by zi := ri + · · ·+ ri+m−1. The sequence of the first n states occurring in ht can be mapped to a subsequence denoted by sz,a0:n−1 that is defined by keeping only the states (si : zi+1 = z ∧ ai+1 = a)n−1i=0 . Similarly, a sequence of m-lagged returns z1:n can be mapped to a subsequence za1:n formed by keeping only the returns (zi : ai = a)ni=1 from z1:n. Our value estimate at time t of taking action a in state s can now be defined as\nQ̂πt (s, a) := ∑ z∈Z z wz,at (s), (4)\nwhere\nwz,at (s) := ρS( s | sz,a0:n−1 ) ρZ(z | za1:n)∑\nz′∈Z ρS(s | sz ′,a 0:n−1) ρZ(z\n′ | za1:n) (5)\napproximates the probability of receiving a return of z if action a is selected in state s.\nImplementation. The action-value function estimate Q̂πt can be computed efficiently by maintaining |Z||A| buckets, each corresponding to a particular return-action pair (z, a). Each bucket contains an instance of the coding distribution ρS encoding the state sequence s z,a 0:n−1. Similarly, |A| buckets containing instances of ρZ are created to encode the various return subsequences. This procedure is summarized in Algorithm 1.\nTo obtain a particular state-action value estimate, Equations 4 and 5 can be computed directly by querying the appropriate bucketed coding distributions. Assuming that the time required to compute each conditional probability using\nAlgorithm 1 CNC POLICY EVALUATION\nRequire: Stationary policy π, environmentM Require: Finite planning horizon m ∈ N Require: Coding distributions ρS and ρZ\n1: for i = 1 to t do 2: Perform ai ∼ π(· | si−1) 3: Observe (si, ri) ∼ µ(· | si−1, ai) 4: if i ≥ m then 5: Update ρS in bucket (zi−m+1, ai−m+1) with si−m 6: Update ρZ in bucket ai−m+1 with zi−m+1 7: end if 8: end for\n9: return Q̂πt\nρS and ρZ is constant, the time complexity for computing Q̂t(s, a) is O(|Z|)."
    }, {
      "heading" : "3.4 Analysis",
      "text" : "We now show that the state-action estimates defined by Equation 4 are consistent provided that consistent density estimators are used for both ρS and ρZ. Also, we will say fn converges stochastically to 0 with rate n−1/2 if and only if\n∃c>0, ∀δ∈ [0, 1] : P ( |fn(ω)| ≤ √ c n ln 2 δ ) ≥ 1− δ,\nand will denote this by writing fn(ω) ∈ OP(n−1/2). Theorem 1. Given an m-horizon, finite state space, finite action space, time homogenous MDPM := (S,A, µ) and a stationary policy π that gives rise to an (IR+EA+PR) HMC, for all > 0, we have that for any state s ∈ S and action a ∈ A that\nlim n→∞\nP [ | Q̂πn(s, a)−Qπ(s, a) | ≥ ] = 0,\nprovided ρS and ρZ are consistent estimators of ν(s|z, a) and ν(z|a) respectively. Furthermore, if |ρS(s|z, a) − ν(s|z, a)| ∈ OP(n−1/2) and |ρZ(z|a) − ν(z|a)| ∈ OP(n\n−1/2) then |Q̂πn(s, a)−Qπ(s, a)| ∈ OP(n−1/2). Next we state consistency results for two types of estima-\ntors often used in model-based reinforcement learning.\nTheorem 2. The frequency estimator ρ(xn|x<n) := 1 n−1 ∑n−1 i=1 Jxn = xiK when used as either ρS or ρZ is a consistent estimator of ν(s|z, a) or ν(z|a) respectively for any s ∈ S , z ∈ Z , and a ∈ A; furthermore, the absolute estimation error converges stochastically to 0 with rate n−1/2.\nNote that the above result is essentially tabular, in the sense that each state is treated atomically. The next result applies to a factored application of multi-alphabet Context Tree Weighting (CTW) (Tjalkens, Shtarkov, and Willems, 1993; Willems, Shtarkov, and Tjalkens, 1995; Veness et al., 2011), which can handle considerably larger state spaces in practice. In the following, we use the notation sn,i to refer to the ith factor of state sn.\nTheorem 3. Given a state space that is factored in the sense that S := B1 × · · · × Bk, the estimator ρ(sn | s<n) :=∏k i=1 CTW(sn,i | sn,<i, s<n,1:i) when used as ρS, is a consistent estimator of ν(s|z, a) for any s ∈ S, z ∈ Z , and a ∈ A; furthermore, the absolute estimation error converges stochastically to 0 at a rate of n−1/2."
    }, {
      "heading" : "4 Experimental Results",
      "text" : "In this section we describe two sets of experiments. The first set is an experimental validation of our theoretical results using a standard policy evaluation benchmark. The second combines CNC with a variety of density estimators and studies the resulting behavior in a large on-policy control task."
    }, {
      "heading" : "4.1 Policy Evaluation",
      "text" : "Our first experiment involves a simplified version of the game of Blackjack (Sutton and Barto, 1998, Section 5.1). In Blackjack, the agent requests cards from the dealer. A game is won when the agent’s card total exceeds the dealer’s own total. We used CNC to estimate the value of the policy that stays if the player’s sum is 20 or 21, and hits in all other cases. A state is represented by the single card held by the dealer, the player’s card total so far, and whether the player holds a usable ace. In total, there are 200 states, two possible actions (hit or stay), and three possible returns (-1, 0 and 1). A Dirichlet-Multinomial model with hyper-parameters αi = 1 2 was used for both ρS and ρZ.\nFigure 2 depicts the estimated MSE and average maximum squared error of Q̂π over 100,000 episodes; the mean and maximum are taken over all possible state-action pairs and averaged over 10,000 trials. We also compared CNC to a first-visit Monte Carlo value estimate (Szepesvári, 2010). The CNC estimate closely tracks the Monte Carlo estimate, even performing slightly better early on due to the smoothing introduced by the use of a Dirichlet prior. As predicted by the analysis in Section 3.4, the MSE decays toward zero."
    }, {
      "heading" : "4.2 On-policy Control",
      "text" : "Our next set of experiments explored the on-policy control behavior of CNC under an -greedy policy. The purpose of these experiments is to demonstrate the potential of CNC to scale to large control tasks when combined with a variety of different density estimators. Note that Theorem 1 does not apply here: using CNC in this way violates the assumption that π is stationary.\nEvaluation Platform. We evaluated CNC using ALE, the Arcade Learning Environment (Bellemare et al., 2013), a reinforcement learning interface to the Atari 2600 video game platform. Observations in ALE consist of frames of 160 × 210 7-bit color pixels generated by the Stella Atari 2600 emulator. Although the emulator generates frames at 60Hz, in our experiments we consider time steps that last 4 consecutive frames, following the existing literature (Bellemare, Veness, and Talvitie, 2014; Mnih et al., 2013). We first focused on the game of PONG, which has an action space of {UP,DOWN,NOOP} and provides a reward of 1 or -1 whenever a point is scored by either the agent or its computer opponent. Episodes end when either player has scored\n21 points; as a result, possible scores for one episode range between -21 to 21, with a positive score corresponding to a win for the agent.\nExperimental Setup. We studied four different CNC agents, with each agent corresponding to a different choice of model for ρS; the Sparse Adapative Dirichlet (SAD) estimator (Hutter, 2013) was used for ρZ for all agents. Each agent used an -greedy policy (Sutton and Barto, 1998) with respect to its current value function estimates. The exploration rate was initialized to 1.0, then decayed linearly to 0.02 over the course of 200,000 time steps. The horizon was set to m = 80 steps, corresponding to roughly 5 seconds of play. The agents were evaluated over 10 trials, each lasting 2 million steps.\nThe first model we consider is a factored application of the SAD estimator, a count based model designed for large, sparse alphabets. The model divides the screen into 16× 16 regions. The probability of a particular image patch occurring within each region is modeled using a region-specific SAD estimator. The probability assigned to a whole screen is the product of the probabilities assigned to each patch.\nThe second model is an auto-regressive application of logistic regression (Bishop, 2006), that assigns a probability to each pixel using a shared set of parameters. The product of these per-pixel probabilities determines the probability of a screen under this model. The features for each pixel prediction correspond to the pixel’s local context, similar to standard context-based image compression techniques (Witten, Moffat, and Bell, 1999). The model’s parameters were updated online using ADAGRAD (Duchi, Hazan, and Singer, 2011). The hyperparameters (including learning rate, choice of context, etc.) were optimized via the random sampling technique of Bergstra and Bengio (2012).\nThe third model uses the LEMPEL-ZIV algorithm (Ziv and Lempel, 1977), a dictionary-based compression technique. It works by adapting its internal data structures over time to assign shorter code lengths to more frequently seen substrings of data. For our application, the pixels in each frame were encoded in row-major order, by first searching for the longest sequence in the history matching the new data to be compressed, and then encoding a triple that describes the temporal location of the longest match, its length, as well as the next unmatched symbol. This process repeats until no data is left. Recalling Section 2.3, the (implicit) conditional probability of a state s under the LEMPEL-ZIV model can\nnow be obtained by computing\nρS(s | sz,a0:n−1) := 2 −[`LZ(sz,a0:n−1s)−`LZ(s z,a 0:n−1)].\nResults. As depicted in Figure 3 (left), all three models improved their policies over time. By the end of training, two of these models had learnt control policies achieving win rates of approximately 50% in PONG. Over their last 50 episodes of training, the LEMPEL-ZIV agents averaged -0.09 points per episode (std. error: 1.79) and the factored SAD agents, 3.29 (std. error: 2.49). While the logistic regression agents were less successful (average -17.87, std. error 0.38) we suspect that further training time would significantly improve their performance. Furthermore, all agents ran at real-time or better. These results highlight how CNC can be successfully combined with fundamentally different approaches to density estimation.\nWe performed one more experiment to illustrate the effects of combining CNC with a more sophisticated density model. We used SKIPCTS, a recent Context Tree Weighting derivative, with a context function tailored to the ALE observation space (Bellemare, Veness, and Talvitie, 2014). As shown in Figure 3 (right), CNC combined with SKIPCTS learns a near-optimal policy in PONG. We also compared our method to existing results from the literature (Bellemare et al., 2013; Mnih et al., 2013), although note that the DQN scores, which correspond to a different training regime and do not include Freeway, are included only for illustrative purposes. As shown in Figure 4, CNC can also learn competitive control policies on FREEWAY and Q*BERT.\nInterestingly, we found SKIPCTS to be insufficiently accurate for effective MCTS planning when used as a forward model, even with enhancements such as double progressive widening (Couëtoux et al., 2011). In particular, our best simulation-based agent did not achieve a score above −14 in PONG, and performed no better than random in Q*BERT and FREEWAY. In comparison, our CNC variants performed significantly better using orders of magnitude less computation. While it would be premature to draw any general conclusions, the CNC approach does appear to be more forgiving of modeling inaccuracies."
    }, {
      "heading" : "5 Discussion and Limitations",
      "text" : "The main strength and key limitation of the CNC approach seems to be its reliance on an appropriate choice of den-\nAv er\nag e\nSc or\ne (S\nca le\nd)\nFreeway Pong Q*bert\n20.0\n16.4\n3190\n-19.0 497.2\n13.0\nDQN BASS CNC\nFigure 4: Average score over the last 500 episodes for three Atari 2600 games. Error bars indicate one inter-trial one standard error.\nsity estimator. One could only expect the method to perform well if the learnt models can capture the observational structure specific to high and low return states. Specifying a model can be thus viewed as committing to a particular kind of compression-based similarity metric over the state space. The attractive part of this approach is that density modeling is a well studied area, which opens up the possibility of bringing in many ideas from machine learning, statistics and information theory to address fundamental questions in reinforcement learning. The downside of course is that density modeling is itself a difficult problem. Further investigation is required to better understand the circumstances under which one would prefer CNC over more traditional modelfree approaches that rely on function approximation to scale to large and complex problems.\nSo far we have only applied CNC to undiscounted, finite horizon problems with finite action spaces, and more importantly, finite (and rather small) return spaces. This setting is favorable for CNC, since the per-step running time depends on |Z| ≤ m|rmax − rmin|; in other words, the worst case running time scales no worse than linearly in the length of the horizon. However, even modest changes to the above setting can change the situation drastically. For example, using discounted return can introduce an exponential dependence on the horizon. Thus an important topic for future work is to further develop the CNC approach for large or continuous return spaces. Since the return space is only one dimensional, it would be natural to consider various discretizations of the return space. For example, one could consider a tree based discretization that recursively subdivides the return space into successively smaller halves. A binary tree of depth d would produce 2d intervals of even size with an accuracy of = m(rmax − rmin)/2d. This implies that to achieve an accuracy of at least we would need to set d ≥ log2 (m(rmax − rmin)/ ), which should be feasible for many applications. Furthermore, one could attempt to adaptively learn the best discretization (Hutter, 2005a) or approximate Equation 4 using Monte Carlo sampling. These enhancements seem necessary before we could consider applying CNC to the complete suite of ALE games."
    }, {
      "heading" : "6 Closing Remarks",
      "text" : "This paper has introduced CNC, an information-theoretic policy evaluation and on-policy control technique for reinforcement learning. The most interesting aspect of this approach is the way in which it uses a learnt probabilistic model that conditions on the future return; remarkably, this counterintuitive idea can be justified both in theory and in practice.\nWhile our initial results show promise, a number of open questions clearly remain. For example, so far the CNC value estimates were constructed by using only the Monte Carlo return as the learning signal. However, one of the central themes in Reinforcement Learning is bootstrapping, the idea of constructing value estimates on the basis of other value estimates (Sutton and Barto, 1998). A natural question to explore is whether bootstrapping can be incorporated into the learning signal used by CNC.\nFor the case of on-policy control, it would be also interesting to investigate the use of compression techniques or density estimators that can automatically adapt to nonstationary data. A promising line of investigation might be to consider the class of meta-algorithms given by György, Linder, and Lugosi (2012), that can convert any stationary coding distribution into its piece-wise stationary extension; efficient algorithms from this class have shown promise for data compression applications, and come with strong theoretical guarantees (Veness et al., 2013). Furthermore, extending the analysis in Section 3.4 to cover the case of onpolicy control or policy iteration (Howard, 1960) would be highly desirable.\nFinally, we remark that information-theoretic perspectives on reinforcement learning have existed for some time; in particular, Hutter (2005b) described a unification of algorithmic information theory and reinforcement learning, leading to the AIXI optimality notion for reinforcement learning agents. Establishing whether any formal connection exists between this body of work and ours is deferred to the future.\nAcknowledgments. We thank Kee Siong Ng, Andras György, Shane Legg, Laurent Orseau and the anonymous reviewers for their helpful feedback on earlier revisions."
    } ],
    "references" : [ {
      "title" : "Learning is planning: near Bayes-optimal reinforcement learning via MonteCarlo tree search",
      "author" : [ "J. Asmuth", "M.L. Littman" ],
      "venue" : "Uncertainty in Artificial Intelligence (UAI), 19–26.",
      "citeRegEx" : "Asmuth and Littman,? 2011",
      "shortCiteRegEx" : "Asmuth and Littman",
      "year" : 2011
    }, {
      "title" : "Planning by Probabilistic Inference",
      "author" : [ "H. Attias" ],
      "venue" : "Proceedings of the 9th International Workshop on Artificial Intelligence and Statistics.",
      "citeRegEx" : "Attias,? 2003",
      "shortCiteRegEx" : "Attias",
      "year" : 2003
    }, {
      "title" : "The Arcade Learning Environment: An Evaluation Platform for General Agents",
      "author" : [ "M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling" ],
      "venue" : "Journal of Artificial Intelligence Research (JAIR) 47:253–279.",
      "citeRegEx" : "Bellemare et al\\.,? 2013",
      "shortCiteRegEx" : "Bellemare et al\\.",
      "year" : 2013
    }, {
      "title" : "Skip Context Tree Switching",
      "author" : [ "M.G. Bellemare", "J. Veness", "E. Talvitie" ],
      "venue" : "Proceedings of the Thirty-First International Conference on Machine Learning (ICML).",
      "citeRegEx" : "Bellemare et al\\.,? 2014",
      "shortCiteRegEx" : "Bellemare et al\\.",
      "year" : 2014
    }, {
      "title" : "Random search for hyperparameter optimization",
      "author" : [ "J. Bergstra", "Y. Bengio" ],
      "venue" : "Journal of Machine Learning Research (JMLR) 13:281–305.",
      "citeRegEx" : "Bergstra and Bengio,? 2012",
      "shortCiteRegEx" : "Bergstra and Bengio",
      "year" : 2012
    }, {
      "title" : "Neuro-Dynamic Programming",
      "author" : [ "D.P. Bertsekas", "J.N. Tsitsiklis" ],
      "venue" : "Athena Scientific, 1st edition.",
      "citeRegEx" : "Bertsekas and Tsitsiklis,? 1996",
      "shortCiteRegEx" : "Bertsekas and Tsitsiklis",
      "year" : 1996
    }, {
      "title" : "Pattern Recognition and Machine Learning (Information Science and Statistics)",
      "author" : [ "C.M. Bishop" ],
      "venue" : "Secaucus, NJ, USA: Springer-Verlag New York, Inc.",
      "citeRegEx" : "Bishop,? 2006",
      "shortCiteRegEx" : "Bishop",
      "year" : 2006
    }, {
      "title" : "Planning as inference",
      "author" : [ "M. Botvinick", "M. Toussaint" ],
      "venue" : "Trends in Cognitive Sciences 10, 485–588.",
      "citeRegEx" : "Botvinick and Toussaint,? 2012",
      "shortCiteRegEx" : "Botvinick and Toussaint",
      "year" : 2012
    }, {
      "title" : "Spam filtering using statistical data compression models",
      "author" : [ "A. Bratko", "G.V. Cormack", "D. R", "B. Filipi", "P. Chan", "T.R. Lynam" ],
      "venue" : "Journal of Machine Learning Research",
      "citeRegEx" : "Bratko et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Bratko et al\\.",
      "year" : 2006
    }, {
      "title" : "Markov chains : Gibbs fields, Monte Carlo simulation and queues",
      "author" : [ "P. Brémaud" ],
      "venue" : "Texts in applied mathematics. New York, Berlin, Heidelberg: Springer.",
      "citeRegEx" : "Brémaud,? 1999",
      "shortCiteRegEx" : "Brémaud",
      "year" : 1999
    }, {
      "title" : "Clustering by compression",
      "author" : [ "R. Cilibrasi", "P.M.B. Vitányi" ],
      "venue" : "IEEE Transactions on Information Theory 51:1523–1545.",
      "citeRegEx" : "Cilibrasi and Vitányi,? 2005",
      "shortCiteRegEx" : "Cilibrasi and Vitányi",
      "year" : 2005
    }, {
      "title" : "Continuous upper confidence trees",
      "author" : [ "A. Couëtoux", "J.-B. Hoock", "N. Sokolovska", "O. Teytaud", "N. Bonnard" ],
      "venue" : "Proceedings of the 5th International Conference on Learning and Intelligent Optimization, LION’05, 433– 445. Springer-Verlag.",
      "citeRegEx" : "Couëtoux et al\\.,? 2011",
      "shortCiteRegEx" : "Couëtoux et al\\.",
      "year" : 2011
    }, {
      "title" : "Elements of information theory",
      "author" : [ "T.M. Cover", "J.A. Thomas" ],
      "venue" : "New York, NY, USA: Wiley-Interscience.",
      "citeRegEx" : "Cover and Thomas,? 1991",
      "shortCiteRegEx" : "Cover and Thomas",
      "year" : 1991
    }, {
      "title" : "The Infinite Partially Observable Markov Decision Process",
      "author" : [ "F. Doshi-Velez" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS) 22.",
      "citeRegEx" : "Doshi.Velez,? 2009",
      "shortCiteRegEx" : "Doshi.Velez",
      "year" : 2009
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "J. Duchi", "E. Hazan", "Y. Singer" ],
      "venue" : "Journal of Machine Learning Research (JMLR) 12:2121–2159.",
      "citeRegEx" : "Duchi et al\\.,? 2011",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2011
    }, {
      "title" : "Text categorization using compression models",
      "author" : [ "E. Frank", "C. Chui", "I.H. Witten" ],
      "venue" : "Proceedings of Data Compression Conference (DCC), 200–209. IEEE Computer Society Press.",
      "citeRegEx" : "Frank et al\\.,? 2000",
      "shortCiteRegEx" : "Frank et al\\.",
      "year" : 2000
    }, {
      "title" : "Efficient BayesAdaptive Reinforcement Learning using Sample-based Search",
      "author" : [ "A. Guez", "D. Silver", "P. Dayan" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS) 25.",
      "citeRegEx" : "Guez et al\\.,? 2012",
      "shortCiteRegEx" : "Guez et al\\.",
      "year" : 2012
    }, {
      "title" : "Efficient Tracking of Large Classes of Experts",
      "author" : [ "A. György", "T. Linder", "G. Lugosi" ],
      "venue" : "IEEE Transactions on Information Theory 58(11):6709–6725.",
      "citeRegEx" : "György et al\\.,? 2012",
      "shortCiteRegEx" : "György et al\\.",
      "year" : 2012
    }, {
      "title" : "Modelling Sparse Dynamical Systems with Compressed Predictive State Representations",
      "author" : [ "W.L. Hamilton", "M.M. Fard", "J. Pineau" ],
      "venue" : "ICML, volume 28 of JMLR Proceedings, 178–186.",
      "citeRegEx" : "Hamilton et al\\.,? 2013",
      "shortCiteRegEx" : "Hamilton et al\\.",
      "year" : 2013
    }, {
      "title" : "Dynamic Programming and Markov Processes",
      "author" : [ "R.A. Howard" ],
      "venue" : "MIT Press.",
      "citeRegEx" : "Howard,? 1960",
      "shortCiteRegEx" : "Howard",
      "year" : 1960
    }, {
      "title" : "Fast non-parametric Bayesian inference on infinite trees",
      "author" : [ "M. Hutter" ],
      "venue" : "Proceedings of 10th International",
      "citeRegEx" : "Hutter,? 2005a",
      "shortCiteRegEx" : "Hutter",
      "year" : 2005
    }, {
      "title" : "Universal Artificial Intelligence: Sequential Decisions Based on Algorithmic Probability",
      "author" : [ "M. Hutter" ],
      "venue" : "Springer.",
      "citeRegEx" : "Hutter,? 2005b",
      "shortCiteRegEx" : "Hutter",
      "year" : 2005
    }, {
      "title" : "Sparse adaptive dirichlet-multinomiallike processes",
      "author" : [ "M. Hutter" ],
      "venue" : "Conference on Computational Learning Theory (COLT), 432–459.",
      "citeRegEx" : "Hutter,? 2013",
      "shortCiteRegEx" : "Hutter",
      "year" : 2013
    }, {
      "title" : "An Introduction to Kolmogorov Complexity and Its Applications",
      "author" : [ "M. Li", "P. Vitányi" ],
      "venue" : "Springer, third edition.",
      "citeRegEx" : "Li and Vitányi,? 2008",
      "shortCiteRegEx" : "Li and Vitányi",
      "year" : 2008
    }, {
      "title" : "Playing atari with deep reinforcement learning",
      "author" : [ "V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller" ],
      "venue" : "arXiv preprint arXiv:1312.5602.",
      "citeRegEx" : "Mnih et al\\.,? 2013",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2013
    }, {
      "title" : "Escaping Local Optima in POMDP Planning as Inference",
      "author" : [ "P. Poupart", "T. Lang", "M. Toussaint" ],
      "venue" : "The 10th International Conference on Autonomous Agents and Multiagent Systems - Volume 3, AAMAS ’11, 1263– 1264.",
      "citeRegEx" : "Poupart et al\\.,? 2011",
      "shortCiteRegEx" : "Poupart et al\\.",
      "year" : 2011
    }, {
      "title" : "Approximate Dynamic Programming: Solving the Curses of Dimensionality",
      "author" : [ "W.B. Powell" ],
      "venue" : "Wiley-Interscience, 2nd edition.",
      "citeRegEx" : "Powell,? 2011",
      "shortCiteRegEx" : "Powell",
      "year" : 2011
    }, {
      "title" : "Goal-directed decision making as probabilistic inference: A computational framework and potential neural correlates",
      "author" : [ "A. Solway", "M. Botvinick" ],
      "venue" : "Psycholological Review 119:120–154.",
      "citeRegEx" : "Solway and Botvinick,? 2012",
      "shortCiteRegEx" : "Solway and Botvinick",
      "year" : 2012
    }, {
      "title" : "Reinforcement learning: An introduction",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : "Cambridge, MA: MIT Press.",
      "citeRegEx" : "Sutton and Barto,? 1998",
      "shortCiteRegEx" : "Sutton and Barto",
      "year" : 1998
    }, {
      "title" : "Algorithms for Reinforcement Learning",
      "author" : [ "C. Szepesvári" ],
      "venue" : "Synthesis Lectures on Artificial Intelligence and Machine Learning. Morgan & Claypool Publishers.",
      "citeRegEx" : "Szepesvári,? 2010",
      "shortCiteRegEx" : "Szepesvári",
      "year" : 2010
    }, {
      "title" : "Model Regularization for Stable Sample Rollouts",
      "author" : [ "E. Talvitie" ],
      "venue" : "Uncertainty in Artificial Intelligence (UAI).",
      "citeRegEx" : "Talvitie,? 2014",
      "shortCiteRegEx" : "Talvitie",
      "year" : 2014
    }, {
      "title" : "Context tree weighting: Multi-alphabet sources",
      "author" : [ "T.J. Tjalkens", "Y.M. Shtarkov", "F.M.J. Willems" ],
      "venue" : "Proceedings of the 14th Symposium on Information Theory Benelux.",
      "citeRegEx" : "Tjalkens et al\\.,? 1993",
      "shortCiteRegEx" : "Tjalkens et al\\.",
      "year" : 1993
    }, {
      "title" : "Cover Tree Bayesian Reinforcement Learning",
      "author" : [ "N. Tziortziotis", "C. Dimitrakakis", "K. Blekas" ],
      "venue" : "Journal of Machine Learning Research (JMLR) 15:2313–2335.",
      "citeRegEx" : "Tziortziotis et al\\.,? 2014",
      "shortCiteRegEx" : "Tziortziotis et al\\.",
      "year" : 2014
    }, {
      "title" : "Reinforcement Learning via AIXI Approximation",
      "author" : [ "J. Veness", "K.S. Ng", "M. Hutter", "D. Silver" ],
      "venue" : "Proceedings of the Conference for the Association for the Advancement of Artificial Intelligence (AAAI).",
      "citeRegEx" : "Veness et al\\.,? 2010",
      "shortCiteRegEx" : "Veness et al\\.",
      "year" : 2010
    }, {
      "title" : "A Monte Carlo AIXI approximation",
      "author" : [ "J. Veness", "K.S. Ng", "M. Hutter", "W. Uther", "D. Silver" ],
      "venue" : "Journal of Artificial Intelligence Research (JAIR) 40:95–142.",
      "citeRegEx" : "Veness et al\\.,? 2011",
      "shortCiteRegEx" : "Veness et al\\.",
      "year" : 2011
    }, {
      "title" : "Partition Tree Weighting",
      "author" : [ "J. Veness", "M. White", "M. Bowling", "A. Gyorgy" ],
      "venue" : "Proceedings of Data Compression Conference (DCC), 321–330.",
      "citeRegEx" : "Veness et al\\.,? 2013",
      "shortCiteRegEx" : "Veness et al\\.",
      "year" : 2013
    }, {
      "title" : "Integrating Sample-Based Planning and Model-Based Reinforcement Learning",
      "author" : [ "T.J. Walsh", "S. Goschin", "M.L. Littman" ],
      "venue" : "Proceedings of the Conference for the",
      "citeRegEx" : "Walsh et al\\.,? 2010",
      "shortCiteRegEx" : "Walsh et al\\.",
      "year" : 2010
    }, {
      "title" : "Stable dual dynamic programming",
      "author" : [ "T. Wang", "M. Bowling", "D. Schuurmans", "D.J. Lizotte" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS) 20, 1569– 1576.",
      "citeRegEx" : "Wang et al\\.,? 2008",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2008
    }, {
      "title" : "Dual representations for dynamic programming and reinforcement learning",
      "author" : [ "T. Wang", "M. Bowling", "D. Schuurmans" ],
      "venue" : "IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning, 44–51.",
      "citeRegEx" : "Wang et al\\.,? 2007",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2007
    }, {
      "title" : "The Context Tree Weighting Method: Basic Properties",
      "author" : [ "F.M. Willems", "Y.M. Shtarkov", "T.J. Tjalkens" ],
      "venue" : "IEEE Transactions on Information Theory 41:653–664.",
      "citeRegEx" : "Willems et al\\.,? 1995",
      "shortCiteRegEx" : "Willems et al\\.",
      "year" : 1995
    }, {
      "title" : "Managing gigabytes: compressing and indexing documents and images",
      "author" : [ "I.H. Witten", "A. Moffat", "T.C. Bell" ],
      "venue" : "Morgan Kaufmann.",
      "citeRegEx" : "Witten et al\\.,? 1999",
      "shortCiteRegEx" : "Witten et al\\.",
      "year" : 1999
    }, {
      "title" : "Arithmetic coding for data compression",
      "author" : [ "I.H. Witten", "R.M. Neal", "J.G. Cleary" ],
      "venue" : "Communications of the ACM. 30:520–540.",
      "citeRegEx" : "Witten et al\\.,? 1987",
      "shortCiteRegEx" : "Witten et al\\.",
      "year" : 1987
    }, {
      "title" : "A universal algorithm for sequential data compression",
      "author" : [ "J. Ziv", "A. Lempel" ],
      "venue" : "Information Theory, IEEE Transactions on 23(3):337–343.",
      "citeRegEx" : "Ziv and Lempel,? 1977",
      "shortCiteRegEx" : "Ziv and Lempel",
      "year" : 1977
    } ],
    "referenceMentions" : [ {
      "referenceID" : 23,
      "context" : "This class of methods can be formally justified by appealing to various notions within algorithmic information theory, such as Kolmogorov complexity (Li and Vitányi, 2008).",
      "startOffset" : 149,
      "endOffset" : 171
    }, {
      "referenceID" : 8,
      "context" : "Noteworthy examples include the compression-based approaches of Frank, Chui, and Witten (2000) and Bratko et al. (2006) to classification, and Cilibrasi and Vitányi (2005) to clustering.",
      "startOffset" : 99,
      "endOffset" : 120
    }, {
      "referenceID" : 8,
      "context" : "Noteworthy examples include the compression-based approaches of Frank, Chui, and Witten (2000) and Bratko et al. (2006) to classification, and Cilibrasi and Vitányi (2005) to clustering.",
      "startOffset" : 99,
      "endOffset" : 172
    }, {
      "referenceID" : 19,
      "context" : "(Howard, 1960), approximate dynamic programming (Bertsekas and Tsitsiklis, 1996; Powell, 2011) and actor-critic methods (Sutton and Barto, 1998), for example, all crucially depend on how well policy evaluation can be performed.",
      "startOffset" : 0,
      "endOffset" : 14
    }, {
      "referenceID" : 5,
      "context" : "(Howard, 1960), approximate dynamic programming (Bertsekas and Tsitsiklis, 1996; Powell, 2011) and actor-critic methods (Sutton and Barto, 1998), for example, all crucially depend on how well policy evaluation can be performed.",
      "startOffset" : 48,
      "endOffset" : 94
    }, {
      "referenceID" : 26,
      "context" : "(Howard, 1960), approximate dynamic programming (Bertsekas and Tsitsiklis, 1996; Powell, 2011) and actor-critic methods (Sutton and Barto, 1998), for example, all crucially depend on how well policy evaluation can be performed.",
      "startOffset" : 48,
      "endOffset" : 94
    }, {
      "referenceID" : 28,
      "context" : "(Howard, 1960), approximate dynamic programming (Bertsekas and Tsitsiklis, 1996; Powell, 2011) and actor-critic methods (Sutton and Barto, 1998), for example, all crucially depend on how well policy evaluation can be performed.",
      "startOffset" : 120,
      "endOffset" : 144
    }, {
      "referenceID" : 30,
      "context" : "The main difficulty that arises when using learnt forward models is that the modeling errors tend to compound when reasoning over long time horizons (Talvitie, 2014).",
      "startOffset" : 149,
      "endOffset" : 165
    }, {
      "referenceID" : 12,
      "context" : "Noteworthy recent examples include the work of Doshi-Velez (2009), Walsh, Goschin, and Littman (2010), Veness et al.",
      "startOffset" : 47,
      "endOffset" : 66
    }, {
      "referenceID" : 12,
      "context" : "Noteworthy recent examples include the work of Doshi-Velez (2009), Walsh, Goschin, and Littman (2010), Veness et al.",
      "startOffset" : 47,
      "endOffset" : 102
    }, {
      "referenceID" : 12,
      "context" : "Noteworthy recent examples include the work of Doshi-Velez (2009), Walsh, Goschin, and Littman (2010), Veness et al. (2010), Veness et al.",
      "startOffset" : 47,
      "endOffset" : 124
    }, {
      "referenceID" : 12,
      "context" : "Noteworthy recent examples include the work of Doshi-Velez (2009), Walsh, Goschin, and Littman (2010), Veness et al. (2010), Veness et al. (2011), Asmuth and Littman (2011), Guez, Silver, and Dayan (2012), Hamilton, Fard, and Pineau (2013) and Tziortziotis, Dimitrakakis, and Blekas (2014).",
      "startOffset" : 47,
      "endOffset" : 146
    }, {
      "referenceID" : 0,
      "context" : "(2011), Asmuth and Littman (2011), Guez, Silver, and Dayan (2012), Hamilton, Fard, and Pineau (2013) and Tziortziotis, Dimitrakakis, and Blekas (2014).",
      "startOffset" : 8,
      "endOffset" : 34
    }, {
      "referenceID" : 0,
      "context" : "(2011), Asmuth and Littman (2011), Guez, Silver, and Dayan (2012), Hamilton, Fard, and Pineau (2013) and Tziortziotis, Dimitrakakis, and Blekas (2014).",
      "startOffset" : 8,
      "endOffset" : 66
    }, {
      "referenceID" : 0,
      "context" : "(2011), Asmuth and Littman (2011), Guez, Silver, and Dayan (2012), Hamilton, Fard, and Pineau (2013) and Tziortziotis, Dimitrakakis, and Blekas (2014).",
      "startOffset" : 8,
      "endOffset" : 101
    }, {
      "referenceID" : 0,
      "context" : "(2011), Asmuth and Littman (2011), Guez, Silver, and Dayan (2012), Hamilton, Fard, and Pineau (2013) and Tziortziotis, Dimitrakakis, and Blekas (2014). Although the aforementioned works demonstrate quite impressive performance on small domains possessing complicated dynamics, scaling these methods to large state or observation spaces has proven challenging.",
      "startOffset" : 8,
      "endOffset" : 151
    }, {
      "referenceID" : 7,
      "context" : "These ideas have been recently explored in both the neuroscience (Botvinick and Toussaint, 2012; Solway and Botvinick, 2012) and machine learning (Attias, 2003; Poupart, Lang, and Toussaint, 2011) literature.",
      "startOffset" : 65,
      "endOffset" : 124
    }, {
      "referenceID" : 27,
      "context" : "These ideas have been recently explored in both the neuroscience (Botvinick and Toussaint, 2012; Solway and Botvinick, 2012) and machine learning (Attias, 2003; Poupart, Lang, and Toussaint, 2011) literature.",
      "startOffset" : 65,
      "endOffset" : 124
    }, {
      "referenceID" : 1,
      "context" : "These ideas have been recently explored in both the neuroscience (Botvinick and Toussaint, 2012; Solway and Botvinick, 2012) and machine learning (Attias, 2003; Poupart, Lang, and Toussaint, 2011) literature.",
      "startOffset" : 146,
      "endOffset" : 196
    }, {
      "referenceID" : 37,
      "context" : "can approximate a type of dual representation (Wang, Bowling, and Schuurmans, 2007; Wang et al., 2008) of the value function.",
      "startOffset" : 46,
      "endOffset" : 102
    }, {
      "referenceID" : 28,
      "context" : "A Markov Decision Process (MDP) is a type of probabilistic model widely used within reinforcement learning (Sutton and Barto, 1998; Szepesvári, 2010) and control (Bertsekas and Tsitsiklis, 1996).",
      "startOffset" : 107,
      "endOffset" : 149
    }, {
      "referenceID" : 29,
      "context" : "A Markov Decision Process (MDP) is a type of probabilistic model widely used within reinforcement learning (Sutton and Barto, 1998; Szepesvári, 2010) and control (Bertsekas and Tsitsiklis, 1996).",
      "startOffset" : 107,
      "endOffset" : 149
    }, {
      "referenceID" : 5,
      "context" : "A Markov Decision Process (MDP) is a type of probabilistic model widely used within reinforcement learning (Sutton and Barto, 1998; Szepesvári, 2010) and control (Bertsekas and Tsitsiklis, 1996).",
      "startOffset" : 162,
      "endOffset" : 194
    }, {
      "referenceID" : 12,
      "context" : "We refer the reader to the standard text of Cover and Thomas (1991) for further information.",
      "startOffset" : 44,
      "endOffset" : 68
    }, {
      "referenceID" : 42,
      "context" : "Well known non-probabilistic compression methods such as LEMPEL-ZIV (Ziv and Lempel, 1977) can be used by forming their associated coding distribution 2−`z(x1:n), where `z(x1:n) is the length of the compressed data x1:n in bits under compression method z.",
      "startOffset" : 68,
      "endOffset" : 90
    }, {
      "referenceID" : 8,
      "context" : "On one hand, it is straightforward to apply generic compression techniques (including those operating at the bit or character level) to complicated input types such as richly formatted text or DNA strings (Frank, Chui, and Witten, 2000; Bratko et al., 2006).",
      "startOffset" : 205,
      "endOffset" : 257
    }, {
      "referenceID" : 9,
      "context" : "To make these statements precise, we will use some standard terminology from the Markov chain literature; for more detail, we recommend the textbook of Brémaud (1999).",
      "startOffset" : 152,
      "endOffset" : 167
    }, {
      "referenceID" : 9,
      "context" : "The second result allows the HMC {Yt}t≥1 to be further augmented to give the snake HMC {Yt:t+m}t≥1 (Brémaud, 1999).",
      "startOffset" : 99,
      "endOffset" : 114
    }, {
      "referenceID" : 1,
      "context" : "ν(s|z, a) conditions on the return, similar in spirit to prior work on planning as inference (Attias, 2003; Botvinick and Toussaint, 2012; Solway and Botvinick, 2012).",
      "startOffset" : 93,
      "endOffset" : 166
    }, {
      "referenceID" : 7,
      "context" : "ν(s|z, a) conditions on the return, similar in spirit to prior work on planning as inference (Attias, 2003; Botvinick and Toussaint, 2012; Solway and Botvinick, 2012).",
      "startOffset" : 93,
      "endOffset" : 166
    }, {
      "referenceID" : 27,
      "context" : "ν(s|z, a) conditions on the return, similar in spirit to prior work on planning as inference (Attias, 2003; Botvinick and Toussaint, 2012; Solway and Botvinick, 2012).",
      "startOffset" : 93,
      "endOffset" : 166
    }, {
      "referenceID" : 34,
      "context" : "The next result applies to a factored application of multi-alphabet Context Tree Weighting (CTW) (Tjalkens, Shtarkov, and Willems, 1993; Willems, Shtarkov, and Tjalkens, 1995; Veness et al., 2011), which can handle considerably larger state spaces in practice.",
      "startOffset" : 97,
      "endOffset" : 196
    }, {
      "referenceID" : 29,
      "context" : "We also compared CNC to a first-visit Monte Carlo value estimate (Szepesvári, 2010).",
      "startOffset" : 65,
      "endOffset" : 83
    }, {
      "referenceID" : 2,
      "context" : "We evaluated CNC using ALE, the Arcade Learning Environment (Bellemare et al., 2013), a reinforcement learning interface to the Atari 2600 video game platform.",
      "startOffset" : 60,
      "endOffset" : 84
    }, {
      "referenceID" : 24,
      "context" : "Although the emulator generates frames at 60Hz, in our experiments we consider time steps that last 4 consecutive frames, following the existing literature (Bellemare, Veness, and Talvitie, 2014; Mnih et al., 2013).",
      "startOffset" : 156,
      "endOffset" : 214
    }, {
      "referenceID" : 22,
      "context" : "We studied four different CNC agents, with each agent corresponding to a different choice of model for ρS; the Sparse Adapative Dirichlet (SAD) estimator (Hutter, 2013) was used for ρZ for all agents.",
      "startOffset" : 154,
      "endOffset" : 168
    }, {
      "referenceID" : 28,
      "context" : "Each agent used an -greedy policy (Sutton and Barto, 1998) with respect to its current value function estimates.",
      "startOffset" : 34,
      "endOffset" : 58
    }, {
      "referenceID" : 6,
      "context" : "The second model is an auto-regressive application of logistic regression (Bishop, 2006), that assigns a probability to each pixel using a shared set of parameters.",
      "startOffset" : 74,
      "endOffset" : 88
    }, {
      "referenceID" : 4,
      "context" : ") were optimized via the random sampling technique of Bergstra and Bengio (2012).",
      "startOffset" : 54,
      "endOffset" : 81
    }, {
      "referenceID" : 42,
      "context" : "The third model uses the LEMPEL-ZIV algorithm (Ziv and Lempel, 1977), a dictionary-based compression technique.",
      "startOffset" : 46,
      "endOffset" : 68
    }, {
      "referenceID" : 2,
      "context" : "We also compared our method to existing results from the literature (Bellemare et al., 2013; Mnih et al., 2013), although note that the DQN scores, which correspond to a different training regime and do not include Freeway, are included only for illustrative purposes.",
      "startOffset" : 68,
      "endOffset" : 111
    }, {
      "referenceID" : 24,
      "context" : "We also compared our method to existing results from the literature (Bellemare et al., 2013; Mnih et al., 2013), although note that the DQN scores, which correspond to a different training regime and do not include Freeway, are included only for illustrative purposes.",
      "startOffset" : 68,
      "endOffset" : 111
    }, {
      "referenceID" : 11,
      "context" : "Interestingly, we found SKIPCTS to be insufficiently accurate for effective MCTS planning when used as a forward model, even with enhancements such as double progressive widening (Couëtoux et al., 2011).",
      "startOffset" : 179,
      "endOffset" : 202
    }, {
      "referenceID" : 20,
      "context" : "Furthermore, one could attempt to adaptively learn the best discretization (Hutter, 2005a) or approximate Equation 4 using Monte Carlo sampling.",
      "startOffset" : 75,
      "endOffset" : 90
    }, {
      "referenceID" : 28,
      "context" : "However, one of the central themes in Reinforcement Learning is bootstrapping, the idea of constructing value estimates on the basis of other value estimates (Sutton and Barto, 1998).",
      "startOffset" : 158,
      "endOffset" : 182
    }, {
      "referenceID" : 35,
      "context" : "A promising line of investigation might be to consider the class of meta-algorithms given by György, Linder, and Lugosi (2012), that can convert any stationary coding distribution into its piece-wise stationary extension; efficient algorithms from this class have shown promise for data compression applications, and come with strong theoretical guarantees (Veness et al., 2013).",
      "startOffset" : 357,
      "endOffset" : 378
    }, {
      "referenceID" : 19,
      "context" : "4 to cover the case of onpolicy control or policy iteration (Howard, 1960) would be highly desirable.",
      "startOffset" : 60,
      "endOffset" : 74
    }, {
      "referenceID" : 24,
      "context" : "However, one of the central themes in Reinforcement Learning is bootstrapping, the idea of constructing value estimates on the basis of other value estimates (Sutton and Barto, 1998). A natural question to explore is whether bootstrapping can be incorporated into the learning signal used by CNC. For the case of on-policy control, it would be also interesting to investigate the use of compression techniques or density estimators that can automatically adapt to nonstationary data. A promising line of investigation might be to consider the class of meta-algorithms given by György, Linder, and Lugosi (2012), that can convert any stationary coding distribution into its piece-wise stationary extension; efficient algorithms from this class have shown promise for data compression applications, and come with strong theoretical guarantees (Veness et al.",
      "startOffset" : 159,
      "endOffset" : 611
    }, {
      "referenceID" : 19,
      "context" : "4 to cover the case of onpolicy control or policy iteration (Howard, 1960) would be highly desirable. Finally, we remark that information-theoretic perspectives on reinforcement learning have existed for some time; in particular, Hutter (2005b) described a unification of algorithmic information theory and reinforcement learning, leading to the AIXI optimality notion for reinforcement learning agents.",
      "startOffset" : 61,
      "endOffset" : 245
    } ],
    "year" : 2014,
    "abstractText" : "This paper describes a new information-theoretic policy evaluation technique for reinforcement learning. This technique converts any compression or density model into a corresponding estimate of value. Under appropriate stationarity and ergodicity conditions, we show that the use of a sufficiently powerful model gives rise to a consistent value function estimator. We also study the behavior of this technique when applied to various Atari 2600 video games, where the use of suboptimal modeling techniques is unavoidable. We consider three fundamentally different models, all too limited to perfectly model the dynamics of the system. Remarkably, we find that our technique provides sufficiently accurate value estimates for effective on-policy control. We conclude with a suggestive study highlighting the potential of our technique to scale to large problems.",
    "creator" : "LaTeX with hyperref package"
  }
}