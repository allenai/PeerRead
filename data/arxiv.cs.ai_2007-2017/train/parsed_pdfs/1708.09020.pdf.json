{
  "name" : "1708.09020.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning to Price with Reference Effects",
    "authors" : [ "Abbas Kazerouni", "Benjamin Van Roy" ],
    "emails" : [ "abbask@stanford.edu", "bvr@stanford.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Learning to Price with Reference Effects\nAbbas Kazerouni Department of Electrical Engineering, Stanford University, Stanford, CA 94305, abbask@stanford.edu\nBenjamin Van Roy Departments of Management Science and Engineering and Electrical Engineering, Stanford University, CA 94305,\nbvr@stanford.edu\nAs a firm varies the price of a product, consumers exhibit reference effects, making purchase decisions based not only on the prevailing price but also the product’s price history. We consider the problem of learning such behavioral patterns as a monopolist releases, markets, and prices products. This context calls for pricing decisions that intelligently trade off between maximizing revenue generated by a current product and probing to gain information for future benefit. Due to dependence on price history, realized demand can reflect delayed consequences of earlier pricing decisions. As such, inference entails attribution of outcomes to prior decisions and effective exploration requires planning price sequences that yield informative future outcomes. Despite the considerable complexity of this problem, we offer a tractable systematic approach. In particular, we frame the problem as one of reinforcement learning and leverage Thompson sampling. We also establish a regret bound that provides graceful guarantees on how performance improves as data is gathered and how this depends on the complexity of the demand model. We illustrate merits of the approach through simulations.\nKey words : dynamic pricing, reference effects, reinforcement learning, exploration-exploitation dilemma"
    }, {
      "heading" : "1. Introduction",
      "text" : "Consider a monopolist that prices and sells a variety of products over time. Accounting for the impact of prices on demand can greatly improve revenue. This dependence is often complex, with purchase decisions influenced not only by prevailing prices but also price histories. For example, purchases can be triggered by price reductions or prices of alternatives. In this paper we develop an approach to learning such behavioral patterns through setting prices and observing sales, with a goal of maximizing cumulative revenue over the course of many product life cycles.\nEfficient learning calls for a thoughtful balance between maximizing revenue generated by a current product and probing to gain information that can be leveraged to increase subsequent revenue. Reference effects, by which we mean dependencies of current demand on past prices, bring substantial complexity to this so-called exploration-exploitation dilemma. First, there can be ambiguity as to whether purchases are triggered by the current price or some relation to past prices. To disambiguate, an effective learning algorithm must attribute delayed consequences to intertemporal pricing decisions. Second, exploration entails coordinated selection of price sequences;\n1\nar X\niv :1\n70 8.\n09 02\n0v 1\n[ cs\n.G T\n] 2\n9 A\nug 2\n01 7\n2 independent selection of spot prices may not suffice. This is because demand may respond favorably to particular price histories, and probing appropriately selected price sequences can be required to efficiently learn that.\nDespite the considerable complexity of this problem, we provide what to our knowledge is the first tractable systematic approach. We proceed by framing the problem as one of reinforcement learning and then, for particular classes of demand models, developing a computationally efficient learning algorithm based on Thompson sampling. To offer some assurance of statistical efficiency, we establish a bound on expected regret. With respect to the number of past products K, the bound grows as √ K logK, which indicates that per-period expected regret vanishes over time. The bound applies very broadly across model classes, as it depends on the Kolmogorov and eluder dimensions, which are statistics that quantify model complexity in relation to data requirements for model fitting and for exploration, respectively. In particular, the dominant term in our bound grows with the geometric mean of the two notions of dimension. We also present simulation results that demonstrate strong performance relative to less sophisticated exploration schemes.\nFor the sake of exposition, most of our discussion will focus on a simplified setting in which the firm sells indistinguishable products in sequence under unchanging market conditions, discontinuing each product before launching the next, and with each product marketed over a fixed number H of time periods. The price can be adjusted in each of these time periods, and the demand for a product in any given period depends on its prevailing and previous prices. In Section 5, we explain how algorithms and results can be extended to treat more complex models that capture important features of realistic problems. This includes models with covariates that capture distinguishing features of products and varying market conditions and that allow for simultaneous pricing and sales of multiple products with overlapping life cycles of varying duration.\nThere is a substantial literature on pricing with reference effects. Mazumdar et al. (2005) provides a comprehensive survey that covers both behavioral research that provides evidence and examines the structure of reference effects and methodological research on how pricing strategies should respond. Strategies for particular model classes have been developed in Greenleaf (1995), Kopalle et al. (1996), Fibich et al. (2003), Ahn et al. (2007), Popescu and Wu (2007), Heidhues and Kőszegi (2014). However, these papers treat the problem of pricing given known demand models, with no learning required. Electronically-mediated markets, the increasing availability of data, and advances in the field of machine learning have fueled a vast and growing literature on learning to price. We refer the reader to den Boer (2015) for a comprehensive review of the literature and research directions. To our knowledge, our work is the first to treat learning in the presence of reference effects.\n3 The fact that pricing decisions can result in delayed consequences has presented an obstacle to\nthe development of efficient algorithms that learn to price effectively with reference effects. In this\npaper, we offer a new approach through framing the problem as one of reinforcement learning and\nbringing to bear recent developments in the application of Thompson sampling to such problems.\nIt is worth noting, however, that we are not the first to apply Thompson sampling to a pricing\nproblem. In particular, Ferreira et al. (2015) considers an approach based on Thompson sampling\nto address a multiproduct pricing problem with resource constraints, though without reference\neffects.\nOur pricing strategy is based on the posterior sampling reinforcement learning (PSRL) algorithm,\noriginally proposed by Strens Strens (2000) under the name Bayesian Dynamic Programming, as\na heuristic for reinforcement learning in Markov decision processes. Building on general results\nestablished in Russo and Van Roy (2014) for Thompson sampling, regret analyses for PSRL are\ndeveloped in Osband et al. (2013), Osband and Van Roy (2014). In principle, results of Osband and\nVan Roy (2014) apply to the problem we consider in this paper, but the associated regret bound\ndepends on a Lipschitz constant which is not clear how to characterize in our context. We instead\nbuild directly on the technical tools of Russo and Van Roy (2014) and Osband and Van Roy (2014)\nto derive custom regret bounds for PSRL in our pricing model.\nThe rest of this paper is organized as follows. In Section 2, we formulate a dynamic pricing\nproblem that addresses reference effects. In Section 3 , we propose Thompson Pricing (TP) as a\nheuristic strategy for the problem and provide a general regret bound in Theorem 1 of that section.\nTo carry out a more concrete study, in Section 4, we consider the special case of linear demand.\nFor this context, we specialize and interpret regret bounds and present computational results that\nillustrate merits of TP. Section 5 discusses how our dynamic pricing model and algorithm can\nbe generalized to accommodate complexities arising in practical settings, such as observation of\ncovariates that inform demand forecasts, coordinated pricing across multiple products, and pricing\nof products with overlapping sales seasons. Section 6 presents an analysis of TP in a general setting\nthat accommodates various aforementioned complexities, leading to the main technical result of\nthe paper (Theorem 4). We offer concluding remarks in Section 7."
    }, {
      "heading" : "2. Problem Formulation",
      "text" : "In this section, we formulate a dynamic pricing problem and highlight the role played by reference\neffects. To facilitate exposition of core ideas that we will develop in the paper, our model leaves out\nmany complexities that may be required to adequately address practical contexts. In Section 5, we\nwill discuss how the model and ideas can be extended to accommodate some such complexities.\n4 Consider a monopolist selling indistinguishable products over a sequence of sales seasons under unchanging market conditions. We will think of each sales season as an episode of interaction between the monopolist and the consumer market. Let each episode last for H time periods. At the start of each time period, the monopolist sets a price, observes random demand, and collects revenue. We assume that the monopolist faces no supply constraint, so that all demands are met. As an illustration, one might think of the monopolist as a seller of coats that are sold over the Autumn and Winter who adjusts price over each week. In this case, each episode is a six month period and each period lasts a week.\nAt the start of each period h of an episode k, the seller sets a price pk,h and observes demand yk,h, which, conditioned on information available when the price is set, is log-normally distributed with parameters dk,h − σ2/2 and σ2. Hence, dk,h denotes expected demand, while σ2 represents uncertainty. The expected revenue upon setting the price is given by rk,h = dk,hpk,h.\nThe expected demand for a product may depend on factors such as the quality of the item, the price, and consumer behavior. If the monopolist does not understand these dependencies a priori, in order to identify an optimal price, he must learn through experimentation."
    }, {
      "heading" : "2.1. Memoryless Demand",
      "text" : "In the simplest case, one might assume that expected demand depends only on the current price; that is dk,h = fθ(pk,h) for some function fθ : R+ → R, where θ ∈ Rl is an unknown parameter representing what the monopolist does not know about demand structure. Given knowledge of θ, the monopolist should maintain a constant price p = arg maxp pfθ(p) over time. However, a monopolist may have to learn θ through experimentation. This calls for a pricing strategy that balances between exploration and exploitation and converges over time to an optimal price. This can be viewed as a structured bandit learning problem, and several pricing algorithms have been proposed for variations of the problem Ferreira et al. (2015), Kincaid and Darling (1963), Gallego and Van Ryzin (1994, 1997), Bitran et al. (1998), Besbes and Zeevi (2009), Araman and Caldentey (2009), Farias and Van Roy (2010), Besbes and Zeevi (2012), Lobo and Boyd (2003).\nIn the memoryless demand model we have described, the way in which consumers respond to a current price does not depend on price history. In reality, reference effects play a substantial role in purchase decisions Mazumdar et al. (2005). For example, offering a discount often increases demand not only because the new price is low, but also because it is lower than the previous price. Black Friday and Cyber Monday sales constitute well-known examples of this phenomenon. While such reference effects naturally occur they cannot be learned using memoryless demand models. A broader class of dynamic models is called for, as well as more sophisticated strategies involving strategic sequencing of prices to maximize cumulative revenue. We now discuss such a model.\n5"
    }, {
      "heading" : "2.2. Reference Effects",
      "text" : "We consider a model in which expected demand over a time period may depend not only on current price but also on n previous prices within the current episode. Here, n is a parameter that represents duration of memory in the demand model. In any period h= 1, . . . ,H of episode k, we consider the state of the demand model to be\nsk,h = [pk,max(1,h−n), · · · , pk,h−2, pk,h−1]>, (1)\nwhich represents the n-step price history of the product. Then, the expected demand at period h is taken to be\ndk,h = fθ(pk,h, sk,h), (2)\nfor a demand function fθ, which depends on an unknown parameter θ. Dependence of the expected demand on the state sk,h captures reference effects.\nNote that prices in environments with reference effects bear delayed consequences: a price does not only influence immediate but also future demand. Therefore, given knowledge of θ, an optimal pricing strategy does not fix a constant price, as would be the case with a memoryless demand model, but rather, plans a sequence of prices that vary over periods of the episode. Such a sequence p = (p1, p2, · · · , pH) generates expected revenue\nVp = H∑ h=1 phfθ(ph, sh), (3)\nover the episode, where sh is the state at the start of period h, under this price sequence. Therefore, an optimal price sequence is given by\np∗ = arg max p Vp. (4)\nNote that this optimization problem can be solved via dynamic programming. We write\np∗ = DP(θ,H,pmax) (5)\nto indicate that p∗ is the solution of the associated dynamic program applied to above optimization problem with environment variable θ and horizon H. We also provide an argument for a price constraint pmax; if pmax <∞ then each price is constrained to the interval [0, pmax].\nSince the environment is unknown to the seller, he must experiment to learn the demand model while earning revenue. A pricing strategy S is a sequence of policies such as (p1,p2,p3, · · · ) to be executed in consecutive episodes, where policy pk is a (possibly random) function of the history\n6 observed prior to episode k. We will assess the performance of a pricing strategy S in terms of its cumulative regret over K episodes defined by\nRK(S) =E [ K∑ k=1 ( Vp∗ −Vpk )] . (6)\nWith reference effects, the agent has the ability to influence purchasing behavior by exploiting the manner in which consumers react to price trajectories. As such, the learning problem involves learning how to influence consumer behavior, which is a deeper issue than that addressed when learning with a memoryless demand model."
    }, {
      "heading" : "3. Thompson Pricing",
      "text" : "In this section, we present Thompson Pricing (TP), a heuristic strategy that learns to price with reference effects and bound its cumulative regret."
    }, {
      "heading" : "3.1. The Algorithm",
      "text" : "With TP, the seller begins with a prior distribution π over the unknown parameter θ. Let\nHk = {(pj,1, sj,1, yj,1, · · · , pj,H , sj,H , yj,H) : j = 1, . . . , k− 1}\ndenote the history of observations made prior to the start of the kth episode. Based on this history, the agent generates a sample θ̂k from the posterior distribution π (·|Hk) and then, treating this sample as truth, computes the policy\npk = DP(θ̂k,H, pmax). (7)\nThe resulting policy is applied through episode k. After episode k, the posterior distribution is updated based on observations made over the episode, and the process repeats. A more precise description of TP is provided as Algorithm 1.\nNote that TP determines the price trajectory for the entire episode at the start of each episode. Each trajectory can probe the market to reveal consequences not only of individual prices but of a price sequence. Sampling from the posterior distribution over models trades off between exploiting what has been learned and exploring the unknown."
    }, {
      "heading" : "3.2. Regret Bound",
      "text" : "In principle, TP can be applied with any distribution over demand functions, though computational requirements vary greatly depending on the problem class. We now provide a general regret bound that applies broadly. In subsequent sections, we specialize TP and its regret bound to more specific problem classes that admit efficient computation.\n7 Algorithm 1 Thompson Pricing (TP)\nInput: episode length H, function f , maximum price pmax and prior π Initialize: H1 = ∅ for k= 1,2, · · · do\nSample θ̂k ∼ π(·|Hk) Compute p̂k = (pk,1, pk,2, · · · , pk,H) = DP(θ̂k,H, pmax) for h= 1,2, · · · ,H do\nSet price pk,h Observe random demand yk,h\nend for Update Hk+1 =Hk ∪{(pk,1, sk,1, yk,1, · · · , pk,H , sk,H , yk,H)}\nend for\nLet Θ denote the support of the prior distribution π. In each time period, the state of the system is characterized by prices quoted so far within the episode. As such, the state space is given by S = ∅∪ni=1 [0, pmax]i. The demand function fθ maps the current price and state to expected demand. Hence, the set of possible demand functions is given by\nF = { fθ : [0, pmax]×S →R ∣∣θ ∈Θ} . (8) We assume that the range of the demand function is bounded.\nAssumption 1. There exists dmax > 0 such that, for all θ ∈Θ, p ∈ [0, pmax], and s ∈ S, fθ(p, s) ∈ [0, dmax].\nWe will provide a regret bound that applies to any class of demand functions. The dependence of regret on the class of demand functions can be characterized by statistics that reflect suitable notions of complexity. The regret bound that we will provide depends on two such statistics.\nLet G be a collection of functions, each mapping a set X to R. For all α> 0, let N(G, α) denote\nthe α-covering number of G with respect to the supremum norm.\nDefinition 1. Let G be a collection of functions, each mapping a set X to R. The Kolmogorov dimension of G, denoted by dK(G), is\ndk(G) = limsup α↓0 logN(G, α) log(1/α) .\nThe Kolmogorov dimension is a notion of complexity commonly used to quantify the number of data samples required to avoid statistical overfit. Sample complexity results in statistical learning that build on this concept typically apply to contexts where data is generated by a stationary\n8 source. In our pricing problem, data is not produced by an exogenous stationary source but rather through probing actions that hone in on an optimal price sequence. To bound sample requirements in such a context, a new notion of complexity is called for. To serve this need, we will use the eluder dimension, as introduced in Russo and Van Roy (2014). To define this, we begin with a notion of dependence.\nDefinition 2. Let G be a collection of functions, each mapping a set X to R. An element x ∈ X is -dependent on {x1, . . . , xl} ⊆ X with respect to G if any pair of functions g1, g2 satisfy-\ning √∑l\nj=1 (g1(xj)− g2(xj)) 2 ≤ also satisfies g1(x) − g2(x) ≤ . Further, x is -independent of\n{x1, . . . , xl} with respect to G if x is not -dependent on {x1, . . . , xl}.\nIntuitively, an action x is independent of {x1, . . . , xl} if two functions that make similar predictions at {x1, . . . , xl} can nevertheless differ significantly in their predictions at x. This concept suggests the following notion of dimension.\nDefinition 3. Let G be a collection of functions, each mapping a set X to R. The -eluder dimension dE(G, ) is the length l of the longest sequence of elements of G such that, for some ′ ≥ , every element is ′-independent of its predecessors.\nThe following theorem provides a general regret bound for TP.\nTheorem 1. Consider dynamic pricing with reference effects, as formulated Section 2.2. Under Assumption 1,\nRK(TP )≤ pmax ( 1 +HdmaxdE(F , (KH)−2) + 4 √ βKdE(F , (KH)−2)KH ) +\n4pmaxdmax KH , (9)\nwhere\nβK = 8σ 2 log((KH)2N(F , (KH)−2)) + 2\nKH\n( 8dmax + √ 8σ2 log 4 ) , (10)\nand, in an asymptotic notation,\nRK(TP ) =O ( pmaxσ √ dK(F)dE(F , (KH)−2)KH log(KH) ) . (11)\nTheorem 1 provides an upper bound on the regret of TP applied to an arbitrary class of demand functions. The regret bound established in this theorem depends on the geometric mean of Kolmogorov and eluder dimensions. Furthermore, this regret bound is increasing in the maximum price pmax and demand uncertainty σ. The proof of this theorem is quite involved and presented in Section 6. In fact, the analysis of Section 6 addresses a more general problem that allows for covariates and multiproduct pricing. Theorem 1 follows immediately from the more general result.\n9"
    }, {
      "heading" : "4. A Linear Demand Model",
      "text" : "In this section, we study the special case of a linear demand model, in which\nfθ(pk,h, sk,h) = α+βpk,h if h= 1α+βpk,h +φ>h−1sk,h if 2≤ h≤ nα+βpk,h +φ>n sk,h if n+ 1≤ h (12) where α,β ∈ R, φ1 ∈ R1, . . . , φn ∈ Rn are unknown parameters of the demand function. Note that the state vector increases in length over the first n time periods, and φ1, . . . , φn represent coefficient vectors that multiply state vectors of different lengths. The model includes a total of 2 + n(n+\n1)/2 unknown parameters, which can be encoded in terms of a vector θ= [ α,β,φ>1 , · · · , φ>n ]> . We consider a normal prior distribution over θ with mean µ and covariance matrix Σ.\nThanks to conjugacy properties of normal distributions, the posterior distribution of θ after any number of episodes remains normal. To specify the update rules for posterior means and covariances, let us define a few auxiliary variables. Given observations (pk,1, sk,1, yk,1, · · · , pk,H , sk,H , yk,H) gathered over the k’th episode, define\nwk,h = log(yk,h) + σ2\n2 ,\nand for h= 1, . . . ,H, let\nxk,h =  [ 1, pk,h, n(n+1) 2︷ ︸︸ ︷ 0 · · · ,0 ]\nif h= 1\n[ 1, pk,h, (h−1)(h−2) 2︷ ︸︸ ︷ 0 · · · ,0 , s>k,h, n(n+1)−h(h−1) 2︷ ︸︸ ︷ 0 · · · ,0 ] if 2≤ h≤ n [ 1, pk,h, n(n−1) 2︷ ︸︸ ︷ 0 · · · ,0, s>k,h ] if h≥ n+ 1.\nThen, the mean and covariance matrix of the posterior distribution are updated according to\nµ← ( Σ−1 + 1\nσ2 H∑ h=1 x>k,hxk,h\n)−1( Σ−1µ+ 1\nσ2 H∑ h=1 wk,hx > k,h\n) , Σ← ( Σ−1 + 1\nσ2 H∑ h=1 x>k,hxk,h\n)−1 .\n(13)\nAt the start of each k’th episode, TP samples θ̂k from the prevailing posterior distribution and\napplies the policy\npk = DP(θ̂k,H, pmax) (14)\nthroughout the episode. Note that the state evolves deterministically over the episode; that is, the state in any time period is determined by the state in the previous time period and the selected price. With these linear dynamics and linear demand model, the dynamic program reduces to a quadratic optimization problem which can be solved efficiently, as carried out by Algorithm 2.\n10\nTo illustrate successive steps of Algorithm 2, we introduce some notation. Given an l1× l2 matrix M and for 1≤ i≤ l1,1≤ j ≤ l2, we let M [i, j] denote the (i, j)’th element of M . For 1≤ i1 < i2 ≤ l1, we take M [i1 : i2, j] to be the submatrix of M consisting of the elements in column j and rows i1 to i2. Similarly for 1≤ j1 < j2 ≤ l2, M [i, j1 : j2] denotes the submatrix of M consisting of the elements in row i and columns j1 to j2. By (3) and (12), the expected revenue of policy p = (p1, p2, · · · , pH)∈ [0, pmax] H in an episode is\nVp = H∑ h=1 αph + H∑ h=1 βp2h + n∑ h=2 αphφ > h−1sh + H∑ h=n+1 αphφ > n sh, (15)\nwhere sh = [pmax(1,h−n), · · · , ph−2, ph−1]>, for 2 ≤ h ≤H. Now, let Mθ be an H ×H matrix which satisfies\n1. Mθ[h,h] = β for 1≤ h≤H, 2. Mθ[max(h−n,1) : h− 1, h] = 12φmax(h−1,n) for all 2≤ h≤H, 3. Mθ[h,max(h−n,1) : h− 1] = 12φ > max(h−1,n) for all 2≤ h≤H, 4. all other entries of M are equal to 0.\nGiven Mθ, (15) can be expressed as\nVp = p >Mθp +α1 >p, (16)\nwhere with a slight abuse of notation, p is treated as an H dimensional vector. Therefore, given the sampled parameter vector θ̂k, the dynamic program in (14) is equivalent to\npk = arg max p∈[0,pmax]H\np>Mθ̂kp + α̂k1 >p. (17)\nThe quadratic optimization problem in (17) can be solved efficiently via the standard convex optimization tools provided that the matrix Mθ̂k is negative semi-definite. On the other hand, if Mθ̂k is not negative semi-definite, the optimization problem in (17) is NP-hard. While, Mθ̂k is not guaranteed to be negative semi-definite for all realizations of θ̂k, for appropriate values of the prior mean µ (for example with mean of β being negatively large), Mθ̂k would be negative semi-definite with high probability. From a practical point of view, at the start of episode k, sampling from the posterior distribution can be repeated until the sampled θ̂k results in a negative semi-definite Mθ̂k . Algorithm 3 describes the specialization of TP to the described linear environment.\nIn addition to TP, let us consider two other pricing strategies. First, consider a seller who is agnostic to reference effects and adopts a simple pricing strategy based on Thompson sampling which is suitable for memoryless demands. Specifically, such a seller assumes that the expected demand at period h in episode k is\ndk,h = βpk,h +α,\n11\nAlgorithm 2 DP-lin\nInput: θ,H,pmax Output: p Extract α,β,φ1, · · · , φn from θ Initialize: M = 0H×H Set M [1,1] = β for h= 2,3, · · · ,H do\nSet M [h,h] = β Set M [max(h−n,1) : h− 1, h] = 1 2 φmax(h−1,n) Set M [h,max(h−n,1) : h− 1] = 1 2 φ>max(h−1,n)\nend for Return p = arg maxx∈[0,pmax]H x >Mx+α1>x\nAlgorithm 3 TP-lin\nInput: H,pmax, µ0,Σ0, σ 2 Initialize: µ= µ0,Σ = Σ0 for k= 1,2, · · · do\nSample θ̂k ∼N(µ,Σ) Compute p̂k = (pk,1, pk,2, · · · , pk,H) = DP-lin(θ̂k,H, pmax) for h= 1,2, · · · ,H do\nSet price pk,h Observe random demand yk,h\nend for Update µ and Σ according to (13)\nend for\nfor some unknown α,β ∈R and considers a normal prior distribution over [α,β]> with mean µ and covariance matrix Σ. At the beginning of each period h in episode k, [α̂k,h, β̂k,h] > is sampled from the prevailing posterior distribution and the price\npk,h = arg max p∈[0,pmax]\nβ̂k,hp 2 + α̂k,hp\nis set for the product throughout the period. Upon observing the random demand yk,h at the end of this period, the posterior distribution remains normal with its mean and covariance updated via\nµ← ( Σ−1 + 1\nσ2 xk,hx\n> k,h )−1 ( Σ−1µ+wk,hxk,h ) , Σ← ( Σ−1 + 1\nσ2 xk,hx\n> k,h\n)−1 ,\n12\nwhere xk,h = [1, pk,h] >.\nThe above memoryless pricing strategy, which performs near optimally in memoryless environments Ferreira et al. (2015), will drastically fail in the presence of reference effects. This failure can be attributed to ignorance towards the reference effects. With the above misspecified demand model, the seller is not taking the delayed consequences of prices into account while the optimal pricing strategy takes advantage this phenomenon. As a result, the memoryless pricing strategy is not able to learn the optimal pricing strategy in an environment with reference effects.\nAs a second pricing strategy, consider a seller who assumes the demand model of (12), but instead of TP employs a weak version of Thompson sampling as follows. At the beginning of each period within an episode, a model is sampled from the prevailing posterior distribution and treating it as truth, a price is set greedily to maximize the expected immediate revenue. Upon observing the random demand at the end of the period, the posterior parameters are updated according to (13) and the process repeats.\nIn this pricing strategy, the seller indeed accounts for the effect of previous prices on current demand when maximizing the expected immediate revenue, but he overlooks the effect of current price on future demands. This is while the optimal pricing strategy determines the price trajectory for an episode in a way to fully exploit the delayed consequences of the prices and maximize the total expected revenue. For example, the optimal pricing strategy might suggest keeping the prices low (and collect a low revenue) at the initial few periods in exchange for large demands (and large revenues) at the subsequent periods. The greedy behavior of the weak version of Thompson sampling does not allow for such strategic plannings and hence prevents the seller from learning the optimal strategy.\nTo compare the performance of TP with the above two alternative pricing strategies, we simulate an environment with linear demand as described. In the simulation, we let H = 20, n= 6, pmax = 1 and σ2 = 10. We assume that the prior distributions of α and β are N(7.5,10) and N(−4,10), respectively. Further, we assume that each component of φi has a N(0,10) prior distribution for 1≤ i≤ n. Figure 1 shows the per-episode regret of these three pricing strategies which are averaged over thousand random realizations. As depicted in this figure, TP (Algorithm 3) quickly learns the optimal strategy and hence its per-episode regret diminishes quickly. However, as a result of model misspecification, the memoryless pricing which ignores the reference effects fails to learn the unknown parameters and suffers from a large non-diminishing per-episode regret. This observation points out that more sophisticated pricing strategy is required in the presence of reference effects and neglecting such effects massively degrades the performance. Figure 1 also depicts the perepisode regret achieved by the weak version of Thompson sampling described above. As discussed\nearlier, this version of Thompson sampling does not plan for the future and, as Figure 1 shows, its\nper-episode regret converges to a non-zero constant.\nTo explore the effect of memory duration n on the performance of TP, we simulated the same\nscenario but with different values for n. Figure 2 shows the expected per-episode regret of TP over\n100 episodes for n= 2,6,10,14. As depicted in this figure, TP suffers more regret when n is larger\nas in this case the prices have more persistent consequences and it takes longer for TP to learn the optimal pricing policy for an episode.\nAn alternative pricing algorithm that is suitable in the described linear environment is one designed based on certainty equivalence principle. The certainty equivalence pricing strategy works similar to TP, except that, instead of sampling from the posterior distribution at the beginning of each episode, it uses the Maximum Likelihood estimate of θ to compute price trajectory within each episode. Furthermore, to enforce exploration in such an algorithm, dithering techniques, such as -greedy, can be adopted. At any period, -greedy pricing strategy follows certainty equivalence strategy with probability 1− and sets a random price with probability . To compare the performance of these alternative pricing strategies with that of TP, we simulate the same scenario described above. Figure 3 shows average per-episode regret of TP, certainty equivalence strategy and -greedy strategy for various values of . The vertical axis in Figure 3 is in logarithmic scale to better present the differences. As depicted in this figure, certainty equivalence strategy works reasonably well in the described environment and adding randomness via dithering does not improve its performance. However, TP presents a superior performance as its per-episode regret converges to 0 at a faster rate. Note that in this scenario, the maximum possible price is 1 and hence the 0.5 difference between the per-episode regret of TP and certainty equivalence strategy after 1000 episodes presents a significant improvement.\nBased on the results of Theorem 1, an upper bound can be established for cumulative regret of TP when applied in the described linear demand environment. Before stating the result, we assume that the parameter space is bounded.\n15\nAssumption 2. There exists τ > 0 such that ∀ θ̃ ∈Θ : ‖θ̃‖2 ≤ τ .\nThe following corollary follows directly from Theorem 1.\nCorollary 1. Consider an environment where expected demand is linearly parameterized as in\n(12). Under Assumptions 1 and 2, cumulative regret of TP after K episodes satisfies\nRK(TP ) =O ( pmaxσn 2 √ KH log(npmaxτKH) ) . (18)\nProof of Corollary 1. Let F be the class of all possible demand functions as in (12). It is easy\nto see that (see, for example, Proposition 2 of Osband and Van Roy (2014))\ndK(F) =O(n2), dE(F , ) =O(n2 log(npmaxτ/ )).\nThen, the statement follows from Theorem 1.\nCorollary 1 shows that per-episode regret of TP in the described linear environment decreases at a rate of logK/ √ K with the number of episodes K. Pricing strategies that have been proposed\nin the literature for memoryless demand models achieve a similar per-episode regret rate in the\nabsence of reference effects Ferreira et al. (2015). This indicates that although dynamic pricing\nwith reference effects entails additional challenges, TP performs efficiently in that context with no\nadditional cost in terms of the regret rate. Moreover, the regret bound established by Corollary\n1 is increasing in the history parameter n. Clearly, as n increases the prices have more persistent\neffects and the optimal strategy admits a more complicated structure. Hence, it takes longer for\nTP to learn the optimal strategy. From another perspective, n dictates the number of unknown\nparameters of the model and it takes longer to effectively learn within a model with more unknown\nparameters. Furthermore, as the horizon H increases, the optimal policy within an episode takes\na more complicated form as more sophisticated planning is required for larger horizons. Therefore,\nas reflected in (18), it takes longer for TP to effectively learn the optimal policy in larger horizons."
    }, {
      "heading" : "5. Extensions",
      "text" : "For the sake of exposition of our main ideas, we have so far focused our attention on a simplified\nsetting where indistinguishable products are sold sequentially and our description of TP has been\nadapted to this scenario. In this section, we discuss how TP can be generalized to incorporate the\neffect of covariates on the demand and carry out multiproduct pricing possibly with variable and\noverlapping life cycles.\n16"
    }, {
      "heading" : "5.1. The Effect of Covariates",
      "text" : "The setting of Section 2 can be extended to the case where the product being sold at subsequent episodes are distinct. For example, at an episode the seller may be selling a certain type of coat while in the next episode, he will be selling a certain type of shoe. Although in both episodes the seller deals with the same environment, the coat and the shoe will experience different demands when offered with the same price. More generally, in addition to prices and consumer behavior, demand for a product depends on different characteristics of the product itself. To allow for such dependencies, we assume that at the beginning of episode k, the agent has access to a context vector zk ∈Rm which encodes different characteristics of the product being sold at that episode such as its lifetime, its production cost and whether a similar product currently exists in the market. Furthermore, zk may contain other covariates which can influence the demand at episode k such as the current inflation rate and the average income of the potential consumers. The context vector may differ from episode to episode while it remains fixed over each episode.\nTo incorporate the effect of the context on the demand, we extend our formulation in (2) and assume that the expected demand observed at period h for product k– the product being sold at episode k– is\ndk,h = fθ(pk,h, sk,h, zk), (19)\nwhere similar to Section 2, pk,h is the price of product k at period h, sk,h is the n−step price history of the product representing the reference effect and θ is an unknown parameter. In this case, a pricing policy p = (p1, p2, · · · , pH)∈ [0, pmax]H achieves an expected revenue of\nV kp = H∑ h=1 phfθ(ph, sh, zk) (20)\nin episode k, where sh is the state induced by policy p at period h. Then, the optimal pricing policy at episode k is p∗k = arg maxp V k p . Note that the optimal pricing policy in episode k depends on the context zk. Similar to Section 2 and given the parameter θ, the optimal policy in episode k can be computed by means of a dynamic program. We write\np∗k = DP(θ,H,pmax, zk)\nto indicate that the policy p∗k is the solution of the associated dynamic program at episode k with H periods and given the parameter θ and context zk. Also, pmax represents the maximum allowable price for the products.\nTo better illustrate the extension of TP to this setting, we focus on a linear demand model.\nSpecifically, we assume that the expected demand at period h in episode k is given by\ndk,h = α+ (z > k β)pk,h if h= 1 α+ (z>k β)pk,h + z > k φh−1sk,h if 2≤ h≤ n\nα+ (z>k β)pk,h + z > k φnsk,h if n+ 1≤ h\n(21)\n17\nwhere α∈R, β ∈Rm and ∀ 1≤ i≤ n : φi ∈Rm×i are unknown parameters of the demand function. There are a total of 1+m+mn(n+ 1)/2 unknown parameters in this model which can be encoded\nin terms of a vector θ = [ α,β>, φ̄>1 , · · · , φ̄>n ]> , where φ̄i is an mi dimensional vector generated by stacking the columns of φi into a single column.\nTP can be adopted in the same way as in Section 4 to generate pricing policies for this problem. Specifically, starting with a N(µ,Σ) prior distribution on θ, TP draws a sample from the posterior distribution at the start of each episode and uses dynamic programming to compute a policy which is then executed throughout the episode. Thanks to conjugacy properties of normal distributions, the posterior distribution of θ after any number of episodes remains normal. To specify the update rules for posterior means and covariances, let us define some auxiliary variables as in Section 4. Given the observations (zk, pk,1, sk,1, yk,1, · · · , pk,H , sk,H , yk,H) gathered at episode k, define\nwk,h = log(yk,h) + σ2\n2 ,\nand for any 1≤ h≤H, let\nxk,h =  [ 1, pk,hz > k , mn(n+1) 2︷ ︸︸ ︷ 0 · · · ,0 ]\nif h= 1\n[ 1, pk,hz > k , m(h−1)(h−2) 2︷ ︸︸ ︷ 0 · · · ,0 , (sk,h⊗ zk)>, m(n(n+1)−h(h−1)) 2︷ ︸︸ ︷ 0 · · · ,0 ] if 2≤ h≤ n [ 1, pk,hz > k , mn(n−1) 2︷ ︸︸ ︷ 0 · · · ,0, (sk,h⊗ zk)> ] if h≥ n+ 1,\nwhere ⊗ denotes the Kronecker product. Then, at the end of episode k, the mean and covariance matrix of the posterior distribution are updated according to\nµ← ( Σ−1 + 1\nσ2 H∑ h=1 x>k,hxk,h\n)−1( Σ−1µ+ 1\nσ2 H∑ h=1 wk,hx > k,h\n) , Σ← ( Σ−1 + 1\nσ2 H∑ h=1 x>k,hxk,h\n)−1 .\n(22)\nNote that the state in any time period is determined by the state in the previous time period and the selected price. Thanks to such deterministic evolution of the states and linearity of the demand function, the dynamic program step in TP reduces to a quadratic optimization problem. To see this, note that from (20) and (21), the expected revenue of a price vector p = [p1, p2, · · · , pH ]> ∈ [0, pmax]H in the k’th episode can be expressed as\nV kp = H∑ h=1 αph + H∑ h=1 (z>k β)p 2 h + n∑ h=2 ph(z > k φh−1sh) + H∑ h=n+1 ph(z > k φnsh). (23)\nNow, given the set of parameters θ, define the H ×H matrix Mθ such that\n1. Mθ[h,h] = z > k β for 1≤ h≤H,\n18\nAlgorithm 4 DP-lin-cov\nInput: θ,H,pmax, z Output: p Extract α,β,φ1, · · · , φn from θ Initialize: M = 0H×H Set M [1,1] = z>β for h= 2,3, · · · ,H do\nSet M [h,h] = z>β Set M [max(h−n,1) : h− 1, h] = 1 2 z>φmax(h−1,n) Set M [h,max(h−n,1) : h− 1] = 1 2 φ>max(h−1,n)z\nend for Return p = arg maxx∈[0,pmax]H x >Mx+α1>x\n2. Mθ[max(h−n,1) : h− 1, h] = 12z > k φmax(h−1,n) for all 2≤ h≤H, 3. Mθ[h,max(h−n,1) : h− 1] = 12φ > max(h−1,n)zk for all 2≤ h≤H, 4. all other entries of M are equal to 0.\nThen, (23) can be expressed as\nV kp = p >Mθp +α1 >p. (24)\nTherefore, given the sampled parameter θ̂k at the beginning of episode k, the policy to be applied in episode k is given by\npk = arg max p∈[0,pmax]H\np>Mθ̂kp + α̂k1 >p. (25)\nSimilar to Section 4, the matrix Mθ̂k is not guaranteed to be negative semi-definite in which case the optimization problem in (25) is not convex. However, with appropriate choice of the prior mean (for example when z>k β̂k is negatively large with high probability) Mθ̂k would be negative semi-definite with high probability. When implementing TP, posterior sampling at the beginning of\nepisode k can be repeated until the sampled θ̂k results in a negative semi-definite Mθ̂k . Algorithm 4 describes successive steps of the above solution and Algorithm 5 presents the generalization of\nTP to incorporate the effect of covariates.\nWe can also generalize the result of Theorem 1 and establish a regret bound for TP in such a scenario. Let Z denote the set of all possible context vectors. We make the following assumption.\nAssumption 3. There exists λ> 0 such that ∀ z ∈Z : ‖z‖2 ≤ λ.\nThe following theorem provides a regret bound for TP in the above linear environment.\n19\nAlgorithm 5 TP-lin-cov\nInput: H,pmax, µ0,Σ0, σ 2 Initialize: µ= µ0,Σ = Σ0 for k= 1,2, · · · do\nObserve context variable zk Sample θ̂k ∼N(µ,Σ) Compute p̂k = (pk,1, pk,2, · · · , pk,H) = DP-lin-cov(θ̂k,H, pmax, zk) for h= 1,2, · · · ,H do\nSet price pk,h Observe random demand yk,h\nend for Update µ and Σ according to (22)\nend for\nTheorem 2. Consider an environment where the expected demand is given as in (21). Under Assumptions 1, 2 and 3, the regret of TP after K episodes would be\nRK(TP ) =O ( pmaxHσmn 2 √ KH log(npmaxλτKH) ) . (26)\nTheorem 2 has been proved in Section 6. This Theorem yields a similar regret bound for TP in the presence of the covariates as in the case of no covariates. The only difference is the dependence of (26) on the number of covariates m. Note that m scales the number of unknown parameters of the model and appears lineraly in the regret bound."
    }, {
      "heading" : "5.2. Multiproduct Pricing",
      "text" : "So far, we have been considering a single product dynamic pricing problem where at each episode, the seller prices and sells a single product. In many practical situations, however, multiple products are being sold by the seller and he needs to simultaneously price all of them. Potentially, these products can be related to each other in a way that the demand for one of them depends on the price of all the products. Specifically, suppose that a set of q products are being sold at episode k. At period h in this episode, the agent sets a price vector Pk,h ∈ [0, pmax]q such that its j’th component, denoted as Pk,h[j], is the price of product j. While we can extend the model to incorporate the effect of covariates on the demand as well, we neglect such effects here to ease the exposition. Let\nSk,h =  Pk,max(1,h−n)\n... Pk,h−2 Pk,h−1\n\n20\nbe the qmin(n,h−1) dimensional state vector at period h in episode k and let Dk,h be the demand vector at this period such that Dk,h[j] is the expected demand for product j. Focusing on a linear demand function, the expected demand can be modeled as\nDk,h = α+βPk,h if h= 1α+βPk,h +φh−1Sk,h if 2≤ h≤ nα+βPk,h +φnSk,h if n+ 1≤ h, (27) where α∈Rq, β ∈Rq×q and ∀ 1≤ i≤ n : φi ∈Rq×qmin(n,i) are the unknown parameters of the demand function. There are a total of q+ q2 + q2n(n+ 1)/2 unknown parameters which can be encoded in\na vector θ= [ α>, β̄>, φ̄>1 , · · · , φ̄>n ]> where β̄ is a vector generated by stacking the columns of β on top of each other and φ̄i is generated in the same way for 1≤ i≤ n. The agent observes a random demand vector Yk,h at period h in episode k such that Yk,h[j], the demand observed for product j, is a log-normal random variable with parameters Dk,h[j]− σ2/2 and σ2. Note that we have E[Yk,h] =Dk,h. The expected revenue achieved at this period is P>k,hDk,h. A pricing policy in this case is a sequence of price vectors P = (P1, · · · , PH) such that ∀1 ≤ h ≤ H : Ph ∈ [0, pmax]q. The optimal pricing policy is the one that maximizes the expected revenue over the episode:\nP∗ = arg max P H∑ h=1 P>h Dh. (28)\nSimilar to the single product setting, this optimization problem can be solved via a dynamic program. We overload our notation and write\nP∗ = DP(θ,H,pmax, q) (29)\nto denote that P∗ is the solution of the optimization problem in (28) when the demand function is governed by the parameter θ.\nTP can be adapted to learn the optimal policy in this multiproduct pricing problem. Similar to single product scenario, TP starts with a N(µ,Σ) prior distribution on θ. Similar to Section 4 and thanks to conjugacy properties of normal distributions, the posterior distribution of θ after any number of episodes remains normal. To specify the update rules for posterior means and covariances, we define some auxiliary variables. Upon observing (Pk,1, Yk,1, Pk,2, Sk,2, Yk,2, · · · , Pk,H , Sk,H , Yk,H) at episode k, let Wk,h ∈RH be\n∀ 1≤ j ≤ q : Wk,h[j] = log(Yk,h[j]) + σ2\n2 ,\nand for any 1≤ h≤H, define Xk,h as\nXk,h =  [ Iq, P > k,h⊗ Iq,0q×q2 n(n+1)2 ] if h= 1[ Iq, P > k,h⊗ Iq,0q×q2 (h−1)(h−2)2 , S > k,h⊗ Iq,0q×q2 n(n+1)−h(h−1)2 ] if 2≤ h≤ n[\nIq, P > k,h⊗ Iq,0q×q2 n(n−1)2 , S > k,h⊗ Iq\n] if h≥ n+ 1.\n21\nAlgorithm 6 DP-lin-mult\nInput: θ,H,pmax, q Output: P Extract α,β,φ1, · · · , φn from θ Initialize: M = 0qH×qH Set M [1 : q,1 : q] = β for h= 2,3, · · · ,H do\nSet M [(h− 1)q+ 1 : hq, (h− 1)q+ 1 : hq] = β Set M [max((h−n− 1)q,0) + 1 : (h− 1)q,h] = 1 2 φ>max(h−1,n) Set M [h,max((h−n− 1)q,0) + 1 : (h− 1)q] = 1 2 φmax(h−1,n)\nend for Build matrix A by stacking α over itself H times Find x∗ = arg maxx∈[0,pmax]qH x >Mx+A>x for h= 1,2, · · · ,H do\nSet Ph = x ∗[(h− 1)q+ 1 : hq]\nend for Return P = (P1, · · · , PH)\nSome linear algebra leads to the following update rules for the mean and covariance matrix at the\nend of episode k:\nµ← ( Σ−1 + 1\nσ2 H∑ h=1 X>k,hXk,h\n)−1( Σ−1µ+ 1\nσ2 H∑ h=1 X>k,hWk,h\n) , Σ← ( Σ−1 + 1\nσ2 H∑ h=1 X>k,hXk,h\n)−1 .\n(30)\nAt the beginning of each k’th episode, TP draws a sample θ̂k from the posterior distribution and\ntreating it as the truth, computes the policy\nPk = DP(θ̂k,H, pmax, q) (31)\nand applies it throughout the episode.\nNote that similar to Section 4, the state vectors in the described multiproduct pricing model\nevolve deterministically. Due to this fact and linearity of the demand function in 27, the dynamic\nprogram in 29 reduces to a quadratic optimization problem as described in Algorithm 6. The proof\nof this reduction is similar to that presented in Section 4 for the single product scenario and is\nomitted here. Algorithm 7 describes the generalization of TP to the above multiproduct pricing\nsetting.\n22\nAlgorithm 7 TP-lin-mult\nInput: H,q, pmax, µ0,Σ0, σ 2 Initialize: µ= µ0,Σ = Σ0 for k= 1,2, · · · do\nSample θ̂k ∼N(µ,Σ) Compute P̂k = (Pk,1, Pk,2, · · · , Pk,H) = DP-lin-mult(θ̂k,H, pmax, q) for h= 1,2, · · · ,H do\nSet price vector Pk,h Observe random demand Yk,h\nend for Update µ and Σ according to (30)\nend for\nThanks to our general analysis in Section 6, we can also provide a regret bound for TP in the described multiproduct pricing problem. As a generalization of Assumption 1, we make the following assumption on demand vectors.\nAssumption 4. There exists constant dmax > 0 such that all the demand vectors D satisfy ‖D‖∞ ≤ dmax.\nThe following theorem provides a regret bound for TP.\nTheorem 3. Consider a multiproduct pricing problem where the expected demand is given by (27). Under Assumptions 2 and 4, the regret of TP after K episodes would be\nRK(TP ) =O ( pmaxσq 3n2 √ KH log(τKH) ) . (32)\nTheorem 3 has been proved in Section 6. The only difference between the above regret bound and the one established in Corollary 1 for the single product scenario is the appearance of the number of products q in the regret bound. As the number of products increases, TP requires more time to effectively learn the influence of a product’s price on demand for other products."
    }, {
      "heading" : "5.3. Asynchronous Product Pricing",
      "text" : "As another interesting scenario, we consider dynamic pricing of multiple products with variable and overlapping life cycles. Particularly, we allow the seller to start selling other products while he is still busy with selling others. Let t= 1,2, · · · be the time index denoting the time since the seller’s marketing campaign has started. Suppose that product k is launched at time tk and needs to be sold in Hk consecutive time periods (i.e., until time tk+Hk−1). We refer to the time interval\n23\n[tk, tk +Hk − 1] as episode k which consists of Hk periods and might overlap with other episodes. Note that period h at episode k corresponds to time index tk,h = (tk + h− 1). We assume that at the beginning of episode k, the seller has access to a context vector zk ∈ Rm which encodes the specific characteristics of product k (the product being sold at episode k) and other covariates. Similar to Subsection 5.1, we assume that the expected demand at period h in episode k is given by\ndk,h = α+ (z > k β)pk,h if h= 1 α+ (z>k β)pk,h + z > k φh−1sk,h if 2≤ h≤ n\nα+ (z>k β)pk,h + z > k φnsk,h if n+ 1≤ h≤Hk,\n(33)\nwhere pk,h is the price of product k at period h in episode k and sk,h = [pk,max(1,h−n), · · · , pk,h−1]> is the price history of product k at this time. Similar to Subsection 5.1, the demand function has 1 +\nm+mn(n+ 1)/2 unknown parameters which can be encoded in a vector θ= [ α,β>, φ̄>1 , · · · , φ̄>n ]> . For the sake of consistency, we assume that n≤Hk for all k ∈N. At period h in episode k, a demand yk,h is observed for product k which is a log-normal random variable with parameters dk,h− σ2/2 and σ2. This indicates that the products are disjoint; that is, the price of a product does not affect the demand for other products being sold at the same time.\nTP can be easily adapted to such an asynchronous pricing problem. In this case, TP starts with a N(µ,Σ) prior distribution on θ. Again and due to the conjugacy properties of normal distributions, the posterior distribution of θ after each time period and for any number of products being sold at the same time remains normal. In this case, TP updates the posterior parameters based on the gathered observations after each period in order to maintain the most up to date posterior distribution at any time. Once the new product k with context zk is launched and episode k of length Hk starts, TP draws a sample θ̂k from the prevailing posterior distribution, computes the policy\npk = DP(θ̂k,Hk, pmax, zk),\nand applies it throughout the episode.\nThe update rules for posterior means and covarainces after each period can be derived similar to Section 5.1. The only difference is that at any given period, there might be observations associated to multiple products in which case the observations are aggregated and used to update the posterior parameters. Algorithm 8 presents a detailed procedure for updating the posterior parameters after each period. Successive steps of TP adapted to the above asynchronous pricing scenario are described in Algorithm 9."
    }, {
      "heading" : "6. Analysis of TP",
      "text" : "Our proposed TP algorithm is an adaptation of PSRL algorithm proposed in Osband et al. (2013), and Osband and Van Roy (2014) provides a regret bound on its performance when employed\n24\nAlgorithm 8 update\nInput: µ,Σ, z,p, h, y, σ2, n,m Consider p as p = [p1, p2, · · · , pH ] if h= 1 then\nLet x= [1, phz >,01×mn(n+1)/2]\nelse if 2≤ h≤ n then\nLet s= [p1, p2, · · · , ph−1] Let x= [1, phz >,01×m(h−1)(h−2)/2, (s⊗ z)>,01×m(n(n+1)−h(h−1))/2]\nelse\nLet s= [ph−n, · · · , ph−2, ph−1] Let x= [1, phz >,01×mn(n−1)/2, (s⊗ z)>]\nend if Let w= log(y) + σ 2\n2\nDefine\nµnew =\n( Σ−1 + 1\nσ2 x>x\n)−1( Σ−1µ+ 1\nσ2 wx>\n) , Σnew = ( Σ−1 + 1\nσ2 x>x )−1 Return µnew and Σnew\nin a general reinforcement learning problem. However, this general regret bound does not take into account the special structure of the dynamic pricing problem. Particularly, the regret bound derived in Osband and Van Roy (2014) depends on a Lipschitz constant (K∗ in equation (4) of Osband and Van Roy (2014)) which is not easy to quantify in the reinforcement learning problem of our interest. Instead, we take an approach similar to Russo and Van Roy (2014) and Osband and Van Roy (2014), and exploiting the special structure of the dynamic pricing problem, derive a regret bound on the performance of TP.\nTo maintain generality, we analyze the performance of TP in an environment with reference effects and in the presence of covariates where multiple products are being sold at each episode. In te remaining of this section, We first describe this general scenario and then state the main result of the paper which is a regret bound for TP in this scenario followed by its proof. Finally and given this general result, we prove the regret bounds established in Theorems 1, 2 and 3."
    }, {
      "heading" : "6.1. A General Scenario",
      "text" : "Consider a seller who markets for and sells q products at each episode consisting of H periods. At the beginning of episode k, a context zk ∈Rm is available to the seller which encodes characterisitcs of the products being sold and other covariates related to episode k. At period h in episode k, the\n25\nAlgorithm 9 TP-lin-asynch\nInput: pmax, µ0,Σ0, σ 2, n,m Initialize: µ= µ0,Σ = Σ0,A= ∅, e= 1 for t= 1,2,3, · · · do\nwhile a new product is launched do\nObserve the episode length He and covariates ze ∈Rm Set the starting position of episode e as te = t Sample θ̂e ∼N(µ,Σ) Compute pk = DP-lin-cov(θ̂e,He, pmax, zk) Add e to A and e← e+ 1\nend while for k ∈A do\nLet h= t− tk + 1 be the period index in episode k Set price pk,h for product k Observe random demand yk,h Update the parameters µ,Σ← update(µ,Σ, zk,pk, h, yk,h, σ2, n,m)\nend for\nend for\nseller selects a price vector Pk,h ∈ [0, pmax]q such that Pk,h[j] denotes the price set for product i at this period, 1≤ i≤ q. The demand for a product at any time may depend on the prevailing and previous prices of its own as well those of other products. To represent such reference effects, we let\nSk,h =  Pk,max(1,h−n)\n... Pk,h−2 Pk,h−1  be the state at period h in episode k which is built by concatenating (at most) n previous price vectors in the same episode. The expected demand vector at period h in episode k can be considered as\nDk,h = fθ(Pk,h, Sk,h, zk), (34)\nfor some parameteric function fθ such that Dk,h[j] represents the expected demand for product j at this period. Let Yk,h be the random demand vector experienced by the seller at period h in episode k and assume that Yk,h[j]– random demand observed for product j at this period– is log-normally distributed with parameters Dk,h[j]−σ2/2 and σ2. To maintain a compact notation, when appropriate, we use fkθ (Pk,h, Sk,h) as a shorthand for fθ(Pk,h, Sk,h, zk).\n26\nA pricing policy for an episode is a sequence of H price vectors such as P = (P1, · · · , PH) such\nthat Ph ∈ [0, pmax]q. The value of policy P at episode k is\nV kP = H∑ h=1 P>h Dh = H∑ h=1 P>h f k θ (Ph, Sh),\nwhere Sh and Dh are the state and expected demand vector induced by policy P at period h, respectively. Given zk and θ, the optimal pricing policy for episode k is P ∗ k = arg maxP V k P ,."
    }, {
      "heading" : "6.2. Main Result",
      "text" : "Note that the above formulation of the dynamic pricing problem is general enough to capture all the special scenarios discussed in Sections 2, 5.1 and 5.2. The implementation of TP in each of these scenarios can be thought of as a special case of the following description of TP. TP starts with a prior distribution on θ and, given the observed demands, updates it to a posterior distribution at the end of each episode. At the start of episode k, TP samples θ̂k from the prevailing posterior distribution, computes a policy Pk that maximizes\nV̂ kP = H∑ h=1 P>h f k θ̂k (Ph, Sh)\nand applies it throughout the episode. The expected regret of TP after K episodes is\nRK(TP ) =E [ K∑ k=1 ( V kP∗ k −V kPk )] ,\nwhere the expectation is taken over the context vectors, the inherent randomness in TP and the randomness in θ itself.\nLet Θ be the collection of all possible parameters of the demand function (i.e., the support of\nthe prior distribution) and let\nF = {fθ : [0, pmax]q ×S ×Z →Rq|θ ∈Θ}\ndenote the class of demand functions spanned by θ ∈Θ, where S = ∅∪ni=1 [0, pmax]qi is the state space and Z denotes the set of all possible context vectors. The following theorem, which is the main technical result of this paper, provides a bound on the expected regret of TP in the abovementioned scenario.\nTheorem 4. Consider an environment with reference effects where q products are being sold at each episode and the expected demand vector at any time is given by (34). Assume that the expeted demand vectors always satisfy ‖D‖∞ ≤ dmax. Then, the regret of TP after K episodes would be\nRK(TP )≤ qpmax ( 1 +HdmaxdE(F , (KH)−2) + 4 √ βKdE(F , (KH)−2)KH ) +\n4qpmaxdmax KH , (35)\n27\nwhere\nβK = 8σ 2 log((KH)2N(F , (KH)−2)) + 2\nKH\n( 8dmax + √ 8σ2 log 4 ) . (36)"
    }, {
      "heading" : "In an asymptotic notation, the expected regret of TP satisfies",
      "text" : "RK(TP ) =O ( qpmaxσ √ dK(F)dE(F , (KH)−2)KH log(KH) ) . (37)\nWe provide a proof for Theorem 4 in three steps. We start by deriving an upper bound on the\nregret of TP at each episode. Then, we build a series of delicate confidence sets for the unknown parameter θ and using the eluder dimension of F bound the cumulative width of these confidence\nsets. Finally, we combine the two previous steps and prove Theorem 4. 6.2.1. Bounding the Regret at One Episode Let ∆k =E [ V kP∗\nk −V k P̂k\n] denote the expected\nregret of TP at episode k. Also for any k ∈N, let Hk = {(Pj,1, Sj,1, Yj,1, · · · , Pj,H , Sj,H , Yj,H)}k−1j=1 be the observations made up to the beginning of episode k. The following lemma gives an alternative\nexpression for ∆k which simplifies the rest of our analysis.\nLemma 1. for any k ∈N, we have\n∆k =E [ V̂ kPk −V k Pk ] . (38)\nProof of Lemma 1. By the definition of ∆k and by tower property, we can write for any k:\n∆k =E [ E [ V kp∗\nk −V kpk ∣∣Hk]] =E [ E [ V kp∗\nk − V̂ kpk ∣∣Hk]+E[V̂ kpk − V̂ kpk ∣∣Hk]] . On the other hand, since θ and θ̂k are identically distributed given the history Hk, then we have\nE [ V kp∗\nk − V̂ kpk ∣∣Hk]= 0. combining the two above equations proves the statement.\nNow, let Pk,h be the price vector selected by TP at period h in episode k and let Sk,h denote the state observed at that period. Where appropriate we may let Uk,h denote the pair (Pk,h, Sk,h). The following lemma is a direct consequence of Lemma 1.\nLemma 2. For any k ∈N, we have\n∆k ≤ qpmaxE [ H∑ h=1 ∥∥∥fkθ̂k (Uk,h)− fkθ (Uk,h)∥∥∥2 ] , (39)\n28\nProof of Lemma 2. From the definition of V and V̂ and using Lemma 1, we have\n∆k =E [ H∑ h=1 [ P>k,hf k θ̂k (Uk,h)−P>k,hfkθ (Uk,h) ]]\n=E [ H∑ h=1 P>k,h [ fk θ̂k (Uk,h)− fkθ (Uk,h) ]]\n≤E [ H∑ h=1 ‖Pk,h‖2 ∥∥∥fkθ̂k (Uk,h)− fkθ (Uk,h)∥∥∥2 ]\n≤ qpmaxE [ H∑ h=1 ∥∥∥fkθ̂k (Uk,h)− fkθ (Uk,h)∥∥∥2 ] ,\nwhere the last inequality follows from the fact that the price vectors are in [0, pmax] q.\n6.2.2. Confidence Sets Define Wk,h ∈Rq as\nWk,h[j] = logYk,h[j] + σ2\n2 , 1≤ j ≤ q.\nSince Yk,h[j] is log-normally distributed with parameters Dk,h[j] − σ2/2 and σ2, then Wk,h is a random vector having a multivariate normal distribution of mean Dk,h and covariance matrix σ 2Iq. For any k ∈N and given the history Hk, let\nθ̄k = arg min θ̃∈Θ k−1∑ j=1 H∑ h=1 ∥∥Wj,h− fkθ̃ (Pj,h, Sj,h)∥∥22 be the least square estimate of the demand function parameter at the beginning of episode k.\nFor given α > 0 and δ ∈ (0,1), let N(F , α) denote the α-covering number of F w.r.t. to the\nsupremum norm and define for any k ∈N\nβk(F , δ,α) = 8σ2 log (N(F , α)/δ) + 2αkH ( 8dmax + √ 8σ2 log(4k2H2/δ) ) .\nSimilar to Osband and Van Roy (2014), we consider the following confidence set for k ∈N:\nCk = { θ̃ ∈Θ ∣∣∣∣∣ k−1∑ j=1 H∑ h=1 ∥∥fk θ̃ (Pj,h, Sj,h)− fkθ̄ (Pj,h, Sj,h) ∥∥2 2 ≤ βk(F , δ,α) } . (40)\nThe following proposition guarantees that these confidence sets always contain the true demand\nfunction parameter with high probability.\nProposition 1 (Prop. 5 of Osband and Van Roy (2014)). For any δ ∈ (0,1) and α> 0,\nP [∀k ∈N : θ ∈ Ck]≥ 1− 2δ. (41)\n29\nGiven a particular U = (P,S) and context vector z, we define the width of the confidence set Ck as\nwk(U,z) = sup θ1,θ2∈Ck ‖fθ1(U,z)− fθ2(U,z)‖2. (42)\nThe key element of our analysis is controlling the sum of the width of the confidence sets defined in (40). To do so, we present the following technical result from Osband and Van Roy (2014).\nLemma 3 (Prop. 6 of Osband and Van Roy (2014)). Assume that ‖D‖∞ ≤ dmax always hold. For any K ∈N and any sequence (z1,U1,1, · · · ,U1,H , · · · , zK ,UK,1, · · · ,UK,H), we have\nK∑ k=1 H∑ h=1 wk(Uk,h, zk)≤ 1 +HdmaxdE(F , (KH)−1) + 4 √ βKdE(F , (KH)−1)KH. (43)\n6.2.3. Proof of Theorem 4 Now, we have all the necessary tools to prove Theorem 4. Proof of Theorem 4. First note that for any k ∈N and given history Hk at the start of episode k, the sampled parameter θ̂k is identically distributed with the true parameter θ. Therefore from\nProposition 1, it follows that P [ ∀k ∈N : θ̂k ∈ Ck ] ≥ 1− 2δ. Now, define the event Ek = {θ, θ̂k ∈ Ck}, and let Eck be its complement. By Lemma 2, we have\nRK(TP ) = K∑ k=1 ∆k\n≤ qpmax K∑ k=1 E [ H∑ h=1 ∥∥∥fkθ̂k (Uk,h)− fkθ (Uk,h)∥∥∥2 ]\n= qpmaxE [ K∑ k=1 H∑ h=1 ∥∥∥fkθ̂k (Uk,h)− fkθ (Uk,h)∥∥∥2 1(Ek) ]\n+ qpmaxE [ K∑ k=1 H∑ h=1 ∥∥∥fkθ̂k (Uk,h)− fkθ (Uk,h)∥∥∥2 1(Eck) ]\n≤ qpmaxE [ K∑ k=1 H∑ h=1 wk(Uk,h, zk) ] + 2qpmaxdmaxE [ K∑ k=1 H∑ h=1 1(Eck) ]\n≤ qpmaxE [ K∑ k=1 H∑ h=1 wk(Uk,h, zk) ] + 2qpmaxdmaxKHP [ θ /∈∩i∈NCi or θ̂k /∈∩i∈NCi ] (a) ≤ qpmaxE\n[ K∑ k=1 H∑ h=1 wk(Uk,h, zk) ] + 4qpmaxdmaxKHδ\n(b) ≤ qpmax ( 1 +HdmaxdE(F , (KH)−1) + 4 √ βKdE(F , (KH)−1)KH ) + 4qpmaxdmaxKHδ,\nwhere (a) and (b) follow from Proposition 1 and Lemma 3, respectively. Now we take δ = α = (KH)−2, which gives\nβK = 8σ 2 log((KH)2N(F , (KH)−2)) + 2\nKH\n( 8dmax + √ 8σ2 log 4 ) ,\n30\nand\nRK(TP )≤ qpmax ( 1 +HdmaxdE(F , (KH)−1) + 4 √ βKdE(F , (KH)−1)KH ) +\n4qpmaxdmax KH . (44)\nAlso, as has been shown in Proposition 7 of Russo and Van Roy (2014), we have the following relation between βk and Kolmogorov dimension of F :\nβK(F , (KH)−2, (KH)−2) = 16σ2(1 + o(1) + dK(F)) log(KH),\nwhich gives βK =O(σ 2dK(F) log(HK)). Combining this with (44) gives the second statement of the theorem."
    }, {
      "heading" : "6.3. Other Technical Results",
      "text" : "Theorem 4 provides a regret bound for TP in a general scenario involving multiproduct pricing, covariates and reference effects. As a result of this generality, all other theoretical results of this paper can be derived using Theorem 4. For example a proof for Theorem 1 is as follows.\nProof of Theorem 1. The setting considered in Theorem 4 can be thought of a special case of the setting of Theorem 4 where q= 1 and all covariate vectors are the singleton ∀ k : zk = 1. Thus, the statement of Theorem 1 follows immediately from Theorem 4 by setting q = 1 and Z = {1} (i.e., the covariates effect can be neglected).\nThe following is a proof for Theorem 2. Proof of Theorem 2. The settin of Theorem 2 can be thought of as a special case of that of Theorem 4 where the number of products being sold at any episode is q = 1 and the demand function fθ is a linear function as in (21). Let F be the class of such demand functions spanned by the parameter θ. It is easy to see that under Assumptions 2 and 3, we have (see, for example, Proposition 2 of Osband and Van Roy (2014))\ndK(F) =O(mn2), dE(F , ) =O(mn2 log(npmaxλτ/ ).\nThe statement then follows from Theorem 4.\nThe following is a proof of Theorem 3. Proof of Theorem 3. The setting of Theorem 3 can be thought of as a special case of that of Theorem 4 with Z = {1} such that the covariates do not matter. Then, with F as the class of demand function as defined in (27), it is easy to see that (see, for example, Proposition 2 of Osband and Van Roy (2014)) under Assumptions 2 and 4\ndK(F) =O(q2n2), dE(F , ) =O(q2n2 log(nqpmaxτ/ )).\nthe statement then follows from Theorem 4.\n31"
    }, {
      "heading" : "7. Concluding Remarks",
      "text" : "We studied the problem of dynamic pricing in an unknown environment in the presence of reference\neffects. Our framework accommodates contexts in which consumers make purchase decisions based\non price histories, not only prevailing prices. The fact that prices impose delayed consequences\nintroduces challenges that call for more sophisticated pricing strategies. In particular, the seller\ncan learn to influence consumer behavior by judiciously sequencing prices.\nTo address this challenge, we formulated the dynamic pricing problem in terms of reinforcement\nlearning. In our framework, the demand for an item depends on its current price, price history, and\nunknown parameters of a demand model. We allow for arbitrary demand functions and propose\nThompson Pricing (TP) as a heuristic for addressing the problem. We provided a very general regret\nbound on the performance of TP in terms of the eluder dimension and Kolmogorov dimensions of\nthe demand function class.\nWe also presented extensions of TP that address contexts with observable demand covariates,\nmultiproduct pricing, and varying and overlapping sales cycles. We provided a general performance\nanalysis of the TP, which specializes to offer regret bounds for each of the aforementioned scenarios."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors would like to thank Mohsen Bayati for his constructive feedback and discussion on this paper. This work was generously supported by a Stanford Graduate Fellowship."
    } ],
    "references" : [ {
      "title" : "Pricing and manufacturing decisions when demand is a function",
      "author" : [ "Hs Ahn", "M Gümüs", "P Kaminsky" ],
      "venue" : null,
      "citeRegEx" : "Ahn et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Ahn et al\\.",
      "year" : 2007
    }, {
      "title" : "A (2012) Blind network revenue management. Operations research 60(6):1537–1550",
      "author" : [ "Besbes O", "Zeevi" ],
      "venue" : "Bitran G, Caldentey R, Mondschein S",
      "citeRegEx" : "O and Zeevi,? \\Q1998\\E",
      "shortCiteRegEx" : "O and Zeevi",
      "year" : 1998
    }, {
      "title" : "Online network revenue management using thompson sampling",
      "author" : [ "KJ Ferreira", "D Simchi-Levi", "H Wang" ],
      "venue" : null,
      "citeRegEx" : "Ferreira et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ferreira et al\\.",
      "year" : 2015
    }, {
      "title" : "Explicit solutions of optimization models and differential games",
      "author" : [ "G Fibich", "A Gavious", "O Lowengart" ],
      "venue" : null,
      "citeRegEx" : "Fibich et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Fibich et al\\.",
      "year" : 2003
    }, {
      "title" : "A multiproduct dynamic pricing problem and its applications to network",
      "author" : [ "G Gallego", "G Van Ryzin" ],
      "venue" : null,
      "citeRegEx" : "Gallego and Ryzin,? \\Q1997\\E",
      "shortCiteRegEx" : "Gallego and Ryzin",
      "year" : 1997
    }, {
      "title" : "Asymmetric reference price effects and dynamic pricing policies",
      "author" : [ "PK Kopalle", "AG Rao", "JL Assuncao" ],
      "venue" : null,
      "citeRegEx" : "Kopalle et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Kopalle et al\\.",
      "year" : 1996
    }, {
      "title" : "Pricing and learning with uncertain demand",
      "author" : [ "MS Lobo", "S Boyd" ],
      "venue" : "INFORMS Revenue Management",
      "citeRegEx" : "Lobo and Boyd,? \\Q2003\\E",
      "shortCiteRegEx" : "Lobo and Boyd",
      "year" : 2003
    }, {
      "title" : "Reference price research: Review and propositions",
      "author" : [ "T Conference. Mazumdar", "S Raj", "I Sinha" ],
      "venue" : "Journal of marketing",
      "citeRegEx" : "Mazumdar et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Mazumdar et al\\.",
      "year" : 2005
    }, {
      "title" : "More) efficient reinforcement learning via posterior sampling",
      "author" : [ "I Osband", "D Russo", "B Van Roy" ],
      "venue" : null,
      "citeRegEx" : "Osband et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Osband et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning to optimize via posterior sampling",
      "author" : [ "D Russo", "B Van Roy" ],
      "venue" : "Mathematics of Operations Research",
      "citeRegEx" : "429",
      "shortCiteRegEx" : "429",
      "year" : 2014
    }, {
      "title" : "A Bayesian framework for reinforcement learning",
      "author" : [ "M Strens" ],
      "venue" : null,
      "citeRegEx" : "Strens,? \\Q2000\\E",
      "shortCiteRegEx" : "Strens",
      "year" : 2000
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Mazumdar et al. (2005) provides a comprehensive survey that covers both behavioral research that provides evidence and examines the structure of reference effects and methodological research on how pricing strategies should respond.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 4,
      "context" : "Mazumdar et al. (2005) provides a comprehensive survey that covers both behavioral research that provides evidence and examines the structure of reference effects and methodological research on how pricing strategies should respond. Strategies for particular model classes have been developed in Greenleaf (1995), Kopalle et al.",
      "startOffset" : 0,
      "endOffset" : 313
    }, {
      "referenceID" : 3,
      "context" : "Strategies for particular model classes have been developed in Greenleaf (1995), Kopalle et al. (1996), Fibich et al.",
      "startOffset" : 81,
      "endOffset" : 103
    }, {
      "referenceID" : 2,
      "context" : "(1996), Fibich et al. (2003), Ahn et al.",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 0,
      "context" : "(2003), Ahn et al. (2007), Popescu and Wu (2007), Heidhues and Kőszegi (2014).",
      "startOffset" : 8,
      "endOffset" : 26
    }, {
      "referenceID" : 0,
      "context" : "(2003), Ahn et al. (2007), Popescu and Wu (2007), Heidhues and Kőszegi (2014).",
      "startOffset" : 8,
      "endOffset" : 49
    }, {
      "referenceID" : 0,
      "context" : "(2003), Ahn et al. (2007), Popescu and Wu (2007), Heidhues and Kőszegi (2014). However, these papers treat the problem of pricing given known demand models, with no learning required.",
      "startOffset" : 8,
      "endOffset" : 78
    }, {
      "referenceID" : 0,
      "context" : "(2003), Ahn et al. (2007), Popescu and Wu (2007), Heidhues and Kőszegi (2014). However, these papers treat the problem of pricing given known demand models, with no learning required. Electronically-mediated markets, the increasing availability of data, and advances in the field of machine learning have fueled a vast and growing literature on learning to price. We refer the reader to den Boer (2015) for a comprehensive review of the literature and research directions.",
      "startOffset" : 8,
      "endOffset" : 403
    }, {
      "referenceID" : 2,
      "context" : "In particular, Ferreira et al. (2015) considers an approach based on Thompson sampling to address a multiproduct pricing problem with resource constraints, though without reference effects.",
      "startOffset" : 15,
      "endOffset" : 38
    }, {
      "referenceID" : 2,
      "context" : "In particular, Ferreira et al. (2015) considers an approach based on Thompson sampling to address a multiproduct pricing problem with resource constraints, though without reference effects. Our pricing strategy is based on the posterior sampling reinforcement learning (PSRL) algorithm, originally proposed by Strens Strens (2000) under the name Bayesian Dynamic Programming, as a heuristic for reinforcement learning in Markov decision processes.",
      "startOffset" : 15,
      "endOffset" : 331
    }, {
      "referenceID" : 2,
      "context" : "In particular, Ferreira et al. (2015) considers an approach based on Thompson sampling to address a multiproduct pricing problem with resource constraints, though without reference effects. Our pricing strategy is based on the posterior sampling reinforcement learning (PSRL) algorithm, originally proposed by Strens Strens (2000) under the name Bayesian Dynamic Programming, as a heuristic for reinforcement learning in Markov decision processes. Building on general results established in Russo and Van Roy (2014) for Thompson sampling, regret analyses for PSRL are developed in Osband et al.",
      "startOffset" : 15,
      "endOffset" : 516
    }, {
      "referenceID" : 2,
      "context" : "In particular, Ferreira et al. (2015) considers an approach based on Thompson sampling to address a multiproduct pricing problem with resource constraints, though without reference effects. Our pricing strategy is based on the posterior sampling reinforcement learning (PSRL) algorithm, originally proposed by Strens Strens (2000) under the name Bayesian Dynamic Programming, as a heuristic for reinforcement learning in Markov decision processes. Building on general results established in Russo and Van Roy (2014) for Thompson sampling, regret analyses for PSRL are developed in Osband et al. (2013), Osband and Van Roy (2014).",
      "startOffset" : 15,
      "endOffset" : 602
    }, {
      "referenceID" : 2,
      "context" : "In particular, Ferreira et al. (2015) considers an approach based on Thompson sampling to address a multiproduct pricing problem with resource constraints, though without reference effects. Our pricing strategy is based on the posterior sampling reinforcement learning (PSRL) algorithm, originally proposed by Strens Strens (2000) under the name Bayesian Dynamic Programming, as a heuristic for reinforcement learning in Markov decision processes. Building on general results established in Russo and Van Roy (2014) for Thompson sampling, regret analyses for PSRL are developed in Osband et al. (2013), Osband and Van Roy (2014). In principle, results of Osband and Van Roy (2014) apply to the problem we consider in this paper, but the associated regret bound depends on a Lipschitz constant which is not clear how to characterize in our context.",
      "startOffset" : 15,
      "endOffset" : 629
    }, {
      "referenceID" : 2,
      "context" : "In particular, Ferreira et al. (2015) considers an approach based on Thompson sampling to address a multiproduct pricing problem with resource constraints, though without reference effects. Our pricing strategy is based on the posterior sampling reinforcement learning (PSRL) algorithm, originally proposed by Strens Strens (2000) under the name Bayesian Dynamic Programming, as a heuristic for reinforcement learning in Markov decision processes. Building on general results established in Russo and Van Roy (2014) for Thompson sampling, regret analyses for PSRL are developed in Osband et al. (2013), Osband and Van Roy (2014). In principle, results of Osband and Van Roy (2014) apply to the problem we consider in this paper, but the associated regret bound depends on a Lipschitz constant which is not clear how to characterize in our context.",
      "startOffset" : 15,
      "endOffset" : 681
    }, {
      "referenceID" : 2,
      "context" : "In particular, Ferreira et al. (2015) considers an approach based on Thompson sampling to address a multiproduct pricing problem with resource constraints, though without reference effects. Our pricing strategy is based on the posterior sampling reinforcement learning (PSRL) algorithm, originally proposed by Strens Strens (2000) under the name Bayesian Dynamic Programming, as a heuristic for reinforcement learning in Markov decision processes. Building on general results established in Russo and Van Roy (2014) for Thompson sampling, regret analyses for PSRL are developed in Osband et al. (2013), Osband and Van Roy (2014). In principle, results of Osband and Van Roy (2014) apply to the problem we consider in this paper, but the associated regret bound depends on a Lipschitz constant which is not clear how to characterize in our context. We instead build directly on the technical tools of Russo and Van Roy (2014) and Osband and Van Roy (2014) to derive custom regret bounds for PSRL in our pricing model.",
      "startOffset" : 15,
      "endOffset" : 925
    }, {
      "referenceID" : 2,
      "context" : "In particular, Ferreira et al. (2015) considers an approach based on Thompson sampling to address a multiproduct pricing problem with resource constraints, though without reference effects. Our pricing strategy is based on the posterior sampling reinforcement learning (PSRL) algorithm, originally proposed by Strens Strens (2000) under the name Bayesian Dynamic Programming, as a heuristic for reinforcement learning in Markov decision processes. Building on general results established in Russo and Van Roy (2014) for Thompson sampling, regret analyses for PSRL are developed in Osband et al. (2013), Osband and Van Roy (2014). In principle, results of Osband and Van Roy (2014) apply to the problem we consider in this paper, but the associated regret bound depends on a Lipschitz constant which is not clear how to characterize in our context. We instead build directly on the technical tools of Russo and Van Roy (2014) and Osband and Van Roy (2014) to derive custom regret bounds for PSRL in our pricing model.",
      "startOffset" : 15,
      "endOffset" : 955
    }, {
      "referenceID" : 2,
      "context" : "This can be viewed as a structured bandit learning problem, and several pricing algorithms have been proposed for variations of the problem Ferreira et al. (2015), Kincaid and Darling (1963), Gallego and Van Ryzin (1994, 1997), Bitran et al.",
      "startOffset" : 140,
      "endOffset" : 163
    }, {
      "referenceID" : 2,
      "context" : "This can be viewed as a structured bandit learning problem, and several pricing algorithms have been proposed for variations of the problem Ferreira et al. (2015), Kincaid and Darling (1963), Gallego and Van Ryzin (1994, 1997), Bitran et al.",
      "startOffset" : 140,
      "endOffset" : 191
    }, {
      "referenceID" : 2,
      "context" : "This can be viewed as a structured bandit learning problem, and several pricing algorithms have been proposed for variations of the problem Ferreira et al. (2015), Kincaid and Darling (1963), Gallego and Van Ryzin (1994, 1997), Bitran et al. (1998), Besbes and Zeevi (2009), Araman and Caldentey (2009), Farias and Van Roy (2010), Besbes and Zeevi (2012), Lobo and Boyd (2003).",
      "startOffset" : 140,
      "endOffset" : 249
    }, {
      "referenceID" : 2,
      "context" : "This can be viewed as a structured bandit learning problem, and several pricing algorithms have been proposed for variations of the problem Ferreira et al. (2015), Kincaid and Darling (1963), Gallego and Van Ryzin (1994, 1997), Bitran et al. (1998), Besbes and Zeevi (2009), Araman and Caldentey (2009), Farias and Van Roy (2010), Besbes and Zeevi (2012), Lobo and Boyd (2003).",
      "startOffset" : 140,
      "endOffset" : 274
    }, {
      "referenceID" : 2,
      "context" : "This can be viewed as a structured bandit learning problem, and several pricing algorithms have been proposed for variations of the problem Ferreira et al. (2015), Kincaid and Darling (1963), Gallego and Van Ryzin (1994, 1997), Bitran et al. (1998), Besbes and Zeevi (2009), Araman and Caldentey (2009), Farias and Van Roy (2010), Besbes and Zeevi (2012), Lobo and Boyd (2003).",
      "startOffset" : 140,
      "endOffset" : 303
    }, {
      "referenceID" : 2,
      "context" : "This can be viewed as a structured bandit learning problem, and several pricing algorithms have been proposed for variations of the problem Ferreira et al. (2015), Kincaid and Darling (1963), Gallego and Van Ryzin (1994, 1997), Bitran et al. (1998), Besbes and Zeevi (2009), Araman and Caldentey (2009), Farias and Van Roy (2010), Besbes and Zeevi (2012), Lobo and Boyd (2003).",
      "startOffset" : 140,
      "endOffset" : 330
    }, {
      "referenceID" : 2,
      "context" : "This can be viewed as a structured bandit learning problem, and several pricing algorithms have been proposed for variations of the problem Ferreira et al. (2015), Kincaid and Darling (1963), Gallego and Van Ryzin (1994, 1997), Bitran et al. (1998), Besbes and Zeevi (2009), Araman and Caldentey (2009), Farias and Van Roy (2010), Besbes and Zeevi (2012), Lobo and Boyd (2003).",
      "startOffset" : 140,
      "endOffset" : 355
    }, {
      "referenceID" : 2,
      "context" : "This can be viewed as a structured bandit learning problem, and several pricing algorithms have been proposed for variations of the problem Ferreira et al. (2015), Kincaid and Darling (1963), Gallego and Van Ryzin (1994, 1997), Bitran et al. (1998), Besbes and Zeevi (2009), Araman and Caldentey (2009), Farias and Van Roy (2010), Besbes and Zeevi (2012), Lobo and Boyd (2003). In the memoryless demand model we have described, the way in which consumers respond to a current price does not depend on price history.",
      "startOffset" : 140,
      "endOffset" : 377
    }, {
      "referenceID" : 2,
      "context" : "This can be viewed as a structured bandit learning problem, and several pricing algorithms have been proposed for variations of the problem Ferreira et al. (2015), Kincaid and Darling (1963), Gallego and Van Ryzin (1994, 1997), Bitran et al. (1998), Besbes and Zeevi (2009), Araman and Caldentey (2009), Farias and Van Roy (2010), Besbes and Zeevi (2012), Lobo and Boyd (2003). In the memoryless demand model we have described, the way in which consumers respond to a current price does not depend on price history. In reality, reference effects play a substantial role in purchase decisions Mazumdar et al. (2005). For example, offering a discount often increases demand not only because the new price is low, but also because it is lower than the previous price.",
      "startOffset" : 140,
      "endOffset" : 615
    }, {
      "referenceID" : 2,
      "context" : "The above memoryless pricing strategy, which performs near optimally in memoryless environments Ferreira et al. (2015), will drastically fail in the presence of reference effects.",
      "startOffset" : 96,
      "endOffset" : 119
    }, {
      "referenceID" : 2,
      "context" : "Pricing strategies that have been proposed in the literature for memoryless demand models achieve a similar per-episode regret rate in the absence of reference effects Ferreira et al. (2015). This indicates that although dynamic pricing with reference effects entails additional challenges, TP performs efficiently in that context with no additional cost in terms of the regret rate.",
      "startOffset" : 168,
      "endOffset" : 191
    }, {
      "referenceID" : 8,
      "context" : "Analysis of TP Our proposed TP algorithm is an adaptation of PSRL algorithm proposed in Osband et al. (2013), and Osband and Van Roy (2014) provides a regret bound on its performance when employed",
      "startOffset" : 88,
      "endOffset" : 109
    }, {
      "referenceID" : 8,
      "context" : "Analysis of TP Our proposed TP algorithm is an adaptation of PSRL algorithm proposed in Osband et al. (2013), and Osband and Van Roy (2014) provides a regret bound on its performance when employed",
      "startOffset" : 88,
      "endOffset" : 140
    } ],
    "year" : 2017,
    "abstractText" : "As a firm varies the price of a product, consumers exhibit reference effects, making purchase decisions based not only on the prevailing price but also the product’s price history. We consider the problem of learning such behavioral patterns as a monopolist releases, markets, and prices products. This context calls for pricing decisions that intelligently trade off between maximizing revenue generated by a current product and probing to gain information for future benefit. Due to dependence on price history, realized demand can reflect delayed consequences of earlier pricing decisions. As such, inference entails attribution of outcomes to prior decisions and effective exploration requires planning price sequences that yield informative future outcomes. Despite the considerable complexity of this problem, we offer a tractable systematic approach. In particular, we frame the problem as one of reinforcement learning and leverage Thompson sampling. We also establish a regret bound that provides graceful guarantees on how performance improves as data is gathered and how this depends on the complexity of the demand model. We illustrate merits of the approach through simulations.",
    "creator" : "LaTeX with hyperref package"
  }
}