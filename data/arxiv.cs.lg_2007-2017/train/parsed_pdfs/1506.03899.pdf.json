{
  "name" : "1506.03899.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Place classification with a graph regularized deep neural network model",
    "authors" : [ "Yiyi Liao", "Sarath Kodagoda", "Yue Wang", "Lei Shi", "Yong Liu" ],
    "emails" : [ "yongliu@iipc.zju.edu.cn)." ],
    "sections" : [ {
      "heading" : null,
      "text" : "I. INTRODUCTION\nPlace classification is one of the important problems in human-robot interactions and mobile robotics, which aims to distinguish differences between environmental locations and assign a label (corridor, office, kitchen, etc.) to each location [33], [17]. It allows robots to achieve spatial awareness through semantic understanding rather than having to rely on precise coordinates in communicating with humans. Furthermore, the semantic labels has the potential to efficiently facilitate other robotic functions such as mapping [20], behavior-based navigation [16], task planning [7] and active object search and rescue [1].\nIn general, place classification is carried out through environment sensing. Laser range finders, cameras and RGBD sensors are the mostly used sensing modalities. Location and topological information can also be informative in place classification. In this work, it is attempted to exploit both the sensory data and location information. We assume all the maps in this paper contain these two parts of information and some of the maps are labeled with human knowledge. Then\n1Yiyi Liao and Yue Wang are with the Institute of Cyber-Systems and Control, Zhejiang University, Zhejiang, 310027, China.\n2Sarath Kodagoda and Lei Shi are with the Centre for Autonomous Systems (CAS), The University of Technology, Sydney, Australia.\n3Yong Liu is with the State Key Laboratory of Industrial Control Technology and Institute of Cyber-Systems and Control, Zhejiang University, Zhejiang, 310027, China (He is the corresponding author of this paper, email: yongliu@iipc.zju.edu.cn).\nthe place classification problem can be stated as predicting the labels of new environments given the labeled maps.\nBy analysing those two forms of data, sensory data and location information, we can gain insights into the characteristics of the place classification problem. Raw sensory data encode the environment information at different locations which can provide discriminative information between different classes. However, this requires an effective feature extraction method and most of the previous works tend to extract hand-engineered features from the raw data [15], [27]. Our opinion is that the hand held features, may not fully exploit the potential to achieve higher generalization ability. On the other hand, the locations encode the spatial information of the environment and indicate the local consistency of the labels, which means the positions at spatial proximity have higher probability to having the same class labels.\nIt is to be noted that another difficulty in place classification is the influence of different field of views (FOV) of the sensors used. For example, if a laser range finder collects 180◦ FOV data facing approximately to a corner of a corridor, it may not contain enough information for classification. If the laser range finder collects 360◦ FOV data at a door of an office room, the robot might be confused to with mixed information from two classes.\nIn order to address these problems, in this paper, we propose a graph regularized deep learning approach classification on multi-layer inputs. The pipeline of our system is illustrated in Fig. 1, which can be split into three parts:\n1) Construction of multi-layer inputs: The environmental information in this paper is represented through the generalized Voronoi graph (GVG) [4], a topological graph in which the nodes correspond to the sensory data and the edges denote the relationships. By fusing the information and eliminating the end-nodes, we implement a recursive algorithm to construct multi-layer inputs with hierarchical GVGs. The inputs of higher layers contain information of larger field of view, represented by increasingly succinct GVG. The features are extracted from each layer of input and classified independently.\n2) The graph regularized deep architecture for feature learning and classification: We adopt the deep architecture that learns features from the raw data automatically. A graph regularizer is imposed to the deep architecture to keep the local consistency, where an adjacency graph is constructed to depict the adjacency and similarity between the samples. Our training map and testing maps are fed into the deep architecture for feature learning at the same time, which forms a semi-supervised learning framework. The output of this step is the predicted labels of different layers.\nar X\niv :1\n50 6.\n03 89\n9v 1\n[ cs\n.R O\n] 1\n2 Ju\nn 20\n15\n3) The confidence tree for decision making: After receiving the classification results of multi-layer inputs, confidence trees are constructed according to the topological graph, and a decision making process is carried out to maximize the overall confidence.\nThe remainder of this paper is organized as follows: Section II reviews the related literature. In Section III, we introduce the construction of our multi-layer inputs and the confidence tree for decision making. The semisupervised classification with graph regularization is given in Section IV. Experimental results are presented in Section V to validate the effectiveness of our end-to-end classification framework. Then the paper is concluded in Section VI."
    }, {
      "heading" : "II. RELATED WORKS",
      "text" : "There are various sensors that help robots to sense environments, such as cameras and laser range finders. Previous works have demonstrated the effectiveness of both camera data and laser range finder data for classifying places. For example, Shi et al. [26] and Viswanathan et al. [31] extracted features from the vision data, while Mozos et al. [15] and Sousa et al. [27] classified the places based on laser range data. In this paper, we focus on the place classification based on laser range data, however, our approach can be easily extended to other modality of sensors such as vision data.\nLaser range finders can provide nonnegative beam sequences describing range and bearing to nearby objects. They contain structural information including clutter in the environment. Mozos et al. [15] extracted features from the 360◦ laser range data and those features were fed into an Adaboost classifier to label the environment. Sousa et al. [27] reported superior results on a binary classification task using a subset of above mentioned features, and the support vector machine as the classifier. In our past work, we implemented a logistic regression based classifier, as a binary and multiclass problem contributing to higher accuracies [24], [25]. The work was further extended to address the generalizability of the solution through a semi-supervised place classification over a generalized Voronoi graph (SPCoGVG) [23]. In all of\nthese methods, the features were extracted from the laser range data based on statistical and geometrical information, or so-called hand-engineered features. For instance, the average and the standard deviation of the beam length, the area and perimeter of the polygon specified by the observed range data and bearing were included in the feature set.\nRecently, the unsupervised feature learning has drawn much attention as the deep learning methods was developed [12], [11], [2]. The deep learning methods achieved remarkable results in many areas, including object recognition [3], [13], natural language processing [5], [8] and speech recognition [6], which demonstrated that discovering and extracting features automatically can usually achieve better results on representation learning [22], [21], [30]. Inspired by the success of unsupervised feature learning, in this article we present an end-to-end framework with the deep learning method that can learn features automatically from the laser range data.\nWe also exploit the local consistency of classes with the assumption that samples located in the same small region are more likely to have the same labels. Previous research has included this particular characteristic for performance promotion and many studies were carried out with consideration of the local consistency [20], [18], [15], [14], [19].\nIn this paper, we consider the local consistency during the feature learning process, where, the features learn to keep the local invariance with a graph regularization. There are some similar works on implementing the graph regularized deep learning models [9], [32]. Both [9] and [32] utilized a margin-based loss function proposed by Hadsell et at. [10]. These works have demonstrated the effectiveness of the graph embedding in dimensionality reduction and image classification."
    }, {
      "heading" : "III. MULTI-LAYER CONSTRUCTION AND DECISION MAKING",
      "text" : "In this paper, we assume a laser range finder with a typical field of view of 180◦. This is a limited field of view which can give rise to many classification inaccuracies due\nto lack of crucial information. However, the full field of view may also lead to misclassifications at the boundaries of the two different classes of places. Therefore, considering these problems, we propose to construct multi-layer inputs for classification followed by fusion of the results."
    }, {
      "heading" : "A. Construction of Multi-layer Inputs",
      "text" : "1) Data Representation on GVG: In this paper, our multilayer inputs is represented by the hierarchical generalized Voronoi graph (GVG) [4], a topological graph which has been successfully applied to navigation, localization and mapping. The general representation of GVG is composed of meet-points (locations of three-way or more equidistance to obstacles) and edges (feasible paths between meet-points which are two-way equidistance to obstacles) [28]. We adopt the same resolution as in our previous work [23] to construct the first layer GVG, and then higher layers of GVGs are constructed to describe the environment at different levels of granularity.\nLet’s denote hierarchical GVGs as 〈G(1), G(2), · · · , G(L)〉 with G(l) = {V (l), E(l)}, where L denotes the number of layer, V (l) denotes nodes in layer l and E(l) denotes edges in layer l. For each layer, the independent sensing information is carried by the nodes in V (l), and the local connectivity is represented by the edges in V (l). More specifically, each node v(l)i ∈ V (l) corresponds to a sequence of range data r (l) i , assigned with the label y (l) i for the training maps, while e (l) ij ∈ E(l) reveals the connection between nodes v (l) i and v (l) j with distance d (l) ij .\nThe first layer G(1) = {V (1), E(1)} describes the environment in most detailed level of granularity with the originally adopted laser range data. As the laser range finder is of 180◦ field of view with 1◦ angular resolution, each node v (1) i ∈ V (1) corresponds to a sequence of range data r (1) i with 180 dimension. 2) Recursive Higher Layer Construction Algorithm: The construction of a higher layer GVG is implemented by fusing the information carried by connected nodes and then eliminating those marginal nodes. Algorithm 1 demonstrates the process of building higher layer GVG from a given lower layer. We make some definitions here for better explanation of the algorithm. N(vi) is defined as the directly connected neighbour set of vi, then vj ∈ N(vi) means there is an edge eij ∈ E between vi and vj . In addition, numel(N) is defined as the number of elements contained in N . Then numel(N(vi)) = 1 means vi is an “end-node”, i.e. the node without children. Further define M(vi) as the set of endnodes connected to vi, which is obviously M(vi) ⊆ N(vi). As seen from Algorithm 1, the construction process fuses the information carried by vi’s neighbors if vi is not an endnode (detailed fusion process is given in section III-A.3), otherwise vi is eliminated from the higher layer.\nThe L layer of data can be generated by recursively applying Algorithm 1 for L−1 times, which means by taking the output of lth layer as the input of (l + 1)th layer. This process can be illustrated in Figure 2 with L = 3. In this\nAlgorithm 1: Generate higher layer of input from the previous layer.\nInput: G(l) = {V (l), E(l)}, the range data r(l)i on each node v(l)i Output: G(l+1) = {V (l+1), E(l+1)}, the range data r (l+1) i on each node v (l+1) i\n1 for v(l)i ∈ V (l) do 2 if numel(N(v(l)i )) > 1 then 3 Preserve v(l)i , i.e. v (l+1) i = v (l) i ; 4 Construct r(l+1)i and r̂ (l+1) i from r (l) i and all of\nthe r(l)j carried by v (l) j ∈ N(v (l) i );\n5 end 6 for v(l)j ∈ N(v (l) i ) do 7 if v(l)j ∈M(v (l) i ) then 8 Eliminate e(l)ij and v (l) j ;\n9 else 10 Preserve e(l)ij , i.e. e (l+1) ij = e (l) ij ; 11 end 12 end 13 end\nexample, the end-nodes are denoted as red. It is to be noted that when moving to higher layers, the number of nodes in each layer decreases with the elimination of the end-nodes. More details are given in the caption of Figure 2.\nAn illustration of the different G(l) = {V (l), E(l)}, l = 1, 2, 3 layers constructed from a specific map is given in Figure 3. In the first layer, the nodes are distributed more densely in the map. When approaching higher layers, the tree structure represents more and more abstract information. It is to be noted that the number of the end-nodes (denoted as red asterisks) decreases as the progression of the layers which is a consideration for choosing the L = 3 in our experiments.\n3) Data generation: This section describes the details about the construction of the higher-layer range data r(l+1)i and r̂(l+1)i , where the latter is generated from the former with fixed length. As stated in Algorithm 1, given v(l)i satisfying numel(N(v\n(l) i )) > 1 (i.e. v (l) i is not end-node), range data\nreceived at the respective nodes are integrated to achieve a better perception.\nGiven each v(l)i with numel(N(v (l) i )) > 1, firstly a local map is generated using occupancy grid mapping [29] based on the respective range data in lth layer, including r (l) j carried by v (l) j ∈ N(v (l) i ) and r (l) i . This is achieved by transforming all r(l)j to r (l) i ’s coordinate frame, which assumes the knowledge of the global robot poses at all times. In this local map, a virtual scan r(l+1)i is then generated by applying ray casting at position v(l)i with 1\n◦ angular resolution, which is the same as the setting of the real laser range finder.\nAs the dimensions of the fused range data r(l+1)i could be different in various nodes, linear interpolation on the data is\nthen performed to keep same dimension of data throughout the process. This leads to an sequence r̂(l+1)i with fixed dimension of 360.\nAcknowledging the fact that the interpolated points may not contain high information, a completeness rate, which is the proportion of the laser measured data (dimension of r(l)i to the whole 360◦ data (dimension of r̂(l)i ) is defined as:\nq (l) i =\nlength(r (l) i ) length(r̂ (l) i )\n(1)\nwhere l = 2 · · ·L. This measure is used in the decision making process which is discussed in the next section, thus we denote q(1)i = 180/360 = 0.5 for uniformity when l = 1. However, we don’t apply linear interpolation to the layer 1 since the initial laser range data r(1)i always has the same dimension of 180 and is not necessary for linear interpolation. By applying this data pre-processing approach, the laser range data in layer 2 to layer L are kept in the fixed length of 360. Note that it is always r(l)i which is employed to construct the next layer, rather than the pre-processed r̂(l)i .\nAs an example, Figure 4 illustrates the construction of a sequence of input in layer 2 using the corresponding inputs in layer 1, followed by the result after linear interpolation. The details are given in the caption of Figure 4."
    }, {
      "heading" : "B. Decision on Multi-layer Results",
      "text" : "1) Construction of the Confidence Tree: With the L layer of inputs, we can obtain the predicted labels from L independent classifiers, which can be formed to be confidence trees with L layers shown in Figure 5(a), where each node denotes the predicted label ŷ(l)i of v (l) i and its corresponding confidence c(l)i . By maximizing the overall confidence of\nAlgorithm 2: Decision making on the confidence trees.\nInput: Confidence trees where each node v(l)i denotes the predicted label ŷ(l)i and the corresponding confidence c(l)i . Output: Optimized labels of leaf nodes ŷ(1)i∗ .\n1 Initialize c(1)i∗ = c (1) i and ŷ (1) i∗ = ŷ (1) i ; 2 for l = 2 · · ·L do 3 for v(l)i ∈ V (l) do 4 Average the optimized confidence of v(l)i ’s\nchildren v(l−1)j as 1 ni ∑ j c (l−1) j∗ ;\n5 if 1ni ∑ j c (l−1) j∗ > c (l) i then 6 Denote c(l)i∗ = 1 ni ∑ j c (l−1) j∗ ; 7 else 8 Denote c(l)i∗ = c (l) i ; 9 All descendants of v(l)i are assigned with the label ŷ(l)i∗ .\n10 end 11 end 12 end\neach tree structure, it is intended to obtain higher accuracy in classification.\nAll of these tree structures are built from the dependencies in Algorithm 1 except for some minor difference — during the construction of these tree structures, a parent node v(l+1)i owns its children v(l)i and v (l) j ∈ M(v (l) i ), while the range data of v(l+1)i is constructed from the range data carried by v (l) i and v (l) j ∈ N(v (l) i ). The reason is that for those nodes v (l) j ∈ N(v (l) i ) and v (l) j /∈ M(v (l) i ), they are also reserved in the higher layer as v(l+1)j and have their own predicted labels, so we don’t consider the influence of v(l+1)i to them. It is to be noted that the number of such tree structures is equal to the number of nodes left in the layer L, where the v (L) i are the root nodes of these trees.\nIn our framework, two factors are considered when computing the confidence c(l)i , one is the probability p (l) i obtained from the classifier for labeling ŷ(l)i and the other is the completeness ratio q(l)i obtained from the input sequence r (l) i which is given in (1). Then the confidence c(l)i is constructed as:\nc (l) i = p (l) i × q (l) i (2)\n2) Decision Algorithm: With the confidence trees denoting the predicted label ŷ(l)i and its corresponding confidence c (l) i for each given v (l) i , the aim of decision making is then to search the optimized confidence c(l)i∗ and assign the optimized label ŷ(l)i∗ to each node, leading to the maximum value of the overall confidence.\nIn each tree structure, we make decisions from children to parents while comparing two consecutive layers based on the decision Algorithm 2. It is to be noted that for the comparison\nbetween layer l and layer l− 1, the confidence of the parent v (l) i is always compared to the average optimized confidence of its children v(l−1)j and we assume the optimized confidences in layer 1 are known as the original confidences. As for the optimized predicted labels, Algorithm 2 tells that they are only changed to follow their ancestor when this ancestor beats its children in confidence. In other words, if none ancestor of a leaf node gain advantages in confidence, then this leaf node would keep the initial label ŷ(1)i as its optimized label ŷ(1)i∗ . Note that although we can obtain the optimized labels for all nodes from this decision algorithm, only the labels of the leaf nodes are exported as output since the classification performance is evaluated based on these leaf nodes. An example is given in Figure 5(b) for better clarity. We can also evaluate the results obtained from those L independent classifiers separately with the help of these constructed trees. To ensure the fairness, results obtained from different layer of classifiers are all compared on the accuracy of bottom layer. Obviously, the results observed from the input of layer 1 do not need to be modified while the higher layers should spread their predicted labels to the bottom layer. Given a specific layer l (l > 1), all of the nodes on the bottom layer are assigned with the same label as their ancestor in layer l. For example, as shown in Figure 5(b), the v(l)1 , v (l) 2 , · · · , v (l) 5 will be labeled by the v (3) 1 ’s predicted label when we evaluate the results of layer 3."
    }, {
      "heading" : "IV. SEMI-SUPERVISED LEARNING AND CLASSIFICATION",
      "text" : "We have introduced the construction of multi-layer inputs\nand decision making on the multi-layer results in Section\nIII. In this section, we discuss the classification problem of how to train on each layer with the input data and obtain the predicted labels of the testing maps. This is implemented by a deep learning structure, with the capability to automatically learn features from the raw input data. The L layer of inputs are trained through L independent deep learning modes as indicated in Figure 1, though, these models have the same structure with raw laser range data being the input and predicted labels being the output as shown in Figure 6. Thus the discussion below in this section is not confined to any specific layer and hence the superscripts are omitted. It is to be noted that our training process is semi-supervised since both the training map and the testing map are employed for model training, where only the labels of the training map are available. The semi-supervised learning process has the advantage of gaining richer information of data distribution, while keeping the spatial consistency as we will introduce in this chapter."
    }, {
      "heading" : "A. Semi-supervised Learning with Graph Regularization",
      "text" : "In the classification problem, we denote the training pairs as (Xl ∈ Rm×nl , Yl ∈ R1×nl ) as a convention, where m denotes the input dimension, nl denotes the number of training samples. Particularly, each column in Xl is a sequence of laser range data r, i.e. xil = ri. The testing data can be defined in the same way as Xu ∈ Rm×nu , where nu denotes the number of testing samples. Then the task of the classification problem is to obtain predicted labels of Xu given Xl and Yl. In addition, we denote X = [Xl Xu] ∈ Rm×n as the combination of training data and testing data with n = nl +nu, since X is fed into the model as a whole during our semi-supervised training process.\nAs illustrated in Figure 6, the input is firstly fed into a set of fixed parameters (denoted as red) to compute the differences between the consecutive beams in each raw scan, as the consecutive differences can also provide rich information to the place classification and is often employed for extracting geometric features in the previous works [15],\n[27]. In the practical experiments, we sort both of the input and consecutive differences to guarantee the rotational invariance.\nFrom this point on, both the input and output of this fixed layer are fed into the stacked auto-encoders for feature learning. Auto-encoder is the widely used structure for building deep architectures, which is composed of an encoder and a decoder. By feeding the representation learned from the previous encoder as the input into another autoencoder, we can obtain the stacked hidden representations as shown in Figure 6. Let’s denote sigmoid function as f(x) = 1/(1 + e−x), then the ith layer of encoder and decoder can be represented as follow:\nHi = f(WiHi−1 + bi)\nĤi−1 = f(W T i Hi + ci)\n(3)\nwhere Hi−1 and Ĥi−1 denote the input and its reconstruction, Hi denotes the hidden representation and Wi, bi, ci denote the weighted parameters respectively1. In this paper, the weights in each pair of encoder and decoder are tied together as shown in (3).\nFor each layer of auto-encoder, the unsupervised pretraining is applied to obtain better parameters than random initialization [12] by minimizing the reconstruction cost:\nJpre = 1\nm ‖Hi−1 − Ĥi−1‖2F (4)\nNote that the decoder is discarded after pre-training while the encoder is preserved. The hidden representation learned by the last auto-encoder can be regarded as the feature for the input to the classifier.\n1When i=1, Hi−1 is the raw input — the combination of X and its consecutive differences Xs.\nIn the work reported here, the softmax classifier is applied to the features learned from stacked auto-encoders for classification, which is formulated as follow:\npi = exp(wTi h)∑ j exp(w T j h)\n(5)\nwhere pi corresponds to the probability that the hidden representation vector h belongs to ith class.\nAfter pre-training and classification, back propagation can be used to fine-tune the whole learning process for further promotion, which means the parameters of preserved encoders and softmax are trained together. In order to keep the local consistency, we add a graph regularization term during fine-tuning to learned representation. The cost function of the fine-tuning is given as follow:\nJfine = Jlabel + Jgraph\n= 1\nnl nl∑ i=1 Jlabel(x i l, y i l) + λ n n∑ i=1 n∑ j=1 sij‖hi − hj‖2\n(6)\nwhere the first term corresponds to the prediction error of the training data, and the second term is the graph regularization. Here hi and hj are the outputs of the last hidden layer with respect to the inputs xi and xj (xi and xj are two arbitrary columns in X), and sij is the similarity measurement between the samples xi and xj that connected in GVG, which is an element of the adjacency graph S = [sij ]n×n. Figure 6 also illustrates the way our cost function work. The costs caused by the prediction error is imposed on the softmax classifier and then our graph regularization is imposed on the last hidden layer. So during the fine-tuning the Jlabel will influence all of the parameters, while Jgraph will only influence the parameters for feature learning."
    }, {
      "heading" : "B. Graph Regularization in Place Classification Problem",
      "text" : "As shown in (6), the learned features hi and hj with large weight sij will be pushed together with the graph regularization term. In this section, we describe the details about the construction of the adjacency graph S which can be built in two steps. Firstly we define the connected relationships between samples and then calculate their weights of the connected edges.\nIn the place classification problem, the connected relationships in the topological graph GVG are directly employed to the adjacency graph. Then the samples with close coordinates are forced to be represented by the features with close distances. As for the weights which corresponds to the strength of the graph regularization, it is inversely associated with two distances, i.e. the distance between coordinates and the distance between the input data, which can be formulated as:\nsij = α\ndij +\nβ\n‖xi − xj‖2 (7)\nwhere α and β are constant weights, dij denotes the Euclidean distance between the sample coordinates. The second term defines the Euclidean distance between the input data.\nThis weighting scheme dose not only evaluate the geometrical information, but also considers the closeness between inputs. For example, given an edge that connects two nodes belonging to corridor and office respectively, although dij is small, ‖xi − xj‖2 can be large. Therefore, these two nodes are not forced to be too close in the representation space however still keeps the discriminative information."
    }, {
      "heading" : "V. EXPERIMENTS",
      "text" : "To validate the effectiveness of our end-to-end multi-layer learning system, we conduct experiments on six data sets collected from six international university indoor environments (including the Centre for Autonomous Systems at the University of Technology, Sydney, several buildings in the University of Freiburg, the German Research Centre for Artificial Intelligence in Saarbruecken, and the Intel Lab in Seattle). As we stated previously, the robot collected range data at the GVG nodes using the 2D laser range finder which has a maximum range of 30m and a horizontal field of view of 180◦.\nIt is to be noted that the classes defined by humans can be somewhat vague and plentiful according to the different functions of places. However, the 2D range data do not contain enough discriminative information to classify all these human-designed classes. Therefore, after careful thinking, we consider 3 target classes as: Class 1-space designed for a small number of individuals including cubicle, office, printer room, kitchen, bathroom, stairwell and elevator; Class 2-space for group activities including meeting room and laboratory; Class 3-corridor.\nAmong these six data sets, two of them (Fr79 and Intellab) contain all of the 3 classes but the others contain only parts of these classes. We consider the leave-many-out training, which means one data set is utilized for training and others are used for testing. Therefore, we obtained two groups of results by training on Fr79 and Intellab respectively.\nThe feature learning and classification model for each layer of input is shown in Figure 6. Given the input X ∈ Rm×n, the dimension configuration for our learning model is m − m − 100 − 24 − 3, which means the consecutive differences layer has the same dimension as the input, and the dimension of our hidden layers are 100 and 24 respectively. Thus the dimension of our learned features is 24. Finally the output of our model represents a probabilistic measure of data belonging to each class. Thus the output dimension is the same as the number of our classes. In addition, since we perform the interpolation to fix the dimension of the higher layers as introduced in Section III-A.3, so we have m = 180 for L = 1, and m = 360 for L = 2, 3, · · · . In this paper, we choose L = 3."
    }, {
      "heading" : "A. Multi-layer Results without Graph Regularization",
      "text" : "We first conduct experiments to evaluate the performance of our multi-layer inputs. Table I and Table II shows the leave-many-out classification results training on Intellab and Fr79 respectively. It is to be noted that the graph regularization is not considered here and therefore, λ = 0 in the"
    }, {
      "heading" : "MULTI-LAYER RESULTS TRAINED ON INTELLAB.",
      "text" : ""
    }, {
      "heading" : "MULTI-LAYER RESULTS TRAINED ON FR79.",
      "text" : "cost function (6). In general, results of higher layers are better than that of lower layers due to the richer information contained in each node on the higher layers."
    }, {
      "heading" : "B. Multi-layer Results with Graph Regularization",
      "text" : "We also carried out experiments to validate the effectiveness of the graph regularization. The algorithm remains the same as previous settings, however, we changed the value of λ = 1 to add the graph regularization. In this experiments, we pay more attention to the geometrical neighborhood, thus we use α = 2/3 and β = 1/3 in (7) for the construction of the adjacency graph. The classification results are shown in Table III and Table IV, which are trained on Intellab and Fr79 respectively. The results have the similar trends as in Table I and Table II, where higher layers give rise to better accuracies. Further comparisons of Table I and Table III show that the feature learning with graph regularization performs better than without it. It reveals that the graph regularization has the advantage of improving classification performances by keeping the local consistency."
    }, {
      "heading" : "C. Fusion Results",
      "text" : "Finally, we show the accuracies of the multi-layer graph regularized method with fusion in Table V and Table VI. When compared with the results of each single layer as"
    }, {
      "heading" : "MULTI-LAYER RESULTS TRAINED ON INTELLAB WITH GRAPH REGULARIZATION.",
      "text" : "shown in Table III and Table IV, the fusion results achieved better accuracies. For the results trained on Intellab, the average accuracy of fusion results risen to 94.02% from L1:87.71%, L2:92.14% and L3:92.66%, and the results trained on Fr79 also reached 93.59% from L1:84.24%, L2:92.17% and L3:92.95%. The fused test results trained on Intellab are diagrammatically illustrated in Figure 7. It is to be noted that confusions between Class 1 (office room and other rooms) and Class 2 (meeting room) account for the major misclassifications especially in the test map of Fr79. The cause might be that Class 1 is featured with narrow environment including massive clutters while the Class 2 is featured with relatively larger spaces, therefore the corners of meeting room are mostly classified as office room and other rooms and some center positions of office room are assigned as office room.\nWe also make comparisons with the results we achieved in our previous work SPCoGVG [23]. SPCoGVG is also a semi-supervised approach, which is composed of support vector machine (SVM) and conditional random field (CRF) to ensure the generalization ability. We use the 24- dimensional hand-engineered features in SPCoGVG, which are extracted from the raw range data with geometrical knowledge. Notice that our learned features have the same dimension as the hand-engineered features in our experiments. Seen from Table V and Table VI, we achieve slightly better average results than SPCoGVG."
    }, {
      "heading" : "VI. CONCLUSIONS",
      "text" : "In this paper, we presented an end-to-end place classification framework. We implemented a multi-layer learning framework, including the construction of multi-layer inputs and decision making on the multi-layer results. Each layer of inputs were fed into a semi-supervised model for feature"
    }, {
      "heading" : "MULTI-LAYER RESULTS TRAINED ON FR79 WITH GRAPH REGULARIZATION.",
      "text" : ""
    }, {
      "heading" : "GRAPH REGULARIZED FUSION RESULTS TRAINED ON INTELLAB AND RESULTS USING SPCOGVG.",
      "text" : ""
    }, {
      "heading" : "GRAPH REGULARIZED FUSION RESULTS TRAINED ON FR79 AND RESULTS USING SPCOGVG.",
      "text" : "learning and classification, which guaranteed the local consistency with a graph regularization.\nExperimental results showed that the higher layer input data led to higher classification accuracy, which validated the effectiveness of the multi-layer structure. By performing the semi-supervised learning with or without graph regularization, we also showed that graph regularization help promoting the classification performance by keeping the local consistency. Furthermore, the fusion results based on the confidence tree achieved comparable results to the stateof-art method. In a nutshell, we achieved the generalization ability and preserved the local consistency in our end-to-end place classification framework. Future work is to apply our framework on other type of sensor data, such as RGB-D data, which have more representative and discriminative ability."
    } ],
    "references" : [ {
      "title" : "Search in the real world: Active visual object search based on spatial relations",
      "author" : [ "A. Aydemir", "K. Sjoo", "J. Folkesson", "A. Pronobis", "P. Jensfelt" ],
      "venue" : "Robotics and Automation (ICRA), 2011 IEEE International Conference on, pages 2818–2824. IEEE",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Greedy layer-wise training of deep networks",
      "author" : [ "Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle" ],
      "venue" : "Advances in neural information processing systems, 19:153",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Ask the locals: multi-way local pooling for image recognition",
      "author" : [ "Y.-L. Boureau", "N. Le Roux", "F. Bach", "J. Ponce", "Y. LeCun" ],
      "venue" : "Computer Vision (ICCV), 2011 IEEE International Conference on, pages 2651– 2658. IEEE",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Sensor based planning",
      "author" : [ "H. Choset", "J. Burdick" ],
      "venue" : "i. the generalized voronoi graph. In Robotics and Automation, 1995. Proceedings., 1995 IEEE International Conference on, volume 2, pages 1649–1655. IEEE",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa" ],
      "venue" : "The Journal of Machine Learning Research, 12:2493–2537",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Context-dependent pretrained deep neural networks for large-vocabulary speech recognition",
      "author" : [ "G.E. Dahl", "D. Yu", "L. Deng", "A. Acero" ],
      "venue" : "Audio, Speech, and Language Processing, IEEE Transactions on, 20(1):30–42",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Robot task planning using semantic maps",
      "author" : [ "C. Galindo", "J.-A. Fernández-Madrigal", "J. González", "A. Saffiotti" ],
      "venue" : "Robotics and Autonomous Systems, 56(11):955–966",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Domain adaptation for largescale sentiment classification: A deep learning approach",
      "author" : [ "X. Glorot", "A. Bordes", "Y. Bengio" ],
      "venue" : "Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 513–520",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Dimensionality reduction by learning an invariant mapping",
      "author" : [ "R. Hadsell", "S. Chopra", "Y. LeCun" ],
      "venue" : "Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Volume 2, CVPR ’06, pages 1735–1742, Washington, DC, USA",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Dimensionality reduction by learning an invariant mapping",
      "author" : [ "R. Hadsell", "S. Chopra", "Y. LeCun" ],
      "venue" : "Computer vision and pattern recognition, 2006 IEEE computer society conference on, volume 2, pages 1735–1742. IEEE",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "A fast learning algorithm for deep belief nets",
      "author" : [ "G. Hinton", "S. Osindero", "Y. Teh" ],
      "venue" : "Neural computation",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Reducing the dimensionality of data with neural networks",
      "author" : [ "G. Hinton", "R. Salakhutdinov" ],
      "venue" : "Science, 313(5786):504 – 507",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Building high-level features using large scale unsupervised learning",
      "author" : [ "Q.V. Le" ],
      "venue" : "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 8595–8598. IEEE",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Supervised semantic labeling of places using information extracted from sensor data",
      "author" : [ "O. Martinez Mozos", "R. Triebel", "P. Jensfelt", "A. Rottmann", "W. Burgard" ],
      "venue" : "Robotics and Autonomous Systems, 55(5):391–402",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Supervised learning of places from range data using adaboost",
      "author" : [ "O.M. Mozos", "C. Stachniss", "W. Burgard" ],
      "venue" : "Robotics and Automation, 2005. ICRA 2005. Proceedings of the 2005 IEEE International Conference on, pages 1730–1735. IEEE",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Place characterization for navigation via behaviour merging for an autonomous mobile robot",
      "author" : [ "A. Poncela", "C. Urdiales", "B. Fernández-Espejo", "F. Sandoval" ],
      "venue" : "Electrotechnical Conference, 2008. MELECON 2008. The 14th IEEE Mediterranean, pages 350–355. IEEE",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Hierarchical multi-modal place categorization",
      "author" : [ "A. Pronobis", "P. Jensfelt" ],
      "venue" : "ECMR, pages 159–164",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Large-scale semantic mapping and reasoning with heterogeneous modalities",
      "author" : [ "A. Pronobis", "P. Jensfelt" ],
      "venue" : "Robotics and Automation (ICRA), 2012 IEEE International Conference on, pages 3515–3522. IEEE",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Pliss: Detecting and labeling places using online change-point detection",
      "author" : [ "A. Ranganathan" ],
      "venue" : "Robotics: Science and Systems",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Visual place categorization in maps",
      "author" : [ "A. Ranganathan", "J. Lim" ],
      "venue" : "Intelligent Robots and Systems (IROS), 2011 IEEE/RSJ International Conference on, pages 3982–3989. IEEE",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Higher order contractive auto-encoder",
      "author" : [ "S. Rifai", "G. Mesnil", "P. Vincent", "X. Muller", "Y. Bengio", "Y. Dauphin", "X. Glorot" ],
      "venue" : "Proceedings of the 2011 European conference on Machine learning and knowledge discovery in databases - Volume Part II, ECML PKDD’11, pages 645– 660, Berlin, Heidelberg",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Contractive Auto-Encoders: Explicit invariance during feature extraction",
      "author" : [ "S. Rifai", "P. Vincent", "X. Muller", "X. Glorot", "Y. Bengio" ],
      "venue" : "ICML",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Towards generalization of semi-supervised  place classification over generalized voronoi graph",
      "author" : [ "L. Shi", "S. Kodagoda" ],
      "venue" : "Robotics and Autonomous Systems, 61(8):785–796",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Laser range data based semantic labeling of places",
      "author" : [ "L. Shi", "S. Kodagoda", "G. Dissanayake" ],
      "venue" : "Intelligent Robots and Systems (IROS), 2010 IEEE/RSJ International Conference on, pages 5941–5946. IEEE",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Multi-class classification for semantic labeling of places",
      "author" : [ "L. Shi", "S. Kodagoda", "G. Dissanayake" ],
      "venue" : "Control Automation Robotics & Vision (ICARCV), 2010 11th International Conference on, pages 2307–2312. IEEE",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Investigating the performance of corridor and door detection algorithms in different environments",
      "author" : [ "W. Shi", "J. Samarabandu" ],
      "venue" : "Information and Automation, 2006. ICIA 2006. International Conference on, pages 206–211. IEEE",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Real-time labeling of places using support vector machines",
      "author" : [ "P. Sousa", "R. Araújo", "U. Nunes" ],
      "venue" : "Industrial Electronics, 2007. ISIE 2007. IEEE International Symposium on, pages 2022–2027. IEEE",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Incremental construction of the saturated-gvg for multi-hypothesis topological slam",
      "author" : [ "T. Tao", "S. Tully", "G. Kantor", "H. Choset" ],
      "venue" : "Robotics and Automation (ICRA), 2011 IEEE International Conference on, pages 3072–3077. IEEE",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Probabilistic robotics",
      "author" : [ "S. Thrun", "W. Burgard", "D. Fox" ],
      "venue" : "MIT press",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "A connection between score matching and denoising autoencoders",
      "author" : [ "P. Vincent" ],
      "venue" : "Neural Comput.,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2011
    }, {
      "title" : "Automated spatial-semantic modeling with applications to place labeling and informed search",
      "author" : [ "P. Viswanathan", "D. Meger", "T. Southey", "J.J. Little", "A. Mackworth" ],
      "venue" : "Computer and Robot Vision, 2009. CRV’09. Canadian Conference on, pages 284–291. IEEE",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Deep learning via semi-supervised embedding",
      "author" : [ "J. Weston", "F. Ratle", "H. Mobahi", "R. Collobert" ],
      "venue" : "Neural Networks: Tricks of the Trade, pages 639–655. Springer",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Robust semantic place recognition with vocabulary tree and landmark detection",
      "author" : [ "L. Yuan", "K.C. Chan", "C.G. Lee" ],
      "venue" : "Proc. IROS 2011 workshop on Active Semantic Perception and Object Search in the Real, World",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 32,
      "context" : ") to each location [33], [17].",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 16,
      "context" : ") to each location [33], [17].",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 19,
      "context" : "Furthermore, the semantic labels has the potential to efficiently facilitate other robotic functions such as mapping [20], behavior-based navigation [16], task planning [7] and active object search and rescue [1].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 15,
      "context" : "Furthermore, the semantic labels has the potential to efficiently facilitate other robotic functions such as mapping [20], behavior-based navigation [16], task planning [7] and active object search and rescue [1].",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 6,
      "context" : "Furthermore, the semantic labels has the potential to efficiently facilitate other robotic functions such as mapping [20], behavior-based navigation [16], task planning [7] and active object search and rescue [1].",
      "startOffset" : 169,
      "endOffset" : 172
    }, {
      "referenceID" : 0,
      "context" : "Furthermore, the semantic labels has the potential to efficiently facilitate other robotic functions such as mapping [20], behavior-based navigation [16], task planning [7] and active object search and rescue [1].",
      "startOffset" : 209,
      "endOffset" : 212
    }, {
      "referenceID" : 14,
      "context" : "However, this requires an effective feature extraction method and most of the previous works tend to extract hand-engineered features from the raw data [15], [27].",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 26,
      "context" : "However, this requires an effective feature extraction method and most of the previous works tend to extract hand-engineered features from the raw data [15], [27].",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 3,
      "context" : "1, which can be split into three parts: 1) Construction of multi-layer inputs: The environmental information in this paper is represented through the generalized Voronoi graph (GVG) [4], a topological graph in which the nodes correspond to the sensory data and the edges denote the relationships.",
      "startOffset" : 182,
      "endOffset" : 185
    }, {
      "referenceID" : 25,
      "context" : "[26] and Viswanathan et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 30,
      "context" : "[31] extracted",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[15] and Sousa et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 26,
      "context" : "[27] classified the places based on laser range data.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[15] extracted features from the 360◦ laser range data and those features were fed into an",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 26,
      "context" : "[27] reported superior results on a binary classification task using a subset of above mentioned features, and the support vector machine as the classifier.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "In our past work, we implemented a logistic regression based classifier, as a binary and multiclass problem contributing to higher accuracies [24], [25].",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 24,
      "context" : "In our past work, we implemented a logistic regression based classifier, as a binary and multiclass problem contributing to higher accuracies [24], [25].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 22,
      "context" : "The work was further extended to address the generalizability of the solution through a semi-supervised place classification over a generalized Voronoi graph (SPCoGVG) [23].",
      "startOffset" : 168,
      "endOffset" : 172
    }, {
      "referenceID" : 11,
      "context" : "Recently, the unsupervised feature learning has drawn much attention as the deep learning methods was developed [12], [11], [2].",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 10,
      "context" : "Recently, the unsupervised feature learning has drawn much attention as the deep learning methods was developed [12], [11], [2].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 1,
      "context" : "Recently, the unsupervised feature learning has drawn much attention as the deep learning methods was developed [12], [11], [2].",
      "startOffset" : 124,
      "endOffset" : 127
    }, {
      "referenceID" : 2,
      "context" : "The deep learning methods achieved remarkable results in many areas, including object recognition [3], [13], natural language processing [5], [8] and speech recognition [6], which demonstrated that discovering and extracting features automatically can usually achieve better results on representation learning [22], [21], [30].",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 12,
      "context" : "The deep learning methods achieved remarkable results in many areas, including object recognition [3], [13], natural language processing [5], [8] and speech recognition [6], which demonstrated that discovering and extracting features automatically can usually achieve better results on representation learning [22], [21], [30].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 4,
      "context" : "The deep learning methods achieved remarkable results in many areas, including object recognition [3], [13], natural language processing [5], [8] and speech recognition [6], which demonstrated that discovering and extracting features automatically can usually achieve better results on representation learning [22], [21], [30].",
      "startOffset" : 137,
      "endOffset" : 140
    }, {
      "referenceID" : 7,
      "context" : "The deep learning methods achieved remarkable results in many areas, including object recognition [3], [13], natural language processing [5], [8] and speech recognition [6], which demonstrated that discovering and extracting features automatically can usually achieve better results on representation learning [22], [21], [30].",
      "startOffset" : 142,
      "endOffset" : 145
    }, {
      "referenceID" : 5,
      "context" : "The deep learning methods achieved remarkable results in many areas, including object recognition [3], [13], natural language processing [5], [8] and speech recognition [6], which demonstrated that discovering and extracting features automatically can usually achieve better results on representation learning [22], [21], [30].",
      "startOffset" : 169,
      "endOffset" : 172
    }, {
      "referenceID" : 21,
      "context" : "The deep learning methods achieved remarkable results in many areas, including object recognition [3], [13], natural language processing [5], [8] and speech recognition [6], which demonstrated that discovering and extracting features automatically can usually achieve better results on representation learning [22], [21], [30].",
      "startOffset" : 310,
      "endOffset" : 314
    }, {
      "referenceID" : 20,
      "context" : "The deep learning methods achieved remarkable results in many areas, including object recognition [3], [13], natural language processing [5], [8] and speech recognition [6], which demonstrated that discovering and extracting features automatically can usually achieve better results on representation learning [22], [21], [30].",
      "startOffset" : 316,
      "endOffset" : 320
    }, {
      "referenceID" : 29,
      "context" : "The deep learning methods achieved remarkable results in many areas, including object recognition [3], [13], natural language processing [5], [8] and speech recognition [6], which demonstrated that discovering and extracting features automatically can usually achieve better results on representation learning [22], [21], [30].",
      "startOffset" : 322,
      "endOffset" : 326
    }, {
      "referenceID" : 19,
      "context" : "Previous research has included this particular characteristic for performance promotion and many studies were carried out with consideration of the local consistency [20], [18], [15], [14], [19].",
      "startOffset" : 166,
      "endOffset" : 170
    }, {
      "referenceID" : 17,
      "context" : "Previous research has included this particular characteristic for performance promotion and many studies were carried out with consideration of the local consistency [20], [18], [15], [14], [19].",
      "startOffset" : 172,
      "endOffset" : 176
    }, {
      "referenceID" : 14,
      "context" : "Previous research has included this particular characteristic for performance promotion and many studies were carried out with consideration of the local consistency [20], [18], [15], [14], [19].",
      "startOffset" : 178,
      "endOffset" : 182
    }, {
      "referenceID" : 13,
      "context" : "Previous research has included this particular characteristic for performance promotion and many studies were carried out with consideration of the local consistency [20], [18], [15], [14], [19].",
      "startOffset" : 184,
      "endOffset" : 188
    }, {
      "referenceID" : 18,
      "context" : "Previous research has included this particular characteristic for performance promotion and many studies were carried out with consideration of the local consistency [20], [18], [15], [14], [19].",
      "startOffset" : 190,
      "endOffset" : 194
    }, {
      "referenceID" : 8,
      "context" : "There are some similar works on implementing the graph regularized deep learning models [9], [32].",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 31,
      "context" : "There are some similar works on implementing the graph regularized deep learning models [9], [32].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 8,
      "context" : "Both [9] and [32] utilized a margin-based loss function proposed by Hadsell et at.",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 31,
      "context" : "Both [9] and [32] utilized a margin-based loss function proposed by Hadsell et at.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 9,
      "context" : "[10].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 3,
      "context" : "1) Data Representation on GVG: In this paper, our multilayer inputs is represented by the hierarchical generalized Voronoi graph (GVG) [4], a topological graph which has been successfully applied to navigation, localization and mapping.",
      "startOffset" : 135,
      "endOffset" : 138
    }, {
      "referenceID" : 27,
      "context" : "The general representation of GVG is composed of meet-points (locations of three-way or more equidistance to obstacles) and edges (feasible paths between meet-points which are two-way equidistance to obstacles) [28].",
      "startOffset" : 211,
      "endOffset" : 215
    }, {
      "referenceID" : 22,
      "context" : "We adopt the same resolution as in our previous work [23] to construct the first layer GVG, and then higher layers of GVGs are constructed to describe the environment at different levels of granularity.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 28,
      "context" : "Given each v i with numel(N(v (l) i )) > 1, firstly a local map is generated using occupancy grid mapping [29] based on the respective range data in lth layer, including",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 14,
      "context" : "scan, as the consecutive differences can also provide rich information to the place classification and is often employed for extracting geometric features in the previous works [15],",
      "startOffset" : 177,
      "endOffset" : 181
    }, {
      "referenceID" : 26,
      "context" : "[27].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "For each layer of auto-encoder, the unsupervised pretraining is applied to obtain better parameters than random initialization [12] by minimizing the reconstruction cost:",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 22,
      "context" : "We also make comparisons with the results we achieved in our previous work SPCoGVG [23].",
      "startOffset" : 83,
      "endOffset" : 87
    } ],
    "year" : 2015,
    "abstractText" : "Place classification is a fundamental ability that a robot should possess to carry out effective human-robot interactions. It is a nontrivial classification problem which has attracted many research. In recent years, there is a high exploitation of Artificial Intelligent algorithms in robotics applications. Inspired by the recent successes of deep learning methods, we propose an end-to-end learning approach for the place classification problem. With the deep architectures, this methodology automatically discovers features and contributes in general to higher classification accuracies. The pipeline of our approach is composed of three parts. Firstly, we construct multiple layers of laser range data to represent the environment information in different levels of granularity. Secondly, each layer of data is fed into a deep neural network model for classification, where a graph regularization is imposed to the deep architecture for keeping local consistency between adjacent samples. Finally, the predicted labels obtained from all the layers are fused based on confidence trees to maximize the overall confidence. Experimental results validate the effectiveness of our end-to-end place classification framework in which both the multi-layer structure and the graph regularization promote the classification performance. Furthermore, results show that the features automatically learned from the raw input range data can achieve competitive results to the features constructed based on statistical and geometrical information.",
    "creator" : "LaTeX with hyperref package"
  }
}