{
  "name" : "1411.7942.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Using Sentence Plausibility to Learn the Semantics of Transitive Verbs",
    "authors" : [ "Tamara Polajnar", "Laura Rimell", "Stephen Clark" ],
    "emails" : [ "tamara.polajnar@cl.cam.ac.uk", "laura.rimell@cl.cam.ac.uk", "stephen.clark@cl.cam.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n41 1.\n79 42\nv1 [\ncs .C\nL ]\n2 8\nN ov"
    }, {
      "heading" : "1 Introduction",
      "text" : "The field of compositional distributional semantics seeks principled ways to combine distributional representations of words to form larger units. Representations of full sentences, besides their theoretical interest, have the potential to be useful for tasks such as automatic summarisation and recognising textual entailment. A number of recent studies have investigated ways to combine distributional representations of Subject, Verb, Object (SVO) triples to form transitive sentences, that is, sentences based on a transitive verb [2–4, 6, 7, 12, 14].\nUnder the functional approach [1–4], argument-taking words such as verbs and adjectives are represented as tensors, which take word vectors as arguments. A transitive verb can be viewed as a third-order tensor with input dimensions for the subject and object, and an output dimension for the meaning of the sentence as a whole. This approach has achieved promising initial results [6–9, 14], but many questions remain. Two outstanding questions are the best method of learning verb tensors from a corpus, and the best sentence space for a variety of different tasks.\nThis paper presents work in progress which addresses both of these questions. It compares two methods for learning verb representations, the distributional model of [7] in which positive examples of subject-object pairs for a given verb are structurally mixed, and the regression model of [14] in which positive and negative examples of subject-object pairs for a given verb are mapped into a plausibility space. A variety of methods for reducing the noun space and composing the verb, subject, and object representations are investigated. The results show that the plausibility training outperforms the distributional method on a verb disambiguation task, while the purely distributional approach performs better on sentence similarity."
    }, {
      "heading" : "2 Methods",
      "text" : "In the definition of the functional approach to compositional distributional semantics [1–4], a transitive verb is a map that takes as arguments noun vectors representing the subject and object, and produces a vector in the sentence space. Typically, noun vectors for subject and object reside in a “topic space” where the dimensions correspond to co-occurrence features; we use a reduced space\nresulting from applying Singular Value Decomposition (SVD) to the raw co-occurrence space. The correct sentence space to use is less obvious; previous approaches have either mapped sentence meaning to the same topic-based noun space [6, 7] or defined a new space for sentence meaning, particularly plausibility space [11, 14].\nIf the verb function is a multi-linear map, then the verb is naturally represented by a third-order tensor. However, tensor training can be expensive and in practice, for some tasks, the verb can be approximated as a matrix [7, 14].\nBelow we describe two ways of learning a verb matrix. In the regression method, the learnt matrix consists of parameters from a plausibility classifier. The classifier is trained to distinguish plausible sentences like animals eat plants from implausible sentences like animals eat planets. In the distributional method, training is based on a sum of positive (i.e. attested) SVO triples. The acquisition of positive SVO data and plausibility training data is described in Section 2.2."
    }, {
      "heading" : "2.1 Verbs",
      "text" : "Regression (reg) Following [14], we formulate regression learning as a plausibility task where the class membership can be estimated with a single variable. To produce a scalar output, we can learn the parameters for a single K×K matrix (V) using standard logistic regression with the mean squared error cost function and K-dimensional subject (ns) and object (no) noun vectors as input:\nO(V) = − 1\nm\n[\nN ∑\ni=1\nt i log hV\n(\nn i s,n i o\n) + (1− ti) log hV ( n i s,n i o )\n]\nwhere ti are the true plausibility labels of the N training examples. The function hV ( n i s,n i o ) = σ((nis)V(n i o)\nT ) is a sigmoid transformation of the scalar that results from the inner product and the objective is regularised by the parameter λ: O(V) + λ\n2 ||V||2.\nAll of the plausibility data is used for training and validation, as we employ only the resulting verb matrix to produce a sentence space for transitive verb phrases in the test data. The regression algorithm is trained through gradient descent with Adagrad [5] and 10% of the training triples are used as a validation set for early stopping.\nDistributional (dist) Following [7], we generate a K ×K matrix for each verb as the average of outer products of subject and verb vectors from the positively labelled subset of the training data:\nV = 1\nNp\n[\nNp ∑\ni=1\nn i s ⊗ n i o\n]\nwhere⊗ is outer product andNp is the number of positive training examples. The intuition is that the matrix encodes higher weights for contextual features of frequently attested subjects and objects; for example, multiplying by the matrix for eat may yield a higher scalar value when its subject exhibits features common to animate nouns, and its object exhibits features common to edible nouns."
    }, {
      "heading" : "2.2 Training data",
      "text" : "In order to generate training data we find plausible SVO triples that occur in an October 2013 dump of Wikipedia. To ensure quality we choose triples whose nouns occur at least 100 times. For some verbs there are thousands of such triples, so we choose the top 300 most frequent triples for each verb. For each verb we generate negative examples by substituting plausible subjects and objects with maximally dissimilar nouns. Specifically, for a given subject (or object) noun we calculate its average sum with the centroid of plausible subject (or object) vectors, and then select the frequencymatched noun with lowest cosine similarity to this average. The noun vectors are generated from the Wikipedia corpus using the t-test weighting scheme and normalisation techniques described in [13]. These techniques enable us to learn using SVD reduced vectors with dimensions as low as 20."
    }, {
      "heading" : "2.3 Composition Methods",
      "text" : "We investigate the following methods of composing the verb matrix with the subject and object vectors to form a vector representation for a transitive sentence.\nRelational: from [7, 10], the meaning of a transitive sentence is a matrix, obtained by the following formula, where ⊗ is outer product, ⊙ is elementwise product, and × is matrix multiplication:\nsubj verb obj = ( −−→ subj ⊗ −→ obj)⊙ verb = ( −−→ subj × −→ obj T )ij · verbij (1)\nCopy-subject: from [9, 10], the meaning of a transitive sentence is a vector, obtained by:\n−−−−−−−−−→ subj verb obj = −−→ subj ⊙ (verb × −→ obj) = −−→ subji · (verb × −→ obj)i (2)\nVerb-object: this method tests whether the verb and object alone are enough to measure sentence similarity. It can be compared directly to Copy-subject and reflects the linguistic generalisation that a verb selects its object more strongly than its subject. The meaning of a transitive sentence is a vector, obtained by:\n−−−−−−−−−→ subj verb obj = verb × −→ obj (3)\nFor the Relational method, sentence similarity is measured as the Frobenius inner product of the two sentence matrices. For Copy-subject and Verb-object, sentence similarity is measured as the cosine of the two sentence vectors."
    }, {
      "heading" : "3 Tasks",
      "text" : "We investigate the performance of the regression learning method on two tasks: verb disambiguation, and transitive sentence similarity. In each case the system must compose SVO triples and compare the resulting semantic representations.\nFor the verb disambiguation task we use the GS2011 dataset [7]. This dataset consists of pairs of SVO triples in which the subject and object are held constant, and the verb is manipulated to highlight different word senses. For example, the verb draw has senses that correspond to attract and depict. The SVO triple report draw attention has high similarity to report attract attention, but low similarity to report depict attention. Conversely, child draw picture has high similarity to child depict picture, but low similarity to child attract picture. The gold standard consists of human judgements of the similarity between pairs, and we measure the correlation of the system’s similarity scores to the gold standard judgements.\nFor the transitive sentence similarity task we use the KS2013 dataset [9]. This dataset also consists of pairs of SVO triples, but the verb as well as the subject and object vary. For example, author write book and delegate buy land are judged by most annotators to be very dissimilar, while programme offer support and service provide help are considered highly similar. Again, we measure the correlation between the system’s similarity scores and the gold standard judgements."
    }, {
      "heading" : "4 Results",
      "text" : "Table 1 and Figure 1 summarise the results of our experiments. Overall, reg performs better than dist on verb disambiguation, while dist performs better on sentence similarity. We hypothesise the\ndifference lies in the nature of the two tasks. The verb disambiguation task is inherently plausibilitybased, because one member of the low-similarity pairs (with the non-relevant sense of the verb) is always implausible. For example, when disambiguating the verb meet, the triple system satisfy requirement has high plausibility, while system visit requirement has low plausibility. In fact, the Verb-object (VO) composition method alone is enough to disambiguate these triples using reg.\nOn the other hand, both triples in the sentence similarity task tend to be highly plausible, even when their topic differs. Because dist uses a topic-based space, it may better capture these distinctions. For example, girl like people and woman ask man are both highly plausible, but annotators have judged the pair to have low similarity on average. The best distributional method (K=100, Relational composition) ranks this pair the middle, while the corresponding regression method (K=100, Relational composition) ranks this pair near the top of the range of similarities.\nFigure 1 shows the overall trends of the two methods. The reg method consistently outperforms dist when low-dimensional vectors are used. Due to the nature of functional composition of distributional representations, where each word-type other than noun is represented by a higher-order tensor, low-dimensional representations are particularly advantageous. On the other hand, the dist method is very quick to train and therefore can be used with higher dimensional vectors for tasks where data storage is not an issue."
    }, {
      "heading" : "5 Conclusion",
      "text" : "The difference in performance of the two methods underlines the need to find the appropriate sentence spaces for particular tasks. This preliminary study indicates that plausibility training may be better suited for disambiguation. Further work will consist of more in depth analysis and optimisation of the training procedure, as well as investigation into ways of low-cost learning of task-specific sentence spaces."
    }, {
      "heading" : "Acknowledgements",
      "text" : "Tamara Polajnar is supported by the ERC Starting Grant, DisCoTex, awarded to Stephen Clark, and Laura Rimell is supported by the EPSRC grant EP/I037512/1: A Unified Model of Compositional and Distributional Semantics: Theory and Applications."
    } ],
    "references" : [ {
      "title" : "Frege in space: A program for compositional distributional semantics",
      "author" : [ "Marco Baroni", "Raffaella Bernardi", "Roberto Zamparelli" ],
      "venue" : "Linguistic Issues in Language Technology,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2014
    }, {
      "title" : "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space",
      "author" : [ "Marco Baroni", "Roberto Zamparelli" ],
      "venue" : "In Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2010
    }, {
      "title" : "Type-driven syntax and semantics for composing meaning vectors",
      "author" : [ "Stephen Clark" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2013
    }, {
      "title" : "Mathematical foundations for a compositional distributional model of meaning",
      "author" : [ "Bob Coecke", "Mehrnoosh Sadrzadeh", "Stephen Clark" ],
      "venue" : "Linguistic Analysis (Lambek Festschrift),",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2010
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "John Duchi", "Elad Hazan", "Yoram Singer" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2011
    }, {
      "title" : "Multi-step regression learning for compositional distributional semantics",
      "author" : [ "Edward Grefenstette", "Georgiana Dinu", "Yao-Zhong Zhang", "Mehrnoosh Sadrzadeh", "Marco Baroni" ],
      "venue" : "Proceedings of the 10th International Conference on Computational Semantics (IWCS",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "Experimental support for a categorical compositional distributional model of meaning",
      "author" : [ "Edward Grefenstette", "Mehrnoosh Sadrzadeh" ],
      "venue" : "In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2011
    }, {
      "title" : "Experimenting with transitive verbs in a discocat",
      "author" : [ "Edward Grefenstette", "Mehrnoosh Sadrzadeh" ],
      "venue" : "In Proceedings of the GEMS 2011 Workshop on Geometrical Models of Natural Languge,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "Prior disambiguation of word tensors for constructing sentence vectors",
      "author" : [ "Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh" ],
      "venue" : "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2013
    }, {
      "title" : "A study of entanglement in a categorical framework of natural language",
      "author" : [ "Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh" ],
      "venue" : "In Proceedings of the 11th Workshop on Quantum Physics and Logic (QPL),",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2014
    }, {
      "title" : "Vector space semantic parsing: A framework for compositional vector space models",
      "author" : [ "Jayant Krishnamurthy", "Tom M Mitchell" ],
      "venue" : "In Proceedings of the 2013 ACL Workshop on Continuous Vector Space Models and their Compositionality, Sofia, Bulgaria,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2013
    }, {
      "title" : "Vector-based models of semantic composition",
      "author" : [ "Jeff Mitchell", "Mirella Lapata" ],
      "venue" : "In Proceedings of ACL-08,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2008
    }, {
      "title" : "Improving distributional semantic vectors through context selection and normalisation",
      "author" : [ "Tamara Polajnar", "Stephen Clark" ],
      "venue" : "In 14th Conference of the European Chapter of the Association for Computational Linguistics,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Reducing dimensions of tensors in type-driven distributional semantics",
      "author" : [ "Tamara Polajnar", "Luana Fagarasan", "Stephen Clark" ],
      "venue" : "In Proceedings of EMNLP",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "A number of recent studies have investigated ways to combine distributional representations of Subject, Verb, Object (SVO) triples to form transitive sentences, that is, sentences based on a transitive verb [2–4, 6, 7, 12, 14].",
      "startOffset" : 207,
      "endOffset" : 226
    }, {
      "referenceID" : 2,
      "context" : "A number of recent studies have investigated ways to combine distributional representations of Subject, Verb, Object (SVO) triples to form transitive sentences, that is, sentences based on a transitive verb [2–4, 6, 7, 12, 14].",
      "startOffset" : 207,
      "endOffset" : 226
    }, {
      "referenceID" : 3,
      "context" : "A number of recent studies have investigated ways to combine distributional representations of Subject, Verb, Object (SVO) triples to form transitive sentences, that is, sentences based on a transitive verb [2–4, 6, 7, 12, 14].",
      "startOffset" : 207,
      "endOffset" : 226
    }, {
      "referenceID" : 5,
      "context" : "A number of recent studies have investigated ways to combine distributional representations of Subject, Verb, Object (SVO) triples to form transitive sentences, that is, sentences based on a transitive verb [2–4, 6, 7, 12, 14].",
      "startOffset" : 207,
      "endOffset" : 226
    }, {
      "referenceID" : 6,
      "context" : "A number of recent studies have investigated ways to combine distributional representations of Subject, Verb, Object (SVO) triples to form transitive sentences, that is, sentences based on a transitive verb [2–4, 6, 7, 12, 14].",
      "startOffset" : 207,
      "endOffset" : 226
    }, {
      "referenceID" : 11,
      "context" : "A number of recent studies have investigated ways to combine distributional representations of Subject, Verb, Object (SVO) triples to form transitive sentences, that is, sentences based on a transitive verb [2–4, 6, 7, 12, 14].",
      "startOffset" : 207,
      "endOffset" : 226
    }, {
      "referenceID" : 13,
      "context" : "A number of recent studies have investigated ways to combine distributional representations of Subject, Verb, Object (SVO) triples to form transitive sentences, that is, sentences based on a transitive verb [2–4, 6, 7, 12, 14].",
      "startOffset" : 207,
      "endOffset" : 226
    }, {
      "referenceID" : 0,
      "context" : "Under the functional approach [1–4], argument-taking words such as verbs and adjectives are represented as tensors, which take word vectors as arguments.",
      "startOffset" : 30,
      "endOffset" : 35
    }, {
      "referenceID" : 1,
      "context" : "Under the functional approach [1–4], argument-taking words such as verbs and adjectives are represented as tensors, which take word vectors as arguments.",
      "startOffset" : 30,
      "endOffset" : 35
    }, {
      "referenceID" : 2,
      "context" : "Under the functional approach [1–4], argument-taking words such as verbs and adjectives are represented as tensors, which take word vectors as arguments.",
      "startOffset" : 30,
      "endOffset" : 35
    }, {
      "referenceID" : 3,
      "context" : "Under the functional approach [1–4], argument-taking words such as verbs and adjectives are represented as tensors, which take word vectors as arguments.",
      "startOffset" : 30,
      "endOffset" : 35
    }, {
      "referenceID" : 5,
      "context" : "This approach has achieved promising initial results [6–9, 14], but many questions remain.",
      "startOffset" : 53,
      "endOffset" : 62
    }, {
      "referenceID" : 6,
      "context" : "This approach has achieved promising initial results [6–9, 14], but many questions remain.",
      "startOffset" : 53,
      "endOffset" : 62
    }, {
      "referenceID" : 7,
      "context" : "This approach has achieved promising initial results [6–9, 14], but many questions remain.",
      "startOffset" : 53,
      "endOffset" : 62
    }, {
      "referenceID" : 8,
      "context" : "This approach has achieved promising initial results [6–9, 14], but many questions remain.",
      "startOffset" : 53,
      "endOffset" : 62
    }, {
      "referenceID" : 13,
      "context" : "This approach has achieved promising initial results [6–9, 14], but many questions remain.",
      "startOffset" : 53,
      "endOffset" : 62
    }, {
      "referenceID" : 6,
      "context" : "It compares two methods for learning verb representations, the distributional model of [7] in which positive examples of subject-object pairs for a given verb are structurally mixed, and the regression model of [14] in which positive and negative examples of subject-object pairs for a given verb are mapped into a plausibility space.",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 13,
      "context" : "It compares two methods for learning verb representations, the distributional model of [7] in which positive examples of subject-object pairs for a given verb are structurally mixed, and the regression model of [14] in which positive and negative examples of subject-object pairs for a given verb are mapped into a plausibility space.",
      "startOffset" : 211,
      "endOffset" : 215
    }, {
      "referenceID" : 0,
      "context" : "In the definition of the functional approach to compositional distributional semantics [1–4], a transitive verb is a map that takes as arguments noun vectors representing the subject and object, and produces a vector in the sentence space.",
      "startOffset" : 87,
      "endOffset" : 92
    }, {
      "referenceID" : 1,
      "context" : "In the definition of the functional approach to compositional distributional semantics [1–4], a transitive verb is a map that takes as arguments noun vectors representing the subject and object, and produces a vector in the sentence space.",
      "startOffset" : 87,
      "endOffset" : 92
    }, {
      "referenceID" : 2,
      "context" : "In the definition of the functional approach to compositional distributional semantics [1–4], a transitive verb is a map that takes as arguments noun vectors representing the subject and object, and produces a vector in the sentence space.",
      "startOffset" : 87,
      "endOffset" : 92
    }, {
      "referenceID" : 3,
      "context" : "In the definition of the functional approach to compositional distributional semantics [1–4], a transitive verb is a map that takes as arguments noun vectors representing the subject and object, and produces a vector in the sentence space.",
      "startOffset" : 87,
      "endOffset" : 92
    }, {
      "referenceID" : 5,
      "context" : "The correct sentence space to use is less obvious; previous approaches have either mapped sentence meaning to the same topic-based noun space [6, 7] or defined a new space for sentence meaning, particularly plausibility space [11, 14].",
      "startOffset" : 142,
      "endOffset" : 148
    }, {
      "referenceID" : 6,
      "context" : "The correct sentence space to use is less obvious; previous approaches have either mapped sentence meaning to the same topic-based noun space [6, 7] or defined a new space for sentence meaning, particularly plausibility space [11, 14].",
      "startOffset" : 142,
      "endOffset" : 148
    }, {
      "referenceID" : 10,
      "context" : "The correct sentence space to use is less obvious; previous approaches have either mapped sentence meaning to the same topic-based noun space [6, 7] or defined a new space for sentence meaning, particularly plausibility space [11, 14].",
      "startOffset" : 226,
      "endOffset" : 234
    }, {
      "referenceID" : 13,
      "context" : "The correct sentence space to use is less obvious; previous approaches have either mapped sentence meaning to the same topic-based noun space [6, 7] or defined a new space for sentence meaning, particularly plausibility space [11, 14].",
      "startOffset" : 226,
      "endOffset" : 234
    }, {
      "referenceID" : 6,
      "context" : "However, tensor training can be expensive and in practice, for some tasks, the verb can be approximated as a matrix [7, 14].",
      "startOffset" : 116,
      "endOffset" : 123
    }, {
      "referenceID" : 13,
      "context" : "However, tensor training can be expensive and in practice, for some tasks, the verb can be approximated as a matrix [7, 14].",
      "startOffset" : 116,
      "endOffset" : 123
    }, {
      "referenceID" : 13,
      "context" : "Regression (reg) Following [14], we formulate regression learning as a plausibility task where the class membership can be estimated with a single variable.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 4,
      "context" : "The regression algorithm is trained through gradient descent with Adagrad [5] and 10% of the training triples are used as a validation set for early stopping.",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 6,
      "context" : "Distributional (dist) Following [7], we generate a K ×K matrix for each verb as the average of outer products of subject and verb vectors from the positively labelled subset of the training data:",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 12,
      "context" : "The noun vectors are generated from the Wikipedia corpus using the t-test weighting scheme and normalisation techniques described in [13].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 6,
      "context" : "Relational: from [7, 10], the meaning of a transitive sentence is a matrix, obtained by the following formula, where ⊗ is outer product, ⊙ is elementwise product, and × is matrix multiplication:",
      "startOffset" : 17,
      "endOffset" : 24
    }, {
      "referenceID" : 9,
      "context" : "Relational: from [7, 10], the meaning of a transitive sentence is a matrix, obtained by the following formula, where ⊗ is outer product, ⊙ is elementwise product, and × is matrix multiplication:",
      "startOffset" : 17,
      "endOffset" : 24
    }, {
      "referenceID" : 8,
      "context" : "Copy-subject: from [9, 10], the meaning of a transitive sentence is a vector, obtained by:",
      "startOffset" : 19,
      "endOffset" : 26
    }, {
      "referenceID" : 9,
      "context" : "Copy-subject: from [9, 10], the meaning of a transitive sentence is a vector, obtained by:",
      "startOffset" : 19,
      "endOffset" : 26
    }, {
      "referenceID" : 6,
      "context" : "For the verb disambiguation task we use the GS2011 dataset [7].",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 8,
      "context" : "For the transitive sentence similarity task we use the KS2013 dataset [9].",
      "startOffset" : 70,
      "endOffset" : 73
    } ],
    "year" : 2017,
    "abstractText" : "The functional approach to compositional distributional semantics considers transitive verbs to be linear maps that transform the distributional vectors representing nouns into a vector representing a sentence. We conduct an initial investigation that uses a matrix consisting of the parameters of a logistic regression classifier trained on a plausibility task as a transitive verb function. We compare our method to a commonly used corpus-based method for constructing a verb matrix and find that the plausibility training may be more effective for disambiguation tasks.",
    "creator" : "LaTeX with hyperref package"
  }
}