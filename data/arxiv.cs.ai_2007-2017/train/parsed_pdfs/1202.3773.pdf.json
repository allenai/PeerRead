{
  "name" : "1202.3773.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Measuring the Hardness of Stochastic Sampling on Bayesian Networks with Deterministic Causalities: the k-Test",
    "authors" : [ "Haohai Yu" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Approximate Bayesian inference is NP-hard. Dagum and Luby defined the Local Variance Bound (LVB) to measure the approximation hardness of Bayesian inference on Bayesian networks, assuming the networks model strictly positive joint probability distributions, i.e. zero probabilities are not permitted. This paper introduces the k-test to measure the approximation hardness of inference on Bayesian networks with deterministic causalities in the probability distribution, i.e. when zero conditional probabilities are permitted. Approximation by stochastic sampling is a widely-used inference method that is known to suffer from inefficiencies due to sample rejection. The k-test predicts when rejection rates of stochastic sampling a Bayesian network will be low, modest, high, or when sampling is intractable."
    }, {
      "heading" : "1 Introduction",
      "text" : "A Bayesian network, belief network, or directed acyclic graphical model, is a probabilistic graph model [28] that has gained wide acceptance in several areas of artificial intelligence [31]. A Bayesian network represents a joint probability distribution (JPD) over a set of statistical variables and structurally models the (conditional) independence relationships between the variables as a directed acyclic graph (DAG). Efficient algorithms exist that perform probabilistic inference on Bayesian networks for reasoning with uncertainty and to solve decision problems with uncertain data.\nExact and approximate probabilistic Bayesian inference is known to be NP-hard [11, 12]. No single Bayesian inference algorithm or a class of algorithms is known to generally outperform others. Approximate inference algorithms are popular due to their anytime\nproperty [17] to produce an approximate result, possibly in real time [23]. For this reason, stochastic sampling algorithms, especially importance sampling, are among the most widely-used approximate inference methods [31]. Examples are SIS [32], AIS-BN [7], DIS [27], RIS [34] and EPIS-BN [35]. Importance sampling algorithms mainly differ in the choice of importance function to sample the JPD. All sampling algorithms perform poorly on Bayesian networks that are known to be hard for approximate inference. When zero probabilities are permitted in the JPDs, sampling algorithms become inefficient due to sample rejection; samples with zero probability do not contribute to the sum estimate and are therefore effectively rejected. Consequently, the sampling algorithm’s performance is poor on Bayesian networks with deterministic causalities, i.e. networks that model JPDs with zero probabilities.\nThe local variance bound (LVB) [13] metric demarcates the boundary between the class of Bayesian networks with tractable approximations and those with intractable approximations. In certain cases the LVB of a Bayesian network can be used as a quantitative estimation of the expected rate of convergence of the approximate solution to the exact solution under stochastic sampling of the network.\nHowever, the LVB metric is not applicable to measure the approximation hardness of Bayesian networks with deterministic causalities. The LVB metric is the ratio of the maximum to the minimum probability value of the conditional probability table (CPT) entries of the variables in the Bayesian network. Any zero entry in the CPT invalidates the LVB, i.e. when zero probabilities are permitted in the JPD. This means that the LVB is undefined for many real-world networks that model JPDs with zero probabilities, such as Munin [3], Pathfinder [24], Andes [10] and the CPCS [29] networks. Hence, their inference tractability classification fails and it is not possible to give an estimation of the hardness of approximate inference.\nThis paper presents the k-test, a metric to determine the approximation hardness of Bayesian inference. By contrast to the LVB metric, the k-test metric can be applied to Bayesian networks that model JPDs that include zero probabilities. This extends the approximation hardness estimation to an important class of realworld Bayesian networks. The k-test on a Bayesian network can be used to indicate when sample rejection rates are expected to be low, modest, or prohibitively high and sampling is intractible.\nNo polynomial-time algorithm exists that can filter samples x with Pr(x) = 0 from important sampling to prevent rejection. Exact determination of Pr(x) = 0 is known to be NP-complete. It turns out that approximate filtering of samples with Pr(x) = 0 is also NP-hard.\nThe remainder of this paper is organized as follows. Section 2 briefly introduces the Bayesian network formalism, inference by importance sampling, and the sample rejection problem. Section 3 introduces the k-test. Results of the k-test on real-world and benchmark Bayesian networks is presented in Section 4. Finally, Section 5 compares related work and summarizes our contribution."
    }, {
      "heading" : "2 Background",
      "text" : "This section briefly introduces the Bayesian network formalism, approximate Bayesian inference by importance sampling, and the sample rejection problem."
    }, {
      "heading" : "2.1 Bayesian Networks",
      "text" : "A Bayesian network is defined as follows.\nDefinition 1 A Bayesian Network BN = (G,Pr) consists of a directed acyclic graph (DAG) G = (V,A) with vertices V, arcs A ⊆ V×V, and a JPD Pr over the discrete random variables V (represented by the vertices of G). Pr is defined by\nPr(V) = ∏ V ∈V Pr(V |π(V )) ,\nwhere π(V ) denotes the set of parents of a vertex V and the conditional probability tables (CPT) of the BN assign domain-specific probabilities to Pr(V |π(V )) for all V ∈ V.\nIn this paper, variables are indicated by uppercase letters and their states by lowercase letters. Sets (vertices, arcs, and states) are represented in boldface, e.g. x ∈ Config(X) denotes a state (configuration of values) x of a set of variables X ⊆ V. The set of evidence variables (or vertices) is denoted as E, E ⊂ V and the set X = V \\E.\nExact probabilistic inference methods compute Pr(X|e) given evidence e for a set of evidence variables E directly from the network. Approximate inference methods estimate Pr(X|e).\nNote that Pr(v) = 0 for configuration of variables V whenever the CPT entry Pr(vi|π(vi)) = 0 for Vi ∈ V. These zero entries are computationally problematic for Bayesian inference with importance sampling."
    }, {
      "heading" : "2.2 Approximate Bayesian Inference by Importance Sampling",
      "text" : "Let g(X) be a function over m variables X = {X1, . . . , Xm} over some domain Ω ⊆ IRm, such that computing g(X) for any X is feasible. Let p be a probability density over X. Consider the problem of estimating the integral\nE[g(X)|p] = ∫\nΩ\ng(x)p(x) dx . (1)\nAssuming that p is a density that is easy to sample from, the integral can be approximated by drawing a set of independent and identically distributed (i.i.d.) samples {x1, . . . ,xN} from p(X) and use these to compute the sample mean\ng̃N = 1 N N∑ i=1 g(xi) . (2)\nAccording to the strong law of large numbers, g̃N almost surely converges to E[g(X)|p]. The basic idea of importance sampling is to draw from a distribution other than p, say I, that is easier to sample from than p. We can rewrite (1) into\nE[g(X)|p] = ∫\nΩ\ng(x) p(x) I(x) I(x) dx , (3)\nwhere I is often referred to as the importance function. The revised sample mean formula\nĝN = N∑ i=1 [g(xi)w(xi)] , (4)\nuses weights w(xi) = p(xi) I(xi) . Again, ĝN almost surely converges to E[g(X)|p].\nAssumption 1 The distribution I(X) is assumed to support p(X) on Ω. That is, ∀x ∈ Ω : p(x) > 0 ⇒ I(x) > 0. Furthermore, it is assumed that 00 = 0 in w(xi) =\np(xi) I(xi) .\nFor more details refer to [30, 36].\nThe goal of the importance function I is to approximate the posterior probability distribution Pr(X|e),\nmodeled by a network given some evidence e for E, without actually updating the network to the posterior, which is prohibitively expensive.\nThe importance function I should be tractable. Here, “tractable” means that there exists a sampling order δ, such that for any valid instantiation xδ(1), · · · , xδ(m) of Xδ(1), · · · , Xδ(m) m ≥ 1, the complexity of computing PrI(xδ(m+1) | xδ(1), · · · , xδ(m)) is polynomial. Here, PrI(·) is the probability distribution induced by I. A sampling order δ that meets these requirements is called a tractable sampling order for I, or simply a tractable order.\nImportance sampling methods generally require tractable sampling that is consistent with a Bayesian network’s topological order of the vertices V. This is true for AIS-BN [7], EPIS-BN [35], and SIS [32]. By contrast, the tractable sampling order of DIS [27] is a reversed elimination order, which may or may not be consistent with a Bayesian network’s topological order of the vertices.\nA well-known problem of importance sampling on Bayesian networks with deterministic causalities is that the performance of sampling can be poor. When the JPD has zero probabilities, many samples may end up having zero weights w(x) = 0 in the sampling process. These samples do not contribute to the sum estimate (4) and are effectively rejected. Such a sample is inconsistent, since x is an impossible event w(xi) = 0⇒ p(xi) = 0 by Assumption 1.\nThe sample rejection problem under evidential reasoning in a Bayesian network with evidence e is a judgement whether Pr(xδ(1) · · ·xδ(m), e) = 0 for sample xδ(1), · · · , xδ(m) and sampling order δ.\n3 The k-Test\nIn this section the k-test is presented to measure the approximation hardness of sampling Bayesian networks with deterministic causalities. The random kSAT problem and the Satisfiability Threshold Conjecture are explained that form the basis for the k-test. An algorithm to efficiently compute the k-test ratio of a Bayesian network is given."
    }, {
      "heading" : "3.1 Hardness of Sample Rejection",
      "text" : "Cooper [11] proved that computing Pr(x) for any x is NP-hard in general. Computing Pr(x) to classify inconsistent samples x with Pr(x) = 0 from consistent ones with Pr(x) > 0 is prohibitively expensive as a measure to determine the hardness of sampling.\nFurthermore, the approximate sample rejection problem is too hard to be polynomial, as found by our proof\nin Appendix A. Dagum and Luby [12] proved that approximate Bayesian inference is NP-hard. However, their proof is not applicable to the sample rejection problem of Bayesian networks, because the JPDs of these networks are not strictly positive (an assumption used in their proof).\nTo empirically estimate the rejection rate requires a significant number of samples to be produced to cover the exponential state space of a network. Furthermore, the choice of a sampling algorithm may also influence the estimation of the ratio, such as by CPT learning adopted by state-of-the-art sampling algorithms.\nTherefore, exact, approximate, and empirical determination of the hardness of sampling Bayesian networks exhibiting zero probabilities poses significant computational difficulties.\nThe sample rejection problem can be transformed into an equivalent random k-SAT problem, which forms the basis of our k-test.\n3.2 Random k-SAT\nFranco and Paull [15] first observed, among other important results, that random instances of the k-SAT problem undergo a “phase transition” as the ratio of clauses to variables passes through a threshold. Let Fn,mk denote a k-CNF with n variables andm k-clauses created by uniformly and randomly choosing m clauses from the Ck = 2k ( n k ) possible clauses. Franco and Paull [15] claim that Fn,m=rnk is with high probability (w.h.p. limn→∞ Pr( n) = 1) unsatisfiable if r ≥ 2k ln 2. The reasons are given as follows. Let a be a truth assignment and let Sk = (2k − 1) ( n k ) be the number of k-clauses consistent with the given assignment a. Then for Fn,mk , Pr(F n,m k (a) = true) = ( Sk m ) / ( Ck m ) ≤ (1 − 2−k)m the expected number of satisfying truth assignments of Fn,mk is at most 2\nn(1 − 2−k)m = o(1) for m/n ≥ 2k ln 2.\nThis result has led to the following popular conjecture.\nSatisfiability Threshold Conjecture. For k ≥ 3, there exists a constant rk such that\nlim n→∞\nPr(Fn,rnk is satisfiable) = {\n1, if r < rk, 0, if r > rk (5)\nSince 1990 much work has been done on this conjecture to narrow the threshold rk. One class of methods is based on mathematical analysis. In the milestone paper [16], Friedgut used the second moment method to prove the existence of a nonuniform satisfiability threshold, i.e. a sequence rk(n), around which the probability of satisfiability goes from 1 to 0. Inspired by [16], Dimitris and Cristopher [1] further narrowed the threshold around O(2k−1 ln 2).\nAnother method is to design and analyze a polynomial algorithm that can find a truth assignment with uniformly positive probability (w.u.p.p. limn→∞ inf Pr( n) > 0) or w.h.p., if, for a satisfiable Fn,rnk , r is smaller than the lower bound of rk. In [6] and [8] this method is used to narrow the lower bound of rk to O(2k/k). The current best result is from [9], which not only presents a polynomial algorithm that finds a satisfying truth assignment w.h.p., if r < (1 − k)2k ln(k)/k, where k → 0 and k > 10, but also points out that if r is above O(2k ln(k)/k) no polynomial algorithm is known to find a satisfying truth assignment with probability Ω(1) – neither on the basis of rigorous or empirical analysis, or any other evidence.\n3.3 The k-Test Algorithm\nIf a one-to-one mapping from the set of consistent samples from a Bayesian network to a set of satisfying truth assignments of a satisfiable k-CNF Fn,mk can be constructed, then the clause density r = m/n can be compared to the threshold value 2k/k to estimate the hardness of the rejection problem to sample the network. The k-test ratio of a Bayesian network is the ratio of the clause density r to the threshold value 2k/k. What remains is to devise an efficient construction method for the k-CNF of a Bayesian network to determine the k-test ratio r : 2k/k. The construction of a k-CNF Fn,mk should satisfy the following requirements:\n1. The satisfying truth assignments of the k-CNF Fn,mk model all and only the consistent configurations of the Bayesian network.\n2. The k-CNF Fn,mk should be minimal. That is, there should not be too many unnecessary binary variables introduced in the k-CNF or too many unnecessary clauses added to the k-CNF.\n3. The construction of the k-CNF and calculation of the clause density r should be performed in polynomial time.\nIn the following, we define an efficient conversion from a Bayesian network to a k-CNF Fn,mk that satisfies these three requirements.\nLet BN = (G,Pr), G = (V,A) be a Bayesian network and E the evidence, where e is the evidence configuration. Let ‖Vi‖ denote the number of states of Vi ∈ V. The conversion of the BN to a Boolean formula proceeds in two steps.\nStep 1. Convert all variables V to Boolean variables by log encoding [18]. For each Vi ∈ V, we create a set\nof Boolean variables:\n{Xi1 , Xi2 , · · · , Xidlog2 ‖Vi‖e} (6)\nMap each Xik to the k th bit of the binary representation of Vi’s discrete value. For example, if ‖Vi‖ = 5, then Vi = 3 is mapped to (Xi3 = 0, Xi2 = 1, Xi1 = 1).\nStep 2. Construct the k-CNF formula Fn,mk . There are three different types of clauses in the resulting kCNF: the CPT Clauses, the Variable Clauses, and the Evidence Clauses. The aim is to construct a k-CNF that is minimal, which is accomplished as follows.\nCPT Clauses: each zero entry in a CPT of the Bayesian network represents a constraint that is translated into a disjunctive clause. For example, assume ‖Vi‖ = 5, ‖Vj‖ = 3, then we can translate Pr(Vi = 3 | Vj = 2) = 0 to ¬(¬Xi3 ∧Xi2 ∧Xi1 ∧ Xj2 ∧ ¬Xj1) = Xi3 ∨ ¬Xi2 ∨ ¬Xi1 ∨ ¬Xj2 ∨Xj1 .\nVariable Clauses: for each Vi ∈ V, if ‖Vi‖ is not a power of 2, then certain assignments of Vi’s translated Boolean variables do not satisfy the formula. For example, if ‖Vi‖ = 5, then {Xi1 = 1, Xi2 = 1, Xi3 = 1} is not a valid assignment, since 7 is not a valid value of Vi. We add a clause for each invalid assignment. For the previous example, this means that ¬(Xi1 ∧ Xi2 ∧ Xi3) is added. However, if this mapping is done näıvely, the result may yield an exponentially large set of clauses, which clearly does not lead to a minimal k-CNF. Consider ‖Vj‖ = 2m + 1, then for Vj as many as 2m−1 clauses will be added to the k-CNF formula. In the worst case, this leads to a (2m−1)/(m+1) clause/variable ratio. Fortunately, we can take advantage of the fact that all valid discrete values of Vj are smaller than ‖Vj‖ to minimize the number of clauses. Let 1bm · · · b1 be the binary representation of ‖Vj‖−1. Then, for k = m, . . . , 1 such that bk = 0, we have that\nXjm+1 = 1 ∧ · · · ∧Xjk+1 = bk+1 → Xjk = 0. (7)\nFormula (7) can be converted to a disjunctive clause by A → B ⇔ ¬A ∨ B, since Xl = 1(0) is simply equivalent to Xl = true(false). In the worst case, the variable clauses only contribute m/(m+ 1) clauses to the clause density.\nEvidence Clauses: for each Vi ∈ E with state Vi = ei, let bmbm−1 · · · b1, m = ‖Vi‖, be the binary representation of ei. Add the clause Xim = bm ∧ Xim−1 = bm−1 ∧ · · · ∧Xi1 = b1 to the k-CNF.\nTo compute the k-test ratio r : 2k/k of a Bayesian network, only the density r of the k-CNF clauses is\nProcedure k-Test (BN ,E) Input: BN = (G,CPT ), evidence set E ⊆ V(G) Output: k, clause density r begin\nn← ∑ Vi∈V(G) dlog2 ‖Vi‖e m← |E| k ← maxE∈E dlog2 ‖E‖e foreach Vi ∈ V(G) do\nki ← dlog2 ‖Vi‖e b← ‖Vi‖ − 1 while b > 0 do\nif b mod 2 = 0 then m← m+ 1 if ki > k then k ← ki end b← bb/2c ki ← ki − 1\nend kmax ← ∑ Vj∈π[Vi] dlog2 ‖Vj‖e + dlog2 ‖Vi‖e foreach Prj ∈ CPT [Vi] do if Prj = 0 then\nm← m+ 1 if kmax > k then k ← kmax\nend end\nend r ← m/n\nend\nAlgorithm 1: k-Test\nrequired. Hence, the k-test algorithm is a simplified version of the k-CNF construction algorithm, in which the number of variables n, number of k-clauses m, and k are computed directly from a Bayesian network as shown in Algorithm 1. The algorithm determines k and the clause density r = m/n given a Bayesian network BN and set of evidence variables E. The time complexity of the algorithm is linear to the sum of the sizes of the CPTs in the network. From our empirical results reported in the next section, we found that the k-test only takes seconds to compute for large networks with hundreds of variables."
    }, {
      "heading" : "4 Results",
      "text" : "In this section the k-test is experimentally verified. Two classes of Bayesian networks were used in the experiments: “real-world” networks and benchmark networks. The real-world networks are shown in Table 1. Munin [3], Pathfinder [24], Andes [10] and CPCS [29] are networks with significant levels of determinism, suggesting difficulties with high rejection rates to sample them. A selection1 of synthetic bench-\n1We selected networks that are easy to hard to sample.\nmark networks from the UAI contest [5] is shown in Table 2. Both tables show the number of variables and arcs of the network, the directly computed k-test ratio r : 2k/k, and the average rejection rate of näıve sampling. To eliminate any bias of advanced sampling techniques toward any of these networks, näıve importance sampling (likelihood sampling) is used in this study. In this way, rejection rates purely depend on the properties of the JPD of the Bayesian network, not on adaptive CPT learning-based optimizations to sample the network as is performed by SIS, AIS-BN, and other advanced algorithms. Furthermore, for each Bayesian network, we randomly generated 50 test cases, and for each test case a random set of 10 to 20 evidence variables and instantiations are randomly selected. The number of samples is 60,000 for each test case.\nFigure 1 compares the directly computed ratio r : 2k/k to the empirically-established rejection rate of likelihood importance sampling for all Bayesian Networks used in this study. In the figure, a rejection rate of 1.0 means that all samples are rejected in the sampling process (100%).\nFrom these results it can be concluded that the k-test\nratio r : 2k/k accurately predicts when the sampling rejection rate will be modest, high, or reaches 100% and sampling becomes intractable. When the k-test ratio < 0.1, sample rejection rates are modest or zero, see Tables 1 and 2. The sampling efficiency is poor when the k-test ratio reaches 0.1. When the k-test ratio > 0.2, sampling is intractable. No importance sampling algorithm can generate consistent samples for the synthetic BNs from BN 69uai to BN 76uai in reasonable time. Likelihood sampling does not even yield a single valid sample in thousands of samples, see Table 2.\nWhen the k-test ratio r : 2k/k of a network is between 0.005 and 0.200, improved importance sampling methods can be used to attempt to lower the sampling rejection rate and thus improve efficiency of sampling. Example sampling improvements are SIS [32], AIS-BN [7], DIS [27], RIS [34] and EPIS-BN [35].\nState-of-art importance sampling algorithms are known to perform well on Andes (k-test ratio 0.018, the average rejection ratio of AIS-BN is 13.9%) and Pathfinder (k-test ratio 0.173, the average rejection ratio of AIS-BN is 49.5%) as these algorithms mitigate the rejection problem. However, their performance is mixed on Munin (k-test ratio 0.112, the average rejection ratio of AIS-BN is 96.1% and the average rejection ratio of EPIS-BN is 28.5%).\nEasy-to-sample networks do not require sophisticated sampling techniques. For those networks the k-test ratio < 0.005. Indeed, sampling the CPCS networks (Table 1) incurs no rejection overhead. Simple sampling methods suffice for these networks."
    }, {
      "heading" : "5 Conclusions",
      "text" : "This paper introduced the k-test to measure the hardness of stochastically sampling Bayesian networks that exhibit zero probabilities. Such networks have deterministic causalities defined by the zeros in the conditional probability tables (CPT), which results in samples being rejected. To empirically estimate the rejection rate requires a significant number of samples to be produced to cover the exponential state space of a network. It also requires the use of a set of sampling algorithms to eliminate algorithm bias (such as CPT learning effects). By contrast, the k-test is a linear-time algorithm to determine the hardness of stochastically sampling a Bayesian network and is a good estimator of the rejection rate for any sampling algorithm. The metric identifies networks for which rejection rates will be low, modest, high, or when sampling is intractable. The k-test is based on recent advances in random kSAT analysis. Experimental results for real-world and benchmark networks show the experimental validity of the k-test.\nSampling algorithms have been modified and improved by many authors to mitigate the generation of inconsistent samples and limit the overhead of sample rejection. The rejection problem in importance sampling has been extensively studied in the work on adaptive sampling schemes [7], in the context of constraint propagation [20], and Boolean satisfiability problems [21]. A restricted form of constraint propagation can be used to reduce the amount of rejection [19]. An approach to circumvent the rejection problem by systematically searching for a nonzero weight sample for constraintbased systems was introduced in [20]. The proposed backtracking algorithm, SampleSearch was further improved in [21] and shown to generate a backtrack-free distribution. In [22], the SampleSearch method is fur-\nther generalized as a scheme in the framework of mixed networks [14, 26]. However, the exact rejection problem is NP-complete and the approximate rejection problem is too hard to be polynomial as we proved in this paper. Because Bayesian networks are special cases of mixed networks, we believe that Corollary 1, Theorem 2 and Theorem 1 can be generalized to mixed networks.\nAlthough most state-of-art importance sampling algorithms have a capability to reduce the generation of inconsistent samples, in worst case they still fail to generate sufficient useful samples in reasonable time. It is therefore critical to identify Bayesian networks that are hard to sample, e.g. by using the k-test.\nThe LVB [13] metric demarcates the boundary between the class of Bayesian networks with tractable approximations and those with intractable approximations. LVB is undefined for many real-world networks that model JPDs with zero probabilities, such as Munin [3], Pathfinder [24], Andes [10] and the CPCS [29] networks. The k-test compliments the LVB by measuring the approximation hardness of these and many other Bayesian networks with deterministic causalities, i.e. networks that model JPDs with zero probabilities. LVB measures the hardness of sampling caused by strictly-positive extreme probability distribution, whereas the k-test measures the difficulties of sampling induced by the rejection problem. Currently there is no satisfactory combination of these two measurements that provides a general metric to measure the hardness of sampling a Bayesian network. This will be an interesting and challenging forthcoming work, because a combined metric enables the measurement of the sampling hardness of networks that exhibit both zero and close-to-zero probabilities."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 Probabilistic Turing Machines\nThe Probabilistic Turing Machine (PTM) formulation is used in the complexity analysis of approximate algorithms and probabilistic algorithms. We briefly introduce PTM and class RP, see [4] for more details.\nDefinition 2 The Probabilistic Turing Machine (PTM) is a Turing machine with two transition function sets λ0, λ1. To execute a PTM M on an input x, we choose in each step with probability 12 to apply the transition function in λ0 and with probability 1 2 to apply λ1. This choice is made independently.\nThe machine M only outputs 1 (“accepted”) or 0 (“rejected”). We denote by M(x) the random variable corresponding to the value M outputs at the end of execution. For a function T : N → N, we say that M runs in T (n)-time if for any input x, M halts on x within T (|x|) steps regardless of the random choices it makes.\nDefinition 3 RTIME(T (n)) contains every language L for which there is a PTM M running in T (n) time such that\nx ∈ L⇒ Pr(M(x) = 1) ≥ 23 x /∈ L⇒ Pr(M(x) = 0) = 1 (8)\nWe define RP = ⋃ c>0 RTIME(n c)\nObviously RP ⊆ NP.\nA.2 Hardness of the Sample Rejection Problem\nFirst, we give a formal definition of sample rejection problem:\nDefinition 4 For any tractable2 importance function I with tractable sampling order δ, For a BN, e are evidences and e 6= ∅. I is an importance function and δ is a sampling order. Pre(·) is the BN’s posterior probability distribution (Pre(·) ≡ 0, if Pr(e) = 0). the Rejection Problem of I with δ is defined as: let e 6= ∅, Pr(e) > 0, be the observed evidence3, if PrI(xδ(1), · · · , xδ(m−1)) > 0 (m ≥ 1), then ∀x ∈ Config(Xδ(m)) determine whether Pre(xδ(1), · · · , xδ(m−1), x) > 0.\nDef. 4 is reasonable, because if during sampling of the mth variable Xδ(m) the rejection problem is solved, then we can pick up a xδ(m) from Config(Xδ(m)) such that Pre(xδ(1), · · · , xδ(m)) > 0. This process repeats until we find a consistent sample.\nNote that Def. 4 requires nothing when Pr(e) = 0. Thus, the exact rejection algorithm is a partial function (Rejection Function) Γδ : Ωδ → {0, 1}, Ωδ = Config(E)× ⋃|X| i=1 Config(Xδ(1) · · ·Xδ(i)). If Pr(e) = 0,\n2Only tractable importance functions with tractable sampling orders are considered, because the sampling process should be polynomial.\n3e 6= ∅ because generating a consistent sample on a BN without evidence is a trivial problem.\nΓδ(e, ·) is undefined (could be any); if Pr(e) > 0,\nΓδ(e, xδ(1) · · ·xδ(m)) = {\n1, Pre(xδ(1) · · ·xδ(m)) > 0 0, Pre(xδ(1) · · ·xδ(m)) = 0\nHere, the sampling probability distribution PrΓI , induced by importance function I and rejection function Γδ, is obtained by:\nPrΓI (xδ(m) | xδ(1) · · ·xδ(m−1)) = PrI(xδ(m)|xδ(1)···xδ(m−1))×Γδ(xδ(m)|xδ(1)···xδ(m−1))P\nx∈Config(Xδ(m)) PrI(x|xδ(1)···xδ(m−1))×Γδ(xδ(1)···xδ(m−1),x)\n.\n(9)\nAssuming Pr(e) > 0, a rejection algorithm solves the rejection problem in computable T (n) time, for input with length n. The ill-defined case Pr(e) = 0 can be bounded by counting down from T (n) and returning a random decision when the counter reaches 0. Hence, it can be assumed that rejection algorithms are time bounded.\nThe sample rejection problem is NP-complete. That is, there is no polynomial time algorithm that generally classifies consistent and inconsistent samples from Bayesian networks.\nLemma 1 For any tractable importance function I with tractable sampling order δ, the rejection problem is in NP.\nLemma 1 is straightforward because verifying whether a sample is consistent is O(n). To prove that the rejection problem is NP-complete, we reduce the 3SAT problem into the rejection problem and the reduction is polynomial.\nCorollary 1 For any tractable importance function I with tractable sampling order δ, the sample rejection problem is NP-complete.\nProof. This follows from [11]. For any 3CNF F , we convert F to PIBNET by [11]. Assume that Y (the descendent vertex of all the other vertices in PIBNET, which represents the value of F) is the only evidence and that Y = True. For any tractable importance function I with sampling order δ, if the rejection problem can be resolved in polynomial time, we can obtain a full sample by Eq. (9) in polynomial time. In case the denominator of Eq. (9) is zero, we can randomly pick a value of Xδ(m). If Pr(Y = True) > 0, then Γ is well defined and ensures one consistent sample (its weight > 0). If Pr(Y = True) = 0 the generated sample must be inconsistent. Hence, we can differentiate Pr(Y = True) = 0 from Pr(Y = True) > 0 by solving the rejection problem. From Lemma 1 the rejection problem is in NP, and therefore the rejection problem is NP-complete. 2\nApproximate sample rejection problem is also NPhard. A rejection algorithm is called approximate, if it may accept xδ(m), even when Pr(e, xδ(1) · · ·xδ(m)) = 0, for Pr(e) > 0 ∧ PrI(xδ(1) · · ·xδ(m−1)) > 0. Still, if Pr(e, xδ(1) · · ·xδ(m)) > 0 then an approximate algorithm must accept it to avoid biased sampling. In other words, an approximate rejection algorithm is an one-side error approximation. Furthermore, an approximate rejection function Γ̂Aδ of an approximate rejection algorithm A is defined like Γδ: if Pr(e) > 0,\nΓ̂Aδ (e, xδ(1) · · ·xδ(m)) = { 1, if A accepts xδ(1) · · ·xδ(m) 0, else\nWhen there is no confusion, Γ̂Aδ is simplified in this text as Γ̂δ or simply Γ̂.\nLet PrΓ̂I denote the sampling probability distribution induced by importance function I and Γ̂ (PrΓ̂I can be computed from Eq. 9 where Γδ is replaced by Γ̂). Then, for inconsistent sample set Ωe = {x | x ∈ Config(X)∧Pre(x) = 0}, PrΓ̂I (Ωe) > 0. Pr Γ̂ I (Ωe) gives the probability that inconsistent samples are generated or the average percentage of inconsistent samples over all the samples. If PrΓ̂I (Ωe) = 0, we get an exact algorithm. Clearly for an approximate rejection algorithm, the smaller PrΓ̂I (Ωe) is, the better is the approximation. We hope that there exists a polynomial ξ up-bounded (ξ up-bounded means ∃ξ, 0 < ξ < 1, for any BN and any possible evidences e (Pr(e) > 0), PrΓ̂I (Ωe) < ξ) approximate rejection algorithm, so that n consistent samples can be retrieved from nξ samples with high probability.\nFurthermore, since both the algorithm for computing the importance function and the rejection algorithm may be stochastic, for a BN and evidence e, the PrΓ̂I (Ωe) may be a random variable. Thus, it is reasonable to define randomly up-bounded or (ξ, σ) up-bounded as ∃ξ σ, 0 < ξ < 1 ∧ 0 ≤ σ < 1, for any BN and BN’s evidence e (Pr(e) > 0), such that Pr[PrΓ̂I (Ωe) < ξ] ≥ 1 − σ. In other words, we relax the requirement of ξ up-bounded to the case where ξ up-bounded is satisfied with high probability. However Theorem 1 gives a pessimistic answer.\nTheorem 1 If there exists a polynomial (ξ, σ) upbounded approximate rejection algorithm for some tractable importance function I with tractable sampling order δ, then NP ⊆ RP.\nProof. Assume that Γ̂Aδ is the approximate rejection function of approximate rejection algorithm A and PrΓ̂I is the sampling probability distribution induced by importance function I and Γ̂Aδ . Let ξ, 0 < ξ < 1 and σ,\n0 ≤ σ < 1. Then for any 3CNF F , we convert F to PIBNET by the method of [11]. In PIBNET, value of vertex Y is corresponding to value of F . For Y = True as the only evidence. Then we independently execute importance sampling process m (m > − ln 3ln(σ+(1−σ)ξ) ) times with tractable importance function I and approximate rejection algorithm A. Since both generating importance function I and rejection algorithm A maybe stochastic, we may obtain m different PrΓ̂I (·). Then we generate a sample from each PrΓ̂I (·). If any one of the m samples is consistent, we accept F . If none of them is consistent, we reject.\n• If F is unsatisfiable, no consistent sample can be generated.\n• If F is satisfiable, Pr[PrΓ̂I (Ωe) < ξ] ≥ 1 − σ ⇒ Pr(F rejected) < ∑m i=0 ( m i ) σm−i(1−σ)iξi. Since∑m\ni=0 ( m i ) σm−i(1 − σ)iξi = (σ + (1 − σ)ξ)m, σ + (1 − σ)ξ < 1 and m > − ln 3ln(σ+(1−σ)ξ) , so Pr(F rejected) < 13 .\nHence, NP ⊆ RP, if a polynomial (ξ, σ) up-bounded approximate rejection algorithm exists. 2\nSince RP ⊆ P/poly [2] and NP ⊆ P/poly ⇒ PH = Σ2 [33, 25], if a polynomial (ξ, σ) up-bounded approximate rejection algorithm exists, then PH will collapse to Σ2. It is widely believed that PH does not collapse to Σ2, thus a polynomial (ξ, σ) up-bounded approximate algorithm is unlikely to exist. Furthermore Theorem 1 tells that all tractable importance sampling algorithms may fail to generate sufficient samples in polynomial time for certain cases. Another implication of Theorem 2 is that for any tractable importance function I sampling order δ, no polynomial approximate rejection algorithm satisfies ∃ξ σ : 0 < ξ < 1 ∧ 0 ≤ σ < 1, for any BN and valid e such that PrΓ̂I (Ωe) < ξ PrI(Ωe) with probability larger than 1−σ unless PH collapses to Σ2. Because PrI(Ωe) < 1, so PrΓ̂I (Ωe) < ξ PrI(Ωe) ⇒ Pr Γ̂ I (Ωe) < ξ. In other words, no polynomial approximate rejection algorithm can help importance sampling to reduce inconsistent samples with high probability.\nBecause (ξ, 0) up-bounded is equivalent to ξ upbounded, It is straightforward to get the corollary 2.\nCorollary 2 If there exists a polynomial ξ up-bounded approximate rejection algorithm for some tractable importance function I with tractable sampling order δ, then NP ⊆ RP."
    } ],
    "references" : [ {
      "title" : "Random k-SAT: Two moments suffice to cross a sharp threshold",
      "author" : [ "D. Achlioptas", "C. Moore" ],
      "venue" : "SIAM J. Comput.,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2006
    }, {
      "title" : "Two theorems on random polynomial time",
      "author" : [ "L. Adleman" ],
      "venue" : "SFCS",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1978
    }, {
      "title" : "MUNIN — an expert EMG assistant",
      "author" : [ "S. Andreassen", "F. Jensen", "S. Andersen", "B. Falck", "U. Kjærulff", "M. Woldbye", "A.R. Sorensen", "A. Rosenfalck" ],
      "venue" : "Computer-Aided Electromyography and Expert Systems,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1989
    }, {
      "title" : "Computational Complexity: A Modern Approach",
      "author" : [ "S. Arora", "B. Barak" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2009
    }, {
      "title" : "Evaluation of Probabilistic Inference System, 2006. http://ssli.ee.washington.edu/~bilmes/ uai06InferenceEvaluation",
      "author" : [ "J. Bilmes", "R. Dechter" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2006
    }, {
      "title" : "Probabilistic analysis of a generalization of the unit clause literal selection heuristic for the k-satisfiability problem",
      "author" : [ "M. Chao", "J. Franco" ],
      "venue" : "Information Science,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1990
    }, {
      "title" : "AIS-BN: An adaptive importance sampling algorithm for evidential reasoning in large Bayesian networks",
      "author" : [ "J. Cheng", "M.J. Druzdzel" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2000
    }, {
      "title" : "Mick gets some (the odds are on his side) (satisfiability)",
      "author" : [ "V. Chvatal", "B. Reed" ],
      "venue" : "SFCS",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1992
    }, {
      "title" : "A better algorithm for random k-SAT",
      "author" : [ "A. Coja-Oghlan" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2010
    }, {
      "title" : "On-line student modeling for coached problem solving using Bayesian networks",
      "author" : [ "C. Conati", "A.S. Gertner", "K. VanLehn", "M.J. Druzdzel" ],
      "venue" : "In Proceedings of the Sixth International Conference on User Modeling",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1997
    }, {
      "title" : "The computational complexity of probabilistic inference using Bayesian belief networks",
      "author" : [ "G.F. Cooper" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1990
    }, {
      "title" : "Approximating probabilistic inference in Bayesian belief networks is NP-hard",
      "author" : [ "P. Dagum", "M. Luby" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1993
    }, {
      "title" : "An optimal approximation algorithm for Bayesian inference",
      "author" : [ "P. Dagum", "M. Luby" ],
      "venue" : "Artif. Intell.,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1997
    }, {
      "title" : "Mixtures of deterministic-probabilistic networks and their and/or search space",
      "author" : [ "R. Dechter", "R. Mateescu" ],
      "venue" : "Proceedings of the 20th conference on Uncertainty in artificial intelligence,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2004
    }, {
      "title" : "Probabilistic analysis of the Davis Putnam procedure for solving the satisfiability problem",
      "author" : [ "J. Franco", "M. Paull" ],
      "venue" : "Discrete Applied Mathematics,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1983
    }, {
      "title" : "Necessary and sufficient conditions for sharp thresholds of graph properties and k- SAT problem",
      "author" : [ "E. Friedgut" ],
      "venue" : "J. Amer. Math. Soc.,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1999
    }, {
      "title" : "A survey of research in deliberative real-time artificial intelligence",
      "author" : [ "A.J. Garvey", "V.R. Lesser" ],
      "venue" : "Real-Time Systems,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1994
    }, {
      "title" : "The log-support encoding of CSP into SAT",
      "author" : [ "M. Gavanelli" ],
      "venue" : "In Proceedings of the 13th international conference on principles and practice of constraint programming,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2007
    }, {
      "title" : "Approximate inference algorithms for hybrid Bayesian networks with discrete constraints",
      "author" : [ "V. Gogate", "R. Dechter" ],
      "venue" : "In Proceedings of Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2005
    }, {
      "title" : "A new algorithm for sampling CSP solutions uniformly at random",
      "author" : [ "V. Gogate", "R. Dechter" ],
      "venue" : "In International Conference on Principles and Practice of Constraint Programming (CP),",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2006
    }, {
      "title" : "Approximate counting by sampling the backtrack-free search space",
      "author" : [ "V. Gogate", "R. Dechter" ],
      "venue" : "In Conference on Artificial Intelligence (AAAI),",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2007
    }, {
      "title" : "SampleSearch: Importance sampling in presence of determinism",
      "author" : [ "V. Gogate", "R. Dechter" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2011
    }, {
      "title" : "A survey on algorithms for real-time Bayesian network inference. In In the joint AAAI-02/KDD-02/UAI-02 workshop on Real-Time Decision Support and Diagnosis",
      "author" : [ "H. Guo", "W. Hsu" ],
      "venue" : null,
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2002
    }, {
      "title" : "Toward normative expert systems: Part I the pathfinder project",
      "author" : [ "D.E. Heckerman", "E.J. Horvitz", "B.N. Nathwani" ],
      "venue" : "Methods of Information in Medicine,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1992
    }, {
      "title" : "Turing machines that take advice",
      "author" : [ "R. Karp", "R. Lipton" ],
      "venue" : "L’Enseignement Mathématiques,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 1982
    }, {
      "title" : "Mixed deterministic and probabilistic networks",
      "author" : [ "R. Mateescu", "R. Dechter" ],
      "venue" : "Annals of Mathematics and Artificial Intelligence,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2008
    }, {
      "title" : "Dynamic importance sampling in Bayesian networks based on probability trees",
      "author" : [ "S. Moral", "A. Salmerón" ],
      "venue" : "International Journal of Approximate Reasoning,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2005
    }, {
      "title" : "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference",
      "author" : [ "J. Pearl" ],
      "venue" : null,
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 1988
    }, {
      "title" : "Knowledge engineering for large belief networks",
      "author" : [ "M. Pradhan", "G. Provan", "B. Middleton", "M. Henrion" ],
      "venue" : "In Proceedings of the 10th conference on Uncertainty in artificial intelligence,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 1994
    }, {
      "title" : "Simulation and Monte Carlo Method",
      "author" : [ "R.Y. Rubinstein" ],
      "venue" : null,
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 1981
    }, {
      "title" : "Artificial intelligence: A modern approach",
      "author" : [ "S. Russell", "P. Norvig" ],
      "venue" : null,
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 1995
    }, {
      "title" : "Simulation approaches to general probabilistic inference on belief networks",
      "author" : [ "R.D. Shachter", "M.A. Peot" ],
      "venue" : "In Proceedings of the 5th Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 1990
    }, {
      "title" : "Halting space-bounded computations",
      "author" : [ "M. Sipser" ],
      "venue" : "Theor. Comput. Sci.,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 1980
    }, {
      "title" : "Refractor importance sampling",
      "author" : [ "H. Yu", "R. van Engelen" ],
      "venue" : "In Proceedings of the 24th Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2008
    }, {
      "title" : "An importance sampling algorithm based on evidence prepropagation",
      "author" : [ "C. Yuan", "M.J. Druzdzel" ],
      "venue" : "In Proceedings of the 19th Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2003
    }, {
      "title" : "Theoretical analysis and practical insights on importance sampling in Bayesian networks",
      "author" : [ "C. Yuan", "M.J. Druzdzel" ],
      "venue" : "Int. J. Approx. Reasoning,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 27,
      "context" : "A Bayesian network, belief network, or directed acyclic graphical model, is a probabilistic graph model [28] that has gained wide acceptance in several areas of",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 30,
      "context" : "artificial intelligence [31].",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 10,
      "context" : "Exact and approximate probabilistic Bayesian inference is known to be NP-hard [11, 12].",
      "startOffset" : 78,
      "endOffset" : 86
    }, {
      "referenceID" : 11,
      "context" : "Exact and approximate probabilistic Bayesian inference is known to be NP-hard [11, 12].",
      "startOffset" : 78,
      "endOffset" : 86
    }, {
      "referenceID" : 16,
      "context" : "Approximate inference algorithms are popular due to their anytime property [17] to produce an approximate result, possibly in real time [23].",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 22,
      "context" : "Approximate inference algorithms are popular due to their anytime property [17] to produce an approximate result, possibly in real time [23].",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 30,
      "context" : "For this reason, stochastic sampling algorithms, especially importance sampling, are among the most widely-used approximate inference methods [31].",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 31,
      "context" : "Examples are SIS [32], AIS-BN [7], DIS [27], RIS [34] and EPIS-BN [35].",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 6,
      "context" : "Examples are SIS [32], AIS-BN [7], DIS [27], RIS [34] and EPIS-BN [35].",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 26,
      "context" : "Examples are SIS [32], AIS-BN [7], DIS [27], RIS [34] and EPIS-BN [35].",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 33,
      "context" : "Examples are SIS [32], AIS-BN [7], DIS [27], RIS [34] and EPIS-BN [35].",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 34,
      "context" : "Examples are SIS [32], AIS-BN [7], DIS [27], RIS [34] and EPIS-BN [35].",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 12,
      "context" : "The local variance bound (LVB) [13] metric demarcates the boundary between the class of Bayesian networks with tractable approximations and those with intractable approximations.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 2,
      "context" : "This means that the LVB is undefined for many real-world networks that model JPDs with zero probabilities, such as Munin [3], Pathfinder [24], Andes [10] and the CPCS [29] networks.",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 23,
      "context" : "This means that the LVB is undefined for many real-world networks that model JPDs with zero probabilities, such as Munin [3], Pathfinder [24], Andes [10] and the CPCS [29] networks.",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 9,
      "context" : "This means that the LVB is undefined for many real-world networks that model JPDs with zero probabilities, such as Munin [3], Pathfinder [24], Andes [10] and the CPCS [29] networks.",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 28,
      "context" : "This means that the LVB is undefined for many real-world networks that model JPDs with zero probabilities, such as Munin [3], Pathfinder [24], Andes [10] and the CPCS [29] networks.",
      "startOffset" : 167,
      "endOffset" : 171
    }, {
      "referenceID" : 29,
      "context" : "For more details refer to [30, 36].",
      "startOffset" : 26,
      "endOffset" : 34
    }, {
      "referenceID" : 35,
      "context" : "For more details refer to [30, 36].",
      "startOffset" : 26,
      "endOffset" : 34
    }, {
      "referenceID" : 6,
      "context" : "This is true for AIS-BN [7], EPIS-BN [35], and SIS [32].",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 34,
      "context" : "This is true for AIS-BN [7], EPIS-BN [35], and SIS [32].",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 31,
      "context" : "This is true for AIS-BN [7], EPIS-BN [35], and SIS [32].",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 26,
      "context" : "By contrast, the tractable sampling order of DIS [27] is a reversed elimination order, which may or may not be consistent with a Bayesian network’s topological order of the vertices.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 10,
      "context" : "Cooper [11] proved that computing Pr(x) for any x is NP-hard in general.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 11,
      "context" : "Dagum and Luby [12] proved that approximate Bayesian inference is NP-hard.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 14,
      "context" : "Franco and Paull [15] first observed, among other important results, that random instances of the k-SAT problem undergo a “phase transition” as the ratio of clauses to variables passes through a threshold.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 14,
      "context" : "Franco and Paull [15] claim that F k is with high probability (w.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 15,
      "context" : "stone paper [16], Friedgut used the second moment method to prove the existence of a nonuniform satisfiability threshold, i.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 15,
      "context" : "Inspired by [16], Dimitris and Cristopher [1] further narrowed the threshold around O(2k−1 ln 2).",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 0,
      "context" : "Inspired by [16], Dimitris and Cristopher [1] further narrowed the threshold around O(2k−1 ln 2).",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 5,
      "context" : "In [6] and [8] this method is used to narrow the lower bound of rk to O(2/k).",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 7,
      "context" : "In [6] and [8] this method is used to narrow the lower bound of rk to O(2/k).",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 8,
      "context" : "The current best result is from [9], which not only presents a polynomial algorithm that finds a satisfying truth assignment w.",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 17,
      "context" : "Convert all variables V to Boolean variables by log encoding [18].",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 2,
      "context" : "Munin [3], Pathfinder [24], Andes [10] and CPCS [29] are networks with significant levels of determinism, suggesting difficulties with high rejection rates to sample them.",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 23,
      "context" : "Munin [3], Pathfinder [24], Andes [10] and CPCS [29] are networks with significant levels of determinism, suggesting difficulties with high rejection rates to sample them.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 9,
      "context" : "Munin [3], Pathfinder [24], Andes [10] and CPCS [29] are networks with significant levels of determinism, suggesting difficulties with high rejection rates to sample them.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 28,
      "context" : "Munin [3], Pathfinder [24], Andes [10] and CPCS [29] are networks with significant levels of determinism, suggesting difficulties with high rejection rates to sample them.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 4,
      "context" : "mark networks from the UAI contest [5] is shown in Table 2.",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 31,
      "context" : "Example sampling improvements are SIS [32], AIS-BN [7], DIS [27], RIS [34] and EPIS-BN [35].",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 6,
      "context" : "Example sampling improvements are SIS [32], AIS-BN [7], DIS [27], RIS [34] and EPIS-BN [35].",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 26,
      "context" : "Example sampling improvements are SIS [32], AIS-BN [7], DIS [27], RIS [34] and EPIS-BN [35].",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 33,
      "context" : "Example sampling improvements are SIS [32], AIS-BN [7], DIS [27], RIS [34] and EPIS-BN [35].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 34,
      "context" : "Example sampling improvements are SIS [32], AIS-BN [7], DIS [27], RIS [34] and EPIS-BN [35].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 6,
      "context" : "The rejection problem in importance sampling has been extensively studied in the work on adaptive sampling schemes [7], in the context of constraint propagation [20], and Boolean satisfiability problems [21].",
      "startOffset" : 115,
      "endOffset" : 118
    }, {
      "referenceID" : 19,
      "context" : "The rejection problem in importance sampling has been extensively studied in the work on adaptive sampling schemes [7], in the context of constraint propagation [20], and Boolean satisfiability problems [21].",
      "startOffset" : 161,
      "endOffset" : 165
    }, {
      "referenceID" : 20,
      "context" : "The rejection problem in importance sampling has been extensively studied in the work on adaptive sampling schemes [7], in the context of constraint propagation [20], and Boolean satisfiability problems [21].",
      "startOffset" : 203,
      "endOffset" : 207
    }, {
      "referenceID" : 18,
      "context" : "A restricted form of constraint propagation can be used to reduce the amount of rejection [19].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 19,
      "context" : "An approach to circumvent the rejection problem by systematically searching for a nonzero weight sample for constraintbased systems was introduced in [20].",
      "startOffset" : 150,
      "endOffset" : 154
    }, {
      "referenceID" : 20,
      "context" : "The proposed backtracking algorithm, SampleSearch was further improved in [21] and shown to generate a backtrack-free distribution.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 21,
      "context" : "In [22], the SampleSearch method is fur-",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 13,
      "context" : "ther generalized as a scheme in the framework of mixed networks [14, 26].",
      "startOffset" : 64,
      "endOffset" : 72
    }, {
      "referenceID" : 25,
      "context" : "ther generalized as a scheme in the framework of mixed networks [14, 26].",
      "startOffset" : 64,
      "endOffset" : 72
    }, {
      "referenceID" : 12,
      "context" : "The LVB [13] metric demarcates the boundary between the class of Bayesian networks with tractable approximations and those with intractable approximations.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 2,
      "context" : "LVB is undefined for many real-world networks that model JPDs with zero probabilities, such as Munin [3], Pathfinder [24], Andes [10] and the CPCS [29] networks.",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 23,
      "context" : "LVB is undefined for many real-world networks that model JPDs with zero probabilities, such as Munin [3], Pathfinder [24], Andes [10] and the CPCS [29] networks.",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 9,
      "context" : "LVB is undefined for many real-world networks that model JPDs with zero probabilities, such as Munin [3], Pathfinder [24], Andes [10] and the CPCS [29] networks.",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 28,
      "context" : "LVB is undefined for many real-world networks that model JPDs with zero probabilities, such as Munin [3], Pathfinder [24], Andes [10] and the CPCS [29] networks.",
      "startOffset" : 147,
      "endOffset" : 151
    } ],
    "year" : 2011,
    "abstractText" : "Approximate Bayesian inference is NP-hard. Dagum and Luby defined the Local Variance Bound (LVB) to measure the approximation hardness of Bayesian inference on Bayesian networks, assuming the networks model strictly positive joint probability distributions, i.e. zero probabilities are not permitted. This paper introduces the k-test to measure the approximation hardness of inference on Bayesian networks with deterministic causalities in the probability distribution, i.e. when zero conditional probabilities are permitted. Approximation by stochastic sampling is a widely-used inference method that is known to suffer from inefficiencies due to sample rejection. The k-test predicts when rejection rates of stochastic sampling a Bayesian network will be low, modest, high, or when sampling is intractable.",
    "creator" : "TeX"
  }
}