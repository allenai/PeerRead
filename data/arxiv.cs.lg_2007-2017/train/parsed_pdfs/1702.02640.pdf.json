{
  "name" : "1702.02640.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "CHARACTER-LEVEL DEEP CONFLATION FOR BUSINESS DATA ANALYTICS",
    "authors" : [ "Zhe Gan", "P. D. Singh", "Ameet Joshi", "Xiaodong He", "Jianshu Chen", "Jianfeng Gao", "Li Deng" ],
    "emails" : [ "zhe.gan@duke.edu,", "deng}@microsoft.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms— Deep conflation, character-level model, convolutional neural network, long-short-term memory"
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "In business data analytics, different fields and attributes related to the same entities (e.g., same person) are stored in different tables in a database or across different databases. It is important to connect these attributes so that we can get a more comprehensive and richer profile of the entity. This is important because exploiting a more comprehensive profile could lead to better prediction in business data analytics.\nSpecifically, the conflation of data aims to connect two rows from the same or different datasets that contain one or more common fields, when the values of the common fields match within a predefined threshold. For example, in the business data considered in this paper, we aim to detect whether two names refer to the same person or not — see the example in Table 1. Row A and row B represent two name fields from different tables in a database, which is a text string consisting of characters. The strings in the same column of Table 1 represent the names of a same person. There are several reasons for the strings in A and B being different: (i) possible mis-spelling typos; (ii) the lack of suffix; (iii) the reverse of family names and given names. Due to these variations and imperfection in\nEmails: zhe.gan@duke.edu, {prabhs, ameetj, xiaohe, jianshuc, jfgao, deng}@microsoft.com\ndata entries, plain keyword matching does not work well [1, 2], and we need a data conflation model in the semantic level; that is, the model should be able to identify two different character strings to be associated with a same entity.\nTo address the aforementioned challenges, we propose characterlevel deep conflation models that take the raw text strings as the input and predict whether two data entries refer to the same entity. The proposed model consists of two parts: (i) a deep feature extractor, and (ii) a ranker. The feature extractor takes the raw text string at the character level and produce a finite dimension representation of the text. In particular, we constructed two different deep architectures of feature extractors: (i) long-short-term-memory (LSTM) recurrent neural network (RNN) [3, 4] and (ii) deep convolutional neural network (CNN) [5, 6]. Both deep architectures are able to retain the order information in the input text and extract high-level features from raw data, as shown their great success in different machine learning tasks, including text classification [5, 7], machine translation [8, 9, 10, 11] and information retrieval [12, 13, 14]. Furthermore, extracting the features from the character-level is critical in many of the recent success in applying deep learning to natural language processing [15, 16, 17, 18, 19]. As we will show later, our proposed deep conflation model achieves high prediction accuracy in the conflation task for business data, and greatly outperform strong baselines."
    }, {
      "heading" : "2. CHARACTER-LEVEL DEEP CONFLATION MODELS",
      "text" : "We formulate the deep conflation problem as a ranking problem. That is, given a query string from field A, we rank all the target strings in field B, with the hope that the most similar string in B is ranked on the top of the list. The proposed deep conflation model consists of two parts: (i) a deep feature extractor; (ii) a ranker. Fig. 1 shows the architecture of the deep conflation model. The deep feature extractors transform the input text strings from character-level into finite dimension feature vectors. Then, the cosine similarity is computed between the query string from field A and all the target strings from field B. The cosine similarity value for each pair of text strings measures the semantic relevance between each pair of the text strings, according to which the target strings are ranked. The entire model will be trained in an end-to-end manner so that the deep feature extractors are encouraged to learn the proper feature vectors\nar X\niv :1\n70 2.\n02 64\n0v 1\n[ cs\n.C L\n] 8\nF eb\n2 01\n7\nDeep Feature Extractor\nDeep Feature Extractor\nDeep Feature Extractor\n……\nQuery string Target string 1 Target string J+1 Strings\nDeep features\n…… Cosine similarity\nFig. 1: Character-level deep conflation model.\nLSTM LSTM LSTM\nWe\n…\nWe We…\ne m … hString:\nFig. 2: LSTM based deep feature extractor.\nthat are measurable by cosine similarity. In the rest of this section, we will explain these two components of the deep conflation model with detail."
    }, {
      "heading" : "2.1. Deep Feature Extractors",
      "text" : "The inputs into the system are text strings, which are sequences of characters. Note that the order of the input characters and words is critical to understand the text correctly. For this reason, we propose to use two deep learning models that are able to retain the order information to extract features from the raw input character sequences. The two deep models we use are: (i) Recurrent Neural Networks (RNNs); (ii) Convolutional Neural Networks (CNNs).\nRNN is a nonlinear dynamic system that can be used for sequence modeling. However, during the training of a regular RNN, the components of the gradient vector can grow or decay exponentially over long sequences. This problem with exploding or vanishing gradients makes it difficult for the regular RNN model to learn long-range dependencies in a sequence [20]. A useful architecture of RNN that overcomes this problem is the Long Short-Term Memory (LSTM) structure. On the other hand, CNN is a deep feedforward neural network that first uses convolutional and max-pooling layers to capture the local and global contextual information of the input sequence, and then uses a fully-connected layer to produce a fixed-length encoding of the sequence. In sequel, we first introduce LSTM, and then CNN."
    }, {
      "heading" : "2.1.1. LSTM feature extractor",
      "text" : "The LSTM architecture [3] addresses the problem of learning longterm dependencies by introducing a memory cell, that is able to preserve the state over long periods of time. Specifically, each LSTM unit has a cell containing a state ct at time t. This cell can be viewed as a memory unit. Reading or writing the memory unit is controlled through sigmoid gates: input gate it, forget gate f t, and output gate\not. The hidden units ht are updated as follows:\nit = σ(Wixt +Uiht−1 + bi) , (1) f t = σ(Wfxt +Ufht−1 + bf ) , (2) ot = σ(Woxt +Uoht−1 + bo) , (3) c̃t = tanh(Wcxt +Ucht−1 + bc) , (4) ct = f t ct−1 + it c̃t , (5) ht = ot tanh(ct) , (6)\nwhere σ(·) denotes the logistic sigmoid function, and represents the element-wise multiply operator. Wi Wf , Wo, Wc, Ui, Uf , Uo, Uc, bi, bf , bo and bc are the free model parameters to be learned from training data.\nGiven the text string q = [q1, . . . , qT ], where qt is the one-hot vector representation of character at position t and T is the number of characters, we first embed the characters into a vector space via a linear transform xt = Weqt, where We is the embedding matrix. Then for every time step, we feed the embedding vector of characters in the text string to LSTM:\nxt = Weqt, t ∈ {1, . . . , T} , (7) ht = LSTM(xt), t ∈ {1, . . . , T} , (8)\nwhere the operator LSTM(·) denotes the operations defined in (1)- (6). For example, in Fig. 2, the string emilio yentsch is fed into the LSTM. The final hidden vector is taken as the feature vector for the string, i.e., y = hT . We repeat this process for the query text and all the target texts so that we will have yQ and yDj (j = 1, . . . J + 1), which will be fed into the ranker to compute cosine similarity (see Sec. 2.2).\nIn the experiments, we use a bidirectional LSTM to extract sequence features, which consists of two LSTMs that are run in parallel: one on the input sequence and the other on the reverse of the input sequence. At each time step, the hidden state of the bidirectional LSTM is the concatenation of the forward and backward hidden states."
    }, {
      "heading" : "2.1.2. CNN feature extractor",
      "text" : "Next, we consider the CNN for string feature extraction. Similar to the LSTM-based model, we first embed characters to vectors xt = Weqt and then concatenating these vectors:\nx1:T = [x1, . . . ,xT ] . (9)\nThen we apply convolution operation on the character embedding vectors. We use three different convolution filters, which have the size of two (bigram), three (trigram) and four (4-gram), respectively. These different convolution filters capture the context information of different lengths. The t-th convolution output using window size c is given by\nhc,t = tanh(Wcxt:t+c−1 + bc) , (10)\nwhere the notation xt:t+c−1 denotes the vector that is constructed by concatenating xt to xt+c−1. That is, the filter is applied only to window t : t + c − 1 of size c. Wc is the convolution weight and bc is the bias. The feature map of the filter with convolution size c is defined as\nhc = [hc,1,hc,2, . . . ,hc,T−c+1] . (11)\nWe apply max-pooling over the feature maps of the convolution size c and denote it as\nĥc = max{hc,1,hc,2, . . . ,hc,T−c+1} , (12)\nwhere the max is a coordinate-wise max operation. For convolution feature maps of different sizes c = 2, 3, 4, we concatenate them to form the feature representation vector of the whole character sequence: h = [ĥ2, ĥ3, ĥ4] . Observe that the convolution operations explicitly capture the local (short-term) context information in the character strings, while the max-pooling operation aggregates the information from different local filters into a global representation of the input sequence. These local and global operations enable the model to encode different levels of dependency in the input sequence.\nThe above vector h is the final feature vector extracted by CNN and will be fed into the ranker, i.e., y = h. We repeat this process for the query text and all the target texts so that we will have yQ and yDj (j = 1, . . . J + 1). The above feature extraction process using CNN is illustrated in Fig. 3.\nThere exist other CNN architectures in the literature [6, 21, 22]. We adopt the CNN model in [5, 23] due to its simplicity and excellent performance on classification. Empirically, we found that it can extract high-quality text string representations for ranking."
    }, {
      "heading" : "2.1.3. Comparison between the two deep feature extractors",
      "text" : "Compared with the LSTM feature extractor, a CNN feature extractor may have the following advantages [24]. First, the sparse connectivity of a CNN, which indicates fewer parameters are required, typically improves its statistical efficiency as well as reduces memory requirements. Second, a CNN is able to encode regional (ngram) information containing rich linguistic patterns. Furthermore, an LSTM encoder might be disproportionately influenced by characters appearing later in the sequence, while the CNN gives largely uniform importance to the signal coming from each of the characters in the sequence. This makes the LSTM excellent at language modeling, but potentially suboptimal at encoding n-gram information placed further back into the sequence."
    }, {
      "heading" : "2.2. Ranker",
      "text" : "Now that we have extracted deep feature vectors yQ, yD1 ,..., yDJ+1 from the query and candidate strings, we can proceed to compute their semantic relevance scores by computing their corresponding\ncosine similarity between query Q and each j-th target string Dj . More formally, it is defined as\nR(Q,Dj) = y>QyDj\n||yQ|| · ||yDj || , (13)\nwhere Dj denotes the j-th target string. At test time, given a query, the candidates are ranked by this relevance scores."
    }, {
      "heading" : "2.3. Training of the deep conflation model",
      "text" : "We now explain how the deep conflation model could be trained in an end-to-end manner. Given that we have the relevance scores between the query string and each of the target stringDj : R(Q,Dj), we define the posterior probability of the correct candidate given the query by the following softmax function\nP (D+|Q) = exp(γR(Q,D +))∑\nD′∈D exp(γR(Q,D ′))\n, (14)\nwhereD+ denotes the correct target string (the positive sign denotes that it is a positive sample), γ is a tuning hyper-parameter in the softmax function (to be tuned empirically on a validation set). D denotes the set of candidate strings to be ranked, which includes the positive sample D+ and J randomly selected incorrect (negative) candidates {D−j ; j = 1, . . . , J}. The model parameters are learned to maximize the likelihood of the correct candidates given the queries across the training set. That is, we minimize the following loss function\nL(θ) = − log ∏\n(Q,D+)\nP (D+|Q) , (15)\nwhere the product is over all training samples, and θ denotes the parameters (to be learned), including all the model parameters in the deep feature extractors. The above cost function is minimized by back propagation and (mini-batch) stochastic gradient descent."
    }, {
      "heading" : "3. EXPERIMENTAL RESULTS",
      "text" : ""
    }, {
      "heading" : "3.1. Dataset",
      "text" : "We evaluate the performance of our proposed deep conflation model on a corporate proprietary business dataset. Since each string can be considered as a sequence of characters, the vocabulary size is 32 (including one period symbol and one space symbol), which includes the following elements:\nDMPSabcdefghijklmnopqrstuvwxyz.\nSpecifically, the dataset contains 10, 000 pairs of query and the associated correct target string (manually annotated). The average length of the string is 14.47 with standard deviation 2.89. The maximum length of the strings is 26 and the minimum length is 6."
    }, {
      "heading" : "3.2. Setup",
      "text" : "We provide the deep conflation results using LSTM and CNN for feature extraction, respectively. Furthermore, we also implement a baseline using Bag-of-Characters (BoC) representation of input text string. This BoC vector is then sent into a two-hidden-layer (fullyconnected) feed-forward neural networks. In our experiment, we implement 10-fold cross validation, and in each fold, we randomly select 80% of the samples as training, 10% as validation, and the\nrest 10% as testing dataset. No specific hyper-parameter tuning is implemented, other than early stopping on the validation set.\nFor the feed-forward neural network encoder based on the BoC representation, we use two hidden layers, each layer contains 300 hidden units, hence each string is embedded as a 300-dimensional vector. For LSTM and CNN encoder, we first embed each character into a 128-dimensional vector. Based on this, for the bidirectional LSTM encoder, we further use one hidden layer of 128 units for sequence embedding, hence each text string is represented as a 256- dimensional vector. For the CNN encoder, we employ filter windows of sizes {2,3,4} with 100 feature maps each, hence each text string is represented as a 300-dimensional vector.\nFor training, all weights in the CNN and non-recurrent weights in the LSTM are initialized from a uniform distribution in [- 0.01,0.01]. Orthogonal initialization is employed on the recurrent matrices in the LSTM [25]. All bias terms are initialized to zero. It is observed that setting a high initial forget gate bias for LSTMs can give slightly better results [26]. Hence, the initial forget gate bias is set to 3 throughout the experiments. Gradients are clipped if the norm of the parameter vector exceeds 5 [10]. The Adam algorithm [27] with learning rate 2 × 10−4 is utilized for optimization. For both the LSTM and CNN models, we use mini-batches of size 100. The hyper-parameter γ is set to 10. The number of negative candidates J is set to 50, which are randomly sampled from the rest of the candidate strings excluding the correct one. All experiments are implemented in Theano [28] on a NVIDIA Tesla K40 GPU. For reference, the training of a CNN model takes around 45 minutes to go through the dataset 20 times."
    }, {
      "heading" : "3.3. Results",
      "text" : "Performance is evaluated using Recall@K, which measures the average times a correct item is found within the top-K retrieved results. Results are summarized in Table 2. As can be seen, both of the proposed deep conflation models with LSTM and CNN feature extractors achieve superior performance compared to the BoC baseline. This is not surprising, since sequential order information is utilized in LSTM and CNN. Furthermore, we observe that CNN significantly outperforms LSTM on this task. We hypothesize that\nthis observation is due to the fact that the local (regional) sequential order information (captured by CNN) is more important than the gloabl sequential order information (captured by LSTM) in matching two names. For example, if we reverse the family name and given name of a given query name, LSTM might be more prone to mistakenly classifying these two names to be different, while in fact they refer to the same person.\nFor further analysis, we checked the CNN results on one predefined train/validation/test splits of the dataset. When CNN is used, for Recall@1, out of 1,000 test samples, only 5 samples are mistakenly retrieved. In Table 4, we show an example of the mistaken case. We can see that the mistakenly retrieved case is quite reasonable. Even humans will make mistakes on these cases. Other four mistakenly retrieved cases are similar and are omitted due to space limit. The average scores for each of the top four retrieved items are given in Table 3. This suggests that, when judging whether two text strings have the same meaning, we can empirically set the threshold to be (0.792 + 0.448)/2 = 0.62. That is, when the similarity score between two strings is higher than 0.62, we can safely conclude that they refer to the same entity, and we can then conflate the corresponding two rows accordingly."
    }, {
      "heading" : "4. CONCLUSION",
      "text" : "We have proposed a deep conflation model for matching two text fields in business data analytics, with two different variants of feature extractors, namely, long-short-term memory (LSTM) and convolutional neural networks (CNN). The model encodes the input text from raw character-level into finite dimensional feature vectors, which are used for computing the corresponding relevance scores. The model is learned in an end-to-end manner by back propagation and stochastic gradient descent. Since both LSTM and CNN feature extractors retain the order information in the text, the deep conflation model achieve superior performance compared to the bag-ofcharacter (BoC) baseline."
    }, {
      "heading" : "5. REFERENCES",
      "text" : "[1] Vetle I Torvik and Neil R Smalheiser, “Author name disambiguation in medline,” TKDD, 2009.\n[2] Staša Milojević, “Accuracy of simple, initials-based methods for author name disambiguation,” Journal of Informetrics, 2013.\n[3] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” in Neural computation, 1997.\n[4] T. Mikolov, M. Karafiát, L. Burget, J. Cernockỳ, and S. Khudanpur, “Recurrent neural network based language model,” in INTERSPEECH, 2010.\n[5] Y. Kim, “Convolutional neural networks for sentence classification,” in EMNLP, 2014.\n[6] N. Kalchbrenner, E. Grefenstette, and P. Blunsom, “A convolutional neural network for modelling sentences,” in ACL, 2014.\n[7] Andrew M Dai and Quoc V Le, “Semi-supervised sequence learning,” in Advances in Neural Information Processing Systems, 2015.\n[8] N. Kalchbrenner and P. Blunsom, “Recurrent continuous translation models.,” in EMNLP, 2013.\n[9] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio, “Learning phrase representations using rnn encoder-decoder for statistical machine translation,” in EMNLP, 2014.\n[10] I. Sutskever, O. Vinyals, and Q. Le, “Sequence to sequence learning with neural networks,” in NIPS, 2014.\n[11] F. Meng, Z. Lu, M. Wang, H. Li, W. Jiang, and Q. Liu, “Encoding source language with convolutional neural network for machine translation,” in ACL, 2015.\n[12] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck, “Learning deep structured semantic models for web search using clickthrough data,” in CIKM, 2013.\n[13] Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, and Grégoire Mesnil, “A latent semantic model with convolutionalpooling structure for information retrieval,” in CIKM, 2014.\n[14] Hamid Palangi, Li Deng, Yelong Shen, Jianfeng Gao, Xiaodong He, Jianshu Chen, Xinying Song, and Rabab Ward, “Deep sentence embedding using long short-term memory networks: Analysis and application to information retrieval,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2016.\n[15] Yoon Kim, Yacine Jernite, David Sontag, and Alexander M Rush, “Character-aware neural language models,” AAAI, 2016.\n[16] Wang Ling, Tiago Luı́s, Luı́s Marujo, Ramón Fernandez Astudillo, Silvio Amir, Chris Dyer, Alan W Black, and Isabel Trancoso, “Finding function in form: Compositional character models for open vocabulary word representation,” arXiv:1508.02096, 2015.\n[17] Xiang Zhang, Junbo Zhao, and Yann LeCun, “Character-level convolutional networks for text classification,” in NIPS, 2015.\n[18] David Golub and Xiaodong He, “Character-level question answering with attention,” EMNLP, 2016.\n[19] Junyoung Chung, Kyunghyun Cho, and Yoshua Bengio, “A character-level decoder without explicit segmentation for neural machine translation,” arXiv:1603.06147, 2016.\n[20] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio, “On the difficulty of training recurrent neural networks.,” in ICML, 2013.\n[21] B. Hu, Z. Lu, H. Li, and Q. Chen, “Convolutional neural network architectures for matching natural language sentences,” in NIPS, 2014.\n[22] R. Johnson and T. Zhang, “Effective use of word order for text categorization with convolutional neural networks,” in NAACL HLT, 2015.\n[23] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa, “Natural language processing (almost) from scratch,” in JMLR, 2011.\n[24] Zhe Gan, Yunchen Pu, Ricardo Henao, Chunyuan Li, Xiaodong He, and Lawrence Carin, “Unsupervised learning of sentence representations using convolutional neural networks,” arXiv:1611.07897, 2016.\n[25] A. M. Saxe, J. L. McClelland, and S. Ganguli, “Exact solutions to the nonlinear dynamics of learning in deep linear neural networks,” in ICLR, 2014.\n[26] Q. V. Le, N. Jaitly, and G. E. Hinton, “A simple way to initialize recurrent networks of rectified linear units,” arXiv:1504.00941, 2015.\n[27] Diederik Kingma and Jimmy Ba, “Adam: A method for stochastic optimization,” in ICLR, 2015.\n[28] F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. Goodfellow, A. Bergeron, N. Bouchard, D. Warde-Farley, and Y. Bengio, “Theano: new features and speed improvements,” arXiv:1211.5590, 2012."
    } ],
    "references" : [ {
      "title" : "Author name disambiguation in medline",
      "author" : [ "Vetle I Torvik", "Neil R Smalheiser" ],
      "venue" : "TKDD, 2009.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Accuracy of simple, initials-based methods for author name disambiguation",
      "author" : [ "Staša Milojević" ],
      "venue" : "Journal of Informetrics, 2013.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Long short-term memory",
      "author" : [ "S. Hochreiter", "J. Schmidhuber" ],
      "venue" : "Neural computation, 1997.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Recurrent neural network based language model",
      "author" : [ "T. Mikolov", "M. Karafiát", "L. Burget", "J. Cernockỳ", "S. Khudanpur" ],
      "venue" : "INTERSPEECH, 2010.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Y. Kim" ],
      "venue" : "EMNLP, 2014.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A convolutional neural network for modelling sentences",
      "author" : [ "N. Kalchbrenner", "E. Grefenstette", "P. Blunsom" ],
      "venue" : "ACL, 2014.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Semi-supervised sequence learning",
      "author" : [ "Andrew M Dai", "Quoc V Le" ],
      "venue" : "Advances in Neural Information Processing Systems, 2015.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Recurrent continuous translation models",
      "author" : [ "N. Kalchbrenner", "P. Blunsom" ],
      "venue" : "EMNLP, 2013.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "author" : [ "K. Cho", "B. Van Merriënboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio" ],
      "venue" : "EMNLP, 2014.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "I. Sutskever", "O. Vinyals", "Q. Le" ],
      "venue" : "NIPS, 2014.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Encoding source language with convolutional neural network for machine translation",
      "author" : [ "F. Meng", "Z. Lu", "M. Wang", "H. Li", "W. Jiang", "Q. Liu" ],
      "venue" : "ACL, 2015.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Learning deep structured semantic models for web search using clickthrough data",
      "author" : [ "Po-Sen Huang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Acero", "Larry Heck" ],
      "venue" : "CIKM, 2013.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A latent semantic model with convolutionalpooling structure for information retrieval",
      "author" : [ "Yelong Shen", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Grégoire Mesnil" ],
      "venue" : "CIKM, 2014.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Deep sentence embedding using long short-term memory networks: Analysis and application to information retrieval",
      "author" : [ "Hamid Palangi", "Li Deng", "Yelong Shen", "Jianfeng Gao", "Xiaodong He", "Jianshu Chen", "Xinying Song", "Rabab Ward" ],
      "venue" : "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2016.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Character-aware neural language models",
      "author" : [ "Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush" ],
      "venue" : "AAAI, 2016.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Finding function in form: Compositional character models for open vocabulary word representation",
      "author" : [ "Wang Ling", "Tiago Luı́s", "Luı́s Marujo", "Ramón Fernandez Astudillo", "Silvio Amir", "Chris Dyer", "Alan W Black", "Isabel Trancoso" ],
      "venue" : "arXiv:1508.02096, 2015.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Character-level convolutional networks for text classification",
      "author" : [ "Xiang Zhang", "Junbo Zhao", "Yann LeCun" ],
      "venue" : "NIPS, 2015.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Character-level question answering with attention",
      "author" : [ "David Golub", "Xiaodong He" ],
      "venue" : "EMNLP, 2016.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "A character-level decoder without explicit segmentation for neural machine translation",
      "author" : [ "Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "arXiv:1603.06147, 2016.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "On the difficulty of training recurrent neural networks",
      "author" : [ "Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio" ],
      "venue" : "ICML, 2013.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Convolutional neural network architectures for matching natural language sentences",
      "author" : [ "B. Hu", "Z. Lu", "H. Li", "Q. Chen" ],
      "venue" : "NIPS, 2014.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Effective use of word order for text categorization with convolutional neural networks",
      "author" : [ "R. Johnson", "T. Zhang" ],
      "venue" : "NAACL HLT, 2015.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa" ],
      "venue" : "JMLR, 2011.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Unsupervised learning of sentence representations using convolutional neural networks",
      "author" : [ "Zhe Gan", "Yunchen Pu", "Ricardo Henao", "Chunyuan Li", "Xiaodong He", "Lawrence Carin" ],
      "venue" : "arXiv:1611.07897, 2016.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "author" : [ "A.M. Saxe", "J.L. McClelland", "S. Ganguli" ],
      "venue" : "ICLR, 2014.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A simple way to initialize recurrent networks of rectified linear units",
      "author" : [ "Q.V. Le", "N. Jaitly", "G.E. Hinton" ],
      "venue" : "arXiv:1504.00941, 2015.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "ICLR, 2015.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Theano: new features and speed improvements",
      "author" : [ "F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I. Goodfellow", "A. Bergeron", "N. Bouchard", "D. Warde-Farley", "Y. Bengio" ],
      "venue" : "arXiv:1211.5590, 2012.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "data entries, plain keyword matching does not work well [1, 2], and we need a data conflation model in the semantic level; that is, the model should be able to identify two different character strings to be associated with a same entity.",
      "startOffset" : 56,
      "endOffset" : 62
    }, {
      "referenceID" : 1,
      "context" : "data entries, plain keyword matching does not work well [1, 2], and we need a data conflation model in the semantic level; that is, the model should be able to identify two different character strings to be associated with a same entity.",
      "startOffset" : 56,
      "endOffset" : 62
    }, {
      "referenceID" : 2,
      "context" : "In particular, we constructed two different deep architectures of feature extractors: (i) long-short-term-memory (LSTM) recurrent neural network (RNN) [3, 4] and (ii) deep convolutional neural network (CNN) [5, 6].",
      "startOffset" : 151,
      "endOffset" : 157
    }, {
      "referenceID" : 3,
      "context" : "In particular, we constructed two different deep architectures of feature extractors: (i) long-short-term-memory (LSTM) recurrent neural network (RNN) [3, 4] and (ii) deep convolutional neural network (CNN) [5, 6].",
      "startOffset" : 151,
      "endOffset" : 157
    }, {
      "referenceID" : 4,
      "context" : "In particular, we constructed two different deep architectures of feature extractors: (i) long-short-term-memory (LSTM) recurrent neural network (RNN) [3, 4] and (ii) deep convolutional neural network (CNN) [5, 6].",
      "startOffset" : 207,
      "endOffset" : 213
    }, {
      "referenceID" : 5,
      "context" : "In particular, we constructed two different deep architectures of feature extractors: (i) long-short-term-memory (LSTM) recurrent neural network (RNN) [3, 4] and (ii) deep convolutional neural network (CNN) [5, 6].",
      "startOffset" : 207,
      "endOffset" : 213
    }, {
      "referenceID" : 4,
      "context" : "Both deep architectures are able to retain the order information in the input text and extract high-level features from raw data, as shown their great success in different machine learning tasks, including text classification [5, 7], machine translation [8, 9, 10, 11] and information retrieval [12, 13, 14].",
      "startOffset" : 226,
      "endOffset" : 232
    }, {
      "referenceID" : 6,
      "context" : "Both deep architectures are able to retain the order information in the input text and extract high-level features from raw data, as shown their great success in different machine learning tasks, including text classification [5, 7], machine translation [8, 9, 10, 11] and information retrieval [12, 13, 14].",
      "startOffset" : 226,
      "endOffset" : 232
    }, {
      "referenceID" : 7,
      "context" : "Both deep architectures are able to retain the order information in the input text and extract high-level features from raw data, as shown their great success in different machine learning tasks, including text classification [5, 7], machine translation [8, 9, 10, 11] and information retrieval [12, 13, 14].",
      "startOffset" : 254,
      "endOffset" : 268
    }, {
      "referenceID" : 8,
      "context" : "Both deep architectures are able to retain the order information in the input text and extract high-level features from raw data, as shown their great success in different machine learning tasks, including text classification [5, 7], machine translation [8, 9, 10, 11] and information retrieval [12, 13, 14].",
      "startOffset" : 254,
      "endOffset" : 268
    }, {
      "referenceID" : 9,
      "context" : "Both deep architectures are able to retain the order information in the input text and extract high-level features from raw data, as shown their great success in different machine learning tasks, including text classification [5, 7], machine translation [8, 9, 10, 11] and information retrieval [12, 13, 14].",
      "startOffset" : 254,
      "endOffset" : 268
    }, {
      "referenceID" : 10,
      "context" : "Both deep architectures are able to retain the order information in the input text and extract high-level features from raw data, as shown their great success in different machine learning tasks, including text classification [5, 7], machine translation [8, 9, 10, 11] and information retrieval [12, 13, 14].",
      "startOffset" : 254,
      "endOffset" : 268
    }, {
      "referenceID" : 11,
      "context" : "Both deep architectures are able to retain the order information in the input text and extract high-level features from raw data, as shown their great success in different machine learning tasks, including text classification [5, 7], machine translation [8, 9, 10, 11] and information retrieval [12, 13, 14].",
      "startOffset" : 295,
      "endOffset" : 307
    }, {
      "referenceID" : 12,
      "context" : "Both deep architectures are able to retain the order information in the input text and extract high-level features from raw data, as shown their great success in different machine learning tasks, including text classification [5, 7], machine translation [8, 9, 10, 11] and information retrieval [12, 13, 14].",
      "startOffset" : 295,
      "endOffset" : 307
    }, {
      "referenceID" : 13,
      "context" : "Both deep architectures are able to retain the order information in the input text and extract high-level features from raw data, as shown their great success in different machine learning tasks, including text classification [5, 7], machine translation [8, 9, 10, 11] and information retrieval [12, 13, 14].",
      "startOffset" : 295,
      "endOffset" : 307
    }, {
      "referenceID" : 14,
      "context" : "Furthermore, extracting the features from the character-level is critical in many of the recent success in applying deep learning to natural language processing [15, 16, 17, 18, 19].",
      "startOffset" : 161,
      "endOffset" : 181
    }, {
      "referenceID" : 15,
      "context" : "Furthermore, extracting the features from the character-level is critical in many of the recent success in applying deep learning to natural language processing [15, 16, 17, 18, 19].",
      "startOffset" : 161,
      "endOffset" : 181
    }, {
      "referenceID" : 16,
      "context" : "Furthermore, extracting the features from the character-level is critical in many of the recent success in applying deep learning to natural language processing [15, 16, 17, 18, 19].",
      "startOffset" : 161,
      "endOffset" : 181
    }, {
      "referenceID" : 17,
      "context" : "Furthermore, extracting the features from the character-level is critical in many of the recent success in applying deep learning to natural language processing [15, 16, 17, 18, 19].",
      "startOffset" : 161,
      "endOffset" : 181
    }, {
      "referenceID" : 18,
      "context" : "Furthermore, extracting the features from the character-level is critical in many of the recent success in applying deep learning to natural language processing [15, 16, 17, 18, 19].",
      "startOffset" : 161,
      "endOffset" : 181
    }, {
      "referenceID" : 19,
      "context" : "This problem with exploding or vanishing gradients makes it difficult for the regular RNN model to learn long-range dependencies in a sequence [20].",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 2,
      "context" : "The LSTM architecture [3] addresses the problem of learning longterm dependencies by introducing a memory cell, that is able to preserve the state over long periods of time.",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 5,
      "context" : "There exist other CNN architectures in the literature [6, 21, 22].",
      "startOffset" : 54,
      "endOffset" : 65
    }, {
      "referenceID" : 20,
      "context" : "There exist other CNN architectures in the literature [6, 21, 22].",
      "startOffset" : 54,
      "endOffset" : 65
    }, {
      "referenceID" : 21,
      "context" : "There exist other CNN architectures in the literature [6, 21, 22].",
      "startOffset" : 54,
      "endOffset" : 65
    }, {
      "referenceID" : 4,
      "context" : "We adopt the CNN model in [5, 23] due to its simplicity and excellent performance on classification.",
      "startOffset" : 26,
      "endOffset" : 33
    }, {
      "referenceID" : 22,
      "context" : "We adopt the CNN model in [5, 23] due to its simplicity and excellent performance on classification.",
      "startOffset" : 26,
      "endOffset" : 33
    }, {
      "referenceID" : 23,
      "context" : "Compared with the LSTM feature extractor, a CNN feature extractor may have the following advantages [24].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 24,
      "context" : "Orthogonal initialization is employed on the recurrent matrices in the LSTM [25].",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 25,
      "context" : "It is observed that setting a high initial forget gate bias for LSTMs can give slightly better results [26].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 9,
      "context" : "Gradients are clipped if the norm of the parameter vector exceeds 5 [10].",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 26,
      "context" : "The Adam algorithm [27] with learning rate 2 × 10−4 is utilized for optimization.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 27,
      "context" : "All experiments are implemented in Theano [28] on a NVIDIA Tesla K40 GPU.",
      "startOffset" : 42,
      "endOffset" : 46
    } ],
    "year" : 2017,
    "abstractText" : "Connecting different text attributes associated with the same entity (conflation) is important in business data analytics since it could help merge two different tables in a database to provide a more comprehensive profile of an entity. However, the conflation task is challenging because two text strings that describe the same entity could be quite different from each other for reasons such as misspelling. It is therefore critical to develop a conflation model that is able to truly understand the semantic meaning of the strings and match them at the semantic level. To this end, we develop a character-level deep conflation model that encodes the input text strings from character level into finite dimension feature vectors, which are then used to compute the cosine similarity between the text strings. The model is trained in an end-to-end manner using back propagation and stochastic gradient descent to maximize the likelihood of the correct association. Specifically, we propose two variants of the deep conflation model, based on long-short-term memory (LSTM) recurrent neural network (RNN) and convolutional neural network (CNN), respectively. Both models perform well on a real-world business analytics dataset and significantly outperform the baseline bag-of-character (BoC) model.",
    "creator" : "LaTeX with hyperref package"
  }
}