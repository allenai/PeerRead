{
  "name" : "1705.01684.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Probabilistic Typology: Deep Generative Models of Vowel Inventories",
    "authors" : [ "Ryan Cotterell", "Jason Eisner" ],
    "emails" : [ "ryan.cotterell@jhu.edu", "eisner@jhu.edu", "[@]" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Human languages exhibit a wide range of phenomena, within some limits. However, some structures seem to occur or co-occur more frequently than others. Linguistic typology attempts to describe the range of natural variation and seeks to organize and quantify linguistic universals, such as patterns of co-occurrence. Perhaps one of the simplest typological questions comes from phonology: which vowels tend to occur and co-occur within the phoneme inventories of different languages? Drawing inspiration from the linguistic literature, we propose models of the probability distribution from which the attested vowel inventories have been drawn.\nIt is a typological universal that every language contains both vowels and consonants (Velupillai, 2012). But which vowels a language contains is guided by softer constraints, in that certain configurations are more widely attested than others. For instance, in a typical phoneme inventory, there tend to be far fewer vowels than consonants. Likewise, all languages contrast vowels based on height, although which contrast is made is language-dependent (Ladefoged and Maddieson, 1996). Moreover, while over 600 unique vowel\nphonemes have been attested cross-linguistically (Moran et al., 2014), certain regions of acoustic space are used much more often than others, e.g., the regions conventionally transcribed as [a], [i], and [u]. Human language also seems to prefer inventories where phonologically distinct vowels are spread out in acoustic space (“dispersion”) so that they can be easily distinguished by a listener. We depict the acoustic space for English in Figure 2.\nIn this work, we regard the proper goal of linguistic typology as the construction of a universal prior distribution from which linguistic systems are drawn.For vowel system typology, we propose three formal probability models based on stochastic point processes. We estimate the parameters of the model on one set of languages and evaluate performance on a held-out set. We explore three questions: (i) How well do the properties of our proposed probability models line up experimentally with linguistic theory? (ii) How well can our models predict held-out vowel systems? (iii) Do our models benefit from a “deep” transformation from formant space to metric space? ar X\niv :1\n70 5.\n01 68\n4v 1\n[ cs\n.C L\n] 4\nM ay\n2 01\n7"
    }, {
      "heading" : "2 Vowel Inventories and their Typology",
      "text" : "Vowel inventories are a simple entry point into the study of linguistic typology. Every spoken language chooses a discrete set of vowels, and the number of vowel phonemes ranges from 3 to 46, with a mean of 8.7 (Gordon, 2016). Nevertheless, the empirical distribution over vowel inventories is remarkably peaked. The majority of languages have 5–7 vowels, and there are only a handful of distinct 4-vowel systems attested despite many possibilities. Reigning linguistic theory (BeckerKristal, 2010) has proposed that vowel inventories are shaped by the principles discussed below."
    }, {
      "heading" : "2.1 Acoustic Phonetics",
      "text" : "One way to describe the sound of a vowel is through its acoustic energy at different frequencies. A spectrogram (Figure 3) is a visualization of the energy at various frequencies over time. Consider the “peak” frequencies F0 < F1 < F2 < . . . that have a greater energy than their neighboring frequencies. F0 is called the fundamental frequency or pitch. The other qualities of the vowel are largely determined by F1, F2, . . ., which are known as formants (Ladefoged and Johnson, 2014). In many languages, the first two formants F1 and F2 contain enough information to identify a vowel: Figure 3 shows how these differ across three English vowels. We consider each vowel listed in the International Phonetic Alphabet (IPA) to be cross-linguistically characterized by some (F1, F2) pair."
    }, {
      "heading" : "2.2 Dispersion",
      "text" : "The dispersion criterion (Liljencrants and Lindblom, 1972; Lindblom, 1986) states that the phonemes of a language must be “spread out” so that they are easily discriminated by a listener. A\n0 Hz\n1000 Hz\n2000 Hz\n3000 Hz\n4000 Hz\n5000 Hz /i/ /u/ /ɑ/\nFigure 3: Example spectrogram of the three English vowels: [i], [u] and [A]. The x-axis is time and y-axis is frequency. The first two formants F1 and F2 are marked in with colored arrows for each vowel. We used the Praat toolkit to generate the spectrogram and find the formants (Boersma et al., 2002).\nlanguage seeks phonemes that are sufficiently “distant” from one another to avoid confusion. Distances between phonemes are defined in some latent “metric space.” We use this term rather than “perceptual space” because the confusability of two vowels may reflect not just their perceptual similarity, but also their common distortions by imprecise articulation or background noise.1"
    }, {
      "heading" : "2.3 Focalization",
      "text" : "The dispersion criterion alone does not seem to capture the whole story. Certain vowels are simply more popular cross-linguistically. A commonly accepted explanation is the quantal theory of speech (Stevens, 1972, 1989). The quantal theory states that certain sounds are easier to articulate and to perceive than others. These vowels may be characterized as those where F1 and F2 have frequencies that are close to one another. On the production side, these vowels are easier to pronounce since they allow for greater articulatory imprecision.On the perception side, they are more salient since the two spectral peaks aggregate and act as one, larger peak to a certain degree.In general, languages will prefer these vowels."
    }, {
      "heading" : "2.4 Dispersion-Focalization Theory",
      "text" : "The dispersion-focalization theory (DFT) combines both of the above notions. A good vowel system now consists of vowels that contrast with each other and are individually desirable (Schwartz et al., 1997). This paper provides the first probabilistic treatment of DFT, and new evaluation metrics for future probabilistic and non-probabilistic treatments of vowel inventory typology.\n1We assume in this paper that the metric space is universal—although it would not be unreasonable to suppose that each language’s vowel system has adapted to avoid confusion in the specific communicative environment of its speakers."
    }, {
      "heading" : "3 Point Process Models",
      "text" : "Given a base set V , a point process is a distribution over its subsets.2 In this paper, we take V to be the set of all IPA symbols corresponding to vowels. Thus a draw from a point process is a vowel inventory V ⊆ V , and the point process itself is a distribution over such inventories. We will consider three basic point process models for vowel systems: the Bernoulli Point Process, the Markov Point Process and the Determinantal Point Process. In this section, we review the relevant theory of point processes, highlighting aspects related to §2."
    }, {
      "heading" : "3.1 Bernoulli Point Processes",
      "text" : "Taking V = {v1, . . . , vN}, a Bernoulli point process (BPP) makes an independent decision about whether to include each vowel in the subset. The probability of a vowel system V ⊆ V is thus\np(V ) ∝ ∏ vi∈V φ(vi), (1)\nwhere φ is a unary potential function, i.e., φ(vi) ≥ 0. Qualitatively, this means that φ(vi) should be large if the ith vowel is good in the sense of §2.3. Marginal inference in a BPP is computationally trivial. The probability that the inventory V contains vi is φ(vi)/(1 + φ(vi)), independent of the other vowels in V . Since a BPP predicts each vowel independently, it only models focalization. Thus, the model provides an appropriate baseline that will let us measure the importance of the dispersion principle—how far can we get with just focalization? A BPP may still tend to generate well-dispersed sets if it defines φ to be large only on certain vowels in V and these are well-dispersed (e.g., [i], [u], [a]). More precisely, it can define φ so that φ(vi)φ(vj) is small whenever vi, vj are similar.3 But it cannot actively encourage dispersion:\n2A point process is a specific kind of stochastic process, which is the technical term for a distribution over functions. Under this view, drawing some subset of V from the point process is regarded as drawing some indicator function on V .\n3We point out that such a scheme would break down if we extended our work to cover fine-grained phonetic modeling of the vowel inventory. In that setting, we ask not just whether the inventory includes /i/ but exactly which pronunciation of /i/ it contains. In the limit, φ becomes a function over a continuous vowel space V = R2, turning the BPP into an inhomogeneous spatial Poisson process. A continuous φ function implies that the model places similar probability on similar vowels. Then if most vowel inventories contain some version of /i/, then many of them will contain several closely related variants of /i/ (independently chosen). By contrast, the other methods in this paper do extend nicely to fine-grained phonetic modeling.\nincluding vi does not lower the probability of also including vj ."
    }, {
      "heading" : "3.2 Markov Point Processes",
      "text" : "A Markov Point Process (MPP) (Van Lieshout, 2000)—also known as a Boltzmann machine (Ackley et al., 1985; Hinton and Sejnowski, 1986)— generalizes the BPP by adding pairwise interactions between vowels. The probability of a vowel system V ⊆ V is now\np(V ) ∝ ∏ vi∈V φ(vi) ∏ vi,vj∈V ψ(vi, vj), (2)\nwhere each φ(vi) ≥ 0 is, again, a unary potential that scores the quality of the ith vowel, and each ψ(vi, vj) ≥ 0 is a binary potential that scores the combination of the ith and jth vowels. Roughly speaking, the potential ψ(vi, vj) should be large if the ith and jth vowel often co-occur. Recall that under the principle of dispersion, the vowels that often co-occur are easily distinguishable. Thus, confusable vowel pairs should tend to have potential ψ(vi, vj) < 1.\nUnlike the BPP, the MPP can capture both focalization and dispersion. In this work, we will consider a fully connected MPP, i.e., there is a potential function for each pair of vowels in V . MPPs closely resemble Ising models (Ising, 1925), but with the difference that Ising models are typically lattice-structured, rather than fully connected.\nInference in MPPs. Inference in fully connected MPPs, just as in general Markov Random Fields (MRFs), is intractable (Cooper, 1990) and we must rely on approximation. In this work, we estimate any needed properties of the MPP distribution by (approximately) drawing vowel inventories from it via Gibbs sampling (Geman and Geman, 1984; Robert and Casella, 2005). Gibbs sampling simulates a discrete-time Markov chain whose stationary distribution is the desired MPP distribution. At each time step, for some random vi ∈ V , it stochastically decides whether to replace the current inventory V with V̄ , where V̄ is a copy of V with vi added (if vi /∈ V ) or removed (if vi ∈ V ). The probability of replacement is p(V̄ )\np(V )+p(V̄ ) ."
    }, {
      "heading" : "3.3 Determinantal Point Processes",
      "text" : "A determinantal point process (DPP) (Macchi, 1975) provides an elegant alternative to an MPP, and one that is directly suited to modeling both focalization and dispersion. Inference requires only\na few matrix computations and runs tractably in O(|V|3) time, even though the model may encode a rich set of multi-way interactions. We focus on the L-ensemble parameterization of the DPP, due to Borodin and Rains (2005).4 This type of DPP defines the probability of an inventory V ⊆ V as\np(V ) ∝ detLV , (3)\nwhere L ∈ RN×N (for N = |V|) is a symmetric positive semidefinite matrix, and LV refers to the submatrix of L with only those rows and columns corresponding to those elements in the subset V .\nAlthough MAP inference remains NP-hard in DPPs (just as in MPPs), marginal inference becomes tractable. We may compute the normalizing constant in closed form as follows:∑\nV ∈2V detLV = det (L+ I) . (4)\nHow does a DPP ensure focalization and dispersion? L is positive semidefinite iff it can be written as E>E for some matrix E ∈ RN×N . It is possible to express p(V ) in terms of the column vectors of E, which we call e1, . . . , eN :\n• For inventories of size 2, p({vi, vj}) ∝ (φ(vi)φ(vj) sin θ)\n2, where φ(vi), φ(vj) represent the quality of vowels vi, vj (as in the BPP) while sin θ ∈ [0, 1] represents their dissimilarity. More precisely, φ(vi), φ(vj) are the lengths of vectors ei, ej while θ is the angle between them. Thus, we should choose the columns of E so that focal vowels get long vectors and similar vowels get vectors of similar direction. • Generalizing beyond inventories of size 2, p(V ) is proportional to the square of the volume of the parallelepiped whose sides are given by {ei : vi ∈ V }. This volume can be regarded as ∏ vi∈V φ(vi) times a term that\nranges from 1 for an orthogonal set of vowels to 0 for a linearly dependent set of vowels. • The events vi ∈ V and vj ∈ V are anti-\ncorrelated (when not independent). That is, while both vowels may individually have high probabilities (focalization), having either one in the inventory lowers the probability of the other (dispersion).\n4Most DPPs are L-ensembles (Kulesza and Taskar, 2012)."
    }, {
      "heading" : "4 Dataset",
      "text" : "At this point it is helpful to introduce the empirical dataset we will model. For each of 223 languages,5 Becker-Kristal (2010) provides the vowel inventory as a set of IPA symbols, listing the first 5 formants for each vowel (or fewer when not available in the original source). Some corpus statistics are shown in Figs. 4 and 5.6 For the present paper, we take V to be the set of all 53 IPA symbols that appear in the corpus. We treat these IPA labels as meaningful, in that we consider two vowels in different languages to be the same vowel in V if (for example) they are both annotated as [O]. We characterize that vowel by its average formant vector across all languages in the corpus that contain the vowel: e.g., (F1, F2, . . .) = (500, 700, . . .) for [O]. In future work, we plan to relax this idealization (see footnote 3), allowing us to investigate natural questions such as whether [u] is pronounced higher (smaller F1) in languages that also contain [o] (to achieve better dispersion)."
    }, {
      "heading" : "5 Model Parameterization",
      "text" : "The BPP, MPP, and DPP models (§3) require us to specify parameters for each vowel in V . In §5.1, we will accomplish this by deriving the parameters for each vowel vi from a possibly high-dimensional embedding of that vowel, e(vi) ∈ Rr.\nIn §5.2, e(vi) ∈ Rr will in turn be defined as some learned function of f(vi) ∈ Rk, where f : V 7→ Rk is the function that maps a vowel to a k-vector of its measurable acoustic properties. This approach allows us to determine reasonable parameters even for rare vowels, based on their measurable properties. It will even enable us in\n5Becker-Kristal lists some languages multiple times with different measurements. When a language had multiple listings, we selected one randomly for our experiments.\n6Caveat: The corpus is a curation of information from various phonetics papers into a common electronic format. No standard procedure was followed across all languages: it was up to individual phoneticists to determine the size of each vowel inventory, the choice of IPA symbols to describe it, and the procedure for measuring the formants. Moreover, it is an idealization to provide a single vector of formants for each vowel type in the language. In real speech, different tokens of the same vowel are pronounced differently, because of coarticulation with the vowel context, allophony, interspeaker variation, and stochastic intraspeaker variation. Even within a token, the formants change during the duration of the vowel. Thus, one might do better to represent a vowel’s pronunciation not by a formant vector, but by a conditional probability distribution over its formant trajectories given its context, or by a parameter vector that characterizes such a conditional distribution. This setting would require richer data than we present here.\nfuture to generalize to vowels that were unseen in the training set, letting us scale to very large or infinite V (footnote 3)."
    }, {
      "heading" : "5.1 Deep Point Processes",
      "text" : "We consider deep versions of all three processes.\nDeep Bernoulli Point Process. We define\nφ(vi) = ||e(vi)|| ≥ 0 (5)\nDeep Markov Point Process. The MPP employs the same unary potential as the BPP, as well as the binary potential\nψ(vi, vj) = exp− 1\nT · ||e(vi)−e(vj)||2 < 1 (6)\nwhere the learned temperature T > 0 controls the relative strength of the unary and binary potentials.\nThis formula is inspired by Coulomb’s law for describing the repulsion of static electrically charged particles. Just as the repulsive force between two particles approaches∞ as they approach each other, the probability of finding two vowels in the same inventory approaches exp−∞ = 0 as they approach each other. The formula is also reminiscent of Shepard (1987)’s “universal law of generalization,” which says here that the probability of responding to vi as if it were vj should fall off exponentially with their distance in some “psychological space” (here, embedding space).\nDeep Determinantal Point Process. For the DPP, we simply define the vector ei to be e(vi), and proceed as before.\nSummary. In the deep BPP, the probability of a set of vowels is proportional to the product of the lengths of their embedding vectors. The deep MPP modifies this by multiplying in pairwise repulsion terms in (0, 1) that increase as the vectors’ endpoints move apart in Euclidean space (or as T → ∞). The deep DPP instead modifies it by multiplying in a single setwise repulsion term in (0, 1) that increases as the embedding vectors become more mutually orthogonal. In the limit, then, the MPP and DPP both approach the BPP."
    }, {
      "heading" : "5.2 Embeddings",
      "text" : "Throughout this work, we simply have f extract the first k = 2 formants, since our dataset does not provide higher formants for all languages.7For\n7In lieu of higher formants, we could have extended the vector f(vi) to encode the binary distinctive features of the IPA vowel vi: round, tense, long, nasal, creaky, etc.\nexample, we have f([O]) = (500, 700). We now describe three possible methods for mapping f(vi) to an embedding e(vi). Each of these maps has learnable parameters.\nNeural Embedding. We first consider directly embedding each vowel vi into a vector space Rr. We achieve this through a feed-forward neural net\ne(vi) = W1 tanh (W0f(vi) + b0) + b1, (7)\nEquation (7) gives an architecture with 1 layer of nonlinearity; in general we consider stacking d ≥ 0 layers. Here W0 ∈ Rr×k,W1 ∈ Rr×r, . . .Wd ∈ Rr×r are weight matrices, b0, . . .bd ∈ Rr are bias vectors, and tanh could be replaced by any pointwise nonlinearity. We treat both the depth d and the embedding size r as hyperparameters, and select the optimal values on a development set.\nInterpretable Neural Embedding. We are interested in the special case of neural embeddings when r = k since then (for any d) the mapping f(vi) 7→ e(vi) is a diffeomorphism:8 a smooth invertible function of Rk. An example of such a diffeomorphism is shown in Figure 1.\nThere is a long history in cognitive psychology of mapping stimuli into some psychological space. The distances in this psychological space may be predictive of generalization (Shepard, 1987) or of perception. Due to the anatomy of the ear, the mapping of vowels from acoustic space to perceptual space is often presumed to be nonlinear (Rosner and Pickering, 1994; Nearey and Kiefte, 2003), and there are many perceptually-oriented phonetic scales, e.g., Bark and Mel, that carry out such nonlinear transformations while preserving the dimensionality k, as we do here. As discussed in §2.2, vowel system typology is similarly believed to be influenced by distances between the vowels in a latent metric space. We are interested in whether a constrained k-dimensional model of these distances can do well in our experiments.\nPrototype-Based Embedding. Unfortunately, our interpretable neural embedding is unfortunately incompatible with the DPP. The DPP assigns probability 0 to any vowel inventory V whose e vectors are linearly dependent. If the vectors are in Rk, then this means that p(V ) = 0 whenever |V | > k. In our setting, this would limit vowel inventories to size 2.\n8Provided that our nonlinearity in (7) is a differentiable invertible function like tanh rather than relu.\nOur solution to this problem is to still construct our interpretable metric space Rk, but then map that nonlinearly to Rr for some large r. This latter map is constrained. Specifically, we choose “prototype” points µ1, . . . ,µr ∈ Rk. These prototype points are parameters of the model: their coordinates are learned and do not necessarily correspond to any actual vowel. We then construct e(vi) ∈ Rr as a “response vector” of similarities of our vowel vi to these prototypes. Crucially, the responses depend on distances measured in the interpretable metric space Rk. We use a Gaussian-density response function, where x(vi) denotes the representation of our vowel vi in the interpretable space:\ne(vi)` = w` p(x(vi);µ`, σ 2I) (8)\n= w` (2πσ 2)−( k 2 ) exp\n( −||x− µ`||2\n2σ2\n) .\nfor ` = 1, 2, . . . , r. We additionally impose the constraints that each w` ≥ 0 and ∑r `=1w` = 1.\nNotice that the sum ∑r\n`=1 e(vi) may be viewed as the density at x(vi) under a Gaussian mixture model. We use this fact to construct a prototypebased MPP as well: we redefine φ(vi) to equal this positive density, while still defining ψ via equation (6). The idea is that dispersion is measured in the interpretable space Rk, and focalization is defined by certain “good” regions in that space that are centered at the r prototypes."
    }, {
      "heading" : "6 Evaluation Metrics",
      "text" : "Fundamentally, we are interested in whether our model has abstracted the core principles of what makes a good vowel system. Our choice of a probabilistic model provides a natural test: how surprised is our model by held-out languages? In other words, how likely does our model think unobserved, but attested vowel systems are? While this is a natural evaluation paradigm in NLP, it has not—to the best of our knowledge—been applied to a quantitative investigation of linguistic typology.\nAs a second evaluation, we introduce a vowel system cloze task that could also be used to evaluate non-probabilistic models. This task is defined by analogy to the traditional semantic cloze task (Taylor, 1953), where the reader is asked to fill in a missing word in the sentence from the context. In our vowel system cloze task, we present a learner with a subset of the vowels in a held-out vowel system and ask them to predict the remaining vowels. Consider, as a concrete example, the\ngeneral American English vowel system (excluding long vowels) {[i], [I], [u], [U], [E], [æ], [O], [A], [@]}. One potential cloze task would be to predict {[i], [u]} given {[I], [U], [E], [æ], [O], [A], [@]} and the fact that two vowels are missing from the inventory. Within the cloze task, we report accuracy, i.e., did we guess the missing vowel right? We consider three versions of the cloze tasks. First, we predict one missing vowel in a setting where exactly one vowel was deleted. Second, we predict up to one missing vowel where a vowel may have been deleted. Third, we predict up to two missing vowels, where one or two vowels may be deleted."
    }, {
      "heading" : "7 Experiments",
      "text" : "We evaluate our models using 10-fold crossvalidation over the 223 languages. We report the mean performance over the 10 folds. The performance on each fold (“test”) was obtained by training many models on 8 of the other 9 folds (“train”), selecting the model that obtained the best task-specific performance on the remaining fold (“development”), and assessing it on the test fold. Minimization of the parameters is performed with the L-BFGS algorithm (Liu and Nocedal, 1989). As a preprocessing step, the first two formants values F1 and F2 are centered around zero and scaled down by a factor of 1000 since the formant values themselves may be quite large.\nSpecifically, we use the development fold to select among the following combinations of hyperparameters. For neural embeddings, we tried r ∈ {2, 10, 50, 100, 150, 200}. For prototype embeddings, we took the number of components r ∈ {20, 30, 40, 50}.We tried network depths d ∈ {0, 1, 2, 3}. We sweep the coefficient for an L2 regularizer on the neural network parameters."
    }, {
      "heading" : "7.1 Results and Discussion",
      "text" : "Figure 1 visualizes the diffeomorphism from formant space to metric space for one of our DPP models (depth d = 3 with r = 20 prototypes). Similar figures can be generated for all of the interpretable models.\nWe report results for cross-entropy and the cloze evaluation in Table 1.9 Under both metrics, we see that the DPP is slightly better than the MPP; both are better than the BPP. This ranking holds for\n9Computing cross-entropy exactly is intractable with the MPP, so we resort to an unbiased importance sampling scheme where we draw samples from the BPP and reweight according to the MPP (Liu et al., 2015).\neach of the 3 embedding schemes. The embedding schemes themselves are compared in the caption.\nWithin each embedding scheme, the BPP performs several points worse on the cloze tasks, confirming that dispersion is needed to model vowel inventories well. Still, the BPP’s respectable performance shows that much of the structure can be capture by focalization. As §3 noted, the BPP may generate well-dispersed sets, as the common vowels tend to be dispersed already (see Figure 4). In this capacity, however, the BPP is not explanatory as it cannot actually tell us why these vowels should be frequent.\nWe mention that depth in the neural network is helpful, with deeper embedding networks performing slightly better than depth d = 0.\nFinally, we identified each model’s favorite complete vowel system of size n (Table 2). For the BPP, this is simply the n most probable vowels. Decoding the DPP and MPP is NP-hard, but we found the best system by brute force (for small n). The dispersion in these models predicts different systems than the BPP."
    }, {
      "heading" : "8 Discussion: Probabilistic Typology",
      "text" : "Typology as Density Estimation? Our goal is to define a universal distribution over all possible vowel inventories. Is this appropriate? We regard this as a natural approach to typology, because it directly describes which kinds of linguistic systems are more or less common. Traditional implicational universals (“all languages with vi have vj”) are softened, in our approach, into conditional probabilities such as “p(vj ∈ V | vi ∈ V ) ≈ 0.9.” Here the 0.9 is not merely an empirical ratio, but a smoothed\nprobability derived from the complete estimated distribution. It is meant to make predictions about unseen languages.\nWhether human language learners exploit any properties of this distribution10is a separate question that goes beyond typology. Jakobson (1941) did find that children acquired phoneme inventories in an order that reflected principles similar to dispersion (“maximum contrast”) and focalization.\nAt any rate, we estimate the distribution given some set of attested systems that are assumed to have been drawn IID from it. One might object that this IID assumption ignores evolutionary relationships among the attested systems, causing our estimated distribution to favor systems that are coincidentally frequent among current human languages, rather than being natural in some timeless sense. We reply that our approach is then appropriate when the goal of typology is to estimate the distribution of actual human languages—a distribution that can be utilized in principle (and also in practice, as we show) to predict properties of actual languages from outside the training set.\nA different possible goal of typology is a theory of natural human languages. This goal would require a more complex approach. One should not imagine that natural languages are drawn in a vacuum from some single, stationary distribution. Rather, each language is drawn conditionally on its parent language. Thus, one should estimate a stochastic model of the evolution of linguistic systems through time, and identify “naturalness” with\n10This could happen because learners have evolved to expect the languages (the Baldwin effect), or because the languages have evolved to be easily learned (universal grammar).\nthe directions in which this system tends to evolve.\nEnergy Minimization Approaches. The traditional energy-based approach (Liljencrants and Lindblom, 1972) to vowel simulation minimizes the following objective (written in our notation):\nE(m) = ∑\n1≤i<j≤m\n1\n||e(vi)− e(vj)||2 , (9)\nwhere the vectors e(vi) ∈ Rr are not spit out of a deep network, as in our case, but rather directly optimized. Liljencrants and Lindblom (1972) propose a coordinate descent algorithm to optimize E(m). While this is not in itself a probabilistic model, they generate diverse vowel systems through random restarts that find different local optima (a kind of deterministic evolutionary mechanism). We note that equation (9) assumes that the number of vowels m is given, and only encodes a notion of dispersion. Roark (2001) subsequently extended equation (9) to include the notion of focalization.\nVowel Inventory Size. A fatal flaw of the traditional energy minimization paradigm is that it has no clear way to compare vowel inventories of different sizes. The problem is quite crippling since, in general, inventories with fewer vowels will have lower energy. This does not match reality—the empirical distribution over inventory sizes (shown in Figure 5) shows that the mode is actually 5 and small inventories are uncommon: no 1-vowel inventory is attested and only one 2-vowel inventory is known. A probabilistic model over all vowel systems must implicitly model the size of the system. Indeed, our models pit all potential inventories against each other, bestowing the extra burden to match the empirical distribution over size.\nFrequency of Inventories. Another problemis the inability to model frequency. While for inventories of a modest size (3-5 vowels) there are very few unique attested systems, there is a plethora of\nattested larger vowel systems. The energy minimization paradigm has no principled manner to tell the scientist how likely a novel system may be. Appealing again to the empirical distribution over attested vowel systems, we consider the relative diversity of systems of each size. We graph this in Figure 5. Consider all vowel systems of size 7. There are (|V| 7 ) potential inventories, yet the empirical distribution is remarkably peaked.Our probabilistic models have the advantage in this context as well, as they naturally quantify the likelihood of an individual inventory.\nTypology is a Small-Data Problem. In contrast to many common problems in applied NLP, e.g., part-of-speech tagging, parsing and machine translation, the modeling of linguistic typology is fundamentally a “small-data” problem. Out of the 7105 languages on earth, we only have linguistic annotation for 2600 of them (Comrie et al., 2013). Moreover, we only have phonetic and phonological annotation for a much smaller set of languages— between 300-500 (Maddieson, 2013). Given the paucity of data, overfitting on only those attested languages is a dangerous possibility—just because a certain inventory has never been attested, it is probably wrong to conclude that it is impossible— or even improbable—on that basis alone. By analogy to language modeling, almost all sentences observed in practice are novel with respect to the training data, but we still must employ a principled manner to discriminate high-probability sentences (which are syntactically and semantically coherent) from low-probability ones. Probabilistic modeling provides a natural paradigm for this sort of investigation—machine learning has developed well-understood smoothing techniques, e.g., regularization with tuning on a held-out dev set, to avoid overfitting in a small-data scenario.\nRelated Work in NLP. Various point processes have been previously applied to potpourri of tasks\nin NLP. Determinantal point processes have found a home in the literature in tasks that require diversity. E.g., DPPs have achieved state-of-the-art results on multi-document document summarization (Kulesza and Taskar, 2011), news article selection (Affandi et al., 2012) recommender systems (Gartrell et al., 2017), joint clustering of verbal lexical semantic properties (Reichart and Korhonen, 2013), inter alia. Poisson point processes have also been applied to NLP problems: Yee et al. (2015) model the emerging topic on social media using a homogeneous point process and Lukasik et al. (2015) apply a log-Gaussian point process, a variant of the Poisson point process, to rumor detection in Twitter. We are unaware of previous attempts to probabilistically model vowel inventory typology.\nFuture Work. This work lends itself to several technical extensions. One could expand the function f to more completely characterize each vowel’s acoustic properties, perceptual properties, or distinctive features (footnote 7). One could generalize our point process models to sample finite subsets from the continuous space of vowels (footnote 3). One could consider augmenting the MPP with a new factor that explicitly controls the size of the vowel inventory. Richer families of point processes might also be worth exploring. For example, perhaps the vowel inventory is generated by some temporal mechanism with latent intermediate steps, such as sequential selection of the vowels or evolutionary drift of the inventory. Another possibility is that vowel systems tend to reuse distinctive features or even follow factorial designs, so that an inventory with creaky front vowels also tends to have creaky back vowels."
    }, {
      "heading" : "9 Conclusions",
      "text" : "We have presented a series of point process models for the modeling of vowel system inventory typology with the goal of a mathematical grounding for research in phonological typology. All models were additionally given a deep parameterization to learn representations similar to perceptual space in cognitive science. Also, we motivated our preference for probabilistic modeling in linguistic typology over previously proposed computational approaches and argued it is a more natural research paradigm. Additionally, we have introduced several novel evaluation metrics for research in vowelsystem typology, which we hope will spark further interest in the area. Their performance was empirically validated on the Becker-Kristal corpus, which includes data from over 200 languages."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The first author was funded by an NDSEG graduate fellowship, and the second author by NSF grant IIS1423276. We would like to thank Tim Vieira and Huda Khayrallah for helpful initial feedback."
    } ],
    "references" : [ {
      "title" : "A learning algorithm for Boltzmann machines",
      "author" : [ "David H. Ackley", "Geoffrey E. Hinton", "Terrence J. Sejnowski." ],
      "venue" : "Cognitive Science 9(1):147–169.",
      "citeRegEx" : "Ackley et al\\.,? 1985",
      "shortCiteRegEx" : "Ackley et al\\.",
      "year" : 1985
    }, {
      "title" : "Markov determinantal point processes",
      "author" : [ "Raja Hafiz Affandi", "Alex Kulesza", "Emily B. Fox." ],
      "venue" : "Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence. pages 26–35.",
      "citeRegEx" : "Affandi et al\\.,? 2012",
      "shortCiteRegEx" : "Affandi et al\\.",
      "year" : 2012
    }, {
      "title" : "Acoustic Typology of Vowel Inventories and Dispersion Theory: Insights from a Large Cross-Linguistic Corpus",
      "author" : [ "Roy Becker-Kristal." ],
      "venue" : "Ph.D. thesis, UCLA.",
      "citeRegEx" : "Becker.Kristal.,? 2010",
      "shortCiteRegEx" : "Becker.Kristal.",
      "year" : 2010
    }, {
      "title" : "Praat, a system for doing phonetics by computer",
      "author" : [ "Paulus Petrus Gerardus Boersma" ],
      "venue" : "Glot International 5.",
      "citeRegEx" : "Boersma,? 2002",
      "shortCiteRegEx" : "Boersma",
      "year" : 2002
    }, {
      "title" : "EynardMehta theorem, Schur process, and their Pfaffian analogs",
      "author" : [ "Alexei Borodin", "Eric M. Rains." ],
      "venue" : "Journal of Statistical Physics 121(34):291–317.",
      "citeRegEx" : "Borodin and Rains.,? 2005",
      "shortCiteRegEx" : "Borodin and Rains.",
      "year" : 2005
    }, {
      "title" : "Introduction",
      "author" : [ "Bernard Comrie", "Matthew S. Dryer", "David Gil", "Martin Haspelmath." ],
      "venue" : "Matthew S. Dryer and Martin Haspelmath, editors, The World Atlas of Language Structures Online, Max Planck Institute for Evolutionary Anthropol-",
      "citeRegEx" : "Comrie et al\\.,? 2013",
      "shortCiteRegEx" : "Comrie et al\\.",
      "year" : 2013
    }, {
      "title" : "The computational complexity of probabilistic inference using Bayesian belief networks",
      "author" : [ "Gregory F. Cooper." ],
      "venue" : "Artificial Intelligence 42(2-3):393–405.",
      "citeRegEx" : "Cooper.,? 1990",
      "shortCiteRegEx" : "Cooper.",
      "year" : 1990
    }, {
      "title" : "Low-rank factorization of determinantal point processes pages 1912–1918",
      "author" : [ "Mike Gartrell", "Ulrich Paquet", "Noam Koenigstein" ],
      "venue" : null,
      "citeRegEx" : "Gartrell et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Gartrell et al\\.",
      "year" : 2017
    }, {
      "title" : "Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images",
      "author" : [ "Stuart Geman", "Donald Geman." ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence (6):721–741.",
      "citeRegEx" : "Geman and Geman.,? 1984",
      "shortCiteRegEx" : "Geman and Geman.",
      "year" : 1984
    }, {
      "title" : "Phonological Typology",
      "author" : [ "Matthew K. Gordon." ],
      "venue" : "Oxford.",
      "citeRegEx" : "Gordon.,? 2016",
      "shortCiteRegEx" : "Gordon.",
      "year" : 2016
    }, {
      "title" : "Learning and relearning in Boltzmann machines",
      "author" : [ "Geoffrey E. Hinton", "Terry J. Sejnowski." ],
      "venue" : "David E. Rumelhart and James L. McClelland, editors, Parallel Distributed Processing, MIT Press, volume 2, chapter 7, pages 282–317.",
      "citeRegEx" : "Hinton and Sejnowski.,? 1986",
      "shortCiteRegEx" : "Hinton and Sejnowski.",
      "year" : 1986
    }, {
      "title" : "Beitrag zur theorie des ferromagnetismus",
      "author" : [ "Ernst Ising." ],
      "venue" : "Zeitschrift für Physik A Hadrons and Nuclei 31(1):253–258.",
      "citeRegEx" : "Ising.,? 1925",
      "shortCiteRegEx" : "Ising.",
      "year" : 1925
    }, {
      "title" : "Kindersprache, Aphasie und allgemeine Lautgesetze",
      "author" : [ "Roman Jakobson." ],
      "venue" : "Suhrkamp Frankfurt aM.",
      "citeRegEx" : "Jakobson.,? 1941",
      "shortCiteRegEx" : "Jakobson.",
      "year" : 1941
    }, {
      "title" : "Learning determinantal point processes",
      "author" : [ "Alex Kulesza", "Ben Taskar." ],
      "venue" : "Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence. pages 419–427.",
      "citeRegEx" : "Kulesza and Taskar.,? 2011",
      "shortCiteRegEx" : "Kulesza and Taskar.",
      "year" : 2011
    }, {
      "title" : "Determinantal point processes for machine learning",
      "author" : [ "Alex Kulesza", "Ben Taskar." ],
      "venue" : "Foundations and Trends R",
      "citeRegEx" : "Kulesza and Taskar.,? 2012",
      "shortCiteRegEx" : "Kulesza and Taskar.",
      "year" : 2012
    }, {
      "title" : "A Course in Phonetics",
      "author" : [ "Peter Ladefoged", "Keith Johnson." ],
      "venue" : "Centage.",
      "citeRegEx" : "Ladefoged and Johnson.,? 2014",
      "shortCiteRegEx" : "Ladefoged and Johnson.",
      "year" : 2014
    }, {
      "title" : "The Sounds of the World’s Languages",
      "author" : [ "Peter Ladefoged", "Ian Maddieson." ],
      "venue" : "Oxford.",
      "citeRegEx" : "Ladefoged and Maddieson.,? 1996",
      "shortCiteRegEx" : "Ladefoged and Maddieson.",
      "year" : 1996
    }, {
      "title" : "Numerical simulation of vowel quality systems: The role of perceptual contrast",
      "author" : [ "Johan Liljencrants", "Björn Lindblom." ],
      "venue" : "Language pages 839–862.",
      "citeRegEx" : "Liljencrants and Lindblom.,? 1972",
      "shortCiteRegEx" : "Liljencrants and Lindblom.",
      "year" : 1972
    }, {
      "title" : "Phonetic universals in vowel systems",
      "author" : [ "Björn Lindblom." ],
      "venue" : "Experimental Phonology pages 13–44.",
      "citeRegEx" : "Lindblom.,? 1986",
      "shortCiteRegEx" : "Lindblom.",
      "year" : 1986
    }, {
      "title" : "On the limited memory BFGS method for large scale optimization",
      "author" : [ "Dong C. Liu", "Jorge Nocedal." ],
      "venue" : "Mathematical Programming 45(1-3):503–528.",
      "citeRegEx" : "Liu and Nocedal.,? 1989",
      "shortCiteRegEx" : "Liu and Nocedal.",
      "year" : 1989
    }, {
      "title" : "Estimating the partition function by discriminance sampling",
      "author" : [ "Qiang Liu", "Jian Peng", "Alexander T. Ihler", "John W. Fisher III." ],
      "venue" : "Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence. pages 514–522.",
      "citeRegEx" : "Liu et al\\.,? 2015",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2015
    }, {
      "title" : "Point process modelling of rumour dynamics in social media",
      "author" : [ "Michal Lukasik", "Trevor Cohn", "Kalina Bontcheva." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference",
      "citeRegEx" : "Lukasik et al\\.,? 2015",
      "shortCiteRegEx" : "Lukasik et al\\.",
      "year" : 2015
    }, {
      "title" : "The coincidence approach to stochastic point processes",
      "author" : [ "Odile Macchi." ],
      "venue" : "Advances in Applied Probability pages 83–122.",
      "citeRegEx" : "Macchi.,? 1975",
      "shortCiteRegEx" : "Macchi.",
      "year" : 1975
    }, {
      "title" : "Vowel quality inventories",
      "author" : [ "Ian Maddieson." ],
      "venue" : "Matthew S. Dryer and Martin Haspelmath, editors, The World Atlas of Language Structures Online, Max Planck Institute for Evolutionary Anthropology, Leipzig. http://wals.info/chapter/2.",
      "citeRegEx" : "Maddieson.,? 2013",
      "shortCiteRegEx" : "Maddieson.",
      "year" : 2013
    }, {
      "title" : "PHOIBLE online",
      "author" : [ "Steven Moran", "Daniel McCloy", "Richard Wright." ],
      "venue" : "Leipzig: Max Planck Institute for Evolutionary Anthropology .",
      "citeRegEx" : "Moran et al\\.,? 2014",
      "shortCiteRegEx" : "Moran et al\\.",
      "year" : 2014
    }, {
      "title" : "Comparison of several proposed perceptual representations of vowel spectra",
      "author" : [ "Terrance M. Nearey", "Michael Kiefte." ],
      "venue" : "Proceedings of the XVth International Congress of Phonetic Sciences 1:1005– 1008.",
      "citeRegEx" : "Nearey and Kiefte.,? 2003",
      "shortCiteRegEx" : "Nearey and Kiefte.",
      "year" : 2003
    }, {
      "title" : "Improved lexical acquisition through DPP-based verb clustering",
      "author" : [ "Roi Reichart", "Anna Korhonen." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Lin-",
      "citeRegEx" : "Reichart and Korhonen.,? 2013",
      "shortCiteRegEx" : "Reichart and Korhonen.",
      "year" : 2013
    }, {
      "title" : "Explaining vowel inventory tendencies via simulation: Finding a role for quantal locations and formant normalization",
      "author" : [ "Brian Roark." ],
      "venue" : "North East Linguistic Society. volume 31, pages 419–434.",
      "citeRegEx" : "Roark.,? 2001",
      "shortCiteRegEx" : "Roark.",
      "year" : 2001
    }, {
      "title" : "Monte Carlo Statistical Methods",
      "author" : [ "Christian P. Robert", "George Casella." ],
      "venue" : "Springer-Verlag New York, Inc., Secaucus, NJ, USA.",
      "citeRegEx" : "Robert and Casella.,? 2005",
      "shortCiteRegEx" : "Robert and Casella.",
      "year" : 2005
    }, {
      "title" : "Vowel Perception and Production",
      "author" : [ "Burton S. Rosner", "John B. Pickering." ],
      "venue" : "Oxford University Press.",
      "citeRegEx" : "Rosner and Pickering.,? 1994",
      "shortCiteRegEx" : "Rosner and Pickering.",
      "year" : 1994
    }, {
      "title" : "The dispersionfocalization theory of vowel systems",
      "author" : [ "Jean-Luc Schwartz", "Louis-Jean Boë", "Nathalie Vallée", "Christian Abry." ],
      "venue" : "Journal of Phonetics 25(3):255–286.",
      "citeRegEx" : "Schwartz et al\\.,? 1997",
      "shortCiteRegEx" : "Schwartz et al\\.",
      "year" : 1997
    }, {
      "title" : "Toward a universal law of generalization for psychological science",
      "author" : [ "Roger N. Shepard." ],
      "venue" : "Science 237(4820):1317–1323.",
      "citeRegEx" : "Shepard.,? 1987",
      "shortCiteRegEx" : "Shepard.",
      "year" : 1987
    }, {
      "title" : "The quantal nature of speech: Evidence from articulatory-acoustic data",
      "author" : [ "Kenneth N. Stevens." ],
      "venue" : "E. E. David and P. B. Denes, editors, Human Communication: A Unified View, McGraw-Hill, pages 51–56.",
      "citeRegEx" : "Stevens.,? 1972",
      "shortCiteRegEx" : "Stevens.",
      "year" : 1972
    }, {
      "title" : "On the quantal nature of speech",
      "author" : [ "Kenneth N Stevens." ],
      "venue" : "Journal of Phonetics 17:3–45.",
      "citeRegEx" : "Stevens.,? 1989",
      "shortCiteRegEx" : "Stevens.",
      "year" : 1989
    }, {
      "title" : "Cloze procedure: a new tool for measuring readability",
      "author" : [ "Wilson L. Taylor." ],
      "venue" : "Journalism and Mass Communication Quarterly 30(4):415.",
      "citeRegEx" : "Taylor.,? 1953",
      "shortCiteRegEx" : "Taylor.",
      "year" : 1953
    }, {
      "title" : "Markov Point Processes and Their Applications",
      "author" : [ "M.N.M. Van Lieshout." ],
      "venue" : "Imperial College Press, London.",
      "citeRegEx" : "Lieshout.,? 2000",
      "shortCiteRegEx" : "Lieshout.",
      "year" : 2000
    }, {
      "title" : "An Introduction to Linguistic Typology",
      "author" : [ "Viveka Velupillai." ],
      "venue" : "John Benjamins Publishing Company.",
      "citeRegEx" : "Velupillai.,? 2012",
      "shortCiteRegEx" : "Velupillai.",
      "year" : 2012
    }, {
      "title" : "Modeling and characterizing social media topics using the gamma distribution",
      "author" : [ "Connie Yee", "Nathan Keane", "Liang Zhou." ],
      "venue" : "EVENTS. pages 117– 122.",
      "citeRegEx" : "Yee et al\\.,? 2015",
      "shortCiteRegEx" : "Yee et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 36,
      "context" : "It is a typological universal that every language contains both vowels and consonants (Velupillai, 2012).",
      "startOffset" : 86,
      "endOffset" : 104
    }, {
      "referenceID" : 16,
      "context" : "Likewise, all languages contrast vowels based on height, although which contrast is made is language-dependent (Ladefoged and Maddieson, 1996).",
      "startOffset" : 111,
      "endOffset" : 142
    }, {
      "referenceID" : 24,
      "context" : "phonemes have been attested cross-linguistically (Moran et al., 2014), certain regions of acoustic",
      "startOffset" : 49,
      "endOffset" : 69
    }, {
      "referenceID" : 9,
      "context" : "7 (Gordon, 2016).",
      "startOffset" : 2,
      "endOffset" : 16
    }, {
      "referenceID" : 15,
      "context" : ", which are known as formants (Ladefoged and Johnson, 2014).",
      "startOffset" : 30,
      "endOffset" : 59
    }, {
      "referenceID" : 17,
      "context" : "The dispersion criterion (Liljencrants and Lindblom, 1972; Lindblom, 1986) states that the phonemes of a language must be “spread out” so that they are easily discriminated by a listener.",
      "startOffset" : 25,
      "endOffset" : 74
    }, {
      "referenceID" : 18,
      "context" : "The dispersion criterion (Liljencrants and Lindblom, 1972; Lindblom, 1986) states that the phonemes of a language must be “spread out” so that they are easily discriminated by a listener.",
      "startOffset" : 25,
      "endOffset" : 74
    }, {
      "referenceID" : 30,
      "context" : "A good vowel system now consists of vowels that contrast with each other and are individually desirable (Schwartz et al., 1997).",
      "startOffset" : 104,
      "endOffset" : 127
    }, {
      "referenceID" : 0,
      "context" : "A Markov Point Process (MPP) (Van Lieshout, 2000)—also known as a Boltzmann machine (Ackley et al., 1985; Hinton and Sejnowski, 1986)— generalizes the BPP by adding pairwise interactions between vowels.",
      "startOffset" : 84,
      "endOffset" : 133
    }, {
      "referenceID" : 10,
      "context" : "A Markov Point Process (MPP) (Van Lieshout, 2000)—also known as a Boltzmann machine (Ackley et al., 1985; Hinton and Sejnowski, 1986)— generalizes the BPP by adding pairwise interactions between vowels.",
      "startOffset" : 84,
      "endOffset" : 133
    }, {
      "referenceID" : 11,
      "context" : "closely resemble Ising models (Ising, 1925), but with the difference that Ising models are typically lattice-structured, rather than fully connected.",
      "startOffset" : 30,
      "endOffset" : 43
    }, {
      "referenceID" : 6,
      "context" : "Inference in fully connected MPPs, just as in general Markov Random Fields (MRFs), is intractable (Cooper, 1990) and we must rely on approximation.",
      "startOffset" : 98,
      "endOffset" : 112
    }, {
      "referenceID" : 8,
      "context" : "In this work, we estimate any needed properties of the MPP distribution by (approximately) drawing vowel inventories from it via Gibbs sampling (Geman and Geman, 1984; Robert and Casella, 2005).",
      "startOffset" : 144,
      "endOffset" : 193
    }, {
      "referenceID" : 28,
      "context" : "In this work, we estimate any needed properties of the MPP distribution by (approximately) drawing vowel inventories from it via Gibbs sampling (Geman and Geman, 1984; Robert and Casella, 2005).",
      "startOffset" : 144,
      "endOffset" : 193
    }, {
      "referenceID" : 22,
      "context" : "A determinantal point process (DPP) (Macchi, 1975) provides an elegant alternative to an MPP, and one that is directly suited to modeling both focalization and dispersion.",
      "startOffset" : 36,
      "endOffset" : 50
    }, {
      "referenceID" : 4,
      "context" : "We focus on the L-ensemble parameterization of the DPP, due to Borodin and Rains (2005).4 This type of DPP defines the probability of an inventory V ⊆ V as",
      "startOffset" : 63,
      "endOffset" : 88
    }, {
      "referenceID" : 14,
      "context" : "Most DPPs are L-ensembles (Kulesza and Taskar, 2012).",
      "startOffset" : 26,
      "endOffset" : 52
    }, {
      "referenceID" : 2,
      "context" : "For each of 223 languages,5 Becker-Kristal (2010) provides the vowel inventory as a set of IPA symbols, listing the first 5 formants for each vowel (or fewer when not available in the original source).",
      "startOffset" : 28,
      "endOffset" : 50
    }, {
      "referenceID" : 31,
      "context" : "The formula is also reminiscent of Shepard (1987)’s “universal law of generalization,” which says here that the proba-",
      "startOffset" : 35,
      "endOffset" : 50
    }, {
      "referenceID" : 31,
      "context" : "The distances in this psychological space may be predictive of generalization (Shepard, 1987) or of perception.",
      "startOffset" : 78,
      "endOffset" : 93
    }, {
      "referenceID" : 29,
      "context" : "ping of vowels from acoustic space to perceptual space is often presumed to be nonlinear (Rosner and Pickering, 1994; Nearey and Kiefte, 2003), and there are many perceptually-oriented phonetic scales, e.",
      "startOffset" : 89,
      "endOffset" : 142
    }, {
      "referenceID" : 25,
      "context" : "ping of vowels from acoustic space to perceptual space is often presumed to be nonlinear (Rosner and Pickering, 1994; Nearey and Kiefte, 2003), and there are many perceptually-oriented phonetic scales, e.",
      "startOffset" : 89,
      "endOffset" : 142
    }, {
      "referenceID" : 34,
      "context" : "This task is defined by analogy to the traditional semantic cloze task (Taylor, 1953), where the reader is asked to fill in a missing word in the sentence from the context.",
      "startOffset" : 71,
      "endOffset" : 85
    }, {
      "referenceID" : 19,
      "context" : "Minimization of the parameters is performed with the L-BFGS algorithm (Liu and Nocedal, 1989).",
      "startOffset" : 70,
      "endOffset" : 93
    }, {
      "referenceID" : 20,
      "context" : "Computing cross-entropy exactly is intractable with the MPP, so we resort to an unbiased importance sampling scheme where we draw samples from the BPP and reweight according to the MPP (Liu et al., 2015).",
      "startOffset" : 185,
      "endOffset" : 203
    }, {
      "referenceID" : 12,
      "context" : "Jakobson (1941)",
      "startOffset" : 0,
      "endOffset" : 16
    }, {
      "referenceID" : 17,
      "context" : "Liljencrants and Lindblom (1972) propose a coordinate descent algorithm to optimize E(m).",
      "startOffset" : 0,
      "endOffset" : 33
    }, {
      "referenceID" : 5,
      "context" : "annotation for 2600 of them (Comrie et al., 2013).",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 23,
      "context" : "Moreover, we only have phonetic and phonological annotation for a much smaller set of languages— between 300-500 (Maddieson, 2013).",
      "startOffset" : 113,
      "endOffset" : 130
    }, {
      "referenceID" : 2,
      "context" : "Figure 4: Percentage of the vowel inventories (y-axis) in the Becker-Kristal corpus (Becker-Kristal, 2010) that have a given vowel (shown in IPA along the x-axis).",
      "startOffset" : 84,
      "endOffset" : 106
    }, {
      "referenceID" : 1,
      "context" : "and Taskar, 2011), news article selection (Affandi et al., 2012) recommender systems (Gartrell et al.",
      "startOffset" : 42,
      "endOffset" : 64
    }, {
      "referenceID" : 7,
      "context" : ", 2012) recommender systems (Gartrell et al., 2017), joint clustering of verbal lexical semantic properties (Reichart and Korhonen, 2013), inter alia.",
      "startOffset" : 28,
      "endOffset" : 51
    }, {
      "referenceID" : 26,
      "context" : ", 2017), joint clustering of verbal lexical semantic properties (Reichart and Korhonen, 2013), inter alia.",
      "startOffset" : 64,
      "endOffset" : 93
    }, {
      "referenceID" : 36,
      "context" : "plied to NLP problems: Yee et al. (2015) model the emerging topic on social media using a homogeneous point process and Lukasik et al.",
      "startOffset" : 23,
      "endOffset" : 41
    }, {
      "referenceID" : 21,
      "context" : "(2015) model the emerging topic on social media using a homogeneous point process and Lukasik et al. (2015) apply a log-Gaussian point process, a variant of the Poisson point process, to rumor detection in",
      "startOffset" : 86,
      "endOffset" : 108
    } ],
    "year" : 2017,
    "abstractText" : "Linguistic typology studies the range of structures present in human language. The main goal of the field is to discover which sets of possible phenomena are universal, and which are merely frequent. For example, all languages have vowels, while most—but not all—languages have an [u] sound. In this paper we present the first probabilistic treatment of a basic question in phonological typology: What makes a natural vowel inventory? We introduce a series of deep stochastic point processes, and contrast them with previous computational, simulation-based approaches. We provide a comprehensive suite of experiments on over 200 distinct languages.",
    "creator" : "LaTeX with hyperref package"
  }
}