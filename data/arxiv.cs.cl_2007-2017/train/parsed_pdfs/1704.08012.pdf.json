{
  "name" : "1704.08012.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Topically Driven Neural Language Model",
    "authors" : [ "Jey Han Lau", "Timothy Baldwin", "Trevor Cohn" ],
    "emails" : [ "jeyhan.lau@gmail.com,", "tb@ldwin.net,", "t.cohn@unimelb.edu.au" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Topic models provide a powerful tool for extracting the macro-level content structure of a document collection in the form of the latent topics (usually in the form of multinomial distributions over terms), with a plethora of applications in NLP (Hall et al., 2008; Newman et al., 2010a; Wang and McCallum, 2006). A myriad of variants of the classical LDA method (Blei et al., 2003) have been proposed, including recent work on neural topic models (Cao et al., 2015; Wan et al., 2012; Larochelle and Lauly, 2012; Hinton and Salakhutdinov, 2009).\nSeparately, language models have long been a foundational component of any NLP task involving generation or textual normalisation of a noisy input (including speech, OCR and the processing of social media text). The primary purpose of a language model is to predict the probability of a\nspan of text, traditionally at the sentence level, under the assumption that sentences are independent of one another, although recent work has started using broader local context such as the preceding sentences (Wang and Cho, 2016; Ji et al., 2016).\nIn this paper, we combine the benefits of a topic model and language model in proposing a topically-driven language model, whereby we jointly learn topics and word sequence information. This allows us to both sensitise the predictions of the language model to the larger document narrative using topics, and to generate topics which are better sensitised to local context and are hence more coherent and interpretable.\nOur model has two components: a language model and a topic model. We implement both components using neural networks, and train them jointly by treating each component as a sub-task in a multi-task learning setting. We show that our model is superior to other language models that leverage additional context, and that the generated topics are potentially more coherent than LDA topics. The architecture of the model provides an extra dimensionality of topic interpretability, in supporting the generation of sentences from a topic (or mix of topics). It is also highly flexible, in its ability to be supervised and incorporate side information, which we show to further improve language model performance. An open source implementation of our model is available at: https://github.com/jhlau/ topically-driven-language-model."
    }, {
      "heading" : "2 Related Work",
      "text" : "Griffiths et al. (2004) propose a model that learns topics and word dependencies using a Bayesian framework. Word generation is driven by either LDA or an HMM. For LDA, a word is generated based on a sampled topic in the document. For the\nar X\niv :1\n70 4.\n08 01\n2v 1\n[ cs\n.C L\n] 2\n6 A\npr 2\n01 7\nHMM, a word is conditioned on previous words. A key difference over our model is that their language model is driven by an HMM, which uses a fixed window and is therefore unable to track longrange dependencies.\nCao et al. (2015) relate the topic model view of documents and words — documents having a multinomial distribution over topics and topics having a multinomial distributional over words — from a neural network perspective by embedding these relationships in differentiable functions. With that, the model lost the stochasticity and Bayesian inference of LDA but gained non-linear complex representations. The authors further propose extensions to the model to do supervised learning where document labels are given.\nWang and Cho (2016) and Ji et al. (2016) relax the sentence independence assumption in language modelling, and use preceeding sentences as additional context. By treating words in preceeding sentences as a bag of words, Wang and Cho (2016) use an attentional mechanism to focus on these words when predicting the next word. The authors show that the incorporation of additional\ncontext helps language models."
    }, {
      "heading" : "3 Architecture",
      "text" : "The architecture of the proposed topically-driven language model (henceforth “tdlm”) is illustrated in Figure 1. There are two components in tdlm: a language model and a topic model. The language model is designed to capture word relations in sentences, while the topic model learns topical information in documents. The topic model works like an auto-encoder, where it is given the document words as input and optimised to predict them.\nThe topic model takes in word embeddings of a document and generates a document vector using a convolutional network. Given the document vector, we associate it with the topics via an attention scheme to compute a weighted mean of topic vectors, which is then used to predict a word in the document.\nThe language model is a standard LSTM language model (Hochreiter and Schmidhuber, 1997; Mikolov et al., 2010), but it incorporates the weighted topic vector generated by the topic model to predict succeeding words.\nMarrying the language and topic models allows the language model to be topically driven, i.e. it models not just word contexts but also the document context where the sentence occurs, in the form of topics."
    }, {
      "heading" : "3.1 Topic Model Component",
      "text" : "Let xi ∈ Re be the e-dimensional word vector for the i-th word in the document. A document of n words is represented as a concatenation of its word vectors:\nx1:n = x1 ⊕ x2 ⊕ ...⊕ xn\nwhere ⊕ denotes the concatenation operator. We use a number of convolutional filters to process the word vectors, but for clarity we will explain the network with one filter.\nLet wv ∈ Reh be a convolutional filter which we apply to a window of hwords to generate a feature. A feature ci for a window of words xi:i+h−1 is given as follows:\nci = I(w ᵀ vxi:i+h−1 + bv)\nwhere bv is a bias term and I is the identity function.1 A feature map c is a collection of features computed from all windows of words:\nc = [c1, c2, ..., cn−h+1]\nwhere c ∈ Rn−h+1. To capture the most salient features in c, we apply a max-over-time pooling operation (Collobert et al., 2011), yielding a scalar:\nd = max i ci\nIn the case where we use a filters, we have d ∈ Ra, and this constitutes the vector representation of the document generated by the convolutional and max-over-time pooling network.\nThe topic vectors are stored in two lookup tables A ∈ Rk×a (input vector) and B ∈ Rk×b (output vector), where k is the number of topics, and a and b are the dimensions of the topic vectors.\nTo align the document vector d with the topics, we compute an attention vector which is used to\n1A non-linear function is typically used here, but preliminary experiments suggest that the identity function works best for tdlm.\ncompute a document-topic representation:2\np = softmax(Ad) (1)\ns = Bᵀp (2)\nwhere p ∈ Rk and s ∈ Rb. Intuitively, s is a weighted mean of topic vectors, with the weighting given by the attention p. This is inspired by the generative process of LDA, whereby documents are defined as having a multinomial distribution over topics.\nFinally s is connected to a dense layer with softmax output to predict each word in the document, where each word is generated independently as a unigram bag-of-words, and the model is optimised using categorical cross-entropy loss. In practice, to improve efficiency we compute loss for predicting a sequence of m1 words in the document, where m1 is a hyper-parameter."
    }, {
      "heading" : "3.2 Language Model Component",
      "text" : "The language model is implemented using LSTM units (Hochreiter and Schmidhuber, 1997):\nit = σ(Wivt +Uiht−1 + bi)\nft = σ(Wfvt +Ufht−1 + bf )\not = σ(Wovt +Uoht−1 + bi)\nĉt = tanh(Wcvt +Ucht−1 + bc)\nct = ft ct−1 + it ĉt ht = ot tanh(ct)\nwhere denotes element-wise product; it, ft, ot are the input, forget and output activations respectively at time step t; and vt, ht and ct are the input word embedding, LSTM hidden state, and cell state, respectively. Hereinafter W, U and b are used to refer to the model parameters.\nTraditionally, a language model operates at the sentence level, predicting the next word given its history of words in the sentence. The language model of tdlm incorporates topical information by assimilating the document-topic representation (s) with the hidden output of the LSTM (ht) at each time step t. To prevent tdlm from memorising the next word via the topic model network, we exclude the current sentence from the document context.\n2The attention mechanism was inspired by memory networks (Graves et al., 2014; Weston et al., 2014; Sukhbaatar et al., 2015; Tran et al., 2016). We explored various attention styles (including traditional schemes which use one vector for a topic), but found this approach to work best.\nWe use a gating unit similar to a GRU (Cho et al., 2014; Chung et al., 2014) to allow tdlm to learn the degree of influence of topical information on the language model:\nzt = σ(Wzs+Uzht + bz)\nrt = σ(Wrs+Urht + br)\nĥt = tanh(Whs+Uh(rt ht) + bh) h′t = (1− zt) ht + zt ĥt\n(3)\nwhere zt and rt are the update and reset gate activations respectively at timestep t. The new hidden state h′t is connected to a dense layer with linear transformation and softmax output to predict the next word, and the model is optimised using standard categorical cross-entropy loss."
    }, {
      "heading" : "3.3 Training and Regularisation",
      "text" : "tdlm is trained using minibatches and SGD.3 For the language model, a minibatch consists of a batch of sentences, while for the topic model it is a batch of documents (each predicting a sequence of m1 words).\nWe treat the language and topic models as subtasks in a multi-task learning setting, and train them jointly using categorical cross-entropy loss. Most parameters in the topic model are shared by the language model, as illustrated by their scopes (dotted lines) in Figure 1.\nHyper-parameters of tdlm are detailed in Table 1. Word embeddings for the topic model and language model components are not shared, although their dimensions are the same (e).4 For m1, m2 and m3, sequences/documents shorter than these thresholds are padded. Sentences longer than m2 are broken into multiple sequences, and documents longer than m3 are truncated. Optimal hyper-parameter settings are tuned using the development set; the presented values are used for experiments in Sections 4 and 5.\nTo regularise tdlm, we use dropout regularisation (Srivastava et al., 2014). We apply dropout to d and s in the topic model, and to the input word embedding and hidden output of the LSTM in the language model (Pham et al., 2013; Zaremba et al., 2014)."
    }, {
      "heading" : "4 Language Model Evaluation",
      "text" : "We use standard language model perplexity as the evaluation metric. In terms of dataset, we use doc-\n3We use Adam as the optimiser (Kingma and Ba, 2014). 4Word embeddings are updated during training.\nument collections from 3 sources: APNEWS, IMDB and BNC. APNEWS is a collection of Associated Press5 news articles from 2009 to 2016. IMDB is a set of movie reviews collected by Maas et al. (2011). BNC is the written portion of the British National Corpus (BNC Consortium, 2007), which contains excerpts from journals, books, letters, essays, memoranda, news and other types of text. For APNEWS and BNC, we randomly sub-sample a set of documents for our experiments.\nFor preprocessing, we tokenise words and sentences using Stanford CoreNLP (Klein and Manning, 2003). We lowercase all word tokens, filter word types that occur less than 10 times, and exclude the top 0.1% most frequent word types.6 We additionally remove stopwords for the topic model document context.7 All datasets are partitioned into training, development and test sets; preprocessed dataset statistics are presented in Table 2.\nWe tune hyper-parameters of tdlm based on development set language model perplexity. In general, we find that optimal settings are fairly robust across collections, with the exception of m3, as document length is collection dependent; optimal hyper-parameter values are given in Table 1. In terms of LSTM size, we explore 2 settings: a small model with 1 LSTM layer and 600 hidden units, and a large model with 2 layers and 900 hidden units.8 For the topic number, we experiment with 50, 100 and 150 topics. Word embeddings are pre-trained 300-dimension word2vec Google News vectors.9\nFor comparison, we compare tdlm with:10\nvanilla-lstm: A standard LSTM language model, using the same tdlm hyper-parameters where applicable. This is the baseline model.\nlclm: A larger context language model that incorporates context from preceding sentences (Wang and Cho, 2016), by treating the preceding sentence as a bag of words, and using an\n5https://www.ap.org/en-gb/. 6For the topic model, we remove word tokens that correspond to these filtered word types; for the language model we represent them as 〈unk〉 tokens (as for unseen words in test).\n7We use Mallet’s stopword list: https://github. com/mimno/Mallet/tree/master/stoplists.\n8Multi-layer LSTMs are vanilla stacked LSTMs without skip connections (Gers and Schmidhuber, 2000) or depthgating (Yao et al., 2015).\n9https://code.google.com/archive/p/ word2vec/.\n10Note that all models use the same pre-trained word2vec vectors.\nattentional mechanism when predicting the next word. An additional hyper-parameter in lclm is the number of preceeding sentences to incorporate, which we tune based on a development set (to 4 sentences in each case). All other hyperparameters (such as nbatch , e, nepoch , k2) are the same as tdlm.\nlstm+lda: A standard LSTM language model that incorporates LDA topic information. We first train an LDA model (Blei et al., 2003; Griffiths and Steyvers, 2004) to learn 50/100/150 topics for APNEWS, IMDB and BNC.11 For a document, the LSTM incorporates the LDA topic distribution (q) by concatenating it with the output hidden state (ht) to predict the next word (i.e. h′t = ht ⊕ q). That is, it incorporates topical information into the language model, but unlike tdlm the language model and topic model are trained separately.\nWe present language model perplexity performance in Table 3. All models outperform the baseline vanilla-lstm, with tdlm performing the\n11Based on Gibbs sampling; α = 0.1, β = 0.01.\nbest across all collections. lclm is competitive over the BNC, although the superiority of tdlm for the other collections is substantial. lstm+lda performs relatively well over APNEWS and IMDB, but very poorly over BNC.\nThe strong performance of tdlm over lclm suggests that compressing document context into topics benefits language modelling more than using extra context words directly.12 Overall, our results show that topical information can help language modelling and that joint inference of topic and language model produces the best results."
    }, {
      "heading" : "5 Topic Model Evaluation",
      "text" : "We saw that tdlm performs well as a language model, but it is also a topic model, and like LDA it produces: (1) a probability distribution over topics for each document (Equation (1)); and (2) a probability distribution over word types for each topic.\n12The context size of lclm (4 sentences) is technically smaller than tdlm (full document), however, note that increasing the context size does not benefit lclm, as the context size of 4 gives the best performance.\nRecall that s is a weighted mean of topic vectors for a document (Equation (2)). Generating the vocabulary distribution for a particular topic is therefore trivial: we can do so by treating s as having maximum weight (1.0) for the topic of interest, and no weight (0.0) for all other topics. Let Bt denote the topic output vector for the t-th topic. To generate the multinomial distribution over word types for the t-th topic, we replace s with Bt before computing the softmax over the vocabulary.\nTopic models are traditionally evaluated using model perplexity. There are various ways to estimate test perplexity (Wallach et al., 2009), but Chang et al. (2009) show that perplexity does not correlate with the coherence of the generated topics. Newman et al. (2010b); Mimno et al. (2011); Aletras and Stevenson (2013) propose automatic approaches to computing topic coherence, and Lau et al. (2014) summarises these methods to understand their differences. We propose using automatic topic coherence as a means to evaluate the topic model aspect of tdlm.\nFollowing Lau et al. (2014), we compute topic coherence using normalised PMI (“NPMI”) scores. Given the top-n words of a topic, coherence is computed based on the sum of pair-\nwise NPMI scores between topic words, where the word probabilities used in the NPMI calculation are based on co-occurrence statistics mined from English Wikipedia with a sliding window (Newman et al., 2010b; Lau et al., 2014).13\nBased on the findings of Lau and Baldwin (2016), we average topic coherence over the top5/10/15/20 topic words. To aggregate topic coherence scores for a model, we calculate the mean coherence over topics.\nIn terms of datasets, we use the same document collections (APNEWS, IMDB and BNC) as the language model experiments (Section 4). We use the same hyper-parameter settings for tdlm and do not tune them.\nFor comparison, we use the following topic models:\nlda: We use a LDA model as a baseline topic model. We use the same LDA models as were used to learn topic distributions for lstm+lda (Section 4).\n13We use this toolkit to compute topic coherence: https://github.com/jhlau/topic_ interpretability.\nntm: ntm is a neural topic model proposed by Cao et al. (2015). The document-topic and topicword multinomials are expressed from a neural network perspective using differentiable functions. Model hyper-parameters are tuned using development loss.\nTopic model performance is presented in Table 4. There are two models of tdlm (tdlm-small and tdlm-large), which specify the size of its LSTM model (1 layer+600 hidden vs. 2 layers+900 hidden; see Section 4). tdlm achieves encouraging results: it has the best performance over APNEWS, and is competitive over IMDB. lda, however, produces more coherent topics over BNC. Interestingly, coherence appears to increase as the topic number increases for lda, but the trend is less pronounced for tdlm. ntm performs the worst of the 3 topic models, and manual inspection reveals that topics are in general not very interpretable. Overall, the results suggest that tdlm topics are competitive: at best they are more coherent than lda topics, and at worst they are as good as lda topics.\nTo better understand the spread of coherence scores and impact of outliers, we present box plots for all models (number of topics = 100) over the 3 domains in Figure 2. Across all domains, ntm has poor performance and larger spread of scores. The difference between lda and tdlm is small (tdlm > lda in APNEWS, but lda < tdlm in BNC), which is consistent with our previous observation that tdlm topics are competitive with lda topics."
    }, {
      "heading" : "6 Extensions",
      "text" : "One strength of tdlm is its flexibility, owing to it taking the form of a neural network. To showcase this flexibility, we explore two simple extensions of tdlm, where we: (1) build a supervised model using document labels (Section 6.1); and (2) incorporate additional document metadata (Section 6.2)."
    }, {
      "heading" : "6.1 Supervised Model",
      "text" : "In datasets where document labels are known, supervised topic model extensions are designed to leverage the additional information to improve modelling quality. The supervised setting also has an additional advantage in that model evaluation is simpler, since models can be quantitatively assessed via classification accuracy.\nTo incorporate supervised document labels, we treat document classification as another sub-task in tdlm. Given a document and its label, we feed the document through the topic model network to generate the document-topic representation s, and connect it to another dense layer with softmax output to generate the probability distribution over classes.\nDuring training, we have additional minibatches for the documents. We start the document classification training after the topic and language models have completed training in each epoch.\nWe use 20NEWS in this experiment, which is a popular dataset for text classification. 20NEWS is a collection of forum-like messages from 20 newsgroups categories. We use the “bydate” version of the dataset, where the train and test partition is separated by a specific date. We sample 2K documents from the training set to create the development set. For preprocessing we tokenise words and sentence using Stanford CoreNLP (Klein and Manning, 2003), and lowercase all words. As with previous experiments (Section 4) we additionally filter low/high frequency word types and stopwords. Preprocessed dataset statistics are presented in Table 5.\nFor comparison, we use the same two topic\nTopic No. Metadata Coherence Perplexity\nmodels as in Section 5: ntm and lda. Both ntm and lda have natural supervised extensions (Cao et al., 2015; McAuliffe and Blei, 2008) for incorporating document labels. For this task, we tune the model hyper-parameters based on development accuracy.14 Classification accuracy for all models is presented in Table 6. We present tdlm results using only the small setting of LSTM (1 layer + 600 hidden), as we found there is little gain when using a larger LSTM. ntm performs very strongly, outperforming both lda and tdlm by a substantial margin. Comparing lda and tdlm, tdlm achieves better performance, especially when there is a smaller number of topics. Upon inspection of the topics we found that ntm topics are much less coherent than those of lda and tdlm, consistent with our observations from Section 5.\n14Most hyper-parameter values for tdlm are similar to those used in the language and topic model experiments; the only exceptions are: a = 80, b = 100, nepoch = 20, m3 = 150. The increase in parameters is unsurprising, as the additional supervision provides more constraint to the model."
    }, {
      "heading" : "6.2 Incorporating Document Metadata",
      "text" : "In APNEWS, each news article contains additional document metadata, including subject classification tags, such as “General News”, “Accidents and Disasters”, and “Military and Defense”. We present an extension to incorporate document metadata in tdlm to demonstrate its flexibility in integrating this additional information.\nAs some of the documents in our original APNEWS sample were missing tags, we re-sampled a set of APNEWS articles of the same size as our original, all of which have tags. In total, approximately 1500 unique tags can be found among the training articles.\nTo incorporate these tags, we represent each of them as a learnable vector and concatenate it with the document vector before computing the attention distribution. Let zi ∈ Rf denote the f -dimension vector for the i-th tag. For the j-th document, we sum up all tags associated with it:\ne = ntags∑ i=1 I(i, j)zi\nwhere ntags is the total number of unique tags, and function I(i, j) returns 1 is the i-th tag is in the j-th document or 0 otherwise. We compute d as before (Section 3.1), and concatenate it with the summed tag vector: d′ = d⊕ e.\nWe train two versions of tdlm on the new APNEWS dataset: (1) the vanilla version that ignores the tag information; and (2) the extended version which incorporates tag information.15 We exper-\n15Model hyper-parameters are the same as the ones used in the language (Section 4) and topic model (Section 5) experiments.\nimented with a few values for the tag vector size (f ) and find that a small value works well; in the following experiments we use f = 5. We evaluate the models based on language model perplexity and topic model coherence, and present the results in Table 7.16\nIn terms of language model perplexity, we see a consistent improvement over different topic settings, suggesting that the incorporation of tags improves modelling. In terms of topic coherence, there is a small but encouraging improvement (with one exception).\nTo investigate whether the vectors learnt for these tags are meaningful, we plot the top-14 most frequent tags in Figure 3.17 The plot seems reasonable: there are a few related tags that are close to each other, e.g. “State government” and “Government and politics”; “Crime” and “Violent Crime”; and “Social issues” and “Social affairs”."
    }, {
      "heading" : "7 Discussion",
      "text" : "Topics generated by topic models are typically interpreted by way of their top-N highest probability words. In tdlm, we can additionally generate sentences related to the topic, providing another way to understand the topics. To do this, we can constrain the topic vector for the language model to be the topic output vector of a particular topic (Equation (3)).\nWe present 4 topics from a APNEWS model (k = 100; LSTM size = “large”) and 3 randomly generated sentences conditioned on each\n16As the vanilla tdlm is trained on the new APNEWS dataset, the numbers are slightly different to those in Tables 3 and 4.\n17The 5-dimensional vectors are compressed using PCA.\ntopic in Table 8.18 The generated sentences highlight the content of the topics, providing another interpretable aspect for the topics. These results also reinforce that the language model is driven by topics."
    }, {
      "heading" : "8 Conclusion",
      "text" : "We propose tdlm, a topically driven neural language model. tdlm has two components: a language model and a topic model, which are jointly trained using a neural network. We demonstrate that tdlm outperforms a state-of-the-art language model that incorporates larger context, and that its topics are potentially more coherent than LDA topics. We additionally propose simple extensions of tdlm to incorporate information such as document labels and metadata, and achieved encouraging results."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank the anonymous reviewers for their insightful comments and valuable suggestions. This work was funded in part by the Australian Research Council."
    } ],
    "references" : [ {
      "title" : "Evaluating topic coherence using distributional semantics",
      "author" : [ "Nikos Aletras", "Mark Stevenson." ],
      "venue" : "Proceedings of the Tenth International Workshop on Computational Semantics (IWCS-10). Potsdam, Germany, pages 13–22.",
      "citeRegEx" : "Aletras and Stevenson.,? 2013",
      "shortCiteRegEx" : "Aletras and Stevenson.",
      "year" : 2013
    }, {
      "title" : "Latent Dirichlet allocation",
      "author" : [ "David M. Blei", "Andrew Y. Ng", "Michael I. Jordan." ],
      "venue" : "Journal of Machine Learning Research 3:993–1022.",
      "citeRegEx" : "Blei et al\\.,? 2003",
      "shortCiteRegEx" : "Blei et al\\.",
      "year" : 2003
    }, {
      "title" : "The British National Corpus, version 3 (BNC XML Edition)",
      "author" : [ "BNC Consortium." ],
      "venue" : "Distributed by Oxford University Computing Services on behalf of the BNC Consortium. http://www.natcorp.ox.ac.uk/.",
      "citeRegEx" : "Consortium.,? 2007",
      "shortCiteRegEx" : "Consortium.",
      "year" : 2007
    }, {
      "title" : "A novel neural topic model and its supervised extension",
      "author" : [ "Ziqiang Cao", "Sujian Li", "Yang Liu", "Wenjie Li", "Heng Ji." ],
      "venue" : "Proceedings of the 29th Annual Conference on Artificial Intelligence (AAAI15). Austin, Texas, pages 2210–2216.",
      "citeRegEx" : "Cao et al\\.,? 2015",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2015
    }, {
      "title" : "Reading tea leaves: How humans interpret topic models",
      "author" : [ "Jonathan Chang", "Sean Gerrish", "Chong Wang", "Jordan L. Boyd-Graber", "David M. Blei." ],
      "venue" : "Advances in Neural Information Processing Systems 21 (NIPS-09). Vancouver, Canada, pages 288–296.",
      "citeRegEx" : "Chang et al\\.,? 2009",
      "shortCiteRegEx" : "Chang et al\\.",
      "year" : 2009
    }, {
      "title" : "On the properties of neural machine translation: Encoder–decoder approaches",
      "author" : [ "Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio." ],
      "venue" : "Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statisti-",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "author" : [ "Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio." ],
      "venue" : "NIPS Deep Learning and Representation Learning Workshop. Montreal, Canada, pages 103–",
      "citeRegEx" : "Chung et al\\.,? 2014",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2014
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa." ],
      "venue" : "Journal of Machine Learning Research 12:2493–2537.",
      "citeRegEx" : "Collobert et al\\.,? 2011",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "Recurrent nets that time and count",
      "author" : [ "Felix A. Gers", "Jürgen Schmidhuber." ],
      "venue" : "Proceedings of the International Joint Conference on Neural Networks (IJCNN’2000). Como, Italy, pages 198–194.",
      "citeRegEx" : "Gers and Schmidhuber.,? 2000",
      "shortCiteRegEx" : "Gers and Schmidhuber.",
      "year" : 2000
    }, {
      "title" : "Neural turing machines",
      "author" : [ "Alex Graves", "Greg Wayne", "Ivo Danihelka." ],
      "venue" : "CoRR abs/1410.5401.",
      "citeRegEx" : "Graves et al\\.,? 2014",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2014
    }, {
      "title" : "Finding scientific topics",
      "author" : [ "Thomas L. Griffiths", "Mark Steyvers." ],
      "venue" : "Proceedings of the National Academy of Sciences 101:5228–5235.",
      "citeRegEx" : "Griffiths and Steyvers.,? 2004",
      "shortCiteRegEx" : "Griffiths and Steyvers.",
      "year" : 2004
    }, {
      "title" : "Integrating topics and syntax",
      "author" : [ "Thomas L. Griffiths", "Mark Steyvers", "David M. Blei", "Joshua B. Tenenbaum." ],
      "venue" : "Advances in Neural Information Processing Systems 17 (NIPS-05). Vancouver, Canada, pages 537–544.",
      "citeRegEx" : "Griffiths et al\\.,? 2004",
      "shortCiteRegEx" : "Griffiths et al\\.",
      "year" : 2004
    }, {
      "title" : "Studying the history of ideas using topic models",
      "author" : [ "David Hall", "Daniel Jurafsky", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP 2008). Honolulu, USA, pages 363–371.",
      "citeRegEx" : "Hall et al\\.,? 2008",
      "shortCiteRegEx" : "Hall et al\\.",
      "year" : 2008
    }, {
      "title" : "Replicated softmax: an undirected topic model",
      "author" : [ "Geoffrey E. Hinton", "Ruslan R. Salakhutdinov." ],
      "venue" : "Advances in Neural Information Processing Systems 21 (NIPS-09). Vancouver, Canada, pages 1607– 1614.",
      "citeRegEx" : "Hinton and Salakhutdinov.,? 2009",
      "shortCiteRegEx" : "Hinton and Salakhutdinov.",
      "year" : 2009
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural Computation 9:1735– 1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Document context language models",
      "author" : [ "Yangfeng Ji", "Trevor Cohn", "Lingpeng Kong", "Chris Dyer", "Jacob Eisenstein." ],
      "venue" : "Proceedings of ICLR-16 Workshop, 2016. Toulon, France.",
      "citeRegEx" : "Ji et al\\.,? 2016",
      "shortCiteRegEx" : "Ji et al\\.",
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "CoRR abs/1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Accurate unlexicalized parsing",
      "author" : [ "Dan Klein", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL 2003). Sapporo, Japan, pages 423–430.",
      "citeRegEx" : "Klein and Manning.,? 2003",
      "shortCiteRegEx" : "Klein and Manning.",
      "year" : 2003
    }, {
      "title" : "A neural autoregressive topic model",
      "author" : [ "Hugo Larochelle", "Stanislas Lauly." ],
      "venue" : "Advances in Neural Information Processing Systems 25. pages 2708– 2716.",
      "citeRegEx" : "Larochelle and Lauly.,? 2012",
      "shortCiteRegEx" : "Larochelle and Lauly.",
      "year" : 2012
    }, {
      "title" : "The sensitivity of topic coherence evaluation to topic cardinality",
      "author" : [ "Jey Han Lau", "Timothy Baldwin." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics — Human Language Technologies",
      "citeRegEx" : "Lau and Baldwin.,? 2016",
      "shortCiteRegEx" : "Lau and Baldwin.",
      "year" : 2016
    }, {
      "title" : "Machine reading tea leaves: Automatically evaluating topic coherence and topic model quality",
      "author" : [ "Jey Han Lau", "David Newman", "Timothy Baldwin." ],
      "venue" : "Proceedings of the 14th Conference of the EACL (EACL 2014). Gothenburg, Sweden, pages 530–539.",
      "citeRegEx" : "Lau et al\\.,? 2014",
      "shortCiteRegEx" : "Lau et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning word vectors for sentiment analysis",
      "author" : [ "Andrew L. Maas", "Raymond E. Daly", "Peter T. Pham", "Dan Huang", "Andrew Y. Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human",
      "citeRegEx" : "Maas et al\\.,? 2011",
      "shortCiteRegEx" : "Maas et al\\.",
      "year" : 2011
    }, {
      "title" : "Supervised topic models",
      "author" : [ "Jon D. McAuliffe", "David M. Blei." ],
      "venue" : "Advances in Neural Information Processing Systems 20 (NIPS-08). Vancouver, Canada, pages 121–128.",
      "citeRegEx" : "McAuliffe and Blei.,? 2008",
      "shortCiteRegEx" : "McAuliffe and Blei.",
      "year" : 2008
    }, {
      "title" : "Recurrent neural network based language model",
      "author" : [ "Tomas Mikolov", "Martin Karafiát", "Lukas Burget", "Jan Cernockỳ", "Sanjeev Khudanpur." ],
      "venue" : "Proceedings of the 11th Annual Conference of the International Speech Communication Association (IN-",
      "citeRegEx" : "Mikolov et al\\.,? 2010",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2010
    }, {
      "title" : "Optimizing semantic coherence in topic models",
      "author" : [ "David Mimno", "Hanna Wallach", "Edmund Talley", "Miriam Leenders", "Andrew McCallum." ],
      "venue" : "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP",
      "citeRegEx" : "Mimno et al\\.,? 2011",
      "shortCiteRegEx" : "Mimno et al\\.",
      "year" : 2011
    }, {
      "title" : "Visualizing document collections and search results using topic mapping",
      "author" : [ "David Newman", "Timothy Baldwin", "Lawrence Cavedon", "Sarvnaz Karimi", "David Martinez", "Justin Zobel." ],
      "venue" : "Journal of Web Semantics 8(2–3):169–175.",
      "citeRegEx" : "Newman et al\\.,? 2010a",
      "shortCiteRegEx" : "Newman et al\\.",
      "year" : 2010
    }, {
      "title" : "Automatic evaluation of topic coherence",
      "author" : [ "David Newman", "Jey Han Lau", "Karl Grieser", "Timothy Baldwin." ],
      "venue" : "Proceedings of Human Language Technologies: The 11th Annual Conference of the North American Chapter of the Association",
      "citeRegEx" : "Newman et al\\.,? 2010b",
      "shortCiteRegEx" : "Newman et al\\.",
      "year" : 2010
    }, {
      "title" : "Dropout improves recurrent neural networks for handwriting recognition",
      "author" : [ "Vu Pham", "Christopher Kermorvant", "Jérôme Louradour." ],
      "venue" : "CoRR abs/1312.4569.",
      "citeRegEx" : "Pham et al\\.,? 2013",
      "shortCiteRegEx" : "Pham et al\\.",
      "year" : 2013
    }, {
      "title" : "Dropout: A simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov." ],
      "venue" : "Journal of Machine Learning Research 15:1929–1958.",
      "citeRegEx" : "Srivastava et al\\.,? 2014",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2014
    }, {
      "title" : "End-to-end memory networks",
      "author" : [ "Sainbayar Sukhbaatar", "Arthur Szlam", "Jason Weston", "Rob Fergus." ],
      "venue" : "Advances in Neural Information Processing Systems 28 (NIPS-15). Montreal, Canada, pages 2440–2448.",
      "citeRegEx" : "Sukhbaatar et al\\.,? 2015",
      "shortCiteRegEx" : "Sukhbaatar et al\\.",
      "year" : 2015
    }, {
      "title" : "Recurrent memory networks for language modeling",
      "author" : [ "Ke Tran", "Arianna Bisazza", "Christof Monz." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics — Human Language Technologies",
      "citeRegEx" : "Tran et al\\.,? 2016",
      "shortCiteRegEx" : "Tran et al\\.",
      "year" : 2016
    }, {
      "title" : "Evaluation methods for topic models",
      "author" : [ "Hanna M. Wallach", "Iain Murray", "Ruslan Salakhutdinov", "David Mimno." ],
      "venue" : "Proceedings of the 26th International Conference on Machine Learning (ICML-09). Montreal, Canada, pages 1105–1112.",
      "citeRegEx" : "Wallach et al\\.,? 2009",
      "shortCiteRegEx" : "Wallach et al\\.",
      "year" : 2009
    }, {
      "title" : "A hybrid neural network-latent topic model",
      "author" : [ "Li Wan", "Leo Zhu", "Rob Fergus." ],
      "venue" : "Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics (AISTATS-12). La Palma, Canary Islands, pages 1287–1294.",
      "citeRegEx" : "Wan et al\\.,? 2012",
      "shortCiteRegEx" : "Wan et al\\.",
      "year" : 2012
    }, {
      "title" : "Largercontext language modelling with recurrent neural network",
      "author" : [ "Tian Wang", "Kyunghyun Cho." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016). Berlin, Germany, pages 1319–1329.",
      "citeRegEx" : "Wang and Cho.,? 2016",
      "shortCiteRegEx" : "Wang and Cho.",
      "year" : 2016
    }, {
      "title" : "Topics over time: a non-Markov continuous-time model of topical trends",
      "author" : [ "Xuerui Wang", "Andrew McCallum." ],
      "venue" : "Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. Philadelphia, USA,",
      "citeRegEx" : "Wang and McCallum.,? 2006",
      "shortCiteRegEx" : "Wang and McCallum.",
      "year" : 2006
    }, {
      "title" : "Memory networks",
      "author" : [ "Jason Weston", "Sumit Chopra", "Antoine Bordes." ],
      "venue" : "CoRR abs/1410.3916.",
      "citeRegEx" : "Weston et al\\.,? 2014",
      "shortCiteRegEx" : "Weston et al\\.",
      "year" : 2014
    }, {
      "title" : "Depth-gated LSTM",
      "author" : [ "Kaisheng Yao", "Trevor Cohn", "Katerina Vylomova", "Kevin Duh", "Chris Dyer." ],
      "venue" : "CoRR abs/1508.03790.",
      "citeRegEx" : "Yao et al\\.,? 2015",
      "shortCiteRegEx" : "Yao et al\\.",
      "year" : 2015
    }, {
      "title" : "Recurrent neural network regularization",
      "author" : [ "Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals." ],
      "venue" : "CoRR abs/1409.2329.",
      "citeRegEx" : "Zaremba et al\\.,? 2014",
      "shortCiteRegEx" : "Zaremba et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "Topic models provide a powerful tool for extracting the macro-level content structure of a document collection in the form of the latent topics (usually in the form of multinomial distributions over terms), with a plethora of applications in NLP (Hall et al., 2008; Newman et al., 2010a; Wang and McCallum, 2006).",
      "startOffset" : 246,
      "endOffset" : 312
    }, {
      "referenceID" : 25,
      "context" : "Topic models provide a powerful tool for extracting the macro-level content structure of a document collection in the form of the latent topics (usually in the form of multinomial distributions over terms), with a plethora of applications in NLP (Hall et al., 2008; Newman et al., 2010a; Wang and McCallum, 2006).",
      "startOffset" : 246,
      "endOffset" : 312
    }, {
      "referenceID" : 34,
      "context" : "Topic models provide a powerful tool for extracting the macro-level content structure of a document collection in the form of the latent topics (usually in the form of multinomial distributions over terms), with a plethora of applications in NLP (Hall et al., 2008; Newman et al., 2010a; Wang and McCallum, 2006).",
      "startOffset" : 246,
      "endOffset" : 312
    }, {
      "referenceID" : 1,
      "context" : "A myriad of variants of the classical LDA method (Blei et al., 2003) have been proposed, including recent work on neural topic models (Cao et al.",
      "startOffset" : 49,
      "endOffset" : 68
    }, {
      "referenceID" : 3,
      "context" : ", 2003) have been proposed, including recent work on neural topic models (Cao et al., 2015; Wan et al., 2012; Larochelle and Lauly, 2012; Hinton and Salakhutdinov, 2009).",
      "startOffset" : 73,
      "endOffset" : 169
    }, {
      "referenceID" : 32,
      "context" : ", 2003) have been proposed, including recent work on neural topic models (Cao et al., 2015; Wan et al., 2012; Larochelle and Lauly, 2012; Hinton and Salakhutdinov, 2009).",
      "startOffset" : 73,
      "endOffset" : 169
    }, {
      "referenceID" : 18,
      "context" : ", 2003) have been proposed, including recent work on neural topic models (Cao et al., 2015; Wan et al., 2012; Larochelle and Lauly, 2012; Hinton and Salakhutdinov, 2009).",
      "startOffset" : 73,
      "endOffset" : 169
    }, {
      "referenceID" : 13,
      "context" : ", 2003) have been proposed, including recent work on neural topic models (Cao et al., 2015; Wan et al., 2012; Larochelle and Lauly, 2012; Hinton and Salakhutdinov, 2009).",
      "startOffset" : 73,
      "endOffset" : 169
    }, {
      "referenceID" : 33,
      "context" : "The primary purpose of a language model is to predict the probability of a span of text, traditionally at the sentence level, under the assumption that sentences are independent of one another, although recent work has started using broader local context such as the preceding sentences (Wang and Cho, 2016; Ji et al., 2016).",
      "startOffset" : 287,
      "endOffset" : 324
    }, {
      "referenceID" : 15,
      "context" : "The primary purpose of a language model is to predict the probability of a span of text, traditionally at the sentence level, under the assumption that sentences are independent of one another, although recent work has started using broader local context such as the preceding sentences (Wang and Cho, 2016; Ji et al., 2016).",
      "startOffset" : 287,
      "endOffset" : 324
    }, {
      "referenceID" : 15,
      "context" : "Wang and Cho (2016) and Ji et al. (2016) relax the sentence independence assumption in language modelling, and use preceeding sentences as additional context.",
      "startOffset" : 24,
      "endOffset" : 41
    }, {
      "referenceID" : 15,
      "context" : "Wang and Cho (2016) and Ji et al. (2016) relax the sentence independence assumption in language modelling, and use preceeding sentences as additional context. By treating words in preceeding sentences as a bag of words, Wang and Cho (2016) use an attentional mechanism to focus on these words when predicting the next word.",
      "startOffset" : 24,
      "endOffset" : 240
    }, {
      "referenceID" : 14,
      "context" : "The language model is a standard LSTM language model (Hochreiter and Schmidhuber, 1997; Mikolov et al., 2010), but it incorporates the weighted topic vector generated by the topic model to predict succeeding words.",
      "startOffset" : 53,
      "endOffset" : 109
    }, {
      "referenceID" : 23,
      "context" : "The language model is a standard LSTM language model (Hochreiter and Schmidhuber, 1997; Mikolov et al., 2010), but it incorporates the weighted topic vector generated by the topic model to predict succeeding words.",
      "startOffset" : 53,
      "endOffset" : 109
    }, {
      "referenceID" : 7,
      "context" : "To capture the most salient features in c, we apply a max-over-time pooling operation (Collobert et al., 2011), yielding a scalar:",
      "startOffset" : 86,
      "endOffset" : 110
    }, {
      "referenceID" : 14,
      "context" : "The language model is implemented using LSTM units (Hochreiter and Schmidhuber, 1997):",
      "startOffset" : 51,
      "endOffset" : 85
    }, {
      "referenceID" : 9,
      "context" : "The attention mechanism was inspired by memory networks (Graves et al., 2014; Weston et al., 2014; Sukhbaatar et al., 2015; Tran et al., 2016).",
      "startOffset" : 56,
      "endOffset" : 142
    }, {
      "referenceID" : 35,
      "context" : "The attention mechanism was inspired by memory networks (Graves et al., 2014; Weston et al., 2014; Sukhbaatar et al., 2015; Tran et al., 2016).",
      "startOffset" : 56,
      "endOffset" : 142
    }, {
      "referenceID" : 29,
      "context" : "The attention mechanism was inspired by memory networks (Graves et al., 2014; Weston et al., 2014; Sukhbaatar et al., 2015; Tran et al., 2016).",
      "startOffset" : 56,
      "endOffset" : 142
    }, {
      "referenceID" : 30,
      "context" : "The attention mechanism was inspired by memory networks (Graves et al., 2014; Weston et al., 2014; Sukhbaatar et al., 2015; Tran et al., 2016).",
      "startOffset" : 56,
      "endOffset" : 142
    }, {
      "referenceID" : 5,
      "context" : "We use a gating unit similar to a GRU (Cho et al., 2014; Chung et al., 2014) to allow tdlm to learn the degree of influence of topical information on the language model:",
      "startOffset" : 38,
      "endOffset" : 76
    }, {
      "referenceID" : 6,
      "context" : "We use a gating unit similar to a GRU (Cho et al., 2014; Chung et al., 2014) to allow tdlm to learn the degree of influence of topical information on the language model:",
      "startOffset" : 38,
      "endOffset" : 76
    }, {
      "referenceID" : 28,
      "context" : "To regularise tdlm, we use dropout regularisation (Srivastava et al., 2014).",
      "startOffset" : 50,
      "endOffset" : 75
    }, {
      "referenceID" : 27,
      "context" : "We apply dropout to d and s in the topic model, and to the input word embedding and hidden output of the LSTM in the language model (Pham et al., 2013; Zaremba et al., 2014).",
      "startOffset" : 132,
      "endOffset" : 173
    }, {
      "referenceID" : 37,
      "context" : "We apply dropout to d and s in the topic model, and to the input word embedding and hidden output of the LSTM in the language model (Pham et al., 2013; Zaremba et al., 2014).",
      "startOffset" : 132,
      "endOffset" : 173
    }, {
      "referenceID" : 16,
      "context" : "We use Adam as the optimiser (Kingma and Ba, 2014).",
      "startOffset" : 29,
      "endOffset" : 50
    }, {
      "referenceID" : 15,
      "context" : "We use Adam as the optimiser (Kingma and Ba, 2014). Word embeddings are updated during training. ument collections from 3 sources: APNEWS, IMDB and BNC. APNEWS is a collection of Associated Press5 news articles from 2009 to 2016. IMDB is a set of movie reviews collected by Maas et al. (2011). BNC is the written portion of the British National Corpus (BNC Consortium, 2007), which contains excerpts from journals, books, letters, essays, memoranda, news and other types of text.",
      "startOffset" : 30,
      "endOffset" : 293
    }, {
      "referenceID" : 17,
      "context" : "For preprocessing, we tokenise words and sentences using Stanford CoreNLP (Klein and Manning, 2003).",
      "startOffset" : 74,
      "endOffset" : 99
    }, {
      "referenceID" : 33,
      "context" : "lclm: A larger context language model that incorporates context from preceding sentences (Wang and Cho, 2016), by treating the preceding sentence as a bag of words, and using an",
      "startOffset" : 89,
      "endOffset" : 109
    }, {
      "referenceID" : 8,
      "context" : "Multi-layer LSTMs are vanilla stacked LSTMs without skip connections (Gers and Schmidhuber, 2000) or depthgating (Yao et al.",
      "startOffset" : 69,
      "endOffset" : 97
    }, {
      "referenceID" : 36,
      "context" : "Multi-layer LSTMs are vanilla stacked LSTMs without skip connections (Gers and Schmidhuber, 2000) or depthgating (Yao et al., 2015).",
      "startOffset" : 113,
      "endOffset" : 131
    }, {
      "referenceID" : 1,
      "context" : "We first train an LDA model (Blei et al., 2003; Griffiths and Steyvers, 2004) to learn 50/100/150 topics for APNEWS, IMDB and BNC.",
      "startOffset" : 28,
      "endOffset" : 77
    }, {
      "referenceID" : 10,
      "context" : "We first train an LDA model (Blei et al., 2003; Griffiths and Steyvers, 2004) to learn 50/100/150 topics for APNEWS, IMDB and BNC.",
      "startOffset" : 28,
      "endOffset" : 77
    }, {
      "referenceID" : 31,
      "context" : "There are various ways to estimate test perplexity (Wallach et al., 2009), but Chang et al.",
      "startOffset" : 51,
      "endOffset" : 73
    }, {
      "referenceID" : 3,
      "context" : ", 2009), but Chang et al. (2009) show that perplexity does not correlate with the coherence of the generated topics.",
      "startOffset" : 13,
      "endOffset" : 33
    }, {
      "referenceID" : 3,
      "context" : ", 2009), but Chang et al. (2009) show that perplexity does not correlate with the coherence of the generated topics. Newman et al. (2010b); Mimno et al.",
      "startOffset" : 13,
      "endOffset" : 139
    }, {
      "referenceID" : 3,
      "context" : ", 2009), but Chang et al. (2009) show that perplexity does not correlate with the coherence of the generated topics. Newman et al. (2010b); Mimno et al. (2011); Aletras and Stevenson (2013) propose automatic approaches to computing topic coherence, and Lau et al.",
      "startOffset" : 13,
      "endOffset" : 160
    }, {
      "referenceID" : 0,
      "context" : "(2011); Aletras and Stevenson (2013) propose automatic approaches to computing topic coherence, and Lau et al.",
      "startOffset" : 8,
      "endOffset" : 37
    }, {
      "referenceID" : 0,
      "context" : "(2011); Aletras and Stevenson (2013) propose automatic approaches to computing topic coherence, and Lau et al. (2014) summarises these methods to understand their differences.",
      "startOffset" : 8,
      "endOffset" : 118
    }, {
      "referenceID" : 26,
      "context" : "Given the top-n words of a topic, coherence is computed based on the sum of pairwise NPMI scores between topic words, where the word probabilities used in the NPMI calculation are based on co-occurrence statistics mined from English Wikipedia with a sliding window (Newman et al., 2010b; Lau et al., 2014).",
      "startOffset" : 265,
      "endOffset" : 305
    }, {
      "referenceID" : 20,
      "context" : "Given the top-n words of a topic, coherence is computed based on the sum of pairwise NPMI scores between topic words, where the word probabilities used in the NPMI calculation are based on co-occurrence statistics mined from English Wikipedia with a sliding window (Newman et al., 2010b; Lau et al., 2014).",
      "startOffset" : 265,
      "endOffset" : 305
    }, {
      "referenceID" : 20,
      "context" : "Following Lau et al. (2014), we compute topic coherence using normalised PMI (“NPMI”) scores.",
      "startOffset" : 10,
      "endOffset" : 28
    }, {
      "referenceID" : 19,
      "context" : "Based on the findings of Lau and Baldwin (2016), we average topic coherence over the top5/10/15/20 topic words.",
      "startOffset" : 25,
      "endOffset" : 48
    }, {
      "referenceID" : 3,
      "context" : "ntm: ntm is a neural topic model proposed by Cao et al. (2015). The document-topic and topicword multinomials are expressed from a neural network perspective using differentiable functions.",
      "startOffset" : 45,
      "endOffset" : 63
    }, {
      "referenceID" : 17,
      "context" : "For preprocessing we tokenise words and sentence using Stanford CoreNLP (Klein and Manning, 2003), and lowercase all words.",
      "startOffset" : 72,
      "endOffset" : 97
    }, {
      "referenceID" : 3,
      "context" : "Both ntm and lda have natural supervised extensions (Cao et al., 2015; McAuliffe and Blei, 2008) for incorporating document labels.",
      "startOffset" : 52,
      "endOffset" : 96
    }, {
      "referenceID" : 22,
      "context" : "Both ntm and lda have natural supervised extensions (Cao et al., 2015; McAuliffe and Blei, 2008) for incorporating document labels.",
      "startOffset" : 52,
      "endOffset" : 96
    } ],
    "year" : 2017,
    "abstractText" : "Language models are typically applied at the sentence level, without access to the broader document context. We present a neural language model that incorporates document context in the form of a topic model-like architecture, thus providing a succinct representation of the broader document context outside of the current sentence. Experiments over a range of datasets demonstrate that our model outperforms a pure sentence-based model in terms of language model perplexity, and leads to topics that are potentially more coherent than those produced by a standard LDA topic model. Our model also has the ability to generate related sentences for a topic, providing another way to interpret topics.",
    "creator" : "LaTeX with hyperref package"
  }
}