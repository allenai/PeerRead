{
  "name" : "1501.07645.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "HYPER-PARAMETER OPTIMIZATION OF DEEP CONVOLUTIONAL NETWORKS FOR OBJECT RECOGNITION",
    "authors" : [ "Sachin S. Talathi", "Diego CA" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms— hyper-parameter optimization, deep convolution networks, sequential model based optimization"
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "The primary task for a supervised machine learning algorithm is to use training dataset {xtr, ytr} to find a function f : x → y, that also generalize well across the test (or the hold out) dataset {xh, yh}. Very often f is obtained through the optimization of a training criterion, C, with respect to a sets of parameters, θ. The learning algorithm used to optimize C usually contains its own set of free parameters λl, referred to as the learning algorithm hyper-parameters. These hyper-parameters are often estimated using grid search cross validation. In addition to the learning algorithm hyper-parameters, λl, neural network models such as deep convolution networks (DCNs) also comprise of hyper-parameters λa /∈ λl, that define the architectural configuration of the network. Grid search techniques are prohibitively expensive to tune λa, given the fact that there are a few tens of these architectural hyper-parameters. As a result, many of the stateof-the-art DCNs are manually designed, making the task of tuning these hyper-parameters more of an art than a science [1].\nIn recent years, there has been a concerted effort in the machine learning community to develop better algorithms to solve the hyper-parameter optimization problem [1, 2, 3, 4, 5]. Many of these works have successfully applied direct search methods for non-linear optimization such as the sequential model based optimization (SMBO) to generate better results on various supervised machine learning tasks than were previously reported. Motivated by these works, in this paper we attempt to address the question: Can SMBO be used to determine superior architectural configurations for DCNs? The paper is organized as follows: In the Methods Section 2, we will briefly formulate the problem of hyper-parameter optimization for DCNs. We then present the general strategy of sequential model based optimization (SMBO) [4] and summarize our approach to SMBO for designing DCN architectures. The results of image classification training and evaluation on the benchmark CIFAR-10 dataset are then presented in the Results Section 3, which is followed by the Conclusion."
    }, {
      "heading" : "2. METHODS",
      "text" : ""
    }, {
      "heading" : "2.1. Formulation of the problem",
      "text" : "Let Mλ(x,w) represent the DCN model that operates on input data x ∈ RD and generates an estimate ŷ for the output data y ∈ ZC2 . The DCN model, M , is parameterized by two set of parameters, the first beingw, which are obtained through the optimization of a training criterion, C, using a gradient descent type learning algorithm such as the back-propagation algorithm and the second being λ = {λa, λl}, which represent the set of the so called hyper-parameters. The hyper-parameters λa define the DCN architecture and the hyper-parameters λl, are associated with the learning algorithm used to optimize C. The objective for DCN hyper-parameter optimization is\nar X\niv :1\n50 1.\n07 64\n5v 2\n[ cs\n.C V\n] 1\n7 M\nay 2\n01 5\nto solve the joint optimization problem as stated below:\n{w, λ} = argmin λ [Ψ(ŷh, yh)] where,\nŷh = Mλ(argmin w (C(xtr, ytr, θ)) , xh) (1)\nwhere {(xtr, ytr), (xh, yh)}∈ (x, y), are the input and output training and hold-out (or the test) data set respectively, Ψ = ∑ {yh} Iyh 6=ŷh ."
    }, {
      "heading" : "2.2. Sequential model based optimization",
      "text" : "SMBO is a direct search method for non-linear optimization, in which one begins by selecting a meta-model of the function for which an extrema is sought. One then applies an active learning strategy to select a query point that provides the most potential to approach the extrema. More specifically, let us assume that we have a database D1:t = {λ1:t, e1:t} of t DCN models, where λi|ti=1 represents the set of hyper-parameters and ei|ti=1 is the validation error on the hold-out dataset generated by each of the t DCN models. The basic idea underlying SMBO is to replace the original optimization problem of finding extrema of a given function, such as Ψ(λ), which is time consuming and computationally expensive, with an equivalent problem of optimization of expected value of an utility function, u(e) [4]. As we will see below, optimizing over the expected value of the utility function is computationally much cheaper and faster than solving the original problem. Under SMBO, one usually begins by assigning a prior distribution p(e) on e. One then uses the database D1:t to obtain an estimate for the likelihood function p(λ1:t|e). The prior and the likelihood function are used to obtain an estimate for the posterior distribution p(e|λ1:t) ∝ p(λ1:t|e)p(e). The objective then is to choose λt+1, which maximizes u(e) under p(e|λ1:t), i.e., λt+1 = argmax\nλ [E(u(e))], where E(u(e)) is given as:\nE(u(e)) = ∫ u(e)p(e|λ1:t)de\n= ∫ u(e)\np(λ1:t|e)p(e) p(λ1:t) de (2)\nA common choice for the utility function u(e), is the Expected Improvement function [4], u(e) = max ((e∗ − e), 0), in which case, Eq. 4 becomes\nE(u(e)) = ∫ e∗ 0 (e∗ − e)p(e|λ1:t)de (3)\nThen under SMBO we have,\nλt+1 = argmax λ\n[ e∗\np(λ1:t) ∫ e∗ 0 p(λ1:t|e)p(e)de ] (4)\nHyperOpt (D1:t, p,N ): While t ≤ N do: 1. Estimate e∗ s.t. p(e1:t < e∗) = 0.5 2. Use e∗ and λ1:t to estimate l(λ) and g(λ) 3. Evaluate λt+1 according to Eq. 5 and Eq. 6 4. Train the new DCN model with λt+1 to estimate et+1 5. t← t+ 1\nTable 1. SMBO algorithm for estimating architecture hyper-parameters for DCN.\nIf p(e < e∗) = γ (a constant), i.e., choose e∗ to be some quantile of observed e values, and we define two density functions; l(λ) = p(λ1:t|e) when e ≤ e∗ and g(λ) = p(λ1:t|e) when e > e∗ as proposed in [1], then,\nλt+1 = argmax λ\n[ e∗γl(λ)\nγl(λ) + (1− γ)g(λ)\n] (5)\nBergstra et.al., [1], proposed an adaptive Parson estimator algorithm to evaluate Eq. 5 so as to maximize the ratio l(λ)/g(λ). In this work, we propose a simple strategy to evaluate Eq. 5 as follows: let l(λ) + g(λ) = U(λ) = k (a constant). In other words, λ is drawn from a uniform distribution. Then, if γ = 0.5, we have from Eq. 5\nλt+1 = 2e∗\nk argmax λ [l(λ)] (6)\nOur proposed simplification in Eq. 6, does not require an estimate for both l(λ) and g(λ). We can simply pick λ ∈ U(λ) and evaluate according to empirical distribution of l(λ) generated from D1:t to evaluate the next potential λt+1. Since the proposed algorithm chooses λ ∈ U(λ) at every step, a much larger space of potential λ’s are explored, which may in turn slow down the convergence rate to an optimal λ∗. In order to counter this,we adopt a hybrid strategy by combining Eq. 5 and Eq. 6. With probability p, at every iteration, we choose the potential λt+1 according to Eq. 5 and with probability 1− p, we choose the potential λt+1 according to Eq. 6. In Table 1, we summarize the steps involved in our proposed algorithm."
    }, {
      "heading" : "3. RESULTS",
      "text" : "We evaluate our proposed SMBO algorithm on the CIFAR-10 benchmark, which consists of 60,000 32x32 color images. The collection is split into 50,000 training and 10,000 testing images. All the DCNs generated\nby our proposed algorithm were trained using cudaconvnet21. We used a 3 step cooling procedure; starting with learning rate l = 0.01, the momentum m = 0.9, the weight decay parameter wc = 0.0005 for the first 120 epochs followed by another 20 epochs by reducing learning rate by a factor of 10 (keeping other parameters the same) and then training for 10 more epochs by further reducing learning rate by a factor of 10.\nSince the primary focus for us in this work is to determine whether SMBO can be used to identify suitable DCN architectures, we fixed the DCN hyper-parameters associated with the back-propagation learning algorithm as described above. The set of DCN architecture hyperparameters that we consider for optimization are listed in Table 2.\nIt has been reported in the literature [7] that very deep networks are difficult to train primarily suffering from vanishing gradient problem at larger depths. In order to alleviate this problem, for our implementation of SMBO for DCN architectural hyper-parameters, all the DCNs are generated to comprise a local logistic regression (LR) cost function layer at the output of one or more of the convolution block.\nFor the results presented here, we consider t = 32 as the size of our initial database based on our analysis of random search hyper-parameter optimization [5] and we set p = 0.9.\n1https://code.google.com/p/cuda-convnet2/\nIn Figure 1a, we plot the mean (std. error, shown in yellow) test error (evaluated in multi-view test mode, [6]) E(i) = 〈ei−10:i〉 and in Figure 1b, we plot the minimum test error M(i) = min(e0:i) as function of the iteration number i, respectively. We see that the average test error gradually decrease towards an optimal solution, the best minimum found also decreases with increasing iterations. Furthermore, our proposed SMBO procedure generated a large number of “good” DCN architectures that produce test-error of < 11 % even with only 150 training epochs (not-reported). In comparison, the best hand-tuned DCN architecture, produced by [6] exhibits 11% test error in multi-view mode and requires a longer training time on the order of 500 epochs.\nSince the state-of-the-art performance numbers for the CIFAR-10 benchmark dataset are usually reported in multi-view mode (with data-augmentation [6]), we report multi-view test error of 7.81% for the best DCN generated by our proposed hyper-parameter optimization strategy, which compares favorably to the current stateof-the-art result on CIFAR-10 of 7.97% [8]. In Table 3, we summarize the top 3 DCN architectures found by our proposed SMBO procedure that produced multi-view test error< 9% on the CIFAR-10 benchmark. In ??, we summarize the number of parameters in each of these 3 DCN models.\nAt the time of writing of this manuscript for camera ready version, we came across a recent paper [9], that reported multi-view test error of 7.25% on CIFAR10 benchmark, using a hand designed DCN network, that is comprised of only convolution layers and has 1.3 M parameters. Yet another paper [10], reported the\nutility of using parametric-relu neuron as opposed to the relu neurons. While none of the optimized DCN networks that we report in Table 3 generate better performance numbers than the latest state-of-the-art numbers reported in [9], we wanted to determine whether the use of parametric-relu neuron can boost the performance of the optimized DCN networks that we have identified through the hyper-parameter optimization approach. Accordingly, we retrained the smallest of the three DCN networks from Table 3 using a version of parametric-rectified non-linear neurons of type y = ax(x ≤ 0) + √ x(x > 0), where a is a learnable parameter, fixed per neuron layer in the DCN network. We were able to obtain multi-view test error score of 6.9 %, which to the best of our knowledge, represents the state-of-the-art score for CIFAR-10 benchmark. In Table 4, we summarize all the known best in class numbers for CIFAR-10 benchmark."
    }, {
      "heading" : "4. CONCLUSION",
      "text" : "In this paper, we have proposed a simple SMBO algorithm and a recipe for hyper-parameter optimization of DCN architectures. We have demonstrated that SMBO can be used to generate a large number of “good” DCN architectures, which may then form a backbone for further investigations. Our results suggest that indeed SMBO can be used to identify superior DCNs. In summary, our work in this paper in addition to those from earlier works [1, 3] broaden the scope of the models that can be realistically investigated, without the need for the researchers to be restricted to manual evaluation of a few architectural parameters at any given time."
    }, {
      "heading" : "5. REFERENCES",
      "text" : "[1] J. Bergstra, R. Bardenet, Y. Bengio, and B. Kegl, “Algorithms for hyper-parameter optimization,” in NIPS, 2011, vol. 24.\n[2] Distributed asynchronous hyper parameter optimization. https://github.com/hyperopt/hyperopt.\n[3] J. Snoek, H. Larochelle, and R.P. Adam, “Practical bayesian optimization for machine learning,” in NIPS, 2012, vol. 25.\n[4] E. Brochu, V.M. Cora, and N. Freitas, “A tutorial on bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning,” Arxiv, vol. arXiv:1012.2599, 2010.\n[5] J. Bergstra and Y. Bengio, “Random search for hyper-parameter optimization,” Journal of Machine Learning Research, vol. 13, pp. 281–305, 2012.\n[6] A. Krizhevsky, “Learning multiple layers of features from tiny images,” Tech. Rep., Dept. Computer Science, University of Toronto, 2009.\n[7] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large scale image recognition,” Arxiv, vol. arXiv:1409.1556, 2014.\n[8] C. Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu, “Deeply supervised nets,” in NIPS, 2014.\n[9] J.T. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller, “Striving for simplicity, all convolution net,” in ICLR, 2015.\n[10] K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into rectifiers, surpassing human-level performance on imagenet classification,” in Arxiv:1502.01852, 2015.\n[11] I.J. Goodfellow, F. Warde, D. Mirza, A. Courville, and Y. Bengio, “Maxout networks,” in ICML, 2013.\n[12] L. Wan, M. Zeiler, S. Zhang, Y. LeCun, and R. Fergus, “Regularization of neural networks using dropconnect,” in ICML, 2013.\n[13] M. F. Stollenga, J. Masci, F. Gomez, , and J Schmidhuber, “Deep networks with internal selective attention through feedback connections.,” in NIPS, 2014.\n[14] M. Lin, Q. Chen, , and S. Yan, “Network in network,” in ICLR, 2014.\n[15] C.Y. Lee, S. Xi, P. Gallagher, Z. Zhang, and Z. Tu, “Deeply supervised nets,” in NIPS, 2014."
    } ],
    "references" : [ {
      "title" : "Algorithms for hyper-parameter optimization",
      "author" : [ "J. Bergstra", "R. Bardenet", "Y. Bengio", "B. Kegl" ],
      "venue" : "NIPS, 2011, vol. 24.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Practical bayesian optimization for machine learning",
      "author" : [ "J. Snoek", "H. Larochelle", "R.P. Adam" ],
      "venue" : "NIPS, 2012, vol. 25.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A tutorial on bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning",
      "author" : [ "E. Brochu", "V.M. Cora", "N. Freitas" ],
      "venue" : "Arxiv, vol. arXiv:1012.2599, 2010.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Random search for hyper-parameter optimization",
      "author" : [ "J. Bergstra", "Y. Bengio" ],
      "venue" : "Journal of Machine Learning Research, vol. 13, pp. 281–305, 2012.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Learning multiple layers of features from tiny images",
      "author" : [ "A. Krizhevsky" ],
      "venue" : "Tech. Rep., Dept. Computer Science, University of Toronto, 2009.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Very deep convolutional networks for large scale image recognition",
      "author" : [ "K. Simonyan", "A. Zisserman" ],
      "venue" : "Arxiv, vol. arXiv:1409.1556, 2014.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Deeply supervised nets",
      "author" : [ "C.Y. Lee", "S. Xie", "P. Gallagher", "Z. Zhang", "Z. Tu" ],
      "venue" : "NIPS, 2014.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Striving for simplicity, all convolution net",
      "author" : [ "J.T. Springenberg", "A. Dosovitskiy", "T. Brox", "M. Riedmiller" ],
      "venue" : "ICLR, 2015.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Delving deep into rectifiers, surpassing human-level performance on imagenet classification",
      "author" : [ "K. He", "X. Zhang", "S. Ren", "J. Sun" ],
      "venue" : "Arxiv:1502.01852, 2015.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1852
    }, {
      "title" : "Maxout networks",
      "author" : [ "I.J. Goodfellow", "F. Warde", "D. Mirza", "A. Courville", "Y. Bengio" ],
      "venue" : "ICML, 2013.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Regularization of neural networks using dropconnect",
      "author" : [ "L. Wan", "M. Zeiler", "S. Zhang", "Y. LeCun", "R. Fergus" ],
      "venue" : "ICML, 2013.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Deep networks with internal selective attention through feedback connections",
      "author" : [ "M.F. Stollenga", "J. Masci", "F. Gomez", "J Schmidhuber" ],
      "venue" : "NIPS, 2014.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Network in network",
      "author" : [ "M. Lin", "Q. Chen", "S. Yan" ],
      "venue" : "ICLR, 2014.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Deeply supervised nets",
      "author" : [ "C.Y. Lee", "S. Xi", "P. Gallagher", "Z. Zhang", "Z. Tu" ],
      "venue" : "NIPS, 2014. 5",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "As a result, many of the stateof-the-art DCNs are manually designed, making the task of tuning these hyper-parameters more of an art than a science [1].",
      "startOffset" : 148,
      "endOffset" : 151
    }, {
      "referenceID" : 0,
      "context" : "In recent years, there has been a concerted effort in the machine learning community to develop better algorithms to solve the hyper-parameter optimization problem [1, 2, 3, 4, 5].",
      "startOffset" : 164,
      "endOffset" : 179
    }, {
      "referenceID" : 1,
      "context" : "In recent years, there has been a concerted effort in the machine learning community to develop better algorithms to solve the hyper-parameter optimization problem [1, 2, 3, 4, 5].",
      "startOffset" : 164,
      "endOffset" : 179
    }, {
      "referenceID" : 2,
      "context" : "In recent years, there has been a concerted effort in the machine learning community to develop better algorithms to solve the hyper-parameter optimization problem [1, 2, 3, 4, 5].",
      "startOffset" : 164,
      "endOffset" : 179
    }, {
      "referenceID" : 3,
      "context" : "In recent years, there has been a concerted effort in the machine learning community to develop better algorithms to solve the hyper-parameter optimization problem [1, 2, 3, 4, 5].",
      "startOffset" : 164,
      "endOffset" : 179
    }, {
      "referenceID" : 2,
      "context" : "We then present the general strategy of sequential model based optimization (SMBO) [4] and summarize our approach to SMBO for designing DCN architectures.",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 2,
      "context" : "The basic idea underlying SMBO is to replace the original optimization problem of finding extrema of a given function, such as Ψ(λ), which is time consuming and computationally expensive, with an equivalent problem of optimization of expected value of an utility function, u(e) [4].",
      "startOffset" : 278,
      "endOffset" : 281
    }, {
      "referenceID" : 2,
      "context" : "A common choice for the utility function u(e), is the Expected Improvement function [4], u(e) = max ((e∗ − e), 0), in which case, Eq.",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 0,
      "context" : ", choose e∗ to be some quantile of observed e values, and we define two density functions; l(λ) = p(λ1:t|e) when e ≤ e∗ and g(λ) = p(λ1:t|e) when e > e∗ as proposed in [1], then,",
      "startOffset" : 168,
      "endOffset" : 171
    }, {
      "referenceID" : 0,
      "context" : ", [1], proposed an adaptive Parson estimator algorithm to evaluate Eq.",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 4,
      "context" : "For normalization layer; we only consider local response normalization across filter maps [6], with a scaling factor of 0.",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 5,
      "context" : "It has been reported in the literature [7] that very deep networks are difficult to train primarily suffering from vanishing gradient problem at larger depths.",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 3,
      "context" : "For the results presented here, we consider t = 32 as the size of our initial database based on our analysis of random search hyper-parameter optimization [5] and we set p = 0.",
      "startOffset" : 155,
      "endOffset" : 158
    }, {
      "referenceID" : 4,
      "context" : "error, shown in yellow) test error (evaluated in multi-view test mode, [6]) E(i) = 〈ei−10:i〉 and in Figure 1b, we plot the minimum test error M(i) = min(e0:i) as function of the iteration number i, respectively.",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 4,
      "context" : "In comparison, the best hand-tuned DCN architecture, produced by [6] exhibits 11% test error in multi-view mode and requires a longer training time on the order of 500 epochs.",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 4,
      "context" : "Since the state-of-the-art performance numbers for the CIFAR-10 benchmark dataset are usually reported in multi-view mode (with data-augmentation [6]), we report multi-view test error of 7.",
      "startOffset" : 146,
      "endOffset" : 149
    }, {
      "referenceID" : 6,
      "context" : "97% [8].",
      "startOffset" : 4,
      "endOffset" : 7
    }, {
      "referenceID" : 7,
      "context" : "At the time of writing of this manuscript for camera ready version, we came across a recent paper [9], that reported multi-view test error of 7.",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 8,
      "context" : "Yet another paper [10], reported the utility of using parametric-relu neuron as opposed to the relu neurons.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 7,
      "context" : "While none of the optimized DCN networks that we report in Table 3 generate better performance numbers than the latest state-of-the-art numbers reported in [9], we wanted to determine whether the use of parametric-relu neuron can boost the performance of the optimized DCN networks that we have identified through the hyper-parameter optimization approach.",
      "startOffset" : 156,
      "endOffset" : 159
    }, {
      "referenceID" : 9,
      "context" : "Maxout [11] maxout 9.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 10,
      "context" : "38 >6 M Dropconnect [12] relu 9.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 11,
      "context" : "32 dasNet [13] maxout 9.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 12,
      "context" : "22 >6 M Network in Network [14] relu 8.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 13,
      "context" : "81 ≈1 M Deeply Supervised [15] relu 7.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 7,
      "context" : "97 ≈1 M All-CNN [9] relu 7.",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 0,
      "context" : "In summary, our work in this paper in addition to those from earlier works [1, 3] broaden the scope of the models that can be realistically investigated, without the need for the researchers to be restricted to manual evaluation of a few architectural parameters at any given time.",
      "startOffset" : 75,
      "endOffset" : 81
    }, {
      "referenceID" : 1,
      "context" : "In summary, our work in this paper in addition to those from earlier works [1, 3] broaden the scope of the models that can be realistically investigated, without the need for the researchers to be restricted to manual evaluation of a few architectural parameters at any given time.",
      "startOffset" : 75,
      "endOffset" : 81
    } ],
    "year" : 2015,
    "abstractText" : "Recently sequential model based optimization (SMBO) has emerged as a promising hyper-parameter optimization strategy in machine learning. In this work, we investigate SMBO to identify architecture hyper-parameters of deep convolution networks (DCNs) object recognition. We propose a simple SMBO strategy that starts from a set of random initial DCN architectures to generate new architectures, which on training perform well on a given dataset. Using the proposed SMBO strategy we are able to identify a number of DCN architectures that produce results that are comparable to state-of-the-art results on object recognition benchmarks.",
    "creator" : "LaTeX with hyperref package"
  }
}