{
  "name" : "1702.00992.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Automatic Prediction of Discourse Connectives",
    "authors" : [ "Eric Malmi", "Daniele Pighin", "Sebastian Krause", "Mikhail Kozhevnikov" ],
    "emails" : [ "eric.malmi@aalto.fi", "biondo@google.com", "qnan@google.com", "sebastian.krause@dfki.de" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Discourse connectives, also referred to as discourse markers, are used to bind together and to explicate the relation between pieces of text. It is a common language class exercise to be asked to impute suitable connectives to a text in order to improve the text flow. Similarly, it is important for computational summarization and text adaptation systems to be able to impute suitable dis-\n∗Work performed during an internship at Google.\ncourse connectives to produce natural-sounding utterances.\nIn this work, we study the problem of automatic discourse connective prediction. We limit ourselves to connectives which appear at the beginning of a sentence, linking the sentence to the preceding one. Even in this limited setting, an automatic discourse connective predictor has many concrete use cases. For example, in a questionanswering setting it could help to generate answers by collating sentences from multiple sources. In extractive text summarization, it could be used to determine what is the best way to join two sentences that used to be separated by one or more sentences. As part of a text-authoring application, it could suggest suitable connectives at the beginning of a new sentence.\nIn the literature, discourse connective prediction has been mainly studied as an intermediate step for the well-studied problem of implicit discourse relation prediction (Zhou et al., 2010; Xu et al., 2012). However, considering the aforementioned applications, we argue that connective prediction makes an interesting and relevant problem in its own right.\nThe contributions of this work can be summarized as follows:\n1. We present, to the best of our knowledge, the first extensive study on the problem of discourse connective prediction. This is an important problem with applications ranging from summarization, to text simplification, to answer synthesis and conversationalization for dialog systems.\n2. We describe the dataset that we collected, consisting of 2.9 million adjacent sentence pairs (with and without a connective) extracted from the English Wikipedia. For\nar X\niv :1\n70 2.\n00 99\n2v 1\n[ cs\n.C L\n] 3\nF eb\n2 01\n7\n10 000 sentences, we also include connectives imputed by human raters. The dataset will be made freely available on publication.\n3. We show that a recently proposed decomposable attention model (Parikh et al., 2016) yields a good experimental performance on the connective prediction task. The model clearly outperforms a word-pair baseline and obtains, quite remarkably, a better performance than human raters on the same task and data.\n4. We show that the collected discourse connective dataset can improve implicit discourse relation prediction via a simple model finetuning scheme, which alleviates the data scarcity problem present in the popular Penn Discourse Treebank (PDTB) dataset."
    }, {
      "heading" : "2 Related Work",
      "text" : "The problem of identifying implicit discourse relations has attracted considerable attention in recent years, but to the best of our knowledge, there is no previous work whose main focus is the prediction of discourse connectives. Both problems can be seen as instances of the broader class of bi-sequence classification tasks (Laha and Raykar, 2016).\nThe currently best performing methods for the relation prediction task are those introduced by Liu and Li (2016), who proposed a bidirectional LSTM with multi-level attention, and by Rutherford and Xue (2014), who employed Brown cluster pairs and co-reference patterns.\nPitler et al. (2008) showed that if a discourse connective is known, the (explicit) discourse relation can be inferred with a 93.09% accuracy, which has inspired several efforts at predicting connectives to improve implicit discourse relation prediction. Zhou et al. (2010) predicted connectives using an N -gram language model1, whereas Xu et al. (2012) employed word pairs and a selection of linguistically informed features. Liu et al. (2016) showed that predicting both connectives and relations using a convolutional neural network in a multi-task setting improves the relation prediction performance.\n1N -gram language models do not generally consider the previous sentence which makes them less suitable for predicting connectives between sentences.\nAlready in 2002, Marcu and Echihabi (2002) proposed to map certain unambiguous connectives to discourse relations in order to collect a large discourse relation dataset. Sporleder and Lascarides (2008), however, showed that models trained using such automatically collected training data do not generalize well. Nevertheless, by classifying freely-omissible connectives and only using those, Rutherford and Xue (2015) obtained improved relation prediction results. In this work, we show that even a simpler approach using automatically collected data to pre-train a model and then human-annotated domain data to fine-tune it yields statistically significant improvements on relation prediction compared to a model trained only on the domain data.\nPredicting the presence of discourse connectives based on linguistic features of the arguments and their discourse relation was studied by Patterson and Kehler (2013).\nThe decomposable attention (DA) model was recently proposed for the natural language inference (NLI) problem which is also closely related to discourse connective prediction (Parikh et al., 2016). DA obtained state-of-the-art performance on the NLI task which is why we chose it as our main approach for discourse connective prediction."
    }, {
      "heading" : "3 Data Collection",
      "text" : "We compile a list of 79 discourse connectives based on the Penn Discourse Treebank (PDTB) (Prasad et al., 2008). Since our focus is on sentence concatenation, we ignore the connectives which typically point to the following sentence (forward) rather than the previous one, such as “After the election, [. . . ]”. However, for several ambiguous connectives, the forward use can be ruled out by requiring a comma after the connective (e.g. Instead,); we include such connectives in our data. Discontinuous connectives, such as “If [. . . ] then [. . . ]”, are not included.\nData samples for discourse connective prediction can be collected from any large unannotated text corpus. In this instance, we use the English Wikipedia2 and collect every pair of consecutive sentences within the same paragraph where the latter sentence begins with one of the 79 discourse connectives. As a result, we obtain a dataset of 1.95 million sentence pairs separated by\n2A snapshot from September 5, 2016.\na connective. Additionally, we collect 0.91 million examples of consecutive sentences not separated by a discourse connective, labeled as [No connective], for a total of 2.86 million sentence pairs.\nThe frequency distribution of the connectives is very skewed; however occurs 720 334 times, whereas else, only 43 times in the beginning of a sentence. In order to make the connective prediction task more feasible for the models and for human raters, we select a subset of sufficiently frequent and distinct connectives (e.g. for example is included but for instance is not since it conveys the same meaning and is less frequent). The resulting 19 connectives are listed in Figure 1.\nFinally, we split the data into train, development, and test sets. For the development and test sets, we pick 500 samples per connective (including [No connective]) by undersampling without replacement. This results in two balanced datasets of 10 000 samples. For the train set, we pick 20 000 samples per connective by under-sampling the majority classes and oversampling the minority classes, creating a balanced dataset of 400 000 samples. Connective samples from a single Wikipedia article are not included in\nmore than one of the three datasets to avoid overfitting through potential repetition within a single article.\nThe collected data along with the used train/development/test splits will be made freely available on publication."
    }, {
      "heading" : "3.1 Comparison with the Penn Discourse Treebank",
      "text" : "The most commonly used dataset for computational discourse studies is the Penn Discourse Treebank (PDTB) (Prasad et al., 2008). This treebank is based on a set of 2 159 Wall Street Journal news articles, whereas our dataset is based on all the articles in the English Wikipedia.\nPDTB contains 18 459 explicit discourse relations where the discourse connective is present and 16 224 implicit discourse relations. The relations have been classified under four major semantic classes by human annotators who have also assigned implicit connectives for the implicit discourse relations. Most discourse prediction studies focus on the implicit discourse relations, splitting the prediction task into four binary problems along the four major classes. PDTB is divided into 22 sections and a common practice is to use Sections 2–20 to train, Sections 0–1 to develop, and Sections 21–22 to test new models.\nThe key characteristics of the two datasets are listed in Table 1. The PDTB dataset is richer in the sense that it contains human annotated discourse relations in addition to discourse connectives. However, the small size of the dataset causes sparsity issues (Li and Nenkova, 2014) which hinder the development of new models, particularly complex neural models that often require large training datasets to generalize well. Moreover, the small test set size makes the comparison of results less reliable.3"
    }, {
      "heading" : "4 Connective prediction models",
      "text" : "The decomposable attention (DA) model was recently introduced by Parikh et al. (2016) for the natural language inference (NLI) problem which aims to classify entailment and contradiction relations between a premise and a hypothesis. Discourse connective prediction is related to the NLI\n3For example, the PDTB test set contains only 55 positive samples of the Temporal relation. Yet, the studies on implicit discourse relation prediction often present the results without any statistical significance tests to measure whether the differences between models are significant.\nH ( )+ +…+=ŷ\nin the\npark alice plays\nso m\neo ne\npl ay\nin g\nm us\nic\nou ts\nid e\nflute a solo G ( , )\nG ( , )\npark outside\nalice someone\nflute+ solo music\n…\nG ( , )=\n=\n= flute music F ( , )\nFigure 2: Overview of the decomposable attention model. Figure reproduced with permission from (Parikh et al., 2016).\nproblem since entailment and contradiction can be explicitly indicated by certain connectives (for instance, therefore and by contrast, respectively). However, the larger number of classes makes connective prediction a more challenging task. DA was shown to yield a state-of-the-art performance on the NLI task while requiring almost one order of magnitude fewer parameters than previous approaches. For all these reasons, it seems natural to apply the DA model to the connective prediction problem.\nMarcu and Echihabi (2002) proposed to use word pairs as the features of a model which predicted discourse relations based on discourse connectives mapped to these relations. Many later implicit discourse relation prediction models are similarly based on word pairs, at least partly. Therefore, we use a model called WORDPAIRS to have a baseline for the DA model."
    }, {
      "heading" : "4.1 The Decomposable Attention Model",
      "text" : "The DA model consists of three steps, attend, compare, and aggregate, which are executed by three different feed-forward neural networks F , G, and H , respectively, as illustrated in Figure 2.\nAs input, the model takes two sentences a and b represented by sequences of word embeddings. The sequences are padded by “NULL” tokens to fix their lengths to 50 tokens.\nIn the attend step (network F ), the model computes non-negative attention scores for each pair of\ntokens across the two input sentences. This computation ignores the order of the tokens and it produces soft-alignments from a to b and vice versa.\nIn the compare step (network G), the model computes comparison vectors between each input token and its aligned sub-phrase. The aligned subphrase is a linear combination of the embedding vectors of the other sentence weighted by the attention scores.\nFinally, in the aggregate step, the comparison vectors are summed over the tokens of a sentence and then the aggregate vectors of the two sentences are concatenated. The resulting vector is fed into H which outputs ŷ containing scores for each class. The predicted class is given by ŷ = argmaxi ŷi.\nThe weights of the three networks are randomly initialized, after which the model is trained in an end-to-end manner. Our implementation of the DA model has the following differences compared to the original model described by Parikh et al. (2016): (i) we do not use the self-attention mechanism which was reported to provide only a small improvement over the vanilla version of DA; (ii) we do not project down the embedding vectors but use 100-dimensional word2vec embeddings (Mikolov et al., 2013) which are updated during the training; (iii) we use layer normalization (Ba et al., 2016) which makes the model converge faster."
    }, {
      "heading" : "4.2 The Word-Pair Baseline",
      "text" : "The WORDPAIRS method considers as features all word pairs which appear across the two arguments (e.g. word A appears in Arg 1 and word B in Arg 2) in at least five samples in the training dataset. Additionally, it incorporates single word features (e.g. word A appears in Arg 2) since these slightly improved the results. With these features, we train logistic regressors using the one-vs-rest scheme to predict one of the 20 different connectives.4"
    }, {
      "heading" : "5 Experiments",
      "text" : "Next we present experimental results on discourse connective prediction using human raters, the DA model and the WORDPAIRS baseline\n4We trained two versions of the WORDPAIRS model: one using stochastic gradient descent with mini-batches and another using LIBLINEAR with 100k samples (i.e. 25% of the training data) which we could fit into the memory of a single 256 GB machine. The results that we report are those obtained with the latter approach, which performed better.\nmodel. For this task, we remove the connective (if any) from the second sentence in each test pair, and measure the ability of the model (or the raters) to identify the removed connective (or [No connective], if the second sentence does not start with one). Additionally, we present results on implicit discourse relation prediction using the collected dataset to pre-train the DA model."
    }, {
      "heading" : "5.1 Accuracy of Human Raters",
      "text" : "A manual inspection of the dataset shows that it is often challenging to infer the connective appearing in the Wikipedia article. In some cases, the relation between the two sentences might be unclear, or there might be multiple suitable options. In other cases, a larger context than the previous sentence is required for inferring the connective. For instance, to correctly decide whether finally, is more suitable than then, one may have to inspect a larger context.\nTo better understand what is reasonable to expect from an automatic predictor, we use a crowdsourcing platform to ask human raters to reconstruct the removed connectives for each of the 10 000 test sentence pairs. Each sentence pair is annotated by three raters who are shown the two sentences, the latter of which starts with a [Connective goes here] placeholder, and asked to select the most suitable connective from the 20 options, including [No connective]. This layer of human annotations is also released as part of the connectives dataset. The raters are instructed to pick the most natural connective in case there are multiple suitable options. Furthermore, they are asked to pick [No connective] only if adding a connective would make the concatenation sound ungrammatical or artificial, or if the two sentences seem to be completely disconnected. The sentences are not pre-processed apart from upper-casing the first character of the second sentence to avoid giving away the presence of a connective in the original sentence. The order of the connectives is randomized, except for [No connective] which is always shown last.\nOn the whole test-set, human annotators achieve a macro-averaged F1 score of 23.72. The confusion matrix generated by the raters’ decisions is presented on the left side of Figure 3. It shows that the raters are strongly biased towards [No connective] despite the indication to refrain from using it. There are at least two possible\nexplanations for this bias: (i) in the sake of clarity and in line with common scientific writing guidelines, Wikipedia editors tend to use connectives quite generously, and (ii) the artificial balancing of the datasets makes [No connective] underrepresented in the test data compared to the actual distribution of discourse connectives vs. [No connective]. Furthermore, the confusion matrix shows that there are clusters of connectives that raters tend to confuse, even though they do not necessarily encode exactly the same relation. Examples are rather, and instead,, for example and in particular,, on the other hand and by contrast,."
    }, {
      "heading" : "5.2 Accuracy of the Models",
      "text" : "In this section, the DA model is employed to perform the same task as the human raters, i.e., learning to reconstruct the connective possibly removed from the beginning of the second sentence in each test pair. A balanced dataset is used for both training and testing the models as described in Section 3. The DA model is evaluated using the following hyper-parameters optimized on the development set: network size (one hidden layer with 200 neurons), batch size (64), dropout ratios for the F , G, and H networks (0.68, 0.14, and 0.44, respectively), and learning rate (0.0018). The model is implemented in TensorFlow (Abadi et al., 2015) and the training is run for 300 000 batch steps. The results, reported in Table 2, show that DA clearly outperforms the WORDPAIRS baseline with an F1 score of 31.80 vs. 14.81."
    }, {
      "heading" : "5.3 Comparison between the Raters and the Decomposable Attention Model",
      "text" : "Table 3 compares the accuracy of DA predictions to the rater decisions. The macro-averaged F1 score of human raters is 23.72 which is, quite remarkably, lower than the F1 score of the DA\nmodel, 31.80. The difference is smaller when considering majority votes on only the 5 714 tasks for which there is a consensus among at least 2 out of 3 raters, which results in a 30.36 F1 score for the raters. On these less ambiguous cases, as expected, the model performance also increases to 32.68.\nAs we mentioned in Section 5.1, human raters are clearly less eager to introduce a connective than Wikipedia editors. Therefore, we also evaluate the setting in which either the ground-truth label, or the rater-assigned majority label, or the model-assigned label is [No connective]. The results, listed in the last line of Table 3, show that under these conditions human raters actually outperform the model, which indicates where\nmost of the headroom for future model improvements lies.\nThe confusion matrix of DA is shown on the right side of Figure 3. For each connective, the true connective is the most frequent prediction. Connective on the other hand has the lowest F1 score (15.06), whereas by then has the highest (57.29). Some of the most frequent mistakes are between similar connectives, such as however vs. nevertheless, and instead, vs. rather,. These errors are by and large consistent with those of human raters (left side of the figure). This confirms that the model is accurately capturing the meaning of the relation, and when it does not select the gold connective it is making similar approximations to what people would do. Furthermore, the Figure 3 shows that raters have a more pronounced tendency to select frequent connectives, such as however and and. To further exemplify, in Table 4 we show a selection of wrong and correct decisions made by DA and human raters."
    }, {
      "heading" : "5.4 Model Interpretation",
      "text" : "An advantage of the DA model is that it is possible to examine which words the model attends to when inferring a connective. In some cases, the attended words are clearly meaningful semantically or linguistically, whereas in other cases the soft-alignment matrix is harder to interpret. Examples of the former case are represented in Fig-\nure 4, which shows the alignment matrices from the tokens of the first sentence (y-axis) to the tokens of the second sentence (x-axis) so that the rows sum to 1. In the first example, the model correctly predicts however as the connective after aligning the word attempt with refuse and not. These word pairs indicate contrast which makes however a likely connective. In the second case, the model aligns the phrase was disqualified with had gone and correctly predicts by then as the connective. The corresponding tenses, i.e., past and past perfect, respectively, are likely clues of the presence of by then."
    }, {
      "heading" : "5.5 Implicit Discourse Relation Prediction",
      "text" : "In this section, we study the usefulness of the connective dataset for the related task of implicit discourse relation prediction, which has been extensively studied in the literature (Marcu and Echihabi, 2002; Pitler et al., 2009; Rutherford and Xue, 2014). These works typically use the PDTB dataset, described in Section 3.1, for training and evaluating the models. Various studies have shown that the performance of relation prediction models can be improved by leveraging discourse connective data (Zhou et al., 2010; Xu et al., 2012; Liu et al., 2016; Rutherford and Xue, 2015) which is also the focus of this experiment.\nModel Fine-Tuning: We adopt a method similar to Rutherford and Xue (2015) and map unambiguous connectives to the corresponding discourse relations (for instance, however is mapped to COMPARISON). We treat a connective as unambiguous when the most frequent relation among the explicit PDTB samples accounts for more than 90% of the samples with this connective. The resulting samples are treated as weakly labeled additional training data. Rutherford and Xue (2015) found out that using the weakly labeled training\ndata indiscriminately in the training phase harms the performance, and hence, they introduce criteria to classify freely-omissible connectives and only use those. The resulting performance improvements are statistically significant.\nWe propose a simpler training scheme where we first pretrain the models with all the weakly labeled samples from Wikipedia and then fine-tune the models with the PDTB training data. Again, we employ the DA model but with different hyperparameters tuned using PDTB development data.\nResults: The results for implicit discourse relation prediction using the DA model trained on PDTB data alone, another DA model pretrained with discourse connective data, and a state-of-theart model by Rutherford and Xue (2015) which also leverages weakly labeled training data are shown in Table 5. We observed large fluctuation in the F1 scores and therefore trained the DA model ten times with different random initialization of the network weights. Standard deviations of the different runs are shown in parentheses and the significance tests are conducted using unpaired two-sample t-test for the two samples with ten F1 scores.\nThe results show that our implementation of the DA model does not perform as well as the method by Rutherford and Xue (2015), which is specifically designed for the task of relation prediction. Nevertheless, pre-training the DA model with discourse connective data is shown to improve the prediction of COMPARISON and TEMPORAL relations, the two least frequent relations in the PDTB corpus. For CONTINGENCY, pretraining the model with weakly labeled data decreases the F1 score, which is in line with the results by Rutherford and Xue (2015)."
    }, {
      "heading" : "6 Conclusions",
      "text" : "We studied the problem of discourse connective prediction, which has many useful applications in text summarization, adaptation and conversationalization. We collected a dataset of 2.9 million pairs of consecutive sentences and connectives, and made it publicly available to facilitate further research on this problem, as well as other related bi-sequence classification tasks. We showed that the recently proposed decomposable attention model performs surprisingly well on the connective prediction task, even better than human raters on the same representative test set consisting of 10 000 samples. We also observed that, unlike the model, human raters have a preference for implicit connectives, as they do outperform the model if the comparison is restricted to the cases in which the majority of raters agrees on an explicit connective. The alignment matrices produced by the model suggest that the predictor is picking up relevant lexical, syntactic and semantic clues. The confusion matrix of the predictor shows very sim-\nilar error patterns to the matrix generated from human raters, further confirming the meaningfulness of the decisions made by the model."
    } ],
    "references" : [ {
      "title" : "Tensorflow: Large-scale machine learning on heterogeneous systems",
      "author" : [ "Martın Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin" ],
      "venue" : null,
      "citeRegEx" : "Abadi et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Abadi et al\\.",
      "year" : 2015
    }, {
      "title" : "Layer normalization",
      "author" : [ "Jimmy Lei Ba", "Jamie Ryan Kiros", "Geoffrey E Hinton." ],
      "venue" : "arXiv preprint arXiv:1607.06450 .",
      "citeRegEx" : "Ba et al\\.,? 2016",
      "shortCiteRegEx" : "Ba et al\\.",
      "year" : 2016
    }, {
      "title" : "An empirical evaluation of various deep learning architectures for bi-sequence classification tasks",
      "author" : [ "Anirban Laha", "Vikas Raykar." ],
      "venue" : "Proc. COLING.",
      "citeRegEx" : "Laha and Raykar.,? 2016",
      "shortCiteRegEx" : "Laha and Raykar.",
      "year" : 2016
    }, {
      "title" : "Reducing sparsity improves the recognition of implicit discourse relations",
      "author" : [ "Junyi Jessy Li", "Ani Nenkova." ],
      "venue" : "Proc. SIGDIAL.",
      "citeRegEx" : "Li and Nenkova.,? 2014",
      "shortCiteRegEx" : "Li and Nenkova.",
      "year" : 2014
    }, {
      "title" : "Recognizing implicit discourse relations via repeated reading: Neural networks with multi-level attention",
      "author" : [ "Yang Liu", "Sujian Li." ],
      "venue" : "Proc. EMNLP.",
      "citeRegEx" : "Liu and Li.,? 2016",
      "shortCiteRegEx" : "Liu and Li.",
      "year" : 2016
    }, {
      "title" : "Implicit discourse relation classification via multi-task neural networks",
      "author" : [ "Yang Liu", "Sujian Li", "Xiaodong Zhang", "Zhifang Sui." ],
      "venue" : "Proc. AAAI.",
      "citeRegEx" : "Liu et al\\.,? 2016",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2016
    }, {
      "title" : "An unsupervised approach to recognizing discourse relations",
      "author" : [ "Daniel Marcu", "Abdessamad Echihabi." ],
      "venue" : "Proc. ACL.",
      "citeRegEx" : "Marcu and Echihabi.,? 2002",
      "shortCiteRegEx" : "Marcu and Echihabi.",
      "year" : 2002
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean." ],
      "venue" : "Proc. NIPS.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "A decomposable attention model for natural language inference",
      "author" : [ "Ankur P Parikh", "Oscar Täckström", "Dipanjan Das", "Jakob Uszkoreit." ],
      "venue" : "Proc. EMNLP.",
      "citeRegEx" : "Parikh et al\\.,? 2016",
      "shortCiteRegEx" : "Parikh et al\\.",
      "year" : 2016
    }, {
      "title" : "Predicting the presence of discourse connectives",
      "author" : [ "Gary Patterson", "Andrew Kehler." ],
      "venue" : "Proc. EMNLP.",
      "citeRegEx" : "Patterson and Kehler.,? 2013",
      "shortCiteRegEx" : "Patterson and Kehler.",
      "year" : 2013
    }, {
      "title" : "Automatic sense prediction for implicit discourse relations in text",
      "author" : [ "Emily Pitler", "Annie Louis", "Ani Nenkova." ],
      "venue" : "Proc. ACL-IJCNLP.",
      "citeRegEx" : "Pitler et al\\.,? 2009",
      "shortCiteRegEx" : "Pitler et al\\.",
      "year" : 2009
    }, {
      "title" : "Easily identifiable discourse relations",
      "author" : [ "Emily Pitler", "Mridhula Raghupathy", "Hena Mehta", "Ani Nenkova", "Alan Lee", "Aravind K Joshi." ],
      "venue" : "Proc. Coling 2008: Companion volume: Posters and Demonstrations.",
      "citeRegEx" : "Pitler et al\\.,? 2008",
      "shortCiteRegEx" : "Pitler et al\\.",
      "year" : 2008
    }, {
      "title" : "The penn discourse treebank",
      "author" : [ "Rashmi Prasad", "Nikhil Dinesh", "Alan Lee", "Eleni Miltsakaki", "Livio Robaldo", "Aravind K Joshi", "Bonnie L Webber" ],
      "venue" : null,
      "citeRegEx" : "Prasad et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Prasad et al\\.",
      "year" : 2008
    }, {
      "title" : "Discovering implicit discourse relations through brown cluster pair representation and coreference patterns",
      "author" : [ "Attapol Rutherford", "Nianwen Xue." ],
      "venue" : "Proc. EACL.",
      "citeRegEx" : "Rutherford and Xue.,? 2014",
      "shortCiteRegEx" : "Rutherford and Xue.",
      "year" : 2014
    }, {
      "title" : "Improving the inference of implicit discourse relations via classifying explicit discourse connectives",
      "author" : [ "Attapol Rutherford", "Nianwen Xue." ],
      "venue" : "Proc. NAACL-HLT .",
      "citeRegEx" : "Rutherford and Xue.,? 2015",
      "shortCiteRegEx" : "Rutherford and Xue.",
      "year" : 2015
    }, {
      "title" : "Using automatically labelled examples to classify rhetorical relations: An assessment",
      "author" : [ "Caroline Sporleder", "Alex Lascarides." ],
      "venue" : "Natural Language Engineering 14(03):369–416.",
      "citeRegEx" : "Sporleder and Lascarides.,? 2008",
      "shortCiteRegEx" : "Sporleder and Lascarides.",
      "year" : 2008
    }, {
      "title" : "Connective prediction using machine learning for implicit discourse relation classification",
      "author" : [ "Yu Xu", "Man Lan", "Yue Lu", "Zheng Yu Niu", "Chew Lim Tan." ],
      "venue" : "Proc. IJCNN.",
      "citeRegEx" : "Xu et al\\.,? 2012",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2012
    }, {
      "title" : "The effects of discourse connectives prediction on implicit discourse relation recognition",
      "author" : [ "Zhi Min Zhou", "Man Lan", "Zheng Yu Niu", "Yu Xu", "Jian Su." ],
      "venue" : "Proc. SIGDIAL.",
      "citeRegEx" : "Zhou et al\\.,? 2010",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "In the literature, discourse connective prediction has been mainly studied as an intermediate step for the well-studied problem of implicit discourse relation prediction (Zhou et al., 2010; Xu et al., 2012).",
      "startOffset" : 170,
      "endOffset" : 206
    }, {
      "referenceID" : 16,
      "context" : "In the literature, discourse connective prediction has been mainly studied as an intermediate step for the well-studied problem of implicit discourse relation prediction (Zhou et al., 2010; Xu et al., 2012).",
      "startOffset" : 170,
      "endOffset" : 206
    }, {
      "referenceID" : 8,
      "context" : "We show that a recently proposed decomposable attention model (Parikh et al., 2016) yields a good experimental performance on the connective prediction task.",
      "startOffset" : 62,
      "endOffset" : 83
    }, {
      "referenceID" : 2,
      "context" : "Both problems can be seen as instances of the broader class of bi-sequence classification tasks (Laha and Raykar, 2016).",
      "startOffset" : 96,
      "endOffset" : 119
    }, {
      "referenceID" : 2,
      "context" : "Both problems can be seen as instances of the broader class of bi-sequence classification tasks (Laha and Raykar, 2016). The currently best performing methods for the relation prediction task are those introduced by Liu and Li (2016), who proposed a bidirectional LSTM with multi-level attention, and by Rutherford and Xue (2014), who employed Brown cluster pairs and co-reference patterns.",
      "startOffset" : 97,
      "endOffset" : 234
    }, {
      "referenceID" : 2,
      "context" : "Both problems can be seen as instances of the broader class of bi-sequence classification tasks (Laha and Raykar, 2016). The currently best performing methods for the relation prediction task are those introduced by Liu and Li (2016), who proposed a bidirectional LSTM with multi-level attention, and by Rutherford and Xue (2014), who employed Brown cluster pairs and co-reference patterns.",
      "startOffset" : 97,
      "endOffset" : 330
    }, {
      "referenceID" : 2,
      "context" : "Both problems can be seen as instances of the broader class of bi-sequence classification tasks (Laha and Raykar, 2016). The currently best performing methods for the relation prediction task are those introduced by Liu and Li (2016), who proposed a bidirectional LSTM with multi-level attention, and by Rutherford and Xue (2014), who employed Brown cluster pairs and co-reference patterns. Pitler et al. (2008) showed that if a discourse connective is known, the (explicit) discourse relation can be inferred with a 93.",
      "startOffset" : 97,
      "endOffset" : 412
    }, {
      "referenceID" : 2,
      "context" : "Both problems can be seen as instances of the broader class of bi-sequence classification tasks (Laha and Raykar, 2016). The currently best performing methods for the relation prediction task are those introduced by Liu and Li (2016), who proposed a bidirectional LSTM with multi-level attention, and by Rutherford and Xue (2014), who employed Brown cluster pairs and co-reference patterns. Pitler et al. (2008) showed that if a discourse connective is known, the (explicit) discourse relation can be inferred with a 93.09% accuracy, which has inspired several efforts at predicting connectives to improve implicit discourse relation prediction. Zhou et al. (2010) predicted connectives using an N -gram language model1, whereas Xu et al.",
      "startOffset" : 97,
      "endOffset" : 665
    }, {
      "referenceID" : 2,
      "context" : "Both problems can be seen as instances of the broader class of bi-sequence classification tasks (Laha and Raykar, 2016). The currently best performing methods for the relation prediction task are those introduced by Liu and Li (2016), who proposed a bidirectional LSTM with multi-level attention, and by Rutherford and Xue (2014), who employed Brown cluster pairs and co-reference patterns. Pitler et al. (2008) showed that if a discourse connective is known, the (explicit) discourse relation can be inferred with a 93.09% accuracy, which has inspired several efforts at predicting connectives to improve implicit discourse relation prediction. Zhou et al. (2010) predicted connectives using an N -gram language model1, whereas Xu et al. (2012) employed word pairs and a selection of linguistically informed features.",
      "startOffset" : 97,
      "endOffset" : 746
    }, {
      "referenceID" : 2,
      "context" : "Both problems can be seen as instances of the broader class of bi-sequence classification tasks (Laha and Raykar, 2016). The currently best performing methods for the relation prediction task are those introduced by Liu and Li (2016), who proposed a bidirectional LSTM with multi-level attention, and by Rutherford and Xue (2014), who employed Brown cluster pairs and co-reference patterns. Pitler et al. (2008) showed that if a discourse connective is known, the (explicit) discourse relation can be inferred with a 93.09% accuracy, which has inspired several efforts at predicting connectives to improve implicit discourse relation prediction. Zhou et al. (2010) predicted connectives using an N -gram language model1, whereas Xu et al. (2012) employed word pairs and a selection of linguistically informed features. Liu et al. (2016) showed that predicting both connectives and relations using a convolutional neural network in a multi-task setting improves the relation prediction performance.",
      "startOffset" : 97,
      "endOffset" : 837
    }, {
      "referenceID" : 8,
      "context" : "The decomposable attention (DA) model was recently proposed for the natural language inference (NLI) problem which is also closely related to discourse connective prediction (Parikh et al., 2016).",
      "startOffset" : 174,
      "endOffset" : 195
    }, {
      "referenceID" : 6,
      "context" : "Already in 2002, Marcu and Echihabi (2002) proposed to map certain unambiguous connectives to discourse relations in order to collect a large discourse relation dataset.",
      "startOffset" : 17,
      "endOffset" : 43
    }, {
      "referenceID" : 6,
      "context" : "Already in 2002, Marcu and Echihabi (2002) proposed to map certain unambiguous connectives to discourse relations in order to collect a large discourse relation dataset. Sporleder and Lascarides (2008), however, showed that models trained using such automatically collected training data do not generalize well.",
      "startOffset" : 17,
      "endOffset" : 202
    }, {
      "referenceID" : 6,
      "context" : "Already in 2002, Marcu and Echihabi (2002) proposed to map certain unambiguous connectives to discourse relations in order to collect a large discourse relation dataset. Sporleder and Lascarides (2008), however, showed that models trained using such automatically collected training data do not generalize well. Nevertheless, by classifying freely-omissible connectives and only using those, Rutherford and Xue (2015) obtained improved relation prediction results.",
      "startOffset" : 17,
      "endOffset" : 418
    }, {
      "referenceID" : 6,
      "context" : "Already in 2002, Marcu and Echihabi (2002) proposed to map certain unambiguous connectives to discourse relations in order to collect a large discourse relation dataset. Sporleder and Lascarides (2008), however, showed that models trained using such automatically collected training data do not generalize well. Nevertheless, by classifying freely-omissible connectives and only using those, Rutherford and Xue (2015) obtained improved relation prediction results. In this work, we show that even a simpler approach using automatically collected data to pre-train a model and then human-annotated domain data to fine-tune it yields statistically significant improvements on relation prediction compared to a model trained only on the domain data. Predicting the presence of discourse connectives based on linguistic features of the arguments and their discourse relation was studied by Patterson and Kehler (2013). The decomposable attention (DA) model was recently proposed for the natural language inference (NLI) problem which is also closely related to discourse connective prediction (Parikh et al.",
      "startOffset" : 17,
      "endOffset" : 914
    }, {
      "referenceID" : 12,
      "context" : "We compile a list of 79 discourse connectives based on the Penn Discourse Treebank (PDTB) (Prasad et al., 2008).",
      "startOffset" : 90,
      "endOffset" : 111
    }, {
      "referenceID" : 12,
      "context" : "The most commonly used dataset for computational discourse studies is the Penn Discourse Treebank (PDTB) (Prasad et al., 2008).",
      "startOffset" : 105,
      "endOffset" : 126
    }, {
      "referenceID" : 3,
      "context" : "However, the small size of the dataset causes sparsity issues (Li and Nenkova, 2014) which hinder the development of new models, particularly complex neural models that often require large training datasets to generalize well.",
      "startOffset" : 62,
      "endOffset" : 84
    }, {
      "referenceID" : 8,
      "context" : "The decomposable attention (DA) model was recently introduced by Parikh et al. (2016) for the natural language inference (NLI) problem which aims to classify entailment and contradiction relations between a premise and a hypothesis.",
      "startOffset" : 65,
      "endOffset" : 86
    }, {
      "referenceID" : 8,
      "context" : "Figure reproduced with permission from (Parikh et al., 2016).",
      "startOffset" : 39,
      "endOffset" : 60
    }, {
      "referenceID" : 6,
      "context" : "Marcu and Echihabi (2002) proposed to use word pairs as the features of a model which predicted discourse relations based on discourse connectives mapped to these relations.",
      "startOffset" : 0,
      "endOffset" : 26
    }, {
      "referenceID" : 7,
      "context" : "(2016): (i) we do not use the self-attention mechanism which was reported to provide only a small improvement over the vanilla version of DA; (ii) we do not project down the embedding vectors but use 100-dimensional word2vec embeddings (Mikolov et al., 2013) which are updated during the training; (iii) we use layer normalization (Ba et al.",
      "startOffset" : 236,
      "endOffset" : 258
    }, {
      "referenceID" : 1,
      "context" : ", 2013) which are updated during the training; (iii) we use layer normalization (Ba et al., 2016) which makes the model converge faster.",
      "startOffset" : 80,
      "endOffset" : 97
    }, {
      "referenceID" : 6,
      "context" : "Our implementation of the DA model has the following differences compared to the original model described by Parikh et al. (2016): (i) we do not use the self-attention mechanism which was reported to provide only a small improvement over the vanilla version of DA; (ii) we do not project down the embedding vectors but use 100-dimensional word2vec embeddings (Mikolov et al.",
      "startOffset" : 109,
      "endOffset" : 130
    }, {
      "referenceID" : 0,
      "context" : "The model is implemented in TensorFlow (Abadi et al., 2015) and the training is run for 300 000 batch steps.",
      "startOffset" : 39,
      "endOffset" : 59
    }, {
      "referenceID" : 6,
      "context" : "In this section, we study the usefulness of the connective dataset for the related task of implicit discourse relation prediction, which has been extensively studied in the literature (Marcu and Echihabi, 2002; Pitler et al., 2009; Rutherford and Xue, 2014).",
      "startOffset" : 184,
      "endOffset" : 257
    }, {
      "referenceID" : 10,
      "context" : "In this section, we study the usefulness of the connective dataset for the related task of implicit discourse relation prediction, which has been extensively studied in the literature (Marcu and Echihabi, 2002; Pitler et al., 2009; Rutherford and Xue, 2014).",
      "startOffset" : 184,
      "endOffset" : 257
    }, {
      "referenceID" : 13,
      "context" : "In this section, we study the usefulness of the connective dataset for the related task of implicit discourse relation prediction, which has been extensively studied in the literature (Marcu and Echihabi, 2002; Pitler et al., 2009; Rutherford and Xue, 2014).",
      "startOffset" : 184,
      "endOffset" : 257
    }, {
      "referenceID" : 17,
      "context" : "Various studies have shown that the performance of relation prediction models can be improved by leveraging discourse connective data (Zhou et al., 2010; Xu et al., 2012; Liu et al., 2016; Rutherford and Xue, 2015) which is also the focus of this experiment.",
      "startOffset" : 134,
      "endOffset" : 214
    }, {
      "referenceID" : 16,
      "context" : "Various studies have shown that the performance of relation prediction models can be improved by leveraging discourse connective data (Zhou et al., 2010; Xu et al., 2012; Liu et al., 2016; Rutherford and Xue, 2015) which is also the focus of this experiment.",
      "startOffset" : 134,
      "endOffset" : 214
    }, {
      "referenceID" : 5,
      "context" : "Various studies have shown that the performance of relation prediction models can be improved by leveraging discourse connective data (Zhou et al., 2010; Xu et al., 2012; Liu et al., 2016; Rutherford and Xue, 2015) which is also the focus of this experiment.",
      "startOffset" : 134,
      "endOffset" : 214
    }, {
      "referenceID" : 14,
      "context" : "Various studies have shown that the performance of relation prediction models can be improved by leveraging discourse connective data (Zhou et al., 2010; Xu et al., 2012; Liu et al., 2016; Rutherford and Xue, 2015) which is also the focus of this experiment.",
      "startOffset" : 134,
      "endOffset" : 214
    }, {
      "referenceID" : 13,
      "context" : "Model Fine-Tuning: We adopt a method similar to Rutherford and Xue (2015) and map unambiguous connectives to the corresponding discourse relations (for instance, however is mapped to COMPARISON).",
      "startOffset" : 48,
      "endOffset" : 74
    }, {
      "referenceID" : 13,
      "context" : "Model Fine-Tuning: We adopt a method similar to Rutherford and Xue (2015) and map unambiguous connectives to the corresponding discourse relations (for instance, however is mapped to COMPARISON). We treat a connective as unambiguous when the most frequent relation among the explicit PDTB samples accounts for more than 90% of the samples with this connective. The resulting samples are treated as weakly labeled additional training data. Rutherford and Xue (2015) found out that using the weakly labeled training data indiscriminately in the training phase harms the performance, and hence, they introduce criteria to classify freely-omissible connectives and only use those.",
      "startOffset" : 48,
      "endOffset" : 465
    }, {
      "referenceID" : 13,
      "context" : "Results: The results for implicit discourse relation prediction using the DA model trained on PDTB data alone, another DA model pretrained with discourse connective data, and a state-of-theart model by Rutherford and Xue (2015) which also leverages weakly labeled training data are shown in Table 5.",
      "startOffset" : 202,
      "endOffset" : 228
    }, {
      "referenceID" : 13,
      "context" : "The results show that our implementation of the DA model does not perform as well as the method by Rutherford and Xue (2015), which is specifically designed for the task of relation prediction.",
      "startOffset" : 99,
      "endOffset" : 125
    }, {
      "referenceID" : 13,
      "context" : "The results show that our implementation of the DA model does not perform as well as the method by Rutherford and Xue (2015), which is specifically designed for the task of relation prediction. Nevertheless, pre-training the DA model with discourse connective data is shown to improve the prediction of COMPARISON and TEMPORAL relations, the two least frequent relations in the PDTB corpus. For CONTINGENCY, pretraining the model with weakly labeled data decreases the F1 score, which is in line with the results by Rutherford and Xue (2015).",
      "startOffset" : 99,
      "endOffset" : 542
    }, {
      "referenceID" : 13,
      "context" : "Table 5: Implicit discourse relation prediction results using the decomposable attention (DA) model with and without pre-training along with results from Rutherford and Xue (2015). Average F1 scores (and standard deviations) over ten random initializations of the DA model parameters are reported.",
      "startOffset" : 154,
      "endOffset" : 180
    } ],
    "year" : 2017,
    "abstractText" : "Accurate prediction of suitable discourse connectives (however, furthermore, etc.) is a key component of any system aimed at building coherent and fluent discourses from shorter sentences and passages. As an example, a dialog system might assemble a long and informative answer by sampling passages extracted from different documents retrieved from the web. We formulate the task of discourse connective prediction and release a dataset of 2.9M sentence pairs separated by discourse connectives for this task. Then, we evaluate the hardness of the task for human raters, apply a recently proposed decomposable attention (DA) model to this task and observe that the automatic predictor has a higher F1 than human raters (32 vs. 30). Nevertheless, under specific conditions the raters still outperform the DA model, suggesting that there is headroom for future improvements. Finally, we further demonstrate the usefulness of the connectives dataset by showing that it improves implicit discourse relation prediction when used for model pre-training.",
    "creator" : "LaTeX with hyperref package"
  }
}