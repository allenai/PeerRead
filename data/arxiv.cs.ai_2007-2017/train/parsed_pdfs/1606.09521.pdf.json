{
  "name" : "1606.09521.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Probabilistic Reasoning in the Description Logic ALCP with the Principle of Maximum Entropy",
    "authors" : [ "Rafael Peñaloza", "Nico Potyka" ],
    "emails" : [ "rafael.penaloza@unibz.it", "npotyka@uni-osnabrueck.de" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 6.\n09 52\n1v 1\n[ cs\n.A I]\n3 0\nJu n"
    }, {
      "heading" : "1 Introduction",
      "text" : "A fundamental element of any intelligent application is storing and manipulating the knowledge from the application domain. Logic-based knowledge representation languages such as description logics (DLs) [1] provide a clear syntax and unambiguous semantics that guarantee the correctness of the results obtained. However, languages based on classical logic are ill-suited for handling the uncertainty inherent to many application domains. To overcome this limitation, various probabilistic logics have been investigated during the last three decades (e.g., [3, 15, 20]). In particular, several probabilistic DLs have been developed [18, 19]. To handle probabilistic knowledge, many approaches require a complete definition of joint probability distributions (JPD) [5, 6, 8, 16, 25]. One approach to avoid a full JPD specification was proposed by Paris [22]: the user gives a partial specification through a set of probabilistic constraints and the partial knowledge is completed by means of the principle of maximum entropy.\nIn this paper we consider a new probabilistic extension of description logics based on the principle of maximum entropy. In our approach we group different axioms from a knowledge base together into so-called contexts, which are identified by a propositional formula. Intuitively, each context corresponds to a possible situation, in which the associated sub-KB is guaranteed to hold. Uncertainty is associated to the contexts through a set of probabilistic constraints, which are interpreted under the principle of maximum entropy.\nTo facilitate the understanding of our approach, we focus on the DL ALC [26] as a prototypical example of a knowledge representation language, and propositional probabilistic constraints as the framework for expressing uncertainty.\nAs reasoning service we consider subsumption relations between concepts given some partial knowledge of the current context. Since the knowledge in a knowledge base is typically incomplete, one cannot expect to obtain a precise probability for a given consequence. Instead, we compute a belief interval that describes all the probability degrees that can be associated to the consequence without contradiction. The lowest bound of the interval corresponds to a sceptical view, considering only the most fundamental models of the knowledge base. The upper bound, in contrast, reflects the credulous belief in which every context that is not explicitly removed is considered. In the worst-case, we get the trivial interval [0, 1], in the best case, we get a point probability where the upper and lower bounds coincide. In some applications, it might be reasonable to consider only one of these bounds. For instance, if the probability interval that a treatment will cause heavy complications is [0.01, 0.05], we might want to use the upper bound 0.05. In contrast, when the probability interval that a treatment will be successful is [0.7, 0.9], we might be more interested in the lower bound 0.7.\nThe main contributions of this paper are the following:\n– we define the new probabilistic description logic ALCP that allows for a flexible description of axiomatic dependencies, and its reasoning problems (Section 3); – we explain in detail how degrees of belief for the subsumption problem can be computed (Section 4); and – we show that ALCP satisfies several desirable properties of probabilistic logics (Section 5)."
    }, {
      "heading" : "2 Maximum Entropy",
      "text" : "We start by recalling the basic notions of probabilistic propositional logic and the principle of maximum entropy.\nLet L be a propositional language constructed over a finite signature sig(L), i.e., a set of propositional variables, in the usual way. An L-interpretation v is a truth assignment of the propositional variables in sig(L). Int(L) denotes the set of all L-interpretations. Satisfaction of a formula φ ∈ L by an L-interpretation v ∈ Int(L) (denoted v |= φ) is defined as usual. A probability distribution over L is a function P : Int(L) → [0, 1] where ∑\nv∈Int(L) P (v) = 1. Probability distributions\nare extended to arbitrary L-formulas φ by setting P (φ) = ∑\nv|=φ P (v).\nDefinition 1 (probabilistic constraints, models). Given the propositional language L, a probabilistic constraint (over L) is an expression of the form\nc0 +\nk ∑\ni=1\nci · p(φi) ≥ 0 (1)\nwhere c0, ci ∈ R, and φi ∈ L, 1 ≤ i ≤ k. A probability distribution P over L is a model of the probabilistic constraint c0 + ∑k i=1 ci · p(φi) ≥ 0 if and only if c0 + ∑k i=1 ci ·P (φi) ≥ 0. The distribution P is a model of the set of probabilistic\ncontraints R (P |= R) iff it satisfies all the constraints in R. The set of all models of R is denoted by Mod(R). If Mod(R) 6= ∅, we say that R is consistent.\nOur probabilistic constraints can express the most common types of constraints considered in the literature of probabilistic logics. For instance, probabilistic conditionals (ψ | φ)[ℓ, u] are satisfied iff ℓ · P (φ) ≤ P (ψ ∧ φ) ≤ u · P (φ) [17]. That is, the conditional is satisfied iff the conditional probability of ψ given φ is between ℓ and u whenever P (φ) > 0. Sometimes P (φ) > 0 is demanded, but strict inequalities are computationally difficult and the semantical differences are negligible in many cases, see [24] for a thorough discussion. These conditions can be expressed in the form (1) as follows\np(ψ ∧ φ)− ℓ · p(φ) ≥ 0, and\nu · p(φ)− p(ψ ∧ φ) ≥ 0.\nProbabilistic constraints can also express more complex restrictions; for example, we can state that the probability that a bird cannot fly is at most one fourth of the probability that a bird flies through the constraint\n1 4 p(bird ∧ flies)− p(bird ∧ ¬flies) ≥ 0. (2)\nTo improve readability, we will often rewrite constraints in a more compact manner, using conditionals as in the first example, or e.g. rewriting (2) as 1 4p(bird ∧ flies) ≥ p(bird ∧ ¬flies).\nIn general, consistent sets of probabilistic constraints have infinitely many models, and there is no obvious way to distinguish between them. One wellstudied approach for dealing with this diversity is to focus on the model that maximizes the entropy\nH(P ) = − ∑\nv∈Int(L)\nP (v) · logP (v).\nFrom an information-theoretic point of view, the maximum entropy (ME) distribution can be regarded as the most conservative one in the sense that it minimizes the information-theoretic distance (that is, the KL-divergence) to the uniform distribution among all probability distributions that satisfy our constraints. In particular, if there are no restrictions on the probability distributions considered, then the uniform distribution is the ME distribution, see, e.g., [27] for a more detailed discussion of these issues. A complete characterization of maximum entropy for the purpose of uncertain reasoning can be found in [22].\nDefinition 2 (ME-model). Let R be a consistent set of probabilistic constraints. The ME-model PMER of R is the unique solution of the maximization problem argmaxP |=RH(P ).\nExistence and uniqueness of PMER follows from the fact that H is strictly concave and continuous, and that the probability distributions that satisfy R form a\ncompact and convex set. PMER is usually computed by deriving an unconstrained optimization problem by means of the Karush-Kuhn-Tucker conditions. The resulting problem can be solved, for instance, by (quasi-)Newton methods with cost |Int(L)|3, see, e.g., [21] for more details on these techniques.\n3 The Probabilistic Description Logic ALCP\nALCP is a probabilistic extension of the classical description logic ALC capable of expressing complex logical and probabilistic relations. As with classical DLs, the main building blocks in ALCP are concepts. Syntactically, ALCP concepts are constructed exactly as ALC concepts. Given two disjoint sets NC of concept names and NR of role names, ALCP concepts are built using the grammar rule C ::= A | ¬C | C ⊓ C | ∃r.C, where A ∈ NC and r ∈ NR. Note that we can derive disjunction, universal quantification and subsumption from these rules by using logical equivalences like C1 ⊔ C2 ≡ ¬(¬C1 ⊓ ¬C2). The knowledge of the application domain is expressed through a finite set of axioms that restrict the way the different concepts and roles may be interpreted. To express both probabilistic and logical relationships, each axiom is annotated with a formula from L that intuitively expresses the context in which this axiom holds.\nDefinition 3 (KB). An L-restricted general concept inclusion (L-GCI) is of the form 〈C ⊑ D : κ〉 where C,D are ALCP concepts and κ is an L-formula. An L-TBox is a finite set of L-GCIs. An ALCP knowledge base (KB) over L is a pair K = (R, T ) where R is a set of probabilistic constraints and T is an L-TBox.\nExample 4. Consider an application modeling beliefs about bacterial and viral infections using the concept names strep (streptococcal infection), bac (bacterial infection), vir (viral infection), inf (infection), and ab (antibiotic); and the role names sf (suffers from), and suc (successful treatment); and the propositional variables res (antibiotic resistance), and h (heavy use of antibiotics by patient). Define the L-TBox Texa containing the L-GCIs\n〈∃sf.bac ⊑ ∃suc.ab : ¬res∧¬h〉, 〈∃sf.vir ⊑ ¬∃suc.ab : ⊤〉 , 〈strep ⊑ bac : ⊤〉 ,\n〈∃sf.bac ⊑ ¬∃suc.ab : res〉, 〈bac ⊑ inf : ⊤〉, 〈vir ⊑ inf : ⊤〉 ,\nwhere ⊤ is any L-tautology. For example, the first L-GCI states that a bacterial infection can be treated successfully with antibiotics if no antibiotic resistance is present and there was no heavy use of antibiotics; the second one states that viral infections can never be treated with antibiotics successfully. Consider additionally the set R containing the probabilistic constraints containing\n(res)[0.05], (res | h)[0.8].\nThat is, the probability of an antibiotic resistance is 5% if no further information is given. However, if the patient used antibiotics heavily, the probability increases to 80%.\nNotice that the probabilistic constraints, and hence the representation of the uncertainty in the knowledge, refer only to the propositional formulas that label the L-GCIs. In ALCP, the uncertainty of the knowledge is handled through these propositional formulas as explained next.\nA possible world interprets both the axiom language (i.e., the concept and role names) and the context language (the propositional variables). Intuitively, it describes a possible context (L-interpretation) together with the relationships between concepts in that situation (ALC-interpretation).\nDefinition 5 (possible world). A possible world is a triple I = (∆I , ·I , vI) where ∆I is a non-empty set (called the domain), vI is an L-interpretation, and ·I is an interpretation function that maps every concept name A to a set AI ⊆ ∆I and every role name r to a binary relation rI ⊆ ∆I ×∆I.\nThe interpretation function ·I is extended to complex concepts as usual in DLs by letting (¬C)I := ∆I \\CI ; (∃r.C)I := {d ∈ ∆I | ∃e ∈ ∆I .(d, e) ∈ rI , e ∈ CI}; and (C ⊓D)I := CI ∩DI . A possible world is a model of an L-GCI iff it satisfies the description logic constraint of the axiom whenever it satisfies the context.\nDefinition 6 (model of TBox). The possible world I = (∆I , ·I , vI) is a model of the L-GCI 〈C ⊑ D : κ〉, denoted as I |= 〈C ⊑ D : κ〉, iff (i) vI 6|= κ, or (ii) CI ⊆ DI . It is a model of the L-TBox T iff it is a model of all the L-GCIs in T .\nThe classical DL ALC is a special case of ALCP where all the axioms are annotated with an L-tautology ⊤. To preserve the syntax of classical DLs, we denote such L-GCIs as C ⊑ D instead of 〈C ⊑ D : ⊤〉. In this case, the condition (i) from Definition 6 cannot be satisfied, and hence a model is required to satisfy CI ⊆ DI for all L-GCIs C ⊑ D in the TBox. For a deeper introduction to classical ALC, see [1].\nAccording to our semantics, we only demand that the L-GCIs are satisfied in some specific contexts. Thus, it is often useful to focus on the classical ALC TBox that contains the knowledge that holds in a particular situation. For a KB K = (R, T ) and v ∈ Int(L), the v-restricted TBox is the ALC TBox\nTv := {C ⊑ D | 〈C ⊑ D : κ〉 ∈ T , v |= κ}.\nThe possible world I satisfies Tv (I |= Tv) if for all L-GCIs C ⊑ D ∈ Tv it holds that CI ⊆ DI . In the following, we will often consider subsumption and strong non-subsumption between concepts w.r.t. a restricted TBox. We say that C is subsumed by D w.r.t. Tv (Tv |= C ⊑ D) if for every I |= Tv it holds that CI ⊆ DI . Dually, C is strongly non-subsumed byD w.r.t. Tv (Tv |= C 6 6⊑ D) if for every I |= Tv, CI 6⊆ DI holds. Notice that strong non-subsumption requires that the inclusion between axioms does not hold in any possible world satisfying Tv. Hence, this condition is more strict than just negating the subsumption relation.\nWe now describe how the probabilistic constraints are handled in our logic. An ALCP-interpretation consists of a finite set of possible worlds and a probability function over these worlds.\nDefinition 7 (ALCP-interpretation). An ALCP-interpretation is a pair of the form P = (I, PI), where I is a non-empty, finite set of possible worlds and PI is a probability distribution over I.\nEach ALCP-interpretation induces a probability distribution over L. The probability of a context can be obtained by adding the probabilities of all possible worlds in which this context holds.\nDefinition 8 (distribution induced by P). Let P = (I, PI) be an ALCP-interpretation. The probability distribution PP : Int(L) → [0, 1] induced by P is defined by PP(v) := ∑\nI∈I|v PI(I), where I|v = {(∆I , ·I , vI) ∈ I | vI = v}.\nAs usual, reasoning is restricted to interpretations that satisfy the restrictions imposed by the knowledge base. In our case, we have to demand that the interpretation is consistent with both the classical and the probabilistic part of our knowledge base. That is, we consider only those possible worlds that satisfy both the terminological knowledge (T ) and the probabilistic constraints (R).\nDefinition 9 (model). Let P = (I, PI) be an ALCP-interpretation. P is consistent with the TBox T if every I ∈ I is a model of T . P is consistent with the set of probabilistic constraints R iff PP |= R. The ALCP-interpretation P is a model of the KB K = (R, T ) iff it is consistent with both T and R. As usual, a KB is consistent iff it has a model.\nNotice that ALCP-KBs can express both, logical and probabilistic dependencies between axioms. For instance, two L-GCIs 〈C1 ⊑ D1 : κ1〉 and 〈C2 ⊑ D2 : κ2〉 where κ1 ⇒ κ2 express that whenever the first L-GCI is satisfied, the second one must also hold. Similarly, the probabilistic dependencies between axioms are expressed via the probabilistic constraints of the labeling formulas.\nWe are interested in computing degrees of belief for subsumption relations between concepts. We define the conditional probability of a subsumption relation given a context with respect to a given ALCP-interpretation following the usual notions of conditioning.\nDefinition 10 (probability of subsumption). Let C,D be concepts, κ a context and P an ALCP-interpretation. The conditional probability of C ⊑ D given κ with respect to P is\nPrP(C ⊑ D | κ) :=\n∑\nI∈I,I|=κ,I|=C⊑DPI(I) ∑\nI∈I,I|=κPI(I) . (3)\nNotice that the denominator in (3) can be rewritten as ∑\nI∈I,I|=κ\nPI(I) = ∑\nv|=κ\n∑\nI∈I|v\nPI(I) = ∑\nv|=κ\nPP(v) = PP(κ).\nAs usual, the conditional probability is only well-defined when PP(κ) > 0. Recall that the set of probabilistic constraints R may be satisfied by an infinite class of probability distributions. In the spirit of maximum entropy reasoning, we consider only the most conservative ones in the sense that they induce the ME-model PMER of R.\nDefinition 11 (ME-ALCP-model). An ALCP-model P of K is called an ME-ALCP-model of K iff PP = PMER . The set of all ME-ALCP-models of K is denoted by ModME(K). K is called ME-consistent iff ModME(K) 6= ∅.\nNote that ME-consistency is a strictly stronger notion of consistency. ME-consistent knowledge bases are always consistent, but the converse does not necessarily hold if the classical TBox obtained from T by restricting to a context is inconsistent as we show in the following example.\nExample 12. Let sig(L) = {x} and K = (R, T ) be the KB with R = ∅ and T = {〈A ⊔ ¬A ⊑ A ⊓ ¬A : x〉}. Since A ⊔ ¬A ⊑ A ⊓ ¬A is contradictorial, each ALCP-model of K must satisfy ¬x. There certainly are such models, but in each such model P , PP(x) = 0. However, since R = ∅, we have PMER (x) = 0.5 and hence K has no ME-model.\nME-inconsistency rules out some undesired cases in which the whole knowledge base is consistent, but the TBox restricted to some context is inconsistent. The following theorem gives a simple characterization of ME-consistency: to verify ME-consistency of a KB, it suffices to check consistency of the TBoxes induced by the L-interpretations that have positive probability with respect to PMER . By the properties of the ME distribution, these are the interpretations that are not explicitly restricted to have zero probability through R.\nTheorem 13. The KB K = (R, T ) is ME-consistent iff for every v ∈ Int(L) such that PMER (v) > 0, Tv is consistent.\nFor the rest of this paper we consider only ME-consistent KBs. Hence, whenever we speak of a KB K, we implicitly assume that K has at least one ME-model.\nWe are interested in computing the probability of a subsumption relation w.r.t. a given KB K. Notice that, although we consider only one probability distribution PMER , there can still exist many different ME-models of K, which yield different probabilities for the same subsumption relation. One way to handle this is to consider the smallest and largest probabilities that can be consistently associated to this relation. We call them the sceptical and the creduluos degrees of belief, respectively.\nDefinition 14 (degree of belief). Let C,D be ALCP concepts, κ a context, and K = (R, T ) an ALCP KB. The sceptical degree of belief of C ⊑ D given κ w.r.t. K is\nBsK(C ⊑ D | κ) := inf P∈ModME(K) PrP(C ⊑ D | κ).\nThe credulous degree of belief of C ⊑ D given κ w.r.t. K is\nBcK(C ⊑ D | κ) := sup P∈ModME(K) PrP(C ⊑ D | κ).\nExample 15. Consider Kexa from Example 4. If we ask for the degrees of belief that a patient who suffers from an infection can be successfully treated with\nantibiotics, we obtain\nBsKexa(∃sf.inf ⊑ ∃suc.ab | ⊤) = 0, BcKexa(∃sf.inf ⊑ ∃suc.ab | ⊤) = 1.\nThese bounds are not very informative, but they are perfectly justified by our knowledge base since we do not know anything about the effectiveness of antibiotics with respect to infections in general. However, for a patient who suffers from a streptococcal infection we get\nBsKexa(∃sf.strep ⊑ ∃suc.ab | ⊤) = 0.9405, BcKexa(∃sf.strep ⊑ ∃suc.ab | ⊤) = 0.95.\nIf we know that this patient used antibiotics heavily in the past, then there is nothing in our knowledge base that guarantees the existence of a successful treatment. Hence, the degrees of belief become\nBsKexa(∃sf.strep ⊑ ∃suc.ab | h) = 0 BcKexa(∃sf.strep ⊑ ∃suc.ab | h) = 0.2.\nOur definition of the sceptical degree of belief raises a philosophical question: should there be no difference between the degree of belief 0 and an infinitely small degree of belief? A dual question arises for the credulous degree of belief and the probability 1. However, as we show in the next section, the sceptical and credulous degrees of belief actually correspond to minimum and maximum rather than to infimum and supremum (see Corollary 30) so that these questions become vacuous. From the following theorem we can conclude that every intermediate degree can also be obtained by some model of the KB.\nTheorem 16 (Intermediate Value Theorem). Let p1 < p2 and P1 and P2 be two ME-ALCP-models of the KB K = (R, T ) such that PrP1(C ⊑ D | κ) = p1 and PrP2(C ⊑ D | κ) = p2. Then for each p between p1 and p2 there exists an ME-ALCP-model P of K such that PrP(C ⊑ D | κ) = p\nAs we will show in Corollary 30, both the sceptical degree BsK(C ⊑ D | κ) and the credulous degree BcK(C ⊑ D | κ) are in fact witnessed by some ME-models. Therefore it is meaningful to consider the whole interval of beliefs between BsK(C ⊑ D | κ) and B c K(C ⊑ D | κ).\nDefinition 17 (belief interval). Let C,D be ALCP concepts, κ ∈ L a context and K = (R, T ) a ALCP KB. The belief interval for C ⊑ D w.r.t. K given κ is\nBK(C ⊑ D | κ) := [B s K(C ⊑ D | κ),B c K(C ⊑ D | κ)]."
    }, {
      "heading" : "4 Computing Beliefs",
      "text" : "In this section we show how to compute the belief interval. The first theorem states that the sceptical degreef of belief for a subsumption relation can be computed by adding the probabilities of those L-interpretations w that entail this subsumption in the corresponding restricted TBox Tw.\nTheorem 18. Let K = (R, T ) be a KB, C,D two concepts, and κ a context such that PMER (κ) > 0. Then\nBsK(C ⊑ D | κ) =\n∑\nw∈Int(L),Tw|=C⊑D,w|=κ PMER (w)\nPMER (κ) .\nDually, the credulous degree of belief for a subsumption relation can be computed by removing all the situations in which this relation cannot possibly hold.\nTheorem 19. Let K = (R, T ) be a KB, C,D two concepts, and κ a context with PMER (κ) > 0. Then\nBcK(C ⊑ D | κ) = 1−\n∑\nw∈Int(L),Tw|=C 6 6⊑D,w|=κ PMER (w)\nPMER (κ) .\nTo prove these theorems, one can build two models of the KB K, P and Q such that PrP(C ⊑ D | κ) and PrQ(C ⊑ D | κ) are those degrees expressed by Theorems 18 and 19, respectively. As a byproduct of these proofs, we obtain that the infimum and supremum that define the sceptical and the credulous degrees of belief actually correspond to minimum and maximum taken by some ME-models, yielding the following corollary.\nCorollary 20. Let K be an ALCP KB, C,D be two concepts, and κ be a context. There exist two ME-models P,Q of K with BsK(C ⊑ D | κ) = PrP (C ⊑ D | κ) and BcK(C ⊑ D | κ) = PrQ(C ⊑ D | κ).\nThe direct consequence of Theorems 18 and 19 is that if we want to compute the belief interval for C ⊑ D given some context, it suffices to identify all L-interpretations whose induced (classical) TBoxes entail the subsumption relation C ⊑ D (for the sceptical belief) or the strong non-subsumption C 6 6⊑ D (for credulous belief). Recall that every set of propositional interpretations can be represented by a propositional formula. This motivates the following definition.\nDefinition 21 (consequence formula). An L-formula φ is a consequence formula for C ⊑ D (respectively C 6 6⊑ D) w.r.t. the L-TBox T if for every w ∈ Int(L) it holds that w |= φ iff Tw |= C ⊑ D (respectively Tw |= C 6 6⊑ D).\nIf we are able to compute these consequence formulas, then the computation of the belief interval can be reduced to the evaluation of the probability of these formulas w.r.t. the ME-distribution satisfying R.\nTheorem 22. Let K = (R, T ) be an ALCP KB, φ and ψ be consequence formulas for C ⊑ D and C 6 6⊑ D w.r.t. T , respectively, and κ a context. Then BsK(C ⊑ D | κ) = P ME R (φ | κ) and B c K(C ⊑ D | κ) = 1− P ME R (ψ | κ).\nAlgorithm 1 Computing degrees of belief\nInput: KB K = (R, T ), concepts C,D, context κ Output: Belief degrees (\nBsK(C ⊑ D|κ), B c K(C ⊑ D|κ)\n)\nℓs ← 0; ℓc ← 0 for all v ∈ Int(L) do\nif v |= κ then if Tv |= C ⊑ D then\nℓs ← ℓs + P ME\nR (v) else if Tv |= C 6 6⊑ D then\nℓc ← ℓc + P ME\nR (v) return ( ℓs/P ME R (κ), 1− ℓc/P ME R (κ) )\nExample 23. In our running example, one can see that a consequence formula for ∃sf.strep ⊑ ∃suc.ab is ¬res ∧ ¬h. Indeed, in order to deduce this consequence it is necessary to satisfy the first axiom of Texa, which is only guaranteed in the context ¬res∧¬h. Similarly, res is a consequence formula for ∃sf.strep 6 6⊑ ∃suc.ab. Knowing both the consequence formulas and the ME-model, we can deduce\nBsKexa(∃sf.strep ⊑ ∃suc.ab | ⊤) = P ME R (¬res ∧ ¬h) = 0.9405\nand BcKexa(∃sf.strep ⊑ ∃suc.ab | h) = 1− P ME R (res | h) = 0.2.\nIn particular, Theorem 22 implies that the belief interval can be computed in two phases. The first phase uses purely logical reasoning to compute the consequence formulas, while the second phase applies probabilistic inferences to compute the degrees of belief from these formulas. We now briefly explain how the consequence formulas can be computed.\nNotice first that subsumption and non-subsumption are monotonic consequences in the sense of [2]; that is, if an ALC TBox T entails the subsumption C ⊑ D, then every superset of T also entails this consequence. Similarly, adding more axioms to a TBox entailing C 6 6⊑ D does not remove this entailment. Moreover, the set of all L-formulas (modulo logical equivalence) forms a distributive lattice ordered by generality, in which L-interpretations are all the join prime elements. Thus, the consequence formulas from Definition 21 are in fact the socalled boundaries from [2]. Hence, they can be computed using any of the known boundary computation approaches.\nAssuming that the number of contexts is small in comparison to the size of the TBox, it is better to compute the degrees of belief through a more direct approach following Theorems 18 and 19. In order to compute BsK(C ⊑ D | κ) and BcK(C ⊑ D | κ), it suffices to enumerate all interpretations v ∈ Int(L) and check whether Tv |= C ⊑ D or Tv |= C 6 6⊑ D, and v |= κ, or not (see Algorithm 1). This approach requires 2|sig(L)| calls to a standard ALC reasoner, and each of these calls runs in exponential time on |T | [9]. Notice that this algorithm has an anytime behaviour: it is possible to stop its execution at any moment and obtain an approximation of the belief interval. Moreover, the longer the algorithm runs,\nthe better this approximation becomes. Thus, this method is adequate for a system where finding good approximations efficiently may be more important than computing the precise answers."
    }, {
      "heading" : "5 Properties",
      "text" : "We now investigate some properties of probabilistic logics [22]. First we show that ALCP is language and representation invariant. Invariance is meant with respect to logical objects. Language invariance means that just extending the language without changing the knowledge base should not affect reasoning results. Representation invariance means that equivalent knowledge bases should yield equal inference results. Notice that different notions of representation dependence exist in the literature. For instance, in [11] a very different notion is considered, where the language and the knowledge base are changed simultaneously. This case is not covered by our notion of representation invariance. ALCP also satisfies an independence property; i.e., reasoning results about a part of the language are not changed, when we add knowledge about an independent part of the language. Finally, ALCP is continuous in the sense that minor changes in the probabilistic knowledge expressed by a knowledge base cannot induce major changes in the reasoning results.\nTheorem 24 (Representation invariance). Let Ki = (Ri, Ti), i ∈ {1, 2}, be two KBs such that Mod(R1) = Mod(R2) and Mod(T1) = Mod(T2). Then for all concepts C,D and contexts κ ∈ L, BK1(C ⊑ D | κ) = BK2(C ⊑ D | κ).\nALCP is not only representation invariant, but also language invariant. This property is of computational interest, in particular in combination with independence, that we investigate subsequently. To illustrate this, suppose that we added knowledge about bone fractures in our medical example, which is independent of the knowledge about infections. Independence guarantees that we can ignore the knowledge about infections when answering queries about bone fractures. In this way, we can decrease the size of the knowledge base. Language invariance guarantees that we can also ignore the concepts, relations and propositional variables related to the infection domain. Thus, we can decrease the size of the language. Exploiting both properties, the size of the computational problems can sometimes be decreased significantly.\nTheorem 25 (Language Invariance). Let K1,K2 be KBs over L1,N1C,N 1 R and L2,N2C,N 2 R, respectively. If K1 = K2, L 1 ⊆ L2,N1C ⊆ N 2 C and N 1 R ⊆ N 2 R, then for all concepts C,D ∈ N1C and contexts κ ∈ L 1, it holds that\nBK1(C ⊑ D | κ) = BK2(C ⊑ D | κ).\nFor an L-TBox T , we define the signature of T to be the set sig(T ) of all concept names and role names appearing in T . Likewise, sig(R) is the set of all propositional variables appearing in R. The signature of a KB K = (R, T ) is sig(K) := sig(R) ∪ sig(T ).\nTheorem 26 (Independence). Let K1,K2 be s.t. sig(K1)∩ sig(K2) = ∅, C,D be two concepts, and κ a context where (sig(C) ∪ sig(D) ∪ sig(κ)) ∩ sig(K2) = ∅. Then B(C ⊑K1 D | κ) = B(C ⊑K1∪K2 D | κ).\nThe last property we consider is continuity. One important practical feature of continuous probabilistic logics is that they guarantee a numerically stable behaviour. That is, minor rounding errors due to floating-point arithmetic will not result in major errors in the computed probabilities. As demonstrated by Paris in [22], measuring the difference between probabilistic knowledge bases is subtle and is best addressed by comparing knowledge bases extensionally; i.e., with respect to their model sets. To this end, Paris considered the Blaschke metric. Formally, the Blaschke distance ‖S1, S2‖B between two convex sets S1, S2 is defined by\ninf{δ ∈ R | ∀P1 ∈ S1∃P2 ∈ S2 : ‖P1, P2‖2 ≤ δ and\n∀P2 ∈ S2∃P1 ∈ S1 : ‖P2, P1‖2 ≤ δ}\nIntuitively, ‖S1, S2‖B is the smallest real number d such that for each distribution in one of the sets, there is a probability distribution in the other that has distance at most d to the former. We say that a sequence of knowledge bases (Ki) converges to a knowledge base K iff the classical part of each Ki is equivalent to the classical part of K and the probabilistic part converges to the probabilistic part of K. Our reasoning approach behaves indeed continuously with respect to this metric.\nTheorem 27 (Continuity). Let (Ki) be a convergent sequence of KBs with limit K and BKi(C ⊑ D | κ) = [ℓi, ui]. If BK(C ⊑ D | κ) = [ℓ, u], then (li) converges to ℓ and (ui) converges to u (with respect to the usual topology on R)."
    }, {
      "heading" : "6 Related Work",
      "text" : "Relational probabilistic logical approaches can be roughly divided into those that consider probability distributions over the domain, those that consider probability distributions over possible worlds and those that combine both ideas [10]. Our framework belongs to the second group. Maximum entropy reasoning in propositional probabilistic logics has been discussed extensively, e.g., in [13,22], and various extensions to first-order languages have been considered in recent years [3, 4, 14, 15]. In these works, the domain is restricted to a finite number of constants or bounded in the limit. We circumvent the need to do so by combining a classical first-order logic with unbounded domain with a probabilistic logic with fixed domain.\nMany probabilistic DLs have also been considered in the last decades [16, 18, 19]. Our approach is closest to Bayesian DLs [5, 6] and disponte [25]. The greatest difference with the former lies in the fact that ALCP KBs do not require a complete specification of the probability distribution, but only a set of probabilistic constraints. Moreover, the previous formalisms consider only the\nsceptical degree of belief, while we are interested in the full belief interval. In contrast to disponte, ALCP is capable of expressing both, logical and probabilistic dependencies between the axioms in a KB; in addition, disponte requires all uncertainty degrees to be assigned as mutually independent point probabilities, while ALCP allows for a more flexible specification."
    }, {
      "heading" : "7 Conclusions",
      "text" : "We have introduced the probabilistic DL ALCP, which extends the classical DL ALC with the capability of expressing and reasoning about uncertain contextual knowledge defined through the principle of maximum entropy. Effective reasoning methods were developed using the decoupling between the logical and the probabilistic components of ALCP KBs. We also studied the properties of this logic in relation to other probabilistic logics.\nWe plan to extend this work in several directions. First, instead of considering the ME-model, we could reason over all probability distributions that satisfy our probabilistic constraints similar to [12, 17, 20]. This will result in larger belief intervals in general. A smaller interval is preferable since it corresponds to a more precise degree of belief. However, when using all probability distributions the size of the interval can be a good indicator for the variation of the possible beliefs in our query with respect to the knowledge base.\nIn some applications it is also useful to allow more expressive propositional or relational context languages like those proposed in [4, 7, 15, 23]. Similarly, we can consider other DLs for our concept language. Indeed, ALC was chosen as a prototypical DL for studying the basic properties of our framework. Including additional constructors into the formalism should be relatively simple. In contrast, considering other reasoning problems beyond subsumption is less straightforward. Recall, for instance, that if an ALCP KB K contains an inconsistent context with positive probability, then K has no models. It is thus unclear how to handle the probability of consistency of a KB.\nPractical reasoning with ALCP can be currently performed by combining existing ME-reasoners3 with any ALC-reasoner4 according to Algorithm 1. Clearly, such an approach can still be further optimized. We are working on combining the classical and probabilistic reasoning parts in more sophisticated ways."
    }, {
      "heading" : "Appendix: Proofs",
      "text" : "Theorem 13. The KB K = (R, T ) is ME-consistent iff for every v ∈ Int(L) such that PMER (v) > 0, Tv is consistent.\nProof. For the “if” direction, let v1, . . . , vn ∈ Int(L) be all the L-interpretations such that PMER (vi) > 0, 1 ≤ i ≤ n. Then, for every i, 1 ≤ i ≤ n the induced TBox Tvi has a classical model Ii = (∆\nIi , ·Ii), by assumption. It is easy to verify that the ALCP-interpretation P = (I, PI) defined by\nI = {Ji = (∆ Ii , ·Ii , vi) | 1 ≤ i ≤ n}\nand PI(Ji) = PMER (vi) for all i, 1 ≤ i ≤ n is an ME-model of K. Thus, K is consistent.\nConversely, let P = (I, PI) be an ME-model of K. Then, for every v ∈ Int(L) with PMER (v) > 0 there is a possible world (∆\nI , ·I ,wI) with wI = v. Since I is a model of T and satisfies all contexts corresponding to the GCIs in Tv, it follows that (∆I , ·I) |= Tv; thus, Tv must be consistent. ⊓⊔\nTheorem 16 (Intermediate Value Theorem). Let p1 < p2 and P1 and P2 be two ME-ALCP-models of the KB K = (R, T ) such that PrP1(C ⊑ D | κ) = p1 and PrP2(C ⊑ D | κ) = p2. Then for each p between p1 and p2 there exists an ME-ALCP-model P of K such that PrP(C ⊑ D | κ) = p\nProof. Assume w.l.o.g. that Pi = (Ii, Pi), i = 1, 2, are such that I1 ∩ I2 = ∅: if there exists some I ∈ I1 ∩ I2, it suffices to rename the elements in ∆I in one of the probabilistic interpretations. Given λ ∈ [0, 1], define Pλ = (I1 ∪ I2, Pλ), where for every I ∈ I1 ∪ I2,\nPλ(I) =\n{\n(1− λ)P1(I) if I ∈ I1\nλP2(I) otherwise.\nPλ is consistent with T since P1,P2 are. We now show that Pλ induces the ME-model of R, which implies that Pλ is an ME-ALCP-model of K. For all v ∈ Int(L), we have\nPPλ(v) = ∑\nI∈(I1∪I2)|v\nPλ(I)\n= ∑\nI∈I1|v\nPλ(I) + ∑\nI∈I2|v\nPλ(I)\n= ∑\nI∈I1|v\n(1 − λ) · P1(I) + ∑\nI∈I2|v\nλ · P2(I)\n= (1− λ) · PP1(v) + λ · PP2(v) = PMER (v),\nwhere the last equation follows from PP1 = PP2 = PMER . Hence, each Pλ is indeed a ME-ALCP-model of K. For the probability of our subsumption relation,\nwe can derive similarly that PPλ(κ)PrPλ(C ⊑ D | κ) is equal to ∑\nI∈I1∪I2, I|=κ,I|=C⊑D\nPλ(I) = ∑\nI∈I1, I|=κ,I|=C⊑D\nPλ(I) + ∑\nI∈I2, I|=κ,I|=C⊑D\nPλ(I)\n= PPλ(κ) ((1− λ)p1 + λp2) .\nFor every p ∈ [p1, p2] there exists a λp ∈ [0, 1] such that p = (1 − λp)p1 + λpp2. Using this value λp we obtain that PrPλp (C ⊑ D | κ) = p ⊓⊔\nIn order to prove Theorems 18 and 19 it is useful to consider a restricted class of ALCP-interpretations in which each context is represented by at most one possible world. We call these interpretations pithy.\nDefinition 28 (pithy). The ALCP-interpretation P=(I, PI) is pithy if for every w ∈ Int(L) there is at most one possible world (∆I , ·I , vI) ∈ I with vI = w.\nAs the following lemma shows, pithy models are sufficient for computing the sceptical and credulous degrees of belief of a conditional subsumption relation and, by extension, the belief interval.\nLemma 29. Let K be an ALCP KB, C,D two concepts and κ ∈ L such that PMER (κ) > 0. For every ALCP-model P of K there exist pithy ALCP-models Q1,Q2 of K such that\nPrQ1(C ⊑ D | κ)≤PrP(C ⊑ D | κ)≤PrQ2(C ⊑ D | κ)\nand PP = PQ1 = PQ2 .\nProof. Let P = (I, PI). If P is already pithy, then the result holds trivially. Otherwise, there must exist two possible worlds I,J ∈ I such that vI = vJ . There are four possible cases: (i) I |= C ⊑ D and J |= C ⊑ D; (ii) I 6|= C ⊑ D and J 6|= C ⊑ D; (iii) I |= C ⊑ D and J 6|= C ⊑ D; and (iv) I 6|= C ⊑ D and J |= C ⊑ D.\nWe construct a new model P ′ by removing one of the possible worlds I,J and redistributing the probability according to these cases, as described next. For the first three cases, define H := I \\ {I} and the probability distribution\nPH(H) :=\n{\nPI(H) H 6= J\nPI(I) + PI(J ) H = J\nfor all H ∈ H. Then PP = PP ′\nand P ′ = (H, PH) is still an ME-model of K. Since the denominator in (3) is PMER (κ) independently of C ⊑ D, we have by construction that\nPrP′(C ⊑ D | κ) ≤ PrP(C ⊑ D | κ).\nThe case (iv) is symmetric to (iii), where the possible world J is removed instead of I. Since H ⊂ I, we can iteratively repeat this process until a pithy model Q1 is obtained.\nQ2 can be constructed symmetrically. ⊓⊔\nNotice that since PP = PQ1 = PQ2 , this lemma in particular implies that for each ME-model there exists a pithy ME-model that yields a smaller or equal probability for the subsumption relation, and dually one that yields a larger or equal probability. Note also that in each pithy ME-model, for all w ∈ Int(L) with PMER (w) > 0, there must be exactly one possible world I with v\nI = w because otherwise PI could not be a probability distribution (the elementary events could not sum to 1). Moreover, since a pithy interpretation can contain at most |Int(L)| possible worlds and the world corresponding to some w ∈ Int(L) must have probability PMER (w), there is only a finite number of probabilities that pithy models can assign to a given subsumption relation. Hence, the infimum and supremum that define the sceptical and the credulous degrees of belief actually correspond to minimum and maximum taken by some pithy ME-models.\nCorollary 30. Given an ALCP KB K, two concepts C,D and a context κ, there exist two pithy ME-models P,Q of K such that BsK(C ⊑ D | κ) = PrP(C ⊑ D | κ) and BcK(C ⊑ D | κ) = PrQ(C ⊑ D | κ).\nTheorem 18. Let K = (R, T ) be a KB, C,D two concepts, and κ a context such that PMER (κ) > 0. Then\nBsK(C ⊑ D | κ) =\n∑\nw∈Int(L),Tw|=C⊑D,w|=κ PMER (w)\nPMER (κ) .\nProof. For every w ∈ Int(L), we construct anALCP-interpretation Iw as follows. If Tw |= C ⊑ D, then Iw is any model (∆Iw , ·Iw , w) of Tw; otherwise, Iw is any model (∆Iw , ·Iw , w) of Tw that does not satisfy C ⊑ D, which must exist by definition. Let now PK = (I, PI) be the ALCP-interpretation such that I = {Iw | w ∈ Int(L)} and PI(Iw) = PME(w) for all w. Then PK is a model of K. Moreover, it holds that\nPrPK(C ⊑ D | κ) = ∑\nIw|=C⊑D,w|=κ\nPI(Iw)/P ME R (κ)\n= ∑\nTw|=C⊑D\nPMER (w)/P ME R (κ).\nThus, PMER (κ)B s K(C ⊑ D | κ) ≤\n∑\nTw|=C⊑D,w|=κ PMER (w). If this inequality is\nstrict, then w.l.o.g. there must exist a pithy probabilistic model P = (J, PJ) of K such that PrP(C ⊑ D | κ) < PrPK(C ⊑ D | κ) (see Lemma 29). Hence for every w ∈ Int(L) with PMER (w) > 0 there exists exactly one Jw ∈ J with v\nJW = w. We thus have\n∑\nJw |=C⊑D\nPJ(Jw) < ∑\nIw|=C⊑D\nPI(Iw).\nSince PI(Iw) = PJ(Jw) for all w, then there must exist a valuation v such that Iv |= C ⊑ D but Jv 6|= C ⊑ D. Since Jv is a model of Tv it follows that Tv 6|= C ⊑ D. By construction, then we have that Iv 6|= C ⊑ D, which is a contradiction. ⊓⊔\nTheorem 19. Let K = (R, T ) be a KB, C,D two concepts, and κ a context with PMER (κ) > 0. Then\nBcK(C ⊑ D | κ) = 1−\n∑\nw∈Int(L),Tw|=C 6 6⊑D,w|=κ PMER (w)\nPMER (κ) .\nProof. For every w ∈ Int(L), construct an ALCP-interpretation Iw as follows. If Tw |= C 6 6⊑ D, then Iw is any model (∆Iw , ·Iw , w) of Tw; otherwise, Iw is any model (∆Iw , ·Iw , w) of Tw that satisfies C ⊑ D. Let PK = (I, PI) be the ALCP-interpretation with I = {Iw | w ∈ Int(L)} and PI(Iw) = PME(w) for all w. Then PK is a model of K. Moreover, it holds that\nPrPK(C ⊑ D | κ) = ∑\nIw|=C⊑D,w|=κ\nPI(Iw)/P ME R (κ)\n= 1− ∑\nTw|=A 6⊑B\nPMER (w)/P ME R (κ).\nThat is, BcK(C ⊑ D | κ) ≥ 1− ∑ Tw|=C⊑D,w|=k PMER (w)/P ME R (κ). If this inequality is strict, then there exists a probabilistic model P = (J, PJ) of K such that PrP(C ⊑ D | κ) > PrPK(C ⊑ D | κ). By Lemma 29, we can assume w.l.o.g. that P is pithy. Hence for every w ∈ Int(L) with PME(w) > 0 there is exactly one Jw ∈ J with vJW = w, and thus,\n∑\nJw |=C⊑D\nPJ(Jw) > ∑\nIw|=C⊑D\nPI(Iw).\nSince PI(Iw) = PJ(Jw) for all w, there must exist some v ∈ Int(L) such that Iv 6|= C ⊑ D but Jv |= C ⊑ D. As Jv is a model of Tv, Tv 6|= C 6 6⊑ D. By construction, we have that Iv |= C ⊑ D, which is a contradiction. ⊓⊔\nTheorem 22. Let K = (R, T ) be an ALCP KB, φ and ψ be consequence formulas for C ⊑ D and C 6 6⊑ D w.r.t. T , respectively, and κ a context. Then BsK(C ⊑ D | κ) = P ME R (φ | κ) and B c K(C ⊑ D | κ) = 1− P ME R (ψ | κ).\nProof. The result is a direct consequence of Definition 21 and Theorems 18 and 19. Indeed,\nBsK(C ⊑ D | κ) = ∑\nTw|=C⊑D,w|=κ\nPMER (w)/P ME R (κ)\n= ∑\nw|=φ∧κ\nPMER (w)/P ME R (κ) = P ME R (φ | κ).\nThe case of the credulous degree of belief is analogous. ⊓⊔\nTheorem 24 (Representation invariance). Let Ki = (Ri, Ti), i ∈ {1, 2}, be two KBs such that Mod(R1) = Mod(R2) and Mod(T1) = Mod(T2). Then for all concepts C,D and contexts κ ∈ L, BK1(C ⊑ D | κ) = BK2(C ⊑ D | κ).\nProof. Let P = (I, PI) be an ALCP-interpretation. Since Mod(T1) = Mod(T2), P is consistent with T1 iff P is consistent with T2. Since Mod(R1) = Mod(R2), R1 and R2 induce the same ME-model and P1 is (ME-)consistent with R iff P2 is (ME-)consistent with R. Hence,\nBsK1(C ⊑ D | κ) = infP∈ModME(K1) PrP(C ⊑ D | κ)\n= inf P∈ModME(K2) PrP(C ⊑ D | κ) = BsK2(C ⊑ D | κ)\nAnalogously, we get that BcK1(C ⊑ D | κ) = B c K2 (C ⊑ D | κ) and therefore BK1(C ⊑ D | κ) = BK2(C ⊑ D | κ). ⊓⊔\nTheorem 25 (Language Invariance). Let K1,K2 be KBs over L 1,N1C,N 1 R and L2,N2C,N 2 R, respectively. If K1 = K2, L 1 ⊆ L2,N1C ⊆ N 2 C and N 1 R ⊆ N 2 R, then for all concepts C,D ∈ N1C and contexts κ ∈ L 1, it holds that\nBK1(C ⊑ D | κ) = BK2(C ⊑ D | κ).\nProof. It suffices to show that for every ALCP-model P1 of K1 there exists a ALCP-model P2 of K2 such that PrP1(C ⊑ D | κ) = PrP2(C ⊑ D | κ) and vice versa. Given an ALCP-model P1 = (I1, PI1) of K1, we build P2 = (I2, PI2) as follows. For each possible world I ∈ I1 with probability p, I2 contains a possible world I ′ with probability p that extends I assigning false to all new propositional variables and the empty set to all new role names and concept names. Since C,D, κ and K2 depend only on sig(L1),N1C,N 1 R, P2 satisfies K2 and PrP1(C ⊑ D | κ) = PrP2(C ⊑ D | κ) holds. Conversely, consider an ALCP-model P2 = (I2, PI2) of K2. We obtain P1 from P2 by restricting the possible worlds in I2 to C,D, κ. As before, it follows that P1 satisfies K1 and PrP1(C ⊑ D | κ) = PrP2(C ⊑ D | κ). ⊓⊔\nIn order to prove Independence, we need the following lemma. It states an independence property of ME-distributions over our context language.\nLemma 31 (ME-independence). Let R1,R2 be two finite sets of probability constraints such that sig(L1) ∩ sig(L2) = ∅, and let R := R1 ∪ R2. Then PMER = P ME 1 · P ME 2 . In particular, for the marginal distributions of P ME\nR , we have PMER (vi) = P ME i (vi) for all vi ∈ Int(Li), i ∈ {1, 2}.\nProof. Since the signatures of Li are disjoint, we can denote the valuations of the language over sig(L1)∪ sig(L2) by (v1, v2), vi ∈ sig(Li). Let us first consider the marginals of P = PME1 · P ME 2 . For all v1 ∈ Int(Li), we have\nP (v1) = ∑\nv2∈Int(Li)\nP (v1, v2) = P ME 1 (v1) ∑\nv2∈Int(Li)\nPME2 (v2)\n= PME1 (v1).\nSymmetrically, we can show that P (v2) = P ME 2 (v2). This means, in particular, that the marginals of P coincide with the corresponding maximum entropy solutions. Therefore,\nH(P ) = ∑\nv1\n∑\nv2\nPME1 (v1)P ME 2 (v2) log(P ME 1 (v1)·P ME 2 (v2))\n= ∑\nv2\nPME2 (v2) ∑\nv1\nPME1 (v1) logP ME 1 (v1)\n+ ∑\nv1\nPME1 (v1) ∑\nv2\nPME2 (v2) logP ME 2 (v2)\n= H(PME1 ) +H(P ME 2 ).\nUsing the independence bound for entropy (see, e.g., [27], Theorem 2.39), we have that\nH(PMER ) ≤ H(P ME 1 ) +H(P ME 2 ) = H(P ).\nHence, it suffices to show that PME1 ·P ME 2 is indeed a model of R1∪R2. But this follows immediately from the facts that PMEi satisfies Ri and that the marginalization of P over one logic corresponds to the ME-distribution over the other.\n⊓⊔\nTheorem 26 (Independence). Let K1,K2 be s.t. sig(K1) ∩ sig(K2) = ∅, C,D be two concepts, and κ a context where (sig(C) ∪ sig(D) ∪ sig(κ)) ∩ sig(K2) = ∅. Then B(C ⊑K1 D | κ) = B(C ⊑K1∪K2 D | κ).\nProof. Let Ki = (Ri, Ti) for i ∈ {1, 2}. Since the signatures of both KBs are disjoint, we will denote the valuations over the set of variables sig(R1)∪ sig(R2) as pairs (w1, w2), where wi is a valuation over sig(Ri). We know from Theorem 18 that\nPME(κ)B(C ⊑K1∪K2 D | κ) = ∑\n(w1,w2)|=κ (T1∪T2)(w1,w2) |=C⊑D\nPMER ((w1, w2))\n= ∑\n(w1,w2)|=κ (T1)w1 |=C⊑D\nPMER ((w1, w2)) (4)\n= ∑\nw1|=κ (T1)w1 |=C⊑D\nPMER (w1) (5)\n= PMER (κ)B(C ⊑K1 D | κ),\nwhere (4) follows from the monotonicity of subsumption in ALC TBoxes, and (5) is a consequence of Lemma 31. ⊓⊔\nIn order to prove Continuity, we start with a lemma that states continuity of ME-distributions over L. The proof is analogous to Paris’ proof of continuity of maximum entropy reasoning in his probabilistic logic [22], which is a sub-logic of our probabilistic logic over L.\nLemma 32 (ME-continuity). Let R be a set of probabilistic constraints and let (Ri) be a sequence of probabilistic constraints such that (Mod(Ri)) converges to Mod(R). Then the sequence (Pi) of ME -Models of Ri converges to the ME-model PMER of R.\nProof. For brevity, let M = Mod(R) and Mi = Mod(Ri). We show that for each ǫ > 0, there is a δ > 0 such that ‖Ki,K‖B < δ implies that ‖PMEi , P ME\nR ‖1 < ǫ. Consider the set S = {P ∈ M | ‖P, PMER ‖1 ≥ ǫ 2} of models of of R that have at least distance ǫ2 to P ME\nR . By continuity of the euclidean distance and compactness of M , S must be compact. Since the entropy function H is continuous, the minimum ν = min{H(PMER ) −H(P ) | P ∈ S} does exist and ν > 0 by unique maximality of PMER . Since H is defined on a compact set (the set of probability distributions over L), H is uniformly continuous. Therefore there exists a δ > 0 such that for all distributions P1, P2 over L, ‖P1, P2‖1 < δ implies that H(P1) −H(P2) < min{ ǫ 2 , ν 2}. In partiular, we can assume that δ < ǫ 2 . Now, if ‖Mi,M‖B < δ, there is a Pi ∈ Mi such that ‖Pi, PMER ‖1 < δ and a P ∈ M such that ‖PMEi , P‖1 < δ. Hence,\nH(PMER ) < H(Pi) + ν\n2 ≤ H(PMEi ) +\nν 2 ,\nH(PMEi ) < H(P ) + ν\n2 ≤ H(PMER ) +\nν\n2\nand therefore |H(PMEi )−H(P ME R )| < ν 2 . In particular,\n|H(P )−H(PMER )|\n≤ |H(P )−H(PMEi )|+ |H(P ME i )−H(P ME R )| < ν\n2 +\nν 2 = ν.\nBy definition of ν, we can conclude that P ∈ M \\S and therefore ‖PMER , P‖1 < ǫ 2 . Hence,\n‖PMER , P ME i ‖1 ≤ ‖P ME R , P‖1 + ‖P, P ME i ‖1 < ǫ\n2 + δ < ǫ.\n⊓⊔\nTheorem 27 (Continuity). Let (Ki) be a convergent sequence of KBs with limit K and BKi(C ⊑ D | κ) = [ℓi, ui]. If BK(C ⊑ D | κ) = [ℓ, u], then (li) converges to ℓ and (ui) converges to u (with respect to the usual topology on R).\nProof. Let K = (R, T ) and Ki = (Ri, Ti). By assumption, (Mod(Ri)) converges to Mod(R). Hence, Lemma 32 implies that the probability distributions induced by ME-models of Ki converge to PMER , which, in turn, is the probability distribution induced by all ME-models of K. Hence, infimum and supremum of the conditonal probability of C ⊑ D given κ with respect to Ki will converge to infimum and supremum with respect to K. ⊓⊔"
    } ],
    "references" : [ {
      "title" : "The Description Logic Handbook: Theory, Implementation, and Applications",
      "author" : [ "F. Baader", "D. Calvanese", "D.L. McGuinness", "D. Nardi", "Patel-Schneider", "P.F. (eds." ],
      "venue" : "Cambridge University Press, 2nd edn.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Context-dependent views to axioms and consequences of semantic web ontologies",
      "author" : [ "F. Baader", "M. Knechtel", "R. Peñaloza" ],
      "venue" : "J. of Web Semantics 12–13, 22–40",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Maximum entropy inference with quantified knowledge",
      "author" : [ "O. Barnett", "J.B. Paris" ],
      "venue" : "Logic Journal of the IGPL 16(1), 85–98",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Extending and completing probabilistic knowledge and beliefs without bias",
      "author" : [ "C. Beierle", "G. Kern-Isberner", "M. Finthammer", "N. Potyka" ],
      "venue" : "KI 29(3), 255–262",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "The Bayesian description logic BEL",
      "author" : [ "I.I. Ceylan", "R. Peñaloza" ],
      "venue" : "Proc. of IJCAR 2014. LNCS, vol. 8562, pp. 480–494. Springer",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Tractable reasoning with Bayesian description logics",
      "author" : [ "C. d’Amato", "N. Fanizzi", "T. Lukasiewicz" ],
      "venue" : "Proc. SUM 2008. LNCS, vol. 5291, pp. 146–159. Springer",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Towards classifying propositional probabilistic logics",
      "author" : [ "G. De Bona", "F.G. Cozman", "M. Finger" ],
      "venue" : "J. of Applied Logic 12(3), 349–368",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Markov Logic: An Interface Layer for Artificial Intelligence",
      "author" : [ "P.M. Domingos", "D. Lowd" ],
      "venue" : "Synthesis Lectures on Artificial Intelligence and Machine Learning, Morgan & Claypool Publishers",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "ExpTime tableaux for ALC",
      "author" : [ "F.M. Donini", "F. Massacci" ],
      "venue" : "Artificial Intelligence 124(1), 87–138",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "An analysis of first-order logics of probability",
      "author" : [ "J.Y. Halpern" ],
      "venue" : "Artificial Intelligence 46, 311–350",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Representation dependence in probabilistic inference",
      "author" : [ "J.Y. Halpern", "D. Koller" ],
      "venue" : "JAIR pp. 319–356",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Merging the local and global approaches to probabilistic satisfiability",
      "author" : [ "P. Hansen", "S. Perron" ],
      "venue" : "Intern. J. of Approx. Reasoning 47(2), 125 – 140",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Conditionals in nonmonotonic reasoning and belief revision",
      "author" : [ "G. Kern-Isberner" ],
      "venue" : "Springer, LNAI 2087",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Novel semantical approaches to relational probabilistic conditionals",
      "author" : [ "G. Kern-Isberner", "M. Thimm" ],
      "venue" : "Proc. KR 2010. pp. 382–391. AAAI Press",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Combining probabilistic logic programming with the power of maximum entropy",
      "author" : [ "G. Kern-Isberner", "T. Lukasiewicz" ],
      "venue" : "Artif. Intell. 157(1-2), 139–202",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "A hybrid method for probabilistic satisfiability",
      "author" : [ "P. Klinov", "B. Parsia" ],
      "venue" : "Proc. CADE 2011. LNCS, vol. 6803, pp. 354–368. Springer",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Probabilistic deduction with conditional constraints over basic events",
      "author" : [ "T. Lukasiewicz" ],
      "venue" : "JAIR 10, 380–391",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Managing uncertainty and vagueness in description logics for the semantic web",
      "author" : [ "T. Lukasiewicz", "U. Straccia" ],
      "venue" : "J. of Web Semantics 6(4), 291–308",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Probabilistic description logics for subjective uncertainty",
      "author" : [ "C. Lutz", "L. Schröder" ],
      "venue" : "Proc. KR 2010. AAAI Press",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Probabilistic logic",
      "author" : [ "N.J. Nilsson" ],
      "venue" : "Artificial Intelligence 28, 71–88",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "Numerical Optimization",
      "author" : [ "J. Nocedal", "S.J. Wright" ],
      "venue" : "Springer, 2nd edn.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "The uncertain reasoner’s companion – A mathematical perspective",
      "author" : [ "J. Paris" ],
      "venue" : "Cambridge University Press",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Reasoning over linear probabilistic knowledge bases with priorities",
      "author" : [ "N. Potyka" ],
      "venue" : "Proc. SUM 2015. vol. 9310, pp. 121–136. Springer",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Relationships between semantics for relational probabilistic conditional logics",
      "author" : [ "N. Potyka" ],
      "venue" : "Computational Models of Rationality, Essays dedicated to Gabriele Kern-Isberner. pp. 332–347. College Publications",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Epistemic and statistical probabilistic ontologies",
      "author" : [ "F. Riguzzi", "E. Bellodi", "E. Lamma", "R. Zese" ],
      "venue" : "Proc. URSW-12. vol. 900, pp. 3–14. CEUR-WS",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Attributive concept descriptions with complements",
      "author" : [ "M. Schmidt-Schauß", "G. Smolka" ],
      "venue" : "Artif. Intell. 48(1), 1–26",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Information theory and network coding",
      "author" : [ "R.W. Yeung" ],
      "venue" : "Springer Science & Business Media",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Logic-based knowledge representation languages such as description logics (DLs) [1] provide a clear syntax and unambiguous semantics that guarantee the correctness of the results obtained.",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 2,
      "context" : ", [3, 15, 20]).",
      "startOffset" : 2,
      "endOffset" : 13
    }, {
      "referenceID" : 14,
      "context" : ", [3, 15, 20]).",
      "startOffset" : 2,
      "endOffset" : 13
    }, {
      "referenceID" : 19,
      "context" : ", [3, 15, 20]).",
      "startOffset" : 2,
      "endOffset" : 13
    }, {
      "referenceID" : 17,
      "context" : "In particular, several probabilistic DLs have been developed [18, 19].",
      "startOffset" : 61,
      "endOffset" : 69
    }, {
      "referenceID" : 18,
      "context" : "In particular, several probabilistic DLs have been developed [18, 19].",
      "startOffset" : 61,
      "endOffset" : 69
    }, {
      "referenceID" : 4,
      "context" : "To handle probabilistic knowledge, many approaches require a complete definition of joint probability distributions (JPD) [5, 6, 8, 16, 25].",
      "startOffset" : 122,
      "endOffset" : 139
    }, {
      "referenceID" : 5,
      "context" : "To handle probabilistic knowledge, many approaches require a complete definition of joint probability distributions (JPD) [5, 6, 8, 16, 25].",
      "startOffset" : 122,
      "endOffset" : 139
    }, {
      "referenceID" : 7,
      "context" : "To handle probabilistic knowledge, many approaches require a complete definition of joint probability distributions (JPD) [5, 6, 8, 16, 25].",
      "startOffset" : 122,
      "endOffset" : 139
    }, {
      "referenceID" : 15,
      "context" : "To handle probabilistic knowledge, many approaches require a complete definition of joint probability distributions (JPD) [5, 6, 8, 16, 25].",
      "startOffset" : 122,
      "endOffset" : 139
    }, {
      "referenceID" : 24,
      "context" : "To handle probabilistic knowledge, many approaches require a complete definition of joint probability distributions (JPD) [5, 6, 8, 16, 25].",
      "startOffset" : 122,
      "endOffset" : 139
    }, {
      "referenceID" : 21,
      "context" : "One approach to avoid a full JPD specification was proposed by Paris [22]: the user gives a partial specification through a set of probabilistic constraints and the partial knowledge is completed by means of the principle of maximum entropy.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 25,
      "context" : "To facilitate the understanding of our approach, we focus on the DL ALC [26] as a prototypical example of a knowledge representation language, and propositional probabilistic constraints as the framework for expressing uncertainty.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 0,
      "context" : "In the worst-case, we get the trivial interval [0, 1], in the best case, we get a point probability where the upper and lower bounds coincide.",
      "startOffset" : 47,
      "endOffset" : 53
    }, {
      "referenceID" : 0,
      "context" : "A probability distribution over L is a function P : Int(L) → [0, 1] where ∑",
      "startOffset" : 61,
      "endOffset" : 67
    }, {
      "referenceID" : 16,
      "context" : "For instance, probabilistic conditionals (ψ | φ)[l, u] are satisfied iff l · P (φ) ≤ P (ψ ∧ φ) ≤ u · P (φ) [17].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 23,
      "context" : "Sometimes P (φ) > 0 is demanded, but strict inequalities are computationally difficult and the semantical differences are negligible in many cases, see [24] for a thorough discussion.",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 26,
      "context" : ", [27] for a more detailed discussion of these issues.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 21,
      "context" : "A complete characterization of maximum entropy for the purpose of uncertain reasoning can be found in [22].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 20,
      "context" : ", [21] for more details on these techniques.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 0,
      "context" : "For a deeper introduction to classical ALC, see [1].",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 0,
      "context" : "The probability distribution P : Int(L) → [0, 1] induced by P is defined by P(v) := ∑",
      "startOffset" : 42,
      "endOffset" : 48
    }, {
      "referenceID" : 1,
      "context" : "Notice first that subsumption and non-subsumption are monotonic consequences in the sense of [2]; that is, if an ALC TBox T entails the subsumption C ⊑ D, then every superset of T also entails this consequence.",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 1,
      "context" : "Thus, the consequence formulas from Definition 21 are in fact the socalled boundaries from [2].",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 8,
      "context" : "This approach requires 2 calls to a standard ALC reasoner, and each of these calls runs in exponential time on |T | [9].",
      "startOffset" : 116,
      "endOffset" : 119
    }, {
      "referenceID" : 21,
      "context" : "We now investigate some properties of probabilistic logics [22].",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 10,
      "context" : "For instance, in [11] a very different notion is considered, where the language and the knowledge base are changed simultaneously.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 21,
      "context" : "As demonstrated by Paris in [22], measuring the difference between probabilistic knowledge bases is subtle and is best addressed by comparing knowledge bases extensionally; i.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 9,
      "context" : "Relational probabilistic logical approaches can be roughly divided into those that consider probability distributions over the domain, those that consider probability distributions over possible worlds and those that combine both ideas [10].",
      "startOffset" : 236,
      "endOffset" : 240
    }, {
      "referenceID" : 12,
      "context" : ", in [13,22], and various extensions to first-order languages have been considered in recent years [3, 4, 14, 15].",
      "startOffset" : 5,
      "endOffset" : 12
    }, {
      "referenceID" : 21,
      "context" : ", in [13,22], and various extensions to first-order languages have been considered in recent years [3, 4, 14, 15].",
      "startOffset" : 5,
      "endOffset" : 12
    }, {
      "referenceID" : 2,
      "context" : ", in [13,22], and various extensions to first-order languages have been considered in recent years [3, 4, 14, 15].",
      "startOffset" : 99,
      "endOffset" : 113
    }, {
      "referenceID" : 3,
      "context" : ", in [13,22], and various extensions to first-order languages have been considered in recent years [3, 4, 14, 15].",
      "startOffset" : 99,
      "endOffset" : 113
    }, {
      "referenceID" : 13,
      "context" : ", in [13,22], and various extensions to first-order languages have been considered in recent years [3, 4, 14, 15].",
      "startOffset" : 99,
      "endOffset" : 113
    }, {
      "referenceID" : 14,
      "context" : ", in [13,22], and various extensions to first-order languages have been considered in recent years [3, 4, 14, 15].",
      "startOffset" : 99,
      "endOffset" : 113
    }, {
      "referenceID" : 15,
      "context" : "Many probabilistic DLs have also been considered in the last decades [16, 18, 19].",
      "startOffset" : 69,
      "endOffset" : 81
    }, {
      "referenceID" : 17,
      "context" : "Many probabilistic DLs have also been considered in the last decades [16, 18, 19].",
      "startOffset" : 69,
      "endOffset" : 81
    }, {
      "referenceID" : 18,
      "context" : "Many probabilistic DLs have also been considered in the last decades [16, 18, 19].",
      "startOffset" : 69,
      "endOffset" : 81
    }, {
      "referenceID" : 4,
      "context" : "Our approach is closest to Bayesian DLs [5, 6] and disponte [25].",
      "startOffset" : 40,
      "endOffset" : 46
    }, {
      "referenceID" : 5,
      "context" : "Our approach is closest to Bayesian DLs [5, 6] and disponte [25].",
      "startOffset" : 40,
      "endOffset" : 46
    }, {
      "referenceID" : 24,
      "context" : "Our approach is closest to Bayesian DLs [5, 6] and disponte [25].",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 11,
      "context" : "First, instead of considering the ME-model, we could reason over all probability distributions that satisfy our probabilistic constraints similar to [12, 17, 20].",
      "startOffset" : 149,
      "endOffset" : 161
    }, {
      "referenceID" : 16,
      "context" : "First, instead of considering the ME-model, we could reason over all probability distributions that satisfy our probabilistic constraints similar to [12, 17, 20].",
      "startOffset" : 149,
      "endOffset" : 161
    }, {
      "referenceID" : 19,
      "context" : "First, instead of considering the ME-model, we could reason over all probability distributions that satisfy our probabilistic constraints similar to [12, 17, 20].",
      "startOffset" : 149,
      "endOffset" : 161
    }, {
      "referenceID" : 3,
      "context" : "In some applications it is also useful to allow more expressive propositional or relational context languages like those proposed in [4, 7, 15, 23].",
      "startOffset" : 133,
      "endOffset" : 147
    }, {
      "referenceID" : 6,
      "context" : "In some applications it is also useful to allow more expressive propositional or relational context languages like those proposed in [4, 7, 15, 23].",
      "startOffset" : 133,
      "endOffset" : 147
    }, {
      "referenceID" : 14,
      "context" : "In some applications it is also useful to allow more expressive propositional or relational context languages like those proposed in [4, 7, 15, 23].",
      "startOffset" : 133,
      "endOffset" : 147
    }, {
      "referenceID" : 22,
      "context" : "In some applications it is also useful to allow more expressive propositional or relational context languages like those proposed in [4, 7, 15, 23].",
      "startOffset" : 133,
      "endOffset" : 147
    } ],
    "year" : 2016,
    "abstractText" : "A central question for knowledge representation is how to encode and handle uncertain knowledge adequately. We introduce the probabilistic description logic ALCP that is designed for representing context-dependent knowledge, where the actual context taking place is uncertain. ALCP allows the expression of logical dependencies on the domain and probabilistic dependencies on the possible contexts. In order to draw probabilistic conclusions, we employ the principle of maximum entropy. We provide reasoning algorithms for this logic, and show that it satisfies several desirable properties of probabilistic logics.",
    "creator" : "LaTeX with hyperref package"
  }
}