{
  "name" : "1306.2593.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A 10-dimensional Phonetic-prosodic Space and its Stochastic Structure A framework for probabilistic modeling of spoken languages and their phonology",
    "authors" : [ "Elaine Tsiang" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "It is a fundamental assumption of phonetics that there is a space of articulatory gestures which encompasses all spoken languages. From this space, a particular language selects a set of differences in these gestures to represent the bits required to encode its lexicon[1]. This implies that this gestural space is more fine-grained than the acknowledged articulatory repertories of specific languages. .\nThe International Phonetic Alphabet(IPA)[2] defines this gestural space via markers that specify the production of the gestures at a resolution sufficiently fine to support all spoken languages. We define here the space for the perception of these gestures at a sufficiently fine resolution to cover the search space of all language-specific recognizers. That is, it supports narrow phonetic-prosodic transcriptions of all languages. This contrasts with other approaches aimed at constructing acoustic models broad enough to be adaptable to multiple languages[3]\nThis perceptual space is a product of two subspaces, the phonetic and the prosodic. Our formulation is based on a theory of speech perception that the articulatory gestures are sub-maximally observable[4]. We define the phonetic\nsubspace to be the 4-dimensional value space of the submaximally observable random variable. We propose an alphabet, the IHear1 Alphabet(IHA) as the set of markers in this phonetic subspace. The prosodic subspace consists of 6 directly measurable physical variables. This division between the phonetic aspect of speech perception as inferentially observable, and the prosodic aspect as directly measurable, and our definitions of the 10 dimensions, to be given below, have resulted from the computational definitions in developing a phone and prosody recognizer2. They allow a self-consistent language-neutral3 definition of the phonetic alphabet, which in turn allows any specific language to be defined solely phonologically.\nThe IHA phone is an instance of a phonetic marker, together with particular values of the prosodic subspace. Spoken utterances as phone strings are then configurations of the random chain of IHA phones. With the IHA, traditional diphthongs naturally generalize to a broader class of language-neutral phonotactic constraints, indicating a correlation structure similar to that of the traditional sonoritybased syllable. Thus motivated, we present the stochastic structure of the IHA random chain, and explain how it can be used to define specific spoken languages, and to account for phonological variations within a specific language."
    }, {
      "heading" : "2. The perceptual phonetic alphabet",
      "text" : "We will present the IHA with reference to the more recent but unofficial IPA[5], pointing out the similarities and differences."
    }, {
      "heading" : "2.1. The phonetic subspace",
      "text" : "We use the names of the traditional phonetic attributes of manner, frontBack, openClose and place to label the four dimensions of the perceptual phonetic space. We propose six values for manner: closure, plosive, fricative, nasal, approximant and vowel. All except closure correspond with the IPA manners. The IHA does not have values for the IPA\n1IHear is a registered trademark of Monowave Corporation 2The recognizer is not within the scope of this paper, but will be the subject of separate articles. The recognized results will be trajectories in the formulated phonetic-prosodic space. 3We use the term “language-neutral” rather than “languageindependent” to mean independence from the characteristics of any particular language, but not from what we understand to be a spoken language in general.\n* Monowave Corporation, Seattle, WA., USA\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported License.\n1 of 6\nmanners of taps, trills and the two lateral manners4. Taps have been relegated to their corresponding approximants. Trills will be marked as sequences of alternating closure and plosive. The phones in the lateral manners will be discussed in the sections detailing the proposed markers below.\nThere are 5 values for frontBack: front, frontLike, central, backLike and back, same as the IPA, but with slightly different naming. Similarly, openClose has 7 values: close, closeLike, closeMid, mid, openMid, openLike and open.\nThe set of values for place is where the IHA differs most from the IPA. There are 6 values: palatAlveoLabial(PAL), velar, uvular, pharyngeal, epiglottal and glottal. Each place has its own 3-dimensional subspace of manner⊗frontBack⊗ openClose. The 8 places of articulation from bilabial to palatal have been absorbed into the one value, palatAlveoLabial, of place, and given frontBack and openClose values. The dimension of place, like that of manner, is less interpretable as descriptive of articulation. We will sometimes refer to the places velar and uvular as the velarUvular, and the places pharyngeal, epiglottal and glottal as pharynGlottal."
    }, {
      "heading" : "2.2. Proposed phonetic markers",
      "text" : "Table 1 through Table 10 exhibit charts for these subspaces. We show only the charts for glottal fricatives as representative of all manners of pharynGlottals and similarly only velars for the velarUvulars, the other manners and place being analogous. We chart all the PALs except for closures. The closures for all values of place have the same frontBack⊗openClose subspaces as the corresponding fricatives, and are represented by subscripting the corresponding symbol with “ ”.₀\nNote that we do not distinguish between voiced and unvoiced phones, and the symbols that have voicing connotation in the IPA do not have such connotation in the IHA. It is well-known that the voiced vs. unvoiced distinction in different languages have varying voice onset times[6]. Also identifying voicing with periodicity is problematic. Whispered speech, for example, retains the perceptual difference between voiced and unvoiced phones. Instead, we define voicing as the presence or absence of the first formant, and the onset and offset times of the first formant will be marked. From this information, and possibly other prosodic markings, it is up to each specific language to define what it considers voiced, or unvoiced, and the number of voicing categories. This implies that all IHA phones may have voiced and unvoiced versions, including nasals, approximants and even vowels. In addition, closures constitute a full manner, with an equal complement of distinct phones as any other manner.\nThe symbols are usually of the unvoiced variety in the IPA, except for approximants, where some symbols are recommissioned from taps or the voiced fricatives. We use vowel superscripts to indicate frontBack and openClose, except for the PALs. In general, we aim for a single unicode point, instead of combinations. In three cases for the PALs, we have resorted to unicodes for Phoenician."
    }, {
      "heading" : "2.2.1. The vowels",
      "text" : "The IHA regards the vowels as glottal, by virtue of which the traditional great divide between vowels and consonants becomes simply a difference in place. However, the vowels\n4Nor does the IHA currently deal with the IPA non-pulmonic phones due to lack of data.\nare unique in that they have no other place than glottal, and only the glottal place has vowel entries.\nThe glottal vowel subspace frontBack⊗openClose is identical with the IPA vowel space. There is a one-to-one correspondence between the IPA vowels and the IHA vowels. However, notationally, the rounded or unrounded twins will be marked with the corresponding more rounded (for example, ) or less rounded (for example, u ) diacritic.ɑ˒ ˓"
    }, {
      "heading" : "2.2.2. The pharynGlottals",
      "text" : "The vowels are, however, less unique in the IHA than the IPA. First, the glottal frontBack⊗openClose subspace as shown in Table 1 also endows the other glottal manners, yielding as many glottal stops, glottal fricatives and glottal approximants as vowels. Second, the other pharynGlottals also have similar manner⊗frontBack⊗openClose subspaces. (The pharynGlottals do not have nasals.) In this way, the pharynGlottal plosives, fricatives and approximants may all serve as “syllabic”. In addition, all IPA diacritic markings of glottalization and pharyngealization are simply pharynGlottals of different frontBack and openClose values that may interpose between a phone and a vowel, or stand on their own. Symbols for glottal closures are obtained by subscripting the corresponding vowel symbol with “ ”, for example, “a ”;₀ ₀ glottal stops by substituting “h” with “ ” and approximantsʔ with “ ”.ɦ"
    }, {
      "heading" : "2.2.3. The velarUvulars",
      "text" : "These are “watershed” phones between the “vowel-like” pharynGlottals, and the “consonant-like” palatAlveoLabials. The velarUvular frontBack⊗openClose subspaces are more constricted than the pharynGlottal ones. The labialized velar, which is regarded as co-articulated and presented as approximants outside of the IPA chart for pulmonic consonants, is simply the close back velar in the IHA. The IPA lateral velar is likewise the closeMid backLike velar (thus reassigning one of the phones in the IPA lateral manners). Because we use the same superscript notation for velarUvulars as for pharynGlottals, we have included the corresponding IPA symbols in parentheses. The nasal and plosive closeMid backLike velars are not present in the IPA. The velars with different frontBack and openClose values constitute the IPA velarization diacritic. On the other hand, the front and frontLike velar fricatives and approximants also serve as the IPA diacritic palatalization of velars. The uvulars frequently occur as “lazy” or fast articulations of velars in languages that do not use the velar/uvular contrast.\n2 of 6"
    }, {
      "heading" : "2.2.4. The palatAlveoLabials",
      "text" : "With the PALs, the place dimension departs most from the articulatory meaning of the place of constriction. The palatal, alveolo-palatal, and the palato-alveolar IPA phones are assigned the value of front or frontLike, consonant with the perceptual proximity of these PALs with the close and closeLike front vowels. Likewise, the labials are assigned the back or backLike values. The central PALs, assigned increasing values of openClose, correspond to the dental, alveolar and retroflex IPA phones. The symbol [ ], used toɻ represent the IPA retroflex approximant, has been recommissioned to represent the marker for a central mid fricative PAL, more familiar to American English listeners as the sound of the grapheme <r>, but represented in its approximant version in both the IHA and the IPA by the inverted [ ]. The lateral alveolar, designated as belonging to aɹ separate lateral manner in the IPA, are assigned the values mid backLike, because of their perceptual similarity to the openMid back vowel, especially in “dark” settings[7]. The IPA lateral retroflex sits back of the retroflex, and assigned the values closeMid backLike.\nTable 7. palatAlveoLabial plosives.\nplosives front frontLike central backLike back close c ʨ t � p closeLike ʧ ʦ closeMid ʎ̌ ʈ ɭ̌\nmid ɻ̌ ɬ̌ In the IHA, the IPA affricates have become the plosive versions of the corresponding fricatives that have no direct plosives in the IPA. Affricates, being homorganic, are indistinguishable from sounds produced by excising the initial portion of their fricative versions (or final portion for the timereversed cases). Likewise, the other PAL plosives, the bilabial, dental, retroflex and palatal plosives, can also be simulated by cutting off the initial portion of their fricative versions. From the perceptual language-neutral point of view, all such homorganic affricates are simply plosives. This generalizes to other PALs such as [ ] and the laterals as well, and constitutesɻ either the IPA's affricates, or homorganic “released” versions of these PALs, both of which are diacritic in the IPA. We use a combining caron for these less common plosives."
    }, {
      "heading" : "2.3. Co-articulation",
      "text" : "The affricates have already been placed into the plosive subspaces. The labialized velars are already in the velar subspace. Thus co-articulation need not be specially marked. The time-aligned output from the IHA phone recognizer will be strictly ordered. The times of two consecutive phones may be very close, but no two phones will be emitted at the same time."
    }, {
      "heading" : "2.4. Some diacritics",
      "text" : "The IHA does not have the full set of diacritics in the IPA. Only glottalization, pharyngealization, velarization, releases and some palatalization are already marked by virtue of the frontBack⊗openClose subspaces as discussed above. Nasalization, however, will be marked, like voicing, with the onset and offset times of the nasal formant.\n3 of 6\nThe more/less rounded diacritic used for vowels may also be applied to other phones. We will discuss rounding, or unrounding in more detail below."
    }, {
      "heading" : "3. The prosodic subspace",
      "text" : "By virtue of the presentation of a time-aligned random chain of IHA phones, durations(D) are directly computable from the time alignment. Instead of defining stress, a complex observable, we present the loudness(L) of the phone. Tone (T) is simply pitch. D, T and L will all be in quantized logarithmic units. Together with voicing(V) on/off, nasalization(N) on/off and rounding (R), they constitute the 6 dimensions of the prosodic subspace.\nOur division between the phonetic and prosodic does not coincide with the conventional distinction of the phonetic as lexically contrastive for speech as code, and the prosodic as qualitatively indicative of emotions, which distinction does not obtain for many spoken languages. Nor do duration, loudness and pitch coincide with the conventional meanings of syllable length, stress and intonation. Voicing is conventionally regarded as phonetic, and nasalization and rounding are regarded as possibly supra-segmental, but not prosodic. However, none of the conventional notions of syllable length, stress, intonation, voicing or rounding are computationally well defined.\nThe merit of the 6 prosodic dimensions is that they are all computationally well-defined, some more simply than others. Rounding is defined to be the negative of the change over time in the logarithm of the effective vocal tract length, which is usually assumed to be a constant per speaker, but is in fact dynamic down to the inter-syllable scale. Rounding is also given in quantized logarithmic units. A positive value indicates “fronting” or “un-rounding”, and a negative value, “rounding”.\nIt is worth remarking that the 6 prosodic dimensions are “musical” attributes. Duration corresponds to tempo, loudness to what is called “dynamics” in music, pitch to pitch, and voicing, nasalization and rounding correspond to more vague characterizations of the quality of a musical instrument.\nThe value space for an IHA phone is then the 10- dimensional manner⊗frontBack⊗openClose⊗place⊗R ⊗N V T D L. We complete this space with one point of⊗ ⊗ ⊗ ⊗ origin, the null phone."
    }, {
      "heading" : "4. The probabilistic framework on the phonetic-prosodic space",
      "text" : ""
    }, {
      "heading" : "4.1. Generalized diphthongs",
      "text" : "Textbooks usually define a diphthong as a transition between two vowels within a syllable. A second common definition has manner going from at least one of the vowels [e], [ ] or [o] toɘ either of the two approximants, [j] or [ ]ɰᵘ . This second formulation indicates that there is a correlation between the manner going from vowel to approximant, openClose decreasing, frontBack tending towards the front or back, and place going from the pharynGlottals towards the PALs.\nIn the IHA, every manner⊗place has a frontBack⊗openClose. Extending the movement from vowel to approximant to the full set of manner values, we immediately get a broad class of phonotactics. So for example,\n[ pɬ ] would be a generalized diphthong, but [fl] would not be, though it is the onset version of [lf], which would be. [h k ]ᵅ ᶦ would be diphthongal, but [h k ] would not. The generalizedⁱ ᵊ diphthongs indicate that certain phonotactics, including conventional diphthongs, are language-neutral, and manifest basic physical constraints on the production of speech gestures. We interpret these constraints to underlie the notion of the sonority-based syllable."
    }, {
      "heading" : "4.2. The stochastic structure of the IHA phone string",
      "text" : "Let the values for manner be ordered as follows\n[closure ]<[plosive ]<[ fricative ]<[nasal]<[approximant ]<[vowel ] . Similarly, frontBack, openClose, and place are ordered or\npartially ordered: [ front ]<[ frontLike ]<[central ]>[backLike]>[back ] ; [close]<[closeLike]<[closeMid ]<[mid] , [mid ]<[openMid ]<[openLike ]<[open ] ; [ velar]<[uvular]<[pharyngeal ]<[epiglottal ]<[glottal ] , [palatAlveoLabial ]<[uvular ]<[pharyngeal ]<[epiglottal]<[glottal] .\nAll probability distributions include the null phone, which represents potential phone deletion.\nAn IHA phone string consists of at least 3 phones, starting with a closure and ending with a closure, including at least one non-closure, and not more than 2 consecutive closures. We will consider all repeating phones to have been collapsed into one phone. Then a syllable is defined as any sub-string from a local minimum to another local minimum according to the orders and partial orders stated above. When ambiguities such as [x ɘ ] arise from the partial orders, all the adjacentʂ phones are considered to be equivalent to one phone. In practice, they rarely occur.\nA syllable in a phone string shares a minimum with each of its neighbors. It is clear that a phone string can always be parsed into a syllable string. We define the maximum phone in each syllable as the nucleus, [νi ] . This means any phone, except for closures, may be considered syllabic5. By this definition, a syllable always has a nucleus. We refer to the phones from the syllable-initial minimum up to and including the nucleus as the onset, denoted\n[ ϕ0i ] ...[ ϕ j\ni ] ...[νi ] ,\nand the phones from the nucleus up to and including the next minimum as the rhyme, denoted\n[ν i ]... [ ϕi\nk ] ... [ ϕ i n ] .\nWe can define the stress of a syllable in terms of its duration, its loudness, the tone of its nucleus, and the number of constituent phones. We can then rank the syllables by this measure. We are not giving any specific definition for stress here, but any formulation must preserve the ranking of the syllables. The stressed syllables are then the local maxima, and the unstressed syllables are the local minima. For phonological considerations, the probability distribution of the syllables are assumed to be dependent only on higher linguistic levels. In the following, all probability distributions are conditioned on given syllables or words.\n5This definition easily accommodates consonant clusters in Tashlhiyt Berber and other languages as syllables[8].\n4 of 6\nFor stressed syllables, the probabilistic dependence of its phones is outwards from the nucleus:\np ( ϕ0i ∣ ϕ 1 i ) ... p ( ϕ j i ∣ϕ j+1 i )... p( ϕ m−1\ni ∣ν̂i ) ,\np ( ϕi n ∣ ϕi n−1 )... p( ϕi k ∣ ϕi k−1 )... p( ϕi m+1 ∣ν̂\ni ) .\nwhere we have indicated the stress on the nucleus. For unstressed syllables, the dependence is inwards to the nucleus:\np ( ϕm−1i ∣ϕ m−2 i ) ... p ( ϕ j+1 i ∣ϕ j i )... p( ϕ 1 i ∣ϕ 0 i ) ,\np ( ϕ i m+1 ∣ ϕi m+2 ) ... p ( ϕ i k−1 ∣ ϕi k )... p( ϕi n−1 ∣ ϕi n ) ,\np (ν̌ i ∣ ϕm−1i ϕi m+1 ) .\nFor the middling syllables, the dependence is left to right, or right to left, from the more stressed to the less stressed neighbor:\np (ν i ∣ϕm−1i )... p( ϕ j+1 i ∣ϕ j i ) ... p ( ϕ 1 i ∣ ϕ 0 i ) ,\np ( ϕi n ∣ ϕi n−1 )... p( ϕi k ∣ ϕi k−1 )... p( ϕi m+1 ∣ν\ni ) ;\np ( ϕ0i ∣ ϕ 1 i ) ... p ( ϕ j i ∣ϕ j+1 i )... p( ϕ m−1\ni ∣νi ) ,\np (ν i ∣ ϕi\nm+1 )... p( ϕi k−1 ∣ ϕi k ) ... p ( ϕi n−1 ∣ ϕi n ) .\nIf the first syllable in the phone string is not stressed, and precedes a second syllable which is stressed, then there is an edge effect with the string beginning serving as a virtual stressed syllable. In this case, it takes on the dependence structure of an unstressed syllable. The end of the string, on the other hand, is always virtually unstressed, and therefore the last rhyme always has left-to-right dependence. The units for modeling the random chain probability distributions are then naturally these onsets, rhymes and nuclei.\nThe generic language-neutral version of these models would have uniform distributions for all phones, subject only to the requirement that all possible phone sequences conform to the diphthongal constraint. Some phones may then be excluded from some of the distributions. However, they must also allow onsets and rhymes to be joined to form syllables and intervocalics. All phones of the generic models are joinable. As a result, some of the phones that are excluded by virtue of the diphthongal constraint are added back into the conditional distributions. We will call these joining probabilities.\nThe distributions in the prosodic subspace prescribe limits on the possible values in these dimensions. Within these limits, the distributions are uniform.\nGiven any phone string and the models, we can then compute its probability as a product of the conditional probabilities."
    }, {
      "heading" : "4.3. Modeling specific languages",
      "text" : "For a specific spoken language, the conditional probability distributions of the phones in onsets and rhymes would be far from uniform, yielding the distinctive syllables, words, and possibly higher linguistic units in that language. These nonuniform distributions can be taken to define that specific spoken language. The distributions in the phonetic subspace define the acknowledged phonemes of that language. In the prosodic subspace, at the lexical level, they prescribe how a\nparticular prosodic dimension, such as voicing or pitch, is or is not involved in contrasts."
    }, {
      "heading" : "4.4. Modeling phonological variations",
      "text" : "Textbook expositions of a language usually cite examples of words, phrases, etc. within the normative ranges for the prosodic variables, especially loudness, rate and pitch, for normal or formal discourse. Out of these normative ranges, however, especially for increasing loudness, rate and pitch, the performance of the speech gestures will increasingly be physically constrained. The perceived phone strings' trajectories in the phonetic subspace, will tend to straighten and shorten, realizing the higher conditional probabilities for these variations, conditioned on the louder, faster or screamed articulations. The same variations, even at lower probabilities in normal discourse, can seed long-term drifts in spoken languages.\nFor example, syncopy or apharesis can be accounted for by increasing null phone probability for unstressed syllables at higher speech rates. Epenthesis may happen when an extrastressed syllable realizes the increasing joining probability that results in an extra syllable. Lenition converts an intervocalic central close PAL plosive into the corresponding approximant as it gets pressed closer to the flanking vowels at higher rates. The much studied assimilation of /n/ into /m/ before /p/ or /b/ is a straightening tendency to stay on course towards the back side of frontBack. However, this assimilation is stopped in the word “pinball” due to the different stress pattern and the reversal of the dependency. The morphing of “nuclear” into “nucular” comes from extra emphasis on the first syllable resulting in epenthesis into a backLike vowel from a back velar, and the simultaneous straightening of the loop from the backLike /l/ to a front vowel and front approximant back to a central /r/ into the shorter step from /l/ to /r/.\nWith the phonetic-prosodic space and the stochastic structure we have proposed, we can now systematically model specific spoken languages and the phonological variations within them directly from data. The literature on the phonology of major spoken languages is substantial, and could be a resource for regularizing the machine learning of such models."
    }, {
      "heading" : "5. Conclusions",
      "text" : "We have formulated a 10-dimensional phonetic- prosodic space and a set of markers, the IHA, in this space. The formulation naturally leads to generalized diphthongs and a probabilistic structure on the phone strings that can be used to systematically define specific spoken languages, and to model their phonological variations.\nReferences [1] Chomsky, N. and Halle, M. The Sound Pattern of English.\nHarper & Row. New York: 1968 [2] http://en.wikipedia.org/wiki/International_Phonetic_Alphabet [3] Siniscalchi, S.M.; Dau-Cheng Lyu; Svendsen, T.; Chin-Hui Lee,\n\"Experiments on Cross-Language Attribute Detection and Phone Recognition With Minimal Target-Specific Training Data,\"\n5 of 6\nAudio, Speech, and Language Processing, IEEE Transactions on , vol.20, no.3, pp.875,887, March 2012 [4] Tsiang, E. “Maximally Informative Observables and Categorical Perception”, http://arxiv.org/abs/1212.5091 [5] http://en.wikipedia.org/wiki/File:IPA_chart_2005.png [6] Lisker, L. and Abramson, A.S., “A cross-language study of\nvoicing in initial stops: acoustical measurements”, Word, Vol. 20, 384-422 , 1964. [7] Olive, J.P., Greenwood, A. and Coleman, J.S., “Acoustics of American English speech: a dynamic approach”, pg. 207, Springer-Verlag, 1993. [8] Dell F. and Elmedlaoui M., “Syllables in Tashlhiyt Berber and in Moroccan Arabic”, International Handbook of Linguistics, Vol. 2, 2002\n6 of 6"
    } ],
    "references" : [ {
      "title" : "The Sound Pattern of English",
      "author" : [ "N. Chomsky", "M. Halle" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1968
    }, {
      "title" : "Experiments on Cross-Language Attribute Detection and Phone Recognition With Minimal Target-Specific Training Data",
      "author" : [ "Siniscalchi, S.M.", "Dau-Cheng Lyu", "Svendsen, T.", "Chin-Hui Lee" ],
      "venue" : "5 of 6  Audio, Speech, and Language Processing, IEEE Transactions on , vol.20, no.3, pp.875,887, March 2012",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A cross-language study of voicing in initial stops: acoustical measurements",
      "author" : [ "L. Lisker", "A.S. Abramson" ],
      "venue" : "Word, Vol. 20,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1964
    }, {
      "title" : "Acoustics of American English speech: a dynamic approach",
      "author" : [ "J.P. Olive", "A. Greenwood", "J.S. Coleman" ],
      "venue" : "pg. 207,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1993
    }, {
      "title" : "Syllables in Tashlhiyt Berber and in Moroccan Arabic",
      "author" : [ "F. Dell", "M. Elmedlaoui" ],
      "venue" : "International Handbook of Linguistics,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2002
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "From this space, a particular language selects a set of differences in these gestures to represent the bits required to encode its lexicon[1].",
      "startOffset" : 138,
      "endOffset" : 141
    }, {
      "referenceID" : 1,
      "context" : "This contrasts with other approaches aimed at constructing acoustic models broad enough to be adaptable to multiple languages[3]",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 2,
      "context" : "unvoiced distinction in different languages have varying voice onset times[6].",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 3,
      "context" : "The lateral alveolar, designated as belonging to a ɹ separate lateral manner in the IPA, are assigned the values mid backLike, because of their perceptual similarity to the openMid back vowel, especially in “dark” settings[7].",
      "startOffset" : 222,
      "endOffset" : 225
    }, {
      "referenceID" : 4,
      "context" : "This definition easily accommodates consonant clusters in Tashlhiyt Berber and other languages as syllables[8].",
      "startOffset" : 107,
      "endOffset" : 110
    } ],
    "year" : 2013,
    "abstractText" : "We formulate a phonetic-prosodic space based on attributes as perceptual observables, rather than articulatory specifications. We propose an alphabet as markers in the phonetic subspace, aiming for a resolution sufficient to support recognition of all spoken languages. The prosodic subspace is made up of directly measurable physical variables. With the proposed alphabet, traditional diphthongs naturally generalize to a broader class of language-neutral phonotactic constraints, indicating a correlation structure similar to that of the traditional sonority-based syllable. We define a stochastic structure on the phone strings based on this diphthongal constraint, and show how a specific spoken language can be defined as a specific set of probability distributions of this stochastic structure. Furthermore, phonological variations within a spoken language can be modeled as varying probability distributions restricted to the phonetic subspace, conditioned on different values in the prosodic subspace.",
    "creator" : "Writer"
  }
}