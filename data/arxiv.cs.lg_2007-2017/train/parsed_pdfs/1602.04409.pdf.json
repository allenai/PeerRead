{
  "name" : "1602.04409.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Convex Optimization For Non-Convex Problems via Column Generation",
    "authors" : [ "Julian Yarkony" ],
    "emails" : [ "julian.e.yarkony@gmail.com", "kamalika@cs.ucsd.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "We consider the general problem of approximating a complex structured object using a non-negative weighted combination of primitive structured objects under regularization [10, 11, 4]. Given a class of primitive structured objects (primitives) the set of complex objects is any weighted combination of the primitives or any weighted combination lying in a convex hull or cone[13]. Such problems are common throughout the machine learning literature and include, sparse coding [39], matrix approximation [44, 47], tensor approximation [22], mixture modeling, autoencoder modeling [4], etc. For example in 3-way tensor (three dimensional) approximation [2, 20, 26, 3] the set of rank-1, 3-way tensors are the primitives; the positive cone over rank-1 3-way tensors is the space optimized over.\nIn this paper we study two families of complex structured object approximation problems. The first (Family One) considers minimizing the L2 loss between a complex structured object versus its approximation with L1 regularization [18] encouraging the use of fewer primitives in the approximation. One example of a problem under Family One is lossy compressing an image so that the colors in the uncompressed image are similar in an L2 sense to the original [47].\nThe second family (Family Two) considers minimizing the cross entropy loss between a complex structured object corresponding to a probability distribution versus its approximation with L1 regularization encouraging a fewer number of components in the approximation. Sparsity in the L1 sense is achieved by encouraging more of the distribution to be explained by the uniform distribution or other white noise, or high entropy distribution. One example of a problem under Family Two is fitting a mixture of gaussians to describe a probability distribution. Here the high entropy distribution could be the maximum likelihood gaussian fit to all of the data.\nThe two families differ in the difficulty of approximation. Approximation in Family One is easier than approximation in Family Two, and thus Family One can be applied in larger scale applications. However the cross entropy loss of Family Two is a far more appropriate loss function in domains where probability distributions are being studied, which is common in machine learning.\nA key difficulty in complex structured object approximation is that the number of primitive structured objects may be infinite or at least exponential and prevent the construction of a quality approxi-\nar X\niv :1\n60 2.\n04 40\n9v 1\n[ cs\n.L G\n] 1\n4 Fe\nb 20\nmation. Much previous work in machine learning relies on non-convex methods such as Expectation Maximization (EM)[17, 43, 5, 48] which make greedy local moves to improve their approximation. Gradient descent also has great practical value especially in the domain of deep neural networks [45, 6, 42, 35]\nAn alternative approach to solving optimization problems is column generation [21, 7, 51]. Column generation is a powerful method generally used for massive scale integer programming problems originating in operations research such routing flights for large airlines. In this line of work one optimizes over the entire space of exponential number of variables in continuous space. The corresponding LP relaxation is a convex optimization and given a finite number of variables can be solved via interior point methods[13, 31] in polynomial time. However the set of all variables can not be enumerated much less used in optimization. Column generation operates by solving the optimization problem given a small subset of the variables. Next one or more variables are identified that will improve the objective if added to the subset under consideration and these are added to the subset under consideration. The optimization problem is then resolved. This continues until convergence.\nIn many optimization problems analysis of Lagrange multipliers allows for the exact or near exact computation of the best variable to add to the subset under consideration. Such analysis consists of solving a dramatically simpler version of the original problem which is often polynomial time solvable, or at least can be approximated to high accuracy in polynomial time perhaps with guarantees. In this paper we apply column generation to complex structure approximation in such a way as to circumvent many concerns pertaining to local optima. To our knowledge we are the first to apply column generation to structured object approximation."
    }, {
      "heading" : "1.1 Outline",
      "text" : "We now provide an outline of this paper. In Section 2 we formally introduce our two families of problems. Next in Section 3 we formulate the problems in terms of convex optimization over enormous or possibly infinite spaces of variables each corresponding to a primitive. Then in Section 4 we formulate optimization in the form of column generation where variables are generated as needed so as to make convex optimization feasible. In Section 5 we show how to identify primitives to add to consideration to improve the objective. We show examples derivations in the domain of tensors (Sections 5.1,5.2) and gaussian mixture models (Section 5.3).\nIn Section 6 we show experimental results on tensor problems. We show as a function of time and iteration the value of results from optimization for various size synthetic symmetric 3-way tensor problems. We use examples from both Family One and Two. In Section 7 we briefly discuss relevant papers and their relationship to our work. In Section 8 we discuss future work and extensions."
    }, {
      "heading" : "2 Formal Model",
      "text" : "We now formally discuss Family One and Two. Consider a structured object consisting of a finite number of values denoted T̄ . Here T̄ may correspond to a tensor or a probability distribution for example. We denote the vectorization of T̄ by column vector T . We denote the set of primitives that may be used to construct T asM and reference its members with m. Primitives may be rank-1 tensors or gaussian distributions in the previous examples. We define a matrix M by horizontally stacking the column vectors m ∈ M together. We define a non-negative weighted combination of the columns of M using a non-negative column vector w of length equal to |M|. In this document we study the two families in parallel since the approaches for approximation introduced in this paper are highly related. Also much of the notation used to discuss the two families is shared. Similarly many problems that are tackled using Family One have corresponding problems in Family Two."
    }, {
      "heading" : "2.1 Family One",
      "text" : "We define the optimal model according to L2 loss under L1 regularization as follows. We use t to denote transpose. We use ~̀ to define a positive constant column vector of length |M| where the constant value is ` ∈ R+.\nmin w≥0\n1 2 (T −Mw)t(T −Mw) + ~̀tw (1)"
    }, {
      "heading" : "2.2 Family Two",
      "text" : "We now define the optimal model according to cross entropy loss under L1 regularization as follows. We use log to denote the element-wise logarithm. We use ~̀a to define a positive constant column vector of length |M| where the constant value is `a ∈ R+. In addition ~̀a that has a single zero valued entry. This entry is associated with a special column m0 which corresponds to the uniform distribution or other white noise distribution. The entry of w corresponding to m0 is denoted w0. This is useful in modeling probability distributions where T is a complex probability distribution and each m is a primitive probability distribution. The corresponding optimization problem is written below.\nmin w≥0 wt1=1\n−T t log(Mw) + ~̀taw (2)\nSince the Eq 2 is non-decreasing in w0 we write Eq 2 as follows.\nEq 2 = min w≥0 wt1≤1 −T t log(Mw) + ~̀taw (3)"
    }, {
      "heading" : "3 Formulating Optimization",
      "text" : "We now consider the treatment of structured object approximation using the tools of quadratic programming (QP) for Family One and linear programming (LP) for Family Two."
    }, {
      "heading" : "3.1 Family One",
      "text" : "We now introduce a surrogate object K which corresponds to element-wise distance between T and Mw. We now write optimization as a quadratic program.\nEq 1 = min w≥0 K≥0\n1 2 KtK + ~̀tw (4)\ns.t. K ≥ T −Mw K ≥Mw − T\nHere Eq 4 corresponds to a convex quadratic program with an intractable |M| number of variables (one for each primitive) and a small number of constraints equal to 2|T |. We refer to Eq 4 as the primal problem for Family One and it is associated with a dual problem, which is also a convex quadratic program. This dual problem is written below using dual variables ψ, λ each of which are of length equal to |T |.\nEq 1 = max λ≥0 ψ≥0\n−λtλ 2 − ψtλ− ψ tψ 2 + T tλ− T tψ (5)\n~̀≥M tλ−M tψ The dual problem above is derived in Appendix A."
    }, {
      "heading" : "3.2 Family Two",
      "text" : "We now introduce a surrogate object K which corresponds to the element-wise logarithm of Mw.\nEq 2 = min w≥0\n1tw≤1 K≥0\nT tK + ~̀taw (6)\ns.t. −K ≤ log(Mw)\nSince the log function is concave we express it as the lower envelope of a set of affine upper bounds each of which is constructed via a first order Taylor expansion. We write this formally below.\nlog(y) = min η∈(0,∞) log(η) + (y − η) 1 η ∀y ∈ (0,∞) (7)\nWe now apply the lower envelope expression of log in Eq 7 to Eq 2. This produces the linear program below.\nEq 2 = min w≥0\n1tw≤1 K≥0\nT tK + ~̀taw (8)\ns.t. −K ≤ log(1η) + (Mw − 1η) η ∀η ∈ (0, 1]\nWe refer to Eq 8 as the primal problem for Family Two. The dual form of Eq 8 which is also a linear program, is written below using dual variables α and β. Here α is a scalar and β is associated with a unique vector of cardinality |T | for every η.\nmax α≥0 β≥0\n−α+ ∑\nη∈(0,1]\n(1− 1 log(η))tβη (9)\nT ≥ ∑\nη∈(0,1]\nβη\n~̀ a + 1α ≥M t ∑ η∈(0,1] 1 η βη\nThe dual problem above is derived in Appendix B."
    }, {
      "heading" : "4 Inference in General Terms",
      "text" : "Solving a quadratic or linear program is done by various methods such as interior points or simplex. However to employ them there must be a finite number of variables and constraints and our problems do not satisfy this criteria. To circumvent it we solve altered versions of the problems that consider only a subset of the constraints and variables. Variables or constraints are then added when and if they are violated or would improve the objective. We iterate between solving the altered problems and adding variables or constraints. The dual problems facilitate the addition of primal variables via analysis of the dual variables."
    }, {
      "heading" : "4.1 Family One",
      "text" : "We solve problems in Family One by solving the dual problem. In order to solve the dual problem we must identify a subset of the constraints such that when enforced no other constraints are violated. We build that subset denoted M̂ , which is called the working set, greedily. Here M̂ is initialized to the empty set or with any subset of the columns of M . We write the corresponding quadratic program below.\nEq 1 ≤ max λ≥0 ψ≥0\n−λtλ 2 − ψtλ− ψ tψ 2 + T tλ− T tψ (10)\n~̀≥ M̂ tλ− M̂ tψ\nFinding the most violated constraint corresponds to selecting a column of M to maximize the following for any λ and ψ. For ease of notation we define a term θ = λ− ψ.\nmax m∈M mt(λ− ψt) = max m∈M θtm (11)\nConsider a setting where we are able to identify the maximizing argument m for Eq 11. The corresponding column generation algorithm is written in Alg 1.\nAlgorithm 1 Dual Optimization M̂ ← {}\nwhile True do [λ, ψ]← Solve Eq 10 given M̂ θ ← λ− ψ [m]← maxm∈M θtm if θtm > ` then\nM̂ ← [M̂,m] else\nBREAK end if\nend while\nAt the termination of Alg 1 it is the case that Eq 10= Eq 1. Also at any time the primal variables w which are produced concurrently with λ, ψ describe a valid sub-optimal solution to Eq 1 when using interior points methods. At termination additional sparsity can be created by running least angle regression given the basis M̂ [18]."
    }, {
      "heading" : "4.2 Family Two",
      "text" : "Applying column generation to Family Two is challenging because there are an infinite number of variables in both the primal and dual. To meet this difficulty we store a subset of the constraints/variables to the primal and dual. We then produce a solution using only those constraints/variables. We then identify those violated constraints and add them to the constraint set. Constraints added in the primal correspond to new variables in the dual and similarly constraints added in the dual correspond to new variables in the primal.\nTo assist our discussion we index T as follows. We index T with x where Tx is the x’th value of vector T . We index K and Mw similarly. We use Ŝ to denote the working subset of x, η pairs and use M̂ to denote the working subset of M . We initialize M̂ with the uniform distribution/white noise/high entropy distribution m0 and initialize Ŝ to include [x, 1|T | ] for all x. The initial setting of Ŝ is an arbitrary choice that worked well in practice. We use m to index the columns of M . The corresponding primal and dual pair are below.\nmin w≥0\n1tw≤1 K≥0\nT tK + ~̀taw (12)\ns.t. −Kx ≤ log(η) + 1\nη (M̂w)x − 1 ∀[x, η] ∈ Ŝ\nEq 12 = max α≥0 β≥0 −α+ ∑ x,η∈Ŝ (1− log(η))βηx (13)\nTx ≥ ∑\n[ẋ,η]∈Ŝ s.t.ẋ=x\nβηx ∀x\n~̀ a + 1α ≥ ∑ x,η∈Ŝ mx 1 η βηx ∀m ∈ M̂\nFor ease of notation we introduce a vector θ defined as follows. θx = ∑\n[ẋ,η]∈Ŝ s.t.ẋ=x\n1 η βηx ∀x (14)\nConsider that we are able to identify violated constraints in the primal and dual. We then apply the following iteration. We solve Eq 13 which provides us with a solution to Eq 12 as well as Eq 13. We then identify violated constraints in the primal and dual. We then add those to the working sets Ŝ and M̂ . Finding constraints that are violated in the primal is trivial. For each Kx we minimize with respect to η the following Kx + 1η (M̂w)x + log(η) − 1. We take the derivative with respect to η then set the derivative equal to zero and finally solve for η.\n0 = −1 η2 (M̂w)x + 1 η (15)\n(M̂w)x = η\nWe then add the pair x, η to Ŝ if the constraint is violated. We write the corresponding optimization algorithm for Family Two in Alg 2. As in optimization in Family One, additional sparsity can be\nAlgorithm 2 Dual Optimization M̂ ← {m0}\nŜ ← {x, 1|T |} ∀x\nwhile True do [α, β, w,K]← Solve Eq 12/13 given M̂ and Ŝ for x do Ŝ ← Ŝ ∪ (x, (M̂w)x)\nend for [m]← maxm∈Mmtθ if θtm > `a + α then\nM̂ ← [M̂,m] end if if No constraints added this round then\nBREAK end if\nend while\ncreated by running least angle regression given M̂ [18]."
    }, {
      "heading" : "5 Identifying the Most Violating m or Highly Violated m",
      "text" : "The previous sections reference solving for the optimal m as the following optimization maxm∈M θ\ntm. The difficulty of solving for the optimalm is problem and problem instance specific. However these problems tend to relatively easy.\nConsider the problem of approximating a high rank 3-way tensor with a low rank tensor under L2 or cross entropy loss and L1 regularization. In this section we demonstrate that solving for m fits the optimal rank-1 tensor to a tensor described by the dual variables.\nIn the case of gaussian mixture models we demonstrate that solving for maxm∈M θtm corresponds to training the maximum likelihood gaussian under a weighting of the points described by θ which can be solved exactly in closed form. In fact θ is an unnormalized probability distribution. Here and in all Family Two examples θ must be normalized before applying inference and shifting the scaler outside of the max in maxm∈M θtm. Normalization is done by dividing each element of θ by 1tθ.\nIt should be observed that local optima are not very problematic when identifying m. One simply needs to find a local optima that has objective value greater than ` or `a + α should one exist. This results in Alg 1/ 2 changing the dual variables and hence θ and allow for one to try again to find a good local optimum on a different and perhaps easier problem. Finding a poor local optimum never results in an increase the objective for either Family One or Two. If a poor quality m is included early in the column generation process it is given zero weight w by the optimizer. Multiple local optima each corresponding to a violated constraint can be computed here and added to M̂ .\nWe consider three examples of finding the most violated m below and with additional examples in Appendix C,D."
    }, {
      "heading" : "5.1 Example, Family One: Symmetric 3-way Tensors",
      "text" : "We now study maxm∈M θtm in the domain where m corresponds to fitting a symmetric 3-way tensor. Consider the case that T corresponds to the vectorization of a symmetric 3-way tensor. Thus each column of M also corresponds to a symmetric 3-way tensor. We describe m using a vector v. Here v ∈ R(|T | 1 3 ) where v is a unit vector. The non-vectorized form of m is denoted m̄ and is indexed by i, j, k ∈ {0, 1, 2...N − 1}. We define m̄ in terms of v below.\nm̄ijk = vivjvk (16)\nWe write optimization below.\nmax m∈M θtm = max v\nvtv=1 ∑ ijk θijkvivjvk (17)\nThe projected gradient update for vi for all i is written below.\nv̇i ← vi + (stepsize) · ∑ jk θijkvjvk ∀i (18)\nv ← v̇ v̇tv̇\nRemarkably the seminal power iteration [37, 40] can be applied in place of projected gradient descent. Furthermore for tensors convergence of the power iteration to a local optima of the objective can be guaranteed [2]. Unlike in the case of 2-way tensor (matrix) global optimality is not guaranteed. The corresponding updates are written below.\nv̇i ← ∑ jk θijkvjvk (19)\nv ← v̇ v̇tv̇\nWe repeatedly update v until convergence. At termination we add m and −m to the working set M̂ when applying the power iteration. This is because the power iteration maximizes the magnitude of θtm without concern for the sign."
    }, {
      "heading" : "5.1.1 Note on Even way Tensors and Optimization",
      "text" : "For even way tensors (2-way,4-way,6-way...) an outer product does not produce all possible rank-1 tensors for M . It fails to create those constructed by an outer product then multiplied by −1. This can be seen by observing the following: multiplying the vector v by −1 does not flip the sign of all elements of m (it flips none) while this is achieved for odd way tensors (3-way, 5-way,7-way...). Thus at termination of optimization power iteration optimization we add m and −m to the working set M̂ .\nProjected gradient optimization must be similarly altered in the case of even way tensors. This is done by computing maxm∈M θtm and maxm∈M (−θ)tm and adding m,−m to M̂ corresponding to violated constraints.\nThis is important because Family One is restricted to have non-negative components w to fit in the standard form for quadratic programming however there need not be a model constraint that the weights are non-negative."
    }, {
      "heading" : "5.2 Example Family Two: Fitting a symmetric 3-way tensor defined by probability distribution",
      "text" : "We now study maxm∈M θtm in the domain where m corresponds to fitting a symmetric 3-way tensor in the setting of Family Two. We use the notation of Section 5.1. In this section we solve optimization relying on Jenson’s inequality [27, 28] to force m to correspond to a probability distribution at all times and avoid gradient descent. We write optimization below.\nmax v≥0 1tv=1 ∑ ijk θijkvivjvk (20)\nRecall that θ is non-negative and we normalize θ to sum to one and shift the normalization constant outside of the max. We now write optimization over the log of maxm∈M θtm.\nmax v≥0 1tv=1 log( ∑ ijk θijkvivjvk) (21)\nAs in EM methods for inference in probabilistic models we define a proposal probability distribution z indexed by ijk. We initialize z as follows to reflect the probability distribution θ though this initialization heuristic and random initialization of z is also valid.\nzijk ← θijk∑ i̇j̇k̇ θi̇j̇k̇\n(22)\nWe now multiply and divide by z as is standard in EM methods and apply Jenson’s inequality .\nEq 21 = max v≥0\n1tv=1\nlog( ∑ ijk zijk zijk θijkvivjvk) (23)\n≥ max v≥0\n1tv=1\n∑ ijk −zijk log zijk + zijk log θijk\n+ ∑ ijk zijk log vi + ∑ ijk zijk log vj + ∑ ijk zijk log vk\nWe now add a Lagrange multiplier τ to enforce that v sums to one. There will be no need to enforce non-negativity in optimization.\nmax v≥0 min τ∈(−∞,∞) τ(1− 1tv) + ∑ ijk −zijk log zijk (24)\n+ ∑ ijk zijk log θijk + ∑ ijk zijk log vi\n+ ∑ ijk zijk log vj + ∑ ijk zijk log vk\nWe now write the optimization with respect to vi. We now take derivative with respect to vi and set it equal to 0. Notice that vi is present in six terms in the derivative.\n0 = −τ + ∑ jk (zijk + zikj + zjik + zkij + zjki + zkji) 1 vi (25)\nτvi = ∑ jk (zijk + zikj + zjik + zkij + zjki + zkji)\nvi = 1\nτ ∑ jk (zijk + zikj + zjik + zkij + zjki + zkji)\nObserve that vi ∝ ∑ jk zijk. Since it is the case that 1 tv = 1. Then the following is true.\nτ = ∑ ijk zijk (26)\nThe optimizing updates for zijk set zijk proportional to θijkvivjvk based on the standard application of the tightest bound for Jenson’s inequality. Therefore the final updates are as follows.\nzijk ∝ θijkvivjvk (27) vi ∝ ∑ jk zijk\nWe repeatedly update z then v until convergence."
    }, {
      "heading" : "5.3 Example Gaussian Mixture models",
      "text" : "Consider that T represents samples drawn from a continuous probability distribution. Here T has one index for every one of N samples. Consider the problem of approximating T using a set of basis functionsM. Specifically consider the case of a gaussian basis on one dimension with fixed variance σ. Each m describes a particular gaussian via displaying the density at each data point x.\nLet us define each m via a unique mean µ. We use px to denote the spatial position of a given point x. We apply optimization via Jenson’s inequality as in Section 5.2 and using the corresponding notation. Consider a probability distribution z indexed by x. We now write the objective for optimization and apply Jenson’s inequality [28].\nRecall that θ is non-negative and we normalize θ to sum to one and shift the normalization constant outside of the max. We now write optimization over the log of maxm∈M θtm.\nmax m∈M log(θtm) = max m∈M log ∑ x (θxmx) (28)\n= max µ log ∑ x θx 1 σ √ 2π e− (px−µ)2 2σ2\n= max µ log( ∑ x zx zx θx 1 σ √ 2π e− (px−µ)2 2σ2 )\n≥ max µ ∑ x −zx log zx (29)\n+zx log( 1\nσ √ 2π θxe\n(px−µ)2\n2σ2 ) ∀[z ≥ 0, 1tz = 1]\nGiven z we can optimize with respect to µ. Optimizing with respect to z is done as is standard in Jensons’ inequality and is written below.\nzx ∝ θx 1\nσ √ 2π e−\n(px−µ)2\n2σ2 (30)\nSimilarly given µ we can optimize z. We now write optimization over µ given z.\nmax µ ∑ x −zx log zx (31)\n+zx log( 1\nσ √ 2π θxe\n−(px−µ)2\n2σ2 )\n= max µ ∑ x −zx log zx − zx log σ − zx 1 2 log(2π)\n+zx log θx + zx −(px − µ)2\n2σ2\nWe now take the derivative with respect to µ and set it equal to zero and thus we obtain a closed form expression of µ .\n0 = 2\n2σ2 ∑ x\nzx(µ− px) (32)∑ x zxpx = µ ∑ x\nzx∑ x zxpx = µ\nWe repeatedly update z then µ until convergence. We begin with a value for µ then solve for z where we initialize µ to correspond to a point px where x is selected with probability proportionate to θx. This is an initialization heuristic but attempts to place the gaussian in a region of high density of probability mass θ."
    }, {
      "heading" : "6 Experiments",
      "text" : "In this section we show the effectiveness of our approach for approximating large symmetric 3-way tensors in Family One and Two."
    }, {
      "heading" : "6.1 Family One",
      "text" : "We now study approximation to symmetric 3-way tensors for Family One to test the effectiveness of our approach. We use various sizes (30,35,40,45), constructed from a convex combination of three to ten unique rank-1 tensors with weights summing to one. We construct each unique rank-1 tensor by the triple outer product of a unique random unit vector. Each such vector has between six and fifteen non-zero elements. We inject noise describing between one and twenty percent of the tensor. For each problem instance we use four different L1 regularizers [0.1, 0.4, 0.7, 1.0]. We consider 9000 problems instances and we continue optimization for up to five minutes per instance after which termination is done after solving the current QP.\nIn Fig 1(a) we show the loss with respect to time averaged over non-terminated problem instances. This plot demonstrates that we rapidly produce low cost solutions.\nIn Fig 1(b) we show a scatter plot of total optimization time vs the loss at termination where each instance is a single data point. We show normalized and un-normalized values where normalization corresponds to subtracting the loss on the ground truth model (note that the ground truth model does not model the noise). This plot demonstrates that we are able to fit the tensors rapidly and are able to overfit which is important for an optimization approach.\nIn Fig 1(c) we show the derivative of the objective with respect to the columns in ground truth model basis. This plot demonstrates that little is gained by adding the ground truth basis tensors to M̂ if not present at convergence. From this we conclude that the power iteration is able to find good local optima of maxm∈M θtm."
    }, {
      "heading" : "6.2 Family Two",
      "text" : "We test the effectiveness our our approach on symmetric 3-way tensors in Family Two exactly as for Family One though on a smaller scale of problems. We use tensors of size twenty where the basis vectors used to construct the tensor have between four and six non-zero values. Each basis vector was non-negative and its elements summed to one instead of having unit norm as for Family One. We consider 750 problems instances and we continue optimization for up to five minutes per instance after which termination is done after solving the current LP.\nIn Fig 1(d) we show the loss with respect to time averaged over non-terminated problem instances. This plot demonstrates that we rapidly produce low cost solutions though not as quickly as in Family One.\nIn Fig 1(e) we show the corresponding plot for Family Two Tensors for 1(b). When normalizing we require the objective of the ground truth model. We add a small amount of probability mass (0.00001) to each entry of the ground truth model so that it does not have any zero values during cross entropy computation. Thus the sum of the values elements in the ground truth model is greater than one. This plot demonstrates that we are able to fit the tensors rapidly and are able to overfit which is important for an optimization approach.\nIn Fig 1(f) we show the corresponding plot for Family Two Tensors for 1(c). This plot demonstrates that little is gained by adding the ground truth basis tensors to M̂ if not present at convergence. From this we conclude that the our Jenson’s inequality based optimization is able to find good local optima of maxm∈M θtm."
    }, {
      "heading" : "7 Literature Review",
      "text" : "Our work can be positioned at the intersection of two well studied areas: 1) column generation for combinatorial optimization and 2) efficient representation of data notably with regards to low rank tensor decompositions. In this section we discuss some of the related work in these two vast areas in the context of our work."
    }, {
      "heading" : "7.1 Column Generation",
      "text" : ""
    }, {
      "heading" : "7.1.1 The Cutting Stock Problem",
      "text" : "Our work is intimately related to the classical work of [21] on the cutting stock problem which originated in the paper industry. In [21] the authors determine how to satisfy a set of demands for rolls of paper of various widths given rolls of paper of longer widths while minimizing the scrap. In this line of work there are a massive number of variables where each variable corresponds to a different way to cut a roll. Each way of cutting a roll is called a pattern. The number of possible patterns can grow exponentially in the number of unique widths demanded.\nThe following very small scale example illustrates the concept of a pattern. Consider cutting a 100 meter wide roll of paper in the context of demands for 30 and 45 meter long rolls. Two example patters would be (1) cut the roll into three 30 meter long rolls with 10 meters of scrap (2) cut the roll into one 45 meter long roll and one 30 meter long roll with 25 meters of scrap. Optimization is formulated as an integer linear program (ILP) that is NP-Hard. The value of a variable in a solution to the ILP denotes the number of rolls that are cut using the pattern associated with that variable. The integer linear program is relaxed to a linear program allowing for a fractional number of each pattern to be used. Optimization is initialized with a fixed number of patterns sufficient to satisfy all orders but likely with excess waste. Optimization proceeds by solving the linear program given the current set of patterns, followed by generating new patterns. This is repeated until convergence.\nFinding the optimal pattern to add corresponds to solving a knapsack problem[30, 29]. The knapsack problem is NP-Hard however it can be approximated very well using a dynamic programming based polynomial time approximation scheme. This can be understood as maximizing the reward obtained by cutting a single roll by choosing a single pattern given reward values associated with each width of paper. Here the rewards correspond to the value of the dual variables in the dual linear program. In this manner the problem of optimizing over a massive number of patterns is reduced to finding a single pattern.\nIn our applications we approximate high rank tensors with low rank tensors. When generating new rank-1 tensors to add to our set of primitives (working set) we construct a tensor to maximize sum of the element-wise products between it and another tensor corresponding to the dual variables. In this manner the problem of optimizing over an infinite number of rank-1 tensors is reduced to iteratively fitting a rank-1 tensor. Like the knapsack problem, fitting the optimal rank-1 tensor is known to be NP-Hard [24] however like the knapsack problem well studied optimization schemes exist [32].\nProducing integer solutions in the cutting stock may be done heuristically by greedily rounding up fractional values to integer values or by a principled but expensive branch and price algorithm[7]. Fortunately our use of L1 regularization does not require integer solutions so branch and price is not needed. Variants of the cutting stock problem include enforcing a hard constraint on the maximum number of patterns used and this can be interpreted as L0 regularization and is akin to our L1 regularization."
    }, {
      "heading" : "7.1.2 Marginals in Graphical Models",
      "text" : "Our work is related to the work of [9] (which is extended in [34]). While column generation is not explicitly used or mentioned, their approach is very similar to column generation and their problem has a cross entropy loss like in Family Two. In this paper the authors attack the problem of computing the marginal distributions of variables in a Markov random field (MRF). This is done by minimizing standard Bethe-style convex variational objective. The authors introduce an algorithm to construct a probability distribution over solutions and adds solutions to the probability distribution greedily. At each step of the algorithm a new solution is generated and added to the probability distribution.\nChoosing the optimal solution to add corresponds to MAP inference which is an integer program and is often NP-Hard. The potentials for MAP inference are a function of the current probability distribution and the potentials that define the original MRF, unlike the dual variables in our methods. MAP Inference can be challenging because problem structure such as sub-modularity [14] which allows for efficient MAP inference is lost when present in the original MRF however LP relaxation techniques[33, 46] can be used to produce efficient approximate solutions. Once computed a line search is used to determine how much to weight this solution versus the other solutions. The determination of the optimal solution via MAP inference can be interpreted as adding a primal variable (primitive) to the working set while the line search is a greedy move towards optimizing the cross entropy loss."
    }, {
      "heading" : "7.1.3 Generating Multiple Primal Variables",
      "text" : "Finding the optimal primitive variable to add in our applications is NP-Hard but multiple local optima can be generated concurrently given the same set of dual variables with each optima being computed on a separate CPU. This can be contrasted to the work of [50] where large numbers of variables each of which improves the objective are generated in polynomial time concurrently as part of a common operation. In [50], tracking thousands of objects in video is formulated as an integer program which is relaxed to a linear program where each variable corresponds to an entire track of an object. Finding the optimal variable to add to the working set corresponds to solving a dynamic program which is polynomial time solvable to global optimality. However dynamic programming provides not only the optimal solution but the optimal solution passing through each position in space-time for free. Each of these or a subset of the most violated can be added to the working set. [50] adds one thousand such tracks per iteration allowing for very fast inference. Surprisingly solving the LP is not hindered by the addition of large numbers of tracks by huge working sets. In our approach computing multiple local optima allows for some of the benefits of multiple solutions per dual solution it does not take advantage of the scale for easy generation of variables of [50]."
    }, {
      "heading" : "7.2 Low Rank Tensor Decompositions",
      "text" : ""
    }, {
      "heading" : "7.2.1 Orthogonal Tensor Decompositions",
      "text" : "In [2], the authors attack the problem of latent variable modeling leveraging modeling assumptions about how the data was generated. If these assumptions are obeyed then they can guarantee globally optimal recovery of the data generating model. Inference consists of computing a low rank approx-\nimation of an orthogonal tensor generated using the triple co-occurrence of moments in the data set (plus whitening). The work of [2] is associated with powerful statistical guarantees. However if the modeling assumptions are not obeyed by the data generation process the recovered solution which is intended to describe a mixture probability of distributions need not produce probability distributions though these can be rounded to probability distributions to produce approximate solutions.\nIn contrast to the work of [2] we focus on optimization, and we ensure that the outputted representation follows the rules that define the primitives. Like [2] we use the power iteration to generate primitives though only in the context of creating a rank-1 representations of the dual variables."
    }, {
      "heading" : "7.3 Gradient Descent and Alternating Least Squares",
      "text" : "Two common and powerful approaches for tensor decomposition are gradient descent [1] and alternating least squares (ALS)[16, 32, 23]. Gradient decent optimizes all modes of the tensor concurrently and converges to a local optima of the objective. Alternatively ALS solves for one of the modes of the tensor at a time keeping the other modes fixed. This is achieved via least squares and is coordinate-wise optimal. ALS is not guaranteed to achieve global optimality but in practice is highly successful and benefits from the lack of a need for a step size as in gradient methods[32].\nIn Appendix C, D we show coordinate-wise updates for Family One and Family Two respectively while gradient descent updates are present in Section 5.1, and in Appendix C. Since we compute only a single, rank-1 tensor at a time our method does not solve least squares problems during coordinate-wise updates. Instead simpler closed form updates exist; for Family One this involve computing a gradient and obtaining an exact solution for the mode.\nA major difference between the gradient descent approaches, and ALS in contrast to our work is the form of regularization. For gradient descent and ALS approaches the number of rank-1 terms is fixed in the beginning of optimization while in our approach it is not. Thus gradient descent and ALS have implicit L0 regularization. We have no ability to enforce an L0 norm in our method and must be content to approximate it with an L1 norm.\nAnother point of difference is that our approach generates new rank-1 tensors but does not adapt those that have already been produced. Gradient descent and ALS continuously alter the tensors they are operating on while ours adds tensors to the working set during optimization leaving the previous ones fixed. Our approach can be used in complementary way to ALS and gradient descent so as to update the rank-1 tensors in the working set perhaps speeding up inference."
    }, {
      "heading" : "7.3.1 Constrained Tensor Decomposition via the Alternating Direction of Multipliers Method",
      "text" : "In [36] the authors employ the alternating direction of multipliers method [12] (ADMM) to break down constrained tensor factorization problems into separate problems (sub-problems) that are enforced to have a common solution using Lagrange multipliers. Solutions to the sub-problems are much easier to compute than solving the original constrained problem though they must be re-solved many times. This difficulty can be reduced by leveraging parallel computation.\nLagrange multipliers are used to couple the problems together and operate in such a way as to not make the unconstrained problems more difficult to solve. The work of [36] is extended in [25] where it is demonstrated to produce state of the art results for non-negative tensor factorization, and other problems. Optimization is guaranteed to converge to a stationary point of the constrained objective. Our approach can be used in a complementary way to [36] by using ADMM during the stage of generating new tensors. However unlike in [36, 25] only a single rank-1 tensor would be constructed at a time."
    }, {
      "heading" : "8 Conclusions and Future Work",
      "text" : "In this document we apply column generation to approximating complex structured objects via a set of primitive structured objects under two families of loss functions with L1 regularization encouraging the use of few structured primitive objects. We attack approximation using convex optimization over an infinite number of variables each of which are generated on demand using the corresponding\ndual problem. We apply our approach to producing low rank approximations to large 3-way tensors. Our work provides a broad domain for extensions and we note a few below.\nIncreasing Scale: Solving much larger problems will require clever use of LP and QP solvers. One can attack this by using sub-optimal dual solutions to compute violated constraints. Similarly one can use the LP and QP solvers in ways that do not restart optimization from scratch each time called or which forget constraints that are not active yet slow optimization.\nDiverse Solutions: Additional speed may be obtained by applying sampling and deterministic sampling approaches such as [41, 49, 8, 34] to add diverse columns to M̂ after each solution to the LP or QP. This would be useful in a domain where solving the QP/LP is significantly more time intensive than maximizing θtm.\nSize Reduction: Applications of tensor size reduction methods as used in [2] may prove to be invaluable to extending our work to much larger problems especially if the corresponding statistical guarantees can be preserved. This is challenging as the constraints on elementsm change in the new reduced size space.\nL0 regularization: The application of branch and bound/price techniques [7] after the construction of M̂ using our approach with L1 regularization, may prove useful for optimizing under L0 regularization.\nPriors and Mixture Modeling: We did not explore the use of priors over the variance of gaussians for learning mixture of gaussians and optimization over the variance. We suspect this can be accomplished by altering the L1 regularizer to penalize lower variance gaussians and enforcing this via the conjugate prior for the gaussian, which is the normal Wishart prior [38]. This may be useful in the domain of quantum chemistry.\nSparse Coding/Topic Models Applying the column generation approach to the Family One problem of sparse coding requires introducing a term to limit the total number of primitives used across documents (data samples). This can be done in an L1 sense by associating each primitive with variable in the QP that is associated with the maximum amount this primitive is used by any document. L1 regularization is then applied to these variables in the QP with the aim of limiting the total number of primitives used across documents. A separate L1 regularization is used to encourage each document to have a sparse representation. The Family Two version of this would be a topic model.\nDeep Relationships: We suspect that there is a relationship explaining the effectiveness of large single hidden layer neural networks studied in [15], using our work and boosting [19]."
    }, {
      "heading" : "A Family One: Dual Derivation",
      "text" : "We now derive the dual problem of Family One given the primal problem of Family One. We begin with the Family One objective function after the introduction of the surrogate complex structured object K.\nmin w≥0 K≥0\n1 2 KtK + ~̀tw (33)\ns.t. K ≥ T −Mw K ≥Mw − T\nNext we replace the constraints with Lagrange Multipliers.\nEq 4 = min w≥0 K≥0 max λ≥0 ψ≥0\n1 2 KtK + ~̀tw (34)\n+ψt(−T +Mw −K) +λt(T −Mw −K)\nSince Eq 4 is a convex quadratic program we are able to flip the order of the min and the max without altering the value of the objective.\nEq 4 = max λ≥0 ψ≥0 min w≥0 K≥0\n1 2 KtK + ~̀tw (35)\n+ψt(−T +Mw −K) +λt(T −Mw −K)\nWe now take the first derivative of Eq 35 with respect to K and then solve for K in terms of the dual variables λ and ψ.\n0 = K − ψ − λ (36) K = ψ + λ\nGiven the closed form solution for K in Eq 36 we write Eq 4 with K replaced by ψ + λ.\nEq 4 = max λ≥0 ψ≥0 min w≥0\n1 2 (λ+ ψ)t(λ+ ψ) + ~̀tw (37)\n+ψt(−T +Mw − λ− ψ) +λt(T −Mw − λ− ψ)\nWe now group the terms by primal variable.\nEq 4 = max λ≥0 ψ≥0 min w≥0\n−1 2 (λ+ ψ)t(λ+ ψ) + λtT − ψtT (38)\n+(~̀− λtM + ψtM)w\nWe now convert the Lagrangian to a convex quadratic program by converting the primal variables w into constraints.\nEq 4 = max λ≥0 ψ≥0\n−λtλ 2 − ψtλ− ψ tψ 2 + T tλ− T tψ (39)\n~̀≥M tλ−M tψ"
    }, {
      "heading" : "B Family Two: Dual Derivation",
      "text" : "We now derive the dual problem of Family Two given the primal problem of Family Two. We begin with the Family Two objective function after the introduction of the surrogate complex structured object K and the replacement of log by a concave envelope of affine functions.\nmin w≥0\n1tw≤1 K≥0\nT tK + ~̀taw (40)\ns.t. −K ≤ log(1η) + (Mw − 1η) η ∀η ∈ (0, 1]\nWe now insert Lagrange multipliers α and β.\nEq 40 = min w≥0 K≥0 max α≥0 β≥0 T tK + ~̀taw (41)\nα(1tw − 1)∑ η∈(0,1] βtη(−K − 1 η Mw + 1− 1 log(η))\nWe now group by primal variable.\nEq 40 = min w≥0 K≥0 max α≥0 β≥0 ∑ η∈(0,1] βtη(1− 1 log(η))− α (42)\n(T t − ∑\nη∈(0,1]\nβtη)K\n(~̀ta + α1 t − ∑ η∈(0,1] 1 η βtηM)w\nWe now convert primal variables to hard constraints.\nEq 40 = max α≥0 β≥0 ∑ η∈(0,1] βtη(1− 1 log(η))− α (43)\nT t − ∑\nη∈(0,1]\nβtη ≥ 0\n~̀t a + α1\nt − ∑\nη∈(0,1]\n1 η βtηM ≥ 0\nWe now take the transpose of our objective and move terms.\nEq 40 = max α≥0 β≥0\n−α+ ∑\nη∈(0,1]\n(1− 1 log(η))tβη (44)\nT ≥ ∑\nη∈(0,1]\nβη\n~̀ a + 1α ≥M t ∑ η∈(0,1] 1 η βη"
    }, {
      "heading" : "C Optimizing θtm, Family One: Non-Symmetric Tensors",
      "text" : "We now study maxm∈M θtm in the domain where m corresponds to fitting a non-symmetric 3-way tensor. Thus each column of M also corresponds to a non-symmetric 3-way tensor. We describe m using vectors unit vectors va, vb, vc which are indexed by i, j, k respectively. The non-vectorized form of m is denoted m̄ and defined below.\nm̄ijk = v a i v b jv c k (45)\nWe write optimization below.\nmax m∈M θtm = max va, vb vc\nvatva=1 vbtvb=1 vctvc=1\n∑ ijk θijkv a i v b jv c k (46)\nThe projected gradient update for vai for all i is written below.\nv̇ai ← vai + stepsize ∑ jk θijkv b jv c k ∀i (47)\nva ← v̇ a\nv̇atv̇a\nAs an alternative or supplement to gradient descent one can perform coordinate-wise updates which update one of the vectors given the other vectors. Consider optimizing va given vb and vc. Observe that gradient of ∑ ijk θijkv a i v b jv c k with respect to v\na has no dependency on va and va is restricted to be unit norm. Thus the optimal va is thus simply the gradient of ∑ ijk θijkv a i v b jv c k with respect to v a properly scaled to be a unit vector. Thus we can write the update for va as the following expression.\nv̇ai ← ∑ jk θijkv b jv c k ∀i (48)\nva ← v̇ a\nv̇atv̇a\nOptimization over va, vb, vc proceeds by cycling through a,b,c optimizing va, then vb, then vc. Higher order generalizations of the singular value decomposition may find use here allowing for example va,vb to be updated given vc."
    }, {
      "heading" : "D Optimizing θtm, Family Two: Non-Symmetric Tensors",
      "text" : "We now consider updates for 3-way non-symmetric tensors in Family Two using the notation of Section C. Recall that θ is non-negative and that we normalize θ to sum to one and shift the normalization constant outside of the max. We now write optimization over the log of maxm∈M θtm.\nmax va≥0, vb≥0, vc≥0\n1tva=1, 1tvb=1, 1tvc=1\nlog( ∑ ijk θijkv a i v b jv c k) (49)\nAs in EM methods for inference in probabilistic models we define a proposal probability distribution z indexed by ijk. We initialize z as follows to reflect the probability distribution θ though this initialization is heuristic and random initialization of z is also valid.\nzijk ← θijk∑ i̇j̇k̇ θi̇j̇k̇\n(50)\nWe now multiply and divide by z as is standard in EM methods and apply Jenson’s inequality .\nEq 49 = max va≥0, vb≥0, vc≥0\n1tva=1, 1tvb=1, 1tvc=1\nlog( ∑ ijk zijk zijk θijkv a i v b jv c k) (51)\n≥ max va≥0, vb≥0, vc≥0\n1tva=1, 1tvb=1, 1tvc=1\n∑ ijk −zijk log zijk + zijk log θijk\n+ ∑ ijk zijk log v a i + ∑ ijk zijk log v b j + ∑ ijk zijk log v c k\nWe now add a Lagrange multiplier τa, τa, τ c, to enforce that va, vb, vc sum to one respectively. There will be no need to enforce non-negativity in optimization.\nmax va≥0 vb≥0 vc≥0 min τa∈(−∞,∞) τb∈(−∞,∞) τc∈(−∞,∞)\nτa(1− 1tva) + τ b(1− 1tvb) + τ c(1− 1tvc) + ∑ ijk −zijk log zijk (52) +zijk log θijk + ∑ ijk zijk log v a i + ∑ ijk zijk log v b j + ∑ ijk zijk log v c k\nWe now write optimization with respect to vai . We now take derivative with respect to v a i and set it equal to 0.\n0 = −τa + ∑ jk (zijk) 1 vai (53)\nτvi = ∑ jk zijk\nvai = 1\nτa ∑ jk zijk\nObserve that vai ∝ ∑ jk zijk. Since it is the case that 1 tva = 1. Then the following is true.\nτa = ∑ ijk zijk (54)\nThe optimizing updates for zijk set zijk proportional to θijkvai v b jv c k based on the standard application of the tightest bound for Jenson’s inequality. Updates for vbj and v c k follow the same form as v a i . Therefore the final updates are as follows.\nzijk ∝ θijkvai vbjvck (55) vai ∝ ∑ jk zijk\nvbj ∝ ∑ ik zijk\nvck ∝ ∑ ij zijk\nWe repeatedly update z then v until convergence."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "We apply column generation to approximating complex structured objects via a<lb>set of primitive structured objects under either the cross entropy or L2 loss. We<lb>use L1 regularization to encourage the use of few structured primitive objects.<lb>We attack approximation using convex optimization over an infinite number of<lb>variables each corresponding to a primitive structured object that are generated<lb>on demand by easy inference in the Lagrangian dual. We apply our approach to<lb>producing low rank approximations to large 3-way tensors.",
    "creator" : "LaTeX with hyperref package"
  }
}