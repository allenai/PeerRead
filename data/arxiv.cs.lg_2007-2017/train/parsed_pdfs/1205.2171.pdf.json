{
  "name" : "1205.2171.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "A Generalized Kernel Approach to Structured Output Learning",
    "authors" : [ "Hachem Kadri", "Mohammad Ghavamzadeh", "Philippe Preux" ],
    "emails" : [ "hachem.kadri@lif.univ-mrs.fr", "mohammad.ghavamzadeh@inria.fr", "philippe.preux@inria.fr" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "In many practical problems such as statistical machine translation (Wang & Shawe-Taylor, 2010) and speech recognition or synthesis (Cortes et al., 2005), we are faced with the task of learning a mapping between\nProceedings of the 30 th International Conference on Machine Learning, Atlanta, Georgia, USA, 2013. JMLR: W&CP volume 28. Copyright 2013 by the author(s).\nobjects of different nature that each can be characterized by complex data structures. Therefore, designing algorithms that are sensitive enough to detect structural dependencies among these complex data is becoming increasingly important. While classical learning algorithms can be easily extended to complex inputs, more refined and sophisticated algorithms are needed to handle complex outputs. In this case, several mathematical and methodological difficulties arise and these difficulties increase with the complexity of the output space. Complex output data can be divided into three classes: 1) Euclidean: vectors or real-valued functions; 2) mildly non-Euclidean: points on manifolds and shapes; and 3) strongly non-Euclidean: structured data like trees and graphs. The focus in the machine learning and statistics communities has been mainly on multi-task learning (vector outputs) and functional data analysis (functional outputs) (Caruana, 1997; Ramsay & Silverman, 2005), where in both cases output data reside in a Euclidean space, but there has also been considerable interest in expanding general learning algorithms to structured outputs.\nOne difficulty encountered when working with structured data is that usual Euclidean methodology cannot be applied in this case. Reproducing kernels provide an elegant way to overcome this problem. Defining a suitable kernel on the structured data allows to encapsulate the structural information in a kernel function and transform the problem to a Euclidean space. Two different, but closely related, kernel-based approaches for structured output learning can be found in the literature (Bakir et al., 2007): kernel dependency estimation (KDE) and joint kernel maps (JKM). KDE is a regression-based approach that was first proposed by Weston et al. (2003) and then reformulated by Cortes et al. (2005). The idea is to define a kernel on the output space Y to project the structured output to ar X\niv :1\n20 5.\n21 71\nv2 [\nst at\n.M L\n] 1\n5 Ju\nl 2 01\na real-valued reproducing kernel Hilbert space (RKHS) FY , and then perform a scalar-valued kernel ridge regression (KRR) between the input space X and the feature space FY . Having the regression coefficients, the prediction is obtained by computing the pre-image from FY . On the other hand, the JKM approach is based on joint kernels, which are nonlinear similarity measures between input-output pairs (Tsochantaridis et al., 2005; Weston et al., 2007). While in KDE separate kernels are used to project input and output data to two (possibly different) feature spaces, the joint kernel in JKM maps them into a single feature space, which then allows us to take advantage of our prior knowledge on both input-output and output correlations. However, this improvement requires an exhaustive pre-image computation during training, a problem that is encountered by KDE only in the test phase. Avoiding this computation during training is an important advantage of KDE over JKM methods.\nIn this paper, we focus on the KDE approach to structured output learning. The main contributions of this paper can be summarized as follows: 1) Building on the works of Caponnetto & De Vito (2007) and Brouard et al. (2011), we propose a more general KDE formulation (prediction and pre-image steps) based on operator-valued (multi-task) kernels instead of scalar-valued ones used by the existing methods (Sec. 3). This extension allows KDE to capture the dependencies between the outputs as well as between the input and output variables, which is an improvement over the existing KDE methods that fail to take into account these dependencies. 2) We also propose a variant (generalization) of the kernel trick to cope with the technical difficulties encountered when working with operator-valued kernels (Sec. 3). This allows us to (i) formulate the pre-image problem using only kernel functions (not feature maps that cannot be computed explicitly), and (ii) avoid the computation of the inner product between feature maps after being modified with an operator whose role is to capture the structure of complex objects. 3) We then introduce a novel family of operator-valued kernels, based on co-\nvariance operators on RKHSs, that allows us to take full advantage of our KDE formulation. These kernels offer a simple and powerful way to address the main limitations of the original KDE formulation, namely the decoupling between outputs in the image space and the inability to use a joint feature space (Sec. 4). 4) We show how the pre-image problem, in the case of covariance and conditional covariance operator-valued kernels, can be expressed only in terms of input and output Gram matrices, and provide a low rank approximation to efficiently compute it (Sec. 4). 5) Finally, we empirically evaluate the performance of our proposed KDE approach and show its effectiveness on three structured output prediction problems involving numeric and non-numerical outputs (Sec. 6). It should be noted that generalizing KDE using operator-valued kernels was first proposed in Brouard et al. (2011). The authors have applied this generalization to the problem of link prediction which did not require a preimage step. Based on this work, we discuss both the regression and pre-image steps of operator-valued KDE, propose new covariance-based operator-valued kernels and show how they can be implemented efficiently."
    }, {
      "heading" : "2. Preliminaries",
      "text" : "In this section, we first introduce the notations used throughout the paper, lay out the setting of the problem studied in the paper, and provide a high-level description of our approach to this problem. Then before reporting our KDE formulation, we provide a brief overview of operator-valued kernels and their associated RKHSs. To assist the reading, we list the notations used in the paper in Table 1."
    }, {
      "heading" : "2.1. Problem Setting and Notations",
      "text" : "Given (xi, yi) n i=1 ∈ X × Y, where X and Y are the input and structured output spaces, we consider the problem of learning a mapping f from X to Y. The idea of KDE is to embed the output data using a mapping Φl between the structured output space Y and a Euclidean feature space FY defined by a scalar-valued\nkernel l. Instead of learning f in order to predict an output y for an input x, the KDE methods first learn the mapping g from X to FY , and then compute the pre-image of g(x) by the inverse mapping of Φl, i.e., y = f(x) = Φ−1l ( g(x) ) (see Fig. 1). All existing KDE methods use ridge regression with a scalarvalued kernel k on X × X to learn the mapping g. This approach has the drawback of not taking into account the dependencies between the data in the feature space FY . The variables in FY can be highly correlated, since they are the projection of yi’s using the mapping Φl, and taking this correlation into account is essential to retain and exploit the structure of the outputs. To overcome this problem, our KDE approach uses an operator-valued (multi-task) kernel to encode the relationship between the output components, and learns g in the vector-valued (function-valued) RKHS built by this kernel using operator-valued kernel-based regression. The advantage of our formulation is that it allows vector-valued regression to be performed by directly optimizing over a Hilbert space of vector-valued functions, instead of solving independent scalar-valued regressions. As shown in Fig. 1, the feature space induced by an operator-valued kernel, FXY , is a joint feature space that contains information of both input space X and output feature space FY . This allows KDE to exploit both the output and input-output correlations. The operator-valued kernel implicitly induces a metric on the joint space of inputs and output features, and then provides a powerful way to devise suitable metrics on the output features, which can be changed depending on the inputs. In this sense, our proposed method is a natural way to incorporate prior knowledge about the structure of the output feature space while taking into account the inputs."
    }, {
      "heading" : "2.2. Operator-valued Kernels and Associated RKHSs",
      "text" : "We now provide a few definitions related to operatorvalued kernels and their associated RKHSs that are used in the paper (see (Micchelli & Pontil, 2005; Caponnetto et al., 2008; Álvarez et al., 2012) for more details). These kernel spaces have recently received more attention, since they are suitable for leaning in problems where the outputs are vectors (as in multitask learning (Evgeniou et al., 2005)) or functions (as in functional regression (Kadri et al., 2010)) instead of scalars. Also, it has been shown recently that these spaces are appropriate for learning conditional mean embeddings (Grunewalder et al., 2012). Let L(FY) be the set of bounded operators from FY to FY .\nDefinition 1 (Non-negative L(FY)-valued kernel) A non-negative L(FY)-valued kernel K is an operator-\nvalued function on X × X , i.e., K : X × X → L(FY), such that:\ni. ∀xi, xj ∈ X , K(xi, xj) = K(xj , xi)∗ (∗ denotes the adjoint),\nii. ∀m ∈ N∗+, ∀x1, . . . , xm ∈ X , ∀ϕi, ϕj ∈ FY m∑\ni,j=1\n〈K(xi, xj)ϕj , ϕi〉FY ≥ 0.\nThe above properties guarantee that the operatorvalued kernel matrix K = [ K(xi, xj) ∈ L(FY) ]n i,j=1 is positive definite. Given a non-negative L(FY)-valued kernel K on X × X , there exists a unique RKHS of FY -valued functions whose reproducing kernel is K.\nDefinition 2 (FY -valued RKHS) A RKHS FXY of FY -valued functions g : X → FY is a Hilbert space such that there is a non-negative L(FY)-valued kernel K with the following properties:\ni. ∀x ∈ X , ∀ϕ ∈ FY K(x, ·)ϕ ∈ FXY ,\nii. ∀g ∈ FXY , ∀x ∈ X , ∀ϕ ∈ FY 〈g,K(x, ·)ϕ〉FXY = 〈g(x), ϕ〉FY .\nEvery RKHS FXY of FY -valued functions is associated with a unique non-negative L(FY)-valued kernel K, called the reproducing kernel."
    }, {
      "heading" : "3. Operator-valued Kernel Formulation of Kernel Dependency Estimation",
      "text" : "In this section, we describe our operator-valued KDE formulation in which the feature spaces associated to\ninput and output kernels can be infinite dimensional, contrary to Cortes et al. (2005) that only considers finite feature spaces. Operator-valued KDE is performed in two steps:\nStep 1 (kernel ridge) Regression: We use operator-valued kernel-based regression and learn the function g in the FY -valued RKHS FXY from the training data ( xi,Φl(yi) )n i=1 ∈ X × FY , where Φl is the mapping from the structured output space Y to the scalar-valued RKHS FY . Similar to other KDE formulations, we consider the following regression problem:\narg min g∈FXY n∑ i=1 ‖g(xi)− Φl(yi)‖2FY + λ‖g‖ 2, (1)\nwhere λ > 0 is a regularization parameter. Using the representer theorem in the vector-valued setting (Micchelli & Pontil, 2005), the solution of (1) has the following form1:\ng(·) = n∑\ni=1\nK(·, xi)ψi , (2)\nwhere ψi ∈ FY . Using (2), we obtain an analytic solution for the optimization problem (1) as\nΨ = (K + λI)−1Φl , (3) where Φl is the column vector of [ Φl(yi) ∈ FY ]n i=1\n. Eq. 3 is a generalization of the scalar-valued kernel ridge regression solution to vector or functional outputs (Caponnetto & De Vito, 2007), in which the kernel matrix is a block operator matrix. Note that features Φl(yi) in this equation can be explicit or implicit. We show in the following that even with implicit features, we are able to formulate the structured output prediction problem in terms of explicit quantities that are computable via input and output kernels.\nStep 2 (pre-image) Prediction: In order to compute the structured prediction f(x) for an input x, we solve the following pre-image problem:\nf(x) = arg min y∈Y\n‖g(x)− Φl(y)‖2FY\n= arg min y∈Y\n‖ n∑\ni=1\nK(xi, x)ψi − Φl(y)‖2FY\n= arg min y∈Y\n‖KxΨ− Φl(y)‖2FY\n= arg min y∈Y\n‖Kx(K + λI)−1Φl − Φl(y)‖2FY\n= arg min y∈Y\nl(y, y)− 2〈Kx(K + λI)−1Φl,Φl(y)〉FY\n1As in the scalar-valued case, operator-valued kernels provide an elegant way of dealing with nonlinear problems (mapping g) by reducing them to linear ones (mapping h) in some feature space FXY (see Figure 1).\nwhere Kx is the row vector of operators corresponding to input x. In many problems, the kernel map Φl is unknown and only implicitly defined through the kernel l. In these problems, the above operator-valued kernel formulation has an inherent difficulty in expressing the pre-image problem and the usual kernel trick is not sufficient to solve it. To overcome this problem, we introduce the following variant (generalization) of the kernel trick: 〈T Φl(y1),Φl(y2)〉FY = [T l(y1, ·)](y2), where T is an operator in L(FY). Note that the usual kernel trick 〈Φl(y1),Φl(y2)〉FY = l(y1, y2) is recovered from this variant when T is the identity operator. It is easy to check that our proposed trick holds if we consider the feature space associated to the kernel l, i.e., Φl(y) = l(y, .). A proof for the more general case in which the features Φl can be any implicit mapping of a Mercer kernel is given for self-adjoint operator T in (Kadri et al., 2012, Appendix A). Using this trick, we may now express f(x) using only kernel functions:\nf(x) = arg min y∈Y\nl(y, y)− 2 [ Kx(K + λI) −1L• ] (y), (4)\nwhere L• is the column vector whose i’th component is l(yi, ·). Note that the KDE regression and prediction steps of Cortes et al. (2005) can be recovered from Eqs. 3 and 4 using an operator-valued kernel K of the form K(xi, xj) = k(xi, xj)I, in which k is a scalarvalued kernel and I is the identity operator in FY .\nNow that we have a general formulation of KDE, we turn our attention to build operator-valued kernels that can take into account the structure of the kernel feature space FY as well as input-output and output correlations. This is described in the next section."
    }, {
      "heading" : "4. Covariance-based Operator-valued Kernels",
      "text" : "In this section, we study the problem of designing operator-valued kernels suitable for structured outputs in the KDE formulation. This is quite important in order to take full advantage of the operator-valued KDE formulation. The main purpose of using the operatorvalued kernel formulation is to take into account the dependencies between the variables Φl(yi), i.e., the projection of yi in the feature space FY , with the objective of capturing the structure of the output data encapsulated in Φl(yi). Operator-valued kernels have been studied more in the context of multi-task learning, where the output is assumed to be in Rd with d the number of tasks (Evgeniou et al., 2005). Some work has also been focused on extending these kernels to the domain of functional data analysis to deal with the problem of regression with functional responses, where outputs are considered to be in the L2-space (Kadri\net al., 2010). However, the operator-valued Kernels used in these contexts for discrete (vector) or continuous (functional) outputs cannot be used in our formulation. This is because in our case the feature space FY can be known only implicitly by the output kernel l, and depending on l, FY can be finite or infinite dimensional. Therefore, we focus our attention to operators that act on scalar-valued RKHSs. Covariance operators on RKHS have recently received considerable attention. These operators that provide the simplest measure of dependency have been successfully applied to the problem of dimensionality reduction (Fukumizu et al., 2004), and played an important role in dealing with a number of statistical test problems (Gretton et al., 2005). We use the following covariance-based operator-valued kernel in our KDE formulation:\nK(xi, xj) = k(xi, xj)CY Y , (5)\nwhere k is a scalar-valued kernel and CY Y : FY → FY is the covariance operator defined for a random variable Y on Y as 〈ϕi, CY Y ϕj〉FY = E [ ϕi(Y )ϕj(Y ) ] . The empirical covariance operator Ĉ (n) Y Y is given by\nĈ (n) Y Y =\n1\nn n∑ i=1 l(·, yi)⊗ l(·, yi), (6)\nwhere ⊗ is the tensor product (ϕ1⊗ϕ2)h = 〈ϕ2, h〉ϕ1. The operator-valued kernel (5) is nonnegative since\nm∑ i,j=1 〈K(xi, xj)ϕj , ϕi〉FY = m∑ i,j 〈k(xi, xj)Ĉ(n)Y Y ϕj , ϕi〉\n= n∑ p=1 m∑ i,j 1 n 〈l(., yp), ϕi〉k(xi, xj)〈l(., yp), ϕj〉 ≥ 0.\nThe last step is due to the positive definiteness of k.\nThe kernel (5) is a separable operator-valued kernel since it operates on the output space, and then encodes the interactions between the outputs, without any reference to the input space. Although this property can be restrictive in specifying input-output correlations, because of its simplicity, most of the operator-valued kernels proposed in the literature belong to this category (see (Álvarez et al., 2012) for a review of separable and beyond separable operator-valued kernels). To address this issue, we propose a variant of the kernel in (5) based on the conditional covariance operator,\nK(xi, xj) = k(xi, xj)CY Y |X , (7)\nwhere CY Y |X = CY Y − CY XC−1XXCXY is the conditional covariance operator on FY . This operator allows the operator-valued kernel to simultaneously encode\nthe correlations between the outputs and to take into account (non-parametrically) the effects of the inputs. In Proposition 1, we show how the pre-image problem (4) can be formulated using the covariance-based operator-valued kernels in (5) and (7), and expressed in terms of input and output Gram matrices. The proof is reported in (Kadri et al., 2012, Appendix B).\nProposition 1 The pre-image problem of Eq. 4 can be written for covariance and conditional covariance operator-valued kernels defined by Eqs. (5) and (7) as\narg min y∈Y\nl(y, y)−2L>y (k>x⊗T)(k⊗T+nλIn2)−1 vec(In),\n(8) where T = L for the covariance operator and T = L− (k +n In)−1kL for the conditional covariance operator in which is a regularization parameter required for the operator inversion, k and L are Gram matrices associated to the scalar-valued kernels k and l, kx and\nLy are the column vectors ( k(x, x1), . . . , k(x, xn) )> and ( l(y, y1), . . . , l(y, yn) )> , vec is the vector operator such that vec(A) is the vector of columns of the matrix A, and finally ⊗ is the Kronecker product.\nNote that in order to compute Eq. 8 we need to store and invert the n2 × n2 matrix (k⊗T + nλIn2), which leads to space and computational complexities of order O(n4) and O(n6), respectively. However, we show that this computation can be performed more efficiently with space and computational complexities of order O (\nmax(nm1m2,m 2 1m 2 2) ) and O(m31m 3 2) using incomplete Cholesky decomposition (Bach & Jordan, 2002), where generally m1 n and m2 n; see (Kadri et al., 2012, Appendix C) for more details."
    }, {
      "heading" : "5. Related Work",
      "text" : "In this section, we discuss related work on kernel-based structured output learning and compare it with our proposed operator-valued kernel formulation."
    }, {
      "heading" : "5.1. KDE",
      "text" : "The main goal of our operator-valued KDE is to generalize KDE by taking into account input-output and output correlations. Existing KDE formulations try to address this issue either by performing a kernel PCA to decorrelate the outputs (Weston et al., 2003), or by incorporating some form of prior knowledge in the regression step using some specific constraints on the regression matrix which performs the mapping between input and output feature spaces (Cortes et al., 2007). Compared to kernel PCA 1) our KDE formulation does not need to have a dimensionality reduc-\ntion step, which may cause loss of information when the spectrum of the output kernel matrix does not decrease rapidly, 2) it does not require to assume that the dimensionality of the low-dimensional subspace (the number of principal components) is known and fixed in advance, and more importantly 3) it succeeds to take into account the effect of the explanatory variables (input data). Moreover, in contrast to (Cortes et al., 2007), our approach allows us to deal with infinitedimensional feature spaces, and encodes prior knowledge on input-output dependencies without requiring any particular form of constraints between input and output mappings. Indeed, information about the output space can be taken into account by the output kernel, and then the conditional covariance operatorvalued kernel is a natural way to capture this information and also input-output relationships, independently of the dimension of the output feature space."
    }, {
      "heading" : "5.2. Joint Kernels Meet Operator-valued Kernels",
      "text" : "Another approach to take into account input-output correlations is to use joint kernels, that are scalarvalued functions (similarity measure) of input-output pairs (Weston et al., 2007). In this context, the problem of learning the mapping f from X to Y is reformulated as learning a function f̂ from X × Y to R using a joint kernel (JK) (Tsochantaridis et al., 2005). Our operator-valued kernel formulation includes the JK approach. Similar to joint kernels, operator-valued kernels induce (implicitly) a similarity measure between input-output pairs. This can be seen from the feature space formulation of operator-valued kernels (Caponnetto et al., 2008; Kadri et al., 2011). A feature map associated with an operator-valued kernelK is a continuous function ΦK such that 〈K(x1, x2)Φl(y1),Φl(y2)〉 = 〈ΦK ( x1,Φl(y1) ) ,ΦK ( x2,Φl(y2) ) 〉. So, the joint kernel is an inner product between an output Φl(y2) and the result of applying the operator-valued kernel K to another output Φl(y1). We now show how two joint kernels in the literature (Weston et al., 2007) can be recovered by a suitable choice of operator-valued kernel.\n1) Tensor product JK: J ( (x1, y1), (x2, y2) ) = k(x1, x2)l(y1, y2) can be recovered from the operatorvalued kernel K(x1, x2) = k(x1, x2)I, where I is the identity operator in FY . 2) Diagonal regularization JK: J ( (x1, y1), (x2, y2) ) =\n(1−λ)k(x1, x2)〈y1, y2〉+λ q∑\ni=1\nxi1x i 2y i 1y i 2 can be recov-\nered by selecting the output kernel l(y1, y2) = 〈y1, y2〉 and the operator-valued kernel K(x1, x2) = [ (1 −\nλ)k(x1, x2) ] I + λ x1 x2 , where is the point-wise product operator."
    }, {
      "heading" : "6. Experimental Results",
      "text" : "We evaluate our operator-valued KDE formulation on three structured output prediction problems; namely, image reconstruction, optical character recognition, and face-to-face mapping. In the first problem, we compare our method using both covariance and conditional covariance operator-valued kernels with the KDE algorithms of Weston et al. (2003) and Cortes et al. (2005). In the second problem, we evaluate the two implementations of our KDE method with a constrained regression version of KDE (Cortes et al., 2007) and Max-Margin Markov Networks (M3Ns) (Taskar et al., 2004). In the third problem, in addition to scalar-valued KDE, we compare them with the joint kernel map (JKM) approach of Weston et al. (2007)."
    }, {
      "heading" : "6.1. Image Reconstruction",
      "text" : "Here we consider the image reconstruction problem used in Weston et al. (2003). This problem takes the top half (the first 8 pixel lines) of a USPS postal digit as input and estimates its bottom half. We use exactly the same dataset and setting as in the experiments of (Weston et al., 2003). We apply our KDE method using both covariance and conditional covariance operator-valued kernels and compare it with the KDE algorithms of Weston et al. (2003) and Cortes et al. (2005). In all these methods, we use RBF kernels for both input and output with the parameters shown in Table 2 (left). This table also contains the ridge parameter used by these algorithms. We tried a number of values for these parameters and those in the table yielded the best performance.\nWe perform 5-fold cross validation on the first 1000 digits of the USPS handwritten 16 by 16 pixel digit database, training with a single fold on 200 examples and testing on the remainder. Given a test input, we solve the problem and then choose as output the pre-image from the training data that is closest to this solution. The loss function used to evaluate the prediction ŷ for an output y is the RBF loss induced by the output kernel, i.e., ||Φl(y) − Φl(ŷ)||2 = 2− 2 exp ( − ||y− ŷ||2/(2σ2l ) ) . Table 2 (left) shows the mean and standard deviation of the RBF loss for the four KDE algorithms described above.\nOur proposed operator-valued kernel approach showed promising results in this experiment. While covariance operator-valued KDE achieved a slight improvement over kPCA-KDE (the algorithm by Weston et al. 2003), the conditional covariance operator-valued KDE outperformed all the other algorithms. This improvement in prediction accuracy is due to the fact that the conditional covariance operator allows us to\ncapture the output correlations while taking into account information about the inputs. In this problem, kPCA-based KDE performed better than the KDE formulation of Cortes et al. (2005). In fact, the latter is equivalent to using an identity-based operator-valued kernel in our formulation, and thus, it is incapable of capturing the dependencies in the output feature space (contrary to the other methods considered here)."
    }, {
      "heading" : "6.2. Optical Character Recognition",
      "text" : "In order to evaluate the effectiveness of our proposed method in problems with non-numerical outputs, we use an optical character recognition (OCR) problem. This problem is the one used in Taskar et al. (2004) and Cortes et al. (2005). The dataset is a subset of the handwritten words collected by Rob Kassel at the MIT Spoken Language Systems Group. It contains 6, 877 word instances with a total of 52, 152 characters. The image of each character has been normalized into a 16 by 8 binary-pixel representation. The OCR task consists in predicting a word from the sequence of pixel-based images of its handwritten characters.\nTable 2 (right) reports the results of our experiments. The performance is measured as the percentage number of word characters correctly recognized (WRC). We compare our approach with a constrained regression version of Cortes’s KDE formulation (Cortes et al., 2007) and Max-Margin Markov Networks (M3Ns) (Taskar et al., 2004). Results for these two methods are reported from (Cortes et al., 2007). We use exactly the same experimental setup described in (Cortes et al., 2007) to evaluate our operator-valued KDE approach. More precisely, we use 1) a 10-fold cross validation on the 6, 877 words of the dataset, training with a single fold (688 words) and testing on the remainder, 2) a polynomial kernel of third degree on the im-\n2These results are obtained using the Spider toolbox available at www.kyb.mpg.de/bs/people/spider.\nage characters, 3) the same input and output feature maps. The feature map associated to an input sequence of images x = x1 . . . xq is defined by\nΦk(x) = [ k(c1, xv(c1)), . . . , k(cN , xv(cN )) ]> , where cm, m = 1, . . . , N , are all the image segments in the training set, v(cm) is the position of the character cm in the word, and k(cm, xv(cm)) = 0 if v(cm) > q. For the output space, the feature map Φl(y) associated to an output string y = y1, . . . , yq is a 26p-dimensional vec-\ntor defined by Φl(y) = [ φl(y1), . . . , φl(yq), 0, . . . , 0 ]> , where p is the maximum length of a sequence of images in the training set, and φl(yj), 1 6 j 6 q, is a 26-dimensional vector whose components are all zero except for the entry of index yj . With this output feature space, the pre-image is easily computed since each position can be obtained separately. Note that this occurs since with the OCR dataset a one-to-one mapping of images to characters is provided.\nExperiments on the OCR task support the results obtained in the image reconstruction of Sec. 6.1. While covariance based operator-valued KDE achieved better (but comparable) results than the existing stateof-the-art methods, conditional covariance operatorvalued KDE outperformed all the other algorithms."
    }, {
      "heading" : "6.3. Face-to-Face Mapping",
      "text" : "In this experiment, we first compare the covariancebased operator-valued KDE with the KDE algorithm of Cortes et al. (2005) and the JKM approach of Weston et al. (2007), and then show how we can speed up the training of our proposed KDE method using incomplete Cholesky decomposition; see (Kadri et al., 2012, Appendix C) for more details on applying incomplete Cholesky decomposition to block kernel matrices associated to separable operator-valued kernels. Similar to the “learning to smile” experiment in Weston et al. (2007), we consider the problem of mapping the rotated view of a face to the plain expression (frontal view) of the same face. For that, we use grey-\nscale views of human faces taken from the MPI face database3 (Troje & Bulthoff, 1996). The database contains 256 × 256 pixels images of 7 views (frontal and rotated) of 200 laser-scanned heads without hair.\nTo show the effectiveness of our approach, we use a relatively small number of training examples in our first experiment (similar to Weston et al. 2007). We consider the problem of predicting plain expression faces from only 30 degree right rotated views. We use 20 examples for training and 80 for testing. We apply a JKM using the patch-wise joint kernel defined in (Weston et al., 2007), with patches of size 10×10 that overlap by 5 pixels. For all the methods (JKM and KDEbased), we use a RBF kernel for inputs and a linear kernel for outputs. Table 3 reports the mean squared error (MSE) obtained by each algorithm. The results indicate that JKM and conditional covariance KDE algorithms outperform identity and conditional KDE methods, and conditional covariance KDE achieves the best performance. This confirms that we can improve the performance by taking into account the relationship between inputs-outputs.\nWe now focus on the scalability of our method and consider the face-to-face mapping problem with a large number of examples. Here we use all the rotated face images (30, 60, and 90 degree left and right rotations) to predict the plain face expression. This gives us 1,200 examples for training and 200 for testing. Fig. 2 compares the performance of the efficient implementation (using incomplete Cholesky decomposition) of our conditional covariance operator-valued KDE method with the original KDE algorithm (Cortes et al., 2005). The parameter n is the number of faces randomly selected from 1,200 training faces in the original KDE and is m1 = m2 = n in the incomplete Cholesky decomposition. The results indicate that the low-rank approximation of our KDE method leads to both a considerable reduction in computation time and a good performance. It obtains a better MSE with m1 = m2 = 30 than the original KDE with all 1,200 examples.\n3Available at http://faces.kyb.tuebingen.mpg.de."
    }, {
      "heading" : "7. Conclusions and Future Work",
      "text" : "In this paper, we presented a general formulation of kernel dependency estimation (KDE) for structured output learning using operator-valued kernels, and illustrated its use in several experiments. We also proposed a new covariance-based operator-valued kernel that takes into account the structure of the output kernel feature space. This kernel encodes the interactions between the outputs, but makes no reference to the input space. We addressed this issue by introducing a variant of our KDE method based on the conditional covariance operator that in addition to the correlation between the outputs takes into account the effects of the input variables.\nIn our work, we focused on regression-based structured output prediction. An interesting direction for future research is to explore operator-valued kernels in the context of classification-based structured output learning. Joint kernels and operator-valued kernels have strong connections, but more investigation is needed to show how operator-valued kernel formulation can be used to improve joint kernel methods, and how to deal with the pre-image problem in this case."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank Arthur Gretton and Alain Rakotomamonjy for their help and discussions. This research was funded by the Ministry of Higher Education and Research and the ANR project LAMPADA (ANR-09-EMER-007)."
    }, {
      "heading" : "8. Appendix",
      "text" : ""
    }, {
      "heading" : "8.1. (Generalized) Kernel Trick",
      "text" : "In this section, we prove the (generalized) kernel trick used in Section 3 of the paper, i.e., 〈T Φl(y1),Φl(y2)〉FY = [T l(y1, ·)](y2), where T ∈ L(FY) and FY is a RKHS with kernel l.\nCase 1: Φl is the feature map associated to the reproducing kernel l, i.e., Φl(y) = l(·, y).\nHere the proof is straightforward, we may write\n〈T Φl(y1),Φl(y2)〉FY = 〈T l(·, y1), l(·, y2)〉FY = [T l(y1, ·)](y2).\nThe second equality follows from the reproducing property.\nCase 2: Φl is an implicit feature map of a Mercer kernel, and T is a self-adjoint operator in L(FY).\nWe first recall the Mercer’s theorem:\nTheorem 2 (Mercer’s theorem) Suppose that l is a symmetric real-valued kernel on Y2 such that the integral operator Tl : L2(Y)→ L2(Y), defined as\n(Tlf)(y1) := ∫ Y l(y1, y2)f(y2)dy2\nis positive. Let γj ∈ L2(Y) be the normalized eigenfunctions of Tl associated with the eigenvalues λj > 0, sorted in non-increasing order. Then\nl(y1, y2) = Nf∑ j=1 λjγj(y1)γj(y2) (9)\nholds for almost all (y1, y2) with Nf ∈ N.\nSince l is a Mercer kernel, the eigenfunctions (γi) Nf i=1 can be chosen to be orthogonal w.r.t. the dot product in L2(Y). Hence, it is straightforward to construct a dot product 〈·, ·〉 such that 〈γi, γj〉 = δij/λj (δij is the Kronecker delta) and the orthonormal basis ( ej )Nf j=1 = (√ λjγj )Nf j=1 (see (Schölkopf et al., 1999) for more details). Therefore, the feature map associated to the Mercer kernel l is of the form\nΦl : y 7−→ (√ λjγj(y) )Nf j=1 .\nUsing (9), we can compute [T l(y1, ·)](y2) as follows:\n[T l(y1, ·)](y2) = Nf∑ j=1 λjγj(y1) [ T γj ] (y2)\n= Nf∑ i,j=1 λjγj(y1)〈T γj , ei〉ei(y2)\n= Nf∑ i,j=1 λjγj(y1)λi〈T γj , γi〉γi(y2). (10)\nLet T̂ = ( T̂ij )Nf i,j=1 be the matrix representation of the operator T in the basis ( ej )Nf j=1 . By definition we have\nT̂ij = 〈T ei, ej〉. Using this and the feature map expression of a Mercer kernel, we obtain\n〈T Φl(y1),Φl(y2)〉FY = Nf∑ i=1 ( T̂ Φl(y1) ) i ( Φl(y2) ) i\n= Nf∑ i=1 Nf∑ j=1 T̂ij √ λjγj(y1) √λiγi(y2) =\nNf∑ i,j=1 〈T ei, ej〉 √ λjγj(y1) √ λiγi(y2)\n= Nf∑ i,j=1 〈T √ λiγi, √ λjγj〉 √ λjγj(y1) √ λiγi(y2)\n= Nf∑ i,j=1 λjγj(y1)〈T γi, γj〉λiγi(y2)\n= Nf∑ i,j=1 λjγj(y1)〈T γj , γi〉λiγi(y2). (11)\nNote that the last equality follows from the fact that T is a self-adjoint operator. The proof follows from Eqs. 10 and 11."
    }, {
      "heading" : "8.2. Proof of Proposition 1",
      "text" : "In this section, we provide the proof of Proposition 1. We only show the proof for the covariance-based operatorvalued kernels, since the proof for the other case (conditional covariance-based operator-valued kernels) is quite similar. Note that the pre-image problem is of the form\nf(x) = arg min y∈Y\nl(y, y)− 2 [ Kx(K + λI) −1L• ] (y),\nand our goal is to compute its Gram matrix expression4 in case K(xi, xj) = k(xi, xj)Ĉ (n) Y Y , where Ĉ (n) Y Y is the empirical covariance operator defined as\nĈ (n) Y Y =\n1\nn n∑ i=1 l(·, yi)⊗ l(·, yi).\nLet h = (hi) n i=1 be a vector of variables in the RKHS FY such that h = (K + λI)−1L•. Since each hi is in the RKHS FY , it can be decomposed as\nhi = α > (i)L• + hi⊥ = n∑ j=1 αj(i)l(·, yj) + hi⊥,\nwhere α(i) ∈ Rn, L• = ( l(·, y1), . . . , l(·, yn) )> , and hi⊥ is orthogonal to all l(·, yi)’s, i = 1, . . . , n. The idea here is similar to the one used by Fukumizu et al. (2011). Now we may write\n(K + λI)h = L•,\nwhich gives us\n∀i ∈ 1, . . . , n l(·, yi) = n∑\nj=1\nKijhj + λhi.\n4Expressing the covariance operator Ĉ (n) Y Y on the RKHS FY using the kernel matrix L.\nUsing the empirical covariance operator, for each i, we may write\nl(·, yi) = n∑\nj=1\nk(xi, xj)Ĉ (n) Y Y hj + λhi\n= n∑ j=1 k(xi, xj) ( 1 n n∑ s=1 l(·, ys)⊗ l(·, ys) ) hj + λhi\n= n∑ j=1 1 n k(xi, xj) ( n∑ s=1 l(·, ys)⊗ l(·, ys) )( n∑ m=1 αm(j)l(·, ym) + hi⊥ ) + λ n∑ m=1 αm(i)l(·, ym) + λhi⊥\n= n∑ j=1 1 n k(xi, xj) n∑ s=1 n∑ m=1 αm(j)l(ys, ym)l(·, ys) + 0 + λL > • α(i) + λhi⊥\n= n∑ j=1 1 n k(xi, xj) n∑ s=1 l(·, ys) n∑ m=1 αm(j)l(ys, ym) + λL > • α(i) + λhi⊥\n= n∑ j=1 1 n k(xi, xj)L > • Lα(j) + λL > • α(i) + λhi⊥.\nNow if take the inner-product of the above equation with all l(ys, ·), s = 1, . . . , n, we obtain that for each i\nl(yi, ys) = n∑ j=1 1 n k(xi, xj)〈l(·, ys),L>• Lα(j)〉+ λ〈l(·, ys),L>• α(i)〉+ λ〈l(·, ys), hi⊥〉\n= n∑ j=1 1 n k(xi, xj)L > ysLα(j) + λL > ysα(i),\nwhich gives us the vector form\nLyi = n∑ j=1 1 n k(xi, xj)LLα(j) + λLα(i). (12)\nDefining the n× n matrix α = (α(1), . . . , α(n)), we may write Eq. 12 in a matrix form as\nL = 1\nn LLαk + λLα,\nwhich gives us 1\nn Lαk + λα = In.\nUsing vec(ABC) = (C> ⊗A) vec(B), we have( 1 n k⊗ L + λIn2 ) vec(α) = vec(In). (13)\nNow we may write\nKx(K + λI) −1L• = Kxh = n∑ i=1 K(x, xi)hi\n= n∑ i=1 k(x, xi)Ĉ (n) Y Y hi = n∑ i=1 1 n k(x, xi)L > • Lα(i) = 1\nn L>• Lαkx =\n1 n L>• vec(Lαkx) = 1 n L>• ( k>x ⊗ L ) vec(α)\n= L>• ( k>x ⊗ L )( k⊗ L + nλIn2 )−1 vec(In),\nwhere the last equality comes from (13). Thus, we may write\nf(x) = arg min y∈Y\nl(y, y)− 2L>y (k>x ⊗ L)(k⊗ L + nλIn2)−1 vec(In),\nwhich concludes the proof."
    }, {
      "heading" : "8.3. Computational Complexity of Solving the Pre-image Problem",
      "text" : "As discussed in Section 4, solving the pre-image problem of Eq. 8 requires computing the following expression:\nC(x, y) = l(y, y)− 2L>y (k>x ⊗ L)(k⊗ L + nλIn2)−1 vec(In). (14)\nSimple computation of C(x, y) requires storing and inverting the matrix (k⊗L+nλIn2) ∈ Rn 2×n2 . This leads to space and computational complexities of order O(n4) and O(n6). In this section, we provide an efficient procedure to for this computation that reduces its space and computational complexities to O ( max(nm1m2 , m 2 1m 2 2) )\nand O(m31m 3 2). We first apply incomplete Cholesky decomposition to the kernel matrices k ∈ Rn×n and L ∈ Rn×n (Bach & Jordan, 2002). This consists of finding the matrices U ∈ Rn×m1 and V ∈ Rn×m2 , with m1 n and m2 n, such that\nk = UU> , L = VV>.\nUsing this decomposition in (14), we obtain\nC(x, y) = l(y, y)− 2L>y (k>x ⊗ L)[UU> ⊗VV> + nλIn2 ] −1 vec(In)\n(a) = l(y, y)− 2L>y (k>x ⊗ L) [ (U⊗V)(U> ⊗V>) + nλIn2 ]−1 vec(In) (b) = l(y, y)− 2\nnλ L>y (k > x ⊗ L)\n[ In2 − (U⊗V) ( nλIm1m2 + (U > ⊗V>)(U⊗V) )−1 (U> ⊗V>) ] vec(In)\n(c) = l(y, y)− 2 nλ L>y (k > x ⊗ L)\n[ vec(In)− (U⊗V)(nλIm1m2 + U >U⊗V>V)−1(U> ⊗V>) vec(In) ]\n= l(y, y)− 2 nλ\nL>y [ (k>x ⊗ L) vec(In)− (k>x ⊗ L)(U⊗V)(nλIm1m2 + U >U⊗V>V)−1(U> ⊗V>) vec(In) ]\n(d) = l(y, y)− 2 nλ L>y [ vec(Lkx)− (k>x U⊗ LV)(nλIm1m2 + U >U⊗V>V)−1 vec(V>U) ] (e) = l(y, y)− 2\nnλ L>y [ Lkx − (k>x U⊗ LV)(nλIm1m2 + U >U⊗V>V)−1 vec(V>U) ]\n(15)\n(a) and (c) follow from the fact that (A⊗B)(C⊗D) = AC⊗BD.\n(b) follows from the Woodbury formula, i.e., (A + BC)−1 = A−1 −A−1B(I + CA−1B)−1CA−1.\n(d) follows from the fact that vec(ABC) = (C> ⊗A) vec(B).\n(e) follows from the fact that Lkx is a n× 1 vector.\nThe most expensive computations in Eq. 15 is the inversion of matrix (nλIm1m2 +U >U⊗V>V) ∈ Rm1m2×m1m2 , with computational cost of order O(m31m 3 2). Therefore, we have reduced the cost of computing C(x, y) from O(n\n6) to O(m31m 3 2). Moreover, the largest size matrix that is needed to be stored in order to compute Eq. 15 is either (nλIm1m2 + U >U ⊗ V>V) ∈ Rm1m2×m1m2 or (k>x U ⊗ LV) ∈ Rn×m1m2 , which reduces the space complexity\nfrom O(n4) to O (\nmax(nm1m2 , m 2 1m 2 2) ) ."
    } ],
    "references" : [ {
      "title" : "Kernels for vector-valued functions: a review",
      "author" : [ "M. Álvarez", "L. Rosasco", "N.D. Lawrence" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Álvarez et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Álvarez et al\\.",
      "year" : 2012
    }, {
      "title" : "Kernel independent component analysis",
      "author" : [ "F. Bach", "M. Jordan" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Bach and Jordan,? \\Q2002\\E",
      "shortCiteRegEx" : "Bach and Jordan",
      "year" : 2002
    }, {
      "title" : "Predicting Structured Data",
      "author" : [ "G. Bakir", "T. Hofmann", "B. Schölkopf", "A. Smola", "B. Taskar", "Vishwanathan", "S. (eds" ],
      "venue" : null,
      "citeRegEx" : "Bakir et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Bakir et al\\.",
      "year" : 2007
    }, {
      "title" : "Semi-supervised penalized output kernel regression for link prediction",
      "author" : [ "C. Brouard", "F. d’Alché Buc", "M. Szafranski" ],
      "venue" : "In Proc. ICML, pp",
      "citeRegEx" : "Brouard et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Brouard et al\\.",
      "year" : 2011
    }, {
      "title" : "Optimal rates for the regularized least-squares algorithm",
      "author" : [ "A. Caponnetto", "E. De Vito" ],
      "venue" : "Foundations of Computational Mathematics,",
      "citeRegEx" : "Caponnetto and Vito,? \\Q2007\\E",
      "shortCiteRegEx" : "Caponnetto and Vito",
      "year" : 2007
    }, {
      "title" : "Universal multi-task kernels",
      "author" : [ "A. Caponnetto", "C. Micchelli", "M. Pontil", "Y. Ying" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Caponnetto et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Caponnetto et al\\.",
      "year" : 2008
    }, {
      "title" : "Multitask learning",
      "author" : [ "R. Caruana" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Caruana,? \\Q1997\\E",
      "shortCiteRegEx" : "Caruana",
      "year" : 1997
    }, {
      "title" : "A general regression technique for learning transductions",
      "author" : [ "C. Cortes", "M. Mohri", "J. Weston" ],
      "venue" : "In Proc. ICML, pp",
      "citeRegEx" : "Cortes et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Cortes et al\\.",
      "year" : 2005
    }, {
      "title" : "A General Regression Framework for Learning String-to-String Mappings",
      "author" : [ "C. Cortes", "M. Mohri", "J. Weston" ],
      "venue" : null,
      "citeRegEx" : "Cortes et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Cortes et al\\.",
      "year" : 2007
    }, {
      "title" : "Learning multiple tasks with kernel methods",
      "author" : [ "T. Evgeniou", "C. Micchelli", "M. Pontil" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Evgeniou et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Evgeniou et al\\.",
      "year" : 2005
    }, {
      "title" : "Dimensionality reduction for supervised learning with reproducing kernel hilbert spaces",
      "author" : [ "K. Fukumizu", "F. Bach", "M. Jordan" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Fukumizu et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Fukumizu et al\\.",
      "year" : 2004
    }, {
      "title" : "Kernel methods for measuring independence",
      "author" : [ "A. Gretton", "R. Herbrich", "A. Smola", "O. Bousquet", "B. Schölkopf" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Gretton et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Gretton et al\\.",
      "year" : 2005
    }, {
      "title" : "Conditional mean embeddings as regressors",
      "author" : [ "S. Grunewalder", "G. Lever", "A. Gretton", "L. Baldassarre", "S. Patterson", "M. Pontil" ],
      "venue" : "In Proc. ICML,",
      "citeRegEx" : "Grunewalder et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Grunewalder et al\\.",
      "year" : 2012
    }, {
      "title" : "Nonlinear functional regression: a functional RKHS approach",
      "author" : [ "H. Kadri", "E. Duflos", "P. Preux", "S. Canu", "M. Davy" ],
      "venue" : "In Proc. AISTATS,",
      "citeRegEx" : "Kadri et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Kadri et al\\.",
      "year" : 2010
    }, {
      "title" : "Functional regularized least squares classification with operator-valued kernels",
      "author" : [ "H. Kadri", "A. Rabaoui", "P. Preux", "E. Duflos", "A. Rakotomamonjy" ],
      "venue" : "In Proc. ICML,",
      "citeRegEx" : "Kadri et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Kadri et al\\.",
      "year" : 2011
    }, {
      "title" : "A generalized kernel approach to structured output learning",
      "author" : [ "H. Kadri", "M. Ghavamzadeh", "P. Preux" ],
      "venue" : "Technical Report 00695631,",
      "citeRegEx" : "Kadri et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Kadri et al\\.",
      "year" : 2012
    }, {
      "title" : "On learning vector-valued functions",
      "author" : [ "C. Micchelli", "M. Pontil" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Micchelli and Pontil,? \\Q2005\\E",
      "shortCiteRegEx" : "Micchelli and Pontil",
      "year" : 2005
    }, {
      "title" : "Functional Data Analysis, 2nd edition",
      "author" : [ "J. Ramsay", "B. Silverman" ],
      "venue" : null,
      "citeRegEx" : "Ramsay and Silverman,? \\Q2005\\E",
      "shortCiteRegEx" : "Ramsay and Silverman",
      "year" : 2005
    }, {
      "title" : "Input space vs. feature space in kernel-based methods",
      "author" : [ "B. Schölkopf", "S. Mika", "C.J.C. Burges", "P. Knirsch", "Müller", "K.-R", "G. Rätsch", "A.J. Smola" ],
      "venue" : "IEEE Trans. on Neural Networks,",
      "citeRegEx" : "Schölkopf et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Schölkopf et al\\.",
      "year" : 1999
    }, {
      "title" : "Max-margin markov networks",
      "author" : [ "B. Taskar", "C. Guestrin", "D. Koller" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Taskar et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Taskar et al\\.",
      "year" : 2004
    }, {
      "title" : "Face recognition under varying poses: The role of texture and shape",
      "author" : [ "N. Troje", "H. Bulthoff" ],
      "venue" : "Vision Research,",
      "citeRegEx" : "Troje and Bulthoff,? \\Q1996\\E",
      "shortCiteRegEx" : "Troje and Bulthoff",
      "year" : 1996
    }, {
      "title" : "Large margin methods for structured and interdependent output variables",
      "author" : [ "I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun" ],
      "venue" : "Journal of machine Learning Research,",
      "citeRegEx" : "Tsochantaridis et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Tsochantaridis et al\\.",
      "year" : 2005
    }, {
      "title" : "A kernel regression framework for SMT",
      "author" : [ "Z. Wang", "J. Shawe-Taylor" ],
      "venue" : "Machine Translation,",
      "citeRegEx" : "Wang and Shawe.Taylor,? \\Q2010\\E",
      "shortCiteRegEx" : "Wang and Shawe.Taylor",
      "year" : 2010
    }, {
      "title" : "Kernel dependency estimation",
      "author" : [ "J. Weston", "O. Chapelle", "A. Elisseeff", "B. Schölkopf", "V. Vapnik" ],
      "venue" : "In Proceedings of the Advances in Neural Information Processing Systems",
      "citeRegEx" : "Weston et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Weston et al\\.",
      "year" : 2003
    }, {
      "title" : "Joint Kernel Maps",
      "author" : [ "J. Weston", "G. BakIr", "O. Bousquet", "B. Schölkopf", "T. Mann", "W. Noble" ],
      "venue" : null,
      "citeRegEx" : "Weston et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Weston et al\\.",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "In many practical problems such as statistical machine translation (Wang & Shawe-Taylor, 2010) and speech recognition or synthesis (Cortes et al., 2005), we are faced with the task of learning a mapping between",
      "startOffset" : 131,
      "endOffset" : 152
    }, {
      "referenceID" : 6,
      "context" : "The focus in the machine learning and statistics communities has been mainly on multi-task learning (vector outputs) and functional data analysis (functional outputs) (Caruana, 1997; Ramsay & Silverman, 2005), where in both cases output data reside in a Euclidean space, but there has also been considerable interest in expanding general learning algorithms to structured outputs.",
      "startOffset" : 167,
      "endOffset" : 208
    }, {
      "referenceID" : 2,
      "context" : "Two different, but closely related, kernel-based approaches for structured output learning can be found in the literature (Bakir et al., 2007): kernel dependency estimation (KDE) and joint kernel maps (JKM).",
      "startOffset" : 122,
      "endOffset" : 142
    }, {
      "referenceID" : 2,
      "context" : "Two different, but closely related, kernel-based approaches for structured output learning can be found in the literature (Bakir et al., 2007): kernel dependency estimation (KDE) and joint kernel maps (JKM). KDE is a regression-based approach that was first proposed by Weston et al. (2003) and then reformulated by Cortes et al.",
      "startOffset" : 123,
      "endOffset" : 291
    }, {
      "referenceID" : 2,
      "context" : "Two different, but closely related, kernel-based approaches for structured output learning can be found in the literature (Bakir et al., 2007): kernel dependency estimation (KDE) and joint kernel maps (JKM). KDE is a regression-based approach that was first proposed by Weston et al. (2003) and then reformulated by Cortes et al. (2005). The idea is to define a kernel on the output space Y to project the structured output to ar X iv :1 20 5.",
      "startOffset" : 123,
      "endOffset" : 337
    }, {
      "referenceID" : 21,
      "context" : "On the other hand, the JKM approach is based on joint kernels, which are nonlinear similarity measures between input-output pairs (Tsochantaridis et al., 2005; Weston et al., 2007).",
      "startOffset" : 130,
      "endOffset" : 180
    }, {
      "referenceID" : 24,
      "context" : "On the other hand, the JKM approach is based on joint kernels, which are nonlinear similarity measures between input-output pairs (Tsochantaridis et al., 2005; Weston et al., 2007).",
      "startOffset" : 130,
      "endOffset" : 180
    }, {
      "referenceID" : 20,
      "context" : "On the other hand, the JKM approach is based on joint kernels, which are nonlinear similarity measures between input-output pairs (Tsochantaridis et al., 2005; Weston et al., 2007). While in KDE separate kernels are used to project input and output data to two (possibly different) feature spaces, the joint kernel in JKM maps them into a single feature space, which then allows us to take advantage of our prior knowledge on both input-output and output correlations. However, this improvement requires an exhaustive pre-image computation during training, a problem that is encountered by KDE only in the test phase. Avoiding this computation during training is an important advantage of KDE over JKM methods. In this paper, we focus on the KDE approach to structured output learning. The main contributions of this paper can be summarized as follows: 1) Building on the works of Caponnetto & De Vito (2007) and Brouard et al.",
      "startOffset" : 131,
      "endOffset" : 909
    }, {
      "referenceID" : 3,
      "context" : "The main contributions of this paper can be summarized as follows: 1) Building on the works of Caponnetto & De Vito (2007) and Brouard et al. (2011), we propose a more general KDE formulation (prediction and pre-image steps) based on operator-valued (multi-task) kernels instead of scalar-valued ones used by the existing methods (Sec.",
      "startOffset" : 127,
      "endOffset" : 149
    }, {
      "referenceID" : 3,
      "context" : "The main contributions of this paper can be summarized as follows: 1) Building on the works of Caponnetto & De Vito (2007) and Brouard et al. (2011), we propose a more general KDE formulation (prediction and pre-image steps) based on operator-valued (multi-task) kernels instead of scalar-valued ones used by the existing methods (Sec. 3). This extension allows KDE to capture the dependencies between the outputs as well as between the input and output variables, which is an improvement over the existing KDE methods that fail to take into account these dependencies. 2) We also propose a variant (generalization) of the kernel trick to cope with the technical difficulties encountered when working with operator-valued kernels (Sec. 3). This allows us to (i) formulate the pre-image problem using only kernel functions (not feature maps that cannot be computed explicitly), and (ii) avoid the computation of the inner product between feature maps after being modified with an operator whose role is to capture the structure of complex objects. 3) We then introduce a novel family of operator-valued kernels, based on covariance operators on RKHSs, that allows us to take full advantage of our KDE formulation. These kernels offer a simple and powerful way to address the main limitations of the original KDE formulation, namely the decoupling between outputs in the image space and the inability to use a joint feature space (Sec. 4). 4) We show how the pre-image problem, in the case of covariance and conditional covariance operator-valued kernels, can be expressed only in terms of input and output Gram matrices, and provide a low rank approximation to efficiently compute it (Sec. 4). 5) Finally, we empirically evaluate the performance of our proposed KDE approach and show its effectiveness on three structured output prediction problems involving numeric and non-numerical outputs (Sec. 6). It should be noted that generalizing KDE using operator-valued kernels was first proposed in Brouard et al. (2011). The authors have applied this generalization to the problem of link prediction which did not require a preimage step.",
      "startOffset" : 127,
      "endOffset" : 2017
    }, {
      "referenceID" : 5,
      "context" : "We now provide a few definitions related to operatorvalued kernels and their associated RKHSs that are used in the paper (see (Micchelli & Pontil, 2005; Caponnetto et al., 2008; Álvarez et al., 2012) for more details).",
      "startOffset" : 126,
      "endOffset" : 199
    }, {
      "referenceID" : 0,
      "context" : "We now provide a few definitions related to operatorvalued kernels and their associated RKHSs that are used in the paper (see (Micchelli & Pontil, 2005; Caponnetto et al., 2008; Álvarez et al., 2012) for more details).",
      "startOffset" : 126,
      "endOffset" : 199
    }, {
      "referenceID" : 9,
      "context" : "These kernel spaces have recently received more attention, since they are suitable for leaning in problems where the outputs are vectors (as in multitask learning (Evgeniou et al., 2005)) or functions (as in functional regression (Kadri et al.",
      "startOffset" : 163,
      "endOffset" : 186
    }, {
      "referenceID" : 13,
      "context" : ", 2005)) or functions (as in functional regression (Kadri et al., 2010)) instead of scalars.",
      "startOffset" : 51,
      "endOffset" : 71
    }, {
      "referenceID" : 12,
      "context" : "Also, it has been shown recently that these spaces are appropriate for learning conditional mean embeddings (Grunewalder et al., 2012).",
      "startOffset" : 108,
      "endOffset" : 134
    }, {
      "referenceID" : 21,
      "context" : "Our generalized formulation consists of learning the mapping g using an operator-valued kernel ridge regression rather than a scalar-valued one as in the formulations of Weston et al. (2003) and Cortes et al.",
      "startOffset" : 170,
      "endOffset" : 191
    }, {
      "referenceID" : 7,
      "context" : "(2003) and Cortes et al. (2005). Using an operator-valued kernel mapping, we construct a joint feature space from information of input and output spaces in which inputoutput and output correlations can be taken into account.",
      "startOffset" : 11,
      "endOffset" : 32
    }, {
      "referenceID" : 7,
      "context" : "input and output kernels can be infinite dimensional, contrary to Cortes et al. (2005) that only considers finite feature spaces.",
      "startOffset" : 66,
      "endOffset" : 87
    }, {
      "referenceID" : 7,
      "context" : "Note that the KDE regression and prediction steps of Cortes et al. (2005) can be recovered from Eqs.",
      "startOffset" : 53,
      "endOffset" : 74
    }, {
      "referenceID" : 9,
      "context" : "Operator-valued kernels have been studied more in the context of multi-task learning, where the output is assumed to be in R with d the number of tasks (Evgeniou et al., 2005).",
      "startOffset" : 152,
      "endOffset" : 175
    }, {
      "referenceID" : 10,
      "context" : "These operators that provide the simplest measure of dependency have been successfully applied to the problem of dimensionality reduction (Fukumizu et al., 2004), and played an important role in dealing with a number of statistical test problems (Gretton et al.",
      "startOffset" : 138,
      "endOffset" : 161
    }, {
      "referenceID" : 11,
      "context" : ", 2004), and played an important role in dealing with a number of statistical test problems (Gretton et al., 2005).",
      "startOffset" : 92,
      "endOffset" : 114
    }, {
      "referenceID" : 0,
      "context" : "Although this property can be restrictive in specifying input-output correlations, because of its simplicity, most of the operator-valued kernels proposed in the literature belong to this category (see (Álvarez et al., 2012) for a review of separable and beyond separable operator-valued kernels).",
      "startOffset" : 202,
      "endOffset" : 224
    }, {
      "referenceID" : 23,
      "context" : "Existing KDE formulations try to address this issue either by performing a kernel PCA to decorrelate the outputs (Weston et al., 2003), or by incorporating some form of prior knowledge in the regression step using some specific constraints on the regression matrix which performs the mapping between input and output feature spaces (Cortes et al.",
      "startOffset" : 113,
      "endOffset" : 134
    }, {
      "referenceID" : 8,
      "context" : ", 2003), or by incorporating some form of prior knowledge in the regression step using some specific constraints on the regression matrix which performs the mapping between input and output feature spaces (Cortes et al., 2007).",
      "startOffset" : 205,
      "endOffset" : 226
    }, {
      "referenceID" : 8,
      "context" : "Moreover, in contrast to (Cortes et al., 2007), our approach allows us to deal with infinitedimensional feature spaces, and encodes prior knowledge on input-output dependencies without requiring any particular form of constraints between input and output mappings.",
      "startOffset" : 25,
      "endOffset" : 46
    }, {
      "referenceID" : 24,
      "context" : "Another approach to take into account input-output correlations is to use joint kernels, that are scalarvalued functions (similarity measure) of input-output pairs (Weston et al., 2007).",
      "startOffset" : 164,
      "endOffset" : 185
    }, {
      "referenceID" : 21,
      "context" : "In this context, the problem of learning the mapping f from X to Y is reformulated as learning a function f̂ from X × Y to R using a joint kernel (JK) (Tsochantaridis et al., 2005).",
      "startOffset" : 151,
      "endOffset" : 180
    }, {
      "referenceID" : 5,
      "context" : "This can be seen from the feature space formulation of operator-valued kernels (Caponnetto et al., 2008; Kadri et al., 2011).",
      "startOffset" : 79,
      "endOffset" : 124
    }, {
      "referenceID" : 14,
      "context" : "This can be seen from the feature space formulation of operator-valued kernels (Caponnetto et al., 2008; Kadri et al., 2011).",
      "startOffset" : 79,
      "endOffset" : 124
    }, {
      "referenceID" : 24,
      "context" : "We now show how two joint kernels in the literature (Weston et al., 2007) can be recovered by a suitable choice of operator-valued kernel.",
      "startOffset" : 52,
      "endOffset" : 73
    }, {
      "referenceID" : 8,
      "context" : "In the second problem, we evaluate the two implementations of our KDE method with a constrained regression version of KDE (Cortes et al., 2007) and Max-Margin Markov Networks (MNs) (Taskar et al.",
      "startOffset" : 122,
      "endOffset" : 143
    }, {
      "referenceID" : 19,
      "context" : ", 2007) and Max-Margin Markov Networks (MNs) (Taskar et al., 2004).",
      "startOffset" : 45,
      "endOffset" : 66
    }, {
      "referenceID" : 20,
      "context" : "In the first problem, we compare our method using both covariance and conditional covariance operator-valued kernels with the KDE algorithms of Weston et al. (2003) and Cortes et al.",
      "startOffset" : 144,
      "endOffset" : 165
    }, {
      "referenceID" : 7,
      "context" : "(2003) and Cortes et al. (2005). In the second problem, we evaluate the two implementations of our KDE method with a constrained regression version of KDE (Cortes et al.",
      "startOffset" : 11,
      "endOffset" : 32
    }, {
      "referenceID" : 7,
      "context" : "(2003) and Cortes et al. (2005). In the second problem, we evaluate the two implementations of our KDE method with a constrained regression version of KDE (Cortes et al., 2007) and Max-Margin Markov Networks (MNs) (Taskar et al., 2004). In the third problem, in addition to scalar-valued KDE, we compare them with the joint kernel map (JKM) approach of Weston et al. (2007).",
      "startOffset" : 11,
      "endOffset" : 374
    }, {
      "referenceID" : 23,
      "context" : "We use exactly the same dataset and setting as in the experiments of (Weston et al., 2003).",
      "startOffset" : 69,
      "endOffset" : 90
    }, {
      "referenceID" : 21,
      "context" : "Here we consider the image reconstruction problem used in Weston et al. (2003). This problem takes the top half (the first 8 pixel lines) of a USPS postal digit as input and estimates its bottom half.",
      "startOffset" : 58,
      "endOffset" : 79
    }, {
      "referenceID" : 21,
      "context" : "Here we consider the image reconstruction problem used in Weston et al. (2003). This problem takes the top half (the first 8 pixel lines) of a USPS postal digit as input and estimates its bottom half. We use exactly the same dataset and setting as in the experiments of (Weston et al., 2003). We apply our KDE method using both covariance and conditional covariance operator-valued kernels and compare it with the KDE algorithms of Weston et al. (2003) and Cortes et al.",
      "startOffset" : 58,
      "endOffset" : 453
    }, {
      "referenceID" : 7,
      "context" : "(2003) and Cortes et al. (2005). In all these methods, we use RBF kernels for both input and output with the parameters shown in Table 2 (left).",
      "startOffset" : 11,
      "endOffset" : 32
    }, {
      "referenceID" : 19,
      "context" : "(Right) Performance (mean and standard deviation of Well Recognized word Characters (WRC)) of Max-Margin Markov Networks (MNs) algorithm (Taskar et al., 2004), constrained regression version of KDE (Cortes et al.",
      "startOffset" : 137,
      "endOffset" : 158
    }, {
      "referenceID" : 8,
      "context" : ", 2004), constrained regression version of KDE (Cortes et al., 2007), and our KDE method on an optical character recognition (OCR) task.",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 20,
      "context" : "(Left) Performance (mean and standard deviation of RBF loss) of the KDE algorithms of Weston et al. (2003) and Cortes et al.",
      "startOffset" : 86,
      "endOffset" : 107
    }, {
      "referenceID" : 7,
      "context" : "(2003) and Cortes et al. (2005), and our KDE method with covariance and conditional covariance operator-valued kernels on an image reconstruction problem of handwritten digits.",
      "startOffset" : 11,
      "endOffset" : 32
    }, {
      "referenceID" : 7,
      "context" : "In this problem, kPCA-based KDE performed better than the KDE formulation of Cortes et al. (2005). In fact, the latter is equivalent to using an identity-based operator-valued kernel in our formulation, and thus, it is incapable of capturing the dependencies in the output feature space (contrary to the other methods considered here).",
      "startOffset" : 77,
      "endOffset" : 98
    }, {
      "referenceID" : 8,
      "context" : "We compare our approach with a constrained regression version of Cortes’s KDE formulation (Cortes et al., 2007) and Max-Margin Markov Networks (MNs) (Taskar et al.",
      "startOffset" : 90,
      "endOffset" : 111
    }, {
      "referenceID" : 19,
      "context" : ", 2007) and Max-Margin Markov Networks (MNs) (Taskar et al., 2004).",
      "startOffset" : 45,
      "endOffset" : 66
    }, {
      "referenceID" : 8,
      "context" : "Results for these two methods are reported from (Cortes et al., 2007).",
      "startOffset" : 48,
      "endOffset" : 69
    }, {
      "referenceID" : 8,
      "context" : "We use exactly the same experimental setup described in (Cortes et al., 2007) to evaluate our operator-valued KDE approach.",
      "startOffset" : 56,
      "endOffset" : 77
    }, {
      "referenceID" : 17,
      "context" : "This problem is the one used in Taskar et al. (2004) and Cortes et al.",
      "startOffset" : 32,
      "endOffset" : 53
    }, {
      "referenceID" : 7,
      "context" : "(2004) and Cortes et al. (2005). The dataset is a subset of the handwritten words collected by Rob Kassel at the MIT Spoken Language Systems Group.",
      "startOffset" : 11,
      "endOffset" : 32
    }, {
      "referenceID" : 7,
      "context" : "In this experiment, we first compare the covariancebased operator-valued KDE with the KDE algorithm of Cortes et al. (2005) and the JKM approach of Weston et al.",
      "startOffset" : 103,
      "endOffset" : 124
    }, {
      "referenceID" : 7,
      "context" : "In this experiment, we first compare the covariancebased operator-valued KDE with the KDE algorithm of Cortes et al. (2005) and the JKM approach of Weston et al. (2007), and then show how we can speed up the training of our proposed KDE method using incomplete Cholesky decomposition; see (Kadri et al.",
      "startOffset" : 103,
      "endOffset" : 169
    }, {
      "referenceID" : 7,
      "context" : "In this experiment, we first compare the covariancebased operator-valued KDE with the KDE algorithm of Cortes et al. (2005) and the JKM approach of Weston et al. (2007), and then show how we can speed up the training of our proposed KDE method using incomplete Cholesky decomposition; see (Kadri et al., 2012, Appendix C) for more details on applying incomplete Cholesky decomposition to block kernel matrices associated to separable operator-valued kernels. Similar to the “learning to smile” experiment in Weston et al. (2007), we consider the problem of mapping the rotated view of a face to the plain expression (frontal view) of the same face.",
      "startOffset" : 103,
      "endOffset" : 529
    }, {
      "referenceID" : 21,
      "context" : "Mean-squared errors (MSE) of the JKM algorithm of Weston et al. (2007), KDE algorithm of Cortes et al.",
      "startOffset" : 50,
      "endOffset" : 71
    }, {
      "referenceID" : 7,
      "context" : "(2007), KDE algorithm of Cortes et al. (2005), and our KDE method with covariance operatorvalued kernels on the face-to-face mapping problem.",
      "startOffset" : 25,
      "endOffset" : 46
    }, {
      "referenceID" : 24,
      "context" : "We apply a JKM using the patch-wise joint kernel defined in (Weston et al., 2007), with patches of size 10×10 that overlap by 5 pixels.",
      "startOffset" : 60,
      "endOffset" : 81
    }, {
      "referenceID" : 7,
      "context" : "2 compares the performance of the efficient implementation (using incomplete Cholesky decomposition) of our conditional covariance operator-valued KDE method with the original KDE algorithm (Cortes et al., 2005).",
      "startOffset" : 190,
      "endOffset" : 211
    }, {
      "referenceID" : 7,
      "context" : "We compare the efficient implementation (using incomplete Cholesky decomposition) of our conditional covariance operator-valued KDE method with the KDE algorithm of Cortes et al. (2005). While the parameter n is m1 = m2 = n in the incomplete Cholesky decomposition, it is the number of faces randomly selected from 1,200 training faces in the KDE algorithm of Cortes et al.",
      "startOffset" : 165,
      "endOffset" : 186
    }, {
      "referenceID" : 7,
      "context" : "We compare the efficient implementation (using incomplete Cholesky decomposition) of our conditional covariance operator-valued KDE method with the KDE algorithm of Cortes et al. (2005). While the parameter n is m1 = m2 = n in the incomplete Cholesky decomposition, it is the number of faces randomly selected from 1,200 training faces in the KDE algorithm of Cortes et al. (2005). The right-most point is the MSE of training on the full training set of n = 1, 200 examples.",
      "startOffset" : 165,
      "endOffset" : 381
    } ],
    "year" : 2015,
    "abstractText" : "We study the problem of structured output learning from a regression perspective. We first provide a general formulation of the kernel dependency estimation (KDE) approach to this problem using operator-valued kernels. Our formulation overcomes the two main limitations of the original KDE approach, namely the decoupling between outputs in the image space and the inability to use a joint feature space. We then propose a covariance-based operator-valued kernel that allows us to take into account the structure of the kernel feature space. This kernel operates on the output space and only encodes the interactions between the outputs without any reference to the input space. To address this issue, we introduce a variant of our KDE method based on the conditional covariance operator that in addition to the correlation between the outputs takes into account the effects of the input variables. Finally, we evaluate the performance of our KDE approach on three structured output problems, and compare it to the state-of-the-art kernelbased structured output regression methods.",
    "creator" : "LaTeX with hyperref package"
  }
}