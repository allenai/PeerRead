{
  "name" : "1606.04646.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Unsupervised Learning of Predictors from Unpaired Input-Output Samples",
    "authors" : [ "Jianshu Chen", "Po-Sen Huang", "Xiaodong He", "Jianfeng Gao" ],
    "emails" : [ "deng}@microsoft.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Unsupervised learning, one major branch of machine learning involving learning without labeled data or without costly pairing input-output training data, has been a long standing research over decades. But it has achieved much less success compared with supervised learning that requires paired training data. Part of the difficulty in unsupervised learning is a lack of solid evaluation measures in the past. In this paper, we take a practical approach to grounding unsupervised learning using the same evaluation measure as that for supervised learning in prediction tasks without requiring paired input-output training samples. If successful, the benefit of such unsupervised learning would be tremendous. For example, in large scale commercial speech recognition systems, the currently dominant supervised learning methods typically require a few thousand hours of training material where each utterance in the acoustic form needs to be explicitly labeled with the corresponding word sequence by human. Although there are millions of hours of natural speech data available for training, labeling all of such acoustic data followed by supervised learning is simply not feasible. To make effective use of such huge amounts of acoustic data in speech recognition, the practical unsupervised learning approach outlined above would be called for.\nIn recent years, supervised learning has shown great successes in several major prediction tasks including speech recognition [13, 8], image recognition [20, 33], machine translation [28, 1], spoken language understanding [23, 22], and image captioning [10, 17, 30, 31]. These successes rely heavily on training highly expressive deep learning models using large amounts of labeled training data. That is, the training examples are input-output pairs, where the outputs are labels obtained typically by costly manual annotations. Unsupervised learning, however, is not as successful on these prediction tasks, although it has found other useful applications such as clustering [32], text analysis [5], etc. The majority of the work on unsupervised learning for prediction tasks in the past has been to\nar X\niv :1\n60 6.\n04 64\n6v 1\n[ cs\n.L G\n] 1\n5 Ju\nn 20\nexploit the learned representations of the input data as feature vectors which are subsequently fed to a separate classifier; e.g., [21]. This approach, albeit widely used, is usually less effective than end-to-end learning with labeled data [6]. Another important line of work on using unsupervised learning to help prediction is pre-training, where an unsupervised model trained using unlabeled data is used to initialize a separate supervised learning algorithm [15, 3, 2, 24, 9]. Pre-training is shown to be effective only when there is a small amount of labeled data available [13]. In prediction tasks with large amounts of paired training data, all the above unsupervised methods have played only an auxiliary role in helping supervised learning.\nIn this paper, we consider the unsupervised learning problem from a new and practical perspective. That is, instead of using unsupervised learning as an auxiliary step for supervised learning, we aim to develop an unsupervised learning algorithm that learns the input-to-output mapping (i.e., the predictor) from unpaired input-output training samples. Our approach has tremendous economic value in that it allows us to use a large amount of unlabeled data directly for prediction tasks. As we proceed to show in the paper, this is a very challenging problem since no clear and effective cost function has been established for such a problem in the literature. This paper represents our initial attempt to address this challenge by exploiting the sequence structure of the output samples to learn the predictor. This is dramatically different from most previous work which often exploits the structure of the input samples. The objective function we defined aims to make the predicted outputs fit well the structure of the output (e.g., a sequence structure that is learned separately using only output samples), while preserving the correlation between the input data and the predicted output labels. We will give a detailed study of this objective function on a predictive task in order to understand the nature and difficulties of the problem, as well as its potential solutions."
    }, {
      "heading" : "2 Related Work",
      "text" : "For unsupervised learning applied to prediction and related tasks, several main approaches have been taken in the past. An important line of research has been to focus on exploiting the structure of input data by learning the data distribution using maximum likelihood rule. The most successful examples in this category include the restricted Boltzmann machine (RBM) [26, 15], the deep belief network [14], topic models [5], etc. The main technical challenge of these methods is the difficulty of computing the gradient of the likelihood function exactly. For this reason, various approximate methods have been developed, such as variational inference [16] and Monte Carlo methods [12].\nAnother important development is the methods that avoid the difficulties that arise in using maximum likelihood rule as the direct learning objective. These methods include autoencoder [2], denoising autoencoder [29], variational autoencoder [18], and generative adversarial network (GAN) [11]. However, these methods have been developed also aiming to model the input data distribution instead of learning the input-to-output mapping from unpaired input-output data.\nA recent study that is more closely related to what we describe in this paper is [27], which proposes the output distribution matching (ODM) as an alternative unsupervised learning objective to the likelihood function of the data. The ODM cost function measures how well the distribution of each predicted output sample matches the distribution of target output samples. Dual autoencoder and GAN are used to implement the learning algorithm approximately. However, ODM does not exploit the structure of the output samples. In contrast, in the study reported in this paper, we explicitly exploit the sequence prior, a type of structure commonly found in speech and natural language data, of the output samples in the form of joint probability distribution of the outputs. We believe that the stronger the prior is, the better chance there is for this approach to work that exploits output distributions as the prior. The sequence prior is very strong, and in many possible applications such as speech recognition, machine translation, and image/video captioning, this sequence prior can be obtained from language models trained using a very large amount of text data freely available. The power of such a strong prior of language models in unsupervised learning has been demonstrated in an earlier study reported in [19].\nIn addition to exploiting output distributions as the structured prior, our approach further exploits other sources of prior information including the correlation between input and output. The latter is implemented in our work as a regularization term of the objective function, which is derived from a generative model with information flow from output to input. The use of generative models in our work is similar to an earlier study reported in [4] and to a more recent study reported in [25]. Finally,\nour proposed unsupervised learning cost can be directly optimized using stochastic gradient descent in an end-to-end manner."
    }, {
      "heading" : "3 Problem Formulation",
      "text" : "In this section, we first formulate the unsupervised learning problem. Let xt be t-th input vector, which is an M -dimensional real-valued vector, and let yt be the t-th output vector. In this paper, we consider the classification problem so that yt is a C-dimensional one-hot vector that represents one of the C classes. In prediction tasks, the objective is to learn the conditional probability p(yt|xt,Wd) from training samples, whereWd represents the model parameter. p(yt|xt,Wd) can be any parametric model such as neural networks.\nIn supervised learning problems, the training algorithm is presented with paired data (xt, yt), which are assumed to be generated from a ground truth distribution p(x1, . . . , xT , y1, . . . , yT ). A common supervised training objective is\nmax Wd T∑ t=1 ln p(yt|xt,Wd) (1)\nwhere T is the number of training examples. It is clear that the supervised learning problem requires us to label each xt with an output (label) yt in order to solve the above optimization problem (1).\nIn this paper, we consider the unsupervised learning of p(yt|xt,Wd) from unpaired training sequences {xt, t = 1, . . . , T} and {yt, t = 1, . . . , T}. The input samples {xt} and the output samples {yt} are unpaired in that they are not necessarily generated from the true joint distribution p(x1, . . . , xT , y1, . . . , yT ) that we are trying to learn, and they are only required to be distributed according to the respective marginal distributions, i.e., {xt} ∼ p(x1, . . . , xT ) and {yt} ∼ p(y1, . . . , yT ). Therefore, {xt} and {yt} could be collected from two completely independent sources. In the rest of the paper, we assume that the probability distribution p(y1, . . . , yT ) of the output samples has a sequence structure, i.e., there is temporal dependency over y1, . . . , yT . Furthermore, we assume that p(y1, . . . , yT ) is known a priori, which, as we pointed out earlier, could be estimated from a different data source that has the same distribution of p(y1, . . . , yT ).\nMore formally, our objective in this paper is to learn the posterior probability p(yt|xt,Wd) (i.e., the predictor) from the input sequence {xt} by exploiting the distribution p(y1, . . . , yT ) on the output sequence, where p(y1, . . . , yT ) is learned from another totally unpaired sequence {y1, . . . , yT }. Therefore, this is an unsupervised learning problem, which we will proceed to solve and analyze in the rest of the paper."
    }, {
      "heading" : "4 Learning to Predict from Unpaired Samples",
      "text" : "We now develop a novel cost function for learning the predictor p(yt|xt,Wd) in an unsupervised manner. The cost function is designed based on the following two key insights. First, given a predictor p(yt|xt,Wd), we want the predicted output sequence ŷ1, . . . , ŷT from the input sequence x1, . . . , xT to be consistent with the output distribution p(y1, . . . , yT ), with the definition of consistency to be explained later. Second, we want the predicted output ŷt to be based on the input xt; that is the output ŷt should be correlated with the input xt rather than completely independent of it. Therefore, our proposed cost function will have two terms. The first term measures how well the predicted output fit into the output distribution, and the second condition is a regularization term, which prevents the learning algorithm from overfitting into p(y1, . . . , yT ) and obtaining trivial solutions that generate ŷt completely independently of the input xt. Below, we formalize these ideas by developing these two terms in the cost function.\nWe first establish the first term in the novel unsupervised learning cost function. Note that, for each input sample xt, the parametric conditional distribution p(yt|xt,Wd) defines a probability of the corresponding output sample yt. When the predictor p(yt|xt,Wd) is applied to each sample in the input sequence x1, . . . , xT , and generates the output according to this distribution, we will generate a random output sequence ŷ1, . . . , ŷT . Then, the log-likelihood ln p(ŷ1, . . . , ŷT ) measures how well the generated sequence fit into the distribution p(y1, . . . , yT ). Motivated by this observation, we define the following term to measure the expected fitness of the predicted output with the current\npredictor:\nE [ ln p(y1, . . . , yT ) ∣∣x1, . . . , xT ] = E[ T∑ t=1 ln p(yt|yt−1, . . . , y1) ∣∣∣xt, . . . , x1]\n= ∑\n(yt,yt−1,...,y1)\nT∏ t=1 p(yt|xt,Wd) T∑ t=1 ln p(yt|yt−1, . . . , y1)\n= T∑ t=1 t−1∏ τ=1 p(yτ |xτ ) ∑ yt p(yt|xt) ln p(yt|yt−1, . . . , y1)\n= T∑ t=1 E [∑ yt p(yt|xt) ln p(yt|yt−1, . . . , y1) ∣∣∣xt−1, . . . , x1] (2)\nwhere the last expectation is evaluated with respect to ∏t−1 τ=1 p(yτ |xτ ,Wd). The learning algorithm seeks to maximize the above objective function (2) in order to make the predicted output sequence fit well into the prior distribution p(y1, . . . , yT ). We will further show in the next section that the global optimal solution to (2) is indeed the ground truth solution if the parametric model p(yt|xt,Wd) includes the ground truth as one of its solution.\nHowever, we will further reveal in the next section that this objective function has many local optima that are badly behaved. These local optima lead to trivial solutions, which completely ignore the input data and produce outputs that fit into p(y1, . . . , yT ). To address this issue, we introduce the second term in the cost function, which penalizes the solution that decouples the inputs and outputs. Specifically, we propose to use the following term\nT∑ t=1 E [ ln p(xt|yt,Wg)|xt ] = T∑ t=1 ∑ yt p(yt|xt,Wd) ln p(xt|yt,Wg) (3)\nwhere p(xt|yt,Wg) is a generative model parameterized by Wg for characterizing the information flow from output to input. The expression (3) has the following interpretation. For a given input sample xt, we generate an output sample yt according to the distribution p(yt|xt,Wd). Then for this particular sample yt, the score ln p(xt|yt,Wg) measures how well the generative model p(xt|yt,Wg) can predict the input xt. During the learning process, we seek to maximize this term with respect to Wg to maximize the generative model’s ability to reconstruct the input from the output. That is, the learning process also learns the best generative model that can reconstruct the input from the output.\nPutting these two terms together, we have the following cost function for learning the predictor from unpaired data:\nmax Wd,Wg T∑ t=1\n{ E [∑ yt p(yt|xt) ln p(yt|yt−1, . . . , y1) ∣∣∣xt−1, . . . , x1]+λ∑ yt p(yt|xt,Wd) ln p(xt|yt,Wg) } (4)\nwhere λ is a positive hyper-parameter that controls the relative ratio between the two terms. In the above optimization problem, we maximize the objective function with respect to both Wd and Wg. As we discussed earlier, the maximization with respect to Wg learns the best generative model to measure the “correlation” between the input and the predicted output from the discriminative model. Expression (3) shows that this term also depends on Wd, which means that by maximizing (4) with respect to Wd, we are also maximizing the correlation between the input and the predicted output, thereby regularizing the learning of the discriminative model p(yt|xt,Wd) to avoid trivial solutions. The above learning problem (4) can be solved by using stochastic gradient, and the gradients can be computed by back propagation if the discriminative model p(yt|xt,Wd) and the generative model p(xt|yt,Wd) are (deep) neural networks."
    }, {
      "heading" : "5 Experiments and Analysis",
      "text" : "In this section, we use a simplified prediction task on a synthetic dataset to study the effectiveness of the proposed approach. We will also analyze the behaviors of the proposed objective function in\norder to understand the nature and difficulties of the unsupervised learning problem for prediction along with its potential solutions."
    }, {
      "heading" : "5.1 Experimental setup",
      "text" : "The synthetic data we use to evaluate the algorithm are generated in the following manner. We first generate the output sequence y1, . . . , yT according to the distribution p(y1, . . . , yT ) = ∏T t=1 p(yt|yt−1), i.e., a Markov chain, which is described by Figure 1. And we consider a four-class classification problem so that yt is a 4-dimensional one-hot vector. After the sequence y1, . . . , yT is generated, we randomly generate a permutation matrix Q and fix it over time. For each yt, we generate xt by multiplying Q to the left of xt, i.e., xt = Qyt. Therefore, the inputs {xt} are also a 4-dimensional one-hot vectors except that each of them is transformed from the output yt according to an unknown permutation. Our objective is to learn p(yt|xt,Wd) from the input sequence x1, . . . , xT without the paired output sequence y1, . . . , yT . Instead, we only have a sequence of unpaired samples y1, . . . , yT that is generated according to the same distribution p(y1, . . . , yT ), from which we could estimate p(y1, . . . , yT ). In our study below, we choose p(yt|xt,Wd) and p(xt|yt,Wg) to be the softmax functions:\np(yt|xt,Wd) = softmax(γWdxt) p(xt|yt,Wd) = softmax(γWgyt) (5)\nwhere γ is a positive number that controls the sharpness of the softmax function. Even though we are using simple linear classifiers, as we proceed to reveal, the unsupervised learning cost is still highly non-convex and the problem remains difficult."
    }, {
      "heading" : "5.2 The landscape of the proposed unsupervised cost function",
      "text" : "We first plot the landscape of the cost function (4) for λ = 0 case and compare it with the supervised cost (cross-entropy) in Figure 2(a). Specifically, we plot the negative of the objective function (4) along the line tWd,0 + (1 − t)Wd,1, where t is a real scalar, Wd,0 is the ground truth (obtained from the permutation matrix) and Wd,1 is the finally converged solution by optimizing (4) without regularization (λ = 0). Obviously, the objective function is highly-nonconvex. On the other hand, the cost function for supervised learning is convex since the classifier is linear. An important observation\nwe can make from Figure 2(a) is that the global optimal solution to (2) (i.e., the first term in (4)) coincides with the global optimal solution of the supervised learning problem. On the other hand, there is a local optimal solution, which the algorithm could easily get stuck in, as shown in the figure. We also note that the cost function of the local optimal solution seems to be very close to that of the global optimal solution. There are two important questions to ask: (i) how good is this local optimal solution in compare with the global optimal solution, and (ii) how does the regularization term (second term in (4)) help the algorithm escape from local optima. To answer the first question, we visualize the weight matrix Wd in the middle part of Figure 2(c). We observe that the columns of the matrix are linearly dependent and the matrix is almost rank one by computing its singular values. With Wd being rank-1 (e.g., Wd ≈ abT ), the probability p(yt|xt,Wd) = softmax(γabTxt) = softmax(a), which is independent of xt. Therefore, this local optimal solution is a trivial solution which totally ignores the inputs, although its cost is close to that of the global optimal solution. We repeated the experiments many times and all the local optimal solutions end up with rank-1. In Figures 3(a) and 3(b), we plot more landscapes of the supervised and unsupervised cost functions along other random lines that pass through the ground truth solution. From the figures, we note similar behaviors as in Figure 2."
    }, {
      "heading" : "5.3 The importance of regularization",
      "text" : "We now address the second question on the importance of regularization. In Figure 2(b), we plot the landscapes of the unsupervised cost function (4) for λ = 0 and λ = 30. The landscapes show the values of the cost function along a random line that passes through the ground truth (global optimal solution). We observe that the regularization term creates a “slope” at the original position of the local optimal solution, which allows the algorithm to escape from the trivial solution. In Figures 3(b) and 3(d), we plot more landscapes for the unsupervised cost with different levels of regularization\nand note similar behaviors, where the local optima are smoothed out by the regularization term. In the end, the obtained solution with λ = 30 is shown in the right part of Figure 2(c). As a reference, we also put the global optimal solution to the supervised problem in the left part of Figure 2(c). We see that the solution obtained from unsupervised learning problem (4) with λ = 30 is very close to the supervised solution.\n5.4 The impact of imperfect p(y1, . . . , yT )\nSo far we have only considered the case where the probability p(y1, . . . , yT ) is precisely known. In practice, this prior probability is estimated from a separate data sequence, which would always have estimation error. To examine the robustness of the algorithm with respect to the estimation error of p(y1, . . . , yT ) (in this synthetic data case, p(y1, . . . , yT ) is represented by the transition matrix P of the Markov chain in Figure 1), we add different levels of noise to the transition matrix be P ← P +N (0, σ2P ) (and normalize the columns of P so that they sum up to one) and evaluate the performance of the unsupervised learning algorithm. The test error for different variance of noise (σ2P ) and different λ are shown in Figure 4. As the estimation error of p(y1, . . . , yT ) increases, the performance of the unsupervised learning algorithm degrades. Furthermore, it is also noticeable that the regularization parameter λ has to be set to a reasonable value to achieve the best performance. This is not surprising because if λ is too small, the “slope” created by the regularization is not steep enough. On the other hand, if λ is too large, the regularization term will overwhelm the first term (which contains the information regarding p(yt|xt)) in (4) so that the algorithm is not able to learn meaningful information."
    }, {
      "heading" : "6 Conclusions",
      "text" : "In this paper we study the important problem of unsupervised learning for prediction tasks, which is to learn to predict without using input-label paired data. We address this challenging problem by exploiting the sequence structure of the output samples to learn the predictor. That is, we proposed an objective function that aims to make the predicted outputs fit into the structure of the output while preserving the correlation between the input and the predicted output. On a synthetic structural prediction problem, we show that, even with simple linear classifiers, the objective function is already highly non-convex. On the other hand, this objective function converges to an optimal solution. We are currently investigating the behavior of more complicated and realistic models with real-world data.\nAlong this line of research, a recent work [7] shows that the local optima during supervised learning of the deep neural networks are well behaved. However, as we have demonstrated in our paper, this is not the case in the unsupervised learning problem, where the other local optimal solutions represent trivial solutions, although the values of the cost function are close to the global optimum. This leads to a further question on how to design even better objective functions to eliminate the trivial solutions from the set of local optima."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1409.0473,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2014
    }, {
      "title" : "Learning deep architectures for AI",
      "author" : [ "Yoshua Bengio" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2009
    }, {
      "title" : "Greedy layer-wise training of deep networks",
      "author" : [ "Yoshua Bengio", "Pascal Lamblin", "Dan Popovici", "Hugo Larochelle" ],
      "venue" : "In Proceedings of the Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2007
    }, {
      "title" : "Unsupervised transcription of historical documents",
      "author" : [ "Taylor Berg-Kirkpatrick", "Greg Durrett", "Dan Klein" ],
      "venue" : "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2013
    }, {
      "title" : "Latent dirichlet allocation",
      "author" : [ "David M. Blei", "Andrew Y. Ng", "Michael I. Jordan" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2003
    }, {
      "title" : "End-to-end learning of lda by mirror-descent back propagation over a deep architecture",
      "author" : [ "Jianshu Chen", "Ji He", "Yelong Shen", "Lin Xiao", "Xiaodong He", "Jianfeng Gao", "Xinying Song", "Li Deng" ],
      "venue" : "In Proceedings of the Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2015
    }, {
      "title" : "The loss surfaces of multilayer networks",
      "author" : [ "Anna Choromanska", "Mikael Henaff", "Michael Mathieu", "Gérard Ben Arous", "Yann LeCun" ],
      "venue" : "In Proceedings of AISTATS,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2015
    }, {
      "title" : "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. Audio, Speech, and Language Processing",
      "author" : [ "George E Dahl", "Dong Yu", "Li Deng", "Alex Acero" ],
      "venue" : "IEEE Transactions on,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2012
    }, {
      "title" : "Semi-supervised sequence learning",
      "author" : [ "Andrew M Dai", "Quoc V Le" ],
      "venue" : "In Proceedings of the Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "From captions to visual concepts and back",
      "author" : [ "Hao Fang", "Saurabh Gupta", "Forrest Iandola", "Rupesh Srivastava", "Li Deng", "Piotr Dollar", "Jianfeng Gao", "Xiaodong He", "Margaret Mitchell", "John Platt", "Lawrence Zitnick", "Geoffrey Zweig" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio" ],
      "venue" : "In Proceedings of the Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2014
    }, {
      "title" : "Monte carlo sampling methods using markov chains and their applications",
      "author" : [ "W Keith Hastings" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1970
    }, {
      "title" : "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups",
      "author" : [ "Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-Rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N. Sainath", "B. Kingsbury" ],
      "venue" : "IEEE Signal Processing Magazine,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "A fast learning algorithm for deep belief nets",
      "author" : [ "Geoffrey E Hinton", "Simon Osindero", "Yee-Whye Teh" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2006
    }, {
      "title" : "Reducing the dimensionality of data with neural networks",
      "author" : [ "Geoffrey E Hinton", "Ruslan R Salakhutdinov" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2006
    }, {
      "title" : "An introduction to variational methods for graphical models",
      "author" : [ "Michael I Jordan", "Zoubin Ghahramani", "Tommi S Jaakkola", "Lawrence K Saul" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1999
    }, {
      "title" : "Deep visual-semantic alignments for generating image descriptions",
      "author" : [ "Andrej Karpathy", "Li Fei-Fei" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2015
    }, {
      "title" : "Auto-encoding variational bayes",
      "author" : [ "Diederik P Kingma", "Max Welling" ],
      "venue" : "arXiv preprint arXiv:1312.6114,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2013
    }, {
      "title" : "Unsupervised analysis for decipherment problems",
      "author" : [ "Kevin Knight", "Anish Nair", "Nishit Rathod", "Kenji Yamada" ],
      "venue" : "In Proceedings of the COLING/ACL,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2006
    }, {
      "title" : "ImageNet classification with deep convolutional neural networks",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton" ],
      "venue" : "In Proceedings of the Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2012
    }, {
      "title" : "Building high-level features using large scale unsupervised learning",
      "author" : [ "Quoc Le", "Marc’Aurelio Ranzato", "Rajat Monga", "Matthieu Devin", "Kai Chen", "Greg Corrado", "Jeff Dean", "Andrew Ng" ],
      "venue" : "In International Conference in Machine Learning,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2012
    }, {
      "title" : "Using recurrent neural networks for slot filling in spoken language understanding",
      "author" : [ "Grégoire Mesnil", "Yann Dauphin", "Kaisheng Yao", "Yoshua Bengio", "Li Deng", "Dilek Hakkani-Tur", "Xiaodong He", "Larry Heck", "Gokhan Tur", "Dong Yu" ],
      "venue" : null,
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2015
    }, {
      "title" : "Investigation of recurrent-neural-network architectures and learning methods for spoken language understanding",
      "author" : [ "Grégoire Mesnil", "Xiaodong He", "Li Deng", "Yoshua Bengio" ],
      "venue" : "In Interspeech,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2013
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean" ],
      "venue" : "arXiv preprint arXiv:1301.3781,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2013
    }, {
      "title" : "Semi-supervised learning with ladder networks",
      "author" : [ "Antti Rasmus", "Mathias Berglund", "Mikko Honkala", "Harri Valpola", "Tapani Raiko" ],
      "venue" : "In Proceedings of the Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2015
    }, {
      "title" : "Parallel distributed processing: Explorations in the microstructure of cognition, vol. 1. chapter Information Processing in Dynamical Systems: Foundations of Harmony Theory, pages 194–281",
      "author" : [ "P. Smolensky" ],
      "venue" : null,
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 1986
    }, {
      "title" : "Towards principled unsupervised learning",
      "author" : [ "Ilya Sutskever", "Rafal Jozefowicz", "Karol Gregor", "Danilo Rezende", "Tim Lillicrap", "Oriol Vinyals" ],
      "venue" : "arXiv preprint arXiv:1511.06440,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2015
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le" ],
      "venue" : "In Proceedings of the Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2014
    }, {
      "title" : "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion",
      "author" : [ "Pascal Vincent", "Hugo Larochelle", "Isabelle Lajoie", "Yoshua Bengio", "Pierre-Antoine Manzagol" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2010
    }, {
      "title" : "Show and tell: A neural image caption generator",
      "author" : [ "Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2015
    }, {
      "title" : "Show, attend and tell: Neural image caption generation with visual attention",
      "author" : [ "Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1502.03044,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2015
    }, {
      "title" : "Survey of clustering algorithms",
      "author" : [ "Rui Xu", "Donald Wunsch" ],
      "venue" : "IEEE Transactions on Neural Networks,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2005
    }, {
      "title" : "Visualizing and understanding convolutional networks",
      "author" : [ "Matthew D Zeiler", "Rob Fergus" ],
      "venue" : "In Proceedings of the European Conference on Computer Vision (ECCV),",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "In recent years, supervised learning has shown great successes in several major prediction tasks including speech recognition [13, 8], image recognition [20, 33], machine translation [28, 1], spoken language understanding [23, 22], and image captioning [10, 17, 30, 31].",
      "startOffset" : 126,
      "endOffset" : 133
    }, {
      "referenceID" : 7,
      "context" : "In recent years, supervised learning has shown great successes in several major prediction tasks including speech recognition [13, 8], image recognition [20, 33], machine translation [28, 1], spoken language understanding [23, 22], and image captioning [10, 17, 30, 31].",
      "startOffset" : 126,
      "endOffset" : 133
    }, {
      "referenceID" : 19,
      "context" : "In recent years, supervised learning has shown great successes in several major prediction tasks including speech recognition [13, 8], image recognition [20, 33], machine translation [28, 1], spoken language understanding [23, 22], and image captioning [10, 17, 30, 31].",
      "startOffset" : 153,
      "endOffset" : 161
    }, {
      "referenceID" : 32,
      "context" : "In recent years, supervised learning has shown great successes in several major prediction tasks including speech recognition [13, 8], image recognition [20, 33], machine translation [28, 1], spoken language understanding [23, 22], and image captioning [10, 17, 30, 31].",
      "startOffset" : 153,
      "endOffset" : 161
    }, {
      "referenceID" : 27,
      "context" : "In recent years, supervised learning has shown great successes in several major prediction tasks including speech recognition [13, 8], image recognition [20, 33], machine translation [28, 1], spoken language understanding [23, 22], and image captioning [10, 17, 30, 31].",
      "startOffset" : 183,
      "endOffset" : 190
    }, {
      "referenceID" : 0,
      "context" : "In recent years, supervised learning has shown great successes in several major prediction tasks including speech recognition [13, 8], image recognition [20, 33], machine translation [28, 1], spoken language understanding [23, 22], and image captioning [10, 17, 30, 31].",
      "startOffset" : 183,
      "endOffset" : 190
    }, {
      "referenceID" : 22,
      "context" : "In recent years, supervised learning has shown great successes in several major prediction tasks including speech recognition [13, 8], image recognition [20, 33], machine translation [28, 1], spoken language understanding [23, 22], and image captioning [10, 17, 30, 31].",
      "startOffset" : 222,
      "endOffset" : 230
    }, {
      "referenceID" : 21,
      "context" : "In recent years, supervised learning has shown great successes in several major prediction tasks including speech recognition [13, 8], image recognition [20, 33], machine translation [28, 1], spoken language understanding [23, 22], and image captioning [10, 17, 30, 31].",
      "startOffset" : 222,
      "endOffset" : 230
    }, {
      "referenceID" : 9,
      "context" : "In recent years, supervised learning has shown great successes in several major prediction tasks including speech recognition [13, 8], image recognition [20, 33], machine translation [28, 1], spoken language understanding [23, 22], and image captioning [10, 17, 30, 31].",
      "startOffset" : 253,
      "endOffset" : 269
    }, {
      "referenceID" : 16,
      "context" : "In recent years, supervised learning has shown great successes in several major prediction tasks including speech recognition [13, 8], image recognition [20, 33], machine translation [28, 1], spoken language understanding [23, 22], and image captioning [10, 17, 30, 31].",
      "startOffset" : 253,
      "endOffset" : 269
    }, {
      "referenceID" : 29,
      "context" : "In recent years, supervised learning has shown great successes in several major prediction tasks including speech recognition [13, 8], image recognition [20, 33], machine translation [28, 1], spoken language understanding [23, 22], and image captioning [10, 17, 30, 31].",
      "startOffset" : 253,
      "endOffset" : 269
    }, {
      "referenceID" : 30,
      "context" : "In recent years, supervised learning has shown great successes in several major prediction tasks including speech recognition [13, 8], image recognition [20, 33], machine translation [28, 1], spoken language understanding [23, 22], and image captioning [10, 17, 30, 31].",
      "startOffset" : 253,
      "endOffset" : 269
    }, {
      "referenceID" : 31,
      "context" : "Unsupervised learning, however, is not as successful on these prediction tasks, although it has found other useful applications such as clustering [32], text analysis [5], etc.",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 4,
      "context" : "Unsupervised learning, however, is not as successful on these prediction tasks, although it has found other useful applications such as clustering [32], text analysis [5], etc.",
      "startOffset" : 167,
      "endOffset" : 170
    }, {
      "referenceID" : 20,
      "context" : ", [21].",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 5,
      "context" : "This approach, albeit widely used, is usually less effective than end-to-end learning with labeled data [6].",
      "startOffset" : 104,
      "endOffset" : 107
    }, {
      "referenceID" : 14,
      "context" : "Another important line of work on using unsupervised learning to help prediction is pre-training, where an unsupervised model trained using unlabeled data is used to initialize a separate supervised learning algorithm [15, 3, 2, 24, 9].",
      "startOffset" : 218,
      "endOffset" : 235
    }, {
      "referenceID" : 2,
      "context" : "Another important line of work on using unsupervised learning to help prediction is pre-training, where an unsupervised model trained using unlabeled data is used to initialize a separate supervised learning algorithm [15, 3, 2, 24, 9].",
      "startOffset" : 218,
      "endOffset" : 235
    }, {
      "referenceID" : 1,
      "context" : "Another important line of work on using unsupervised learning to help prediction is pre-training, where an unsupervised model trained using unlabeled data is used to initialize a separate supervised learning algorithm [15, 3, 2, 24, 9].",
      "startOffset" : 218,
      "endOffset" : 235
    }, {
      "referenceID" : 23,
      "context" : "Another important line of work on using unsupervised learning to help prediction is pre-training, where an unsupervised model trained using unlabeled data is used to initialize a separate supervised learning algorithm [15, 3, 2, 24, 9].",
      "startOffset" : 218,
      "endOffset" : 235
    }, {
      "referenceID" : 8,
      "context" : "Another important line of work on using unsupervised learning to help prediction is pre-training, where an unsupervised model trained using unlabeled data is used to initialize a separate supervised learning algorithm [15, 3, 2, 24, 9].",
      "startOffset" : 218,
      "endOffset" : 235
    }, {
      "referenceID" : 12,
      "context" : "Pre-training is shown to be effective only when there is a small amount of labeled data available [13].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 25,
      "context" : "The most successful examples in this category include the restricted Boltzmann machine (RBM) [26, 15], the deep belief network [14], topic models [5], etc.",
      "startOffset" : 93,
      "endOffset" : 101
    }, {
      "referenceID" : 14,
      "context" : "The most successful examples in this category include the restricted Boltzmann machine (RBM) [26, 15], the deep belief network [14], topic models [5], etc.",
      "startOffset" : 93,
      "endOffset" : 101
    }, {
      "referenceID" : 13,
      "context" : "The most successful examples in this category include the restricted Boltzmann machine (RBM) [26, 15], the deep belief network [14], topic models [5], etc.",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 4,
      "context" : "The most successful examples in this category include the restricted Boltzmann machine (RBM) [26, 15], the deep belief network [14], topic models [5], etc.",
      "startOffset" : 146,
      "endOffset" : 149
    }, {
      "referenceID" : 15,
      "context" : "For this reason, various approximate methods have been developed, such as variational inference [16] and Monte Carlo methods [12].",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 11,
      "context" : "For this reason, various approximate methods have been developed, such as variational inference [16] and Monte Carlo methods [12].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 1,
      "context" : "These methods include autoencoder [2], denoising autoencoder [29], variational autoencoder [18], and generative adversarial network (GAN) [11].",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 28,
      "context" : "These methods include autoencoder [2], denoising autoencoder [29], variational autoencoder [18], and generative adversarial network (GAN) [11].",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 17,
      "context" : "These methods include autoencoder [2], denoising autoencoder [29], variational autoencoder [18], and generative adversarial network (GAN) [11].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 10,
      "context" : "These methods include autoencoder [2], denoising autoencoder [29], variational autoencoder [18], and generative adversarial network (GAN) [11].",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 26,
      "context" : "A recent study that is more closely related to what we describe in this paper is [27], which proposes the output distribution matching (ODM) as an alternative unsupervised learning objective to the likelihood function of the data.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 18,
      "context" : "The power of such a strong prior of language models in unsupervised learning has been demonstrated in an earlier study reported in [19].",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 3,
      "context" : "The use of generative models in our work is similar to an earlier study reported in [4] and to a more recent study reported in [25].",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 24,
      "context" : "The use of generative models in our work is similar to an earlier study reported in [4] and to a more recent study reported in [25].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 6,
      "context" : "Along this line of research, a recent work [7] shows that the local optima during supervised learning of the deep neural networks are well behaved.",
      "startOffset" : 43,
      "endOffset" : 46
    } ],
    "year" : 2016,
    "abstractText" : "Unsupervised learning is the most challenging problem in machine learning and especially in deep learning. Among many scenarios, we study an unsupervised learning problem of high economic value — learning to predict without costly pairing of input data and corresponding labels. Part of the difficulty in this problem is a lack of solid evaluation measures. In this paper, we take a practical approach to grounding unsupervised learning by using the same success criterion as for supervised learning in prediction tasks but we do not require the presence of paired input-output training data. In particular, we propose an objective function that aims to make the predicted outputs fit well the structure of the output while preserving the correlation between the input and the predicted output. We experiment with a synthetic structural prediction problem and show that even with simple linear classifiers, the objective function is already highly non-convex. We further demonstrate the nature of this non-convex optimization problem as well as potential solutions. In particular, we show that with regularization via a generative model, learning with the proposed unsupervised objective function converges to an optimal solution.",
    "creator" : "LaTeX with hyperref package"
  }
}