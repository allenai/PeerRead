{
  "name" : "1507.04646.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Dependency-Based Neural Network for Relation Classification",
    "authors" : [ "Yang Liu", "Furu Wei Sujian Li", "Heng Ji Ming Zhou", "Houfeng Wang" ],
    "emails" : [ "wanghf}@pku.edu.cn", "mingzhou}@microsoft.com", "jih@rpi.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 7.\n04 64\n6v 1\n[ cs\n.C L\n] 1\n6 Ju\nl 2 01\n5"
    }, {
      "heading" : "1 Introduction",
      "text" : "Relation classification aims to classify the semantic relations between two entities in a sentence. It plays a vital role in robust knowledge extraction from unstructured texts and serves as an intermediate step in a variety of natural language processing applications. Most existing approaches follow a machine learning based framework and focus on designing effective features to obtain better classification performance.\nThe effectiveness of using dependency relations between entities for relation classification has been reported in previous approaches (Bach and Badaskar, 2007). For example, Suchanek et al. (2006) carefully selected a set of features from tokenization and dependency parsing, and extended some of them to\n∗Contribution during internship at Microsoft Research.\ngenerate high order features in different ways. Culotta and Sorensen (2004) designed a dependency tree kernel and attached more information including Part-of-Speech tag, word chunking tag to each node in the tree. Interestingly, Bunescu and Mooney (2005) provided an important insight that the shortest path between two entities in a dependency graph concentrates most of the information for identifying the relation between them. Nguyen et al. (2007) developed these ideas by analyzing multiple subtrees with the guidance of pre-extracted keywords. Previous work showed that the most useful dependency information in relation classification includes the shortest dependency path and dependency subtrees. These two kinds of information serve different functions and their collaboration can boost the performance of relation classification (see Section 2 for detailed examples). However, how to uniformly and efficiently combine these two components is still an open problem. In this paper, we propose a novel structure named Augmented Dependency Path (ADP) which attaches dependency subtrees to words on a shortest dependency path and focus on exploring the semantic representation behind the ADP structure.\nRecently, deep learning techniques have been widely used in modeling complex structures. This provides us an opportunity to model the ADP structure in a neural network framework. Thus, we propose a dependency-based neural network where two sub-neural networks are used to model shortest dependency paths and dependency subtrees respectively. One convolutional neural network (CNN) is applied over the shortest dependency path, because CNN is suitable for capturing the most useful features in a flat structure. A recursive neural network (RNN) is used for extracting semantic representations from the dependency subtrees, since RNN is good at modeling hierarchical structures. To connect these two sub-\nnetworks, each word on the shortest path is combined with a representation generated from its subtree, strengthening the semantic representation of the shortest path. In this way, the augmented dependency path is represented as a continuous semantic vector which can be further used for relation classification.\nThe major contributions of the work presented in this paper are as follows.\n1. We extend the shortest dependency path into the augmented dependency path to better model the relation between two entities.\n2. We propose a dependency-based neural network, DepNN, to model the augmented dependency path. It combines the advantages of the convolutional neural network and the recursive neural network.\n3. We conduct extensive experiments on the SemEval 2010 dataset and the experimental results show that DepNN outperforms baseline methods and yields state-of-the-art F1 measure on the relation classification task."
    }, {
      "heading" : "2 Problem Definition and Motivation",
      "text" : "The task of relation classification can be defined as follows. Given a sentence S with a pair of entities e1 and e2 annotated, the task is to identify the semantic relation between e1 and e2 in accordance with a set of predefined relation\ntypes. According to the the official guideline of SemEval-2010 task 8 (Hendrickx et al., 2010), there are 9 ordered relation types. We list them in Table 1 with their simplified definitions. Instances don’t fall in any of these types are labeled as Other. For example, in Figure 2, the relation between two entities e1=thief and e2=screwdriver is Instrument-Agency.\nBunescu and Mooney (2005) reported that, for the relation classification task, the shortest dependency path between two entities plays a vital role. They pointed out that this kind of paths can capture the predicate-argument sequences, providing helpful information for relation classification. For example, in Figure 2a, the shortest path includes\nthe structure of “broke prep with screwdriver”, helping judging the Instrument-Agency relation.\nAlthough the shortest dependency paths prove useful for relation classification, there exists other information on the dependency tree that can be exploited to represent the relation more precisely. For example, Figure 2a and 2b show two instances which have similar shortest dependency paths but belong to different relation types. In this situation, if we only use the shortest dependency paths for judging relation types, it is difficult for us to distinguish these two instances. However, we notice that the subtrees attached to the shortest dependency paths such as “dobj→commandment” and “dobj→ignition” can provide supplemental information for relation classification. Based on many observations like this, we propose the idea that we should employ these subtrees and combine them with the shortest path to form a more precise structure for classifying relations. This combined structure is called “augmented dependency path (ADP)”, as illustrated in Figure 2.\nNext, our goal is to capture the semantic representation of the ADP structure between two entities. The key problem here is how to combine the two components of ADP to incorporate more information. We propose that on the augmented dependency path, a word should be represented by both itself and its attached subtree. This is because the word itself contains its general meaning while the subtree can provide semantic information about how this word functions in this specific sentence. With this idea, we adopt the re-\ncursive neural network (RNN) that is proved suitable for modeling hierarchical structures to build semantic embeddings for the words on the shortest path along with their subtrees. After obtaining these more precise word representations, a convolutional neural network (CNN) can be applied, since it is good at modeling flat structures and can generate a fix-sized vector containing the most relevant features."
    }, {
      "heading" : "3 Dependency-Based Neural Networks",
      "text" : "In this section, we will introduce how we use neural network techniques and dependency information to explore the semantic connection between two entities. We name our architecture of modeling ADP structures as dependency-based neural networks (DepNN). Figure 3 illustrates DepNN with a concrete example. First, we associate each word w and dependency relation r with a vector representation xw,xr ∈ Rdim. For each word w on the shortest dependency path, we develop an RNN from its leaf words up to the root to generate a subtree embedding cw and concatenate cw with xw to serve as the final representation of w.\nNext, a CNN is designed to model the shortest dependency path based on the representation of its words and relations. Finally our framework can efficiently represent the semantic connection between two entities with consideration of more comprehensive dependency information."
    }, {
      "heading" : "3.1 Modeling Dependency Subtree",
      "text" : "The goal of modeling dependency subtrees is to find an appropriate representation for the words on the shortest path. As mentioned above, we assume that each word w can be interpreted by itself and its children on the dependency subtree. Then, for each word w on the subtree, its word embedding xw ∈ Rdim and subtree representation cw ∈ Rdimc are concatenated to form its final representation pw ∈ Rdim+dimc . For a word that does not have a subtree, we set its subtree representation as cLEAF . The subtree representation of a word is derived through transforming the representations of its children words. During the bottom-up construction of the subtree, each word is associated with a dependency relation such as dobj as in Figure 3.\nTake the ADP in Figure 3 for example, we first compute leaves’ representations like pthe,\npthe = [xthe, cLEAF ] (1)\nOnce all leaves are finished, we move to interior nodes with already processed children. In the example, continuing from “the” to its parent, “Sabbath”, we compute\npSabbath = [xSabbath, cSabbath] (2)\ncSabbath = f(Wdet · pthe + b) (3)\nwhere f is a non-linear activation function such as tanh, Wdet is the transformation matrix associated with dependency relation det and b is a bias term. We repeat this process until we reach the root on the shortest path, which in this case is “broke”,\npbroke = [xbroke, cbroke]\ncbroke = f(Wprep−on · pSabbath\n+Wdobj · pcommandament)\nThe composition equation for any word w with children Q(w) is,\ncw = f( ∑\nq∈Children(w)\nWR(w,q) · pq + b) (4)\npq = [xq, cq] (5)\nwhere R(w,q) denotes the dependency relation between word w and its child word q. This process continues recursively from leaves up to the root words on the shortest path. Each of these words will have a vector representation after this stage (ppriests, pbroke and pwork in this example)."
    }, {
      "heading" : "3.2 Modeling Shortest Dependency Path",
      "text" : "To classify the relation between two entities, we further explore the semantic representation behind their shortest dependency path, which can be seen as sequence of words interspersed with dependency relations. Take the shortest dependency path in last subsection for example. The sequence S will be,\nS: [priests nsubj broke prep-with work]\nw1 r1 w2 r2 w3\nAs the convolutional neural network (CNN) is good at capturing the salient features from a sequence of objects, we design a CNN to tackle the shortest dependency path.\nA CNN contains a convolution operation over windows of object representations, followed by a pooling operation. As we know, a word w on the shortest path is associated with the representation pw through modeling the subtree. For a dependency relation r on the shortest path, we set its representation as a vector xr ∈ Rdim. As a sliding window is applied on the sequence, we set the window size as k. For example, when k = 3, the sliding windows of S are {[rs w1 r1], [r1 w2 r2], [r2 w3 re]} where rs and re are used to denote the beginning and end of a shortest dependency path between two entities.\nWe concatenate k neighboring word (or dependency relation) representations within one window into a new vector. Assume Xi ∈ Rdim·k+dimc·nw as the concatenated representation of the i-th window, where nw is the number of words in one window. A convolution operation involves a filter W1 ∈ Rl×(dim·k+dimc·nw), which operates on Xi to produce a new feature vector Li with l dimensions,\nLi = W1Xi (6)\nwhere the bias term is ignored for simplicity. Then W1 is applied to each possible window in the shortest dependency path to produce a feature map: [L0,L1,L2, · · · ]. Next, we adopt the widely-used max-over-time pooling operation (Collobert et al., 2011), which can retain the most important features, to obtain the final representation L from the feature map. That is, L = max(L0,L1,L2, . . . ).\nBy this means, we are able to obtain the semantic representation of ADP with advantages of both RNN and CNN."
    }, {
      "heading" : "3.3 Learning",
      "text" : "Like other relation classification systems, we also incorporate some lexical level features which are proved useful for this task. This includes named entity tags and WordNet hypernyms of e1 and e2. We concatenate them with the ADP representation L to produce a combined vector M . We then pass M to a fully connected softmax layer whose output is the probability distribution y over relation labels.\nM = [L,LEX] (7)\ny = softmax(W2M) (8)\nWe define the ground-truth label vector t for each instance as a binary vector. If the instance belongs to the the i-th type, only ti is 1 and the other dimensions are set to 0. To learn the parameters, we optimize the cross-entropy error between y and t using stochastic gradient descent (Bottou, 2004). For each training instance, we define the objective function as:\nmin θ (−\nln∑\nj\ntjlog(yj)) (9)\nwhere θ represents the parameters. Gradients are computed using backpropagation (Rumelhart et al., 1988)."
    }, {
      "heading" : "4 Experiments",
      "text" : "Our experiments are performed on SemEval-2010 dataset (Hendrickx et al., 2010). The training part of the dataset includes 8000 instances, and the test part includes 2717 instances. Table 2 shows the statistics of the annotated relation types of this dataset. We can see that the distribution of relation types in the test set is similar to that in the training set. The official evaluation metric is the macro-averaged F1-score (excluding Other). We use dependency trees generated by the Stanford Parser (Klein and Manning, 2003) with the “collapsed” option, which regards a preposition as a kind of dependency relation. As de Marneffe and Manning (2008) pointed out, this option is more useful for event relation extraction."
    }, {
      "heading" : "4.1 Analysis of DepNN",
      "text" : ""
    }, {
      "heading" : "4.1.1 Contributions of different components",
      "text" : "We first show the contributions from different components of DepNN. In our experiments, two\nkinds of word embeddings are used for initialization. One is the 50-d embeddings provided by SENNA (Collobert et al., 2011). The second is the 200-d embeddings (Yu et al., 2014) trained on Gigaword with word2vec1. The corresponding hyperparameters are set with 5-fold cross validation, including window size k, learning rate λ, subtree embedding’s dimension dimc, and hidden layer size l. The final settings are shown in Table 3.\nFor evaluation, we first design a relation extraction system (named PATH) which only models the shortest dependency path with a CNN. Based on PATH, We consider to incorporate the two kinds of lexical features including named entity tags (NER) and WordNet hypernyms (WN). Then, we get two systems which are named PATH+WN and PATH+NER respectively. We also add the attached subtrees (SUB) modeled by an RNN to form the complete augmented dependency path.\nFrom Table 4, we can verify the effectiveness\n1https://code.google.com/p/word2vec/\nof modeling the shortest dependency path with a CNN, since PATH can achieve a relatively high result. The experiment results also indicate that both the NER and WordNet features can improve the performance of relation extraction. WordNet seems less useful than NER, which conforms to the results of Yu et al. (2014) , since a large number of WordNet hypernyms may cause overfitting. Furthermore, the attached subtrees, as we expect, can provide an obvious boost to DepNN. The NER tags, WordNet hypernyms and subtrees all contribute to the performance by providing supplemental information for words on the shortest path. The experiments show that the subtree information does a better job than the other two kinds of information and can help build more precise representations for words in a sentence. To get a deeper understanding of what semantic information can be captured behind the ADP structure, we will look into our model and analyze it with specific examples. Since the Gigaword embeddings, with its larger corpus and dimensions, can significantly improve the classification performance, the following experiments and analysis are all based on Gigaword embeddings."
    }, {
      "heading" : "4.1.2 Intuitive Analysis of Shortest Path",
      "text" : "We take the output vector of the CNN layer as the distributed representation of a dependency path. In this way, we can calculate the cosine similarity between any two paths and illustrate some paths with high similarity. Table 5 shows three training instances with different relation types and their three most similar paths in the test set.\nFrom Table 5, we can see that our approach can capture the core meaning of the shortest dependency paths. For example, for the InstrumentAgency relation, we infer that the dependency relations “nsubj inv”, “dobj” and “prep with” in the dependency path play a main role in the representation and our model can capture these similar paths. For the Product-Producer relation, our model focuses on representing the structure of “nsubj inv verb1 xcomp verb2 dobj” and exploits some words like “pencil” and “create” in the path representation. This is clearer for the MessageTopic relation, where the similarity of words like “point”, “explore”, “address” and “relate” are well learned."
    }, {
      "heading" : "4.1.3 Influence of Attached Subtree",
      "text" : "In this subsection, we will discuss the role of attached subtree (SUB) in relation classification. By comparing the results of DepNN before and after adding the subtree, we find the influence of this structure varies from different relation types. Table 6 shows the F1 measures of each relation type before and after adding the subtree.\nWe can see that the subtree information generally has a positive impact on all the relation types. It is especially salient for the Instrument-Agency and Product-Producer relations. With only using the shortest dependency paths, these two kinds of relation types are easily confused, as they both rely on the dependency paths such as “. . . verb prep-by/prep-with/using . . . ”. But after considering the subtree information, we can better distinguish these two relation types. Figure 4 lists two instances that can be classified correctly only af-\nter adding the subtrees. Figure 4a belongs to the Producer-Produce relation which can be reflected by the subtree structures like “conj-and→valves” and “amod→manufacturing”. Figure 4b belongs to the Instrument-Agency relation, and the subtree structure attached to the word “scaled” provides more supplemental information to the shortest path as explained above."
    }, {
      "heading" : "4.2 Comparison with Baselines",
      "text" : "In this subsection, we compare DepNN with several baseline approaches of relation classification.\nSVM (Rink and Harabagiu, 2010): This is the top performed system in SemEval-2010. It depends on the human compiled feature templates and then utilizes many external corpora to extract features for an SVM classifier.\nMV-RNN (Socher et al., 2012): This model associates each word with a matrix. Based on the constituent parse tree structure, this model finds\n2MV-RNN achieves a higher F1-score (82.7) on SENNA embeddings reported in the original paper.\nthe path between two entities and learns the distributed representation of their highest parent node through the composition in a recursive neural network.\nDT-RNN (Socher et al., 2014) : This model uses an RNN for modeling dependency trees. It assigns a composition matrix to each dependency relation. Different from our model DepNN, the embedding of each node is a linear combination of its children. The network is trained using the method provided by (Iyyer et al., 2014). We average the learned vectors of all nodes, stack it with the root node’s embedding and additional features, and feed them into a softmax classifier.\nCNN: Zeng et al. (2014) build a convolutional model to learn a sentence representation over the words in a sentence. To represent each word, they use a special position vector to indicate the relative distances of current input word to two marked entities, concatenating the position vector with the corresponding word embedding. Then the sentence representation is staked with some lexical features and fed into a softmax classifier.\nFCM (Yu et al., 2014): FCM decomposes a sentence into some substructures and learns substructure embedding from each of them. Then the substructure embeddings in a sentence are combined via a sum-pooling operation and put into a softmax classifier.\nTable 7 compares DepNN with the baseline approaches. Since many of our baselines are neural network models, it is convenient for them to use some features extracted with external resources or tools to enhance performance. We call these features “additional features” (AF) and list them in the second column. The F1-measures on SemEval-2010 dataset with/out these additional\nfeatures are shown in the last two columns. From Table 7, we can see that DepNN achieves the best result (83.6) with the NER features. SVM achieves a comparable result, though the quality of feature engineering highly relies on human experience and external NLP resources. MV-RNN models the constituent parse trees with a recursive procedure and its F1-measures with/out AF are about 1.7 percent and 4.6 percent lower than those of DepNN. This to some extent indicates that our proposed ADP structure is more suitable for relation classification task. Meanwhile, MVRNN is very slow to train, since each word is associated with a matrix. Both CNN and FCM use features from the whole sentence and achieve similar performance. DT-RNN is the worst of all baselines, though it also considers the information from shortest dependency paths and attached subtrees. As we analyze, shortest dependency paths and subtrees play different roles in relation classification. But, we can see that DT-RNN does not distinguish the modeling processes of shortest paths and subtrees, and deems the representation of each node as a linear combination of its children."
    }, {
      "heading" : "5 Related Work",
      "text" : "Relation classification is one traditional subproblem of Information Extraction (IE). It aims to detect and classify relations between the predefined types of objects in the corpus. These objects could be named entities or marked nominals3. Much research has been performed in this field, most of which considers it as a supervised multi-classification task. Depending on the input to the classifier, these approaches can be further divided into feature-based, tree kernel-based and composite kernel-based.\nFeature-based methods extract various kinds of linguistic features, including both syntactic features and semantic cues. These features are combined to form a feature vector employed in a Max Entropy (Kambhatla, ) or an SVM (Zhou et al., 2005; GuoDong et al., 2005) classifier. Feature-based methods usually need handcrafted features and lack the ability to represent structural information (e.g., parsing tree, word order).\nKernel methods use a more natural way of ex-\n3ACE Evaluation uses the named entities while the SemEval evaluation is based on nominals.\nploring structural features by computing the inner product of two objects in the high-dimensional latent feature space. Zelenko et al. (2003) designed a tree kernel to compute the structural commonality between shallow parse trees by a weighted sum of the number of common subtrees. Culotta and Sorensen (2004) transferred this kernel to a dependency tree and attached more information including POS tag, word chunk tag to each node. Zhou et al. (2007) proposed a contextsensitive convolution tree kernel that used context information beyond the local tree. In another view, Bunescu and Mooney (2005) provided an important insight that the shortest path between the two entities concentrates most of the information for identifying the relation between them. Nguyen et al. (2007) used the dependency subtrees in a different manner by modeling the subtrees between entities and keywords of certain relations. Zhang et al. (2006) further proposed composite kernels to combine a tree kernel and a feature-based kernel to promote the performance.\nRecently, Deep Neural Networks (DNN) have been developed to solve the relation classification problem. By associating each word a distributed representation, DNN can overcome the sparsity problem in traditional methods and automatically learn appropriate features. Socher et al. (2012) proposed a recursive neural network model by constructing compositional semantics for the minimal constituent of a constituent parse tree including both marked entities. Zeng et al. (2014) used a convolutional neural network over the whole sentence combined with some lexical features. They also pointed out that the position of each word in the sentence is very important for relation classification and concatenated a special position feature vector with the corresponding word embedding. Yu et al. (2014) proposed the Factor-based Compositional Embedding Model which extracted features from the substructures of a sentence and combined them through a sum-pooling layer."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we propose to classify relations between entities by modeling the augmented dependency path in a neural network framework. For a given instance, we generate its ADP by combining the shortest path between two entities and the attached subtrees. We present a novel approach, DepNN, to taking advantages of both convolu-\ntional neural network and recursive neural network to model this structure. Experiment results demonstrate that DepNN achieves state-of-the-art performance."
    } ],
    "references" : [ {
      "title" : "Stochastic learning",
      "author" : [ "Léon Bottou" ],
      "venue" : "In Advanced lectures on machine learning,",
      "citeRegEx" : "Bottou.,? \\Q2004\\E",
      "shortCiteRegEx" : "Bottou.",
      "year" : 2004
    }, {
      "title" : "A Shortest Path Dependency Kernel for Relation Extraction",
      "author" : [ "Bunescu", "Mooney2005] Razvan C. Bunescu", "Raymond J. Mooney" ],
      "venue" : "In North American Chapter of the Association for Computational Linguistics",
      "citeRegEx" : "Bunescu et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Bunescu et al\\.",
      "year" : 2005
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Collobert et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "Dependency Tree Kernels for Relation Extraction",
      "author" : [ "Culotta", "Sorensen2004] Aron Culotta", "Jeffrey S. Sorensen" ],
      "venue" : "In Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Culotta et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Culotta et al\\.",
      "year" : 2004
    }, {
      "title" : "The Stanford typed dependencies representation",
      "author" : [ "de Marneffe", "Christopher D. Manning" ],
      "venue" : "In International Conference on Computational Linguistics",
      "citeRegEx" : "Marneffe et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Marneffe et al\\.",
      "year" : 2008
    }, {
      "title" : "Exploring various knowledge in relation extraction",
      "author" : [ "GuoDong et al.2005] Zhou GuoDong", "Su Jian", "Zhang Jie", "Zhang Min" ],
      "venue" : "In Proceedings of the 43rd annual meeting on association for computational linguistics,",
      "citeRegEx" : "GuoDong et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "GuoDong et al\\.",
      "year" : 2005
    }, {
      "title" : "SemEval-2010 Task 8: Multi-Way Classification of Semantic Relations Between Pairs",
      "author" : [ "Zornitsa Kozareva", "Preslav Nakov", "Sebastian Pad ok", "Marco Pennacchiotti", "Lorenza Romano", "Stan Szpakowicz" ],
      "venue" : null,
      "citeRegEx" : "Hendrickx et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Hendrickx et al\\.",
      "year" : 2010
    }, {
      "title" : "A neural network for factoid question answering over paragraphs",
      "author" : [ "Iyyer et al.2014] Mohit Iyyer", "Jordan Boyd-Graber", "Leonardo Claudino", "Richard Socher", "Hal Daumé III" ],
      "venue" : "In Proceedings of the 2014 Conference on Empir-",
      "citeRegEx" : "Iyyer et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Iyyer et al\\.",
      "year" : 2014
    }, {
      "title" : "Accurate Unlexicalized Parsing",
      "author" : [ "Klein", "Manning2003] Dan Klein", "Christopher D. Manning" ],
      "venue" : "In Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Klein et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Klein et al\\.",
      "year" : 2003
    }, {
      "title" : "Relation extraction from wikipedia using subtree mining",
      "author" : [ "Nguyen et al.2007] Dat PT Nguyen", "Yutaka Matsuo", "Mitsuru Ishizuka" ],
      "venue" : "In Proceedings of the National Conference on Artificial Intelligence,",
      "citeRegEx" : "Nguyen et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2007
    }, {
      "title" : "Utd: Classifying semantic relations by combining lexical and semantic resources",
      "author" : [ "Rink", "Harabagiu2010] Bryan Rink", "Sanda Harabagiu" ],
      "venue" : "In Proceedings of the 5th International Workshop on Semantic Evaluation,",
      "citeRegEx" : "Rink et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Rink et al\\.",
      "year" : 2010
    }, {
      "title" : "Learning representations by back-propagating errors",
      "author" : [ "Geoffrey E Hinton", "Ronald J Williams" ],
      "venue" : "Cognitive modeling,",
      "citeRegEx" : "Rumelhart et al\\.,? \\Q1988\\E",
      "shortCiteRegEx" : "Rumelhart et al\\.",
      "year" : 1988
    }, {
      "title" : "Semantic compositionality through recursive matrix-vector spaces",
      "author" : [ "Brody Huval", "Christopher D. Manning", "Andrew Y. Ng" ],
      "venue" : "In Proceedings of the 2012 Joint Conference on Empirical Methods",
      "citeRegEx" : "Socher et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2012
    }, {
      "title" : "Grounded compositional semantics for finding and describing images",
      "author" : [ "Andrej Karpathy", "Quoc V Le", "Christopher D Manning", "Andrew Y Ng" ],
      "venue" : null,
      "citeRegEx" : "Socher et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2014
    }, {
      "title" : "Combining linguistic and statistical analysis to extract relations from web documents",
      "author" : [ "Georgiana Ifrim", "Gerhard Weikum" ],
      "venue" : "In Proceedings of the 12th ACM SIGKDD international conference on Knowl-",
      "citeRegEx" : "Suchanek et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Suchanek et al\\.",
      "year" : 2006
    }, {
      "title" : "A re-examination of dependency path kernels for relation extraction",
      "author" : [ "Mengqiu Wang" ],
      "venue" : "In IJCNLP,",
      "citeRegEx" : "Wang.,? \\Q2008\\E",
      "shortCiteRegEx" : "Wang.",
      "year" : 2008
    }, {
      "title" : "Factor-based compositional embedding models",
      "author" : [ "Yu et al.2014] Mo Yu", "Matthew Gormley", "Mark Dredze" ],
      "venue" : "In NIPS Workshop on Learning Semantics",
      "citeRegEx" : "Yu et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2014
    }, {
      "title" : "Kernel Methods for Relation Extraction",
      "author" : [ "Chinatsu Aone", "Anthony Richardella" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Zelenko et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Zelenko et al\\.",
      "year" : 2003
    }, {
      "title" : "Relation classification via convolutional deep neural network",
      "author" : [ "Zeng et al.2014] Daojian Zeng", "Kang Liu", "Siwei Lai", "Guangyou Zhou", "Jun Zhao" ],
      "venue" : "In Proceedings of COLING",
      "citeRegEx" : "Zeng et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2014
    }, {
      "title" : "A Composite Kernel to Extract Relations between Entities with Both Flat and Structured Features",
      "author" : [ "Zhang et al.2006] Min Zhang", "Jie Zhang", "Jian Su", "Guodong Zhou" ],
      "venue" : "In Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Zhang et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2006
    }, {
      "title" : "Exploring Various Knowledge in Relation Extraction",
      "author" : [ "Zhou et al.2005] Guodong Zhou", "Jian Su", "Jie Zhang", "Min Zhang" ],
      "venue" : "In Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Zhou et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2005
    }, {
      "title" : "Tree kernel-based relation extraction with contextsensitive structured parse tree information",
      "author" : [ "Zhou et al.2007] GuoDong Zhou", "Min Zhang", "Dong Hong Ji", "Qiaoming Zhu" ],
      "venue" : null,
      "citeRegEx" : "Zhou et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "For example, Suchanek et al. (2006) carefully selected a set of features from tokenization and dependency parsing, and extended some of them to",
      "startOffset" : 13,
      "endOffset" : 36
    }, {
      "referenceID" : 9,
      "context" : "Nguyen et al. (2007) developed these ideas by analyzing multiple subtrees with the guidance of pre-extracted keywords.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 6,
      "context" : "According to the the official guideline of SemEval-2010 task 8 (Hendrickx et al., 2010), there are 9 ordered relation types.",
      "startOffset" : 63,
      "endOffset" : 87
    }, {
      "referenceID" : 6,
      "context" : "According to the the official guideline of SemEval-2010 task 8 (Hendrickx et al., 2010), there are 9 ordered relation types. We list them in Table 1 with their simplified definitions. Instances don’t fall in any of these types are labeled as Other. For example, in Figure 2, the relation between two entities e1=thief and e2=screwdriver is Instrument-Agency. Bunescu and Mooney (2005) reported that, for the relation classification task, the shortest dependency path between two entities plays a vital role.",
      "startOffset" : 64,
      "endOffset" : 385
    }, {
      "referenceID" : 2,
      "context" : "Next, we adopt the widely-used max-over-time pooling operation (Collobert et al., 2011), which can retain the most important features, to obtain the final representation L from the feature map.",
      "startOffset" : 63,
      "endOffset" : 87
    }, {
      "referenceID" : 0,
      "context" : "To learn the parameters, we optimize the cross-entropy error between y and t using stochastic gradient descent (Bottou, 2004).",
      "startOffset" : 111,
      "endOffset" : 125
    }, {
      "referenceID" : 11,
      "context" : "Gradients are computed using backpropagation (Rumelhart et al., 1988).",
      "startOffset" : 45,
      "endOffset" : 69
    }, {
      "referenceID" : 6,
      "context" : "Our experiments are performed on SemEval-2010 dataset (Hendrickx et al., 2010).",
      "startOffset" : 54,
      "endOffset" : 78
    }, {
      "referenceID" : 6,
      "context" : "Our experiments are performed on SemEval-2010 dataset (Hendrickx et al., 2010). The training part of the dataset includes 8000 instances, and the test part includes 2717 instances. Table 2 shows the statistics of the annotated relation types of this dataset. We can see that the distribution of relation types in the test set is similar to that in the training set. The official evaluation metric is the macro-averaged F1-score (excluding Other). We use dependency trees generated by the Stanford Parser (Klein and Manning, 2003) with the “collapsed” option, which regards a preposition as a kind of dependency relation. As de Marneffe and Manning (2008) pointed out, this option is more useful for event relation extraction.",
      "startOffset" : 55,
      "endOffset" : 655
    }, {
      "referenceID" : 2,
      "context" : "One is the 50-d embeddings provided by SENNA (Collobert et al., 2011).",
      "startOffset" : 45,
      "endOffset" : 69
    }, {
      "referenceID" : 16,
      "context" : "The second is the 200-d embeddings (Yu et al., 2014) trained on Gigaword with word2vec1.",
      "startOffset" : 35,
      "endOffset" : 52
    }, {
      "referenceID" : 16,
      "context" : "WordNet seems less useful than NER, which conforms to the results of Yu et al. (2014) , since a large number of WordNet hypernyms may cause overfitting.",
      "startOffset" : 69,
      "endOffset" : 86
    }, {
      "referenceID" : 12,
      "context" : "MV-RNN (Socher et al., 2012): This model associates each word with a matrix.",
      "startOffset" : 7,
      "endOffset" : 28
    }, {
      "referenceID" : 13,
      "context" : "DT-RNN (Socher et al., 2014) : This model uses an RNN for modeling dependency trees.",
      "startOffset" : 7,
      "endOffset" : 28
    }, {
      "referenceID" : 7,
      "context" : "The network is trained using the method provided by (Iyyer et al., 2014).",
      "startOffset" : 52,
      "endOffset" : 72
    }, {
      "referenceID" : 18,
      "context" : "CNN: Zeng et al. (2014) build a convolutional model to learn a sentence representation over the words in a sentence.",
      "startOffset" : 5,
      "endOffset" : 24
    }, {
      "referenceID" : 16,
      "context" : "FCM (Yu et al., 2014): FCM decomposes a sentence into some substructures and learns substructure embedding from each of them.",
      "startOffset" : 4,
      "endOffset" : 21
    }, {
      "referenceID" : 20,
      "context" : "These features are combined to form a feature vector employed in a Max Entropy (Kambhatla, ) or an SVM (Zhou et al., 2005; GuoDong et al., 2005) classifier.",
      "startOffset" : 103,
      "endOffset" : 144
    }, {
      "referenceID" : 5,
      "context" : "These features are combined to form a feature vector employed in a Max Entropy (Kambhatla, ) or an SVM (Zhou et al., 2005; GuoDong et al., 2005) classifier.",
      "startOffset" : 103,
      "endOffset" : 144
    }, {
      "referenceID" : 16,
      "context" : "Zelenko et al. (2003) designed a tree kernel to compute the structural commonality between shallow parse trees by a weighted sum of the number of common subtrees.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 16,
      "context" : "Zelenko et al. (2003) designed a tree kernel to compute the structural commonality between shallow parse trees by a weighted sum of the number of common subtrees. Culotta and Sorensen (2004) transferred this kernel to a dependency tree and attached more information including POS tag, word chunk tag to each node.",
      "startOffset" : 0,
      "endOffset" : 191
    }, {
      "referenceID" : 16,
      "context" : "Zelenko et al. (2003) designed a tree kernel to compute the structural commonality between shallow parse trees by a weighted sum of the number of common subtrees. Culotta and Sorensen (2004) transferred this kernel to a dependency tree and attached more information including POS tag, word chunk tag to each node. Zhou et al. (2007) proposed a contextsensitive convolution tree kernel that used context information beyond the local tree.",
      "startOffset" : 0,
      "endOffset" : 333
    }, {
      "referenceID" : 16,
      "context" : "Zelenko et al. (2003) designed a tree kernel to compute the structural commonality between shallow parse trees by a weighted sum of the number of common subtrees. Culotta and Sorensen (2004) transferred this kernel to a dependency tree and attached more information including POS tag, word chunk tag to each node. Zhou et al. (2007) proposed a contextsensitive convolution tree kernel that used context information beyond the local tree. In another view, Bunescu and Mooney (2005) provided an important insight that the shortest path between the two entities concentrates most of the information for identifying the relation between them.",
      "startOffset" : 0,
      "endOffset" : 481
    }, {
      "referenceID" : 9,
      "context" : "Nguyen et al. (2007) used the dependency subtrees in a different manner by modeling the subtrees between entities and keywords of certain relations.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 9,
      "context" : "Nguyen et al. (2007) used the dependency subtrees in a different manner by modeling the subtrees between entities and keywords of certain relations. Zhang et al. (2006) further proposed composite kernels to combine a tree kernel and a feature-based kernel to promote the performance.",
      "startOffset" : 0,
      "endOffset" : 169
    }, {
      "referenceID" : 12,
      "context" : "Socher et al. (2012) proposed a recursive neural network model by constructing compositional semantics for the minimal constituent of a constituent parse tree including both marked entities.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 12,
      "context" : "Socher et al. (2012) proposed a recursive neural network model by constructing compositional semantics for the minimal constituent of a constituent parse tree including both marked entities. Zeng et al. (2014) used a convolutional neural network over the whole sentence combined with some lexical features.",
      "startOffset" : 0,
      "endOffset" : 210
    }, {
      "referenceID" : 12,
      "context" : "Socher et al. (2012) proposed a recursive neural network model by constructing compositional semantics for the minimal constituent of a constituent parse tree including both marked entities. Zeng et al. (2014) used a convolutional neural network over the whole sentence combined with some lexical features. They also pointed out that the position of each word in the sentence is very important for relation classification and concatenated a special position feature vector with the corresponding word embedding. Yu et al. (2014) proposed the Factor-based Compositional Embedding Model which extracted features from the substructures of a sentence and combined them through a sum-pooling layer.",
      "startOffset" : 0,
      "endOffset" : 529
    } ],
    "year" : 2015,
    "abstractText" : "Previous research on relation classification has verified the effectiveness of using dependency shortest paths or subtrees. In this paper, we further explore how to make full use of the combination of these dependency information. We first propose a new structure, termed augmented dependency path (ADP), which is composed of the shortest dependency path between two entities and the subtrees attached to the shortest path. To exploit the semantic representation behind the ADP structure, we develop dependency-based neural networks (DepNN): a recursive neural network designed to model the subtrees, and a convolutional neural network to capture the most important features on the shortest path. Experiments on the SemEval-2010 dataset show that our proposed method achieves state-of-art results.",
    "creator" : "LaTeX with hyperref package"
  }
}