{
  "name" : "1206.5333.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "TempEval-3: Evaluating Events, Time Expressions, and Temporal Relations",
    "authors" : [ "Naushad UzZaman", "Hector Llorens", "James Allen", "Leon Derczynski", "Marc Verhagen", "James Pustejovsky" ],
    "emails" : [ "naushad@cs.rochester.edu", "james@cs.rochester.edu", "hllorens@dlsi.ua.es", "leon@dcs.shef.ac.uk", "jamesp@cs.brandeis.edu", "marc@cs.brandeis.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n20 6.\n53 33\nv2 [\ncs .C\nIn this proposal, we describe the forthcoming TempEval-3 task which is currently in preparation for the SemEval-2013 evaluation exercise. The aim of TempEval is to advance research on temporal information processing. TempEval-3 follows on from previous TempEval events, incorporating: a three-part task structure covering event, temporal expression and temporal relation extraction; a larger dataset; and single overall task quality scores."
    }, {
      "heading" : "1 Introduction",
      "text" : "The TempEval task was added as a new task in SemEval-2007 (Verhagen et al., 2009), focusing on the identification of temporal relations. The automatic identification of all temporal referring expressions, events, and temporal relations within a text is the ultimate aim of research in this area. The area is too broad to address completely in a first evaluation challenge, and a staged approach was taken instead. TempEval (henceforth TempEval-1) was an initial evaluation exercise based on three fixed-scope tasks (identifying links between: events and timexes in the same sentence; events and document creation time DCT; main events in successive sentences) that were considered realistic both from the perspective of assembling resources for development and testing and from the perspective of developing systems capable of addressing the tasks.\nTempEval-2 (Verhagen et al., 2010) extended TempEval-1, growing into a multilingual task, and consisting of six subtasks rather than three. This included event and timex extraction, as well as the three relation tasks from TempEval-1, with the addition of a relation task where one event subordinates another.\nTemporal annotation is a time-consuming task for humans, which has limited the size of annotated data in previous TempEval exercises. Current systems,\n1\nhowever, are performing close to the inter-annotator reliability for entity recognition. This suggests that larger corpora could be built from automatically annotated data with minor human reviews. As part of TempEval-3, we explore whether there is value in adding a large automatically created silver standard to a hand-crafted gold standard.\nAutomatic performance on temporal relation annotation is still limited; in TempEval-2, systems achieved an above-baseline error reduction of less than 20% for most tasks. This suggests that the temporal relation problem is still open and remains a topic of intensive contemporary research.\nWith these points in mind, this paper describes the next upcoming temporal evaluation shared task – TempEval-3 – to be held with SemEval-2013."
    }, {
      "heading" : "2 TempEval changes",
      "text" : "As proposed, TempEval-3 is a follow-up to TempEval-1 and 2. TempEval-3 differs from its ancestors in the following respects:\n(i) size of the corpus: the dataset used comprises about 500K tokens of silver standard data and about 100K tokens of gold standard data for training, compared to the corpus of roughly 50K tokens corpus used in TempEval 1 and 2;\n(ii) temporal relation task: the temporal relation classification tasks are to be performed from raw text, i.e. participants need to extract events and temporal expressions first, determine which ones to link and then obtain the relation types;\n(iii) tasks not independent: participants must annotate temporal expressions and events in order to do the relation task;\n(iv) temporal relation types: the full set of temporal interval relations in TimeML (Pustejovsky et al., 2005) is used, rather than the reduced set used in earlier TempEvals;\n(v) annotation: most of the corpus was automatically annotated by the stateof-the-art systems from TempEval-2, a portion of the corpus, including the test dataset, that is human reviewed;\n(vi) evaluation: we will report a temporal awareness score for evaluating temporal relations, to help to rank systems with a single score.\nThis is the TempEval-3 proposal, not the main paper, which is in the SemEval-2013 proceedings."
    }, {
      "heading" : "3 Tasks",
      "text" : "The tasks proposed for TempEval-3 are:"
    }, {
      "heading" : "3.1 Task A: Temporal expression extraction and normalization",
      "text" : "Determine the extent of the time expressions in a text as defined by the TimeML TIMEX3 tag. In addition, determine the value of the features TYPE and VAL. The possible values of TYPE are time, date, duration, and set; the value of VAL is a normalized value as defined by the TIMEX3 standard. The main attribute to annotate is VAL."
    }, {
      "heading" : "3.2 Task B: Event extraction",
      "text" : "As in TempEval-2, participants will determine the extent of the events in a text as defined by the TimeML EVENT tag. In addition, systems may determine the value of the features CLASS, TENSE, ASPECT, POLARITY, MODALITY and also identify if the event is a main event or not. The main attribute to annotate is CLASS."
    }, {
      "heading" : "3.3 Task C: Annotating temporal relations",
      "text" : "Identify the pairs of temporal entities (events or temporal expressions) that have a temporal link and classify the temporal relation between them as a TLINK. Possible pairs of entities that can have a temporal link are: (i) event and temporal expressions in the same sentence, (ii) event and document creation time, (iii) main events of consecutive sentences and (iv) pairs of events in the same sentence. For this task, we now require that the participating systems determine which entities need to be linked.\nThe relation labels will be same as in TimeML, i.e.: before, after, includes, is-included, during, simultaneous, immediately after, immediately before, identity, begins, ends, begun-by and ended-by."
    }, {
      "heading" : "3.4 Task selection",
      "text" : "Participants may choose to do task A, B, or C. Choosing task C (relation annotation) entails doing tasks A and B (interval annotation). However, a participant may perform only task C by applying existing tools to carry out tasks A and B.\nThis is the TempEval-3 proposal, not the main paper, which is in the SemEval-2013 proceedings."
    }, {
      "heading" : "4 Dataset creation",
      "text" : "In TempEval-3, we release new data, as well as significantly reviewing and modifying existing corpora."
    }, {
      "heading" : "4.1 Reviewing Existing Corpora",
      "text" : "We considered the existing TimeBank (Pustejovsky et al., 2003), TempEval-1, TempEval-2 and AQUAINT1 data for review in TempEval-3. TimeBank v1.2, TempEval-1 and TempEval-2 had the same documents but different relation types and sometimes different sets of events. We will refer to this body of temporally-annotated newswire documents as TimeBank.\nFor both TimeBank and AQUAINT, we cleaned up the formatting for all files making it easy to review and read, made all files XML and TimeML schema compatible and added some missing events and temporal expressions. In AQUAINT, we added the temporal relations between event and DCT (document creation time), which was missing for many documents in that corpus. In particular, for the TimeBank documents, we borrowed the events from the TempEval-2 corpus and the temporal relations from the TimeBank corpus, which contains a full set of temporal relations (TempEval-2 used a simpler, coarse-grained set of temporal relations).\nA standard datafile format has been adopted, which is a subset of valid ISOTimeML. It begins with an outer TimeML element as normal. The document name is contained in a child DOCID element, any newswire preamble in an optional EXTRAINFO element, headline in an optional TITLE element, the document timestamp in a DCT element (usually with an ID of t0) and the main body of the text to be annotated in a TEXT element. For example:\n<?xml version=\"1.0\" ?> <TimeML xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\nxsi:noNamespaceSchemaLocation=\"http://timeml.org/timeMLdocs/TimeML 1.2.1.xsd\">\n<DOCID>XIN ENG 20061119.0021</DOCID> <DCT>HANOI, <TIMEX3 functionInDocument=\"CREATION TIME\" temporalFunction=\"false\"\ntid=\"t0\" type=\"TIME\" value=\"2006-11-19\">Nov. 19 , 2006</TIMEX3> (Xinhua)</DCT>\n<TITLE>URGENT: Russia, US sign agreement on WTO deal in Vietnam</TITLE> <TEXT> Russia and the United States Sunday <EVENT aspect=\"NONE\" class=\"OCCURRENCE\"\neid=\"e1\" eiid=\"ei1\" polarity=\"POS\" pos=\"VERB\" tense=\"PAST\">signed</EVENT> a bilateral <EVENT aspect=\"NONE\" class=\"OCCURRENCE\" eid=\"e2\" eiid=\"ei2\" polarity=\"POS\" pos=\"NOUN\" tense=\"PAST\">agreement</EVENT> on Russia’s accession to the World Trade Organization (WTO) on the sidelines of the ongoing Asia- Pacific Economic Cooperaiton Economic Leaders’ Meeting in Hanoi.\n</TEXT> <TLINK eventInstanceID=\"ei1\" lid=\"l1\" relType=\"NONE\" relatedToTime=\"t0\"/> <TLINK eventInstanceID=\"ei2\" lid=\"l2\" relType=\"NONE\"\nrelatedToEventInstance=\"ei1\"/>\n</TimeML>\n1http://timeml.org/site/timebank/timebank.html\nThis is the TempEval-3 proposal, not the main paper, which is in the SemEval-2013 proceedings."
    }, {
      "heading" : "4.2 Automatically Creating New Large Corpora",
      "text" : "A large portion of the TempEval-3 data is automatically generated, using a temporal merging system. We collected the half-million token text corpus from English Gigaword2. We automatically annotated this corpus using TIPSem, TIPSem-B (Llorens et al., 2010) and TRIOS (UzZaman and Allen, 2010). These systems were re-trained on the TimeBank and AQUAINT corpus, using the TimeML temporal relation set. We then merged these three state-of-the-art system outputs using our merging algorithm (UzZaman et al., 2012). In our merging configuration, all entities and relations suggested by the best system (TIPSem) are added to the merge output. Suggestions from two other systems (TRIOS and TIPSem-B) are added to the merge output if they are supported by at least 2 of the 3 systems overall. The weights used in our configuration are: TIPSem 0.36, TIPSemB 0.32, TRIOS 0.32.\nThis automatically created corpus is referred as silver data. A portion of the silver data is in the process of human reviewing for release as additional gold training data, in addition to reviewed and re-curated versions of TimeBank and AQUAINT. The parts described in Table 1 comprise our released dataset.\nThe exploration of the benefits of both very large automatically temporally annotated corpora (silver data) and of smaller human annotated/reviewed temporal annotated corpora (gold data) with our TempEval-3 release is left to task participants and to future research."
    }, {
      "heading" : "5 Evaluation",
      "text" : "Evaluation on tasks A and B will be a standard F-score (incorporating Precision and Recall metrics) on extents and F-score/Kappa on attributes on the response extents that overlap with the key extents. Evaluation on task C will be incorporated from our proposed graph-based evaluation metric (see UzZaman and Allen (2011) for details). This metric uses temporal closure to reward relation annotations that are equivalent but distinct and then finds precision and recall. Our temporal awareness score is a combined measure of a system’s performance\n2http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC2011T07\nThis is the TempEval-3 proposal, not the main paper, which is in the SemEval-2013 proceedings.\n(i.e. it evaluates how a system extracts events, temporal expressions and also identifies all temporal relations)."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We have described the task, dataset and evaluation style for TempEval-3. The event will be part of SemEval-2013. Training will begin in autumn 2012, and the evaluation period ends January 2013. Further information can be found on the task website3 and via the TempEval group list4."
    } ],
    "references" : [ {
      "title" : "TIPSem (English and Spanish): Evaluating CRFs and Semantic Roles in TempEval-2.",
      "author" : [ "H. Llorens", "E. Saquete", "B. Navarro" ],
      "venue" : "In Proceedings of the 5th International Workshop on Semantic Evaluation,",
      "citeRegEx" : "Llorens et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Llorens et al\\.",
      "year" : 2010
    }, {
      "title" : "The TimeBank corpus.",
      "author" : [ "J. Pustejovsky", "P. Hanks", "R. Sauri", "A. See", "R. Gaizauskas", "A. Setzer", "D. Radev", "B. Sundheim", "D. Day", "L. Ferro" ],
      "venue" : "In Corpus Linguistics,",
      "citeRegEx" : "Pustejovsky et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Pustejovsky et al\\.",
      "year" : 2003
    }, {
      "title" : "The specification language TimeML.",
      "author" : [ "J. Pustejovsky", "B. Ingria", "R. Sauri", "J. Castano", "J. Littman", "R. Gaizauskas", "A. Setzer", "G. Katz", "I. Mani" ],
      "venue" : "The Language of Time: A reader,",
      "citeRegEx" : "Pustejovsky et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Pustejovsky et al\\.",
      "year" : 2005
    }, {
      "title" : "TRIPS and TRIOS system for TempEval2: Extracting temporal information from text.",
      "author" : [ "N. UzZaman", "J.F" ],
      "venue" : "In Proceedings of the 5th International Workshop on Semantic Evaluation,",
      "citeRegEx" : "UzZaman and J.F.,? \\Q2010\\E",
      "shortCiteRegEx" : "UzZaman and J.F.",
      "year" : 2010
    }, {
      "title" : "Temporal Evaluation.",
      "author" : [ "N. UzZaman", "J.F" ],
      "venue" : null,
      "citeRegEx" : "UzZaman and J.F.,? \\Q2011\\E",
      "shortCiteRegEx" : "UzZaman and J.F.",
      "year" : 2011
    }, {
      "title" : "Merging Temporal Annotations.",
      "author" : [ "N. UzZaman", "H. Llorens", "J.F" ],
      "venue" : "In Proceedings of the TIME Conference",
      "citeRegEx" : "UzZaman et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "UzZaman et al\\.",
      "year" : 2012
    }, {
      "title" : "The TempEval challenge: identifying temporal relations in text.",
      "author" : [ "M. Verhagen", "R. Gaizauskas", "F. Schilder", "M. Hepple", "J. Moszkowicz", "J. Pustejovsky" ],
      "venue" : "Language Resources and Evaluation,",
      "citeRegEx" : "Verhagen et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Verhagen et al\\.",
      "year" : 2009
    }, {
      "title" : "SemEval-2010 task 13: TempEval-2.",
      "author" : [ "M. Verhagen", "R. Sauri", "T. Caselli", "J. Pustejovsky" ],
      "venue" : "In Proceedings of the 5th International Workshop on Semantic Evaluation,",
      "citeRegEx" : "Verhagen et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Verhagen et al\\.",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "The TempEval task was added as a new task in SemEval-2007 (Verhagen et al., 2009), focusing on the identification of temporal relations.",
      "startOffset" : 58,
      "endOffset" : 81
    }, {
      "referenceID" : 7,
      "context" : "TempEval-2 (Verhagen et al., 2010) extended TempEval-1, growing into a multilingual task, and consisting of six subtasks rather than three.",
      "startOffset" : 11,
      "endOffset" : 34
    }, {
      "referenceID" : 2,
      "context" : "(iv) temporal relation types: the full set of temporal interval relations in TimeML (Pustejovsky et al., 2005) is used, rather than the reduced set used in earlier TempEvals;",
      "startOffset" : 84,
      "endOffset" : 110
    }, {
      "referenceID" : 1,
      "context" : "1 Reviewing Existing Corpora We considered the existing TimeBank (Pustejovsky et al., 2003), TempEval-1, TempEval-2 and AQUAINT data for review in TempEval-3.",
      "startOffset" : 65,
      "endOffset" : 91
    }, {
      "referenceID" : 0,
      "context" : "We automatically annotated this corpus using TIPSem, TIPSem-B (Llorens et al., 2010) and TRIOS (UzZaman and Allen, 2010).",
      "startOffset" : 62,
      "endOffset" : 84
    }, {
      "referenceID" : 5,
      "context" : "We then merged these three state-of-the-art system outputs using our merging algorithm (UzZaman et al., 2012).",
      "startOffset" : 87,
      "endOffset" : 109
    } ],
    "year" : 2014,
    "abstractText" : "In this proposal, we describe the forthcoming TempEval-3 task which is currently in preparation for the SemEval-2013 evaluation exercise. The aim of TempEval is to advance research on temporal information processing. TempEval-3 follows on from previous TempEval events, incorporating: a three-part task structure covering event, temporal expression and temporal relation extraction; a larger dataset; and single overall task quality scores.",
    "creator" : "LaTeX with hyperref package"
  }
}