{
  "name" : "1705.07807.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Use Privacy in Data-Driven Systems",
    "authors" : [ "Anupam Datta", "Matt Fredrikson", "Gihyuk Ko", "Piotr Mardziel", "Shayak Sen" ],
    "emails" : [ "permissions@acm.org." ],
    "sections" : [ {
      "heading" : null,
      "text" : "CCS CONCEPTS • Security and privacy→ Privacy protections;\nKEYWORDS use privacy"
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Restrictions on information use occupy a central place in privacy regulations and legal frameworks [28, 54, 61, 62]. We introduce the term use privacy to refer to privacy norms governing information use. A number of recent cases have evidenced that inappropriate\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. CCS ’17, October 30-November 3, 2017, Dallas, TX, USA © 2017 Association for Computing Machinery. ACM ISBN 978-1-4503-4946-8/17/10. . . $15.00 https://doi.org/http://dx.doi.org/10.1145/3133956.3134097\ninformation use can lead to violations of both privacy laws [68] and user expectations [16, 19], prompting calls for technology to assist with enforcement of use privacy requirements [53]. In order to meet these regulatory imperatives and user expectations, companies dedicate resources toward compliance with privacy policies governing information use [53, 57]. A large body of work has emerged around use privacy compliance governing the explicit use of protected information types (see Tschantz et al. [64] for a survey). Such methods are beginning to see deployment in major technology companies like Microsoft [58].\nIn this paper, we initiate work on formalizing and enforcing a richer class of use privacy restrictions—those governing the use of protected information indirectly through proxies in data-driven systems. Data-driven systems include machine learning and artificial intelligence systems that use large swaths of data about individuals in order to make decisions about them. The increasing adoption of these systems in a wide range of sectors, including advertising, education, healthcare, employment, and credit, underscores the critical need to address use privacy concerns [53, 57].\nWe start with a set of examples to motivate these privacy concerns and identify the key research challenges that this paper will tackle to address them. In 2012, the department store Target drew flak from privacy advocates and data subjects for using the shopping history of their customers to predict their pregnancy status and market baby items based on that information [19]. While Target intentionally inferred the pregnancy status and used it for marketing, the privacy concern persists even if the inference were not explicitly drawn. Indeed, the use of health condition-related search terms and browsing history—proxies (i.e., strong predictors) for health conditions—for targeted advertising have been the basis for legal action and public concern from a privacy standpoint [16, 44, 68]. Similar privacy concerns have been voiced about the use of personal information in the Internet of Things [40, 49, 52, 67]. Use privacy To address these threats, this paper articulates the problem of protecting use privacy in data-driven systems.\nUse privacy constraints restrict the use of protected information types and some of their proxies in data-driven systems.\nSetting A use privacy constraint may require that health information or its proxies not be used for advertising. Indeed there are calls for this form of privacy constraint [17, 46, 53, 68]. In this paper, we consider the setting where a data-driven system is audited to ensure that it complies with such use privacy constraints. The auditing could be done by a trusted data processor who is operating the system or by a regulatory oversight organization who has access to the data processors’ machine learning models and knowledge of\nar X\niv :1\n70 5.\n07 80\n7v 3\n[ cs\n.C R\n] 7\nS ep\n2 01\n7\nthe distribution of the dataset. In other words, we assume that the data processor does not act to evade the detection algorithm, and provides accurate information. This trusted data processor setting is similar to the one assumed in differential privacy [25].\nIn this setting, it is impossible to guarantee that data processors with strong background knowledge are not able to infer certain facts about individuals (e.g., their pregnancy status) [21]. Even in practice, data processors often have access to detailed profiles of individuals and can infer sensitive information about them [19, 66]. Use privacy instead places a more pragmatic requirement on data-driven systems: that they simulate ignorance of protected information types (e.g., pregnancy status) by not using them or their proxies in their decision-making. This requirement is met if the systems (e.g., machine learning models) do not infer protected information types or their proxies (even if they could) or if such inferences do not affect decisions.\nRecognizing that not all instances of proxy use of a protected information type are inappropriate, our theory of use privacy makes use of a normative judgment oracle that makes this inappropriateness determination for a given instance. For example, while using health information or its proxies for credit decisions may be deemed inappropriate, an exception could be made for proxies that are directly relevant to the credit-worthiness of the individual (e.g., her income and expenses).\nProxy use A key technical contribution of this paper is a formalization of proxy use of protected information types in programs. Our formalization relates proxy use to intermediate computations obtained by decomposing a program. We begin with a qualitative definition that identifies two essential properties of the intermediate computation (the proxy): 1) its result perfectly predicts the protected information type in question, and 2) it has a causal affect on the final output of the program.\nIn practice, this qualitative definition of proxy use is too rigid for machine learning applications along two dimensions. First, instead of demanding that proxies are perfect predictors, we use a standard measure of association strength from the quantitative information flow security literature to define an ϵ-proxy of a protected information type; here ϵ ∈ [0, 1] with higher values indicating a stronger proxy. Second, qualitative causal effects are not sufficiently informative for our purpose. Instead we use a recently introduced causal influence measure [14] to quantitatively characterize influence. We call it the δ -influence of a proxy where δ ∈ [0, 1] with higher values indicating stronger influence. Combining these two notions, we define a notion of (ϵ,δ )-proxy use.\nWe arrive at this program-based definition after a careful examination of the space of possible definitions. In particular, we prove that it is impossible for a purely semantic notion of intermediate computations to support a meaningful notion of proxy use as characterized by a set of natural properties or axioms (Theorem 1). The program-based definition arises naturally from this exploration by replacing semantic decomposition with decompositions of the program. An important benefit of this choice of restricting the search for intermediate computations to those that appear in the text of the program is that it supports natural algorithms for detection and repair of proxy use. Our framework is parametric in the choice of a programming language in which the programs (e.g., machine learnt\nmodels) are expressed and the population to which it is applied. The choice of the language reflects the level of white-box access that the analyst has into the program.\nDetection We instantiate our definition to a simple programming language that contains conditionals, arithmetic and logical operations, and decompositions that involve single variables and associative arithmetic. For example, decompositions of linear models include additive sets of linear terms, and decision forests include subtrees, and sets of decision trees. For this instantiation of the definition, we present a program analysis technique that detects proxy use in a model, and provides a witness that identifies which parts of the corresponding program exhibit the behavior (Algorithm 4). Our algorithm assumes access to the text of a program that computes the model, as well as a dataset that has been partitioned into analysis and validation subsets. The algorithm is program-directed and is directly inspired by the definition of proxy use. We prove that the algorithm is complete relative to our instantiation of the proxy use definition — it identifies every instance of proxy use in the program (Theorem 3) and outputs witnesses (i.e. intermediate computations that are the proxies). We provide three optimizations that leverage sampling, pre-computation, and reachability to speed up the detection algorithm.\nRepair If a found instance of proxy use is deemed inappropriate, our repair algorithm (Algorithm 5) uses the witness to transform the model into one that provably does not exhibit that instance of proxy use (Theorem 4), while avoiding changes that unduly affect classification accuracy. We leverage the witnesses that localize where in the program a violation occurs in order to focus repair there. To repair a violation, we search through expressions local to the violation, replacing the one which has the least impact on the accuracy of the model and at the same time reduces the association or influence of the violation to below the (ϵ,δ ) threshold.\nEvaluation We empirically evaluate our proxy use definition, detection and repair algorithms on four real datasets used to train decision trees, linear models, and random forests. Our evaluation demonstrates the typical workflow for practitioners who use our tools for a simulated financial services application. It highlights how they help them uncover more proxy uses than a baseline procedure that simply eliminates features associated with the protected information type. For three other simulated settings on real data sets—contraception advertising, student assistance, and credit advertising—we find interesting proxy uses and discuss how the outputs of our detection tool could aid a normative judgment oracle determine the appropriateness of proxy uses. We evaluate the performance of the detection algorithm and show that, in particular cases, the runtime of our system scales linearly in the size of the model. We demonstrate the completeness of the detection algorithm by having it discover artificially injected violations into real data sets. Finally, we evaluate impact of repair on model accuracy, in particular, showing a graceful degradation in accuracy as the influence of the violating proxy increases.\nClosely related work The emphasis on restricting use of information by a system rather than the knowledge possessed by agents distinguishes our work from a large body of work in privacy (see Smith [59] for a survey). The privacy literature on use\nrestrictions has typically focused on explicit use of protected information types, and not on proxy use (see Tschantz et al. [64] for a survey and Lipton and Regan [46]). Recent work on discovering personal data use by black-box web services focuses mostly on explicit use of protected information types by examining causal effects [2, 16, 27, 35–37, 43, 44, 47, 69, 71]); some of this work also examines associational effects [43, 44]. Associational effects capture some forms of proxy use but not others as we argue in Section 3.\nIn a setting similar to ours of a trusted data processor, differential privacy [25] protects against a different type of privacy harm. For a computation involving data contributed by a set of individuals, differential privacyminimizes any knowledge gains by an adversary that are caused by the contribution of a single individual. This requirement, however, says nothing about what information types about an individual are actually used by the data processor, the central concern of use privacy.\nLipton and Regan’s notion of “effectively private\" captures the idea that a protected feature is not explicitly used to make decisions, but does not account for proxy use [46]. Prior work on fairness has also recognized the importance of dealing with proxies in machine learning systems [22, 29, 63]. However treatments of proxy use considered there do not match the requirements of use privacy. We elaborate on this point in Section 3. In Section 7, we provide a more detailed comparison with related work highlighting that use privacy enhancing technology (PET) complements existing work on PETs. It is not meant to supplant other PETs geared toward restricting data collection and release. While the results of this paper represent significant progress toward enabling use privacy, as elaborated in Section 8, a host of challenging problems remain open.\nContributions In summary, we make the following contributions:\n• An articulation of the problem of protecting use privacy in data-driven systems. Use privacy restricts the use of protected information types and some of their proxies (i.e., strong predictors) in automated decision-making systems (§1, 2). • A formal definition of proxy use—a key building block for use privacy–and an axiomatic basis for this definition (§3). • An algorithm for detection and tracing of an instantiation of proxy use in a machine learnt program, and proof that this algorithm is sound and complete (§4). • A repair algorithm that provably removes violations of the proxy use instantiation in a machine learning model that are identified by our detection algorithm and deemed inappropriate by a normative judgment oracle (§5). • An implementation and evaluation of our approach on popular machine learning algorithms applied to real datasets (§6)."
    }, {
      "heading" : "2 USE PRIVACY",
      "text" : "We use the Target example described earlier in the paper to motivate our notion of use privacy. Historically, data collected in a context of interaction between a retailer and a consumer is not expected to result in flows of health information. However, such flow constraints considered in significant theories of privacy (e.g., see\nNissenbaum [51]) cannot be enforced because of possible statistical inferences. In particular, prohibited information types (e.g., pregnancy status) could be inferred from legitimate flows (e.g., shopping history). Thus, the theory of use privacy instead ensures that the data processing systems “simulate ignorance” of protected information types (e.g., pregnancy status) and their proxies (e.g., purchase history) by not using them in their decision-making. Because not all instances of proxy use of a protected information type are inappropriate, our theory of use privacy makes use of a normative judgment oracle that makes this inappropriateness determination for a given instance.\nWe model the personal data processing system as a program p. The use privacy constraint governs a protected information type Z . Our definition of use privacy makes use of two building blocks: (1) a function that given p, Z , and a population distribution \uD835\uDCAB returns a witnessw of proxy use of Z in a program p (if it exists); and (2) a normative judgment oracle \uD835\uDCAA(w) that given a specific witness returns a judgment on whether the specific proxy use is appropriate (true) or not (false).\nDefinition 1 (Use Privacy). Given a program p, protected information type Z , normative judgment oracle \uD835\uDCAA, and population distribution \uD835\uDCAB , use privacy in a program p is violated if there exists a witnessw in p of proxy use of Z in \uD835\uDCAB such that \uD835\uDCAA(w) returns false.\nIn this paper, we formalize the computational component of the above definition of use privacy, by formalizing what it means for an algorithm to use a protected information type directly or through proxies (§3) and designing an algorithm to detect proxy uses in programs (§4). We assume that the normative judgment oracle is given to us and use it to identify inappropriate proxy uses and then repair them (§5). In our experiments, we illustrate how such an oracle would use the outputs of our proxy use analysis and recommend the repair of uses deemed inappropriate by it (§6).\nThis definition cleanly separates computational considerations that are automatically enforceable and ethical judgments that require input from human experts. This form of separation exists also in some prior work on privacy [33] and fairness [23]."
    }, {
      "heading" : "3 PROXY USE: A FORMAL DEFINITION",
      "text" : "We now present an axiomatically justified, formal definition of proxy use in data-driven programs. Our definition for proxy use of a protected information type involves decomposing a program to find an intermediate computation whose result exhibits two properties:\n• Proxy: strong association with the protected type • Use: causal influence on the output of the program\nIn § 3.1, we present a sequence of examples to illustrate the challenge in identifying proxy use in systems that operate on data associated with a protected information type. In doing so, we will also contrast our work with closely-related work in privacy and fairness. In §3.2, we formalize the notions of proxy and use, preliminaries to the definition. The definition itself is presented in §3.3 and §3.4. Finally, in §3.5, we provide an axiomatic characterization of the notion of proxy use that guides our definitional choices. We note that readers keen to get to the detection and repair mechanisms may skip §3.5 without loss of continuity."
    }, {
      "heading" : "3.1 Examples of Proxy Use",
      "text" : "Prior work on detecting use of protected information types [15, 30, 44, 63] and leveraging knowledge of detection to eliminate inappropriate uses [30] have treated the system as a black-box. Detection relied either on experimental access to the black-box [15, 44] or observational data about its behavior [30, 63]. Using a series of examples motivated by the Target case, we motivate the need to peek inside the black-box to detect proxy use.\nExample 3.1. (Explicit use, Fig. 1a) A retailer explicitly uses pregnancy status from prescription data available at its pharmacy to market baby products.\nThis form of explicit use of a protected information type can be discovered by existing black-box experimentation methods that establish causal effects between inputs and outputs (e.g., see [15, 44]).\nExample 3.2. (Inferred use, Fig. 1b) Consider a situation where purchase history can be used to accurately predict pregnancy status. A retailer markets specific products to individuals who have recently purchased products indicative of pregnancy (e.g., a1,a2 ∈ purchases).\nThis example, while very similar in effect, does not use health information directly. Instead, it infers pregnancy status via associations and then uses it. Existing methods (see [30, 63]) can detect such associations between protected information types and outcomes in observational data.\nExample 3.3. (No use, Fig. 1c) Retailer uses some uncorrelated selection of products (a1,n1 ∈ purchases) to suggest ads.\nIn this example, even though the retailer could have inferred pregnancy status from the purchase history, no such inference was used in marketing products. As associations are commonplace, a definition of use disallowing such benign use of associated data would be too restrictive for practical enforcement.\nExample 3.4. (Masked proxy use, Fig. 1d) Consider a more insidious version of Example 3.2. To mask the association between the outcome and pregnancy status, the company also markets baby products to people who are not pregnant, but have low retail engagement, so these advertisements would not be viewed in any case.\nWhile there is no association between pregnancy and outcome in both Example 3.3 and Example 3.4, there is a key difference between them. In Example 3.4, there is an intermediate computation based on aspects of purchase history that is a predictor for pregnancy status, and this predictor is used to make the decision, and therefore is a case of proxy use. In contrast, in Example 3.3, the intermediate computation based on purchase history is uncorrelated with pregnancy status. Distinguishing between these examples by measuring associations using black box techniques is non-trivial. Instead, we leverage white-box access to the code of the classifier to identify the intermediate computation that serves as a proxy for pregnancy status. Precisely identifying the particular proxy used also aids the normative decision of whether the proxy use is appropriate in this setting."
    }, {
      "heading" : "3.2 Notation and Preliminaries",
      "text" : "We assume individuals are drawn from a population distribution \uD835\uDCAB , in which our definitions are parametric. Random variables W ,X ,Y ,Z , . . . are functions over \uD835\uDCAB , and the notation W ∈ \uD835\uDCB2 represents that the type of random variable isW : \uD835\uDCAB → \uD835\uDCB2 . An important random variable used throughout the paper is X, which represents the vector of features of an individual that is provided to a predictive model. A predictive model is denoted by ⟨X,\uD835\uDC9C⟩\uD835\uDCAB , where \uD835\uDC9C is a function that operates onX. For simplicity, we assume that\uD835\uDCAB is discrete, and that models are deterministic. Table 1 summarizes all the notation used in this paper, in addition to the notation for programs that is introduced later in the paper.\n3.2.1 Proxies. A perfect proxy for a random variable Z is a random variable X that is perfectly correlated with Z . Informally, if X is a proxy of Z , then X or Z can be interchangeably used in any computation over the same distribution. One way to state this is to require that Pr(X = Z ) = 1, i.e. X and Z are equal on the distribution. However, we require our definition of proxy to be invariant under renaming. For example, if X is 0 whenever Z is 1 and vice versa, we should still identify X to be a proxy for Z . In order to achieve invariance under renaming, our definition only requires the existence of mappings between X and Z , instead of equality.\nDefinition 2 (Perfect Proxy). A random variable X ∈ \uD835\uDCB3 is a perfect proxy for Z ∈ \uD835\uDCB5 if there exist functions f : \uD835\uDCB3 → \uD835\uDCB5,д : \uD835\uDCB5 → \uD835\uDCB3 , such that Pr(Z = f (X )) = Pr(д(Z ) = X ) = 1.\nWhile this notion of a proxy is too strong in practice, it is useful as a starting point to explain the key ideas in our definition of proxy use. This definition captures two key properties of proxies, equivalence and invariance under renaming.\nEquivalence Definition 2 captures the property that proxies admit predictors in both directions: it is possible to construct a predictor of X from Z , and vice versa. This condition is required to ensure that our definition of proxy only identifies the part of the input that corresponds to the protected attribute and not the input attribute as a whole. For example, if only the final digit of a zip code is a proxy for race, the entirety of the zip code will not be identified as a proxy even though it admits a predictor in one direction. Only if the final digit is used, that use will be identified as proxy use.\nThe equivalence criterion distinguishes benign use of associated information from proxy use as illustrated in the next example. For machine learning in particular, this is an important pragmatic\nmedical records\nad2 not pregnant\nad1pregna nt\n(a) Explicit Use\npurchases\nad2 n1, n2\nad1a1, a2\n(b) Use via proxy\npurchases\nad2 a2, n2\nad1a1, n1\n(c) No use\npurchases\nretail eng. ad1high\nad2lown1, n2\nretail eng. ad2high\nad1low\na1, a 2\n(d) Masked use via proxy\nFigure 1: Examples of models (decision trees) used by a retailer for offering medicines and for selecting advertisements to show to customers. The retailer uses pregnancy status, past purchases, and customer’s level of retail engagement. Products a1 and a2 are associated with pregnancy (e.g., prenatal vitamins, scent-free lotions) whereas products n1 and n2 are associated with a lack of pregnancy (e.g., alcohol, camping gear); all four products are equally likely. Retail engagement, (high or low), indicating whether the customer views ads or not, is independent of pregnancy.\nrequirement; given enough input features one can expect any protected class to be predictable from the set of inputs. In such cases, the input features taken together are a strong associate in one direction, and prohibiting such one-sided associates from being used would rule out most machine learnt models.\nExample 3.5. Recall that in Figure 1, a1,a2 is a proxy for pregnancy status. In contrast, consider Example 3.3, where purchase history is an influential input to the program that serves ads. Suppose that the criteria is to serve ads to those with a1, n1 in their purchase history. According to Definition 2, neither purchase history or a1,n1 are proxies, because pregnancy status does not predict purchase history or a1,n1. However, if Definition 2 were to allow one-sided associations, then purchase history would be a proxy because it can predict pregnancy status. This would have the unfortunate effect of implying that the benign application in Example 3.3 has proxy use of pregnancy status.\nInvariance under renaming This definition of a proxy is invariant under renaming of the values of a proxy. Suppose that a random variable evaluates to 1when the protected information type is 0 and vice versa, then this definition still identifies the random variable as a proxy.\n3.2.2 Influence. Our definition of influence aims to capture the presence of a causal dependence between a variable and the output of a function. Intuitively, a variable x is influential on f if it is possible to change the value of f by changing x while keeping the other input variables fixed.\nDefinition 3. For a function f (x ,y), x is influential if and only if there exists values x1, x2, y, such that f (x1,y) , f (x2,y).\nIn Figure 1a, pregnancy status is an influential input of the system, as just changing pregnancy status while keeping all other inputs fixed changes the prediction. Influence, as defined here, is identical to the notion of interference used in the information flow literature."
    }, {
      "heading" : "3.3 Definition",
      "text" : "We use an abstract framework of program syntax to reason about programs without specifying a particular language to ensure that our definition remains general. Our definition relies on syntax to reason about decompositions of programs into intermediate\ncomputations, which can then be identified as instances of proxy use using the concepts described above.\nProgramdecomposition Weassume thatmodels are represented by programs. For a set of random variables X, ⟨X,p⟩\uD835\uDCAB denotes the assumption that p will run on the variables in X. Programs are given meaning by a denotation function J·KX that maps programs to functions. If ⟨X,p⟩\uD835\uDCAB , then JpK is a function on variables in X, and JpK(X) represents the random variable of the outcome of p, when evaluated on the input random variables X. Programs support substitution of free variables with other programs, denoted by [p1/X ]p2, such that if p1 and p2 programs that run on the variables X and X,X , respectively, then [p1/X ]p2 is a program that operates on X.\nA decomposition of program p is a way of rewriting p as two programs p1 and p2 that can be combined via substitution to yield the original program.\nDefinition 4 (Decomposition). Given a program p, a decomposition (p1,X ,p2) consists of two programs p1, p2, and a fresh variable X , such that p = [p1/X ]p2.\nFor the purposes of our proxy use definition we view the first component p1 as the intermediate computation suspected of proxy use, and p2 as the rest of the computation that takes in p1 as an input.\nDefinition 5 (Influential Decomposition). Given a program p, a decomposition (p1,X ,p2) is influential iff X is influential in p2.\nMain definition\nDefinition 6 (Proxy Use). A program ⟨X,p⟩\uD835\uDCAB has proxy use of Z if there exists an influential decomposition (p1,X ,p2) of ⟨X,p⟩\uD835\uDCAB , and Jp1K(X) is a proxy for Z .\nExample 3.6. In Figure 1d, this definition would identify proxy use using the decomposition (p1,U ,p2), where p2 is the entire tree, but with the condition (a1,a2 ∈ purchases) replaced by the variable U . In this example, U is influential in p2, since changing the value of U changes the outcome. Also, we assumed that the condition (a1,a2 ∈ purchases) is a perfect predictor for pregnancy, and is therefore a proxy for pregnancy. Therefore, according to our definition of proxy use, the model in 1d has proxy use of pregnancy status."
    }, {
      "heading" : "3.4 A Quantitative Relaxation",
      "text" : "Definition 6 is too strong in one sense and too weak in another. It requires that intermediate computations be perfectly correlated with a protected attribute, and that there exists some input, however improbable, in which the result of the intermediate computation is relevant to the model. For practical purposes, we would like to capture imperfect proxies that are strongly associated with an attribute, but only those whose influence on the final model is appreciable. To relax the requirement of perfect proxies and non-zero influence, we quantify these two notions to provide a parameterized definition. Recognizing that neither perfect privacy nor perfect utility are practical, the quantitative definition provides a means for navigating privacy vs. utility tradeoffs.\nϵ-proxies We wish to measure how strongly a random variable X is a proxy for a random variable Z . Recall the two key requirements from the earlier definition of a proxy: (i) the association needs to be capture equivalence and measure association in both directions, and (ii) the association needs to be invariant under renaming of the random variables. The variation of information metric dvar(X ,Z ) = H (X |Z ) + H (Z |X ) [12] is one measure that satisfies these two requirements. The first component in the metric, the conditional entropy of X given Z , H (X |Z ), measures how well X can be predicted from Z , and H (Z |X ) measures how well Z can be predicted from X , thus satisfying the requirement for the metric measuring association in both directions. Additionally, one can show that conditional entropies are invariant under renaming, thus satisfying our second criteria. To obtain a normalized measure in [0, 1], we choose 1− dvar(X ,Z )H (X ,Z ) as our measure of association, where the measure being 1 implies perfect proxies, and 0 implies statistical independence. Interestingly, this measure is identical to normalized mutual information [12], a standard measure that has also been used in prior work in identifying associations in outcomes of machine learning models [63].\nDefinition 7 (Proxy Association). Given two random variables"
    }, {
      "heading" : "X and Z , the strength of a proxy is given by normalized mutual",
      "text" : "information,\nd(X ,Z ) def= 1 − H (X |Z ) + H (Z |X ) H (X ,Z )\nwhere X is defined to be an ϵ-proxy for Z if d(X ,Z ) ≥ ϵ .\nWe do not present the complexity of association computation independently of detection as we rely on pre-computations to reduce the amortized runtime of the entire detection algorithm. The complexity as part of our detection algorithm is discussed in Appendix E.2.\nδ-influential decomposition Recall that for a decomposition (p1,X ,p2), in the qualitative sense, influence is interference which implies that there exists x , x1, x2, such that Jp2K(x ,x1) , Jp2K(x ,x2). Herex1,x2 are values ofp1, that for a givenx , change the outcome of p2. However, this definition is too strong as it requires only a single pair of values x1, x2 to show that the outcome can be changed by p1 alone. To measure influence, we quantify interference by using Quantitative Input Influence (QII), a causal measure of input influence introduced in [14]. In our context, for a decomposition\n(p1,X ,p2), the influence of p1 on p2 is given by: ι(p1,p2) def = EX,X′ $←\uD835\uDCAB Pr ( Jp2K(X, Jp1K(X)) , Jp2K(X, Jp1K(X′)) ) .\nIntuitively, this quantity measures the likelihood of finding randomly chosen values of the output of p1 that would change the outcome of p2. Note that this general definition allows for probabilistic models though in this work we only evaluate our methods on deterministic models.\nThe time complexity of influence computation as part of our detection algorithm can be found in Appendix E.2, along with discussion on estimating influence.\nDefinition 8 (Decomposition Influence). Given a decomposition (p1,X ,p2), the influence of the decomposition is given by the QII of X on p2. A decomposition (p1,X ,p2) is defined to be δ -influential if ι(p1,p2) > δ . (ϵ,δ )-proxy use Now that we have quantitative versions of the primitives used in Definition 6, we are in a position to define quantitative proxy use (Definition 9). The structure of this definition is the same as before, with quantitative measures substituted in for the qualitative assertions used in Definition 6.\nDefinition 9 ((ϵ,δ )-proxy use). A program ⟨X,p⟩\uD835\uDCAB has (ϵ,δ )- proxy use of random variable Z iff there exists a δ -influential decomposition (p1,X ,p2), such that JpK(X) is an ϵ-proxy for Z .\nThis definition is a strict relaxation of Definition 6, which reduces to (1, 0)-proxy use."
    }, {
      "heading" : "3.5 Axiomatic Basis for Definition",
      "text" : "We now motivate our definitional choices by reasoning about a natural set of properties that a notion of proxy use should satisfy. We first prove an important impossibility result that shows that no definition of proxy use can satisfy four natural semantic properties of proxy use. The central reason behind the impossibility result is that under a purely semantic notion of function composition, the causal effect of a proxy can be made to disappear. Therefore, we choose a syntactic notion of function composition for the definition of proxy use presented above. The syntactic definition of proxy use is characterized by syntactic properties which map very closely to the semantic properties.\nProperty 1. (Explicit Use) IfZ is an influential input of the model ⟨{X,Z },\uD835\uDC9C⟩\uD835\uDCAB , then ⟨{X,Z },\uD835\uDC9C⟩\uD835\uDCAB has proxy use of Z .\nThis property identifies the simplest case of proxy use: if an input to the model is influential, then the model exhibits proxy use of that input.\nProperty 2. (Preprocessing) If a model ⟨{X,X },\uD835\uDC9C⟩\uD835\uDCAB has proxy use of random variableZ , then for any function f such that Pr (f (X) = X ) = 1, let \uD835\uDC9C′(x) def= \uD835\uDC9C(x , f (x)). Then, ⟨X,\uD835\uDC9C′⟩\uD835\uDCAB has proxy use of Z .\nThis property covers the essence of proxy use where instead of being provided a protected information type explicitly, the program uses a strong predictor for it instead. This property states that models that use inputs explicitly and via proxies should not be differentiated under a reasonable theory of proxy use.\nProperty 3. (Dummy) Given ⟨X,\uD835\uDC9C⟩\uD835\uDCAB , define \uD835\uDC9C′ such that for all x ,x ′, \uD835\uDC9C′(x ,x ′) def= \uD835\uDC9C(x), then ⟨X,\uD835\uDC9C⟩\uD835\uDCAB has proxy use for some Z iff ⟨{X,X },\uD835\uDC9C′⟩\uD835\uDCAB has proxy use of Z .\nThis property states that the addition of an input to a model that is not influential, i.e., has no effect on the outcomes of the model, has no bearing on whether a program has proxy use or not. This property is an important sanity check that ensures that models aren’t implicated by the inclusion of inputs that they do not use.\nProperty 4. (Independence) If X is independent of Z in \uD835\uDCAB , then ⟨X,\uD835\uDC9C⟩\uD835\uDCAB does not have proxy use of Z .\nIndependence between the protected information type and the inputs ensures that the model cannot infer the protected information type for the population \uD835\uDCAB . This property captures the intuition that if the model cannot infer the protected information type then it cannot possibly use it.\nWhile all of these properties seem intuitively desirable, it turns out that these properties can not be achieved simultaneously.\nTheorem 1. No definition of proxy use can satisfy Properties 1-4 simultaneously.\nSee Appendix A for a proof of the impossibility result and a discussion. The key intuition behind this result is that Property 2 requires proxy use to be preserved when an input is replaced with a function that predicts that input via composition. However, with a purely semantic notion of function composition, after replacement, the proxy may get canceled out. To overcome this impossibility result, we choose a more syntactic notion of function composition, which is tied to how the function is represented as a program, and looks for evidence of proxy use within the representation.\nWe now proceed to the axiomatic justification of our definition of proxy use. As in our attempt to formalize a semantic definition, we base our definition on a set of natural properties given below. These are syntactic versions of their semantic counterparts defined earlier.\nProperty 5. (Syntactic Explicit Use) If X is a proxy of Z , and X is an influential input of ⟨{X,X },p⟩\uD835\uDCAB , then ⟨{X,X },p⟩\uD835\uDCAB has proxy use.\nProperty 6. (Syntactic Preprocessing) If ⟨{X,X },p1⟩\uD835\uDCAB has proxy use ofZ , then for anyp2 such that Pr (Jp2K(X) = X ) = 1, ⟨X, [p2/X ]p1⟩\uD835\uDCAB has proxy use of Z .\nProperty 7. (Syntactic Dummy)Given a program ⟨X,p⟩\uD835\uDCAB , ⟨X,p⟩\uD835\uDCAB has proxy use for some Z iff ⟨{X,X },p⟩\uD835\uDCAB has proxy use of Z .\nProperty 8. (Syntactic Independence) If X is independent of Z , then ⟨X,p⟩\uD835\uDCAB does not have proxy use of Z .\nProperties 5 and 6 together characterize a complete inductive definition, where the induction is over the structure of the program. Suppose we can decompose programs p into (p1,X ,p2) such that p = [p1/X ]p2. Now if X , which is the output of p1, is a proxy for Z and is influential in p2, then by Property 5, p2 has proxy use. Further, since p = [p1/X ]p2, by Property 6, p has proxy use. This inductive definition where we use Property 5 as the base case and Property 6 for the induction step, precisely characterizes Definition 6. Additionally, it can be shown that Definition 6 also satisfies Properties 7 and 8. Essentially, by relaxing our notion of function composition to a syntactic one, we obtain a practical definition of proxy use characterized by the natural axioms above.\nAlgorithm 1 Detection for expression programs.\nRequire: association (d), influence(ι) measures procedure ProxyDetect(p,X,Z , ϵ,δ )\nP ← ∅ for each subprogram p1 appearing in p do\nfor each program p2 such that [p2/u]p1 = p do if ι(p1,p2) ≥ δ ∧ d(Jp1K(X),Z ) ≥ ϵ then\nP ← P ∪ {(p1,p2)} return P"
    }, {
      "heading" : "4 DETECTING PROXY USE",
      "text" : "In this section, we present an algorithm for identifying proxy use of specified variables in a given machine-learning model (Algorithm 1, Appendix B contains a more formal presentation of the algorithm for the interested reader). The algorithm is program-directed and is directly inspired by the definition of proxy use in the previous section. We prove that the algorithm is complete in a strong sense — it identifies every instance of proxy use in the program (Theorem 3). We also describe three optimizations that speed up the detection algorithm: sampling, reachability analysis, and contingency tables."
    }, {
      "heading" : "4.1 Environment Model",
      "text" : "The environment in which our detection algorithm operates is comprised of a data processor, a dataset that has been partitioned into analysis and validation subsets, and a machine learning model trained over the analysis subset. We assume that the data processor does not act to evade the detection algorithm, and the datasets correspond to a representative sample from the population we wish to test proxy use with respect to. Additionally, we assume that information types we wish to detect proxies of are also part of the validation data. We discuss these points further in Section 8.\nFor the rest of this paper we focus on an instance of the proxy use definition, where we assume that programs are written in the simple expression language shown in Figure 2. However, our techniques are not tied to this particular language, and the key ideas behind them apply generally. This language is rich enough to support commonly-used models such as decision trees, linear and logistic regression, Naive Bayes, and Bayesian rule lists. Programs are functions that evaluate arithmetic terms, which are constructed from real numbers, variables, common arithmetic operations, and if-then-else (ite(·, ·, ·)) terms. Boolean terms, which are used as conditions in ite terms, are constructed from the usual connectives and relational operations. Finally, we use λ-notation for functions, i.e., λx .e denotes a function over x which evaluates e after replacing all instances of x with its argument. Details on how machine learning models such as linear models, decision trees, and random forests are translated to this expression language are discussed in Appendix B.2 and consequences of the choice of language and decomposition in that language are further discussed in more detail in Section 8. Distributed proxies Our use of program decomposition provides for partial handling of distributed representations, the idea that concepts can be distributed among multiple entities. In our case, influence and association of a protected information type can be distributed among multiple program points. First, substitution\n⟨aexp⟩ ::= R | var | op(⟨aexp⟩, . . . , ⟨aexp⟩) | ite(⟨bexp⟩, ⟨aexp⟩, ⟨aexp⟩) ⟨bexp⟩ ::= T | F | ¬ ⟨bexp⟩ | op(⟨bexp⟩, . . . , ⟨bexp⟩) | relop(⟨aexp⟩, ⟨aexp⟩) ⟨prog⟩ ::= λvar1, . . . , varn . ⟨aexp⟩\nFigure 2: Syntax for the language used in our analysis.\n(denoted by [p1/X ]p2) is defined to replace all instances of variable X in p2 with the program p1. If there are multiple instances of X in p2, they are still describing a single decomposition and thus the multiple instances of p2 in p1 are viewed as a single proxy. Further, implementations of substitution can be (and is in our implementation) associativity-aware: programs like x1 + x2 + x3 can be equivalent regardless of the order of the expressions in that they can be decomposed in exactly the same set of ways. If a proxy is distributed among x1 and x3, it will still be considered by our methods because x1 + (x2 + x3) is equivalent to (x1 + x3) + x2, and the subexpression x1 + x3 is part of a valid decomposition. Allowing such equivalences within the implementation of substitution partially addresses the problem that our theory does not respect semantic equivalence, which is a necessary consequence of Theorem 1."
    }, {
      "heading" : "4.2 Analyzing Proxy Use",
      "text" : "Algorithm 1 describes a general technique for detecting (ϵ,δ )-proxy use in expression programs. In addition to the parameters and expression, it takes as input a description of the distribution governing the feature variables X and Z . In practice this will nearly always consist of an empirical sample, but for the sake of presentation we simplify here by assuming the distribution is explicitly given. In Section E.2, we describe how the algorithm can produce estimates from empirical samples.\nThe algorithm proceeds by enumerating sub-expressions of the given program. For each sub-expression e appearing inp, ProxyDetect computes the set of positions at which e appears. If e occurs multiple times, we consider all possible subsets of occurrences as potential decompositions1 . It then iterates over all combinations of these positions, and creates a decomposition for each one to test for (ϵ,δ )-proxy use. Whenever the provided thresholds are exceeded, the decomposition is added to the return set. This proceeds until there are no more subterms to consider. While not efficient in the worst-case, this approach is both sound and complete with respect to Definition 9.\nTheorem 2 (Detection soundness). Any decomposition (p1,p2) returned by ProxyDetect(p,X, ϵ,δ ) is a decomposition of the input program p and had to pass the ϵ,δ thresholds, hence is a (ϵ,δ )-proxy use.\nTheorem 3 (Detection completeness). Every decomposition which could be a (ϵ,δ )-proxy use is enumerated by the algorithm. Thus, if (p1,p2) is a decomposition of p with ι(p1,p2) ≥ d and d(Jp1K(X),Z ) ≥ ϵ , it will be returned by ProxyDetect(p,X, ϵ,δ ).\n1This occurs often in decision forests (see Figure 8).\nOur detection algorithm considers single terms in its decomposition. Sometimes a large number of syntactically different proxies with weak influence might collectively have high influence. A stronger notion of program decomposition that allows a collection of multiple terms to be considered a proxy would identify such a case of proxy use but will have to search over a larger space of expressions. Exploring this tradeoff between scalability and richer proxies is an important topic for future work.\nThe detection algorithm runs in time\uD835\uDCAA (|p | c (|\uD835\uDC9F | + k |\uD835\uDC9F |))where |\uD835\uDC9F | is the size of a dataset employed in the analysis, c is the number of decompositions of a program, k is the maximum number of elements in the ranges of all sub-programs (|\uD835\uDC9F | in the worst case), and |p | is the number of sub-expressions of a program. The number of decompositions varies from \uD835\uDCAA (|p |) to \uD835\uDCAA ( 2 |p | ) depending on\nthe type of program analyzed. Details can be found in Appendix E along with more refined bounds for several special cases."
    }, {
      "heading" : "5 REMOVING PROXY USE VIOLATIONS",
      "text" : "In this section we present a repair algorithm for removing violations of (ϵ,δ )-Proxy Use in a model. Our approach has two parts: first (Algorithm 2) is the iterative discovery of proxy uses via the ProxyDetect procedure described in the previous section and second (Algorithm 3) is the repair of the ones found by the oracle to be violations. We describe these algorithms informally here, and Appendix C contains formal descriptions of these algorithms. The iterative discovery procedure guarantees that the returned program is free of violations (Algorithm 5). Our repair procedures operate on the expression language, so they can be applied to any model that can be written in the language. Further, our violation repair algorithm does not require knowledge of the training algorithm that produced the model. The witnesses of proxy use localize where in the program violations occur. To repair a violation we search through expressions local to the violation, replacing the one which has the least impact on the accuracy of the model that at the same time reduces the association or influence of the violation to below the (ϵ,δ ) threshold.\nAt the core of our violation repair algorithm is the simplification of sub-expressions in a model that are found to be violations. Simplification here means the replacement of an expression that is not a constant with one that is. Simplification has an impact on the model’s performance hence we take into account the goal of preserving utility of the machine learning program we repair. We parameterize the procedure with a measure of utilityv that informs the selection of expressions and constants for simplification. We briefly discuss options and implementations for this parameter later in this section.\nThe repair procedure (Algorithm 3) works as follows. Given a program p and a decomposition (p1,p2), it first finds the best simplification to apply to p that would make (p1,p2) no longer a violation. This is done by enumerating expressions that are local to p1 in p2 (Line 3). Local expressions are sub-expressions of p1 as well as p1 itself and if p1 is a guard in an if-then-else expression, then local expressions of p1 also include that if-then-else’s true and false branches as well as their sub-expressions. Each of the local expressions corresponds to a decomposition of p into the local expression p′1 and the context around it p ′ 2. For each of these local\nAlgorithm 2Witness-driven repair.\nRequire: association (d), influence (ι), utility (v) measures, oracle (\uD835\uDCAA) procedure Repair(p,X,Z , ϵ,δ )\nP ← {d ∈ ProxyDetect(p,X,Z , ϵ,δ ) : not \uD835\uDCAA(d)} if P , ∅ then\n(p1,p2) ← element of P p′ ← ProxyRepair(p, (p1,p2),X,Z , ϵ,δ ) return Repair(p′,X,Z , ϵ,δ )\nelse return p\nAlgorithm 3 Local Repair.\nRequire: association (d), influence (ι), utility (v) measures 1: procedure ProxyRepair(p, (p1,p2),X,Z , ϵ,δ ) 2: R ← {} 3: for each subprogram p′1 of p1 do 4: r∗ ← Optimal constant for replacing p′1 5: (p′′1 ,p ′′ 2 ) ← (p1,p2) with r\n∗ subst. for p′1 6: if ι(p′′1 ,p ′′ 2 ) ≤ δ ∨ d(Jp′′1 K(X),Z ) ≤ ϵ then 7: R ← R ∪ [u/r∗]p′2 8: return argmaxp∗∈R v (p∗)\ndecompositions we discover the best constant, in terms of utility, to replace p′1 with (Line 4). We then make the same simplification to the original decomposition (p1,p2), resulting in (p′′1 ,p ′′ 2 ) (Line 5) Using this third decomposition we check whether making the simplification would repair the original violation (Line 6), collecting those simplified programs that do. Finally, we take the best simplification of those found to remove the violation (Line 8). Details on how the optimal constant is selected is described in Appendix C.1.\nTwo important things to note about the repair procedure. First, there is always at least one subprogram on Line 3 that will fix the violation, namely the decomposition (p1,p2) itself. Replacing p1 with a constant in this case would disassociate it from the sensitive information type. Secondly, the procedure produces a model that is smaller than the one given to it as it replaces a non-constant expression with a constant. These two let us state the following:\nTheorem 4. Algorithm 2 terminates and returns a program that does not have any (ϵ,δ )-Proxy Use violations (instances of (ϵ,δ )-Proxy Use for which oracle returns false)."
    }, {
      "heading" : "6 EVALUATION",
      "text" : "In this section we empirically evaluate our definition and algorithms on several real datasets. In particular, we simulate a financial services application and demonstrate a typical workflow for a practitioner using our tools to detect and repair proxy use in decision trees and linear models (§6.1). We highlight that this workflow identifies more proxy uses over a baseline procedure that simply removes features associated with a protected information type. For three other simulated settings on real data sets—contraception advertising, student assistance, and credit advertising—we describe our findings of interesting proxy uses and demonstrate how the outputs of our detection tool would allow a normative judgment\noracle to determine the appropriateness of proxy uses (§6.2). In §6.3, we evaluate the performance of our detection and repair algorithms and show that in particular cases, the runtime of our system scales linearly in the size of the model. Also, by injecting violations into real data sets so that we have ground truth, we evaluate the completeness of our algorithm, and show a graceful degradation in accuracy as the influence of the violating proxy increases.\nModels and Implementation Our implementation currently supports linear models, decision trees, random forests, and rule lists. Note that these model types correspond to a range of commonlyused learning algorithms such as logistic regression, support vector machines [10], CART [6], and Bayesian rule lists [45]. Also, these models represent a significant fraction of models used in practice in predictive systems that operate on personal information, ranging from advertising [9], psychopathy [38], criminal justice [4, 5], and actuarial sciences [32, 34]. Our prototype implementation was written in Python, and we use scikit-learn package to train the models used in the evaluation. The benchmarks we describe later in this section were recorded on a Ubuntu Desktop with 4.2 GHz Intel Core i7 and 32GB RAM."
    }, {
      "heading" : "6.1 Example Workflow",
      "text" : "A financial services company would like to expand its client base by identifying potential customers with high income. To do so, the company hires an analyst to build a predictive model that uses age, occupation, education level, and other socio-economic features to predict whether an individual currently has a “high” or “low” income. This practice is in line with the use of analytics in the financial industry that exploit the fact that high-income individuals are more likely to purchase financial products [70].\nBecause demographic data is known to correlate with marital status [50], the data processor would like to ensure that the trained model used to make income predictions does not effectively infer individuals’ marital status from the other demographic variables that are explicitly used. In this context, basing the decision of which clients to pursue onmarital status could be perceived as a privacy violation, as other socio-economic variables are more directly related to one’s interest and eligibility in various financial services.\nTo evaluate this scenario, we trained an income prediction model from the UCI Adult dataset which consists of roughly 48,000 rows containing economic and demographic information for adults derived from publicly-available U.S. Census data. One of the features available in this data is marital status, so we omitted it during training, and later used it when evaluating our algorithms. In this scenario, we act as the oracle in order to illustrate the kind of normative judgments an analyst would need to make as an oracle.\nAfter training a classifier on the preprocessed dataset, we found a strong proxy for marital status in terms of an expression involving relationship status. Figure 3 visualizes all of the expressions making up the model (marked as •), along with their association and influence measures. In decision trees, sub-expressions like these coincide with decompositions in our proxy use definition; each sub-expression can be associated with a decomposition that cuts out that sub-expression from the tree, and leaves a variable in its place. The connecting lines in the figure denote the sub-expression relationship. Together with the placement of points on the influence\nand association scales, this produces an overview of the decision tree and the relationship of its constituent parts to the sensitive attribute.\nOn further examination the relationship status was essentially a finer-grained version of marital status. While not interesting in itself, this occurrence demonstrates an issue with black-box use of machine learning without closely examining the structure of the data. In particular, one can choose to remove this feature, and the model obtained after retraining will make predictions that have low association with marital status. However, one submodel demonstrated relatively strong proxy use (ϵ = 0.1,δ = 0.1): age ≤ 31 and sex = 0 and capital_loss ≤ 1882.50 (labeled A in Figure 4). This demonstrates that simply removing a feature does not ensure that proxies are removed. When the model is retrained, the learning algorithm might select new computations over other features to embed in the model, as it did in this example. Also, note that the new proxy combines three additional features. Eliminating all of these features from the data could impact model performance. Instead we can use our repair algorithm to remove the proxy: we designate the unacceptable ϵ,δ thresholds (the darkest area in Figure 4) and repair any proxies in that range. The result is the decision tree marked with + in the figure. Note that this repaired version has no sub-expressions in the prohibited range and that most of the tree remains unchanged (the • and + markers largely coincide)."
    }, {
      "heading" : "6.2 Other Case Studies",
      "text" : "We now briefly discuss interesting examples for proxy use from other case studies, demonstrating how our framework aids normative use privacy judgments. More details on these datasets and experiments are in Appendix D.1. Targeted contraception advertising We consider a scenario in which a data processor wishes to show targeted advertisements for contraceptives to females. We evaluated this scenario using data collected for the 1987 National Indonesia Contraceptive Survey [1], which contains a number of socio-economic features, including feature indicating whether the individual’s religious beliefs were Islam. A decision tree trained on this dataset illustrates an interesting case of potential use privacy via the following proxy for religion:\nite(educ < 4∧ nchild ≤ 3∧ age < 31, no, yes). This term predicts that women younger than 31, with below-average education background and fewer than four children will not use contraception. In fact, just the “guard” term educ < 4 alone is more closely associated with religion, and its influence on the model’s output is nearly as high. This reveals a surprising association between education levels and religion leading to a potentially concerning case of proxy use.\nStudent assistance A current trend in education is the use of predictive analytics to identify students who are likely to benefit from certain types of interventions [31, 39]. We look at a scenario where a data processor builds a model to predict whether a secondary school student’s grades are likely to suffer, based on a range of demographic features, social information, and academic information. To evaluate this scenario, we trained a model on the UCI Student Alcohol Consumption dataset [11], with alcohol use as the sensitive feature. Our algorithm found the following proxy for alcohol use: studytime < 2. This finding suggests that this instance of proxy use can be deemed an appropriate use, and not a privacy violation, as the amount of time a student spends studying is clearly relevant to their academic performance.\nCredit advertisements We consider a situation where a credit card company wishes to send targeted advertisements for credit cards based on demographic information. In this context, the use of health status for targeted advertising is a legitimate privacy concern [18]. To evaluate this scenario, we trained a model to predict interest in credit cards using the PSID dataset. From this, we trained two models: one that identifies individuals with student loans and another that identifies individuals with existing credit cards as the two groups to be targeted. The first model had a number of instances of proxy use. One particular subcomputation that was concerning was a subtree of the original decision tree that branched on the number of children in the family. This instance provided negative outcomes to individuals with more children, and may be deemed inappropriate for use in this context. In the second model, one proxy was a condition involving income income ≤ 33315. The use of income in this context is justifiable, and therefore this may be regarded as not being a use privacy violation."
    }, {
      "heading" : "6.3 Detection and Repair",
      "text" : "For the remainder of the section we focus on evaluating the performance and efficacy of the detection and repair algorithms. We begin by exploring the impact of the dataset and model size on the detection algorithm’s runtime.\nFigure 5 demonstrates the runtime of our detection algorithm on three models trained on the UCI Adult dataset vs. the size of the dataset used for the association and influence computations. The algorithm here was forced to compute the association and influence metrics for each decomposition (normally influence can be skipped if association is below threshold) and thus represents a worst-case runtime. The runtime for the random forest and decision tree scales linearly in dataset size due to several optimizations. The logistic regression does not benefit from these and scales quadratically. Further, runtime for each model scales linearly in the number of decompositions (see Appendix D.2) , but logistic regression models contain an exponential number of decompositions as a function of their size.\nTo determine the completeness of our detection algorithm we inserted a proxy in a trained model to determine whether we can detect it. To do this, we used the UCI Student Alcohol Consumption dataset to train two decision trees: one to predict students’ grades, and one to predict alcohol consumption. We then inserted the second tree into random positions of the first tree thereby introducing a proxy for alcohol consumption. We observed that in each case, we were able to detect the introduced proxy. While not interesting in itself due to our completeness theorem, we used this experiment to explore how much utility is actually lost due to repair. We evaluate our repair algorithm on a set of similar models with inserted violations of various influence magnitude. The results can be seen in Figure 6. We can see that the accuracy (i.e., ratio of instances that have agreement between repaired and unrepaired models) falls linearly with the influence of the inserted proxy. This implies that repair of less influential proxies will incur a smaller accuracy penalty than repair of more influential proxies. In other words, our repair methods do not unduly sacrifice accuracy when repairing only minor violations.\nA point not well visible in this figure is that occasionally repair incurs no loss of utility. This is due to our use of the scikit-learn library for training decision trees as it does not currently support pruning unnecessary nodes. Occasionally such nodes introduce associations without improving the model’s accuracy. These nodes can be replaced by constants without loss. We have also observed this in some of our case studies."
    }, {
      "heading" : "7 RELATEDWORK",
      "text" : ""
    }, {
      "heading" : "7.1 Definition",
      "text" : "Minimizing disclosures In the computer science literature, privacy has been thought of as the ability to protect against undesired flows of information to an adversary. Much of the machinery developed in cryptography, such as encryption, anonymous communication, private computation, and database privacy have been motivated by such a goal. Differential privacy [25] is one of themain pillars of privacy research in the case of computations over data aggregated from a number of individuals, where any information gained by an adversary observing the computation is not caused by an individual’s participation. However, none of these technologies cover the important setting of individual-level data analytics, where one may want to share some information while hiding others from adversaries with arbitrary background knowledge. This absence is with good reason, as in the general case it is impossible to prevent flows of knowledge from individual-level data, while preserving the utility of such data, in the presence of arbitrary inferences that may leverage the background knowledge of an adversary [21]. In this work, we do not attempt to solve this problem either.\nNevertheless, the setting of individual level data analytics is pervasive, especially in the case of predictive systems that use machine learning. Since these systems are largely opaque, even developers do not have a handle on information they may be inadvertently using via inferences. Therefore, in this work, we make the case for proxy use restrictions in data driven systems and develop techniques to detect and repair violations of proxy use. Restrictions on information use, however do not supplant the need for other privacy enhancing technologies geared for restricting information\ncollection and disclosure, which may be useful in conjunction with the enforcement of use restrictions. For example, when machine learning models are trained using personal data, it is desirable to minimize disclosures pertaining to individuals in the training set, and to reduce the use of protected information types for the individuals the models are applied to.\nIdentifying explicit use The privacy literature on use restrictions has typically focused on explicit use of protected information types, not on proxy use (see Tschantz et al. [64] for a survey and Lipton and Regan [46]). Recent work on discovering personal data use by black-box web services focuses mostly on explicit use of protected information types by examining causal effects [16, 44]; some of this work also examines associational effects [43, 44]. Associational effects capture some forms of proxy use but not others as we argued in Section 3."
    }, {
      "heading" : "7.2 Detection and Repair Models",
      "text" : "Our detection algorithm operates with white-box access to the prediction model. Prior work requires weaker access assumptions.\nAccess to observational data Detection techniques working under an associative use definition [30, 63] usually only require access to observational data about the behavior of the system.\nAccess to black-box experimental data Detection techniques working under an explicit use definition of information use [16, 44] typically require experimental access to the system. This access allows the analyst to control some inputs to the system and observe relevant outcomes.\nThe stronger white-box access level allows us to decompose the model and trace an intermediate computation that is a proxy. Such traceability is not afforded by the weaker access assumptions in prior work. Thus, we explore a different point in the space by giving up on the weaker access requirement to gain the ability to trace and repair proxy use.\nTramèr et al. [63] solve an important orthogonal problem of efficiently identifying populations where associations may appear. Since our definition is parametric in the choice of the population, their technique could allow identifying relevant populations for further analysis using our methods.\nRepair Removal of violations of privacy can occur at different points of the typical machine learning pipeline. Adjusting the training dataset is the most popular approach, including variations that relabel only the class attribute [48], modify entire instances while maintaining the original schema [30], and transform the dataset into another space of features [24, 72]. Modifications to the training algorithm are specific to the trainer employed (or to a class of trainers). Adjustments to Naive Bayes [7] and trainers amiable to regularization [42] are examples. Several techniques for producing differentially-private machine learning models modify trained models by perturbing coefficients [3, 8]. Other differentially-private data analysis techniques [26] instead perturb the output by adding symmetric noise to the true results of statistical queries. All these repair techniques aim to minimize associations or inference from the outcomes rather than constrain use."
    }, {
      "heading" : "8 DISCUSSION",
      "text" : "Beyond strict decomposition Theorem 1 shows that a definition satisfying natural semantic properties is impossible. This result motivates our syntactic definition, parameterized by a programming language and a choice of program decomposition. In our implementation, the choice of program decomposition is strict. It only considers single terms in its decomposition. However, proxies may be distributed across different terms in the program. As discussed in Section 4.1, single term decompositions can also deal with a restricted class of such distributed proxies. Our implementation does not identify situations where each of a large number of syntactically different proxies have weak influence but together combine to result in high influence. A stronger notion of program decomposition that allows a collection of multiple terms to be considered a proxy would identify such a case of proxy use.\nThe choice of program decomposition also has consequences for the tractability of the detection and repair algorithms. The detection and repair algorithms presented in this paper currently enumerate through all possible subprograms in the worst case. Depending on the flexibility of the language chosen and the model2 being expressed there could be an exponentially large number of subprograms, and our enumeration would be intractable.\nImportant directions of future work are therefore organized along two thrusts. The first thrust is to developmore flexible notions of program decompositions that identify a wide class of proxy uses for other kinds of machine learning models, including deep learning models that will likely require new kinds of abstraction techniques due to their large size. The second thrust is to identify scalable algorithms for detecting and repairing proxy use for these flexible notions of program decompositions.\nData and access requirements Our definitions and algorithms require (i) a specification of which attributes are protected, (ii) entail reasoning using data about these protected information types for individuals, and (iii) white box access to models and a representative dataset of inputs. Obtaining a complete specification of protected information types can be challenging when legal requirements and privacy expectations are vague regarding protected information types. However, in many cases, protected types are specified in laws and regulations governing the system under study (e.g., HIPAA, GDPR), and also stated in the data processor’s privacy policies.\nFurther, data about protected information types is often not explicitly collected. Pregnancy status, for example, would rarely find itself as an explicit feature in a purchases database (though it was the case in the Target case). Therefore, to discover unwanted proxy uses of protected information types, an auditor might need to first infer the protected attribute from the collected data to the best extent available to them. Though it may seem ethically ambiguous to perform a protected inference in order to (discover and) prevent protected inferences, it is consistent with the view that privacy is a function of both information and the purpose for which that information is being used [65]3. In our case, the inference and use of protected information by an auditor has a different (and ethically\n2Though deep learning models can be expressed in the example language presented in this paper, doing so would result in prohibitively large programs. 3This principle is exemplified by law in various jurisdictions including the PIPEDA Act in Canada [54], and the HIPAA Privacy Rule in the USA [55].\njustified) purpose than potential inferences in model being audited. Further, protected information has already been used by public and private entities in pursuit of social good: affirmative action requires the inference or explicit recording of minority membership, search engines need to infer suicide tendency in order to show suicide prevention information in their search results[60], health conditions can potentially be detected early from search logs of affected individuals [56]. Supported by law and perception of public good, we think it justified to expect system owners be cooperative in providing the necessary information or aiding in the necessary inference for auditing.\nFinally, in order to mitigate concerns over intellectual property due to access requirements for data and models, the analyst will need to be an internal auditor or trusted third party; existing privacy-compliance audits (Sen et al. [58]) that operate under similar requirements could be augmented with our methods.\nNormative judgments Appropriateness decisions by the analyst will be made in accordance with legal requirements and ethical norms. Operationally, this task might fall on privacy compliance teams. In large companies, such teams include law, ethics, and technology experts. Our work exposes the specific points where these complex decisions need to be made. In our evaluation, we observed largely human-interpretable witnesses for proxies. For more complex models, additional methods from interpretable machine learning might be necessary to make witnesses understandable.\nAnother normative judgment is the choice of acceptable ϵ,δ parameters. Similar to differential privacy, the choice of parameters requires identifying an appropriate balance between utility and privacy. Our quantitative theory could provide guidance to the oracle on how to prioritize efforts, e.g., by focusing on potentially blatant violations (high ϵ,δ values)."
    }, {
      "heading" : "9 CONCLUSION",
      "text" : "We develop a theory of use privacy in data-driven systems. Distinctively, our approach constrains not only the direct use of protected information types but also their proxies (i.e. strong predictors), unless allowed by exceptions justified by ethical considerations.\nWe formalize proxy use and present a program analysis technique for detecting it in a model. In contrast to prior work, our analysis is white-box. The additional level of access enables our detection algorithm to provide a witness that localizes the use to a part of the algorithm. Recognizing that not all instances of proxy use of a protected information type are inappropriate, our theory of use privacy makes use of a normative judgment oracle that makes this appropriateness determination for a given witness. If the proxy use is deemed inappropriate, our repair algorithm uses the witness to transform the model into one that does not exhibit proxy use. Using a corpus of social datasets, our evaluation shows that these algorithms are able to detect proxy use instances that would be difficult to find using existing techniques, and subsequently remove them while maintaining acceptable classification performance.\nAcknowledgments We would like to thank Amit Datta, Sophia Kovaleva, and Michael C. Tschantz for their thoughtful discussions throughout the development of this work. We thank our shepherd Aylin Caliskan and anonymous reviewers for their numerous suggestions that improved this paper.\nThis work was developed with the support of NSF grants CNS-1704845, CNS-1064688 as well as by DARPA and the Air Force Research Laboratory under agreement number FA8750-15-2-0277. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes not withstanding any copyright notation thereon. The views, opinions, and/or findings expressed are those of the author(s) and should not be interpreted as representing the official views or policies of DARPA, the Air Force Research Laboratory, the National Science Foundation, or the U.S. Government."
    }, {
      "heading" : "A PROOF OF THEOREM 1",
      "text" : "Theorem 1. No definition of proxy use can satisfy Properties 1-4 simultaneously.\nProof. Proof by contradiction. Assume that a definition of proxy use satisfies all four properties. Let X , Y , and Z be uniform binary random variables, such that Pr(Y = X ⊕ Z ) = 1, but X , Y and Z are pairwise independent. By (explicit use of proxy), the model \uD835\uDC9C(Y ,Z ) = Y ⊕ Z has proxy use of Z . By (dummy), the model \uD835\uDC9C′(Y ,Z ,X ) = Y ⊕Z has proxy use of Z . Choose f (x , z) = x ⊕ z. By our assumption earlier, Pr (Y = f (X ,Z )) = 1. Therefore, by (preprocessing), the model\uD835\uDC9C′′(Z ,X ) = \uD835\uDC9C′(f (X ,Z ),Z ,X ) has proxy use of Z . Note that \uD835\uDC9C′′(Z ,X ) = X ⊕ Z ⊕ Z = X . Therefore, by (dummy), \uD835\uDC9C′′′(X ) = X has proxy use of Z . But, by (independence), \uD835\uDC9C′′′ does not have proxy use of Z . Therefore, we have a contradiction. □\nThe key intuition behind this result is that Property 2 requires proxy use to be preserved when an input is replaced with a function that predicts that input via composition. However, with a purely semantic view of function composition, the causal effect of the proxy can disappear. The particular example of this observation we use in the proof is Y ⊕Z , where Z is the protected information type. This function has proxy use of Z . However, if X ⊕ Z is a perfect predictor for Y , then the example can be reduced to X ⊕Z ⊕Z = X , which has no proxy use of Z . To overcome this impossibility result, we choose a more syntactic notion of function composition, which is tied to how the function is represented as a program, and looks for evidence of proxy use within the representation."
    }, {
      "heading" : "B ALGORITHM FOR DETECTION",
      "text" : "In this section we provide technical details about the detection algorithm skipped from themain body of the paper. In particular, we formally define the decomposition used in the implementation, how machine learning models are translated to the term language, and how associational tests mitigate spurious results due to sampling.\nB.1 Decomposition Before we present the formal algorithm for detection, we need to develop notation for precisely denoting decompositions. Decomposition follows naturally from the subterm relation on expressions. However, as identical subterms can occur multiple times in an expression, care must be taken during substitution to distinguish between occurrences. For this reason we define substitution positionally, where the subterm of expression e = op(e1, . . . , en ) at\nAlgorithm 4 Detection for expression programs.\nRequire: association (d), influence(ι) measures procedure ProxyDetect(p,X,Z , ϵ,δ )\nP ← ∅ for each term e appearing in p do\np1 ← λx1, . . . ,xn .e Q ← {q | p |q = e} for each k ∈ [1, . . . , |Q |], (q1, . . . ,qk ) ∈ Q do\np2 ← λx1, . . . ,xn ,u .p[u]q1, ...,qk if ι(p1,p2) ≥ δ ∧ d(Jp1K(X),Z ) ≥ ϵ then\nP ← P ∪ {(p1,p2)} end if\nend for end for return P\nposition q, written e |q , is defined inductively:\nop(e1, . . . , en )|q =  op(e1, . . . , en ) if q = ϵ ei |q′ if q = iq′ ∧ 1 ≤ i ≤ n op(ei1 , . . . , eik ) if q = {i1, . . . , ik } ⊥ otherwise\nWe denote q as ‘positional indicator’. Specifically, q has the syntax of the following. ⟨q⟩ ::= ϵ | i⟨q⟩ | {i1, . . . , ik } We then define the term obtained by substituting s in e at position q, written e[s]q , to be the term where e[s]q |q = s , and e[s]q |q′ = eq′ for all q′ that are not prefixed by q. For a sequence of positions q1, . . . ,qn and terms s1, . . . , sn , we write e[s1, . . . , sn ]q1, ...,qn to denote the sequential replacement obtained in order from 1 to n. Given a program p = λ®x .e , we will often write p |q or p[s]q for brevity to refer to e |q and e[s]q , respectively. The set of decompositions of a program p is then defined by the set of positions q such that p |q ,⊥. Given position q, the corresponding decomposition is simply (λ®x .p |q ,u, λ®x ,u .p[u]q ).\nExample B.1. Consider a simple model,\np = λx ,y.ite(x + y ≤ 0, 1, 0) = λx ,y.ite(≤ (+(x ,y), 0), 1, 0)\nThere are eight positions in the body expression, namely {ϵ, 1, 2, 3, 11, 12, 111, 112}. The subexpression at position 112 isy, andp[u]11 = ite(u ≤ 0, 1, 0). This corresponds to the decomposition:\n(λx ,y.x + y,u, λx ,y,u .ite(u ≤ 0, 1, 0))\nWith this notation in place, we can formally describe the detection algorithm in Algorithm 4.\nB.2 Translation This section describes the translation of machine learning models used in our implementation to the term language.\nB.2.1 Decision trees and Rule lists. Decision trees can be written in this language as nested ite terms, as shown in Figure 7. The Boolean expression in each term corresponds to a guard, and the arithmetic expressions to either a proper subtree or a leaf. Bayesian\nrule lists are a special kinds of decision trees, where the left subtree is always a leaf.\nB.2.2 Linear models. Linear regression models are expressed by direct translation into an arithmetic term, and linear classification models (e.g., logistic regression, linear support vector machines, Naive Bayes) are expressed as a single ite term, i.e.,\nsgn( ®w · ®x + b) becomes λ®x.ite( ®w · ®x + b ≥ 0, 1, 0)\nImportantly, the language supports n-ary operations when they are associative, and allows for rearranging operands according to associative and distributive equivalences. In other words, the language computes on terms modulo an equational theory. Without allowing such rearrangement, when a linear model is expressed using binary operators, such as ((((w1×x1)+ (w2×x2))+ (w3×x3)), then the algorithm cannot select the decomposition:\np1 = λ®x.(w1 × x1) + (w3 × x3) p2 = λ®x,u .u + (w2 × x2)\nB.2.3 Decision Forests. Decision forests are linear models where each linear term is a decision tree. We combine the two translations described above to obtain the term language representation for decision forests.\nB.3 Validity Testing We use mutual information to determine the strength of the statistical association between Jp1K(X) and Z . Each test of this metric against the threshold ϵ amounts to a hypothesis test against a null hypothesis which assumes that d(Jp1K(X),Z ) < ϵ . Because we potentially take this measure for each valid decomposition of p, it amounts to many simultaneous hypothesis tests from the same data source. To manage the likelihood of encountering false positives, we employ commonly-used statistical techniques. The first approach that we use is cross-validation. We partition the primary dataset n times into training and validation sets, run Algorithm 4 on each training set, and confirm the reported proxy uses on the corresponding validation set. We only accept reported uses that appear at least t times as valid.\nThe second approach uses bootstrap testing to compute a pvalue for each estimate d̂(p1(X),Z ), and applying Bonferroni correction [20] to account for the number of simultaneous hypothesis tests. Specifically, the bootstrap test that we apply takes n samples of (X,Z ), [(X̂i , Ẑi )]1≤i≤n , and permutes each X̂i , Ẑi to account for the null hypothesis thatX and Z are independent. We then estimate\nAlgorithm 5Witness-driven repair.\nRequire: association (d), influence (ι), utility (v) measures, oracle (\uD835\uDCAA) procedure Repair(p,X,Z , ϵ,δ )\nP ← {d ∈ ProxyDetect(p,X,Z , ϵ,δ ) : not \uD835\uDCAA(d)} if P , ∅ then\n(p1,p2) ← element of P p′ ← ProxyRepair(p, (p1,p2),X,Z , ϵ,δ ) return Repair(p′,X,Z , ϵ,δ )\nelse return p end if\nAlgorithm 6 Local Repair.\nRequire: association (d), influence (ι), utility (v) measures 1: procedure ProxyRepair(p, (p1,p2),X,Z , ϵ,δ ) 2: R ← {} 3: for each decomp. (p′1,p ′ 2) w/ p ′ 1 local to p1 in p2 do\n4: r∗ ← argmaxr v ( [u/r ]p′2 ) 5: (p′′1 ,p ′′ 2 ) ← (p1,p2) with r\n∗ substituted for p′1 6: if ι(p′′1 ,p ′′ 2 ) ≤ δ ∨ d(Jp′′1 K(X),Z ) ≤ ϵ then\n7: p∗ ← [u/r∗]p′2 8: R ← R ∪ {p∗} 9: end if 10: end for 11: return argmaxp∗∈R v (p∗)\nthe p-value by computing:\np = 1 n ∑ 1≤i≤n 1(d(X̂i , Ẑi ) < d(Jp1K(X),Z ))\nAfter correction, we can bound the false positive discovery rate by only accepting instances that yield p ≤ α , for sufficiently small α . We note, however, that this approach is only correct when the association strength ϵ = 1, as the null hypothesis in this test assumes that Jp1K is independent of Z . To use this approach in general, we would need to sample [(X̂i , Ẑi )]1≤i≤n under the assumption that d(X̂i , Ẑi ) ≥ ϵ . We leave this detail to future work."
    }, {
      "heading" : "C ALGORITHMS FOR REPAIR",
      "text" : "We now provide a formal description of the repair algorithms informally described in the paper. Algorithm 5, and 6 correspond to 2, and 3 respectively.\nC.1 Optimal constant selection As constant terms cannot be examples of (ϵ,δ )-Proxy Use, there is freedom in their selections as replacements for implicated subprograms. In Algorithm 6 we pick the replacement that optimizes some measure of utility of the patched program. If the given program was constructed as a classifier, we define utility as the patched program’s prediction accuracy on the data set using 0-1 loss. Similarly, if the program were a regression model, v would correspond to mean-squared error.\nIf the program computes a continuous convex function, as in the case of most commonly-used regression models, then off-the-shelf convex optimization procedures can be used in this step. However, because we do not place restrictions on the functions computed by programs submitted for repair, the objective function might not satisfy the conditions necessary for efficient optimization. In these cases, it might be necessary to develop a specialized procedure for the model class. Below we describe such a procedure for the case of decision trees.\nDecision trees Decision trees are typically used for classification of instances into a small number of classesC . For these models, the only replacement constants that will provide reasonable accuracy are those that belong toC , so in the worst case, the selection procedure must only consider a small finite set of candidates. However, it is possible to calculate the optimal constant with a single pass through the dataset.\nGiven a decomposition (p1,p2) of p, let ϕ be the weakest formula over p’s variables such that ∀®x .p1(®x) = p(®x). ϕ corresponds to the conjoined conditions on the path in p prefixing p1. We can then define the objective function:\nv(r ) = ∑ ®x ∈ ®X 1(ϕ(®x) → ®xc = r )\nThis objective is minimized when r matches the greatest number of class labels for samples that pass through p1. This minimizes classification error over ®X , and is easily computed by taking the class-label mode of training samples that satisfy ϕ.\nExample C.1. Consider the tree in Figure 7, and assume that x1 and x2 are distributed according to\uD835\uDCA9 ( 12 , 1), and x3 = x1 + x2. For simplicity, assume that the class label for each instance is given exactly by the tree. Then given the decomposition:\np1 = λ®x .ite(x3 ≤ 0, 0, 1) p2 = λ®x ,u .ite(x1 ≤ 1/2, 0, ite(x2 ≤ 1,u, 0))\nwe need to find an optimal constant to replace the subtree rooted at x3. In this case, ϕ def = x1 > 1 2 ∧ x2 ≤ 1, so we select ®Xϕ = {®x ∈ ®X |x1 > 12 ∧ x2 ≤ 1} and take the mode of the empirical sample [p(®x)]®x ∈ ®Xϕ ."
    }, {
      "heading" : "D OTHER EXPERIMENTS D.1 Details of Case Studies",
      "text" : "Targeted contraception advertising We consider a scenario in which a data processor wishes to show targeted advertisements for contraceptives to females. To support this goal, the processor collects a dataset from a randomly-sampled female population containing age, level of education, number of children, current employment status, income, level of media exposure, information about the partner’s education and occupation, and the type of contraception used by the individual (if any). This dataset is used to train a model that predicts whether an individual uses no contraception, short-term contraception, or long-term contraception. This model is then used to determine who to display advertisements to, under the assumption that individuals who already use short-term contraception are more likely to be receptive to the advertisements.\nBecause certain religions ban the use of contraception, users belonging to such a religion are on the whole less likely to purchase contraceptives after seeing such an advertisement. The ad-targeting model does not explicitly use a feature corresponding to religion, as this information is not available to the system when ads are displayed. Furthermore, some users may view the use of this information for advertising purposes as a violation of their privacy, so the data processor would like to ensure that the targeting model has not inferred a proxy for this information that is influential in determining whether to show an advertisement.\nWe evaluated this scenario using data collected for the 1987 National Indonesia Contraceptive Survey [1], which contains the features mentioned above, as well as a feature indicating whether the individual’s religious beliefs were Islam. To simulate the data processor, we trained a decision tree classifier to predict contraceptive use over all available features except the one corresponding to religion. We then used our detection algorithm to look for a proxy use of religion, using the available data as ground truth to evaluate the effectiveness of our approach.\nAlthough this data is representative of a single country, it illustrates an interesting case of potential use privacy. Our detection algorithm identified the following intermediate computation, which was one of the most influential in the entire model, and the one most closely associated with the religion variable: ite(educ < 4∧nchild ≤ 3∧age < 31, no, yes). This term predicts that women younger than 31, with below-average education background and fewer than four children will not use contraception. Given that the dataset is comprised entirely of females, closer examination in fact reveals that just the “guard” term educ < 4 alone is even more closely associated with religion, and its influence on the model’s output is nearly as high. This reveals that themodel is using the variable for education background as a proxy for religion, which may be concerning given that this application is focused on advertising.\nStudent assistance A current trend in education is the use of predictive analytics to identify students who are likely to benefit from certain types of interventions to ensure on-time graduation and other benchmark goals [31, 39]. We look at a scenario where a data processor builds a model to predict whether a secondary school student’s grades are likely to suffer in the near future, based on a range of demographic features (such as age, gender, and family characteristics), social information (such as involvement in extracurricular activities, amount of reported free time after school), and academic information (e.g., number of reported absences, use of paid tutoring services, intention to continue on to higher education). Based on the outcome of this prediction, the student’s academic advisor can decide whether to pursue additional interventions.\nBecause of the wide-ranging nature of the model’s input features, and sensitivity towards the privacy rights of minors, the data processor would like to ensure that the model does not base its decision on inferred facts about certain types of activities that the student might be involved with. For example, alcohol consumption may be correlated with several of the features used by the model, and it may not be seen as appropriate to impose an intervention on a student because their profile suggests that they may have engaged in this activity. Depending on the context in which such\nan inference were made, the processor would view this as a privacy violation, and attempt to remove it from the model.\nTo evaluate this scenario, we trained a model on the UCI Student Alcohol Consumption dataset [11]. This data contains approximately 700 records collected from Portuguese public school students, and includes features corresponding to the variables mentioned above. Our algorithm found the following proxy for alcohol use: ite(studytime < 2 ∧ dad_educ < 4, fail, ...), which predicts that a student who spends at most five hours per week studying, and whose father’s level of education is below average, is likely to fail a class in at least one term. Further investigation reveals that studytime < 2 was more influential on the model’s output, and nearly as associated with alcohol consumption, as the larger term. This finding suggests that this instance of proxy use can be deemed an appropriate use, and not a privacy violation, as the amount of time a student spends studying is clearly relevant to their academic performance. If instead dad_educ < 4 alone had turned out to be a proxy use of alcohol consumption, then it may have been a concerning inference about the student’s behavior from information about their family history. Our algorithm correctly identified that this is not the case.\nCredit advertisements We consider a situation where a credit card company wishes to send targeted advertisements for credit cards based on demographic information. In this context, the use of health status for targeted advertising is a legitimate privacy concern [18].\nTo evaluate this scenario, we trained a model to predict interest in credit cards using the PSID dataset, which contains detailed\ndemographic information for roughly 10,000 families and includes features such as age, employment status income, education, and the number of children. From this, we trained two models: one that identifies individuals with student loans and another that identifies individuals with existing credit cards as the two groups to be targeted.\nThe first model had a number of instances of proxy use. One particular subcomputation that was concerning was a subtree of the original decision tree that branched on the number of children in the family. This instance provided negative outcomes to individuals withmore children, andmay be deemed inappropriate for use in this context. In the second model, one proxy was a condition involving income income ≤ 33315. The use of income in this context is justifiable, and therefore this may not be regarded as a use privacy violation.\nD.2 Algorithm Runtime vs. Model Size Figure 9 demonstrates the runtime of the detection algorithm as a function of the number of decompositions of the analyzed model (a proxy for its size). We show two trends in that figure. The black line demonstrates the worst case detection that requires both association and influence computation for each decomposition while the gray line demonstrates the best case where only the association computation is performed. The runtime in practice would thus fall somewhere between these two cases, both linear in the number of decompositions."
    }, {
      "heading" : "E COMPLEXITY",
      "text" : "The complexity of the presented algorithms depend on several factors, including the type of model being analyzed, the number of elements in the ranges of sub-programs, and reachability of subprograms by dataset instances . In this section we describe the the complexity characteristics of the detection and repair algorithms under various assumptions. Complexity is largely a property of the association and influence computations and the number of decompositions of the analyzed program. We begin by noting our handling of probability distributions as specified by datasets, several quantities of interest, discuss the complexity of components of our algorithms, and conclude with overall complexity bounds.\nE.1 Distributions, datasets, and probability It is rarely the case that one has access to the precise distribution from which data is drawn. Instead, a finite sample must be used as a surrogate when reasoning about random variables. In our formalism we wrote X\n$← \uD835\uDCAB to designate sampling of a value from a population. Given a dataset surrogate \uD835\uDC9F, this operation is implemented as an enumeration x ∈ \uD835\uDC9F, with each element having probability 1/|\uD835\uDC9F |. We will overload the notation and use \uD835\uDC9F also as the random variable distributed in the manner just described. We assume here that the sensitive attribute Z is a part of the random variable X .\nThe following sections use the following quantities to express complexity bounds, mostly overloading prior notations:\n• \uD835\uDC9F - The number of instances in the population dataset. • p - The number of expressions in a programp being analyzed.\n• Z - The number of elements in the support of Z . • k - The maximum number of unique elements in support of every sub-expression, that is maxp′∈p |support (Jp′K\uD835\uDC9F)|. • c - The number of decompositions in a given program. We will elaborate on this quantity under several circumstances later in this section. • b - The minimum branching factor of sub-expressions in a given program. We will assume that the number of syntactic copies of any subexpression in a program is no more than some constant. This means we will ignore the asymptotic effect of decompositions with multiple copies of the same sub-program p1.\nThe elementary operation in our algorithms is a lookup of a probability of a value according to some random variable. We precompute several probabilities related to reachability and contingency tables to aid in this operation. When we write “p1 is reached”, we mean that the evaluation of p, containing p1, on a given instance X, will reach the sub-expression p1 (or that p1 needs to be evaluated to evaluate p on X).\nProbability pre-computation For every decomposition Jp2K (X , Jp1KX ) = JpK (X ), we compute:\n(1) the r.v. (Jp1KX,XZ ) for X $← \uD835\uDC9F, (2) the r.v. X| (p1 is reached by X) for X $← \uD835\uDC9F, and (3) the value PrX $←\uD835\uDC9F (p1 is reached by X).\nIn point (1) abovewewriteXZ to designate the sensitive attribute component of X, hence this point computes the r.v. representing\nthe output of p1 along with the sensitive attribute Z . This will be used for the association computation.\nThe complexity of these probability computations varies depending on circumstances. In the worst case, the complexity is \uD835\uDCAA (c\uD835\uDC9Fp). However, under some assumptions related to programs p and datasets \uD835\uDC9F, these bounds can be improved. We define two types of special cases which we call splitting and balanced:\nDefinition E.1. p is splitting for \uD835\uDC9F iff it has at most a constant number of reachable op operands (arguments of op expressions).\nThe Decision trees are local for any dataset as they do not contain any op operands (they do contain relop operands). Further, if number of trees in random forests or number of coefficients in linear regression are held constant, then these models too are splitting for any dataset. The reasoning behind this definition is to prohibit arbitrarily large programs that do not split inputs using if-then-else expressions. It is possible to create such programs using arithmetic and boolean operations, but not using purely relational operations.\nDefinition E.2. p is b-balanced for\uD835\uDC9F iff all but a constant number of sub-expressions e ′ have parent e with b > 1 sub-expressions which split the instances that reach them approximately equally among their children.\nBalanced implies splitting as op operands do not satisfy the balanced split property hence there has to be only a constant number of them. Also, the definition is more general than necessary for the language presented in this paper where the branching factor is always 2 because the if-then-else expressions are the only ones that can satisfy the balanced split condition. Decision trees trained using sensible algorithms are usually balanced due to the branch split criteria employed preferring approximately equal splits of training instances. For the same reason, if the number of trees are held constant, then random forests are also likely to be balanced.\nWhen p is splitting for \uD835\uDC9F, the probability computation step reduces to \uD835\uDCAA ( \uD835\uDC9Fp2 ) . This stems from the fact that the number of decompositions is asymptotically equal to the number of subexpressions (limits to operands prevent more decompositions). Further, if p is b-balanced for \uD835\uDC9F, the probability pre-computation reduces to \uD835\uDCAA ( \uD835\uDC9F logb \uD835\uDC9F ) . In the language presented b = 2. These bounds derive similarly to the typical divide and conquer program analysis; there are logb \uD835\uDC9F layers of computation, each processing \uD835\uDC9F instances.\nE.2 Influence and Association Our proxy definition further relies on two primary quantities used in Algorithm 1, influence and association. We describe the methods we use to compute them here. Quantitative decomposition influence Given a decomposition (p1,u,p2) of p, the influence of p1 on p2’s output is defined as:\nι(p1,p2) def = E\nX ,X ′ $←\uD835\uDC9F\n[ Pr ( Jp2K (X, Jp1KX) , Jp2K ( X, Jp1KX′ ) ) ] This quantity requires \uD835\uDC9F2 samples to compute in general. Each\nsample takes at most\uD835\uDCAA (p) time, for a total of\uD835\uDCAA ( p\uD835\uDC9F2 ) . However, we can take advantage of the pre-computations described in the prior section along with balanced reachability criteria and limited ranges\nof values in expression outputs to do better. We break down the definition of influence into two components based on reachability of p1:\nι(p1,p2) def = E\nX,X′ $←\uD835\uDC9F\n[ Pr ( Jp2K (X, Jp1KX) , Jp2K ( X, Jp1KX′ ) ) ] = E\nX $←\uD835\uDC9F\n[ E\nX′ $←\uD835\uDC9F\n[ Pr ( Jp2K (X, Jp1KX) , Jp2K ( X, Jp1KX′ ) ) ] ] = Pr (p1 not reached) · E\nX $←\uD835\uDC9F |p1 not reached\n[· · ·]\n+ Pr (p1 reached) · E X $←\uD835\uDC9F |p1 reached [· · ·]\n= 0 + Pr (p1 reached) ·\nE X $←\uD835\uDC9F |p1 reached\n[ E\nX $←\uD835\uDC9F\n[ Pr ( JpK(X) , Jp2K ( X, Jp1KX′ ) ) ] ] = Pr (p1 reached) ·\nE X $←\uD835\uDC9F |p1 reached\n[ E\nY $←Jp1K\uD835\uDC9F\n[Pr (JpK(X) , Jp2K (X,Y))] ]\nNote that all both random variables and one probability value in the final form of influence above have been pre-computed. Further, if the number of elements in the support of Jp1KX is bounded by k , we compute influence using k\uD835\uDC9F samples (at most \uD835\uDC9F for X and at most k for Y), for total time of \uD835\uDCAA (kp\uD835\uDC9F).\nInfluence can also be estimated, ι̂ by taking a sample from\uD835\uDC9F×\uD835\uDC9F. By Hoeffding’s inequality [41], we select the subsample size n to be at least log(2/β)/2α2 to ensure that the probability of the error ι̂(p1,p2) − ι(p1,p2) being greater than β is bounded by α .\nAssociation As discussed in Section 3, we use mutual information to measure the association between the output of a subprogram and Z . In our pre-computation steps we have already constructed the r.v. (Jp1KX,XZ ) for X\n$← \uD835\uDC9F. This joint r.v. contains both the subprogram outputs and the sensitive attribute hence it is sufficient to compute association metrics. In case of normalized mutual information, this can be done in time \uD835\uDCAA (kZ ), linear in the size of the support of this random variable.\nE.3 Decompositions The number of decompositions of amodel determines the number of proxies that need to be checked in detection and repair algorithms. We consider two cases, splitting and non-splitting programs. For splitting models, the number of decompositions is bounded by the size of the program analyzed, whereas in case of non-splitting models, the number of decompositions can be exponential in the size of the model. These quantities are summarized in Table 2.\nE.4 Detection The detection algorithm can be written \uD835\uDCAA (A + B ·C), a combination of three components. A is probability pre-computation as described earlier in this section, B is the complexity of association and influence computations, and C is the number of decompositions.\nThe complexity in terms of the number of decompositions under various conditions is summarized in Table 3. Instantiating the parameters, the overall complexity ranges from \uD835\uDCAA ( \uD835\uDC9F logb \uD835\uDC9F + p2\uD835\uDC9F )\nin case of models like balanced decision trees with a constant number of classes, to \uD835\uDCAA ( p2p\uD835\uDC9F2 ) in models with many values and associative expressions like linear regression. If the model size is held constant, these run-times become \uD835\uDCAA ( \uD835\uDC9F logb \uD835\uDC9F ) and \uD835\uDCAA ( \uD835\uDC9F2 ) , respectively."
    } ],
    "references" : [ {
      "title" : "Adscape: Harvesting and Analyzing Online Display Ads",
      "author" : [ "Paul Barford", "Igor Canadi", "Darja Krushevskaja", "Qiang Ma", "S. Muthukrishnan" ],
      "venue" : "In Proceedings of the 23rd International Conference on World Wide Web. International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "Private Empirical Risk Minimization: Efficient Algorithms and Tight Error Bounds",
      "author" : [ "Raef Bassily", "Adam Smith", "Abhradeep Thakurta" ],
      "venue" : "In 55th IEEE Annual Symposium on Foundations of Computer Science,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "Forecasts of Violence to Inform Sentencing Decisions",
      "author" : [ "Richard Berk", "Justin Bleich" ],
      "venue" : "Journal of Quantitative Criminology 30,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2014
    }, {
      "title" : "Forecasting Domestic Violence: A Machine Learning Approach to Help Inform Arraignment Decisions",
      "author" : [ "Richard A. Berk", "Susan B. Sorenson", "Geoffrey Barnes" ],
      "venue" : "Journal of Empirical Legal Studies 13,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2016
    }, {
      "title" : "Three naive Bayes approaches for discrimination-free classification",
      "author" : [ "Toon Calders", "Sicco Verwer" ],
      "venue" : "Data Mining and Knowledge Discovery 21,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2010
    }, {
      "title" : "Differentially Private Empirical Risk Minimization",
      "author" : [ "Kamalika Chaudhuri", "Claire Monteleoni", "Anand D. Sarwate" ],
      "venue" : "Journal of Machine Learning Research",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "A Decision Theoretic Approach to Targeted Advertising",
      "author" : [ "David Maxwell Chickering", "David Heckerman" ],
      "venue" : "In Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2000
    }, {
      "title" : "Using data mining to predict secondary school student performance",
      "author" : [ "Paulo Cortez", "Alice Maria Goncalves Silva" ],
      "venue" : "Technical Report, Department of Computer Science,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2008
    }, {
      "title" : "Elements of information theory",
      "author" : [ "Thomas M Cover", "Joy A Thomas" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2012
    }, {
      "title" : "Use Privacy in Data-Driven Systems: Theory and Experiments with Machine Learnt Programs",
      "author" : [ "Anupam Datta", "Matthew Fredrikson", "Gihyuk Ko", "Piotr Mardziel", "Shayak Sen" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2017
    }, {
      "title" : "Algorithmic Transparency via Quantitative Input Influence: Theory and Experiments with Learning Systems",
      "author" : [ "Anupam Datta", "Shayak Sen", "Yair Zick" ],
      "venue" : "In Proceedings of IEEE Symposium on Security & Privacy",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2016
    }, {
      "title" : "Automated Experiments on Ad Privacy Settings: A Tale of Opacity, Choice, and Discrimination",
      "author" : [ "A. Datta", "M.C. Tschantz" ],
      "venue" : "In Proceedings on Privacy Enhancing Technologies (PoPETs",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2015
    }, {
      "title" : "Automated Experiments on Ad Privacy Settings: A Tale of Opacity, Choice, and Discrimination",
      "author" : [ "Amit Datta", "Michael Carl Tschantz", "Anupam Datta" ],
      "venue" : "In Proceedings on Privacy Enhancing Technologies (PoPETs). De Gruyter Open",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2015
    }, {
      "title" : "FTC’s Julie Brill Tells Ad Tech Companies To Improve Privacy Protections",
      "author" : [ "Wendy Davis" ],
      "venue" : "http://www.mediapost.com/publications/article/ 259210/ftcs-julie-brill-tells-ad-tech-companies-to-impro.html Accessed Nov",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2016
    }, {
      "title" : "The Scoring of America: How Secret Consumer Scores Threaten Your Privacy and Your Future",
      "author" : [ "Pam Dixon", "Robert Gellman" ],
      "venue" : "http://www.worldprivacyforum.org/wp-content/uploads/2014/04/WPF-",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2014
    }, {
      "title" : "How Companies Learn Your Secrets",
      "author" : [ "Charles Duhigg" ],
      "venue" : "http:// www.nytimes.com/2012/02/19/magazine/shopping-habits.html (Accessed Aug",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2012
    }, {
      "title" : "Estimation of the Medians for Dependent Variables",
      "author" : [ "Olive Jean Dunn" ],
      "venue" : "The Annals of Mathematical Statistics 30,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1959
    }, {
      "title" : "Differential Privacy. In Automata, Languages and Programming, 33rd International Colloquium, ICALP 2006, Venice, Italy",
      "author" : [ "Cynthia Dwork" ],
      "venue" : "July 10-14,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2006
    }, {
      "title" : "Fairness Through Awareness",
      "author" : [ "Cynthia Dwork", "Moritz Hardt", "Toniann Pitassi", "Omer Reingold", "Richard Zemel" ],
      "venue" : "In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference (ITCS ’12)",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2012
    }, {
      "title" : "Fairness Through Awareness",
      "author" : [ "C. Dwork", "M. Hardt", "T. Pitassi", "O. Reingold", "R. Zemel" ],
      "venue" : "In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference (ITCS",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2012
    }, {
      "title" : "Calibrating Noise to Sensitivity in Private Data Analysis",
      "author" : [ "Cynthia Dwork", "Frank Mcsherry", "Kobbi Nissim", "Adam Smith" ],
      "venue" : "In Theory of Cryptography",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2006
    }, {
      "title" : "Calibrating Noise to Sensitivity in Private Data Analysis",
      "author" : [ "Cynthia Dwork", "Frank McSherry", "Kobbi Nissim", "Adam Smith" ],
      "venue" : "In TCC",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2006
    }, {
      "title" : "Web Privacy Measurement: Scientific principles, engineering platform, and new results. Manuscript posted at http: //randomwalker.info/publications/WebPrivacyMeasurement.pdf",
      "author" : [ "Steven Englehardt", "Christian Eubank", "Peter Zimmerman", "Dillon Reisman", "Arvind Narayanan" ],
      "venue" : null,
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2014
    }, {
      "title" : "Certifying and Removing Disparate Impact",
      "author" : [ "Michael Feldman", "Sorelle A. Friedler", "John Moeller", "Carlos Scheidegger", "Suresh Venkatasubramanian" ],
      "venue" : "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD ’15)",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2015
    }, {
      "title" : "Certifying and Removing Disparate Impact",
      "author" : [ "Michael Feldman", "Sorelle A. Friedler", "John Moeller", "Carlos Scheidegger", "Suresh Venkatasubramanian" ],
      "venue" : "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2015
    }, {
      "title" : "How Big Data is helping students graduate on time",
      "author" : [ "Nicole Freeling" ],
      "venue" : "https://www.universityofcalifornia.edu/news/how-big-data-helpingstudents-graduate-time (Accessed Nov",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2016
    }, {
      "title" : "Predictive Modeling Applications in Actuarial Science",
      "author" : [ "Edward W. Frees", "Richard A. Derrig", "Glenn Meyers" ],
      "venue" : null,
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2014
    }, {
      "title" : "Policy auditing over incomplete logs: theory, implementation and applications",
      "author" : [ "Deepak Garg", "Limin Jia", "Anupam Datta" ],
      "venue" : "In Proceedings of the ACM Conference on Computer and Communications Security (CCS)",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2011
    }, {
      "title" : "A Comparative Analysis of Decision Trees Vis-a-vis Other Computational Data Mining Techniques in Automotive Insurance Fraud Detection",
      "author" : [ "Adrian Gepp", "J. Holton Wilson", "Kuldeep Kumar", "Sukanto Bhattacharya" ],
      "venue" : "Journal of Data Science 10,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2012
    }, {
      "title" : "Challenges in Measuring Online Advertising Systems",
      "author" : [ "Saikat Guha", "Bin Cheng", "Paul Francis" ],
      "venue" : "In Proceedings of the 10th ACM SIGCOMM Conference on Internet Measurement (IMC ’10)",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2010
    }, {
      "title" : "Measuring Personalization of Web Search",
      "author" : [ "Aniko Hannak", "Piotr Sapiezynski", "Arash Molavi Kakhki", "Balachander Krishnamurthy", "David Lazer", "Alan Mislove", "Christo Wilson" ],
      "venue" : "In Proceedings of the 22nd International Conference on World Wide Web (WWW ’13)",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2013
    }, {
      "title" : "Measuring Price Discrimination and Steering on E-commerce Web Sites",
      "author" : [ "Aniko Hannak", "Gary Soeller", "David Lazer", "Alan Mislove", "Christo Wilson" ],
      "venue" : "In Proceedings of the 2014 Conference on Internet Measurement Conference (IMC ’14)",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2014
    }, {
      "title" : "Manual For the Revised Psychopathy Checklist",
      "author" : [ "Robert Hare" ],
      "venue" : "Multi-Health Systems",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2003
    }, {
      "title" : "The Future of Big Data and Analytics in K-12 Education",
      "author" : [ "Benjamin Harold" ],
      "venue" : null,
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2016
    }, {
      "title" : "Every Step You Fake: A Comparative Analysis of Fitness Tracker",
      "author" : [ "Andrew Hilts", "Christopher Parsons", "Jeffrey Knockel" ],
      "venue" : "Privacy and Security",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2016
    }, {
      "title" : "Fairness-aware learning through regularization approach",
      "author" : [ "Toshihiro Kamishima", "Shotaro Akaho", "Jun Sakuma" ],
      "venue" : "In Proceedings of the Workshop on Privacy Aspects of Data Mining",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2011
    }, {
      "title" : "XRay: Enhancing the Web’s Transparency with Differential Correlation",
      "author" : [ "Mathias Lécuyer", "Guillaume Ducoffe", "Francis Lan", "Andrei Papancea", "Theofilos Petsios", "Riley Spahn", "Augustin Chaintreau", "Roxana Geambasu" ],
      "venue" : "In Proceedings of the 23rd USENIX Conference on Security Symposium (SEC’14)",
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 2014
    }, {
      "title" : "Sunlight: Fine-grained Targeting Detection at Scale with Statistical Confidence",
      "author" : [ "Mathias Lecuyer", "Riley Spahn", "Yannis Spiliopolous", "Augustin Chaintreau", "Roxana Geambasu", "Daniel Hsu" ],
      "venue" : "In Proceedings of the 22Nd ACM SIGSAC Conference on Computer and Communications Security (CCS ’15)",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 2015
    }, {
      "title" : "Interpretable classifiers using rules and Bayesian analysis: Building a better  stroke prediction model",
      "author" : [ "Benjamin Letham", "Cynthia Rudin", "Tyler H. McCormick", "David Madigan" ],
      "venue" : "Ann. Appl. Stat. 9,",
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 2015
    }, {
      "title" : "Making Public Information Secret",
      "author" : [ "Richard J. Lipton", "Kenneth W. Regan" ],
      "venue" : "https://rjlipton.wordpress.com/2016/05/20/making-public-informationsecret/ Accessed Aug",
      "citeRegEx" : "46",
      "shortCiteRegEx" : "46",
      "year" : 2016
    }, {
      "title" : "Dependence Makes You Vulnerable: Differential Privacy Under Dependent Tuples",
      "author" : [ "Changchang Liu", "Supriyo Chakraborty", "Prateek Mittal" ],
      "venue" : "In Network and Distributed System Security Symposium (NDSS). The Internet Society",
      "citeRegEx" : "47",
      "shortCiteRegEx" : "47",
      "year" : 2016
    }, {
      "title" : "k-NN As an Implementation of Situation Testing for Discrimination Discovery and Prevention",
      "author" : [ "Binh Thanh Luong", "Salvatore Ruggieri", "Franco Turini" ],
      "venue" : "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)",
      "citeRegEx" : "48",
      "shortCiteRegEx" : "48",
      "year" : 2011
    }, {
      "title" : "The Dark Side of Wearables",
      "author" : [ "Teena Maddox" ],
      "venue" : "http: //www.techrepublic.com/article/the-dark-side-of-wearables-how-theyresecretly-jeopardizing-your-security-and-privacy/ Accessed Nov",
      "citeRegEx" : "49",
      "shortCiteRegEx" : "49",
      "year" : 2016
    }, {
      "title" : "An Analysis of Us Household Socioeconomic Profiles Based on Marital Status and Gender",
      "author" : [ "Sumaria Mohan-Neill", "Indira Neill Hoch", "Meng li" ],
      "venue" : "Journal of Economics and Economic Education Research",
      "citeRegEx" : "50",
      "shortCiteRegEx" : "50",
      "year" : 2014
    }, {
      "title" : "Privacy in Context: Technology, Policy, and the Integrity of Social Life",
      "author" : [ "Helen Nissenbaum" ],
      "venue" : null,
      "citeRegEx" : "51",
      "shortCiteRegEx" : "51",
      "year" : 2009
    }, {
      "title" : "A Value for n-Person Games. In Quantified: Biosensing Technologies in Everyday Life",
      "author" : [ "Helen Nissenbaum", "Heather Patterson" ],
      "venue" : null,
      "citeRegEx" : "52",
      "shortCiteRegEx" : "52",
      "year" : 2016
    }, {
      "title" : "Screening for Pancreatic Adenocarcinoma Using Signals From Web Search Logs: Feasibility Study and Results",
      "author" : [ "John Paparrizos", "Ryen W. White", "Eric Horvitz" ],
      "venue" : "Journal of Oncology Practice 12,",
      "citeRegEx" : "56",
      "shortCiteRegEx" : "56",
      "year" : 2016
    }, {
      "title" : "The Black Box Society: The Secret Algorithms That Control Money and Information",
      "author" : [ "Frank Pasquale" ],
      "venue" : null,
      "citeRegEx" : "57",
      "shortCiteRegEx" : "57",
      "year" : 2015
    }, {
      "title" : "Bootstrapping Privacy Compliance in Big Data Systems",
      "author" : [ "Shayak Sen", "Saikat Guha", "Anupam Datta", "Sriram K. Rajamani", "Janice Tsai", "Jeannette M. Wing" ],
      "venue" : "In Proceedings of the 2014 IEEE Symposium on Security and Privacy (SP ’14)",
      "citeRegEx" : "58",
      "shortCiteRegEx" : "58",
      "year" : 2014
    }, {
      "title" : "Recent Developments in Quantitative Information Flow (Invited Tutorial)",
      "author" : [ "Geoffrey Smith" ],
      "venue" : "In Proceedings of the 2015 30th Annual ACM/IEEE Symposium on Logic in Computer Science (LICS) (LICS ’15)",
      "citeRegEx" : "59",
      "shortCiteRegEx" : "59",
      "year" : 2015
    }, {
      "title" : "How do search engines respond when you Google ‘suicide’",
      "author" : [ "S.E. Smith" ],
      "venue" : "https://www.dailydot.com/via/germanwings-suicide-hotline/ Accessed May",
      "citeRegEx" : "60",
      "shortCiteRegEx" : "60",
      "year" : 2015
    }, {
      "title" : "A Taxonomy of Privacy",
      "author" : [ "Daniel J. Solove" ],
      "venue" : "University of Pennsylvania Law Review 154,",
      "citeRegEx" : "61",
      "shortCiteRegEx" : "61",
      "year" : 2006
    }, {
      "title" : "Privacy Policy Guidance Memorandum: The Fair Information Practice Principles: Framework for Privacy Policy at the Department of Homeland Security. Memorandum Number: 2008-01",
      "author" : [ "Hugo Teufel III" ],
      "venue" : null,
      "citeRegEx" : "62",
      "shortCiteRegEx" : "62",
      "year" : 2008
    }, {
      "title" : "Discovering Unwarranted Associations in Data-Driven Applications with the FairTest Testing Toolkit",
      "author" : [ "Florian Tramèr", "Vaggelis Atlidakis", "Roxana Geambasu", "Daniel J. Hsu", "Jean-Pierre Hubaux", "Mathias Humbert", "Ari Juels", "Huang Lin" ],
      "venue" : null,
      "citeRegEx" : "63",
      "shortCiteRegEx" : "63",
      "year" : 2015
    }, {
      "title" : "Formalizing and Enforcing Purpose Restrictions in Privacy Policies",
      "author" : [ "Michael Carl Tschantz", "Anupam Datta", "Jeannette M. Wing" ],
      "venue" : "In Proceedings of the 2012 IEEE Symposium on Security and Privacy",
      "citeRegEx" : "64",
      "shortCiteRegEx" : "64",
      "year" : 2012
    }, {
      "title" : "Formalizing and Enforcing Purpose Restrictions in Privacy Policies",
      "author" : [ "Michael Carl Tschantz", "Anupam Datta", "Jeannette M. Wing" ],
      "venue" : "In IEEE Symposium on Security and Privacy, SP 2012,",
      "citeRegEx" : "65",
      "shortCiteRegEx" : "65",
      "year" : 2012
    }, {
      "title" : "The Daily You: How the New Advertising Industry Is Defining Your Identity and Your Worth",
      "author" : [ "Joseph Turow" ],
      "venue" : null,
      "citeRegEx" : "66",
      "shortCiteRegEx" : "66",
      "year" : 2011
    }, {
      "title" : "The Aisles Have Eyes: How Retailers Track Your Shopping, Strip Your Privacy, and Define",
      "author" : [ "J. Turow" ],
      "venue" : null,
      "citeRegEx" : "67",
      "shortCiteRegEx" : "67",
      "year" : 2017
    }, {
      "title" : "Crying wolf? On the price discrimination of online airline tickets",
      "author" : [ "Thomas Vissers", "Nick Nikiforakis", "Nataliia Bielova", "Wouter Joosen" ],
      "venue" : "In 7th Workshop on Hot Topics in Privacy Enhancing Technologies (HotPETs",
      "citeRegEx" : "69",
      "shortCiteRegEx" : "69",
      "year" : 2014
    }, {
      "title" : "Business Intelligence and Predictive Analytics for Financial Services: The Untapped Potential of Soft Information. In Digits: Center for Digital Innovation, Technology, and Strategy “Research in Practice",
      "author" : [ "Siva Viswanathan" ],
      "venue" : "Paper Series. Robert H. Smith School of Business, University of Maryland",
      "citeRegEx" : "70",
      "shortCiteRegEx" : "70",
      "year" : 2010
    }, {
      "title" : "Understanding what they do with what they know",
      "author" : [ "Craig E. Wills", "Can Tatar" ],
      "venue" : "In Proceedings of the 2012 ACMWorkshop on Privacy in the Electronic Society",
      "citeRegEx" : "71",
      "shortCiteRegEx" : "71",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 51,
      "context" : "Restrictions on information use occupy a central place in privacy regulations and legal frameworks [28, 54, 61, 62].",
      "startOffset" : 99,
      "endOffset" : 115
    }, {
      "referenceID" : 52,
      "context" : "Restrictions on information use occupy a central place in privacy regulations and legal frameworks [28, 54, 61, 62].",
      "startOffset" : 99,
      "endOffset" : 115
    }, {
      "referenceID" : 12,
      "context" : "3134097 information use can lead to violations of both privacy laws [68] and user expectations [16, 19], prompting calls for technology to assist with enforcement of use privacy requirements [53].",
      "startOffset" : 95,
      "endOffset" : 103
    }, {
      "referenceID" : 15,
      "context" : "3134097 information use can lead to violations of both privacy laws [68] and user expectations [16, 19], prompting calls for technology to assist with enforcement of use privacy requirements [53].",
      "startOffset" : 95,
      "endOffset" : 103
    }, {
      "referenceID" : 47,
      "context" : "In order to meet these regulatory imperatives and user expectations, companies dedicate resources toward compliance with privacy policies governing information use [53, 57].",
      "startOffset" : 164,
      "endOffset" : 172
    }, {
      "referenceID" : 54,
      "context" : "[64] for a survey).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 48,
      "context" : "Such methods are beginning to see deployment in major technology companies like Microsoft [58].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 47,
      "context" : "The increasing adoption of these systems in a wide range of sectors, including advertising, education, healthcare, employment, and credit, underscores the critical need to address use privacy concerns [53, 57].",
      "startOffset" : 201,
      "endOffset" : 209
    }, {
      "referenceID" : 15,
      "context" : "In 2012, the department store Target drew flak from privacy advocates and data subjects for using the shopping history of their customers to predict their pregnancy status and market baby items based on that information [19].",
      "startOffset" : 220,
      "endOffset" : 224
    }, {
      "referenceID" : 12,
      "context" : ", strong predictors) for health conditions—for targeted advertising have been the basis for legal action and public concern from a privacy standpoint [16, 44, 68].",
      "startOffset" : 150,
      "endOffset" : 162
    }, {
      "referenceID" : 37,
      "context" : ", strong predictors) for health conditions—for targeted advertising have been the basis for legal action and public concern from a privacy standpoint [16, 44, 68].",
      "startOffset" : 150,
      "endOffset" : 162
    }, {
      "referenceID" : 34,
      "context" : "Similar privacy concerns have been voiced about the use of personal information in the Internet of Things [40, 49, 52, 67].",
      "startOffset" : 106,
      "endOffset" : 122
    }, {
      "referenceID" : 42,
      "context" : "Similar privacy concerns have been voiced about the use of personal information in the Internet of Things [40, 49, 52, 67].",
      "startOffset" : 106,
      "endOffset" : 122
    }, {
      "referenceID" : 45,
      "context" : "Similar privacy concerns have been voiced about the use of personal information in the Internet of Things [40, 49, 52, 67].",
      "startOffset" : 106,
      "endOffset" : 122
    }, {
      "referenceID" : 57,
      "context" : "Similar privacy concerns have been voiced about the use of personal information in the Internet of Things [40, 49, 52, 67].",
      "startOffset" : 106,
      "endOffset" : 122
    }, {
      "referenceID" : 13,
      "context" : "Indeed there are calls for this form of privacy constraint [17, 46, 53, 68].",
      "startOffset" : 59,
      "endOffset" : 75
    }, {
      "referenceID" : 39,
      "context" : "Indeed there are calls for this form of privacy constraint [17, 46, 53, 68].",
      "startOffset" : 59,
      "endOffset" : 75
    }, {
      "referenceID" : 20,
      "context" : "This trusted data processor setting is similar to the one assumed in differential privacy [25].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 17,
      "context" : ", their pregnancy status) [21].",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 15,
      "context" : "Even in practice, data processors often have access to detailed profiles of individuals and can infer sensitive information about them [19, 66].",
      "startOffset" : 135,
      "endOffset" : 143
    }, {
      "referenceID" : 56,
      "context" : "Even in practice, data processors often have access to detailed profiles of individuals and can infer sensitive information about them [19, 66].",
      "startOffset" : 135,
      "endOffset" : 143
    }, {
      "referenceID" : 10,
      "context" : "Instead we use a recently introduced causal influence measure [14] to quantitatively characterize influence.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 49,
      "context" : "Closely related work The emphasis on restricting use of information by a system rather than the knowledge possessed by agents distinguishes our work from a large body of work in privacy (see Smith [59] for a survey).",
      "startOffset" : 197,
      "endOffset" : 201
    }, {
      "referenceID" : 54,
      "context" : "[64] for a survey and Lipton and Regan [46]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 39,
      "context" : "[64] for a survey and Lipton and Regan [46]).",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 0,
      "context" : "Recent work on discovering personal data use by black-box web services focuses mostly on explicit use of protected information types by examining causal effects [2, 16, 27, 35–37, 43, 44, 47, 69, 71]); some of this work also examines associational effects [43, 44].",
      "startOffset" : 161,
      "endOffset" : 199
    }, {
      "referenceID" : 12,
      "context" : "Recent work on discovering personal data use by black-box web services focuses mostly on explicit use of protected information types by examining causal effects [2, 16, 27, 35–37, 43, 44, 47, 69, 71]); some of this work also examines associational effects [43, 44].",
      "startOffset" : 161,
      "endOffset" : 199
    }, {
      "referenceID" : 22,
      "context" : "Recent work on discovering personal data use by black-box web services focuses mostly on explicit use of protected information types by examining causal effects [2, 16, 27, 35–37, 43, 44, 47, 69, 71]); some of this work also examines associational effects [43, 44].",
      "startOffset" : 161,
      "endOffset" : 199
    }, {
      "referenceID" : 29,
      "context" : "Recent work on discovering personal data use by black-box web services focuses mostly on explicit use of protected information types by examining causal effects [2, 16, 27, 35–37, 43, 44, 47, 69, 71]); some of this work also examines associational effects [43, 44].",
      "startOffset" : 161,
      "endOffset" : 199
    }, {
      "referenceID" : 30,
      "context" : "Recent work on discovering personal data use by black-box web services focuses mostly on explicit use of protected information types by examining causal effects [2, 16, 27, 35–37, 43, 44, 47, 69, 71]); some of this work also examines associational effects [43, 44].",
      "startOffset" : 161,
      "endOffset" : 199
    }, {
      "referenceID" : 31,
      "context" : "Recent work on discovering personal data use by black-box web services focuses mostly on explicit use of protected information types by examining causal effects [2, 16, 27, 35–37, 43, 44, 47, 69, 71]); some of this work also examines associational effects [43, 44].",
      "startOffset" : 161,
      "endOffset" : 199
    }, {
      "referenceID" : 36,
      "context" : "Recent work on discovering personal data use by black-box web services focuses mostly on explicit use of protected information types by examining causal effects [2, 16, 27, 35–37, 43, 44, 47, 69, 71]); some of this work also examines associational effects [43, 44].",
      "startOffset" : 161,
      "endOffset" : 199
    }, {
      "referenceID" : 37,
      "context" : "Recent work on discovering personal data use by black-box web services focuses mostly on explicit use of protected information types by examining causal effects [2, 16, 27, 35–37, 43, 44, 47, 69, 71]); some of this work also examines associational effects [43, 44].",
      "startOffset" : 161,
      "endOffset" : 199
    }, {
      "referenceID" : 40,
      "context" : "Recent work on discovering personal data use by black-box web services focuses mostly on explicit use of protected information types by examining causal effects [2, 16, 27, 35–37, 43, 44, 47, 69, 71]); some of this work also examines associational effects [43, 44].",
      "startOffset" : 161,
      "endOffset" : 199
    }, {
      "referenceID" : 58,
      "context" : "Recent work on discovering personal data use by black-box web services focuses mostly on explicit use of protected information types by examining causal effects [2, 16, 27, 35–37, 43, 44, 47, 69, 71]); some of this work also examines associational effects [43, 44].",
      "startOffset" : 161,
      "endOffset" : 199
    }, {
      "referenceID" : 60,
      "context" : "Recent work on discovering personal data use by black-box web services focuses mostly on explicit use of protected information types by examining causal effects [2, 16, 27, 35–37, 43, 44, 47, 69, 71]); some of this work also examines associational effects [43, 44].",
      "startOffset" : 161,
      "endOffset" : 199
    }, {
      "referenceID" : 36,
      "context" : "Recent work on discovering personal data use by black-box web services focuses mostly on explicit use of protected information types by examining causal effects [2, 16, 27, 35–37, 43, 44, 47, 69, 71]); some of this work also examines associational effects [43, 44].",
      "startOffset" : 256,
      "endOffset" : 264
    }, {
      "referenceID" : 37,
      "context" : "Recent work on discovering personal data use by black-box web services focuses mostly on explicit use of protected information types by examining causal effects [2, 16, 27, 35–37, 43, 44, 47, 69, 71]); some of this work also examines associational effects [43, 44].",
      "startOffset" : 256,
      "endOffset" : 264
    }, {
      "referenceID" : 20,
      "context" : "In a setting similar to ours of a trusted data processor, differential privacy [25] protects against a different type of privacy harm.",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 39,
      "context" : "Lipton and Regan’s notion of “effectively private\" captures the idea that a protected feature is not explicitly used to make decisions, but does not account for proxy use [46].",
      "startOffset" : 171,
      "endOffset" : 175
    }, {
      "referenceID" : 18,
      "context" : "Prior work on fairness has also recognized the importance of dealing with proxies in machine learning systems [22, 29, 63].",
      "startOffset" : 110,
      "endOffset" : 122
    }, {
      "referenceID" : 23,
      "context" : "Prior work on fairness has also recognized the importance of dealing with proxies in machine learning systems [22, 29, 63].",
      "startOffset" : 110,
      "endOffset" : 122
    }, {
      "referenceID" : 53,
      "context" : "Prior work on fairness has also recognized the importance of dealing with proxies in machine learning systems [22, 29, 63].",
      "startOffset" : 110,
      "endOffset" : 122
    }, {
      "referenceID" : 44,
      "context" : ", see Nissenbaum [51]) cannot be enforced because of possible statistical inferences.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 27,
      "context" : "This form of separation exists also in some prior work on privacy [33] and fairness [23].",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 19,
      "context" : "This form of separation exists also in some prior work on privacy [33] and fairness [23].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 11,
      "context" : "Prior work on detecting use of protected information types [15, 30, 44, 63] and leveraging knowledge of detection to eliminate inappropriate uses [30] have treated the system as a black-box.",
      "startOffset" : 59,
      "endOffset" : 75
    }, {
      "referenceID" : 24,
      "context" : "Prior work on detecting use of protected information types [15, 30, 44, 63] and leveraging knowledge of detection to eliminate inappropriate uses [30] have treated the system as a black-box.",
      "startOffset" : 59,
      "endOffset" : 75
    }, {
      "referenceID" : 37,
      "context" : "Prior work on detecting use of protected information types [15, 30, 44, 63] and leveraging knowledge of detection to eliminate inappropriate uses [30] have treated the system as a black-box.",
      "startOffset" : 59,
      "endOffset" : 75
    }, {
      "referenceID" : 53,
      "context" : "Prior work on detecting use of protected information types [15, 30, 44, 63] and leveraging knowledge of detection to eliminate inappropriate uses [30] have treated the system as a black-box.",
      "startOffset" : 59,
      "endOffset" : 75
    }, {
      "referenceID" : 24,
      "context" : "Prior work on detecting use of protected information types [15, 30, 44, 63] and leveraging knowledge of detection to eliminate inappropriate uses [30] have treated the system as a black-box.",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 11,
      "context" : "Detection relied either on experimental access to the black-box [15, 44] or observational data about its behavior [30, 63].",
      "startOffset" : 64,
      "endOffset" : 72
    }, {
      "referenceID" : 37,
      "context" : "Detection relied either on experimental access to the black-box [15, 44] or observational data about its behavior [30, 63].",
      "startOffset" : 64,
      "endOffset" : 72
    }, {
      "referenceID" : 24,
      "context" : "Detection relied either on experimental access to the black-box [15, 44] or observational data about its behavior [30, 63].",
      "startOffset" : 114,
      "endOffset" : 122
    }, {
      "referenceID" : 53,
      "context" : "Detection relied either on experimental access to the black-box [15, 44] or observational data about its behavior [30, 63].",
      "startOffset" : 114,
      "endOffset" : 122
    }, {
      "referenceID" : 11,
      "context" : ", see [15, 44]).",
      "startOffset" : 6,
      "endOffset" : 14
    }, {
      "referenceID" : 37,
      "context" : ", see [15, 44]).",
      "startOffset" : 6,
      "endOffset" : 14
    }, {
      "referenceID" : 24,
      "context" : "Existing methods (see [30, 63]) can detect such associations between protected information types and outcomes in observational data.",
      "startOffset" : 22,
      "endOffset" : 30
    }, {
      "referenceID" : 53,
      "context" : "Existing methods (see [30, 63]) can detect such associations between protected information types and outcomes in observational data.",
      "startOffset" : 22,
      "endOffset" : 30
    }, {
      "referenceID" : 8,
      "context" : "The variation of information metric dvar(X ,Z ) = H (X |Z ) + H (Z |X ) [12] is one measure that satisfies these two requirements.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 8,
      "context" : "Interestingly, this measure is identical to normalized mutual information [12], a standard measure that has also been used in prior work in identifying associations in outcomes of machine learning models [63].",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 53,
      "context" : "Interestingly, this measure is identical to normalized mutual information [12], a standard measure that has also been used in prior work in identifying associations in outcomes of machine learning models [63].",
      "startOffset" : 204,
      "endOffset" : 208
    }, {
      "referenceID" : 10,
      "context" : "To measure influence, we quantify interference by using Quantitative Input Influence (QII), a causal measure of input influence introduced in [14].",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 38,
      "context" : "Note that these model types correspond to a range of commonlyused learning algorithms such as logistic regression, support vector machines [10], CART [6], and Bayesian rule lists [45].",
      "startOffset" : 179,
      "endOffset" : 183
    }, {
      "referenceID" : 6,
      "context" : "Also, these models represent a significant fraction of models used in practice in predictive systems that operate on personal information, ranging from advertising [9], psychopathy [38], criminal justice [4, 5], and actuarial sciences [32, 34].",
      "startOffset" : 164,
      "endOffset" : 167
    }, {
      "referenceID" : 32,
      "context" : "Also, these models represent a significant fraction of models used in practice in predictive systems that operate on personal information, ranging from advertising [9], psychopathy [38], criminal justice [4, 5], and actuarial sciences [32, 34].",
      "startOffset" : 181,
      "endOffset" : 185
    }, {
      "referenceID" : 2,
      "context" : "Also, these models represent a significant fraction of models used in practice in predictive systems that operate on personal information, ranging from advertising [9], psychopathy [38], criminal justice [4, 5], and actuarial sciences [32, 34].",
      "startOffset" : 204,
      "endOffset" : 210
    }, {
      "referenceID" : 3,
      "context" : "Also, these models represent a significant fraction of models used in practice in predictive systems that operate on personal information, ranging from advertising [9], psychopathy [38], criminal justice [4, 5], and actuarial sciences [32, 34].",
      "startOffset" : 204,
      "endOffset" : 210
    }, {
      "referenceID" : 26,
      "context" : "Also, these models represent a significant fraction of models used in practice in predictive systems that operate on personal information, ranging from advertising [9], psychopathy [38], criminal justice [4, 5], and actuarial sciences [32, 34].",
      "startOffset" : 235,
      "endOffset" : 243
    }, {
      "referenceID" : 28,
      "context" : "Also, these models represent a significant fraction of models used in practice in predictive systems that operate on personal information, ranging from advertising [9], psychopathy [38], criminal justice [4, 5], and actuarial sciences [32, 34].",
      "startOffset" : 235,
      "endOffset" : 243
    }, {
      "referenceID" : 59,
      "context" : "This practice is in line with the use of analytics in the financial industry that exploit the fact that high-income individuals are more likely to purchase financial products [70].",
      "startOffset" : 175,
      "endOffset" : 179
    }, {
      "referenceID" : 43,
      "context" : "Because demographic data is known to correlate with marital status [50], the data processor would like to ensure that the trained model used to make income predictions does not effectively infer individuals’ marital status from the other demographic variables that are explicitly used.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 25,
      "context" : "Student assistance A current trend in education is the use of predictive analytics to identify students who are likely to benefit from certain types of interventions [31, 39].",
      "startOffset" : 166,
      "endOffset" : 174
    }, {
      "referenceID" : 33,
      "context" : "Student assistance A current trend in education is the use of predictive analytics to identify students who are likely to benefit from certain types of interventions [31, 39].",
      "startOffset" : 166,
      "endOffset" : 174
    }, {
      "referenceID" : 7,
      "context" : "To evaluate this scenario, we trained a model on the UCI Student Alcohol Consumption dataset [11], with alcohol use as the sensitive feature.",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 14,
      "context" : "In this context, the use of health status for targeted advertising is a legitimate privacy concern [18].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 20,
      "context" : "Differential privacy [25] is one of themain pillars of privacy research in the case of computations over data aggregated from a number of individuals, where any information gained by an adversary observing the computation is not caused by an individual’s participation.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 17,
      "context" : "This absence is with good reason, as in the general case it is impossible to prevent flows of knowledge from individual-level data, while preserving the utility of such data, in the presence of arbitrary inferences that may leverage the background knowledge of an adversary [21].",
      "startOffset" : 274,
      "endOffset" : 278
    }, {
      "referenceID" : 54,
      "context" : "[64] for a survey and Lipton and Regan [46]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 39,
      "context" : "[64] for a survey and Lipton and Regan [46]).",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 12,
      "context" : "Recent work on discovering personal data use by black-box web services focuses mostly on explicit use of protected information types by examining causal effects [16, 44]; some of this work also examines associational effects [43, 44].",
      "startOffset" : 161,
      "endOffset" : 169
    }, {
      "referenceID" : 37,
      "context" : "Recent work on discovering personal data use by black-box web services focuses mostly on explicit use of protected information types by examining causal effects [16, 44]; some of this work also examines associational effects [43, 44].",
      "startOffset" : 161,
      "endOffset" : 169
    }, {
      "referenceID" : 36,
      "context" : "Recent work on discovering personal data use by black-box web services focuses mostly on explicit use of protected information types by examining causal effects [16, 44]; some of this work also examines associational effects [43, 44].",
      "startOffset" : 225,
      "endOffset" : 233
    }, {
      "referenceID" : 37,
      "context" : "Recent work on discovering personal data use by black-box web services focuses mostly on explicit use of protected information types by examining causal effects [16, 44]; some of this work also examines associational effects [43, 44].",
      "startOffset" : 225,
      "endOffset" : 233
    }, {
      "referenceID" : 24,
      "context" : "Access to observational data Detection techniques working under an associative use definition [30, 63] usually only require access to observational data about the behavior of the system.",
      "startOffset" : 94,
      "endOffset" : 102
    }, {
      "referenceID" : 53,
      "context" : "Access to observational data Detection techniques working under an associative use definition [30, 63] usually only require access to observational data about the behavior of the system.",
      "startOffset" : 94,
      "endOffset" : 102
    }, {
      "referenceID" : 12,
      "context" : "Access to black-box experimental data Detection techniques working under an explicit use definition of information use [16, 44] typically require experimental access to the system.",
      "startOffset" : 119,
      "endOffset" : 127
    }, {
      "referenceID" : 37,
      "context" : "Access to black-box experimental data Detection techniques working under an explicit use definition of information use [16, 44] typically require experimental access to the system.",
      "startOffset" : 119,
      "endOffset" : 127
    }, {
      "referenceID" : 53,
      "context" : "[63] solve an important orthogonal problem of efficiently identifying populations where associations may appear.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 41,
      "context" : "Adjusting the training dataset is the most popular approach, including variations that relabel only the class attribute [48], modify entire instances while maintaining the original schema [30], and transform the dataset into another space of features [24, 72].",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 24,
      "context" : "Adjusting the training dataset is the most popular approach, including variations that relabel only the class attribute [48], modify entire instances while maintaining the original schema [30], and transform the dataset into another space of features [24, 72].",
      "startOffset" : 188,
      "endOffset" : 192
    }, {
      "referenceID" : 4,
      "context" : "Adjustments to Naive Bayes [7] and trainers amiable to regularization [42] are examples.",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 35,
      "context" : "Adjustments to Naive Bayes [7] and trainers amiable to regularization [42] are examples.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 1,
      "context" : "Several techniques for producing differentially-private machine learning models modify trained models by perturbing coefficients [3, 8].",
      "startOffset" : 129,
      "endOffset" : 135
    }, {
      "referenceID" : 5,
      "context" : "Several techniques for producing differentially-private machine learning models modify trained models by perturbing coefficients [3, 8].",
      "startOffset" : 129,
      "endOffset" : 135
    }, {
      "referenceID" : 21,
      "context" : "Other differentially-private data analysis techniques [26] instead perturb the output by adding symmetric noise to the true results of statistical queries.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 55,
      "context" : "Though it may seem ethically ambiguous to perform a protected inference in order to (discover and) prevent protected inferences, it is consistent with the view that privacy is a function of both information and the purpose for which that information is being used [65]3.",
      "startOffset" : 264,
      "endOffset" : 268
    }, {
      "referenceID" : 50,
      "context" : "Further, protected information has already been used by public and private entities in pursuit of social good: affirmative action requires the inference or explicit recording of minority membership, search engines need to infer suicide tendency in order to show suicide prevention information in their search results[60], health conditions can potentially be detected early from search logs of affected individuals [56].",
      "startOffset" : 316,
      "endOffset" : 320
    }, {
      "referenceID" : 46,
      "context" : "Further, protected information has already been used by public and private entities in pursuit of social good: affirmative action requires the inference or explicit recording of minority membership, search engines need to infer suicide tendency in order to show suicide prevention information in their search results[60], health conditions can potentially be detected early from search logs of affected individuals [56].",
      "startOffset" : 415,
      "endOffset" : 419
    }, {
      "referenceID" : 48,
      "context" : "[58]) that operate under similar requirements could be augmented with our methods.",
      "startOffset" : 0,
      "endOffset" : 4
    } ],
    "year" : 2017,
    "abstractText" : "This paper presents an approach to formalizing and enforcing a class of use privacy properties in data-driven systems. In contrast to prior work, we focus on use restrictions on proxies (i.e. strong predictors) of protected information types. Our definition relates proxy use to intermediate computations that occur in a program, and identify two essential properties that characterize this behavior: 1) its result is strongly associated with the protected information type in question, and 2) it is likely to causally affect the final output of the program. For a specific instantiation of this definition, we present a program analysis technique that detects instances of proxy use in a model, and provides a witness that identifies which parts of the corresponding program exhibit the behavior. Recognizing that not all instances of proxy use of a protected information type are inappropriate, we make use of a normative judgment oracle that makes this inappropriateness determination for a given witness. Our repair algorithm uses the witness of an inappropriate proxy use to transform the model into one that provably does not exhibit proxy use, while avoiding changes that unduly affect classification accuracy. Using a corpus of social datasets, our evaluation shows that these algorithms are able to detect proxy use instances that would be difficult to find using existing techniques, and subsequently remove them while maintaining acceptable classification performance.",
    "creator" : "LaTeX with hyperref package"
  }
}