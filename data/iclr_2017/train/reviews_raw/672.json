{"conference": "ICLR 2017 conference submission", "title": "Joint Multimodal Learning with Deep Generative Models", "abstract": "We investigate deep generative models that can exchange multiple modalities bi-directionally, e.g., generating images from corresponding texts and vice versa. Recently, some studies handle multiple modalities on deep generative models, such as variational autoencoders (VAEs). However, these models typically assume that modalities are forced to have a conditioned relation, i.e., we can only generate modalities in one direction. To achieve our objective, we should extract a joint representation that captures high-level concepts among all modalities and through which we can exchange them bi-directionally. As described herein, we propose a joint multimodal variational autoencoder (JMVAE), in which all modalities are independently conditioned on joint representation. In other words, it models a joint distribution of modalities. Furthermore, to be able to generate missing modalities from the remaining modalities properly, we develop an additional method, JMVAE-kl, that is trained by reducing the divergence between JMVAE's encoder and prepared networks of respective modalities. Our experiments show that our proposed method can obtain appropriate joint representation from multiple modalities and that it can generate and reconstruct them more properly than conventional VAEs. We further demonstrate that JMVAE can generate multiple modalities bi-directionally.", "reviews": [{"is_meta_review": true, "comments": "This paper proposes an alternative to Conditional Variational Auto-Encoders and Conditional MultiModal Auto-Encoders to perform inference of missing modalities in dataset with multiple modalities. The proposed approach is a Variational Auto-Encoder jointly on all the modalities  with additional KL divergence penalties between the approximate posterior given all the modalities and the approximate posterior given a subset of the modalities. The approach is named Joint Multimodal Variational Auto-Encoder.\nThe authors make a connection between this approach and the Variation of Information. It is unclear why the authors chose the JMVAE approach instead of a more elegant Variation of Information approach.\nAnother unaddressed issue is the scalability of the method. As far as I can tell (given that no code is provided and the specification of the encoder is missing), this approach requires a new encoder per subset of missing modalities. Right now this approach seems to scale since there are only two modalities.\nThe fact that the estimating the log-likelihood log(p(x)) using multiple modalities provide a lower value than with just one in Table 1 is a bit odd. Could you explain that ?\nThe comparison with between the representation learned by JMVAE and CVAE might be unfair given that the representation of CVAE is learned conditionally, on the label in the case of MNIST, and should therefore not consider the label in this representation. Intuitively, this representation could represent \"style\" as shown in (Kingma et al., 2014) in their conditional generation figure.\nFor CelebA, comparing log-likelihood on models that use GANs is probably not significant since GAN does not optimizes log-likelihood. \nOverall this is an interesting problem and there are also interesting ideas worth exploring further, but the execution of the paper requires more work.", "IS_META_REVIEW": true}, {"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "This paper addresses the importance task of learning generative models of multiple modalities. There are two concerns about the paper: limited novelty, which will not have sufficient impact; ineffectiveness of evaluation. The paper extends VAEs in an interesting way, but this extension on its own does provide sufficient new insight understanding. And the log-likelihood evaluations and data sets are not enough to be convincing. As a result, the paper is not yet ready for acceptance at the conference.", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "The JMVAE is rather straightforward extension of VAE.", "is_meta_review": false, "comments": "The paper introduces the joint multimodal variational autoencoder, a directed graphical model for modeling multimodal data with latent variable. the model is rather straightforward extension of standard VAE where two data modalities are generated from a shared latent representation independently. In order to deal with missing input modalities or bi-directional inference between two modalities the paper introduces modality-specific encoder that is trained to minimize the KL divergence of latent variable distributions between joint and modality-specific recognition networks. The paper demonstrates its effectiveness on MNIST and CelebA datasets, both in terms of test log-likelihoods and the conditional image generation and editing.\n\nThe proposed method is rather straightforward extension of VAE and therefore the model should inherent the probabilistic inference methods of VAE. For example, for missing data modalities, the model should be able to infer joint representation as well as filling in the missing modalities via iterative sampling as introduced by Rezende et al. (2014). Given marginal improvement, I am not convinced by the contribution of modality-specific encoders in Section 3.3. In addition, the inference methods introduced for generating Figure 5 looks somewhat unprincipled; I am wondering the conditional image generation results by following more principled approach (e.g., iterative sampling). Experimental results on joint image-attribute generation is also missing.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Some interesting ideas, but has major problems", "is_meta_review": false, "comments": "The proposed method of modeling multimodal datasets is a VAE with an inference network for every combination of missing and present modalities. The method is evaluated on modeling MNIST and CelebA datasets.\n\nMNIST is hardly a multimodal dataset. The authors propose to use the labels as a separate modality that gets modeled with a variational autoencoder. The reviewer finds this choice perplexing.\nEven then the modalities are never actually missing, so the applicability of the suggested method is questionable.\nIn addition the differences in log-likelihoods between different models are tiny, and likely to be due to noise.\n\nThe other experiment reports log-likelihood of models that were not trained to maximize log-likelihood. It is not clear what conclusions can be drawn from such comparison.", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "No Title", "is_meta_review": false, "comments": "This paper proposes an alternative to Conditional Variational Auto-Encoders and Conditional MultiModal Auto-Encoders to perform inference of missing modalities in dataset with multiple modalities. The proposed approach is a Variational Auto-Encoder jointly on all the modalities  with additional KL divergence penalties between the approximate posterior given all the modalities and the approximate posterior given a subset of the modalities. The approach is named Joint Multimodal Variational Auto-Encoder.\nThe authors make a connection between this approach and the Variation of Information. It is unclear why the authors chose the JMVAE approach instead of a more elegant Variation of Information approach.\nAnother unaddressed issue is the scalability of the method. As far as I can tell (given that no code is provided and the specification of the encoder is missing), this approach requires a new encoder per subset of missing modalities. Right now this approach seems to scale since there are only two modalities.\nThe fact that the estimating the log-likelihood log(p(x)) using multiple modalities provide a lower value than with just one in Table 1 is a bit odd. Could you explain that ?\nThe comparison with between the representation learned by JMVAE and CVAE might be unfair given that the representation of CVAE is learned conditionally, on the label in the case of MNIST, and should therefore not consider the label in this representation. Intuitively, this representation could represent \"style\" as shown in (Kingma et al., 2014) in their conditional generation figure.\nFor CelebA, comparing log-likelihood on models that use GANs is probably not significant since GAN does not optimizes log-likelihood. \nOverall this is an interesting problem and there are also interesting ideas worth exploring further, but the execution of the paper requires more work.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016 (modified: 17 Dec 2016)", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Question on Equation (5)", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "is_meta_review": false}, {"TITLE": "Masking", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "IS_META_REVIEW": false, "DATE": "28 Nov 2016", "is_meta_review": false}, {"TITLE": "Related work", "OTHER_KEYS": "Diane Nicole Bouchacourt", "comments": "Have you considered the following related work : \"Multi-modal Auto-Encoders as Joint Estimators for\nRobotics Scene Understanding\" Cadena et al. ? Could you explain how your method differs from theirs ?\n\nThanks,\n\nDiane", "IS_META_REVIEW": false, "DATE": "18 Nov 2016", "is_meta_review": false}], "SCORE": 5, "authors": "Masahiro Suzuki, Kotaro Nakayama, Yutaka Matsuo", "accepted": false, "id": ""}
