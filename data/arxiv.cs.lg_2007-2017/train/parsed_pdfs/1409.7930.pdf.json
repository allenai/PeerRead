{
  "name" : "1409.7930.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Cognitive Learning of Statistical Primary Patterns via Bayesian Network",
    "authors" : [ "Weijia Han", "Huiyan Sang", "Min Sheng", "Jiandong Li", "Shuguang Cui" ],
    "emails" : [ "alfret@gmail.com,", "msheng@mail.xidian.edu.cn,", "jdli@pcn.xidian.edu.cn).", "huiyan@stat.tamu.edu).", "cui@ece.tamu.edu)." ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 9.\n79 30\nv5 [\ncs .L\nG ]\n9 F\neb 2\n01 5\nIndex Terms—Cognitive radio, Bayesian network learning, network structure learning.\nI. INTRODUCTION\nSince the terminology was coined in 1999 [1], cognitive radio (CR) has been developed for more than fifteen years, which has drawn attention from both academic and industrial communities since it is intended to enable smart use of the scarce spectrum resource with the initial objective of maximizing spectrum utilization. Recently, cognitive network design goes beyond spectrum utilization and target at broader network objectives such as higher quality of service, lower energy cost, etc. To achieve such new objectives, the statistical knowledge on the primary network status becomes necessary [2] for resource management and system control, which gets us\nFunding acknowledgement: The work was supported in part by DoD with grant HDTRA1-13-1-0029, by NSF with grants CNS-1343155, ECCS1305979, and CNS-1265227, by National Natural Science Foundation of China under Grants 61401320, 61231008, 91338114, 61172079, 61201141, 61301176, and 61328102, by the National High Technology Research and Development Program of China (863 Program) under Grant 2014AA01A701, by the 111 Project of China under Grant B08038.\nW. Han, M. Sheng, and J. Li are with Broadband Wireless Communications Lab. & State Key Lab. (ISN), Information Science Institute, Xidian University, Xi’an, Shaanxi, 710071, China (Emails: alfret@gmail.com, msheng@mail.xidian.edu.cn, jdli@pcn.xidian.edu.cn).\nH. Sang is with the Department of Statistics, Texas A&M University (email: huiyan@stat.tamu.edu).\nS. Cui is with the Department of Electrical and Computer Engineering, Texas A&M University (e-mail: cui@ece.tamu.edu). S. Cui is also a Distinguished Adjunct Professor at King Abdulaziz University in Saudi Arabia.\ncloser to the ideal CR operation that integrates spectrum sensing, environment learning, statistical reasoning, and predictive acting. This will go beyond most of the existing CR sensing literature, which usually focus on detecting the presence of primary users only [3]–[5].\nIn practice, the network behavior is impacted by many factors which may change dynamically, such that the uncertainty of a network cannot be priorly represented by a certain statistical distribution. To cope with such an issue, the statistical machine learning methodology becomes a feasible solution for understanding the network activity pattern. In this paper, we introduce the Bayesian network (BN) [6] structure learning method to obtain the statistical primary networking pattern, via observing the on/off status of primary base stations. The Bayesian model has been well known in the field of artificial intelligence (AI) [7]. When considering the probability and uncertainty, BN is a distinct technique for modeling the complex interaction among real world facts [8]. In particular, the BN structure learning is an effective new modeling tool in both spatial and temporal domains. However, the associated computational complexity is high since it needs to evaluate the dependence between each pair of variables in a target system and compute the corresponding conditional probability table1 [7], which becomes the major drawback in applying BN structure learning. In this paper, we focus on reducing the computational load for efficiently learning the statistical behavior patterns of primary users.\nFor BN structure learning, the related algorithms could be sorted into two categories. One is to use heuristic searching to construct a probable model and then evaluate it by a scoring function [10]. The structure with the highest score is preferred as the learning outcome. The score-based approach of learning BNs has been proven as a NP-hard problem [11]. The other one is to use conditional independence test to measure every possible dependence relationships one by one, and then determine the structure based on the evaluated dependence. In [12], the authors show a much more efficient approach to learn the ordered BN by using mutual information to check the dependence of any possible pairs of nodes. However, this approach cannot be adaptively adjusted when the number of variable changes. To overcome the drawbacks of the current learning methods, we propose a structure learning algorithm based on a completely connected graph and the conditional mutual information, which could efficiently learn both the structure and the corresponding conditional probability table.\n1In statistics, the conditional probability table is defined for a set of discrete random variables to quantify the marginal probability of a single variable with respect to the others [9].\n2 In particular, each pair of variables in the BN is defined with a generalized relationship where the independence case is unified as the weakest dependence case. As a result, the BN network structure is completely connected. Accordingly, the network structure becomes very regular, such that the conditional mutual information based learning could be formulated as a sequence of closed-form function evaluations. With this, our learning algorithm not only has the same computational overhead as that in [12], but can also dynamically adapt to different numbers of variables. Moreover, for further reducing\nthe computational complexity, we simplify the conditional mutual information function and explore the prior knowledge that the CR sensing results are binary.\nBesides the complexity problem, there are some special issues in learning BN structure in the context of CR, due to the lack of collaboration from the primary user side. In conventional learning cases, many existing works assume the number of variables and their related observations are usually prior knowledge and hence only focus on learning the structure among the variables. However, in CR, the observations are usually collected without recording the correct time epoch, causing the observation to be unidentifiable on which time that it belongs to. This is mainly due to the fact that the statistical period of the BN structure, which reflects the temporal pattern, is not known a priori. In essence, the above issues belong to the unsupervised classification problem, with a new challenge to consider the missing time period. To address this, we propose a blind variable identification algorithm, combining with the proposed structure learning algorithm, to learn the period via the fast Fourier transformation (FFT). In conclusion, the proposed structure and period learning algorithms constitute a complete BN structure learning scheme, by which CR could understand the statistical pattern of primary base stations in both spatial and temporal domains.\nThe remainder of the paper is organized as follows. In Section II, the system model is presented in details. Section III briefly introduces the relationship between the BN method and the wireless communication problem in CR. In Section IV, an efficient algorithm is proposed for jointly learning the structure and the corresponding conditional probability table. In Section V, an algorithm is proposed to obtain the statistical period of BN structure. In Section VI, the simulation and learning results are presented to validate the proposed scheme. Finally, we conclude the paper in Section VII."
    }, {
      "heading" : "II. SYSTEM MODEL",
      "text" : "Our system model consists of a primary cellular network and multiple secondary sensors over an observation area, of which the detailed specifications are given as follows.\nThe primary cellular network consists of several primary base stations and multiple mobile primary users. The observation area consists of the cells and a road as illustrated in Fig. 1, where we consider the mobile primary users only moving along a one-way (from right to left) road for our case of study. The arrival of the users at the entrance of the road follows a Poisson process. The arrived mobile users pass along the road at a speed generated by a uniform distribution. Once the\nPrimary base station Mobile primary user Secondary sensor\n#1\n#2\n#3\nFig. 1. System Diagram\nmobile users move out the road, they are no longer observed. Additionally, the primary network obeys the following setup: 1) The primary base stations are located based on a predesigned network deployment plan (i.e., at the centers of the cells); 2) the primary users share a single channel and access a primary base station depending on which cell they are located geographically; 3) an ideal TDMA-based multiple access (MAC) scheme is adopted, and the arrival and departure of primary data traffic at each user follow a Poisson process and an exponential service law, respectively. Hence, the on/off status of a primary base station is determined by the overall data traffic generated from the mobile primary users in its cell (here we only consider the uplink transmissions).\nA cognitive secondary sensor is installed very close to each primary base station, which implies that the accuracy of sensing could be assumed perfect, such that no sensing errors are considered in this paper. The secondary sensors sense the on/off status of primary base stations periodically in a synchronous fashion. Let set M ∶= {1,2,⋯,M} denote the observed primary base stations, and set T ∶= {1,2,⋯, T } denote the sequence of time epoch t, t ∈ T. In addition, let fi,t be a variable denoting the state of the i-th primary base station at time t, and fi,t ∈ O ∶= {0,1}, where 0 and 1 represent the off and on statuses respectively."
    }, {
      "heading" : "III. COGNITIVE BAYESIAN NETWORK",
      "text" : "In CR, as we argued before, it is valuable to know the statistical behavior of the primary network. By exploring such knowledge, the secondary network could exploit the idle spectrum resource more efficiently and more broadly. In our setup, each secondary sensor could obtain a number of observations (or samples) about the on/off status of the observed primary base station, which is the key element of the primary network. The existing works with respect to CR sensing have been mainly focused on the busy/idle status of a particular spectrum hole by observing a specific primary base station. Contrastingly, our objective is to learn the statistical pattern of the spatial and temporal behavior of multiple networked primary base stations by mining the obtained observations over the whole network across different time epochs. To achieve our objective, BN structure learning is deployed as the key methodology. The BN framework has been known in the field of artificial intelligence and exploited in different expert systems to model complex interactions among\n3\ncauses and consequences, while BN structure learning aims to derive and quantify the complex interaction from data."
    }, {
      "heading" : "A. Bayesian Network Approach",
      "text" : "With BN, the spatial and temporal interactions among primary base stations are expressed by a directed graph G = (V,E), where V is a finite, nonempty set whose elements are called the nodes denoting the variable fi,t, and E is a set of directed lines called the edges connecting the pairs of distinct elements in V. If there is a directed edge from fi1,t1 to fi2,t2 where fi1,t1 , fi2,t2 ∈ E, it means that fi2,t2 is impacted by fi1,t1 i.e., fi2,t2 depends on fi1,t1 . In BN analysis, such a directional relationship is generally expressed by a conditional probability P (fi2,t2 ∣fi1,t1). For the graphical BN, it has a distinguishing feature that: For an arbitrary fi,t ∈ V, it is conditionally independent of the set of all other indirectly connected nodes given the set of all directly connected nodes. Apparently, if we have the complete knowledge on (V,E) together with all the values of P (fi2,t2 ∣fi1,t1), the statistical pattern of the network behavior is readily available."
    }, {
      "heading" : "B. BN Structure Learning in CR",
      "text" : "As explained above, for quantifying the BN and the related graph from observations, we need to determine both the variables (nodes) and the dependence of each pair of variables (edge). In many applications, the number of variables is priorly known. Hence, most of learning results mainly focus on how to learn the edges efficiently. In our CR scenario, since the observations are collected from the deployed sensors, it is easy to identify M for the range of i in fi,t. However, due to the randomness of the primary user number, speed, and traffic, the temporal information about T, which defines the range for t of fi,t, is not directly known. Thus, BN learning in CR not only needs to efficiently mine the relationship and interaction among the variables, but also has to identify the temporal scale T of BN. Since T is not known, we cannot simply sort the observations into the corresponding nodes. Hence, the unknown time scale T becomes a critical issue in the proposed CR BN learning. In machine learning language, such an issue belongs to the unsupervised classification problem, which is widely considered difficult.\nFor edge learning, the traditional learning methods are based on scoring or dependence checking functions. Given a scoring\nfunction, the computational complexity of determining the BN structure increases exponentially when the number of variables increases. The score-based approach for learning Bayesian networks has been shown NP-hard [11], which is a key challenge in the learning community. Recently, a relatively efficient way to learn an ordered BN is given in [12] by using the conditional mutual information to check the dependence between any possible pair of nodes. Formally, the conditional mutual information is defined as\nI(X ;Y ∣Z) = ∑ z∈Z ∑ y∈Y ∑ x∈X PX,Y,Z(x, y, z)\n× log PZ(z)PX,Y,Z(x, y, z) PX,Z(x, z)PY,Z(y, z) ,\n(1)\nwhich is a measure of the mutual dependence between variables X and Y given variable Z , where the marginal, joint, and/or conditional probability mass functions are denoted by P with appropriate subscripts. Evidently, this method demands for multiple nested for-loops for implementation, and the number of for-loops is determined by the number of variables in the BN. Hence, this method is not directly applicable for the online learning case where the number of variables may be time-varying. Moreover, in [12], the conditional mutual information checking is performed for every combination of all possible parent nodes of X . In other words, I(fi,t;fi,t−1∣Ft−1) is computed for all Ft−1 where Ft−1 denotes the k-combination of set {f1,t−1,⋯, fM,t−1}, k ∈ M. The combination based checking introduces the huge computational overhead."
    }, {
      "heading" : "C. Cognitive Bayesian Network Learning",
      "text" : "Considering the unknown T and the high computational complexity issues, we propose a BN model for our CR sensing case, and term it as cognitive BN (CBN). The CBN has four characteristics: 1) It is a first-order BN and its nodes are ordered in the temporal domain; 2) its structure is completely connected which means ∀i ∈ M,∀t ∈ T/{1}, there exists an edge between any fi,t and fi,t−1; 3) the observation of each variable is binary; 4) it has an unknown operation period. Here, “ordered in the temporal domain” means that for a given i, fi,t, fi,t+1⋯ are ordered in t, t + 1⋯. Characteristics 3 and 4 are two direct outcomes from the system model. Thus we only explain characteristics 1 and 2 below.\nIn our system model, the user movement and data service behaviors of primary users at the current time epoch t could be solely determined by the system states in the former time epoch t−1. In other words, our system model has the first-order Markov property, which has been adopted previously [13]. In [13], it is shown that a wireless communication network could be represented by a Markov state transition system. Hence, we have characteristic 1 in our model, which leads to huge complexity reduction in checking the multiple ordered nodes. Next, we explain characteristic 2 in detail, which is unique and critical in further reducing the computational burden.\nWhen learning the edges, the conventional approaches usually consider the edges either existing or absent, by analyzing the directed dependence that is based on the empirical probabilities generated from observations, where the edge weight could be considered as a bi-level quantization of the directed\n4 dependence [14]. In contrast, this paper first considers the existence of every possible edge in the first-order BN model, and then quantifies the existence with an analog value to reflect the dependence level between any two nodes. In other words, we consider independence as an extreme case of dependence with edge value = 0, which implies that each pair of variables in the CBN has a generalized relationship. Based on such an approach, the CBN has a completely connected structure that is highly regular, as shown in Fig. 2. In the next section, we show that, by exploring the regularity of the CBN structure, both the analog-valued edges and the conditional probability table could be learned efficiently."
    }, {
      "heading" : "IV. EFFICIENT LEARNING IN CBN",
      "text" : "In this section, we propose an efficient learning algorithm which could correctly work under an arbitrary period T , while the problem of an unknown T is studied in the next section. As explained before, high computational complexity is a critical issue in BN structure learning. It has been shown [12] that the mutual information check based approach leads to the desired efficiency; but it cannot handle a dynamic number of variables. Here, we proposed an efficient learning algorithm of the same complexity as the I(X ;Y ∣Z) based method, and can cope with the varying number of variables.\nWhen employing the conditional mutual information, every possible edge will be checked in turn. In other words, the number of possible edges affects the learning overhead. Recall that our CBN is a first-order BN and ordered in the temporal domain. It means that, in CBN, the direction of edges is known and the edge only connects the adjacent nodes for a given i in fi,t as shown in Fig. 2. Hence, the computational complexity of learning such a CBN is proportional to learning a subgraph enclosed by the bold-line rectangle in Fig. 2. If we do not consider the direction of edges, this subgraph is called a clique in a Markov network. For the clarity of expression, we here call the targeted subgraph as a C-clique. According to characteristic 1, the number of edges in a CBN is T times as that in a C-clique. Apparently, the computational complexity of learning CBN is linearly proportional to the overhead of learning a C-clique. Hence, in the following study, we focus on how to efficiently learn a C-clique."
    }, {
      "heading" : "A. Learning a C-clique by Conventional Methods",
      "text" : "Before introducing our idea, we need to show how the current approaches learn a CBN by using the conditional mutual information check, which is helpful for us to understand the computational complexity of learning a C-clique with the proposed algorithm. In conventional methods, when learning the edges in a C-clique, we need to determine the existence of possible edges one by one. For example, to check the edge between f1,t and f1,t+1 with a realization of M = 3 as shown in Fig. 2, the corresponding conditional mutual information I(f1,t;f1,t+1∣Ft) is performed by three times for Ft ∶= {f2,t}, Ft ∶= {f3,t}, and Ft ∶= {f2,t, f3,t}.\nActually, according to information theory, there exists\nI(fi,t;fi,t+1∣Ft) = I(fi,t;fi,t+1∣Fp,t,Ft/Fp,t)\n= I(fi,t;fi,t+1∣Fp,t) (2)\nwhere Ft ∶= {f1,t, f2,t,⋯} is a set containing all nodes at t, and Fp,t ⊂ Ft is a set containing all parent nodes of fi,t+1 at t. (2) means that it is not necessary to check the mutual information conditioned on every possible Ft. To utilize such results, the proposed completely connected structure shows a distinct merit that we only need perform the following checking once,\nI(f1,t;f1,t+1∣f2,t, f3,t)\n= ∑ f1,t ∑ f2,t ∑ f3,t ∑ f1,t+1 P (f1,t, f2,t, f3,t, f1,t+1)\n× log P (f2,t, f3,t)P (f1,t, f2,t, f3,t, f1,t+1) P (f1,t, f2,t, f3,t)P (f1,t+1, f2,t, f3,t) ,\n(3)\nwhere each probability is estimated by an empirical probability2.\nOn the other hand, the computational load of calculating I(f1,t;f1,t+1∣f2,t, f3,t) is mainly determined by using the related observations to calculate both the empirical probability P (f1,t, f2,t, f3,t, f1,t+1) and the structure of I(f1,t;f1,t+1∣f2,t, f3,t) function itself. Apparently, the related computation requires all the realizations of {f1,t, f2,t, f3,t, f1,t+1}, e.g. {f1,t = 0, f2,t = 0, f3,t = 0, f1,t+1 = 0}, {f1,t = 0, f2,t = 0, f3,t = 0, f1,t+1 = 1}, ⋯, {f1,t = 1, f2,t = 1, f3,t = 1, f1,t+1 = 1}. As a result, the computational complexity of measuring an edge depends on computing 2(3+1) times of P (f1,t, f2,t, f3,t, f1,t+1) log\nP (f2,t,f3,t)P (f1,t,f2,t,f3,t,f1,t+1) P (f1,t,f2,t,f3,t)P (f1,t+1,f2,t,f3,t) . 3\nIn general, since there are M2 edges in a C-clique, the computational complexity of learning a Cclique is determined by running M22(M+1) times of P (f1,t, f2,t, f3,t, f1,t+1) log\nP (f2,t,f3,t)P (f1,t,f2,t,f3,t,f1,t+1) P (f1,t,f2,t,f3,t)P (f1,t+1,f2,t,f3,t) .\nSuch outcome implies that the computational complexity of learning a C-clique could be reduced by improving the dependency checking function. On the other hand, each term in (3) is actually based on the conditional probability table considering that the C-clique is completely connected. When each term in (3) is estimated by the empirical probabilities, the computational complexity of (3) is proportional to that of computing the conditional probability table. Evidently, if we could efficiently learn the complete conditional probability table and quantify each edge by closed-form expressions and without nested for-loops, the issues of varying variables and high computational complexity will be solved and released, respectively. Based on the above two ideas, we next propose an efficient algorithm based on analyzing the completely connected structure and exploring the fact of binary observations."
    }, {
      "heading" : "B. Efficient Learning Algorithm for Conditional Probability Table",
      "text" : "As explained before, the efficiency of the mutual information based CBN learning is determined by two factors: the measurement method of dependence and the adaptation to the number of variables. Among the two factors, the latter one\n2For the clarity of expression, the empirical probability and the true probability are expressed by the same notation system.\n3When implementing this calculation in code, it needs 2(3+1) for-loops.\n5 plays a leading role. In this subsection, we show our effort to handle the second factor by utilizing characteristic 2 of our CBN."
    }, {
      "heading" : "1) Completely Connected Structure and Its Benefit:",
      "text" : "In a completely connected BN, we need to compute P (fi,t∣pa(fi,t)), ∀i ∈M and ∀t ∈ T, to obtain the conditional probability table, where pa(fi,t) means fi,t’s parents that are the nodes connecting to fi,t directly. From a glance, the completely connected BN is of high computational complexity because of the large number of edges. However, the fact is just the opposite. Since the structure of a completely connected BN is perfectly symmetry, the algorithm of computing the conditional probability table could be designed efficiently, which will be discussed next.\nFor a variable fi,t, consider its related observations contained in a column vector oi,t. For brevity, we take M = 2 along with binary observations to explain our idea, and then extend the related results to a general case with M ≥ 2. From the perspective of frequentist probability, the empirical conditional probability table of P (fi,t∣pa(fi,t)) in a C-clique is given by\nP (f1,t = 1∣f1,t−1 = 1, f2,t−1 = 1) = o T 1,t−1 ○ o T 2,t−1\noT1,t−1o2,t−1 o1,t, (4)\nwhere ○ is the Hadamard product,\nP (f1,t = 1∣f1,t−1 = 0, f2,t−1 = 1) = ō T 1,t−1 ○ o T 2,t−1\nōT1,t−1o2,t−1 o1,t, (5)\nwhere ōT1,t−1 = 1 − o T 1,t−1,\nP (f1,t = 1∣f1,t−1 = 1, f2,t−1 = 0) = o T 1,t−1 ○ ō T 2,t−1\noT1,t−1ō2,t−1 o1,t, (6)\nwhere ōT2,t−1 = 1 − o T 2,t−1,\nP (f1,t = 1∣f1,t−1 = 0, f2,t−1 = 0) = ō T 1,t−1 ○ ō T 2,t−1\nōT1,t−1ō2,t−1 o1,t, (7)\nP (f2,t = 1∣f1,t−1 = 1, f2,t−1 = 1) = o T 1,t−1 ○ o T 2,t−1\noT1,t−1o2,t−1 o2,t, (8)\nP (f2,t = 1∣f1,t−1 = 0, f2,t−1 = 1) = ō T 1,t−1 ○ o T 2,t−1\nōT1,t−1o2,t−1 o2,t, (9)\nP (f2,t = 1∣f1,t−1 = 1, f2,t−1 = 0) = o T 1,t−1 ○ ō T 2,t−1\noT1,t−1ō2,t−1 o2,t,\n(10) and\nP (f2,t = 1∣f1,t−1 = 0, f2,t−1 = 0) = ō T 1,t−1 ○ ō T 2,t−1\nōT1,t−1ō2,t−1 o2,t.\n(11) It is easy to see that the empirical conditional probability could be calculated as multiplying the observation om,t by a regular arithmetic operator denoted by F . We add an index c to F to express the condition, e.g., Fc with c = 00 stands for the arithmetic operator under f1,t−1 = 0 and f2,t−1 = 0, such that we have F00 = ō T 1,t−1○ōT2,t−1 ōT 1,t−1ō2,t−1 .\nFrom (4)-(7) and (8)-(11), we see that the computation of the conditional probability table is transformed to obtaining the arithmetic operator Fc with every realization of index c, where the structure of Fc is very regular, which benefits from characteristic 2 of our CBN. Based on the regularity of Fc, the edges no longer need to be learned one by one, which is explained as follows.\n2) Binary Observation based Conditional Probability Table (BbCPT) Learning Algorithm: In CR, the on/off behavior of a primary base station is expressed by a binary value, which leads to our learning algorithm exploring this fact.\nGiven binary observations, the number of realizations of c is 2M , based on which we define x ∈ {0,1,⋯,2M − 1}. Accordingly, for a given x, we could use (12) defined below to generate a corresponding vector cx leading to c = cx[1] × cx[2] ×⋯× cx[M], with\ncx[i] = ⌊ x 2i−1 ⌋ /2 (12) where / denotes the modulo operation and i ∈ {1,2,⋯,M}. For example, when M = 2 and x = 2, c2 = [1 0], which means c = 10. Further, we could generate a matrix C where each row contains a realization of c given x:\nC[x,M − i + 1] = ⌊ x 2i−1 ⌋ /2, i ∈M. (13) On the other hand, when the observation is binary and we take M = 2 as in the previous subsection, the numerators of F11, F10, F01, and F00 could be reformulated as\no T 1,t−1 ○ o T 2,t−1\n= ⌊oT1,t−1 + oT2,t−1 2 ⌋ = ⌊1oT1,t−1 + 1oT2,t−1 + 0(1 − oT1,t−1) + 0(1 − oT2,t−1)\n2 ⌋\n= ⌊c3[oT1,t−1,oT2,t−1] + (1 − c3)(1 − [oT1,t−1,oT2,t−1]) 2 ⌋ ,\n(14)\no T 1,t−1 ○ ō T 2,t−1\n= ⌊oT1,t−1 + (1 − oT2,t−1) 2\n⌋ = ⌊1oT1,t−1 + 0oT2,t−1 + 0(1 − oT1,t−1) + 1(1 − oT2,t−1)\n2 ⌋\n= ⌊c2[oT1,t−1,oT2,t−1] + (1 − c2)(1 − [oT1,t−1,oT2,t−1]) 2 ⌋ ,\n(15)\nō T 1,t−1 ○ o T 2,t−1\n= ⌊(1 − oT1,t−1) + oT2,t−1 2\n⌋ = ⌊c1[oT1,t−1,oT2,t−1] + (1 − c1)(1 − [oT1,t−1,oT2,t−1])\n2 ⌋ ,\n(16)\n6 ō T 1,t−1 ○ ō T 2,t−1\n=⌊(1 − oT1,t−1) + (1 − oT2,t−1) 2\n⌋ =⌊c0[oT1,t−1,oT2,t−1] + (1 − c0)(1 − [oT1,t−1,oT2,t−1])\n2 ⌋ ,\n(17)\nrespectively. Let nom(x) and dnom(x) respectively denote the numerator and denominator of x, andF be a matrix containing all Fc’s. Then, we arrive at a simple form for each realization of Fc as\nnom(F) =[nom(F00);nom(F01);nom(F10);nom(F11)] = ⌊COt−1 + (1 −C)(1 −Ot−1)\n2 ⌋\n(18)\nwhere Ot−1 = [o1,t−1,o2,t−1]T , (19)\nC = ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ 0 0 0 1 1 0 1 1 ⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ , (20)\nwith C obtained from (13). On the other hand, since nom(Fc) is a matrix, we have dnom(Fc) = nom(Fc)1N , where 1N is a column vector of N ones. Thus, the denominator of F is given by\ndnom(F) = nom(F)1N . (21) Extending (18) to an arbitrary value of M yields a general\narithmetic operator F , F =nom(F)./(nom(F)1N ⊗ 1TN), (22) where ⊗ denotes the Kronecker product, and\nnom(F) = ⌊COt−1 + (1 −C)(1 −Ot−1)] M\n⌋ , (23) Ot−1 = [o1,t−1,o2,t−1,⋯,oM,t−1]T . (24)\nHence, the empirical conditional probability table could be calculated as Bt−1 = [B1,t−1,⋯,BM,t−1] =FOTt , (25) where Bi,t−1 is a column vector with size 2M containing every probability that fi,t = 1 holds conditioned on c.\nConsequently, the BbCPT learning algorithm is given by (13) and (22)-(25). When compared with conventional methods, the proposed BbCPT algorithm effectively reduces the number of for-loops to just two for-loops (with matrix computation), and the overall empirical conditional probability table could be obtained by a sequence of closed-form function evaluations efficiently.\nAt the beginning of this subsection, we stated that the total computational overhead is affected by two factors, with the latter one already discussed in this subsection. Next, we study the first one to show how to efficiently measure the dependence between any pair of two variables in a C-clique."
    }, {
      "heading" : "C. Efficient Dependence Measure",
      "text" : "Given discrete random variables X with support X and Y with support Y , the conditional entropy between X and Y is given by\nH(Y ∣X) = ∑ x∈X P (x)H(Y ∣X = x) = − ∑\nx∈X P (x) ∑ y∈Y P (y∣x) logP (y∣x)\n= ∑ x∈X ,y∈Y P (x, y) log P (x) P (x, y) .\n(26)\nIf Y is completely determined by X , we have H(Y ∣X) = 0; if Y is independent of X , we have H(Y ∣X) = H(Y ). Hence, H(Y ∣X) ∈ [0,H(Y )] reflects the dependence of Y on X . According to information theory, H(Y ∣X) +H(X ∣Y ) is equivalent to I(X ;Y ) when measuring the dependence between X and Y 4.\nFor the CR case where X ∶= O and Y ∶= O, the conditional entropy is given by\nH(Y ∣X) = − ∑ x∈X P (x)P (y = 1∣x) logP (y = 1∣x) − ∑\nx∈X P (x)P (y = 0∣x) logP (y = 0∣x), (27)\nwhich is always smaller or equal to H(Y ) over all possible P (x). According to (27), there is H(Y ∣X) = H(Y ), i.e., Y independs of X , when the following holds.\nlogP (y∣x = 0)− logP (y∣x = 1) = 0, (28) for y = 0 or 1.\nSimilarly, we could conclude the same observations for H(X ∣Y ). Evidently, given any values of P (x) and P (y), smaller values of ∣ logP (y = 0∣x = 0)−logP (y = 0∣x = 1)∣ and∣ logP (y = 1∣x = 0) − logP (y = 1∣x = 1)∣ imply larger values of H(Y ∣X) or H(X ∣Y ), which further implies weaker dependence between Y and X . Therefore, we arrive at the following dependence metric, where Dp(y;x) is termed as the conditional probability based dependence (CPbD),\nDp(y;x) = ∑ y∈O ∣ logP (y∣x = 0)− logP (y∣x = 1)∣ (29) which is symmetric with respect to P (y = 1∣x = 1) and P (y = 1∣x = 0). The proposed CPbD Dp(y;x) is not affected by P (y) and P (x), and the value range of Dp(y;x) is[0,+∞). For multiple-variable cases, the CPbD between x and y conditioned on z is given by\nDp(y;x∣z) = ∑ y,z∈O ∣ logP (y∣z, x = 0) − logP (y∣z, x = 1)∣. (30)\nTaking the example of M = 2 in CBN, the CPbD between\n4In probability theory and information theory, H(Y ∣X) + H(X ∣Y ) is a measure between variables for evaluating the variation of information or shared information distance.\n7 f1,t−1 and f1,t conditional on f2,t−1 is given by\nDp(f1,t;f1,t−1∣f2,t−1) =∣ logP (f1,t = 1∣f1,t−1 = 1, f2,t−1 = 1) − logP (f1,t = 1∣f1,t−1 = 0, f2,t−1 = 1)∣ + ∣ logP (f1,t = 1∣f1,t−1 = 1, f2,t−1 = 0)\n− logP (f1,t = 1∣f1,t−1 = 0, f2,t−1 = 0)∣ + ∣ logP (f1,t = 0∣f1,t−1 = 1, f2,t−1 = 1)\n− logP (f1,t = 0∣f1,t−1 = 0, f2,t−1 = 1)∣ + ∣ logP (f1,t = 0∣f1,t−1 = 1, f2,t−1 = 0)\n− logP (f1,t = 0∣f1,t−1 = 0, f2,t−1 = 0)∣\n(31)\nwhere each term inside the log operation could be directly obtained from the conditional probability table derived in Section IV-B. Moreover, each term has a regular relative index c corresponding to the conditional probability table. As shown in the previous subsection, the conditional probability table with respect to f1,t is given as B1,t−1 = [B00,B01,B10,B11]T . Let B̄1,t−1 = 1 −B1,t−1; we have logB1,t−1 = [logB00, logB01, logB10, logB11]T and log B̄1,t−1 = log(1 −B1,t−1). Then Dp(f1,t;f1,t−1∣f2,t−1) and Dp(f1,t;f2,t−1∣f1,t−1) can be calculated as Dp(f1,t;f1,t−1∣f2,t−1) = (∣logBT1,t−1L1∣ + ∣log B̄T1,t−1L1∣)12, (32) Dp(f1,t;f2,t−1∣f1,t−1) = (∣logBT1,t−1L2∣ + ∣log B̄T1,t−1L2∣)12, (33) where\nL1 = ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎣ 0 1 1 0 0 −1 −1 0 ⎤⎥⎥⎥⎥⎥⎥⎥⎥⎦ ,L2 = ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎣ 0 1 0 −1 1 0 −1 0 ⎤⎥⎥⎥⎥⎥⎥⎥⎥⎦ , (34)\nFor every possible edge pointing to f1,t, the corresponding CPbD is given by\n[Dp(f1,t;f1,t−1∣f2,t−1),Dp(f1,t;f2,t−1∣f1,t−1)] =(∣logBT1,t−1L∣ + ∣log B̄T1,t−1L∣)S (35)\nwhere L = [L1,L2], (36) S = I ⊗ 12. (37)\nSimilarly as the above, it is easy to check that L and S obtained for f1,t is still suitable for f2,t. Thus, when M = 2, the CPbD of the C-clique at t − 1 is given by\nDt−1 =(∣logBTt−1L∣ + ∣log B̄Tt−1L∣)S (38) where\nDt−1 = [Dp(f1,t;f1,t−1∣f2,t−1) Dp(f1,t;f2,t−1∣f1,t−1) Dp(f2,t;f1,t−1∣f2,t−1) Dp(f2,t;f2,t−1∣f1,t−1)] ,\n(39) Bt−1 = [B1,t−1,B2,t−1], (40)\nL = [L1,L2], (41) S = I ⊗ 12. (42)\nIn our system model, the behavior of each primary station follows a same birth-death process, which can be formulated by a Bayesian graph containing two nodes. It means that if we only study the statistical pattern of one base station, the related Bayesian structure consists of one edges and two nodes which means T = 1. Therefore, the value of Dp(f1,t;f1,t−1∣f2,t−1) and Dp(f2,t;f2,t−1∣f1,t−1) are same and keep constant over every C-clique of a whole BN graph which expresses the pattern of M primary base stations. On the other hand, the limited number of observations leads that the empirical Dp(f1,t;f1,t−1∣f2,t−1) and Dp(f2,t;f2,t−1∣f1,t−1) are different. Hence, we normalize the entries in Dt−1 normalized as follows for reflecting the spatial relationship clearly.\nD̄t−1 =Λ−1(Dt−1)Dt−1 = ⎡⎢⎢⎢⎢⎣ Dp(f1,t;f1,t−1 ∣f2,t−1) Dp(f1,t;f1,t−1 ∣f2,t−1) Dp(f1,t;f2,t−1 ∣f1,t−1) Dp(f1,t;f1,t−1 ∣f2,t−1) Dp(f2,t;f1,t−1 ∣f2,t−1) Dp(f2,t;f2,t−1 ∣f1,t−1) Dp(f2,t;f2,t−1 ∣f1,t−1) Dp(f2,t;f2,t−1 ∣f1,t−1) ⎤⎥⎥⎥⎥⎦ , (43) where Λ(Dt−1) denotes the diagonal matrix with the same main diagonal as Dt−1.\nConsequently, for an arbitrary M , the normalized CPbD of C-clique at t − 1 is given by D̄t−1 =Λ−1(Dt−1)Dt−1, (44) where\nDt−1 =(∣logBTt−1L∣ + ∣log B̄Tt−1L∣)S, (45) Bt−1 = [B1,t−1,B2,t−1,⋯, ,BM,t−1], (46)\nS = I ⊗ 1M . (47)\nIt is worth emphasizing that matrix L does not depend on the number of C-cliques, but only on the number of variables in one C-clique. Hence, matrix L only needs to be computed once for a given M . It could be computed offline prior to the online CBN learning. This paper does not study the optimal approach to obtain L; alternatively, we provide a feasible method to arrive at L, given in Table I."
    }, {
      "heading" : "D. Efficient C-clique Learning Procedure",
      "text" : "Based on the previously discussed BbCPT and CPbD, the procedure of learning a C-clique is summarized as:\nInitialization of C , L, and S; 1) Obtain the conditional probability table by (22)-(25); 2) Obtain the CPbD of all edges by (44)-(47).\n8 As we have emphasized before, C and L are only required to calculate once according to the number of variables in a C-clique, which means that they could be calculated and stored priorly. When regarding C and L as prior knowledge, the above procedure shows that our learning algorithm is just a sequence of closed-form function evaluations and has a simpler dependence check function compared with that in [12]. Consequently, the proposed learning method has lower computational complexity and is capable of adapting to different numbers of variables. Note that the learned CPbD is a soft measure on the variable dependence, which will be useful in the next section to derive the general CBN learning algorithm. We could also now apply simple binary threshold over CPbD to decide the existence of the edges in the learned clique, as in [12]."
    }, {
      "heading" : "V. LEARNING CBN WITH UNKNOWN T",
      "text" : "The previous section introduces the CBN learning algorithm with a given T . Sometimes, the value of T is not known a priori. For example. we do not know the exact value of T in our CR system model. The uncertainty of T implies that the number of variables in the temporal domain is unknown in the CBN model. Consequently, after a number of observations being collected, we do not exactly know which temporal variables generated the collected observations. In the structure learning literature, observations are usually assumed already associated with different variables correctly. As such, the uncertainty of T is a challenging issue, less studied in the BN community. For this problem, we propose a heuristic but efficient solution as follows.\nTo estimate T , we first formulate this uncertainty issue into a mathematical problem. Let Dp(x) = ∑i,k∈MDp(fi,t+x;fk,t∣f1∶M,t/fk,t), which is the sum of the dependence measures over all edges between the variables at times t and t + x. In addition, let Tp, Ts, and T ∗ denote the period of CPbD, the interval between two sequential observations of the same variable, and the minimal value of period T , respectively. Note that all the periods in the set{T ∣T = kT ∗, k ∈ Z} are feasible for the BN structure (here Z denotes the positive integer set). But a longer period implies higher computational complexity. Hence, we pick the minimal possible value T ∗ as the optimal choice. For T ∗, it not only needs to ensure that the observations are statistically periodic along the temporal variables but also has to guarantee that the observations of a variable are statistically independent over Ts. Accordingly, when N → ∞, the optimal period T ∗ could be obtained by solving\nT ∗ = argmin t\nst. t = kTp,where k ∈ Z,\nt ≥ T ∗s − 1,\n(48)\nwhere T ∗s =min{Ts∣Dp(Ts) = 0}."
    }, {
      "heading" : "A. Learning T ∗s",
      "text" : "Specifically, the period T should ensure that the observations of any variable fi,t are statistically independent over\nTs. Regarding our system, the temporal correlation of two observations decreases as their interval increases. Obviously, it is better to set a long enough period Ts to ensure the statistically independent condition. However, since the number of available observations N is limited in practice, we choose to estimate Ts and Tp empirically. It is worth noting that: i) it is impossible to deduce the true probability or distribution of a variable from limited observations, and ii) the empirical statistical information is widely used in real system analysis. From the perspective of both statistics and engineering, the empirical probability is valuable and useful as long as it is close to the true one. In addition, it is impossible to find a feasible value of Ts to ensure Dp(Ts) = 0 using empirical probabilities. Hence, given the finite observations, we propose to obtain the empirical Ts by finding the first valley value of the CPbD between f1∶M,t and f1∶M,t+Ts , denoted by Dt,t+Ts . Here f1∶M,t is a short form for f1,t,⋯, fM,t. The pseudo-code of the proposed algorithm denoted by Ts is given in Table II, where ∣∣Dp∣∣1\nTs is the Dt,t+Ts value averaged over Ts and edges.\nNext, we give our algorithm that obtains Tp by exploring the regularity of CPbD."
    }, {
      "heading" : "B. Learning Tp",
      "text" : "As discussed earlier, the CPbD has the capability of reflecting the dependence between variables. Hence, when finding the correct value of Tp, the CPbD should demonstrate a certain regularity that could help with learning Tp. This subsection first shows the existence of CPbD regularity mathematically. We need to emphasize that the observations are assumed to have certain correlation in the user and temporal domains, though we do not know the statistical characteristics of such correlation, since if all observations are independent, there will be no edges in the BN structure. To ensure that the unknown Tp could be obtained by learning from the observations, we have the following theoretical results.\nFirst, we derive Proposition 1 and Proposition 2 to show that P (fi,t+1 = 1∣fi,t = 1) is regular when considering T as a variable, which means that the calculation of the empirical P (fi,t+1 = 1∣fi,t = 1) has a regular pattern either. Let Tf and Tr denote the sets containing the false and true periods respectively. Obviously, there is T ∶= Tf ∪ Tr. And let Ze ∶= {2k ∶ k is positive integer} and Zo ∶= {2k + 1 ∶\n9 k is positive integer}. Additionally, consider T ′f is a variable similar as Tf .\nProposition 1: When Tf , T ′f ∈ Tf , Tr ∈ Tr∩Zo, there exist: P1.1) P (fi,t+1 = 1∣fi,t = 1)∣T=Tf ≠ P (fi,t+1 = 1∣fi,t = 1)∣T=Tr . P1.2) P (fi,t+1 = 1∣fi,t = 1)∣T=Tf = P (fi,t+1 = 1∣fi,t =\n1)∣T=T ′ f .\nProposition 2: When Tf , T ′f ∈ Tf , Tr ∈ Tr∩Ze, there exist: P2.1) P (fi,t+1 = 1∣fi,t = 1)∣T=Tf ≠ P (fi,t+1 = 1∣fi,t = 1)∣T=Tr . P2.2) P (fi,t+1 = 1∣fi,t = 1)∣T=Tf = P (fi,t+1 = 1∣fi,t =\n1)∣T=T ′ f under the condition that Tf , T ′f ∈ Ze. P2.3) P (fi,t+1 = 1∣fi,t = 1)∣T=Tf = P (fi,t+1 = 1∣fi,t =\n1)∣T=T ′ f under the condition that Tf , T ′f ∈ Zo. P2.4) P (fi,t+1 = 1∣fi,t = 1)∣T=Tf ≠ P (fi,t+1 = 1∣fi,t =\n1)∣T=T ′ f under the condition that Tf ∈ Zo and T ′f ∈ Ze.\nProof: We prove Proposition 1 at first. Let{o1,1, o1,2,⋯, o1,N} denote the N temporal observations of user-1. Then, given the period T , we fold this time series at T . For example,\n● When T = 2, we have o1,1∣T=2 = [o1,1, o1,3, o1,5,⋯]T and o1,2∣T=2 = [o1,2, o1,4, o1,6,⋯]T . ● When T = 3, we have o1,1∣T=3 = [o1,1, o1,4, o1,7,⋯]T , o1,2∣T=3 = [o1,2, o1,5, o1,8,⋯]T , and o1,3∣T=3 =[o1,3, o1,6, o1,9,⋯]T . ● When T = 4, we have o1,1∣T=4 =[o1,1, o1,5, o1,9,⋯]T , o1,2∣T=4 = [o1,2, o1,6, o1,10,⋯]T , o1,3∣T=4 = [o1,3, o1,7, o1,11,⋯]T , and o1,4∣T=4 =[o1,4, o1,8, o1,12,⋯]T . ● When T = 5, there exist o1,1∣T=5 = [o1,1, o1,6, o1,11,⋯]T , o1,2∣T=5 = [o1,2, o1,7, o1,12,⋯]T , o1,3∣T=5 =[o1,3, o1,8, o1,13,⋯]T , o1,4∣T=5 = [o1,4, o1,9, o1,14,⋯]T , and o1,5∣T=5 = [o1,5, o1,10, o1,15,⋯]T . ● When T = 6, there exist o1,1∣T=6 = [o1,1, o1,7, o1,13,⋯]T , o1,2∣T=6 = [o1,2, o1,8, o1,14,⋯]T , o1,3∣T=6 =[o1,3, o1,9, o1,15,⋯]T , o1,4∣T=6 = [o1,4, o1,10, o1,16,⋯]T , o1,5∣T=6 = [o1,5, o1,11, o1,17,⋯]T , and o1,6∣T=6 =[o1,6, o1,12, o1,18,⋯]T .\nAccordingly, when Tr = 5, we have\nP (f1,2 = 1∣f1,1 = 1, Tf = 2, Tr = 5) = lim N→∞ o T 1,1o1,2\noT1,11 ∣ T=Tf=2\n= lim N→∞\n∑4t=1 o T 1,to1,t+1 + o T 1,5S(o1,1) (∑5t=1 oT1,t)1 ∣T=Tr=5, (49)\nwhere S(.) denotes an operation shifting the entries of a vector with a single step circularly,\nP (f1,2 = 1∣f1,1 = 1, Tf = 3, Tr = 5) = lim N→∞ o T 1,1o1,2\noT1,11 ∣ T=Tf=3\n= lim N→∞\n∑4t=1 o T 1,to1,t+1 + o T 1,5S(o1,1) (∑5t=1 oT1,t)1 ∣T=Tr=5, (50)\nP (f1,2 = 1∣f1,1 = 1, Tf = 4, Tr = 5) = lim N→∞ o T 1,1o1,2\noT1,11 ∣ T=Tf=4\n= lim N→∞\n∑4t=1 o T 1,to1,t+1 + o T 1,5S(o1,1) (∑5t=1 oT1,t)1 ∣T=Tr=5, (51)\nP (f1,2 = 1∣f1,1 = 1, Tf = Tr = 5) = lim N→∞ o T 1,1o1,2\noT1,11 ∣ T=Tf=5 . (52)\nFrom (49)-(52), it is easy to see that P (f1,2 = 1∣f1,1 = 1, Tf = 2, Tr = 5) = P (f1,2 = 1∣f1,1 = 1, Tf = 3, Tr = 5) = P (f1,2 = 1∣f1,1 = 1, Tf = 4, Tr = 5) ≠ P (f1,2 = 1∣f1,1 = 1, Tf = Tr = 5). By a similar approach, we show that such a relationship holds for general Tf and Tr (where Tr ∈ Zo) as follows.\n● When T = Tf , there exist o1,1∣T=Tf =[o1,1, o1,1+Tf , o1,1+2Tf ,⋯]T , o1,2∣T=Tf =[o1,2, o1,2+Tf , o1,2+2Tf ,⋯]T , ⋯, and o1,Tf ∣T=Tf =[o1,Tf , o1,2Tf , o1,3Tf ,⋯]T . ● When T = Tr, there exist o1,1∣T=Tr =[o1,1, o1,1+Tr , o1,1+2Tr ,⋯]T , o1,2∣T=Tr =[o1,2, o1,2+Tr , o1,2+2Tr ,⋯]T , ⋯, and o1,Tr ∣T=Tr =[o1,Tr , o1,2Tr , o1,3Tr ,⋯]T .\nAccordingly, we have\no1,tf [n]o1,tf+1[n] =o1,tf+(n−1)Tf o1,tf+1+(n−1)Tf =o1,tr+(l−1)Tro1,tr+1+(l−1)Tr\n= ⎧⎪⎪⎪⎨⎪⎪⎪⎩ o1,tr [l]o1,tr+1[l],when tr ≠ 0 o1,Tr [l]S(o1,1)[l],when tr = 0 , (53)\nfor the numerator, where n and l are indices, tr and tf respectively denote t∣T=Tr and t∣T=Tf , and\ntr = [tf + (n − 1)Tf]/Tr, (54) l = ⌊ tf + (n − 1)Tf\nTr ⌋ + 1. (55)\nOn the other hand, for the denominator, there exists o1,tf [n] = o1,tf+(n−1)Tf = o1,tr+(l−1)Tr = o1,tr[l]. Given Tr ∈ Zo, the modulo operation in (54) leads to tr ∈{0,1,2,⋯, Tr − 1} when n increases. Hence, we could obtain P (f1,t+1 = 1∣f1,t = 1, Tf , Tr)\n= lim N→∞ o T 1,to1,t+1 oT1,t1 ∣ T=Tf\n= lim N→∞\n∑Tr−1t=1 o T 1,to1,t+1 + o T 1,Tr S(o1,1) (∑Trt=1 oT1,t)1 ∣T=Tr ,\n(56)\nand then could extend it to the following result,\nP (fi,t+1 = 1∣fi,t = 1, Tf , Tr) = lim N→∞ o T i,toi,t+1 oTi,t1 ∣ T=Tf\n= lim N→∞\n∑Tr−1t=1 o T i,toi,t+1 + o T i,Tr S(oi,1) (∑Trt=1 oTi,t)1 ∣T=Tr .\n(57)\n10\nWhen jointly reviewing the first and second equations in (57), it is obvious that ∀Tf ∈ Tf , P (fi,t+1 = 1∣fi,t = 1, Tf , Tr) keeps constant. In other words, ∀Tf , T ′f ∈ Tf , P (fi,t+1 = 1∣fi,t = 1, Tf , Tr) and P (fi,t+1 = 1∣fi,t = 1, T ′f , Tr) have the same value. Hence, P1.1) is proved. On the other hand, we also could see that P (fi,t+1 = 1∣fi,t = 1, T = Tf) and P (fi,t+1 = 1∣fi,t = 1, T = Tr) are different. Consequently, Proposition 1 is proved. Next, we prove Proposition 2.\nWhen Tr = 6, we have\nP (f1,2 = 1∣f1,1 = 1, Tf = 2, Tr = 6) = lim N→∞ o T 1,1o1,2\noT1,11 ∣ T=Tf=2\n= lim N→∞\n∑t∈{1,3,5} o T 1,to1,t+1 (∑t={1,3,5} oT1,t)1 ∣T=Tr=6, (58)\nP (f1,2 = 1∣f1,1 = 1, Tf = 3, Tr = 6) = lim N→∞ o T 1,1o1,2\noT1,11 ∣ T=Tf=3\n= lim N→∞\n∑5t=1 o T 1,to1,t+1 + o T 1,6S(o1,1) (∑6t=1 oT1,t)1 ∣T=Tr=6, (59)\nP (f1,2 = 1∣f1,1 = 1, Tf = 4, Tr = 6) = lim N→∞ o T 1,1o1,2\noT1,11 ∣ T=Tf=4\n= lim N→∞\n∑t∈{1,3,5} o T 1,to1,t+1 (∑t={1,3,5} oT1,t)1 ∣T=Tr=6, (60)\nP (f1,2 = 1∣f1,1 = 1, Tf = 5, Tr = 6) = lim N→∞ o T 1,1o1,2\noT1,11 ∣ T=Tf=5\n= lim N→∞\n∑5t=1 o T 1,to1,t+1 + o T 1,6S(o1,1) (∑6t=1 oT1,t)1 ∣T=Tr=6. (61)\nFrom (58)-(61), it is obvious that P (f1,2 = 1∣f1,1 = 1, Tf = 2, Tr = 6) = P (f1,2 = 1∣f1,1 = 1, Tf = 4, Tr = 6) ≠ P (f1,2 = 1∣f1,1 = 1, Tf = 3, Tr = 6) = P (f1,2 = 1∣f1,1 = 1, Tf = 5, Tr = 6) ≠ P (f1,2 = 1∣f1,1 = 1, Tf = Tr = 6). By a similar approach, we show that such a relationship holds for general Tf and Tr (where Tr ∈ Ze) as follows.\nWhen Tr, Tf ∈ Ze and tf ∈ Zo, the modulo operation in (54) leads to tr ∈ {1,3,⋯, Tr − 1}; when Tr, Tf ∈ Ze and tf ∈ Ze, the modulo operation in (54) leads to tr ∈ {2,4,⋯, Tr}. Hence, we have\nP (fi,t+1 = 1∣fi,t = 1, Tf , Tr) = lim N→∞ o T i,toi,t+1 oTi,t1 ∣ T=Tf\n= ⎧⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎩ limN→∞ ∑t∈{1,3,⋯,Tr−1} o T i,toi,t+1 (∑t∈{1,3,⋯,Tr−1} oTi,t)1 ∣ T=Tr ,when t∣T=Tf ∈ Zo limN→∞ ∑t∈{2,4,⋯,Tr} o T i,toi,t+1 (∑t∈{2,4,⋯,Tr} oTi,t)1 ∣ T=Tr ,when t∣T=Tf ∈ Ze .\n(62)\nOn the other hand, when Tr ∈ Ze and Tf ∈ Zo, the modulo operation in (54) leads to the same results as (57). Thus, by a similar approach as the proof in the first paragraph below (57), Proposition 2 is proved.\nProposition 1 and Proposition 2 show that there are some regular patterns as the value of T changes. In addition, such patterns are different when the true period is odd or even. Based on the derived two propositions, we have the following two corollaries for the CPbD.\nCorollary 1: When Tf , T ′f ∈ Tf , Tr ∈ Tr ∩Zo, there exist: C1.1) D(fi,t+1;fi,t)∣T=Tf ≠D(fi,t+1;fi,t)∣T=Tr . C1.2) D(fi,t+1;fi,t)∣T=Tf =D(fi,t+1;fi,t)∣T=T ′f . Corollary 2: When Tf , T ′f ∈ Tf , Tr ∈ Tr ∩Ze, there exist: C2.1) D(fi,t+1;fi,t)∣T=Tf ≠D(fi,t+1;fi,t)∣T=Tr . C2.2) D(fi,t+1;fi,t)∣T=Tf = D(fi,t+1;fi,t)∣T=T ′f under the condition that Tf and T ′f are odd. C2.3) D(fi,t+1;fi,t)∣T=Tf = D(fi,t+1;fi,t)∣T=T ′f under the condition that Tf and T ′f are even. C2.4) D(fi,t+1;fi,t)∣T=Tf ≠ D(fi,t+1;fi,t)∣T=T ′f under the\ncondition that Tf and T ′f are even and odd respectively.\nProof: Corollary 1 and Corollary 2 could be proved by jointly applying (29), Proposition 1, and Proposition 2. The detailed proof is omitted here.\nCorollary 1 and Corollary 2 show that the CPbD of a CBN given Tr is different from that given Tf . But the CPbD has the same value for all Tf ∈ Tf , and has the same value for all Tr ∈ Tr. Hence, the CPbD of a CBN is periodical when T monotone increasing. Based on such results, we propose to use the first peak value of the fast Fourier transformation (FFT) of the averaged CPbD between f1∶M,t and f1∶M,t+Ts to find the empirical Tp. As well-known, the Fourier analysis methodology is a feasible approach to find the periodical information in broad applications. Let nextpow2(x) be a function that returns y, which is the maximal integer satisfying x − 2y ≥ 0, and fft(x) denote the FFT of x. The proposed algorithm denoted by Tp is given in Table III."
    }, {
      "heading" : "C. Overall CBN Learning Scheme",
      "text" : "Based on (48) and the algorithms Ts and Tp, the complete learning scheme for the CBN is given in Table IV, by which\n11\nwe could efficiently obtain the CBN with measured edges and the related conditional probability table."
    }, {
      "heading" : "VI. SIMULATION RESULTS",
      "text" : "In this section, we first give the comparison of computational complexity between the proposed CPbD algorithm and the conventional one [12]. Then we present the learning outcomes."
    }, {
      "heading" : "A. Comparison of Computational Complexity",
      "text" : "The computational complexity is evaluated by running the Matlab codes of the proposed algorithm and the conventional one in the same desktop. The corresponding running time is recorded to reflect the real computational cost. In this subsection, the learning complexity is evaluated when considering the CBN structure that only consists of one C-clique, since the learning time of the whole CBN structure is linear to that of a C-clique. Additionally, the impacts of observation numbers and variable numbers are considered for a comprehensive comparison. In our simulations, the observations are generated based on M binomial distributions 5. The simulated results are presented in Fig. 3 and Fig. 4, where the computational cost is the total running time for obtaining the conditional probability table and the measurement of every edge. It is obvious that the computational cost of the proposed algorithm is much less than the conventional one, especially as the numbers of base stations and observations increase. In particular, when M = 12 and N = 36000, the conventional algorithm demands for 502.2 seconds to learn. It implies that the conventional algorithm is not suitable for online learning even when the number of variables is not large. In Fig. 4, we draw the ratio between the “Conv” and “Prop” costs over M , where it is obvious that the proposed scheme is more efficient. According to Fig. 4, the proposed algorithm only requires 1/38 of the time cost as the conventional one when M = 12 and N = 36000. In addition, note that the proposed algorithm could adapt to any number of variables, while the conventional one cannot."
    }, {
      "heading" : "B. Learning Outcomes",
      "text" : "The simulation scenario is illustrated by Fig. 1, where mobile users move from the right to the left and access one of the three primary base stations according to their locations. The base stations are respectively labeled #1, #2, and #3 from right to left. The simulation configuration is as follows:\n5Actually, the distributions of observations do not impact the computational cost of learning.\nThe arrival of mobile users follows a Poisson distribution with mean λs = 1; the length of the road is 600 meters, covered by the three cells; the data traffic arrival rate at each user follows a Poisson distribution with mean λt = 0.002;6 the service time of data traffic follows an exponential distribution with a mean of 2 seconds; and spectrum sensing is performed once per second.\nIn this paper, we are interested in the relationship among base station activities, which is introduced by the user movement. Such statistical correlation is useful for CR operation but has not been well studied in the past. To show the learning results, the data of observation is collected in three different cases corresponding to the uniformly distributed user speed regions as [43.2 72], [72 129.6], and [100.8 158.4]\n6The value of λt is set small since we have to ensure that the primary base stations have some idle slots.\n12\nkilometers per hour, respectively. The learning results of T (empirical period of CBN structure) are presented in Table V, where the unit of T is second. It is obvious that the period T is similar under different numbers of observations, which shows that the proposed algorithm could work robustly. According to the learning results, it could be concluded that the statistical relationship of the observed three base stations are mainly affected by the user speed, and the period is inversely proportional to the user speed. Since the case of N = 3600 × 10 has the most number of observations, it could show the most trustable results when compared with the other two. Interestingly, we observe that T × v ≈ 600\n2 from our\nsimulation results, where v is the averaged speed over the speed region. Actually, such relationship is reasonable, since 600\n2 is the mean of the distances between the mobile users and\nthe left end of the simulated road.\nNext, we show the learning results of CPbD. For clarity of expression, we only show the results in the case of[100.8 158.4] and N = 3600 × 10. When T = 8, the learned statistical pattern D̄1∶7 is given as follows,\nD̄1 = RRRRRRRRRRRRRRRR 1.0000 0.3818 0.0917 0.0844 1.0000 0.3592 0.2058 0.1779 1.0000 RRRRRRRRRRRRRRRR , (63)\nD̄2 = RRRRRRRRRRRRRRRR 1.0000 0.5724 0.2733 0.3201 1.0000 0.4926 0.2173 0.2721 1.0000 RRRRRRRRRRRRRRRR , (64)\nD̄3 = RRRRRRRRRRRRRRRR 1.0000 0.5041 0.1137 0.3084 1.0000 0.5247 0.2524 0.2279 1.0000 RRRRRRRRRRRRRRRR , (65)\nD̄4 = RRRRRRRRRRRRRRRR 1.0000 0.4337 0.0345 0.0527 1.0000 0.3740 0.2376 0.2636 1.0000 RRRRRRRRRRRRRRRR , (66)\nD̄5 = RRRRRRRRRRRRRRRR 1.0000 0.4379 0.1431 0.2354 1.0000 0.6035 0.0716 0.0659 1.0000 RRRRRRRRRRRRRRRR , (67)\nD̄6 = RRRRRRRRRRRRRRRR 1.0000 0.5247 0.3372 0.2607 1.0000 0.5282 0.2976 0.2644 1.0000 RRRRRRRRRRRRRRRR , (68)\nD̄7 = RRRRRRRRRRRRRRRR 1.0000 0.5490 0.2898 0.2796 1.0000 0.5051 0.0760 0.1120 1.0000 RRRRRRRRRRRRRRRR . (69)\nApparently, the entries in the upper triangular part of D̄ is generally larger than the entries in the lower triangular part, which implies that the status of base stations #2 and #3 is heavily impacted by base station #1, but the reverse does not hold. In addition, we use the learned CPbD D̄1∶7 to draw a CBN shown in Fig. 5, where the line width is proportional to the magnitude of dependence and we see that the trend of dependence between base station #1 and base station #3 increases as t increases during a period. It is due to the fact that the users served by base station #1 at the beginning of a period will arrive in the range of base station #3 at the end of the period. Note that if we add a binary thresholding to make decisions on the existence of edges, which is used for binary testing of each edge in [12], the learning outcomes will be similar to those in [12]. The estimated conditional probability table B1∶7 is also reported in (70)-(76). Based on the learned conditional probability table, we could generate the joint distribution of the on/off behaviors of the three primary base stations and predict the status of network for the future, to serve broader CR applications.\nIn brief, the simulated outcomes show that the proposed CBN structure learning algorithm not only can quantify the strength of dependence but also correctly reflect the unidirectional statistical pattern caused by the mobile user’s one-way movement, indicating that our learning scheme can efficiently learn the CBN structure to represent the true statistical behavior of the underlying network."
    }, {
      "heading" : "VII. CONCLUSION",
      "text" : "In this paper, we proposed a learning scheme to obtain the statistical pattern of a primary network’s activity in both spatial and temporal domains simultaneously. The proposed scheme incurs significantly lower computational complexity when compared with the traditional ones. Additionally, it is capable of learning the statistical period of the CBN structure. By simulations, we show that the learning results also correctly reflect certain network behavior beyond spectrum usage, which could be useful for broader CR network control applications.\n13\nB1 = RRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRR 0.6664 0.6664 0.6664 0.7953 0.7342 0.0010 0.7403 0.1119 0.8881 0.7649 0.0139 0.0139 0.1674 0.7329 0.7329 0.1995 0.7495 0.0208 0.0176 0.2048 0.7495 0.0115 0.0109 0.0232 RRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRR , (70)\nB2 = RRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRR 0.9990 0.9990 0.9990 0.8688 0.8471 0.0227 0.7994 0.2006 0.6996 0.7438 0.0187 0.0212 0.2672 0.7661 0.8992 0.2024 0.7266 0.0128 0.0089 0.1778 0.7790 0.0142 0.0118 0.0177 RRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRR , (71)\nB3 = RRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRR 0.5001 0.9990 0.9990 0.7994 0.8357 0.0192 0.7935 0.2652 0.7935 0.7687 0.0164 0.0215 0.2075 0.7925 0.7581 0.2000 0.7558 0.0136 0.0090 0.1919 0.7326 0.0112 0.0124 0.0170 RRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRR , (72)\nB4 = RRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRR 0.7994 0.7994 0.9990 0.7772 0.7957 0.0195 0.7415 0.1942 0.7415 0.7553 0.0138 0.0241 0.2339 0.7329 0.7994 0.2252 0.7533 0.0286 0.0094 0.2181 0.7359 0.0126 0.0126 0.0228 RRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRR , (73)\nB5 = RRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRR 0.5001 0.9990 0.7496 0.7772 0.7033 0.0380 0.8372 0.2438 0.7023 0.7815 0.0138 0.0215 0.1436 0.8921 0.7139 0.2030 0.7397 0.0161 0.0168 0.2140 0.7505 0.0136 0.0139 0.0204 RRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRR , (74)\nB6 = RRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRR 0.9990 0.9990 0.9990 0.8254 0.7387 0.0227 0.6919 0.1290 0.8199 0.7568 0.0131 0.0083 0.1570 0.8431 0.6559 0.1948 0.7329 0.0241 0.0372 0.2062 0.7173 0.0113 0.0137 0.0196 RRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRR , (75)\nB7 = RRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRR 0.9990 0.9990 0.5002 0.7495 0.7703 0.0218 0.7709 0.1436 0.7994 0.7599 0.0132 0.0156 0.2313 0.7687 0.6536 0.1843 0.7866 0.0214 0.0092 0.2055 0.7250 0.0119 0.0128 0.0219 RRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRR . (76)"
    } ],
    "references" : [ {
      "title" : "Cognitive radio: making software radios more personal,",
      "author" : [ "J. Mitola", "G.Q.J. Maguire" ],
      "venue" : "Personal Communications, IEEE,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1999
    }, {
      "title" : "Practical issues for spectrum management with cognitive radios,",
      "author" : [ "S. Dudley", "W. Headley", "M. Lichtman", "E. Imana", "X. Ma", "M. Abdelbar", "A. Padaki", "A. Ullah", "M. Sohul", "T. Yang", "J. Reed" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "Signal processing in cognitive radio,",
      "author" : [ "J. Ma", "G. Li", "B.-H. Juang" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2009
    }, {
      "title" : "Spatial false alarm in cognitive radio network,",
      "author" : [ "W. Han", "J. Li", "Z. Li", "J. Si", "Y. Zhang" ],
      "venue" : "Signal Processing, IEEE Transactions on,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2013
    }, {
      "title" : "Optimal linear cooperation for spectrum sensing in cognitive radio networks,",
      "author" : [ "Z. Quan", "S. Cui", "A. Sayed" ],
      "venue" : "Selected Topics in Signal Processing, IEEE Journal of,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2008
    }, {
      "title" : "S",
      "author" : [ "K. Murphy" ],
      "venue" : "Mian et al., “Modelling gene expression data using dynamic bayesian networks,” Technical report, Computer Science Division, University of California, Berkeley, CA, Tech. Rep.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Review: Learning bayesian networks: Approaches and issues,",
      "author" : [ "R. Daly", "Q. Shen", "S. Aitken" ],
      "venue" : "Knowl. Eng. Rev.,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2011
    }, {
      "title" : "Dynamic bayesian networks: A state of the art,” University of Twente, Centre for Telematics and Information",
      "author" : [ "V. Mihajlovic", "M. Petkovic" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2001
    }, {
      "title" : "Machine learning: a probabilistic perspective",
      "author" : [ "K.P. Murphy" ],
      "venue" : "MIT press",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Evaluating the causal explanatory value of bayesian network structure learning algorithms,",
      "author" : [ "P. Shaughnessy", "G. Livingston" ],
      "venue" : "Research Paper, Department of Computer Science, University of Massachusetts Lowell.,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2005
    }, {
      "title" : "Large-sample learning of bayesian networks is np-hard,",
      "author" : [ "D.M. Chickering", "D. Heckerman", "C. Meek" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2004
    }, {
      "title" : "Learning bayesian networks from data: An information-theory based approach,",
      "author" : [ "J. Cheng", "R. Greiner", "J. Kelly", "D. Bell", "W. Liu" ],
      "venue" : "Artif. Intell.,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2002
    }, {
      "title" : "Data networks",
      "author" : [ "D.P. Bertsekas", "R.G. Gallager", "P. Humblet" ],
      "venue" : "2nd ed. Prentice-Hall, Englewood Cliffs, NJ",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "Efficient soft decision fusion rule in cooperative spectrum sensing,",
      "author" : [ "W. Han", "J. Li", "Z. Li", "J. Si", "Y. Zhang" ],
      "venue" : "Signal Processing, IEEE Transactions on,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Since the terminology was coined in 1999 [1], cognitive radio (CR) has been developed for more than fifteen years, which has drawn attention from both academic and industrial communities since it is intended to enable smart use of the scarce spectrum resource with the initial objective of maximizing spectrum utilization.",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 1,
      "context" : "To achieve such new objectives, the statistical knowledge on the primary network status becomes necessary [2] for resource management and system control, which gets us",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 2,
      "context" : "This will go beyond most of the existing CR sensing literature, which usually focus on detecting the presence of primary users only [3]–[5].",
      "startOffset" : 132,
      "endOffset" : 135
    }, {
      "referenceID" : 4,
      "context" : "This will go beyond most of the existing CR sensing literature, which usually focus on detecting the presence of primary users only [3]–[5].",
      "startOffset" : 136,
      "endOffset" : 139
    }, {
      "referenceID" : 5,
      "context" : "In this paper, we introduce the Bayesian network (BN) [6] structure learning method to obtain the statistical primary networking pattern, via observing the on/off status of primary base stations.",
      "startOffset" : 54,
      "endOffset" : 57
    }, {
      "referenceID" : 6,
      "context" : "The Bayesian model has been well known in the field of artificial intelligence (AI) [7].",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 7,
      "context" : "When considering the probability and uncertainty, BN is a distinct technique for modeling the complex interaction among real world facts [8].",
      "startOffset" : 137,
      "endOffset" : 140
    }, {
      "referenceID" : 6,
      "context" : "However, the associated computational complexity is high since it needs to evaluate the dependence between each pair of variables in a target system and compute the corresponding conditional probability table1 [7], which becomes the major drawback in applying BN structure learning.",
      "startOffset" : 210,
      "endOffset" : 213
    }, {
      "referenceID" : 9,
      "context" : "One is to use heuristic searching to construct a probable model and then evaluate it by a scoring function [10].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 10,
      "context" : "The score-based approach of learning BNs has been proven as a NP-hard problem [11].",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 11,
      "context" : "In [12], the authors show a much more efficient approach to learn the ordered BN by using mutual information to check the dependence of any possible pairs of nodes.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 8,
      "context" : "1In statistics, the conditional probability table is defined for a set of discrete random variables to quantify the marginal probability of a single variable with respect to the others [9].",
      "startOffset" : 185,
      "endOffset" : 188
    }, {
      "referenceID" : 11,
      "context" : "With this, our learning algorithm not only has the same computational overhead as that in [12], but can also dynamically adapt to different numbers of variables.",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 10,
      "context" : "The score-based approach for learning Bayesian networks has been shown NP-hard [11], which is a key challenge in the learning community.",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 11,
      "context" : "Recently, a relatively efficient way to learn an ordered BN is given in [12] by using the conditional mutual information to check the dependence between any possible pair of nodes.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 11,
      "context" : "Moreover, in [12], the conditional mutual information checking is performed for every combination of all possible parent nodes of X .",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 12,
      "context" : "In other words, our system model has the first-order Markov property, which has been adopted previously [13].",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 12,
      "context" : "In [13], it is shown that a wireless communication network could be represented by a Markov state transition system.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 13,
      "context" : "dependence [14].",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 11,
      "context" : "It has been shown [12] that the mutual information check based approach leads to the desired efficiency; but it cannot handle a dynamic number of variables.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : "Accordingly, for a given x, we could use (12) defined below to generate a corresponding vector cx leading to c = cx[1] × cx[2] ×⋯× cx[M], with cx[i] = ⌊ x 2i−1 ⌋ /2 (12)",
      "startOffset" : 115,
      "endOffset" : 118
    }, {
      "referenceID" : 1,
      "context" : "Accordingly, for a given x, we could use (12) defined below to generate a corresponding vector cx leading to c = cx[1] × cx[2] ×⋯× cx[M], with cx[i] = ⌊ x 2i−1 ⌋ /2 (12)",
      "startOffset" : 123,
      "endOffset" : 126
    }, {
      "referenceID" : 0,
      "context" : "For example, when M = 2 and x = 2, c2 = [1 0], which means c = 10.",
      "startOffset" : 40,
      "endOffset" : 45
    }, {
      "referenceID" : 1,
      "context" : "1) for m = 1 ∶ 1 ∶ M 2) e1 = 02m×1, e1[2,1] = −1, e1[2/2,1] = 1, 3) generate a circulant matrix e based on e 1 , each row is a backward shifting of e 1 , 4) l1 = e[1 ∶ m, ∶] , 5) for n = 2 ∶ 1 ∶ m 6) ln = I2×2 ⊗Ln−1, Ln−1 = ln−1, 7) end 8) end 9) L = [L1,L2,⋯,LM ].",
      "startOffset" : 38,
      "endOffset" : 43
    }, {
      "referenceID" : 0,
      "context" : "1) for m = 1 ∶ 1 ∶ M 2) e1 = 02m×1, e1[2,1] = −1, e1[2/2,1] = 1, 3) generate a circulant matrix e based on e 1 , each row is a backward shifting of e 1 , 4) l1 = e[1 ∶ m, ∶] , 5) for n = 2 ∶ 1 ∶ m 6) ln = I2×2 ⊗Ln−1, Ln−1 = ln−1, 7) end 8) end 9) L = [L1,L2,⋯,LM ].",
      "startOffset" : 38,
      "endOffset" : 43
    }, {
      "referenceID" : 11,
      "context" : "When regarding C and L as prior knowledge, the above procedure shows that our learning algorithm is just a sequence of closed-form function evaluations and has a simpler dependence check function compared with that in [12].",
      "startOffset" : 218,
      "endOffset" : 222
    }, {
      "referenceID" : 11,
      "context" : "We could also now apply simple binary threshold over CPbD to decide the existence of the edges in the learned clique, as in [12].",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 11,
      "context" : "In this section, we first give the comparison of computational complexity between the proposed CPbD algorithm and the conventional one [12].",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 11,
      "context" : "“Prop” and “Conv” are abbreviations for our proposed scheme and the conventional one proposed in [12], respectively.",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 11,
      "context" : "Note that if we add a binary thresholding to make decisions on the existence of edges, which is used for binary testing of each edge in [12], the learning outcomes will be similar to those in [12].",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 11,
      "context" : "Note that if we add a binary thresholding to make decisions on the existence of edges, which is used for binary testing of each edge in [12], the learning outcomes will be similar to those in [12].",
      "startOffset" : 192,
      "endOffset" : 196
    } ],
    "year" : 2015,
    "abstractText" : "In cognitive radio (CR) technology, the trend of sensing is no longer to only detect the presence of active primary users. A large number of applications demand for more comprehensive knowledge on primary user behaviors in spatial, temporal, and frequency domains. To satisfy such requirements, we study the statistical relationship among primary users by introducing a Bayesian network (BN) based framework. How to learn such a BN structure is a long standing issue, not fully understood even in the statistical learning community. Besides, another key problem in this learning scenario is that the CR has to identify how many variables are in the BN, which is usually considered as prior knowledge in statistical learning applications. To solve such two issues simultaneously, this paper proposes a BN structure learning scheme consisting of an efficient structure learning algorithm and a blind variable identification scheme. The proposed approach incurs significantly lower computational complexity compared with previous ones, and is capable of determining the structure without assuming much prior knowledge about variables. With this result, cognitive users could efficiently understand the statistical pattern of primary networks, such that more efficient cognitive protocols could be designed across different network layers.",
    "creator" : "LaTeX with hyperref package"
  }
}