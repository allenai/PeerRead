{
  "name" : "1703.00420.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Virtual-to-real Deep Reinforcement Learning: Continuous Control of Mobile Robots for Mapless Navigation",
    "authors" : [ "Lei Tai", "Ming Liu" ],
    "emails" : [ "lei.tai@my.cityu.edu.hk", "eelium@ust.hk", "giupaolo@student.ethz.ch" ],
    "sections" : [ {
      "heading" : null,
      "text" : "I. INTRODUCTION"
    }, {
      "heading" : "A. Motivation",
      "text" : "1) Deep Reinforcement Learning in mobile robots: Deep Reinforcement Learning (deep-RL) methods achieve great success in many tasks including video games [1] and simulation control agents [2]. The applications of deep reinforcement learning in robotics are mostly limited in manipulation [3] where the workspace is fully observable and stable. In terms of mobile robots, deep-RL methods normally sampled the action from a discrete space to simplify the problem [4], [5], [6]. Thus, in this paper, we focus on the navigation problem of nonholonomic mobile robots with continuous control of deep-RL, which is the essential ability for the most widely used robot.\n2) Mapless navigation: Motion planning, as an fundamental function for mobile robots, aims at navigating robots to the desired target from the current position without colliding with obstacles in the environment. For mobile nonholonomic ground robots, traditional methods, like simultaneous localization and mapping (SLAM), handle this\n∗This work was supported by the Research Grant Council of Hong Kong SAR Government, China, under project No. 16206014 and No. 16212815; National Natural Science Foundation of China No. 6140021318, awarded to Prof. Ming Liu\n1Lei Tai is with Department of MBE, City University of Hong Kong. lei.tai@my.cityu.edu.hk\n2Lei Tai and Ming Liu are with Department of ECE, the Hong Kong University of Science and Technology. eelium@ust.hk\n3Giuseppe Paolo is with Department of Mechanical and Process Engineering, ETH Zurich. giupaolo@student.ethz.ch\nproblem through the prior map of the navigation environment [7] based on laser range findings. There are two less addressed issues for this task: (1) the time-consuming building and updating of the environment map, and (2) the high dependence on the precise laser sensor for the mapping work and the local cost-map prediction. It is still a challenge to rapidly generate appropriate navigation behaviors for mobile robots without a map and based on low-dimensional range information.\nAiming to achieve the navigation behavior directly from the raw sensor observation and the target position, we present a learning-based mapless motion planner. In virtual environments, a nonholonomic differential drive robot was trained to learn how to arrive at the target position with obstacle avoidance through asynchronous deep reinforcement learning [8]. Through effective definition of the reward function, the deep-RL model motivated the planner to output appropriate continuous control commands based on the instant observation and the target position with respect to the mobile robot coordinate frame. For a map-based motion planning, data preprocessing, path calculation, map updating and object detection have to be accomplished in real time, then the navigation planner can output the moving commands for the actuators. Compared with a map-based motion planner, the model trained through deep reinforcement learning does not need a global map as a reference and outputs the moving commands from observations directly. We show that the complicated motion planning strategy can be learned from scratch without any demonstrations from human beings. In real-time planning, the model outputs continuous control commands including angular and linear velocities based on ar X iv :1\n70 3.\n00 42\n0v 1\n[ cs\n.R O\n] 1\nM ar\n2 01\n7\nrange information, the previous action and the relative target position, as shown in the right of Fig 1. Our planner also provides a planning solution for other low-cost localization methods like wifi localization [9] and visible-light communication [10].\n3) From virtual to real world: Most of the training of deep reinforcement learning is implemented in a virtual environment naturally because the trial-and-error training process may lead to unexpected damage to the real robot for specific tasks, like obstacle avoidance in our case. The huge difference between the structural simulation environment and the highly complicated real-world environment is the central challenge to transfer the trained model to a real robot directly. In this paper, we only used 10-dimensional range findings as the observation input. This highly abstracted observation was sampled from specific angles of the raw laser range findings based on a trivial distribution, as shown in Fig. 1. The range information from 10 directions was used to represent the observation state. This brings two advantages: the first is the reduction of the gap between the virtual and real environments based on this abstracted observation, and the second is the potential extension to low-cost range sensors with distance information from only 10 directions. We implemented two different training environments to show the influence of the environment conditions on the model behavior. The planner was tested in both virtual and real previously unseen scenarios. Both quantitative and qualitative evaluations for target-driven navigation were accomplished and compared with the traditional map-based method."
    }, {
      "heading" : "B. Contribution",
      "text" : "We list the main contributions of this paper: • A mapless motion planner was proposed by taking\nonly 10-dimensional range findings and target relative information as references. • The motion planner was trained end-to-end from scratch through an asynchronous deep reinforcement learning method without any feature engineering or demonstrations. The planner can output continuous linear and angular velocities directly. • The planner can generalize to a real nonholonomic differential robot platform without any fine-tuning to real-world samples.\nThe rest of this paper is organized as follows. We present related mapless motion planning and deep reinforcement learning works in Section II. The revision of asynchronous deep reinforcement learning is described in Section III. We show the implementation of the approach in Section IV. Training and evaluation details are presented in Section V. Analysis of the experiments is discussed in Section VI. Section VII concludes the paper."
    }, {
      "heading" : "II. RELATED WORK",
      "text" : ""
    }, {
      "heading" : "A. Mapless Motion Planning",
      "text" : "State-of-the-art 2D range sensor based motion planners are mostly map-based, like Move Base1. Precise laser range\n1http://wiki.ros.org/move base\nsensors are often needed to calculate the local costmap. For mapless motion planners, robots perform navigation behavior without prior knowledge of the environment. McCarthy et al. [11] used optical flow to navigate a mobile robot in corridor-like environments. They calculated the optical flow through Lucas and Kanade’s gradient-based method to finish the visual odometry. Guzel et al. [12] built a mapless motion planner based on a homing system. Such kinds of visionbased methods often cost a lot of computation resources. For range-sensor-based methods, Ulrich et al. [13] developed a vector field histogram (VFH) for obstacle avoidance. Kamon et al. [14] used three-dimensional range data to plan a three-dimensional motion. However, their planners are only able to output very naive local motion behavior like wall following. Generally, robust mapless motion planners based on low-dimensional range sensors still deserve attention from researchers."
    }, {
      "heading" : "B. Deep-Learning-based navigation",
      "text" : "Deep learning has recently been very successful in computer vision tasks. Benefiting from the improvement of highperformance computational hardware, deep neural networks show great potential for solving complex estimation problems. For learning-based obstacle avoidance, deep neural networks have been successfully applied on monocular images [15] and depth images [16]. Chen et al. [17] used semantics information extracted from the image by deep neural networks to decide the behavior of the autonomous vehicle. However, their control commands are simply discrete actions like turn left and turn right which may lead to rough navigation behaviors.\nRegarding learning from demonstrations, Pfeiffer et al. [18] used a deep learning model to map the laser range findings and target position to the moving commands. The training data was collected through the navigation behavior output by Move Base. This model navigated the robot in an unseen environment successfully. Like the supervised learning methods mentioned before, their effects are seriously limited by the quality of the labeled training datasets. A time-consuming data collection procedure is also inevitable. Kretzschmar et al. [19] used inverse reinforcement learning methods to make robots interact with humans in a socially compliant way. Such kinds of trained models are highly dependent on the demonstration information."
    }, {
      "heading" : "C. Deep Reinforcement Learning",
      "text" : "Reinforcement learning has been widely applied in robotic tasks [20]. Minh et al. [1] utilized deep neural networks for the function estimation of value-based reinforcement learning which was called deep reinforcement learning or deep Q-network (DQN), and achieved a huge improvement in playing Atari games. The original DQN can only be used in tasks with a discrete action space. To extend it to continuous control, Lillicrap et al. [2] proposed deep deterministic policy gradients (DDPG) to use deep neural networks on the actorcritic reinforcement learning method where both the policy and value of the reinforcement learning were represented\nthrough hierarchical networks. Gu et al. [3] proposed continuous DQN based on the normalized advantage function (NAF). The successes of these deep reinforcement learning methods are mainly attributed to the memory replay strategy in fact. As off-policy reinforcement learning methods, all of the transitions can be used repeatedly, but their effects are seriously influenced by the quantity and the distribution of collected samples. Therefore, asynchronous deep reinforcement learning [8] used separated threads for training and sample collection (A3C). With multiple sample collection threads working in parallel, the training efficiency of the specific policy improved significantly.\nAsynchronous deep reinforcement learning methods have been proven to be effective through a lot of simulated control tasks. In terms of real world tasks, Gu et al. [21] proposed asynchronous NAF and trained the model with real-world samples where a door opening task was accomplished by a real robot arm. Asynchronous deep reinforcement learning was also considered for mobile robot navigation: Mirowski et al. [22] trained a simulated agent to learn navigation in a virtual environment through raw images based on A3C. Loop closure and depth estimation were proposed as well through parallel supervised learning, but the holonomic motion behavior was difficult to transfer to the real environment. Zhu et al. [4] trained an image-based planner where the robot learned to navigate to the referenced image place based on the instant view. However, they defined a discrete action space to simplify the task.\nGenerally, this paper focuses on developing a mapless motion planner based on low-dimensional range findings. We believe that this is the first time a deep reinforcement learning method being applied on the real world continuous control of differential drive mobile robots for navigation."
    }, {
      "heading" : "III. ASYNCHRONOUS DEEP REINFORCEMENT LEARNING",
      "text" : "In this section, we present the extension of asynchronous deep-RL in DDPG [2]. As an off-policy reinforcement learning method, the speed of samples collection is an essential factor for the training result. Asynchronous deep reinforcement learning methods separate the original deep-RL process in a training thread and other sample collecting threads to execute the current policy, which leads the training procedure to be more efficient, especially when the speed of sample collection limits the calculation of the whole framework. It also provides a solution to collect experiences from multiple workers at the same time. It has been successfully applied to many deep-RL methods like Advantage-Actor-Critic [8] and NAF [21].\nWe present the asynchronous version of DDPG in Algorithm 1. Compared with the original DDPG, we separate the sample collecting process to another thread. In the training thread, the weights of the critic-network θQ and the actor-network θµ are updated through the batch of samples collected from the replay buffer at every iteration step. The predicted target of the critic-network is calculated based on the instant reward ri and the discounted estimated Q-value γQ′. Q′ is the output of the target-critic-network with weight\nθQ ′\ngiven by taking the next state st+1 and the optimal action at+1 = µ ′(si+1|θµ ′ ) estimated from the target-actor-network θµ ′\nas input. The actor-network is updated through the policy gradient of the sampled batch of transitions. Both of the two target-networks are updated softly as [2].\nThe sample collection thread is executed in parallel and the action is decided by the actor-network. In the training time, a random process N is added to motivate the exploration of the action space. New transitions are saved to the reply buffer which is shared by the training and sample collecting threads. Asynchronous DDPG (ADDPG) can also be implemented with multiple data collection threads as other asynchronous methods.\nAlgorithm 1 Asynchronous DDPG // training thread Randomly initialize critic-net θQ and actor-net θµ\nInitialize the target-net θQ ′ = θQ and θµ ′ = θµ Initialize replay buffer R for iteration = 1, I do\nSample a minibatch of m transitions from R Calculate the estimated Q-value based on target-net: yi = ri + γQ ′(si+1, µ ′(si+1|θµ ′ )|θQ′ Update the weight of critic-net by minimizing the loss: L = 1N ∑m i (yi −Q(si, ai|θQ))2 Update the weight of actor-net through policy gradient: ∇θJ = 1N ∑m i ∇aQ(s, a|θQ)|s=si,a=µ(si)∇θµ(s|θµ)|si Update the target-net: θQ ′ = τθQ + (1− τ)θQ′ θµ ′ = τθµ + (1− τ)θµ′\nend for // sample collecting thread for episode =1, T do\nInitialize a random process N for action exploration Receive the start observation state s0 for t=1,M do\nSelect action at = µ(st|θµ) +Nt Execute at, get transition (st ,at ,rt ,st+1) Send the transition to replay buffer R\nend for end for\nTo show the effectiveness of the ADDPG algorithm, we tested it in an OpenAI Gym task Pendulum-v0 2 with one sample collecting thread and one training thread. The comparison was implemented on a laptop with an Intel Core i7-4700 CPU. Trivial neural network structures were applied on the actor and critic networks of this test model. The result is presented in Fig. 2. The mean Q-value of the training batch sampled from the replay buffer in every back-propagation step was calculated, as shown in the left of Fig. 2. The increasing of the Q-value of ADDPG is much faster than the original DDPG, which means ADDPG is able to learn the policy to finish this task in different states more efficiently. This is mainly attributed to the samples collection thread in\n2https://gym.openai.com/envs/Pendulum-v0\nparallel. As shown in the right of Fig. 2, the original DDPG collects one sample every back-propagation iteration while the parallel ADDPG collects almost four times more samples than the original DDPG in every step. This trivial task proves the advancement of the Asynchronous DDPG compared with the original one."
    }, {
      "heading" : "IV. MOTION PLANNER IMPLEMENTATION",
      "text" : ""
    }, {
      "heading" : "A. Problem Definition",
      "text" : "This paper aims to provide a mapless motion planner for mobile ground robots. We try to find such a translation function:\nvt = f(xt, pt, vt−1),\nwhere xt is the observation from the raw sensor information, pt is the relative position of the target, and vt−1 is the velocity of the mobile robot in the last time step. They can be regarded as the instant state st of the mobile robot. The model directly maps the state to the action, which is the next time velocity vt, as shown in Fig 1. As an effective motion planner, the control frequency must be guaranteed so that the robot can react to new observations immediately. We focus on the control of a differential two-wheel mobile robot which takes the essential place among home service robots."
    }, {
      "heading" : "B. Network Structure",
      "text" : "The problem can be naturally transferred to a reinforcement learning problem. As we mentioned before, the original DQN [1] can only be applied to a discrete action space. But for a mobile robot motion planner, the continuous actions space is obviously necessary. In this paper, we use DDPG [2] to train our model and extend it to an asynchronous version, as described in Section III.\nAs presented in Fig. 1 and the definition function, the abstracted 10-dimensional laser range findings, the previous action and the relative target position are merged together as a 14-dimensional input vector. As we mentioned before, the 10-dimensional laser range findings are sampled from the raw laser findings between -90 and 90 degrees in a trivial and\nfixed angle distribution. The range information is normalized to (0,1). The 2-dimensional action of every time step includes the angular and the linear velocities of the differential mobile robot. The 2-dimensional target position is represented in polar coordinates (distance and angle) with respect to the mobile robot coordinate frame. As shown in Fig. 3, after 3 fully-connected neural network layers with 512 nodes, the input vector is transferred to the linear and angular velocity commands of the mobile robot. To constrain the range of angular velocity in (−1, 1), a hyperbolic tangent function (tanh) is used as the activation function. Moreover, the range of the linear velocity is constrained in (0, 1) through a sigmoid function. Backward moving is not expected because laser findings cannot cover the back area of the mobile robot. The output actions are multiplied with two hyperparameters to decide the final linear and angular velocities directly executed by the mobile robot. Considering the real dynamics of a Turtlebot, we choose 0.5 m/s as the max linear velocity and 1 rad/s as the max angular velocity.\nFor the critic-network, the Q-value of the state and action pair is predicted. We still use 3 fully-connected neural network layers to process the input state. The action is merged in the second fully-connected neural network layers. The Q-value is finally activated through a linear activation function:\ny = kx+ b,\nwhere x is the input of the last layer, y is the predicted Qvalue, and k and b are the trained weights and bias of this layer."
    }, {
      "heading" : "C. Reward Function Definition",
      "text" : "The mobile robot tries to get to the desired target position without obstacle collisions. There are three different conditions for the reward:\nr(st, at) =  rarrive if dt < cdrcollision if minxt < co cr(dt−1 − dt)\nIf the robot arrives at the target through distance threshold checking, a positive reward rarrive is arranged, but if the robot collides with the obstacle through a minimum range findings checking, a negative reward rcollision is arranged. Both of these two conditions stop the training episode. Otherwise, the reward is the difference in the distance from the target compared with last time step, dt−1−dt, multiplied by a hyper-parameter cr. This motivates the robot to get closer to the target position. The reward is directly used by the critic network without clipping or normalization."
    }, {
      "heading" : "V. EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "A. Training in simulation",
      "text" : "The training procedure of our model was implemented in virtual environments simulated by V-REP [23] . We constructed two indoor environments to show the influence of the training environment on the motion planner, as shown in Fig. 4. The environments are surrounded by walls. Obstacles in Env-2 are more compact around the robot initial position. Both models of these two environments were learned from scratch. A Kobuki based Turtlebot is used as the robot platform. The target is represented by a cylinder object, as labeled in the figure, which is only for visual purposes, in fact it cannot be rendered by the laser sensor mounted on the Turtlebot. In every episode, the target position was initialized randomly in the whole area and guaranteed to be collision free with other obstacles. To add randomness, other objects were also initialized with a small random noise in every episode so that the initial state of the episode would be widely distributed. In every forward step, an extra small noise was added to the target position. This helped to avoid over-fitting of the training.\nTraining parameters and their values for ADDPG are listed in Table I. The learning rates for the critic and actor network are the same where the hyper-parameters for the reward function were set trivially. Moreover, the experiments results also show that the effects of ADDPG are not depending on the tuning of hyperparameters. We trained the model from scratch with an Adam [24] optimizer on a single Nvidia GeForce GTX 1080 GPU3 for 800 k training steps which took almost 20 hours.\nThe mean Q-value of the training batch samples of the two environments is shown in Fig. 5. The compact environment Env-2 received more collision samples, so the Q-value is much smaller than the Env-1, but the mean Q-value of Env2 increases much faster than Env-1."
    }, {
      "heading" : "B. Evaluation",
      "text" : "To show the performance of the motion planner when it is deployed on a real robot, we tested it both in the virtual and real world on a laptop with an Intel Core i7-4700 CPU. We used a Kobuki based Turtlebot as the mobile ground platform. The robot subscribed the laser range findings from a SICK TiM5514 which has a field of view (FOV) of 270◦ and an angular resolution of 0.33◦. The scanning range is from 0.05 m to 10 m. When implemented in the real world,\n3https://www.nvidia.com/en-us/geforce/products/10series/geforce-gtx1080/\n4https://www.sick.com/us/en/detection-and-ranging-solutions/2d-laserscanners/tim5xx/tim551-2050001/p/p343045\nas shown in Fig. 6, a battery should be mounted to support the laser sensor separately. This paper mainly introduces the mapless motion planner so we did not test the planner effects with different localization methods. The real time position of the robot was provided by amcl 5 to calculate the polar coordinates of the target position. The robot operating system (ROS)[25] provided the communication interface between different agents.\n1) Baseline: We compared the deep-RL trained motion planners with the state-of-art Move Base motion planner. Move Base uses the full laser range information for local cost-map calculation, while our mapless motion planner only needs 10-dimensional range findings from specific directions for motion planning. Therefore we implemented a 10- dimensional Move Base using the laser range findings from specific angles as the trained model. These 10-dimensional findings were extended to a 810-dimensional vector covering the field of view through an RBF kernel Gaussian process regression [26] for the local cost-map prediction that we called 10-dimensional Move Base in the following experiments. The evaluation experiments focused on comparisons between the 10-dimension Move Base planner and Env-1 and Env-2 trained models.\nHere the deep-RL trained models only considered the final position of the robot but not the orientation of the desired target, so we set a trivial and appropriate final orientation value for Move Base.\n2) Virtual Environment Evaluation: To show the generic adaptability of the model in other environments, we firstly built a virtual test environment, as shown in Fig. 7, consisting of a 7×10 m2 area with multiple obstacles. We set 10 target positions for the motion planner. The motion planner should navigate the robot to the target positions along the sequence\n5http://wiki.ros.org/amcl\nnumber, as shown in the figure. For Move Base, a map of the environment should be built before the navigation task so that the planner can calculate the path. For our deep-RL trained models, as shown in Fig 7(c) and Fig 7(d), the map is only for trajectory visualization. In the real-time planning, only range findings and target relative positions are necessary for the trained mapless motion planner.\nThe trajectory tracking of the four planners is shown in Fig. 7 as a qualitative evaluation. Every motion planner was executed five times for all of the target positions and one of the paths is highlighted in the figure.\nAs shown in 7(b), the 10-dimensional Move Base cannot finish the navigation task successfully: the navigation was interrupted and aborted because of incorrect prediction of the local cost-map so the robot was not able to find the path by itself and human intervention had to be added to help the robot finish all of the navigation tasks. The interruption parts are labeled as black segments in 7(b). However, deepRL trained mapless motion planners accomplished the tasks collision free, as shown in Fig. 7(c) and Fig. 7(d). The deepRL trained planners show great adaptability to unseen environments. We chose three metrics to evaluate the different planners quantitatively, as listed in Fig. 8. • max control frequency: max moving commands output\ntimes per minute. • time: traveling time for all of the 10 target positions. • distance: path distance for all of the 10 target positions. The max control frequency reflects the query efficiency of the motion planner: the query of trained mapless motion planners only took almost 1 ms which is 7 times faster than the map-based motion planner. Compared with the 10- dimensional Move Base, Env-2 took almost the same time to finish all of the navigation tasks even though the path was not the optimally shortest. The Env-1 motion planning results seem not as smooth as the other motion planners.\n3) Real Environment Evaluation: The performance of the trained mapless motion planner in the simulated environment has been described before. In this section, we implemented a similar navigation task but in the real world environment. According to the trajectory in Fig. 7, the motion planner trained in Env-2 generated a smoother trajectory than the one trained in Env-1, and the Env-2 model seemed to be more sensitive to the obstacles than the Env-1 model. Preliminary experiments in the real world showed that the Env-1 model was not able to finish the navigation task successfully. So in this section, we only compared the trajectory in the real world between 10-dimensional Move Base and the Env-2 model.\nWe navigated the robot in a complex indoor office environment, as shown in Fig. 9. The task was the same as the virtual experiment mentioned previously: the robot should have arrived at the target based on the sequence labeled in the figure.\nFrom the trajectory figure, 10-dimensional Move Base cannot go across the seriously narrow area because of the misprediction of the local cost-map based on the limited range findings. 10-dimensional Move Base was not able to\nfind an effective path to arrive at the desired target. We added human intervention to help the 10-dimensional Move Base finish the navigation task. The intervention segments of the path are labeled in black in Fig 9.\nThe Env-2 model was able to accomplish all of the tasks successfully. However, sometimes the robot was not able to go across the narrow route smoothly. A recovery behavior like the rotating recovery in Move Base was developed by the mapless motion planner. Even then, obstacle collision never happened for the mapless motion planner. A brief video about the performance of the mapless planner in different training stages and in virtual and real evaluation environments is provided in the attached video file (Highresolution version is available at https://youtu.be/ _MAkZY6irIU)."
    }, {
      "heading" : "VI. DISCUSSION",
      "text" : "The experiments in the virtual and real world proved that the deep-RL trained mapless motion planner can be transferred directly to unseen environments. The different navigation trajectories of the two training environments showed that the trained planner is influenced by the environment to some extent. Env-2 is much more aggressive with closer obstacles so that the robot can navigate in the complex real environment successfully.\nIn this paper, we compared our deep learning trained model with the original and low-dimensional map-based motion planner. Compared with the trajectories of Move Base, the path generated from our planner is more tortuous. A possible explanation is that the network has neither the memory of the previous observation nor the long-term prediction ability. Thus LSTM and RNN [27] are possible solutions for that problem. We set this revision as future work.\nHowever, we are not aiming to replace the map-based motion planner: it is obvious that when the application scenario is on a large scale and in a complex environment, the map of the environment can always provide a reliable navigation path. Our target is to provide a low-cost solution for an indoor service robot with several range sensors, like a light-weight sweeping robot. The experiments showed that Move Base with 10-dimensional range findings can not be adapted to narrow indoor office environments. Although we used the range findings from a laser sensor, it is certain that this 10-dimensional information can be replaced by low-cost range sensors.\nIn addition, reinforcement learning methods provide a considerable online learning strategy. The effects of the motion planner can be developed considerably with training in different environments continuously. In this developing procedure, no feature revision or human labeling is needed. On the other hand, the application of the deep neural networks provides a solution for multiple sensor inputs like RGB image and depth. The proposed model has shown the ability to understand different information combinations like range sensor findings and target position."
    }, {
      "heading" : "VII. CONCLUSION",
      "text" : "In this paper, a mapless motion planner was trained endto-end through deep reinforcement learning from scratch. We revised the state-of-art continuous deep reinforcement learning method so that the training and sample collection can be executed in parallel. By taking the 10-dimensional range sensor findings and the target position as input, the proposed motion planner can be directly applied in unseen real environments without fine-tuning, even though it is only trained in a virtual environment. When compared to the lowdimensional map-based motion planner, our approach proved to be more robust to extremely complicated environments."
    } ],
    "references" : [ {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski" ],
      "venue" : "Nature, vol. 518, no. 7540, pp. 529–533, 2015.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Continuous control with deep reinforcement learning",
      "author" : [ "T.P. Lillicrap", "J.J. Hunt", "A. Pritzel", "N. Heess", "T. Erez", "Y. Tassa", "D. Silver", "D. Wierstra" ],
      "venue" : "arXiv preprint arXiv:1509.02971, 2015.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Continuous deep qlearning with model-based acceleration",
      "author" : [ "S. Gu", "T. Lillicrap", "I. Sutskever", "S. Levine" ],
      "venue" : "Proceedings of The 33rd International Conference on Machine Learning, 2016, pp. 2829–2838.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Target-driven visual navigation in indoor scenes using deep reinforcement learning",
      "author" : [ "Y. Zhu", "R. Mottaghi", "E. Kolve", "J.J. Lim", "A. Gupta", "L. Fei-Fei", "A. Farhadi" ],
      "venue" : "arXiv preprint arXiv:1609.05143, 2016.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "cad) 2 rl: Real single-image flight without a single real image",
      "author" : [ "F. Sadeghi", "S. Levine" ],
      "venue" : "arXiv preprint arXiv:1611.04201, 2016.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Towards cognitive exploration through deep reinforcement learning for mobile robots",
      "author" : [ "L. Tai", "M. Liu" ],
      "venue" : "arXiv preprint arXiv:1610.01733, 2016.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Simultaneous localization and mapping: part i",
      "author" : [ "H. Durrant-Whyte", "T. Bailey" ],
      "venue" : "IEEE robotics & automation magazine, vol. 13, no. 2, pp. 99–110, 2006.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Asynchronous methods for deep reinforcement learning",
      "author" : [ "V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T.P. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu" ],
      "venue" : "International Conference on Machine Learning, 2016.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Wifi signal strength-based robot indoor localization",
      "author" : [ "Y. Sun", "M. Liu", "M.Q.-H. Meng" ],
      "venue" : "Information and Automation (ICIA), 2014 IEEE International Conference on. IEEE, 2014, pp. 250–256.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Let the light guide us: Vlc-based localization",
      "author" : [ "K. Qiu", "F. Zhang", "M. Liu" ],
      "venue" : "IEEE Robotics & Automation Magazine, vol. 23, no. 4, pp. 174–183, 2016.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Performance of optical flow techniques for indoor navigation with a mobile robot",
      "author" : [ "C. McCarthy", "N. Bames" ],
      "venue" : "Robotics and Automation, 2004. Proceedings. ICRA’04. 2004 IEEE International Conference on, vol. 5. IEEE, 2004, pp. 5093–5098.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "A behaviour-based architecture for mapless navigation using vision",
      "author" : [ "M.S. Guzel", "R. Bicker" ],
      "venue" : "International Journal of Advanced Robotic Systems, vol. 9, no. 1, p. 18, 2012.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Vfh+: Reliable obstacle avoidance for fast mobile robots",
      "author" : [ "I. Ulrich", "J. Borenstein" ],
      "venue" : "Robotics and Automation, 1998. Proceedings. 1998 IEEE International Conference on, vol. 2. IEEE, 1998, pp. 1572–1577.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Range-sensor based navigation in three dimensions",
      "author" : [ "I. Kamon", "E. Rimon", "E. Rivlin" ],
      "venue" : "Robotics and Automation, 1999. Proceedings. 1999 IEEE International Conference on, vol. 1. IEEE, 1999, pp. 163–169.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Off-road obstacle avoidance through end-to-end learning",
      "author" : [ "Y. LeCun", "U. Muller", "J. Ben", "E. Cosatto", "B. Flepp" ],
      "venue" : "NIPS, 2005, pp. 739–746.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "A Deep-network Solution Towords Modelless Obstacle Avoidence",
      "author" : [ "L. Tai", "S. Li", "M. Liu" ],
      "venue" : "IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2016, 2016.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Deepdriving: Learning affordance for direct perception in autonomous driving",
      "author" : [ "C. Chen", "A. Seff", "A. Kornhauser", "J. Xiao" ],
      "venue" : "Proceedings of the IEEE International Conference on Computer Vision, 2015, pp. 2722–2730.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "From perception to decision: A data-driven approach to end-toend motion planning for autonomous ground robots",
      "author" : [ "M. Pfeiffer", "M. Schaeuble", "J. Nieto", "R. Siegwart", "C. Cadena" ],
      "venue" : "arXiv preprint arXiv:1609.07910, 2016.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Socially compliant mobile robot navigation via inverse reinforcement learning",
      "author" : [ "H. Kretzschmar", "M. Spies", "C. Sprunk", "W. Burgard" ],
      "venue" : "The International Journal of Robotics Research, vol. 35, no. 11, pp. 1289–1307, 2016.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Reinforcement learning in robotics: A survey",
      "author" : [ "J. Kober", "J.A. Bagnell", "J. Peters" ],
      "venue" : "The International Journal of Robotics Research, vol. 32, no. 11, pp. 1238–1274, 2013.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates",
      "author" : [ "S. Gu", "E. Holly", "T. Lillicrap", "S. Levine" ],
      "venue" : "arXiv preprint arXiv:1610.00633, 2016.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Learning to navigate in complex environments",
      "author" : [ "P. Mirowski", "R. Pascanu", "F. Viola", "H. Soyer", "A. Ballard", "A. Banino", "M. Denil", "R. Goroshin", "L. Sifre", "K. Kavukcuoglu" ],
      "venue" : "arXiv preprint arXiv:1611.03673, 2016.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "V-rep: A versatile and scalable robot simulation framework",
      "author" : [ "E. Rohmer", "S.P. Singh", "M. Freese" ],
      "venue" : "Intelligent Robots and Systems (IROS), 2013 IEEE/RSJ International Conference on. IEEE, 2013, pp. 1321–1326.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "D. Kingma", "J. Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980, 2014.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Ros: an open-source robot operating system",
      "author" : [ "M. Quigley", "K. Conley", "B. Gerkey", "J. Faust", "T. Foote", "J. Leibs", "R. Wheeler", "A.Y. Ng" ],
      "venue" : "ICRA workshop on open source software, vol. 3, no. 3.2. Kobe, 2009, p. 5.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Gaussian processes for machine learning",
      "author" : [ "C.E. Rasmussen" ],
      "venue" : "2006.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Deep recurrent q-learning for partially observable mdps",
      "author" : [ "M. Hausknecht", "P. Stone" ],
      "venue" : "arXiv preprint arXiv:1507.06527, 2015.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "1) Deep Reinforcement Learning in mobile robots: Deep Reinforcement Learning (deep-RL) methods achieve great success in many tasks including video games [1] and simulation control agents [2].",
      "startOffset" : 153,
      "endOffset" : 156
    }, {
      "referenceID" : 1,
      "context" : "1) Deep Reinforcement Learning in mobile robots: Deep Reinforcement Learning (deep-RL) methods achieve great success in many tasks including video games [1] and simulation control agents [2].",
      "startOffset" : 187,
      "endOffset" : 190
    }, {
      "referenceID" : 2,
      "context" : "The applications of deep reinforcement learning in robotics are mostly limited in manipulation [3] where the workspace is fully observable and stable.",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 3,
      "context" : "In terms of mobile robots, deep-RL methods normally sampled the action from a discrete space to simplify the problem [4], [5], [6].",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 4,
      "context" : "In terms of mobile robots, deep-RL methods normally sampled the action from a discrete space to simplify the problem [4], [5], [6].",
      "startOffset" : 122,
      "endOffset" : 125
    }, {
      "referenceID" : 5,
      "context" : "In terms of mobile robots, deep-RL methods normally sampled the action from a discrete space to simplify the problem [4], [5], [6].",
      "startOffset" : 127,
      "endOffset" : 130
    }, {
      "referenceID" : 6,
      "context" : "problem through the prior map of the navigation environment [7] based on laser range findings.",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 7,
      "context" : "In virtual environments, a nonholonomic differential drive robot was trained to learn how to arrive at the target position with obstacle avoidance through asynchronous deep reinforcement learning [8].",
      "startOffset" : 196,
      "endOffset" : 199
    }, {
      "referenceID" : 8,
      "context" : "Our planner also provides a planning solution for other low-cost localization methods like wifi localization [9] and visible-light communication [10].",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 9,
      "context" : "Our planner also provides a planning solution for other low-cost localization methods like wifi localization [9] and visible-light communication [10].",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 10,
      "context" : "[11] used optical flow to navigate a mobile robot in corridor-like environments.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[12] built a mapless motion planner based on a homing system.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[13] developed a vector field histogram (VFH) for obstacle avoidance.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[14] used three-dimensional range data to plan a three-dimensional motion.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "For learning-based obstacle avoidance, deep neural networks have been successfully applied on monocular images [15] and depth images [16].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 15,
      "context" : "For learning-based obstacle avoidance, deep neural networks have been successfully applied on monocular images [15] and depth images [16].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 16,
      "context" : "[17] used semantics information extracted from the image by deep neural networks to decide the behavior of the autonomous vehicle.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[18] used a deep learning model to map the laser range findings and target position to the moving commands.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "[19] used inverse reinforcement learning methods to make robots interact with humans in a socially compliant way.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "tasks [20].",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 0,
      "context" : "[1] utilized deep neural networks for the function estimation of value-based reinforcement learning which was called deep reinforcement learning or deep Q-network (DQN), and achieved a huge improvement in playing Atari games.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "[2] proposed deep deterministic policy gradients (DDPG) to use deep neural networks on the actorcritic reinforcement learning method where both the policy and value of the reinforcement learning were represented",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[3] proposed continuous DQN based on the normalized advantage function (NAF).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "Therefore, asynchronous deep reinforcement learning [8] used separated threads for training and sample collection (A3C).",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 20,
      "context" : "[21] proposed asynchronous NAF and trained the model with real-world samples where a door opening task was accomplished by a real robot arm.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "[22] trained a simulated agent to learn navigation in a virtual environment through raw images based on A3C.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 3,
      "context" : "[4] trained an image-based planner where the robot learned to navigate to the referenced image place based on the instant view.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "In this section, we present the extension of asynchronous deep-RL in DDPG [2].",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 7,
      "context" : "It has been successfully applied to many deep-RL methods like Advantage-Actor-Critic [8] and NAF [21].",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 20,
      "context" : "It has been successfully applied to many deep-RL methods like Advantage-Actor-Critic [8] and NAF [21].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 1,
      "context" : "Both of the two target-networks are updated softly as [2].",
      "startOffset" : 54,
      "endOffset" : 57
    }, {
      "referenceID" : 0,
      "context" : "DQN [1] can only be applied to a discrete action space.",
      "startOffset" : 4,
      "endOffset" : 7
    }, {
      "referenceID" : 1,
      "context" : "In this paper, we use DDPG [2] to train our model and extend it to an asynchronous version, as described in Section III.",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 22,
      "context" : "The virtual training environments were simulated by V-REP [23].",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 22,
      "context" : "The training procedure of our model was implemented in virtual environments simulated by V-REP [23] .",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 23,
      "context" : "We trained the model from scratch with an Adam [24] optimizer on a single Nvidia GeForce GTX 1080 GPU3 for 800 k training steps which took almost 20 hours.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 24,
      "context" : "The robot operating system (ROS)[25] provided the communication interface between different agents.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 25,
      "context" : "These 10-dimensional findings were extended to a 810-dimensional vector covering the field of view through an RBF kernel Gaussian process regression [26] for the local cost-map prediction that we called 10-dimensional Move Base in the following experiments.",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 26,
      "context" : "Thus LSTM and RNN [27] are possible solutions for that problem.",
      "startOffset" : 18,
      "endOffset" : 22
    } ],
    "year" : 2017,
    "abstractText" : "Deep Reinforcement Learning has been successful in various virtual tasks, but it is still rarely used in real world applications especially for continuous control of mobile robots navigation. In this paper, we present a learning-based mapless motion planner by taking the 10-dimensional range findings and the target position as input and the continuous steering commands as output. Traditional motion planners for mobile ground robots with a laser range sensor mostly depend on the map of the navigation environment where both the highly precise laser sensor and the map building work of the environment are indispensable. We show that, through an asynchronous deep reinforcement learning method, a mapless motion planner can be trained end-to-end without any manually designed features and prior demonstrations. The trained planner can be directly applied in unseen virtual and real environments. We also evaluated this learning-based motion planner and compared it with the traditional motion planning method, both in virtual and real environments. The experiments show that the proposed mapless motion planner can navigate the nonholonomic mobile robot to the desired targets without colliding with any obstacles.",
    "creator" : "LaTeX with hyperref package"
  }
}