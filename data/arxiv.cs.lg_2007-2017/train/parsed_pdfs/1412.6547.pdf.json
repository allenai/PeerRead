{
  "name" : "1412.6547.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Fast Label Embeddings via Randomized Linear Algebra",
    "authors" : [ "Paul Mineiro", "Nikos Karampatziakis" ],
    "emails" : [ "pmineiro@microsoft.com", "nikosk@microsoft.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Recent years have witnessed the emergence of many multiclass and multilabel datasets with increasing number of possible labels, such as ImageNet [12] and the Large Scale Hierarchical Text Classification (LSHTC) datasets [25]. One could argue that all problems of vision and language in the wild have extremely large output spaces.\nWhen the number of possible outputs is modest, multiclass and multilabel problems can be dealt with directly (via a max or softmax layer) or with a reduction to binary classification. However, when the output space is large, these strategies are too generic and do not fully exploit some of the common properties that these problems exhibit. For example, often the alternatives in the output space have varying degrees of similarity between them so that typical examples from similar classes tend to be closer1 to each other than from dissimilar classes. More concretely, classifying an image of a Labrador retriever as a golden retriever is a more benign mistake than classifying it as a rowboat.\nShouldn’t these problems then be studied as structured prediction problems, where an algorithm can take advantage of the structure? That would be the case if for every problem there was an unequivocal structure (e.g. a hierarchy) that everyone agreed on and that structure was designed with the goal of being beneficial to a classifier. When this is not the case, we can instead let the algorithm uncover a structure that matches its own capabilities.\n1 or more confusable, by machines and humans alike.\nar X\niv :1\n41 2.\n65 47\nv7 [\ncs .L\nG ]\n5 J\nul 2\n01 5\nIn this paper we use label embeddings as the underlying structure that can help us tackle problems with large output spaces, also known as extreme classification problems. Label embeddings can offer improved computational efficiency because the embedding dimension is much smaller than the dimension of the output space. If designed carefully and applied judiciously, embeddings can also offer statistical efficiency because the number of parameters can be greatly reduced without increasing, or even reducing, generalization error."
    }, {
      "heading" : "1.1 Contributions",
      "text" : "We motivate a particular label embedding defined by the low-rank approximation of a particular matrix, based upon a correspondence between label embedding and the optimal rank-constrained least squares estimator. Assuming realizability and infinite data, the matrix being decomposed is the expected outer product of the conditional label probabilities. In particular, this indicates two labels are similar when their conditional probabilities are linearly dependent across the dataset. This unifies prior work utilizing the confusion matrix for multiclass [5] and the empirical label covariance for multilabel [42].\nWe apply techniques from randomized linear algebra [19] to develop an efficient and scalable algorithm for constructing the embeddings, essentially via a novel randomized algorithm for rank-constrained squared loss regression. Intuitively, this technique implicitly decomposes the prediction matrix of a model which would be prohibitively expensive to form explicitly. The first step of our algorithm resembles compressed sensing approaches to extreme classification that use random matrices [21]. However our subsequent steps tune the embeddings to the data at hand, providing the opportunity for empirical superiority."
    }, {
      "heading" : "2 Algorithm Derivation",
      "text" : ""
    }, {
      "heading" : "2.1 Notation",
      "text" : "We denote vectors by lowercase letters x, y etc. and matrices by uppercase letters W , Z etc. The input dimension is denoted by d, the output dimension by c and the embedding dimension by k. For multiclass problems y is a one hot (row) vector (i.e. a vertex of the c− 1 unit simplex) while for multilabel problems y is a binary vector (i.e. a vertex of the unit c-cube). For an m×n matrix X ∈ Rm×n we use ||X||F for its Frobenius norm, X† for the pseudoinverse, ΠX,L for the projection onto the left singular subspace of X, and X1:k for the matrix resulting by taking the first k columns of X. We use X∗ to denote a matrix obtained by solving an optimization problem over matrix parameter X. The expectation of a random variable v is denoted by E[v]."
    }, {
      "heading" : "2.2 Background",
      "text" : "In this section we offer an informal discussion of randomized algorithms for approximating the principal components analysis of a data matrix X ∈ Rn×d\nAlgorithm 1 Randomized PCA\n1: function RPCA(k,X ∈ Rn×d) 2: (p, q)← (20, 1) . These hyperparameters rarely need adjustment. 3: Q← randn(d, k + p) 4: for i ∈ {1, . . . , q} do . Randomized range finder for X>X 5: Ψ ← X>XQ . Ψ can be computed in one pass over the data 6: Q← orthogonalize(Ψ) . orth complexity O(dk2) is independent of n 7: end for . NB: total of (q + 1) data passes, including next line 8: F ← (X>XQ)>(X>XQ) . F ∈ R(k+p)×(k+p) is “small’ 9: (V,Σ2)← eig(F, k) . Exact optimization on small matrix 10: V ← (X>XQ)V Σ† . Back out the solution 11: return (V,Σ) 12: end function\nwith n examples and d features. For a very thorough and more formal discussion see [19].\nAlgorithm 1 shows a recipe for performing randomized PCA. In both theory and practice, the algorithm is insensitive to the parameters p and q as long as they are large enough (in our experiments we use p = 20 and q = 1). We start with a set of k+p random vectors and use them to probe the range of X>X. Since principal eigenvectors can be thought as “frequent directions” [28], the range of Ψ will tend to be more aligned with the space spanned by the top eigenvectors of X>X. We compute an orthogonal basis for the range of Ψ and repeat the process q times. This can also be thought as orthogonal (aka subspace) iteration for finding eigenvectors with the caveat that we early stop (i.e., q is small). Once we are done and we have a good approximation for the principal subspace of X>X, we optimize fully over that subspace and back out the solution. The last few steps are cheap because we are only working with a (k+ p)× (k+ p) matrix and the largest bottleneck is either the computation of Ψ in a single machine setting or the orthogonalization step if parallelization is employed. An important observation we use below is that X or X>X need not be available explicitly; to run the algorithm we only need to be able to compute the result of multiplying with X>X."
    }, {
      "heading" : "2.3 Rank-Constrained Estimation and Embedding",
      "text" : "We begin with a setting superficially unrelated to label embedding. Suppose we seek an optimal squared loss predictor of a high-cardinality target vector y ∈ Rc which is linear in a high dimensional feature vector x ∈ Rd. Due to sample complexity concerns, we impose a low-rank constraint on the weight matrix. In matrix form,\nW ∗ = arg min W∈Rd×c| rank(W )≤k ‖Y −XW‖2F , (1)\nwhere Y ∈ Rn×c and X ∈ Rn×d are the target and design matrices respectively. This is a special case of a more general problem studied by [14]; specializing\nAlgorithm 2 Rembrandt: Response EMBedding via RANDomized Techniques. A reference implementation is publicly available.[32]\n1: function Rembrandt(k,X ∈ Rn×d, Y ∈ Rn×c) 2: (p, q)← (20, 1) . These hyperparameters rarely need adjustment. 3: Q← randn(c, k + p) 4: for i ∈ {1, . . . , q} do . Randomized range finder for Y >ΠX,LY 5: Z ← arg min ‖Y Q−XZ‖2F 6: Q← orthogonalize(Y >XZ) 7: end for . NB: total of (q + 1) data passes, including next line 8: F ← (Y >XZ)>(Y >XZ) . F ∈ R(k+p)×(k+p) is “small” 9: (V,Σ2)← eig(F, k) 10: V ← (Y >XZ)V Σ† . V ∈ Rc×k is the embedding 11: return (V,Σ) 12: end function\ntheir result yields the solution W ∗ = X†(ΠX,LY )k, where ΠX,L projects onto the left singular subspace of X, and (·)k denotes optimal Frobenius norm rank-k approximation, which can be computed2 via SVD. The expression for W ∗ can be written in terms of the SVD ΠX,LY = UΣV\n>, which, after simple algebra, yields W ∗ = ( X†(Y V1:k) ) V >1:k. This is equivalent to the following procedure:\n1. Y V1:k: Project Y down to k dimensions using the top right singular vectors of ΠX,LY . 2. X†(Y V1:k) Least squares fit the projected labels using X and predict them. 3. ( X†(Y V1:k) ) V >1:k: Map predictions to the original output space, using the\ntranspose of the top right singular vectors of ΠX,LY .\nThis motivates the use of the right singular vectors of ΠX,LY as a label embedding. The ΠX,LY term can be demystified: it corresponds to the predictions of the optimal unconstrained model,\nZ∗ = arg min Z∈Rd×c ‖Y −XZ‖2F ,\nΠX,LY = XZ ∗ def= Ŷ .\nThe right singular vectors V of ΠX,LY are therefore the eigenvectors of Ŷ >Ŷ , i.e., the matrix formed by the sum of outer products of the optimal unconstrained model’s predictions on each example. Note that actually computing and materializing Z∗ ∈ Rd×c would be expensive; a key aspect of the randomized algorithm is that we get the same result while avoiding this intermediate. In particular we can find the product of ΠX,LY with another matrix Q ∈ Rc×k via\nZ∗Q = arg min Z∈Rd×k ‖Y Q−XZ‖2F ,\nΠX,LY Q = XZ ∗Q. (2)\n2 if X = UXΣXV > X is the SVD of X, then ΠX,L = UXU > X .\nBecause squared loss is a proper scoring rule it is minimized at the conditional mean. In the limit of infinite training data (n → ∞) and sufficient model flexibility (so that ŷ = E[y|x]) we have that\n1 n Ŷ >Ŷ a.s.−→ E[E[y|x]>E[y|x]] (3)\nby the strong law of large numbers. An embedding based upon the eigendecomposition of E[E[y|x]>E[y|x]] is not practically actionable, but does provide valuable insights. For example, the principal label space transformation of [42] is an eigendecomposition of the empirical label covariance Y >Y . This is a plausible approximation to E[E[y|x]>E[y|x]] in the multilabel case. However, for multiclass (or multilabel where most examples have at most one nonzero component), the low-rank constraint alone cannot produce good generalization if the input representation is sufficiently flexible; the eigendecomposition of the prediction covariance will merely select a basis for the k most frequent labels due to the absence of empirical cooccurence statistics. Under these conditions we must further regularize (i.e., tradeoff variance for bias) beyond the low-rank constraint, so that Ŷ better approximates E[Y |X] rather than the observed Y . Our procedure admits tuning the bias-variance tradeoff via choice of model (features) used in line 5 of Algorithm 2."
    }, {
      "heading" : "2.4 Rembrandt",
      "text" : "Our proposal is Rembrandt, described in Algorithm 2. In the previous section, we motivated the use of the top right singular space of ΠX,LY as a label embedding, or equivalently, the top principal components of Y >ΠX,LY (leveraging the fact that the projection is idempotent). Using randomized techniques, we can decompose this matrix without explicitly forming it, because we can compute the product of ΠX,LY with another matrix Q via equation 2. Algorithm 2 is a specialization of randomized PCA to this particular form of the matrix multiplication operator. Starting from a random label embedding which satisfies the conditions for randomized PCA (e.g., a Gaussian random matrix), the algorithm first fits the embedding, outer products the embedding with the labels, orthogonalizes and repeats for some number of iterations. Then a final exact eigendecomposition is used to remove the additional dimensions of the embedding that were added to improve convergence. Note that the optimization of 2 is over Rd×(k+p), not Rd×c, although the result is equivalent; this is the main computational advantage of our technique.\nThe connection to compressed sensing approaches to extreme classification is now clear, as the random sensing matrix corresponds to the starting point of the iterations in Algorithm 2. In other words, compressed sensing corresponds to Algorithm 2 with q = 0 and p = 0, which results in a whitened random projection of the labels as the embedding. Additional iterations (q > 0) and oversampling (p > 0) improve the approximation of the top eigenspace, hence the potential for improved performance. However when the model is sufficiently\nflexible, an embedding matrix which ignores the training data might be superior to one which overfits the training data.\nEquation (2) is inexpensive to compute. The matrix vector product Y Q is a sparse matrix-vector product so complexity O(nsk) depends only on the average (label) sparsity per example s and the embedding dimension k, and is independent of the number of classes c. The fit is done in the embedding space and therefore is independent of the number of classes c, and the outer product with the predicted embedding is again a sparse product with complexity O(nsk). The orthogonalization step is O(ck2), but this is amortized over the data set and essentially irrelevant as long as n > c. While random projection theory suggests k should grow logarithmically with c, this is only a mild dependence on the number of classes."
    }, {
      "heading" : "3 Related Work",
      "text" : "Low-dimensional dense embeddings of sparse high-cardinality output spaces have been leveraged extensively in the literature, due to their beneficial impact on multiple algorithmic desiderata. As this work emphasizes, there are potential statistical (i.e., regularization) benefits to label embeddings, corresponding to the rich literature of low-rank regression regularization [22]. Another common motivation is to mitigate space or time complexity at training or inference time. Finally, embeddings can be part of a strategy for zero-shot learning [35], i.e., designing a classifier which is extensible in the output space.\n[21], motivated by advances in compressed sensing, utilized a random embedding of the labels along with greedy sparse decoding strategy. For the multilabel case, [42] construct a low-dimensional embedding using principal components on the empirical label covariance, which they utilize along with a greedy sparse decoding strategy. For multivariate regression, [7] use the principal components of the empirical label covariance to define a shrinkage estimator which exploits correlations between the labels to improve accuracy. In these works, the motivation for embeddings was primarily statistical benefit. Conversely, [45] motivate their ranking-loss optimized embeddings solely by computational considerations of inference time and space complexity.\nMultiple authors leverage side information about the classes, such as a taxonomy or graph, in order to learn a label representation which is felicitous for classification, e.g. when composed with online learning [11]; Bayesian learning [10]; support vector machines [6]; and decision tree ensembles [39]. Our embedding approach neither requires nor exploits such side information, and is therefore applicable to different scenarios, but is potentially suboptimal when side information is present. However, our embeddings can be complementary to such techniques when side information is not present, as some approaches condense side information into a similarity matrix between classes, e.g., the sub-linear inference approach of [9] and the large margin approach of [44]. Our embeddings provide a low-rank similarity matrix between classes in factored form, i.e., represented in O(kc) rather than O(c2) space, which can be composed with these\ntechniques. Analogously, [5] utilize a surrogate classifier rather than side information to define a similarity matrix between classes; our procedure can efficiently produce a similarity matrix which can ease the computational burden of this portion of their procedure.\nAnother intriguing use of side information about the classes is to enable zeroshot learning. To this end, several authors have exploited the textual nature of classes in image annotation to learn an embedding over the classes which generalizes to novel classes, e.g., [15] and [40]. Our embedding technique does not address this problem.\n[18] focus nearly exclusively on the statistical benefit of incorporating label structure by overcoming the space and time complexity of large-scale oneagainst-all classification via distributed training and inference. Specifically, they utilize side information about the classes to regularize a set of one-against-all classifiers towards each other. This leads to state-of-the-art predictive performance, but the resulting model has high space complexity, e.g., terabytes of parameters for the LSHTC [24] dataset we utilize in section 4.3. This necessitates distributed learning and distributed inference, the latter being a more serious objection in practice. In contrast, our embedding technique mitigates space complexity and avoids model parallelism.\nOur objective in equation (1) is highly related to that of partial least squares [16], as Algorithm 2 corresponds to a randomized algorithm for PLS if the features have been whitened.3 Unsurprisingly, supervised dimensionality reduction techniques such as PLS can be much better than unsupervised dimensionality reduction techniques such as PCA regression in the discriminative setting if the features vary in ways irrelevant to the classification task [2].\nTwo other classical procedures for supervised dimensionality reduction are Fisher Linear Discriminant [38] and Canonical Correlation Analysis [20]. For multiclass problems these two techniques yield the same result [3,2], although for multilabel problems they are distinct. Indeed, extension of FLD to the multilabel case is a relatively recent development [43] whose straightforward implementation does not appear to be computationally viable for large number of classes. CCA and PLS are highly related, as CCA maximizes latent correlation and PLS maximizes latent covariance [2]. Furthermore, CCA produces equivalent results to PLS if the features are whitened [41]. Therefore, there is no obvious statistical reason to prefer CCA to our proposal in this context.\nRegarding computational considerations, scalable CCA algorithms are available [30,33], but it remains open how to specialize them to this context to leverage the equivalent of equation (2); whereas, if CCA is desired, Algorithm 2 can be utilized in conjunction with whitening pre-processing.\nText is one the common input domains over which large-scale multiclass and multilabel problems are defined. There has been substantial recent work on text embeddings, e.g., word2vec [31], which (empirically) provide analogous statistical and computational benefits despite being unsupervised. The text embedding technique of [27] is a particularly interesting comparison because it is a variant\n3 More precisely, if the feature covariance is a rotation.\nAlgorithm 3 Stagewise classification algorithm utilized for experiments. Loss is either log loss (multiclass) or independent log loss per class (multilabel).\n1: function DecoderTrain(k,X ∈ Rn×d, Y ∈ Rn×c, φ) 2: (R,∼)← Rembrandt(k,X, Y ) . or other comparison embedding 3: W ∗ ← arg minW ‖Y R−XW‖2F 4: Ŷ ← φ(XW ∗) . φ is an optional random feature map 5: Q∗ ← arg minQ loss(Y, Ŷ Q) . early-stopped, see text 6: return (W ∗, Q∗) 7: end function\nof Hellinger PCA which leverages sequential information. This suggests that unsupervised dimensionality reduction approaches can work well when additional structure of the input domain is incorporated, in this case by modeling word burstiness with the square root nonlinearity [23] and word order via decomposing neighborhood statistics. Nonetheless [27] note that when maximum statistical performance is desired, the embeddings must be fine-tuned to the particular task, i.e., supervised dimensionality reduction is required.\nAnother plausible regularization technique which mitigates inference space and time complexity is L1 regularization [29]. One reason to prefer low-rank regularization to L1 regularization is if the prediction covariance of equation (3) is well-modeled by a low-rank matrix."
    }, {
      "heading" : "4 Experiments",
      "text" : "The goal of these experiments is to demonstrate the computational viability and statistical benefits of the embedding algorithm, not to advocate for a particular classification algorithm per se. We utilize classification tasks for demonstration, and utilize our embedding strategy as part of algorithm 3, but focus our attention on the impact of the embedding on the result.\nIn table 1 we present some statistics about the datasets we use in this section as well as times required to compute an embedding for the dataset. Unless otherwise indicated, all timings presented in the experiments section are for a Matlab implementation running on a standard desktop, which has dual 3.2Ghz Xeon E5-1650 CPU and 48Gb of RAM."
    }, {
      "heading" : "4.1 ALOI",
      "text" : "ALOI is a color image collection of one-thousand small objects recorded for scientific purposes [17]. The number of classes in this data set does not qualify as extreme by current standards, but we begin with it as it will facilitate comparison with techniques which in our other experiments are intractable on a single machine. For these experiments we will consider test classification accuracy utilizing the same train-test split and features from [8]. Specifically there is a fixed train-test split of 90:10 for all experiments and the representation is linear in 128 raw pixel values.\nAlgorithm 2 produces an embedding matrix whose transpose is a squaredloss optimal decoder. In practice, optimizing the decode matrix for logistic loss as described in Algorithm 3 gives much better results. This is by far the most computationally demanding step in this experiment, e.g., it takes 4 seconds to compute the embedding but 300 seconds to perform the logistic regression. Fortunately the number of features (i.e., embedding dimensionality) for this logistic regression is modest so the second order techniques of [1] are applicable (in particular, their Algorithm 1 with a simple modification to include acceleration [34,4]). We determine the number of fit iterations for the logistic regression by extracting a hold-out set from the training set and monitoring held-out loss. We do not use a random feature map, i.e., φ in line 4 of Algorithm 3 is the identity function.\nWe compare to several different strategies in table 2. OAA is the one-againstall reduction of multiclass to binary. LR is a standard logistic regression, i.e., learning directly from the original features. Both of these options are intractable on a single machine for our other data sets. We also compare against Lomtree (LT), which has training and test time complexity logarithmic in the number of classes [8]. Both OAA and LT are provided by the Vowpal Wabbit [26] machine learning tool.\nThe remaining techniques are variants of Algorithm 3 using different embedding strategies. PCA + LR refers to logistic regression after first projecting the features onto their top principal components. CS + LR refers to logistic regression on a label embedding which is a random Gaussian matrix suitable for compressed sensing. Finally RE + LR is Rembrandt composed with logistic regression. These techniques were all implemented in Matlab.\nInterestingly, OAA underperforms the full logistic regression. Rembrandt combined with logistic regression outperforms logistic regression, suggesting a beneficial effect from low-rank regularization. Compressed sensing is able to match the performance of the full logistic regression while being computationally\n(a) Performance of logistic regression on ALOI when combined with either a feature embedding (PCA) or label embedding (RE).\nrank 100 101 102 103\nsi ng\nul ar\nv al\nue\n100\n102\n104\n106\n(b) The empirical label covariance spectrum for LSHTC.\nmore tractable, but underperforms Rembrandt. Lomtree has the worst prediction performance but the lowest computational overhead when the number of classes is large.\nAt k = 50, there is no difference in quality between using the Rembrandt (label) embedding and the PCA (feature) embedding. This is not surprising considering the effective rank of the covariance matrix of ALOI is 70. For small embedding dimensionalities, however, PCA underperforms Rembrandt as indicated in Figure 1a. For larger numbers of output classes, where the embedding dimension will be a small fraction of the number of classes by computational necessity, we anticipate PCA regression will not be competitive.\nNote that, in addition to better statistical performance, all of the “embedding + LR” approaches have lower space complexity O(k(c + d)) than direct logistic regression O(cd). For ALOI the savings are modest (255600 bytes vs. 516000 bytes) because the input dimensionality is only d = 128, but for larger problems the space savings are necessary for feasible implementation on a single commodity computer. Inference time on ALOI is identical for embedding and direct approaches in practice (both achieving ≈ 170k examples/sec)."
    }, {
      "heading" : "4.2 ODP",
      "text" : "The Open Directory Project [13] is a public human-edited directory of the web which was processed by [6] into a multiclass data set. For these experiments we will consider test classification error utilizing the same train-test split, features, and labels from [8]. Specifically there is a fixed train-test split of 2:1 for all experiments, the representation of document is a bag of words, and the unique class assignment for each document is the most specific category associated with the document.\nThe procedures are the same as in the previous experiment, except that we do not compare to OAA or full logistic regression due to intractability on a single machine.\nThe combination of Rembrandt and logistic regression result is, to the best of our knowledge, the best published result on this dataset. PCA logistic regression has a performance gap compared to Rembrandt and logistic regression. The poor performance of PCA logistic regression is doubly unsurprising, both for general reasons previously discussed, and due to the fact that covariance matrices of text data typically have a long plateau of weak spectral decay. In other words, for text problems projection dimensions quickly become nearly equivalent in terms of input reconstruction error, and common words and word combinations are not discriminative. In contrast, Rembrandt leverages the spectral properties of the prediction covariance of equation (3), rather than the spectral properties of the input features.\nFinally, we remark the following: although inference (i.e., finding the maximum output) is linear in the number of classes, the constant factors are favorable due to modern vectorized processors, and therefore proceeds at ≈ 1700 examples/sec for the embedding based approaches."
    }, {
      "heading" : "4.3 LSHTC",
      "text" : "The Large Scale Hierarchical Text Classification Challenge (version 4) was a public competition involving multilabel classification of documents into approximately 300,000 categories [24]. The training and test files are available from the Kaggle platform. The features are bag of words representations of each document.\nEmbedding Quality Assessment A representation of a DAG hierarchy associated with the classes is also available. We used this to assess the quality of various embedding strategies independent of classification performance. In particular, we computed the fraction of class embeddings whose nearest neighbor was also a sibling in the DAG, as shown in Table 4. “Most fraternal” refers to\nan embedding which arranges for every category’s nearest neighbor in the embedding to be the node in the DAG with the most siblings, i.e., the constant predictor baseline for this task. PLST [42] has performance close to Rembrandt according to this metric, so the 3.2 average nonzero classes per example is apparently enough for the approximation underlying PLST to be reasonable.\nEmpirical Label Covariance Spectrum Our embedding approach is based upon a low-rank assumption for the (unobservable) prediction covariance of equation (3). Because LSHTC is a multi-label dataset, we can use the empirical label covariance as a proxy to investigate the spectral properties of the prediction covariance and test our assumption. We used Algorithm 1 (i.e., two pass randomized PCA) to estimate the spectrum of the empirical label covariance, shown in Figure 1b. The spectrum decays modestly and suggests that an embedding dimension of k ≈ 1000 or more might be necessary for good classification performance.\nClassification Performance We built an end-to-end classifier using an approximate kernelized variant of Algorithm 3, where we processed the embeddings with Random Fourier Features [37], i.e., in line 4 of Algorithm 3 we use a random cosine feature map for φ. We found Cauchy distributed random vectors, corresponding to the Laplacian kernel, gave good results. We used 4,000 random features and tuned the kernel bandwidth via cross-validation on the training set.\nThe LSHTC competition metric is macro-averaged F1, which emphasizes performance on rare classes. However, we are using a multilabel classification algorithm which maximizes accuracy of predictions without regard to the importance of rare classes. Therefore we compare with published results of [36], who report example-averaged precision-at-k on the label ordering induced for each example. To facilitate comparison we do a 75:25 train-test split of the public training set, which is the same proportions as in their experiments (albeit a different split).\nBased upon the previous spectral analysis, we anticipate a large embedding dimension is required for best results. With our current implementation, up to the limit of available memory in our desktop machine (k = 800) we found increasing embedding dimensionality improved performance.\n“RE (k = . . .) + ILR” corresponds for Rembrandt coupled with independent (kernel) logistic regression, i.e., Algorithm 3. LPSR-NB is the Label Partitioning by Sub-linear Ranking algorithm of [46] composed with a Naive Bayes base learner, as reported in [36], where they also introduce and report precision for the multilabel tree learning algorithm FastXML. Inference for our best model\nproceeds at ≈ 60 examples/sec, substantially slower than for ODP, due to the larger output space, larger embedding dimensionality, and the use of random Fourier features."
    }, {
      "heading" : "5 Discussion",
      "text" : "In this paper we identify a correspondence between rank constrained regression and label embedding, and we exploit that correspondence along with randomized matrix decomposition techniques to develop a fast label embedding algorithm.\nTo facilitate analysis and implementation, we focused on linear prediction, which is equivalent to a simple neural network architecture with a single linear hidden layer bottleneck. Because linear predictors perform well for text classification, we obtained excellent experimental results, but more sophistication is required for tasks where deep architectures are state-of-the-art. Although the analysis presented herein would not strictly be applicable, it is plausible that replacing line 5 in Algorithm 2 with an optimization over a deep architecture could yield good embeddings. This would be computationally beneficial as reducing the number of outputs (i.e., predicting embeddings rather than labels) would mitigate space constraints for GPU training.\nOur technique leverages the (putative) low-rank structure of the prediction covariance of equation (3). For some problems a low-rank plus sparse assumption might be more appropriate. In such cases combining our technique with L1 regularization, e.g., on a classification residual or on separately regularized direct connections from the original inputs, might yield superior results.\nAcknowledgments We thank John Langford for providing the ALOI and ODP data sets."
    } ],
    "references" : [ {
      "title" : "Least squares revisited: Scalable approaches for multi-class prediction",
      "author" : [ "A. Agarwal", "S.M. Kakade", "N. Karampatziakis", "L. Song", "G. Valiant" ],
      "venue" : "Proceedings of The 31st International Conference on Machine Learning. pp. 541–549",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Partial least squares for discrimination",
      "author" : [ "M. Barker", "W. Rayens" ],
      "venue" : "Journal of chemometrics 17(3), 166–173",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Further aspects of the theory of multiple regression",
      "author" : [ "M.S. Bartlett" ],
      "venue" : "Mathematical Proceedings of the Cambridge Philosophical Society. vol. 34, pp. 33–40. Cambridge Univ Press",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1938
    }, {
      "title" : "A fast iterative shrinkage-thresholding algorithm for linear inverse problems",
      "author" : [ "A. Beck", "M. Teboulle" ],
      "venue" : "SIAM Journal on Imaging Sciences 2(1), 183–202",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Label embedding trees for large multi-class tasks",
      "author" : [ "S. Bengio", "J. Weston", "D. Grangier" ],
      "venue" : "Advances in Neural Information Processing Systems. pp. 163–171",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Refined experts: improving classification in large taxonomies",
      "author" : [ "P.N. Bennett", "N. Nguyen" ],
      "venue" : "Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval. pp. 11–18. ACM",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Predicting multivariate responses in multiple linear regression",
      "author" : [ "L. Breiman", "J.H. Friedman" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology) 59(1), 3–54",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Logarithmic time online multiclass prediction",
      "author" : [ "A. Choromanska", "J. Langford" ],
      "venue" : "arXiv preprint arXiv:1406.1822",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Learning compact class codes for fast inference in large multi class classification",
      "author" : [ "M. Cissé", "T. Artières", "P. Gallinari" ],
      "venue" : "Machine Learning and Knowledge Discovery in Databases, pp. 506–520. Springer",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Bayesian aggregation for hierarchical genre classification",
      "author" : [ "C. DeCoro", "Z. Barutcuoglu", "R. Fiebrink" ],
      "venue" : "ISMIR. pp. 77–80",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Large margin hierarchical classification",
      "author" : [ "O. Dekel", "J. Keshet", "Y. Singer" ],
      "venue" : "Proceedings of the twenty-first international conference on Machine learning. p. 27. ACM",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Imagenet: A large-scale hierarchical image database",
      "author" : [ "J. Deng", "W. Dong", "R. Socher", "L.J. Li", "K. Li", "L. Fei-Fei" ],
      "venue" : "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on. pp. 248–255. IEEE",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Generalized rank-constrained matrix approximations",
      "author" : [ "S. Friedland", "A. Torokhti" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications 29(2), 656–659",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Devise: A deep visual-semantic embedding model",
      "author" : [ "A. Frome", "G.S. Corrado", "J. Shlens", "S. Bengio", "J. Dean", "T Mikolov" ],
      "venue" : "Advances in Neural Information Processing Systems. pp. 2121–2129",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Partial least-squares regression: a tutorial",
      "author" : [ "P. Geladi", "B.R. Kowalski" ],
      "venue" : "Analytica chimica acta 185, 1–17",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "The Amsterdam library of object images",
      "author" : [ "J.M. Geusebroek", "G.J. Burghouts", "A.W. Smeulders" ],
      "venue" : "International Journal of Computer Vision 61(1), 103–112",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Recursive regularization for large-scale classification with hierarchical and graphical dependencies",
      "author" : [ "S. Gopal", "Y. Yang" ],
      "venue" : "Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining. pp. 257–265. ACM",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions",
      "author" : [ "N. Halko", "P.G. Martinsson", "J.A. Tropp" ],
      "venue" : "SIAM review 53(2), 217–288",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Relations between two sets of variates",
      "author" : [ "H. Hotelling" ],
      "venue" : "Biometrika pp. 321–377",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1936
    }, {
      "title" : "Multi-label prediction via compressed sensing",
      "author" : [ "D. Hsu", "S. Kakade", "J. Langford", "T. Zhang" ],
      "venue" : "NIPS. vol. 22, pp. 772–780",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Reduced-rank regression for the multivariate linear model",
      "author" : [ "A.J. Izenman" ],
      "venue" : "Journal of multivariate analysis 5(2), 248–264",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1975
    }, {
      "title" : "On the burstiness of visual elements",
      "author" : [ "H. Jégou", "M. Douze", "C. Schmid" ],
      "venue" : "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on. pp. 1169–1176. IEEE",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "The ECIR 2010 large scale hierarchical classification workshop",
      "author" : [ "A. Kosmopoulos", "E. Gaussier", "G. Paliouras", "S. Aseervatham" ],
      "venue" : "ACM SIGIR Forum. vol. 44, pp. 23–32. ACM",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Vowpal Wabbit",
      "author" : [ "J. Langford" ],
      "venue" : "https://github.com/JohnLangford/vowpal_ wabbit/wiki",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Word emdeddings through hellinger pca",
      "author" : [ "R. Lebret", "R. Collobert" ],
      "venue" : "arXiv preprint arXiv:1312.5542",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Simple and deterministic matrix sketching",
      "author" : [ "E. Liberty" ],
      "venue" : "Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining. pp. 581–588. ACM",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "The lasso and generalised linear models",
      "author" : [ "J. Lokhorst" ],
      "venue" : "Tech. rep., University of Adelaide, Adelaide",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Large scale canonical correlation analysis with iterative least squares",
      "author" : [ "Y. Lu", "D.P. Foster" ],
      "venue" : "arXiv preprint arXiv:1407.4508",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "T. Mikolov", "K. Chen", "G. Corrado", "J. Dean" ],
      "venue" : "arXiv preprint arXiv:1301.3781",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "randembed",
      "author" : [ "P. Mineiro" ],
      "venue" : "https://github.com/pmineiro/randembed",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A randomized algorithm for CCA",
      "author" : [ "P. Mineiro", "N. Karampatziakis" ],
      "venue" : "arXiv preprint arXiv:1411.3409",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A method of solving a convex programming problem with convergence rate O(1/k)",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Dokl. Akad. Nauk SSSR 269, 543–547",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 1983
    }, {
      "title" : "Zero-shot learning with semantic output codes",
      "author" : [ "M. Palatucci", "D. Pomerleau", "G.E. Hinton", "T.M. Mitchell" ],
      "venue" : "Advances in neural information processing systems. pp. 1410–1418",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Fastxml: a fast, accurate and stable tree-classifier for extreme multi-label learning",
      "author" : [ "Y. Prabhu", "M. Varma" ],
      "venue" : "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. pp. 263–272. ACM",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Random features for large-scale kernel machines",
      "author" : [ "A. Rahimi", "B. Recht" ],
      "venue" : "Advances in neural information processing systems. pp. 1177–1184",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "The utilization of multiple measurements in problems of biological classification",
      "author" : [ "C.R. Rao" ],
      "venue" : "Journal of the Royal Statistical Society. Series B (Methodological)",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 1948
    }, {
      "title" : "Predicting gene function using hierarchical multi-label decision tree ensembles",
      "author" : [ "L. Schietgat", "C. Vens", "J. Struyf", "H. Blockeel", "D. Kocev", "S. Džeroski" ],
      "venue" : "BMC Bioinformatics 11, 2",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Zero-shot learning through crossmodal transfer",
      "author" : [ "R. Socher", "M. Ganjoo", "C.D. Manning", "A. Ng" ],
      "venue" : "Advances in Neural Information Processing Systems. pp. 935– 943",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "On the equivalence between canonical correlation analysis and orthonormalized partial least squares",
      "author" : [ "L. Sun", "S. Ji", "S. Yu", "J. Ye" ],
      "venue" : "IJCAI. vol. 9, pp. 1230–1235",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Multilabel classification with principal label space transformation",
      "author" : [ "F. Tai", "H.T. Lin" ],
      "venue" : "Neural Computation 24(9), 2508–2542",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Multi-label linear discriminant analysis",
      "author" : [ "H. Wang", "C. Ding", "H. Huang" ],
      "venue" : "Computer Vision–ECCV 2010, pp. 126–139. Springer",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Large margin taxonomy embedding for document categorization",
      "author" : [ "K.Q. Weinberger", "O. Chapelle" ],
      "venue" : "Advances in Neural Information Processing Systems. pp. 1737– 1744",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Wsabie: Scaling up to large vocabulary image annotation",
      "author" : [ "J. Weston", "S. Bengio", "N. Usunier" ],
      "venue" : "IJCAI. vol. 11, pp. 2764–2770",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Label partitioning for sublinear ranking",
      "author" : [ "J. Weston", "A. Makadia", "H. Yee" ],
      "venue" : "Proceedings of the 30th International Conference on Machine Learning (ICML13). pp. 181–189",
      "citeRegEx" : "46",
      "shortCiteRegEx" : null,
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "Recent years have witnessed the emergence of many multiclass and multilabel datasets with increasing number of possible labels, such as ImageNet [12] and the Large Scale Hierarchical Text Classification (LSHTC) datasets [25].",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 22,
      "context" : "Recent years have witnessed the emergence of many multiclass and multilabel datasets with increasing number of possible labels, such as ImageNet [12] and the Large Scale Hierarchical Text Classification (LSHTC) datasets [25].",
      "startOffset" : 220,
      "endOffset" : 224
    }, {
      "referenceID" : 4,
      "context" : "This unifies prior work utilizing the confusion matrix for multiclass [5] and the empirical label covariance for multilabel [42].",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 39,
      "context" : "This unifies prior work utilizing the confusion matrix for multiclass [5] and the empirical label covariance for multilabel [42].",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 17,
      "context" : "We apply techniques from randomized linear algebra [19] to develop an efficient and scalable algorithm for constructing the embeddings, essentially via a novel randomized algorithm for rank-constrained squared loss regression.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 19,
      "context" : "The first step of our algorithm resembles compressed sensing approaches to extreme classification that use random matrices [21].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 17,
      "context" : "For a very thorough and more formal discussion see [19].",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 25,
      "context" : "Since principal eigenvectors can be thought as “frequent directions” [28], the range of Ψ will tend to be more aligned with the space spanned by the top eigenvectors of X>X.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 12,
      "context" : "This is a special case of a more general problem studied by [14]; specializing",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 29,
      "context" : "[32]",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 39,
      "context" : "For example, the principal label space transformation of [42] is an eigendecomposition of the empirical label covariance Y >Y .",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 20,
      "context" : ", regularization) benefits to label embeddings, corresponding to the rich literature of low-rank regression regularization [22].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 32,
      "context" : "Finally, embeddings can be part of a strategy for zero-shot learning [35], i.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 19,
      "context" : "[21], motivated by advances in compressed sensing, utilized a random embedding of the labels along with greedy sparse decoding strategy.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 39,
      "context" : "For the multilabel case, [42] construct a low-dimensional embedding using principal components on the empirical label covariance, which they utilize along with a greedy sparse decoding strategy.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 6,
      "context" : "For multivariate regression, [7] use the principal components of the empirical label covariance to define a shrinkage estimator which exploits correlations between the labels to improve accuracy.",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 42,
      "context" : "Conversely, [45] motivate their ranking-loss optimized embeddings solely by computational considerations of inference time and space complexity.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 10,
      "context" : "when composed with online learning [11]; Bayesian learning [10]; support vector machines [6]; and decision tree ensembles [39].",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 9,
      "context" : "when composed with online learning [11]; Bayesian learning [10]; support vector machines [6]; and decision tree ensembles [39].",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 5,
      "context" : "when composed with online learning [11]; Bayesian learning [10]; support vector machines [6]; and decision tree ensembles [39].",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 36,
      "context" : "when composed with online learning [11]; Bayesian learning [10]; support vector machines [6]; and decision tree ensembles [39].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 8,
      "context" : ", the sub-linear inference approach of [9] and the large margin approach of [44].",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 41,
      "context" : ", the sub-linear inference approach of [9] and the large margin approach of [44].",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 4,
      "context" : "Analogously, [5] utilize a surrogate classifier rather than side information to define a similarity matrix between classes; our procedure can efficiently produce a similarity matrix which can ease the computational burden of this portion of their procedure.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 13,
      "context" : ", [15] and [40].",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 37,
      "context" : ", [15] and [40].",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 16,
      "context" : "[18] focus nearly exclusively on the statistical benefit of incorporating label structure by overcoming the space and time complexity of large-scale oneagainst-all classification via distributed training and inference.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "Our objective in equation (1) is highly related to that of partial least squares [16], as Algorithm 2 corresponds to a randomized algorithm for PLS if the features have been whitened.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 1,
      "context" : "Unsurprisingly, supervised dimensionality reduction techniques such as PLS can be much better than unsupervised dimensionality reduction techniques such as PCA regression in the discriminative setting if the features vary in ways irrelevant to the classification task [2].",
      "startOffset" : 268,
      "endOffset" : 271
    }, {
      "referenceID" : 35,
      "context" : "Two other classical procedures for supervised dimensionality reduction are Fisher Linear Discriminant [38] and Canonical Correlation Analysis [20].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 18,
      "context" : "Two other classical procedures for supervised dimensionality reduction are Fisher Linear Discriminant [38] and Canonical Correlation Analysis [20].",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 2,
      "context" : "For multiclass problems these two techniques yield the same result [3,2], although for multilabel problems they are distinct.",
      "startOffset" : 67,
      "endOffset" : 72
    }, {
      "referenceID" : 1,
      "context" : "For multiclass problems these two techniques yield the same result [3,2], although for multilabel problems they are distinct.",
      "startOffset" : 67,
      "endOffset" : 72
    }, {
      "referenceID" : 40,
      "context" : "Indeed, extension of FLD to the multilabel case is a relatively recent development [43] whose straightforward implementation does not appear to be computationally viable for large number of classes.",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 1,
      "context" : "CCA and PLS are highly related, as CCA maximizes latent correlation and PLS maximizes latent covariance [2].",
      "startOffset" : 104,
      "endOffset" : 107
    }, {
      "referenceID" : 38,
      "context" : "Furthermore, CCA produces equivalent results to PLS if the features are whitened [41].",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 27,
      "context" : "Regarding computational considerations, scalable CCA algorithms are available [30,33], but it remains open how to specialize them to this context to leverage the equivalent of equation (2); whereas, if CCA is desired, Algorithm 2 can be utilized in conjunction with whitening pre-processing.",
      "startOffset" : 78,
      "endOffset" : 85
    }, {
      "referenceID" : 30,
      "context" : "Regarding computational considerations, scalable CCA algorithms are available [30,33], but it remains open how to specialize them to this context to leverage the equivalent of equation (2); whereas, if CCA is desired, Algorithm 2 can be utilized in conjunction with whitening pre-processing.",
      "startOffset" : 78,
      "endOffset" : 85
    }, {
      "referenceID" : 28,
      "context" : ", word2vec [31], which (empirically) provide analogous statistical and computational benefits despite being unsupervised.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 24,
      "context" : "The text embedding technique of [27] is a particularly interesting comparison because it is a variant",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 21,
      "context" : "This suggests that unsupervised dimensionality reduction approaches can work well when additional structure of the input domain is incorporated, in this case by modeling word burstiness with the square root nonlinearity [23] and word order via decomposing neighborhood statistics.",
      "startOffset" : 220,
      "endOffset" : 224
    }, {
      "referenceID" : 24,
      "context" : "Nonetheless [27] note that when maximum statistical performance is desired, the embeddings must be fine-tuned to the particular task, i.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 26,
      "context" : "Another plausible regularization technique which mitigates inference space and time complexity is L1 regularization [29].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 15,
      "context" : "ALOI is a color image collection of one-thousand small objects recorded for scientific purposes [17].",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 7,
      "context" : "For these experiments we will consider test classification accuracy utilizing the same train-test split and features from [8].",
      "startOffset" : 122,
      "endOffset" : 125
    }, {
      "referenceID" : 0,
      "context" : ", embedding dimensionality) for this logistic regression is modest so the second order techniques of [1] are applicable (in particular, their Algorithm 1 with a simple modification to include acceleration [34,4]).",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 31,
      "context" : ", embedding dimensionality) for this logistic regression is modest so the second order techniques of [1] are applicable (in particular, their Algorithm 1 with a simple modification to include acceleration [34,4]).",
      "startOffset" : 205,
      "endOffset" : 211
    }, {
      "referenceID" : 3,
      "context" : ", embedding dimensionality) for this logistic regression is modest so the second order techniques of [1] are applicable (in particular, their Algorithm 1 with a simple modification to include acceleration [34,4]).",
      "startOffset" : 205,
      "endOffset" : 211
    }, {
      "referenceID" : 7,
      "context" : "We also compare against Lomtree (LT), which has training and test time complexity logarithmic in the number of classes [8].",
      "startOffset" : 119,
      "endOffset" : 122
    }, {
      "referenceID" : 23,
      "context" : "Both OAA and LT are provided by the Vowpal Wabbit [26] machine learning tool.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 5,
      "context" : "The Open Directory Project [13] is a public human-edited directory of the web which was processed by [6] into a multiclass data set.",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 7,
      "context" : "For these experiments we will consider test classification error utilizing the same train-test split, features, and labels from [8].",
      "startOffset" : 128,
      "endOffset" : 131
    }, {
      "referenceID" : 39,
      "context" : "PLST [42] has performance close to Rembrandt according to this metric, so the 3.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 34,
      "context" : "Classification Performance We built an end-to-end classifier using an approximate kernelized variant of Algorithm 3, where we processed the embeddings with Random Fourier Features [37], i.",
      "startOffset" : 180,
      "endOffset" : 184
    }, {
      "referenceID" : 33,
      "context" : "Therefore we compare with published results of [36], who report example-averaged precision-at-k on the label ordering induced for each example.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 43,
      "context" : "LPSR-NB is the Label Partitioning by Sub-linear Ranking algorithm of [46] composed with a Naive Bayes base learner, as reported in [36], where they also introduce and report precision for the multilabel tree learning algorithm FastXML.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 33,
      "context" : "LPSR-NB is the Label Partitioning by Sub-linear Ranking algorithm of [46] composed with a Naive Bayes base learner, as reported in [36], where they also introduce and report precision for the multilabel tree learning algorithm FastXML.",
      "startOffset" : 131,
      "endOffset" : 135
    } ],
    "year" : 2015,
    "abstractText" : "Many modern multiclass and multilabel problems are characterized by increasingly large output spaces. For these problems, label embeddings have been shown to be a useful primitive that can improve computational and statistical efficiency. In this work we utilize a correspondence between rank constrained estimation and low dimensional label embeddings that uncovers a fast label embedding algorithm which works in both the multiclass and multilabel settings. The result is a randomized algorithm whose running time is exponentially faster than naive algorithms. We demonstrate our techniques on two large-scale public datasets, from the Large Scale Hierarchical Text Challenge and the Open Directory Project, where we obtain state of the art results.",
    "creator" : "LaTeX with hyperref package"
  }
}