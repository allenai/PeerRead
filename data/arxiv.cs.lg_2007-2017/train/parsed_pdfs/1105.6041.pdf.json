{
  "name" : "1105.6041.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n10 5.\n60 41\nv1 [\ncs .L\nG ]\n3 0\nM ay\nKeywords: Online learning, classification, maximum margin."
    }, {
      "heading" : "1 Introduction",
      "text" : "It is a common belief that learning machines able to produce solution hyperplanes with large margins exhibit greater generalization ability [21] and this justifies the enormous interest in Support Vector Machines (SVMs) [21, 2]. Typically, SVMs obtain large margin solutions by solving a constrained quadratic optimization problem using dual variables. In their native form, however, efficient implementation is hindered by the quadratic dependence of their memory requirements in the number of training examples a fact which renders prohibitive the processing of large datasets. To overcome this problem decomposition methods [15, 6] were developed that apply optimization only to a subset of the training set. Although such methods led to improved convergence rates, in practice their superlinear dependence on the number of examples, which can be even cubic, can still lead to excessive runtimes when large datasets are processed. Recently, the so-called linear SVMs [7, 8, 13] made their appearance. They take advantage of linear kernels in order to allow parts of them to be written in primal notation and were shown to outperform decomposition SVMs when dealing with massive datasets.\nThe above considerations motivated research in alternative large margin classifiers naturally formulated in primal space long before the advent of linear SVMs. Such algorithms are mostly based on the perceptron [16, 12], the simplest online learning algorithm for binary linear classification. Like the perceptron, they focus on the primal problem by updating a weight vector which represents\nat each step the current state of the algorithm whenever a data point presented to it satisfies a specific condition. It is the ability of such algorithms to process one example at a time1 that allows them to spare time and memory resources and consequently makes them able to handle large datasets. The first algorithm of that kind is the perceptron with margin [3] which is much older than SVMs. It is an immediate extension of the perceptron which provably achieves solutions with only up to 1/2 of the maximum margin [10]. Subsequently, various algorithms succeeded in approximately attaining maximum margin by employing modified perceptron-like update rules. Such algorithms include ROMMA [11], ALMA [5], CRAMMA [19] and MICRA [20]. Very recently, the same goal was accomplished by a generalized perceptron with margin, the margitron [14].\nThe most straightforward way of obtaining large margin solutions through a perceptron is by requiring that the weight vector be updated every time the example presented to the algorithm has (normalized) margin which does not exceed a predefined value [17, 18, 1]. The obvious problem with this idea, however, is that the algorithm with such a fixed margin condition will definitely not converge unless the target value of the margin is smaller than the unknown maximum margin. In an earlier work [14] we noticed that the upper bound ‖at‖ /t on the maximum margin, with ‖at‖ being the length of the weight vector and t the number of updates, that comes as an immediate consequence of the perceptron update rule is very accurate and tends to improve as the algorithm achieves larger margins. In the present work we replace the fixed target margin value with a fraction 1− ǫ of this varying upper bound on the maximum margin. The hope is that as the algorithm keeps updating its state the upper bound will keep approaching the maximum margin and convergence to a solution with the desired accuracy ǫ will eventually occur. Thus, the resulting algorithm may be regarded as a realizable implementation of the perceptron with fixed margin condition.\nThe rest of this paper is organized as follows. Section 2 contains some preliminaries and a motivation of the algorithm based on a qualitative analysis. In Sect. 3 we give a formal theoretical analysis. Section 4 is devoted to implementational issues. Section 5 contains our experimental results while Sect. 6 our conclusions."
    }, {
      "heading" : "2 Motivation of the Algorithm",
      "text" : "Let us consider a linearly separable training set {(xk, lk)}mk=1, with vectors xk ∈ IRd and labels lk ∈ {+1,−1}. This training set may either be the original dataset or the result of a mapping into a feature space of higher dimensionality [21, 2]. Actually, there is a very well-known construction [4] making linear separability always possible, which amounts to the adoption of the 2-norm soft margin. By placing xk in the same position at a distance ρ in an additional dimension, i.e. by extending xk to [xk, ρ], we construct an embedding of our data into the socalled augmented space [3]. This way, we construct hyperplanes possessing bias\n1 The conversion of online algorithms to the batch setting is done by cycling repeatedly through the dataset and using the last hypothesis for prediction.\nin the non-augmented feature space. Following the augmentation, a reflection with respect to the origin of the negatively labeled patterns is performed by multiplying every pattern with its label. This allows for a uniform treatment of both categories of patterns. Also, R ≡ max\nk ‖yk‖ with yk ≡ [lkxk, lkρ] the kth\naugmented and reflected pattern. Obviously, R ≥ ρ. The relation characterizing optimally correct classification of the training patterns yk by a weight vector u of unit norm in the augmented space is\nu · yk ≥ γd ≡ max u ′ : ∥ ∥ ∥ u ′ ∥ ∥ ∥ = 1 min i {u′ · yi} ∀k . (1)\nWe shall refer to γd as the maximum directional margin. It coincides with the maximum margin in the augmented space with respect to hyperplanes passing through the origin. For the maximum directional margin γd and the maximum geometric margin γ in the non-augmented feature space, it holds that 1 ≤ γ/γd ≤ R/ρ. As ρ → ∞, R/ρ → 1 and, consequently, γd → γ [17, 18].\nWe consider algorithms in which the augmented weight vector at is initially set to zero, i.e. a0 = 0, and is updated according to the classical perceptron rule\nat+1 = at + yk (2)\neach time an appropriate misclassification condition is satisfied by a training pattern yk. Taking the inner product of (2) with the optimal direction u and using (1) we get u · at+1 − u · at = u · yk ≥ γd a repeated application of which gives [12]\n‖at‖ ≥ u · at ≥ γdt . (3) From (3) we readily obtain\nγd ≤ ‖at‖ t\n(4)\nprovided t > 0. Notice that the above upper bound on the maximum directional margin γd is an immediate consequence of the classical perceptron rule and holds independent of the misclassification condition.\nIt would be very desirable that ‖at‖ /t approaches γd with t increasing since this would provide an after-run estimate of the accuracy achieved by an algorithm employing the classical perceptron update. More specifically, with γ′d being the directional margin achieved upon convergence of the algorithm in tc updates, it holds that\nγd − γ′d γd ≤ 1− γ ′ dtc ‖atc‖ . (5)\nIn order to understand the mechanism by which ‖at‖ /t evolves we consider the difference between two consecutive values of ‖at‖2 /t2 which may be shown to be given by the relation\n‖at‖2 t2 −‖at+1‖ 2 (t+ 1)2 =\n1\nt(t+ 1)\n{(\n‖at‖2 t − at · yk\n)\n+\n(\n‖at+1‖2 t+ 1 − at+1 · yk\n)}\n.\n(6)\nLet us assume that satisfaction of the misclassification condition by a pattern yk has as a consequence that ‖at‖2/t > at · yk (i.e., the normalized margin ut · yk of yk (with ut ≡ at/ ‖at‖) is smaller than the upper bound (4) on γd). Let us further assume that after the update has taken place yk still satisfies the misclassification condition and therefore ‖at+1‖2/(t+ 1) > at+1 · yk. Then, the r.h.s. of (6) is positive and ‖at‖ /t decreases as a result of the update. In the event, instead, that the update leads to violation of the misclassification condition, ‖at+1‖2/(t+ 1) is not necessarily larger than at+1 · yk and ‖at‖ /t may not decrease as a result of the update. We expect that statistically, at least in the early stages of the algorithm, most updates do not lead to correctly classified patterns (i.e., patterns which violate the misclassification condition) and as a consequence ‖at‖ /t will have the tendency to decrease. Obviously, the rate at which this will take place depends on the size of the difference ‖at‖2 /t−at · yk which, in turn, depends on the misclassification condition.\nIf we are interested in obtaining solutions possessing margin the most natural choice of misclassification condition is the fixed (normalized) margin condition\nat · yk ≤ (1− ǫ)γd ‖at‖ (7)\nwith the accuracy parameter ǫ satisfying 0 < ǫ ≤ 1. This is an example of a misclassification condition which if it is satisfied ensures that ‖at‖2/t > at·yk. Moreover, by making use of (4) and (7) it may easily be shown that ‖at+1‖2/(t+ 1) ≥ at+1 · yk for t ≥ ǫ−1R2/γ2d. Thus, after at most ǫ−1R2/γ2d updates ‖at‖ /t decreases monotonically. The perceptron algorithm with fixed margin condition (PFM) is known to converge in a finite number of updates to an ǫ-accurate approximation of the maximum directional margin hyperplane [17, 18, 1]. Although it appears that PFM demands exact knowledge of the value of γd, we notice that only the value of β ≡ (1 − ǫ)γd, which is the quantity entering (7), needs to be set and not the values of ǫ and γd separately. That is why the after-run estimate (5) is useful in connection with the algorithm in question. Nevertheless, in order to make sure that β < γd a priori knowledge of a fairly good lower bound on γd is required and this is an obvious defect of PFM.\nThe above difficulty associated with the fixed margin condition may be remedied if the unknown γd is replaced for t > 0 with its varying upper bound ‖at‖ /t\nat · yk ≤ (1− ǫ) ‖at‖2\nt . (8)\nCondition (8) ensures that ‖at‖2/t − at · yk ≥ ǫ‖at‖2/t > 0. Moreover, as in the case of the fixed margin condition, ‖at+1‖2/(t+ 1) − at+1 · yk ≥ 0 for t ≥ ǫ−1R2/γ2d. As a result, after at most ǫ−1R2/γ2d updates the r.h.s. of (6) is bounded from below by ǫ ‖at‖2 /t2(t + 1) ≥ ǫγ2d/(t + 1) and ‖at‖ /t decreases monotonically and sufficiently fast. Thus, we expect that ‖at‖ /t will eventually approach γd close enough, thereby allowing for convergence of the algorithm to an ǫ-accurate approximation of the maximum directional margin hyperplane. It is also apparent that the decrease of ‖at‖ /t will be faster for larger values of ǫ.\nThe Perceptron with Dynamic Margin\nInput: A linearly separable augmented dataset S = (y1, . . . ,yk, . . . ,ym) with reflection assumed Fix: ǫ Define: qk = ‖yk‖\n2 , ǭ = 1− ǫ Initialize: t = 0, a0 = 0, ℓ0 = 0, θ0 = 0 repeat\nfor k = 1 to m do ptk = at · yk if ptk ≤ θt then\nat+1 = at + yk ℓt+1 = ℓt + 2ptk + qk t ← t+ 1 θt = ǭ ℓt/t\nuntil no update made within the for loop\nThe perceptron algorithm employing the misclassification condition (8) (with its threshold set to 0 for t = 0), which may be regarded as originating from (7) with γd replaced for t > 0 by its dynamic upper bound ‖at‖ /t, will be named the perceptron with dynamic margin (PDM).\n3 Theoretical Analysis\nFrom the discussion that led to the formulation of PDM it is apparent that if the algorithm converges it will achieve by construc-\ntion a solution possessing directional margin at least as large as (1 − ǫ)γd. (We remind the reader that convergence assumes violation of the misclassification condition (8) by all patterns. In addition, (4) holds.) The same obviously applies to PFM. Thus, for both algorithms it only remains to be demonstrated that they converge in a finite number of steps. This has already been shown for PFM [17, 18, 1] but no general ǫ-dependent bound in closed form has been derived. Our purpose in this section is to demonstrate convergence of PDM and provide explicit bounds for both algorithms.\nBefore we proceed with our analysis we will need the following result.\nLemma 1. Let the variable t ≥ e−C satisfy the inequality\nt < δ(1 + C + ln t) , (9)\nwhere δ, C are constants and δ > e−C. Then\nt ≤ t0 ≡ (1 + e−1)δ (C + ln ((1 + e)δ)) . (10)\nProof. If t ≥ e−C then (1 + C + ln t) ≥ 1 and inequality (9) is equivalent to f(t) = t/(1 + C + ln t) − δ < 0. For the function f(t) defined in the interval [e−C ,+∞) it holds that f(e−C) < 0 and df/dt = (C + ln t)/(1 + C + ln t)2 > 0 for t > e−C . Stated differently, f(t) starts from negative values at t = e−C and increases monotonically. Therefore, if f(t0) ≥ 0 then t0 is an upper bound of all t for which f(t) < 0. Indeed, it is not difficult to verify that t0 > δ > e −C and\nf(t0) = δ\n(\n(1 + e−1)\n(\n1 + ln ln(eC(1 + e)δ)\nln(eC(1 + e)δ)\n)−1 − 1 ) ≥ 0\ngiven that ln lnx/ lnx ≤ e−1. ⊓⊔\nNow we are ready to derive an upper bound on the number of steps of PFM.\nTheorem 1. The number t of updates of the perceptron algorithm with fixed margin condition satisfies the bound\nt ≤ (1 + e −1)\n2ǫ\nR2 γ2d\n{\n4 γd R ( 1− γd R (1− ǫ) ) + ln\n(\n(1 + e)\nǫ\nR\nγd\n( 1− γd R (1− ǫ) )\n)}\n.\nProof. From (2) and (7) we get\n‖at+1‖2 = ‖at‖2 + ‖yk‖2 + 2at · yk ≤ ‖at‖2 ( 1 + R2\n‖at‖2 + 2(1− ǫ)γd ‖at‖\n)\n.\nThen, taking the square root and using the inequality √ 1 + x ≤ 1+x/2 we have\n‖at+1‖ ≤ ‖at‖ ( 1 + R2\n‖at‖2 + 2(1− ǫ)γd ‖at‖\n) 1 2\n≤ ‖at‖ ( 1 + R2\n2 ‖at‖2 + (1− ǫ)γd ‖at‖\n)\n.\nNow, by making use of ‖at‖ ≥ γdt, we observe that\n‖at+1‖ − ‖at‖ ≤ R2\n2 ‖at‖ + (1− ǫ)γd ≤\nR2\n2γd\n1 t + (1− ǫ)γd .\nA repeated application of the above inequality t−N times (t > N ≥ 1) gives\n‖at‖ − ‖aN‖ ≤ R2\n2γd\nt−1 ∑\nk=N\nk−1 + (1 − ǫ)γd(t−N)\n< R2\n2γd\n(\n1\nN +\n∫ t\nN\nk−1dk\n)\n+ (1− ǫ)γd(t−N)\nfrom where using the obvious bound ‖aN‖ ≤ RN we get an upper bound on ‖at‖\n‖at‖ < R2\n2γd\n(\n1\nN + ln\nt\nN\n)\n+ (1− ǫ)γd(t−N) +RN .\nCombining the above upper bound on ‖at‖, which holds not only for t > N but also for t = N , with the lower bound from (3) we obtain\nt < 1\n2ǫ\nR2 γ2d\n{\n1 N − lnN + 2γd R ( 1− γd R (1− ǫ) ) N + ln t\n}\n.\nSetting\nδ = 1\n2ǫ\nR2 γ2d , α = 2 γd R ( 1− γd R (1− ǫ) )\nand choosing N = 1+ [α−1], with [x] being the integer part of x ≥ 0, we finally get\nt < δ(1 + 2α+ lnα+ ln t) . (11)\nNotice that in deriving (11) we made use of the fact that αN +N−1 − lnN < 1 + 2α + lnα. Inequality (11) has the form (9) with C = 2α + lnα. Obviously, e−C < α−1 < N ≤ t and e−C < α−1 ≤ δ. Thus, the conditions of Lemma 1 are satisfied and the required bound, which is of the form (10), follows from (11). ⊓⊔\nFinally, we arrive at our main result which is the proof of convergence of PDM in a finite number of steps and the derivation of the relevant upper bound.\nTheorem 2. The number t of updates of the perceptron algorithm with dynamic margin satisfies the bound\nt ≤\n\n      \n       \nt0\n(\n1− 11−2ǫ R 2\nγ2 d\nt−10\n) 1 2ǫ\n, t0 ≡ [ǫ−1] (\nR γd\n) 1 ǫ (\n1 + [ǫ −1]−1\n1−2ǫ\n) 1 2ǫ\nif ǫ < 12\n(1 + e−1)R 2\nγ2 d\nln ( (1 + e)R 2\nγ2 d\n)\nif ǫ = 12\nt0 ( 1− 2(1− ǫ)t1−2ǫ0 ) , t0 ≡ ǫ(3−2ǫ)2ǫ−1 R 2\nγ2 d\nif ǫ > 12 .\nProof. From (2) and (8) we get\n‖at+1‖2 = ‖at‖2 + 2at · yk + ‖yk‖2 ≤ ‖at‖2 ( 1 + 2(1− ǫ)\nt\n)\n+R2 . (12)\nLet us assume that ǫ < 1/2. Then, using the inequality (1+ x)ζ ≥ 1+ ζx for x ≥ 0, ζ = 2(1− ǫ) ≥ 1 in (12) we obtain\n‖at+1‖2 ≤ ‖at‖2 ( 1 + 1\nt\n)2(1−ǫ)\n+R2\nfrom where by dividing both sides with (t+ 1)2(1−ǫ) we arrive at\n‖at+1‖2 (t+ 1)2(1−ǫ) − ‖at‖ 2 t2(1−ǫ) ≤ R 2 (t+ 1)2(1−ǫ) .\nA repeated application of the above inequality t−N times (t > N ≥ 1) gives\n‖at‖2 t2(1−ǫ) − ‖aN‖ 2 N2(1−ǫ) ≤ R2 t ∑\nk=N+1\nk−2(1−ǫ) ≤ R2 ∫ t\nN\nk−2(1−ǫ)dk\n= R2N2ǫ−1\n2ǫ− 1\n(\n(\nt\nN\n)2ǫ−1 − 1 ) .(13)\nNow, let us define\nαt ≡ ‖at‖ Rt\nand observe that the bounds ‖at‖ ≤ Rt and ‖at‖ ≥ γdt confine αt to lie in the range\nγd R ≤ αt ≤ 1 .\nSetting ‖aN‖ = αNRN in (13) we get the following upper bound on ‖at‖2\n‖at‖2 ≤ t2(1−ǫ)α2NR2N2ǫ { 1 + α−2N N −1\n2ǫ− 1\n(\n(\nt\nN\n)2ǫ−1 − 1 )}\nwhich combined with the lower bound ‖at‖2 ≥ γ2dt2 leads to\nt2ǫ ≤ α2N R2\nγ2d N2ǫ\n{\n1 + α−2N N −1\n2ǫ− 1\n(\n(\nt\nN\n)2ǫ−1 − 1 )} . (14)\nFor ǫ < 1/2 the term proportional to (t/N)2ǫ−1 in (14) is negative and may be dropped to a first approximation leading to the looser upper bound t0\nt0 ≡ N ( αN R\nγd\n) 1 ǫ (\n1 + α−2N N −1\n1− 2ǫ\n) 1 2ǫ\n(15)\non the number t of updates. Then, we may replace t with its upper bound t0 in the r.h.s. of (14) and get the improved bound\nt ≤ t0 ( 1− 1 1− 2ǫ R2 γ2d t−10 )\n1 2ǫ\n.\nThis is allowed given that the term proportional to (t/N)2ǫ−1 in (14) is negative and moreover t is raised to a negative power. Choosing N = [ǫ−1] and αN = 1 (i.e., setting αN to its upper bound which is the least favorable assumption) we obtain the bound stated in Theorem 2 for ǫ < 1/2.\nNow, let ǫ > 1/2. Then, using the inequality (1+x)ζ + ζ(1− ζ)x2/2 ≥ 1+ ζx for x ≥ 0, 0 ≤ ζ = 2(1− ǫ) ≤ 1 in (12) and the bound ‖at‖ ≤ Rt we obtain\n‖at+1‖2 ≤ ‖at‖2 ( 1 + 1\nt\n)2(1−ǫ) + (1 − ǫ)(2ǫ− 1)‖at‖ 2\nt2 +R2\n≤ ‖at‖2 ( 1 + 1\nt\n)2(1−ǫ)\n+ ǫ(3− 2ǫ)R2 .\nBy dividing both sides of the above inequality with (t+ 1)2(1−ǫ) we arrive at\n‖at+1‖2 (t+ 1)2(1−ǫ) − ‖at‖ 2 t2(1−ǫ) ≤ ǫ(3− 2ǫ) R 2 (t+ 1)2(1−ǫ) (16)\na repeated application of which, using also ‖a1‖2 ≤ R2 ≤ ǫ(3− 2ǫ)R2, gives\n‖at‖2 t2(1−ǫ) ≤ ǫ(3− 2ǫ)R2 t ∑\nk=1\nk−2(1−ǫ) ≤ ǫ(3− 2ǫ)R2 ( 1 + ∫ t\n1\nk−2(1−ǫ)dk\n)\n= ǫ(3− 2ǫ)R2 ( 1 + t2ǫ−1 − 1 2ǫ− 1 ) .\nCombining the above bound with the bound ‖at‖2 ≥ γ2dt2 we obtain\nt2ǫ ≤ ǫ(3− 2ǫ)R 2\nγ2d\n(\n1 + t2ǫ−1 − 1 2ǫ− 1\n)\n(17)\nor\nt ≤ ǫ(3− 2ǫ) 2ǫ− 1 R2\nγ2d\n( 1− 2(1− ǫ)t1−2ǫ ) . (18)\nFor ǫ > 1/2 the term proportional to t1−2ǫ in (18) is negative and may be dropped to a first approximation leading to the looser upper bound t0\nt0 ≡ ǫ(3− 2ǫ) 2ǫ− 1 R2\nγ2d\non the number t of updates. Then, we may replace t with its upper bound t0 in the r.h.s. of (18) and get the improved bound stated in Theorem 2 for ǫ > 1/2. This is allowed given that the term proportional to t1−2ǫ in (18) is negative and moreover t is raised to a negative power.\nFinally, taking the limit ǫ → 1/2 in (14) (with N = 1, αN = 1) or in (17) we get\nt ≤ R 2\nγ2d (1 + ln t)\nwhich on account of Lemma 1 leads to the bound of Theorem 2 for ǫ = 1/2. ⊓⊔\nRemark 1. The bound of Theorem 2 holds for PFM as well on account of (4).\nThe worst-case bound of Theorem 2 for ǫ ≪ 1 behaves like ǫ−1(R/γd) 1 ǫ which suggests an extremely slow convergence if we require margins close to the maximum. From expression (15) for t0, however, it becomes apparent that a more favorable assumption concerning the value of αN (e.g., αN ≪ 1 or even as low as αN ∼ γd/R) after the first N ≫ α−2N updates does lead to tremendous improvement provided, of course, that N is not extremely large. Such a sharp decrease of ‖at‖ /t in the early stages of the algorithm, which may be expected from relation (6) and the discussion that followed, lies behind its experimentally exhibited rather fast convergence.\nIt would be interesting to find a procedure by which the algorithm will be forced to a guaranteed sharp decrease of the ratio ‖at‖ /t. The following two observations will be vital in devising such a procedure. First, we notice that when PDM with accuracy parameter ǫ has converged in tc updates the threshold (1−ǫ)‖atc‖2/tc of the misclassification condition must have fallen below γd ‖atc‖. Otherwise, the normalized margin utc ·yk of all patterns yk would be larger than γd. Thus, αtc < (1− ǫ)−1γd/R. Second, after convergence of the algorithm with accuracy parameter ǫ1 in tc1 updates we may lower the accuracy parameter from the value ǫ1 to the value ǫ2 and continue the run from the point where convergence with parameter ǫ1 has taken place since for all updates that took place during the first run the misclassified patterns would certainly satisfy (at that time) the condition associated with the smaller parameter ǫ2. This way, the first run is legitimately fully incorporated into the second one and the tc1 updates required for convergence during the first run may be considered the first tc1 updates of the second run under this specific policy of presenting patterns to the algorithm. Combining the above two observations we see that by employing\na first run with accuracy parameter ǫ1 we force the algorithm with accuracy parameter ǫ2 < ǫ1 to have αt decreased from a value ∼ 1 to a value αtc1 < (1− ǫ1)−1γd/R in the first tc1 updates.\nThe above discussion suggests that we consider a decreasing sequence of parameters ǫn such that ǫn+1 = ǫn/η (η > 1) starting with ǫ0 = 1/2 and ending with the required accuracy ǫ and perform successive runs of PDM with accuracies ǫn until convergence in tcn updates is reached. According to our earlier discussion tcn includes the updates that led the algorithm to convergence in the current and all previous runs. Moreover, at the end of the run with parameter ǫn we will have ensured that αtcn < (1− ǫn)−1γd/R. Therefore, tcn+1 satisfies tcn+1 ≤ t0 or\ntcn+1 ≤ tcn ( 1\n1− ǫn\n)η/ǫn (\n1 + (1− ǫn)2 1− 2ǫn/η R2 γ2d t−1cn\n)η/2ǫn\n.\nThis is obtained by substituting in (15) the values ǫ = ǫn+1 = ǫn/η, N = tcn and αN = (1− ǫn)−1γd/R which is the least favorable choice for αtcn . Let us assume that ǫn ≪ 1 and set tcn = ξ−1n R2/γ2d with ξn ≪ 1. Then, 1/(1− ǫn)η/ǫn ≃ eη and\n(\n1 + (1− ǫn)2 1− 2ǫn/η R2 γ2d t−1cn\n)η/2ǫn\n≃ (1 + ξn)η/2ǫn ≃ eηξn/2ǫn .\nFor ξn ≃ ǫn the term above becomes approximately eη/2 while for ξn ≪ ǫn approaches 1. We see that under the assumption that PDM with accuracy parameter ǫn converges in a number of updates ≫ R2/γ2d the ratio tcn+1/tcn in the successive run scenario is rather tightly constrained. If, instead, our assumption is not satisfied then convergence of the algorithm is fast anyway. Notice, that the value of tcn+1/tcn inferred from the bound of Theorem 2 is ∼ η (R/γd)(η−1)/ǫn which is extremely large. We conclude that PDM employing the successive run scenario (PDM-succ) potentially converges in a much smaller number of steps."
    }, {
      "heading" : "4 Efficient Implementation",
      "text" : "To reduce the computational cost involved in running PDM, we extend the procedure of [14, 13] and construct a three-member nested sequence of reduced “active sets” of data points. As we cycle once through the full dataset, the (largest) first-level active set is formed from the points of the full dataset satisfying at · yk ≤ c1(1 − ǫ) ‖at‖2 /t with c1 = 2.2. Analogously, the second-level active set is formed as we cycle once through the first-level active set from the points which satisfy at · yk ≤ c2(1 − ǫ) ‖at‖2 /t with c2 = 1.1. The third-level active set comprises the points that satisfy at · yk ≤ (1− ǫ) ‖at‖ 2 /t as we cycle once through the second-level active set. The third-level active set is presented repetitively to the algorithm for Nep3 mini-epochs. Then, the second-level active set is presented Nep2 times. During each round involving the second-level set, a new third-level set is constructed and a new cycle of Nep3 passes begins. When the number of Nep2 cycles involving the second-level set is reached the first-level\nset becomes active again leading to the population of a new second-level active set. By invoking the first-level set for the (Nep1+1)\nth time, we trigger the loading of the full dataset and the procedure starts all over again until no point is found misclassified among the ones comprising the full dataset. Of course, the Nep1 , Nep2 and Nep3 rounds are not exhausted if no update takes place during a round. In all experiments we choose Nep1 = 9, Nep2 = Nep3 = 12. In addition, every time we make use of the full dataset we actually employ a permuted instance of it. Evidently, the whole procedure amounts to a different way of sequentially presenting the patterns to the algorithm and does not affect the applicability of our theoretical analysis. A completely analogous procedure is followed for PFM.\nAn additional mechanism providing a substantial improvement of the computational efficiency is the one of performing multiple updates [14, 13] once a data point is presented to the algorithm. It is understood, of course, that in order for a multiple update to be compatible with our theoretical analysis it should be equivalent to a certain number of updates occuring as a result of repeatedly presenting to the algorithm the data point in question. For PDM when a pattern yk is found to satisfy the misclassification condition (8) we perform λ = [µ+] + 1 updates at once. Here, µ+ is the smallest non-negative root of the quadratic equation in the variable µ derivable from the relation (t+ µ)at+µ · yk − (1 − ǫ) ‖at+µ‖2 = 0 in which at+µ ·yk = at ·yk +µ ‖yk‖2 and ‖at+µ‖2 = ‖at‖2 +2µat ·yk +µ2 ‖yk‖2. Thus, we require that as a result of the multiple update the pattern violates the misclassification condition. Similarly, we perform multiple updates for PFM.\nFinally, in the case of PDM (no successive runs) when we perform multiple updates we start doing so after the first full epoch. This way, we avoid the excessive growth of the length of the weight vector due to the contribution to the solution of many aligned patterns in the early stages of the algorithm which hinders the fast decrease of ‖at‖ /t. Moreover, in this scenario when we select the first-level active set as we go through the full dataset for the first time (first full epoch) we found it useful to set c1 = c2 = 1.1 instead of c1 = 2.2."
    }, {
      "heading" : "5 Experimental Evaluation",
      "text" : "We compare PDM with several other large margin classifiers on the basis of their ability to achieve fast convergence to a certain approximation of the “optimal” hyperplane in the feature space where the patterns are linearly separable. For linearly separable data the feature space is the initial instance space whereas for inseparable data (which is the case here) a space extended by as many dimensions as the instances is considered where each instance is placed at a distance ∆ from the origin in the corresponding dimension2 [4]. This extension generates a margin of at least ∆/ √ m. Moreover, its employment relies on the well-known\n2 yk = [ȳk, lk∆δ1k, . . . , lk∆δmk], where δij is Kronecker’s δ and ȳk the projection of the kth extended instance yk (multiplied by its label lk) onto the initial instance space. The feature space mapping defined by the extension commutes with a possible augmentation (with parameter ρ) in which case ȳk = [lkx̄k, lkρ]. Here x̄k represents the kth data point.\nequivalence between the hard margin optimization in the extended space and the soft margin optimization in the initial instance space with objective function ‖w‖2 +∆−2 ∑\niξ̄ 2 i involving the weight vector w and the 2-norm of the slacks ξ̄i\n[2]. Of course, all algorithms are required to solve identical hard margin problems.\nThe datasets we used for training are: the Adult (m = 32561 instances, n = 123 attributes) and Web (m = 49749, n = 300) UCI datasets as compiled by Platt [15], the training set of the KDD04 Physics dataset (m = 50000, n = 70 after removing the 8 columns containing missing features) obtainable from http://kodiak.cs.cornell.edu/kddcup/datasets.html, the Real-sim (m = 72309, n = 20958), News20 (m = 19996, n = 1355191) and Webspam (unigram treatment with m = 350000, n = 254) datasets all available at http:// www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets, the multiclass Covertype UCI dataset (m = 581012, n = 54) and the full Reuters RCV1 dataset (m = 804414, n = 47236) obtainable from http://www.jmlr.org/papers/ volume5/lewis04a/lyrl2004_rcv1v2_README.htm. For the Covertype dataset we study the binary classification problem of the first class versus rest while for the RCV1 we consider both the binary text classification tasks of the C11 and CCAT classes versus rest. The Physics and Covertype datasets were rescaled by a multiplicative factor 0.001. The experiments were conducted on a 2.5 GHz Intel Core 2 Duo processor with 3 GB RAM running Windows Vista. Our codes written in C++ were compiled using the g++ compiler under Cygwin.\nThe parameter ∆ of the extended space is chosen from the set {3, 1, 0.3, 0.1} in such a way that it corresponds approximately to R/10 or R/3 depending on the size of the dataset such that the ratio γd/R does not become too small (given that the extension generates a margin of at least ∆/ √ m). More specifically, we have chosen ∆ = 3 for Covertype, ∆ = 1 for Adult, Web and Physics, ∆ = 0.3 for Webspam, C11 and CCAT and ∆ = 0.1 for Real-sim and News20. We also verified that smaller values of ∆ do not lead to a significant decrease of the training error. For all datasets and for algorithms that introduce bias through augmentation the associated parameter ρ was set to the value ρ = 1.\nWe begin our experimental evaluation by comparing PDM with PFM. We run PDM with accuracy parameter ǫ = 0.01 and subsequently PFM with the fixed margin β = (1− ǫ)γd set to the value γ′d of the directional margin achieved by PDM. This procedure is repeated using PDM-succ with step η = 8 (i.e., ǫ0 = 0.5, ǫ1 = 0.0625, ǫ2 = ǫ = 0.01). Our results (the value of the directional margin γ′d achieved, the number of required updates (upd) for convergence and the CPU time for training in seconds (s)) are presented in Table 1. We see that PDM is considerably faster than PFM as far as training time is concerned in spite of the fact that PFM needs much less updates for convergence. The successive run scenario succeeds, in accordance with our expectations, in reducing the number of updates to the level of the updates needed by PFM in order to achieve the same value of γ′d at the expense of an increased runtime. We believe that it is fair to say that PDM-succ with η = 8 has the overall performance of PFM without the defect of the need for a priori knowledge of the value of γd. We also notice that although the accuracy ǫ is set to the same value for both scenarios\nthe margin achieved with successive runs is lower. This is an indication that PDM-succ obtains a better estimate of the maximum directional margin γd.\nWe also considered other large margin classifiers representing classes of algorithms such as perceptron-like algorithms, decomposition SVMs and linear SVMs with the additional requirement that the chosen algorithms need only specification of an accuracy parameter. From the class of perceptron-like algorithms we have chosen (aggressive) ROMMA which is much faster than ALMA in the light of the results presented in [9, 14]. Decomposition SVMs are represented by SVMlight [7] which, apart from being one of the fastest algorithms of this class, has the additional advantage of making very efficient use of memory, thereby making possible the training on very large datasets. Finally, from the more recent class of linear SVMs we have included in our study the dual coordinate descent (DCD) algorithm [8] and the margin perceptron with unlearning (MPU)3 [13]. We considered the DCD versions with 1-norm (DCDL1) and 2-norm (DCD-L2) soft margin which for the same value of the accuracy parameter produce identical solutions if the penalty parameter is C = ∞ for DCD-L1 and C = 1/(2∆2) for DCD-L2. The source for SVMlight (version 6.02) is available at http://smvlight.joachims.org and for DCD at http: //www.csie.ntu.edu.tw/~cjlin/liblinear. The absence of publicly available implementations for ROMMA necessitated the writing of our own code in C++ employing the mechanism of active sets proposed in [9] and incorporating a mechanism of permutations performed at the beginning of a full epoch. For MPU the implementation followed closely [13] with active set parameters c̄ = 1.01, Nep1 = Nep2 = 5, gap parameter δb = 3R 2 and early stopping.\nThe experimental results (margin values achieved and training runtimes) involving the above algorithms with the accuracy parameter set to 0.01 for all of\n3 MPU uses dual variables but is not formulated as an optimization. It is a perceptron incorporating a mechanism of reduction of possible contributions from “very-well classified” patterns to the weight vector which is an essential ingredient of SVMs.\nthem are summarized in Table 2. Notice that for SVMlight we give the geometric margin γ′ instead of the directional one γ′d because SVM\nlight does not introduce bias through augmentation. For the rest of the algorithms considered, including PDM and PFM, the geometric margin γ′ achieved is not listed in the tables since it is very close to the directional margin γ′d if the augmentation parameter ρ is set to the value ρ = 1. Moreover, for DCD-L1 and DCD-L2 the margin values coincide as we pointed out earlier. From Table 2 it is apparent that ROMMA and SVMlight are orders of magnitude slower than DCD and MPU. Comparing the results of Table 1 with those of Table 2 we see that PDM is orders of magnitude faster than ROMMA which is its natural competitor since they both belong to the class of perceptron-like algorithms. PDM is also much faster than SVMlight but statistically a few times slower than DCD, especially for the larger datasets. Moreover, PDM is a few times slower than MPU for all datasets. Finally, we observe that the accuracy achieved by PDM is, in general, closer to the beforerun accuracy 0.01 since in most cases PDM obtains lower margin values. This indicates that PDM succeeds in obtaining a better estimate of the maximum margin than the remaining algorithms with the possible exception of MPU.\nBefore we conclude our comparative study it is fair to point out that PDM is not the fastest perceptron-like large margin classifier. From the results of [14] the fastest algorithm of this class is the margitron which has strong before-run guarantees and a very good after-run estimate of the achieved accuracy through (5). However, its drawback is that an approximate knowledge of the value of γd (preferably an upper bound) is required in order to fix the parameter controlling the margin threshold. Although there is a procedure to obtain this information, taking all the facts into account the employment of PDM seems preferable."
    }, {
      "heading" : "6 Conclusions",
      "text" : "We introduced the perceptron with dynamic margin (PDM), a new approximate maximum margin classifier employing the classical perceptron update, demonstrated its convergence in a finite number of steps and derived an upper bound on them. PDM uses the required accuracy as the only input parameter. Moreover, it is a strictly online algorithm in the sense that it decides whether to perform an update taking into account only its current state and irrespective of whether the pattern presented to it has been encountered before in the process of cycling repeatedly through the dataset. This certainly does not hold for linear SVMs. Our experimental results indicate that PDM is the fastest large margin classifier enjoying the above two very desirable properties."
    }, {
      "heading" : "1. Blum, A.: Lectures on machine learning theory. Carnegie Mellon University, USA.",
      "text" : "Available at http://www.cs.cmu.edu/ avrim/ML09/lect0126.pdf 2. Cristianini, N., Shawe-Taylor, J.: An introduction to support vector machines (2000) Cambridge, UK: Cambridge University Press 3. Duda, R.O., Hart, P.E.: Pattern classsification and scene analysis (1973) Wiley 4. Freund, Y., Shapire, R.E.: Large margin classification using the perceptron algorithm. Machine Learning 37(3) (1999) 277–296 5. Gentile, C.: A new approximate maximal margin classification algorithm. Journal of Machine Learning Research 2 (2001) 213–242 6. Joachims, T.: Making large-scale SVM learning practical. In Advances in kernel methods-support vector learning (1999) MIT Press 7. Joachims, T.: Training linear SVMs in linear time. KDD (2006) 217–226 8. Hsieh, C.-J., Chang, K.-W., Lin, C.-J., Keerthi, S.S., Sundararajan, S.: A dual coordinate descent method for large-scale linear SVM. ICML (2008) 408–415 9. Ishibashi, K., Hatano, K., Takeda, M.: Online learning of maximum p-norm margin classifiers. COLT (2008) 69-80. 10. Krauth, W., Mézard, M.: Learning algorithms with optimal stability in neural networks. Journal of Physics A20 (1987) L745–L752 11. Li, Y., Long, P.: The relaxed online maximummargin algorithm. Machine Learning, 46(1-3) (2002) 361–387 12. Novikoff, A.B.J.: On convergence proofs on perceptrons. In Proc. Symp. Math. Theory Automata, Vol. 12 (1962) 615–622 13. Panagiotakopoulos, C., Tsampouka, P.: The margin perceptron with unlearning. ICML (2010) 855-862 14. Panagiotakopoulos, C., Tsampouka, P.: The margitron: A generalized perceptron with margin. IEEE Transactions on Neural Networks 22(3) (2011) 395-407 15. Platt, J.C.: Sequential minimal optimization: A fast algorithm for training support vector machines. Microsoft Res. Redmond WA, Tech. Rep. MSR-TR-98-14 (1998) 16. Rosenblatt, F.: The perceptron: A probabilistic model for information storage and organization in the brain. Psychological Review, 65(6) (1958) 386–408 17. Tsampouka, P., Shawe-Taylor, J.: Perceptron-like large margin classifiers. Tech. Rep., ECS, University of Southampton, UK (2005). Obtainable from http://eprints.ecs.soton.ac.uk/10657"
    }, {
      "heading" : "18. Tsampouka, P., Shawe-Taylor, J.: Analysis of generic perceptron-like large margin",
      "text" : "classifiers. ECML (2005) 750–758 19. Tsampouka, P., Shawe-Taylor, J.: Constant rate approximate maximum margin algorithms. ECML (2006) 437–448 20. Tsampouka, P., Shawe-Taylor, J.: Approximate maximum margin algorithms with rules controlled by the number of mistakes. ICML (2007) 903–910 21. Vapnik, V.: Statistical learning theory (1998) Wiley"
    } ],
    "references" : [ {
      "title" : "An introduction to support vector machines",
      "author" : [ "N. Cristianini", "J. Shawe-Taylor" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2000
    }, {
      "title" : "Pattern classsification and scene analysis",
      "author" : [ "R.O. Duda", "P.E. Hart" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1973
    }, {
      "title" : "Large margin classification using the perceptron algorithm",
      "author" : [ "Y. Freund", "R.E. Shapire" ],
      "venue" : "Machine Learning 37(3)",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "A new approximate maximal margin classification algorithm",
      "author" : [ "C. Gentile" ],
      "venue" : "Journal of Machine Learning Research 2",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Making large-scale SVM learning practical",
      "author" : [ "T. Joachims" ],
      "venue" : "In Advances in kernel methods-support vector learning",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Training linear SVMs in linear time",
      "author" : [ "T. Joachims" ],
      "venue" : "KDD",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "A dual coordinate descent method for large-scale linear SVM",
      "author" : [ "Hsieh", "C.-J.", "Chang", "K.-W.", "Lin", "C.-J.", "S.S. Keerthi", "S. Sundararajan" ],
      "venue" : "ICML",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Online learning of maximum p-norm margin classifiers",
      "author" : [ "K. Ishibashi", "K. Hatano", "M. Takeda" ],
      "venue" : "COLT",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Learning algorithms with optimal stability in neural networks",
      "author" : [ "W. Krauth", "M. Mézard" ],
      "venue" : "Journal of Physics A20",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "The relaxed online maximummargin algorithm",
      "author" : [ "Y. Li", "P. Long" ],
      "venue" : "Machine Learning, 46(1-3)",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "On convergence proofs on perceptrons",
      "author" : [ "A.B.J. Novikoff" ],
      "venue" : "In Proc. Symp. Math. Theory Automata, Vol. 12",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1962
    }, {
      "title" : "The margin perceptron with unlearning",
      "author" : [ "C. Panagiotakopoulos", "P. Tsampouka" ],
      "venue" : "ICML",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "The margitron: A generalized perceptron with margin",
      "author" : [ "C. Panagiotakopoulos", "P. Tsampouka" ],
      "venue" : "IEEE Transactions on Neural Networks 22(3)",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Sequential minimal optimization: A fast algorithm for training support vector machines",
      "author" : [ "J.C. Platt" ],
      "venue" : "Microsoft Res. Redmond WA, Tech. Rep. MSR-TR-98-14",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "The perceptron: A probabilistic model for information storage and organization in the brain",
      "author" : [ "F. Rosenblatt" ],
      "venue" : "Psychological Review, 65(6)",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1958
    }, {
      "title" : "Perceptron-like large margin classifiers",
      "author" : [ "P. Tsampouka", "J. Shawe-Taylor" ],
      "venue" : "Tech. Rep., ECS, University of Southampton, UK",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Analysis of generic perceptron-like large margin classifiers",
      "author" : [ "P. Tsampouka", "J. Shawe-Taylor" ],
      "venue" : "ECML",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Constant rate approximate maximum margin algorithms",
      "author" : [ "P. Tsampouka", "J. Shawe-Taylor" ],
      "venue" : "ECML",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Approximate maximum margin algorithms with rules controlled by the number of mistakes",
      "author" : [ "P. Tsampouka", "J. Shawe-Taylor" ],
      "venue" : "ICML",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Statistical learning theory",
      "author" : [ "V. Vapnik" ],
      "venue" : null,
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1998
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : "It is a common belief that learning machines able to produce solution hyperplanes with large margins exhibit greater generalization ability [21] and this justifies the enormous interest in Support Vector Machines (SVMs) [21, 2].",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 19,
      "context" : "It is a common belief that learning machines able to produce solution hyperplanes with large margins exhibit greater generalization ability [21] and this justifies the enormous interest in Support Vector Machines (SVMs) [21, 2].",
      "startOffset" : 220,
      "endOffset" : 227
    }, {
      "referenceID" : 0,
      "context" : "It is a common belief that learning machines able to produce solution hyperplanes with large margins exhibit greater generalization ability [21] and this justifies the enormous interest in Support Vector Machines (SVMs) [21, 2].",
      "startOffset" : 220,
      "endOffset" : 227
    }, {
      "referenceID" : 13,
      "context" : "To overcome this problem decomposition methods [15, 6] were developed that apply optimization only to a subset of the training set.",
      "startOffset" : 47,
      "endOffset" : 54
    }, {
      "referenceID" : 4,
      "context" : "To overcome this problem decomposition methods [15, 6] were developed that apply optimization only to a subset of the training set.",
      "startOffset" : 47,
      "endOffset" : 54
    }, {
      "referenceID" : 5,
      "context" : "Recently, the so-called linear SVMs [7, 8, 13] made their appearance.",
      "startOffset" : 36,
      "endOffset" : 46
    }, {
      "referenceID" : 6,
      "context" : "Recently, the so-called linear SVMs [7, 8, 13] made their appearance.",
      "startOffset" : 36,
      "endOffset" : 46
    }, {
      "referenceID" : 11,
      "context" : "Recently, the so-called linear SVMs [7, 8, 13] made their appearance.",
      "startOffset" : 36,
      "endOffset" : 46
    }, {
      "referenceID" : 14,
      "context" : "Such algorithms are mostly based on the perceptron [16, 12], the simplest online learning algorithm for binary linear classification.",
      "startOffset" : 51,
      "endOffset" : 59
    }, {
      "referenceID" : 10,
      "context" : "Such algorithms are mostly based on the perceptron [16, 12], the simplest online learning algorithm for binary linear classification.",
      "startOffset" : 51,
      "endOffset" : 59
    }, {
      "referenceID" : 1,
      "context" : "The first algorithm of that kind is the perceptron with margin [3] which is much older than SVMs.",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 8,
      "context" : "It is an immediate extension of the perceptron which provably achieves solutions with only up to 1/2 of the maximum margin [10].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 9,
      "context" : "Such algorithms include ROMMA [11], ALMA [5], CRAMMA [19] and MICRA [20].",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 3,
      "context" : "Such algorithms include ROMMA [11], ALMA [5], CRAMMA [19] and MICRA [20].",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 17,
      "context" : "Such algorithms include ROMMA [11], ALMA [5], CRAMMA [19] and MICRA [20].",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 18,
      "context" : "Such algorithms include ROMMA [11], ALMA [5], CRAMMA [19] and MICRA [20].",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 12,
      "context" : "Very recently, the same goal was accomplished by a generalized perceptron with margin, the margitron [14].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 15,
      "context" : "The most straightforward way of obtaining large margin solutions through a perceptron is by requiring that the weight vector be updated every time the example presented to the algorithm has (normalized) margin which does not exceed a predefined value [17, 18, 1].",
      "startOffset" : 251,
      "endOffset" : 262
    }, {
      "referenceID" : 16,
      "context" : "The most straightforward way of obtaining large margin solutions through a perceptron is by requiring that the weight vector be updated every time the example presented to the algorithm has (normalized) margin which does not exceed a predefined value [17, 18, 1].",
      "startOffset" : 251,
      "endOffset" : 262
    }, {
      "referenceID" : 12,
      "context" : "In an earlier work [14] we noticed that the upper bound ‖at‖ /t on the maximum margin, with ‖at‖ being the length of the weight vector and t the number of updates, that comes as an immediate consequence of the perceptron update rule is very accurate and tends to improve as the algorithm achieves larger margins.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 19,
      "context" : "This training set may either be the original dataset or the result of a mapping into a feature space of higher dimensionality [21, 2].",
      "startOffset" : 126,
      "endOffset" : 133
    }, {
      "referenceID" : 0,
      "context" : "This training set may either be the original dataset or the result of a mapping into a feature space of higher dimensionality [21, 2].",
      "startOffset" : 126,
      "endOffset" : 133
    }, {
      "referenceID" : 2,
      "context" : "Actually, there is a very well-known construction [4] making linear separability always possible, which amounts to the adoption of the 2-norm soft margin.",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 1,
      "context" : "by extending xk to [xk, ρ], we construct an embedding of our data into the socalled augmented space [3].",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 15,
      "context" : "As ρ → ∞, R/ρ → 1 and, consequently, γd → γ [17, 18].",
      "startOffset" : 44,
      "endOffset" : 52
    }, {
      "referenceID" : 16,
      "context" : "As ρ → ∞, R/ρ → 1 and, consequently, γd → γ [17, 18].",
      "startOffset" : 44,
      "endOffset" : 52
    }, {
      "referenceID" : 10,
      "context" : "Taking the inner product of (2) with the optimal direction u and using (1) we get u · at+1 − u · at = u · yk ≥ γd a repeated application of which gives [12] ‖at‖ ≥ u · at ≥ γdt .",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 15,
      "context" : "The perceptron algorithm with fixed margin condition (PFM) is known to converge in a finite number of updates to an ǫ-accurate approximation of the maximum directional margin hyperplane [17, 18, 1].",
      "startOffset" : 186,
      "endOffset" : 197
    }, {
      "referenceID" : 16,
      "context" : "The perceptron algorithm with fixed margin condition (PFM) is known to converge in a finite number of updates to an ǫ-accurate approximation of the maximum directional margin hyperplane [17, 18, 1].",
      "startOffset" : 186,
      "endOffset" : 197
    }, {
      "referenceID" : 15,
      "context" : "This has already been shown for PFM [17, 18, 1] but no general ǫ-dependent bound in closed form has been derived.",
      "startOffset" : 36,
      "endOffset" : 47
    }, {
      "referenceID" : 16,
      "context" : "This has already been shown for PFM [17, 18, 1] but no general ǫ-dependent bound in closed form has been derived.",
      "startOffset" : 36,
      "endOffset" : 47
    }, {
      "referenceID" : 12,
      "context" : "To reduce the computational cost involved in running PDM, we extend the procedure of [14, 13] and construct a three-member nested sequence of reduced “active sets” of data points.",
      "startOffset" : 85,
      "endOffset" : 93
    }, {
      "referenceID" : 11,
      "context" : "To reduce the computational cost involved in running PDM, we extend the procedure of [14, 13] and construct a three-member nested sequence of reduced “active sets” of data points.",
      "startOffset" : 85,
      "endOffset" : 93
    }, {
      "referenceID" : 12,
      "context" : "An additional mechanism providing a substantial improvement of the computational efficiency is the one of performing multiple updates [14, 13] once a data point is presented to the algorithm.",
      "startOffset" : 134,
      "endOffset" : 142
    }, {
      "referenceID" : 11,
      "context" : "An additional mechanism providing a substantial improvement of the computational efficiency is the one of performing multiple updates [14, 13] once a data point is presented to the algorithm.",
      "startOffset" : 134,
      "endOffset" : 142
    }, {
      "referenceID" : 2,
      "context" : "For linearly separable data the feature space is the initial instance space whereas for inseparable data (which is the case here) a space extended by as many dimensions as the instances is considered where each instance is placed at a distance ∆ from the origin in the corresponding dimension [4].",
      "startOffset" : 293,
      "endOffset" : 296
    }, {
      "referenceID" : 0,
      "context" : "equivalence between the hard margin optimization in the extended space and the soft margin optimization in the initial instance space with objective function ‖w‖ +∆ ∑ iξ̄ 2 i involving the weight vector w and the 2-norm of the slacks ξ̄i [2].",
      "startOffset" : 238,
      "endOffset" : 241
    }, {
      "referenceID" : 13,
      "context" : "The datasets we used for training are: the Adult (m = 32561 instances, n = 123 attributes) and Web (m = 49749, n = 300) UCI datasets as compiled by Platt [15], the training set of the KDD04 Physics dataset (m = 50000, n = 70 after removing the 8 columns containing missing features) obtainable from http://kodiak.",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 7,
      "context" : "From the class of perceptron-like algorithms we have chosen (aggressive) ROMMA which is much faster than ALMA in the light of the results presented in [9, 14].",
      "startOffset" : 151,
      "endOffset" : 158
    }, {
      "referenceID" : 12,
      "context" : "From the class of perceptron-like algorithms we have chosen (aggressive) ROMMA which is much faster than ALMA in the light of the results presented in [9, 14].",
      "startOffset" : 151,
      "endOffset" : 158
    }, {
      "referenceID" : 5,
      "context" : "Decomposition SVMs are represented by SVM [7] which, apart from being one of the fastest algorithms of this class, has the additional advantage of making very efficient use of memory, thereby making possible the training on very large datasets.",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 6,
      "context" : "Finally, from the more recent class of linear SVMs we have included in our study the dual coordinate descent (DCD) algorithm [8] and the margin perceptron with unlearning (MPU) [13].",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 11,
      "context" : "Finally, from the more recent class of linear SVMs we have included in our study the dual coordinate descent (DCD) algorithm [8] and the margin perceptron with unlearning (MPU) [13].",
      "startOffset" : 177,
      "endOffset" : 181
    }, {
      "referenceID" : 7,
      "context" : "The absence of publicly available implementations for ROMMA necessitated the writing of our own code in C++ employing the mechanism of active sets proposed in [9] and incorporating a mechanism of permutations performed at the beginning of a full epoch.",
      "startOffset" : 159,
      "endOffset" : 162
    }, {
      "referenceID" : 11,
      "context" : "For MPU the implementation followed closely [13] with active set parameters c̄ = 1.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 12,
      "context" : "From the results of [14] the fastest algorithm of this class is the margitron which has strong before-run guarantees and a very good after-run estimate of the achieved accuracy through (5).",
      "startOffset" : 20,
      "endOffset" : 24
    } ],
    "year" : 2013,
    "abstractText" : "The classical perceptron rule provides a varying upper bound on the maximum margin, namely the length of the current weight vector divided by the total number of updates up to that time. Requiring that the perceptron updates its internal state whenever the normalized margin of a pattern is found not to exceed a certain fraction of this dynamic upper bound we construct a new approximate maximum margin classifier called the perceptron with dynamic margin (PDM). We demonstrate that PDM converges in a finite number of steps and derive an upper bound on them. We also compare experimentally PDM with other perceptron-like algorithms and support vector machines on hard margin tasks involving linear kernels which are equivalent to 2-norm soft margin.",
    "creator" : "dvips(k) 5.98 Copyright 2009 Radical Eye Software"
  }
}