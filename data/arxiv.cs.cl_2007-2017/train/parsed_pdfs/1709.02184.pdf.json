{
  "name" : "1709.02184.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "firstname.lastname@insight-centre.org" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 9.\n02 18\n4v 1\n[ cs\n.C L\n] 7\nS ep\n2 01\n7\ncuses on the translation of domain-specific expressions represented in semantically structured resources, like ontologies or knowledge graphs. To make knowledge accessible beyond language borders, these resources need to be translated into different languages. The challenge of translating labels or terminological expressions represented in ontologies lies in the highly specific vocabulary and the lack of contextual information, which can guide a machine translation system to translate ambiguous words into the targeted domain. Due to the challenges, we train and translate the terminological expressions in the medial and financial domain with statistical as well as with neural machine translation methods. We evaluate the translation quality of domainspecific expressions with translation systems trained on a generic dataset and experiment domain adaptation with terminological expressions. Furthermore we perform experiments on the injection of external knowledge into the translation systems. Through these experiments, we observed a clear advantage in domain adaptation and terminology injection of NMT methods over SMT. Nevertheless, through the specific and unique terminological expressions, subword segmentation within NMT does not outperform a word based neural translation model."
    }, {
      "heading" : "1 Introduction",
      "text" : "Most of the labels stored in semantically structured resources, i.e. ontologies, taxonomies or knowledge graphs, are represented in English only (Gracia et al., 2012). Therefore, applications in information retrieval, question answering or\nknowledge management, that use these monolingual resources are therefore limited to the language in which the information, i.e. terms, labels or metadata, is stored. To enable knowledge access across languages, these resources need to be enriched with multilingual information. This enhancement can enable information extraction on documents beyond English, e.g. for cross-lingual business intelligence in the financial domain (O’Riain et al., 2013; Arcan et al., 2013), providing information related to an ontology label, e.g. other intangible assets, in Spanish, German or Italian.\nSince manual translation of semantically structured resources is a time consuming and expensive process, since it is mostly performed by domain experts, we engage and evaluate the performance of statistical machine translation (SMT) and neural machine translation (NMT) methods to automatically translate the domain-specific expressions. Although automatically generated translations of these expressions is far from perfect, studies have shown significant productivity gains when human translators are supported by machine translation output rather than starting a translation task from scratch (Federico et al., 2012; Läubli et al., 2013; Green et al., 2013).\nDue to the large success of NMT in recent years (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014), we evaluate its the translation performance against the usage of SMT, comparing which method handles better the translation of domain-specific expressions in isolation, i.e. without any contextual information. Within the neural architecture, the encoder transforms a source sentence into a vector representation, which is is passed to the decoder generating the target sentence word by word from the vector representation.\nFor both translation methods, we translate the labels stored in the ICD (medical domain) and IFRS ontology (financial domain), as well as Wikipedia titles (open-domain). Since large paral-\nlel corpora of in-domain data are not always available, the vocabulary of these resources was translated with translation models trained on generic parallel data only. Furthermore we evaluate how domain-specific expressions, within the same domain, can contribute to the adaptation of translation models to improve the translation quality. At last, we compare the availability of injecting external knowledge into the translation process to minimise the out-of-vocabulary issue. Since the knowledge in ontologies or knowledge graphs is mostly represented in English only, we translate the expressions from English to German.\nWhile using models trained (and tuned) on generic data, SMT showed in general better performance over NMT on the task of domainspecific expression translation. Nevertheless, our experiments on domain adaptation showed a significant improvement of translation quality with the usage of NMT for all targeted resources. Although the results for external knowledge injection show better performance for the SMT approach, NMT nevertheless demonstrates outperforms SMT on translating the ontology labels in the targeted medical domain."
    }, {
      "heading" : "2 Related Work",
      "text" : "Most of the previous work on translation knowledge resources, i.e. ontologies or taxonomies, tackled this problem by accessing multilingual lexical resources, e.g. EuroWordNet or IATE (Declerck et al., 2006; Cimiano et al., 2010). Their work focuses on the identification of the lexical overlap between the ontology labels and the multilingual resource. Since the replacement of the source and target vocabulary guarantees a high precision but a low recall, external translation services, e.g. BabelFish, SDL FreeTranslation tool or Google Translate, were used to overcome this issue (Fu et al., 2009; Espinoza et al., 2009). Additionally, ontology label disambiguation was performed by Espinoza et al. (2009) and McCrae et al. (2011), where the structure of the ontology along with existing multilingual ontologies was used to annotate the labels with their semantic senses. Furthermore, McCrae et al. (2016) show positive effects of different domain adaptation techniques, i.e., using Web resources as additional bilingual knowledge, re-scoring translations with Explicit Semantic Analysis (ESA) and language model adaptation for automatic ontology translation. A different approach on ontology label disambiguation was shown in Arcan et al. (2015), where the authors\nidentified relevant in-domain parallel sentences and used them to train an ontology-specific SMT system.\nAs a lexical resource, Wikipedia with its rich semantic knowledge was used as a resource for bilingual term identification in the context of SMT. Tyers and Pieanaar (2008) extracts bilingual dictionary entries from Wikipedia to support the machine translation system. Based on exact string matching they query Wikipedia with a list of around 10,000 noun lemmas to generate the bilingual dictionary. Besides the interwiki link system, Erdmann et al. (2009) enhance their bilingual dictionary by using redirection page titles and anchor text within Wikipedia. To cast the problem of ambiguous Wikipedia titles, Niehues and Waibel (2011) and Arcan et al. (2014) use the information of Wikipedia categories and the text of the articles to provide the SMT system domain-specific bilingual knowledge.\nDue to the recency of NMT, terminology translation with neural models is still less examined. This can be explained due to the issue that neural model are incapable of translating rare words, like domain-specific expressions, because they have a fixed size of vocabulary. Nevertheless, without the help of subword segmentation, Luong et al. (2014) utilized the out-of-vocabulary issue by a post-processing step that replaced every unknown word with the usage of a dictionary. Differently to the post-processing step, Chatterjee et al. (2017) propose a mechanism that guides an existing NMT decoder with the ability to prioritize and adequately handle translation candidates provided by the external resource. In the case of domain adaptation, most work focuses on fine tuning, where an out-of-domain model is further trained on in-domain data (Sennrich et al., 2016; Luong and Manning, 2015; Servan et al., 2016). In addition to the fine-tuning method, Chu et al. (2017) tune the neural model with in- and out-domain data, whereby they use tags to annotate the domains within the used corpora. Differently to these approaches, which focused on document translation, our research focuses entirely on the translation of short and domainspecific expression, without any contextual information guiding the adaptation or translation approach."
    }, {
      "heading" : "3 Methodology",
      "text" : "We use the approaches to SMT and NMT for translating the domain-specific expressions, whereby we specifically focus on the performance of NMT and how it handles the translation of expressions infrequently found in the parallel training data. Therefore we explore how translation quality may benefit from the usage of subword segmentation, which helps to overcome the issue of the vocabulary limitation. We furthermore perform domain adaptation with domainspecific expressions and experiment with different approaches and methods on injecting external knowledge into the translation process. These methods are detailed in the following subsections."
    }, {
      "heading" : "3.1 Byte Pair Encoding",
      "text" : "A common problem in machine translation in general are rare and unknown words, e.g. terminological expressions, which the system has rarely or never seen. Therefore, if the training method does not see a specific word or phrase multiple times during training, it will not learn the correct translation. This challenge is even more evident in NMT due to the complexity associated with neural networks. Therefore the vocabulary is often limited only to 50,000 or 100,000 words (in comparison to 200,000 or more words in a two million corpus). To overcome this limitation, different methods were suggested, i.e. character based NMT (Costa-Jussà and Fonollosa, 2016; Ling et al., 2015) or using subword units, e.g. Byte Pair Encoding (BPE). The latter one was successfully adapted for word segmentation specifically for the NMT scenario Sennrich et al. (2015). BPE (Gage, 1994) is a form of data compression that iteratively replaces the most frequent pair of bytes in a sequence with a single, unused byte. Instead of merging frequent pairs of bytes as shown in the original algorithm, characters or character sequences are merged for the purposes of NMT. To achieve this, the symbol vocabulary is initialized with the character vocabulary, and each word is represented as a sequence of characters—plus a special end-of-word symbol, which allows to restore the original tokenization after the translation step. This process is repeated as many times as new symbols are created."
    }, {
      "heading" : "3.2 Domain Adaptation",
      "text" : "Furthermore we experimented, how the translation models, either SMT and NMT, can be adapted to the targeted domain and the text style of the\ntargeted knowledge resources. To adapt the loglinear weights of the SMT system to the resource type and domain, we rerun MERT (Och, 2003) using the development set of the available resources. To evaluate, if pre-tuned weights have any influence on domain adaptation, we applied two strategies: first we start the re-tuning approach from the already adapted weights of the different models, based on the generic development set (generic→dom.-specific in Table 3). The second strategy starts adapting the weights from the unadapted, uniformed weights (dom.-specific). We additionally perform weight adaptation across different domains, to see if the text type can improve the translation quality regardless the domain.\nFor the domain adaptation within NMT, we used the models trained on the generic data (or generic and Wikipedia resource) and retrained the weights in the neural network again only on the small development set of each domain."
    }, {
      "heading" : "3.3 Integration of Terminological Knowledge into Machine Translation",
      "text" : "Due to the fact that domain-specific bilingual information is missing and cannot be learned from the parallel sentences, some of the terminological expressions may not be automatically translatable with an SMT or NMT system. Therefore we use the information obtained from Wikipedia or in-domain data of the ontology1 to improve the translation of expressions, which are not known to the translation systems.\nXML Markup within SMT With the help of the XML markup approach, external knowledge can be directly passed to the decoder by specifying the translation of a specific span of the source sentence. In the case of multiple translations of the same source span, a score can be used to indicate the level of association between the source and target phrases. In the case of using Wikipedia titles as external knowledge, we perform two experiments. In the first experiment, we set all probabilities of the translation candidates to 1.0. In the second experiment, we calculate the cosine similarity between the vocabulary of the ontology and the Wikipedia abstracts associated with the titles, which form our translation candidates. As example for the targeted medical domain, we obtain a higher cosine similarity for the preferred candidate\n1We used the development set as external, in-domain knowledge, which was not used in the training process for this experiment.\nvessel-Gefäß2 than vessel-Schiff 3 translation candidate in the technical domain.\nFurthermore, we compared three different XML settings, i.e., exclusive, inclusive and constraint. In the exclusive setting, only the proposed translations are used for the input phrase. Translation candidates stored in the phrase table and overlapping with that span are ignored. Differently, the proposed translations compete with the translation candidates in the phrase table, if the inclusive setting is selected. In the constraint setting, the proposed translations compete with phrase table choices that contain the specified translation.\nProviding External Knowledge to NMT The OpenNMT allows to provide external knowledge for replacement of unknown words in the neural models.4 Instead of inserting the <unk> token for unknown word, it will lookup in the external knowledge for a possible translation. In addition to provide external knowledge, OpenNMT enables to substitute unknown words with source words that have the highest attention weight.5 We use both options to evaluate the performance of the translation quality."
    }, {
      "heading" : "4 Experimental Setting",
      "text" : "In this section, we give an overview on the datasets and the translation tools used in our experiment. Furthermore, we describe the existing approaches and give insights into the SMT and NMT evaluation techniques, considering the translation direction from English to German."
    }, {
      "heading" : "4.1 Training Datasets",
      "text" : "For a broader domain coverage of the generic training dataset necessary for the SMT system, we merged parts of JRCAcquis 3.0 (Steinberger et al., 2006), Europarl v7 (Koehn, 2005) and OpenSubtitles2013 (Tiedemann, 2012), obtaining a training corpus of almost two million sentences, containing around 38M running words (Table 1).6\nDue to the challenging task on terminology translation, we perform an additional experiment where we combine the generic corpus of two million sentences with Wikipedia entries (∼876k), which have an interlingua link between the En-\n2an object used as a container 3a craft designed for water transportation 4OpenNMT option -phrase table 5OpenNMT option -replace unk 6For reproducibility and future evaluation we take the first\none-third part of each corpus.\nglish and German language in the Wikipedia repository."
    }, {
      "heading" : "4.2 Evaluation Dataset",
      "text" : "For our evaluation tasks we apply SMT and NMT translation models on different text types and domains. For a general overview and comparison of the SMT and NMT performance, we use the WMT157 English-German evaluation set. For the main task on domain-specific expression translation, we evaluate the translation quality based on the ICD ontology labels in the medical domain, as well as IFRS ontology labels in the financial domain. In addition to that, we perform translations on Wikipedia titles, which covers generic as well as domain-specific expressions.\nICD ontology For our experiments we used the International Classification of Diseases (ICD) ontology as the gold standard,8 with the labels alignend between the English and German language. The ICD ontology, translated into 43 languages, is used to monitor diseases and to report the general health situation of the population in a country. This stored information also provides an overview of the national mortality rate and appearance of diseases of countries inside the WHO.\nIFRS ontology The IFRS ontology (International Accounting Standards Board, 2007) is used for providing electronic financial reports for auditing. The terms contained within this taxonomy are frequently long (on average 11 tokens) and are entirely noun phrases.\nWikipedia Titles Wikipedia9 is a multilingual, freely available encyclopaedia that was built by a collaborative effort of voluntary contributors. All combined Wikipedias hold approximately 40 million articles or more than 27 billion words in more than 293 languages, making it the largest collection of freely available knowledge.10 In our work we use the DBpedia (Lehmann et al., 2015) repository,11 since it provides a structured and preprocessed Wikipedia abstracts. The DBpedia project aims to extract structured content from the knowledge added to the Wikipedia repository. DBpedia allows users to semantically query relationships and properties of Wikipedia resources, including links to other related datasets.\n7Workshop on Statistical Machine Translation, http://www.statmt.org/wmt15/index.html\n8 http://www.who.int/classifications/icd\n9http://www.wikipedia.org 10 http://en.wikipedia.org/wiki/Wikipedia:Size_comparison 11DBpedia version 2016-04"
    }, {
      "heading" : "4.3 Machine Translation tools",
      "text" : "Moses For our SMT translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where the word alignments were built with the GIZA++ toolkit (Och and Ney, 2003). The KenLM toolkit (Heafield, 2011) was used to build the 5-gram language model.\nOpenNMT OpenNMT (Klein et al., 2017) is a generic deep learning framework mainly specialized in sequence-to-sequence (seq2seq) models covering a variety of tasks such as machine translation, summarisation, image to text, and speech recognition. We used the default OpenNMT parameters, i.e. 2 layers, 500 hidden LSTM units, input feeding enabled, batch size of 64, 0.3 dropout probability and a dynamic learning rate decay."
    }, {
      "heading" : "4.4 Evaluation Metrics",
      "text" : "The automatic translation evaluation is based on the correspondence between the SMT output and reference translation (gold standard). For the automatic evaluation we used the BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and chrF (Popović, 2015) metrics. BLEU (Bilingual Evaluation Understudy) is calculated for individual translated segments (n-grams) by comparing them with a data set of reference trans-\nlations. Those scores, between 0 and 100 (perfect translation), are then averaged over the whole evaluation data set to reach an estimate of the translation’s overall quality. METEOR (Metric for Evaluation of Translation with Explicit ORdering) is based on the harmonic mean of precision and recall, whereby recall is weighted higher than precision. Along with exact word (or phrase) matching it has additional features, i.e. stemming, paraphrasing and synonymy matching. In contrast to BLEU, the metric produces good correlation with human judgement at the sentence or segment level. chrF3 is a character n-gram metric, which has shown very good correlations with human judgements on the WMT2015 shared metric task (Stanojević et al., 2015), especially when translating from English into morphologically rich(er) languages."
    }, {
      "heading" : "5 Evaluation",
      "text" : "In this section, we report a general quality comparison between SMT and NMT based on the WMT15 dataset. Furthermore we evaluate the translation quality of ontology labels and Wikipedia titles with the SMT and NMT methods. Additionally, we perform experiments with domain adaptation with in- and out-domain knowledge. The final experiment supports the translation system with in-domain lexical knowledge and knowledge identified in the Wikipedia repository."
    }, {
      "heading" : "5.1 WMT15 Dataset Evaluation",
      "text" : "We translate the WMT15 evaluation set to evaluate the generic performance of the trained SMT and NMTmodels, considering the translation generation from English to German. We trained the translation models for SMT and NMT from the generic corpus, as well as from the combination of the generic corpus and Wikipedia entries. For NMT, we limit the vocabulary to 50,000 and 100,000 words, respectively. Additionally, we applying subword segmentation (BPE) and limit the vocabulary to 32,000 (BPE32k) and 90,000 (BPE32k) units.\nAs seen in the Figure 1, adding domain-specific knowledge (Wikipedia titles) to the generic system does not automatically improve the translation quality of the WMT15 dataset in terms of BLEU. This is specifically evident for SMT, where the BLEU score drops from 16.02 (generic data) to 14.16 (generic+Wikipedia data).\nTranslating the WMT15 evaluation set with NMT, best performance (21.62) is shown by the generic model, where the vocabulary is limited to\nEnglish→ German\n100,000 words. Similarly to SMT, when adding Wikipedia knowledge to the generic dataset, the BLEU score drops to 19.66 for the same neural architecture (NMT-generic+Wiki (100k)). On the other side, due to the limited vocabulary of 50,000 words (NMT-generic+Wiki (50k) in Figure 1), the additional Wikipedia knowledge slightly improves the translation quality of the WMT15 dataset, compared to the model with generic data only (18.84 vs 18.52).\nThe subword neural models (BPE32k/BPE90k) do not outperform any of the approaches mentioned before and similar performance of about 11 BLEU points is observed for all neural models."
    }, {
      "heading" : "5.2 Translation of Domain-Specific Expression Evaluation",
      "text" : "In this evaluation section we focus on the domainspecific vocabulary stored in the ICD and IFRS ontology as well as the Wikipedia titles. Different to the WMT15 evaluation, the entries in these datasets appear infrequent in the training data and are translated in isolation, i.e. without any contextual information, which may help in the disambiguation approach of ambiguous expressions.\nSimilarly to theWMT15 evaluation experiment, adding Wikipedia knowledge to the generic corpus, does not always improve the translation quality when translating the ontology labels (Table 2). Focusing on the SMT method, the performance in terms of BLEU improves for the ICD ontology labels (6.39 to 7.40), whereby the performance of the IFRS ontology labels drops (10.51 vs 9.03). Due to the vocabulary similarity of the merged generic andWikipedia dataset, we see a significant improvement on translating the Wikipedia evalua-\ntion set.\nApplying the neural translation models, we see that the model limited to 100,000 words always outperforms the 50,000 one, whereby the additional Wikipedia knowledge causes a constant drop of the translation performance when translating the domain-specific expressions (except translating the Wikipedia evaluation set). Comparing the best NMT performance with SMT, the former generates better translations for IFRS labels when only generic data is used, and for the Wikipedia entries, when the generic and Wikipedia datasets are merged.\nSegmenting the domain-specific expressions into subwords does not improve the translation quality in general. Only for the ICD evaluation set it outperforms the 100,000 vocabulary neural model (4.28 vs 3.20), but does not outperform the SMT generated translations for the same evaluation set."
    }, {
      "heading" : "5.3 Domain Adaptation with Domain-Specific Expressions",
      "text" : "Performing the experiment on domain adaptation we use the development sets of the resources mentioned before, which adapts the weights of the loglinear models in SMT or the weights in the neural network architecture.\nAs seen in in Table 3, using the development set of the same domain, the translation quality improves over the SMT system, trained and tuned on generic data. As an example, the BLEU score of translations of the ICD labels increases from 6.39 to 8.02, if the weights are adapted to the targeted domain. Similarly, significant improvements are observed for the IFRS and Wikipedia evaluation\ndataset.\nWhen using the models with generic sentences and Wikipedia knowledge, the translation quality improves significantly for the ICD (9.11 vs 7.40) and IFRS ontology labels (14.24 vs. 9.25). On the other side, the experiment shows minor improvements for the Wikipedia evaluation set (37.98 vs. 37.81).\nA comparison between domain adaptation with pre-adapted weights and adapted weights on terminological resources only did not show any significant changes for the ICD and Wikipedia dataset. Differently, the translation performance of the IFRS ontology labels drops, if the weight adaptation was done only with the in-domain development set. Additionally we observed that MERT converged much faster in the setting, if\nweights were previously adapted on the generic development set.\nIn comparison to the domain adaptation of the SMTmethod, domain adaptation with NMTmethods demonstrates a significant improvement for the domain-specific ontologies, ICD and IFRS. The BLEU score improves from 9.11 (obtained from the SMT adapted model) to 25.71 for the ICD ontology labels, and from 14.24 to 58.85 for the IFRS ontology. On the other hand, the BLEU score for the Wikipedia evaluation set decreases from 37.98 to 26.27. In addition to that, we observed that using the neural network trained from generic and Wikipedia resource harms the translation quality compared to the generic models."
    }, {
      "heading" : "5.4 External Knowledge Injection into Machine Translation",
      "text" : "As the last experiment, we compare the performance on injecting external knowledge to the SMT and NMT systems. To simulate a common scenario, we only use the generic models and inject in-domain and the Wikipedia knowledge as en external resource into the translation process.\nAs mentioned in Section 3.3, in the first XML markup setting we give all translation candidates the probability of 1.0. In the second setting, we adapt the probability depending on the similarity between the domain modelled by the ontology and the Wikipedia abstracts associated with\nthe Wikipedia titles. We learned from Table 4 that the adapted probability (Wiki vocab. / adapted prob.) shows minor improvement over the nonadapted probabilities. Nevertheless, adding external knowledge to the SMT system it does not outperform the generic system, which shows that adding all Wikipedia entries as an external resource does not help in translation performance. Similar observation was shown in Srivastava et al. (2017), where the authors also used Wikipedia entries with similar outcome. In detail, the performance drops from 6.39 to 5.03 for the ICD ontology labels, with similar results for the IFRS ontology (10.54 vs 10.51). Focusing on the Wikipedia evaluation dataset, the similar vocabulary helps to outperform the generic model without the external knowledge (42.84 vs 12.49).12\nAdditionally, we used the development set of each ontology as an external resource and injected the in-domain translation candidates into the translation process. Compared to the usage of Wikipedia as the external resource, we observe an increase of 3 BLEU points for the ICD dataset, and almost 20 for the IFRS dataset.\nFocusing on Wikipedia knowledge injection into a generic neural translation model, the unigram replacement method (unigram rep. in Table 4) shows best performance for the ICD and Wikipedia evaluation set, whereby the lexical\n12We did not perform any ICD or IFRS domain vocabulary injection for the Wikipedia evaluation set, due to small sizes of the ICD, IFRS resources.\nalignment (lex. alignment), where the provided word is chosen based on the closest source word embedding vector, shows best results for the IFRS ontology labels.\nWith the usage of in-domain knowledge as an external resource, the translation quality of the ontology labels improves over theWikipedia injected knowledge, with almost identical performance between unigram replacement and lexical alignment. Compared to the SMT performance on injecting of an external resource, we gain translation improvement for the ICD ontology labes (10.41 vs. 8.05), but observe a significant drop in terms of BLEU for the IFRS ontology labels (14.66 vs. 29.69)."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We presented in this work a performance comparison between SMT and NMT on translating highly domain-specific expressions, i.e. terminological expressions, stored in the ICD ontology, in the medical, and IFRS in the financial domain. Furthermore, we perform experiments on translating Wikipedia titles, which can be domain-specific as well as generic expressions. We show that the Wikipedia resource can be beneficial in the translation approach, but due to the lexical ambiguity of the Wikipedia titles, the translation candidates should be ranked accordingly to the targeted domain. We show that domain adaptation with terminological expressions improves significantly the translation quality, which is specifically evident if an existing generic neural network is retrained with vocabulary of the targeted domain. Our fu-\nture work focuses further on terminology injection into NMT systems and the subword segmentation of terminological expressions to improve the translation of domain-specific translation stored in resources, like ontologies or knowledge graphs."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This publication has emanated from research conducted with the financial support of Science Foundation Ireland (SFI) under Grant Number SFI/12/RC/2289 (Insight)."
    } ],
    "references" : [ {
      "title" : "Translating the FINREP Taxonomy using a Domainspecific Corpus",
      "author" : [ "Arcan et al.2013] Mihael Arcan", "SusanMarie Thomas", "Derek De Brandt", "Paul Buitelaar" ],
      "venue" : "In Machine Translation Summit XIV",
      "citeRegEx" : "Arcan et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Arcan et al\\.",
      "year" : 2013
    }, {
      "title" : "Identification of Bilingual Terms from Monolingual Documents for Statistical Machine Translation",
      "author" : [ "Arcan et al.2014] Mihael Arcan", "Claudio Giuliano", "Marco Turchi", "Paul Buitelaar" ],
      "venue" : "In Proceedings of the 4th International Workshop on Com-",
      "citeRegEx" : "Arcan et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Arcan et al\\.",
      "year" : 2014
    }, {
      "title" : "Knowledge portability with semantic expansion of ontology labels",
      "author" : [ "Arcan et al.2015] Mihael Arcan", "Marco Turchi", "Paul Buitelaar" ],
      "venue" : "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Arcan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Arcan et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Guiding neural machine translation decoding with external knowledge",
      "author" : [ "Matteo Negri", "Marco Turchi", "Marcello Federico", "Lucia Specia", "Frédéric Blain" ],
      "venue" : "In Proceedings of the Second Conference on Machine",
      "citeRegEx" : "Chatterjee et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Chatterjee et al\\.",
      "year" : 2017
    }, {
      "title" : "An empirical comparison of domain adaptationmethods for neural machine translation",
      "author" : [ "Chu et al.2017] Chenhui Chu", "Raj Dabre", "Sadao Kurohashi" ],
      "venue" : "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Vol-",
      "citeRegEx" : "Chu et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Chu et al\\.",
      "year" : 2017
    }, {
      "title" : "A note on ontology localization",
      "author" : [ "Elena MontielPonsoda", "Paul Buitelaar", "Mauricio Espinoza", "Asunción Gómez-Pérez" ],
      "venue" : null,
      "citeRegEx" : "Cimiano et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Cimiano et al\\.",
      "year" : 2010
    }, {
      "title" : "Character-based neural machine translation. CoRR, abs/1603.00810",
      "author" : [ "Costa-Jussà", "José A.R. Fonollosa" ],
      "venue" : null,
      "citeRegEx" : "Costa.Jussà et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Costa.Jussà et al\\.",
      "year" : 2016
    }, {
      "title" : "Multilingual lexical semantic resources for ontology translation",
      "author" : [ "Declerck", "Asunción Gómez Pérez", "Ovidiu Vela", "Zeno Gantner", "David Manzano." ],
      "venue" : "In Proceedings of the 5th International Con-",
      "citeRegEx" : "Declerck et al\\.,? 2006",
      "shortCiteRegEx" : "Declerck et al\\.",
      "year" : 2006
    }, {
      "title" : "Meteor universal: Language specific translation evaluation for any target language",
      "author" : [ "Denkowski", "Lavie2014] Michael Denkowski", "Alon Lavie" ],
      "venue" : "In Proceedings of the EACL 2014 Workshop on Statistical Machine Translation",
      "citeRegEx" : "Denkowski et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Denkowski et al\\.",
      "year" : 2014
    }, {
      "title" : "Improving the extraction of bilingual terminology from wikipedia",
      "author" : [ "Kotaro Nakayama", "Takahiro Hara", "Shojiro Nishio" ],
      "venue" : "ACM Trans. Multimedia Comput. Commun. Appl.,",
      "citeRegEx" : "Erdmann et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Erdmann et al\\.",
      "year" : 2009
    }, {
      "title" : "Ontology localization",
      "author" : [ "Elena Montiel-Ponsoda", "Asunción Gómez-Pérez" ],
      "venue" : "In Proceedings of the Fifth International Conference on Knowledge Capture,",
      "citeRegEx" : "Espinoza et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Espinoza et al\\.",
      "year" : 2009
    }, {
      "title" : "Measuring user productivity in machine translation enhanced computer assisted translation",
      "author" : [ "Alessandro Cattelan", "Marco Trombetti" ],
      "venue" : "In Proceedings of the Tenth Conference of the Association for Machine",
      "citeRegEx" : "Federico et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Federico et al\\.",
      "year" : 2012
    }, {
      "title" : "Cross-lingual ontology mapping - an investigation of the impact of machine translation",
      "author" : [ "Fu et al.2009] Bo Fu", "Rob Brennan", "Declan O’Sullivan" ],
      "venue" : null,
      "citeRegEx" : "Fu et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Fu et al\\.",
      "year" : 2009
    }, {
      "title" : "A new algorithm for data compression",
      "author" : [ "Philip Gage" ],
      "venue" : "C Users J.,",
      "citeRegEx" : "Gage.,? \\Q1994\\E",
      "shortCiteRegEx" : "Gage.",
      "year" : 1994
    }, {
      "title" : "Challenges for the multilingual web of data. Web Semantics: Science, Services and Agents on the World Wide",
      "author" : [ "Gracia et al.2012] Jorge Gracia", "Elena MontielPonsoda", "Philipp Cimiano", "Asunción Gómez-Pérez", "Paul Buitelaar", "John McCrae" ],
      "venue" : null,
      "citeRegEx" : "Gracia et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Gracia et al\\.",
      "year" : 2012
    }, {
      "title" : "The efficacy of human post-editing for language translation",
      "author" : [ "Green et al.2013] Spence Green", "Jeffrey Heer", "Christopher D Manning" ],
      "venue" : "In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,",
      "citeRegEx" : "Green et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Green et al\\.",
      "year" : 2013
    }, {
      "title" : "KenLM: faster and smaller language model queries",
      "author" : [ "Kenneth Heafield" ],
      "venue" : "In Proceedings of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation,",
      "citeRegEx" : "Heafield.,? \\Q2011\\E",
      "shortCiteRegEx" : "Heafield.",
      "year" : 2011
    }, {
      "title" : "Recurrent continuous translation models. Seattle, October",
      "author" : [ "Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom" ],
      "venue" : "Association for Computational Linguistics",
      "citeRegEx" : "Kalchbrenner et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kalchbrenner et al\\.",
      "year" : 2013
    }, {
      "title" : "Opennmt: Open-source toolkit for neural machine translation. CoRR, abs/1701.02810",
      "author" : [ "Yoon Kim", "Yuntian Deng", "Jean Senellart", "Alexander M. Rush" ],
      "venue" : null,
      "citeRegEx" : "Klein et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Klein et al\\.",
      "year" : 2017
    }, {
      "title" : "Moses: Open source toolkit for statistical machine translation",
      "author" : [ "Herbst." ],
      "venue" : "Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, Stroudsburg, PA, USA.",
      "citeRegEx" : "Herbst.,? 2007",
      "shortCiteRegEx" : "Herbst.",
      "year" : 2007
    }, {
      "title" : "Europarl: A Parallel Corpus for Statistical Machine Translation",
      "author" : [ "Philipp Koehn" ],
      "venue" : "In Conference Proceedings: the tenth Machine Translation Summit. AAMT",
      "citeRegEx" : "Koehn.,? \\Q2005\\E",
      "shortCiteRegEx" : "Koehn.",
      "year" : 2005
    }, {
      "title" : "Assessing post-editing efficiency in a realistic translation environment. InMachine Translation Summit XIV",
      "author" : [ "Läubli et al.2013] Samuel Läubli", "Mark Fishel", "Gary Massey", "Maureen Ehrensberger-Dow", "Martin Volk" ],
      "venue" : null,
      "citeRegEx" : "Läubli et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Läubli et al\\.",
      "year" : 2013
    }, {
      "title" : "DBpedia - a large-scale, multilin",
      "author" : [ "Lehmann et al.2015] Jens Lehmann", "Robert Isele", "Max Jakob", "Anja Jentzsch", "Dimitris Kontokostas", "Pablo N. Mendes", "Sebastian Hellmann", "Mohamed Morsey", "Patrick van Kleef", "Sören Auer", "Christian Bizer" ],
      "venue" : null,
      "citeRegEx" : "Lehmann et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lehmann et al\\.",
      "year" : 2015
    }, {
      "title" : "Character-based neural machine translation. CoRR, abs/1511.04586",
      "author" : [ "Ling et al.2015] Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan W. Black" ],
      "venue" : null,
      "citeRegEx" : "Ling et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ling et al\\.",
      "year" : 2015
    }, {
      "title" : "Stanford neural machine translation systems for spoken language domains",
      "author" : [ "Luong", "Manning2015] Minh-Thang Luong", "Christopher D Manning" ],
      "venue" : "In Proceedings of the International Workshop on Spoken Language Translation,",
      "citeRegEx" : "Luong et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Addressing the rare word problem in neural machine translation. CoRR, abs/1410.8206",
      "author" : [ "Luong et al.2014] Thang Luong", "Ilya Sutskever", "Quoc V. Le", "Oriol Vinyals", "Wojciech Zaremba" ],
      "venue" : null,
      "citeRegEx" : "Luong et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2014
    }, {
      "title" : "Combining statistical and semantic approaches to the translation of ontologies and taxonomies",
      "author" : [ "McCrae et al.2011] John McCrae", "Mauricio Espinoza", "ElenaMontiel-Ponsoda", "GuadalupeAguado-de Cea", "Philipp Cimiano" ],
      "venue" : "In Fifth workshop on Syn-",
      "citeRegEx" : "McCrae et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "McCrae et al\\.",
      "year" : 2011
    }, {
      "title" : "Domain adaptation for ontology localization",
      "author" : [ "Mihael Arcan", "Kartik Asooja", "Jorge Gracia", "Paul Buitelaar", "Philipp Cimiano" ],
      "venue" : "Web Semantics,",
      "citeRegEx" : "McCrae et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "McCrae et al\\.",
      "year" : 2016
    }, {
      "title" : "Using Wikipedia to Translate Domain-specific Terms in SMT",
      "author" : [ "Niehues", "Waibel2011] Jan Niehues", "Alex Waibel" ],
      "venue" : "In International Workshop on Spoken Language Translation,",
      "citeRegEx" : "Niehues et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Niehues et al\\.",
      "year" : 2011
    }, {
      "title" : "Minimum error rate training in statistical machine translation",
      "author" : [ "Franz Josef Och" ],
      "venue" : "In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1,",
      "citeRegEx" : "Och.,? \\Q2003\\E",
      "shortCiteRegEx" : "Och.",
      "year" : 2003
    }, {
      "title" : "Cross-lingual querying and comparison of linked financial and business data",
      "author" : [ "O’Riain et al.2013] Seán O’Riain", "Barry Coughlan", "Paul Buitelaar", "Thierry Declerck", "Uli Krieger", "Susan Marie Thomas" ],
      "venue" : null,
      "citeRegEx" : "O.Riain et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "O.Riain et al\\.",
      "year" : 2013
    }, {
      "title" : "BLEU: a method for automatic evaluation of machine translation",
      "author" : [ "Salim Roukos", "Todd Ward", "Wei-Jing Zhu" ],
      "venue" : "In Proceedings of the 40th Annual Meeting on Association",
      "citeRegEx" : "Papineni et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "chrf: character n-gram f-score for automatic mt evaluation",
      "author" : [ "Maja Popović" ],
      "venue" : "In Proceedings of the Tenth Workshop on Statistical Machine Translation,",
      "citeRegEx" : "Popović.,? \\Q2015\\E",
      "shortCiteRegEx" : "Popović.",
      "year" : 2015
    }, {
      "title" : "Neural machine translation of rare words with subword units. CoRR, abs/1508.07909",
      "author" : [ "Barry Haddow", "Alexandra Birch" ],
      "venue" : null,
      "citeRegEx" : "Sennrich et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2015
    }, {
      "title" : "Improving neural machine translation models with monolingual data",
      "author" : [ "Barry Haddow", "Alexandra Birch" ],
      "venue" : "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Sennrich et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Domain specialization: a post-training domain adaptation for neural machine translation. CoRR, abs/1612.06141",
      "author" : [ "Josep Maria Crego", "Jean Senellart" ],
      "venue" : null,
      "citeRegEx" : "Servan et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Servan et al\\.",
      "year" : 2016
    }, {
      "title" : "Improving machine translation through linked data",
      "author" : [ "Georg Rehm", "Felix Sasaki" ],
      "venue" : "The Prague Bulletin of Mathematical Linguistics,",
      "citeRegEx" : "Srivastava et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2017
    }, {
      "title" : "Results of the WMT15 Metrics Shared Task",
      "author" : [ "Amir Kamran", "Philipp Koehn", "Ondřej Bojar" ],
      "venue" : "In Proceedings of the 10th Workshop on Statistical Machine Translation",
      "citeRegEx" : "Stanojević et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Stanojević et al\\.",
      "year" : 2015
    }, {
      "title" : "The JRC-Acquis: A multilingual aligned parallel corpus with 20+ languages",
      "author" : [ "Bruno Pouliquen", "Anna Widiger", "Camelia Ignat", "Tomaz Erjavec", "Dan Tufis", "Dániel Varga" ],
      "venue" : null,
      "citeRegEx" : "Steinberger et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Steinberger et al\\.",
      "year" : 2006
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Oriol Vinyals", "Quoc V. Le" ],
      "venue" : "In Proceedings of the 27th International Conference on Neural Information Processing Systems,",
      "citeRegEx" : "Sutskever et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Parallel data, tools and interfaces in opus",
      "author" : [ "Jörg Tiedemann" ],
      "venue" : null,
      "citeRegEx" : "Tiedemann.,? \\Q2012\\E",
      "shortCiteRegEx" : "Tiedemann.",
      "year" : 2012
    }, {
      "title" : "Extracting bilingual word pairs from wikipedia. In Collaboration: interoperability between people in the creation of language resources for less-resourced languages",
      "author" : [ "Tyers", "Pieanaar2008] Francis M. Tyers", "Jacques A. Pieanaar" ],
      "venue" : null,
      "citeRegEx" : "Tyers et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Tyers et al\\.",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "ontologies, taxonomies or knowledge graphs, are represented in English only (Gracia et al., 2012).",
      "startOffset" : 76,
      "endOffset" : 97
    }, {
      "referenceID" : 31,
      "context" : "for cross-lingual business intelligence in the financial domain (O’Riain et al., 2013; Arcan et al., 2013), providing information related to an ontology label, e.",
      "startOffset" : 64,
      "endOffset" : 106
    }, {
      "referenceID" : 0,
      "context" : "for cross-lingual business intelligence in the financial domain (O’Riain et al., 2013; Arcan et al., 2013), providing information related to an ontology label, e.",
      "startOffset" : 64,
      "endOffset" : 106
    }, {
      "referenceID" : 12,
      "context" : "Although automatically generated translations of these expressions is far from perfect, studies have shown significant productivity gains when human translators are supported by machine translation output rather than starting a translation task from scratch (Federico et al., 2012; Läubli et al., 2013; Green et al., 2013).",
      "startOffset" : 258,
      "endOffset" : 322
    }, {
      "referenceID" : 22,
      "context" : "Although automatically generated translations of these expressions is far from perfect, studies have shown significant productivity gains when human translators are supported by machine translation output rather than starting a translation task from scratch (Federico et al., 2012; Läubli et al., 2013; Green et al., 2013).",
      "startOffset" : 258,
      "endOffset" : 322
    }, {
      "referenceID" : 16,
      "context" : "Although automatically generated translations of these expressions is far from perfect, studies have shown significant productivity gains when human translators are supported by machine translation output rather than starting a translation task from scratch (Federico et al., 2012; Läubli et al., 2013; Green et al., 2013).",
      "startOffset" : 258,
      "endOffset" : 322
    }, {
      "referenceID" : 3,
      "context" : "Due to the large success of NMT in recent years (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014), we evaluate its the translation performance against the usage of SMT, comparing which method handles better the translation of domain-specific expressions in isolation, i.",
      "startOffset" : 48,
      "endOffset" : 127
    }, {
      "referenceID" : 40,
      "context" : "Due to the large success of NMT in recent years (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014), we evaluate its the translation performance against the usage of SMT, comparing which method handles better the translation of domain-specific expressions in isolation, i.",
      "startOffset" : 48,
      "endOffset" : 127
    }, {
      "referenceID" : 8,
      "context" : "EuroWordNet or IATE (Declerck et al., 2006; Cimiano et al., 2010).",
      "startOffset" : 20,
      "endOffset" : 65
    }, {
      "referenceID" : 6,
      "context" : "EuroWordNet or IATE (Declerck et al., 2006; Cimiano et al., 2010).",
      "startOffset" : 20,
      "endOffset" : 65
    }, {
      "referenceID" : 13,
      "context" : "BabelFish, SDL FreeTranslation tool or Google Translate, were used to overcome this issue (Fu et al., 2009; Espinoza et al., 2009).",
      "startOffset" : 90,
      "endOffset" : 130
    }, {
      "referenceID" : 11,
      "context" : "BabelFish, SDL FreeTranslation tool or Google Translate, were used to overcome this issue (Fu et al., 2009; Espinoza et al., 2009).",
      "startOffset" : 90,
      "endOffset" : 130
    }, {
      "referenceID" : 3,
      "context" : ", 2006; Cimiano et al., 2010). Their work focuses on the identification of the lexical overlap between the ontology labels and the multilingual resource. Since the replacement of the source and target vocabulary guarantees a high precision but a low recall, external translation services, e.g. BabelFish, SDL FreeTranslation tool or Google Translate, were used to overcome this issue (Fu et al., 2009; Espinoza et al., 2009). Additionally, ontology label disambiguation was performed by Espinoza et al. (2009) and McCrae et al.",
      "startOffset" : 8,
      "endOffset" : 510
    }, {
      "referenceID" : 3,
      "context" : ", 2006; Cimiano et al., 2010). Their work focuses on the identification of the lexical overlap between the ontology labels and the multilingual resource. Since the replacement of the source and target vocabulary guarantees a high precision but a low recall, external translation services, e.g. BabelFish, SDL FreeTranslation tool or Google Translate, were used to overcome this issue (Fu et al., 2009; Espinoza et al., 2009). Additionally, ontology label disambiguation was performed by Espinoza et al. (2009) and McCrae et al. (2011), where the structure of the ontology along with existing multilingual ontologies was used to annotate the labels with their semantic senses.",
      "startOffset" : 8,
      "endOffset" : 535
    }, {
      "referenceID" : 3,
      "context" : ", 2006; Cimiano et al., 2010). Their work focuses on the identification of the lexical overlap between the ontology labels and the multilingual resource. Since the replacement of the source and target vocabulary guarantees a high precision but a low recall, external translation services, e.g. BabelFish, SDL FreeTranslation tool or Google Translate, were used to overcome this issue (Fu et al., 2009; Espinoza et al., 2009). Additionally, ontology label disambiguation was performed by Espinoza et al. (2009) and McCrae et al. (2011), where the structure of the ontology along with existing multilingual ontologies was used to annotate the labels with their semantic senses. Furthermore, McCrae et al. (2016) show positive effects of different domain adaptation techniques, i.",
      "startOffset" : 8,
      "endOffset" : 710
    }, {
      "referenceID" : 0,
      "context" : "A different approach on ontology label disambiguation was shown in Arcan et al. (2015), where the authors identified relevant in-domain parallel sentences and used them to train an ontology-specific SMT system.",
      "startOffset" : 67,
      "endOffset" : 87
    }, {
      "referenceID" : 7,
      "context" : "Besides the interwiki link system, Erdmann et al. (2009) enhance their bilingual dictionary by using redirection page titles and anchor text within Wikipedia.",
      "startOffset" : 35,
      "endOffset" : 57
    }, {
      "referenceID" : 7,
      "context" : "Besides the interwiki link system, Erdmann et al. (2009) enhance their bilingual dictionary by using redirection page titles and anchor text within Wikipedia. To cast the problem of ambiguous Wikipedia titles, Niehues and Waibel (2011) and Arcan et al.",
      "startOffset" : 35,
      "endOffset" : 236
    }, {
      "referenceID" : 0,
      "context" : "To cast the problem of ambiguous Wikipedia titles, Niehues and Waibel (2011) and Arcan et al. (2014) use the information of Wikipedia categories and the text of the articles to provide the SMT system domain-specific bilingual knowledge.",
      "startOffset" : 81,
      "endOffset" : 101
    }, {
      "referenceID" : 35,
      "context" : "In the case of domain adaptation, most work focuses on fine tuning, where an out-of-domain model is further trained on in-domain data (Sennrich et al., 2016; Luong and Manning, 2015; Servan et al., 2016).",
      "startOffset" : 134,
      "endOffset" : 203
    }, {
      "referenceID" : 36,
      "context" : "In the case of domain adaptation, most work focuses on fine tuning, where an out-of-domain model is further trained on in-domain data (Sennrich et al., 2016; Luong and Manning, 2015; Servan et al., 2016).",
      "startOffset" : 134,
      "endOffset" : 203
    }, {
      "referenceID" : 23,
      "context" : "Nevertheless, without the help of subword segmentation, Luong et al. (2014) utilized the out-of-vocabulary issue by a post-processing step that replaced every unknown word with the usage of a dictionary.",
      "startOffset" : 56,
      "endOffset" : 76
    }, {
      "referenceID" : 4,
      "context" : "Differently to the post-processing step, Chatterjee et al. (2017) propose a mechanism that guides an existing NMT decoder with the ability to prioritize and adequately handle translation candidates provided by the external resource.",
      "startOffset" : 41,
      "endOffset" : 66
    }, {
      "referenceID" : 4,
      "context" : "Differently to the post-processing step, Chatterjee et al. (2017) propose a mechanism that guides an existing NMT decoder with the ability to prioritize and adequately handle translation candidates provided by the external resource. In the case of domain adaptation, most work focuses on fine tuning, where an out-of-domain model is further trained on in-domain data (Sennrich et al., 2016; Luong and Manning, 2015; Servan et al., 2016). In addition to the fine-tuning method, Chu et al. (2017) tune the neural model with in- and out-domain data, whereby they use tags to annotate the domains within the used corpora.",
      "startOffset" : 41,
      "endOffset" : 495
    }, {
      "referenceID" : 24,
      "context" : "character based NMT (Costa-Jussà and Fonollosa, 2016; Ling et al., 2015) or using subword units, e.",
      "startOffset" : 20,
      "endOffset" : 72
    }, {
      "referenceID" : 14,
      "context" : "BPE (Gage, 1994) is a form of data compression that iteratively replaces the most frequent pair of bytes in a sequence with a single, unused byte.",
      "startOffset" : 4,
      "endOffset" : 16
    }, {
      "referenceID" : 23,
      "context" : "character based NMT (Costa-Jussà and Fonollosa, 2016; Ling et al., 2015) or using subword units, e.g. Byte Pair Encoding (BPE). The latter one was successfully adapted for word segmentation specifically for the NMT scenario Sennrich et al. (2015). BPE (Gage, 1994) is a form of data compression that iteratively replaces the most frequent pair of bytes in a sequence with a single, unused byte.",
      "startOffset" : 54,
      "endOffset" : 247
    }, {
      "referenceID" : 30,
      "context" : "To adapt the loglinear weights of the SMT system to the resource type and domain, we rerun MERT (Och, 2003) using the development set of the available resources.",
      "startOffset" : 96,
      "endOffset" : 107
    }, {
      "referenceID" : 39,
      "context" : "0 (Steinberger et al., 2006), Europarl v7 (Koehn, 2005) and OpenSubtitles2013 (Tiedemann, 2012), obtaining a training corpus of almost two million sentences, containing around 38M running words (Table 1).",
      "startOffset" : 2,
      "endOffset" : 28
    }, {
      "referenceID" : 21,
      "context" : ", 2006), Europarl v7 (Koehn, 2005) and OpenSubtitles2013 (Tiedemann, 2012), obtaining a training corpus of almost two million sentences, containing around 38M running words (Table 1).",
      "startOffset" : 21,
      "endOffset" : 34
    }, {
      "referenceID" : 41,
      "context" : ", 2006), Europarl v7 (Koehn, 2005) and OpenSubtitles2013 (Tiedemann, 2012), obtaining a training corpus of almost two million sentences, containing around 38M running words (Table 1).",
      "startOffset" : 57,
      "endOffset" : 74
    }, {
      "referenceID" : 23,
      "context" : "In our work we use the DBpedia (Lehmann et al., 2015) repository, since it provides a structured and preprocessed Wikipedia abstracts.",
      "startOffset" : 31,
      "endOffset" : 53
    }, {
      "referenceID" : 17,
      "context" : "The KenLM toolkit (Heafield, 2011) was used to build the 5-gram language model.",
      "startOffset" : 18,
      "endOffset" : 34
    }, {
      "referenceID" : 19,
      "context" : "OpenNMT OpenNMT (Klein et al., 2017) is a generic deep learning framework mainly specialized in sequence-to-sequence (seq2seq) models covering a variety of tasks such as machine translation, summarisation, image to text, and speech recognition.",
      "startOffset" : 16,
      "endOffset" : 36
    }, {
      "referenceID" : 32,
      "context" : "For the automatic evaluation we used the BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and chrF (Popović, 2015) metrics.",
      "startOffset" : 46,
      "endOffset" : 69
    }, {
      "referenceID" : 33,
      "context" : ", 2002), METEOR (Denkowski and Lavie, 2014) and chrF (Popović, 2015) metrics.",
      "startOffset" : 53,
      "endOffset" : 68
    }, {
      "referenceID" : 38,
      "context" : "task (Stanojević et al., 2015), especially when translating from English into morphologically rich(er) languages.",
      "startOffset" : 5,
      "endOffset" : 30
    }, {
      "referenceID" : 37,
      "context" : "Similar observation was shown in Srivastava et al. (2017), where the authors also used Wikipedia entries with similar outcome.",
      "startOffset" : 33,
      "endOffset" : 58
    } ],
    "year" : 2017,
    "abstractText" : "Our work presented in this paper focuses on the translation of domain-specific expressions represented in semantically structured resources, like ontologies or knowledge graphs. To make knowledge accessible beyond language borders, these resources need to be translated into different languages. The challenge of translating labels or terminological expressions represented in ontologies lies in the highly specific vocabulary and the lack of contextual information, which can guide a machine translation system to translate ambiguous words into the targeted domain. Due to the challenges, we train and translate the terminological expressions in the medial and financial domain with statistical as well as with neural machine translation methods. We evaluate the translation quality of domainspecific expressions with translation systems trained on a generic dataset and experiment domain adaptation with terminological expressions. Furthermore we perform experiments on the injection of external knowledge into the translation systems. Through these experiments, we observed a clear advantage in domain adaptation and terminology injection of NMT methods over SMT. Nevertheless, through the specific and unique terminological expressions, subword segmentation within NMT does not outperform a word based neural translation model.",
    "creator" : "LaTeX with hyperref package"
  }
}