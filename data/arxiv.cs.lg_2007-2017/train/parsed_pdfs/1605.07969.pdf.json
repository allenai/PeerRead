{
  "name" : "1605.07969.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Adaptive Neural Compilation",
    "authors" : [ "Rudy Bunel", "Alban Desmaison", "Pushmeet Kohli", "Philip H.S. Torr", "M. Pawan Kumar" ],
    "emails" : [ "rudy@robots.ox.ac.uk", "alban@robots.ox.ac.uk", "pkohli@microsoft.com", "philip.torr@eng.ox.ac.uk", "pawan@robots.ox.ac.uk" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Algorithm design often requires making simplifying assumptions about the input data. Consider, for instance, the computational problem of accessing an element in a linked list. Without the knowledge of the input data distribution, one can only specify an algorithm that runs in a time linear in the number of elements of the list. However, suppose all the linked lists that we encountered in practice were ordered in memory. Then it would be advantageous to design an algorithm specifically for this task as it can lead to a constant running time. Unfortunately, the input data distribution of a real world problem cannot be easily specified as in the above simple example. The best that one can hope for is to obtain samples drawn from the distribution. A natural question that arises from these observations: “How can we adapt a generic algorithm for a computational task using samples from an unknown input data distribution?” The process of finding the most efficient implementation of an algorithm has received considerable attention in the theoretical computer science and code optimisation community. Recently, Conditionally Correct Superoptimization [14] was proposed as a method for leveraging samples of the input data distribution to go beyond semantically equivalent optimisation and towards data-specific performance improvements. The underlying procedure is based on a stochastic search over the space of all possible programs. Additionally, they restrict their applications to reasonably small, loop-free programs, thereby limiting their impact in practice. In this work, we take inspiration from the recent wave of machine-learning frameworks for estimating programs. Using recurrent models, Graves et al. [2] introduced a fully differentiable\n∗The first two authors contributed equally.\nar X\niv :1\n60 5.\n07 96\n9v 2\n[ cs\n.A I]\n2 6\nM ay\nrepresentation of a program, enabling the use of gradient-based methods to learn a program from examples. Many other models have been published recently [3, 5, 6, 8] that build and improve on the early work by Graves et al. [2]. Unfortunately, these models are usually complex to train and need to rely on methods such as curriculum learning or gradient noise to reach good solutions as shown by Neelakantan et al. [10]. Moreover, their interpretability is limited. The learnt model is too complex for the underlying algorithm to be recovered and transformed into a regular computer program. The main focus of the machine-learning community has thus far been on learning programs from scratch, with little emphasis on running time. However, for nearly all computational problems, it is feasible to design generic algorithms for the worst-case. We argue that a more pragmatic goal for the machine learning community is to design methods for adapting existing programs for specific input data distributions. To this end, we propose the Adaptive Neural Compiler (ANC). We design a compiler capable of mechanically converting algorithms to a differentiable representation, thereby providing adequate initialisation to the difficult problem of optimal program learning. We then present a method to improve this compiled program using data-driven optimisation, alleviating the need to perform a wide search over the set of all possible programs. We show experimentally that this framework is capable of adapting simple generic algorithms to perform better on given datasets."
    }, {
      "heading" : "2 Related Works",
      "text" : "The idea of compiling programs to neural networks has previously been explored in the literature. Siegelmann [15] described how to build a Neural Network that would perform the same operations as a given program. A compiler has been designed by Gruau et al. [4] targeting an extended version of Pascal. A complete implementation was achieved when Neto et al. [11] wrote a compiler for NETDEF, a language based on the Occam programming language. While these methods allow us to obtain an exact representation of a program as a neural network, they do not lend themselves to optimisation to improve the original program. Indeed, in their formulation, each elementary step of a program is expressed as a group of neurons with a precise topology, set of weights and biases, thereby rendering learning via gradient descent infeasible. Performing gradient descent in this parameter space would result in invalid operations and thus is unlikely to lead to any improvement. The recent work by Reed and de Freitas [12] on Neural Programmer-Interpreters (NPI) can also be seen as a way to compile any program into a neural network by learning a model that mimic the program. While more flexible than the previous approaches, the NPI is unable to improve on a learned program due to its dependency on a non-differentiable environment. Another approach to this learning problem is the one taken by the code optimisation community. By exploring the space of all possible programs, either exhaustively [9] or in a stochastic manner [13], they search for programs having the same results but being more efficient. The work of Sharma et al. [14] broadens the space of acceptable improvements to data-specific optimisations as opposed to the provably equivalent transformations that were previously the only ones considered. However, this method is still reliant on nongradient-based methods for efficient exploration of the space. By representing everything in a differentiable manner, we aim to obtain gradients to guide the exploration. Recently, Graves et al. [2] introduced a learnable representation of programs, called the Neural Turing Machine (NTM). The NTM uses an LSTM as a Controller, which outputs commands to be executed by a deterministic differentiable Machine. From examples of input/output sequences, they manage to learn a Controller such that the model becomes capable of performing simple algorithmic tasks. Extensions of this model have been proposed in [3, 5] where the memory tape was replaced by differentiable versions of stacks or lists. Kurach et al. [8] modified the NTM to introduce a notion of pointers making it more amenable to represent traditional programs. Parallel works have been using Reinforcement Learning techniques such as the REINFORCE algorithm [1, 16, 17] or Q-learning [18] to be able to work with non differentiable versions of the above mentioned models. All these models are trained only with a loss based on the difference between the output of the model and the expected output. This weak supervision leads to a complex training. For instance\nthe Neural RAM [8] requires a high number of random restarts before converging to a correct solution [10], even when using the best hyperparameters obtained through a large grid search. In our work, we will first show that we can design a new neural compiler whose target will be a Controller-Machine model. This makes the compiled model amenable to learning from examples. Moreover, we can use it as initialisation for the learning procedure, allowing us to aim for the more complex task of finding an efficient algorithm."
    }, {
      "heading" : "3 Model",
      "text" : "Our model is composed of two parts: (i) a Controller, in charge of specifying what should be executed; and (ii) a Machine, following the commands of the Controller. We start by describing the global architecture of the model. For the sake of simplicity, the general description will present a non-differentiable version of the model. Section 3.2 will then explain the modifications required to make this model completely differentiable. A more detailed description of the model is provided in appendix A."
    }, {
      "heading" : "3.1 General Model",
      "text" : "We first define for each timestep t the memory tape that contains M integer values Mt = {mt1,mt2, . . . ,mtM}, the registers that contain R values Rt = {rt1, rt2, . . . , rtR} and the instruction register that contain a single value IRt. We also define a set of instructions that can be executed, whose main role is to perform computations using the registers. For example, add the values contained in two registers. We also define as a side effect any action that involves elements other than the input and output values of the instruction. Interaction with the memory is an example of such side effect. All the instructions, their computations and side effects are detailed in Figure 1b. As can be seen in Figure 1a the execution model takes as input an initial memory tapeM0 and outputs a final memory tape MT after T steps. At each step t, the Controller uses the instruction register IRt to compute the command for the Machine. The command is a 4-tuple e, a, b, o. The first element e is the instruction that should be executed by the Machine, enumerated as an integer. The elements a and b specify which registers should be used as arguments for the given instruction. The last element o specifies in which register the output of the instruction should be written. For example, the command {ADD, 2, 3, 1} means that only the value of the first register should change, following rt+11 = ADD(rt2, rt3). Then the Machine will execute this command, updating the values of the memory, the registers and the instruction register. The Machine always performs two other operations apart from the required instruction. It outputs a stop flag that allows the model to decide when to stop the execution. It also increments the instruction register IRt by one at each iteration."
    }, {
      "heading" : "3.2 Differentiability",
      "text" : "The model presented above is a simple execution machine but it is not differentiable. In order to be able to train this model end-to-end from a loss defined over the final memory tape, we need to make every intermediate operation differentiable.\nTo achieve this, we replace every discrete value in our model by a multinomial distribution over all the possible values that could have been taken. Moreover, each hard choice that would have been non-differentiable is replaced by a continuous soft choice. We will henceforth use bold letters to indicate the probabilistic version of a value. First, the memory tapeMt is replaced by an M ×M matrix Mt, where Mti,j corresponds to the probability of mti taking the value j. The same change is applied to the registers Rt, replacing them with an R×M matrix Rt, where Rti,j represents the probability of rti taking the value j. Finally, the instruction register is also transformed from a single value IRt to a vector of size M noted IRt, where the i-th element represents its probability to take the value i. The Machine does not contain any learnable parameter and will just execute a given command. To make it differentiable, the Machine now takes as input four probability distributions et, at, bt and ot, where et is a distribution over instructions, and at,bt and ot are distributions over the registers. We compute the argument values arg1t and arg2t as convex combinations of the different registers:\narg1t = R∑\ni=1 atirti arg2t = R∑ i=1 btirti, (1)\nwhere ati and bti are the i-th values of the vectors at and bt. Using these values, we can compute the output value of each instruction k using the following formula:\n∀0 ≤ c ≤M outtk,c = ∑\n0≤i,j≤M arg1ti · arg2tj · 1[gk(i, j) = c mod M ], (2)\nwhere gk is the function associated to the k-th instruction as presented in Table 1b. Since the executed instruction is controlled by the probability e, the output written to the register will also be a convex combination: outt = ∑N k=1 etkout t k, where N is the number of instructions. This value is then stored into the registers by performing a soft-write parametrised by ot. A special case is associated with the stop signal. When executing the model, we keep track of the probability that the program should have terminated before this iteration based on the probability associated at each iteration with the specific instruction that controls this flag. Once this probability goes over a threshold ηstop ∈ (0, 1], the execution is halted. We applied the same techniques to make the side-effects differentiable, this is presented in appendix A.1. The Controller is the only learnable part of our model. The first learnable part is the initial values for the registers R0 and for the instruction register IR0. The second learnable part is the parameters of the Controller which computes the required distributions using:\net = We ∗ IRt, at = Wa ∗ IRt, bt = Wb ∗ IRt, ot = Wo ∗ IRt (3) where We is an N ×M matrix and Wa, Wb and Wo are R×M matrices. A representation of these matrices can be found in Figure 4c. The Controller as defined above is composed of four independent, fully-connected layers. In Section 4.3 we will see that this complexity is sufficient for our model to be able to represent any program. Henceforth, we will denote by θ = {R0,IR0,We,Wa,Wb,Wo} the set of all the learnable parameters of this model."
    }, {
      "heading" : "4 Adaptative Neural Compiler",
      "text" : "We will now present the Adaptive Neural Compiler. Its goal is to find the best set of weights θ∗ for a given dataset such that our model will perform the correct input/output mapping as efficiently as it can. We begin by describing our learning objective in details. The two subsequent sections will focus on making the optimisation of our learning objective computationally feasible."
    }, {
      "heading" : "4.1 Objective function",
      "text" : "Our goal is to solve a given algorithmic problem efficiently. The algorithmic problem is defined as a set of input/output pairs. We also have access to a generic program that is able\nto perform the required mapping. In our example of accessing elements in a linked list, the transformation would consist in writing down the desired value at the specified position in the tape. The program given to us would iteratively go through the elements of the linked list, find the desired value and write it down at the desired position. If there exists some bias that would allow this traversal to be faster, we expect the program to exploit it. Our approach to this problem is to construct a differentiable objective function, mapping controller parameters to a loss. We define this loss based on the states of the memory tape and outputs of the Controller at each step of the execution. The precise mathematical formulation for each term of the loss is given in appendix B. Here we present the motivation behind each of them.\nCorrectness For a given input, we have the expected output. We compare the values of the expected output with the final memory tape provided by the execution.\nHalting To prevent programs to take an infinite amount of time without stopping, we defined a maximum number of iterations Tmax after which the execution is halted. Moreover, we add a penalty in the loss if the Controller didn’t halt before this limit.\nEfficiency We penalise each iteration taken by the program where it does not stop.\nConfidence We add a term which will penalise probability of stopping if the current state of the memory is not the expected one. If only the correctness term was considered, nothing would encourage the learnt algorithm to halt as soon as it finished. If only correctness and halting were considered, then the program may not halt as early as possible. Confidence enables the algorithm to evaluate better when to stop. The loss is a weighted sum of the four above-mentioned terms. We denote the loss of the i-th training sample, given parameters θ, as Li(θ). Our learning objective is then specified as:\nmin θ ∑ i Li(θ) s.t. θ ∈ Θ, (4)\nwhere Θ is a set over the parameters such that the outputs of the Controller, the initial values of each register and of the instruction register are all probability distributions. The above optimisation is a highly non-convex problem. To be able to solve it using standard gradient descent based methods, we will first need to transform it to an unconstrained problem. We also know that the result of the optimisation of a non-convex objective function is strongly dependent on the initialisation point. In the rest of this section, we will first present a small modification to the model that will remove the constraints. We will then present our Neural Compiler that will provide a good initialisation to solve this problem."
    }, {
      "heading" : "4.2 Reformulation",
      "text" : "In order to use gradient descent methods without having to project the parameters on Θ, we alter the formulation of the controller. We add a softmax layer after each linear layer ensuring that the constraints on the Controller’s output will be respected. We also apply a softmax to the initial values of the registers and the instructions register, ensuring they will also respect the original constraints. This way, we transform the constrained-optimisation problem into an unconstrained one, allowing us to use standard gradient descent methods. As discussed in other works [10], this kind of model is hard to train and requires a high number of random restarts before converging to a good solution. We will now present a Neural Compiler that will provide good initialisations to help with this problem."
    }, {
      "heading" : "4.3 Neural Compiler",
      "text" : "The goal for the Neural Compiler is to convert an algorithm, written as an unambiguous program, to a set of parameters. These parameters, when put into the controller, will reproduce the exact steps of the algorithm. This is very similar to the problem framed by Reed and de Freitas [12], but we show here a way to accomplish it without any learning.\nThe different steps of the compilation are illustrated in Figure 4. The first step is to go from the written version of the program to the equivalent list of low level instruction. This step can be seen as going from Figure 2a to Figure 3a. The illustrative example uses a fairly low-level language but traditional features of programming languages such as loops or if-statements can be supported using the JEZ instruction. The use of constants as arguments or as values is handled by introducing new registers that hold these values. The value required to be passed as target position to the JEZ instruction can be resolved at compile time. Having obtained this intermediate representation, generating the parameters is straightforward. As can be seen in Figure 3a, each line contains one instruction, the two input registers and the output register, and corresponds to a command that the Controller will have to output. If we ensure that IR is a Dirac-delta distribution on a given value, then the matrix-vector product is equivalent to selecting a row of the weight matrix. As IR is incremented at each iteration, the Controller outputs the rows of the matrix in order. We thus have a one-to-one mapping between the lines of the intermediate representation and the rows of the weight matrix. An example of these matrices can be found in Figure 4c. The weight matrix has 10 rows, corresponding to the number of lines of code of our intermediate representation. On the first line of the matrix corresponding to the first argument (4cii), the fifth element has value 1, and is linked to the first line of code where the first argument to the READ operation is the fifth register. The number of rows of the weight matrix is linear in the number of lines of code in the original program. To output a command, we must be able to index its line with the instruction register IR, which means that the largest representable number in our Machine needs to be greater than the number of lines in our program. Moreover, any program written in a regular assembly language can be rewritten to use only our restricted set of instructions. This can be done first because all the conditionals of the the assembly language can be expressed as a combination of arithmetic and JEZ instructions. Secondly because all the arithmetic operations can be represented as a combination of our simple arithmetic operations, loops and ifs statements. This means that any program that can run on a regular computer, can be first rewritten to use our restricted set of instructions and then compiled down to a set of weights for our model. Even though other models use LSTM as controller, we showed here that a Controller composed of simple linear functions is expressive enough. The advantage of this simpler model is that we can now easily interpret\nthe weights of our model in a way that would not have be possible if we had a recurrent network as a controller. The most straightforward way to leverage the results of the compilation is to initialise the Controller with the weights obtained through compilation of the generic algorithm. To account for the extra softmax layer, we need to multiply the weights produced by the compiler by a large constant to output Dirac-delta distributions. Some results associated with this technique can be found in Section 5.1. However, if we initialise with exactly this sharp set of parameters, the training procedure is not able to move away from the initialisation as the gradients associated with the softmax in this region are very small. Instead, we initialise the controller with a non-ideal version of the generic algorithm. This means that the choice with the highest probability in the output of the Controller is correct, but the probability of other choices is not zero. As can be seen in Section 5.2, this allows the Controller to learn by gradient descent a new algorithm, different from the original one, that has a lower loss than the ideal version of the compiled program."
    }, {
      "heading" : "5 Experiments",
      "text" : "We performed two sets of experiments. The first shows the capability of the Neural Compiler to perfectly reproduce any given program. The second shows that our Neural Compiler can adapt and improve the performance of programs. We present results of data-specific optimisation being carried out and show decreases in runtime for all the algorithms and additionally, for some algorithms, show that the runtime is a different computationalcomplexity class altogether. All the code required to reproduce these experiments is available online 1."
    }, {
      "heading" : "5.1 Compilation",
      "text" : "The compiler described in section 4.3 allows us to go from a program written using our instruction set to a set of weights θ for our Controller. To illustrate this point, we implemented simple programs that can solve the tasks introduced by Kurach et al. [8] and a shortest path problem. One of these implementations can be found in Figure 2a, while the others are available in appendix F. These programs are written in a specific language, and are transformed by the Neural Compiler into parameters for the model. As expected, the resulting models solve the original tasks exactly and can generalise to any input sequence."
    }, {
      "heading" : "5.2 ANC experiments",
      "text" : "In addition to being able to reproduce any given program as was done by Reed and de Freitas [12], we have the possibility of optimising the resulting program further. We exhibit this by compiling program down to our model and optimising their performance. The efficiency gains for these tasks come either from finding simpler, equivalent algorithms or by exploiting some bias in the data to either remove instructions or change the underlying algorithm. We identify three different levels of interpretability for our model: The first type corresponds to weights containing only Dirac-delta distributions, there is an exact one-to-one mapping between lines in the weight matrices and lines of assembly code. In the second type where all probabilities are Dirac-delta except the ones associated with the execution of the JEZ instruction, we can recover an exact algorithm that will use if statements to enumerate the different cases arising from this conditional jump. In the third type where any operation other than JEZ is executed in a soft way or use a soft argument, it is not possible to recover a program that will be as efficient as the learned one. We present here briefly the considered tasks and biases, and report the reader to appendix F for a detailed encoding of the input/output tape.\n1https://github.com/albanD/adaptive-neural-compilation\nFor each of these tasks, we perform a grid search on the loss parameters and on our hyperparameters. Training is performed using Adam [7]. We choose the best set of hyperparameters and run the optimisation with 100 different random seeds. We consider that a program has been successfully optimised when two conditions are fulfilled. First, it needs to output the correct solution for all test cases presenting the same bias. Second, the average number of iterations taken to solve a problem must be lower than the algorithm used for initialisation. Note that if we cared only about the first criterion, the methods presented in Section 5.1 would already provide a success rate of 100%, without requiring any training. The results are presented in Table 1. For each of these tasks, we manage to find faster algorithms. In the simple cases of Access and Swap, the optimal algorithm for the presented datasets are obtained. Exploiting the bias of the data, successful heuristics are incorporated in the algorithm and appropriate constants get stored in the initial value of registers. The learned programs for these tasks are always in the first case of interpretability, this means that we can recover the most efficient algorithm from the learned weights. While ListK and Addition have lower success rates, the improvements between the original and learned algorithms are still significant. Both were initialised with iterative algorithms with O(n) complexities. They managed to find constant time O(1) algorithms to solve the given problems, making the runtime independent of the input. Achieving this means that the equivalence between the two approaches has been identified, similar to how optimising compilers operate. Moreover, on the ListK task, some learned programs corresponds to the second type of interpretability. Indeed these programs use soft jumps to condition the execution on the value of k. Even though these program would not generalise to other values of k, some learned programs for this task achieve a type one interpretability and a study of the learned algorithm reveal that they can generalise to any value of k.\nFinally, the Increment task achieves an unexpected result. Indeed, it is able to outperform our best possible algorithm. By looking at the learned program, we can see that it is actually leveraging the possibility to perform soft writes over multiple elements of the memory at the same time to reduce its runtime. This is the only case where we see a learned program associated with the third type of interpretability. While our ideal algorithm would give a confidence of 1 on the output, this algorithm is unable to do so, but it has a high enough confidence of 0.9 to be considered a correct algorithm. In practice, for all but the most simple tasks, we observe that further optimisation is possible, as some useless instructions remain present. Some transformations of the controller are indeed difficult to achieve through the local changes operated by the gradient descent algorithm. An analysis of these failure modes of our algorithm can be found in appendix G.4. This motivates us to envision the use of approaches other than gradient descent to address these issues."
    }, {
      "heading" : "6 Discussion",
      "text" : "The work presented here is a first step towards adaptive learning of programs. It opens up several interesting directions of future research. For exemple, the definition of efficiency that we considered in this paper is flexible. We chose to only look at the average number of operations executed to generate the output from the input. We leave the study of other potential measures such as Kolmogorov Complexity and sloc, to name a few, for future works. As shown in the experiment section, our current method is very good at finding efficient solutions for simple programs. For more complex programs, only a solution close to the initialisation can be found. Even though training heuristics could help with the tasks considered here, they would likely not scale up to real applications. Indeed, the main problem we identified is that the gradient-descent based optimisation is unable to explore the space of programs effectively, by performing only local transformations. In future work, we want to explore different optimisation methods. One approach would be to mix global and local exploration to improve the quality of the solutions. A more ambitious plan would be to leverage the structure of the problem and use techniques from combinatorial optimisation to try and solve the original discrete problem."
    }, {
      "heading" : "7 Appendix",
      "text" : ""
    }, {
      "heading" : "A Detailed Model Description",
      "text" : "In this section, we are going to precisely define the non differentiable model used above. This model can be seen as a recurrent network. Indeed, it takes as input an initial memory tape, performs a certain number of iterations and outputs a final memory tape. The memory tape is an array of M cells, where a cell is an element holding a single integer value. The internal state of this recurrent model are the memory, the registers and the instruction register. The registers are another set of R cells that are internal to the model. The instruction register is a single cell used in a specific way described later. These internal states are noted Mt = {mt1,mt2, . . . ,mtM}, Rt = {rt1, rt2, . . . , rtR} and IR\nt for the memory, the registers and the instruction register respectively. Figure 1 describes in more detail how the different elements interact with each other. At each iteration, the Controller takes as input the value of the instruction register IRt and outputs four values: et, at, bt, ot = Controller(IRt). (5) The first value et is used to select one of the instruction of the Machine to execute at this iteration. The second and third values at and bt will identify which registers to use as the first and second argument for the selected instruction. The fourth value ot identity the output register where to write the result of the executed instruction. The Machine then takes as input these four values and the internal state and computes the updated value of the internal state and a stop flag:\nMt+1,Rt+1, IRt+1, stop = Machine(Mt,Rt, IRt, et, at, bt, ot). (6)\nThe stop flag is a binary flag. When its value is 1, it means that the model will stop the execution and the current memory state will be returned.\nThe Machine The machine is a deterministic function that increments the instruction register and executes the command given by the Controller to update the current internal state. The set of instructions that can be executed by the Machine can be found in Table 1b. Each instruction takes two values as arguments and returns a value. Additionally, some of these instructions have side effects. This mean that they do not just output a value, they perform another task. This other task can be for example to modify the content of the memory. All the considered side effects can be found in Table 1b. By convention, instructions that don’t have a value to return and that are used only for their side-effect will return a value of 0.\nThe Controller The Controller is a function that takes as input a single value and outputs four different values. The Controller’s internal parameters, the initial values for the registers and the initial value of the instruction register define uniquely a given Controller. The usual choice in the literature is to use an LSTM network[2, 3, 8] as controller. Our choice was to instead use a simpler model. Indeed, our Controller associates a command to each possible value of the instruction register. Since the instruction register’s value will increase by one at each iteration, this will enforce the Controller to encode in its weights what to do at each iteration. If we were using a recurrent controller the same instruction register could potentially be associated to different sets of outputs and we would lose this one to one mapping. To make this clearer, we first rewrite the instruction register as an indicator vector with a 1 at the position of its value:\nIi = { 1 if i = IRt\n0 otherwise . (7)\nIn this case, we can write a single output at of the Controller as the result of a linear function of I: at = Wa ∗ I , (8) whereWa is the 1xM matrix containing the value that need to be chosen as first arguments for each possible value of the instruction register and ∗ represent a matrix vector multiplication."
    }, {
      "heading" : "A.1 Mathematical details of the differentiable model",
      "text" : "In order to make the model differentiable, every value and every choice are replaced by probability distributions over the possible choices. Using convex combinations of probability, the execution of the Machine is made differentiable. We present here the mathematical formulation of this procedure for the case of the side-effects.\nSTOP In the discrete model, the execution is halted when the STOP instruction is executed. However, in the differentiable model, the STOP instruction may be executed with a probability smaller than 1. To take this into account, when executing the model, we keep track of the probability that the program should have terminated before this iteration based on the probability associated to the STOP instruction at each iteration. Once this probability goes over a threshold ηstop ∈]0, 1], the execution is halted.\nREAD The mechanism is entirely the same as the one used to compute the arguments based on the registers and a probability distribution over the registers.\nJEZ We note IRt+1jez and IR t+1 njez the new value of IR t if we had respectively executed or not the JEZ instruction. We also have etjez the probability of executing this instruction at iteration t. The new value of the instruction register is:\nIRt+1 = IRt+1njez · (1− etjez) + IR t+1 jez · etjez (9)\nIRt+1jez is himself computed based on several probability distribution. If we consider that the instruction JEZ is executed with probabilistic arguments cond and label, its value is given by\nIRt+1jez = label · cond0 + INC(IR t) · (1− cond0) (10)\nWith a probability equals to the one that the first argument is equal to zero, the new value of IRt is label. With the complement, it is equal to the incremented version of its current value, as the machine automatically increments the instruction register.\nWRITE The mechanism is fairly similar to the one of the JEZ instruction. We note Mt+1W RIT E and M t+1 nW RIT E the new value of Mt if we had respectively executed or not the WRITE instruction. We also have etwrite the probability of executing this instruction at iteration t. The new value of the memory matrix register is:\nMt+1 = Mt+1nW RIT E · (1− e t write) + Mt+1W RIT E · e t W RIT E (11)\nAs with the JEZ instruction, the value of Mt+1W RIT E is dependent on the two probability distribution given as input: addr and val. The probability that the i-th cell of the memory tape contains the value j after the update is:\nM t+1i,j = addri · valj + (1− addri) ·M t i,j (12)\nNote that this can done using linear algebra operations so as to update everything in one global operation.\nMt+1 = ( ((1− addr)1T )⊗Mt ) + (addr valT ) (13)"
    }, {
      "heading" : "B Specification of the loss",
      "text" : "This loss contains four terms that will balance the correctness of the learnt algorithm, proper usage of the stop signal and speed of the algorithms. The parameters defining the models are the weight of the Controller’s function and the initial value of the registers. When running the model with the parameters θ, we consider that the execution ran for T time steps. We consider the memory to have a size M and that each number can be an integer between 0 and M − 1. Mt was the state of the memory at the t-th step. T and C are the target memory and the 0-1 mask of the elements we want to consider. All these elements are matrices where for example Mti,j is the probability of the i-th entry of the memory to take the value j at the step t. We also note pstop,t the probability outputted by the Machine that it should have stopped before iteration t.\nCorrectness The first term corresponds to the correctness of the given algorithm. For a given input, we have the expected output and a mask. The mask allows us to know which elements in the memory we should consider when comparing the solutions. For the given input, we will compare the values specified by the mask of the expected output with the final memory tape provided by the execution. We compare them with the L2 distance in the probability space. Using the notations from above, we can write this term as:\nLc(θ) = ∑ i,j Ci,j(MTi,j(θ)−Ti,j)2. (14)\nIf we optimised only this first term, nothing would encourage the learnt algorithm to use the STOP instruction and halt as soon as it finished.\nHalting To prevent programs to take an infinite amount of time without stopping, we defined a maximum number of iterations Tmax after which the execution is halted. During training, we also add a penalty if the Controller didn’t halt before this limit:\nLsTmax(θ) = (1− pstop−T (θ)) · [T == Tmax] (15)\nEfficiency If we consider only the above mentioned losses, the program will make sure to halt by itself but won’t do it as early as possible. We incentivise this behaviour by penalising each iteration taken by the program where it does not stop:\nLt(θ) = ∑\nt∈[1,T−1]\n(1− pstop,t(θ)). (16)\nConfidence Moreover, we want the algorithm to have a good confidence to stop when it has found the correct output. To do so, we add the following term which will penalise probability of stopping if the current state of the memory is not the expected one:\nLst(θ) = ∑\nt∈[2,T ] ∑ i,j (pstop,t(θ)− pstop,t−1(θ))Ci,j(Mti,j(θ)−Ti,j)2. (17)\nThe increase in probability (pstop,t − pstop,t−1) corresponds to the probability of stopping exactly at iteration t. So, this is equivalent to the expected error made.\nTotal loss The complete loss that we use is then the following: L(θ) = αLc(θ) + βLsTmax(θ) + γLst(θ) + δLt(θ). (18)"
    }, {
      "heading" : "C Distributed representation of the program",
      "text" : "For the most of out experiments, the learned weights are fully interpretable as they fit in the first type of interpretability. However, in some specific cases, under the pressure of our loss encouraging a smaller number of iterations, an interesting behavior emerges.\nRemarks It is interesting to note that the decompiled version is not straightforward to interpret. Indeed when we reach a program that has non Dirac-delta distributions in its weights, we cannot perform the inverse of the one-to-one mapping performed by the compiler. In fact, it relies on this blurriness to be able to execute the program with a smaller number of instruction. Notably, by having some blurriness on the JEZ instruction, the program can hide additional instructions, by creating a distributed state. We now explain the mechanism used to achieve this.\nCreating a distributed state Consider the following program and assume that the initial value of IR is 0: Initial Registers: R1 = 0;R2 = 1;R3 = 4, R4 = 0\nProgram: 0 : R1 = READ (R1, R4) 1 : R4 = JEZ (R1, R3) 2 : R4 = WRITE(R1, R1) 3 : R4 = WRITE(R1, R3)\nIf you take this program and execute it for three iterations, it will: read the first value of the tape into R1. Then, if this value is zero, it will jump to State 4, otherwise it will just increment IR. This means that depending on the value that was in R1, the next instruction that will be executed will be different (in this case, the difference between State 3 and State 4 is which registers they will be writing from). This is our standard way of implementing conditionals. Imagine that, after learning, the second instruction in our example program has 0.5 probability of being a JEZ and 0.5 probability of being a ZERO. If the content of R1 is a zero, according to the JEZ, we should jump to State 4, but this instruction is executed with a probability of 0.5. We also have 0.5 probability of executing the ZERO instruction, which would lead to State 3. Therefore, IR is not a Dirac-delta distribution anymore but points to State 3 with probability 0.5 and State 4 with probability 0.5.\nExploiting a distributed state To illustrate, we will discuss how the Controller computes a for a model with 3 registers. The Table 2 show an example of some weights for such a controller. If we are in State 1, the output of the controller is going to be\nout = softmax([20, 5,−20]) = [0.9999..., 3e−7, 4e−18] (19)\nIf we are in State 2, the output of the controller is going to be out = softmax([−20, 5, 20]) = [4e−18, 3e−7, 0.9999...] (20)\nIn both cases, the output of the controller is therefore going to be almost discrete. In State 1, R1 would be chosen and in State 2, R3 would be chosen. However, in the case where we have a distributed state with probability 0.5 over State 1 and 0.5 over State 2, the output would be:\nout = softmax(0.5 ∗ [−20, 5, 20] + 0.5[20, 5,−20]) = softmax([0, 10, 0]) = [4e−5, 0.999, 4e−5].\n(21)\nNote that the result of the distributed state is actually different from the result of the discrete states. Moreover it is still a discrete choice of the second register. Because this program contains distributed elements, it is not possible to perform the one-toone mapping between the weights and the lines of code. Though every instruction executed by the program, except for the JEZ, are binary. This means that this model can be translated to a regular program that will take exactly the same runtime, but will require more lines of codes than the number of lines in the matrix."
    }, {
      "heading" : "D Alternative Learning Strategies",
      "text" : "A critique that can be made to this method is that we will still initialise close to a local minimum. Another approach might be to start from a random initialisation but adding a penalty on the value of the weights such that they are encourage to be close to the generic algorithm. This can be seen as L2 regularisation but instead of pushing the weights to 0, we push then with the value corresponding to the generic algorithm. If we start with a very high value of this penalty but use an annealing schedule where its importance is very quickly reduced, this is going to be equivalent to the previous method."
    }, {
      "heading" : "E Possible Extension",
      "text" : ""
    }, {
      "heading" : "E.1 Making objective function differentiable",
      "text" : "These experiments showed that we can transform any program that perform a mapping between an input memory tape to an output memory tape to a set of parameters and execute it using our model. The first point we want to make here is that this means that we take any program and transform it into a differentiable function easily. For example, if we want to learn a model that given a graph and two nodes a and b, will output the list of nodes to go through to go from a to b in the shortest amount of time. We can easily define the loss of the length of the path outputted by the model. Unfortunately, the function that computes this length from the set of nodes is not differentiable. Here we could implement this function in our model and use it between the prediction of the model and the loss function to get an end to end trainable system.\nE.2 Beyond mimicking and towards open problems It would even be possible to generalise our learning procedure to more complex problems for which we don’t have a ground truth output. For example, we could consider problems where the exact answer for a given input is not computable or not unique. If the goodness of a solution can be computed easily, this value could be used as training objective. Any program giving a solution could be used as initialisation and our framework would improve it, making it generate better solutions."
    }, {
      "heading" : "F Example tasks",
      "text" : "This section will present the programs that we use as initialisation for the experiment section."
    }, {
      "heading" : "F.1 Access",
      "text" : "In this task, the first element in the memory is a value k. Starting from the second element, the memory contains a zero-terminated list. The goal is to access the k-th element in the list that is zero-indexed. The program associated with this task can be found in Listing 1.\n1 var k = 0 2 k = READ (0) 3 k = INC(k) 4 k = READ(k) 5 WRITE (0, k) 6 STOP ()\nListing 1: Access Task\nExample input: 6 9 1 2 7 9 8 1 3 5 Output: 1 9 1 2 7 9 8 1 3 5"
    }, {
      "heading" : "F.2 Copy",
      "text" : "In this task, the first element in the memory is a pointer p. Starting from the second element, the memory contains a zero-terminated list. The goal is to copy this list at the given pointer. The program associated with this task can be found in Listing 2.\n1 var read_addr = 0 2 var read_value = 0 3 var write_addr = 0 4\n5 write_addr = READ (0) 6 l_loop : read_value = READ( read_addr ) 7 JEZ(read_value , l_stop ) 8 WRITE (write_addr , read_value ) 9 read_addr = INC( read_addr )\n10 write_addr = INC( write_addr ) 11 JEZ (0, l_loop ) 12\n13 l_stop : STOP () Listing 2: Copy Task\nExample input: 9 11 3 1 5 14 0 0 0 0 0 0 0 0 0 Output: 9 11 3 1 5 14 0 0 0 11 3 1 5 14 0\nF.3 Increment\nIn this task, the memory contains a zero-terminated list. The goal is to increment each value in the list by 1. The program associated with this task can be found in Listing 3.\n1 var read_addr = 0 2 var read_value = 0 3\n4 l_loop : read_value = READ( read_addr ) 5 JEZ(read_value , l_stop ) 6 read_value = INC( read_value ) 7 WRITE (read_addr , read_value ) 8 read_addr = INC( read_addr ) 9 JEZ (0, l_loop )\n10\n11 l_stop : STOP () Listing 3: Increment Task\nExample input: 1 2 2 3 0 0 0 Output: 2 3 3 4 0 0 0"
    }, {
      "heading" : "F.4 Reverse",
      "text" : "In this task, the first element in the memory is a pointer p. Starting from the second element, the memory contains a zero-terminated list. The goal is to copy this list at the given pointer in the reverse order. The program associated with this task can be found in Listing 4.\n1 var read_addr = 0 2 var read_value = 0 3 var write_addr = 0 4\n5 write_addr = READ( write_addr ) 6 l_count_phase : read_value = READ( read_addr ) 7 JEZ(read_value , l_copy_phase ) 8 read_addr = INC( read_addr ) 9 JEZ (0, l_count_phase )\n10\n11 l_copy_phase : read_addr = DEC( read_addr ) 12 JEZ(read_addr , l_stop ) 13 read_value = READ( read_addr ) 14 WRITE (write_addr , read_value ) 15 write_addr = INC( write_addr ) 16 JEZ (0, l_copy_phase ) 17\n18 l_stop : STOP () Listing 4: Reverse Task\nExample input: 5 7 2 13 14 0 0 0 0 0 0 0 0 0 0 Output: 5 7 2 13 14 14 13 2 7 0 0 0 0 0 0"
    }, {
      "heading" : "F.5 Permutation",
      "text" : "In this task, the memory contains two zero-terminated list one after the other. The first contains a set of indices. the second contains a set of values. The goal is to fill the first list with the values in the second list at the given index. The program associated with this task can be found in Listing 5.\n1 var read_addr = 0 2 var read_value = 0 3 var write_offset = 0 4\n5 l_count_phase : read_value = READ( write_offset ) 6 write_offset = INC( write_offset ) 7 JEZ(read_value , l_copy_phase ) 8 JEZ (0, l_count_phase ) 9\n10 l_copy_phase : read_value = DEC( read_addr ) 11 JEZ(read_value , l_stop ) 12 read_value = ADD( write_offset , read_value ) 13 read_value = READ( read_value ) 14 WRITE (read_addr , read_value ) 15 read_addr = INC( read_addr ) 16 JEZ (0, l_copy_phase ) 17 l_stop : STOP ()\nListing 5: Permutation Task\nExample input: 2 1 3 0 13 4 6 0 0 0 0 0 0 0 0 Output: 4 13 6 0 13 4 6 0 0 0 0 0 0 0 0"
    }, {
      "heading" : "F.6 Swap",
      "text" : "In this task, the first two elements in the memory are pointers p and q. Starting from the third element, the memory contains a zero-terminated list. The goal is to swap the elements pointed by p and q in the list that is zero-indexed. The program associated with this task can be found in Listing 6.\n1 var p = 0 2 var p_val = 0 3 var q = 0 4 var q_val = 0 5\n6 p = READ (0) 7 q = READ (1) 8 p_val = READ(p) 9 q_val = READ(q)\n10 WRITE (q, p_val) 11 WRITE (p, q_val) 12 STOP ()\nListing 6: Swap Task\nExample input: 1 3 7 6 7 5 2 0 0 0 Output: 1 3 7 5 7 6 2 0 0 0"
    }, {
      "heading" : "F.7 ListSearch",
      "text" : "In this task, the first three elements in the memory are a pointer to the head of the linked list, the value we are looking for v and a pointer to a place in memory where to store the result. The rest of the memory contains the linked list. Each element in the linked list is two values, the first one is the pointer to the next element, the second is the value contained in this element. By convention, the last element in the list points to the address 0. The goal is to return the pointer to the first element whose value is equal to v. The program associated with this task can be found in Listing 7.\n1 var p_out = 0 2 var p_current = 0 3 var val_current = 0 4 var val_searched = 0 5\n6 val_searched = READ (1) 7 p_out = READ (2) 8 l_loop : p_current = READ( p_current ) 9 val_current = INC( p_current )\n10 val_current = READ( val_current ) 11 val_current = SUB( val_current , val_searched ) 12 JEZ( val_current , l_stop ) 13 JEZ (0, l_loop ) 14 l_stop : WRITE (p_out , p_current ) 15 STOP ()\nListing 7: ListSearch Task\nExample input: 11 10 2 9 4 3 10 0 6 7 13 5 12 0 0 Output: 11 10 5 9 4 3 10 0 6 7 13 5 12 0 0"
    }, {
      "heading" : "F.8 ListK",
      "text" : "In this task, the first three elements in the memory are a pointer to the head of the linked list, the number of hops we want to do k in the list and a pointer to a place in memory where to store the result. The rest of the memory contains the linked list. Each element in the linked list is two values, the first one is the pointer to the next element, the second is the value contained in this element. By convention, the last element in the list points to the address 0. The goal is to return the value of the k-th element of the linked list. The program associated with this task can be found in Listing 8.\n1 var p_out = 0 2 var p_current = 0 3 var val_current = 0 4 var k = 0 5\n6 k = READ (1) 7 p_out = READ (2) 8 l_loop : p_current = READ( p_current ) 9 k = DEC(k)\n10 JEZ(k, l_stop ) 11 JEZ (0, l_loop ) 12 l_stop : p_current = INC( p_current ) 13 p_current = READ( p_current ) 14 WRITE (p_out , p_current ) 15 STOP ()\nListing 8: ListK Task\nExample input: 3 2 2 9 15 0 0 0 1 15 17 7 13 0 0 11 Output: 3 2 17 9 15 0 0 0 1 15 17 7 13 0 0 11 10 0 0 0 10 0 0 0"
    }, {
      "heading" : "F.9 Walk BST",
      "text" : "In this task, the first two elements in the memory are a pointer to the head of the BST and a pointer to a place in memory where to store the result. Starting at the third element, there\nis a zero-terminated list containing the instructions on how to traverse in the BST. The rest of the memory contains the BST. Each element in the BST has three values, the first one is the value of this node, the second is the pointer to the left node and the third is the pointer to the right element. By convention, the leafs points to the address 0. The goal is to return the value of the node we get at after following the instructions. The instructions are 1 or 2 to go respectively to the left or the right. The program associated with this task can be found in Listing 9.\n1 var p_out = 0 2 var p_current = 0 3 var p_instr = 0 4 var instr = 0 5\n6 p_current = READ (0) 7 p_out = READ (1) 8 instr = READ (2) 9\n10 l_loop : JEZ(instr , l_stop ) 11 p_current = ADD(p_current , instr) 12 p_current = READ( p_current ) 13 p_instr = INC( p_instr ) 14 JEZ (0, l_loop ) 15\n16 l_stop : p_current = READ( p_current ) 17 WRITE (p_out , p_current ) 18 STOP ()\nListing 9: WalkBST Task\nExample input: 12 1 1 2 0 0 15 0 9 23 0 0 11 15 6 Output: 12 10 1 2 0 0 15 0 9 23 0 0 11 15 6 8 0 24 0 0 0 0 0 0 10 0 0 0 0 0 8 0 24 0 0 0 0 0 0 10 0 0 0 0 0"
    }, {
      "heading" : "F.10 Merge",
      "text" : "In this task, the first three elements in the memory are pointers to respectively, the first list, the second list and the output. The two lists are zero-terminated sorted lists. The goal is to merge the two lists into a single sorted zero-terminated list that starts at the output pointer. The program associated with this task can be found in Listing 10.\n1 var p_first_list = 0 2 var val_first_list = 0 3 var p_second_list = 0 4 var val_second_list = 0 5 var p_output_list = 0 6 var min = 0 7\n8 p_first_list = READ (0) 9 p_second_list = READ (1)\n10 p_output_list = READ (2) 11\n12 l_loop : val_first_list = READ( p_first_list ) 13 val_second_list = READ( p_second_list ) 14 JEZ( val_first_list , l_first_finished ) 15 JEZ( val_second_list , l_second_finished ) 16 min = MIN( val_first_list , val_second_list ) 17 min = SUB( val_first_list , min) 18 JEZ(min , l_first_smaller ) 19\n20 WRITE ( p_output_list , val_first_list ) 21 p_output_list = INC( p_output_list ) 22 p_first_list = INC( p_first_list ) 23 JEZ (0, l_loop ) 24\n25 l_first_smaller : WRITE ( p_output_list , val_second_list ) 26 p_output_list = INC( p_output_list ) 27 p_second_list = INC( p_second_list ) 28 JEZ (0, l_loop ) 29\n30 l_first_finished : p_first_list = ADD( p_second_list , 0) 31 val_first_list = ADD( val_second_list , 0) 32\n33 l_second_finished : WRITE ( p_output_list , val_first_list ) 34 p_first_list = INC( p_first_list ) 35 p_output_list = INC( p_output_list ) 36 val_first_list = READ( p_first_list ) 37 JEZ( val_first_list , l_stop ) 38 JEZ (0, l_second_finished ) 39\n40 l_stop : STOP () Listing 10: Merge Task\nExample input: 3 8 11 27 17 16 1 0 29 26 0 0 0 0 0 Output: 3 8 11 27 17 16 1 0 29 26 0 29 27 26 17 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 16 1 0 0 0 0 0 0 0 0 0 0 0 0 0"
    }, {
      "heading" : "F.11 Dijkstra",
      "text" : "In this task, we are provided with a graph represented in the input memory as follow. The first element is a pointer pout indicating where to write the results. The following elements contain a zero-terminated array with one entry for each vertex in the graph. Each entry is a pointer to a zero-terminated list that contains a pair of values for each outgoing edge of the considered node. Each pair of value contains first the index in the first array of the child node and the second value contains the cost of this edge. The goal is to write a zero-terminated list at the address provided by pout that will contain the value of the shortest path from the first node in the list to this node. The program associated with this task can be found in Listings 11 and 12.\n1 var min = 0 2 var argmin = 0 3\n4 var p_out = 0 5 var p_out_temp = 0 6 var p_in = 1 7 var p_in_temp = 1 8\n9 var nnodes = 0 10\n11 var zero = 0 12 var big = 99 13\n14 var tmp_node = 0 15 var tmp_weight = 0 16 var tmp_current = 0 17 var tmp = 0 18\n19 var didsmth = 0 20\n21 p_out = READ(p_out) 22 p_out_temp = ADD(p_out , zero) 23\n24 tmp_current = INC(zero) 25 l_loop_nnodes :tmp = READ( p_in_temp ) 26 JEZ(tmp , l_found_nnodes ) 27 WRITE (p_out_temp , big) 28 p_out_temp = INC( p_out_temp ) 29 WRITE (p_out_temp , tmp_current ) 30 p_out_temp = INC( p_out_temp ) 31 p_in_temp = INC( p_in_temp ) 32 nnodes = INC( nnodes ) 33 JEZ(zero , l_loop_nnodes ) 34\n35 l_found_nnodes :WRITE (p_out , zero) 36 JEZ(zero , l_find_min ) 37 l_min_return : p_in_temp = ADD(p_in , argmin ) 38 p_in_temp = READ( p_in_temp ) 39\n40 l_loop_sons : tmp_node = READ( p_in_temp ) 41 JEZ(tmp_node , l_find_min ) 42 tmp_node = DEC( tmp_node ) 43 p_in_temp = INC( p_in_temp ) 44 tmp_weight = READ( p_in_temp ) 45 p_in_temp = INC( p_in_temp ) 46\n47 p_out_temp = ADD(p_out , tmp_node ) 48 p_out_temp = ADD(p_out_temp , tmp_node ) 49 tmp_current = READ( p_out_temp ) 50 tmp_weight = ADD(min , tmp_weight ) 51\n52 tmp = MIN( tmp_current , tmp_weight ) 53 tmp = SUB( tmp_current , tmp) 54 JEZ(tmp , l_loop_sons ) 55 WRITE (p_out_temp , tmp_weight ) 56 JEZ(zero , l_loop_sons )\nListing 11: Dijkstra Algorithm (Part 1)\n57 l_find_min : p_out_temp = DEC(p_out) 58 tmp_node = DEC(zero) 59 min = ADD(big , zero) 60 argmin = DEC(zero) 61\n62 l_loop_min : p_out_temp = INC( p_out_temp ) 63 tmp_node = INC( tmp_node ) 64 tmp = SUB(tmp_node , nnodes ) 65 JEZ(tmp , l_min_found ) 66\n67 tmp_weight = READ( p_out_temp ) 68\n69 p_out_temp = INC( p_out_temp ) 70 tmp = READ( p_out_temp ) 71 JEZ(tmp , l_loop_min ) 72\n73 tmp = MAX(min , tmp_weight ) 74 tmp = SUB(tmp , tmp_weight ) 75 JEZ(tmp , l_loop_min ) 76 min = ADD(tmp_weight , zero) 77 argmin = ADD(tmp_node , zero) 78 JEZ(zero , l_loop_min ) 79\n80 l_min_found :tmp = SUB(min , big) 81 JEZ(tmp , l_stop ) 82 p_out_temp = ADD(p_out , argmin ) 83 p_out_temp = ADD(p_out_temp , argmin ) 84 p_out_temp = INC( p_out_temp ) 85 WRITE (p_out_temp , zero) 86 JEZ(zero , l_min_return ) 87\n88 l_stop :STOP () Listing 12: Dijkstra Algorithm (Part 2)\nExample omitted for space reasons"
    }, {
      "heading" : "G Learned optimisation: Case study",
      "text" : "Here we present an analysis of the optimisation achieved by the ANC. We take the example of the ListK task and study the difference between the learned program and the initialisation used."
    }, {
      "heading" : "G.1 Representation",
      "text" : "The representation chosen is under the form of the intermediary representation described in Figure (2b). Based on the parameters of the Controller, we can recover the approximate representation described in Figure (2b): 1For each possible \"discrete state\" of the instruction register, we can compute the commands outputted by the controller. We report the most probable value for each distribution, as well as the probability that the compiler would assign to this value. If no value has a probability higher than 0.5, we only report a neutral token (R-, -, NOP)."
    }, {
      "heading" : "G.2 Biased ListK",
      "text" : "Figure 5 represents the program that was used as initialisation to the optimisation problem. This is the direct result from the compilation performed by the Neural Compiler of the program described in Listing 8. A version with a probability of 1 for all necessary instructions would have been easily obtained but not amenable to learning.\nFigure 6 similarly describes the program that was obtained after learning. As a remainder, the bias introduced in the ListK task is that the linked list is well organised in memory. In the general case, the element could be in any order. An input memory tape to the problem of asking for the third element in the linked list containing {4, 5, 6, 7} would be: 9 3 2 0 0 11 5 0 7 5 4 7 6 0 0 0 0 0 0 0\nor 5 3 2 0 0 7 4 15 5 0 7 0 0 0 0 9 6 0 0 0\nIn the biased version of the task, all the elements are arranged in order and contiguously positioned on the tape. The only valid representation of this problems is: 3 3 2 5 4 7 5 9 6 0 7 0 0 0 0 0 0 0 0 0"
    }, {
      "heading" : "G.3 Solutions",
      "text" : "Because of the additional structure of the problem, the bias in the data, a more efficient algorithm to find the solution exists. Let us dive into the comparison of the two different solutions. Both use their first two states to read the parameters of the given instance of the task. Which element of the list should be returned is read at line (0:) and where to write the returned value is read at line (1:). Step (2:) to (6:) are dedicated to putting the address of the k-th value of the linked list into the registers R1. Step (7:) to (9:) perform the same task in both solution: reading the value at the address contained in R1, writing it at the desired position and stopping. The difference between the two programs lies in how they put the address of the k-th value into R1.\nGeneric The initialisation program, used for initialisation, works in the most general case so needs to perform a loop where it put the address of the next element in the linked list in R1 (2:), decrement the number of jumps remaining to be made (3:), checking whether the wanted element has been reached (4:) and going back to the start of the loop if not (5:). Once the desired element is reached, R1 is incremented so as to point on the value of the linked list element.\nSpecific On the other hand, in the biased version of the problem, the position of the desired value can be analytically determined. The function parameters occupy the first three cells of the tape. After those, each element of the linked list will occupy two cells (one for the pointer to the next address and one for the value). Therefore, the address of the desired value is given by\nR1 = 3 + (2 ∗ (k − 1) + 1)− 1 + 1 = 3 + 2 ∗ k − 1 (22)\n(the -1 comes from the fact that the address are 0-indexed and the final +1 from the fact that we are interested in the position of the value and not of the pointer.) The way this is computed is as follows:\n- R1 = 3 + k by adding the constant 3 to the registers R2 containing K. (2:) - R2 = k − 1 (3:) - R1 = 3 + 2 ∗ k − 1 by adding the now reduced value of R2. (6:)\nThe algorithm implemented by the learned version is therefore much more efficient for the biased dataset, due to its capability to ignore the loop."
    }, {
      "heading" : "G.4 Failure analysis",
      "text" : "An observation that can be made is that in the learned version of the program, Step (4:) and (5:) are not contributing to the algorithms. They execute instructions that have no side effect and store the results into the registers R7 that is never used later in the execution. The learned algorithm could easily be more efficient by not performing these two operations. However, such an optimisation, while perhaps trivial for a standard compiler, capable of detecting unused values, is fairly hard for our optimisers to discover. Because we are only doing gradient descent, the action of \"moving some instructions earlier in the program\" which would be needed here to make the useless instructions disappear, is fairly hard, as it involves modifying several rows of the program at once in a coherent manner."
    } ],
    "references" : [ {
      "title" : "Learning efficient algorithms with hierarchical attentive memory",
      "author" : [ "Marcin Andrychowicz", "Karol Kurach" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2016
    }, {
      "title" : "Learning to transduce with unbounded memory",
      "author" : [ "Edward Grefenstette", "Karl Moritz Hermann", "Mustafa Suleyman", "Phil Blunsom" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2015
    }, {
      "title" : "Ratajszczak, and Gilles Wiber",
      "author" : [ "Frédéric Gruau", "Jean-Yves" ],
      "venue" : "A neural compiler. Theoretical Computer Science,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1995
    }, {
      "title" : "Inferring algorithmic patterns with stack-augmented recurrent nets",
      "author" : [ "Armand Joulin", "Tomas Mikolov" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2015
    }, {
      "title" : "Neural gpus learn algorithms",
      "author" : [ "Łukasz Kaiser", "Ilya Sutskever" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2016
    }, {
      "title" : "A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Adam" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2015
    }, {
      "title" : "Neural random-access machines",
      "author" : [ "Karol Kurach", "Marcin Andrychowicz", "Ilya Sutskever" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2016
    }, {
      "title" : "Superoptimizer: a look at the smallest program",
      "author" : [ "Henry Massalin" ],
      "venue" : "In ACM SIGPLAN Notices,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1987
    }, {
      "title" : "Adding gradient noise improves learning for very deep networks",
      "author" : [ "Arvind Neelakantan", "Luke Vilnis", "Quoc V Le", "Ilya Sutskever", "Lukasz Kaiser", "Karol Kurach", "James Martens" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2016
    }, {
      "title" : "Symbolic processing in neural networks",
      "author" : [ "João Pedro Neto", "Hava Siegelmann", "Félix Costa" ],
      "venue" : "Journal of the Brazilian Computer Society,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2003
    }, {
      "title" : "Neural programmer-interpreters",
      "author" : [ "Scott Reed", "Nando de Freitas" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2016
    }, {
      "title" : "Stochastic superoptimization",
      "author" : [ "Eric Schkufza", "Rahul Sharma", "Alex Aiken" ],
      "venue" : "In ACM SIGARCH Computer Architecture News,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2013
    }, {
      "title" : "Conditionally correct superoptimization",
      "author" : [ "Rahul Sharma", "Eric Schkufza", "Berkeley Churchill", "Alex Aiken" ],
      "venue" : "In OOPSLA,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2015
    }, {
      "title" : "Neural programming language",
      "author" : [ "Hava Siegelmann" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1994
    }, {
      "title" : "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
      "author" : [ "Ronald Williams" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1992
    }, {
      "title" : "Reinforcement learning neural turing machines",
      "author" : [ "Wojciech Zaremba", "Ilya Sutskever" ],
      "venue" : "arXiv preprint arXiv:1505.00521,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2015
    }, {
      "title" : "Learning simple algorithms from examples",
      "author" : [ "Wojciech Zaremba", "Tomas Mikolov", "Armand Joulin", "Rob Fergus" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "Recently, Conditionally Correct Superoptimization [14] was proposed as a method for leveraging samples of the input data distribution to go beyond semantically equivalent optimisation and towards data-specific performance improvements.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 1,
      "context" : "Many other models have been published recently [3, 5, 6, 8] that build and improve on the early work by Graves et al.",
      "startOffset" : 47,
      "endOffset" : 59
    }, {
      "referenceID" : 3,
      "context" : "Many other models have been published recently [3, 5, 6, 8] that build and improve on the early work by Graves et al.",
      "startOffset" : 47,
      "endOffset" : 59
    }, {
      "referenceID" : 4,
      "context" : "Many other models have been published recently [3, 5, 6, 8] that build and improve on the early work by Graves et al.",
      "startOffset" : 47,
      "endOffset" : 59
    }, {
      "referenceID" : 6,
      "context" : "Many other models have been published recently [3, 5, 6, 8] that build and improve on the early work by Graves et al.",
      "startOffset" : 47,
      "endOffset" : 59
    }, {
      "referenceID" : 8,
      "context" : "[10].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "Siegelmann [15] described how to build a Neural Network that would perform the same operations as a given program.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 2,
      "context" : "[4] targeting an extended version of Pascal.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "[11] wrote a compiler for NETDEF, a language based on the Occam programming language.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "The recent work by Reed and de Freitas [12] on Neural Programmer-Interpreters (NPI) can also be seen as a way to compile any program into a neural network by learning a model that mimic the program.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 7,
      "context" : "By exploring the space of all possible programs, either exhaustively [9] or in a stochastic manner [13], they search for programs having the same results but being more efficient.",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 11,
      "context" : "By exploring the space of all possible programs, either exhaustively [9] or in a stochastic manner [13], they search for programs having the same results but being more efficient.",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 12,
      "context" : "[14] broadens the space of acceptable improvements to data-specific optimisations as opposed to the provably equivalent transformations that were previously the only ones considered.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 1,
      "context" : "Extensions of this model have been proposed in [3, 5] where the memory tape was replaced by differentiable versions of stacks or lists.",
      "startOffset" : 47,
      "endOffset" : 53
    }, {
      "referenceID" : 3,
      "context" : "Extensions of this model have been proposed in [3, 5] where the memory tape was replaced by differentiable versions of stacks or lists.",
      "startOffset" : 47,
      "endOffset" : 53
    }, {
      "referenceID" : 6,
      "context" : "[8] modified the NTM to introduce a notion of pointers making it more amenable to represent traditional programs.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "Parallel works have been using Reinforcement Learning techniques such as the REINFORCE algorithm [1, 16, 17] or Q-learning [18] to be able to work with non differentiable versions of the above mentioned models.",
      "startOffset" : 97,
      "endOffset" : 108
    }, {
      "referenceID" : 14,
      "context" : "Parallel works have been using Reinforcement Learning techniques such as the REINFORCE algorithm [1, 16, 17] or Q-learning [18] to be able to work with non differentiable versions of the above mentioned models.",
      "startOffset" : 97,
      "endOffset" : 108
    }, {
      "referenceID" : 15,
      "context" : "Parallel works have been using Reinforcement Learning techniques such as the REINFORCE algorithm [1, 16, 17] or Q-learning [18] to be able to work with non differentiable versions of the above mentioned models.",
      "startOffset" : 97,
      "endOffset" : 108
    }, {
      "referenceID" : 16,
      "context" : "Parallel works have been using Reinforcement Learning techniques such as the REINFORCE algorithm [1, 16, 17] or Q-learning [18] to be able to work with non differentiable versions of the above mentioned models.",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 6,
      "context" : "the Neural RAM [8] requires a high number of random restarts before converging to a correct solution [10], even when using the best hyperparameters obtained through a large grid search.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 8,
      "context" : "the Neural RAM [8] requires a high number of random restarts before converging to a correct solution [10], even when using the best hyperparameters obtained through a large grid search.",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 8,
      "context" : "As discussed in other works [10], this kind of model is hard to train and requires a high number of random restarts before converging to a good solution.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 10,
      "context" : "This is very similar to the problem framed by Reed and de Freitas [12], but we show here a way to accomplish it without any learning.",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 6,
      "context" : "[8] and a shortest path problem.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 10,
      "context" : "In addition to being able to reproduce any given program as was done by Reed and de Freitas [12], we have the possibility of optimising the resulting program further.",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 5,
      "context" : "Training is performed using Adam [7].",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 0,
      "context" : "[1] Marcin Andrychowicz and Karol Kurach.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "[3] Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, and Phil Blunsom.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[4] Frédéric Gruau, Jean-Yves Ratajszczak, and Gilles Wiber.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[5] Armand Joulin and Tomas Mikolov.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[6] Łukasz Kaiser and Ilya Sutskever.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[7] Diederik Kingma and Jimmy Adam.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[8] Karol Kurach, Marcin Andrychowicz, and Ilya Sutskever.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[9] Henry Massalin.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "[10] Arvind Neelakantan, Luke Vilnis, Quoc V Le, Ilya Sutskever, Lukasz Kaiser, Karol Kurach, and James Martens.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 9,
      "context" : "[11] João Pedro Neto, Hava Siegelmann, and Félix Costa.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[12] Scott Reed and Nando de Freitas.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[13] Eric Schkufza, Rahul Sharma, and Alex Aiken.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[14] Rahul Sharma, Eric Schkufza, Berkeley Churchill, and Alex Aiken.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[15] Hava Siegelmann.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[16] Ronald Williams.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[17] Wojciech Zaremba and Ilya Sutskever.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[18] Wojciech Zaremba, Tomas Mikolov, Armand Joulin, and Rob Fergus.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 1,
      "context" : "The usual choice in the literature is to use an LSTM network[2, 3, 8] as controller.",
      "startOffset" : 60,
      "endOffset" : 69
    }, {
      "referenceID" : 6,
      "context" : "The usual choice in the literature is to use an LSTM network[2, 3, 8] as controller.",
      "startOffset" : 60,
      "endOffset" : 69
    }, {
      "referenceID" : 8,
      "context" : "5[20, 5,−20]) = softmax([0, 10, 0]) = [4e−5, 0.",
      "startOffset" : 24,
      "endOffset" : 34
    } ],
    "year" : 2016,
    "abstractText" : "This paper proposes an adaptive neural-compilation framework to address the problem of efficient program learning. Traditional code optimisation strategies used in compilers are based on applying pre-specified set of transformations that make the code faster to execute without changing its semantics. In contrast, our work involves adapting programs to make them more efficient while considering correctness only on a target input distribution. Our approach is inspired by the recent works on differentiable representations of programs. We show that it is possible to compile programs written in a low-level language to a differentiable representation. We also show how programs in this representation can be optimised to make them efficient on a target distribution of inputs. Experimental results demonstrate that our approach enables learning specifically-tuned algorithms for given data distributions with a high success rate.",
    "creator" : "LaTeX with hyperref package"
  }
}