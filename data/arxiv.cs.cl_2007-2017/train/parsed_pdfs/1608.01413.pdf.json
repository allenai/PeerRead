{
  "name" : "1608.01413.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Solving General Arithmetic Word Problems",
    "authors" : [ "Subhro Roy", "Dan Roth" ],
    "emails" : [ "danr}@illinois.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In recent years there is growing interest in understanding natural language text for the purpose of answering science related questions from text as well as quantitative problems of various kinds. In this context, understanding and solving arithmetic word problems is of specific interest. Word problems arise naturally when reading the financial section of a newspaper, following election coverage, or when studying elementary school arithmetic word problems. These problems pose an interesting challenge to the NLP community, due to its concise and relatively straightforward text, and seemingly simple semantics. Arithmetic word problems are usually directed towards elementary school students, and can be solved by combining the numbers mentioned in text with basic operations (addition, subtraction, multiplication, division). They are simpler than algebra word problems which require students to identify variables, and form equations with these variables to solve the problem.\nInitial methods to address arithmetic word problems have mostly focussed on subsets of problems, restricting the number or the type of operations used (Roy et al., 2015; Hosseini et al., 2014) but could not deal with\nmulti-step arithmetic problems involving all four basic operations. The template based method of (Kushman et al., 2014), on the other hand, can deal with all types of problems, but implicitly assumes that the solution is generated from a set of predefined equation templates.\nIn this paper, we present a novel approach which can solve a general class of arithmetic problems without predefined equation templates. In particular, it can handle multiple step arithmetic problems as shown in Example 1.\nExample 1 Gwen was organizing her book case making sure each of the shelves had exactly 9 books on it. She has 2 types of books - mystery books and picture books. If she had 3 shelves of mystery books and 5 shelves of picture books, how many books did she have in total?\nThe solution involves understanding that the number of shelves needs to be summed up, and that the total number of shelves needs to be multiplied by the number of books each shelf can hold. In addition, one has to understand that the number “2” is not a direct part of the solution of the problem.\nWhile a solution to these problems eventually requires composing multi-step numeric expressions from text, we believe that directly predicting this complex expression from text is not feasible.\nAt the heart of our technical approach is the novel notion of an Expression Tree. We show that the arithmetic expressions we are interested in can always be represented using an Expression Tree that has some unique decomposition properties. This allows us to decompose the problem of mapping the text to the arithmetic expression to a collection of simple prediction problems, each determining the lowest common ancestor operation between a pair of quantities mentioned in the problem. We then formulate the decision problem of composing the final expression tree as a joint inference problem, via an objective function that consists of all these decomposed prediction problems, along with legitimacy and background knowledge constraints.\nLearning to generate the simpler decomposed expressions allows us to support generalization across problems types. In particular, our system could solve Example 1 even though it has never seen a problem that requires both addition and multiplication operations.\nar X\niv :1\n60 8.\n01 41\n3v 2\n[ cs\n.C L\n] 2\n0 A\nug 2\n01 6\nWe also introduce a second concept, that of quantity schema, that allows us to focus on the information relevant to each quantity mentioned in the text. We show that features extracted from quantity schemas help reasoning effectively about the solution. Moreover, quantity schemas help identify unnecessary text snippets in the problem text. For instance, in Example 2, the information that “Tom washed cars over the weekend” is irrelevant; he could have performed any activity to earn money. In order to solve the problem, we only need to know that he had $76 last week, and now he has $86.\nExample 2 Last week Tom had $74. He washed cars over the weekend and now has $86. How much money did he make from the job?\nWe combine the classifiers’ decisions using a constrained inference framework that allows for incorporating world knowledge as constraints. For example, we deliberatively incorporate the information that, if the problems asks about an “amount”, the answer must be positive, and if the question starts with “how many”, the answer will most likely be an integer.\nOur system is evaluated on two existing datasets of arithmetic word problems, achieving state of the art performance on both. We also create a new dataset of multistep arithmetic problems, and show that our system achieves competitive performance in this challenging evaluation setting.\nThe next section describes the related work in the area of automated math word problem solving. We then present the theory of expression trees and our decomposition strategy that is based on it. Sec. 4 presents the overall computational approach, including the way we use quantity schemas to learn the mapping from text to expression tree components. Finally, we discuss our experimental study and conclude."
    }, {
      "heading" : "2 Related Work",
      "text" : "Previous work in automated arithmetic problem solvers has focussed on a restricted subset of problems. The system described in (Hosseini et al., 2014) handles only addition and subtraction problems, and requires additional annotated data for verb categories. In contrast, our system does not require any additional annotations and can handle a more general category of problems. The approach in (Roy et al., 2015) supports all four basic operations, and uses a pipeline of classifiers to predict different properties of the problem. However, it makes assumptions on the number of quantities mentioned in the problem text, as well as the number of arithmetic steps required to solve the problem. In contrast, our system does not have any such restrictions, effectively handling problems mentioning multiple quantities and requiring multiple steps. Kushman’s approach to automatically solving algebra word problems (Kushman et al., 2014) might be the most re-\nlated to ours. It tries to map numbers from the problem text to predefined equation templates. However, they implicitly assume that similar equation forms have been seen in the training data. In contrast, our system can perform competitively, even when it has never seen similar expressions in training.\nThere is a recent interest in understanding text for the purpose of solving scientific and quantitative problems of various kinds. Our approach is related to work in understanding and solving elementary school standardized tests (Clark, 2015). The system described in (Berant et al., 2014) attempts to automatically answer biology questions, by extracting the structure of biological processes from text. There has also been efforts to solve geometry questions by jointly understanding diagrams and associated text (Seo et al., 2014). A recent work (Sadeghi et al., 2015) tries to answer science questions by visually verifying relations from images.\nOur constrained inference module falls under the general framework of Constrained Conditional Models (CCM) (Chang et al., 2012). In particular, we use the L + I scheme of CCMs, which predicts structured output by independently learning several simple components, combining them at inference time. This has been successfully used to incorporate world knowledge at inference time, as well as getting around the need for large amounts of jointly annotated data for structured prediction (Roth and Yih, 2005; Punyakanok et al., 2005; Punyakanok et al., 2008; Clarke and Lapata, 2006; Barzilay and Lapata, 2006; Roy et al., 2015)."
    }, {
      "heading" : "3 Expression Tree and Problem Decomposition",
      "text" : "We address the problem of automatically solving arithmetic word problems. The input to our system is the problem text P , which mentions n quantities q1, q2, . . . , qn. Our goal is to map this problem to a read-once arithmetic expression E that, when evaluated, provides the problem’s solution. We define a read-once arithmetic expression as one that makes use of each quantity at most once. We say that E is a valid expression, if it is such a Read-Once arithmetic expression, and we only consider in this work problems that can be solved using valid expressions (it’s possible that they can be solved also with invalid expressions).\nAn expression tree T for a valid expression E is a binary tree whose leaves represent quantities, and each internal node represents one of the four basic operations. For a non-leaf node n, we represent the operation associated with it as (n), and its left and right child as lc(n) and rc(n) respectively. The numeric value of the quantity associated with a leaf node n is denoted as Q(n). Each node n also has a value associated with it, represented as VAL(n), which can be computed in a\nrecursive way as follows:\nVAL(n) ={ Q(n) if n is a leaf VAL(lc(n)) (n) VAL(rc(n)) otherwise (1)\nFor any expression tree T for expression E with root node nroot, the value of VAL(nroot) is exactly equal to the numeric value of the expression E. Therefore, this gives a natural representation of numeric expressions, providing a natural parenthesization of the numeric expression. Fig 1 shows an example of an arithmetic problem with solution expression and an expression tree for the solution expression.\nDefinition An expression tree T for a valid expression E is called monotonic if it satisfies the following conditions:\n1. If an addition node is connected to a subtraction node, then the subtraction node is the parent.\n2. If a multiplication node is connected to a division node, then the division node is the parent.\n3. Two subtraction nodes cannot be connected to each other.\n4. Two division nodes cannot be connected to each other.\nFig 2 shows two different expression trees for the same expression. Fig 2b is monotonic whereas fig 2a is not.\nOur decomposition relies on the idea of monotonic expression trees. We try to predict for each pair of quantities qi, qj , the operation at the lowest common ancestor (LCA) node of the monotonic expression tree for the solution expression. We also predict for each quantity, whether it is relevant to the solution. Finally, an inference module combines all these predictions.\nIn the rest of the section, we show that for any pair of quantities qi, qj in the solution expression, any monotonic tree for the solution expression has the same LCA\noperation. Therefore, predicting the LCA operation becomes a multiclass classification problem.\nThe reason that we consider the monotonic representation of the expression tree is that different trees could otherwise give different LCA operation for a given pair of quantities. For example, in Fig 2, the LCA operation for quantities 5 and 8 can be + or−, depending on which tree is considered.\nDefinition We define an addition-subtraction chain of an expression tree to be the maximal connected set of nodes labeled with addition or subtraction.\nThe nodes of an addition-subtraction (AS) chain C represent a set of terms being added or subtracted. These terms are sub-expressions created by subtrees rooted at neighboring nodes of the chain. We call these terms the chain terms of C, and the whole expression, after node operations have been applied to the chain terms, the chain expression of C. For example, in fig 2, the shaded nodes form an addition-subtraction chain. The chain expression is (3×5)+7−8−9, and the chain terms are 3×5, 7, 8 and 9. We define a multiplicationdivision (MD) chain in a similar way.\nTheorem 3.1. Every valid expression can be represented by a monotonic expression tree.\nProof. The proof is procedural, that is, we provide a method to convert any expression tree to a monotonic expression tree for the same expression. Consider a non-monotonic expression tree E, and without loss of generality, assume that the first condition for monotonicity is not valid. Therefore, there exists an addition node ni and a subtraction node nj , and ni is the parent of nj . Consider an addition-subtraction chain C which includes ni, nj . We now replace the nodes of C and its subtrees in the following way. We add a single subtraction node n−. The left subtree of n− has all the addition chain terms connected by addition nodes, and the right subtree of n− has all the subtraction chain terms connected by addition nodes. Both subtrees of n− only require addition nodes, hence monotonicity condition is satisfied. We can construct the monotonic tree in Fig 2b from the non-monotonic tree of Fig 2a using this procedure. The addition chain terms are 3 × 5 and 7, and the subtraction chain terms are 8 and 9. As as was described above, we introduce the root subtraction node in Fig 2b and attach the addition chain terms\nto the left and the subtraction chain terms to the right. The same line of reasoning can be used to handle the second condition with multiplication and division replacing addition and subtraction, respectively.\nTheorem 3.2. Consider two valid expression trees T 1 and T 2 for the same expression E. Let C1, C2 be the chain containing the root nodes of T 1 and T2 respectively. The chain type (addition-subtraction or multiplication-division) as well as the the set of chain terms of C1 and C2 are identical.\nProof. We first prove that the chains containing the roots are both AS or both MD, and then show that the chain terms are also identical.\nWe prove by contradiction that the chain type is same. Let C1’s type be “addition-subtraction” and C2’s type be “multiplication-division” (without loss of generality). Since both C1 and C2 generate the same expression E, we have that E can be represented as sum (or difference) of two expressions as well as product(or division) of two expressions. Transforming a sum (or difference) of expressions to a product (or division) requires taking common terms from the expressions, which imply that the sum (or difference) had duplicate quantities. The opposite transformation adds same term to various expressions leading to multiple uses of the same quantity. Therefore, this will force at least one of C1 and C2 to use the same quantity more than once, violating validity.\nWe now need to show that individual chain terms are also identical. Without loss of generality, let us assume that both C1 and C2 are “addition-subtraction” chains. Suppose the chain terms of C1 and C2 are not identical. The chain expression for both the chains will be the same (since they are root chains, the chain expressions has to be the same as E). Let the chain expression for C1 be ∑ i ti − ∑ i t ′ i, where ti’s are the addition chain terms and t′i are the subtraction chain terms. Similarly, let the chain expression for C2 be ∑ i si − ∑ i s ′ i. We\nknow that ∑ i ti − ∑ i t ′ i = ∑ i si − ∑ i s ′ i, but the set of ti’s and t′i’s is not the same as the set of si and s ′ i’s. However it should be possible to transform one form to the other using mathematical manipulations. This transformation will involve taking common terms, or multiplying two terms, or both. Following previous explanation, this will force one of the expressions to have duplicate quantities, violating validity. Hence, the chain terms of C1 and C2 are identical.\nConsider an expression tree T for a valid expression E. For a distinct pair of quantities qi, qj participating in expression E, we denote by ni, nj the leaves of the expression tree T representing qi, qj , respectively. Let nLCA(qi, qj ; T ) to be the lowest common ancestor node of ni and nj . We also define order(qi, qj ; T ) to be true if ni appears in the left subtree of nLCA(qi, qj ; T ) and nj appears in the right\nsubtree of nLCA(qi, qj ; T ) and set order(qi, qj ; T ) to false otherwise. Finally we define LCA(qi, qj ; T ) for a pair of quantities qi, qj as follows :\nLCA(qi, qj , T ) = + if (nLCA(qi, qj ; T )) = + × if (nLCA(qi, qj ; T )) = × − if (nLCA(qi, qj ; T )) = − and order(qi, qj ; T ) = true −reverse if (nLCA(qi, qj ; T )) = − and order(qi, qj ; T ) = false ÷ if (nLCA(qi, qj ; T )) = ÷ and order(qi, qj ; T ) = true ÷reverse if (nLCA(qi, qj ; T )) = ÷ and\norder(qi, qj ; T ) = false\n(2)\nDefinition Given two expression trees T 1 and T 2 for the same expression E, T 1 is LCA-equivalent to T 2 if for every pair quantities qi, qj in the expression E, we have LCA(qi, qj , T 1) = LCA(qi, qj , T 2). Theorem 3.3. All monotonic expression trees for an expression are LCA-equivalent to each other.\nProof. We prove by induction on the number of quantities used in an expression. For all expressions E with 2 quantities, there exists only one monotonic expression tree, and hence, the statement is trivially true. This satisfies our base case.\nFor the inductive case, we assume that for all expressions with k < n quantities, the theorem is true. Now, we need to prove that any expression with n nodes will also satisfy the property.\nConsider a valid (as in all cases) expression E, with monotonic expression trees T 1 and T 2. From theorem 3.2, we know that the chains containing the roots of T 1 and T 2 have identical type and terms. Given two quantities qi, qj of E, the lowest common ancestor of both T 1 and T 2 will either both belong to the chain containing the root, or both belong to one of the chain terms. If the LCA node is part of the chain for both T 1 and T 2, monotonic property ensures that the LCA operation will be identical. If the LCA node is part of a chain term (which is an expression tree of size less than n), the property is satisfied by induction hypothesis.\nThe theory just presented suggests that it is possible to uniquely decompose the overall problem to simpler steps and this will be exploited in the next section."
    }, {
      "heading" : "4 Mapping Problems to Expression Trees",
      "text" : "Given the uniqueness properties proved in Sec. 3, it is sufficient to identify the operation between any two relevant quantities in the text, in order to determine the unique valid expression. In fact, identifying the operation between any pair of quantities provides much\nneeded redundancy given the uncertainty in identifying the operation from text, and we exploit it in our final joint inference.\nConsequently, our overall method proceeds as follows: given the problem text P , we detect quantities q1, q2, . . . , qn. We then use two classifiers, one for relevance and other to predict the LCA operations for a monotonic expression tree of the solution. Our training makes use of the notion of quantity schemas, which we describe in Section 4.2. The distributional output of these classifiers is then used in a joint inference procedure that determines the final expression tree.\nOur training data consists of problem text paired with a monotonic expression tree for the solution expression and alignment of quantities in the expression to quantity mentions in the problem text. Both the relevance and LCA operation classifiers are trained on gold annotations."
    }, {
      "heading" : "4.1 Global Inference for Expression Trees",
      "text" : "In this subsection, we define the scoring functions corresponding to the decomposed problems, and show how we combine these scores to perform global inference. For a problem P with quantities q1, q2, . . . , qn, we define the following scoring functions:\n1. PAIR(qi, qj , op) : Scores the likelihood of LCA(qi, qj , T ) = op, where T is a monotone expression tree of the solution expression of P . A multiclass classifier trained to predict LCA operations (Section 4.4) can provide these scores.\n2. IRR(q) : Scores the likelihood of quantity q being an irrelevant quantity in P , that is, q is not used in creating the solution. A binary classifier trained to predict whether a quantity q is relevant or not (Section 4.3), can provide these scores.\nFor an expression E, let I(E) be the set of all quantities in P which are not used in expression E. Let T be a monotonic expression tree for E. We define Score(E) of an expression E in terms of the above scoring functions and a scaling parameter wIRR as follows:\nScore(E) =wIRR ∑\nq∈I(E)\nIRR(q)+ (3)\n∑ qi,qj /∈I(E) PAIR(qi, qj , LCA(qi, qj , T ))\nOur final expression tree is an outcome of a constrained optimization process, following (Roth and Yih, 2004; Chang et al., 2012). Our objective function makes use of the scores returned by IRR(·) and PAIR(·) to determine the expression tree and is constrained by legitimacy and background knowledge constraints, detailed below.\n1. Positive Answer: Most arithmetic problems asking for amounts or number of objects usually have a positive number as an answer. Therefore, while\nsearching for the best scoring expression, we reject expressions generating negative answer.\n2. Integral Answer: Problems with questions such as ‘how many” usually expect integral solutions. We only consider integral solutions as legitimate outputs for such problems.\nLet C be the set of valid expressions that can be formed using the quantities in a problem P , and which satisfy the above constraints. The inference algorithm now becomes the following:\nargmax E∈C Score(E) (4)\nThe space of possible expressions is large, and we employ a beam search strategy to find the highest scoring constraint satisfying expression (Chang et al., 2012). We construct an expression tree using a bottom up approach, first enumerating all possible sets of irrelevant quantities, and next over all possible expressions, keeping the top k at each step. We give details below.\n1. Enumerating Irrelevant Quantities: We generate a state for all possible sets of irrelevant quantities, ensuring that there is at least two relevant quantities in each state. We refer to each of the relevant quantities in each state as a term. Therefore, each state can be represented as a set of terms.\n2. Enumerating Expressions: For generating a next state S′ from S, we choose a pair of terms ti and tj in S and one of the four basic operations, and form a new term by combining terms ti and tj with the operation. Since we do not know which of the possible next states will lead to the optimal goal state, we enumerate all possible next states (that is, enumerate all possible pairs of terms and all possible operations); we prune the beam to keep only the top k candidates. We terminate when all the states in the beam have exactly one term.\nOnce we have a top k list of candidate expression trees, we choose the highest scoring tree which satisfies the constraints. However, there might not be any tree in the beam which satisfies the constraints, in which case, we choose the top candidate in the beam. We use k = 200 in our experiments.\nIn order to choose the value for the wIRR, we search over the set {10−6, 10−4, 10−2, 1, 102, 104, 106}, and choose the parameter setting which gives the highest accuracy on the training data."
    }, {
      "heading" : "4.2 Quantity Schema",
      "text" : "In order to generalize across problem types as well as over simple manipulations of the text, it is necessary to train our system only with relevant information from the problem text. E.g., for the problem in example 2, we do not want to take decisions based on how Tom earned money. Therefore, there is a need to extract the relevant information from the problem text.\nTo this end, we introduce the concept of a quantity schema which we extract for each quantity in the problem’s text. Along with the question asked, the quantity schemas provides all the information needed to solve most arithmetic problems.\nA quantity schema for a quantity q in problem P consists of the following components.\n1. Associated Verb For each quantity q, we detect the verb associated with it. We traverse up the dependency tree starting from the quantity mention, and choose the first verb we reach. We used the easy first dependency parser (Goldberg and Elhadad, 2010).\n2. Subject of Associated Verb We detect the noun phrase, which acts as subject of the associated verb (if one exists).\n3. Unit We use a shallow parser to detect the phrase p in which the quantity q is mentioned. All tokens of the phrase (other than the number itself) are considered as unit tokens. Also, if p is followed by the prepositional phrase “of” and a noun phrase (according to the shallow parser annotations), we also consider tokens from this second noun phrase as unit tokens. Finally, if no unit token can be extracted, we assign the unit of the neighboring quantities as the unit of q (following previous work (Hosseini et al., 2014)).\n4. Related Noun Phrases We consider all noun phrases which are connected to the phrase p containing quantity q, with NP-PP-NP attachment. If only one quantity is mentioned in a sentence, we consider all noun phrases in it as related.\n5. Rate We determine whether quantity q refers to a rate in the text, as well as extract two unit components defining the rate. For example, “7 kilometers per hour” has two components “kilometers” and “hour”. Similarly, for sentences describing unit cost like “Each egg costs 2 dollars”, “2” is a rate, with units “dollars” and “egg”.\nIn addition to extracting the quantity schemas for each quantity, we extract the surface form text which poses the question. For example, in the question sentence, “How much will John have to pay if he wants to buy 7 oranges?”, our extractor outputs “How much will John have to pay” as the question."
    }, {
      "heading" : "4.3 Relevance Classifier",
      "text" : "We train a binary SVM classifier to determine, given problem text P and a quantity q in it, whether q is needed in the numeric expression generating the solution. We train on gold annotations and use the score of the classifier as the scoring function IRR(·)."
    }, {
      "heading" : "4.3.1 Features",
      "text" : "The features are extracted from the quantity schemas and can be broadly categorized into three groups:\n1. Unit features: Most questions specifically mention the object whose amount needs to be computed, and hence questions provide valuable clue as to which quantities can be irrelevant. We add a feature for whether the unit of quantity q is present in the question tokens. Also, we add a feature based on whether the units of other quantities have better matches with question tokens (based on the number of tokens matched), and one based on the number of quantities which have the maximum number of matches with the question tokens.\n2. Related NP features: Often units are not enough to differentiate between relevant and irrelevant quantities. Consider the following:\nExample 3 Problem : There are 8 apples in a pile on the desk. Each apple comes in a package of 11. 5 apples are added to the pile. How many apples are there in the pile? Solution : (8 + 5) = 13\nThe relevance decision depends on the noun phrase “the pile”, which is absent in the second sentence. We add a feature indicating whether a related noun phrase is present in the question. Also, we add a feature based on whether the related noun phrases of other quantities have better match with the question. Extraction of related noun phrases is described in Section 4.2.\n3. Miscellaneous Features: When a problem mentions only two quantities, both of them are usually relevant. Hence, we also add a feature based on the number of quantities mentioned in text.\nWe include pairwise conjunction of the above features."
    }, {
      "heading" : "4.4 LCA Operation Classifier",
      "text" : "In order to predict LCA operations, we train a multiclass SVM classifier. Given problem text P and a pair of quantities pi and pj , the classifier predicts one of the six labels described in Eq. 2. We consider the confidence scores for each label supplied by the classifier as the scoring function PAIR(·)."
    }, {
      "heading" : "4.4.1 Features",
      "text" : "We use the following categories of features:\n1. Individual Quantity features: Dependent verbs have been shown to play significant role in solving addition and subtraction problems (Hosseini et al., 2014). Hence, we add the dependent verb of the quantity as a feature. Multiplication and\ndivision problems are largely dependent on rates described in text. To capture that, we add a feature based on whether the quantity is a rate, and whether any component of rate unit is present in the question. In addition to these quantity schema features, we add selected tokens from the neighborhood of the quantity mention. Neighborhood of quantities are often highly informative of LCA operations, for example, “He got 80 more marbles”, the term “more” usually indicates addition. We add as features adverbs and comparative adjectives mentioned in a window of size 5 around the quantity mention.\n2. Quantity Pair features: For a pair (qi, qj) we add features to indicate whether they have the same dependent verbs, to indicate whether both dependent verbs refer to the same verb mention, whether the units of qi and qj are the same and, if one of them is a rate, which component of the unit matches with the other quantity’s unit. Finally, we add a feature indicating whether the value of qi is greater than the value of qj .\n3. Question Features: Finally, we add a few features based on the question asked. In particular, for arithmetic problems where only one operation is needed, the question contains signals for the required operation. Specifically, we add indicator features based on whether the question mentions comparison-related tokens (e.g., “more”, “less” or “than”), or whether the question asks for a rate (indicated by tokens such as “each” or “one”).\nWe include pairwise conjunction of the above features. For both classifiers, we use the Illinois-SL package 1 under default settings."
    }, {
      "heading" : "5 Experimental Results",
      "text" : "In this section, we evaluate the proposed method on publicly available datasets of arithmetic word problems. We evaluate separately the relevance and LCA operation classifiers, and show the contribution of various features. Lastly, we evaluate the performance of the full system, and quantify the gains achieved by the constraints."
    }, {
      "heading" : "5.1 Datasets",
      "text" : "We evaluate our system on three datasets, each of which comprise a different category of arithmetic word problems.\n1. AI2 Dataset: This is a collection of 395 addition and subtraction problems, released by (Hosseini et al., 2014). They performed a 3-fold cross validation, with every fold containing problems from\n1 http://cogcomp.cs.illinois.edu/page/software view/IllinoisSL\ndifferent sources. This helped them evaluate robustness to domain diversity. We follow the same evaluation setting.\n2. IL Dataset: This is a collection of arithmetic problems released by (Roy et al., 2015). Each of these problems can be solved by performing one operation. However, there are multiple problems having the same template. To counter this, we perform a few modifications to the dataset. First, for each problem, we replace the numbers and nouns with the part of speech tags, and then we cluster the problems based on unigrams and bigrams from this modified problem text. In particular, we cluster problems together whose unigram-bigram similarity is over 90%. We next prune each cluster to keep at most 5 problems in each cluster. Finally we create the folds ensuring all problems in a cluster are assigned to the same fold, and each fold has similar distribution of all operations. We have a final set of 562 problems, and we use a 5-fold cross validation to evaluate on this dataset.\n3. Commoncore Dataset: In order to test our system’s ability to handle multi-step problems, we create a new dataset of multi-step arithmetic problems. The problems were extracted from www.commoncoresheets.com. In total, there were 600 problems, 100 for each of the following types:\n(a) Addition followed by Subtraction (b) Subtraction followed by Addition (c) Addition and Multiplication (d) Addition and Division (e) Subtraction and Multiplication (f) Subtraction and Division\nThis dataset had no irrelevant quantities. Therefore, we did not use the relevance classifier in our evaluations.\nIn order to test our system’s ability to generalize across problem types, we perform a 6-fold cross validation, with each fold containing all the problems from one of the aforementioned categories. This is a more challenging setting relative to the individual data sets mentioned above, since we are evaluating on multi-step problems, without ever looking at problems which require the same set of operations."
    }, {
      "heading" : "5.2 Relevance Classifier",
      "text" : "Table 2 evaluates the performance of the relevance classifier on the AI2 and IL datasets. We report two accuracy values: Relax - fraction of quantities which the classifier got correct, and Strict - fraction of math problems, for which all quantities were correctly classified. We report accuracy using all features and then removing each feature group, one at a time.\nWe see that features related to units of quantities play the most significant role in determining relevance of quantities. Also, the related NP features are not helpful for the AI2 dataset."
    }, {
      "heading" : "5.3 LCA Operation Classifier",
      "text" : "Table 1 evaluates the performance of the LCA Operation classifier on the AI2, IL and CC datasets. As before, we report two accuracies - Relax - fraction of quantity pairs for which the classifier correctly predicted the LCA operation, and Strict - fraction of math problems, for which all quantity pairs were correctly classified. We report accuracy using all features and then removing each feature group, one at a time.\nThe strict and relaxed accuracies for IL dataset are identical, since each problem in IL dataset only requires one operation. The features related to individual quantities are most significant; in particular, the accuracy goes to 0.0 in the CC dataset, without using individual quantity features. The question features are not helpful for classification in the CC dataset. This can be attributed to the fact that all problems in CC dataset require multiple operations, and questions in multi-step problems usually do not contain information for each of the required operations."
    }, {
      "heading" : "5.4 Global Inference Module",
      "text" : "Table 3 shows the performance of our system in correctly solving arithmetic word problems. We show the impact of various contraints, and also compare against previously best known results on the AI2 and IL datasets. We also show results using each of the two constraints separately, and using no constraints at all.\nThe previously known best result in the AI2 dataset is reported in (Hosseini et al., 2014). Since we follow the exact same evaluation settings, our results are directly comparable. We achieve state of the art results, without having access to any additional annotated data, unlike (Hosseini et al., 2014), who use labeled data for verb categorization. For the IL dataset, we acquired the system of (Roy et al., 2015) from the authors, and ran it with the same fold information. We outperform their system by an absolute gain of over 20%. We believe that the improvement was mainly due to the dependence of the system of (Roy et al., 2015) on lexical and neighborhood of quantity features. In contrast, features from quantity schemas help us generalize across problem types. Finally, we also compare against the template based system of (Kushman et al., 2014). (Hosseini et al., 2014) mentions the result of running the system of (Kushman et al., 2014) on AI2 dataset, and we report their result here. For IL and CC datasets, we used the system released by (Kushman et al., 2014).\nThe integrality constraint is particularly helpful when division is involved, since it can lead to fractional answers. It does not help in case of the AI2 dataset, which involves only addition and subtraction problems. The role of the constraints becomes more significant in case of multi-step problems and, in particular, they contribute an absolute improvement of over 15% over the system without constraints on the CC dataset. The template based system of (Kushman et al., 2014) performs on par with our system on the IL dataset. We believe that it is due to the small number of equation templates in the IL dataset. It performs poorly on the CC dataset, since we evaluate on unseen problem types, which do not ensure that equation templates in the test data will be seen in the training data."
    }, {
      "heading" : "5.5 Discussion",
      "text" : "The leading source of errors for the classifiers are erroneous quantity schema extraction and lack of understanding of unknown or rare verbs. For the relevance classifier on the AI2 dataset, 25% of the errors were due to mistakes in extracting the quantity schemas and 20% could be attributed to rare verbs. For the LCA operation classifier on the same dataset, 16% of the errors were due to unknown verbs and 15% were due to mistakes in extracting the schemas. The erroneous extraction of accurate quantity schemas is very significant for the IL dataset, contributing 57% of the errors for the relevance classifier and 39% of the errors for the LCA operation classifier. For the operation classifier on the CC dataset, 8% of the errors were due to verbs and 16% were due to faulty quantity schema extraction. Quantity Schema extraction is challenging due to parsing issues as well as some non-standard rate patterns, and it will be one of the future work targets. For example, in the sentence, “How many 4-dollar toys can he buy?”, we fail to extract the rate component of the quantity 4."
    }, {
      "heading" : "6 Conclusion",
      "text" : "This paper presents a novel method for understanding and solving a general class of arithmetic word problems. Our approach can solve all problems whose solution can be expressed by a read-once arithmetic expression, where each quantity from the problem text appears at most once in the expression. We develop a novel theoretical framework, centered around the notion of monotone expression trees, and showed how this representation can be used to get a unique decomposition of the problem. This theory naturally leads to a computational solution that we have shown to uniquely determine the solution - determine the arithmetic operation between any two quantities identified in the text. This theory underlies our algorithmic solution - we develop classifiers and a constrained inference approach that exploits redundancy in the information, and show that this yields strong performance on several benchmark collections. In particular, our approach achieves state of the art performance on two publicly available arithmetic problem datasets and can support natural generalizations. Specifically, our approach performs competitively on multistep problems, even when it has never observed the particular problem type before.\nAlthough we develop and use the notion of expression trees in the context of numerical expressions, the concept is more general. In particular, if we allow leaves of expression trees to represent variables, we can express algebraic expressions and equations in this framework. Hence a similar approach can be targeted towards algebra word problems, a direction we wish to investigate in the future.\nThe datasets used in the paper are available for download at http://cogcomp.cs.illinois.edu/page/resource view/98."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This research was sponsored by DARPA (under agreement number FA8750-13-2-0008), and a grant from AI2. Any opinions, findings, conclusions or recommendations are those of the authors and do not necessarily reflect the view of the agencies."
    } ],
    "references" : [ {
      "title" : "Aggregation via Set Partitioning for Natural Language Generation. In Human Language Technologies - North American Chapter of the Association for Computational Linguistics, June",
      "author" : [ "Barzilay", "Lapata2006] R. Barzilay", "M. Lapata" ],
      "venue" : null,
      "citeRegEx" : "Barzilay et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Barzilay et al\\.",
      "year" : 2006
    }, {
      "title" : "Modeling biological processes for reading comprehension",
      "author" : [ "Berant et al.2014] J. Berant", "V. Srikumar", "P. Chen", "A.V. Linden", "B. Harding", "B. Huang", "P. Clark", "C.D. Manning" ],
      "venue" : "Proceedings of EMNLP",
      "citeRegEx" : "Berant et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Berant et al\\.",
      "year" : 2014
    }, {
      "title" : "Structured learning with constrained conditional models",
      "author" : [ "Chang et al.2012] M. Chang", "L. Ratinov", "D. Roth" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Chang et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Chang et al\\.",
      "year" : 2012
    }, {
      "title" : "Elementary School Science and Math Tests as a Driver for AI: Take the Aristo Challenge",
      "author" : [ "P. Clark" ],
      "venue" : "In Proceedings of IAAI",
      "citeRegEx" : "Clark.,? \\Q2015\\E",
      "shortCiteRegEx" : "Clark.",
      "year" : 2015
    }, {
      "title" : "Constraint-based sentence compression: An integer programming approach",
      "author" : [ "Clarke", "Lapata2006] J. Clarke", "M. Lapata" ],
      "venue" : "In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),",
      "citeRegEx" : "Clarke et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Clarke et al\\.",
      "year" : 2006
    }, {
      "title" : "An efficient algorithm for easy-first non-directional dependency parsing",
      "author" : [ "Goldberg", "Elhadad2010] Y. Goldberg", "M. Elhadad" ],
      "venue" : "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association",
      "citeRegEx" : "Goldberg et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Goldberg et al\\.",
      "year" : 2010
    }, {
      "title" : "Learning to solve arithmetic word problems with verb categorization",
      "author" : [ "H. Hajishirzi", "O. Etzioni", "N. Kushman" ],
      "venue" : "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Hosseini et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hosseini et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning to automatically solve algebra word problems",
      "author" : [ "Kushman et al.2014] N. Kushman", "L. Zettlemoyer", "R. Barzilay", "Y. Artzi" ],
      "venue" : "In ACL,",
      "citeRegEx" : "Kushman et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kushman et al\\.",
      "year" : 2014
    }, {
      "title" : "The necessity of syntactic parsing for semantic role labeling",
      "author" : [ "D. Roth", "W. Yih" ],
      "venue" : "In Proc. of the International Joint Conference on Artificial Intelligence (IJCAI),",
      "citeRegEx" : "Punyakanok et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Punyakanok et al\\.",
      "year" : 2005
    }, {
      "title" : "The importance of syntactic parsing and inference in semantic role labeling",
      "author" : [ "D. Roth", "W. Yih" ],
      "venue" : "Computational Linguistics,",
      "citeRegEx" : "Punyakanok et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Punyakanok et al\\.",
      "year" : 2008
    }, {
      "title" : "A linear programming formulation for global inference in natural language tasks",
      "author" : [ "Roth", "Yih2004] D. Roth", "W. Yih" ],
      "venue" : "In Hwee Tou Ng and Ellen Riloff, editors, Proc. of the Conference on Computational Natural Language Learning (CoNLL),",
      "citeRegEx" : "Roth et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Roth et al\\.",
      "year" : 2004
    }, {
      "title" : "Integer linear programming inference for conditional random fields",
      "author" : [ "Roth", "Yih2005] D. Roth", "W. Yih" ],
      "venue" : "In Proc. of the International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Roth et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Roth et al\\.",
      "year" : 2005
    }, {
      "title" : "Reasoning about quantities in natural language",
      "author" : [ "Roy et al.2015] S. Roy", "T. Vieira", "D. Roth" ],
      "venue" : "Transactions of the Association for Computational Linguistics,",
      "citeRegEx" : "Roy et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Roy et al\\.",
      "year" : 2015
    }, {
      "title" : "Viske: Visual knowledge extraction and question answering by visual verification of relation phrases",
      "author" : [ "Sadeghi et al.2015] F. Sadeghi", "S.K. Divvala", "A. Farhadi" ],
      "venue" : "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June",
      "citeRegEx" : "Sadeghi et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sadeghi et al\\.",
      "year" : 2015
    }, {
      "title" : "Diagram understanding in geometry questions",
      "author" : [ "Seo et al.2014] M.J. Seo", "H. Hajishirzi", "A. Farhadi", "O. Etzioni" ],
      "venue" : "In Proceedings of the TwentyEighth AAAI Conference on Artificial Intelligence, July",
      "citeRegEx" : "Seo et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Seo et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "Initial methods to address arithmetic word problems have mostly focussed on subsets of problems, restricting the number or the type of operations used (Roy et al., 2015; Hosseini et al., 2014) but could not deal with multi-step arithmetic problems involving all four basic operations.",
      "startOffset" : 151,
      "endOffset" : 192
    }, {
      "referenceID" : 6,
      "context" : "Initial methods to address arithmetic word problems have mostly focussed on subsets of problems, restricting the number or the type of operations used (Roy et al., 2015; Hosseini et al., 2014) but could not deal with multi-step arithmetic problems involving all four basic operations.",
      "startOffset" : 151,
      "endOffset" : 192
    }, {
      "referenceID" : 7,
      "context" : "The template based method of (Kushman et al., 2014), on the other hand, can deal with all types of problems, but implicitly assumes that the solution is generated from a set of predefined equation templates.",
      "startOffset" : 29,
      "endOffset" : 51
    }, {
      "referenceID" : 6,
      "context" : "The system described in (Hosseini et al., 2014) handles only addition and subtraction problems, and requires additional annotated data for verb categories.",
      "startOffset" : 24,
      "endOffset" : 47
    }, {
      "referenceID" : 12,
      "context" : "The approach in (Roy et al., 2015) supports all four basic operations, and uses a pipeline of classifiers to predict different properties of the problem.",
      "startOffset" : 16,
      "endOffset" : 34
    }, {
      "referenceID" : 7,
      "context" : "Kushman’s approach to automatically solving algebra word problems (Kushman et al., 2014) might be the most related to ours.",
      "startOffset" : 66,
      "endOffset" : 88
    }, {
      "referenceID" : 3,
      "context" : "Our approach is related to work in understanding and solving elementary school standardized tests (Clark, 2015).",
      "startOffset" : 98,
      "endOffset" : 111
    }, {
      "referenceID" : 1,
      "context" : "The system described in (Berant et al., 2014) attempts to automatically answer biology questions, by extracting the structure of biological processes from text.",
      "startOffset" : 24,
      "endOffset" : 45
    }, {
      "referenceID" : 14,
      "context" : "There has also been efforts to solve geometry questions by jointly understanding diagrams and associated text (Seo et al., 2014).",
      "startOffset" : 110,
      "endOffset" : 128
    }, {
      "referenceID" : 13,
      "context" : "A recent work (Sadeghi et al., 2015) tries to answer science questions by visually verifying relations from images.",
      "startOffset" : 14,
      "endOffset" : 36
    }, {
      "referenceID" : 2,
      "context" : "Our constrained inference module falls under the general framework of Constrained Conditional Models (CCM) (Chang et al., 2012).",
      "startOffset" : 107,
      "endOffset" : 127
    }, {
      "referenceID" : 8,
      "context" : "This has been successfully used to incorporate world knowledge at inference time, as well as getting around the need for large amounts of jointly annotated data for structured prediction (Roth and Yih, 2005; Punyakanok et al., 2005; Punyakanok et al., 2008; Clarke and Lapata, 2006; Barzilay and Lapata, 2006; Roy et al., 2015).",
      "startOffset" : 187,
      "endOffset" : 327
    }, {
      "referenceID" : 9,
      "context" : "This has been successfully used to incorporate world knowledge at inference time, as well as getting around the need for large amounts of jointly annotated data for structured prediction (Roth and Yih, 2005; Punyakanok et al., 2005; Punyakanok et al., 2008; Clarke and Lapata, 2006; Barzilay and Lapata, 2006; Roy et al., 2015).",
      "startOffset" : 187,
      "endOffset" : 327
    }, {
      "referenceID" : 12,
      "context" : "This has been successfully used to incorporate world knowledge at inference time, as well as getting around the need for large amounts of jointly annotated data for structured prediction (Roth and Yih, 2005; Punyakanok et al., 2005; Punyakanok et al., 2008; Clarke and Lapata, 2006; Barzilay and Lapata, 2006; Roy et al., 2015).",
      "startOffset" : 187,
      "endOffset" : 327
    }, {
      "referenceID" : 2,
      "context" : "Our final expression tree is an outcome of a constrained optimization process, following (Roth and Yih, 2004; Chang et al., 2012).",
      "startOffset" : 89,
      "endOffset" : 129
    }, {
      "referenceID" : 2,
      "context" : "The space of possible expressions is large, and we employ a beam search strategy to find the highest scoring constraint satisfying expression (Chang et al., 2012).",
      "startOffset" : 142,
      "endOffset" : 162
    }, {
      "referenceID" : 6,
      "context" : "Finally, if no unit token can be extracted, we assign the unit of the neighboring quantities as the unit of q (following previous work (Hosseini et al., 2014)).",
      "startOffset" : 135,
      "endOffset" : 158
    }, {
      "referenceID" : 6,
      "context" : "Individual Quantity features: Dependent verbs have been shown to play significant role in solving addition and subtraction problems (Hosseini et al., 2014).",
      "startOffset" : 132,
      "endOffset" : 155
    }, {
      "referenceID" : 6,
      "context" : "AI2 Dataset: This is a collection of 395 addition and subtraction problems, released by (Hosseini et al., 2014).",
      "startOffset" : 88,
      "endOffset" : 111
    }, {
      "referenceID" : 12,
      "context" : "IL Dataset: This is a collection of arithmetic problems released by (Roy et al., 2015).",
      "startOffset" : 68,
      "endOffset" : 86
    }, {
      "referenceID" : 6,
      "context" : "(Hosseini et al., 2014) 77.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 12,
      "context" : "(Roy et al., 2015) - 52.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 7,
      "context" : "(Kushman et al., 2014) 64.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 6,
      "context" : "The previously known best result in the AI2 dataset is reported in (Hosseini et al., 2014).",
      "startOffset" : 67,
      "endOffset" : 90
    }, {
      "referenceID" : 6,
      "context" : "We achieve state of the art results, without having access to any additional annotated data, unlike (Hosseini et al., 2014), who use labeled data for verb categorization.",
      "startOffset" : 100,
      "endOffset" : 123
    }, {
      "referenceID" : 12,
      "context" : "For the IL dataset, we acquired the system of (Roy et al., 2015) from the authors, and ran it with the same fold information.",
      "startOffset" : 46,
      "endOffset" : 64
    }, {
      "referenceID" : 12,
      "context" : "We believe that the improvement was mainly due to the dependence of the system of (Roy et al., 2015) on lexical and neighborhood of quantity features.",
      "startOffset" : 82,
      "endOffset" : 100
    }, {
      "referenceID" : 7,
      "context" : "Finally, we also compare against the template based system of (Kushman et al., 2014).",
      "startOffset" : 62,
      "endOffset" : 84
    }, {
      "referenceID" : 6,
      "context" : "(Hosseini et al., 2014) mentions the result of running the system of (Kushman et al.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 7,
      "context" : ", 2014) mentions the result of running the system of (Kushman et al., 2014) on AI2 dataset, and we report their result here.",
      "startOffset" : 53,
      "endOffset" : 75
    }, {
      "referenceID" : 7,
      "context" : "For IL and CC datasets, we used the system released by (Kushman et al., 2014).",
      "startOffset" : 55,
      "endOffset" : 77
    }, {
      "referenceID" : 7,
      "context" : "The template based system of (Kushman et al., 2014) performs on par with our system on the IL dataset.",
      "startOffset" : 29,
      "endOffset" : 51
    } ],
    "year" : 2016,
    "abstractText" : "This paper presents a novel approach to automatically solving arithmetic word problems. This is the first algorithmic approach that can handle arithmetic problems with multiple steps and operations, without depending on additional annotations or predefined templates. We develop a theory for expression trees that can be used to represent and evaluate the target arithmetic expressions; we use it to uniquely decompose the target arithmetic problem to multiple classification problems; we then compose an expression tree, combining these with world knowledge through a constrained inference framework. Our classifiers gain from the use of quantity schemas that supports better extraction of features. Experimental results show that our method outperforms existing systems, achieving state of the art performance on benchmark datasets of arithmetic word problems.",
    "creator" : "LaTeX with hyperref package"
  }
}