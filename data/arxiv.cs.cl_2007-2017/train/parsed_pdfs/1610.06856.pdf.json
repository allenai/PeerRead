{
  "name" : "1610.06856.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Automated Big Text Security Classification",
    "authors" : [ "Khudran Alzhrani", "Ethan M. Rudd", "Terrance E. Boult", "Edward Chow" ],
    "emails" : [ "kalzhran@uccs.edu", "cchow@uccs.edu", "erudd@vast.uccs.edu", "tboult@vast.uccs.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "I. INTRODUCTION\nDrawing a line between security and convenience that separates users’ rights from the misuse of data has never been without complication. A common industrial and academic approach is to quantify and limit the amount of sensitive information revealed to insider threats – i.e., legitimate but potentially malicious users[1], [2]. However, this approach does not scale well with information explosion, which makes the information leak prevention problem considerably more difficult. Data sizes are expanding exceptionally every year. Gantz et al. [3] stated that more than 2.8 trillion gigabytes of data were generated in 2012 alone. Undoubtedly, among the vast amount of newly generated data, some sensitive information will simply go unlabeled, and a malicious or unaware user might leak unlabeled, sensitive information without being noticed. Concerning unstructured textual data, documents may or may not consist of a mixture of sensitive and non-sensitive contents. Thus some form of autonomous paragraph-based security labeling is needed to close the gap between security requirements and data availability.\nSensitive information leakage occurs when data passes from trusted to untrusted channels. Leading cybersecurity vendors\nhave started to adopt DLP [4] [5] to counter the problem via a detection model that differentiates sensitive information from non-sensitive information. Automated data sensitivity detection empowers DLP with the ability to monitor users’ actions toward only particular relevant portions of sensitive data rather than tracking all data at all times. However, many current DLP detection models are incapable of capturing unlabeled sensitive texts. This raises the following question: How can we detect substantial amounts of unlabeled sensitive texts anywhere within a network without relying on users? Our approach in this paper addresses the issue by partitioning a large text corpus into smaller groups of similar paragraphs wherein multiple similarity-based classification models can be built to predict a paragraph’s security label. The challenge that sensitive text detection presents is that even a few words can possess a great deal of value.\nThe main contribution of our research is a paragraph-level content driven data leak detection method which we evaluate on genuine sensitive data. The datasets that we constructed for our evaluation are derived from U.S. diplomatic cables made public by the WikiLeaks organization. These datasets, which we collectively refer to as the WikiLeaks Dataset, contain both sensitive and non-sensitive data along with paragraphlevel annotations. We are not aware of any other paper that has experimented on WikiLeaks for both sensitive and nonsensitive data. Unlike most prior research, which has treated automated text security detection as a binary classification problem, our approach more realistically performs classification across multiple security levels. The paragraph-based classification approach that we employ in this paper is also more practical than document-based classification, especially for large documents and can be trivially converted to documentbased classification. Finally, note that our ACESS detection model is built with integration into DLP systems in mind. The remainder of this paper is structured as follows: related work and an overview of DLP are presented in Sec. II and Sec. III. Sec. IV details our ACESS approach. Sec. V describes the WikiLeaks dataset. The results of our evaluations are presented in Sec. VI. Conclusions and future work are discussed in Sec. VIII."
    }, {
      "heading" : "II. RELATED WORK",
      "text" : "While the use of statistical analysis to detect sensitive text is relatively recent, several techniques have been proposed. Katz et al. [6] employed the k-means algorithm on cosine\n- 1 -\nar X\niv :1\n61 0.\n06 85\n6v 1\n[ cs\n.C R\n] 2\n1 O\nct 2\n01 6\nsimilarity to cluster all the documents in a corpus regardless of their sensitivity level. They then assigned a confidentiality score to each document by calculating the confidential term probability. Alneyadi et al. [7] used L1-norms [8] between ngram category profiles, assigning the document to the category of shortest distance. Hart et al. [9] proposed a new training method to overcome the problem of imbalanced data by implementing class-specific classifiers. Gomez-Hidalgoy et al. [10] proposed the usage of named entity recognition to detect “sensitive” tweets.\nIn each of the aforementioned works, however, artificial “sensitive documents” were constructed from publicly available sources like articles, tweets, and forums for performance evaluations. None of these works evaluated performance with actual sensitive information. Therefore, how well performance on the evaluations in these works maps to performance on the actual problem is questionable.\nWhile we designed our ACESS system with DLP in mind, sensitive text classification has other applications too. An obvious forensic extension is to assist in monitoring and tracking down suspicious users post mortem. The benefit of logging users’ activities on classified texts is extended when integrated with Security Information and Event Management (SIEM) solutions. SIEM collects security related logs to analyze and correlate between security events generated from different sources, such as firewalls and IDS [11]. Additionally, text sensitivity classification can offer support in understanding insiders’ behavior by emulating sensitive documents in honeypots [12]. By replacing sensitive documents with similar non-sensitive documents, a monitoring agent could collect and analyze user behavior."
    }, {
      "heading" : "III. A PROTOTYPICAL DLP FRAMEWORK",
      "text" : "DLP mitigates the threat of sensitive data leakage from insiders by discovering, detecting, protecting, and monitoring sensitive information assets residing on both network and host components (see Fig. 1). During the discovery phase, unlabeled data at rest is located by remotely scanning the targeted device with a scanning agent [13]. This unlabeled data is passed to DLP’s detection module for sensitivity assessment. Data is then labeled according to its detected sensitivity level. Depending on the detected sensitivity level, data is either transmitted to untrusted channels or passed to the protection module. The protection can protect sensitive data in a variety of different ways and may deal differently with data in transit and data at rest. For instance, sensitive data in transit might be encrypted, a security officer might be alerted, or the transmission might be halted. For data at rest, on the other hand, the DLP’s protection module might perform encryption and change access rights. Finally, the monitoring module ensures labeling consistency and tracks users’ actions on the sensitive data within the network. While commercial DLP solutions are not yet standardized, they typically consist of the four modules illustrated in Fig. 1. Our work in this paper focuses on extending detection modules by assigning sensitivity labels to textual data. In contrast to hard-signature detection approaches\nwhich rely on keywords, regular expressions, exact matching and partial matching [13], our approach seeks to infer a more general statistical model to better accommodate previously unseen unstructured texts."
    }, {
      "heading" : "IV. LEARNING THE SECRETS",
      "text" : "A schematic overview of our ACESS system is presented in Fig. 2. ACESS is a paragraph-based security detection model. Unlike document-based classification, the classifier is built and evaluated on paragraphs independently, regardless of their documents’ origins. Research (e.g., [14]) suggests that for a large text corpus such as the WikiLeaks dataset, it can become difficult to differentiate actual security features from spurious similarities brought about by large and diverse paragraphs. We hypothesize that a solution to this problem is to partition the dataset into multiple smaller sets of similar paragraphs and build a classifier for each set. This approach also has the advantage of producing smaller sets of features to examine for each classifier, allowing for a more principled selection of security features. One way to do this is to group similar paragraphs together using clustering algorithms. Each similarity cluster and its associated security feature set can then be used to build a Similarity-Based Classification Model, wherein each similarity cluster is treated as a small dataset extracted from the original, large dataset.\nThe optimal number of clusters is indeterminate a priori and impacts the final detection rate. One way to determine this number is through cross-validation on the training set, evaluating the number of clusters across a broad range. The Group of Similarity Clusters is a representation of the dataset. Dataset representations, or Group of Similarity Clusters, are independent of one another. Classification Model Group is the set of similarity-based classification models generated from the same Group of Similarity Clusters. Finally, the Optimal Classification Model Group is the Classification Model Group that achieves the highest accumulative F-Measure. Another approach would be to use a held-out validation set.\nA. Generation of Collection of Groups of Similarity Clusters\nACESS’s approach to generating the group of similarity clusters is expressed algorithmically in Alg.1, and can be described in three steps:\n1) Preprocessing: During preprocessing, tokenization is first applied, using white space as a delimiter. Punctuation and special characters are then removed, and all characters are converted to lowercase. Finally, stop-words are deleted.\n2) Indexing and Defining Similarity Features: Tokens are vectorized and transformed to normalized term frequencyinverse document frequency (TF-IDF) values. By thresholding TF-IDF values and running validation a selected subset of the TF-IDF features can be used to define a similarity space for clustering paragraphs.\n3) Group of Similarity Clusters: The number of produced clusters varies depending on the size of the dataset. In this approach, we assumed that there is one cluster for every hundred paragraphs. Several algorithms can be used for clustering, but ACESS uses the k-means algorithm [15]. The number of clusters (k) is selected via validation and based on the criterion that there be at least two security classes in each cluster. If the latter condition is not met, the number of clusters is decremented until all clusters meet this condition. A collection of groups of similarity clusters is generated with the same set\nof similarity features. The entire generation process is outlined in Algorithm 1.\nAlgorithm 1: Generation of Collection of Groups of Similarity Clusters\n1 Preprocessing Function() 3 Input : Labeled Paragraphs 5 for each paragraph do 6 Tokenize Paragraphs; 8 Remove Special Characters;\n10 Remove Stop-Words; 12 Convert letters to lower case; 13 end 14 Vector f ← All unique features; 15 Transform f to Normalized TF-IDF values; 16 k ← Maximum allowed # of similarity clusters; 17 p← 0.6; 18 Vector v ← p× f (Features); 19 K-Means Algorithm() 21 Input : (k, v, Processed paragraphs) 23 Output: k similarity clusters 24 if each cluster in k has at least 2 classes then 25 repeat 26 Generate k − 1 similarity clusters 27 until k < 2; 28 else 29 if p>0.01 then 30 p = p− 0.1; 31 goto 18 32 else 33 k = k − 1; 34 goto 17 35 end 36 end\nB. Similarity-Based Classification Models Fig. 3 describes the overall process required to construct similarity-based classification models from similarity clusters. Each cluster within the group is used to train its own separate classification model. Instances are preserved in their original form without any preprocessing. The following is performed on every group of similarity clusters to determine the best set of models. As in any text classification, a preprocessing identical to the one illustrated in subsection IV-A1 is first applied to the paragraphs.\n1) Indexing and Security Features Selection: Tokens generated in the preprocessing procedure are vectorized and weighted with non-normalized frequency values. Through exhaustive experimenting, we found that TF-IDF and normalization do not perform as well as term frequencies. The most effective threshold of selected features varies among similarity clusters; thus, determining a decisive model’s security feature threshold requires an extensive search. We used a correlation feature selection algorithm from the Weka toolkit [16] to arrive at a distinct feature space representation per cluster, ranking each feature’s value in terms of Pearson correlation with respect to the targeted class [17]. It should be noted that features are extracted from within a similarity cluster. Similarity features will not hold much weight, since they are distributed across all the security classes. This will assist in the selection of a new set of features with stronger relation to the classes.\n2) Training Classifiers and Selecting Optimal Classification Model Group: Models are evaluated through a 10 fold crossvalidation technique to assess the prediction model performance. Based on the selected set of security features, various\nsimilarity-based classification models can be built from a single similarity cluster. A similarity-based classification model that achieves the highest F-Measure score is chosen as the predictive model. However, there are multiple similarity-based classification models in the classification model group. Some of those models might result in high F-measure, while others do not perform as well. Therefore, true and false positive and negative values of all the similarity-based classification models within a group were summed to calculate the overall classification model group F-Measure. The similarity-based classification model group with the highest F-measure for all security classes was then selected as the optimal classification model group."
    }, {
      "heading" : "V. THE WIKILEAKS DATASET",
      "text" : "WikiLeaks is an organization which collects and publicizes leaked sensitive documents. The organization gained particular notoriety when it published sensitive U.S. diplomatic cables dated between 2003 and February 2010, the largest collection of leaked sensitive documents distributed by the organization to date. These cables originated from U.S. Embassies and Consulates from around the globe, and are stored on the WikiLeaks web page in static HTML format, mixed and sorted by their dates of creation. By reorganizing these documents based on their geographic locations, we were able to create several distinct datasets of related cables – one per embassy. We then stripped the HTML tags, and categorized each of the paragraphs in terms of three fields: a UID consisting of sender, receiver, and time stamp; the raw text itself; and the classification label. We then divided classifications into three categories: Unclassified, Confidential, and Secret. Due to the sparsity of meta-labels, e.g., C/FORN and C/NOFORN corresponding to whether confidential information should or should not be shown to foreign nationals, we merged the\ncorresponding entries into more general classification categories, e.g., Confidential. In total, we created datasets from the cables of four embassies: Baghdad, London, Berlin, and Damascus. We collectively refer to these datasets as the WikiLeaks Dataset. Statistics for each of the collected datasets in terms of document and paragraph classifications are shown in Tables I and II. Since classified documents may contain paragraphs labeled across multiple security levels, we classify each document by the highest security label across all paragraphs.\nNote that the number of Unclassified or Confidential instances in tables I and II is always greater than the number of Secret instances; often by an order of magnitude. Whether this is an artifact of the existence of fewer Secret documents in the wild or whether Secret documents are better protected and thus harder to leak is an important question to consider when using the WikiLeaks Dataset to evaluate a DLP system."
    }, {
      "heading" : "VI. EXPERIMENTAL EVALUATION",
      "text" : "In this section, we evaluate our proposed methodology on the datasets described in Sec. V. For ease of repetition and implementation, we adopt the widely used practice of extracting TF-IDF features across each entire dataset. In an actual deployed system, TF-IDF features would be extracted from the training set and possibly background datasets for language priors, e.g., large volumes of text representing common language usage. Our evaluation, however serves as a feasibility analysis.\nA. Evaluation Metrics We assume that all security classes – Unclassified, Confidential, and Secret – are equally important, which means that raw accuracy numbers alone do not provide an adequate measure due to dataset biases (cf. Tables I and II). Rather, we turn to precision, recall, and F1-measure per-class. Where TP , FP , and FN are the numbers of true positives, false positives, and false negatives respectively, these measures are defined as\nPrecision = TPTP+FP Recall = TP TP+FN\nF1−Measure = 2×Precision×RecallPrecision+Recall .\nB. Baseline\nSince no previous evaluations have been conducted on the WikiLeaks dataset, we established baseline classification results by testing the multinomial Naive Bayes algorithm and the multi-class 1-vs-1 linear SVM algorithm in a stratified 10-fold cross-validation. Across each fold, we conducted a cross-validated parameter grid search across the training partition, and found that the linear SVM noticeably outperformed Naive Bayes. We also tried several non-linear kernel SVMs, including polynomial and RBF, but found that the respective F1-Measures were at best statistically comparable to the linear SVM [18] – a finding which we attribute to the sparsity of the training set and the already high dimensionality of the unkernelized feature space.\nThe feature space used for the baseline classifier was obtained by running a correlation algorithm across term frequencies for each embassy’s dataset, ranking their frequencies by correlation, and keeping between 0.01% and 0.6% of the features. For ACESS, we used TF-IDF features for clustering and term frequencies to build classifiers for each cluster. Feature selection was performed independently for each cluster, using correlation with respect to training points in the cluster. Classifiers were built using linear SVMs since they performed the best on the training set.\nC. Results\nThe results of our evaluation are presented in Fig. 4. For the two baseline classifiers, the precision, recall, and F-Measure of each of the three classes are shown. A dimension is added to the plot for the ACESS model, showing the number of clusters (k in k-means) selected. For all k > 1, ACESS outperforms both baselines.\nThroughout our evaluation, we found that found that for datasets with more instances, more clusters are required to obtain optimal results, for example, Baghdad is the largest of the WikiLeaks datasets that we evaluated and it achieved “optimal” performance with 488 clusters. Berlin, Damascus, and London, respectively achieved “optimal” performance with 82, 67, and 55 clusters. While the number of clusters cannot be known a priori, it can likely be estimated via crossvalidation on the training set. Interestingly, the optimal number of clusters across all datasets was strongly tied to the number of instances in each dataset: Across all four datasets, the optimal cluster count constituted 0.88± 0.07% of the data."
    }, {
      "heading" : "VII. DISCUSSION",
      "text" : "The fact that ACESS outperforms the baseline algorithms suggests that several classifiers trained in locally derived feature spaces are more discriminative for the type of sensitive data contained in the WikiLeaks dataset than a single monolithic classifier trained in one globally learnt feature space.\nEquivalently, aggregating all paragraphs under a single TFIDF representation seems to attenuate the local signal. In our evaluations we were not able to solve this through nonlinearity in the classifier alone, as kernelized SVMs did not provide statistically significantly better performance in TF-IDF space.\nInstead, ACESS achieves superior classification performance by using global features to direct the query to the right cluster, then perform local, fine-grained classification using that cluster’s feature space and decision boundary. In this respect, clustering serves as a loose form of topic modeling, and an alternative approach, e.g., latent semantic analysis could perhaps be employed.\nThe surprising result that the optimal number of clusters constitutes roughly the same fraction of each embassy’s dataset leads us to hypothesize that there is similarity in topic distribution / sampling across datasets. More analysis is required to verify this hypothesis, but if it holds, several interesting applications could be conducted – e.g., domain adaptation between embassies and other sensitive contexts could be made more trivial. Domain adaptation for classification of sensitive information is an important topic in its own right, since stakeholders might not trust a DLP developer with their sensitive data but might still wish to use the DLP system.\nNotice that in the surface plots in Fig. 4, performance noisily tends to increase, peak, then decrease with the number of clusters, corresponding to the transition from an overly-globalized ACESS model to an overly-localized ACESS model. Overglobalization results from too much aggregation in the local feature spaces, while over-localization is a result of a combined under-sampling in the local feature space representation itself and a local decision boundary with too little support. In practice, however, the superior performance that we achieve over a broad range in number of clusters suggests that whether slightly over-specialized or slightly over-globalized, a useful value of k should be trivial to ascertain."
    }, {
      "heading" : "VIII. CONCLUSION",
      "text" : "To our knowledge, the WikiLeaks dataset is the first dataset available to the research community consisting of actual sensitive information. The dataset is labeled per-paragraph, with multiple levels of sensitivity, and we hope that it will be of use to future researchers. Our experiments show compelling evidence suggesting that ACESS can enhance the accuracy of generalized machine-learnt DLP detection modules by synthesizing both local and global feature space information. Integrating the ACESS model into DLP Systems ensures labeling consistency through instant detection of sensitive documents. We have proven that even with an enormous chunk of textual information, it is possible to identify various sensitivity levels within a document. Automating the paragraphs’ security classification maximizes the accessibility to unclassified or public texts that exist along with the sensitive ones."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "In recent years, traditional cybersecurity safeguards have proven ineffective against insider threats. Famous cases of sensitive information leaks caused by insiders, including the WikiLeaks release of diplomatic cables and the Edward Snowden incident, have greatly harmed the U.S. government’s relationship with other governments and with its own citizens. Data Leak Prevention (DLP) is a solution for detecting and preventing information leaks from within an organization’s network. However, state-of-art DLP detection models are only able to detect very limited types of sensitive information, and research in the field has been hindered due to the lack of available sensitive texts. Many researchers have focused on document-based detection with artificially labeled “confidential documents” for which security labels are assigned to the entire document, when in reality only a portion of the document is sensitive. This type of whole-document based security labeling increases the chances of preventing authorized users from accessing non-sensitive information within sensitive documents. In this paper, we introduce Automated Classification Enabled by Security Similarity (ACESS), a new and innovative detection model that penetrates the complexity of big text security classification/detection. To analyze the ACESS system, we constructed a novel dataset, containing formerly classified paragraphs from diplomatic cables made public by the WikiLeaks organization. To our knowledge this paper is the first to analyze a dataset that contains actual formerly sensitive information annotated at paragraph granularity.",
    "creator" : "LaTeX with hyperref package"
  }
}