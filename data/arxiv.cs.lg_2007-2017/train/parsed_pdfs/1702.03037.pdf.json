{
  "name" : "1702.03037.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Multi-agent Reinforcement Learning in Sequential Social Dilemmas",
    "authors" : [ "Joel Z. Leibo", "Vinicius Zambaldi", "Marc Lanctot", "Janusz Marecki", "Thore Graepel" ],
    "emails" : [ "jzl@google.com", "vzambaldi@google.com", "lanctot@google.com", "tartel@google.com", "thore@google.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "CCS Concepts •Computing methodologies→Multi-agent reinforcement learning; Agent / discrete models; Stochastic games;\nKeywords Social dilemmas, cooperation, Markov games, agent-based social simulation, non-cooperative games"
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "Social dilemmas expose tensions between collective and individual rationality [1]. Cooperation makes possible better outcomes for all than any could obtain on their own. However, the lure of free riding and other such parasitic strategies implies a tragedy of the commons that threatens the stability of any cooperative venture [2].\n1These authors contributed equally.\nAppears in: Proceedings of the 16th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2017), S. Das, E. Durfee, K. Larson, M. Winikoff (eds.), May 8–12, 2017, São Paulo, Brazil. Copyright © 2017, International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.\nThe theory of repeated general-sum matrix games provides a framework for understanding social dilemmas. Fig. 1 shows payoff matrices for three canonical examples: Prisoner’s Dilemma, Chicken, and Stag Hunt. The two actions are interpreted as cooperate and defect respectively. The four possible outcomes of each stage game are R (reward of mutual cooperation), P (punishment arising from mutual defection), S (sucker outcome obtained by the player who cooperates with a defecting partner), and T (temptation outcome achieved by defecting against a cooperator). A matrix game is a social dilemma when its four payoffs satisfy the following social dilemma inequalities (this formulation from [3]):\n1. R > P Mutual cooperation is preferred to mutual defection. (1)\n2. R > S Mutual cooperation is preferred to being exploited by a defector. (2)\n3. 2R > T + S This ensures that mutual cooperation is preferred to an equal probability of unilateral cooperation and defection. (3)\n4. either greed : T > R Exploiting a cooperator is preferred over mutual cooperation or fear : P > S Mutual defection is preferred over being exploited. (4)\nMatrix Game Social Dilemmas (MGSD) have been fruitfully employed as models for a wide variety of phenomena in theoretical social science and biology. For example, there is a large and interesting literature concerned with mechanisms through which the socially preferred outcome of mutual cooperation can be stabilized, e.g., direct reciprocity [4, 5, 6, 7], indirect reciprocity [8], norm enforcement [9, 10], simple reinforcement learning variants [3], multiagent reinforcement learning [11, 12, 13, 14, 15], spatial structure [16], emotions [17], and social network effects [18, 19].\nHowever, the MGSD formalism ignores several aspects of real world social dilemmas which may be of critical importance.\n1. Real world social dilemmas are temporally extended.\n2. Cooperation and defection are labels that apply to policies implementing strategic decisions.\n3. Cooperativeness may be a graded quantity.\nar X\niv :1\n70 2.\n03 03\n7v 1\n[ cs\n.M A\n] 1\n0 Fe\nb 20\n17\n4. Decisions to cooperate or defect occur only quasi-simultaneously since some information about what player 2 is starting to do can inform player 1’s decision and vice versa.\n5. Decisions must be made despite only having partial information about the state of the world and the activities of the other players.\nWe propose a Sequential Social Dilemma (SSD) model to better capture the above points while, critically, maintaining the mixed motivation structure of MGSDs. That is, analogous inequalities to (1) – (4) determine when a temporallyextended Markov game is an SSD.\nTo demonstrate the importance of capturing sequential structure in social dilemma modeling, we present empirical game-theoretic analyses [20, 21] of SSDs to identify the empirical payoff matrices summarizing the outcomes that would arise if cooperate and defect policies were selected as one-shot decisions. The empirical payoff matrices are themselves valid matrix games. Our main result is that both of the SSDs we considered, Gathering and Wolfpack, have empirical payoff matrices that are Prisoner’s Dilemma (PD). This means that if one were to adhere strictly to the MGSDmodeling paradigm, PD models should be proposed for both situations. Thus any conclusions reached from simulating them would necessarily be quite similar in both cases (and to other studies of iterated PD). However, when viewed as SSDs, the formal equivalence of Gathering and Wolfpack disappears. They are clearly different games. In fact, there are simple experimental manipulations that, when applied to Gathering and Wolfpack, yield opposite predictions concerning the emergence and stability of cooperation.\nMore specifically, we describe a factor that promotes the emergence of cooperation in Gathering while discouraging its emergence in Wolfpack, and vice versa. The straightforward implication is that, for modeling real-world social dilemmas with SSDs, the choice of whether to use a Gatheringlike or Wolfpack-like model is critical. And the differences between the two cannot be captured by MGSD modeling.\nAlong the way to these results, the present paper also makes a small methodological contribution. Owing to the greater complexity arising from their sequential structure, it is more computationally demanding to find equilibria of SSD models than it is for MGSD models. Thus the standard evolution and learning approaches to simulating MGSDs cannot be applied to SSDs. Instead, more sophisticated multiagent reinforcement learning methods must be used (e.g [22, 23, 24]). In this paper we describe how deep Q-networks (e.g [25]) may be applied to this problem of finding equilibria of SSDs."
    }, {
      "heading" : "2. DEFINITIONS AND NOTATION",
      "text" : "We model sequential social dilemmas as general-sum Markov (simultaneous move) games with each agent having only a partial observation onto their local environment. Agents must learn an appropriate policy while coexisting with one another. A policy is considered to implement cooperation or defection by properties of the realizations it generates. A Markov game is an SSD if and only if it contains outcomes arising from cooperation and defection policies that satisfy the same inequalities used to define MGSDs (eqs. 1 – 4). This definition is stated more formally in sections 2.1 and 2.2 below."
    }, {
      "heading" : "2.1 Markov Games",
      "text" : "A two-player partially observable Markov game M is defined by a set of states S and an observation function O : S×{1, 2} → Rd specifying each player’s d-dimensional view, along with two sets of actions allowable from any state A1 and A2, one for each player, a transition function T : S × A1 × A2 → ∆(S), where ∆(S) denotes the set of discrete probability distributions over S, and a reward function for each player: ri : S × A1 × A2 → R for player i. Let Oi = {oi | s ∈ S, oi = O(s, i)} be the observation space of player i. To choose actions, each player uses policy πi : Oi → ∆(Ai).\nFor temporal discount factor γ ∈ [0, 1] we can define the long-term payoff V ~πi (s0) to player i when the joint policy\n~π = (π1, π2) is followed starting from state s0 ∈ S.\nV ~πi (s0) = E~at∼~π(O(st)),st+1∼T (st,~at) [ ∞∑ t=0 γtri(st,~at) ] . (5)\nMatrix games are the special case of two-player perfectly observable (Oi(s) = s) Markov games obtained when |S| = 1. MGSDs also specify A1 = A2 = {C,D}, where C and D are called (atomic) cooperate and defect respectively.\nThe outcomes R(s), P (s), S(s), T (s) that determine when a matrix game is a social dilemma are defined as follows.\nR(s) := V π C ,πC 1 (s) = V πC ,πC 2 (s), (6) P (s) := V π D,πD 1 (s) = V πD,πD 2 (s), (7) S(s) := V π C ,πD 1 (s) = V πD,πC 2 (s), (8) T (s) := V π D,πC 1 (s) = V πC ,πD 2 (s), (9)\nwhere πC and πD are cooperative and defecting policies as described next. Note that a matrix game is a social dilemma when R,P, S, T satisfy the inequalities (1) – (4)."
    }, {
      "heading" : "2.2 Definition of Sequential Social Dilemma",
      "text" : "This definition is based on a formalization of empirical game-theoretic analysis [20, 21]. We define the outcomes (R,P, S, T ) := (R(s0), P (s0), S(s0), T (s0)) induced by initial state s0, and two policies π\nC , πD, through their longterm expected payoff (5) and the definitions (6) – (9). We refer to the game matrix with R, P , S, T organized as in Fig. 1-left. as an empirical payoff matrix following the terminology of [21].\nDefinition: A sequential social dilemma is a tuple (M,ΠC ,ΠD) where ΠC and ΠD are disjoint sets of policies that are said to implement cooperation and defection respectively. M is a Markov game with state space S. Let the empirical payoff matrix (R(s), P (s), S(s), T (s)) be induced by policies (πC ∈ ΠC , πD ∈ ΠD) via eqs. (5) – (9). A Markov game is an SSD when there exist states s ∈ S for which the induced empirical payoff matrix satisfies the social dilemma inequalities (1) – (4).\nRemark: There is no guarantee that ΠC ⋃\nΠD = Π, the set of all legal policies. This reflects the fact that, in practice for sequential behavior, cooperativeness is usually a graded property. Thus we are forced to define ΠC and ΠD by thresholding a continuous social behavior metric. For example, to construct an SSD for which a policy’s level of aggressiveness α : Π → R is the relevant social behavior metric, we pick threshold values αc and αd so that α(π) < αc ⇐⇒ π ∈ ΠC and α(π) > αd ⇐⇒ π ∈ ΠD."
    }, {
      "heading" : "3. LEARNING ALGORITHMS",
      "text" : "Most previous work on finding policies for Markov games takes the prescriptive view of multiagent learning [26]: that is, it attempts to answer “what should each agent do?” Several algorithms and analyses have been developed for the two-player zero-sum case [22, 27, 28, 29, 30]. The generalsum case is significantly more challenging [31], and algorithms either have strong assumptions or need to either track several different potential equilibria per agent [32, 33], model other players to simplify the problem [34], or must find a\ncyclic strategy composed of several policies obtained through multiple state space sweeps [35]. Researchers have also studied the emergence of multi-agent coordination in the decentralized, partially observable MDP framework [36, 37, 38]. However, that approach relies on knowledge of the underlying Markov model, an unrealistic assumption for modeling real-world social dilemmas.\nIn contrast, we take a descriptive view, and aim to answer “what social effects emerge when each agent uses a particular learning rule?” The purpose here then is to study and characterize the resulting learning dynamics, as in e.g., [13, 15], rather than on designing new learning algorithms. It is well-known that the resulting “local decision process” could be non-Markovian from each agent’s perspective [39]. This is a feature, not a bug in descriptive work since it is a property of the real environment that the model captures.\nWe use deep reinforcement learning as the basis for each agent in part because of its recent success with solving complex problems [25, 40]. Also, temporal difference predictions have been observed in the brain [41] and this class of reinforcement learning algorithm is seen as a candidate theory of animal habit-learning [42]."
    }, {
      "heading" : "3.1 Deep Multiagent Reinforcement Learning",
      "text" : "Modern deep reinforcement learning methods take the perspective of an agent that must learn to maximize its cumulative long-term reward through trial-and-error interactions with its environment [43, 44].\nIn the multi-agent setting, the i-th agent stores a function Qi : Oi×Ai → R represented by a deep Q-network (DQN). See [25] for details in the single agent case. In our case the true state s is observed differently by each player, as oi = O(s, i). However for consistency of notation, we use a shorthand: Qi(s, a) = Qi(O(s, i), a).\nDuring learning, to encourage exploration we parameterize the i-th agent’s policy by\nπi(s) = { argmaxa∈AiQi(s, a) with probability 1− U(Ai) with probability\nwhere U(Ai) denotes a sample from the uniform distribution\nover Ai. Each agent updates its policy given a stored batch1 of experienced transitions {(s, a, ri, s′)t : t = 1, . . . T} such that\nQi(s, a)← Qi(s, a) + α [ ri + γ max\na′∈Ai Qi(s\n′, a′)−Qi(s, a) ]\nThis is a“growing batch”approach to reinforcement learning in the sense of [45]. However, it does not grow in an unbounded fashion. Rather, old data is discarded so the batch can be constantly refreshed with new data reflecting more recent transitions. We compared batch sizes of 1e5 (our default) and 1e6 in our experiments (see Sect. 5.3). The network representing the function Q is trained through gradient descent on the mean squared Bellman residual with the expectation taken over transitions uniformly sampled from the batch (see [25]). Since the batch is constantly refreshed, the Q-network may adapt to the changing data distribution arising from the effects of learning on π1 and π2.\nIn order to make learning in SSDs tractable, we make the extra assumption that each individual agent’s learning depends only on the other agent’s learning via the (slowly) changing distribution of experience it generates. That is, the two learning agents are “independent” of one another and each regard the other as part of the environment. From the perspective of player one, the learning of player two shows up as a non-stationary environment. The independence assumption can be seen as a particular kind of bounded rationality: agents do no recursive reasoning about one another’s learning. In principle, this restriction could be dropped through the use of planning-based reinforcement learning methods like those of [24]."
    }, {
      "heading" : "4. SIMULATION METHODS",
      "text" : "Both games studied here were implemented in a 2D gridworld game engine. The state st and the joint action of all players ~a determines the state at the next time-step st+1. Observations O(s, i) ∈ R3×16×21 (RGB) of the true state st depended on the player’s current position and orientation. The observation window extended 15 grid squares ahead and 10 grid squares from side to side (see Fig. 3B). Actions a ∈ R8 were agent-centered: step forward, step backward, step left, step right, rotate left, rotate right, use beam and stand still. Each player appears blue in its own local view, light-blue in its teammates view and red in its opponent’s view. Each episode lasted for 1, 000 steps. Default neural networks had two hidden layers with 32 units, interleaved with rectified linear layers which projected to the output layer which had 8 units, one for each action. During training, players implemented epsilon-greedy policies, with epsilon decaying linearly over time (from 1.0 to 0.1). The default per-time-step discount rate was 0.99."
    }, {
      "heading" : "5. RESULTS",
      "text" : "In this section, we describe three experiments: one for each game (Gathering and Wolfpack), and a third experiment investigating parameters that influence the emergence of cooperation versus defection."
    }, {
      "heading" : "5.1 Experiment 1: Gathering",
      "text" : "The goal of the Gathering game is to collect apples, represented by green pixels (see Fig. 3A). When a player collects\n1The batch is sometimes called a “replay buffer” e.g. [25].\nan apple it receives a reward of 1 and the apple is temporarily removed from the map. The apple respawns after Napple frames. Players can direct a beam in a straight line along their current orientation. A player hit by the beam twice is “tagged” and removed from the game for Ntagged frames. No rewards are delivered to either player for tagging. The only potential motivation for tagging is competition over the apples. Refer to the Gathering gameplay video2 for demonstration.\nIntuitively, a defecting policy in this game is one that is aggressive—i.e., involving frequent attempts to tag rival players to remove them from the game. Such a policy is motivated by the opportunity to take all the apples for oneself that arises after eliminating the other player. By contrast, a cooperative policy is one that does not seek to tag the other player. This suggests the use of a social behavior met-\n2https://goo.gl/2xczLc\nric (section 2.2) that measures a policy’s tendency to use the beam action as the basis for its classification as defection or cooperation. To this end, we counted the number of beam actions during a time horizon and normalized it by the amount of time in which both agents were playing (not removed from the game).\nBy manipulating the rate at which apples respawn after being collected, Napple, we could control the abundance of apples in the environment. Similarly, by manipulating the number of timesteps for which a tagged agent is removed from the game, Ntagged, we could control the cost of potential conflict. We wanted to test whether conflict would emerge from learning in environments where apples were scarce. We considered the effect of abundance (Napple) and conflict-cost (Ntagged) on the level of aggressiveness (beamuse rate) that emerges from learning. Fig. 4A shows the beam-use rate that evolved after training for 40 million steps as a function of abundance (Napple) and conflict-cost (Ntagged). Supplementary video 3 shows how such emergent conflict evolves over the course of learning. In this case, differences in beam-use rate (proxy for the tendency to defect) learned in the different environments emerge quite early in training and mostly persist throughout. When learning does change beam-use rate, it is almost always to increase it.\nWe noted that the policies learned in environments with low abundance or high conflict-cost were highly aggressive while the policies learned with high abundance or low conflictcost were less aggressive. That is, the Gathering game predicts that conflict may emerge from competition for scarce resources, but is less likely to emerge when resources are plentiful.\nTo further characterize the mixed motivation structure of the Gathering game, we carried out the empirical gametheoretic analysis suggested by the definition of section 2.2. We chose the set of policies ΠC that were trained in the high abundance / low conflict-cost environments (low aggression policies) and ΠD as policies trained in the low abundance and high conflict-cost environments (high aggression policies), and used these to compute empirical payoff matrices as follows. Two pairs of policies (πC1 , π D 1 ) and (π C 2 , π D 2 ) are sampled from ΠC and ΠD and matched against each other in the Gathering game for one episode. The resulting rewards are assigned to individual cells of a matrix game, in which πCi corresponds the cooperative action for player i, and πDj , the defective action for player j. This process is repeated until convergence of the cell values, and generates estimates of R,P, S, and T for the game corresponding to each abundance / conflict-cost (Napple, Ntagged) level tested. See Figure 5 for an illustration of this workflow. Fig. 6A summarizes the types of empirical games that were found given our parameter spectrum. Most cases where the social dilemma inequalities (1) – (4) held, i.e., the strategic scenario was a social dilemma, turned out to be a prisoner’s dilemma. The greed motivation reflects the temptation to take out a rival and collect all the apples oneself. The fear motivation reflected the danger of being taken out oneself by a defecting rival. P is preferred to S in the Gathering game because mutual defection typically leads to both players alternating tagging one another, so each gets some time alone to collect apples. Whereas the agent receiving the outcome S does not try to tag its rival and thus never gets this\n3https://goo.gl/w2VqlQ\nchance."
    }, {
      "heading" : "5.2 Experiment 2: Wolfpack",
      "text" : "The Wolfpack game requires two players (wolves) to chase a third player (the prey). When either wolf touches the prey, all wolves within the capture radius (see Fig. 3B) receive a reward. The reward received by the capturing wolves is proportional to the number of wolves in the capture radius. The idea is that a lone wolf can capture the prey, but is at risk of losing the carcass to scavengers. However, when the two wolves capture the prey together, they can better protect the carcass from scavengers and hence receive a higher reward. A lone-wolf capture provides a reward of rlone and a capture involving both wolves is worth rteam. Refer to the Wolfpack gameplay video4 for demonstration.\nThe wolves learn to catch the prey over the course of training. Fig. 4B shows the effect on the average number of wolves per capture obtained from training in environments with varying levels of group capture bonus rteam/rlone and capture radius. Supplementary video 5 shows how this dependency evolves over learning time. Like in the Gathering game, these results show that environment parameters influence how cooperative the learned policies will be. It is interesting that two different cooperative policies emerged from these experiments. On the one hand, the wolves could cooperate by first finding one another and then moving together to hunt the prey, while on the other hand, a wolf could first find the prey and then wait for the other wolf to arrive before capturing it.\nAnalogous to our analysis of the Gathering game, we choose ΠC and ΠD for Wolfpack to be the sets of policies learned in the high radius / group bonus and low radius /group bonus environments respectively. The procedure for estimating R,P, S, and T was the same as in section 5.1. Fig. 6B summarizes these results. Interestingly, it turns out that all three classic MGSDs, chicken, stag hunt, and prisoner’s dilemma can be found in the empirical payoff matrices of Wolfpack."
    }, {
      "heading" : "5.3 Experiment 3: Agent parameters influencing the emergence of defection",
      "text" : "So far we have described how properties of the environment influence emergent social outcomes. Next we consider the impact of manipulating properties of the agents. Psychological research attempting to elucidate the motivational factors underlying human cooperation is relevant here. In particular, Social Psychology has advanced various hypotheses concerning psychological variables that may influence cooperation and give rise to the observed individual differences in human cooperative behavior in laboratory-based social dilemmas [2]. These factors include consideration-of-futureconsequences [46], trust [47], affect (interestingly, it is negative emotions that turn out to promote cooperation [48]), and a personality variable called social value orientation characterized by other-regarding-preferences. The latter has been studied in a similar Markov game social dilemma setup to our SSD setting by [49].\nObviously the relatively simple DQN learning agents we consider here do not have internal variables that directly correspond to the factors identified by Social Psychology. Nor\n4https://goo.gl/AgXtTn 5https://goo.gl/vcB8mU\nshould they be expected to capture the full range of human individual differences in laboratory social dilemmas. Nevertheless, it is interesting to consider just how far one can go down this road of modeling Social Psychology hypotheses using such simple learning agents6. Recall also that DQN is in the class of reinforcement learning algorithms that is generally considered to be the leading candidate theory of animal habit-learning [50, 42]. Thus, the interpretation of our model is that it only addresses whatever part of cooperative behavior arises “by habit” as opposed to conscious deliberation.\nExperimental manipulations of DQN parameters yield consistent and interpretable effects on emergent social behavior. Each plot in Fig. 7 shows the relevant social behavior metric, conflict for Gathering and lone-wolf behavior for Wolfpack, as a function of an environment parameter: Napple, Ntagged (Gathering) and rteam/rlone (Wolfpack). The figure shows that in both games, agents with greater discount parameter (less time discounting) more readily defect than agents that discount the future more steeply. For Gathering this likely occurs because the defection policy of tagging the other player to temporarily remove them from the game only provides a delayed reward in the form of the increased opportunity to collect apples without interference. However, when abundance is very high, even the agents with higher discount factors do not learn to defect. In such paradisiacal settings, the apples respawn so quickly that an individual agent cannot collect them quickly enough. As a consequence, there is no motivation to defect regardless of the temporal discount rate. Manipulating the size of the storedand-constantly-refreshed batch of experience used to train each DQN agent has the opposite effect on the emergence of defection. Larger batch size translates into more experience with the other agent’s policy. For Gathering, this means that avoiding being tagged becomes easier. Evasive action benefits more from extra experience than the ability to target the other agent. For Wolfpack, larger batch size allows\n6The contrasting approach that seeks to build more structure into the reinforcement learning agents to enable more interpretable experimental manipulations is also interesting and complementary e.g., [24].\ngreater opportunity to learn to coordinate to jointly catch the prey.\nPossibly the most interesting effect on behavior comes from the number of hidden units in the neural network behind the agents, which may be interpreted as their cognitive capacity. Curves for tendency to defect are shown in the right column of Fig. 7, comparing two different network sizes. For Gathering, an increase in network size leads to an increase in the agent’s tendency to defect, whereas for Wolfpack the opposite is true: Greater network size leads to less defection.\nThis can be explained as follows. In Gathering, defection behavior is more complex and requires a larger network size to learn than cooperative behavior. This is the case because defection requires the difficult task of targeting the opposing agent with the beam whereas peacefully collecting apples is almost independent of the opposing agent’s behavior. In Wolfpack, cooperation behavior is more complex and requires a larger network size because the agents need to coordinate their hunting behaviors to collect the team reward whereas the lone-wolf behavior does not require coordination with the other agent and hence requires less network capacity.\nNote that the qualitative difference in effects for network size supports our argument that the richer framework of SSDs is needed to capture important aspects of real social dilemmas. This rather striking difference between Gathering and Wolfpack is invisible to the purely matrix game based MGSD-modeling. It only emerges when the different complexities of cooperative or defecting behaviors, and hence the difficulty of the corresponding learning problems is modeled in a sequential setup such as an SSD."
    }, {
      "heading" : "6. DISCUSSION",
      "text" : "In the Wolfpack game, learning a defecting lone-wolf policy is easier than learning a cooperative pack-hunting policy. This is because the former does not require actions to be conditioned on the presence of a partner within the capture radius. In the Gathering game the situation is reversed. Cooperative policies are easier to learn since they need only be concerned with apples and may not depend\non the rival player’s actions. However, optimally efficient cooperative policies may still require such coordination to prevent situations where both players simultaneously move on the same apple. Cooperation and defection demand differing levels of coordination for the two games. Wolfpack’s cooperative policy requires greater coordination than its defecting policy. Gathering’s defection policy requires greater coordination (to successfully aim at the rival player).\nBoth the Gathering and Wolfpack games contain embedded MGSDs with prisoner’s dilemma-type payoffs. The MGSD model thus regards them as structurally identical. Yet, viewed as SSDs, they make rather different predictions. This suggests a new dimension on which to investigate classic questions concerning the evolution of cooperation. For any to-be-modeled phenomenon, the question now arises: which SSD is a better description of the game being played? If Gathering is a better model, then we would expect cooperation to be the easier-to-learn “default” policy, probably requiring less coordination. For situations where Wolfpack is the better model, defection is the easier-to-learn “default” behavior and cooperation is the harder-to-learn policy requiring greater coordination. These modeling choices are somewhat orthogonal to the issue of assigning values to the various possible outcomes (the only degree of freedom in MGSD-modeling), yet they make a large difference to the results.\nSSD models address similar research questions as MGSD models, e.g. the evolution of cooperation. However, SSD models are more realistic since they capture the sequential structure of real-world social dilemmas. Of course, in modeling, greater verisimilitude is not automatically virtuous. When choosing between two models of a given phenomenon,\nOccam’s razor demands we prefer the simpler one. If SSDs were just more realistic models that led to the same conclusions as MGSDs then they would not be especially useful. This however, is not the case. We argue the implication of the results presented here is that standard evolutionary and learning-based approaches to modeling the trial and error process through which societies converge on equilibria of social dilemmas are unable to address the following important learning related phenomena.\n1. Learning which strategic decision to make, abstractly, whether to cooperate or defect, often occurs simultaneously with learning how to efficiently implement said decision.\n2. It may be difficult to learn how to implement an effective cooperation policy with a partner bent on defection— or vice versa.\n3. Implementing effective cooperation or defection may involve solving coordination subproblems, but there is no guarantee this would occur, or that cooperation and defection would rely on coordination to the same extent. In some strategic situations, cooperation may require coordination, e.g., standing aside to allow a partner’s passage through a narrow corridor while in others defection may require coordination e.g. blocking a rival from passing.\n4. Some strategic situations may allow for multiple different implementations of cooperation, and each may require coordination to a greater or lesser extent. The same goes for multiple implementations of defection.\n5. The complexity of learning how to implement effective cooperation and defection policies may not be equal. One or the other might be significantly easier to learn—solely due to implementation complexity— in a manner that cannot be accounted for by adjusting outcome values in an MGSD model.\nOur general method of tracking social behavior metrics in addition to reward while manipulating parameters of the learning environment is widely applicable. One could use these techniques to simulate the effects of external interventions on social equilibria in cases where the sequential structure of cooperation and defection are important. Notice that several of the examples in Schelling’s seminal book Micromotives and Macrobehavior [51] can be seen as temporally extended social dilemmas for which policies have been learned over the course of repeated interaction, including the famous opening example of lecture hall seating behavior. It is also possible to define SSDs that model the extraction of renewable vs non-renewable resources and track the sustainability of the emergent social behaviors while taking into account the varying difficulties of learning sustainable (cooperating) vs. non-sustainable (defecting) policies. Effects stemming from the need to learn implementations for strategic decisions may be especially important for informed policy-making concerning such real-world social dilemmas."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors would like to thank Chrisantha Fernando, Toby Ord, and Peter Sunehag for fruitful discussions in the leadup to this work, and Charles Beattie, Denis Teplyashin, and Stig Petersen for software engineering support."
    } ],
    "references" : [ {
      "title" : "Prisoner’s dilemma–recollections and observations. In Game Theory as a Theory of a Conflict Resolution, pages 17–34",
      "author" : [ "Anatol Rapoport" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1974
    }, {
      "title" : "The psychology of social dilemmas: A review",
      "author" : [ "Paul AM Van Lange", "Jeff Joireman", "Craig D Parks", "Eric Van Dijk" ],
      "venue" : "Organizational Behavior and Human Decision Processes,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2013
    }, {
      "title" : "Learning dynamics in social dilemmas",
      "author" : [ "Michael W Macy", "Andreas Flache" ],
      "venue" : "Proceedings of the National Academy of Sciences,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2002
    }, {
      "title" : "The evolution of reciprocal altruism",
      "author" : [ "Robert L. Trivers" ],
      "venue" : "Quarterly Review of Biology,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1971
    }, {
      "title" : "The Evolution of Cooperation",
      "author" : [ "Robert Axelrod" ],
      "venue" : "Basic Books,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1984
    }, {
      "title" : "Tit for tat in heterogeneous populations",
      "author" : [ "Martin A Nowak", "Karl Sigmund" ],
      "venue" : "Nature, 355(6357):250–253,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1992
    }, {
      "title" : "A strategy of win-stay, lose-shift that outperforms tit-for-tat in the prisoner’s dilemma",
      "author" : [ "Martin Nowak", "Karl Sigmund" ],
      "venue" : "game. Nature,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1993
    }, {
      "title" : "Evolution of indirect reciprocity by image scoring",
      "author" : [ "Martin A Nowak", "Karl Sigmund" ],
      "venue" : "Nature, 393(6685):573–577,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1998
    }, {
      "title" : "An evolutionary approach to norms",
      "author" : [ "Robert Axelrod" ],
      "venue" : "American political science review,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1986
    }, {
      "title" : "Cooperation emergence under resource-constrained peer punishment",
      "author" : [ "Samhar Mahmoud", "Simon Miles", "Michael Luck" ],
      "venue" : "In Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2016
    }, {
      "title" : "Multiagent reinforcement learning in the iterated prisoner’s",
      "author" : [ "T.W. Sandholm", "R.H. Crites" ],
      "venue" : "dilemma. Biosystems,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1996
    }, {
      "title" : "Learning to cooperate in multi-agent social dilemmas",
      "author" : [ "Enrique Munoz de Cote", "Alessandro Lazaric", "Marcello Restelli" ],
      "venue" : "In Proceedings of the Fifth International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS),",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2006
    }, {
      "title" : "Classes of multiagent Q-learning dynamics with greedy exploration",
      "author" : [ "M. Wunder", "M. Littman", "M. Babes" ],
      "venue" : "In Proceedings of the 27th International Conference on Machine Learning,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2010
    }, {
      "title" : "Empirically evaluating multiagent learning",
      "author" : [ "Erik Zawadzki", "Asher Lipson", "Kevin Leyton-Brown" ],
      "venue" : "algorithms. CoRR,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2014
    }, {
      "title" : "Evolutionary dynamics of multi-agent learning: A survey",
      "author" : [ "Daan Bloembergen", "Karl Tuyls", "Daniel Hennes", "Michael Kaisers" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2015
    }, {
      "title" : "Evolutionary games and spatial chaos",
      "author" : [ "Martin A Nowak", "Robert M May" ],
      "venue" : "Nature, 359(6398):826–829,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1992
    }, {
      "title" : "Emotional multiagent reinforcement learning in spatial social dilemmas",
      "author" : [ "Chao Yu", "Minjie Zhang", "Fenghui Ren", "Guozhen Tan" ],
      "venue" : "IEEE Transactions on Neural Networks and Learning Systems,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2015
    }, {
      "title" : "A simple rule for the evolution of cooperation on graphs and social",
      "author" : [ "Hisashi Ohtsuki", "Christoph Hauert", "Erez Lieberman", "Martin A Nowak" ],
      "venue" : "networks. Nature,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2006
    }, {
      "title" : "A new route to the evolution of cooperation",
      "author" : [ "Francisco C Santos", "Jorge M Pacheco" ],
      "venue" : "Journal of Evolutionary Biology,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2006
    }, {
      "title" : "Analyzing complex strategic interactions in multi-agent systems",
      "author" : [ "William E Walsh", "Rajarshi Das", "Gerald Tesauro", "Jeffrey O Kephart" ],
      "venue" : "Workshop on Game-Theoretic and Decision-Theoretic Agents,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2002
    }, {
      "title" : "Methods for empirical game-theoretic analysis (extended abstract)",
      "author" : [ "Michael Wellman" ],
      "venue" : "In Proceedings of the National Conference on Artificial Intelligence (AAAI),",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2006
    }, {
      "title" : "Markov games as a framework for multi-agent reinforcement learning",
      "author" : [ "M.L. Littman" ],
      "venue" : "In Proceedings of the 11th International Conference on Machine Learning (ICML),",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1994
    }, {
      "title" : "Game theory and multiagent reinforcement learning",
      "author" : [ "Ann Nowé", "Peter Vrancx", "Yann-Michaël De Hauwere" ],
      "venue" : "Reinforcement Learning: State-of-the-Art,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2012
    }, {
      "title" : "Coordinate to cooperate or compete: abstract goals  and joint intentions in social interaction",
      "author" : [ "Max Kleiman-Weiner", "M K Ho", "J L Austerweil", "Michael L Littman", "Josh B Tenenbaum" ],
      "venue" : "In Proceedings of the 38th Annual Conference of the Cognitive Science Society,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2016
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis" ],
      "venue" : "Nature, 518(7540):529–533,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2015
    }, {
      "title" : "If multi-agent learning is the answer, what is the question",
      "author" : [ "Y. Shoham", "R. Powers", "T. Grenager" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2007
    }, {
      "title" : "Value function approximation in zero-sum Markov games",
      "author" : [ "M.G. Lagoudakis", "R. Parr" ],
      "venue" : "In Proceedings of the 18th Conference on Uncertainty in Artificial Intelligence (UAI),",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2002
    }, {
      "title" : "Approximate dynamic programming for two-player zero-sum Markov games",
      "author" : [ "J. Pérolat", "B. Scherrer", "B. Piot", "O. Pietquin" ],
      "venue" : "In Proceedings of the International Conference on Machine Learning (ICML),",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2015
    }, {
      "title" : "Softened approximate policy iteration for Markov games",
      "author" : [ "J. Pérolat", "B. Piot", "M. Geist", "B. Scherrer", "O. Pietquin" ],
      "venue" : "In Proceedings of the International Conference on Machine Learning (ICML),",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2016
    }, {
      "title" : "Algorithms for computing strategies in two-player simultaneous move games",
      "author" : [ "Branislav Bošanský", "Viliam Lisý", "Marc Lanctot", "Jǐŕı Čermák", "Mark H.M. Winands" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2016
    }, {
      "title" : "Cyclic equilibria in Markov games",
      "author" : [ "M. Zinkevich", "A. Greenwald", "M. Littman" ],
      "venue" : "In Neural Information Processing Systems,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2006
    }, {
      "title" : "Multiagent reinforcement learning: Theoretical framework and an algorithm",
      "author" : [ "J. Hu", "M.P. Wellman" ],
      "venue" : "In Proceedings of the 15th International Conference on Machine Learning (ICML),",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 1998
    }, {
      "title" : "Correlated-Q learning",
      "author" : [ "A. Greenwald", "K. Hall" ],
      "venue" : "In Proceedings of the 20th International Conference on Machine Learning (ICML),",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2003
    }, {
      "title" : "Friend-or-foe Q-learning in general-sum games",
      "author" : [ "Michael Littman" ],
      "venue" : "In Proceedings of the Eighteenth International Conference on Machine Learning,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2001
    }, {
      "title" : "On the use of non-stationary strategies for solving two-player zero-sum Markov games",
      "author" : [ "J. Pérolat", "B. Piot", "B. Scherrer", "O. Pietquin" ],
      "venue" : "In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2016
    }, {
      "title" : "A framework for sequential planning in multi-agent settings",
      "author" : [ "Piotr J Gmytrasiewicz", "Prashant Doshi" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2005
    }, {
      "title" : "Exploiting coordination locales in distributed POMDPs via social model shaping",
      "author" : [ "Pradeep Varakantham", "Jun-young Kwak", "Matthew E Taylor", "Janusz Marecki", "Paul Scerri", "Milind Tambe" ],
      "venue" : "In Proceedings of the 19th International Conference on Automated Planning and Scheduling,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2009
    }, {
      "title" : "Solving transition independent decentralized Markov decision processes",
      "author" : [ "Raphen Becker", "Shlomo Zilberstein", "Victor Lesser", "Claudia V Goldman" ],
      "venue" : "Journal of  Artificial Intelligence Research,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2004
    }, {
      "title" : "The world of independent learners is not Markovian",
      "author" : [ "Guillaume J. Laurent", "Laëtitia Matignon", "N. Le Fort-Piat" ],
      "venue" : "Int. J. Know.-Based Intell. Eng. Syst.,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2011
    }, {
      "title" : "Mastering the game of Go with deep neural networks and tree",
      "author" : [ "David Silver", "Aja Huang", "Chris J. Maddison", "Arthur Guez", "Laurent Sifre", "George van den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot", "Sander Dieleman", "Dominik Grewe", "John Nham", "Nal Kalchbrenner", "Ilya Sutskever", "Timothy Lillicrap", "Madeleine Leach", "Koray Kavukcuoglu", "Thore Graepel", "Demis Hassabis" ],
      "venue" : "search. Nature,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2016
    }, {
      "title" : "A neural substrate of prediction and reward",
      "author" : [ "W. Schultz", "P. Dayan", "P.R. Montague" ],
      "venue" : null,
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 1997
    }, {
      "title" : "Reinforcement learning in the brain",
      "author" : [ "Y. Niv" ],
      "venue" : "The Journal of Mathematical Psychology,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2009
    }, {
      "title" : "Introduction to Reinforcement Learning",
      "author" : [ "Richard S. Sutton", "Andrew G. Barto" ],
      "venue" : null,
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 1998
    }, {
      "title" : "Reinforcement learning improves behaviour from evaluative",
      "author" : [ "Michael L Littman" ],
      "venue" : "feedback. Nature,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 2015
    }, {
      "title" : "Batch reinforcement learning",
      "author" : [ "Sascha Lange", "Thomas Gabel", "Martin Riedmiller" ],
      "venue" : "In Reinforcement learning,",
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 2012
    }, {
      "title" : "Time, uncertainty, and individual differences in decisions to cooperate in resource dilemmas",
      "author" : [ "Katherine V Kortenkamp", "Colleen F Moore" ],
      "venue" : "Personality and Social Psychology Bulletin,",
      "citeRegEx" : "46",
      "shortCiteRegEx" : "46",
      "year" : 2006
    }, {
      "title" : "High and low trusters’ responses to fear in a payoff matrix",
      "author" : [ "Craig D Parks", "Lorne G Hulbert" ],
      "venue" : "Journal of Conflict Resolution,",
      "citeRegEx" : "47",
      "shortCiteRegEx" : "47",
      "year" : 1995
    }, {
      "title" : "When happiness makes us selfish, but sadness makes us fair: Affective influences on interpersonal strategies in the dictator game",
      "author" : [ "Hui Bing Tan", "Joseph P Forgas" ],
      "venue" : "Journal of Experimental Social Psychology,",
      "citeRegEx" : "48",
      "shortCiteRegEx" : "48",
      "year" : 2010
    }, {
      "title" : "How other-regarding preferences can promote cooperation in non-zero-sum grid games",
      "author" : [ "Joseph L. Austerweil", "Stephen Brawner", "Amy Greenwald", "Elizabeth Hilliard", "Mark Ho", "Michael L. Littman", "James MacGlashan", "Carl Trimbach" ],
      "venue" : "In Proceedings of the AAAI Symposium on Challenges and Opportunities in Multiagent Learning for the Real World,",
      "citeRegEx" : "49",
      "shortCiteRegEx" : "49",
      "year" : 2016
    }, {
      "title" : "Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control",
      "author" : [ "Nathaniel D Daw", "Yael Niv", "Peter Dayan" ],
      "venue" : "Nature neuroscience,",
      "citeRegEx" : "50",
      "shortCiteRegEx" : "50",
      "year" : 2005
    }, {
      "title" : "Micromotives and macrobehavior",
      "author" : [ "Thomas C. Schelling" ],
      "venue" : "WW Norton & Company,",
      "citeRegEx" : "51",
      "shortCiteRegEx" : "51",
      "year" : 1978
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Social dilemmas expose tensions between collective and individual rationality [1].",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 1,
      "context" : "However, the lure of free riding and other such parasitic strategies implies a tragedy of the commons that threatens the stability of any cooperative venture [2].",
      "startOffset" : 158,
      "endOffset" : 161
    }, {
      "referenceID" : 2,
      "context" : "A matrix game is a social dilemma when its four payoffs satisfy the following social dilemma inequalities (this formulation from [3]):",
      "startOffset" : 129,
      "endOffset" : 132
    }, {
      "referenceID" : 3,
      "context" : ", direct reciprocity [4, 5, 6, 7], indirect reciprocity [8], norm enforcement [9, 10], simple reinforcement learning variants [3], multiagent reinforcement learning [11, 12, 13, 14, 15], spatial structure [16], emotions [17], and social network effects [18, 19].",
      "startOffset" : 21,
      "endOffset" : 33
    }, {
      "referenceID" : 4,
      "context" : ", direct reciprocity [4, 5, 6, 7], indirect reciprocity [8], norm enforcement [9, 10], simple reinforcement learning variants [3], multiagent reinforcement learning [11, 12, 13, 14, 15], spatial structure [16], emotions [17], and social network effects [18, 19].",
      "startOffset" : 21,
      "endOffset" : 33
    }, {
      "referenceID" : 5,
      "context" : ", direct reciprocity [4, 5, 6, 7], indirect reciprocity [8], norm enforcement [9, 10], simple reinforcement learning variants [3], multiagent reinforcement learning [11, 12, 13, 14, 15], spatial structure [16], emotions [17], and social network effects [18, 19].",
      "startOffset" : 21,
      "endOffset" : 33
    }, {
      "referenceID" : 6,
      "context" : ", direct reciprocity [4, 5, 6, 7], indirect reciprocity [8], norm enforcement [9, 10], simple reinforcement learning variants [3], multiagent reinforcement learning [11, 12, 13, 14, 15], spatial structure [16], emotions [17], and social network effects [18, 19].",
      "startOffset" : 21,
      "endOffset" : 33
    }, {
      "referenceID" : 7,
      "context" : ", direct reciprocity [4, 5, 6, 7], indirect reciprocity [8], norm enforcement [9, 10], simple reinforcement learning variants [3], multiagent reinforcement learning [11, 12, 13, 14, 15], spatial structure [16], emotions [17], and social network effects [18, 19].",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 8,
      "context" : ", direct reciprocity [4, 5, 6, 7], indirect reciprocity [8], norm enforcement [9, 10], simple reinforcement learning variants [3], multiagent reinforcement learning [11, 12, 13, 14, 15], spatial structure [16], emotions [17], and social network effects [18, 19].",
      "startOffset" : 78,
      "endOffset" : 85
    }, {
      "referenceID" : 9,
      "context" : ", direct reciprocity [4, 5, 6, 7], indirect reciprocity [8], norm enforcement [9, 10], simple reinforcement learning variants [3], multiagent reinforcement learning [11, 12, 13, 14, 15], spatial structure [16], emotions [17], and social network effects [18, 19].",
      "startOffset" : 78,
      "endOffset" : 85
    }, {
      "referenceID" : 2,
      "context" : ", direct reciprocity [4, 5, 6, 7], indirect reciprocity [8], norm enforcement [9, 10], simple reinforcement learning variants [3], multiagent reinforcement learning [11, 12, 13, 14, 15], spatial structure [16], emotions [17], and social network effects [18, 19].",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 10,
      "context" : ", direct reciprocity [4, 5, 6, 7], indirect reciprocity [8], norm enforcement [9, 10], simple reinforcement learning variants [3], multiagent reinforcement learning [11, 12, 13, 14, 15], spatial structure [16], emotions [17], and social network effects [18, 19].",
      "startOffset" : 165,
      "endOffset" : 185
    }, {
      "referenceID" : 11,
      "context" : ", direct reciprocity [4, 5, 6, 7], indirect reciprocity [8], norm enforcement [9, 10], simple reinforcement learning variants [3], multiagent reinforcement learning [11, 12, 13, 14, 15], spatial structure [16], emotions [17], and social network effects [18, 19].",
      "startOffset" : 165,
      "endOffset" : 185
    }, {
      "referenceID" : 12,
      "context" : ", direct reciprocity [4, 5, 6, 7], indirect reciprocity [8], norm enforcement [9, 10], simple reinforcement learning variants [3], multiagent reinforcement learning [11, 12, 13, 14, 15], spatial structure [16], emotions [17], and social network effects [18, 19].",
      "startOffset" : 165,
      "endOffset" : 185
    }, {
      "referenceID" : 13,
      "context" : ", direct reciprocity [4, 5, 6, 7], indirect reciprocity [8], norm enforcement [9, 10], simple reinforcement learning variants [3], multiagent reinforcement learning [11, 12, 13, 14, 15], spatial structure [16], emotions [17], and social network effects [18, 19].",
      "startOffset" : 165,
      "endOffset" : 185
    }, {
      "referenceID" : 14,
      "context" : ", direct reciprocity [4, 5, 6, 7], indirect reciprocity [8], norm enforcement [9, 10], simple reinforcement learning variants [3], multiagent reinforcement learning [11, 12, 13, 14, 15], spatial structure [16], emotions [17], and social network effects [18, 19].",
      "startOffset" : 165,
      "endOffset" : 185
    }, {
      "referenceID" : 15,
      "context" : ", direct reciprocity [4, 5, 6, 7], indirect reciprocity [8], norm enforcement [9, 10], simple reinforcement learning variants [3], multiagent reinforcement learning [11, 12, 13, 14, 15], spatial structure [16], emotions [17], and social network effects [18, 19].",
      "startOffset" : 205,
      "endOffset" : 209
    }, {
      "referenceID" : 16,
      "context" : ", direct reciprocity [4, 5, 6, 7], indirect reciprocity [8], norm enforcement [9, 10], simple reinforcement learning variants [3], multiagent reinforcement learning [11, 12, 13, 14, 15], spatial structure [16], emotions [17], and social network effects [18, 19].",
      "startOffset" : 220,
      "endOffset" : 224
    }, {
      "referenceID" : 17,
      "context" : ", direct reciprocity [4, 5, 6, 7], indirect reciprocity [8], norm enforcement [9, 10], simple reinforcement learning variants [3], multiagent reinforcement learning [11, 12, 13, 14, 15], spatial structure [16], emotions [17], and social network effects [18, 19].",
      "startOffset" : 253,
      "endOffset" : 261
    }, {
      "referenceID" : 18,
      "context" : ", direct reciprocity [4, 5, 6, 7], indirect reciprocity [8], norm enforcement [9, 10], simple reinforcement learning variants [3], multiagent reinforcement learning [11, 12, 13, 14, 15], spatial structure [16], emotions [17], and social network effects [18, 19].",
      "startOffset" : 253,
      "endOffset" : 261
    }, {
      "referenceID" : 19,
      "context" : "To demonstrate the importance of capturing sequential structure in social dilemma modeling, we present empirical game-theoretic analyses [20, 21] of SSDs to identify the empirical payoff matrices summarizing the outcomes that would arise if cooperate and defect policies were selected as one-shot decisions.",
      "startOffset" : 137,
      "endOffset" : 145
    }, {
      "referenceID" : 20,
      "context" : "To demonstrate the importance of capturing sequential structure in social dilemma modeling, we present empirical game-theoretic analyses [20, 21] of SSDs to identify the empirical payoff matrices summarizing the outcomes that would arise if cooperate and defect policies were selected as one-shot decisions.",
      "startOffset" : 137,
      "endOffset" : 145
    }, {
      "referenceID" : 21,
      "context" : "g [22, 23, 24]).",
      "startOffset" : 2,
      "endOffset" : 14
    }, {
      "referenceID" : 22,
      "context" : "g [22, 23, 24]).",
      "startOffset" : 2,
      "endOffset" : 14
    }, {
      "referenceID" : 23,
      "context" : "g [22, 23, 24]).",
      "startOffset" : 2,
      "endOffset" : 14
    }, {
      "referenceID" : 24,
      "context" : "g [25]) may be applied to this problem of finding equilibria of SSDs.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 0,
      "context" : "For temporal discount factor γ ∈ [0, 1] we can define the long-term payoff V ~π i (s0) to player i when the joint policy",
      "startOffset" : 33,
      "endOffset" : 39
    }, {
      "referenceID" : 19,
      "context" : "This definition is based on a formalization of empirical game-theoretic analysis [20, 21].",
      "startOffset" : 81,
      "endOffset" : 89
    }, {
      "referenceID" : 20,
      "context" : "This definition is based on a formalization of empirical game-theoretic analysis [20, 21].",
      "startOffset" : 81,
      "endOffset" : 89
    }, {
      "referenceID" : 20,
      "context" : "as an empirical payoff matrix following the terminology of [21].",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 25,
      "context" : "Most previous work on finding policies for Markov games takes the prescriptive view of multiagent learning [26]: that is, it attempts to answer “what should each agent do?” Several algorithms and analyses have been developed for the two-player zero-sum case [22, 27, 28, 29, 30].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 21,
      "context" : "Most previous work on finding policies for Markov games takes the prescriptive view of multiagent learning [26]: that is, it attempts to answer “what should each agent do?” Several algorithms and analyses have been developed for the two-player zero-sum case [22, 27, 28, 29, 30].",
      "startOffset" : 258,
      "endOffset" : 278
    }, {
      "referenceID" : 26,
      "context" : "Most previous work on finding policies for Markov games takes the prescriptive view of multiagent learning [26]: that is, it attempts to answer “what should each agent do?” Several algorithms and analyses have been developed for the two-player zero-sum case [22, 27, 28, 29, 30].",
      "startOffset" : 258,
      "endOffset" : 278
    }, {
      "referenceID" : 27,
      "context" : "Most previous work on finding policies for Markov games takes the prescriptive view of multiagent learning [26]: that is, it attempts to answer “what should each agent do?” Several algorithms and analyses have been developed for the two-player zero-sum case [22, 27, 28, 29, 30].",
      "startOffset" : 258,
      "endOffset" : 278
    }, {
      "referenceID" : 28,
      "context" : "Most previous work on finding policies for Markov games takes the prescriptive view of multiagent learning [26]: that is, it attempts to answer “what should each agent do?” Several algorithms and analyses have been developed for the two-player zero-sum case [22, 27, 28, 29, 30].",
      "startOffset" : 258,
      "endOffset" : 278
    }, {
      "referenceID" : 29,
      "context" : "Most previous work on finding policies for Markov games takes the prescriptive view of multiagent learning [26]: that is, it attempts to answer “what should each agent do?” Several algorithms and analyses have been developed for the two-player zero-sum case [22, 27, 28, 29, 30].",
      "startOffset" : 258,
      "endOffset" : 278
    }, {
      "referenceID" : 30,
      "context" : "The generalsum case is significantly more challenging [31], and algorithms either have strong assumptions or need to either track several different potential equilibria per agent [32, 33], model other players to simplify the problem [34], or must find a Figure 3: Left: Gathering.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 31,
      "context" : "The generalsum case is significantly more challenging [31], and algorithms either have strong assumptions or need to either track several different potential equilibria per agent [32, 33], model other players to simplify the problem [34], or must find a Figure 3: Left: Gathering.",
      "startOffset" : 179,
      "endOffset" : 187
    }, {
      "referenceID" : 32,
      "context" : "The generalsum case is significantly more challenging [31], and algorithms either have strong assumptions or need to either track several different potential equilibria per agent [32, 33], model other players to simplify the problem [34], or must find a Figure 3: Left: Gathering.",
      "startOffset" : 179,
      "endOffset" : 187
    }, {
      "referenceID" : 33,
      "context" : "The generalsum case is significantly more challenging [31], and algorithms either have strong assumptions or need to either track several different potential equilibria per agent [32, 33], model other players to simplify the problem [34], or must find a Figure 3: Left: Gathering.",
      "startOffset" : 233,
      "endOffset" : 237
    }, {
      "referenceID" : 34,
      "context" : "cyclic strategy composed of several policies obtained through multiple state space sweeps [35].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 35,
      "context" : "Researchers have also studied the emergence of multi-agent coordination in the decentralized, partially observable MDP framework [36, 37, 38].",
      "startOffset" : 129,
      "endOffset" : 141
    }, {
      "referenceID" : 36,
      "context" : "Researchers have also studied the emergence of multi-agent coordination in the decentralized, partially observable MDP framework [36, 37, 38].",
      "startOffset" : 129,
      "endOffset" : 141
    }, {
      "referenceID" : 37,
      "context" : "Researchers have also studied the emergence of multi-agent coordination in the decentralized, partially observable MDP framework [36, 37, 38].",
      "startOffset" : 129,
      "endOffset" : 141
    }, {
      "referenceID" : 12,
      "context" : ", [13, 15], rather than on designing new learning algorithms.",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 14,
      "context" : ", [13, 15], rather than on designing new learning algorithms.",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 38,
      "context" : "It is well-known that the resulting “local decision process” could be non-Markovian from each agent’s perspective [39].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 24,
      "context" : "We use deep reinforcement learning as the basis for each agent in part because of its recent success with solving complex problems [25, 40].",
      "startOffset" : 131,
      "endOffset" : 139
    }, {
      "referenceID" : 39,
      "context" : "We use deep reinforcement learning as the basis for each agent in part because of its recent success with solving complex problems [25, 40].",
      "startOffset" : 131,
      "endOffset" : 139
    }, {
      "referenceID" : 40,
      "context" : "Also, temporal difference predictions have been observed in the brain [41] and this class of reinforcement learning algorithm is seen as a candidate theory of animal habit-learning [42].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 41,
      "context" : "Also, temporal difference predictions have been observed in the brain [41] and this class of reinforcement learning algorithm is seen as a candidate theory of animal habit-learning [42].",
      "startOffset" : 181,
      "endOffset" : 185
    }, {
      "referenceID" : 42,
      "context" : "Modern deep reinforcement learning methods take the perspective of an agent that must learn to maximize its cumulative long-term reward through trial-and-error interactions with its environment [43, 44].",
      "startOffset" : 194,
      "endOffset" : 202
    }, {
      "referenceID" : 43,
      "context" : "Modern deep reinforcement learning methods take the perspective of an agent that must learn to maximize its cumulative long-term reward through trial-and-error interactions with its environment [43, 44].",
      "startOffset" : 194,
      "endOffset" : 202
    }, {
      "referenceID" : 24,
      "context" : "See [25] for details in the single agent case.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 44,
      "context" : "This is a“growing batch”approach to reinforcement learning in the sense of [45].",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 24,
      "context" : "The network representing the function Q is trained through gradient descent on the mean squared Bellman residual with the expectation taken over transitions uniformly sampled from the batch (see [25]).",
      "startOffset" : 195,
      "endOffset" : 199
    }, {
      "referenceID" : 23,
      "context" : "In principle, this restriction could be dropped through the use of planning-based reinforcement learning methods like those of [24].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 24,
      "context" : "[25].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 1,
      "context" : "In particular, Social Psychology has advanced various hypotheses concerning psychological variables that may influence cooperation and give rise to the observed individual differences in human cooperative behavior in laboratory-based social dilemmas [2].",
      "startOffset" : 250,
      "endOffset" : 253
    }, {
      "referenceID" : 45,
      "context" : "These factors include consideration-of-futureconsequences [46], trust [47], affect (interestingly, it is negative emotions that turn out to promote cooperation [48]), and a personality variable called social value orientation characterized by other-regarding-preferences.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 46,
      "context" : "These factors include consideration-of-futureconsequences [46], trust [47], affect (interestingly, it is negative emotions that turn out to promote cooperation [48]), and a personality variable called social value orientation characterized by other-regarding-preferences.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 47,
      "context" : "These factors include consideration-of-futureconsequences [46], trust [47], affect (interestingly, it is negative emotions that turn out to promote cooperation [48]), and a personality variable called social value orientation characterized by other-regarding-preferences.",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 48,
      "context" : "The latter has been studied in a similar Markov game social dilemma setup to our SSD setting by [49].",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 49,
      "context" : "Recall also that DQN is in the class of reinforcement learning algorithms that is generally considered to be the leading candidate theory of animal habit-learning [50, 42].",
      "startOffset" : 163,
      "endOffset" : 171
    }, {
      "referenceID" : 41,
      "context" : "Recall also that DQN is in the class of reinforcement learning algorithms that is generally considered to be the leading candidate theory of animal habit-learning [50, 42].",
      "startOffset" : 163,
      "endOffset" : 171
    }, {
      "referenceID" : 23,
      "context" : ", [24].",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 50,
      "context" : "Notice that several of the examples in Schelling’s seminal book Micromotives and Macrobehavior [51] can be seen as temporally extended social dilemmas for which policies have been learned over the course of repeated interaction, including the famous opening example of lecture hall seating behavior.",
      "startOffset" : 95,
      "endOffset" : 99
    } ],
    "year" : 2017,
    "abstractText" : "Matrix games like Prisoner’s Dilemma have guided research on social dilemmas for decades. However, they necessarily treat the choice to cooperate or defect as an atomic action. In real-world social dilemmas these choices are temporally extended. Cooperativeness is a property that applies to policies, not elementary actions. We introduce sequential social dilemmas that share the mixed incentive structure of matrix game social dilemmas but also require agents to learn policies that implement their strategic intentions. We analyze the dynamics of policies learned by multiple self-interested independent learning agents, each using its own deep Qnetwork, on two Markov games we introduce here: 1. a fruit Gathering game and 2. a Wolfpack hunting game. We characterize how learned behavior in each domain changes as a function of environmental factors including resource abundance. Our experiments show how conflict can emerge from competition over shared resources and shed light on how the sequential nature of real world social dilemmas affects cooperation.",
    "creator" : "LaTeX with hyperref package"
  }
}