{
  "name" : "1411.4116.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Investigating the Role of Prior Disambiguation in Deep-learning Compositional Models of Meaning",
    "authors" : [ "Jianpeng Cheng" ],
    "emails" : [ "jianpeng.cheng@cs.ox.ac.uk", "kartsak@cs.ox.ac.uk", "etg@google.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n41 1.\n41 16\nv1 [\ncs .C\nL ]\n1 5\nN ov"
    }, {
      "heading" : "1 Introduction",
      "text" : "While distributed representations of meaning began largely at the word level, the need for representations of the semantics of larger units of text, from phrases to entire documents, is evident. Early attempts at compositionality in distributed representations used fixed algebraic operations such as vector addition and component-wise multiplication [11] to obtain semantic representations of larger units of text from their constitutent representations. More recently, models represented relational words as tensors of various orders and tensor contraction was adopted as the mean of composition [1, 3, 4]. Alongside these principally multi-linear methods of composition, we are observing an emergence of non-linear neural network-based compositional approaches, which derive a sentence vector by recursively applying neural networks to each pair of word vectors [15, 16, 6].\nAlthough their levels of sophistication vary, all compositional methods for distributed semantics share the same problem: they take ambiguous word vectors as input, where each token is represented by a single vector regardless of the actual number of senses that the word has, and of the context in which it appears. To solve this problem, Reddy et al. [14] propose to disambiguate each word vector before composition for simple additive and multiplicative compositional models. This idea has now been successfully tested on a series of multi-linear compositional distributional models by Kartsaklis and colleagues [9, 10, 8]. In this paper we move one step further, by evaluating the effectiveness of a prior disambiguation step on neural compositional models. In Section 2 we discuss the models evaluated in this paper, followed by a description of the disambiguation procedure of [9], adapted to these models, in Section 3. Experimental evaluation of this procedure is presented and discussed in Sections 4–5. We conclude in Section 6 that prior disambiguation has a positive effect on neural models of composition."
    }, {
      "heading" : "2 Neural networks for composing meaning",
      "text" : "Deep learning algorithms are capable of modelling complex relationships between inputs and outputs in NLP [16, 17, 6]. In this paper we are interested in capturing the meaning of a sentence by composing the vectorial representations of the words therein. In the most generic form of such a composition, a neural network is applied to each pair of words w1 and w2:\n−→v = f(W[−→w1 : −→w2] + b) (1)\nwhere [−→w1 : −→w2] denotes the concatenation of the two vectors assigned to the words, W and b are model parameters, and f is a non-linear activation function. The compositional result, −→v , is a vector representing the meaning of the bigram, and can be used again as an input to compute the representation of a larger text constituent in a recursive fashion. This process continues until all vectors of the words in a sentence have been merged into a single vectorial representation, which will serve as a semantic representation of that sentence or phrase. This class of models is known as recursive neural networks (RecNNs) [15].\nIn a variation of the above structure, each intermediate composition is performed via an autoencoder, instead of a feed-forward network. A recursive auto-encoder (RAE) [16, 6] learns to reconstruct the input, encoded via a hidden layer, as faithfully as possible. The state of hidden layer of the RAE can be used as a compressed representation of the two original inputs. Since the optimization is based on the reconstruction error, a RAE is trained in an unsupervised fashion."
    }, {
      "heading" : "3 Combining NNs with context-based word sense disambiguation",
      "text" : "The usual practice in deep learning models of meaning is to use as inputs ambiguous word representations, in which all possible meanings of a token are merged into a single vector. In this paper we evaluate a new methodology, in which each input word is associated with a set of vectors, each representing different meanings of the word in the training corpus. As input to the compositional network, we select the most probable meaning vector for each word given its context.\nOur general methodology essentially recasts the approach of [9] in a deep learning setting; this is depicted in Figure 1. We first use a word sense induction step in order to discover the latent senses of each target word. For every occurrence of a target word wt in the corpus, we calculate a context vector as the average of its neighbours, that is, −→ct = 1n ( −→w1 + −→w2 + · · · + −→wn) where −→wj is the distributional (ambiguous) vector of the jth neighbour. After creating all the context vectors for the target word, we apply hierarchical agglomerative clustering to them in order to discover sensible groupings that hopefully correspond to different meanings of the word. As a vectorial representation for each meaning cluster, we use its centroid. Up to this point, each target word wt is associated to an ambiguous vector −→wt and a set of 3 meaning vectors S.\nAssuming now an arbitrary word in some context C, we can select the most probable meaning vector for that word by creating a context vector −→ct for C as before (i.e. by averaging all the other words in C), and choosing the meaning vector which is the closest to −→ct . For a set of meaning vectors S and a distance metric d(−→v ,−→u ), this is given as:\n−̂→v = argmin −→v ∈S d(−→v ,−→c ) (2)\nOther works that combine NNs with WSD, but not in a compositional setting as here, are [7, 13]."
    }, {
      "heading" : "4 Experiments",
      "text" : "In order to test the effect of prior disambiguation on deep learning compositional models, we disambiguate the constituent words of simple sentences of the form subject-verb-object and verb phrases verb-object before composition in a number of tasks. Furthermore, we evaluate two disambiguation strategies: in the first we disambiguate every word in a sentence, while in the second disambiguation applies only to verbs, which are usually the most ambiguous part of language. We evaluate the quality of the compositional results by measuring the similarity between sentence vectors—a good compositional model should be able to construct sentence vectors that reflect the true semantic relationships among the sentences.\nTowards this purpose we use three phrase similarity datasets from the work of Grefenstette and Sadrzadeh [5] (G&S), Kartsaklis et al. [10] (K&S) and Mitchell and Lapata [12] (M&L), consisting of pairs of sentences or phrases annotated with similarity scores by human evaluators. Our task is to measure to what extent the similarity computed by the composite vectors matches that of human judgements. In the first two datasets, which are based on subject-verb-object structures, each pair of sentences is constructed around ambiguous verbs, while subject and object nouns are the same for the two sentences. The two datasets differ in the way ambiguous verbs were selected (in the former the selection was done automatically, while in the latter by humans), and in the fact that in the K&S dataset every noun is modified by an appropriate adjective. For the M&L dataset (comprised of verb-object constructs) word ambiguity does not play a specific role, so from this aspect this dataset constitutes a more natural evaluation test for our models “in the wild”.\nIn terms of neural composition models, we implement a RecNN and a RAE. Furthermore, we use simple additive and multiplicative models as baselines, where the representation of a sentence is derived by summing up the word vectors or taking the component-wise multiplication of them.\nFor each dataset and each model, the evaluation is conducted in two ways. First, we measure the Spearman’s correlation between the computed cosine similarities of the composite sentence vectors and the corresponding human scores. Second, we apply a more relaxed evaluation, based on a binary classification task. Specifically, we use the human score that corresponds to each pair of sentences in order to decide a label for that pair (1 if the two sentences are highly similar and 0 otherwise), and we use the training set that results in from this procedure as input to a logistics regression classifier. We report the 4-fold cross validation accuracy as a measure of the matching rate. The results for each dataset and experiment are listed in the Tables 1–3."
    }, {
      "heading" : "5 Discussion",
      "text" : "The results are quite promising, since they suggest that disambiguation as an extra step prior to composition can bring at least marginal benefits to deep learning compositional models. Comparing the numbers we got from the three datasets, the effect of disambiguation is clearest for the M&L dataset. In both evaluations we carried out, disambiguation has a positive effect for the subsequent composition. This is very encouraging, since the words in this dataset were not chosen to be ambiguous on purpose. In other words, the results imply that for a generic sentence prior disambiguation can act as a useful pre-processing step, which might improve the final outcome (if the sentence has ambiguous words) or not (if all words are unambiguous), but never decrease the performance.\nThe effect of disambiguation seems also to be quite clear for the K&S dataset, whereas the result for the G&S dataset, although positive, is less definite. We speculate that the reason behind this difference is the way each dataset was constructed: for example, the K&S dataset contains verbs and alternatives meanings of them that correspond to distinct homonymous cases, e.g. such as verb ‘file’ with alternative meanings ‘register’ and ‘smooth’. On the other hand, the G&S dataset contains many polysemous cases, where there exist very slight variations in the senses of verbs, such as between the verb ‘write’ and the alternative meanings ‘spell’ and ‘publish’.\nIn terms of the comparison between deep learning models and algebraic baselines, the results are not very clear. Despite the well-known benefits of the deep learning methods in natural language processing, this work suggests for one more time that simple component-wise compositional operators might constitute a hard-to-beat baseline for certain tasks: Although the two deep learning models in general returned superior results for the second evaluation task, they could not beat the additive approach in the Spearman’s correlation measure. In fact, similar findings have been reported previously in the study of Blacoe and Lapata [2] and Kartsaklis et al. [9]. However, when trying to interpret the effectiveness of the two approaches, we need to consider a generic scenario in which sentences and phrases are not restricted to a fixed length and structure. Obviously, the advantage of deep learning models would be more significant when dealing with longer text segments."
    }, {
      "heading" : "6 Conclusion",
      "text" : "The main contribution of this paper is that it suggests that explicitly dealing with the issue of disambiguation can be an effective way to improve the performance of deep learning compositional models of meaning. For our simple approach of adding a prior disambiguation step to word vectors, the benefits are small. A reasonable future direction, then, would be to incorporate an explicit disambiguation step within the architecture of the compositional model, that deals with ambiguity during the training process itself. The current work indicates that such an approach, which is much more aligned with the concept of deep learning, could result in drastic improvements in the performance of a compositional model."
    } ],
    "references" : [ {
      "title" : "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space",
      "author" : [ "M. Baroni", "R. Zamparelli" ],
      "venue" : "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2010
    }, {
      "title" : "A comparison of vector-based representations for semantic composition",
      "author" : [ "W. Blacoe", "M. Lapata" ],
      "venue" : "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2012
    }, {
      "title" : "Mathematical foundations for a compositional distributional model of meaning",
      "author" : [ "B. Coecke", "M. Sadrzadeh", "S. Clark" ],
      "venue" : "arXiv preprint arXiv:1003.4394,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2010
    }, {
      "title" : "Category-Theoretic Quantitative Compositional Distributional Models of Natural Language Semantics",
      "author" : [ "E. Grefenstette" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2013
    }, {
      "title" : "Experimental support for a categorical compositional distributional model of meaning",
      "author" : [ "E. Grefenstette", "M. Sadrzadeh" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2011
    }, {
      "title" : "The Role of Syntax in Vector Space Models of Compositional Semantics. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
      "author" : [ "K.M. Hermann", "P. Blunsom" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "Improving word representations via global context and multiple word prototypes",
      "author" : [ "E.H. Huang", "R. Socher", "C.D. Manning", "A.Y. Ng" ],
      "venue" : "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2012
    }, {
      "title" : "Resolving lexical ambiguity in tensor regression models of meaning",
      "author" : [ "D. Kartsaklis", "N. Kalchbrenner", "M. Sadrzadeh" ],
      "venue" : "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Vol. 2: Short Papers),",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2014
    }, {
      "title" : "Prior disambiguation of word tensors for constructing sentence vectors",
      "author" : [ "D. Kartsaklis", "M. Sadrzadeh" ],
      "venue" : "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2013
    }, {
      "title" : "Separating disambiguation from composition in distributional semantics",
      "author" : [ "D. Kartsaklis", "M. Sadrzadeh", "S. Pulman" ],
      "venue" : "In Proceedings of 17th Conference on Natural Language Learning (CoNLL),",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2013
    }, {
      "title" : "Vector-based models of semantic composition",
      "author" : [ "J. Mitchell", "M. Lapata" ],
      "venue" : "In ACL,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2008
    }, {
      "title" : "Composition in distributional models of semantics",
      "author" : [ "J. Mitchell", "M. Lapata" ],
      "venue" : "Cognitive science,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2010
    }, {
      "title" : "Efficient non-parametric estimation of multiple embeddings per word in vector space",
      "author" : [ "A. Neelakantan", "J. Shankar", "A. Passos", "A. McCallum" ],
      "venue" : "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Dynamic and static prototype vectors for semantic composition",
      "author" : [ "S. Reddy", "I.P. Klapaftis", "D. McCarthy", "S. Manandhar" ],
      "venue" : "In IJCNLP,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2011
    }, {
      "title" : "Learning continuous phrase representations and syntactic parsing with recursive neural networks",
      "author" : [ "R. Socher", "C.D. Manning", "A.Y. Ng" ],
      "venue" : "In Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2010
    }, {
      "title" : "Semi-supervised recursive autoencoders for predicting sentiment distributions",
      "author" : [ "R. Socher", "J. Pennington", "E.H. Huang", "A.Y. Ng", "C.D. Manning" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2011
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "R. Socher", "A. Perelygin", "J.Y. Wu", "J. Chuang", "C.D. Manning", "A.Y. Ng", "C. Potts" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "Early attempts at compositionality in distributed representations used fixed algebraic operations such as vector addition and component-wise multiplication [11] to obtain semantic representations of larger units of text from their constitutent representations.",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 0,
      "context" : "More recently, models represented relational words as tensors of various orders and tensor contraction was adopted as the mean of composition [1, 3, 4].",
      "startOffset" : 142,
      "endOffset" : 151
    }, {
      "referenceID" : 2,
      "context" : "More recently, models represented relational words as tensors of various orders and tensor contraction was adopted as the mean of composition [1, 3, 4].",
      "startOffset" : 142,
      "endOffset" : 151
    }, {
      "referenceID" : 3,
      "context" : "More recently, models represented relational words as tensors of various orders and tensor contraction was adopted as the mean of composition [1, 3, 4].",
      "startOffset" : 142,
      "endOffset" : 151
    }, {
      "referenceID" : 14,
      "context" : "Alongside these principally multi-linear methods of composition, we are observing an emergence of non-linear neural network-based compositional approaches, which derive a sentence vector by recursively applying neural networks to each pair of word vectors [15, 16, 6].",
      "startOffset" : 256,
      "endOffset" : 267
    }, {
      "referenceID" : 15,
      "context" : "Alongside these principally multi-linear methods of composition, we are observing an emergence of non-linear neural network-based compositional approaches, which derive a sentence vector by recursively applying neural networks to each pair of word vectors [15, 16, 6].",
      "startOffset" : 256,
      "endOffset" : 267
    }, {
      "referenceID" : 5,
      "context" : "Alongside these principally multi-linear methods of composition, we are observing an emergence of non-linear neural network-based compositional approaches, which derive a sentence vector by recursively applying neural networks to each pair of word vectors [15, 16, 6].",
      "startOffset" : 256,
      "endOffset" : 267
    }, {
      "referenceID" : 13,
      "context" : "[14] propose to disambiguate each word vector before composition for simple additive and multiplicative compositional models.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 8,
      "context" : "This idea has now been successfully tested on a series of multi-linear compositional distributional models by Kartsaklis and colleagues [9, 10, 8].",
      "startOffset" : 136,
      "endOffset" : 146
    }, {
      "referenceID" : 9,
      "context" : "This idea has now been successfully tested on a series of multi-linear compositional distributional models by Kartsaklis and colleagues [9, 10, 8].",
      "startOffset" : 136,
      "endOffset" : 146
    }, {
      "referenceID" : 7,
      "context" : "This idea has now been successfully tested on a series of multi-linear compositional distributional models by Kartsaklis and colleagues [9, 10, 8].",
      "startOffset" : 136,
      "endOffset" : 146
    }, {
      "referenceID" : 8,
      "context" : "In Section 2 we discuss the models evaluated in this paper, followed by a description of the disambiguation procedure of [9], adapted to these models, in Section 3.",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 15,
      "context" : "Deep learning algorithms are capable of modelling complex relationships between inputs and outputs in NLP [16, 17, 6].",
      "startOffset" : 106,
      "endOffset" : 117
    }, {
      "referenceID" : 16,
      "context" : "Deep learning algorithms are capable of modelling complex relationships between inputs and outputs in NLP [16, 17, 6].",
      "startOffset" : 106,
      "endOffset" : 117
    }, {
      "referenceID" : 5,
      "context" : "Deep learning algorithms are capable of modelling complex relationships between inputs and outputs in NLP [16, 17, 6].",
      "startOffset" : 106,
      "endOffset" : 117
    }, {
      "referenceID" : 14,
      "context" : "This class of models is known as recursive neural networks (RecNNs) [15].",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 15,
      "context" : "A recursive auto-encoder (RAE) [16, 6] learns to reconstruct the input, encoded via a hidden layer, as faithfully as possible.",
      "startOffset" : 31,
      "endOffset" : 38
    }, {
      "referenceID" : 5,
      "context" : "A recursive auto-encoder (RAE) [16, 6] learns to reconstruct the input, encoded via a hidden layer, as faithfully as possible.",
      "startOffset" : 31,
      "endOffset" : 38
    }, {
      "referenceID" : 8,
      "context" : "Our general methodology essentially recasts the approach of [9] in a deep learning setting; this is depicted in Figure 1.",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 6,
      "context" : "Other works that combine NNs with WSD, but not in a compositional setting as here, are [7, 13].",
      "startOffset" : 87,
      "endOffset" : 94
    }, {
      "referenceID" : 12,
      "context" : "Other works that combine NNs with WSD, but not in a compositional setting as here, are [7, 13].",
      "startOffset" : 87,
      "endOffset" : 94
    }, {
      "referenceID" : 4,
      "context" : "Towards this purpose we use three phrase similarity datasets from the work of Grefenstette and Sadrzadeh [5] (G&S), Kartsaklis et al.",
      "startOffset" : 105,
      "endOffset" : 108
    }, {
      "referenceID" : 9,
      "context" : "[10] (K&S) and Mitchell and Lapata [12] (M&L), consisting of pairs of sentences or phrases annotated with similarity scores by human evaluators.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[10] (K&S) and Mitchell and Lapata [12] (M&L), consisting of pairs of sentences or phrases annotated with similarity scores by human evaluators.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 1,
      "context" : "In fact, similar findings have been reported previously in the study of Blacoe and Lapata [2] and Kartsaklis et al.",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 8,
      "context" : "[9].",
      "startOffset" : 0,
      "endOffset" : 3
    } ],
    "year" : 2014,
    "abstractText" : "This paper aims to explore the effect of prior disambiguation on neural networkbased compositional models, with the hope that better semantic representations for text compounds can be produced. We disambiguate the input word vectors before they are fed into a compositional deep net. A series of evaluations shows the positive effect of prior disambiguation for such deep models.",
    "creator" : "LaTeX with hyperref package"
  }
}