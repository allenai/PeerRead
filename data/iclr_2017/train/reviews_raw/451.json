{"conference": "ICLR 2017 conference submission", "title": "Topology and Geometry of Half-Rectified Network Optimization", "abstract": "The loss surface of deep neural networks has recently attracted interest  in the optimization and machine learning communities as a prime example of  high-dimensional non-convex problem. Some insights were recently gained using spin glass  models and mean-field approximations, but at the expense of strongly simplifying the nonlinear nature of the model.  In this work, we do not make any such approximation and study conditions  on the data distribution and model architecture that prevent the existence  of bad local minima. Our theoretical work quantifies and formalizes two  important folklore facts: (i) the landscape of deep linear networks has a radically different topology  from that of deep half-rectified ones, and (ii) that the energy landscape  in the non-linear case is fundamentally controlled by the interplay between the smoothness of the data distribution and model over-parametrization. Our main theoretical contribution is to prove that half-rectified single layer networks are asymptotically connected, and we provide explicit bounds that reveal the aforementioned interplay.  The conditioning of gradient descent is the next challenge we address.  We study this question through the geometry of the level sets, and we introduce an algorithm to efficiently estimate the regularity of such sets on large-scale networks.  Our empirical results show that these level sets remain connected throughout  all the learning phase, suggesting a near convex behavior, but they become  exponentially more curvy as the energy level decays, in accordance to what is observed in practice with  very low curvature attractors.", "reviews": [{"is_meta_review": true, "comments": "This work contributes to understanding the landscape of deep networks in terms of its topology and geometry. The paper analyzes the former theoretically, and studies the latter empirically. Although the provided contributions are very specific (ReLU nets with single hidden layer, and a heuristic to calculate the normalized geodesic), the results are original and of interest. Thus, they could potentially be used as stepping stones for deeper developments in this area.\n\nPros:\n1. Providing new theory about existence of \"poor\" local minima for ReLU networks with a hidden unit that relies on input distribution properties as well as the size of the hidden layer.\n2. Coming up with a heuristic algorithm to compute the normalized geodesic between two solution points. The latter reflects how curved the path between the two is. \n\nCons:\nThe results are very specific in both topology and geometry analysis.\n1. The analysis is performed only over a \"single\" hidden layer ReLU network. Given the importance of depth in deep architectures, this result cannot really explain the kinds of architectures we are interested in practically. \n2. The normalized geodesic criterion is somewhat limited in representing how easy it is to connect two equally good points. For example, there might exist a straight line between the two (which is considered as easy by the geodesic criterion), but this line might be going through a very narrow valley, challenging gradient based optimization algorithms (and thus extremely difficult to navigate in practice). In addition, the proposed algorithm for computing the normalized geodesic is a greedy heuristic, which as far as I can tell, makes it difficult to know how we can trust in the estimated geodesics obtained by this algorithm.\n\nWith all cons said, I stress that I understand both problems tackled in the paper are challenging, and thus I find the contributions valuable and interesting.", "IS_META_REVIEW": true}, {"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "The paper presents an analysis of deep ReLU networks, and contrasts it with linear networks. It makes good progress towards providing a theoretical explanation of the difficult problem of characterizing the critical points of this highly nonconvex function. \n\nI agree with the authors that their approach is superior to earlier works based on spin-glass models or other approximations that make highly unrealistic assumptions. The ReLU structure allows them to provide concrete analysis, without making such approximations, as opposed to more general activation units.\n\nThe relationship between smoothness of data distribution and the level of model overparamterization is used to characterize the ability to reach the global optimum. The intuition that having no approximation error (due to overparameterization) leads to more tractable optimization problem is intuitive. Such findings have been seen before (e.g. in tensor decomposition, the global optima can be reached when the amount of noise is small). I recommend the authors to connect it to such findings for other nonconvex problems.  The paper does not address overfitting, as they themselves point out in the conclusion. However, I do expect it to be a very challenging problem.", "IS_META_REVIEW": false, "DATE": "06 Feb 2017 (modified: 09 Feb 2017)", "is_meta_review": false}, {"TITLE": "final evaluation - strong rejection", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "I'd like to sincerely thank all the authors for clarifications. Unfortunately I still think that it is a strong reject paper. Authors' responses did not solve main problems that I pointed out. This paper is badly written, the results are incremental and presented in the unclear way. The proofs are presented in such a way that I cannot really verify whether proposed theoretical claims (which to the best of my understanding constitute the core of the paper) are correct even though I consider myself to be very familiar with the mathematical tools used here.\nThe authors should consider submitting this paper to a journal, but after a major revision.", "IS_META_REVIEW": false, "DATE": "20 Jan 2017", "is_meta_review": false}, {"TITLE": "comparison with previous results", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_META_REVIEW": false, "DATE": "19 Dec 2016", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "incremental result on the loss surface of deep neural networks", "is_meta_review": false, "comments": "This is an incremental result (several related results that the authors of the paper mentioned here were already published). The authors claim that they can get rid of the technical assumptions from the previous papers but the results they propose are significantly weaker and also quite technical. The main theoretical result - Theorem 2.4 is not convincing at all. Furthermore, the paper is badly written. No theoretical intuition is given, the experimental section is weak and in some places the formatting is wrong.", "IS_META_REVIEW": false, "RECOMMENDATION": 2, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Interesting analysis", "is_meta_review": false, "comments": "This paper studies the energy landscape of the loss function in neural networks.  It is generally clearly written and nicely provides intuitions for the results.  One main contribution is to show that the level sets of the loss becomes connected as the network is increasingly overparameterized.  It also quantifies, in a way, the degree of disconnectedness possible in terms of the increase in loss that one must allow to find a connected path.  It would seem that this might have some implications for the likelihood of escaping local minima with stochastic gradient descent.  The paper also presents a simple algorithm for finding geodesic paths between two networks such that the loss is decreasing along the path.  Using this they show that the loss seems to become more nonconvex when the loss is smaller.  This is also quite interesting.\n\nThe work does have some significant limitations, which is not surprising given the difficulty of fully analyzing the network loss function.  However, the authors are quite clear about these limitations, which especially include not yet analyzing deep networks and analyzing only the oracle loss, and not the empirical loss.  I would have also appreciated a little more practical discussion of the bound in Theorem 2.4.  It is hard to tell whether this bound is tight enough to be practically relevant.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Insightful Results", "is_meta_review": false, "comments": "This work contributes to understanding the landscape of deep networks in terms of its topology and geometry. The paper analyzes the former theoretically, and studies the latter empirically. Although the provided contributions are very specific (ReLU nets with single hidden layer, and a heuristic to calculate the normalized geodesic), the results are original and of interest. Thus, they could potentially be used as stepping stones for deeper developments in this area.\n\nPros:\n1. Providing new theory about existence of \"poor\" local minima for ReLU networks with a hidden unit that relies on input distribution properties as well as the size of the hidden layer.\n2. Coming up with a heuristic algorithm to compute the normalized geodesic between two solution points. The latter reflects how curved the path between the two is. \n\nCons:\nThe results are very specific in both topology and geometry analysis.\n1. The analysis is performed only over a \"single\" hidden layer ReLU network. Given the importance of depth in deep architectures, this result cannot really explain the kinds of architectures we are interested in practically. \n2. The normalized geodesic criterion is somewhat limited in representing how easy it is to connect two equally good points. For example, there might exist a straight line between the two (which is considered as easy by the geodesic criterion), but this line might be going through a very narrow valley, challenging gradient based optimization algorithms (and thus extremely difficult to navigate in practice). In addition, the proposed algorithm for computing the normalized geodesic is a greedy heuristic, which as far as I can tell, makes it difficult to know how we can trust in the estimated geodesics obtained by this algorithm.\n\nWith all cons said, I stress that I understand both problems tackled in the paper are challenging, and thus I find the contributions valuable and interesting.", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Switching from matrix weights to vector weights?", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "is_meta_review": false}, {"TITLE": "Overparameterization", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "is_meta_review": false}, {"TITLE": "ICLR Paper Format", "OTHER_KEYS": "Tara N Sainath", "comments": "Dear Authors,\n\nPlease resubmit your paper in the ICLR 2017 format with the correct marging spacing for your submission to be considered. Thank you!", "IS_META_REVIEW": false, "DATE": "07 Nov 2016", "is_meta_review": false}], "SCORE": 8, "authors": "C. Daniel Freeman, Joan Bruna", "KEYWORDS": "We provide theoretical, algorithmical and experimental results concerning the optimization landscape of deep neural networks", "accepted": true, "id": ""}
