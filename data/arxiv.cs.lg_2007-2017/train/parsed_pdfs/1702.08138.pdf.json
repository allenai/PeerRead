{
  "name" : "1702.08138.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Deceiving Google’s Perspective API Built for Detecting Toxic Comments",
    "authors" : [ "Hossein Hosseini", "Sreeram Kannan", "Baosen Zhang" ],
    "emails" : [ "rp3}@uw.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 2.\n08 13\n8v 1\n[ cs\n.L G\n] 2\n7 Fe\nb 20\n17\nIn this paper, we propose an attack on the Perspective toxic detection system based on the adversarial examples. We show that an adversary can subtly modify a highly toxic phrase in a way that the system assigns significantly lower toxicity score to it. We apply the attack on the sample phrases provided in the Perspective website and show that we can consistently reduce the toxicity scores to the level of the non-toxic phrases. The existence of such adversarial examples is very harmful for toxic detection systems and seriously undermines their usability.\nI. INTRODUCTION\nSocial media platforms provide an environment where people can learn about the trends and news, freely share their opinions and engage in discussions. Unfortunately, the lack of a moderating entity in these platforms has caused several problems, ranging from the wide spread of fake news to online harassment [2]. Due to the growing concern about the impact of online harassment on the people’s experience of the Internet, many platforms are taking steps to enhance the safety of the online environments [3], [4].\nSome of the platforms employ approaches such as refining the information based on crowdsourcing (upvotes/downvotes), turning off comments or manual moderation to mitigate the effect of the inappropriate contents [5]. These approaches however are inefficient and not scalable. As a result, there has been many calls for researchers to develop methods to automatically detect abusive or toxic context in the real time [6].\nRecent advances in machine learning have transformed many domains such as computer vision [7], speech recognition [8], and language processing [9]. Many researchers have explored using machine learning to also tackle the problem of online harassment. Recently, Google and Jigsaw launched a project called Perspective [1], which uses machine learning to automatically detect online insults, harassment, and abusive speech. The system intends to bring Conversation\nThis work was supported by ONR grants N00014-14-1-0029 and N0001416-1-2710, ARO grant W911NF-16-1-0485 and NSF grant CNS-1446866.\nAI to help with providing a safe environment for online discussions [10].\nPerspective is an API that enables the developers to use the toxic detector running on Google’s servers, to identify harassment and abuse on social media or more efficiently filtering invective from the comments on a news website. Jigsaw has partnered with online communities and publishers, such as Wikipedia [3] and The New York Times [11], to implement this toxicity measurement system.\nRecently, a demonstration website has been launched, which allows anyone to type a phrase in the Perspective’s interface and instantaneously see how it rates on the “toxicity” scale [1]. The Perspective website has also open sourced the experiments, models and research data in order to explore the strengths and weaknesses of using machine learning as a tool for online discussion.\nThe implicit assumption of learning models is that they will be deployed in benign settings. However, many works have pointed out their vulnerability in adversarial scenarios [12]– [14]. One type of the vulnerabilities of machine learning algorithms is that an adversary can change the algorithm output by subtly perturbing the input, often unnoticeable by humans. Such inputs are called adversarial examples [15], and have been shown to be effective against different machine learning algorithms even when the adversary has only a blackbox access to the target model [16].\nIn this paper, we demonstrate the vulnerability of the recently-released Google’s Perspective system against the adversarial examples. In the text classification task of the Perspective, adversarial examples can be defined as modified texts which contain the same highly abusive language as the original text, yet receive a significantly lower toxicity score from the learning model. Through different experiments, we show that an adversary can deceive the system by misspelling the abusive words or by adding punctuations between the letters. The existence of adversarial examples is very harmful for toxic detector systems and seriously undermines their usability, especially since these systems are likely to be employed in adversarial settings. We conclude the paper by proposing some countermeasures to the proposed attack."
    }, {
      "heading" : "II. BACKGROUND",
      "text" : "A. Brief Description of Google’s Perspective API\nPerspective is an API created by Jigsaw and Google’s Counter Abuse Technology team in Conversation-AI. Conversation AI is a collaborative research effort exploring ML as a\ntool for better discussions online [17]. The API uses machine learning models to score the toxicity of an input text, where toxic is defined as “a rude, disrespectful, or unreasonable comment that is likely to make one leave a discussion.”\nGoogle and Jigsaw developed the measurement tool by taking millions of comments from different publishers and then asking panels of ten people to rate the comments on a scale from “very toxic” to “very healthy” contribution. The resulting judgments provided a large set of training examples for the machine learning model.\nJigsaw has partnered with online communities and publishers to implement the toxicity measurement system. Wikipedia use it to perform a study of its editorial discussion pages [3] and The New York Times is planning to use it as a first pass of all its comments, automatically flagging abusive ones for its team of human moderators [11]. The API outputs the scores in real-time, so that publishers can integrate it into their website to show toxicity ratings to commenters even during the typing [5]."
    }, {
      "heading" : "B. Adversarial Examples for Learning Systems",
      "text" : "Machine learning models are generally designed to yield the best performance on clean data and in benign settings. As a result, they are subject to attacks in adversarial scenarios [12]– [14]. One type of the vulnerabilities of the machine learning algorithms is that an adversary can change the algorithm prediction score by perturbing the input slightly, often unnoticeable by humans. Such inputs are called adversarial examples [15].\nAdversarial examples have been applied to models for different tasks, such as images classification [15], [18], [19], music content analysis [20] and malware classification [21]. In this work, we generate adversarial examples on a real-world text classifier system. In the context of scoring the toxicity, adversarial examples can be defined as modified phrases that contain the same highly abusive language as the original one, yet receive a significantly lower toxicity score by the model.\nIn a similar work [22], the authors presented a method for gender obfuscating in social media writing. The proposed\nmethod modifies the text such that the algorithm classifies the writer gender as a certain target gender, under limited knowledge of the classifier and while preserving the text’s fluency and meaning. The modified text is not required to be adversarial, i.e., a human may also classify it as the target gender. In contrast, in the application of toxic text detection, the adversary intends to deceive the classifier, while maintaining the abusive content of the text."
    }, {
      "heading" : "III. THE PROPOSED ATTACKS",
      "text" : "Recently, a website has been launched for Perspective demonstration, which allows anyone to type a phrase in the interface and instantaneously receive its toxicity score [1]. The website provides samples phrases for three categories of topics “that are often difficult to discuss online”. The categories are 1) Climate Change, 2) Brexit and 3) US Election.\nIn this section, we demonstrate an attack on the Perspective toxic detection system, based on the adversarial examples. In particular, we show that an adversary can subtly modify a toxic phrase such that the model will output a very low toxicity score for the modified phrase. The attack setting is as follows. The adversary possesses a phrase with a toxic content and tries different perturbations on the words, until she succeeds with significantly reducing the confidence of the model that the phrase is toxic. Note that the adversary does not have access to the model or training data, and can only query the model and get the toxicity score.\nTable I demonstrates the attack on sample phrases provided by the Perspective website. The first column represents the original phrases along with the toxicity scores and the second column provides the adversarially modified phrases and their corresponding toxicity scores. 1 For better demonstration of the attack, we chose phrases with different toxic words and also introduced different types of errors, rather than searching for the best error type that would potentially yield lower toxicity score. The boldface words are the toxic words that the adversary has modified. The modifications are adding a\n1The experiments are done on the interface of the Perspective website on Feb. 24, 2017.\ndot between two letters, adding spaces between all letters or misspelling the word (repeating one letter twice or swapping two letters). As can be seen, we can consistently reduce the toxicity score to the level of the benign phrases by subtly modifying the toxic words.\nMoreover, we observed that the adversarial perturbations transfer among different phrases, i.e., if a certain modification to a word reduces the toxicity score of a phrase, the same modification to the word is likely to reduce the toxicity score also for another phrase. Using this property, an adversary can form a dictionary of the adversarial perturbations for every word and significantly simplify the attack process.\nThrough the experiments, we made the following observa-\ntions:\n• Susceptibility to false alarm: we observed that the\nPerspective system also wrongly assigns high toxicity scores to the apparently benign phrases. Table II demonstrates the false alarm on the same sample phrases of Table I. The first column represents the original phrases along with the toxicity scores and the second column provides the negated phrases and the corresponding toxicity scores. The boldface words are added to toxic phrases. As can be seen, the system consistently fails to capture the inherent semantic of the modified phrases and wrongly assigns high toxicity scores to them. • Robustness to random misspellings: we observed that\nthe system assigns 34% toxicity score to most of the misspelled and random words. Also, it is somewhat robust to phrases that contain randomly modified toxic words. • Vulnerability to poisoning attack: The Perspective\ninterface allows users to provide a feedback on the toxicity score of phrases, suggesting that the learning algorithm updates itself using the new data. This can expose the system to poisoning attacks, where an adversary modifies the training data (in this case, the labels) so that the model assigns low toxicity scores to certain phrases."
    }, {
      "heading" : "IV. OPEN PROBLEMS IN DEFENSE METHODS",
      "text" : "The developers of Perspective have mentioned that the system is in the early days of research and development, and\nthat the experiments, models, and research data are published to explore the strengths and weaknesses of using machine learning as a tool for online discussion.\nIn section III, we showed the vulnerability of the Perspective system against the adversarial examples. Scoring the semantic toxicity of a phrase is clearly a very challenging task. In this following, we briefly review some of the possible approaches for improving the robustness of the toxic detection systems:\n• Adversarial Training: In this approach, during the training\nphase, we generate the adversarial examples and train the model to assign the original label to them [18]. In the context of toxic detection systems, we need to include different modified versions of the toxic words into the training data. While this approach may improve the robustness of the system against the adversarial examples, it does not seem practical to train the model on all variants of every word. • Spell checking: Many of the adversarial examples can be\ndetected by first applying a spell checking filter before the toxic detection system. This approach may however increase the false alarm. • Blocking suspicious users for a period of time: The\nadversary needs to try different error patterns to finally evade the toxic detection system. Once a user fails to pass the threshold for a number of times, the system can block her for a while. This approach can force the users to less often use toxic language."
    }, {
      "heading" : "V. CONCLUSION",
      "text" : "In this paper, we presented an attack on the recentlyreleased Google’s Perspective API built for detecting toxic comments. We showed that the system can be deceived by slightly perturbing the abusive phrases to receive very low toxicity scores, while preserving the intended meaning. We also showed that the system has high false alarm rate in scoring high toxicity to benign phrases. We provided detailed examples for the studied cases. Our future work includes development of countermeasures against such attacks.\nDisclaimer: The phrases used in Tables I and II are chosen from the examples provided in the Perspective website [1] for the purpose of demonstrating the results and do not represent the view or opinions of the authors or sponsoring agencies.\nREFERENCES\n[1] “https://www.perspectiveapi.com/,” [2] M. Duggan, Online harassment. Pew Research Center, 2014. [3] “https://meta.wikimedia.org/wiki/Research:Detox,” [4] “https://www.nytimes.com/interactive/2016/09/20/insider/approve-or-\nreject-moderation-quiz.html,” [5] “https://www.wired.com/2017/02/googles-troll-fighting-ai-now-belongs-\nworld/,” [6] E. Wulczyn, N. Thain, and L. Dixon, “Ex machina: Personal attacks\nseen at scale,” arXiv preprint arXiv:1610.08914, 2016. [7] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification\nwith deep convolutional neural networks,” in Advances in neural information processing systems, pp. 1097–1105, 2012. [8] G. E. Dahl, D. Yu, L. Deng, and A. Acero, “Context-dependent pretrained deep neural networks for large-vocabulary speech recognition,” IEEE Transactions on Audio, Speech, and Language Processing, vol. 20, no. 1, pp. 30–42, 2012. [9] R. Collobert and J. Weston, “A unified architecture for natural language processing: Deep neural networks with multitask learning,” in Proceedings of the 25th international conference on Machine learning, pp. 160– 167, ACM, 2008.\n[10] “https://jigsaw.google.com/,” [11] “http://www.nytco.com/the-times-is-partnering-with-jigsaw-to-expand-\ncomment-capabilities/,” [12] M. Barreno, B. Nelson, R. Sears, A. D. Joseph, and J. D. Tygar,\n“Can machine learning be secure?,” in Proceedings of the 2006 ACM Symposium on Information, computer and communications security, pp. 16–25, ACM, 2006. [13] M. Barreno, B. Nelson, A. D. Joseph, and J. Tygar, “The security of machine learning,” Machine Learning, vol. 81, no. 2, pp. 121–148, 2010. [14] L. Huang, A. D. Joseph, B. Nelson, B. I. Rubinstein, and J. Tygar, “Adversarial machine learning,” in Proceedings of the 4th ACM workshop on Security and artificial intelligence, pp. 43–58, ACM, 2011. [15] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus, “Intriguing properties of neural networks,” arXiv preprint arXiv:1312.6199, 2013. [16] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, and A. Swami, “Practical black-box attacks against deep learning systems using adversarial examples,” arXiv preprint arXiv:1602.02697, 2016. [17] “https://conversationai.github.io/,” [18] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing\nadversarial examples,” arXiv preprint arXiv:1412.6572, 2014. [19] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and\nA. Swami, “The limitations of deep learning in adversarial settings,” in 2016 IEEE European Symposium on Security and Privacy (EuroS&P), pp. 372–387, IEEE, 2016. [20] C. Kereliuk, B. L. Sturm, and J. Larsen, “Deep learning and music adversaries,” IEEE Transactions on Multimedia, vol. 17, no. 11, pp. 2059– 2071, 2015. [21] K. Grosse, N. Papernot, P. Manoharan, M. Backes, and P. McDaniel, “Adversarial perturbations against deep neural networks for malware classification,” arXiv preprint arXiv:1606.04435, 2016. [22] S. Reddy, M. Wellesley, K. Knight, and C. Marina del Rey, “Obfuscating gender in social media writing,” NLP+ CSS 2016, p. 17, 2016."
    } ],
    "references" : [ {
      "title" : "Online harassment",
      "author" : [ "M. Duggan" ],
      "venue" : "Pew Research Center,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "Ex machina: Personal attacks seen at scale",
      "author" : [ "E. Wulczyn", "N. Thain", "L. Dixon" ],
      "venue" : "arXiv preprint arXiv:1610.08914, 2016.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "Advances in neural information processing systems, pp. 1097–1105, 2012.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Context-dependent pretrained deep neural networks for large-vocabulary speech recognition",
      "author" : [ "G.E. Dahl", "D. Yu", "L. Deng", "A. Acero" ],
      "venue" : "IEEE Transactions on Audio, Speech, and Language Processing, vol. 20, no. 1, pp. 30–42, 2012.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A unified architecture for natural language processing: Deep neural networks with multitask learning",
      "author" : [ "R. Collobert", "J. Weston" ],
      "venue" : "Proceedings of the 25th international conference on Machine learning, pp. 160– 167, ACM, 2008.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Can machine learning be secure",
      "author" : [ "M. Barreno", "B. Nelson", "R. Sears", "A.D. Joseph", "J.D. Tygar" ],
      "venue" : "Proceedings of the 2006 ACM Symposium on Information, computer and communications security, pp. 16–25, ACM, 2006.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "The security of machine learning",
      "author" : [ "M. Barreno", "B. Nelson", "A.D. Joseph", "J. Tygar" ],
      "venue" : "Machine Learning, vol. 81, no. 2, pp. 121–148, 2010.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Adversarial machine learning",
      "author" : [ "L. Huang", "A.D. Joseph", "B. Nelson", "B.I. Rubinstein", "J. Tygar" ],
      "venue" : "Proceedings of the 4th ACM workshop on Security and artificial intelligence, pp. 43–58, ACM, 2011.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Intriguing properties of neural networks",
      "author" : [ "C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I. Goodfellow", "R. Fergus" ],
      "venue" : "arXiv preprint arXiv:1312.6199, 2013.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Practical black-box attacks against deep learning systems using adversarial examples",
      "author" : [ "N. Papernot", "P. McDaniel", "I. Goodfellow", "S. Jha", "Z.B. Celik", "A. Swami" ],
      "venue" : "arXiv preprint arXiv:1602.02697, 2016.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Explaining and harnessing adversarial examples",
      "author" : [ "I.J. Goodfellow", "J. Shlens", "C. Szegedy" ],
      "venue" : "arXiv preprint arXiv:1412.6572, 2014.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "The limitations of deep learning in adversarial settings",
      "author" : [ "N. Papernot", "P. McDaniel", "S. Jha", "M. Fredrikson", "Z.B. Celik", "A. Swami" ],
      "venue" : "2016 IEEE European Symposium on Security and Privacy (EuroS&P), pp. 372–387, IEEE, 2016.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Deep learning and music adversaries",
      "author" : [ "C. Kereliuk", "B.L. Sturm", "J. Larsen" ],
      "venue" : "IEEE Transactions on Multimedia, vol. 17, no. 11, pp. 2059– 2071, 2015.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Adversarial perturbations against deep neural networks for malware classification",
      "author" : [ "K. Grosse", "N. Papernot", "P. Manoharan", "M. Backes", "P. McDaniel" ],
      "venue" : "arXiv preprint arXiv:1606.04435, 2016.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Obfuscating gender in social media writing",
      "author" : [ "S. Reddy", "M. Wellesley", "K. Knight", "C. Marina del Rey" ],
      "venue" : "NLP+ CSS 2016, p. 17, 2016. 4",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Unfortunately, the lack of a moderating entity in these platforms has caused several problems, ranging from the wide spread of fake news to online harassment [2].",
      "startOffset" : 158,
      "endOffset" : 161
    }, {
      "referenceID" : 1,
      "context" : "As a result, there has been many calls for researchers to develop methods to automatically detect abusive or toxic context in the real time [6].",
      "startOffset" : 140,
      "endOffset" : 143
    }, {
      "referenceID" : 2,
      "context" : "Recent advances in machine learning have transformed many domains such as computer vision [7], speech recognition [8], and language processing [9].",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 3,
      "context" : "Recent advances in machine learning have transformed many domains such as computer vision [7], speech recognition [8], and language processing [9].",
      "startOffset" : 114,
      "endOffset" : 117
    }, {
      "referenceID" : 4,
      "context" : "Recent advances in machine learning have transformed many domains such as computer vision [7], speech recognition [8], and language processing [9].",
      "startOffset" : 143,
      "endOffset" : 146
    }, {
      "referenceID" : 5,
      "context" : "However, many works have pointed out their vulnerability in adversarial scenarios [12]– [14].",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 7,
      "context" : "However, many works have pointed out their vulnerability in adversarial scenarios [12]– [14].",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 8,
      "context" : "Such inputs are called adversarial examples [15], and have been shown to be effective against different machine learning algorithms even when the adversary has only a blackbox access to the target model [16].",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 9,
      "context" : "Such inputs are called adversarial examples [15], and have been shown to be effective against different machine learning algorithms even when the adversary has only a blackbox access to the target model [16].",
      "startOffset" : 203,
      "endOffset" : 207
    }, {
      "referenceID" : 5,
      "context" : "As a result, they are subject to attacks in adversarial scenarios [12]– [14].",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 7,
      "context" : "As a result, they are subject to attacks in adversarial scenarios [12]– [14].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 8,
      "context" : "Such inputs are called adversarial examples [15].",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 8,
      "context" : "Adversarial examples have been applied to models for different tasks, such as images classification [15], [18], [19], music content analysis [20] and malware classification [21].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 10,
      "context" : "Adversarial examples have been applied to models for different tasks, such as images classification [15], [18], [19], music content analysis [20] and malware classification [21].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 11,
      "context" : "Adversarial examples have been applied to models for different tasks, such as images classification [15], [18], [19], music content analysis [20] and malware classification [21].",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 12,
      "context" : "Adversarial examples have been applied to models for different tasks, such as images classification [15], [18], [19], music content analysis [20] and malware classification [21].",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 13,
      "context" : "Adversarial examples have been applied to models for different tasks, such as images classification [15], [18], [19], music content analysis [20] and malware classification [21].",
      "startOffset" : 173,
      "endOffset" : 177
    }, {
      "referenceID" : 14,
      "context" : "In a similar work [22], the authors presented a method for gender obfuscating in social media writing.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 10,
      "context" : "• Adversarial Training: In this approach, during the training phase, we generate the adversarial examples and train the model to assign the original label to them [18].",
      "startOffset" : 163,
      "endOffset" : 167
    } ],
    "year" : 2017,
    "abstractText" : "Social media platforms provide an environment where people can freely engage in discussions. Unfortunately, they also enable several problems, such as online harassment. Recently, Google and Jigsaw started a project called Perspective, which uses machine learning to automatically detect toxic language. A demonstration website has been also launched, which allows anyone to type a phrase in the interface and instantaneously see the toxicity score [1]. In this paper, we propose an attack on the Perspective toxic detection system based on the adversarial examples. We show that an adversary can subtly modify a highly toxic phrase in a way that the system assigns significantly lower toxicity score to it. We apply the attack on the sample phrases provided in the Perspective website and show that we can consistently reduce the toxicity scores to the level of the non-toxic phrases. The existence of such adversarial examples is very harmful for toxic detection systems and seriously undermines their usability.",
    "creator" : "LaTeX with hyperref package"
  }
}