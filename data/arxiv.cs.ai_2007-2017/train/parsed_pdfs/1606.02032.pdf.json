{
  "name" : "1606.02032.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Human vs. Computer Go: Review and Prospect",
    "authors" : [ "Chang-Shing Lee", "Mei-Hui Wang", "Shi-Jim Yen", "Ting-Han Wei", "I-Chen Wu", "Ping-Chiang Chou", "Chun-Hsun Chou" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "development. This article discusses the development of computational intelligence (CI) and its relative strength in comparison with human intelligence for the game of Go. We first summarize the milestones achieved for computer Go from 1998 to 2016. Then, the computer Go programs that have participated in previous IEEE CIS competitions as well as methods and techniques used in AlphaGo are briefly introduced. Commentaries from three high-level professional Go players on the five AlphaGo versus Lee Sedol games are also included. We conclude that AlphaGo beating Lee Sedol is a huge achievement in artificial intelligence (AI) based largely on CI methods. In the future, powerful computer Go programs such as AlphaGo are expected to be instrumental in promoting Go education and AI real-world applications."
    }, {
      "heading" : "I. Computer Go Competitions",
      "text" : "The IEEE Computational Intelligence Society (CIS) has funded human vs. computer Go competitions in IEEE CIS flagship conferences since 2009. Fig. 1 shows the year and the location of the conferences. The descriptions of competitions held from 1998 to 2016 are listed in detail in an online version of this article [1-8]. The handicaps for the human vs. computer 19×19 game have been decreased from 29 in 1998 to 0 in 2016. The skill of amateur players in Go is ranked according to kyu (K) in the lower tier, where a smaller number stands for stronger playing skill (with 1K being the highest skill level), and dan (D) in the higher tier, where a larger number stands for stronger playing skill. Professional Go players are ranked entirely in dan, abbreviated with the letter P (e.g. Lee Sedol is ranked at 9P). In the amateur level, each difference in rank roughly translates to a single stone of handicap (H), where the weaker player is allowed to place an additional stone on the board prior to play to even out the game. The skill difference between professional ranks is much less than one stone for every rank difference. Go is typically played on 19×19 size boards, but 9×9 size boards are also common for beginners. The complexity of the 9×9 game is far less than the standard game, and the 9×9 game had been one of the interim goals for computer Go programs. Go is a game that is inherently biased for the first player to play, Black. To compensate for this first player advantage, White is awarded additional points at the end of the game, which is referred to as komi. The related statistics for the IEEE CIS human vs. computer Go competitions are listed in the online version of this article [8]. It is worth noting that with a komi of 7.5, White may end up with an advantage in both 9×9 games and handicapped 19×19 games, regardless of whether White is played by humans or computers. Fig. 2 shows the certificate awarded to the MoGoTW program (16cores / 48GB / 9×9) by the Taiwan Go Association in 2010 for playing at a 3D amateur level\n* Corresponding author: Chang-Shing Lee (E-mail: leecs@mail.nutn.edu.tw).\novercome to achieve comparable performance to AlphaGo. For more information, eleven programs (Aya, CGI, Coldmilk/Jimmy, Erica, Fuego, MoGo/MoGoTW, Many Faces of Go, Pachi, and Zen) from 7 countries that have participated in past IEEE CIS conferences are listed in alphabetical order in the online version of this article [8].\nII. AlphaGo In this section, we briefly introduce the past techniques used in computer Go programs, then provide an estimate for why AlphaGo is able to outperform contemporary programs so dramatically. Currently, Monte Carlo tree search (MCTS), minorization-maximization (MM), and deep convolutional neural networks (DCNNs) have demonstrated great success in Go. MCTS was successfully applied to Go in 2006 [9, 10], leading to a significant improvement in playing skill. One year later, MM was applied so that programs may recognize move patterns using supervised learning, with expert game records as the training sample [11]. Though not as revolutionary as MCTS, MM has also had a long-lasting impact on Go programs from 2007 up to 2014. In December of 2014, two teams (one of which is the DeepMind AlphaGo team) applied DCNNs to Go independently [12, 13]. Clark and Storkey [12] first published a paper that applied DCNNs to Go, which, when given a game position, could estimate how expert human players respond with a prediction rate of 41%-44%, exceeding the rate of previous methods. Meanwhile, DeepMind’s method, which was released 10 days later, had a prediction rate of 55%. Among many of DCNN’s applications, it has seen success in image and video recognition. When applied to Go, DCNN is able to recognize move patterns at a significantly lower error rate than MM. For this reason, most stateof-the-art computer Go programs use MCTS combined with either MM or DCNN.\nAlphaGo is able to perform leaps and bounds above other contemporary programs because of its extensive use of high quality neural networks, which cannot be easily reproduced by other teams due to insufficient experience and/or inadequate hardware resources. To illustrate, let us consider the three main neural networks used in AlphaGo: a supervised learning (SL) policy network, a reinforcement learning (RL) policy network, and the value network [14]. Both the SL policy network and the value network were used in AlphaGo during competitive play; the RL policy network was used only for generating the training samples for the value network. The SL policy network takes a game board position and attempts to guess where expert players will play next. This SL process was performed with 30 million game positions, and involved 340 million training steps, taking a total of three weeks with 50 graphic processing units (GPUs) [14]. The SL algorithm tends to mimic what it has learned from game records instead of favoring moves that yield the highest winning rate when given a choice. To improve on this, the SL policy network was used to train a separate policy network using RL. Training for the RL policy network takes one day with 50 GPUs. The key to AlphaGo’s playing skill is its value network, which\nweek with 50 GPUs, for a total training time of four weeks and a day for all three networks.\nThe most time-consuming and most difficult process to reproduce, however, is not the training of these three networks, but the generation of self-play game positions. For each of the 30 million self-play positions, 100 playouts are performed; for each playout, we assume on average 200 moves until game completion, so a total of 600 billion move data samples need to be generated to train the value network. Let us assume, for the sake of demonstration, that a research team has access to four GPUs. The training of the three networks will take [(4 weeks × 7 days / week) + 1 day] × 50 GPUs\n4 GPUs = 362.5 days. Assuming a\nprocessing speed of 720 moves/s for a single GPU (with a batch size of 16), an optimistic estimate for the generation of self-play game samples is 600 × 109 moves / (4 × 720 moves / second) ≅ 208 million seconds, which works out to 2411 days or about 80 months. In addition to the total time required for generating and training the networks, we must consider the fact that parameters involved in the entire process are rarely tuned to fit the requirements in a single trial. This includes a wide variety of settings such as the number of layers and neurons in the neural network, the features to use for the Go positions, the collection of expert game records that are used to train the initial SL policy network, etc. This quick estimate of required resources does not even take into account the knowledge and experience that the DeepMind team has acquired since its inception. As a side note, the distributed version of AlphaGo uses 280 GPUs [14].\nIII. Human Intelligence View Demis Hassabis, CEO and Co-Founder of Google DeepMind, described Go as the “Mt. Everest” of AI [15] because Go is a very complex board game that requires intuitive, creative, and strategic thinking [16]. In the past decade, the techniques of MCTS had revolutionized the field of computer game-playing. The playing strength of computer Go programs has progressed to about a four-stone handicap against top professional Go players in 2012-2015. More precisely, until AlphaGo’s emergence in Oct. 2015, the world’s strongest program, Zen, was able to beat a top professional Go player with a handicap of four stones, while losing when the handicap was decreased to three stones. Google DeepMind introduced a new approach that combined MCTS with deep learning in their program AlphaGo [14], which subsequently broke this four-stone handicap barrier in the recent competition with Lee Sedol (9P) in Korea, in Mar. 2016. AlphaGo’s performance sent shock waves through the community of professional Go players and AI researchers. Since then, people on the Internet and media, particularly in Go-playing cultures such as in Korea, Taiwan, Japan, and China, were buzzing with discussion on related topics [17- 19]. In this section, we invited three high-level professional Go players who have spent time helping the development of computer Go programs, including Coldmilk and MoGoTW, and who have also been part of shaping Go trends for many years, to comment on the game results of the challenge match from Mar. 9 to Mar. 15, 2016. Fig. 3 shows the game record for match No. 5 and Comment 1 lists three professional Go players’ brief commentaries. For the other four matches’ commentaries and the full commentary on match No. 5, readers can refer to the online version of this article [8]. Additionally, readers may find details for the Go terminologies used in Comments 1 and 2 at Sensei’s Library [20]. From the perspective of professional Go player (Ping-Chiang Chou / 6P), the strengths and weaknesses of AlphaGo are listed in Comment 2.\nComment 1. Professional Go players’ comments on match No. 5.\nComments by Ping-Chiang Chou (6P / Taiwan) Theoretically, Black made a profit from the fight in the bottom right corner. Hence, up to White 68, Black’s situation was slightly better from the view point of professional players. Chou concluded that perhaps Black 79 was decided under the premise that Black already had the lead. If the match was against other professional players under the same situation, Black 79 would have intuitively been played at L14. In reality, from White 80 to White 90 (jump), the situation had turned favorable for White. Comments by Chun-Hsun Chou (9P / Taiwan) White started to play an unusual move at move 18. White 20, 22, and 24 are indicative of AlphaGo’s unconventional playing style, which sacrifices a few stones to gain a sente and obtain the maximum profit. Black 79 and 81 were two slow moves intended for stability. Unfortunately, they were shut in by White 80 and 82 so the situation favored White by a little. Although there were variations in the following moves, White took hold of the situation with a weak superiority until the endgame. As a result, Black ended up losing by about 2.5 points after komi 7.5. Comments by Ming-Wan Wang (9P / Japan) Shortly after the game began, White again made mistakes, allowing Black to have the upper hand. This unexpected turn of events caused Black (Lee Sedol) to change his playing style so that he may conservatively aim to make life. After being laid siege by White, Black realized he was behind, and started to make every effort to regain his lead, whereupon the situation became quite chaotic. It was unfortunate that Black did not play the strongest move at the critical moment, which ended the game with a small loss for Black.\nIV. Discussions and Future Studies It has been estimated that when human players play against a new computer program for the first time, their strengths are usually weakened by about 1-2 levels. This is because human players will usually play a probe move when they have no idea about the program’s strength. Another key point is that Go programs tend to be designed differently from how human Go players tend to think; winning ten points is essentially the same as winning one point as far as the program’s objective function is concerned. As a result, the Go programs tend to maximize the winning rates, while human players tend to maximize territory. Additionally, programs have other advantages when playing against humans, such as relatively\ndisadvantages. There are weaknesses in computer programs, however, such as hidden bugs, imperfect judgement on the overall situation at the beginning of the game, and imperfect parameter adjustment. According to cognitive psychologists [21], the difference between experts (for example, Go teachers or professional Go players) and newcomers is that the former can store a variety of Go patterns in their brains, allowing them to quickly find a more precise move than the latter. Upon seeing the formed shapes of stones on the board, they are able to scan their memory to find the best matching pattern, then play an effective move. Newcomers, on the other hand, are short of such a quick connection.\nComputer Go, in particular AlphaGo, has been an extremely hot topic lately, not only to AI researchers but also to the general public. The purpose of this article is to help the readership better understand how the development of computer Go programs has arrived at this milestone of winning against one of the top human players, and how CIS has been involved in this process. This article’s main contribution is to place the win of AlphaGo over Lee Sedol in recent historical context. This huge achievement in AI is based largely on CI methods, including DCNNs, SL from expert games, RL, the use of the value network and policy network, and MCTS. The strength of AlphaGo, especially as measured against the other computer Go programs, is absolutely amazing. Playing with contemporary computer Go programs like Crazy Stone, Zen, Pachi, Fuego, and GnuGo, with no handicaps on either side, the single-machine version of AlphaGo was able to win 494 games out of 495 in total, while the distributed version of AlphaGo won all games against these competing programs [14].\nAs an alternative method of evaluation, the DeepMind team used an Elo rating scale, where each player gets a numerical strength estimation computed from past game results. AlphaGo, human European champion Fan Hui, and the above listed competing programs were evaluated. The distributed version of AlphaGo had an Elo rating of 3140, the non-distributed version was at 2890, and Fan Hui was at 2908 [14]. Since then, AlphaGo’s playing strength has grown even stronger. At the time of writing, according to an online ranking [22] curated by Go programmer and author of Crazy Stone, Rémi Coulom, distributed AlphaGo’s Elo rating is 3590 (the second highest rating in the world), where Ke Jie (3624) and Lee Sedol (3525) are ranked as the first and fourth highest Elo rating in the world, respectively. It is worth noting that the Elo rating system computed in [22] is not exactly the same as the one used in [14]. The ratings are given here only for the benefit of establishing some intuition for AlphaGo’s progress. In addition to the professional players’ comments on the five games between AlphaGo and Lee Sedol, this article also provides overall comments from Ming-Wan Wang (9P / Japan), Ping-Chiang Chou (6P / Taiwan), and Tai-Hsiung Yang (7D / Taiwan, director of Haifong Weiqi Academy, Taiwan) in Comment 2. AlphaGo’s victory over the world champion Lee Sedol in Mar. 2016 will be marked in history as a remarkable achievement. However, this would not have been possible without the considerable time and effort of countless contributors to computer Go in the past. AlphaGo’s impact will almost assuredly popularize and improve Go learning worldwide, especially if a personalized version with reduced hardware costs becomes available. Finally, AlphaGo’s performance was truly astonishing, and will undoubtedly be a continued source of inspiration for professional Go players and AI researchers around the world."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The authors would like to thank the Ministry of Science and Technology of Taiwan for its financial support under the grant MOST 105-2919-I-024-001-A1, MOST 104-2221-E-024-015, and MOST 104- 2622-E-024-005-CC2. Additionally, the authors also would like to thank 1) Mr. Ti-Rong Wu (National Chiao Tung University, Taiwan) for providing the resource estimate in generating self-play game positions (see Section II); 2) Mr. Sheng-Shu Chang (President of Click108 Company, Taiwan) for his financial support on past human vs. computer Go competitions @ IEEE CIS flagship conferences; and 3) Dr. Olivier Teytaud and INRIA TAO team members as well as Taiwanese team members and National Center for High-Performance Computing (NCHC) under the grant NSC 99-2923-E-024-003-MY3. Finally, we would like to thank the anonymous referees for their constructive and useful comments."
    } ],
    "references" : [ {
      "title" : "The 2010 contest: MoGoTW vs. human Go players",
      "author" : [ "M.-H. Wang", "C.-S. Lee", "Y.-L. Wang", "M.-C. Cheng", "O. Teytaud", "S.-J. Yen" ],
      "venue" : "ICGA Journal, vol. 33, no. 1, pp. 47–50, Mar. 2010.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "The computational intelligence of MoGo revealed in Taiwan’s computer Go tournaments",
      "author" : [ "C.-S. Lee", "M.-H. Wang", "G. Chaslot", "J.-B. Hoock", "A. Rimmel", "O. Teytaud", "S.-R. Tsai", "S.-C. Hsu", "T.-P. Hong" ],
      "venue" : "IEEE Transactions on Computational Intelligence and AI in Games, vol. 1, no. 1, pp. 73–89, Mar. 2009.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Current frontiers in computer Go",
      "author" : [ "A. Rimmel", "O. Teytaud", "C.-S. Lee", "S.-J. Yen", "M.-H. Wang", "S.-R. Tsai" ],
      "venue" : "IEEE Transactions on Computational Intelligence and AI in Games, vol. 2, no. 4, pp. 229–238. Dec. 2010.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "The game of Go @ IEEE WCCI 2010",
      "author" : [ "C.-S. Lee", "M.-H. Wang", "O. Teytaud", "Y.-L. Wang" ],
      "venue" : "IEEE This article is accepted and will be published in IEEE Computational Intelligence Magazine in August 2016 6  Computational Intelligence Magazine, vol. 5, no. 4, pp. 6–7, Nov. 2010.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Computational intelligence meets game of Go @ IEEE WCCI 2012",
      "author" : [ "C.-S. Lee", "O. Teytaud", "M.-H. Wang", "S.-J. Yen" ],
      "venue" : "IEEE Computational Intelligence Magazine, vol. 7, no. 4, pp. 10–12, Nov. 2012.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "T2FS-based adaptive linguistic assessment system for semantic analysis and human performance evaluation on game of Go",
      "author" : [ "C.-S. Lee", "M.-H. Wang", "M.-J. Wu", "O. Teytaud", "S.-J. Yen" ],
      "venue" : "IEEE Transactions on Fuzzy Systems, vol. 23, no. 2, pp. 400–420, Apr. 2015.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Intelligent agents for the game of Go",
      "author" : [ "J.-B. Hoock", "C.-S. Lee", "A. Rimmel", "F. Teytaud", "M.-H. Wang", "O. Teytaud" ],
      "venue" : "IEEE Computational Intelligence Magazine, vol. 5, no. 4, pp. 28–42, Nov. 2010.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Human vs. computer Go: review and prospect @ IEEE CIS: integrated human and computational intelligence for future Go learning based on Google AlphaGo’s historic achievement",
      "author" : [ "C.-S. Lee", "M.-H. Wang", "S.-J. Yen", "T.-H. Wei", "I.-C. Wu", "P.-C. Chou", "C.-H. Chou", "M.-W. Wang", "T.-H. Yang" ],
      "venue" : "Apr. 2016, [Online] Available: https://sites.google.com/site/nutnoaselabenglish/computergo.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Efficient selectivity and backup operators in Monte-Carlo tree search",
      "author" : [ "R. Coulom" ],
      "venue" : "H. Jaap van den Herik, P. Ciancarini, and H. H. L. M. (Jeroen) Donkers (editors), Computers and Games, Berlin, Heidelberg, Springer, 2006, pp. 72–83.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Bandit based Monte-Carlo planning",
      "author" : [ "L. Kocsis", "C. Szepesvári" ],
      "venue" : "Proceeding of 17th European Conference on Machine Learning (ECML 2006), Berlin, Germany, Sept. 18–22, 2006, pp. 282–293.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Computing Elo ratings of move patterns in the game of Go",
      "author" : [ "R. Coulom" ],
      "venue" : "Proceeding of Computer Games Workshop 2007 (CGW 2007), Amsterdam, The Netherlands, Jun. 15–17, 2007, pp. 113–124.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Teaching deep convolutional neural networks to play Go",
      "author" : [ "C. Clark", "A. Storkey" ],
      "venue" : "Dec. 2014, [Online] Available: http://arxiv.org/abs/1412.3409.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Move evaluation in Go using deep convolutional neural networks",
      "author" : [ "C.J. Maddison", "A. Huang", "I. Sutskever", "D. Silver" ],
      "venue" : "Dec. 2014, [Online] Available: http://arxiv.org/abs/1412.6564.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Mastering the game of Go with deep neural networks and tree search",
      "author" : [ "D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. van den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot", "S. Dieleman", "D. Grewe", "J. Nham", "N. Kalchbrenner", "I. Sutskever", "T. Lillicrap", "M. Leach", "K. Kavukcuoglu", "T. Graepel", "D. Hassabis" ],
      "venue" : "Nature, vol. 529, pp. 484–489, Jan. 2016.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Human go champ beats supercomputer",
      "author" : [ "Taipei Times" ],
      "venue" : "Mar. 14, 2016, [Online] Available: http://www.taipeitimes.com/News/front/archives/2016/03/14/2003641528.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "AlphaGo versus Lee Sedol",
      "author" : [ "Wikipedia" ],
      "venue" : "Mar. 2016, [Online] Available: https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol#Difficult_challenge_in_artificial_intelligence.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Google’s AI wins first game in historic match with Go champion",
      "author" : [ "Wired" ],
      "venue" : "Mar. 2016, [Online] Available: http://www.wired.com/2016/03/googles-ai-wins-first-game-historic-match-go-champion/?mbid=nl_3916.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "In two moves, AlphaGo and Lee Sedol redefined the future",
      "author" : [ "Wired" ],
      "venue" : "Mar. 2016, [Online] Available: http://www.wired.com/2016/03/two-moves-alphago-lee-sedol-redefined-future/.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Official Google Blog: What we learned in Seoul with AlphaGo",
      "author" : [ "D. Hassabis" ],
      "venue" : "Mar. 2016 [Online] Available: https://googleblog.blogspot.tw/2016/03/what-we-learned-in-seoul-with-alphago.html?m=1.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Sensei’s library",
      "author" : [ "A. Hollosi", "M. Pahle" ],
      "venue" : "Apr. 2016, [Online] Available: http://senseis.xmp.net/.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Computer program beats European Go champion",
      "author" : [ "F. Gobet", "M.H. Ereku" ],
      "venue" : "Feb. 2016, [Online] Available: https://www.psychologytoday.com/blog/inside-expertise/201602/computer-program-beats-european-gochampion.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Go Ratings",
      "author" : [ "R. Coulom" ],
      "venue" : "Apr. 2016, [Online] Available: http://www.goratings.org/.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "The descriptions of competitions held from 1998 to 2016 are listed in detail in an online version of this article [1-8].",
      "startOffset" : 114,
      "endOffset" : 119
    }, {
      "referenceID" : 1,
      "context" : "The descriptions of competitions held from 1998 to 2016 are listed in detail in an online version of this article [1-8].",
      "startOffset" : 114,
      "endOffset" : 119
    }, {
      "referenceID" : 2,
      "context" : "The descriptions of competitions held from 1998 to 2016 are listed in detail in an online version of this article [1-8].",
      "startOffset" : 114,
      "endOffset" : 119
    }, {
      "referenceID" : 3,
      "context" : "The descriptions of competitions held from 1998 to 2016 are listed in detail in an online version of this article [1-8].",
      "startOffset" : 114,
      "endOffset" : 119
    }, {
      "referenceID" : 4,
      "context" : "The descriptions of competitions held from 1998 to 2016 are listed in detail in an online version of this article [1-8].",
      "startOffset" : 114,
      "endOffset" : 119
    }, {
      "referenceID" : 5,
      "context" : "The descriptions of competitions held from 1998 to 2016 are listed in detail in an online version of this article [1-8].",
      "startOffset" : 114,
      "endOffset" : 119
    }, {
      "referenceID" : 6,
      "context" : "The descriptions of competitions held from 1998 to 2016 are listed in detail in an online version of this article [1-8].",
      "startOffset" : 114,
      "endOffset" : 119
    }, {
      "referenceID" : 7,
      "context" : "The descriptions of competitions held from 1998 to 2016 are listed in detail in an online version of this article [1-8].",
      "startOffset" : 114,
      "endOffset" : 119
    }, {
      "referenceID" : 7,
      "context" : "computer Go competitions are listed in the online version of this article [8].",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 0,
      "context" : "2 [1].",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 7,
      "context" : "For more information, eleven programs (Aya, CGI, Coldmilk/Jimmy, Erica, Fuego, MoGo/MoGoTW, Many Faces of Go, Pachi, and Zen) from 7 countries that have participated in past IEEE CIS conferences are listed in alphabetical order in the online version of this article [8].",
      "startOffset" : 266,
      "endOffset" : 269
    }, {
      "referenceID" : 8,
      "context" : "MCTS was successfully applied to Go in 2006 [9, 10], leading to a significant improvement in playing skill.",
      "startOffset" : 44,
      "endOffset" : 51
    }, {
      "referenceID" : 9,
      "context" : "MCTS was successfully applied to Go in 2006 [9, 10], leading to a significant improvement in playing skill.",
      "startOffset" : 44,
      "endOffset" : 51
    }, {
      "referenceID" : 10,
      "context" : "One year later, MM was applied so that programs may recognize move patterns using supervised learning, with expert game records as the training sample [11].",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 11,
      "context" : "In December of 2014, two teams (one of which is the DeepMind AlphaGo team) applied DCNNs to Go independently [12, 13].",
      "startOffset" : 109,
      "endOffset" : 117
    }, {
      "referenceID" : 12,
      "context" : "In December of 2014, two teams (one of which is the DeepMind AlphaGo team) applied DCNNs to Go independently [12, 13].",
      "startOffset" : 109,
      "endOffset" : 117
    }, {
      "referenceID" : 11,
      "context" : "Clark and Storkey [12] first published a paper that applied DCNNs to Go, which, when given a game position, could estimate how expert human players respond with a prediction rate of 41%-44%, exceeding the rate of previous methods.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 13,
      "context" : "To illustrate, let us consider the three main neural networks used in AlphaGo: a supervised learning (SL) policy network, a reinforcement learning (RL) policy network, and the value network [14].",
      "startOffset" : 190,
      "endOffset" : 194
    }, {
      "referenceID" : 13,
      "context" : "This SL process was performed with 30 million game positions, and involved 340 million training steps, taking a total of three weeks with 50 graphic processing units (GPUs) [14].",
      "startOffset" : 173,
      "endOffset" : 177
    }, {
      "referenceID" : 13,
      "context" : "As a side note, the distributed version of AlphaGo uses 280 GPUs [14].",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 14,
      "context" : "Everest” of AI [15] because Go is a very complex board game that requires intuitive, creative, and strategic thinking [16].",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 15,
      "context" : "Everest” of AI [15] because Go is a very complex board game that requires intuitive, creative, and strategic thinking [16].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 13,
      "context" : "Google DeepMind introduced a new approach that combined MCTS with deep learning in their program AlphaGo [14], which subsequently broke this four-stone handicap barrier in the recent competition with Lee Sedol (9P) in Korea, in Mar.",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 7,
      "context" : "5, readers can refer to the online version of this article [8].",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 19,
      "context" : "Additionally, readers may find details for the Go terminologies used in Comments 1 and 2 at Sensei’s Library [20].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 20,
      "context" : "According to cognitive psychologists [21], the difference between experts (for example, Go teachers or professional Go players) and newcomers is that the former can store a variety of Go patterns in their brains, allowing them to quickly find a more precise move than the latter.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 13,
      "context" : "side, the single-machine version of AlphaGo was able to win 494 games out of 495 in total, while the distributed version of AlphaGo won all games against these competing programs [14].",
      "startOffset" : 179,
      "endOffset" : 183
    }, {
      "referenceID" : 13,
      "context" : "The distributed version of AlphaGo had an Elo rating of 3140, the non-distributed version was at 2890, and Fan Hui was at 2908 [14].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 21,
      "context" : "At the time of writing, according to an online ranking [22] curated by Go programmer and author of Crazy Stone, Rémi Coulom, distributed AlphaGo’s Elo rating is 3590 (the second highest rating in the world), where Ke Jie (3624) and Lee Sedol (3525) are ranked as the first and fourth highest Elo rating in the world, respectively.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 21,
      "context" : "It is worth noting that the Elo rating system computed in [22] is not exactly the same as the one used in [14].",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 13,
      "context" : "It is worth noting that the Elo rating system computed in [22] is not exactly the same as the one used in [14].",
      "startOffset" : 106,
      "endOffset" : 110
    } ],
    "year" : 2016,
    "abstractText" : "The Google DeepMind challenge match in March 2016 was a historic achievement for computer Go development. This article discusses the development of computational intelligence (CI) and its relative strength in comparison with human intelligence for the game of Go. We first summarize the milestones achieved for computer Go from 1998 to 2016. Then, the computer Go programs that have participated in previous IEEE CIS competitions as well as methods and techniques used in AlphaGo are briefly introduced. Commentaries from three high-level professional Go players on the five AlphaGo versus Lee Sedol games are also included. We conclude that AlphaGo beating Lee Sedol is a huge achievement in artificial intelligence (AI) based largely on CI methods. In the future, powerful computer Go programs such as AlphaGo are expected to be instrumental in promoting Go education and AI real-world applications. I. Computer Go Competitions The IEEE Computational Intelligence Society (CIS) has funded human vs. computer Go competitions in IEEE CIS flagship conferences since 2009. Fig. 1 shows the year and the location of the conferences. The descriptions of competitions held from 1998 to 2016 are listed in detail in an online version of this article [1-8]. The handicaps for the human vs. computer 19×19 game have been decreased from 29 in 1998 to 0 in 2016. The skill of amateur players in Go is ranked according to kyu (K) in the lower tier, where a smaller number stands for stronger playing skill (with 1K being the highest skill level), and dan (D) in the higher tier, where a larger number stands for stronger playing skill. Professional Go players are ranked entirely in dan, abbreviated with the letter P (e.g. Lee Sedol is ranked at 9P). In the amateur level, each difference in rank roughly translates to a single stone of handicap (H), where the weaker player is allowed to place an additional stone on the board prior to play to even out the game. The skill difference between professional ranks is much less than one stone for every rank difference. Go is typically played on 19×19 size boards, but 9×9 size boards are also common for beginners. The complexity of the 9×9 game is far less than the standard game, and the 9×9 game had been one of the interim goals for computer Go programs. Go is a game that is inherently biased for the first player to play, Black. To compensate for this first player advantage, White is awarded additional points at the end of the game, which is referred to as komi. The related statistics for the IEEE CIS human vs. computer Go competitions are listed in the online version of this article [8]. It is worth noting that with a komi of 7.5, White may end up with an advantage in both 9×9 games and handicapped 19×19 games, regardless of whether White is played by humans or computers. Fig. 2 shows the certificate awarded to the MoGoTW program (16cores / 48GB / 9×9) by the Taiwan Go Association in 2010 for playing at a 3D amateur level * Corresponding author: Chang-Shing Lee (E-mail: leecs@mail.nutn.edu.tw). This article is accepted and will be published in IEEE Computational Intelligence Magazine in August 2016 2 [1]. In this article, we attempt to demonstrate the massive barrier that competing programs need to overcome to achieve comparable performance to AlphaGo. For more information, eleven programs (Aya, CGI, Coldmilk/Jimmy, Erica, Fuego, MoGo/MoGoTW, Many Faces of Go, Pachi, and Zen) from 7 countries that have participated in past IEEE CIS conferences are listed in alphabetical order in the online version of this article [8]. Fig. 1. Past human vs. computer Go competitions in IEEE CIS flagship conferences. Fig. 2. Amateur 3D certificate awarded to MoGoTW in 2010.",
    "creator" : "Microsoft® Word 2013"
  }
}