{
  "name" : "1609.05152.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Style Imitation and Chord Invention in Polyphonic Music with Exponential Families",
    "authors" : [ "Gaëtan Hadjeres", "Jason Sakellariou", "François Pachet" ],
    "emails" : [ "gaetan.hadjeres@etu-upmc.fr,", "js.sakel@gmail.com,", "pachetcsl@gmail.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Polyphonic tonal music is often considered as a highlight of Western civilization. Today’s music is still largely based on complex structures invented and developed since the Renaissance period, and modeled, e.g. by Jean Philippe Rameau [23] in the XVIIth century. In particular, polyphonic music is characterized by an intricate interplay between melody (single-voice stream of notes) and harmony (progression of simultaneously-heard notes). Additionally, composers tend to develop a specific style, that influences the way notes are combined together to form a musical piece.\nMany models of polyphonic music have been proposed since the 50s (see [11] for a comprehensive survey), starting with the famous Illiac Suite, which used Markov chains to produce 4-voice music, controlled by hand-made rules [15]. In this paper we address the issue of learning agnostically the style of a polyphonic composer, with the aim of producing new musical pieces, that satisfy additional user constraints.\nIn practice, a good model should satisfy three requirements: statistical accuracy (capturing faithfully statistics of correlations at various ranges, horizontally\nar X\niv :1\n60 9.\n05 15\n2v 1\n[ cs\n.A I]\n1 6\nSe p\n20 16\nand vertically), flexibility (coping with arbitrary user constraints), and generalization capacity (inventing new material, while staying in the style of the training corpus). Models proposed so far fail on at least one of these requirements. [10] propose a chord invention framework but is not based on agnostic learning, and requires a hand-made ontology. The approach described in [21] consists in a dynamic programming template enriched by constrained Markov chains. This approach generates musically convincing results [20] but is ad hoc and specialized for jazz. Furthermore it does not invent any new voicing by construction (the vertical ordering of the notes in a chord). [14] and [1] describe a HMM approach trained on an annotated corpus. This model imitates the style of Bach chorales, as shown by cross entropy measures. However, it is also not able to produce new voicings by construction, and only replicates voicings found in the training corpus. Another related approach is [17] which uses HMMs on chord representations (based on an expert knowledge of the common-practice harmony) called General Chord Type (GCT) [6] to generate homorhythmic sequences. Those models are not agnostic in the sense that they include a priori knowledge about music such as the concept of dissonance, consonance, tonality or scale degrees. Agnostic approaches using neural networks have been investigated with promising results. In [5], chords are modeled with Restricted Boltzmann Machines (RBMs). Their temporal dependencies are learned using Recurrent Neural networks (RNNs). Variations of these architectures have been developed, based on Long Short-Term Memory (LSTM) units [18] or GRU (Gated Recurrent Units) [8]. However, these models require large and coherent training sets which are not always available. More importantly, it is not clear how to enforce additional user constraints (flexibility). Moreover, their invention capacity is not demonstrated.\nIn this paper we introduce a graphical model based on the maximum entropy principle for learning and generating polyphony. Such models have been used for music retrieval applications [22], but never, to our knowledge, for polyphonic music generation.\nThis model requires no expert knowledge about music and can be trained on small corpora. Moreover, generation is extremely fast.\nWe show that this model can capture and reproduce pairwise statistics at possibly long range, both horizontally and vertically. These pairwise statistics are also able, to some extent, to capture implicitly higher order correlations, such as the structure of 4-note chords. The model is flexible, as it allows the user to post arbitrary unary constraints on any voice. Most importantly, we show that this model exhibits a remarkable chord invention capacity. In particular we show that it produces harmonically consistent sequences using chords which did not appear in the original corpus.\nIn Sect. 2 we present the model for n-parts polyphony generation. In Sect. 3.2, we report experimental results about chord invention. In Section 3.4 we discuss a range of interactive applications in music generation. Finally, we discuss how the “musical interest” of the generated sequences depends on the choice of our model’s hyperparameters in Sect. 3.5."
    }, {
      "heading" : "2 The model",
      "text" : "The model we propose is based on a maximum entropy model described in [25]. This model is extended to handle several voices instead of one, and to establish vertical as well as diagonal interactions between notes. We formulate the model as an exponential family obtained by a product of experts (one for each voice)."
    }, {
      "heading" : "2.1 Description of the model",
      "text" : "We aim to learn sequences s of n-part chord sequences. A sequence s = [c1, . . . , cl] is composed of l chords where the jth chord is denoted\ncj := [s1j , s2j , . . . , snj ],\nwith note sij considered as an integer pitch belonging to the pitch range Ai ⊂ Z. The ith part or voice corresponds to\nvi := [si1, si2, . . . , sil].\nOur model is based on the idea that chord progressions can be generated by replicating the occurrences of pairs of neighboring notes. It is invariant by translation in time, it aims at capturing the local “texture” of the chord sequences. Similar ideas have been shown to be successful in modeling highly combinatorial and arbitrary structures such as English four-letter words [26].\nWe denote by K the model scope, which means that we consider that chords distant by more than K time steps are conditionally independent given all other variables. We focus on the interaction between neighbouring notes and try to replicat the co-occurrences notes. A natural way to formalize this is to introduce a family of functions (or features) such that each member of this family counts the number of occurrence of a given pair of notes. As a result, the finite number of features we want to learn can be written as a family{\nfab,ijk s.t. a ∈ Ai, b ∈ Aj i, j ∈ [1, n], k ∈ [−K,K]\n} (1)\nof functions over chord sequences where\nfab,ijk(s) := ] {m s.t. si,m = a and sj,m+k = b} (2)\nstands for the number of occurrences of pairs of notes (a, b) such that note a at voice i precedes by k time steps note b at voice j in the chord sequence s. We can represent this family of binary connections as a graphical model as can be seen in Fig. 1 where each subfamily\n{fab,ijk, ∀a ∈ Ai, b ∈ Aj}\nis represented as a link between two notes. Our model has also unary parameters, acting on single notes and modeling the single note marginal distributions. For\nnotational convenience we will treat these unary parameters as binary connections between pairs of identical notes (connections such that a=b, i=j, k=0) and call them local fields after the corresponding statistical physics terminology. We differentiate four types of connections: unary (local fields) and binary “horizontal”, “vertical” and “diagonal” connections. We implicitly identify fab,ijk with fba,ji(−k), for all k ∈ [−K,K], i, j ∈ [1, n].\nFrom now on, we will denote the set of indexes of the family {fab,ijk} by P. We note that there is approximately\nn2(2K + 1) |A|2 (3)\nindexes, where |A| stands for the mean alphabet size\n|A| = 1 n n∑ i=1 |Ai|.\nUsing only a subset of the family {fab,ijk} given by (1) can reduce the number of parameters while leading to good results. Indeed, if we consider that notes in different voices are conditionally independent if they are distant by more than L ≤ K time steps, we obtain a index set of size approximately equal to\n(n×K + n 2 − n\n2 L)|A|2.\nIn the following, P can designate the whole set of indexes within scope K as well as any of its subset.\nLet µab,ijk for (ab, ijk) ∈ P be real numbers. From all distributions P such that the averages over all possible sequences of length l verify∑\ns\nP (s)fab,ijk(s) = µ ab,ijk ∀(ab, ijk) ∈ P, (4)\nit is known that the exponential distribution with statistics {fab,ijk} and parameters {µab,ijk} is the one of maximum entropy (i.e. which makes the least amount of assumptions) satisfying Eq. (4). We thus consider an energy-based model of parameter\nθ := { θab,ijk ∈ R ∀(ab, ijk) ∈ P } given by\nP (s|θ) = e −E(s,θ)\nZ(θ) , (5)\nwhere E(s, θ) := − ∑ ab,ijk θab,ijkfab,ijk(s) (6)\nis usually called the energy of the sequence s and Z(θ) = log( ∑ s e−E(s,θ))\nthe normalizer or partition function such that P defines a probability function over sequences of size l. The sum in the partition function is for every s of size l. There are approximately |A|l such sequences, which make the exact computation of the partition function intractable in general."
    }, {
      "heading" : "2.2 Training",
      "text" : "We are given a training dataset D composed of N n-part sequences s(1), . . . , s(N) that we suppose, for clarity, to be concatenated in one sequence s. Since we are dealing with discrete data, gradient techniques such as score matching [16] cannot be used. Instead, we choose to minimize the negative pseudo-log-likelihood [24,9] of the data to find an approximation of the true maximum likelihood estimator. It consists in approximating the negative log-likelihood function\nL(θ, s) = − logP (s|θ) (7)\nby the mean of conditional log-likelihoods of a note given the others, that is\nL(θ, s) = − 1 nl ∑ ij logP (sij |s \\ sij , θ), (8)\nwhere s \\ sij denotes all notes in s except sij . The conditional probabilities are calculated as\nP (sij |s \\ sij , θ) = P (s, θ)∑\nc∈Ai s′\\s′ij=s\\sij\nsij=c\nP (s′, θ) (9)\nwhere the sum in the denominator is on chord sequences s′ equal to s except for the note in position ij.\nDue to the particular structure of the probability density function (5) and the choice of the statistics (1), we note that we can write\nP (sij |s \\ sij , θ) = P (sij |NK(i, j, s), θ),\nwhere NK(i, j, s) stands for the neighbors of note sij in s which are at a distance inferior to K time steps. We now express our dataset D as a set of samples (x,N ) consisting of a note x and its K-distant neighbors, ignoring border terms whose effect is negligible. More precisely, we write the dataset\nD = {(sij ,NK(i, j, s)) , ∀i ∈ [1, n], j ∈ [K + 1, l −K − 1]}\n(10)\nand split it into n datasets Di such as\nDi = {(sij ,NK(i, j, s)) , j ∈ [K + 1, l −K − 1]} .\nEach element in this dataset can still be considered as the subsequence it comes from also written (x,N ). Those notations set, we can rewrite Eq. 8 as\nL(θ,D) = − 1 ]D ∑ (x,N )∈D logP (x|N , θ)\n:= 1\nn n∑ i=1 Li(θ,Di), (11)\nwhere\nLi(θ,Di) = − 1\n]Di ∑ (x,N )∈Di logP (x|N , θ)\nis the negative conditional log-likelihood function for voice i. This consists in minimizing the mean of n negative log-likelihood functions Li (one for each voice) over the data Di. This method has the advantage of being tractable since there are only ]Ai terms in the denominator of Eq. 9 and can lead to good estimates [2]. This can be seen as the likelihood of a product of experts using n modified (addition of vertical and diagonal connections) copies of the model presented in [25].\nWe need to find the parameters minimizing the sum of n convex functions. Computing the gradient of Li for i ∈ [1, n] with respect to any parameter θ∗, ∗ ∈ P, gives us\n∂Li(θ,Di) ∂θ∗ =\n1\n]Di ∑ s=(x,N ) (x,N )∈Di  f∗(s)e−E(s,θ)∑ s′=(z,N ) z∈Ai f∗(s′)e−E(s ′,θ) − f∗(s)  . (12)\nThis can be written as\n∂Li(θ,Di) ∂θ∗ = 〈f∗〉P (.|N ,θ) − 〈f∗〉Di\nwhich is the difference between the average value of f∗ taken with respect to the conditional distribution (9) and its empirical value.\nA preprocessing of the corpus is introduced in order to efficiently compute the gradient sums.\nFinally, the function g(θ) we will optimize is the L1-regularized version of L(θ,D) with regularization parameter λ, which means that we consider\ng(θ) = L(θ,D) + λ‖θ‖1, (13)\nwhere ‖.‖1 is the usual L1-norm, which is the sum of the absolute values of the coordinates of the parameter. This is known as the Lasso regularization whose effects are widely discussed throughout statistical learning literature [12]."
    }, {
      "heading" : "2.3 Generation",
      "text" : "Generation is performed with the Metropolis-Hastings algorithm, which is an extensively used sampling algorithm (see [7] for an introduction). Its main feature is the possibility to sample from an unnormalized distribution since it only needs to compute ratios of probabilities\nα := P (s′, θ)\nP (s, θ) (14)\nbetween two sequences s′ and s. With the specific proposals detailed below, we just need to start from a random sequence s as our current sequence and repeat the following: choose a proposal s′, compute α and then accept as our current sequence s′ with probability min(α, 1) or reject this proposal with probability 1− min(α, 1) and keep s. After a sufficient number of iterations of this procedure, we are assured that the sequences obtained are distributed according the objective distribution P (.|θ).\nWe used this algorithm to generate chord sequences by choosing s′ uniformly among all sequences differing by only one note from s. In fact, with this algorithm, we can also enforce unary constraints on the produced sequences. If we only propose sequences s′ containing a sequence {nij}(ij)∈C of imposed notes, i.e. such that sij = nij , ∀(ij) ∈ C where C contains the indexes of the constrained notes, the Metropolis-Hastings algorithm samples from the distribution\nP (s|sij = nij , ∀(ij) ∈ C).\nThis enables us to provide reharmonizations of a given melody. We can also add pitch range constraints on given notes, which means that we are given a set\n{Aij ⊂ Ai, ∀(ij) ∈ C}\nsuch that we only propose sequences s′ where\nsij ∈ Aij , ∀(ij) ∈ C.\nThis can be used to impose chord labels without imposing its voicing."
    }, {
      "heading" : "3 Experimental Results",
      "text" : "We report experiments made using a set of 343 four-voice (n = 4) chorale harmonizations by Johann Sebastian Bach [4]. In order to evaluate the chord creation capabilities, we only retained in our chord sequences notes heard on beats. Sect.3.6 shows how we can easily produce rhythm using our model. We transposed every chorale in the key of C and considered 2 corpora: a corpus with chorales in a major key and a corpus with chorales in a minor key.\nThe quadratic number of parameters (thanks to the exclusive use of binary connections) makes the learning phase computationally tractable.\nWe used the L-BFGS method from Stanford CoreNLP optimization package [19] to perform the gradient descent.\nIn the next sections, we report on accuracy (the style imitation capacity of the model), its invention capacity, and flexibility."
    }, {
      "heading" : "3.1 Style imitation",
      "text" : "We investigated the capabilities of the system to reproduce pair-wise correlations of the training set. Fig.3.1 shows a scatter plot comparing the (normalized) values of each binary connection fab,ijk in the generated sequences versus the ones of the original training corpus. The model was trained on a corpus of 51 major chorales, which represents the equivalent of a 3244-beat long chord sequence. We chose to differentiate horizontal connections from vertical and diagonal ones by introducing a parameter L as mentioned in Sect. 2.1. We took K = 4, L = 2, λ = 3e − 5 as parameters and generated a 100000-beat long sequence. For a discussion on the choice of the regularization parameter, see Sect. 3.5. We see that despite the small amount of data, the alignment between the generated pair occurrences and the original ones is quite convincing.\nThe generation procedure needs solely to compute the ratios (14), which can be done in approximately O(nK) operations. Indeed, since the sequences differ by only one note, only contributions of its neighboring notes has to be taken into account. This has to be compared with the approximate number (3) of parameters. Experimentally, we found that the number of metropolis steps to achieve convergence is of order O(nlA) which enables these models to be used in real-time applications.\nWe argue that this model does not only reproduce pairwise statistics but can in fact capture higher-order interactions which makes it suitable for style imitation. Indeed, how the binary connections are combined in Eq. 5 makes the model able to reproduce correct voicings even if the way notes composing a chord are distributed among the different voices is a conceptually an interaction of order n. Fig. 3 show chords with nice voicings, voices do not cross, triads have correct doublings and each separate voice has a coherent shape. A detailed analysis of the chord creation capacity of the system is made in Sec. 3.2. We discuss the capability to reproduce other higher-order patterns in Sect. 3.3."
    }, {
      "heading" : "3.2 Chord Invention",
      "text" : "We claim that the competition between the horizontal, vertical and diagonal correlations of our probabilistic model can generate new chords in the learned “style”. Three categories of generated chords can be distinguished: the cited chords which appear in the model’s training set, the discovered chords which do not appear in the training set but can be found in other Bach’s chorales and the invented chords which do not belong to any of the above categories. We used the same model as above to plot in Fig. 4 the mean repartition of chords during the Metropolis-Hastings generation as a function of the number of Metropolis steps (divided by |A|nl). These curves highly depend on the parameters chosen for the model and on the corpus’s size and structure. Nonetheless we can note the characteristic time for the model to sample from the equilibrium distribution. For every parameter set we tested, we observed that when convergence is reached, the proportion of invented and discovered chords seems fixed and significant.\nA closer investigation shows that most of these “invented” chords can in fact be classified as valid in the style of Bach by an expert. The majority of the invented chords is composed of “correct” voicings of minor or major triads, seventh chords and chords with nonchord tones. Fig. 3 exhibits interesting “inventions” such as an (unprepared) 9 − 8 resolution, a dominant ninth and a diminished seventh. Other invented chords are discordant. A blindfolded evaluation was conducted to assess to which extent listeners distinguish invented chords from cited ones. Three non professional music-loving adult listeners were presented with a series of invented chords extracted from generated sequences, and played with their context (i.e. 4 chords before and 4 chords after). They were asked whether the central chord was “good” or not. Results show that in average 75% of the invented chords were considered as acceptable."
    }, {
      "heading" : "3.3 Higher order interactions",
      "text" : "The same analysis as in Sec. 3.2 can be made for other structures than chords. We chose to investigate to which extent the model is able to reproduce the occurrences of quadrilateral tuples. For a sequence s, we define the quadrilateral tuple (see Fig.3) between voices i and i′ at position j to be the tuple\n(si(j), si(j+1), si′(j), si′(j+1)).\nThese tuples are of particular interest since many harmonic rules, such as the prohibition of consecutive fiths and consecutive octaves which are often considered to be forbidden in counterpoint, apply to them. Table. 1 compares the percentage of cited/discovered/invented generated quadrilateral tuples by the model of Sec. 3.1, by a model containing only vertical interactions and by an independant model (which reproduces only pitch frequencies).\nIt shows that an important part of those higher order structures is reproduced. However, analysis exhibits limitations on the higher-order statistics that can be captured (see for instance Fig. 3). Indeed, even if our preprocessed corpus contains some of these “rules violations”, our model is unable to statistically reproduce the number of such structures (they are 2 to 10 times more frequent than in the original corpus). We discuss non agnostic methods to integrate these particular rules in Sec. 4."
    }, {
      "heading" : "3.4 Flexibility",
      "text" : "As claimed in Sec. 2.3, we can use our model to generate new harmonizations of a melody. Indeed, the simplicity and adaptability of the model allows it to be “twisted” in order to enforce unary constraints while still generating sequences in the learned style. As our model is in a specified key (all chorales were transposed in the same key), we can thus provide convincing Bach-like harmonizations of plainsong melodies provided they “fit” in the training key. Fig. 5 shows two reharmonizations of Beethoven’s Ode to Joy with different unary constraints3. It is worth noting that even if we put constraints on isolated notes and not on full chords, the constraints propagate well both vertically (the voicings are correct)\n3 Music examples can be heard on the http://flowmachines.jimdo.com/ website\nand horizontally (the progression of chords around the constrained notes is coherent). This opens up a wide range of applications. Those examples show how enforcing simple unary constraints can be used to produce interesting musical phenomena during reharmonization such as:\n– original harmonies (see for instance the 1st, 2nd and 4th constraints in Fig. 5) – expected harmonies (3rd constraint in Fig. 5) – a sense of long-range correlation by creating cadenzas (last constraint in\nFig. 5).\nBy modifying the constrained metropolis sampling scheme of Sect. 2.3 we can harmonize any melody v0 = [s01, s02, . . . , s0l] of size l. For each beat j ∈ [1, l], we use a melody analyzer to yield the current key kj at beat j. If the new proposed sequence s′ differs from the current one s by a note at beat j, we compute the acceptance ratio (14) by using the probability distribution of the model trained in key kj . By doing so, we choose the appropriate model for each chunk of the melody and “glue” the results together seamlessly."
    }, {
      "heading" : "3.5 Impact of the regularization parameter",
      "text" : "In this section we discuss the choice of the regularization parameter λ of Eq. 13. The benefits of introducing a L1-regularization are multiple: it makes the loss function (13) strictly convex (in our case, we do not need to determine if the family (1) is a sufficient family), tends to reduce the number of non zero parameter coordinates and prevents overfitting As our model possesses a important number of parameters compared to the number of samples, adding a regularization term during the training phase appears to be mandatory in obtaining good results in the applications we mentioned Sect. 2.3 and 3.4.\nWe thus evaluate the impact of the choice of λ on the cited/discovered/invented classification curves (Fig. 3). We compare the mean repartition of chords in the unconstrained generation case and in the reharmonization case where the first voice is constrained. The same training corpus as in Sect. 2.3 is used, with K = 4, L = 2 and varying λ . We use the first voice of chorales from the testing corpus as constraints. Results are presented in Fig. 6.\nA clear influence of λ appears for both the unconstrained and constrained generations. However, these curves are not sharply peaked and it seems not clear which regularization parameter could be the most musically-interesting one.\nFor this, we investigated to which extent the regularization parameter influences the model’s ability to reproduce a wide variety of chords, either seen in the training set or rediscovered in the testing set.\nWe introduce two quantities revealing the diversity in the generated sequences: the percentage of restitution of the training corpus (the number of cited chords counted without repetitions normalized by the total number of different chords in the training corpus) and the percentage of discovery of the testing corpus (the number of different discovered chords normalized by the number of chords counted without repetitions in the testing corpus which are not in\nthe training corpus). The evolution of the restitution/discovery percentages as a function of λ is plotted in Fig. 7 for both the constrained and unconstrained generation cases.\nBoth figures exhibit the same behavior. High values of λ lead to uniform models and low values of λ to models which overfit the training data. But the most interesting observation is that their maximum is not attained for the same value of λ revealing the possibility to control the tradeoff between the invention capacity, the diversity and the faithfulness with respect to the training corpus in the generated sequences."
    }, {
      "heading" : "3.6 Rhythm",
      "text" : "In this work we focus on chord reproduction and invention. It is an interesting question since the model is by construction pairwise and chords are, in our examples, four-notes objects. In order to simplify the analysis on four-notes chords we chose to work on homorhythmic sequences and we discared all notes not falling on the beat in our corpus, J.S. Bach’s chorales. However, real music is not necessarily homorhythmic. It has rhythm, i.e. notes come in varying durations to form temporal patterns which take place with respect to some periodic pulse, a kind of temporal canvas.\nWe propose a simple way to extend our model in order to account for rhythmic patterns. The model initially presented in [25] and extended in 2 is translation invariant. For rhythmic patterns to emerge we need to break this translation invariance. We do that by introducing position dependent parameters. More specifically, we choose a cycle which is repeated over time and within which the translation invariance is broken. Such cycle can be for example one or two bars of music. We then divide this cycle in equal time bins which correspond to all possible positions where a note can start or end. We call these time-bins metrical positions. We could then define different parameters between notes on different metrical positions but that would lead to a very large number of parameters, yielding a very inaccurate learning (it can be argued that the number of parameters must be smaller than the number of data points). We have found that a good compromise is to let the unary parameters (local fields) be position-dependent while keeping the translation invariance for the true binary parameters. This leads to a negligible increase in the number of parameters since the unary parameters are of order |A| whereas the true binary ones are of order |A|2. Finaly, in order to obtain a variety of note durations as well as rests, we introduce two additional symbols in the alphabet. One symbol for rests and one symbol that signifies the continuation of the previous note in the current metrical position.\nThe above procedure has the following effect: the position-dependent parameters are biasing localy the occurence of symbols (pitches, rests or continuations of the previous pitch) in a way that is consistent with the original corpus, which leads to the emergence of rhythic patterns of the same kind as the ones found in the corpus. An example can be seen in 8. For this example we used a cycle of one bar and divided it in 8 equal parts, corresponding to eighth notes (quavers) which is also the smallest division found in the corpus (here Missa Sanctorum Meritis, Credo by Giovanni Pierluigi da Palestrina)."
    }, {
      "heading" : "4 Discussion and future work",
      "text" : "We proposed a probabilistic model for chord sequences that captures pairwise dependencies between neighboring notes. The model is able to reproduce harmonic progressions without any prior information and invent new “stylistically correct” chords. The possibility to sample with arbitrary unary user-defined constraints makes this model applicable in a wide range of situations. We focused mainly on the chord creation and restitution capabilities which is in our point of view its most interesting feature, an analysis of the plagiarism of monophonic\n.\ngraphical models being made in [25]. We showed that even if the original training set is highly combinatorial, these probabilistic methods behave impressively well even if high-order hard constraints such as parallel fifths of octaves cannot be captured. This method is general and applies to all discrete n-tuple sequences. Indeed, we used as features the occurrences of notes, but any other family of functions could be selected. For instance, adding occurrences of parallel fifths or parallel octaves in the family (1) would be possible and would only require O(n2|A|2) parameters, which does not increase our model’s complexity.\nThe utmost importance of the regularization parameter suggests to investigate finer and more problem-dependent regularizations such as group lasso [13] or other hierarchical sparsity-inducing norms [3]. We believe that having more than a single scalar regularization parameter λ can lead to a better control of the “creativity” of our model."
    }, {
      "heading" : "Acknowledgment",
      "text" : "This research is conducted within the Flow Machines project which received funding from the European Research Council under the European Unions Seventh Framework Programme (FP/2007-2013) / ERC Grant Agreement n. 291156."
    } ],
    "references" : [ {
      "title" : "Harmonising chorales by probabilistic inference",
      "author" : [ "M. Allan", "C.K. Williams" ],
      "venue" : "Advances in neural information processing systems 17, 25–32",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Pseudolikelihood estimation: some examples",
      "author" : [ "B.C. Arnold", "D. Strauss" ],
      "venue" : "Sankhyā: The Indian Journal of Statistics, Series B pp. 233–243",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Structured sparsity-inducing norms through submodular functions",
      "author" : [ "F.R. Bach" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2010
    }, {
      "title" : "Chorales (Choral-Gesange): SATB (German Language Edition)",
      "author" : [ "J. Bach" ],
      "venue" : "Kalmus Classic Edition, Alfred Publishing Company",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1985
    }, {
      "title" : "Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription",
      "author" : [ "N. Boulanger-lewandowski", "Y. Bengio", "P. Vincent" ],
      "venue" : "Proceedings of the 29th International Conference on Machine Learning (ICML-12). pp. 1159–1166",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "An idiomindependent representation of chords for computational music analysis and generation",
      "author" : [ "E. Cambouropoulos", "M. Kaliakatsos-Papakostas", "C. Tsougras" ],
      "venue" : "Ann Arbor, MI: Michigan Publishing, University of Michigan Library",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Understanding the metropolis-hastings algorithm",
      "author" : [ "S. Chib", "E. Greenberg" ],
      "venue" : "The American Statistician 49(4), 327–335",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "author" : [ "J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio" ],
      "venue" : "arXiv preprint arXiv:1412.3555",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Improved contact prediction in proteins: using pseudolikelihoods to infer potts models",
      "author" : [ "M. Ekeberg", "C. Lövkvist", "Y. Lan", "M. Weigt", "E. Aurell" ],
      "venue" : "Physical Review E 87(1), 012707",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Computational invention of cadences and chord progressions by conceptual chord-blending",
      "author" : [ "M. Eppe", "R. Confalonieri", "E. Maclean", "M. Kaliakatsos", "E. Cambouropoulos", "M. Schorlemmer", "M. Codescu", "K.U. Kühnberger" ],
      "venue" : "International Joint Conference on Artificial Intelligence (IJCAI)",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Ai methods in algorithmic composition: A comprehensive survey",
      "author" : [ "J.D. Fernández", "F. Vico" ],
      "venue" : "Journal of Artificial Intelligence Research pp. 513–582",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "The elements of statistical learning, vol",
      "author" : [ "J. Friedman", "T. Hastie", "R. Tibshirani" ],
      "venue" : "1. Springer series in statistics Springer, Berlin",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "A note on the group lasso and a sparse group lasso",
      "author" : [ "J. Friedman", "T. Hastie", "R. Tibshirani" ],
      "venue" : "arXiv preprint arXiv:1001.0736",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Harmonet: A neural net for harmonizing chorales in the style of js bach",
      "author" : [ "H. Hild", "J. Feulner", "W. Menzel" ],
      "venue" : "Advances in neural information processing systems. pp. 267–274",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Experimental music : composition with an electronic computer",
      "author" : [ "L. Hiller", "L. Issacson" ],
      "venue" : "McGraw-Hill",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1959
    }, {
      "title" : "Estimation of non-normalized statistical models by score matching",
      "author" : [ "A. Hyvärinen" ],
      "venue" : "Journal of Machine Learning Research. pp. 695–709",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Probabilistic harmonization with fixed intermediate chord constraints",
      "author" : [ "M. Kaliakatsos-Papakostas", "E. Cambouropoulos" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2014
    }, {
      "title" : "Modelling high-dimensional sequences with lstm-rtrbm: application to polyphonic music generation",
      "author" : [ "Q. Lyu", "Z. Wu", "J. Zhu", "H. Meng" ],
      "venue" : "Proceedings of the 24th International Conference on Artificial Intelligence. pp. 4138–4139. AAAI Press",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "The stanford corenlp natural language processing toolkit",
      "author" : [ "C.D. Manning", "M. Surdeanu", "J. Bauer", "J. Finkel", "S.J. Bethard", "D. McClosky" ],
      "venue" : "ACL 2014 p. 55",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A joyful ode to automatic orchestration",
      "author" : [ "F. Pachet" ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology, Special Issue on Intelligent Music Systems and Applications (2016),",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2016
    }, {
      "title" : "Non-conformant harmonization: the real book in the style of take 6",
      "author" : [ "F. Pachet", "P. Roy" ],
      "venue" : "5th International Conference on Computational Creativity (ICCC 2014). pp. 100–107. Ljubljiana (Slovenia)",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Markov random fields and maximum entropy modeling for music information retrieval",
      "author" : [ "J. Pickens", "C.S. Iliopoulos" ],
      "venue" : "ISMIR. pp. 207–214. Citeseer",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Traité de l’harmonie réduite à ses principes naturels",
      "author" : [ "J.P. Rameau" ],
      "venue" : "Imp. de J.-B.C. Ballard",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1722
    }, {
      "title" : "High-dimensional ising model selection using l1-regularized logistic regression",
      "author" : [ "P. Ravikumar", "M.J. Wainwright", "Lafferty", "J.D" ],
      "venue" : "The Annals of Statistics 38(3), 1287–1319",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Maximum entropy model for melodic patterns",
      "author" : [ "J. Sakellariou", "F. Tria", "V. Loreto", "F. Pachet" ],
      "venue" : "ICML Workshop on Constructive Machine Learning. Paris (France)",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Toward a statistical mechanics of four letter words",
      "author" : [ "G.J. Stephens", "W. Bialek" ],
      "venue" : "arXiv preprint arXiv:0801.0253",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 22,
      "context" : "by Jean Philippe Rameau [23] in the XVII century.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 10,
      "context" : "Many models of polyphonic music have been proposed since the 50s (see [11] for a comprehensive survey), starting with the famous Illiac Suite, which used Markov chains to produce 4-voice music, controlled by hand-made rules [15].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 14,
      "context" : "Many models of polyphonic music have been proposed since the 50s (see [11] for a comprehensive survey), starting with the famous Illiac Suite, which used Markov chains to produce 4-voice music, controlled by hand-made rules [15].",
      "startOffset" : 224,
      "endOffset" : 228
    }, {
      "referenceID" : 9,
      "context" : "[10] propose a chord invention framework but is not based on agnostic learning, and requires a hand-made ontology.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "The approach described in [21] consists in a dynamic programming template enriched by constrained Markov chains.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 19,
      "context" : "This approach generates musically convincing results [20] but is ad hoc and specialized for jazz.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 13,
      "context" : "[14] and [1] describe a HMM approach trained on an annotated corpus.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "[14] and [1] describe a HMM approach trained on an annotated corpus.",
      "startOffset" : 9,
      "endOffset" : 12
    }, {
      "referenceID" : 16,
      "context" : "Another related approach is [17] which uses HMMs on chord representations (based on an expert knowledge of the common-practice harmony) called General Chord Type (GCT) [6] to generate homorhythmic sequences.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 5,
      "context" : "Another related approach is [17] which uses HMMs on chord representations (based on an expert knowledge of the common-practice harmony) called General Chord Type (GCT) [6] to generate homorhythmic sequences.",
      "startOffset" : 168,
      "endOffset" : 171
    }, {
      "referenceID" : 4,
      "context" : "In [5], chords are modeled with Restricted Boltzmann Machines (RBMs).",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 17,
      "context" : "Variations of these architectures have been developed, based on Long Short-Term Memory (LSTM) units [18] or GRU (Gated Recurrent Units) [8].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 7,
      "context" : "Variations of these architectures have been developed, based on Long Short-Term Memory (LSTM) units [18] or GRU (Gated Recurrent Units) [8].",
      "startOffset" : 136,
      "endOffset" : 139
    }, {
      "referenceID" : 21,
      "context" : "Such models have been used for music retrieval applications [22], but never, to our knowledge, for polyphonic music generation.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 24,
      "context" : "The model we propose is based on a maximum entropy model described in [25].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 25,
      "context" : "Similar ideas have been shown to be successful in modeling highly combinatorial and arbitrary structures such as English four-letter words [26].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 15,
      "context" : "Since we are dealing with discrete data, gradient techniques such as score matching [16] cannot be used.",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 23,
      "context" : "Instead, we choose to minimize the negative pseudo-log-likelihood [24,9] of the data to find an approximation of the true maximum likelihood estimator.",
      "startOffset" : 66,
      "endOffset" : 72
    }, {
      "referenceID" : 8,
      "context" : "Instead, we choose to minimize the negative pseudo-log-likelihood [24,9] of the data to find an approximation of the true maximum likelihood estimator.",
      "startOffset" : 66,
      "endOffset" : 72
    }, {
      "referenceID" : 1,
      "context" : "9 and can lead to good estimates [2].",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 24,
      "context" : "This can be seen as the likelihood of a product of experts using n modified (addition of vertical and diagonal connections) copies of the model presented in [25].",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 11,
      "context" : "This is known as the Lasso regularization whose effects are widely discussed throughout statistical learning literature [12].",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 6,
      "context" : "Generation is performed with the Metropolis-Hastings algorithm, which is an extensively used sampling algorithm (see [7] for an introduction).",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 3,
      "context" : "We report experiments made using a set of 343 four-voice (n = 4) chorale harmonizations by Johann Sebastian Bach [4].",
      "startOffset" : 113,
      "endOffset" : 116
    }, {
      "referenceID" : 18,
      "context" : "We used the L-BFGS method from Stanford CoreNLP optimization package [19] to perform the gradient descent.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 0,
      "context" : "[0] [1] [2] [3]",
      "startOffset" : 4,
      "endOffset" : 7
    }, {
      "referenceID" : 1,
      "context" : "[0] [1] [2] [3]",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 2,
      "context" : "[0] [1] [2] [3]",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 24,
      "context" : "The model initially presented in [25] and extended in 2 is translation invariant.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 24,
      "context" : "graphical models being made in [25].",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 12,
      "context" : "The utmost importance of the regularization parameter suggests to investigate finer and more problem-dependent regularizations such as group lasso [13] or other hierarchical sparsity-inducing norms [3].",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 2,
      "context" : "The utmost importance of the regularization parameter suggests to investigate finer and more problem-dependent regularizations such as group lasso [13] or other hierarchical sparsity-inducing norms [3].",
      "startOffset" : 198,
      "endOffset" : 201
    } ],
    "year" : 2016,
    "abstractText" : "Modeling polyphonic music is a particularly challenging task because of the intricate interplay between melody and harmony. A good model should satisfy three requirements: statistical accuracy (capturing faithfully the statistics of correlations at various ranges, horizontally and vertically), flexibility (coping with arbitrary user constraints), and generalization capacity (inventing new material, while staying in the style of the training corpus). Models proposed so far fail on at least one of these requirements. We propose a statistical model of polyphonic music, based on the maximum entropy principle. This model is able to learn and reproduce pairwise statistics between neighboring note events in a given corpus. The model is also able to invent new chords and to harmonize unknown melodies. We evaluate the invention capacity of the model by assessing the amount of cited, re-discovered, and invented chords on a corpus of Bach chorales. We discuss how the model enables the user to specify and enforce user-defined constraints, which makes it useful for style-based, interactive music generation.",
    "creator" : "LaTeX with hyperref package"
  }
}