{
  "name" : "1506.05230.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "cdyer}@cs.cmu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 6.\n05 23\n0v 1\n[ cs\n.C L\n] 1\n7 Ju\nn 20\n15"
    }, {
      "heading" : "1 Introduction",
      "text" : "Distributed representations of words have been shown to benefit a diverse set of NLP tasks including syntactic parsing (Lazaridou et al., 2013; Bansal et al., 2014), named entity recognition (Guo et al., 2014) and sentiment analysis (Socher et al., 2013). Additionally, because they can be induced directly from unannotated corpora, they are likewise available in domains and languages where traditional linguistic resources do not exhaust. Intrinsic evaluations on various tasks are helping refine vector learning methods to discover representations that captures many facts about lexical semantics (Turney, 2001; Turney and Pantel, 2010).\nYet induced word vectors do not look anything like the representations described in most lexical semantic theories, which focus on identifying classes of words (Levin, 1993; Baker et al., 1998; Schuler, 2005; Miller, 1995). Though expensive to construct, conceptualizing word meanings sym-\nbolically is important for theoretical understanding and interpretability is desired in computational models.\nOur contribution to this discussion is a new technique that constructs task-independent word vector representations using linguistic knowledge derived from pre-constructed linguistic resources like WordNet (Miller, 1995), FrameNet (Baker et al., 1998), Penn Treebank (Marcus et al., 1993) etc. In such word vectors every dimension is a linguistic feature and 1/0 indicates the presence or absence of that feature in a word, thus the vector representations are binary while being highly sparse (≈ 99.9%). Since these vectors do not encode any word cooccurrence information, they are non-distributional. An additional benefit of constructing such vectors is that they are fully interpretable i.e, every dimension of these vectors maps to a linguistic feature unlike distributional word vectors where the vector dimensions have no interpretability.\nOf course, engineering feature vectors from linguistic resources is established practice in many applications of discriminative learning; e.g., parsing (McDonald and Pereira, 2006; Nivre, 2008) or part of speech tagging (Ratnaparkhi, 1996; Collins, 2002). However, despite a certain common inventories of features that re-appear across many tasks, feature engineering tends to be seen as a task-specific problem, and engineered feature vectors are not typically evaluated independently of the tasks they are designed for. We evaluate the quality of our linguistic vectors on a number of tasks that have been proposed for evaluating distributional word vectors. We show that linguistic word vectors are comparable to current state-ofthe-art distributional word vectors trained on billions of words as evaluated on a battery of semantic and syntactic evaluation benchmarks.1\n1Our vectors can be downloaded at: https://github.com/mfaruqui/non-distributional"
    }, {
      "heading" : "2 Linguistic Word Vectors",
      "text" : "We construct linguistic word vectors by extracting word level information from linguistic resources. Table 1 shows the size of vocabulary and number of features induced from every lexicon. We now describe various linguistic resources that we use for constructing linguistic word vectors.\nWordNet. WordNet (Miller, 1995) is an English lexical database that groups words into sets of synonyms called synsets and records a number of relations among these synsets or their members. For a word we look up its synset for all possible part of speech (POS) tags that it can assume. For example, film will have SYNSET.FILM.V.01 and SYNSET.FILM.N.01 as features as it can be both a verb and a noun. In addition to synsets, we include the hyponym (for ex. HYPO.COLLAGEFILM.N.01), hypernym (for ex. HYPER:SHEET.N.06) and holonym synset of the word as features. We also collect antonyms and pertainyms of all the words in a synset and include those as features in the linguistic vector.\nSupsersenses. WordNet partitions nouns and verbs into semantic field categories known as supsersenses (Ciaramita and Altun, 2006; Nastase, 2008). For example, lioness evokes the supersense SS.NOUN.ANIMAL. These supersenses were further extended to adjectives (Tsvetkov et al., 2014).2 We use these supsersense tags for nouns, verbs and adjectives as features in the linguistic word vectors.\nFrameNet. FrameNet (Baker et al., 1998; Fillmore et al., 2003) is a rich linguistic resource that contains information about lexical and predicate-argument semantics in English. Frames can be realized on the surface by many different word types, which suggests that the\n2http://www.cs.cmu.edu/˜ytsvetko/adj-supersenses.tar.gz\nword types evoking the same frame should be semantically related. For every word, we use the frame it evokes along with the roles of the evoked frame as its features. Since, information in FrameNet is part of speech (POS) disambiguated, we couple these feature with the corresponding POS tag of the word. For example, since appreciate is a verb, it will have the following features: VERB.FRAME.REGARD, VERB.FRAME.ROLE.EVALUEE etc.\nEmotion & Sentiment. Mohammad and Turney (2013) constructed two different lexicons that associate words to sentiment polarity and to emotions resp. using crowdsourcing. The polarity is either positive or negative but there are eight different kinds of emotions like anger, anticipation, joy etc. Every word in the lexicon is associated with these properties. For example, cannibal evokes POL.NEG, EMO.DISGUST and EMO.FEAR. We use these properties as features in linguistic vectors.\nConnotation. Feng et al. (2013) construct a lexicon that contains information about connotation of words that are seemingly objective but often allude nuanced sentiment. They assign positive, negative and neutral connotations to these words. This lexicon differs from Mohammad and Turney (2013) in that it has a more subtle shade of sentiment and it extends to many more words. For example, delay has a negative connotation CON.NOUN.NEG, floral has a positive connotation CON.ADJ.POS and outline has a neutral connotation CON.VERB.NEUT.\nColor. Most languages have expressions involving color, for example green with envy and grey with uncertainly are phrases used in English. The word-color associtation lexicon produced by Mohammad (2011) using crowdsourcing lists the colors that a word evokes in English. We use every color in this lexicon as a feature in the vector. For example, COLOR.RED is a feature evoked by the word blood.\nPart of Speech Tags. The Penn Treebank (Marcus et al., 1993) annotates naturally occurring text for linguistic structure. It contains syntactic parse trees and POS tags for every word in the corpus. We collect all the possible POS tags that a word is annotated with and use it as features in the linguistic vectors. For example, love has\nPTB.NOUN, PTB.VERB as features.\nSynonymy & Antonymy. We use Roget’s thesaurus (Roget, 1852) to collect sets of synonymous words.3 For every word, its synonymous word is used as a feature in the linguistic vector. For example, adoration and affair have a feature SYNO.LOVE, admissible has a feature SYNO.ACCEPTABLE. The synonym lexicon contains 25,338 words after removal of multiword phrases. In a similar manner, we also use antonymy relations between words as features in the word vector. The antonymous words for a given word were collected from Ordway (1913).4 An example would be of impartiality, which has features ANTO.FAVORITISM and ANTO.INJUSTICE. The antonym lexicon has 10,355 words. These features are different from those induced from WordNet as the former encode word-word relations whereas the latter encode word-synset relations.\nAfter collecting features from the various linguistic resources described above we obtain linguistic word vectors of length 172,418 dimensions. These vectors are 99.9% sparse i.e, each vector on an average contains only 34 non-zero features out of 172,418 total features. On average a linguistic feature (vector dimension) is active for 15 word types. The linguistic word vectors contain 119,257 unique word types. Table 2 shows linguistic vectors for some of the words."
    }, {
      "heading" : "3 Experiments",
      "text" : "We first briefly describe the evaluation tasks and then present results."
    }, {
      "heading" : "3.1 Evaluation Tasks",
      "text" : "Word Similarity. We evaluate our word representations on three different benchmarks to measure word similarity. The first one is the widely used WS-353 dataset (Finkelstein et al., 2001),\n3http://www.gutenberg.org/ebooks/10681 4https://archive.org/details/synonymsantonyms00ordwiala\nwhich contains 353 pairs of English words that have been assigned similarity ratings by humans. The second is the RG-65 dataset (Rubenstein and Goodenough, 1965) of 65 words pairs. The third dataset is SimLex (Hill et al., 2014) which has been constructed to overcome the shortcomings of WS-353 and contains 999 pairs of adjectives, nouns and verbs. Word similarity is computed using cosine similarity between two words and Spearman’s rank correlation is reported between the rankings produced by vector model against the human rankings.\nSentiment Analysis. Socher et al. (2013) created a treebank containing sentences annotated with fine-grained sentiment labels on phrases and sentences from movie review excerpts. The coarse-grained treebank of positive and negative classes has been split into training, development, and test datasets containing 6,920, 872, and 1,821 sentences, respectively. We use average of the word vectors of a given sentence as features in an ℓ2-regularized logistic regression for classification. The classifier is tuned on the dev set and accuracy is reported on the test set.\nNP-Bracketing. Lazaridou et al. (2013) constructed a dataset from the Penn TreeBank (Marcus et al., 1993) of noun phrases (NP) of length three words, where the first can be an adjective or a noun and the other two are nouns. The task is to predict the correct bracketing in the parse tree for a given noun phrase. For example, local (phone company) and (blood pressure) medicine exhibit left and right bracketing respectively. We append the word vectors of the three words in the NP in order and use them as features in an ℓ2-regularized logistic regression classifier. The dataset contains 2,227 noun phrases split into 10 folds. The classifier is tuned on the first fold and cross-validation accuracy is reported on the re aining nine folds."
    }, {
      "heading" : "3.2 Linguistic Vs. Distributional Vectors",
      "text" : "In order to make our linguistic vectors comparable to publicly available distributional word vectors, we perform singular value decompostion (SVD) on the linguistic matrix to obtain word vectors of lower dimensionality. If L ∈ {0, 1}N×D is the linguistic matrix with N word types and D linguistic features, then we can obtain U ∈ RN×K from the SVD of L as follows: L = UΣV⊤, with K being the desired length of the lower dimensional space.\nWe compare both sparse and dense linguistic vectors to three widely used distributional word vector models. The first two are the pre-trained Skip-Gram (Mikolov et al., 2013)5 and Glove (Pennington et al., 2014)6 word vectors each of length 300, trained on 300 billion and 6 billion words respectively. We used latent semantic analysis (LSA) to obtain word vectors from the SVD decomposition of a word-word cooccurrence matrix (Turney and Pantel, 2010). These were trained on 1 billion words of Wikipedia with vector length 300 and context window of 5 words."
    }, {
      "heading" : "3.3 Results",
      "text" : "Table 3 shows the performance of different word vector types on the evaluation tasks. It can be seen that although Skip-Gram, Glove & LSA perform better than linguistic vectors on WS-353, the linguistic vectors outperform them by a huge margin on SimLex. Linguistic vectors also perform better at RG-65. On sentiment analysis, linguistic vectors are competitive with Skip-Gram vectors and on the NP-bracketing task they outperform all distributional vectors with a statistically significant margin (p < 0.05, McNemar’s test Dietterich (1998)). We append the sparse linguistic vectors to Skip-Gram vectors and evaluate the resultant vectors as shown in the bottom row of Table 3. The combined vector outperforms SkipGram on all tasks, showing that linguistic vectors\n5https://code.google.com/p/word2vec 6http://www-nlp.stanford.edu/projects/glove/\ncontain useful information orthogonal to distributional information.\nIt is evident from the results that linguistic vectors are either competitive or better to state-of-theart distributional vector models. Sparse linguistic word vectors are high dimensional but they are also sparse, which makes them computationally easy to work with."
    }, {
      "heading" : "4 Discussion",
      "text" : "Linguistic resources like WordNet have found extensive applications in lexical semantics, for example, for word sense disambiguation, word similarity etc. (Resnik, 1995; Agirre et al., 2009). Recently there has been interest in using linguistic resources to enrich word vector representations. In these approaches, relational information among words obtained from WordNet, Freebase etc. is used as a constraint to encourage words with similar properties in lexical ontologies to have similar word vectors (Xu et al., 2014; Yu and Dredze, 2014; Bian et al., 2014; Fried and Duh, 2014; Faruqui et al., 2015a). Distributional representations have also been shown to improve by using experiential data in addition to distributional context (Andrews et al., 2009). We have shown that simple vector concatenation can likewise be used to improve representations (further confirming the established finding that lexical resources and cooccurrence information provide somewhat orthogonal information), but it is certain that more careful combination strategies can be used.\nAlthough distributional word vector dimensions cannot, in general, be identified with linguistic properties, it has been shown that some vector construction strategies yield dimensions that are relatively more interpretable (Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015; Faruqui et al., 2015b). However, such analysis is difficult to generalize across models of representation. In constrast to distribu-\ntional word vectors, linguistic word vectors have interpretable dimensions as every dimension is a linguistic property.\nLinguistic word vectors require no training as there are no parameters to be optimized, meaning they are computationally economical. While good quality linguistic word vectors may only be obtained for languages with rich linguistic resources, such resources do exist in many languages and should not be disregarded."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We have presented a novel method of constructing word vector representations solely using linguistic knowledge from pre-existing linguistic resources. These non-distributional, linguistic word vectors are competitive to the current models of distributional word vectors as evaluated on a battery of tasks. Linguistic vectors are fully interpretable as every dimension is a linguistic feature and are highly sparse, so they are computationally easy to work with."
    }, {
      "heading" : "Acknowledgement",
      "text" : "We thank Nathan Schneider for giving comments on an earlier draft of this paper and the anonymous reviewers for their feedback."
    } ],
    "references" : [ {
      "title" : "A study on similarity and relatedness using distributional and wordnet-based approaches",
      "author" : [ "Agirre et al.2009] Eneko Agirre", "Enrique Alfonseca", "Keith Hall", "Jana Kravalova", "Marius Paşca", "Aitor Soroa" ],
      "venue" : "In Proc. of NAACL",
      "citeRegEx" : "Agirre et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Agirre et al\\.",
      "year" : 2009
    }, {
      "title" : "Integrating experiential and distributional data to learn semantic representations",
      "author" : [ "Andrews et al.2009] Mark Andrews", "Gabriella Vigliocco", "David Vinson" ],
      "venue" : "Psychological review,",
      "citeRegEx" : "Andrews et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Andrews et al\\.",
      "year" : 2009
    }, {
      "title" : "The berkeley framenet project",
      "author" : [ "Charles J. Fillmore", "John B. Lowe" ],
      "venue" : "In Proc. of ACL",
      "citeRegEx" : "Baker et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Baker et al\\.",
      "year" : 1998
    }, {
      "title" : "Tailoring continuous word representations for dependency parsing",
      "author" : [ "Bansal et al.2014] Mohit Bansal", "Kevin Gimpel", "Karen Livescu" ],
      "venue" : "In Proc. of ACL",
      "citeRegEx" : "Bansal et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bansal et al\\.",
      "year" : 2014
    }, {
      "title" : "Knowledge-powered deep learning for word embedding",
      "author" : [ "Bian et al.2014] Jiang Bian", "Bin Gao", "Tie-Yan Liu" ],
      "venue" : "In Proc. of MLKDD",
      "citeRegEx" : "Bian et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bian et al\\.",
      "year" : 2014
    }, {
      "title" : "Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger",
      "author" : [ "Ciaramita", "Yasemin Altun" ],
      "venue" : "In Proc. of EMNLP",
      "citeRegEx" : "Ciaramita et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Ciaramita et al\\.",
      "year" : 2006
    }, {
      "title" : "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms",
      "author" : [ "Michael Collins" ],
      "venue" : "In Proc. of EMNLP",
      "citeRegEx" : "Collins.,? \\Q2002\\E",
      "shortCiteRegEx" : "Collins.",
      "year" : 2002
    }, {
      "title" : "Approximate statistical tests for comparing supervised classification learning algorithms",
      "author" : [ "Thomas G. Dietterich" ],
      "venue" : "Neural Computation",
      "citeRegEx" : "Dietterich.,? \\Q1998\\E",
      "shortCiteRegEx" : "Dietterich.",
      "year" : 1998
    }, {
      "title" : "Retrofitting word vectors to semantic lexicons",
      "author" : [ "Jesse Dodge", "Sujay K. Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A. Smith" ],
      "venue" : "In Proc. of NAACL",
      "citeRegEx" : "Faruqui et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Faruqui et al\\.",
      "year" : 2015
    }, {
      "title" : "Sparse overcomplete word vector representations",
      "author" : [ "Yulia Tsvetkov", "Dani Yogatama", "Chris Dyer", "Noah A. Smith" ],
      "venue" : "In Proc. of ACL",
      "citeRegEx" : "Faruqui et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Faruqui et al\\.",
      "year" : 2015
    }, {
      "title" : "Connotation lexicon: A dash of sentiment beneath the surface meaning",
      "author" : [ "Feng et al.2013] Song Feng", "Jun Seok Kang", "Polina Kuznetsova", "Yejin Choi" ],
      "venue" : "In Proc. of ACL",
      "citeRegEx" : "Feng et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2013
    }, {
      "title" : "Lexicographic relevance: selecting information from corpus evidence",
      "author" : [ "Christopher Johnson", "Miriam Petruck" ],
      "venue" : "International Journal of Lexicography",
      "citeRegEx" : "Fillmore et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Fillmore et al\\.",
      "year" : 2003
    }, {
      "title" : "Placing search in context: the concept revisited",
      "author" : [ "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin" ],
      "venue" : "In Proc. of WWW",
      "citeRegEx" : "Finkelstein et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Finkelstein et al\\.",
      "year" : 2001
    }, {
      "title" : "Incorporating both distributional and relational semantics in word representations. arXiv preprint arXiv:1412.4369",
      "author" : [ "Fried", "Duh2014] Daniel Fried", "Kevin Duh" ],
      "venue" : null,
      "citeRegEx" : "Fried et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Fried et al\\.",
      "year" : 2014
    }, {
      "title" : "Interpretable semantic vectors from a joint model of brain- and text- based meaning",
      "author" : [ "Fyshe et al.2014] Alona Fyshe", "Partha P. Talukdar", "Brian Murphy", "Tom M. Mitchell" ],
      "venue" : "In Proc. of ACL",
      "citeRegEx" : "Fyshe et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Fyshe et al\\.",
      "year" : 2014
    }, {
      "title" : "A compositional and interpretable semantic space",
      "author" : [ "Fyshe et al.2015] Alona Fyshe", "Leila Wehbe", "Partha P. Talukdar", "Brian Murphy", "Tom M. Mitchell" ],
      "venue" : "In Proc. of NAACL",
      "citeRegEx" : "Fyshe et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Fyshe et al\\.",
      "year" : 2015
    }, {
      "title" : "Revisiting embedding features for simple semi-supervised learning",
      "author" : [ "Jiang Guo", "Wanxiang Che", "Haifeng Wang", "Ting Liu" ],
      "venue" : "In Proc. of EMNLP",
      "citeRegEx" : "Guo et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2014
    }, {
      "title" : "Simlex-999: Evaluating semantic models with (genuine) similarity estimation",
      "author" : [ "Hill et al.2014] Felix Hill", "Roi Reichart", "Anna Korhonen" ],
      "venue" : null,
      "citeRegEx" : "Hill et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hill et al\\.",
      "year" : 2014
    }, {
      "title" : "Fish transporters and miracle homes: How compositional distributional semantics can help NP parsing",
      "author" : [ "Eva Maria Vecchi", "Marco Baroni" ],
      "venue" : "In Proc. of EMNLP",
      "citeRegEx" : "Lazaridou et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Lazaridou et al\\.",
      "year" : 2013
    }, {
      "title" : "English verb classes and alternations : a preliminary investigation",
      "author" : [ "Beth Levin" ],
      "venue" : null,
      "citeRegEx" : "Levin.,? \\Q1993\\E",
      "shortCiteRegEx" : "Levin.",
      "year" : 1993
    }, {
      "title" : "Building a large annotated corpus of english: The penn treebank",
      "author" : [ "Mary Ann Marcinkiewicz", "Beatrice Santorini" ],
      "venue" : null,
      "citeRegEx" : "Marcus et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Marcus et al\\.",
      "year" : 1993
    }, {
      "title" : "Online learning of approximate dependency parsing algorithms",
      "author" : [ "McDonald", "Pereira2006] Ryan T McDonald", "Fernando CN Pereira" ],
      "venue" : "In Proc. of EACL",
      "citeRegEx" : "McDonald et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "McDonald et al\\.",
      "year" : 2006
    }, {
      "title" : "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781",
      "author" : [ "Kai Chen", "Greg Corrado", "Jeffrey Dean" ],
      "venue" : null,
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Wordnet: a lexical database for english",
      "author" : [ "George A Miller" ],
      "venue" : "Communications of the ACM",
      "citeRegEx" : "Miller.,? \\Q1995\\E",
      "shortCiteRegEx" : "Miller.",
      "year" : 1995
    }, {
      "title" : "Crowdsourcing a wordemotion association lexicon",
      "author" : [ "Mohammad", "Turney2013] Saif M. Mohammad", "Peter D. Turney" ],
      "venue" : "Computational Intelligence,",
      "citeRegEx" : "Mohammad et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mohammad et al\\.",
      "year" : 2013
    }, {
      "title" : "Colourful language: Measuring word-colour associations",
      "author" : [ "Saif Mohammad" ],
      "venue" : "In Proc. of the Workshop on Cognitive Modeling and Computational Linguistics",
      "citeRegEx" : "Mohammad.,? \\Q2011\\E",
      "shortCiteRegEx" : "Mohammad.",
      "year" : 2011
    }, {
      "title" : "Learning effective and interpretable semantic models using non-negative sparse embedding",
      "author" : [ "Murphy et al.2012] Brian Murphy", "Partha Talukdar", "Tom Mitchell" ],
      "venue" : "In Proc. of COLING",
      "citeRegEx" : "Murphy et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Murphy et al\\.",
      "year" : 2012
    }, {
      "title" : "Unsupervised allwords word sense disambiguation with grammatical dependencies",
      "author" : [ "Vivi Nastase" ],
      "venue" : "In Proc. of IJCNLP",
      "citeRegEx" : "Nastase.,? \\Q2008\\E",
      "shortCiteRegEx" : "Nastase.",
      "year" : 2008
    }, {
      "title" : "Algorithms for deterministic incremental dependency parsing",
      "author" : [ "Joakim Nivre" ],
      "venue" : "Computational Linguistics,",
      "citeRegEx" : "Nivre.,? \\Q2008\\E",
      "shortCiteRegEx" : "Nivre.",
      "year" : 2008
    }, {
      "title" : "Synonyms and Antonyms: An Alphabetical List of Words in Common Use, Grouped with Others of Similar and Opposite Meaning",
      "author" : [ "Edith Bertha Ordway" ],
      "venue" : "Sully and Kleinteich",
      "citeRegEx" : "Ordway.,? \\Q1913\\E",
      "shortCiteRegEx" : "Ordway.",
      "year" : 1913
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Richard Socher", "Christopher D. Manning" ],
      "venue" : "In Proc. of EMNLP",
      "citeRegEx" : "Pennington et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "A maximum entropy model for part-of-speech tagging",
      "author" : [ "Adwait Ratnaparkhi" ],
      "venue" : "In Proc. of EMNLP",
      "citeRegEx" : "Ratnaparkhi.,? \\Q1996\\E",
      "shortCiteRegEx" : "Ratnaparkhi.",
      "year" : 1996
    }, {
      "title" : "Using information content to evaluate semantic similarity in a taxonomy",
      "author" : [ "Philip Resnik" ],
      "venue" : "In Proc. of IJCAI",
      "citeRegEx" : "Resnik.,? \\Q1995\\E",
      "shortCiteRegEx" : "Resnik.",
      "year" : 1995
    }, {
      "title" : "Contextual correlates of synonymy",
      "author" : [ "Rubenstein", "John B. Goodenough" ],
      "venue" : "Commun. ACM,",
      "citeRegEx" : "Rubenstein et al\\.,? \\Q1965\\E",
      "shortCiteRegEx" : "Rubenstein et al\\.",
      "year" : 1965
    }, {
      "title" : "Verbnet: A Broad-coverage",
      "author" : [ "Karin Kipper Schuler" ],
      "venue" : "Comprehensive Verb Lexicon. Ph.D. thesis,",
      "citeRegEx" : "Schuler.,? \\Q2005\\E",
      "shortCiteRegEx" : "Schuler.",
      "year" : 2005
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts" ],
      "venue" : "In Proc. of EMNLP",
      "citeRegEx" : "Socher et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Augmenting english adjective senses with supersenses",
      "author" : [ "Nathan Schneider", "Dirk Hovy", "Archna Bhatia", "Manaal Faruqui", "Chris Dyer" ],
      "venue" : "In Proc. of LREC",
      "citeRegEx" : "Tsvetkov et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Tsvetkov et al\\.",
      "year" : 2014
    }, {
      "title" : "From frequency to meaning : Vector space models of semantics",
      "author" : [ "Turney", "Pantel2010] Peter D. Turney", "Patrick Pantel" ],
      "venue" : null,
      "citeRegEx" : "Turney et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Turney et al\\.",
      "year" : 2010
    }, {
      "title" : "Mining the web for synonyms: Pmi-ir versus lsa on toefl",
      "author" : [ "Peter D. Turney" ],
      "venue" : "In Proc. of ECML",
      "citeRegEx" : "Turney.,? \\Q2001\\E",
      "shortCiteRegEx" : "Turney.",
      "year" : 2001
    }, {
      "title" : "Rc-net: A general framework for incorporating knowledge into word representations",
      "author" : [ "Xu et al.2014] Chang Xu", "Yalong Bai", "Jiang Bian", "Bin Gao", "Gang Wang", "Xiaoguang Liu", "Tie-Yan Liu" ],
      "venue" : "In Proc. of CIKM",
      "citeRegEx" : "Xu et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2014
    }, {
      "title" : "Improving lexical embeddings with semantic knowledge",
      "author" : [ "Yu", "Dredze2014] Mo Yu", "Mark Dredze" ],
      "venue" : "In Proc. of ACL",
      "citeRegEx" : "Yu et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "Distributed representations of words have been shown to benefit a diverse set of NLP tasks including syntactic parsing (Lazaridou et al., 2013; Bansal et al., 2014), named entity recognition (Guo et al.",
      "startOffset" : 119,
      "endOffset" : 164
    }, {
      "referenceID" : 3,
      "context" : "Distributed representations of words have been shown to benefit a diverse set of NLP tasks including syntactic parsing (Lazaridou et al., 2013; Bansal et al., 2014), named entity recognition (Guo et al.",
      "startOffset" : 119,
      "endOffset" : 164
    }, {
      "referenceID" : 16,
      "context" : ", 2014), named entity recognition (Guo et al., 2014) and sentiment analysis (Socher et al.",
      "startOffset" : 34,
      "endOffset" : 52
    }, {
      "referenceID" : 35,
      "context" : ", 2014) and sentiment analysis (Socher et al., 2013).",
      "startOffset" : 31,
      "endOffset" : 52
    }, {
      "referenceID" : 38,
      "context" : "Intrinsic evaluations on various tasks are helping refine vector learning methods to discover representations that captures many facts about lexical semantics (Turney, 2001; Turney and Pantel, 2010).",
      "startOffset" : 159,
      "endOffset" : 198
    }, {
      "referenceID" : 19,
      "context" : "cal semantic theories, which focus on identifying classes of words (Levin, 1993; Baker et al., 1998; Schuler, 2005; Miller, 1995).",
      "startOffset" : 67,
      "endOffset" : 129
    }, {
      "referenceID" : 2,
      "context" : "cal semantic theories, which focus on identifying classes of words (Levin, 1993; Baker et al., 1998; Schuler, 2005; Miller, 1995).",
      "startOffset" : 67,
      "endOffset" : 129
    }, {
      "referenceID" : 34,
      "context" : "cal semantic theories, which focus on identifying classes of words (Levin, 1993; Baker et al., 1998; Schuler, 2005; Miller, 1995).",
      "startOffset" : 67,
      "endOffset" : 129
    }, {
      "referenceID" : 23,
      "context" : "cal semantic theories, which focus on identifying classes of words (Levin, 1993; Baker et al., 1998; Schuler, 2005; Miller, 1995).",
      "startOffset" : 67,
      "endOffset" : 129
    }, {
      "referenceID" : 23,
      "context" : "Our contribution to this discussion is a new technique that constructs task-independent word vector representations using linguistic knowledge derived from pre-constructed linguistic resources like WordNet (Miller, 1995), FrameNet (Baker et al.",
      "startOffset" : 206,
      "endOffset" : 220
    }, {
      "referenceID" : 2,
      "context" : "Our contribution to this discussion is a new technique that constructs task-independent word vector representations using linguistic knowledge derived from pre-constructed linguistic resources like WordNet (Miller, 1995), FrameNet (Baker et al., 1998), Penn Treebank (Marcus et al.",
      "startOffset" : 231,
      "endOffset" : 251
    }, {
      "referenceID" : 20,
      "context" : ", 1998), Penn Treebank (Marcus et al., 1993) etc.",
      "startOffset" : 23,
      "endOffset" : 44
    }, {
      "referenceID" : 28,
      "context" : ", parsing (McDonald and Pereira, 2006; Nivre, 2008) or part of speech tagging (Ratnaparkhi, 1996; Collins, 2002).",
      "startOffset" : 10,
      "endOffset" : 51
    }, {
      "referenceID" : 31,
      "context" : ", parsing (McDonald and Pereira, 2006; Nivre, 2008) or part of speech tagging (Ratnaparkhi, 1996; Collins, 2002).",
      "startOffset" : 78,
      "endOffset" : 112
    }, {
      "referenceID" : 6,
      "context" : ", parsing (McDonald and Pereira, 2006; Nivre, 2008) or part of speech tagging (Ratnaparkhi, 1996; Collins, 2002).",
      "startOffset" : 78,
      "endOffset" : 112
    }, {
      "referenceID" : 23,
      "context" : "WordNet (Miller, 1995) is an English lexical database that groups words into sets of synonyms called synsets and records a number of relations among these synsets or their members.",
      "startOffset" : 8,
      "endOffset" : 22
    }, {
      "referenceID" : 27,
      "context" : "WordNet partitions nouns and verbs into semantic field categories known as supsersenses (Ciaramita and Altun, 2006; Nastase, 2008).",
      "startOffset" : 88,
      "endOffset" : 130
    }, {
      "referenceID" : 36,
      "context" : "These supersenses were further extended to adjectives (Tsvetkov et al., 2014).",
      "startOffset" : 54,
      "endOffset" : 77
    }, {
      "referenceID" : 2,
      "context" : "FrameNet (Baker et al., 1998; Fillmore et al., 2003) is a rich linguistic resource that contains information about lexical",
      "startOffset" : 9,
      "endOffset" : 52
    }, {
      "referenceID" : 11,
      "context" : "FrameNet (Baker et al., 1998; Fillmore et al., 2003) is a rich linguistic resource that contains information about lexical",
      "startOffset" : 9,
      "endOffset" : 52
    }, {
      "referenceID" : 25,
      "context" : "Mohammad and Turney (2013) constructed two different lexicons that associate words to sentiment polarity and to emotions resp.",
      "startOffset" : 0,
      "endOffset" : 27
    }, {
      "referenceID" : 10,
      "context" : "Feng et al. (2013) construct a lexicon that contains information about connotation of words that are seemingly objective but often allude nuanced sentiment.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 10,
      "context" : "Feng et al. (2013) construct a lexicon that contains information about connotation of words that are seemingly objective but often allude nuanced sentiment. They assign positive, negative and neutral connotations to these words. This lexicon differs from Mohammad and Turney (2013) in that it has a more subtle shade of sentiment and it extends to many more words.",
      "startOffset" : 0,
      "endOffset" : 282
    }, {
      "referenceID" : 25,
      "context" : "The word-color associtation lexicon produced by Mohammad (2011) using crowdsourcing lists the colors that a word evokes in English.",
      "startOffset" : 48,
      "endOffset" : 64
    }, {
      "referenceID" : 20,
      "context" : "The Penn Treebank (Marcus et al., 1993) annotates naturally occur-",
      "startOffset" : 18,
      "endOffset" : 39
    }, {
      "referenceID" : 29,
      "context" : "The antonymous words for a given word were collected from Ordway (1913).4 An example would be of impartiality, which has features ANTO.",
      "startOffset" : 58,
      "endOffset" : 72
    }, {
      "referenceID" : 12,
      "context" : "The first one is the widely used WS-353 dataset (Finkelstein et al., 2001),",
      "startOffset" : 48,
      "endOffset" : 74
    }, {
      "referenceID" : 17,
      "context" : "The third dataset is SimLex (Hill et al., 2014) which has been constructed to overcome the shortcomings of WS-353 and contains 999 pairs of adjectives, nouns and verbs.",
      "startOffset" : 28,
      "endOffset" : 47
    }, {
      "referenceID" : 35,
      "context" : "Socher et al. (2013) created a treebank containing sentences annotated with fine-grained sentiment labels on phrases and sentences from movie review excerpts.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 20,
      "context" : "(2013) constructed a dataset from the Penn TreeBank (Marcus et al., 1993) of noun phrases (NP) of",
      "startOffset" : 52,
      "endOffset" : 73
    }, {
      "referenceID" : 18,
      "context" : "Lazaridou et al. (2013) constructed a dataset from the Penn TreeBank (Marcus et al.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 22,
      "context" : "The first two are the pre-trained Skip-Gram (Mikolov et al., 2013)5 and Glove (Pennington et al.",
      "startOffset" : 44,
      "endOffset" : 66
    }, {
      "referenceID" : 30,
      "context" : ", 2013)5 and Glove (Pennington et al., 2014)6 word vectors each of length 300, trained on 300 billion and 6 billion words respectively.",
      "startOffset" : 19,
      "endOffset" : 44
    }, {
      "referenceID" : 7,
      "context" : "05, McNemar’s test Dietterich (1998)).",
      "startOffset" : 19,
      "endOffset" : 37
    }, {
      "referenceID" : 32,
      "context" : "(Resnik, 1995; Agirre et al., 2009).",
      "startOffset" : 0,
      "endOffset" : 35
    }, {
      "referenceID" : 0,
      "context" : "(Resnik, 1995; Agirre et al., 2009).",
      "startOffset" : 0,
      "endOffset" : 35
    }, {
      "referenceID" : 39,
      "context" : "is used as a constraint to encourage words with similar properties in lexical ontologies to have similar word vectors (Xu et al., 2014; Yu and Dredze, 2014; Bian et al., 2014; Fried and Duh, 2014; Faruqui et al., 2015a).",
      "startOffset" : 118,
      "endOffset" : 219
    }, {
      "referenceID" : 4,
      "context" : "is used as a constraint to encourage words with similar properties in lexical ontologies to have similar word vectors (Xu et al., 2014; Yu and Dredze, 2014; Bian et al., 2014; Fried and Duh, 2014; Faruqui et al., 2015a).",
      "startOffset" : 118,
      "endOffset" : 219
    }, {
      "referenceID" : 1,
      "context" : "tributional representations have also been shown to improve by using experiential data in addition to distributional context (Andrews et al., 2009).",
      "startOffset" : 125,
      "endOffset" : 147
    }, {
      "referenceID" : 26,
      "context" : "dimensions that are relatively more interpretable (Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015; Faruqui et al., 2015b).",
      "startOffset" : 50,
      "endOffset" : 134
    }, {
      "referenceID" : 14,
      "context" : "dimensions that are relatively more interpretable (Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015; Faruqui et al., 2015b).",
      "startOffset" : 50,
      "endOffset" : 134
    }, {
      "referenceID" : 15,
      "context" : "dimensions that are relatively more interpretable (Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015; Faruqui et al., 2015b).",
      "startOffset" : 50,
      "endOffset" : 134
    } ],
    "year" : 2015,
    "abstractText" : "Data-driven representation learning for words is a technique of central importance in NLP. While indisputably useful as a source of features in downstream tasks, such vectors tend to consist of uninterpretable components whose relationship to the categories of traditional lexical semantic theories is tenuous at best. We present a method for constructing interpretable word vectors from hand-crafted linguistic resources like WordNet, FrameNet etc. These vectors are binary (i.e, contain only 0 and 1) and are 99.9% sparse. We analyze their performance on state-of-the-art evaluation methods for distributional models of word vectors and find they are competitive to standard distributional approaches.",
    "creator" : "LaTeX with hyperref package"
  }
}