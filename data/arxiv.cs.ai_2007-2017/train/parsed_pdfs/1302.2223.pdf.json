{
  "name" : "1302.2223.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "WNtags: A Web-Based Tool For Image Labeling And Retrieval With Lexical Ontologies",
    "authors" : [ "Marko Horvat", "Anton Grbin", "Gordan Gledec" ],
    "emails" : [ "Gordan.Gledec}@fer.hr" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Image repositories annotated with ground truth labels are welcome tools in many areas of computer science which address the problems of visual data classification, recognition or retrieval. Prevalence of visual media on the Web stresses practical uses of such repositories. Image retrieval algorithms with high precision and recall, and low fall-out are being increasingly sought as the number of indexed pictures on the Internet is getting larger. But because building a large dataset of annotated images is a costly and lengthy endeavor, it is important to create a flexible and modular system with small footprint that reuses available off the shelve components as much as possible. These motives guided planning and implementation of the WNtags ontologybased image annotation tool.\nThe WNtags allows image tagging with weighted sets of concepts. In the current version WNtags uses WordNet (hence the tool’s name) as the knowledge database but its system architecture can accommodate concepts from other truly formal ontologies as image semantics descriptors. Apart from providing user friendly Web interface for collaborative image annotation, WNtags is also a tool for searching emotionally annotated real-life images. The image retrieval is based on semantic similarity between the search query and stored image annotations. This is accomplished using circumplex model of emotions [1], ontology-based image labels [2], corpus independent graph-\nbased distance metrics [3] which, as the entire tool, has modular implementation and can be optimized or replaced independently from other parts of the system.\nWNtags tool is different from other similar tools for manual image annotation (per example see [4][5]) in several respects. Firstly, WNtags uses International Affective Picture System (IAPS) dataset with semantically and emotionally described photographs that induce negative, neutral or positive affective reactions in stimulated persons [6]. IAPS pictures are annotated sparsely and inadequately using free-text keywords [7]. WNtags can also be viewed as a tool that significantly improves functionality and usability of this and other similar databases. Secondly, WNtags’ modular construction test enables experimentation with different algorithms in various stages of image retrieval. Thirdly, WNtags uses WordNet [8] for description of image content and in the future envisages applying Suggested Upper Merged Ontology (SUMO) [9] to formally schematize image concepts. Finally, WNtags uses weighted measures and other heuristics to more accurately describe the most semantically important concepts in a picture and to reconcile different measures of observer variability.\nThe remainder of this paper is organized as follows. Chapter 2 discusses the WNtags as the collaborative web-based research tool in the area of image annotation, knowledge representation and document retrieval and describes its architecture. Chapter 3 describes formal methods of image annotation. Chapter 4 focuses on image retrieval and discusses the results. Finally, chapter 5 gives insight into future work and concludes the paper."
    }, {
      "heading" : "2 The WNtags collaborative web-based annotation tool",
      "text" : "The WNtags is primarily a research tool in areas of image annotation, knowledge representation and multimedia document retrieval on the Web. Its development was motivated by a number of objectives and possible applications. In this section we describe the goals behind WNtags inception, its system architecture and the images used to test the tool’s annotation and retrieval capabilities. There are two groups of motives or incentives for development of WNtags tool (Table 1). The goals are divided in two broad and complementary areas in computer science and artificial intelligence: knowledge representation and image retrieval.\nThe most important goal of this project was to develop a collaborative online tool that could be modified and customized with minimal effort. In doing this, at least in the first version of WNtags, the system requirements were confined to representation of high-level semantics in static images. Some compromises had to be made since we wanted semantics annotations to be formal, but at the same close to a natural lan-\nguage. With the aid of a reasoning engine, the semantics annotations should be able to yield new knowledge about the image content. This is by no means an easy task, so we decided to use WordNet glossary to tag objects, events and affect in images. Our aim was to get an exhaustive set of relevant high-level semantics for each image in the database described with tags from a controlled glossary with graph-like structure. Such tag vocabulary can describe minimum necessary functional relationships between tags and discriminate among different meanings of the same lexical unit (i.e. word). Furthermore, we wanted all tags to be weighted to indicate their importance in aggregation of image content.\nRepresentation of picture segments and objects is currently not in our focus as with some other image annotation tools [4] [5]. However, we plan to scale out the tool to be able to annotate and store lower level semantics as well. In the future we also want to add support for other multimedia formats like sounds or video-clips, but foremost for specific domains of static images such as facial expressions and especially affectively annotated images. Our goal is to optimally reuse the existing code base and gradually build up the application’s features.\nThe secondary goal behind the WNtags project was experimentation in information retrieval using the described weighted high-level semantics from a controlled graph glossary. Instead of using statistical methods and bag-of-words concept in document indexing and retrieval, we would like to see images precisely annotated with least number of semantically meaningful tags. With formal logical systems based on Description Logic (DL) [10] these annotation may constitute a knowledge database of an intelligent expert system. Through the process of reasoning, the annotations could infer new knowledge about image content, effectively expanding the initial set of image tags. In this scheme, a search query given by a WNtags user becomes a search strategy goal in which the system must find the most closely matched images, fetch them from the database and sort the results by relevance. Procedures of user feedback and information refinement are also possible.\nFinally, WNtags could also be interpreted as a step towards improvement of emotionally annotated databases such as IAPS [6], International Affective Digitized Sounds (IADS) [11] or Geneva Affective Picture Database (GAPED) [12]. These databases of multimedia stimuli are important for research of human emotion and attention [6], but also find their uses in a wide range of research (examples [13][14]). As mentioned before, their description schemes of multimedia semantics are currently quite rudimentary and limiting making image retrieval difficult and operator intensive [7]. Enhancement of multimedia content retrieval in these databases could substantially extend their use and promote further applications in cognitive sciences, psychology, psychiatry and neurology among others areas."
    }, {
      "heading" : "3 Formal image annotation",
      "text" : "Optimally the annotation tool should not restrict the users in selection of text labels for describing images. Most users prefer to provide short and basic-level object labels (e.g. “child”, “car”, “person”, “airplane”). This is not enough to establish a rich tag glossary which would enable quality image retrieval. At the same time, there can be a\nlarge variance of terms describing the same object category. Taken together, these make analysis and retrieval of the object labels difficult, since the system has to be aware of label synonyms, subsumed labels, member labels, and distinguish between object identity, events executed by objects, and attributes of objects and events.\nWe believe that only a knowledge database based on a large upper ontology and extended with appropriate domain ontologies can encapsulate semantics present in an image repository [15]. Naturally, domain ontologies would be selected depending on the repository’s purpose and integrated with the basis upper ontology. Creating such image repository with large knowledge database and filling it with appropriately annotated images is an immense task. However, a compromise can be made with selection of WordNet as the tagging glossary.\nWordNet is a lexical database of English language [8] which may also be interpreted as an informal and lexical ontology [16]. It is readily available, simple to use and – most importantly – contains over 100,000 concepts, i.e. synsets. This is ideal in terms of glossary size and encompasses any label a user may enter. However, WordNet’s linguistic tokens lack the formality of ontological concepts. A formal ontology contains rules specifying complex relations that cannot be captured explicitly with simple links in graph-like knowledge structure. In spite of these shortcomings, WordNet is still far superior in image annotation than existing IAPS keywords, which are inconsistent, ambiguous and non-contiguous [7][17].\nIn WNtags, images are manually annotated with individual senses 1 2, , , ns s s… of\nWordNet synsets 1 2, , , msyn syn syn… . Each image is tagged with at least three senses by\na group of two or more individuals. Every sense i s is weighted in a range [0,1] i w ∈ according to its perceived relevance. Apart from weighted semantic labels, each image\ni img retained its original semantic description from IAPS i iaps . IAPS keyword vocabulary IAPSV is unsupervised and each IAPS picture is described with a single free text keyword. Effectively,\ni iaps is one keyword from this corpus. IAPS uses\ncircumplex model of affect to annotate stimuli emotion values [1][6]. In a nutshell, this model numerically describes the meaning of the emotional impulse in a 3-D space with the respect to the axes of valence or pleasure (denoted val), the axis of excitation or arousal (ar) and the axis of dominance over emotion (dom). Values of all axis are numerical and normalized in interval [1, 9].\nEvery image i img in WNtags repository retained its original IAPS emotional tuple\nvalue i emo with values i val , i ar and i dom . If i sem is set of all WordNet senses for\ni img , ˆ i sem set of weighted senses i sem , and i iaps is one keyword from IAPS vocab-\nulary then the cumulative semantic description i desc for one image i img stored in WNtags image repository from becomes a tuple\nˆ{ , , } i i i i desc sem emo iaps= (1)\nwhere\n1 2 ( , , , ) ni i i i sem s s s= …\n(2)\n1 1 2 2 ˆ ( , , , ) n ni i i i i i i sem w s w s w s= …\n( , , ) i i i i emo val ar dom= IAPS\ni i iaps V∈\nFor every sense i s WNtags considers WordNet’s built-in semantic relations hypernymy, hyponymy, holonymy and meronymy within a preset node distance d. WNtags also considers synonyms (coordinate terms) of\ni s . Therefore for every sense\nk s in WordNet database and every sense i s in i desc broadens image description with\nall senses of all its neighboring synsets\n( ) 1 2\n1 2 , , ,\n( , , , ) , n\nj i i i n\ni i i i j k\ns s s s sem s s s s s d =\n  = ∪ ≤ \n   …\n… ∪ (3)\nAs an example, IAPS annotated image (7175.jpg) with weighted labels and interaction with currently implemented WordNet knowledge database is shown in the next figure:\nAll indexed images are manually and collaboratively annotated. Personal bias of users will certainly result in different weights assigned to the image labels. Therefore,\nif there are k weight ratings for i s they are averaged out 1\n1 k i ij\nj w w k = = ∑ . Almost all\nusers agreed on sets of 3-5 distinct labels per single image while finding additional descriptions in many cases proved difficult especially for semantically simple or abstract pictures.\nIn order to prove the validity of the WNtags concept we have randomly selected 100 emotionally annotated images with different object categories from IAPS database and tagged them using 956 different WordNet synsets. On average a single image is annotated with median of 20 WordNet tags (mean = 20.56436, sd = 2.76917, min=13, max=28) which were weighted according to their subjective importance in a particular image."
    }, {
      "heading" : "4 Image retrieval",
      "text" : "The described WNtags annotating configuration allows multimodal image retrieval by three different information dimensions: affect, original free-text keywords and sets of synonyms from the WordNet lexical ontology. WNtags has modular architecture and in the current version, to save processing time, semantic similarity is loaded from the freely available WordNet::Similarity dataset with prepared similarity and relatedness pairs [18].\nThe search algorithm uses all senses of all collocations (i.e. non-permutable combinations) of query concepts. Some more complex multiword collocations may be found as specific senses in WordNet. All such detected query senses\ni qs in query k q\nare individually aligned with annotating senses i s for all images l img , 1,2,..., imgl N=\nwhere imgN is the total number of images stored in the repository. Semantic distance\nbetween each pair is calculated together with subjective importance jw of each js in\nimage l img . Search goal function is\n, max ( , ) i k j l j i j qs q s img w sim qs s ∀ ∈ ∀ ∈ ∑ (4)\nAn exhaustive search is executed and the retrieved results are sorted from the highest to the lowest relevance with possible values in range. The final results are rendered and displayed to the user.\nTo test the retrieval performance we performed semantic searches. All queries consisted of one WordNet concept randomly selected within d = 30 node distance between the nearest image tag. For example (as in Fig. 2.): “aircraft”, “car”, “boat”, “helicopter”, “road”, “bus”, etc.\nAfter N = 40 queries the average precision is 68.93 % and average recall 6.15. As expected, queries had different recall, i.e. results count. Some queries yielded only one result, while others produced over 15 IAPS images ranked according to their semantic similarity with the querying concepts. Precision is calculated for each sequence number in the result set and recall is normalized accordingly. The highest recall (R = 20) is for query “animal” because of the repository content and the fact that the most images were already manually tagged with this concept. As can be seen in the next figure, precision and recall fall as the result size increases, as expected. The first result has precision 84.21 %, second 81.58 %, third 65.78 % and so on. Number of currently indexed images is too low for a thorough evaluation of retrieval performance. However, some indicative conclusions can be drawn from these results. The aggregated and averaged results for all N = 40 queries are shown in the next figure with query count on x-axis and value range of precision and recall on y-axis. Precision of all queries is indicated with a blue dotted line, while recall is represented with a solid red line."
    }, {
      "heading" : "4.1 Discussion",
      "text" : "Overall, we found images to be tagged quite exhaustively, which is good for establishing the ground truth in image content, but can be an obstacle for retrieval with\nsemantically similar concepts leading to unrealistically high precision. As can be seen in Fig.3, precision oscillates as the number of results increases, which is a consequence of this over-tagging problem. This problem may be easily resolved if only a randomized subset of tags is used in retrieval.\nThe second problem is how to define a realistic, useful and at the same time generic measure of semantic relatedness between concepts in ontology. A variety of information-based (corpus dependent) and node distance-based (information independent) similarity measures have different successes in diverse application domains. Many such measures exist and finding the most suitable semantic similarity measure is an ongoing knowledge representation problem.\nAnother problem is objectivity of tag weight coefficients values. Although potentially a separate source of information valuable for image retrieval, weight values are a product of individual, and subsequently, potentially biased criterion. Tags with unequally distributed weights will produce erroneous relatedness with query tag, especially when coupled with suboptimal semantic distance metrics. However, kappa statistic may be used to indicate weights that have inadequate inter-annotator agreement. Tags with such weights should be re-annotated or outlier values left out and weights recalculated. Either way, tag weights have to be consistent across the whole image repository or they will deteriorate the semantic relatedness function. The best way to do this is to establish a clear and formal set of rules for calculating the weights so the influence of personal bias is annulled.\nThirdly, the structure of WordNet’s semantic network may be contrary to intuition and too complex [19]. Some subsuming concepts such as “artifact, artifact -- (a manmade object taken as a whole)” or “whole, unit -- (an assemblage of parts that is regarded as a single entity…)” are useful for construction of a well-indented taxonomy, but semantically ambiguous or seemingly redundant to an average person. Intuitively, it seems that a pruned down and lightweight version of WordNet, with new semantic relations such as “Relates-To”, “Similar-To” or “Occurs-With”, may be better as a knowledge taxonomy for image labeling than the complete WordNet, but without relations that may be lexically ambiguous but could provide fresh and important information content.\nWordNet is a huge benefit, but at the same time also a problem. Expressivity of WNtags image annotations is adequate to support content rich and diverse image extraction, but qualitatively the lack of formalism and complex functional relationships between concepts proved limiting and led to ambiguities with different concept senses. For example, a single WordNet concept “snake” has a synonyms “serpent”. An average WNtags user expects that semantic distance between all synonyms and another different concept to be the same. However, this is not the case with node distance metrics based on WordNet senses; semantic neighborhoods in WordNet semantic graph around synonyms are not identical. This leads to different image retrieval results when querying these concepts, which may be confusing to the user.\nFor this reason we noticed that it is unnecessary to observe a wide semantic neighborhood around the search query. Distance of up to 5 or 10 nodes is mostly enough to pick up the majority of concepts related to a query. Larger distances will increase recall, but substantially decrease accuracy and precision in semantic similarity. However, an adaptable algorithm may be envisioned to find a good balance between these two opposing parameters. Such algorithm could start with a small seman-\ntic neighborhood around the query and then iteratively increase the distance or include different semantic relations until enough related concepts are found or some other stopping criterion is met. Further research is required to validate this idea.\nAs mentioned before, it is obvious that only lexically rich knowledge database is not enough for high quality annotation and retrieval. Ontology with a sufficient number of annotating concepts representing objects, events and their various functional relations should allow more accurate image retrieval but also retain a rich annotating glossary which is desirable from the user’s perspective."
    }, {
      "heading" : "5 Conclusion",
      "text" : "WNtags is web-based image annotation tool that may be used to manually identify objects, events and affect that occur in images. Owing to the tool’s friendly user interface, new images can easily be added in the document repository and labeled with semantic concepts. WNtags performs image retrieval using WordNet’s ontology topology, node distance metrics and collaboratively weighted tags. The results are sorted relative to the query semantic similarity.\nThe WNtags system was developed as a flexible and modular research tool in areas of annotation and retrieval of real-life emotionally annotated images, but the problems outlined in the previous chapter vividly illustrate difficulties with using WordNet as the knowledge database for image annotation. Only a large ontology deprived of lexical dependencies is able to formally represent the ground truth meaning of image content. For this reason we plan to add SUMO to the WNtags knowledge database. In doing this we would like to retain existing WordNet image labels and map them to SUMO concepts [20]. The implemented concept relatedness metrics is another point which requires further work. A lot of false positive retrieval results are due to inaccurate similarity estimates between query and image concepts. We plan to modularly add different semantic similarity algorithms for information retrieval including information-based metrics.\nWe would also like to explore modalities of indexing additional multimedia formats apart from pictures, not withstanding specific affective-related domains of static images such as facial expressions and visual stimuli."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This research has been partially supported by the Ministry of Science, Education and Sports of the Republic of Croatia, grant no. 036-0000000-2029. The WNtags tool was developed by student team: Anton Grbin (project leader), Aleksandar Dukovski, Anton Grbin, Jerko Jurin, Dino Milačić, Matija Stepanić, Hrvoje Šimić and Dominik Trupčević as part of their Programming Project course at the University of Zagreb, Faculty of Electrical Engineering and Computing."
    } ],
    "references" : [ {
      "title" : "Emotion representation and physiology assignments in digital systems",
      "author" : [ "C. Peter", "A. Herbon" ],
      "venue" : "Interacting with Computers, Vol. 18. Elsevier",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Ontology-based photo annotation",
      "author" : [ "Schreiber", "A. Th.", "B. Dubbeldam", "Wielemaker", "B.J.J. Wielinga" ],
      "venue" : "IEEE Intelligent Systems, 16(3)",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Development and application of a metric on semantic nets",
      "author" : [ "R. Rada", "H. Mili", "E. Bicknell", "M. Blettner" ],
      "venue" : "IEEE Transations on Systems, Man, and Cybernetics, 19(1)",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "LabelMe: a database and webbased tool for image annotation",
      "author" : [ "B.C. Russell", "A. Torralba", "K.P. Murphy", "W.T. Freeman" ],
      "venue" : "Technical report, MIT",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "ImageNet: a large-scale hierarchical image database",
      "author" : [ "J. Deng", "W. Dong", "R. Socher", "Li", "L.-J.", "K. Li", "L. Fei-Fei" ],
      "venue" : "Proc. CVPR.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "International affective picture system (IAPS): Affective ratings of pictures and instruction manual (Technical Report A-8)",
      "author" : [ "P.J. Lang", "M.M. Bradley", "B.N. Cuthbert" ],
      "venue" : "University of Florida, Gainesville, FL",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Tagging multimedia stimuli with ontologies",
      "author" : [ "M. Horvat", "S. Popović", "N. Bogunović", "K. Ćosić" ],
      "venue" : "Proceedings of the 32nd International Convention MIPRO 2009, Croatian Society for Information and Communication Technology, Electronics and Microelectronics – MIPRO, Opatija, Croatia",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "WordNet: An Electronic Lexical Database",
      "author" : [ "C. Fellbaum" ],
      "venue" : "MIT Press",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Ontology: A Practical Guide",
      "author" : [ "A. Pease" ],
      "venue" : "Articulate Software Press, Angwin, CA",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Semantic Web for the Working Ontologist: Effective Modeling in RDFS and OWL",
      "author" : [ "D. Allemang", "J. Hendler" ],
      "venue" : "Elsevier",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "International affective digitized sounds (IADS): Stimuli, instruction manual and affective ratings (Tech",
      "author" : [ "P.J. Lang", "M.M. Bradley", "B.N. Cuthbert" ],
      "venue" : "Rep. No. B-2). The Center for Research in Psychophysiology, University of Florida, U.S.A.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Towards Modeling of Readers’ Emotional State Response for the Automated Annotation of Documents",
      "author" : [ "D. Tsonos", "K. Ikospentaki", "G. Kouroupetrolgou" ],
      "venue" : "IEEE World Congress on Computational Intelligence (WCCI 2008)",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Facial expressions and complex IAPS pictures",
      "author" : [ "J.C. Britton", "S.F. Taylor", "K.D. Sudheimer", "I. Liberzo" ],
      "venue" : "Common and differential networks. NeuroImage,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2006
    }, {
      "title" : "Ontology-based reasoning techniques for multimedia interpretation and retrieval",
      "author" : [ "B. Neumann", "R. Mӧller" ],
      "venue" : "In Semantic Multimedia and Ontologies: Theory and Applications, Springer",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Towards a standard upper ontology",
      "author" : [ "Niles I", "A. Pease" ],
      "venue" : "Proceedings of the international conference on Formal Ontology in Information Systems,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2001
    }, {
      "title" : "Physiology-driven adaptive virtual reality stimulation for prevention and treatment of stress related disorders",
      "author" : [ "K. Ćosić", "S. Popović", "D. Kukolja", "M. Horvat", "B. Dropuljić" ],
      "venue" : "CyberPsychology, Behavior, and Social Networking, Vol. 13:1",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "WordNet::Similarity - Measuring the Relatedness of Concepts",
      "author" : [ "T. Pedersen", "S. Patwardhan", "J. Michelizzi" ],
      "venue" : "Proceedings of the Nineteenth National Conference on Artificial Intelligence (AAAI-04), San Jose, CA (Intelligent Systems Demonstration)",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Semantic Similarity in a Taxonomy: An Information-Based Measure and its Application to Problems of Ambiguity in Natural Language",
      "author" : [ "P. Resnik" ],
      "venue" : "Journal Of Artificial Intelligence Research, Vol. 11",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Linking lexicons and ontologies: Mapping wordnet to the suggested upper merged ontology",
      "author" : [ "Niles I.", "A. Pease" ],
      "venue" : "In Proc. of the 2003 Int. Conf. on Information and Knowledge Engineering",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "This is accomplished using circumplex model of emotions [1], ontology-based image labels [2], corpus independent graph-",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 1,
      "context" : "This is accomplished using circumplex model of emotions [1], ontology-based image labels [2], corpus independent graph-",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 2,
      "context" : "based distance metrics [3] which, as the entire tool, has modular implementation and can be optimized or replaced independently from other parts of the system.",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 3,
      "context" : "WNtags tool is different from other similar tools for manual image annotation (per example see [4][5]) in several respects.",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 4,
      "context" : "WNtags tool is different from other similar tools for manual image annotation (per example see [4][5]) in several respects.",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 5,
      "context" : "Firstly, WNtags uses International Affective Picture System (IAPS) dataset with semantically and emotionally described photographs that induce negative, neutral or positive affective reactions in stimulated persons [6].",
      "startOffset" : 215,
      "endOffset" : 218
    }, {
      "referenceID" : 6,
      "context" : "IAPS pictures are annotated sparsely and inadequately using free-text keywords [7].",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 7,
      "context" : "Thirdly, WNtags uses WordNet [8] for description of image content and in the future envisages applying Suggested Upper Merged Ontology (SUMO) [9] to formally schematize image concepts.",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 8,
      "context" : "Thirdly, WNtags uses WordNet [8] for description of image content and in the future envisages applying Suggested Upper Merged Ontology (SUMO) [9] to formally schematize image concepts.",
      "startOffset" : 142,
      "endOffset" : 145
    }, {
      "referenceID" : 3,
      "context" : "Representation of picture segments and objects is currently not in our focus as with some other image annotation tools [4] [5].",
      "startOffset" : 119,
      "endOffset" : 122
    }, {
      "referenceID" : 4,
      "context" : "Representation of picture segments and objects is currently not in our focus as with some other image annotation tools [4] [5].",
      "startOffset" : 123,
      "endOffset" : 126
    }, {
      "referenceID" : 9,
      "context" : "With formal logical systems based on Description Logic (DL) [10] these annotation may constitute a knowledge database of an intelligent expert system.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 5,
      "context" : "Finally, WNtags could also be interpreted as a step towards improvement of emotionally annotated databases such as IAPS [6], International Affective Digitized Sounds (IADS) [11] or Geneva Affective Picture Database (GAPED) [12].",
      "startOffset" : 120,
      "endOffset" : 123
    }, {
      "referenceID" : 10,
      "context" : "Finally, WNtags could also be interpreted as a step towards improvement of emotionally annotated databases such as IAPS [6], International Affective Digitized Sounds (IADS) [11] or Geneva Affective Picture Database (GAPED) [12].",
      "startOffset" : 173,
      "endOffset" : 177
    }, {
      "referenceID" : 5,
      "context" : "These databases of multimedia stimuli are important for research of human emotion and attention [6], but also find their uses in a wide range of research (examples [13][14]).",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 11,
      "context" : "These databases of multimedia stimuli are important for research of human emotion and attention [6], but also find their uses in a wide range of research (examples [13][14]).",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 12,
      "context" : "These databases of multimedia stimuli are important for research of human emotion and attention [6], but also find their uses in a wide range of research (examples [13][14]).",
      "startOffset" : 168,
      "endOffset" : 172
    }, {
      "referenceID" : 6,
      "context" : "As mentioned before, their description schemes of multimedia semantics are currently quite rudimentary and limiting making image retrieval difficult and operator intensive [7].",
      "startOffset" : 172,
      "endOffset" : 175
    }, {
      "referenceID" : 13,
      "context" : "We believe that only a knowledge database based on a large upper ontology and extended with appropriate domain ontologies can encapsulate semantics present in an image repository [15].",
      "startOffset" : 179,
      "endOffset" : 183
    }, {
      "referenceID" : 7,
      "context" : "WordNet is a lexical database of English language [8] which may also be interpreted as an informal and lexical ontology [16].",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 14,
      "context" : "WordNet is a lexical database of English language [8] which may also be interpreted as an informal and lexical ontology [16].",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 6,
      "context" : "In spite of these shortcomings, WordNet is still far superior in image annotation than existing IAPS keywords, which are inconsistent, ambiguous and non-contiguous [7][17].",
      "startOffset" : 164,
      "endOffset" : 167
    }, {
      "referenceID" : 15,
      "context" : "In spite of these shortcomings, WordNet is still far superior in image annotation than existing IAPS keywords, which are inconsistent, ambiguous and non-contiguous [7][17].",
      "startOffset" : 167,
      "endOffset" : 171
    }, {
      "referenceID" : 0,
      "context" : "Every sense i s is weighted in a range [0,1] i w ∈",
      "startOffset" : 39,
      "endOffset" : 44
    }, {
      "referenceID" : 0,
      "context" : "circumplex model of affect to annotate stimuli emotion values [1][6].",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 5,
      "context" : "circumplex model of affect to annotate stimuli emotion values [1][6].",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 0,
      "context" : "Values of all axis are numerical and normalized in interval [1, 9].",
      "startOffset" : 60,
      "endOffset" : 66
    }, {
      "referenceID" : 8,
      "context" : "Values of all axis are numerical and normalized in interval [1, 9].",
      "startOffset" : 60,
      "endOffset" : 66
    }, {
      "referenceID" : 16,
      "context" : "WNtags has modular architecture and in the current version, to save processing time, semantic similarity is loaded from the freely available WordNet::Similarity dataset with prepared similarity and relatedness pairs [18].",
      "startOffset" : 216,
      "endOffset" : 220
    }, {
      "referenceID" : 17,
      "context" : "Thirdly, the structure of WordNet’s semantic network may be contrary to intuition and too complex [19].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 18,
      "context" : "In doing this we would like to retain existing WordNet image labels and map them to SUMO concepts [20].",
      "startOffset" : 98,
      "endOffset" : 102
    } ],
    "year" : 2012,
    "abstractText" : "Ever growing number of image documents available on the Internet continuously motivates research in better annotation models and more efficient retrieval methods. Formal knowledge representation of objects and events in pictures, their interaction as well as context complexity becomes no longer an option for a quality image repository, but a necessity. We present an ontologybased online image annotation tool WNtags and demonstrate its usefulness in several typical multimedia retrieval tasks using International Affective Picture System emotionally annotated image database. WNtags is built around WordNet lexical ontology but considers Suggested Upper Merged Ontology as the preferred labeling formalism. WNtags uses sets of weighted WordNet synsets as high-level image semantic descriptors and query matching is performed with word stemming and node distance metrics. We also elaborate our near future plans to expand image content description with induced affect as in stimuli for research of human emotion and attention.",
    "creator" : "PScript5.dll Version 5.2.2"
  }
}