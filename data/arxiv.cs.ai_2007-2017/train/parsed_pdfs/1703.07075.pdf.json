{
  "name" : "1703.07075.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Pseudorehearsal in value function approximation",
    "authors" : [ "Vladimir Marochko", "Leonard Johard", "Manuel Mazzara" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: Reinforcement learning, rehearsal, pseudorehearsal, catastrophic forgetting"
    }, {
      "heading" : "1 Introduction",
      "text" : "Reinforcement learning is a more general problem formulation than the commonly used supervised learning framework. As such, it can be applied to a wider range of problems. It is also a more difficult problem to optimize for, as the feedback is more limited.\nReinforcement learning covers space of prediction and control problems in partially unknown space with unknown behavior. The agent should explore the environment and find optimal actions it has to perform to reach the goal. It is used in cases when the optimal policy is unknown so there is no way to train agents using supervised learning algorithms. The basic idea can be described by the metaphor of a player starting an unknown game and, after a number of turns, he/she receives a message stating that he/she has lost or won. After a number of games he/she will figure out how to act to win as often as possible."
    }, {
      "heading" : "1.1 Supervised agents in reinforcement learning problems",
      "text" : "In order to solve the practical problems that can be assumed to be approximately Markov decision process (MDP) [1], like robot’s navigation, playing chess or trading on stock exchange, we can use a value function approximation to speed up the learning process [2] [3]. The weakness of this approach is that convergence is not guaranteed if the MDP approximation is incorrect, or in cases where the inputs are continuous, and which necessitates non-linear function approximation [4]. Furthermore if we have continuous outputs the value approximation needs to be combined with an additional optimization technique, such as REINFORCE [5], in order to search for optimal outputs. If the outputs are discrete, a simple ar X\niv :1\n70 3.\n07 07\n5v 1\n[ cs\n.A I]\n2 1\nM ar\n2 01\n7\nmaximization is sufficient and allows us to make use of the Q-learning framework [6][7].\nAlthough a value function simplifies the learning problem by effectively converting a reinforcement learning problem to a supervised learning problem through the use of bootstrapping [8], it is still a more difficult problem than conventional supervised learning. One of these additional difficulties is that the policydependent rewards introduces a concept drift [9]. This introduces the risk of unstable oscillations, but is generally solvable at the cost of slower learning if the learning rate is set sufficiently small in the beginning. This ill conditioning of the problem has been considered one of the main challenges of reinforcement learning."
    }, {
      "heading" : "1.2 Catastrophic forgetting",
      "text" : "A different problem which recently got more attention is catastrophic forgetting [10]. This problem is most commonly described in an online unsupervised Hebbian learning task, where the ability to retrieve previously stored patterns is lost as we update weights in the training of new patterns.This catastrophic forgetting of the original patterns takes place even when parameter space is more than sufficient to store both sets of patterns and is a consequence of the limited mixing of input objects.\nSimply alternating between the new and old patterns group with a sufficiently low learning rate would in theory solve the problem, but this has a potential impact on the convergence rate and requires explicit memorization of all training patterns. In order to minimize the effect on convergence rate, we would like to maximize the mixing of the presented inputs."
    }, {
      "heading" : "2 Catastrophic forgetting in reinforcement learning",
      "text" : "The online nature of reinforcement learning means that catastrophic forgetting is a key bottleneck. There are two principal non-sharpening approaches to the catastrophic forgetting problem suggested in literature: rehearsal and pseudorehearsal. In addition, other methods based on sparse representations [11] [12] have been used less frequently. This latter approach has a theoretical downside in its negative impact on the ability to generalize, but has shown at least mixed results and is possible to use in conjunction with the rehearsal methods."
    }, {
      "heading" : "2.1 Rehearsal approaches",
      "text" : "The first and most straight-forward principal approach for mitigating catastrophic forgetting is rehearsal [13], [14]. A rehearsal strategy simply stores a subset of all previous experiences in a buffer. When a new pattern is presented, this pattern is combined with several patterns from the buffer in order to form a learning batch with good mixing. There are several possible heuristics for selecting patterns for rehearsal.\nThe importance of catastrophic forgetting in reinforcement learning was identified early. Lin introduced the term Experience Replay [15] for referring to the\nuse of rehearsal strategies in the reinforcement learning setting. Such rehearsal has shown very promising results in robotics [16] and on more complex environments, such as Deep Q-learning for playing Atari games [17]."
    }, {
      "heading" : "2.2 Pseudorehearsal approaches",
      "text" : "A second principal approach to solving catastrophic forgetting is pseudorehearsal [18], which does not require explicit storage of patterns. Instead, it uses a twostep process where generative models are learnt alongside with the main task. These generative models create pseudopatterns, which are combined in batches with real patterns for training the agent.\nAn interesting questions is whether these generated approximations of the real data are sufficiently accurate in practice to reduce forgetting. Remarkably, even extremely crude generative models have proven highly effective. In the original work in this area by [18], pure noise fed to the network was able to almost completely eliminate catastrophic interference. The argument of the authors was that, although the input is completely random, the activation distributions in deeper levels of the network will be representative of the learnt input data.\nAn analytical approach by Frean and Robins [19] in single perceptrons suggest an alternative explanation for the surprising efficiency of random pseudopatterns. They suggest that the pseudopatterns approximate the mean of the input. Training on this mean of the input leads to decorrelation of the input patterns, which in high dimensional inputs makes the different patterns’ weight updates orthogonal to each other. In addition, they demonstrated that using this mean directly was at least efficient as generating pseudopatterns. Further work in this direction was done in a thesis by Goodrich [20], where some of these results where expanded to multilayer perceptrons.\nRegardless of the reason for such networks, pseudorehearsal methods have been demonstrated to significantly decrease and almost completely eliminate the catastrophic forgetting in unsupervised learning [18], supervised learning [13] and reinforcement learning [21]. It is interesting to note that the results of Baddeley suggest that the widely studied ill conditioning might not be the main bottleneck of reinforcement learning after all. Instead, their results indicate that the catastrophic forgetting is the main bottleneck for reinforcement learning problems.\nPseudorehearsal algorithms For testing pseudorehearsal approach we used two different pseudorehearsal types and the online learning with one backpropagation step as an example of learning without pseudorehearsal. One algorithm is based on correcting of the weight updates, other is a batch-backpropagation learning.\nThe first pseudorehearsal algorithm is the one used by Frean and Robins [19] with a simplified weighting equation and changed for non-linear neural network inner assignments. The idea behind algorithm is to generate pseudoset, feed it through the network and save activations on each neuron for every pseudoexample. Then the agent is learned online, but when the real example fed to the network we use equation\n∆wi = errbi 1 pr ∑pr j=1 bixij ·xij−xijxij ·bi bi·bixij ·xij−bi·xijbi·xij\nto update wi - weights at the i th layer, where errbi - vector backpropagation errors of the learned example at the ith layer, pr - size of pseudoset, bi - vector of activations of the learned example when fed forward through the network and xij is vector of activations of the j\nth pseudoset on the ith layer. The second one - is straight-forward using pseudosets in batch backpropagation learning - we generate set of pseudoexamples, feed it through the network, save the network outputs as the targets and then create a matrix of feature vectors where first vector is real example, others are pseudovectors, and matrix of targets where first vector is target for the real example and others are saved earlier networks outputs on pseudoset, then each time we learn agent on the whole set."
    }, {
      "heading" : "2.3 Biological forgetting",
      "text" : "An interesting particular case of catastrophic forgetting problem is learning in the human brain. Dual network models were initially inspired by biological learning [22]. As a consequence of promising experimental results of such networks, pseudorehearsal was indeed found to be the most plausible explanation for the otherwise cryptic need for dual learning systems in the brain [23].\nMore biologically detailed extensions of these models have recently been explored by Hattori, where they again showed excellent improvements on the ability to store information [24].\nThe pseudorehearsal approach also contributes to the urgent need for new biological plasticity rules in large scale neurosimulation and especially for their developmental varieties (e.g. BioDynaMo [25]). Real full scale brain simulation is approaching, but we are still lacking even a basic understanding of the role dreams play in the learning process. This despite the fact that sleep stages are of considerable length and evident in even the simplest of biological neural networks."
    }, {
      "heading" : "3 Experimental design",
      "text" : "We will reevaluate the analytic results of Frean and Robins [19] in a real reinforcement learning task. We evaluate and compare two algorithms for pseudorehearsal on a pole balancing task using Q-learning with function approximation. Further, we will study the effect of sparsity in fulfilling the requirement for a high dimensional input space these algorithms relied on. Our agent is a classic Q-learning agent with -greedy policy using a feed-forward-backpropagation neural network as function approximator, discounted factor of agent is 0.9. The environments used for training is the single-pole balancing cart.\nObservation Two different observations are used for experimental comparison. The first observation type given to the agents constitute a fully observable MDP\nand includes current position, velocity, acceleration of the cart, as well as the current angle, angular velocity and angular acceleration of the pole. The second observation type make the problem partially observable - here the agent knows only cart’s position and pole’s angle. If the cart reaches the end of track or the pole falls for angle more than predefined pole failing angle - the game is lost and the agent is gained negative reward for this task.\nWe represented the observations as a feature vector by two different methods. The first method was a representation of input values where the i-th observation value sets the 2 ∗ i-th feature if it is positive or on 2 ∗ i + 1-th if it is negative. The second method was to use sparse unary vectors where feature vector is concatenation of parameter vectors, similar to a table. Each of parameter vectors consisted of elements associated with discrete values inside the range possible for each parameter - [−20; 20] for linear parameters, [−60; 60] for angular. All the elements of the vector were set to zeros except two - the element associated with the rounded value of the parameter was set to one, and the next element is set to the fractional part of this parameter.\nPerformance metric Agent tries to balance pole or poles as long as it can for 5000 tries, for two sets of parameters we also made an averaged variants where 10 iterations of this 5000 tries are averaged to make sure that convergence tendency is reproducible and not a set of random successful moves. We also have results for fully random policy to compare with.\nParameter settings The task was repeated with different sizes of pseudoitem batches, with different numbers of iterations between reinitialization of the pseudosets and with different learning rates. The learning rates used were 0.1, 0.01 and 0.001. The discount factor was set to 0.9. The sizes of pseudosets were 10, 30, 50 and 100 pseudoitems, respectively. We resample a new set of pseudoitems after every 1, 10 or 100 runs. Parameters are chosen to define influence of size of the pseudoset and frequency of it’s reinitialisation on learning. We try to cover a wide range without trying all the possible values. And we decrease parameters in close to geometrical progression to see if the influence is logarithmic. For 30 and 50 item pseudorehearsal batches we also tried 30 and 50 reinitialisation gaps to increase coverage.\nPerformance metrics We did not stop learning after an agent reaches satisfactory result, because the continued learning contains cases of catastrophic forgetting which we would like to explore. As we had a very short learning time during which the performance increases followed by long row of tries with unstable behavior, we evaluate the efficiency of different approaches by measuring mean and median number of steps per try for each approach and compare these two numbers.\nMean > median indicates that some agent’s tries were highly effective and agent balanced pole for a long time, while the most of runs were weak, so no convergence occurred or the influence of catastrophic interference is too high to handle needed weights for a long time. Mean < median shows that agent successfully converge to some optimal policy, and it’s policy is stable but some tries are failed so bad that it affected the whole picture, so in this case we can\nsee successful learning, strong influence of catastrophic forgetting and successful avoidance of this influence. Mean ≈ meadian means that catastrophic forgetting and its avoidance has nearly the same influence. Results were averaged over ten\nruns when training times allowed and other cases we presents results over single runs."
    }, {
      "heading" : "3.1 Results",
      "text" : "Results for all observations show different learning for different cases, while averaged approach and comparison with the random agent assure us that the learning has place and in case without pseudorehearsal it depends on learning rate mostly. For the agents using pseudorehearsal learning depends on sets of parameters - learning rate, pseudoset size and relearning gap. Some of them can make agent to balance pole for significantly larger time than a random run, while the others perform the same, or worse, or even worse than a random agent.\nFor both used metrics we discovered a roughly bell-shaped graph of dependencies for each of used learning rates and for each of used techniques. All pseudorehearsal approaches have different sets of parameters providing optimal learning, a suboptimal and worse - in case of parameters a little different from the optimal ones. Further away from optimal parameters agents started to diverge, e.g. tried to drop the pole about four times faster than if it would use random policy, etc.\nBoth Frean-Robins and batch approaches perform similarly in for this observation, but their optimal parameters differ for the same learning rates. All this results are summarized in table 1 for MDPs and table 2 for partially observable MDPs - POMDPs."
    }, {
      "heading" : "3.2 MDP",
      "text" : "For the cases of fully observable MDP, where agent knows anything about the current state of the pole cart and the pole the learning time when the agent’s performance goes from some initial random to the final one is very short, agent quickly converges at some number of steps it can balance and most of its next moves holds around this result with some deviations, sometimes very large, caused by catastrophic forgetting. If some learning case makes agent signifi-\ncantly change its behaviour - change is as quick as initial learning and on graph looks like immediate change of performance."
    }, {
      "heading" : "3.3 POMDP",
      "text" : "Partial observability tends to suffer less from forgetting, possibly because each part of the smaller space are more frequently visited. Pseudorehearsal has a more significant impact here: although it shares the same optimal-suboptimal-worst sets of parameters, optimal ones further decrease the number of runs needed to learn and decrease influence of the catastrophic forgetting: if the agent in current set of parameters doesn’t diverges to the worst possible case - it’s median for all runs is always higher then the mean, indicating a relatively stable behaviour after training. On the other hand, agents in POMDPs agents that can’t converge change their policy more frequently than agents in MDPs, and while the agent in fully observable MDP has a minor chance to reach good performance after it reached a suboptimal solution, agent in POMDP easily changes its policy both ways.\nThe pseudorehearsal approach taken from Frean and Robins decreases this serious context switches and all the agents using this type of pseudorehearsal show nearly the same performance during the all runs, holding around some value with occasional deviations, results much better or worse can be met, but they are rare compared to the results close to this mean. Stability is maintained for all sets of parameters, while the mean value can differ. Different effect is caused by batch-backpropacation learning using pseudosets: picture of agent’s performance is the same as in learning without pseudorehearsal, but efficiency switching occurs only after reinitialization of the pseudoitem vector. While same pseudorehearsal parameters may lead to different agent’s behaviour, the efficiency of each set of parameters evaluated by averaging for ten iterations shows that some sets make agent to increase it’s performance during the time, while the others do not."
    }, {
      "heading" : "4 Discussion and conclusions",
      "text" : "The experiment has shown us that pseudorehearsal can deal with catastrophic interference, but it has its own effects which in some cases cause divergence that worsen performance, so this tool should be used carefully and the parameters - learning rate, pseudoset size and relearning frequency have to be chosen properly to guarantee high performance on the current task.\nFor the fully observable MDPs pseudorehearsal decreases influence of the catastrophic forgetting if the optimal parameters for the task are known. In the best cases, optimal performance was reached quickly with pseudorehearsal, but the further parameters from the optimal, the worse performance was. In the case if modeling this environment might be too complex - optimal parameters can be defined only empirically before starting learning, which may be unacceptable if the cost of mistake is high.\nFor partially observable environments all the problems met by fully observable ones remain the same and some additional effects were noted: agent’s policy doesn’t only converge, diverge or stay random, but also converge to some value with the majority of tries having results in a some range around this value, and strongly deviating runs are more rare and separated by wide gaps of convergence. Another notable effect of the pseudorehearsal in POMDP agents with both pseudorehearsal cases is a significant decrease of the number of steps to converge to the number of steps needed by fully observable agent. If an agent in fully observable environment can converge it converges at about 20-30 runs as with pseudorehearsal, so without, if agent in partially observable environment can converge it converges at 100-250 runs without pseudorehearsal and at 10-30 runs with pseudorehearsal.\nPseudorehearsal is known to be a powerful tool for improving performance of supervised learning agents. We have shown that it can be useful to assist learning even in relatively quickly mixing continuous reinforcement learning tasks, if parameters are chosen correctly. Pseudorehearsal reduces this forgetting effect and maintains stable solutions for longer. While pseudorehearsal may strongly improve agent’s performance and accelerate learning, empirical defining of the\noptimal pseudoset size and relearning gap is required. One of possible extension of this research would be exploration of mathematical way to figure out this parameters. We will also explore new, more complex reinforcement learning challenges and try more advanced dual network generation of pseudoexamples."
    } ],
    "references" : [ {
      "title" : "A survey of pomdp solution",
      "author" : [ "K.P. Murphy" ],
      "venue" : "techniques,” environment,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2000
    }, {
      "title" : "A connectionist actor-critic algorithm for faster learning and biological plausibility,",
      "author" : [ "L. Johard", "E. Ruffaldi" ],
      "venue" : "IEEE International Conference on Robotics and Automation (ICRA). IEEE,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "Learning to predict by the methods of temporal differences,",
      "author" : [ "R.S. Sutton" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1988
    }, {
      "title" : "An analysis of temporal-difference learning with function approximation,",
      "author" : [ "J.N. Tsitsiklis", "B. Van Roy" ],
      "venue" : "IEEE transactions on automatic control,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1997
    }, {
      "title" : "Simple statistical gradient-following algorithms for connectionist reinforcement learning,",
      "author" : [ "R.J. Williams" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1992
    }, {
      "title" : "A brief survey of parametric value function approximation,",
      "author" : [ "M. Geist", "O. Pietquin" ],
      "venue" : "Rapport interne, Supélec,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2010
    }, {
      "title" : "Reinforcement learning: An introduction",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : "MIT press Cambridge,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1998
    }, {
      "title" : "On evaluating stream learning algorithms,",
      "author" : [ "J. Gama", "R. Sebastião", "P.P. Rodrigues" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2013
    }, {
      "title" : "Catastrophic interference in connectionist networks: The sequential learning problem,",
      "author" : [ "M. McCloskey", "N.J. Cohen" ],
      "venue" : "Psychology of learning and motivation,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1989
    }, {
      "title" : "Semi-distributed representations and catastrophic forgetting in connectionist networks,",
      "author" : [ "R.M. French" ],
      "venue" : "Connection Science,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1992
    }, {
      "title" : "Connectionist models of recognition memory: constraints imposed by learning and forgetting functions.",
      "author" : [ "R. Ratcliff" ],
      "venue" : "Psychological review,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1990
    }, {
      "title" : "Using fast weights to deblur old memories,",
      "author" : [ "G.E. Hinton", "D.C. Plaut" ],
      "venue" : "Proceedings of the ninth annual conference of the Cognitive Science Society,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1987
    }, {
      "title" : "Reinforcement learning for robots using neural networks,",
      "author" : [ "L.-J. Lin" ],
      "venue" : "DTIC Document, Tech. Rep.,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1993
    }, {
      "title" : "Experience replay for real-time reinforcement learning control,",
      "author" : [ "S. Adam", "L. Busoniu", "R. Babuska" ],
      "venue" : "IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews),",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2012
    }, {
      "title" : "Catastrophic forgetting, rehearsal and pseudorehearsal,",
      "author" : [ "A. Robins" ],
      "venue" : "Connection Science,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1995
    }, {
      "title" : "Catastrophic forgetting in simple networks: an analysis of the pseudorehearsal solution,",
      "author" : [ "M. Frean", "A. Robins" ],
      "venue" : "Network: Computation in Neural Systems,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1999
    }, {
      "title" : "Neuron clustering for mitigating catastrophic forgetting in supervised and reinforcement learning,",
      "author" : [ "B.F. Goodrich" ],
      "venue" : "Ph.D. dissertation, University of Tennessee,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2015
    }, {
      "title" : "Reinforcement learning in continuous time and space: Interference and not ill conditioning is the main problem when using distributed function approximators,",
      "author" : [ "B. Baddeley" ],
      "venue" : "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics),",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2008
    }, {
      "title" : "Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory.",
      "author" : [ "J.L. McClelland", "B.L. McNaughton", "R.C. O’Reilly" ],
      "venue" : "Psychological review,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1995
    }, {
      "title" : "The consolidation of learning during sleep: comparing the pseudorehearsal and unlearning accounts,",
      "author" : [ "A. Robins", "S. McCallum" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1999
    }, {
      "title" : "Talanov, “The biodynamo project: Creating a platform for large-scale reproducible biological simulations,",
      "author" : [ "L. Breitwieser", "R. Bauer", "A.D. Meglio", "L. Johard", "M. Kaiser", "M. Manca", "M. Mazzara", "F. Rademakers" ],
      "venue" : "4th Workshop on Sustainable Software for Science: Practice and Experiences",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "In order to solve the practical problems that can be assumed to be approximately Markov decision process (MDP) [1], like robot’s navigation, playing chess or trading on stock exchange, we can use a value function approximation to speed up the learning process [2] [3].",
      "startOffset" : 111,
      "endOffset" : 114
    }, {
      "referenceID" : 1,
      "context" : "In order to solve the practical problems that can be assumed to be approximately Markov decision process (MDP) [1], like robot’s navigation, playing chess or trading on stock exchange, we can use a value function approximation to speed up the learning process [2] [3].",
      "startOffset" : 260,
      "endOffset" : 263
    }, {
      "referenceID" : 2,
      "context" : "In order to solve the practical problems that can be assumed to be approximately Markov decision process (MDP) [1], like robot’s navigation, playing chess or trading on stock exchange, we can use a value function approximation to speed up the learning process [2] [3].",
      "startOffset" : 264,
      "endOffset" : 267
    }, {
      "referenceID" : 3,
      "context" : "The weakness of this approach is that convergence is not guaranteed if the MDP approximation is incorrect, or in cases where the inputs are continuous, and which necessitates non-linear function approximation [4].",
      "startOffset" : 209,
      "endOffset" : 212
    }, {
      "referenceID" : 4,
      "context" : "Furthermore if we have continuous outputs the value approximation needs to be combined with an additional optimization technique, such as REINFORCE [5], in order to search for optimal outputs.",
      "startOffset" : 148,
      "endOffset" : 151
    }, {
      "referenceID" : 5,
      "context" : "maximization is sufficient and allows us to make use of the Q-learning framework [6][7].",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 6,
      "context" : "Although a value function simplifies the learning problem by effectively converting a reinforcement learning problem to a supervised learning problem through the use of bootstrapping [8], it is still a more difficult problem than conventional supervised learning.",
      "startOffset" : 183,
      "endOffset" : 186
    }, {
      "referenceID" : 7,
      "context" : "One of these additional difficulties is that the policydependent rewards introduces a concept drift [9].",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 8,
      "context" : "A different problem which recently got more attention is catastrophic forgetting [10].",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 9,
      "context" : "In addition, other methods based on sparse representations [11] [12] have been used less frequently.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 10,
      "context" : "The first and most straight-forward principal approach for mitigating catastrophic forgetting is rehearsal [13], [14].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 11,
      "context" : "The first and most straight-forward principal approach for mitigating catastrophic forgetting is rehearsal [13], [14].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 12,
      "context" : "Lin introduced the term Experience Replay [15] for referring to the",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 13,
      "context" : "Such rehearsal has shown very promising results in robotics [16] and on more complex environments, such as Deep Q-learning for playing Atari games [17].",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 14,
      "context" : "A second principal approach to solving catastrophic forgetting is pseudorehearsal [18], which does not require explicit storage of patterns.",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 14,
      "context" : "In the original work in this area by [18], pure noise fed to the network was able to almost completely eliminate catastrophic interference.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 15,
      "context" : "An analytical approach by Frean and Robins [19] in single perceptrons suggest an alternative explanation for the surprising efficiency of random pseudopatterns.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 16,
      "context" : "Further work in this direction was done in a thesis by Goodrich [20], where some of these results where expanded to multilayer perceptrons.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 14,
      "context" : "Regardless of the reason for such networks, pseudorehearsal methods have been demonstrated to significantly decrease and almost completely eliminate the catastrophic forgetting in unsupervised learning [18], supervised learning [13] and reinforcement learning [21].",
      "startOffset" : 202,
      "endOffset" : 206
    }, {
      "referenceID" : 10,
      "context" : "Regardless of the reason for such networks, pseudorehearsal methods have been demonstrated to significantly decrease and almost completely eliminate the catastrophic forgetting in unsupervised learning [18], supervised learning [13] and reinforcement learning [21].",
      "startOffset" : 228,
      "endOffset" : 232
    }, {
      "referenceID" : 17,
      "context" : "Regardless of the reason for such networks, pseudorehearsal methods have been demonstrated to significantly decrease and almost completely eliminate the catastrophic forgetting in unsupervised learning [18], supervised learning [13] and reinforcement learning [21].",
      "startOffset" : 260,
      "endOffset" : 264
    }, {
      "referenceID" : 15,
      "context" : "The first pseudorehearsal algorithm is the one used by Frean and Robins [19] with a simplified weighting equation and changed for non-linear neural network inner assignments.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 18,
      "context" : "Dual network models were initially inspired by biological learning [22].",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 19,
      "context" : "As a consequence of promising experimental results of such networks, pseudorehearsal was indeed found to be the most plausible explanation for the otherwise cryptic need for dual learning systems in the brain [23].",
      "startOffset" : 209,
      "endOffset" : 213
    }, {
      "referenceID" : 20,
      "context" : "BioDynaMo [25]).",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 15,
      "context" : "We will reevaluate the analytic results of Frean and Robins [19] in a real reinforcement learning task.",
      "startOffset" : 60,
      "endOffset" : 64
    } ],
    "year" : 2017,
    "abstractText" : "Catastrophic forgetting is of special importance in reinforcement learning, as the data distribution is generally non-stationary over time. We study and compare several pseudorehearsal approaches for Qlearning with function approximation in a pole balancing task. We have found that pseudorehearsal seems to assist learning even in such very simple problems, given proper initialization of the rehearsal parameters.",
    "creator" : "LaTeX with hyperref package"
  }
}