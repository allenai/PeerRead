{
  "name" : "1509.06842.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Feature-Based Comparison of Evolutionary Computing Techniques for Constrained Continuous Optimisation",
    "authors" : [ "Shayan Poursoltan" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "There have been many algorithmic approaches proposed to solve complex optimisation problems, including constrained optimisation problems (COP). Several approaches have been proposed to tackle the constraints in constrained problems. Most of the research has been focused on introducing differential evolution (DE) [14], particle swarm optimisation (PSO) [2] and evolutionary strategies (ES) [13] to solve numerical optimisation problems. In order to deal with these constrained problems, there have been techniques that applied to these algorithms such as penalty functions, special operators (separating the constraint and objective function treatment) and decoder based methods. We refer the reader for a survey of constraint handling techniques in evolutionary computing methods to [9].\nIn order to compare and evaluate the evolutionary algorithms many approaches have been used. One is finding which algorithm performs better on a set of continuous\nar X\niv :1\n50 9.\n06 84\n2v 1\n[ cs\n.A I]\nproblems using benchmarks sets [3, 6]. Recently, there has been an increasing interest to analyse the problem features that make it hard to solve. Initial studies have been carried out in the field of continuous optimisation in [8]. Furthermore, there have been techniques that generate a variation of problem instances from easy to hard. Then, the features of this problem instances are analysed in order to find which of them make the problems hard or easy to solve. Generating the variety of problem instances from easy to hard ensures that the knowledge obtained from analysis is reliable.\nAlthough there is not only a standalone feature that makes a problem hard to solve, but it is assumed that constraints are very important in constrained continuous problems. The evolving approach that has been used to analyse the constraint features and their effects on COP’s difficulty is discussed in [10, 11]. The idea is to evolve constrained problem instances (by using an evolutionary algorithm) in order to identify the constraint features with more contribution to problem difficulty.\nIn this paper, by using a single-objective evolutionary algorithm, we generate hard and easy COP instances for DE, ES and PSO algorithms. Later, we solve the generated instances using one algorithm by the other algorithms. The results show that the hardest generated instances using one algorithm are still hard for the other ones. To get better insight, we use multi-objective evolving approach to generate instances that are hard for one algorithm but still easy for the others. By analysing how an algorithm fails in conditions where the rest perform well, we can derive its strengths and weaknesses over constraint features. Our study shows the effectiveness of constraint features that make the problems hard for one and easy for the other algorithms. It can be translated as over which features of constraints, they make the problems hard for a certain algorithm but still easy for the others.\nThe remainder of this paper is as follows: In Section 2 we introduce the concept of COPs. Then we discuss the evolver (single and multi-objective evolutionary approach) and the solver algorithms (DE, ES and PSO) we use in our experiments. In Section 3 we analyse the performance of various algorithms on each others hard and easy instances (using the single-objective evolver). Section 4 includes the multi-objective approach that generates hard instances for one but easy for the other algorithms. Furthermore, we carry out the analysis of linear and quadratic constraint features that make the problem hard for one and still easy for the rest. Finally, we conclude with some remarks."
    }, {
      "heading" : "2 Preliminaries",
      "text" : ""
    }, {
      "heading" : "2.1 Constrained continuous optimisation problems",
      "text" : "In this study, constrained continuous optimisation problems with inequality and equality constraints are investigated. These problems are optimisation problems where a function f (x) should be optimised with respect to a given set of constraints.\nSingle-objective functions f : S→ R with S ⊆ Rn are considered in this research. The constraints impose a feasible subset F ⊆ S of the search space S and the aim is finding x ∈ S∩F which minimises f . Formally, we state the problems as follows:\nminimize f (x), x = (x1, . . . ,xn) ∈ Rn\nsubject to gi(x)≤ 0 ∀i ∈ {1, . . . ,q} h j(x) = 0 ∀ j ∈ {q+1, . . . , p}\n(1)\nwhere x = (x1,x2, . . . ,xn) is an n dimensional vector and x ∈ S∩F . The gi(x) (inequality) and h j(x) (equality) constraints could be linear/nonlinear. Also, the equality constraints are usually replaced by |h j(x)| ≤ ε where ε = 10e−4 [6]. The feasible region F ⊆ S of the search space S is defined by\nli ≤ xi ≤ ui, 1≤ i≤ n (2)\nwhere li and ui denote lower and upper bounds respectively for the ith variable in which 1≤ i≤ n. In this paper, we focus on the ability of constraints (linear, quadratic) to make a problem hard or easy. The features of these constraints and their effect on problem difficulty is discussed. The constraints are of the following form:\nlinear constraint g(x) = b+a1x1 + . . .+anxn (3)\nquadratic constraint g(x) = b+a1x21 +a2x1 . . .+a2n−1x 2 n +a2nxn (4)\nor a combination of them, where x1,x2 . . . ,xn are values from Equation 1 and a1,a2, . . . ,an are coefficients within lower (li) and upper bounds (ui). We assume univariant quadratic function to analyse each xn (with exponent 2) independently. Also, unvivarient quadratic constraints are more popular in recent benchmarks [6]. In order to include the optimum of objective function in feasible area, we set b ≤ 0 (we assume the objective function optimum is zero)."
    }, {
      "heading" : "2.2 Algorithms",
      "text" : "We now introduce the algorithms for constrained continuous optimisation that are subject to our investigation.\nOne of the most prominent evolutionary algorithms for COPs is ε-constrained differential evolution with an archive and gradient-based mutation (εDEag). The algorithm is the winner of 2010 CEC competition for continuous COPs [6]. The εDEag uses ε-constrained method to transform algorithms for unconstrained problems to constrained ones. It adopts ε-level comparison to order the possible solutions. In other words, the lexicographic order is used in which constraint violation (φ(x)) has more priority and proceeds the function value ( f (x)). For more details we refer the reader to [16].\nThe second algorithm we use in this paper is a (1+ 1) CMA-ES for constrained optimisation [1]. The (1+ 1) CMA-ES in [4] is a variant of (1+ 1)-ES which adapts the covariance matrix of its offspring distribution in addition to its global step size. The idea behind the constraint handling approach of this algorithm is to obtain approximations to the normal vectors directions in the vicinity of the current solutions locations by low-pass filtering steps which violates the respective constraints and reducing the\nvariance of the offspring distribution in these directions. Incorporating this constraint handling approach with (1+ 1) CMA-ES makes an algorithm which is significantly more efficient than other approaches for constrained evolutionary algorithms. Also, the selected algorithm is not sensitive to the rotation of the problem search space. We refer the reader to [1] for more details and implementation.\nThe third algorithm that is used in our investigation is a particle swarm optimisation. This algorithm (HMPSO) applies a method that uses parallel search operator in which it divides the current swarm into various sub-swarms and locates the solution between them. In each sub-swarm, all particles follow the local best (fittest particle) which improves them to be more fitter. Also, since all sub-swarms are located around different optima (in parallel), then it is more possible to locate multiple optima which improves the diversity of algorithm. Dividing the swarms into sub-swarms improves the diversity of the algorithm. Also, choosing the local best in each sub-swarm can attract the other particles to fitter positions. We refer the reader to [17] for detailed algorithm and implementation."
    }, {
      "heading" : "2.3 Features of Constraints",
      "text" : "In this paper we analyse the constraint features of generated problem instances. These features are constraint coefficients relationships such as standard derivation, angle between constraint hyperplanes, feasibility ratio in vicinity of optimum, number of constraints, shortest distance of constraint hyperplane to optimum. The details of these features are discussed in [11]."
    }, {
      "heading" : "3 Single-objective Investigations",
      "text" : "We first consider different algorithms and compare their relative performance on each other’s generated hard and easy instances. We use single-objective evolver to evolve and generate hard and easy instances for all types of algorithms. The detailed procedure and results for DE instances are discussed in [11]. For this experiment, we perform 30 independent runs generating easy and hard instances for PSO and ES solvers. It means, the single-objective evolver only generates instances that are hard/easy for one type of algorithm (PSO, ES and DE). The required function evaluation number (FEN) for solving these instances (PSO, ES and DE) is used as fitness value for single-objective evolver. The parameters for solvers are identical to [1, 16, 17]. Also, we run our experiments on Sphere function (bowl shaped)[3]. We now have three groups of easy and hard instances generated for DE, ES and PSO algorithms. We then compare the DE, ES and PSO algorithms by applying them on each other’s easy and hard instances. The analysis is done by comparing the required FEN for an algorithm to solve the other’s generated problem instances. Then, it is possible to derive strengths and weaknesses of the considered algorithms by observing how well one algorithm performs in conditions where the other algorithms fail (or it is difficult for them). Table 1 and 2 show different algorithms performance on Sphere objective functions with linear/quadratic constraints (1 to 5 constraints). We also run our experiments on different objective functions such as Ackley and Rosenbrock. The results are shown in Tables 3, 4, 5 and 6. It is interest-\ning that all objective functions follow similar pattern. Considering the required FEN to solve each instances, it is observed that hard instances are still the hardest for their own algorithms and hard for the others. It implies that the hard instances share some common features to make it difficult to solve for all solvers. However, the obtained knowledge is not enough to compare the algorithm capabilities to solve hard problem instances."
    }, {
      "heading" : "4 Multi-objective Investigations",
      "text" : "Based on the experiment results in previous section, hard instances for each algorithm are still hard for the others. In order to extract more useful knowledge about the strengths or weaknesses of certain algorithms on constraint algorithms, we need problem instances that are hard for one and easy for the others. Analysing the features of these instances helps us extracting knowledge regarding the strengths and weaknesses of algorithms by examining why an algorithm performs better on some groups of features while the others fails. This will help us developing more efficient prediction model for automated algorithm selection.\nTo do this, we use a multi-objective DE algorithm (DEMO) described in [12] to minimise the FEN for one algorithm and maximise it for the others. In other words, the FEN for generated problem instances is higher (harder) for a certain algorithm and lower (easier) for the others. In order to find instances that are hard for one algorithm type and easy for the others, we need to find solution as diverse as possible. Also, the solutions need to be close to pareto front. Satisfying these two aims makes us to use multi-objective evolutionary algorithm to generate problem instances. Hence, we use differential evolution for multi-objective optimisation (DEMO) proposed by Robic in [12]. Based on results in [12], the DEMO achieves efficiently the above two goals. In DEMO, the candidate solution replaces parent when it dominates it and if the parent dominates it, the candidate is discarded. Otherwise, if the candidate and parent cannot dominate each other, the candidate is added to the population. The major difference between DEMO and other multi-objective evolutionary algorithms is that the newly generated good candidates are immediately used in creation of the subsequent candidates. This improves fast convergence to the true pareto front, while the use of non-dominated sorting and crowding distance metric in truncation of the extended population promotes the uniform spread of solutions. We refer the reader to [12] for further details and implementation.\nIn the following, we discuss the results for algorithms performances comparison. We carry out 30 independent runs for each number of constraints that are hard for one algorithm but still easy for the others. We set the evolving algorithm (DEMO) generation number to 5000 and the other parameters of evolving algorithm are set to pop size = 40, CR = 0.5, scaling factor = 0.9 and FENmax is 300K. Values for these parameters have been obtained by optimising the performance of the evolving algorithm in order to achieve the more easier and harder problem instances. For each of three algorithms, their best parameters are chosen [3, 16, 17]. First, the (εDEg) algorithm parameters are considered as: generation number = 1500, pop size = 100, CR = 0.5, scaling factor = 0.5. Also, the parameters for e-constraint method are described in [11]. Moreover,\nfor evolutionary strategy we perform (1,7)-ES algorithm with 1500 generation using Pf = 0.4 with tendency to focus on feasible solution. In HMPSO algorithm, the swarm size N is set to 60, each sub-swarm size (Ns) is 8 and all the PSO parameters are considered as Krohling and Coelho’s PSO [5]. In order to solve generated COPs, HMPSO generation number is set to 1500. We need to say the parameters for the solvers are identical to those given in [1, 12, 16, 17]\nIn our all experiments, we generate set of problem instances that are hard to one algorithm and easy to the other ones. Tables 7, 8, 9, 10, 11 and 12 show the function evaluation number (FEN) required for each algorithm to solve DE/ES/PSO hard instances for Sphere, Ackley and Rosenbrock objective functions (with 1 to 5 linear/quadratic constraints). As it is observed, there is more difference between the required FEN of instances generated by multi-objective algorithm evolver than the single-objective one. For instance, the required FEN for solving DE hard instances are higher for DE algorithm than solving it by ES and PSO algorithm. It means the DE hard instances are only hard for DE algorithm and easy for the others. In the following we start analysing constraint features of instances that are hard for one and easy for others."
    }, {
      "heading" : "4.1 Analysis for Linear Constraints",
      "text" : "We run our experiments on Sphere, Ackley and Rosenbrock objective functions. The linear constraints are considered as in Equation 3 with all coefficients ans that are in the range of [−5,5]. Also, the problem dimension is set to 30. As it mentioned before, to analyse and discuss some features such as shortest distance, we assume that the optimum is zero (b ≤ 0). We use three types of problem instances. DE hard denotes problem instances that are hard for DE algorithm but still easy for PSO and ES algorithms. Also, ES hard instances are easy for DE and PSO algorithm in this section. PSO hard means the instances that are hard for PSO but easy for the rest. Each constraint is generated using multi-objective evolver to generate instances that are hard for one algorithm but easy for others. In the following we discuss the features of linear constraints.\nFigure 1 represents some evidence of linear constraint coefficient relationship (standard deviation). It is shown that standard deviation of (1 to 5) linear constraints are higher for DE hard instances than ES and PSO hard ones. This result is similar for all Sphere, Ackley and Rosenbrock objective functions. This means, the instances that are hard for DE algorithm but easy for ES and PSO have higher standard deviation for their constraints coefficients. In other words, this constraint feature has influence on problem difficulty. This improves the prediction ability for algorithm selection framework.\nBox plots shown in Figure 2 represent the shortest distance from optimum feature for hard instances. Based on the experiments, hard instances for ES algorithm have higher value (closer to optimum) shortest distance than the other algorithms. It is noteworthy that lower value in Figure 2 means the constraint hyperplane is further from optimum. In other words, the constraints hyperplanes are closer to the optimum in ES hard instances. This relationship holds the pattern for all objective functions in linear constraints. We also study the feasibility ratio in vicinity of the optimum. As observed in Table 14, hard DE instances have lower feasibility ratio comparing to PSO and ES hard instances. This follows the same pattern for all experimented objective functions.\nTable 2: The comparison of algorithms performance on each other’s easy and hard instances based on required FEN for Sphere objective function with quadratic constraints. DE Easy (1 c) means instances that are easy for DE and with 1 constraint.\nInstances DE algorithm ES algorithm PSO algorithm DE Easy (1 c) 24.2K 23.6K 24.9K ES Easy (1 c) 24.8K 24.2K 25.4K PSO Easy (1 c) 26.4K 25.4K 26.4K DE Easy (2 c) 25.3K 28.1K 26.4K ES Easy (2 c) 24.1K 27.2K 27.4K PSO Easy (2 c) 23.5K 29.3K 271.K DE Easy (3 c) 27.9K 31.9K 35.5K ES Easy (3 c) 29.4K 32.1K 28.5K PSO Easy (3 c) 28.1K 28.7K 29.4K DE Easy (4 c) 34.1K 28.9K 36.4K ES Easy (4 c) 35.2K 35.3K 31.6K PSO Easy (4 c) 31.8K 29.5K 33.2K DE Easy (5 c) 38.7K 29.2K 37.2K ES Easy (5 c) 35.6K 28.2K 39.5K PSO Easy (5 c) 36.3K 31.5K 36.2K DE Hard (1 c) 129.3K 102.7K 105.3K ES Hard (1 c) 104.3K 121.2K 108.2K PSO Hard (1 c) 108.2K 104.2K 119.8K DE Hard (2 c) 132.6K 114.2K 114.9K ES Hard (2 c) 111.2K 127.1K 112.4K PSO Hard (2 c) 109.4K 112.4K 125.3K DE Hard (3 c) 136.2K 116.3K 112.4K ES Hard (3 c) 117.2K 132.1K 109.9K PSO Hard (3 c) 119.8K 119.2K 132.6K DE Hard (4 c) 141.2K 119.9K 119.6K ES Hard (4 c) 113.8K 131.2K 121.9K PSO Hard (4 c) 115.4K 121.4K 138.9K DE Hard (5 c) 149.3K 129.7K 122.9K ES Hard (5 c) 124.4K 149.6K 126.4K PSO Hard (5 c) 123.9K 124.2K 148.3K\nTable 4: The comparison of algorithms performance on each other’s easy and hard instances based on required FEN for Ackley objective function with quadratic constraints. DE Easy (1 c) means instances that are easy for DE and with 1 constraint.\nInstances DE algorithm ES algorithm PSO algorithm DE Easy (1 c) 38.1K 39.4K 37.2K ES Easy (1 c) 37.1K 38.1K 39.0K PSO Easy (1 c) 41.2K 39.9K 40.7K DE Easy (2 c) 38.9K 43.9K 41.7K ES Easy (2 c) 40.1K 41.2K 43.2K PSO Easy (2 c) 39.1K 43.1K 46.1K DE Easy (3 c) 46.3K 47.9K 45.1K ES Easy (3 c) 49.1K 48.1K 47.2K PSO Easy (3 c) 42.1K 45.1K 49.1K DE Easy (4 c) 49.2K 48.7K 48.4K ES Easy (4 c) 49.7K 49.9K 52.9K PSO Easy (4 c) 56.1K 55.1K 54.1K DE Easy (5 c) 50.0K 51.2K 52.4K ES Easy (5 c) 51.3K 56.2K 54.1K PSO Easy (5 c) 61.2K 58.9K 59.1K DE Hard (1 c) 133.4K 93.1K 94.6K ES Hard (1 c) 92.1K 135.1K 94.1K PSO Hard (1 c) 95.2K 94.9K 134.2K DE Hard (2 c) 139.1K 98.2K 97.1K ES Hard (2 c) 97.1K 138.2K 99.1K PSO Hard (2 c) 109.1K 107.2K 145.2K DE Hard (3 c) 145.3K 112.6K 109.6K ES Hard (3 c) 111.6K 141.2K 108.9K PSO Hard (3 c) 111.2K 109.2K 152.K DE Hard (4 c) 167.2K 132.1K 135.1K ES Hard (4 c) 131.1K 167.9K 133.9K PSO Hard (4 c) 132.1K 133.2K 169.1K DE Hard (5 c) 177.1K 143.2K 131.3K ES Hard (5 c) 141.2K 181.2K 144.9K PSO Hard (5 c) 139.1K 142.9K 182.1K\nTable 6: The comparison of algorithms performance on each other’s easy and hard instances based on required FEN for Rosenbrock objective function with quadratic constraints. DE Easy (1 c) means instances that are easy for DE and with 1 constraint.\nInstances DE algorithm ES algorithm PSO algorithm DE Easy (1 c) 41.2K 40.2K 38.7K ES Easy (1 c) 39.2K 39.3K 36.1K PSO Easy (1 c) 43.1K 39.6K 42.2K DE Easy (2 c) 39.1K 44.4K 43.2K ES Easy (2 c) 42.6K 44.5K 41.8K PSO Easy (2 c) 41.2K 45.6K 47.2K DE Easy (3 c) 47.1K 48.5K 49.2K ES Easy (3 c) 46.8K 49.5K 48.8K PSO Easy (3 c) 46.2K 42.7K 48.4K DE Easy (4 c) 48.7K 49.1K 51.2K ES Easy (4 c) 50.2K 52.4K 55.2K PSO Easy (4 c) 59.2K 54.5K 51.9K DE Easy (5 c) 52.5K 56.1K 55.7K ES Easy (5 c) 56.0K 55.7K 53.8K PSO Easy (5 c) 66.3K 59.8K 60.4K DE Hard (1 c) 137.2K 93.7K 92.1K ES Hard (1 c) 93.7K 138.2K 99.2K PSO Hard (1 c) 93.1K 96.2K 138.0K DE Hard (2 c) 142.7K 99.7K 98.4K ES Hard (2 c) 98.8K 141.6K 101.4K PSO Hard (2 c) 112.7K 109.5K 148.1K DE Hard (3 c) 148.2K 115.3K 112.8K ES Hard (3 c) 114.1K 144.8K 113.2K PSO Hard (3 c) 113.6K 113.1K 157.3K DE Hard (4 c) 171.7K 136.7K 133.4K ES Hard (4 c) 134.7K 168.2K 135.3K PSO Hard (4 c) 134.7K 139.3K 172.6K DE Hard (5 c) 179.6K 146.1K 144.8K ES Hard (5 c) 143.8K 185.1K 147.4K PSO Hard (5 c) 143.7K 141.4K 186.4K\nTable 8: TThe FEN required for each algorithm to solve DE/ES/PSO hard instances (Sphere for 1 to 5 quadratic constraints)\nInstances DE algorithm ES algorithm PSO algorithm DE hard (1 c) 92.3K 50.2K 51.9K ES hard (1 c) 48.8K 91.3K 49.3K PSO hard (1 c) 44.5K 46.8K 93.1K DE hard (2 c) 93.5K 52.9K 54.2K ES hard (2 c) 50.9K 95.9K 51.2K PSO hard (2 c) 50.2K 53.2K 96.3K DE hard (3 c) 95.9K 54.3K 55.3K ES hard (3 c) 53.9K 97.4K 52.4K PSO hard (3 c) 57.3K 56.3K 98.9K DE hard (4 c) 98.3K 56.4K 57.3K ES hard (4 c) 56.3K 102.3K 52.1K PSO hard (4 c) 59.2K 58.2K 101.6K DE hard (5 c) 102.1K 58.3K 59.4K ES hard (5 c) 59.2K 103.2K 60.2K PSO hard (5 c) 62.6K 63.8K 105.2K\nTable 10: The FEN required for each algorithm to solve DE/ES/PSO hard instances (Ackley for 1 to 5 quadratic constraints)\nInstances DE algorithm ES algorithm PSO algorithm DE hard (1 c) 142.5K 60.1K 62.5K ES hard (1 c) 58.5K 148.2K 61.4K PSO hard (1 c) 53.2K 53.9K 147.7K DE hard (2 c) 153.3K 58.1K 58.1K ES hard (2 c) 59.2K 155.5K 59.2K PSO hard (2 c) 57.8K 56.3K 157.2K DE hard (3 c) 167.3K 65.2K 68.1K ES hard (3 c) 63.2K 169.2K 69.8K PSO hard (3 c) 65.7K 67.9K 167.6K DE hard (4 c) 174.8K 71.2K 75.1K ES hard (4 c) 66.8K 169.1K 72.9K PSO hard (4 c) 69.1K 68.3K 172.9K DE hard (5 c) 179.5K 75.1K 76.1K ES hard (5 c) 72.8K 174.9K 77.4.2K PSO hard (5 c) 75.1K 74.9K 175.9K\nTable 12: The FEN required for each algorithm to solve DE/ES/PSO hard instances (Rosenbrock for 1 to 5 quadratic constraints)\nInstances DE algorithm ES algorithm PSO algorithm DE hard (1 c) 143.1K 61.4K 63.7K ES hard (1 c) 59.6K 149.7K 62.5K PSO hard (1 c) 54.3K 54.2K 143.8K DE hard (2 c) 155.2K 59.2K 59.2K ES hard (2 c) 61.3K 154.8K 57.9K PSO hard (2 c) 59.2K 57.1K 158.0K DE hard (3 c) 168.9K 63.7K 66.8K ES hard (3 c) 65.2K 170.1K 68.1K PSO hard (3 c) 63.7K 68.1K 168.9K DE hard (4 c) 175.1K 73.8K 76.9K ES hard (4 c) 68.2K 172.7K 75.2K PSO hard (4 c) 67.7K 69.1K 176.2K DE hard (5 c) 180.2K 74.2K 77.8K ES hard (5 c) 74.2K 175.1K 79.2K PSO hard (5 c) 73.6K 74.4K 179.4K\nAlso increasing the number of constraints decreases the problem optimum-local feasibility for all algorithm problem instances. The angle between linear constraints feature is analysed for linear constraints. As it is observed in Table 13, ES hard instances have lower angle values for all Sphere, Ackley and Rosenbrock objective functions. This means, instances that are hard for ES have less angle value between their constraint hyperplanes. Interestingly, all objective function that we use in this experiment follow the same relationship.\nAs it is observed, to compare the instances, DE hard instances have higher linear constraint coefficient standard deviation. It can be translated as DE algorithm has more difficulty to coefficients standard deviation feature than PSO and ES algorithms. Also, the local-optimum feasibility ratio value is higher in ES and PSO hard instances than DE hard ones. This means, ES and PSO algorithms are more effective to problems with higher optimum feasibility ratio feature. The shortest distance and angle features for ES is less than DE and PSO hard instances. Interestingly, this features are similar for all used objective functions. The linear constraint feature based analysis gives us helpful knowledge to implement algorithm selection framework."
    }, {
      "heading" : "4.2 Analysis for Quadratic Constraints",
      "text" : "In this section, we carry out our experiments on Sphere, Ackley and Rosenbrock objective function with quadratic constraints (see Equation 4) using same setup as previous section. In the following we do feature based analysis of constraints in hard DE, PSO and ES instances (that are easy for the other algorithms).\nFigure 1 shows some evidence of quadratic constraint coefficients relationship. Based on our experiments, in each constraint, the quadratic coefficient has more ability than linear coefficients to make problem harder to solve. In other words, in Equation 4, a1 is more contributing than a2 to problem difficulty. As it is shown in the box plots, the standard deviation of 1 to 5 quadratic constraints in DE hard instances are higher comparing the other two algorithm hard instances. In contrast, our results show no systematic relationship between problem difficulty and linear coefficients in each quadratic constraints and quadratic coefficients have more contribution in problem difficulty.\nAs it is observed in Figure 2, the shortest distance feature for DE, PSO and ES hard instances are compared. In instances that are hard for ES and easy for the other algorithms, the quadratic constraint hyperplanes are closer to optimum (zero). This applies to all experimented objective functions. Also, calculating the angle feature for quadratic constraint does not show any systematic relationship to problem difficulty. The feasibility ratio near the optimum is analysed for DE, ES and PSO hard instances. As it is shown in Table 15, the feasibility ratio in DE hard instances are lower than the other algorithms hard instances. All objective functions have the same pattern. Also, the number of constraint has a systematic relationship with feasibility ratio.\nBased on the results, to compare COP instances with quadratic constraints, DE hard instances have higher coefficient standard deviation value than the other algorithm hard ones. It is translated as the DE algorithm has more difficulty solving instances with higher standard deviation value for their quadratic constraints than ES and PSO. Also, the quadratic constraints are closer to optimum in ES instances than the other experi-\nmented algorithms. In other words, ES algorithm is more influenced by constraint with closer to optimum instances. Moreover, the optimum feasibility ratio in DE instances are lower than PSO and ES."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we carried out an algorithm performance comparison on each others constrained problem instances. We then analysed the features and characteristics of constraints that make them hard to solve for certain algorithm but easy for the others. It is observed that some constraint features are more contributing to problem difficulty for certain algorithms. In linear constraints, some features such as coefficient relationship, angle, local-optimum feasibility ratio and shortest distance play an important role in problem difficulty to DE and ES algorithms. Considering quadratic instances, angle does not show any relationship to problem difficulty.\nBy analysing how well one algorithm performs in conditions where other algorithms fail, we can derive its strengths and weaknesses over constrained problems. These results can help us to improve the efficiency of algorithm prediction model."
    }, {
      "heading" : "Acknowledgements",
      "text" : "Frank Neumann has been supported by ARC grants DP130104395 and DP140103400."
    } ],
    "references" : [ {
      "title" : "A new optimizer using particle swarm theory",
      "author" : [ "R. Eberhart", "J. Kennedy" ],
      "venue" : "Micro Machine and Human Science, 1995. MHS’95., Proceedings of the Sixth International Symposium on, pages 39–43. IEEE,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Real-parameter black-box optimization benchmarking 2010: Experimental setup",
      "author" : [ "N. Hansen", "A. Auger", "S. Finck", "R. Ros" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2010
    }, {
      "title" : "A computational efficient covariance matrix update and a (1+ 1)-cma for evolution strategies",
      "author" : [ "C. Igel", "T. Suttorp", "N. Hansen" ],
      "venue" : "Proceedings of the 8th annual conference on Genetic and evolutionary computation, pages 453–460. ACM,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Coevolutionary particle swarm optimization using gaussian distribution for solving constrained optimization problems. Systems, Man, and Cybernetics, Part B: Cybernetics",
      "author" : [ "R.A. Krohling", "L. dos Santos Coelho" ],
      "venue" : "IEEE Transactions on,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2006
    }, {
      "title" : "Problem definitions and evaluation criteria for the cec 2010 competition on constrained real-parameter optimization",
      "author" : [ "R. Mallipeddi", "P.N. Suganthan" ],
      "venue" : "Nanyang Technological University, Singapore,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Exploratory landscape analysis",
      "author" : [ "O. Mersmann", "B. Bischl", "H. Trautmann", "M. Preuss", "C. Weihs", "G. Rudolph" ],
      "venue" : "Proceedings of the 13th annual conference on Genetic and evolutionary computation, pages 829–836. ACM,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Benchmarking evolutionary algorithms: Towards exploratory landscape analysis",
      "author" : [ "O. Mersmann", "M. Preuss", "H. Trautmann" ],
      "venue" : "Springer,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Constraint-handling in natureinspired numerical optimization: past, present and future",
      "author" : [ "E. Mezura-Montes", "C.A. Coello Coello" ],
      "venue" : "Swarm and Evolutionary Computation, 1(4):173–194,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A feature-based analysis on the impact of linear constraints for ε-constrained differential evolution",
      "author" : [ "S. Poursoltan", "F. Neumann" ],
      "venue" : "Evolutionary Computation (CEC), 2014 IEEE Congress on, pages 3088–3095. IEEE,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A feature-based analysis on the impact of set of constraints for e-constrained differential evolution",
      "author" : [ "S. Poursoltan", "F. Neumann" ],
      "venue" : "CoRR, abs/1506.06848,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Demo: Differential evolution for multiobjective optimization",
      "author" : [ "T. Robič", "B. Filipič" ],
      "venue" : "Evolutionary Multi-Criterion Optimization, pages 520–533. Springer,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Evolution and optimum seeking: the sixth generation",
      "author" : [ "H.-P.P. Schwefel" ],
      "venue" : "John Wiley & Sons, Inc.,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Differential evolution–a simple and efficient heuristic for global optimization over continuous spaces",
      "author" : [ "R. Storn", "K. Price" ],
      "venue" : "Journal of global optimization, 11(4):341–359,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Constrained optimization by ε constrained differential evolution with dynamic ε-level control",
      "author" : [ "T. Takahama", "Sakai" ],
      "venue" : "In Advances in Differential Evolution,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2008
    }, {
      "title" : "Constrained optimization by the ε constrained differential evolution with an archive and gradient-based mutation",
      "author" : [ "T. Takahama", "S. Sakai" ],
      "venue" : "Evolutionary Computation (CEC), 2010 IEEE Congress on, pages 1–9. IEEE,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A hybrid multi-swarm particle swarm optimization to solve constrained optimization problems",
      "author" : [ "Y. Wang", "Z. Cai" ],
      "venue" : "Frontiers of Computer Science in China, 3(1):38–52,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "Most of the research has been focused on introducing differential evolution (DE) [14], particle swarm optimisation (PSO) [2] and evolutionary strategies (ES) [13] to solve numerical optimisation problems.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 0,
      "context" : "Most of the research has been focused on introducing differential evolution (DE) [14], particle swarm optimisation (PSO) [2] and evolutionary strategies (ES) [13] to solve numerical optimisation problems.",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 11,
      "context" : "Most of the research has been focused on introducing differential evolution (DE) [14], particle swarm optimisation (PSO) [2] and evolutionary strategies (ES) [13] to solve numerical optimisation problems.",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 7,
      "context" : "We refer the reader for a survey of constraint handling techniques in evolutionary computing methods to [9].",
      "startOffset" : 104,
      "endOffset" : 107
    }, {
      "referenceID" : 1,
      "context" : "problems using benchmarks sets [3, 6].",
      "startOffset" : 31,
      "endOffset" : 37
    }, {
      "referenceID" : 4,
      "context" : "problems using benchmarks sets [3, 6].",
      "startOffset" : 31,
      "endOffset" : 37
    }, {
      "referenceID" : 6,
      "context" : "Initial studies have been carried out in the field of continuous optimisation in [8].",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 8,
      "context" : "The evolving approach that has been used to analyse the constraint features and their effects on COP’s difficulty is discussed in [10, 11].",
      "startOffset" : 130,
      "endOffset" : 138
    }, {
      "referenceID" : 9,
      "context" : "The evolving approach that has been used to analyse the constraint features and their effects on COP’s difficulty is discussed in [10, 11].",
      "startOffset" : 130,
      "endOffset" : 138
    }, {
      "referenceID" : 4,
      "context" : "Also, the equality constraints are usually replaced by |h j(x)| ≤ ε where ε = 10e−4 [6].",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 4,
      "context" : "Also, unvivarient quadratic constraints are more popular in recent benchmarks [6].",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 4,
      "context" : "The algorithm is the winner of 2010 CEC competition for continuous COPs [6].",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 14,
      "context" : "For more details we refer the reader to [16].",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 2,
      "context" : "The (1+ 1) CMA-ES in [4] is a variant of (1+ 1)-ES which adapts the covariance matrix of its offspring distribution in addition to its global step size.",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 15,
      "context" : "We refer the reader to [17] for detailed algorithm and implementation.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 9,
      "context" : "The details of these features are discussed in [11].",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 9,
      "context" : "The detailed procedure and results for DE instances are discussed in [11].",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 14,
      "context" : "The parameters for solvers are identical to [1, 16, 17].",
      "startOffset" : 44,
      "endOffset" : 55
    }, {
      "referenceID" : 15,
      "context" : "The parameters for solvers are identical to [1, 16, 17].",
      "startOffset" : 44,
      "endOffset" : 55
    }, {
      "referenceID" : 1,
      "context" : "Also, we run our experiments on Sphere function (bowl shaped)[3].",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 10,
      "context" : "To do this, we use a multi-objective DE algorithm (DEMO) described in [12] to minimise the FEN for one algorithm and maximise it for the others.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 10,
      "context" : "Hence, we use differential evolution for multi-objective optimisation (DEMO) proposed by Robic in [12].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 10,
      "context" : "Based on results in [12], the DEMO achieves efficiently the above two goals.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 10,
      "context" : "We refer the reader to [12] for further details and implementation.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 1,
      "context" : "For each of three algorithms, their best parameters are chosen [3, 16, 17].",
      "startOffset" : 63,
      "endOffset" : 74
    }, {
      "referenceID" : 14,
      "context" : "For each of three algorithms, their best parameters are chosen [3, 16, 17].",
      "startOffset" : 63,
      "endOffset" : 74
    }, {
      "referenceID" : 15,
      "context" : "For each of three algorithms, their best parameters are chosen [3, 16, 17].",
      "startOffset" : 63,
      "endOffset" : 74
    }, {
      "referenceID" : 9,
      "context" : "Also, the parameters for e-constraint method are described in [11].",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 3,
      "context" : "In HMPSO algorithm, the swarm size N is set to 60, each sub-swarm size (Ns) is 8 and all the PSO parameters are considered as Krohling and Coelho’s PSO [5].",
      "startOffset" : 152,
      "endOffset" : 155
    }, {
      "referenceID" : 10,
      "context" : "We need to say the parameters for the solvers are identical to those given in [1, 12, 16, 17] In our all experiments, we generate set of problem instances that are hard to one algorithm and easy to the other ones.",
      "startOffset" : 78,
      "endOffset" : 93
    }, {
      "referenceID" : 14,
      "context" : "We need to say the parameters for the solvers are identical to those given in [1, 12, 16, 17] In our all experiments, we generate set of problem instances that are hard to one algorithm and easy to the other ones.",
      "startOffset" : 78,
      "endOffset" : 93
    }, {
      "referenceID" : 15,
      "context" : "We need to say the parameters for the solvers are identical to those given in [1, 12, 16, 17] In our all experiments, we generate set of problem instances that are hard to one algorithm and easy to the other ones.",
      "startOffset" : 78,
      "endOffset" : 93
    } ],
    "year" : 2015,
    "abstractText" : "Evolutionary algorithms have been frequently applied to constrained continuous optimisation problems. We carry out feature based comparisons of different types of evolutionary algorithms such as evolution strategies, differential evolution and particle swarm optimisation for constrained continuous optimisation. In our study, we examine how sets of constraints influence the difficulty of obtaining close to optimal solutions. Using a multi-objective approach, we evolve constrained continuous problems having a set of linear and/or quadratic constraints where the different evolutionary approaches show a significant difference in performance. Afterwards, we discuss the features of the constraints that exhibit a difference in performance of the different evolutionary approaches under consideration.",
    "creator" : "LaTeX with hyperref package"
  }
}