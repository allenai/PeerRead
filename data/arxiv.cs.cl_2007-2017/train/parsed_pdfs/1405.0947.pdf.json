{
  "name" : "1405.0947.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning Bilingual Word Representations by Marginalizing Alignments",
    "authors" : [ "Tomáš Kočiský", "Karl Moritz Hermann", "Phil Blunsom" ],
    "emails" : [ "tomas.kocisky@cs.ox.ac.uk", "karl.moritz.hermann@cs.ox.ac.uk", "phil.blunsom@cs.ox.ac.uk" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Distributed representations have become an increasingly important tool in machine learning. Such representations—typically continuous vectors learned in an unsupervised setting—can frequently be used in place of hand-crafted, and thus expensive, features. By providing a richer representation than what can be encoded in discrete settings, distributed representations have been successfully used in many areas. This includes AI and reinforcement learning (Mnih et al., 2013), image retrieval (Kiros et al., 2013), language modelling (Bengio et al., 2003), sentiment analysis (Socher et al., 2011; Hermann and Blunsom, 2013), framesemantic parsing (Hermann et al., 2014), and document classification (Klementiev et al., 2012).\nIn Natural Language Processing (NLP), the use of distributed representations is motivated by the idea that they could capture semantics and/or syntax, as well as encoding a continuous notion of similarity, thereby enabling information sharing between similar words and other units. The success of distributed approaches to a number of tasks, such as listed above, supports this notion and its implied benefits (see also Turian et al. (2010) and Collobert and Weston (2008)).\nWhile most work employing distributed representations has focused on monolingual tasks, multilingual representations would also be useful for\nseveral NLP-related tasks. Such problems include document classification, machine translation, and cross-lingual information retrieval, where multilingual data is frequently the norm. Furthermore, learning multilingual representations can also be useful for cross-lingual information transfer, that is exploiting resource-fortunate languages to generate supervised data in resource-poor ones.\nWe propose a probabilistic model that simultaneously learns word alignments and bilingual distributed word representations. As opposed to previous work in this field, which has relied on hard alignments or bilingual lexica (Klementiev et al., 2012; Mikolov et al., 2013), we marginalize out the alignments, thus capturing more bilingual semantic context. Further, this results in our distributed word alignment (DWA) model being the first probabilistic account of bilingual word representations. This is desirable as it allows better reasoning about the derived representations and furthermore, makes the model suitable for inclusion in higher-level tasks such as machine translation.\nThe contributions of this paper are as follows. We present a new probabilistic similarity measure which is based on an alignment model and prior language modeling work which learns and relates word representations across languages. Subsequently, we apply these embeddings to a standard document classification task and show that they outperform the current published state of the art (Hermann and Blunsom, 2014b). As a by-product we develop a distributed version of FASTALIGN (Dyer et al., 2013), which performs on par with the original model, thereby demonstrating the efficacy of the learned bilingual representations."
    }, {
      "heading" : "2 Background",
      "text" : "The IBM alignment models, introduced by Brown et al. (1993), form the basis of most statistical machine translation systems. In this paper we base our alignment model on FASTALIGN (FA), a vari-\nar X\niv :1\n40 5.\n09 47\nv1 [\ncs .C\nL ]\n5 M\nay 2\n01 4\nation of IBM model 2 introduced by Dyer et al. (2013). This model is both fast and produces alignments on par with the state of the art. Further, to induce the distributed representations we incorporate ideas from the log-bilinear language model presented by Mnih and Hinton (2007)."
    }, {
      "heading" : "2.1 IBM Model 2",
      "text" : "Given a parallel corpus with aligned sentences, an alignment model can be used to discover matching words and phrases across languages. Such models are an integral part of most machine translation pipelines. An alignment model learns p(f ,a|e) (or p(e,a′|f)) for the source and target sentences e and f (sequences of words). a represents the word alignment across these two sentences from source to target. IBM model 2 (Brown et al., 1993) learns alignment and translation probabilities in a generative style as follows:\np(f ,a|e) = p(J |I) J∏\nj=1\np(aj |j, I, J) p ( fj |eaj ) ,\nwhere p(J |I) captures the two sentence lengths; p(aj |j, I, J) the alignment and p ( fj |eaj ) the translation probability. Sentence likelihood is given by marginalizing out the alignments, which results in the following equation:\np(f |e) = p(J |I) J∏\nj=1 I∑ i=0 p(i|j, I, J) p(fj |ei) .\nWe use FASTALIGN (FA) (Dyer et al., 2013), a log-linear reparametrization of IBM model 2. This model uses an alignment distribution defined by a single parameter that measures how close the alignment is to the diagonal. This replaces the original multinomial alignment distribution which often suffered from sparse counts. This improved model was shown to run an order of magnitude faster than IBM model 4 and yet still outperformed it in terms of the BLEU score and, on ChineseEnglish data, in alignment error rate (AER)."
    }, {
      "heading" : "2.2 Log-Bilinear Language Model",
      "text" : "Language models assign a probability measure to sequences of words. We use the log-bilinear language model proposed by Mnih and Hinton (2007). It is an n-gram based model defined in terms of an energy function E(wn;w1:n−1). The probability for predicting the next word wn given its preceding context of n − 1 words is expressed\nusing the energy function\nE(wn;w1:n−1)=− ( n−1∑ i=1 rTwiCi ) rwn−bTr rwn−bwn\nas p(wn|w1:n−1) = 1Zc exp (−E(wn;w1:n−1)) where Zc = ∑ wn\nexp (−E(wn;w1:n−1)) is the normalizer, rwi ∈ Rd are word representations, Ci ∈ Rd×d are context transformation matrices, and br ∈ Rd, bwn ∈ R are representation and word biases respectively. Here, the sum of the transformed context-word vectors endeavors to be close to the word we want to predict, since the likelihood in the model is maximized when the energy of the observed data is minimized.\nThis model can be considered a variant of a log-linear language model in which, instead of defining binary n-gram features, the model learns the features of the input and output words, and a transformation between them. This provides a vastly more compact parameterization of a language model as n-gram features are not stored."
    }, {
      "heading" : "2.3 Multilingual Representation Learning",
      "text" : "There is some recent prior work on multilingual distributed representation learning. Similar to the model presented here, Klementiev et al. (2012) and Zou et al. (2013) learn bilingual embeddings using word alignments. These two models are non-probabilistic and conditioned on the output of a separate alignment model, unlike our model, which defines a probability distribution over translations and marginalizes over all alignments. These models are also highly related to prior work on bilingual lexicon induction (Haghighi et al., 2008). Other recent approaches include Sarath Chandar et al. (2013), Lauly et al. (2013) and Hermann and Blunsom (2014a, 2014b). These models avoid word alignment by transferring information across languages using a composed sentence-level representation.\nWhile all of these approaches are related to the model proposed in this paper, it is important to note that our approach is novel by providing a probabilistic account of these word embeddings. Further, we learn word alignments and simultaneously use these alignments to guide the representation learning, which could be advantageous particularly for rare tokens, where a sentence based approach might fail to transfer information.\nRelated work also includes Mikolov et al. (2013), who learn a transformation matrix to\nreconcile monolingual embedding spaces, in an l2 norm sense, using dictionary entries instead of alignments, as well as Schwenk et al. (2007) and Schwenk (2012), who also use distributed representations for estimating translation probabilities. Faruqui and Dyer (2014) use a technique based on CCA and alignments to project monolingual word representations to a common vector space."
    }, {
      "heading" : "3 Model",
      "text" : "Here we describe our distributed word alignment (DWA) model. The DWA model can be viewed as a distributed extension of the FA model in that it uses a similarity measure over distributed word representations instead of the standard multinomial translation probability employed by FA. We do this using a modified version of the log-bilinear language model in place of the translation probabilities p(fj |ei) at the heart of the FA model. This allows us to learn word representations for both languages, a translation matrix relating these vector spaces, as well as alignments at the same time.\nOur modifications to the log-bilinear model are as follows. Where the original log-bilinear language model uses context words to predict the next word—this is simply the distributed extension of an n-gram language model—we use a word from the source language in a parallel sentence to predict a target word. An additional aspect of our model, which demonstrates its flexibility, is that it is simple to include further context from the source sentence, such as words around the aligned word or syntactic and semantic annotations. In this paper we experiment with a transformed sum over k context words to each side of the aligned source word. We evaluate different context sizes and report the results in Section 5. We define the energy function for the translation probabilities to be\nE(f, ei) = −\n( k∑\ns=−k rTei+sTs\n) rf−bTr rf−bf (1)\nwhere rei , rf ∈ Rd are vector representations for source and target words ei+s ∈ VE , f ∈ VF in their respective vocabularies, Ts ∈ Rd×d is the transformation matrix for each surrounding context position, br ∈ Rd are the representation biases, and bf ∈ R is a bias for each word f ∈ VF .\nThe translation probability is given by p(f |ei) = 1Zei exp (−E(f, ei)) , where Zei = ∑ f exp (−E(f, ei)) is the normalizer.\nIn addition to these translation probabilities, we\nhave parameterized the translation probabilities for the null word using a softmax over an additional weight vector."
    }, {
      "heading" : "3.1 Class Factorization",
      "text" : "We improve training performance using a class factorization strategy (Morin and Bengio, 2005) as follows. We augment the translation probability to be p(f |e) = p(cf |e) p(f |cf , e) where cf is a unique predetermined class of f ; the class probability is modeled using a similar log-bilinear model as above, but instead of predicting a word representation rf we predict the class representation rcf (which is learned with the model) and we add respective new context matrices and biases. Note that the probability of the word f depends on both the class and the given context words: it is normalized only over words in the class cf .\nIn our training we create classes based on word frequencies in the corpus as follows. Considering words in the order of their decreasing frequency, we add word types into a class until the total frequency of the word types in the currently considered class is less than total tokens√\n|VF | and the class size is less than √ |VF |. We have found that the maximal class size affects the speed the most."
    }, {
      "heading" : "4 Learning",
      "text" : "The original FA model optimizes the likelihood using the expectation maximization (EM) algorithm where, in the M-step, the parameter update is analytically solvable, except for the λ parameter (the diagonal tension), which is optimized using gradient descent (Dyer et al., 2013). We modified the implementations provided with CDEC (Dyer et al., 2010), retaining its default parameters.\nIn our model, DWA, we optimize the likelihood using the EM as well. However, while training we fix the counts of the E-step to those computed by FA, trained for the default 5 iterations, to aid the convergence rate, and optimize the M-step only. Let θ be the parameters for our model. Then the gradient for each sentence is given by\n∂\n∂θ log p(f |e) = J∑\nk=1 I∑ l=0 [ p(l|k, I, J) p(fk|el)∑I i=0 p(i|k, I, J) p(fk|ei)\n· ∂ ∂θ log(p(l|k, I, J) p(fk|el))\n]\nwhere the first part are the counts from the FA model and second part comes from our model.\nWe compute the gradient for the alignment probabilities in the same way as in the FA model, and the gradient for the translation probabilities using back-propagation (Rumelhart et al., 1986). For parameter update, we use ADAGRAD as the gradient descent algorithm (Duchi et al., 2011)."
    }, {
      "heading" : "5 Experiments",
      "text" : "We first evaluate the alignment error rate of our approach, which establishes the model’s ability to both learn alignments as well as word representations that explain these alignments. Next, we use a cross-lingual document classification task to verify that the representations are semantically useful. We also inspect the embedding space qualitatively to get some insight into the learned structure."
    }, {
      "heading" : "5.1 Alignment Evaluation",
      "text" : "We compare the alignments learned here with those of the FASTALIGN model which produces very good alignments and translation BLEU scores. We use the same language pairs and datasets as in Dyer et al. (2013), that is the FBIS Chinese-English corpus, and the French-English section of the Europarl corpus (Koehn, 2005). We used the preprocessing tools from CDEC and further replaced all unique tokens with UNK. We trained our models with 100 dimensional representations for up to 40 iterations, and the FA model for 5 iterations as is the default.\nTable 1 shows that our model learns alignments on part with those of the FA model. This is in line with expectation as our model was trained using the FA expectations. However, it confirms that the learned word representations are able to explain translation probabilities. Surprisingly, context seems to have little impact on the alignment error, suggesting that the model receives sufficient information from the aligned words themselves."
    }, {
      "heading" : "5.2 Document Classification",
      "text" : "A standard task for evaluating cross-lingual word representations is document classification where training is performed in one and evaluation in another language. This tasks require semantically plausible embeddings (for classification) which are valid across two languages (for the semantic transfer). Hence this task requires more of the word embeddings than the previous task.\nWe mainly follow the setup of Klementiev et al. (2012) and use the German-English parallel corpus of the European Parliament proceedings to train the word representations. We perform the classification task on the Reuters RCV1/2 corpus. Unlike Klementiev et al. (2012), we do not use that corpus during the representation learning phase. We remove all words occurring less than five times in the data and learn 40 dimensional word embeddings in line with prior work.\nTo train a classifier on English data and test it on German documents we first project word representations from English into German: we select the most probable German word according to the learned translation probabilities, and then compute document representations by averaging the word representations in each document. We use these projected representations for training and subsequently test using the original German data and representations. We use an averaged perceptron classifier as in prior work, with the number of epochs (3) tuned on a subset of the training set.\nTable 2 shows baselines from previous work and classification accuracies. Our model outperforms the model by Klementiev et al. (2012), and it also outperforms the most comparable models by Hermann and Blunsom (2014b) when training on German data and performs on par with it when training on English data.1 It seems that our model learns more informative representations towards document classification, even without additional monolingual language models or context information. Again the impact of context is inconclusive.\n1From Hermann and Blunsom (2014a, 2014b) we only compare with models equivalent with respect to embedding dimensionality and training data. They still achieve the state of the art when using additional training data."
    }, {
      "heading" : "5.3 Representation Visualization",
      "text" : "Following the document classification task we want to gain further insight into the types of features our embeddings learn. For this we visualize word representations using t-SNE projections (van der Maaten and Hinton, 2008). Figure 1 shows an extract from our projection of the 2,000 most frequent German words, together with an expected representation of a translated English word given translation probabilities. Here, it is interesting to see that the model is able to learn related representations for words chair and ratspräsidentschaft (presidency) even though these words were not aligned by our model. Figure 2 shows an extract from the visualization of the 10,000 most frequent English words trained on another corpus. Here again, it is evident that the embeddings are semantically plausible with similar words being closely aligned."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We presented a new probabilistic model for learning bilingual word representations. This distributed word alignment model (DWA) learns both representations and alignments at the same time. We have shown that the DWA model is able to learn alignments on par with the FASTALIGN alignment model which produces very good alignments, thereby determining the efficacy of the learned representations which are used to calculate\nword translation probabilities for the alignment task. Subsequently, we have demonstrated that our model can effectively be used to project documents from one language to another. The word representations our model learns as part of the alignment process are semantically plausible and useful. We highlighted this by applying these embeddings to a cross-lingual document classification task where we outperform prior work, achieve results on par with the current state of the art and provide new state-of-the-art results on one of the tasks. Having provided a probabilistic account of word representations across multiple languages, future work will focus on applying this model to machine translation and related tasks, for which previous approaches of learning such embeddings are less suited. Another avenue for further study is to combine this method with monolingual language models, particularly in the context of semantic transfer into resource-poor languages."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work was supported by a Xerox Foundation Award and EPSRC grant number EP/K036580/1. We acknowledge the use of the Oxford ARC."
    } ],
    "references" : [ {
      "title" : "A neural probabilistic language model",
      "author" : [ "Yoshua Bengio", "Réjean Ducharme", "Pascal Vincent", "Christian Jauvin." ],
      "venue" : "Journal of Machine Learning Research, 3:1137–1155, February.",
      "citeRegEx" : "Bengio et al\\.,? 2003",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2003
    }, {
      "title" : "The mathematics of statistical machine translation: parameter estimation",
      "author" : [ "Peter F. Brown", "Vincent J. Della Pietra", "Stephen A. Della Pietra", "Robert L. Mercer." ],
      "venue" : "Computational Linguistics, 19(2):263–311, June.",
      "citeRegEx" : "Brown et al\\.,? 1993",
      "shortCiteRegEx" : "Brown et al\\.",
      "year" : 1993
    }, {
      "title" : "A unified architecture for natural language processing: deep neural networks with multitask learning",
      "author" : [ "Ronan Collobert", "Jason Weston." ],
      "venue" : "Proceedings of ICML.",
      "citeRegEx" : "Collobert and Weston.,? 2008",
      "shortCiteRegEx" : "Collobert and Weston.",
      "year" : 2008
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "John Duchi", "Elad Hazan", "Yoram Singer." ],
      "venue" : "Journal of Machine Learning Research, 12:2121–2159, July.",
      "citeRegEx" : "Duchi et al\\.,? 2011",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2011
    }, {
      "title" : "cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models",
      "author" : [ "Chris Dyer", "Adam Lopez", "Juri Ganitkevitch", "Jonathan Weese", "Ferhan Ture", "Phil Blunsom", "Hendra Setiawan", "Vladimir Eidelman", "Philip Resnik" ],
      "venue" : null,
      "citeRegEx" : "Dyer et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Dyer et al\\.",
      "year" : 2010
    }, {
      "title" : "A simple, fast, and effective reparameterization of IBM model 2",
      "author" : [ "Chris Dyer", "Victor Chahuneau", "Noah A. Smith." ],
      "venue" : "Proceedings of NAACLHLT.",
      "citeRegEx" : "Dyer et al\\.,? 2013",
      "shortCiteRegEx" : "Dyer et al\\.",
      "year" : 2013
    }, {
      "title" : "Improving Vector Space Word Representations Using Multilingual Correlation",
      "author" : [ "Manaal Faruqui", "Chris Dyer." ],
      "venue" : "Proceedings of EACL.",
      "citeRegEx" : "Faruqui and Dyer.,? 2014",
      "shortCiteRegEx" : "Faruqui and Dyer.",
      "year" : 2014
    }, {
      "title" : "Learning bilingual lexicons from monolingual corpora",
      "author" : [ "Aria Haghighi", "Percy Liang", "Taylor Berg-Kirkpatrick", "Dan Klein." ],
      "venue" : "Proceedings of ACLHLT.",
      "citeRegEx" : "Haghighi et al\\.,? 2008",
      "shortCiteRegEx" : "Haghighi et al\\.",
      "year" : 2008
    }, {
      "title" : "The Role of Syntax in Vector Space Models of Compositional Semantics",
      "author" : [ "Karl Moritz Hermann", "Phil Blunsom." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Hermann and Blunsom.,? 2013",
      "shortCiteRegEx" : "Hermann and Blunsom.",
      "year" : 2013
    }, {
      "title" : "Multilingual Distributed Representations without Word Alignment",
      "author" : [ "Karl Moritz Hermann", "Phil Blunsom." ],
      "venue" : "Proceedings of ICLR.",
      "citeRegEx" : "Hermann and Blunsom.,? 2014a",
      "shortCiteRegEx" : "Hermann and Blunsom.",
      "year" : 2014
    }, {
      "title" : "Multilingual Models for Compositional Distributional Semantics",
      "author" : [ "Karl Moritz Hermann", "Phil Blunsom." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Hermann and Blunsom.,? 2014b",
      "shortCiteRegEx" : "Hermann and Blunsom.",
      "year" : 2014
    }, {
      "title" : "Semantic Frame Identification with Distributed Word Representations",
      "author" : [ "Karl Moritz Hermann", "Dipanjan Das", "Jason Weston", "Kuzman Ganchev." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Hermann et al\\.,? 2014",
      "shortCiteRegEx" : "Hermann et al\\.",
      "year" : 2014
    }, {
      "title" : "Multimodal neural language models",
      "author" : [ "Ryan Kiros", "Richard S Zemel", "Ruslan Salakhutdinov." ],
      "venue" : "NIPS Deep Learning Workshop.",
      "citeRegEx" : "Kiros et al\\.,? 2013",
      "shortCiteRegEx" : "Kiros et al\\.",
      "year" : 2013
    }, {
      "title" : "Inducing crosslingual distributed representations of words",
      "author" : [ "Alexandre Klementiev", "Ivan Titov", "Binod Bhattarai." ],
      "venue" : "Proceedings of COLING.",
      "citeRegEx" : "Klementiev et al\\.,? 2012",
      "shortCiteRegEx" : "Klementiev et al\\.",
      "year" : 2012
    }, {
      "title" : "Europarl: A Parallel Corpus for Statistical Machine Translation",
      "author" : [ "Philipp Koehn." ],
      "venue" : "Proceedings of the 10th Machine Translation Summit.",
      "citeRegEx" : "Koehn.,? 2005",
      "shortCiteRegEx" : "Koehn.",
      "year" : 2005
    }, {
      "title" : "Learning multilingual word representations using a bag-of-words autoencoder",
      "author" : [ "Stanislas Lauly", "Alex Boulanger", "Hugo Larochelle." ],
      "venue" : "NIPS Deep Learning Workshop.",
      "citeRegEx" : "Lauly et al\\.,? 2013",
      "shortCiteRegEx" : "Lauly et al\\.",
      "year" : 2013
    }, {
      "title" : "Exploiting similarities among languages for machine translation",
      "author" : [ "Tomas Mikolov", "Quoc V. Le", "Ilya Sutskever." ],
      "venue" : "CoRR, abs/1309.4168.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Three new graphical models for statistical language modelling",
      "author" : [ "Andriy Mnih", "Geoffrey Hinton." ],
      "venue" : "Proceedings of ICML.",
      "citeRegEx" : "Mnih and Hinton.,? 2007",
      "shortCiteRegEx" : "Mnih and Hinton.",
      "year" : 2007
    }, {
      "title" : "Playing atari with deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller." ],
      "venue" : "NIPS Deep Learning Workshop.",
      "citeRegEx" : "Mnih et al\\.,? 2013",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2013
    }, {
      "title" : "Hierarchical probabilistic neural network language model",
      "author" : [ "Frederic Morin", "Yoshua Bengio." ],
      "venue" : "Robert G. Cowell and Zoubin Ghahramani, editors, Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics, pages 246–252.",
      "citeRegEx" : "Morin and Bengio.,? 2005",
      "shortCiteRegEx" : "Morin and Bengio.",
      "year" : 2005
    }, {
      "title" : "Learning representations by backpropagating errors",
      "author" : [ "D.E. Rumelhart", "G.E. Hinton", "R.J. Williams." ],
      "venue" : "Nature, 323:533–536, October.",
      "citeRegEx" : "Rumelhart et al\\.,? 1986",
      "shortCiteRegEx" : "Rumelhart et al\\.",
      "year" : 1986
    }, {
      "title" : "Multilingual deep learning",
      "author" : [ "A P Sarath Chandar", "M Khapra Mitesh", "B Ravindran", "Vikas Raykar", "Amrita Saha." ],
      "venue" : "Deep Learning Workshop at NIPS.",
      "citeRegEx" : "Chandar et al\\.,? 2013",
      "shortCiteRegEx" : "Chandar et al\\.",
      "year" : 2013
    }, {
      "title" : "Smooth bilingual n-gram translation",
      "author" : [ "Holger Schwenk", "Marta R. Costa-jussa", "Jose A.R. Fonollosa." ],
      "venue" : "Proceedings of EMNLP-CoNLL.",
      "citeRegEx" : "Schwenk et al\\.,? 2007",
      "shortCiteRegEx" : "Schwenk et al\\.",
      "year" : 2007
    }, {
      "title" : "Continuous space translation models for phrase-based statistical machine translation",
      "author" : [ "Holger Schwenk." ],
      "venue" : "Proceedings of COLING: Posters.",
      "citeRegEx" : "Schwenk.,? 2012",
      "shortCiteRegEx" : "Schwenk.",
      "year" : 2012
    }, {
      "title" : "Semi-supervised recursive autoencoders for predicting sentiment distributions",
      "author" : [ "Richard Socher", "Jeffrey Pennington", "Eric H. Huang", "Andrew Y. Ng", "Christopher D. Manning." ],
      "venue" : "Proceedings of EMNLP.",
      "citeRegEx" : "Socher et al\\.,? 2011",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2011
    }, {
      "title" : "Word representations: A simple and general method for semi-supervised learning",
      "author" : [ "Joseph Turian", "Lev-Arie Ratinov", "Yoshua Bengio." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Turian et al\\.,? 2010",
      "shortCiteRegEx" : "Turian et al\\.",
      "year" : 2010
    }, {
      "title" : "Visualizing high-dimensional data using t-sne",
      "author" : [ "L.J.P. van der Maaten", "G.E. Hinton." ],
      "venue" : "Journal of Machine Learning Research, 9:2579–2605.",
      "citeRegEx" : "Maaten and Hinton.,? 2008",
      "shortCiteRegEx" : "Maaten and Hinton.",
      "year" : 2008
    }, {
      "title" : "Bilingual Word Embeddings for Phrase-Based Machine Translation",
      "author" : [ "Will Y. Zou", "Richard Socher", "Daniel Cer", "Christopher D. Manning." ],
      "venue" : "Proceedings of EMNLP.",
      "citeRegEx" : "Zou et al\\.,? 2013",
      "shortCiteRegEx" : "Zou et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "This includes AI and reinforcement learning (Mnih et al., 2013), image retrieval (Kiros et al.",
      "startOffset" : 44,
      "endOffset" : 63
    }, {
      "referenceID" : 12,
      "context" : ", 2013), image retrieval (Kiros et al., 2013), language modelling (Bengio et al.",
      "startOffset" : 25,
      "endOffset" : 45
    }, {
      "referenceID" : 0,
      "context" : ", 2013), language modelling (Bengio et al., 2003), sentiment analysis (Socher et al.",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 24,
      "context" : ", 2003), sentiment analysis (Socher et al., 2011; Hermann and Blunsom, 2013), framesemantic parsing (Hermann et al.",
      "startOffset" : 28,
      "endOffset" : 76
    }, {
      "referenceID" : 8,
      "context" : ", 2003), sentiment analysis (Socher et al., 2011; Hermann and Blunsom, 2013), framesemantic parsing (Hermann et al.",
      "startOffset" : 28,
      "endOffset" : 76
    }, {
      "referenceID" : 11,
      "context" : ", 2011; Hermann and Blunsom, 2013), framesemantic parsing (Hermann et al., 2014), and document classification (Klementiev et al.",
      "startOffset" : 58,
      "endOffset" : 80
    }, {
      "referenceID" : 13,
      "context" : ", 2014), and document classification (Klementiev et al., 2012).",
      "startOffset" : 37,
      "endOffset" : 62
    }, {
      "referenceID" : 0,
      "context" : ", 2013), language modelling (Bengio et al., 2003), sentiment analysis (Socher et al., 2011; Hermann and Blunsom, 2013), framesemantic parsing (Hermann et al., 2014), and document classification (Klementiev et al., 2012). In Natural Language Processing (NLP), the use of distributed representations is motivated by the idea that they could capture semantics and/or syntax, as well as encoding a continuous notion of similarity, thereby enabling information sharing between similar words and other units. The success of distributed approaches to a number of tasks, such as listed above, supports this notion and its implied benefits (see also Turian et al. (2010) and Collobert and Weston (2008)).",
      "startOffset" : 29,
      "endOffset" : 662
    }, {
      "referenceID" : 0,
      "context" : ", 2013), language modelling (Bengio et al., 2003), sentiment analysis (Socher et al., 2011; Hermann and Blunsom, 2013), framesemantic parsing (Hermann et al., 2014), and document classification (Klementiev et al., 2012). In Natural Language Processing (NLP), the use of distributed representations is motivated by the idea that they could capture semantics and/or syntax, as well as encoding a continuous notion of similarity, thereby enabling information sharing between similar words and other units. The success of distributed approaches to a number of tasks, such as listed above, supports this notion and its implied benefits (see also Turian et al. (2010) and Collobert and Weston (2008)).",
      "startOffset" : 29,
      "endOffset" : 694
    }, {
      "referenceID" : 13,
      "context" : "As opposed to previous work in this field, which has relied on hard alignments or bilingual lexica (Klementiev et al., 2012; Mikolov et al., 2013), we marginalize out the alignments, thus capturing more bilingual semantic context.",
      "startOffset" : 99,
      "endOffset" : 146
    }, {
      "referenceID" : 16,
      "context" : "As opposed to previous work in this field, which has relied on hard alignments or bilingual lexica (Klementiev et al., 2012; Mikolov et al., 2013), we marginalize out the alignments, thus capturing more bilingual semantic context.",
      "startOffset" : 99,
      "endOffset" : 146
    }, {
      "referenceID" : 10,
      "context" : "Subsequently, we apply these embeddings to a standard document classification task and show that they outperform the current published state of the art (Hermann and Blunsom, 2014b).",
      "startOffset" : 152,
      "endOffset" : 180
    }, {
      "referenceID" : 5,
      "context" : "As a by-product we develop a distributed version of FASTALIGN (Dyer et al., 2013), which performs on par with the original model, thereby demonstrating the efficacy of the learned bilingual representations.",
      "startOffset" : 62,
      "endOffset" : 81
    }, {
      "referenceID" : 1,
      "context" : "The IBM alignment models, introduced by Brown et al. (1993), form the basis of most statistical machine translation systems.",
      "startOffset" : 40,
      "endOffset" : 60
    }, {
      "referenceID" : 4,
      "context" : "ation of IBM model 2 introduced by Dyer et al. (2013). This model is both fast and produces alignments on par with the state of the art.",
      "startOffset" : 35,
      "endOffset" : 54
    }, {
      "referenceID" : 4,
      "context" : "ation of IBM model 2 introduced by Dyer et al. (2013). This model is both fast and produces alignments on par with the state of the art. Further, to induce the distributed representations we incorporate ideas from the log-bilinear language model presented by Mnih and Hinton (2007).",
      "startOffset" : 35,
      "endOffset" : 282
    }, {
      "referenceID" : 1,
      "context" : "IBM model 2 (Brown et al., 1993) learns alignment and translation probabilities in a generative style as follows:",
      "startOffset" : 12,
      "endOffset" : 32
    }, {
      "referenceID" : 5,
      "context" : "We use FASTALIGN (FA) (Dyer et al., 2013), a log-linear reparametrization of IBM model 2.",
      "startOffset" : 22,
      "endOffset" : 41
    }, {
      "referenceID" : 17,
      "context" : "We use the log-bilinear language model proposed by Mnih and Hinton (2007). It is an n-gram based model defined in terms of an energy function E(wn;w1:n−1).",
      "startOffset" : 51,
      "endOffset" : 74
    }, {
      "referenceID" : 7,
      "context" : "These models are also highly related to prior work on bilingual lexicon induction (Haghighi et al., 2008).",
      "startOffset" : 82,
      "endOffset" : 105
    }, {
      "referenceID" : 9,
      "context" : "Similar to the model presented here, Klementiev et al. (2012) and Zou et al.",
      "startOffset" : 37,
      "endOffset" : 62
    }, {
      "referenceID" : 9,
      "context" : "Similar to the model presented here, Klementiev et al. (2012) and Zou et al. (2013) learn bilingual embeddings using word alignments.",
      "startOffset" : 37,
      "endOffset" : 84
    }, {
      "referenceID" : 7,
      "context" : "These models are also highly related to prior work on bilingual lexicon induction (Haghighi et al., 2008). Other recent approaches include Sarath Chandar et al. (2013), Lauly et al.",
      "startOffset" : 83,
      "endOffset" : 168
    }, {
      "referenceID" : 7,
      "context" : "These models are also highly related to prior work on bilingual lexicon induction (Haghighi et al., 2008). Other recent approaches include Sarath Chandar et al. (2013), Lauly et al. (2013) and Hermann and Blunsom (2014a, 2014b).",
      "startOffset" : 83,
      "endOffset" : 189
    }, {
      "referenceID" : 7,
      "context" : "These models are also highly related to prior work on bilingual lexicon induction (Haghighi et al., 2008). Other recent approaches include Sarath Chandar et al. (2013), Lauly et al. (2013) and Hermann and Blunsom (2014a, 2014b). These models avoid word alignment by transferring information across languages using a composed sentence-level representation. While all of these approaches are related to the model proposed in this paper, it is important to note that our approach is novel by providing a probabilistic account of these word embeddings. Further, we learn word alignments and simultaneously use these alignments to guide the representation learning, which could be advantageous particularly for rare tokens, where a sentence based approach might fail to transfer information. Related work also includes Mikolov et al. (2013), who learn a transformation matrix to",
      "startOffset" : 83,
      "endOffset" : 836
    }, {
      "referenceID" : 21,
      "context" : "reconcile monolingual embedding spaces, in an l2 norm sense, using dictionary entries instead of alignments, as well as Schwenk et al. (2007) and Schwenk (2012), who also use distributed representations for estimating translation probabilities.",
      "startOffset" : 120,
      "endOffset" : 142
    }, {
      "referenceID" : 21,
      "context" : "reconcile monolingual embedding spaces, in an l2 norm sense, using dictionary entries instead of alignments, as well as Schwenk et al. (2007) and Schwenk (2012), who also use distributed representations for estimating translation probabilities.",
      "startOffset" : 120,
      "endOffset" : 161
    }, {
      "referenceID" : 6,
      "context" : "Faruqui and Dyer (2014) use a technique based on CCA and alignments to project monolingual word representations to a common vector space.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 19,
      "context" : "We improve training performance using a class factorization strategy (Morin and Bengio, 2005) as follows.",
      "startOffset" : 69,
      "endOffset" : 93
    }, {
      "referenceID" : 5,
      "context" : "The original FA model optimizes the likelihood using the expectation maximization (EM) algorithm where, in the M-step, the parameter update is analytically solvable, except for the λ parameter (the diagonal tension), which is optimized using gradient descent (Dyer et al., 2013).",
      "startOffset" : 259,
      "endOffset" : 278
    }, {
      "referenceID" : 4,
      "context" : "We modified the implementations provided with CDEC (Dyer et al., 2010), retaining its default parameters.",
      "startOffset" : 51,
      "endOffset" : 70
    }, {
      "referenceID" : 20,
      "context" : "We compute the gradient for the alignment probabilities in the same way as in the FA model, and the gradient for the translation probabilities using back-propagation (Rumelhart et al., 1986).",
      "startOffset" : 166,
      "endOffset" : 190
    }, {
      "referenceID" : 3,
      "context" : "For parameter update, we use ADAGRAD as the gradient descent algorithm (Duchi et al., 2011).",
      "startOffset" : 71,
      "endOffset" : 91
    }, {
      "referenceID" : 14,
      "context" : "(2013), that is the FBIS Chinese-English corpus, and the French-English section of the Europarl corpus (Koehn, 2005).",
      "startOffset" : 103,
      "endOffset" : 116
    }, {
      "referenceID" : 4,
      "context" : "We use the same language pairs and datasets as in Dyer et al. (2013), that is the FBIS Chinese-English corpus, and the French-English section of the Europarl corpus (Koehn, 2005).",
      "startOffset" : 50,
      "endOffset" : 69
    }, {
      "referenceID" : 13,
      "context" : "We mainly follow the setup of Klementiev et al. (2012) and use the German-English parallel corpus of the European Parliament proceedings to train the word representations.",
      "startOffset" : 30,
      "endOffset" : 55
    }, {
      "referenceID" : 13,
      "context" : "We mainly follow the setup of Klementiev et al. (2012) and use the German-English parallel corpus of the European Parliament proceedings to train the word representations. We perform the classification task on the Reuters RCV1/2 corpus. Unlike Klementiev et al. (2012), we do not use that corpus during the representation learning phase.",
      "startOffset" : 30,
      "endOffset" : 269
    }, {
      "referenceID" : 10,
      "context" : "Our model outperforms the model by Klementiev et al. (2012), and it also outperforms the most comparable models by Hermann and Blunsom (2014b) when training on German data and performs on par with it when training on English data.",
      "startOffset" : 35,
      "endOffset" : 60
    }, {
      "referenceID" : 8,
      "context" : "(2012), and it also outperforms the most comparable models by Hermann and Blunsom (2014b) when training on German data and performs on par with it when training on English data.",
      "startOffset" : 62,
      "endOffset" : 90
    }, {
      "referenceID" : 13,
      "context" : "Baselines are the majority class, glossed, and MT (Klementiev et al., 2012).",
      "startOffset" : 50,
      "endOffset" : 75
    }, {
      "referenceID" : 9,
      "context" : "(2012), BiCVM ADD (Hermann and Blunsom, 2014a), and BiCVM BI (Hermann and Blunsom, 2014b).",
      "startOffset" : 18,
      "endOffset" : 46
    }, {
      "referenceID" : 10,
      "context" : "(2012), BiCVM ADD (Hermann and Blunsom, 2014a), and BiCVM BI (Hermann and Blunsom, 2014b).",
      "startOffset" : 61,
      "endOffset" : 89
    }, {
      "referenceID" : 10,
      "context" : "Baselines are the majority class, glossed, and MT (Klementiev et al., 2012). Further, we are comparing to Klementiev et al. (2012), BiCVM ADD (Hermann and Blunsom, 2014a), and BiCVM BI (Hermann and Blunsom, 2014b).",
      "startOffset" : 51,
      "endOffset" : 131
    } ],
    "year" : 2014,
    "abstractText" : "We present a probabilistic model that simultaneously learns alignments and distributed representations for bilingual data. By marginalizing over word alignments the model captures a larger semantic context than prior work relying on hard alignments. The advantage of this approach is demonstrated in a cross-lingual classification task, where we outperform the prior published state of the art.",
    "creator" : "TeX"
  }
}