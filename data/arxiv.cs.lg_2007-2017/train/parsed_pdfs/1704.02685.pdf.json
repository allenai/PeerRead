{
  "name" : "1704.02685.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Learning Important Features Through Propagating Activation Differences",
    "authors" : [ "Avanti Shrikumar", "Peyton Greenside", "Anshul Kundaje" ],
    "emails" : [ "(avanti@stanford.edu),", "(pgreens@stanford.edu),", "(akundaje@stanford.edu)." ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "As neural networks become increasingly popular, their black box reputation is a barrier to adoption when interpretability is paramount. Here, we present DeepLIFT (Deep Learning Important FeaTures), a novel algorithm to assign importance score to the inputs for a given output. Our approach is unique in two regards: first, it frames the question of importance in terms of differences from a ‘reference’ state, where the ‘reference’ is chosen by the user according to what is appropriate for the problem at hand. In contrast to most gradient-based methods, using a difference-from-reference allows DeepLIFT to propagate an importance signal even in situations where the gradient is zero and avoids artifacts caused by discontinuities in the gradient. Second, by optionally giving separate consideration to the effects of positive and negative contribu-\n1Stanford University, Stanford, California, USA. Correspondence to: A Shrikumar (avanti@stanford.edu), P Greenside (pgreens@stanford.edu), A Kundaje (akundaje@stanford.edu).\ntions at nonlinearities, DeepLIFT can reveal dependencies missed by other approaches. As DeepLIFT scores are computed using a backpropagation-like algorithm, they can be obtained efficiently in a single backward pass through the network after a prediction has been made."
    }, {
      "heading" : "2. Previous Work",
      "text" : "This section provides a review of existing approaches to assign importance scores for a given task and input example."
    }, {
      "heading" : "2.1. Perturbation-based forward propagation approaches",
      "text" : "These approaches make perturbations to individual inputs or neurons and observe the impact on later neurons in the network. Zeiler & Fergus (Zeiler & Fergus, 2013) occluded different segments of an input image and visualized the change in the activations of later layers. “In-silico mutagenesis” (Zhou & Troyanskaya, 2015) introduced virtual mutations at individual positions in a genomic sequence and quantified the their impact on the output. Zintgraf et al. (Zintgraf et al., 2017) proposed a clever strategy for analyzing the difference in a prediction after marginalizing over each input patch. However, such methods can be computationally inefficient as each perturbation requires a separate forward propagation through the network. They may also underestimate the importance of features that have saturated their contribution to the output (Fig. 1).\nar X\niv :1\n70 4.\n02 68\n5v 1\n[ cs\n.C V\n] 1\n0 A\npr 2\n01 7"
    }, {
      "heading" : "2.2. Backpropagation-based approaches",
      "text" : "In contrast to perturbation methods, backpropagation approaches are computationally efficient as they propagate an importance signal from the output neuron backwards through the layers towards the input in a single pass. DeepLIFT belongs to this family of approaches."
    }, {
      "heading" : "2.2.1. GRADIENTS, DECONVOLUTIONAL NETWORKS AND GUIDED BACKPROPAGATION",
      "text" : "Simonyan et al. (Simonyan et al., 2013) proposed using the gradient of the output w.r.t. pixels of an input image to compute a “saliency map” of the image in the context of image classification tasks. The authors showed that this was similar to deconvolutional networks (Zeiler & Fergus, 2013) except for the handling of the nonlinearity at rectified linear units (ReLUs). When backpropagating importance using gradients, the gradient coming into a ReLU during the backward pass is zero’d out if the input to the ReLU during the forward pass is negative. By contrast, when backpropagating an importance signal in deconvolutional networks, the importance signal coming into a ReLU during the backward pass is zero’d out if and only if it is negative, with no regard to sign of the input to the ReLU during the forward pass. Springenberg et al., (Springenberg et al., 2014) combined these two approaches into Guided Backpropagation, which zero’s out the importance signal at a ReLU if either the input to the ReLU during the forward pass is negative or the importance signal during the backward pass is negative. Guided Backpropagation can be thought of as equivalent to computing gradients, with the caveat that any gradients that become negative during the backward pass are discarded at ReLUs. Due to the zero-ing out of negative gradients, both guided backpropagation and deconvolutional networks can fail to highlight inputs that contribute negatively to the output. Additionally, none of the three approaches would address the saturation problem illustrated in Fig. 1, as the gradient of y w.r.t. h is negative (causing Guided Backprop and deconvolutional networks to assign zero importance), and the gradient of h w.r.t both i1 and i2 is zero when i1 + i2 > 1 (causing both gradients and Guided Backprop to be zero). Discontinuities in the gradients can also cause undesirable artifacts (Fig. 2)."
    }, {
      "heading" : "2.2.2. LAYERWISE RELEVANCE PROPAGATION AND GRADIENT × INPUT",
      "text" : "Bach et al. (Bach et al., 2015) proposed an approach for propagating importance scores called Layerwise Relevance Propagation (LRP). Shrikumar et al. and Kindermans et al. (Shrikumar et al., 2016; Kindermans et al., 2016) showed that absent modifications to deal with numerical stability, the original LRP rules were equivalent within a scaling factor to an elementwise product between the saliency maps of\nSimonyan et al. and the input (in other words, gradient × input). In our experiments, we compare DeepLIFT to gradient × input as the latter is easily implemented on a GPU, whereas LRP does not currently have GPU implementations available to our knowledge.\nWhile gradient × input is often preferable to gradients alone as it leverages the sign and strength of the input, it still does not address the saturation problem in Fig. 1 or the thresholding artifact in Fig. 2."
    }, {
      "heading" : "2.2.3. INTEGRATED GRADIENTS",
      "text" : "Instead of computing the gradients at only the current value of the input, one can integrate the gradients as the inputs are scaled up from some starting value (eg: all zeros) to their current value (Sundararajan et al., 2016). This addressess the saturation and thresholding problems of Fig. 1 and Fig. 2, but numerically obtaining high-quality integrals adds computational overhead. Further, this approach can still give highly misleading results (see Section 3.4.3)."
    }, {
      "heading" : "2.3. Grad-CAM and Guided CAM",
      "text" : "Grad-CAM (Selvaraju et al., 2016) computes a coarsegrained feature-importance map by associating the feature maps in the final convolutional layer with particular classes based on the gradients of each class w.r.t. each feature map, and then using the weighted activations of the feature maps as an indication of which inputs are most important. To obtain more fine-grained feature importance, the authors proposed performing an elementwise product between the scores obtained from Grad-CAM and the scores obtained from Guided Backpropagation, termed Guided Grad-CAM. However, this strategy inherits the limitations of Guided Backpropagation caused by zero-ing out negative gradients during backpropagation. It is also specific to convolutional neural networks."
    }, {
      "heading" : "3. The DeepLIFT Method",
      "text" : ""
    }, {
      "heading" : "3.1. The DeepLIFT Philosophy",
      "text" : "DeepLIFT explains the difference in output from some ‘reference’ output in terms of the difference of the input from some ‘reference’ input. The ‘reference’ input represents some default or ‘neutral’ input that is chosen according to what is appropriate for the problem at hand (see Section 3.3 for more details). Formally, let t represent some target output neuron of interest and let x1, x2, ..., xn represent some neurons in some intermediate layer or set of layers that are necessary and sufficient to compute t. Let t0 represent the reference activation of t. We define the quantity ∆t to be the difference-from-reference, that is ∆t = t− t0. DeepLIFT assigns contribution scores C∆xi∆t to ∆xi s.t.:\nn∑ i=1 C∆xi∆t = ∆t (1)\nWe call Eq. 1 the summation-to-delta property. C∆xi∆t can be thought of as the amount of difference-fromreference in t that is attributed to or ‘blamed’ on the difference-from-reference of xi. Note that when a neuron’s transfer function is well-behaved, the output is locally linear in its inputs, providing additional motivation for Eq. 1.\nC∆xi∆t can be non-zero even when ∂t ∂xi is zero. This allows DeepLIFT to address a fundamental limitation of gradients because, as illustrated in Fig. 1, a neuron can be signaling meaningful information even in the regime where its gradient is zero. Another drawback of gradients addressed by DeepLIFT is illustrated in Fig. 2, where the discontinuous nature of gradients causes sudden jumps in the importance score over infinitesimal changes in the input. By contrast, the difference-from-reference is continuous, allowing DeepLIFT to avoid discontinuities caused by bias terms."
    }, {
      "heading" : "3.2. Multipliers and the Chain Rule",
      "text" : ""
    }, {
      "heading" : "3.2.1. DEFINITION OF MULTIPLIERS",
      "text" : "For a given input neuron x with difference-from-reference ∆x, and target neuron t with difference-from-reference ∆t that we wish to compute the contribution to, we define the multiplier m∆x∆t as:\nm∆x∆t = C∆x∆t\n∆x (2)\nIn other words, the multiplier m∆x∆t is the contribution of ∆x to ∆t divided by ∆x. Note the close analogy to the\nidea of partial derivatives: the partial derivative ∂t∂x is the infinitesimal change in t caused by an infinitesimal change in x, divided by the infinitesimal change in x. The multiplier is similar in spirit to a partial derivative, but over finite differences instead of infinitesimal ones."
    }, {
      "heading" : "3.2.2. THE CHAIN RULE FOR MULTIPLIERS",
      "text" : "Assume we have an input layer with neurons x1, ..., xn, a hidden layer with neurons y1, ..., yn, and some target output neuron z. Given values for m∆xi∆yj and m∆yj∆z , the following definition of m∆xi∆z is consistent with the summation-to-delta property in Eq. 1 (see Appendix A for the proof):\nm∆xi∆z = ∑ j m∆xi∆yjm∆yj∆z (3)\nWe refer to Eq. 3 as the chain rule for multipliers. Given the multipliers for each neuron to its immediate successors, we can compute the multipliers for any neuron to a given target neuron efficiently via backpropagation - analogous to how the chain rule for partial derivatives allows us to compute the gradient w.r.t. the output via backpropagation."
    }, {
      "heading" : "3.3. Defining the reference",
      "text" : "When formulating the DeepLIFT rules described in Section 3.5, we assume that the reference of a neuron is its activation on the reference input. Formally, say we have a neuron x with inputs i1, i2, ... such that x = f(i1, i2, ...). Given the reference activations i01, i 0 2, ... of the inputs, we can calculate the reference activation x0 of the output as:\nx0 = f(i01, i 0 2, ...) (4)\ni.e. references for all neurons can be found by choosing a reference input and propagating activations through the net.\nThe choice of a reference input is critical for obtaining insightful results from DeepLIFT. In practice, choosing a good reference would rely on domain-specific knowledge, and in some cases it may be best to compute DeepLIFT scores against multiple different references. As a guiding principle, we can ask ourselves “what am I interested in measuring differences against?”. For MNIST, we use a reference input of all-zeros as this is the background of the images. For the binary classification tasks on DNA sequence inputs (strings over the alphabet {A,C,G,T}), we obtain sensible results using either a reference input containing the expected frequencies of ACGT in the background (Fig. 5), or by averaging the results over multiple reference inputs for each sequence that are generated by shuffling each original sequence (Appendix J).\nIt is important to note that gradient×input implicitly uses a reference of all-zeros (it is equivalent to a first-order Taylor approximation of gradient×∆input where ∆ is measured w.r.t. an input of zeros). Similary, integrated gradients (Section 2.2.3) requires the user to specify a starting point for the integral, which is conceptually similar to specifying a reference for DeepLIFT. While Guided Backprop and pure gradients don’t use a reference, we argue that this is a limitation as these methods only describe the local behaviour of the output at the specific input value, without considering how the output behaves over a range of inputs."
    }, {
      "heading" : "3.4. Separating positive and negative contributions",
      "text" : "We will see in Section 3.5.3 that, in some situations, it is essential to treat positive and negative contributions differently. To do this, for every neuron xi, we will introduce ∆x+i and ∆x − i that represent the positive and negative components of ∆xi, such that:\n∆xi = ∆x + i + ∆x − i\nC∆xi∆t = C∆x+i ∆t + C∆x−i ∆t\nThe importance of this will become apparent when discussing the RevealCancel rule (Section 3.5.3), where we find that m∆x+i ∆t and m∆x−i ∆t may be different. However, for the Linear and Rescale rules (Section 3.5.1 and Section 3.5.2), we have m∆xi∆t = m∆x+i ∆t = m∆x−i ∆t."
    }, {
      "heading" : "3.5. Rules for assigning contribution scores",
      "text" : "We present the rules for assigning contribution scores for each neuron to its immediate inputs. In conjunction with the chain rule for multipliers (Section 3.2), these rules can be used to find the contributions of any input (not just the immediate inputs) to a target output via backpropagation."
    }, {
      "heading" : "3.5.1. THE LINEAR RULE",
      "text" : "This applies to Dense and Convolutional layers (excluding nonlinearities). Let y be a linear function of its inputs xi such that y = b + ∑ i wixi. We have ∆y = ∑ i wi∆xi. We define the positive and negative parts of ∆y as:\n∆y+ = ∑ i 1{wi∆xi > 0}wi∆xi\n= ∑ i 1{wi∆xi > 0}wi(∆x+i + ∆x − i )\n∆y− = ∑ i 1{wi∆xi < 0}wi∆xi\n= ∑ i 1{wi∆xi < 0}wi(∆x+i + ∆x − i )\nWhich leads to the following choice for the contributions: C∆x+i ∆y+ = 1{wi∆xi > 0}wi∆x+i\nC∆x−i ∆y+ = 1{wi∆xi > 0}wi∆x−i C∆x+i ∆y− = 1{wi∆xi < 0}wi∆x+i C∆x−i ∆y− = 1{wi∆xi < 0}wi∆x−i\nWe can then find multipliers using the definition in Section 3.2.1, which givesm∆x+i ∆y+ = m∆x−i ∆y+ = 1{wi∆xi > 0}wi and m∆x+i ∆y− = m∆x−i ∆y− = 1{wi∆xi < 0}wi.\nWhat about when ∆xi = 0? While setting multipliers to 0 in this case would be consistent with summation-to-delta, it is possible that ∆x+i and ∆x − i are nonzero (and cancel each other out), in which case setting the multiplier to 0 would fail to propagate importance to them. To avoid this, we set m∆x+i ∆y+ = m∆x+i ∆y− = 0.5wi when ∆xi is 0 (similarly for ∆x−). See Appendix B for how to compute these multipliers using standard neural network ops."
    }, {
      "heading" : "3.5.2. THE RESCALE RULE",
      "text" : "This rule applies to nonlinear transformations that take a single input, such as the ReLU, tanh or sigmoid operations. Let neuron y be a nonlinear transformation of its input x such that y = f(x). Because y has only one input, we have by summation-to-delta that C∆x∆y = ∆y, and consequently m∆x∆y = ∆y∆x . For the Rescale rule, we set ∆y + and ∆y− proportional to ∆x+ and ∆x− as follows:\n∆y+ = ∆y\n∆x ∆x+ = C∆x+∆y+\n∆y− = ∆y\n∆x ∆x− = C∆x−∆y−\nBased on this, we get:\nm∆x+∆y+ = m∆x−∆y− = m∆x∆y = ∆y\n∆x\nIn the case where x→ x0, we have ∆x→ 0 and ∆y → 0. The definition of the multiplier approaches the derivative, i.e. m∆x∆y → dydx , where the dy dx is evaluated at x = x\n0. We can thus use the gradient instead of the multiplier when x is close to its reference to avoid numerical instability issues caused by having a small denominator.\nNote that the Rescale rule addresses both the saturation and the thresholding problems illustrated in Fig. 1 and Fig. 2. In the case of Fig. 1, if i01 = i02 = 0, then at i1 + i2 > 1 we have ∆h = −1 and ∆y = 1, giving m∆h∆y = ∆y ∆h = −1 even though dy dh = 0 (in other words, using difference-from-reference allows information to flow even when the gradient is zero). In the case of Fig. 2, assuming x0 = y0 = 0, at x = 10 + we have ∆y = , giving m∆x∆y = 10+ and C∆x∆y = ∆x ×m∆x∆y = . By contrast, gradient×input assigns a contribution of 10+ to x and−10 to the bias term (DeepLIFT never assigns importance to bias terms).\nAs revealed in previous work (Lundberg & Lee, 2016), there is a connection between DeepLIFT and Shapely values. Briefly, the Shapely values measure the average marginal effect of including an input over all possible orderings in which inputs can be included. If we define “including” an input as setting it to its actual value instead of its reference value, DeepLIFT can be thought of as a fast approximation of the Shapely values. At the time, Lundberg & Lee cited a preprint of DeepLIFT which described only the Linear and Rescale rules with no separate treatment of positive and negative contributions."
    }, {
      "heading" : "3.5.3. AN IMPROVED APPROXIMATION OF THE",
      "text" : "SHAPELY VALUES: THE REVEALCANCEL RULE\nWhile the Rescale rule improves upon simply using gradients, there are still some situations where it can provide misleading results. Consider the min(i1, i2) operation depicted in Fig. 3, with reference values of i1 = 0 and i2 = 0. Using the Rescale rule, all importance would be assigned either to i1 or to i2 (whichever is smaller). This can obscure the fact that both inputs are relevant for the min operation.\nTo understand why this occurs, consider the case when i1 > i2. We have h1 = (i1 − i2) > 0 and h2 = max(0, h1) = h1. By the Linear rule, we calculate that C∆i1∆h1 = i1 and C∆i2∆h1 = −i2. By the Rescale rule, the multiplier m∆h1∆h2 is ∆h2 ∆h1\n= 1, and thus C∆i1∆h2 = m∆h1∆h2C∆i1∆h1 = i1 and C∆i2∆h2 = m∆h1∆h2C∆i2∆h1 = −i2. The total contribution of i1 to the output o becomes (i1 − C∆i1∆h2) = (i1 − i1) = 0, and the total contribution of i2 to o is −C∆i2∆h2 = i2. This calculation is misleading as it discounts the fact that C∆i2∆h2 would be 0 if i1 were 0 - in other words, it ignores a dependency induced between i1 and i2 that comes from i2 canceling out i1 in the nonlinear neuron h2. A similar failure occurs when i1 < i2; the Rescale rule results in C∆i1∆o = i1 and C∆i2∆o = 0. Note that gradients, gradient×input, Guided Backpropagation and integrated gradients would also assign all importance to either i1 or i2, because for any given input the gradient is zero for one of i1 or i2 (see Appendix C for a detailed calculation).\nOne way to address this is by treating the positive and negative contributions separately. We again consider the nonlinear neuron y = f(x). Instead of assuming that ∆y+ and ∆y− are proportional to ∆x+ and ∆x− and that m∆x+∆y+ = m∆x−∆y− = m∆x∆y (as is done for the Rescale rule), we define them as follows:\n∆y+ = 1\n2\n( f(x0 + ∆x+)− f(x0) ) + 1\n2\n( f(x0 + ∆x− + ∆x+)− f(x0 + ∆x−) ) ∆y− = 1\n2\n( f(x0 + ∆x−)− f(x0) )\n+ 1\n2\n( f(x0 + ∆x+ + ∆x−)− f(x0 + ∆x+) ) m∆x+∆y+ = C∆x+y+\n∆x+ =\n∆y+ ∆x+ ; m∆x−∆y− = ∆y− ∆x−\nIn other words, we set ∆y+ to the average impact of ∆x+ after no terms have been added and after ∆x− has been added, and we set ∆y− to the average impact of ∆x− after no terms have been added and after ∆x+ has been added. This can be thought of as the Shapely values of ∆x+ and ∆x− contributing to y.\nBy considering the impact of the positive terms in the absence of negative terms, and the impact of negative terms in the absence of positive terms, we alleviate some of the issues that arise from positive and negative terms canceling each other out. In the case of Fig. 3, RevealCancel would assign a contribution of 0.5 min(i1, i2) to both inputs (see Appendix C for a detailed calculation).\nWhile the RevealCancel rule also avoids the saturation and thresholding pitfalls illustrated in Fig. 1 and Fig. 2, there are some circumstances where we might prefer to use the Rescale rule. Specifically, consider a thresholded ReLU where ∆y > 0 iff ∆x ≥ b. If ∆x < b merely indicates noise, we would want to assign contributions of 0 to both ∆x+ and ∆x− (as done by the Rescale rule) to mitigate the noise. RevealCancel may assign nonzero contributions by considering ∆x+ in the absence of ∆x− and vice versa."
    }, {
      "heading" : "3.6. Choice of target layer",
      "text" : "In the case of softmax or sigmoid outputs, we may prefer to compute contributions to the linear layer preceding the final nonlinearity rather than the final nonlinearity itself. This would be to avoid an attentuation caused by the summation-to-delta property described in Section 3.1. For example, consider a sigmoid output o = σ(y), where y is the logit of the sigmoid function. Assume y = x1 + x2, where x01 = x 0 2 = 0. When x1 = 50 and x2 = 0, the output o saturates at very close to 1 and the contributions of x1 and x2 are 0.5 and 0 respectively. However, when x1 = 100 and x2 = 100, the output o is still very close\nto 0, but the contributions of x1 and x2 are now both 0.25. This can be misleading when comparing scores across different inputs because a stronger contribution to the logit would not always translate into a higher DeepLIFT score. To avoid this, we compute contributions to y rather than o.\nAdjustments for softmax layers\nIf we compute contributions to the linear layer preceding the softmax rather than the softmax output, an issue that could arise is that the final softmax output involves a normalization over all classes, but the linear layer before the softmax does not. To address this, we can normalize the contributions to the linear layer by subtracting the mean contribution to all classes. Formally, if n is the number of classes, C∆x∆ci represents the unnormalized contribution to class ci in the linear layer and C ′∆x∆ci represents the normalized contribution, we have:\nC ′∆x∆ci = C∆x∆ci − 1\nn n∑ j=1 C∆x∆cj (5)\nAs a justification for this normalization, we note that subtracting a fixed value from all the inputs to the softmax leaves the output of the softmax unchanged."
    }, {
      "heading" : "4. Results",
      "text" : ""
    }, {
      "heading" : "4.1. Digit classification (MNIST)",
      "text" : "We train a convolutional neural network on MNIST (LeCun et al., 1999) using Keras (Chollet, 2015) to perform digit classification and obtain 99.2% test-set accuracy. The architecture consists of two convolutional layers, followed by a fully connected layer, followed by the softmax output layer (see Appendix D for full details on model architecture and training). We used convolutions with stride > 1 instead of pooling layers, which did not result in a drop in performance as is consistent with previous work (Springenberg et al., 2014). For DeepLIFT and integrated gradients, we used a reference input of all zeros.\nTo evaluate importance scores obtained by different methods, we design the following task: given an image that originally belongs to class co, we identify which pixels to erase to convert the image to some target class ct. We do this by finding Sxidiff = Sxico − Sxict (where Sxic is the score for pixel xi and class c) and erasing up to 157 pixels (20% of the image) ranked in descending order of Sxidiff for which Sxidiff > 0. We then evaluate the change in the log-odds score between classes co and ct for the original image and the image with the pixels erased.\nAs shown in Fig. 4, DeepLIFT with the RevealCancel rule outperformed the other backpropagation-based methods. Integrated gradients (Section 2.2.3) computed numerically over either 5 or 10 intervals produced results comparable\nto each other, suggesting that adding more intervals would not change the result. Integrated gradients also performed comparably to gradient*input, suggesting that saturation and thresholding failure modes are not common on MNIST data. Guided Backprop discards negative gradients during backpropagation, perhaps explaining its poor performance at discriminating between classes. We also explored using the Rescale rule instead of RevealCancel on various layers and found that it degraded performance (Appendix E)."
    }, {
      "heading" : "4.2. Classifying regulatory DNA sequences (Genomics)",
      "text" : "Next, we compared the importance scoring methods when applied to classification tasks on DNA sequence inputs (strings over the alphabet {A,C,G,T}). The human genome has millions of DNA sequence elements ( 200-1000 in length) containing specific combinations of short functional words to which regulatory proteins (RPs) bind to regulate gene activity. Each RP (e.g. GATA1) has binding\naffinity to specific collections of short DNA words (motifs) (e.g. GATAA and GATTA). A key problem in computational genomics is the discovery of motifs in regulatory DNA elements that give rise to distinct molecular signatures (labels) which can be measured experimentally. Here, in order to benchmark DeepLIFT and competing methods to uncover predictive patterns in DNA sequences, we design a simple simulation that captures the essence of the motif discovery problem described above.\nBackground DNA sequences of length 200 were generated by sampling the letters ACGT at each position with probabilities 0.3, 0.2, 0.2 and 0.3 respectively. Motif instances were randomly sampled from previously known probabilistic motif models (See Appendix F) of two RPs\nnamed GATA1 and TAL1 (Fig. 6a)(Kheradpour & Kellis, 2014), and 0-3 instances of a given motif were inserted at random non-overlapping positions in the DNA sequences. We trained a multi-task neural network with two convolutional layers, global average pooling and one fullyconnected layer on 3 binary classification tasks. Positive labeled sequences in task 1 represented “both GATA1 and TAL1 present”, task 2 represented “GATA1 present” and in task 3 represented “TAL1 present”. 14 of sequences had both GATA1 and TAL1 motifs (labeled 111), 14 had only GATA1 (labeled 010), 14 had only TAL1 (labeled 001), and 1 4 had no motifs (labeled 000). Details of the simulation, network architecture and predictive performance are given in Appendix F. For DeepLIFT and integrated gradients,\nwe used a reference input that had the expected frequencies of ACGT at each position (i.e. we set the ACGT channel axis to 0.3, 0.2, 0.2, 0.3; see Appendix J for results using shuffled sequences as a reference). For fair comparison, this reference was also used for gradient×input and Guided Backprop×input (“input” is more accurately called ∆input where ∆ measured w.r.t the reference). For DNA sequence inputs, we found Guided Backprop×input performed better than vanilla Guided Backprop; thus, we used the former.\nGiven a particular subsequence, it is possible to compute the log-odds score that the subsequence was sampled from a particular motif vs. originating from the background distribution of ACGT. To evaluate different importancescoring methods, we found the top 5 matches (as ranked by their log-odds score) to each motif for each sequence from the test set, as well as the total importance allocated to the match by different importance-scoring methods for each task. The results are shown in Fig. 5 (for TAL1) and Appendix E (for GATA1). Ideally, we expect an importance scoring method to show the following properties: (1) high scores for TAL1 motifs on task 2 and (2) low scores for TAL1 on task 1, with (3) higher scores corresponding to stronger log-odds matches; analogous pattern for GATA1 motifs (high for task 1, low for task 2); (4) high scores for both TAL1 and GATA1 motifs for task 0, with (5) higher scores on sequences containing both kinds of motifs vs. sequences containing only one kind (revealing cooperativity; corresponds to red dots lying above green dots in Fig. 5).\nWe observe Guided Backprop×input fails (2) by assigning positive importance to TAL1 on task 1 (see Appendix H for an example sequence). It fails property (4) by failing to identify cooperativity in task 0 (red dots overlay green dots). Both Guided Backprop×input and gradient×input show suboptimal behavior regarding property (3), in that there is a sudden increase in importance when the log-odds score is around 7, but little differentiation at higher logodds scores (by contrast, the other methods show a more gradual increase). As a result, Guided Backprop×input and gradient×input can assign unduly high importance to weak motif matches (Fig. 6). This is a practical consequence of the thresholding problem from Fig. 2. The large discontinuous jumps in gradient also result in inflated scores (note the scale on the y-axes) relative to other methods.\nWe explored three versions of DeepLIFT: Rescale rule at all nonlinearities (DeepLIFT-Rescale), RevealCancel at all nonlinearities (DeepLIFT-RevealCancel), and Rescale rule at the two convolutional layers with RevealCancel at the fully connected layer (DeepLIFT-fc-RC-conv-RS). In contrast to the results on MNIST, we found that DeepLIFT-fcRC-conv-RS reduced noise relative to pure RevealCancel. We think this is because of the noise-suppression property discussed at the end of Section 3.5.3; if the convolutional\nlayers act like motif detectors, the input to convolutional neurons that do not fire may just represent noise and importance should not be propagated to them (see Fig. 6 for an example sequence).\nGradient×inp, integrated gradients and DeepLIFT-Rescale occasionally miss relevance of TAL1 for Task 0 (Fig. 5b), which is corrected by using RevealCancel on the fully connected layer (see example sequence in Fig. 6). Note that the RevealCancel scores seem to be tiered. As illustrated in Appendix I, this is related to having multiple instances of a given motif in a sequence (eg: when there are multiple TAL1 motifs, the importance assigned to the presence of TAL1 is distributed across all the motifs)."
    }, {
      "heading" : "5. Conclusion",
      "text" : "We have presented DeepLIFT, a novel approach for computing importance scores based on explaining the difference of the output from some ‘reference’ output in terms of differences of the inputs from their ‘reference’ inputs. Using the difference-from-reference allows information to propagate even when the gradient is zero (Fig. 1), which could prove especially useful in Recurrent Neural Networks where saturating activations like sigmoid or tanh are popular. DeepLIFT avoids placing potentially misleading importance on bias terms (in contrast to gradient*input - see Fig. 2). By allowing separate treatment of positive and negative contributions, the DeepLIFT-RevealCancel rule can identify dependencies missed by other methods (Fig.\n3). Open questions include how to apply DeepLIFT to RNNs, how to compute a good reference empirically from the data, and how best to propagate importance through ‘max’ operations (as in Maxout or Maxpooling neurons) beyond simply using the gradients."
    }, {
      "heading" : "6. Appendix",
      "text" : "The appendix can be downloaded at: https://goo.gl/T114x4"
    }, {
      "heading" : "7. Author Contributions",
      "text" : "AS & PG conceptualized DeepLIFT. AS implemented DeepLIFT. AS ran experiments on MNIST. AS & PG ran experiments on genomic data. AK provided guidance and feedback. AS, PG and AK wrote the manuscript."
    }, {
      "heading" : "8. Acknowledgements",
      "text" : "We thank Anna Shcherbina for early experiments applying DeepLIFT to image data and beta-testing."
    } ],
    "references" : [ {
      "title" : "Systematic discovery and characterization of regulatory motifs in encode tf binding experiments",
      "author" : [ "Kheradpour", "Pouya", "Kellis", "Manolis" ],
      "venue" : "Nucleic acids research,",
      "citeRegEx" : "Kheradpour et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kheradpour et al\\.",
      "year" : 2014
    }, {
      "title" : "Investigating the influence of noise and distractors on the interpretation of neural networks",
      "author" : [ "Kindermans", "Pieter-Jan", "Schtt", "Kristof", "Mller", "KlausRobert", "Dhne", "Sven" ],
      "venue" : "CoRR, abs/1611.07270,",
      "citeRegEx" : "Kindermans et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kindermans et al\\.",
      "year" : 2016
    }, {
      "title" : "The mnist database of handwritten digits",
      "author" : [ "LeCun", "Yann", "Cortes", "Corinna", "Burges", "Christopher J.C" ],
      "venue" : "http://yann.lecun.com/exdb/mnist/,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1999
    }, {
      "title" : "An unexpected unity among methods for interpreting model predictions",
      "author" : [ "Lundberg", "Scott", "Lee", "Su-In" ],
      "venue" : "CoRR, abs/1611.07478,",
      "citeRegEx" : "Lundberg et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lundberg et al\\.",
      "year" : 2016
    }, {
      "title" : "Grad-cam: Why did you say that? visual explanations from deep networks via gradient-based localization",
      "author" : [ "tra", "Dhruv" ],
      "venue" : "CoRR, abs/1610.02391,",
      "citeRegEx" : "tra and Dhruv.,? \\Q2016\\E",
      "shortCiteRegEx" : "tra and Dhruv.",
      "year" : 2016
    }, {
      "title" : "Not just a black box: Learning important features through propagating activation differences",
      "author" : [ "Shrikumar", "Avanti", "Greenside", "Peyton", "Shcherbina", "Anna", "Kundaje", "Anshul" ],
      "venue" : "arXiv preprint arXiv:1605.01713,",
      "citeRegEx" : "Shrikumar et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Shrikumar et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep inside convolutional networks: Visualising image classification models and saliency maps",
      "author" : [ "Simonyan", "Karen", "Vedaldi", "Andrea", "Zisserman", "Andrew" ],
      "venue" : "arXiv preprint arXiv:1312.6034,",
      "citeRegEx" : "Simonyan et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Simonyan et al\\.",
      "year" : 2013
    }, {
      "title" : "Striving for simplicity: The all convolutional net",
      "author" : [ "Springenberg", "Jost Tobias", "Dosovitskiy", "Alexey", "Brox", "Thomas", "Riedmiller", "Martin A" ],
      "venue" : "CoRR, abs/1412.6806,",
      "citeRegEx" : "Springenberg et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Springenberg et al\\.",
      "year" : 2014
    }, {
      "title" : "Gradients of counterfactuals",
      "author" : [ "Sundararajan", "Mukund", "Taly", "Ankur", "Yan", "Qiqi" ],
      "venue" : "CoRR, abs/1611.02639,",
      "citeRegEx" : "Sundararajan et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Sundararajan et al\\.",
      "year" : 2016
    }, {
      "title" : "Visualizing and understanding convolutional networks",
      "author" : [ "Zeiler", "Matthew D", "Fergus", "Rob" ],
      "venue" : "CoRR, abs/1311.2901,",
      "citeRegEx" : "Zeiler et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Zeiler et al\\.",
      "year" : 2013
    }, {
      "title" : "Predicting effects of noncoding variants with deep learning-based sequence model",
      "author" : [ "Zhou", "Jian", "Troyanskaya", "Olga G" ],
      "venue" : "Nat Methods,",
      "citeRegEx" : "Zhou et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2015
    }, {
      "title" : "Visualizing deep neural network decisions: Prediction difference analysis",
      "author" : [ "Zintgraf", "Luisa M", "Cohen", "Taco S", "Adel", "Tameem", "Welling", "Max" ],
      "venue" : "ICLR,",
      "citeRegEx" : "Zintgraf et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Zintgraf et al\\.",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "(Zintgraf et al., 2017) proposed a clever strategy for analyzing the difference in a prediction after marginalizing over each input patch.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 6,
      "context" : "(Simonyan et al., 2013) proposed using the gradient of the output w.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 7,
      "context" : ", (Springenberg et al., 2014) combined these two approaches into Guided Backpropagation, which zero’s out the importance signal at a ReLU if either the input to the ReLU during the forward pass is negative or the importance signal during the backward pass is negative.",
      "startOffset" : 2,
      "endOffset" : 29
    }, {
      "referenceID" : 5,
      "context" : "(Shrikumar et al., 2016; Kindermans et al., 2016) showed that absent modifications to deal with numerical stability, the original LRP rules were equivalent within a scaling factor to an elementwise product between the saliency maps of Simonyan et al.",
      "startOffset" : 0,
      "endOffset" : 49
    }, {
      "referenceID" : 1,
      "context" : "(Shrikumar et al., 2016; Kindermans et al., 2016) showed that absent modifications to deal with numerical stability, the original LRP rules were equivalent within a scaling factor to an elementwise product between the saliency maps of Simonyan et al.",
      "startOffset" : 0,
      "endOffset" : 49
    }, {
      "referenceID" : 8,
      "context" : "Instead of computing the gradients at only the current value of the input, one can integrate the gradients as the inputs are scaled up from some starting value (eg: all zeros) to their current value (Sundararajan et al., 2016).",
      "startOffset" : 199,
      "endOffset" : 226
    }, {
      "referenceID" : 2,
      "context" : "We train a convolutional neural network on MNIST (LeCun et al., 1999) using Keras (Chollet, 2015) to perform digit classification and obtain 99.",
      "startOffset" : 49,
      "endOffset" : 69
    }, {
      "referenceID" : 7,
      "context" : "We used convolutions with stride > 1 instead of pooling layers, which did not result in a drop in performance as is consistent with previous work (Springenberg et al., 2014).",
      "startOffset" : 146,
      "endOffset" : 173
    } ],
    "year" : 2017,
    "abstractText" : "The purported “black box” nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its ‘reference activation’ and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. A detailed video tutorial on the method is at http://goo.gl/qKb7pL and code is at http://goo.gl/RM8jvH.",
    "creator" : "LaTeX with hyperref package"
  }
}