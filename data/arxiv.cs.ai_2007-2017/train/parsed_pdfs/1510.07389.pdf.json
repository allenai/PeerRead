{
  "name" : "1510.07389.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "The Human Kernel",
    "authors" : [ "Andrew Gordon Wilson", "Christoph Dann", "Christopher G. Lucas", "Eric P. Xing" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Truly intelligent systems can learn and make decisions without human intervention. Therefore it is not surprising that early machine learning efforts, such as the perceptron, have been neurally inspired [1]. In recent years, probabilistic modelling has become a cornerstone of machine learning approaches [2, 3, 4], with applications in neural processing [5, 6, 3, 7] and human learning [8, 9].\nFrom a probabilistic perspective, the ability for a model to automatically discover patterns and perform extrapolation is determined by its support (which solutions are a priori possible), and inductive biases (which solutions are a priori likely). Ideally, we want a model to be able to represent many possible solutions to a given problem, with inductive biases which can extract intricate structure from limited data. For example, if we are performing character recognition, we would want our support to contain a large collection of potential characters, accounting even for rare writing styles, and our inductive biases to reasonably reflect the probability of encountering each character [10, 11].\nThe support and inductive biases of a wide range of probabilistic models, and thus the ability for these models to learn and generalise, is implicitly controlled by a covariance kernel, which determines the similarities between pairs of datapoints. For example, Bayesian basis function regression (including, e.g., all polynomial models), splines, and infinite neural networks, can all exactly be represented as a Gaussian process with a particular kernel function [12, 11, 10]. Moreover, the Fisher kernel provides a mechanism to reformulate probabilistic generative models as kernel methods [13].\nIn this paper, we wish to reverse engineer human-like support and inductive biases for function learning, using a Gaussian process based kernel learning formalism. In particular:\n• We create new human function learning datasets, including novel function extrapolation problems and multiple-choice questions that explore human intuitions about simplicity and explanatory power. To participate in these experiments, and view demonstrations, see http://functionlearning.com/\n• We develop a statistical framework for kernel learning from the predictions of a model, conditioned on the (training) information that model is given. The ability to sample multiple sets of posterior predictions from a model, at any input locations of our choice, given any dataset of our choice, provides unprecedented statistical strength for kernel learning. By\nar X\niv :1\n51 0.\n07 38\n9v 3\n[ cs\n.L G\n] 3\nD ec\n2 01\n5\ncontrast, standard kernel learning involves fitting a kernel to a fixed dataset that can only be viewed as a single realisation from a stochastic process. Our framework leverages spectral mixture kernels [14] and non-parametric estimates.\n• We exploit this framework to directly learn kernels from human responses, which contrasts with all prior work on human function learning, where one compares a fixed model to human responses. Moreover, we consider individual rather than averaged human extrapolations.\n• We interpret the learned kernels to gain scientific insights into human inductive biases, including the ability to adapt to new information for function learning. We also use the learned “human kernels” to inspire new types of covariance functions which can enable extrapolation on problems which are difficult for conventional Gaussian process models.\n• We study Occam’s razor in human function learning, and compare to Gaussian process marginal likelihood based model selection, which we show is biased towards under-fitting.\n• We provide an expressive quantitative means to compare existing machine learning algorithms with human learning, and a mechanism to directly infer human prior representations.\nOur work is intended as a preliminary step towards building probabilistic kernel machines that encapsulate human-like support and inductive biases. Since state of the art machine learning methods perform conspicuously poorly on a number of extrapolation problems which would be easy for humans [10], such efforts have the potential to help automate machine learning and improve performance on a wide range of tasks – including settings which are difficult for humans to process (e.g., big data and high dimensional problems). Finally, the presented framework can be considered in a more general context, where one wishes to efficiently reverse engineer interpretable properties of any model (e.g., a deep neural network) from its predictions.\nWe further describe related work in section 2. In section 3 we introduce a framework for learning kernels from human responses, and employ this framework in section 4. In the supplement, we provide background on Gaussian processes [12], which we recommend as a review."
    }, {
      "heading" : "2 Related Work",
      "text" : "Historically, efforts to understand human function learning have focused on rule-based relationships (e.g., polynomial or power-law functions) [15, 16], or interpolation based on similarity learning [17, 18]. Griffiths et al. [19] were the first to note that a Gaussian process framework can be used to unify these two perspectives. They introduced a GP model with a mixture of RBF and polynomial kernels to reflect the human ability to learn arbitrary smooth functions while still identifying simple parametric functions. They applied this model to a standard set of evaluation tasks, comparing predictions on simple functions to averaged human judgments, and interpolation performance to human error rates. Lucas et al. [20] extended this model to accommodate a wider range of phenomena, using an infinite mixture of Gaussian process experts [21], and Lucas et al. [22] used this model to shed new light on human predictions given sparse data.\nOur work complements these pioneering Gaussian process models and prior work on human function learning, but has many features that distinguish it from previous contributions: (1) rather than iteratively building models and comparing them to human predictions, based on fixed assumptions about the regularities humans can recognize, we are directly learning the properties of the human model through advanced kernel learning techniques; (2) essentially all models of function learning, including past GP models, are evaluated on averaged human responses, setting aside individual differences and erasing critical statistical structure in the data1. By contrast, our approach uses individual responses; (3) many recent model evaluations rely on relatively small and heterogeneous sets of experimental data. The evaluation corpora using recent reviews [23, 19] are limited to a small set of parametric forms, i.e., polynomial, power-law, logistic, logarithmic, exponential, and sinusoidal, and the more detailed analyses tend to involve only linear, quadratic, and logistic functions. Other projects have collected richer and more detailed data sets [24, 25], but we are only aware of\n1For example, averaging prior draws from a Gaussian process would remove the structure necessary for kernel learning, leaving us simply with an approximation of the prior mean function.\ncoarse-grained, qualitative analyses using these data. Moreover, experiments that depart from simple parametric functions tend to use very noisy data. Thus it is unsurprising that participants tend to revert to the prior mode that arises in almost all function learning experiments: linear functions, especially with slope-1 and intercept-0 [24, 25] (but see [26]). In a departure from prior work, we create original function learning problems with no simple parametric description and no noise – where it is obvious that human learners cannot resort to simple rules – and acquire the human data ourselves. We hope these novel datasets will inspire more detailed findings on function learning; (4) we learn kernels from human responses, which (i) provide insights into the biases that drive human function learning and the human ability to progressively adapt to new information, and (ii) enable human-like extrapolations on problems that are difficult for conventional Gaussian process models; and (5) we investigate Occam’s razor in human function learning and nonparametric model selection."
    }, {
      "heading" : "3 The Human Kernel",
      "text" : "The rule-based and associative theories for human function learning can be unified as part of a Gaussian process framework. Indeed, Gaussian processes contain a large array of probabilistic models, and have the non-parametric flexibility to produce infinitely many consistent (zero training error) fits to any dataset. Moreover, the support and inductive biases of a Gaussian process are encaspulated by a covariance kernel. Our goal is to learn Gaussian process covariance kernels from predictions made by humans on function learning experiments, to gain a better understanding of human learning, and to inspire new machine learning models, with improved extrapolation performance, and minimal human intervention."
    }, {
      "heading" : "3.1 Problem Setup",
      "text" : "A (human) learner is given access to data y at training inputs X , and makes predictions y∗ at testing inputs X∗. We assume the predictions y∗ are samples from the learner’s posterior distribution over possible functions, following results showing that human inferences and judgments resemble posterior samples across a wide range of perceptual and decision-making tasks [27, 28, 29]. We assume we can obtain multiple draws of y∗ for a given X and y."
    }, {
      "heading" : "3.2 Kernel Learning",
      "text" : "In standard Gaussian process applications, one has access to a single realisation of data y, and performs kernel learning by optimizing the marginal likelihood of the data with respect to covariance function hyperparameters θ, as described in the supplementary material. However, with only a single realisation of data we are highly constrained in our ability to learn an expressive kernel function – requiring us to make strong assumptions, such as RBF covariances, to extract any useful information from the data. One can see this by simulating N datapoints from a GP with a known kernel, and then visualising the empirical estimate yy> of the known covariance matrix K. The empirical estimate, in most cases, will look nothing like K. However, perhaps surprisingly, if we have even a small number of multiple draws from a GP, we can recover a wide array of covariance matrices K using the empirical estimator Y Y >/M − ȳȳ>, where Y is an N ×M data matrix, for M draws, and ȳ is a vector of empirical means.\nThe typical goal in choosing (learning) a kernel is to minimize some loss function evaluated on training data, ultimately to minimize generalisation error. But here we want to reverse engineer the (prediction) kernel of a (human) model, based on both training data and predictions of that model given that training data. If we have a single sample extrapolation, y∗, at test inputs X∗, based on training points y, and Gaussian noise, the probability p(y∗|y, kθ) is given by the posterior predictive distribution of a Gaussian process, with f∗ ≡ y∗. One can use this probability as a utility function for kernel learning, much like the marginal likelihood. See the supplementary material for details of these distributions.\nOur problem setup affords unprecedented opportunities for flexible kernel learning. If we have multiple sample extrapolations from a given set of training data, y(1)∗ ,y (2) ∗ , . . . ,y (W ) ∗ , then the pre-\ndictive conditional marginal likelihood becomes ∏W\nj=1 p(y (j) ∗ |y, kθ). One could apply this new\nobjective, for instance, if we were to view different human extrapolations as multiple draws from a common generative model. Clearly this assumption is not entirely correct, since different people will have different biases, but it naturally suits our purposes: we are not as interested in the differences between people as their shared inductive biases, and assuming multiple draws from a common generative model provides extraordinary statistical strength for learning these shared biases. Ultimately, we will consider modelling the human responses both separately and collectively, studying the differences and similarities between the responses.\nOne option for learning a prediction kernel is to specify a flexible parametric form for k and then learn θ by optimizing our chosen objective functions. For this approach, we choose the recent spectral mixture kernels of Wilson and Adams [14], which can model a wide range of stationary covariances, and are intended to help automate kernel selection. However, we note that our objective function can readily be applied to other parametric forms. We also consider empirical non-parametric kernel estimation, since non-parametric kernel estimators can have the flexibility to converge to any positive definite kernel, and thus become appealing when we have the signal strength provided by multiple draws from a stochastic process."
    }, {
      "heading" : "4 Human Experiments",
      "text" : "We wish to discover kernels that capture human inductive biases for learning functions and extrapolating from complex or ambiguous training data. We start by testing the consistency of our kernel learning procedure in section 4.1. In section 4.2, we study progressive function learning. Indeed, humans participants will have a different representation (e.g., learned kernel) for different observed data, and examining how these representations progressively adapt with new information can shed light on our prior biases. In section 4.3, we learn human kernels to extrapolate on tasks which are difficult for Gaussian processes with standard kernels. In section 4.4, we study model selection in human function learning. All human participants were recruited using Amazon’s mechanical turk and saw experimental materials that are described in the supplement, with demonstrations provided at http://functionlearning.com/. When we are considering stationary ground truth kernels, we use a spectral mixture for kernel learning; otherwise, we use a non-parametric empirical estimate."
    }, {
      "heading" : "4.1 Reconstructing Ground Truth Kernels",
      "text" : "We use simulations with a known ground truth to test the consistency of our kernel learning procedure, and the effects of multiple posterior draws, in converging to a kernel which has been used to make predictions.\nWe sample 20 datapoints y from a GP with RBF kernel (the supplement describes GPs), kRBF(x,x\n′) = exp(−0.5||x − x′||/`2), at random input locations. Conditioned on these data, we then sample multiple posterior draws, y(1)∗ , . . . ,y (W ) ∗ , each containing 20 datapoints, from a GP with a spectral mixture kernel [14] with two components (the prediction kernel). The prediction kernel has deliberately not been trained to fit the data kernel. To reconstruct the prediction kernel, we learn the parameters θ of a randomly initialized spectral mixture kernel with five components, by optimizing the predictive conditional marginal likelihood ∏W j=1 p(y (j) ∗ |y, kθ) wrt θ.\nFigure 1 compares the learned kernels for different numbers of posterior draws W against the data kernel (RBF) and the prediction kernel (spectral mixture). For a single posterior draw, the learned kernel captures the high-frequency component of the prediction kernel but fails at reconstructing the low-frequency component. Only with multiple draws does the learned kernel capture the longerrange dependencies. The fact that the learned kernel converges to the prediction kernel, which is different from the data kernel, shows the consistency of our procedure, which could be used to infer aspects of human inductive biases."
    }, {
      "heading" : "4.2 Progressive Function Learning",
      "text" : "We asked humans to extrapolate beyond training data in two sets of 5 functions, each drawn from GPs with known kernels. The learners extrapolated on these problems in sequence, and thus had an opportunity to progressively learn more about the underlying kernel in each set. To further test\nprogressive function learning, we repeated the first function at the end of the experiment, for six functions in each set. We asked for extrapolation judgments because they provide more information about inductive biases than interpolation, and pose difficulties for conventional Gaussian process kernels [14, 10, 30].\nThe observed functions are shown in black in Figure 2, the human responses in blue, and the true extrapolation in dashed black. In the first two rows, the black functions are drawn from a GP\nwith a rational quadratic (RQ) kernel [12] (for heavy tailed correlations), and there are 20 human participants.\nWe show the learned human kernel, the data generating kernel, the human kernel learned from a spectral mixture, and an RBF kernel trained only on the data, in Figures 2(g) and 2(h), respectively corresponding to Figures 2(a) and 2(f). Initially, both the human learners and RQ kernel show heavy tailed behaviour, and a bias for decreasing correlations with distance in the input space, but the human learners have a high degree of variance. By the time they have seen Figure 2(h), they are more confident in their predictions, and more accurately able to estimate the true signal variance of the function. Visually, the extrapolations look more confident and reasonable. Indeed, the human learners adapt their representations (e.g., learned kernels) to more data. However, we can see in Figure 2(f) that the human learners are still over-estimating the tails of the kernel, perhaps suggesting a strong prior bias for heavy-tailed correlations.\nThe learned RBF kernel, by contrast, cannot capture the heavy tailed nature of the training data (long range correlations), due to its Gaussian parametrization. Moreover, the learned RBF kernel underestimates the signal variance of the data, because it overestimates the noise variance (not shown), to explain away the heavy tailed properties of the data (its model misspecification).\nIn the second two rows, we consider a problem with highly complex structure, and only 10 participants. Here, the functions are drawn from a product of spectral mixture and linear kernels. As the participants see more functions, they appear to expect linear trends, and become more similar in their predictions. In Figures 2(o) and 2(p), we show the learned and true predictive correlation matrices using empirical estimators which indicate similar correlation structure."
    }, {
      "heading" : "4.3 Discovering Unconventional Kernels",
      "text" : "The experiments reported in this section follow the same general procedure described in section 4.2. In this case, 40 human participants were asked to extrapolate from two single training sets, in counterbalanced order: a sawtooth function (Figure 3(a)), and a step function (Figure 3(b)), with training data shown as dashed black lines.\nThese types of functions are notoriously difficult for standard Gaussian process kernels [12], due to sharp discontinuities and non-stationary behaviour. In Figures 3(a), 3(b), 3(c), we used agglomerative clustering to process the human responses into three categories, shown in purple, green, and blue. The empirical covariance matrix of the first cluster (Figure 3(d)) shows the dependencies of the sawtooth form that characterize this cluster. In Figures 3(e), 3(f), 3(g), we sample from the learned human kernels, following the same colour scheme. The samples appear to replicate the human behaviour, and the purple samples provide reasonable extrapolations. By contrast, posterior samples from a GP with a spectral mixture kernel trained on the black data in this case quickly revert to a prior mean, as shown in Fig 3(h). The data are sufficiently sparse, non-differentiable, and non-stationary, that the spectral mixture kernel is less inclined to produce a long range extrapolation than human learners, who attempt to generalise from a very small amount of information.\nFor the step function, we clustered the human extrapolations based on response time and total variation of the predicted function. Responses that took between 50 and 200 seconds and did not vary by more than 3 units, shown in Figure 3(i), appeared reasonable. The other responses are shown in Figure 3(j). The empirical covariance matrices of both sets of predictions in Figures 3(k) and 3(l) show the characteristics of the responses. While the first matrix exhibits a block structure indicating step-functions, the second matrix shows fast changes between positive and negative dependencies characteristic of the high frequency responses. Posterior sample extrapolations using the empirical human kernels are shown in Figures 3(m) and 3(n). In Figures 3(o) and 3(p) we show posterior samples from GPs with spectral mixture and RBF kernels, trained on the black data (e.g., given the same information as the human learners). The spectral mixture kernel is able to extract some structure (some horizontal and vertical movement), but is overconfident, and unconvincing compared to the human kernel extrapolations. The RBF kernel is unable to learn much structure in the data."
    }, {
      "heading" : "4.4 Human Occam’s Razor",
      "text" : "If you were asked to predict the next number in the sequence 9, 15, 21, . . . , you are likely more inclined to guess 27 than 149.5. However, we can produce either answer using different hypotheses\nthat are entirely consistent with the data. Occam’s razor describes our natural tendency to favour the simplest hypothesis that fits the data, and is of foundational importance in statistical model selection. For example, MacKay [31] argues that Occam’s razor is automatically embodied by the marginal likelihood in performing Bayesian inference: indeed, in our number sequence example, marginal likelihood computations show that 27 is millions of times more probable than 149.5, even if the prior odds are equal.\nOccam’s razor is vitally important in nonparametric models such as Gaussian processes, which have the flexibility to represent infinitely many consistent solutions to any given problem, but avoid overfitting through Bayesian inference. For example, the marginal likelihood of a Gaussian process (supplement) separates into automatically calibrated model fit and model complexity terms, sometimes referred to as automatic Occam’s razor [32].\nThe marginal likelihood p(y|M) is the probability that if we were to randomly sample parameters from M that we would create dataset y [e.g., 32, 10]. Simple models can only generate a small number of datasets, but because the marginal likelihood must normalise, it will generate these datasets with high probability. Complex models can generate a wide range of datasets, but each with typically low probability. For a given dataset, the marginal likelihood will favour a model of more appropriate complexity. This argument is illustrated in Fig 4(a). Fig 4(b) illustrates this principle with GPs.\nHere we examine Occam’s razor in human learning, and compare the Gaussian process marginal likelihood ranking of functions, all consistent with the data, to human preferences. We generated a\ndataset sampled from a GP with an RBF kernel, and presented users with a subsample of 5 points, as well as seven possible GP function fits, internally labelled as follows: (1) the predictive mean of a GP after maximum marginal likelihood hyperparameter estimation; (2) the generating function; (3-7) the predictive means of GPs with larger to smaller length-scales (simpler to more complex fits). We repeated this procedure four times, to create four datasets in total, and acquired 50 human rankings on each, for 200 total rankings. Each participant was shown the same unlabelled functions but with different random orderings. The datasets, along with participant instructions, are in the supplement, and available at http://functionlearning.com/.\n1 2 3 4 5 6 7 0\n20\n40\n60\n80\nFunction Label\nF irs\nt P la\nce V\not es\n(a)\n1 2 3 4 5 6 7 1\n2\n3\n4\n5\n6\n7\nFirst Choice Ranking\nA ve\nra ge\nH um\nan R\nan ki\nng\n(b)\n1 2 3 4 5 6 7\n2\n3\n4\n5\n6\n7\nGP Marginal Likelihood Ranking\nAv er\nag e\nH um\nan R\nan ki\nng\nTruthML\n-1.0\n-1.5\n-2.5\n+0.5 +1.0\n(c)\nFigure 5: Human Occam’s Razor. (a) Number of first place (highest ranking) votes for each function. (b) Average human ranking (with standard deviations) of functions compared to first place ranking defined by (a). (c) Average human ranking vs. average GP marginal likelihood ranking of functions. ‘ML’ = marginal likelihood optimum, ‘Truth’ = true extrapolation. Blue numbers are offsets to the log length-scale from the marginal likelihood optimum. Positive offsets correspond to simpler solutions.\nFigure 5(a) shows the number of times each function was voted as the best fit to the data. The proportion of first place votes for each function follows the internal (latent) ordering defined above. The maximum marginal likelihood solution receives the most (37%) first place votes. Functions 2, 3, and 4 received similar numbers (between 15% and 18%) of first place votes; these choices were all strongly favoured, and in total have more votes than the maximum marginal likelihood solution. The solutions which have a smaller length-scale (greater complexity) than the marginal likelihood best fit – represented by functions 5, 6, and 7 – received a relatively small number of first place votes. These findings suggest that on average humans prefer overly simple to overly complex explanations of the data. Moreover, participants generally agree with the GP marginal likelihood’s first choice preference, even over the true generating function. However, these data also suggest that participants have a wide array of prior biases, leading to different people often choosing very different looking functions as their first choice fit. Furthermore, 86% (43/50) of participants responded that their first ranked choice was “likely to have generated the data” and looks “very similar” to what they would have imagined.\nIt’s possible for highly probable solutions to be underrepresented in Figure 5(a): we might imagine, for example, that a particular solution is never ranked first, but always second. In Figure 5(b), we show the average rankings, with standard deviations (the standard errors are stdev/ √ 200), compared\nto the first choice rankings, for each function. There is a general correspondence between rankings, suggesting that although human distributions over functions have different modes, these distributions have a similar allocation of probability mass. The standard deviations suggest that there is relatively more agreement that the complex small length-scale functions (labels 5, 6, 7) are improbable, than about specific preferences for functions 1, 2, 3, and 4.\nFinally, in Figure 5(c), we compare the average human rankings with the average GP marginal likelihood rankings. There are clear trends: (1) humans agree with the GP marginal likelihood about the best fit, and that empirically decreasing the length-scale below the best fit value monotonically decreases a solution’s probability; (2) humans penalize simple solutions less than the marginal likelihood, with function 4 receiving a last (7th) place ranking from the marginal likelihood.\nDespite the observed human tendency to favour simplicity more than the GP marginal likelihood, Gaussian process marginal likelihood optimisation is surprisingly biased towards under-fitting in function space. If we generate data from a GP with a known length-scale, the mode of the marginal likelihood, on average, will over-estimate the true length-scale (Figures 1 and 2 in the supplement). If we are unconstrained in estimating the GP covariance matrix, we will converge to the maximum likelihood estimator, K̂ = (y− ȳ)(y− ȳ)>, which is degenerate and therefore biased. Parametrizing a covariance matrix by a length-scale (for example, by using an RBF kernel), restricts this matrix to a low-dimensional manifold on the full space of covariance matrices. A biased estimator will remain biased when constrained to a lower dimensional manifold, as long as the manifold allows movement in the direction of the bias. Increasing a length-scale moves a covariance matrix towards the degeneracy of the unconstrained maximum likelihood estimator. With more data, the low-dimensional manifold becomes more constrained, and less influenced by this under-fitting bias."
    }, {
      "heading" : "5 Discussion",
      "text" : "We have shown that (1) human learners have systematic expectations about smooth functions that deviate from the inductive biases inherent in the RBF kernels that have been used in past models of function learning; (2) it is possible to extract kernels that reproduce qualitative features of human inductive biases, including the variable sawtooth and step patterns; (3) that human learners favour smoother or simpler functions, even in comparison to GP models that tend to over-penalize complexity; and (4) that is it possible to build models that extrapolate in human-like ways which go beyond traditional stationary and polynomial kernels.\nWe have focused on human extrapolation from noise-free nonparametric relationships. This approach complements past work emphasizing simple parametric functions and the role of noise [e.g., 25], but kernel learning might also be applied in these other settings. In particular, iterated learning (IL) experiments [24] provide a way to draw samples that reflect human learners’ a priori expectations. Like most function learning experiments, past IL experiments have presented learners with sequential data. Our approach, following Little and Shiffrin [25], instead presents learners with plots of functions. This method is useful in reducing the effects of memory limitations and other sources of noise (e.g., in perception). It is possible that people show different inductive biases across these two presentation modes. Future work, using multiple presentation formats with the same underlying relationships, will help resolve these questions.\nFinally, the ideas discussed in this paper could be applied more generally, to discover interpretable properties of unknown models from their predictions. Here one encounters fascinating questions at the intersection of active learning, experimental design, and information theory."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank Tom Minka for helpful discussions."
    } ],
    "references" : [ {
      "title" : "A logical calculus of the ideas immanent in nervous activity",
      "author" : [ "W.S. McCulloch", "W. Pitts" ],
      "venue" : "Bulletin of mathematical biology,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1943
    }, {
      "title" : "Pattern Recognition and Machine Learning",
      "author" : [ "Christopher M. Bishop" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2006
    }, {
      "title" : "Bayesian brain: probabilistic approaches to neural coding",
      "author" : [ "K. Doya", "S. Ishii", "A. Pouget", "R.P.N. Rao" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2007
    }, {
      "title" : "Probabilistic machine learning and artificial",
      "author" : [ "Zoubin Ghahramani" ],
      "venue" : "intelligence. Nature,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2015
    }, {
      "title" : "An internal model for sensorimotor integration",
      "author" : [ "Daniel M Wolpert", "Zoubin Ghahramani", "Michael I Jordan" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1995
    }, {
      "title" : "Perception as Bayesian inference",
      "author" : [ "David C Knill", "Whitman Richards" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1996
    }, {
      "title" : "Bayesian spiking neurons i: inference",
      "author" : [ "Sophie Deneve" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2008
    }, {
      "title" : "Optimal predictions in everyday cognition",
      "author" : [ "Thomas L Griffiths", "Joshua B Tenenbaum" ],
      "venue" : "Psychological Science,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2006
    }, {
      "title" : "How to grow a mind: Statistics, structure, and abstraction",
      "author" : [ "J.B. Tenenbaum", "C. Kemp", "T.L. Griffiths", "N.D. Goodman" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2011
    }, {
      "title" : "Covariance kernels for fast automatic pattern discovery and extrapolation with Gaussian processes",
      "author" : [ "Andrew Gordon Wilson" ],
      "venue" : "PhD thesis, University of Cambridge,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2014
    }, {
      "title" : "Bayesian Learning for Neural Networks",
      "author" : [ "R.M. Neal" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1996
    }, {
      "title" : "Gaussian processes for Machine Learning",
      "author" : [ "C.E. Rasmussen", "C.K.I. Williams" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2006
    }, {
      "title" : "Exploiting generative models in discriminative classifiers",
      "author" : [ "Tommi Jaakkola", "David Haussler" ],
      "venue" : "Advances in neural information processing systems,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1998
    }, {
      "title" : "Gaussian process kernels for pattern discovery and extrapolation",
      "author" : [ "Andrew Gordon Wilson", "Ryan Prescott Adams" ],
      "venue" : "International Conference on Machine Learning (ICML),",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2013
    }, {
      "title" : "Functional learning: The learning of continuous functional mappings relating stimulus and response continua",
      "author" : [ "J Douglas Carroll" ],
      "venue" : "ETS Research Bulletin Series,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1963
    }, {
      "title" : "Function learning: Induction of continuous stimulusresponse relations",
      "author" : [ "Kyunghee Koh", "David E Meyer" ],
      "venue" : "Journal of Experimental Psychology: Learning, Memory, and Cognition,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1991
    }, {
      "title" : "Extrapolation: The sine qua non for abstraction in function learning",
      "author" : [ "Edward L DeLosh", "Jerome R Busemeyer", "Mark A McDaniel" ],
      "venue" : "Journal of Experimental Psychology: Learning, Memory, and Cognition,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1997
    }, {
      "title" : "Learning functional relations based on experience with input-output pairs by humans and artificial neural networks",
      "author" : [ "Jerome R Busemeyer", "Eunhee Byun", "Edward L Delosh", "Mark A McDaniel" ],
      "venue" : "Concepts and Categories,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1997
    }, {
      "title" : "Modeling human function learning with gaussian processes",
      "author" : [ "Thomas L Griffiths", "Chris Lucas", "Joseph Williams", "Michael L Kalish" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2009
    }, {
      "title" : "A rational model of function learning",
      "author" : [ "Christopher G Lucas", "Thomas L Griffiths", "Joseph J Williams", "Michael L Kalish" ],
      "venue" : "Psychonomic bulletin & review,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2015
    }, {
      "title" : "Infinite mixtures of Gaussian process experts. In Advances in neural information processing systems",
      "author" : [ "C.E. Rasmussen", "Z. Ghahramani" ],
      "venue" : "14: proceedings of the 2002 conference,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2002
    }, {
      "title" : "Superspace extrapolation reveals inductive biases in function learning",
      "author" : [ "Christopher G Lucas", "Douglas Sterling", "Charles Kemp" ],
      "venue" : "In Cognitive Science Society,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2012
    }, {
      "title" : "The conceptual basis of function learning and extrapolation: Comparison of rule-based and associative-based models",
      "author" : [ "Mark A Mcdaniel", "Jerome R Busemeyer" ],
      "venue" : "Psychonomic bulletin & review,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2005
    }, {
      "title" : "Iterated learning: Intergenerational knowledge transmission reveals inductive biases",
      "author" : [ "Michael L Kalish", "Thomas L Griffiths", "Stephan Lewandowsky" ],
      "venue" : "Psychonomic Bulletin & Review,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2007
    }, {
      "title" : "Simplicity bias in the estimation of causal functions",
      "author" : [ "Daniel R Little", "Richard M Shiffrin" ],
      "venue" : "In Proceedings of the 31st Annual Conference of the Cognitive Science Society,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2009
    }, {
      "title" : "Simplicity and goodness-of-fit in explanation: The case of intuitive curve-fitting",
      "author" : [ "Samuel GB Johnson", "Andy Jin", "Frank C Keil" ],
      "venue" : "In Proceedings of the 36th Annual Conference of the Cognitive Science Society,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2014
    }, {
      "title" : "Multistability and perceptual inference",
      "author" : [ "Samuel J Gershman", "Edward Vul", "Joshua B Tenenbaum" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2012
    }, {
      "title" : "Bridging levels of analysis for probabilistic models of cognition",
      "author" : [ "Thomas L Griffiths", "Edward Vul", "Adam N Sanborn" ],
      "venue" : "Current Directions in Psychological Science,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2012
    }, {
      "title" : "One and done? optimal decisions from very few samples",
      "author" : [ "Edward Vul", "Noah Goodman", "Thomas L Griffiths", "Joshua B Tenenbaum" ],
      "venue" : "Cognitive science,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2014
    }, {
      "title" : "Fast kernel learning for multidimensional pattern extrapolation",
      "author" : [ "Andrew Gordon Wilson", "Elad Gilboa", "Arye Nehorai", "John P. Cunningham" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2014
    }, {
      "title" : "Information theory, inference, and learning algorithms",
      "author" : [ "David JC MacKay" ],
      "venue" : null,
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2003
    }, {
      "title" : "Occam’s razor",
      "author" : [ "Carl Edward Rasmussen", "Zoubin Ghahramani" ],
      "venue" : "In Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2001
    }, {
      "title" : "Computation with infinite neural networks",
      "author" : [ "Christopher KI Williams" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 1998
    }, {
      "title" : "A process over all stationary kernels",
      "author" : [ "Andrew Gordon Wilson" ],
      "venue" : null,
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Therefore it is not surprising that early machine learning efforts, such as the perceptron, have been neurally inspired [1].",
      "startOffset" : 120,
      "endOffset" : 123
    }, {
      "referenceID" : 1,
      "context" : "In recent years, probabilistic modelling has become a cornerstone of machine learning approaches [2, 3, 4], with applications in neural processing [5, 6, 3, 7] and human learning [8, 9].",
      "startOffset" : 97,
      "endOffset" : 106
    }, {
      "referenceID" : 2,
      "context" : "In recent years, probabilistic modelling has become a cornerstone of machine learning approaches [2, 3, 4], with applications in neural processing [5, 6, 3, 7] and human learning [8, 9].",
      "startOffset" : 97,
      "endOffset" : 106
    }, {
      "referenceID" : 3,
      "context" : "In recent years, probabilistic modelling has become a cornerstone of machine learning approaches [2, 3, 4], with applications in neural processing [5, 6, 3, 7] and human learning [8, 9].",
      "startOffset" : 97,
      "endOffset" : 106
    }, {
      "referenceID" : 4,
      "context" : "In recent years, probabilistic modelling has become a cornerstone of machine learning approaches [2, 3, 4], with applications in neural processing [5, 6, 3, 7] and human learning [8, 9].",
      "startOffset" : 147,
      "endOffset" : 159
    }, {
      "referenceID" : 5,
      "context" : "In recent years, probabilistic modelling has become a cornerstone of machine learning approaches [2, 3, 4], with applications in neural processing [5, 6, 3, 7] and human learning [8, 9].",
      "startOffset" : 147,
      "endOffset" : 159
    }, {
      "referenceID" : 2,
      "context" : "In recent years, probabilistic modelling has become a cornerstone of machine learning approaches [2, 3, 4], with applications in neural processing [5, 6, 3, 7] and human learning [8, 9].",
      "startOffset" : 147,
      "endOffset" : 159
    }, {
      "referenceID" : 6,
      "context" : "In recent years, probabilistic modelling has become a cornerstone of machine learning approaches [2, 3, 4], with applications in neural processing [5, 6, 3, 7] and human learning [8, 9].",
      "startOffset" : 147,
      "endOffset" : 159
    }, {
      "referenceID" : 7,
      "context" : "In recent years, probabilistic modelling has become a cornerstone of machine learning approaches [2, 3, 4], with applications in neural processing [5, 6, 3, 7] and human learning [8, 9].",
      "startOffset" : 179,
      "endOffset" : 185
    }, {
      "referenceID" : 8,
      "context" : "In recent years, probabilistic modelling has become a cornerstone of machine learning approaches [2, 3, 4], with applications in neural processing [5, 6, 3, 7] and human learning [8, 9].",
      "startOffset" : 179,
      "endOffset" : 185
    }, {
      "referenceID" : 9,
      "context" : "For example, if we are performing character recognition, we would want our support to contain a large collection of potential characters, accounting even for rare writing styles, and our inductive biases to reasonably reflect the probability of encountering each character [10, 11].",
      "startOffset" : 273,
      "endOffset" : 281
    }, {
      "referenceID" : 10,
      "context" : "For example, if we are performing character recognition, we would want our support to contain a large collection of potential characters, accounting even for rare writing styles, and our inductive biases to reasonably reflect the probability of encountering each character [10, 11].",
      "startOffset" : 273,
      "endOffset" : 281
    }, {
      "referenceID" : 11,
      "context" : ", all polynomial models), splines, and infinite neural networks, can all exactly be represented as a Gaussian process with a particular kernel function [12, 11, 10].",
      "startOffset" : 152,
      "endOffset" : 164
    }, {
      "referenceID" : 10,
      "context" : ", all polynomial models), splines, and infinite neural networks, can all exactly be represented as a Gaussian process with a particular kernel function [12, 11, 10].",
      "startOffset" : 152,
      "endOffset" : 164
    }, {
      "referenceID" : 9,
      "context" : ", all polynomial models), splines, and infinite neural networks, can all exactly be represented as a Gaussian process with a particular kernel function [12, 11, 10].",
      "startOffset" : 152,
      "endOffset" : 164
    }, {
      "referenceID" : 12,
      "context" : "Moreover, the Fisher kernel provides a mechanism to reformulate probabilistic generative models as kernel methods [13].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 13,
      "context" : "Our framework leverages spectral mixture kernels [14] and non-parametric estimates.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 9,
      "context" : "Since state of the art machine learning methods perform conspicuously poorly on a number of extrapolation problems which would be easy for humans [10], such efforts have the potential to help automate machine learning and improve performance on a wide range of tasks – including settings which are difficult for humans to process (e.",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 11,
      "context" : "In the supplement, we provide background on Gaussian processes [12], which we recommend as a review.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 14,
      "context" : ", polynomial or power-law functions) [15, 16], or interpolation based on similarity learning [17, 18].",
      "startOffset" : 37,
      "endOffset" : 45
    }, {
      "referenceID" : 15,
      "context" : ", polynomial or power-law functions) [15, 16], or interpolation based on similarity learning [17, 18].",
      "startOffset" : 37,
      "endOffset" : 45
    }, {
      "referenceID" : 16,
      "context" : ", polynomial or power-law functions) [15, 16], or interpolation based on similarity learning [17, 18].",
      "startOffset" : 93,
      "endOffset" : 101
    }, {
      "referenceID" : 17,
      "context" : ", polynomial or power-law functions) [15, 16], or interpolation based on similarity learning [17, 18].",
      "startOffset" : 93,
      "endOffset" : 101
    }, {
      "referenceID" : 18,
      "context" : "[19] were the first to note that a Gaussian process framework can be used to unify these two perspectives.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[20] extended this model to accommodate a wider range of phenomena, using an infinite mixture of Gaussian process experts [21], and Lucas et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "[20] extended this model to accommodate a wider range of phenomena, using an infinite mixture of Gaussian process experts [21], and Lucas et al.",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 21,
      "context" : "[22] used this model to shed new light on human predictions given sparse data.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "The evaluation corpora using recent reviews [23, 19] are limited to a small set of parametric forms, i.",
      "startOffset" : 44,
      "endOffset" : 52
    }, {
      "referenceID" : 18,
      "context" : "The evaluation corpora using recent reviews [23, 19] are limited to a small set of parametric forms, i.",
      "startOffset" : 44,
      "endOffset" : 52
    }, {
      "referenceID" : 23,
      "context" : "Other projects have collected richer and more detailed data sets [24, 25], but we are only aware of",
      "startOffset" : 65,
      "endOffset" : 73
    }, {
      "referenceID" : 24,
      "context" : "Other projects have collected richer and more detailed data sets [24, 25], but we are only aware of",
      "startOffset" : 65,
      "endOffset" : 73
    }, {
      "referenceID" : 23,
      "context" : "Thus it is unsurprising that participants tend to revert to the prior mode that arises in almost all function learning experiments: linear functions, especially with slope-1 and intercept-0 [24, 25] (but see [26]).",
      "startOffset" : 190,
      "endOffset" : 198
    }, {
      "referenceID" : 24,
      "context" : "Thus it is unsurprising that participants tend to revert to the prior mode that arises in almost all function learning experiments: linear functions, especially with slope-1 and intercept-0 [24, 25] (but see [26]).",
      "startOffset" : 190,
      "endOffset" : 198
    }, {
      "referenceID" : 25,
      "context" : "Thus it is unsurprising that participants tend to revert to the prior mode that arises in almost all function learning experiments: linear functions, especially with slope-1 and intercept-0 [24, 25] (but see [26]).",
      "startOffset" : 208,
      "endOffset" : 212
    }, {
      "referenceID" : 26,
      "context" : "We assume the predictions y∗ are samples from the learner’s posterior distribution over possible functions, following results showing that human inferences and judgments resemble posterior samples across a wide range of perceptual and decision-making tasks [27, 28, 29].",
      "startOffset" : 257,
      "endOffset" : 269
    }, {
      "referenceID" : 27,
      "context" : "We assume the predictions y∗ are samples from the learner’s posterior distribution over possible functions, following results showing that human inferences and judgments resemble posterior samples across a wide range of perceptual and decision-making tasks [27, 28, 29].",
      "startOffset" : 257,
      "endOffset" : 269
    }, {
      "referenceID" : 28,
      "context" : "We assume the predictions y∗ are samples from the learner’s posterior distribution over possible functions, following results showing that human inferences and judgments resemble posterior samples across a wide range of perceptual and decision-making tasks [27, 28, 29].",
      "startOffset" : 257,
      "endOffset" : 269
    }, {
      "referenceID" : 13,
      "context" : "For this approach, we choose the recent spectral mixture kernels of Wilson and Adams [14], which can model a wide range of stationary covariances, and are intended to help automate kernel selection.",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 13,
      "context" : ",y (W ) ∗ , each containing 20 datapoints, from a GP with a spectral mixture kernel [14] with two components (the prediction kernel).",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 13,
      "context" : "We asked for extrapolation judgments because they provide more information about inductive biases than interpolation, and pose difficulties for conventional Gaussian process kernels [14, 10, 30].",
      "startOffset" : 182,
      "endOffset" : 194
    }, {
      "referenceID" : 9,
      "context" : "We asked for extrapolation judgments because they provide more information about inductive biases than interpolation, and pose difficulties for conventional Gaussian process kernels [14, 10, 30].",
      "startOffset" : 182,
      "endOffset" : 194
    }, {
      "referenceID" : 29,
      "context" : "We asked for extrapolation judgments because they provide more information about inductive biases than interpolation, and pose difficulties for conventional Gaussian process kernels [14, 10, 30].",
      "startOffset" : 182,
      "endOffset" : 194
    }, {
      "referenceID" : 11,
      "context" : "with a rational quadratic (RQ) kernel [12] (for heavy tailed correlations), and there are 20 human participants.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 11,
      "context" : "These types of functions are notoriously difficult for standard Gaussian process kernels [12], due to sharp discontinuities and non-stationary behaviour.",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 30,
      "context" : "For example, MacKay [31] argues that Occam’s razor is automatically embodied by the marginal likelihood in performing Bayesian inference: indeed, in our number sequence example, marginal likelihood computations show that 27 is millions of times more probable than 149.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 31,
      "context" : "For example, the marginal likelihood of a Gaussian process (supplement) separates into automatically calibrated model fit and model complexity terms, sometimes referred to as automatic Occam’s razor [32].",
      "startOffset" : 199,
      "endOffset" : 203
    }, {
      "referenceID" : 23,
      "context" : "In particular, iterated learning (IL) experiments [24] provide a way to draw samples that reflect human learners’ a priori expectations.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 24,
      "context" : "Our approach, following Little and Shiffrin [25], instead presents learners with plots of functions.",
      "startOffset" : 44,
      "endOffset" : 48
    } ],
    "year" : 2015,
    "abstractText" : "Bayesian nonparametric models, such as Gaussian processes, provide a compelling framework for automatic statistical modelling: these models have a high degree of flexibility, and automatically calibrated complexity. However, automating human expertise remains elusive; for example, Gaussian processes with standard kernels struggle on function extrapolation problems that are trivial for human learners. In this paper, we create function extrapolation problems and acquire human responses, and then design a kernel learning framework to reverse engineer the inductive biases of human learners across a set of behavioral experiments. We use the learned kernels to gain psychological insights and to extrapolate in humanlike ways that go beyond traditional stationary and polynomial kernels. Finally, we investigate Occam’s razor in human and Gaussian process based function learning.",
    "creator" : "LaTeX with hyperref package"
  }
}