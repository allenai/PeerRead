{
  "name" : "1705.07250.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Speedup from a different parametrization within the Neural Network algorithm",
    "authors" : [ "Michael F. Zimmer" ],
    "emails" : [ "mzimmer@neomath.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Artificial Neural Networks (or Neural Nets (NN)) are a powerful and versatile machine learning algorithm. They act as a universal function approximator, and can be used in classification and regression problems[3, 1, 2]. However, despite their successes, a problem that persists is long training times of the NN. This paper addresses that problem by proposing a different parametrization of the hyperplanes used in the algorithm.\nThis paper demonstrates the new approach on five autoencoder examples. It allows the training to proceed much more rapidly, using in some cases only 1/8-th the number of epochs of the existing approach to reach a given training error value. Results are given for individual runs, averaged runs, and the \"epoch speedup\". Also, this different parametrization is more intuitive, making it easier to understand how the parameters work, and thus provides insight into how one might best initialize a NN."
    }, {
      "heading" : "2 NN definitions",
      "text" : "Here the architecture and variables are defined for the 3-layer NN that will be used in the examples. It is a feed-forward NN with d1, d2, d3 nodes on each of the three layers; they are labeled as n1(i), n2(j), n3(k), with indices i = 1, .., d1, j = 1, ..., d2, and k = 1, ..., d3. In Fig. 1 is shown an example where d1 = 4, d2 = 2, and d3 = 4. For example, the transformations on n1 which lead to n2 are defined as:\nn2(j) = f(h1(j)) h1(j) = w1(j, 0) + ∑ 1,d2 w1(j, i)n1(i)\nf(h) = 2/(1 + e−h)− 1\nNote that this choice of f enforces a (−1, 1) encoding, as opposed to the (0, 1) encoding; this will be discussed later. The training error (E) is a mean-squared error; a classification error could also be used.\nE = ∑ k [n3(k)− t(k)]2\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nar X\niv :1\n70 5.\n07 25\n0v 3\n[ cs\n.L G\n] 2\nJ un\n2 01\nBack-propagation is used, which is based on differential updates made to w = (w0, w1, ..., wd) using:\n∆w = −η ∂E ∂w\nwhere η is the learning rate."
    }, {
      "heading" : "3 Different Parameterization",
      "text" : "It may be the reader’s experience, as it has been the author’s, that there are advantages to having a simple parametrization of the system being optimized. In particular, it’s beneficial to have a 1-to-1 mapping between parameter values with the actual configuration of your system (i.e., the hyperplane configurations); that is not the case here. E.g., all w values could be scaled up and down, and they would all describe the same hyperplane w · x = 0; that scale change is important for the function f , however. Another point is the lack of intuition that the usual parametrization provides: it’s simply not clear by looking at the parameter values, without any additional calculation, whether the w-values are appropriate for a given data set. Toward finding a different parametrization that doesn’t suffer from these shortcomings is the motivation for this paper.\nThe key point about the different parametrization is that it is based on features of the hyperplane– i.e., the minimum distance between the origin and the hyperplane (c), and the orientation of the hyperplane (u). Using those variables a hyperplane (L) can be written as L = c + u · x, where x is a point in space (i.e., the data). While clearly it’s almost the same as what has been traditionally used, it has the advantage that it’s a unique representation of the hyperplane. It can easily be checked that for each (c, u) combination, there is exactly one hyperplane, and vice versa. Also, it is easy to partition all possible hyperplanes, by first fixing c or u.\nThe two parametrizations are trivially connected (using x0 = 1); the difference is having the overall scale pulled out:\nh = w0 + w1x1 + ...+ wdxd = s(c+ u · x)\nwhere w′ = (w1, ..., wd), s = ||w′||, c = w0/||w′||, ui = wi/||w′||, i = 1, ..., d, and || · || denotes the L2 norm. For simplicity, the new parameters are combined into one as z = (s, c, u). The new parametrization will be referred to as z-param, and the traditional one as w-param. Optimizations for z-param will be done against the scalars s,c and the vector u."
    }, {
      "heading" : "4 Examples of New Approach",
      "text" : "A comparison is now made between the two parametrizations on a clean but sufficiently challenging test case: an autoencoder. Five 3-layer autoencoder examples, ranging in size from 8-3-8 to 128-7-128,"
    }, {
      "heading" : "8 0.180 0.45",
      "text" : ""
    }, {
      "heading" : "16 0.110 0.22",
      "text" : ""
    }, {
      "heading" : "32 0.040 0.12",
      "text" : ""
    }, {
      "heading" : "64 0.016 0.07",
      "text" : ""
    }, {
      "heading" : "128 0.010 0.06",
      "text" : "Code for running these examples is available on a public repository[6]. Pseudocode will not be shown, since it is the same as the existing NN algorithm, and differs only with respect to the parameter updating. Here ∆w = −η∂E/∂w is replaced by its z-equivalent, and since z = (s, c, u) there are three separate derivatives for each layer. This leads to the 6 derivatives shown below, which will be used for updating those parameters via ∆z = −η∂E/∂z,\n∂E/∂s2(p) = c2(p) + ∑ j u2(p, j)n2(j) A(p) ∂E/∂c2(p) = s2(p)A(p)\n∂E/∂u2(p, q) = s2(p)n2(q)A(p)\n∂E/∂s1(p) = [ c1(p) +\n∑ i u1(p, i)n1(i)\n] B(p)\n∂E/∂c1(p) = s1(p)B(p)\n∂E/∂u1(p, q) = s1(p)n1(q)B(p)\nwhere A(p) = f ′(h2(p))[n3(p)− t(p)]\nB(p) = f ′(h1(p)) ∑ m s2(m)u2(m, p)A(m)\nhv(p) = sv(p) [ cv(p) +\n∑ i uv(p, q)nv(q) ] and v = 1, 2. Noticeable is that there is nothing in the model that enforces ||u|| = 1. It is skipped in this presentation and will be discussed at a future time. The initial values for z = (s, c, u) are: s = 1, c = 0, u = random unit vector. Note that the same initialization is used for both w-param and z-param; the difference in the results follows from the different updating schemes applied to each. The choice of the initialization value s = 1 was made to simplify the comparison between these two algorithms. In fact, using a very small non-zero s leads to better results, and will be explored briefly later in this section.\nBefore comparing the two approaches over those different examples, an (approximate) optimal η will be determined for each. This was done using a grid search over η, averaging 20 runs for each point (10 runs for d1 = 128). The results for the best η for z-param (i.e., ηz) and for w-param (i.e., ηw) are in Table 1.\nUsing the ηw and ηz , the w-param and z-param algorithms were run over 1500 epochs, starting from the same initial conditions (as described earlier). The results are overlaid on each other: one plot\nfor d1 = 16 (Fig. 2) and one for d1 = 64 (Fig. 3). Note that in both graphs the better-performing,\nlower grouping corresponds to z-param, the new parametrization. These results were typical of the entire set: the z-param always performed better, reaching a lower training error more quickly. Most noticeable is that as d1 increases, the variance of the final E increases, as well as the final E value itself.\nTo facilitate seeing the general trend as d1 increases, those runs are averaged for each d1 to produce Fig. 4 through Fig. 7. This is displayed in the figures for d1 = 16, 32, 64, 128. These graphs make it more obvious that z-param always outperforms w-param. Indeed, the final E of the z-param curves\nare typically about 1/10-th that of the w-param. It bears repeating that these two cases began from the same initial conditions on the hyperplanes and used the same mean squared error; the only difference is that the hyperplanes were parametrized and updated differently.\nTo better quantify the advantage z-param has over w-param, an \"epoch speedup\" is visually defined in Fig. 8 with respect to the averaged runs (cf. Fig. 4); it’s simply the ratio of epochs (t2/t1) needed to reach a given E[7]. Note this can only be computed at a training error (E) where both curves have a value. E.g., for very low training errors, only z-param has values, and the speedup cannot be computed. Thus the window of training error ranges from the min of the max E values to the max of the min E values, for each pair of w and z training error values. That range is then scaled from 0 to 100, and is referred to as the \"Percent toward 0\".\nShown in Fig. 9 is a typical graph of the epoch speedup, in this case for d1 = 16. Of note is how the speedup increases as the original curves reached lower E values. There is no reason to believe this trend would not continue for higher epoch number. Also, in a sense these graphs don’t tell the whole story– the w-param appears to not be able to reach (in a reasonable time) the low E values of the z-param.\nThe maximum speedup recorded for all the cases is shown in Table 2\nSmall s\nThe above comparisons were all made using the initial condition of s = 1, which was done in large part to simplify the comparison. Having done that, one may now explore the effects of small s. Shown in Fig. 10 are the w-param and z-param for d1 = 128 for s = 0.1 Again z-param does better, but what is notable is how both outperformed their corresponding s = 1.0 versions. Qualitatively, one can see the different runs coming together, and having a smaller variance for larger epoch values. There is also the emergence of a plateau early on, more easily seen in the w-param case. To explore this still further, the same graph was produced in Fig. 11, except using s = 0.0001 The quality of the z-param (lower curves) is evident: it’s shifted left, reaches lower values, and has reduced variance. The w-param graphs have an increased plateau and less variance, but don’t reach noticeably lower E values."
    }, {
      "heading" : "5 Parameter Initialization",
      "text" : "This paper, while begun as an investigation into a different hyperplane parametrization, has also shed light on optimal parameter initializations. Beginning from the parameters for the z-param case, there are three types of parameters: the scale of h (i.e., s), the distance of hyperplane to origin (c), and the orientation of hyperplane (u).\nThe scale (s) was best chosen to be a small number. In Fig. 11 it was seen to lead to a plateau region initially (especially for the w-param case). Presumably this allowed more time for c and u parameters to find more optimal values, thereby negating the influence of an initial random u. This would happen because, since f(h) = f(s(c + u.x)), a small s would hide modest changes in c and u, thereby initially allowing those parameters to more easily evolve toward optimal values. There is evidence for this in that the curves have coalesced for s = 0.1 in Fig. 10, and have strongly coalesced as s was made smaller to s = 0.0001 in Fig. 11. Thus, when using z-param, it seems best to set 0 < s 1, at least when there is a significant variance in individual runs as was seen earlier. Likewise, for the w-param case, the scale is set by ||w′||, where again w′ = (w1, ..., wd), so the corresponding scaled\nvalues are: wi → (s/||w′||)wi\nwhere s is the same as for z-param.\nThe distance of a hyperplane to origin (c) was set to 0, under the assumption that the data was shifted about its mean. This would ensure that all the data points do not fall on one side of the plane, making the hyperplane much less useful[2]. The effect of having a hyperplane completely on one side of a data cloud is that h will become very large in magnitude, which leads to f(h) being pinned to its min/max values, and also f ′(h) being near zero. This is known as the zero-gradient problem. Here it is called being \"caught in the doldrums\", a sailing term used to describe a situation near the equator, where there is no wind, and hence no prospect of making progress. Being caught in the doldrums is equally disadvantageous for training a NN. To avoid this scenario, and ensure the hyperplanes (nearly) always intersect the data cloud, the input layer is shifted about its mean. For later layers, it’s preferable to not have to keep doing that, so to mitigate that the (−1, 1) encoding is used instead of the (0, 1) encoding. This has the advantage of having positive and negative values, and hence being somewhat self-averaging (toward 0).\nThe orientation of the hyperplane (u) should obviously be in a random direction. A correct prescription for constructing such a vector is to draw from a normal distribution for each component, and then normalize the result[5]. If one instead draws from a uniform distribution (as is commonly done), and normalizes that, the resulting u will be biased toward the corners of the enclosing hypercube. The same approach can be used with w′ (in w-param) as for u (in z-param)."
    }, {
      "heading" : "6 Final Remarks",
      "text" : "In this paper, the ramifications of a different parametrization (z-param) of the hyperplanes were investigated in the NN algorithm. The motivation was that a more intuitive and straightforward representation could help the hyperplanes more easily explore their parameter space.\nThe results show that by switching to z-param, learning can be achieved much more quickly. It’s evident from the \"epoch speedup\" graphs (cf. Fig. 9) that as training continues, there is increasingly more to be gained from using the z-param. Keep in mind though that the epoch speedup numbers are not perfectly representative of the wall clock time for implementing this algorithm. The FLOPS are slightly higher for z-param, and the parallelizability may be different (although it appears similar).\nIn addition to the above, the z-param approach presented a simpler situation with respect to initializing the parameters. In the process it was noted, at least for the examples studied, that high variance training error runs might be a motivation for instead initializing with a very small s.\nInvestigations into this new parametrization have just begun and there are clearly many avenues of research left to be pursued. Also, since this is only a change in the parametrization of the hyperplanes, it means it can be substituted into existing NN applications that use the existing w-param. Hence, this new parametrization can be applied on a wide scale."
    } ],
    "references" : [ {
      "title" : "Neural Networks for Pattern",
      "author" : [ "C.M. Bishop" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1995
    }, {
      "title" : "Neural Networks, a systematic introduction",
      "author" : [ "R. Rojas" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1996
    }, {
      "title" : "An Efficient Method for Generating Uniformly Distributed Points on the Surface of an n-Dimensional Sphere",
      "author" : [ "J.S. Hicks", "R.F. Wheeling" ],
      "venue" : "Comm. Assoc. Comput. Mach. 2,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1959
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "They act as a universal function approximator, and can be used in classification and regression problems[3, 1, 2].",
      "startOffset" : 104,
      "endOffset" : 113
    }, {
      "referenceID" : 1,
      "context" : "They act as a universal function approximator, and can be used in classification and regression problems[3, 1, 2].",
      "startOffset" : 104,
      "endOffset" : 113
    }, {
      "referenceID" : 1,
      "context" : "This would ensure that all the data points do not fall on one side of the plane, making the hyperplane much less useful[2].",
      "startOffset" : 119,
      "endOffset" : 122
    }, {
      "referenceID" : 2,
      "context" : "A correct prescription for constructing such a vector is to draw from a normal distribution for each component, and then normalize the result[5].",
      "startOffset" : 141,
      "endOffset" : 144
    } ],
    "year" : 2017,
    "abstractText" : "A different parametrization of the hyperplanes is used in the neural network algorithm. As demonstrated on several autoencoder examples it significantly outperforms the usual parametrization, reaching lower training error values with only a fraction of the number of epochs. It’s argued that it makes it easier to understand and initialize the parameters.",
    "creator" : "LaTeX with hyperref package"
  }
}