{
  "name" : "1708.09175.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Calibrating chemical multisensory devices for real world applications: An in-depth comparison of quantitative Machine Learning approaches",
    "authors" : [ "S. De Vito", "E. Esposito", "M. Salvato", "O. Popoola", "G. Di Francia" ],
    "emails" : [ "saverio.devito@enea.it" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Several research works have now highlighted the viability of low cost sensors based air quality monitoring systems (AQMS) for pervasive monitoring tasks [1]. Most of them are based on passive sampling, in which the sensors are freely exposed to the air to be analyzed. In these systems, real time sensing is hence continuously performed, without resorting to typical artificial olfaction measurement procedures. Eventually, the availability of fixed and mobile analyzers will probably lead to hybrid networks in which indicative low cost measurement systems will supplement the use of conventional analyzers. As the AQMS are based on low-cost technologies, it will then possible to solve the sparsity problem that negatively affect the current monitoring strategies [2] [3] [4]. Some of these technologies could become available for citizen’s in terms of wearable systems allowing them to obtain information on their personal pollutants exposure. Citizens involvement, as well as their contribution to the actual city wide measurement process, is foreseen on the basis of factual data by the results of several pilot projects [5, 6, 7]. Furthermore, this\nar X\niv :1\n70 8.\n09 17\n5v 1\n[ cs\n.A I]\n3 0\nA ug\ndevelopment will encourage good working relationship between citizens and public authorities, especially in those countries where this relationship is actually severely hampered [8].\nThe European Commission has defined in [9] data quality objectives (DQO) for novel devices devised to be integrated in the operative air quality network. In that legal framework, these novel technologies are termed indicative measurement systems (IMS). In particular, their performance results needs to meet specific requirements including bias and uncertainty metrics. If met, the captured data could eventally be used in official air quality assessments and reports. In order to achieve quantitative pollution estimation capability, chemical sensors data needs to undergo a processing step, typically involving a regression algorithm, sometimes refered to as calibration function. Some sensors non-selective behaviour, relatively slow dynamic, instability and sensitivity to environmental conditions, severely hinder the use of raw sensor data as estimations for real world measurements. To this regard, the Aveiro intercomparison exercise have set a a valuable point of reference evaluating the actual field performance of several multisensors, based on different classes of sensors, targeted to multiple pollutants and comparing their performances with EU DQOs [10]. This achievement required the involvment of several research groups across Europe and multiple large, medium and small enterprises. Specifically, chemical microsensors devices are, in general, subjected to interferent gases that modify their response to the target gas [11]. For this reason, any attempt to rely on a univariate calibration procedure, neglecting interferents influence, is prone to failure [12]. Any available information on interferent gases concentration should be exploited in order to solve this issue, hence advocating the use of multivariate calibration algorithms. Combined with sensors non linearity, cross-sensitivity have suggested the use of machine learning algorithms to solve the calibration problem. Besides, chemical microsensors response generally changes in time due to several effects including poisoning and environmental variables sensitivity [13]. As a consequence, long term stability is a significant concern given the need to limit recalibration and maintenance burden on a pervasive network of possibly hundreds of AQMS [14]. Adaptive/semi-supervised calibration schemes could represent one of the possible solutions to measurement drift issue [15]. Furthermore, owing to method of fabrications, some sensors have been shown to have poor sensor-to-sensor reproducibility. This hampers the use of a single calibration function, thus requiring an ad hoc calibration procedure for each chemical multisensor device or, alternatively, the development of calibration transfer strategies [16], [17]. As such, calibration transfer is an active and relevant field for researchers in artificial olfaction. A significant research effort is also needed to further develop our chemical sensor data fusion capabilities, in particular, to precisely reconstruct a 3D picture of pollution in both indoor or outdoor [18],[19],[20].\nThe possibility to quantitatively assess our own exposure to pollutants when moving in the city, is currently a strong and desiderable objective with the added advantage of possible impact on citizen’s mobility, active life and health consciousness. While these are desirable, it may require the execution of such intelligent data processing algorithms on board of the citizen’s mobile or pervasively distributed fixed nodes. This will make the resulting monitoring network a true smart cyber chemical system. In fact, scalability issues, that particularly affect networking performances in a more and more crowded IoT scenario, will require the local execution of data preprocessing without resorting to the cloud. On these basis, the emerging ”fog” and ”edge” computing frameworks are also pushing low semantic extraction computations towards the very edge of a sensing or control network [21]. Besides, such a monitoring network will be based on heterogeneous sensing systems and the possibility of obtaining local precise and accurate estimations of pollutants concentrations will be a mandatory requirement to achieve the needed plug and sense capability. Several algorithms have been proposed during the last decade to implement the needed quantitative calibration of the chemical multisensor device but currently no effective comparison have been performed [14, 22, 11]. The main reason is the lack of publicly available datasets to assess performance of the different approaches. As such, many questions are still open, such like: which approach best perform under certain conditions, which one is\nthe most efficient or in other words which one need less samples to produce satisfactory results. Other questions include: which one is the most computationally efficient and which one has the most efficient knowledge representation. These, are perceived by many as the most interesting in order to advance towards the development of smart cyber chemical systems. Very recently, the availability of low-cost technologies and open source hardware has facilitated the development of chemical multi-sensors prototypes. Field tests are generating a consistent flow of interesting data. Simultaneously, a new generation of researchers are actively sharing the data they gather both in laboratories and real world deployment making it possible to build comprehensive comparisons. In this work, we perform a comprehensive benchmarking of the most promising techniques that have shown reliability in the depicted scenarios at least in laboratory experiments, focusing on machine learning approaches. The use of multiple datasets (two of which are publicly available) will provide the needed robust test of our analysis. The presented techniques will be assessed for their ability to be implemented on-board the devices as well as how well they could meet EU requirements for indicative methods in terms of accuracy and precision level. In section two, we will review the multivariate regression system,s recently proposed in literature for chemical sensor data analysis, selecting the most promising ones according to the results obtained by researchers in the field. Section three will define the benchmarking scenarios chosen to estimate algorithms performances in this challenging framework. This include performance drivers, dataset description and performance indicators selection. Our results and conclusions are covered in the last two sections."
    }, {
      "heading" : "2 Related works",
      "text" : "Historically, quantitative calibration of chemical multi-sensor devices relied on univariate calibration and simple environmental factors correction strategies. Sensor responses captured during laboratory calibrations were used to obtain a linear relationship among the sensor response and its target gas concentration. The limited performance obtained by this strategy has led to the development of multivariate calibration strategies relying on the use of complex synthetic gas mixtures [23, 24] or on field recorded data [25] to cope with specificity and stability problems. In the first case, researchers usually relied on the use of steady state responses for the calibration computation. This usually requires a significant amount of time in order to explore a wide experimental space generated by the possible combinations of different concentrations of several gases. Furthermore, most of the recent systems operate in open sampling scenarios in which sensors rarely reach a steady state [26]. Dealing with the use of mobile chemical sensing systems, Hishida et al. [27], for example, explicitly proposed the use of steady state responses as approximates of the real sensors responses in field deployments. Conversely, on field recorded data made it possible to obtain convenient and efficient calibration datasets. In fact, these datasets cover the actual manifold (trajectory) of gas concentrations values that a multi-sensor will likely encounter when deployed in the field. Simultaneously, machine learning experts have proposed data intensive approaches that simplified the need to develop apriori and complex non-linear models based on the actual physics of the devices response towards their target gas and interfering species. A recent example of a practical, physically rooted, model has been provided by Masson et al. The model can be calibrated with laboratory based sensor response recordings and tuned with on field recorded data to compensate for temperature interference [28]. However, the approach should be improved to address multiple interferents. Furthermore, to the best of our knowledge no such class of solution has yet been proposed to simultaneously model the sensor dynamic behaviour. Machine learning approaches, instead, rely on a quasi-black box approach in which the knowledge on the sensor model can be used for response descriptive features development. The actual model selection is performed automatically, by tuning a generic nonlinear model, allowing it to learn using example in a typical supervised fashion. In the last few years, multiple methodologies have been proposed relying on the most common and efficient machine learning\nregression strategies like shallow neural networks, support vector machines, gaussian processes and more recently, reservoir computing. The last has the advantage of implicitly learning a dynamic model of the multisensor device, thereby reducing the effects on the performance of the slow sensors dynamic [29]. These effects can be paramount when dealing with pervasive and mobile deployments where systems are likely to encounter rapid concentrations transients. Advanced learning techniques like semi-supervised learning proved to be promising in reducing the number of calibration samples needed as well as improving robustness to drift by complementing learning from un-labeled samples with the classic supervised approach [15]. Shallow artificial neural networks (ANN) have been proposed by multiple researchers. De Vito et al., for example, have shown promising results using hourly averaged sensors data from field measurements; this work showed the possibility to let them learn a nonlinear multiple regression model for benzene concentration estimation [30]. Further works from the same group have evaluated neural networks generalization properties in terms of performance against number of training samples, suggesting the possibility of using feature selection to improve performance, resulting in better insights on the actual cross sensitivities and correlation behaviour. The effects of the changes of pollutants joint concentrations distribution, which can be due on seasonal effects, on the performances were also highlighted [12]. Spinelle et al., have confirmed that shallow neural networks, can significantly outperform linear univariate and multivariate models, thus highlighting the possibility to exploit multivariate information to reach EU set DQOs [31]. Very recently, Al Barekh et al. [32] proposed a fuzzy logic based strategy for differentiating among different type of air pollution and estimating a pollution index using a neural network. The resulting algorithm was used for calibrating a small fleet of three open sampling chemical multi-sensor devices. Support Vectors Machines (SVMs) have been proposed for gas mixture detection, identification and quantification problems using multi-sensor systems by several authors. Unfortunately, only a few works are related with open sampling systems. Kai Song et al. [33] used a least square SVM for calibrating a wireless chemical multi-sensors system which targeted explosive gases (methane, hydrogen) concentration estimations. Their on-board implemented (Least Squares)SVM solution outperformed an ANN solution on a small scale validation experiment with near low explosion limit concentrations. In order to cope with sensors dynamic behaviour, Vembu et al. [34] proposed the use of time series kernel based SVMs for enhancing the identification capabilities of a small network of pervasive open sampling multi-sensors systems. In this work, authors reported an enhanced accuracy performance with respect to basic Radial Basis Function (RBF) kernels SVM working on time series features. In an effort to improve the response of an open sampling system while exposed to rapid concentration changes, Fonollosa et al. [29], [35], investigated the use of reservoir computing strategies. In this approach, a recurring neural network using a large reservoir of randomly connected nonlinear computational units and a layered linear regressor system was used to train the model. The ultimate goal was to precisely estimate the actual concentration of the stimulus gases in a pseudo-controlled setting and the authors reported superior performances of their Reservoir Computing (RC) system with respect to static SVM and linear approaches. Interestingly enough, the random wiring of the recurrent units of this architecture allowed to save the time needed for selecting the optimal sensors time serie window of observation that is paramount for the dynamic modelling problem. Esposito et al. [36], investigated the performance of the tapped delay approach with real world data, recorded with open sampling systems. They highlight that dynamic approaches may be well suited for tackling rapid transient exposure encountered in the field by mobile systems or fixed roadside stations. Robotics olfaction is a closely related field that will definitely benefit from improved real time gas quantification capability. Monroy et al. described an interesting probabilistic quantification approach based on the well-known Gaussian processes (GP) framework [37]. In their first work [38], instead of attempting to describe sensors dynamics, they propose the prediction of the uncertainty caused by slow sensors operation in an open sampling scenario. They validate the approach within a simulated field environment, in which ethanol is emitted and transported\nby an air flow towards MOX (Metal Oxide) sensors located in a room. A photo-ionization device (PID) VOC sensor was used as a reference. They reported similar performance with respect to state of the art static SVRs. In follow up work, they developed the use of GPs towards a dynamic approach, coupling a tapped delay line and computing sensor derivatives. Their results showed a slight improvement of accuracy performance indicators in the case of derivatives features."
    }, {
      "heading" : "3 Benchmarking scenarios",
      "text" : "In the previous sections, we briefly reviewed relevant works that have addressed the chemical multi-sensor arrays calibration problem with machine learning approaches. As mentioned above, no comprehensive comparison of the proposed architectures is currently available, partially due to the lack of public datasets to work with. In this section, we introduce the comparison methodology that we have pursued. It is based on the availability of three different datasets. A selection of the most promising machine learning techniques have been trained alongside with their dynamic extension to estimate the concentration of different pollutants by using instantaneous sensor responses and, for dynamic architectures, their recent trajectories. Specifically, a small window of observation including recent past sensor responses is, in these cases, used to train the model thereby taking into account the sensor dynamic behaviour (see Fig. 1 ).\nA small set of literature based performance indicators has been chosen to represent the performance of the architectures under comparison. Since the ultimate goal of this systems is to be operated on-board or in high efficiency big data processing facilities, we briefly review the complexity of these methods with special focus on their operative phase, i.e. the computational and storage burden of the concentration estimation procedure."
    }, {
      "heading" : "3.1 Machine Learning Techniques",
      "text" : "The Machine Learning (ML) techniques that we have taken into account for comparison purposes are among the most popular including Neural Networks (NN), Support Vector Regressors (SVR), Gaussian Processes Regressors (GPR), Multiple Linear Regressors (MLR) and Reservoir Computing (RC) along with their dynamic versions, obtained using different Tapped Delay Lengths (TDL) at the input. The tapped delay allows for a time domain expansion of the inputs generating an observation window for the machine learning technique that allows for learning and exploiting a knowledge on dynamic relationships between input (sensor responses) and outputs (pollutant actual concentrations). The considered techniques have been analysed for their capability to provide a solution to the need of on-board implementation as well as how well they meet EU requirements for indicative methods in terms of accuracy and precision level. Neural networks have been used in this scenario since the last decade [39, 40]. Their basic architecture is characterized by multiple layers of information-processing nonlinear functional units interacting by means of one way weighted connections. Mathematically, their prediction equation may be written as in the following:\nuk = n∑ j=1 wkj · xj\nyk = ϕ(uk + bk),\n(1)\nwhere xj , j = 1, . . . , n are the input vectors, uk is the weighted sum of inputs, bk is the bias term, ϕ(X) is the activation function (e. g. sigmoid functions, hyperbolic tangent, sign function, etc.) and yk is the output of the single NN neuron k. Usually this structural pattern is replicated multiple times, one time for each layer, until the final one outputtinig the prediction. Support Vector Regressors (SVR), are usually selected for their potential enhanced performance and computational advantages. Their regression function can be actually computed by means of a small subset of training points called the support vectors. Practically, in SVR, the input X is first mapped onto a m-dimensional feature space using some fixed (nonlinear) mapping, and then a linear model is constructed in this feature space. Using mathematical notation, the linear model (in the feature space) f(X, ω) is given by\nf(X, ω) = m∑ j=1 ωjgj(X) + b,\nwhere gj(X), j = 1, . . . ,m denotes a set of nonlinear transformations, and b the bias term. In the primal optimization problem, we would like to minimize the sum of the empirical estimation errors on the training set samples in a trade off with a solution complexity term, ωTω, regulated by a constant C. Using an epsilon insensitive loss function, i.e. by disregarding errors not exceeding a fixed term epsilon, we ensure existence of the global minimum and, at the same time, the optimization of reliable generalization bound. It is well known that SVR generalization performance (estimation accuracy) depends on a good setting of hyperparameters C, and the kernel parameters1. The problem of optimal parameter selection is further complicated by the fact that SVR model complexity (and hence its generalization performance) depends simultaneously on all three parameters,\n1The solution of the dual problem is given by:\nf(X) = nsV∑ i=1 (αi − α∗i )K(Xi,X) + b,\ns.t. 0 ≤ α∗i ≤ C, 0 ≤ αi ≤ C, where nsV is the number of Support Vectors (SVs) and the kernel function\nK(X,Xi) = m∑ j=1 gj(X)gj(Xi).\nGP take a nonparametric approach to regression and offers a stochastic regression function. It is completely characterized by its mean and covariance function or kernel. Consider the training set {(xi, yi); i = 1, 2, . . . , n}, where xi ∈ Rd and yi ∈ R, an istance of response y from a GPR model can be modeled as\nP (yi|f(xi), xi) ∼ N ( yi|h(xi)Tβ + f(xi), σ2 ) .\nHence, a GPR model is a probabilistic model. There is a latent variable f(xi) introduced for each observation xi, which makes the GPR model nonparametric. In vector form, this model is equivalent to P (y|f,X) ∼ N(y|Hβ + f, σ2I), where\nX =  xT1 xT2 ... xTn  , y =  y1 y2 ... yn  , H =  h(xT1 ) h(xT2 ) ... h(xTn )  , f =  f(x1) f(x2) ... f(xn)  . The joint distribution of f(x1), . . . , f(xn) is\nP (f |X) ∼ N(f |0,K(X,X)),\nwhere K(X,X) looks as follows:\nK(X,X) =  k(x1, x1) k(x1, x2) · · · k(x1, xn) k(x2, x1) k(x2, x2) · · · k(x2, xn) ... ... ... ...\nk(xn, x1) k(xn, x2) · · · k(xn, xn)  The covariance function k(x, x′) is usually parametrized by a set of kernel hyperparameters θ. In particular, we have used the Matlab function fitrgp, which estimates the noise variance σ2 and the hyperparameters θ of the kernel function from the data while training the GPR model. MLR regressors are used as a state of the art systems for comparison reasons. Assuming that X is the input features, the classic mathematical formulation of MLR model is\ny = Xβ + ,\nwith is the intercept and y the predicted value. In this work we foster the use the Reservoir computing approach, originating from the echostate network paradigm and introduced in chemical multisensors field by [29]. This two-stages approach is based on a network structure that is characterized by multiple recurrently interconnected non linear units, called the reservoir, that realize a time-expansion of the input sequence. After this expansion, the so processed inputs are fed in a MLR based final processing layer that eventually output the prediction. An interesting feature of this algorithm is that the recurrent non-linear dynamic units, more properly their weighted interconnections, are not trained, only the output linear layer is. Actually, the non-linear elements in the reservoir are randomly interconnected at the start of the training phase actually determining the expansion behavior that will remain fixed. More specifically the random interconnection process is regulated by two parameters namely input scaling, chosen such that the input activity can induce sufficient activity in the reservoir and spectral radius, chosen to ensure that the resultant activity of the reservoir, when the inputs occurr, is sufficiently diverse for different inputs. The time behavior of the RC model, is mathematically defined by:\nx̃(n) = tanh(Win[1;u(n)] +Wx(n− 1)), (2) x(n) = (1− α)x(n− 1) + αx̃(n), (3)\nwhere n is discrete time, u(n) ∈ RNu is the input signal, x(n) ∈ RNx is a vector of reservoir neuron activations and x̃(n) ∈ RNx is its update at time step n. Win ∈ RNx×Nu is the input weighted matrix and W ∈ RNx×Nx is the recurrent weighted matrix; α ∈ (0, 1] is the leaking rate. we assume α = 1, and thus x̃(n) ≡ x(n). The linear readout is defined as\ny(n) = Wout[1;u(n);x(n)], (4)\nwhere y(n) ∈ RNy is the network output and Wout ∈ RNy the output weighted matrix. we used the open-source Python library Oger [41] for the implementation of the algorithms developed to compute gas concentration estimations."
    }, {
      "heading" : "3.1.1 Computational Costs",
      "text" : "The working principle of Machine Learning algorithms encompasses two main steps that is a training phase, in which learning by example take place, and an operative phase, during which the trained system process input data and performs its estimations. Training phase is usually far more computationally intensive that a single estimation operation, usually requiring the evaluation of hundreds or thousands of sensory data instances and their respective reference data in order to tune the machine learning algorithm parameters. However, in most applications, training is performed just once for the entire operative lifespan of the intelligent device. In our specific case, machine learning algorithms are usually first trained using sensor responses to different air quality conditions for which reference data about pollutant concentrations are known. This phase usually is carried out off-line by dedicated systems. Afterwise, operative life is started and the tuned machine learning algorithm is continuously fed with sensor responses for carrying out estimations of pollutant concentrations. Some authors highlighted the possibility to retrain systems in order to correct for sensors and concept drift but even in this case, training operation is performed very rarely, possibly once every sixth months. Smart cyber chemical systems are built by networks of chemical multisensory devices relying on microcontrollers with limited computing and storage capabilities. Their field recorded data are sent to centralized cloud computing facilities devised to process the incoming stream of sensory data coming from all the networked devices. In both cases, computational and storage resources of both subsystems are challenged by the amount of data to be processed in real time using complex machine learning algorithms. Hence, the computational and storage complexity of such algorithms is highly relevant and can determine the choice of the actual machine learning algorithm to implement. Due to the rare need to execute a (re) training phase, the operational phase computational and storage complexities are much more relevant, for our scenario, than those of the training phase.\nThe operational computational cost of a NN can be split into two main contributions. The first one is due to the linear part of the NN prediction equation, deriving from the operations needed to perform the sum of the weighted inputs of each neuron. The second one, is related to the computation of the neurons activation function. Considering eq. 1, we have k function invocations for each hidden layer being k the number of hidden neurons in the layer. Furthermore we have k dot n products coming from matrix products (where n is the dimension of input vectors space). Storage and computational complexity is hence related with the total number of interconnection weights in each layer. Ultimately, for shallow neural networks including a single hidden layer and a single output, it scales like O(nk).\nConversely, as we shown in the previous section, prediction complexity of kernel SVR depends on the choice of kernel and it is typically proportional to the number of support vectors. For most kernels, including polynomial and RBF, computation and storage complexity is bound by O(nSV d), where nSV is the number of support vectors and d is the dimension of the input space.\nGPR model, in order to provide predictions for new data, requires:\n• Knowledge of the coefficient vector β of fixed basis functions;\n• Evaluation of the covariance function k(x, x′ |θ), given the kernel parameters θ;\n• Knowledge of the variance σ2, that appears in the density P (yi|f(xi), xi).\nTraining a GPR model requires the inversion of an n+n kernel matrix k(X,X). The memory requirement for this step scales as O(n2) since k(X,X) must be stored in memory. One evaluation of log(y|X) scales as O(n3). Therefore, the computational cost is O(kn3), where k is the number of function evaluations needed for maximization and n is the number of observations. The computation of the prediction values involves the estimation of α = (K(X,X|θ)+σ2In)(−1)(y−Hβ), which has the computational complexity equal to O(n3) and the memory requirement is O(n2). For large n, estimation of parameters or computing predictions can be very expensive. The approximation methods usually involve rearranging the computation so as to avoid the inversion of an n× n matrix.\nMLR has been trained using the Matlab function fitlm, in which we have selected the standard model, i.e. the model containing an intercept and linear terms for each predictor. As know, the computational cost of this algorithm depends on the size of the input matrix, in particular the matrix product Xβ.\nAs we can see in eq. 4, the estimation of the RC computing prediction ytarget(n), involves the computation of outputs weights via Linear Regression. In particular, the vectors [1;u(n);x(n)] are collected in a matrix X and y(n) in a matrix Y, both having a column for every training time step n. The computation of the expansions x(n) costs Nx function invocations, Nx · Nu products for the computation ofW in[1;u(n)] and N2x products for the computation ofWx(n−1). Since, typically, Nx Nu, the computational complexity is O(N2x), the storage complexity being bound by the same term. These insights will be useful for completing a fair comparison of the tested techniques in section 4."
    }, {
      "heading" : "3.2 Datasets Description",
      "text" : "The comparison was carried out taking into account three different datasets: the first was from a specific deployment of multi-sensory devices (SNAQ systems, see [42] for detailed description of the deployment), developed by the Centre for Atmospheric Sciences (CAS), Chemistry Department, University of Cambridge (UK). The second dataset was from a field deployment of multisensory device located in significantly polluted roadside within an Italian city. Finally, the third dataset was collected in a gas delivery platform facility at the ChemoSignals Laboratory in the BioCircuits Institute, University of California UCSD, USA. The measurement system platform provides versatility for obtaining the desired concentrations of the chemical substances of interest with high accuracy and in a highly reproducible manner."
    }, {
      "heading" : "3.2.1 SNAQ Dataset",
      "text" : "The passive multisensory device was equipped with the following sensors array:\n• two different NO2 electrochemical (EC) sensor units (Alphasense NO2−B4 termed in the following as NO2(A) and NO2(B));\n• one NO Alphasense EC sensor unit (Alphasense NO −B4);\n• two different O3 Alphasense EC sensor units (Alphasense O3−B4 termed in the following as O3(A) and O3(B));\n• temperature and relative humidity (RH) sensor units;\n• wind speed and wind direction device.\nMultiple instances of the multisensor were deployed within the city Centre of the city of Cambridge (UK) as a part of a pervasive deployment. One of them was located on the roof of the Chemical Dept. together with a conventional reference station operated by CAS. A couple of\nthe SNAQ units were deployed within Cambridge city centre (UK), as a part of a pervasive deployment. One of them was located on the roof of the Chemical Department, University of Cambridge. together with a conventional reference station operated by CAS. This station relied on certified chemiluminescence and spectrometer based analysers. While the SNAQ units samples at 20s interval, the reference station had a temporal resolution of 1 minute. This reference station monitored toxic gases including carbon monoxide (CO), nitric oxide (NO),nitrogen dioxide (NO2), ozone (O3), SO2.In this work, we considered all raw instantaneous sensors readings for calibration purposes, comparing the estimations results with the conventional analyzer samples when available (one out of three sensor readings). Here, it is worth to note that electrochemical sensors, when operating at low ppb levels, are also prone to interference issues that limits the performance outcome. Actually a known cross sensitivity has been reported for, O3 and NO2 [43] and [44]. Together with temperature interference this effect is expected to represent the main limit to absolute performance in this dataset. Sensors were calibrated by the manifacturer with a linear univariate procedure, so raw sensors signals units were ppb. Baseline and temperature correction by using datasheet procedure were also implemented. Finally, the reference data was processed for unusable data period when daily calibration was carried out. These rejected data spans few minutes every day. In order to build a suitable dataset, five weeks of continuous measurements were used in this study."
    }, {
      "heading" : "3.2.2 ENEA Pirelli Dataset",
      "text" : "The second dataset came from the source: https://archive.ics.uci.edu/ml/datasets/ Air+Quality. It contains 9358 instances of hourly averaged responses from an array of 5 metal oxide chemical sensors embedded in an air quality chemical multi-sensor Device. Data were recorded from March 2004 to February 2005 (one year) representing the longest freely available field deployed air quality data from this type of chemical sensor devices. Reference hourly averaged concentrations for CO, non methane hydrocarbons (NMHC), benzene, total Nitrogen Oxides (NOx) and NO2 were obtained from reference certified analyser where the multi-sensors were co-located. Evidences of cross-sensitivities as well as both concept2 and sensor drifts are present as described in De Vito et al. [30]. These eventually represent the main drivers that affect sensors concentration estimation capabilities in the long term, determining the performance limits. The sensors array is equipped in the following way:\n1. PT08.S1 (tin oxide) hourly averaged sensor response (CO species);\n2. PT08.S2 (titanium) hourly averaged sensor response (NMHC species);\n3. PT08.S3 (tungsten oxide) hourly averaged sensor response (NOx species);\n4. PT08.S4 (tungsten oxide) hourly averaged sensor response (NO2 species);\n5. PT08.S5 (indium oxide) hourly averaged sensor response (O3 species);\n6. Temperature, RH and Absolute Humidity.\nFor all the sensor array, recorded signal unit were ohms. Similarly to the previous dataset, the device was operated in continuous operation mode but the timeframe differs considerably in that only hourly averages of both sensor recordings and reference data are available. 2"
    }, {
      "heading" : "3.2.3 UCSD dynamic gas mixtures Dataset",
      "text" : "The final dataset (available on https://archive.ics.uci.edu/ml/datasets/Gas+sen sor+array+under+dynamic+gas+mixtures) contains the data from laboratory tests in which\n2Here we refer to Concept drift as the usually slow variation of process relevant variables distributions. In our case, these are the target and non target pollutants concentrations as well as RH and T.\n16 chemical sensors were exposed to gas mixtures at varying concentration levels for 12 hours withou interruption. The sensor array was placed in a 60ml measurement chamber, where the gas sample was injected at a constant flow of 300ml/min.\nIn particular, two gas mixtures were generated: ethylene and methane in air, and ethylene and CO in air. The latter has been used in this work with concentrations designed to elicit significant interference in the involved sensors array. Specifically, the sensor array included 16 MOX chemical sensors (Figaro Inc., US) made up of 4 different types: TGS-2600, TGS-2602, TGS-2610, TGS2620 (4 units of each type). They were integrated with custom built signal conditioning and control electronics. The operating voltage of the sensors, which controls the sensors operating temperature, was kept constant at 5 V for the whole duration of the experiments. The sensors conductivities were acquired continuously at a sampling frequency of 100Hz. However, this was subsequently averaged to create 1s data. No drift correction procedure was implemented. The targeted reference concentration levels (set-points) were changed randomly each 80 to 120 seconds, and abruptly, involving significant concentration changes (e.g. from 0 to 300 ppm of CO). As above mentioned, the relatively high flow rate of the carrier gas (300mlmin) allowed for fast exchange in the 60ml sensors chamber (∼ 12s required to fill the chamber), reducing (but not nullifying) the dead volume inertia influence. However, no correction to target gas concentrations have been performed by taking into account sensor chamber dynamics. This challenged the relatively slow dynamics of the sensors. Fast and random target concentrations (set points) transients make this dataset differ from the previous ones in which the true concentration levels varied smoothly in uncontrolled way. Indeed, the dataset was purposedly designed to induce significant transient and cross-interference errors that are the main drivers for limited absolute performances. The UCSD dataset was also built such that several gas mixtures compositions, included pure gases, were considered."
    }, {
      "heading" : "3.3 Performances analysis",
      "text" : "The above mentioned methodologies have been tested for their ability to model and create a generalised relationship between sensors response and the target gas concentration. The three datasets have been subset into training, validation and test sets by keeping the natural timing sequence i.e. by selecting sensor responses measured at subsequent times. The validation subsets were used for model selection i.e. for selecting the best performing hyper-parameters value set and tapped delay length. The test subset were used to evaluate the predictive performance. We have also varied training set length to investigate the response of the different methodologies in terms of generalization capabilities. We optimised by conducting extensive exploration of model hyper-parameters subspaces for all architectures. Mean absolute error (MAE) defined as the sample mean of absolute prediction error and its standard deviation (STD) were used to control the hyper-parameter selection and reported as performance indicators. For NN, GPR and RC approaches, each training and test procedure were repeated 30 times to reduce the uncertainty induced in performance indicators by the random choice of booting parameters, respectively: NN initial weights, results of kernel hyperparameters selection and random reservoir units wiring. Best performing architectures for each ML technique, defined by their hyper-parameters t-uple, have been selected to be compared.\nFor NN, the hyperparameters that we considered were:\n• Hidden neurons number (HNN),\n• Epochs number (EN),\nfor each tapped delay length. In particular, HNN varied in the [3, 5, 7, 10, 15, 20] set while EN ∈ [100, 200, 300, 400, 500, 600, 900]. For SVR we investigated the hyperparameters space consisting of:\n• Kernel Scale factor γ ∈ ( 2−15, 25 ) ;\n• Box Constraint C ∈ ( 2−5, 215 ) ;\n• half the width of epsilon-insensitive band ∈ (0.1 : 0.1 : 11);\n• Kernel Function ’rbf’.\nFor GPR methodology, the considered hyperparametes are:\n• initial value for the noise standard deviation of the Gaussian process model\nσ ∈ ( 1e− 2std(target_train), std(target_train)√\n2\n) ;\n• Kernel (Covariance) Function∈ (’squaredexponential’, ’matern32’, ’matern52’)2.\nFor RC, the parameters considered are:\n• ρ ∈ (0.1 : 0.1 : 1),\n• IS ∈ (0.1 : 0.1 : 0.9),\n• RU ∈ [10, 20, 30, 50, 100, 150, 200, 250]."
    }, {
      "heading" : "4 Results",
      "text" : "As mentioned above, all the methodologies taken into account in this work have been tested with different training set lengths (TSL) and different observation windows lengths (TDL). The comparison results are reported in Table 2. In particular, there we show MAE and STD as performance indicators, for all the best performing models in the different scenarios (TSL,TDL). For the first dataset, the target gas was NO2 (ranging from 0.30 to 48.50 ppb) and MAE estimations are expressed in ppb. For the second dataset, the results obtained for CO target gas estimation are in mg/m3 while for the third dataset, CO was expressed in ppm. Graphically, Table 2 is divided into three main sections, each one reporting results for the three different datasets. In each section different subsection reports the results of the different ML techniques. Different rows indicate different training, validation and test set lengths. As mentioned above, we also compared the static architectures with the corresponding dynamic version, using different time series length (columns in 2). Results from the static versions are reported in the leftmost sub column of the third main column (Tapped Delay Length) for each dataset and for all the proposed techniques except for reservoir computing. For its structure, RC is inherently a dynamic\n2Squared Exponential Kernel is one of the most commonly used covariance functions; it is defined as k (xl, xj |θ) = σ2f exp [ − 1\n2 (xl−xj)T (xl−xj) σ2 l\n] . Matern32 is a covariance function defined as k (xl, xj |θ) =\nσ2f\n( 1 +\n√ 3r σl\n) exp ( − √ 3r σl ) . Matern52 is defined as k (xl, xj |θ) = σ2f ( 1 + √ 5r σl + 5r 2 σl ) exp ( − √ 5r σl ) , where r =√\n(xl − xj)T (xl − xj) is the Euclidean distance between xl and xj .\ntechnique and it is not coupled to a tapped delay with a specific length, instead its dynamic behaviour is controlled by the reservoir dimension. For this reason its results are reported in a single coloumn. At a first glance, we observe that dynamic versions of the machine learning architectures outperform, almost every time, their static counterparts. In fact, for the first and third dataset, regardless of the training set length and the applied ML methodology, the use of an observation window always gives best results. The absence of a consistent performance improvement in the second dataset is due to the sampling methodology (hourly averages) that average out any sensor related dynamic information from the dataset. By looking at performance dependence from the length of the tapped delay, in the first and third dataset, we can find a very interesting consistency in the minimal length required in order to boost static performance. Specifically, for the first dataset, we observe that best performances, regardless of the length of the training set, are obtained, for each methodology, at a minimum observation window length of 3 (SVR, GPR, MLR) or 4 minutes (NN). For the third dataset a similar behaviour is observed and most of the best performances are obtained for each training set length with an observation window of 30 seconds considering all the ML methodologies. This is graphically expressed by Fig. 10, 11 and 12 that, respectively, shows the MAE performance versus TDL relationship for GPR (dataset 1), SVR (dataset 2) and NN (dataset 3), at each training set length. The pictures clearly highligh the MAE reductions obtainable by using dynamic architectures. These findings suggest that the dynamic architectures are truly capable to grasp and embed a useful knowledge on sensors dynamic behaviour provided the adoption of a sufficiently long observation window. Of course, curse of dimensionality issues may apply when a too large observation window is adopted. Further analyses are needed to correlate the observed results on optimal length with the time constants or, better, the T90 parameter of the slowest sensor in the sensor array. This could lead to a more concise feature set or, in other words, smaller input dimensionality and consequently a more concise representation. Summarizing these results, it seems clear that dynamic architectures prove to be the best approach whenever prompt responsiveness is needed. The results obtained in Table 2 highlight (in bold) the best methodology for each dataset at each (TDL, TSL) setting. For SNAQ dataset SVR with training set length equal to 3 weeks and TDL = 3min gave the best overall results, i. e. MAE = 1.05 ppb compared toMAE = 1.10 ppb obtained with NN with three weeks long training set. Dividing this value by the experimental range of the NO2 target gas, SVR achieved a relative MAE of 2.2%. For ENEA Pirelli dataset, the best results are obtained using GPR (MAE = 0.47 mg/m3 at three weeks training set accounting for a relative MAE of 3.9%). For the third dataset (UCSD) the best technique is once again SVR, with MAE = 39.10 ppm at 5.6 hr long training set scilicet a relative MAE of 7.3%. Considering the relative performances of the individual techniques, we observe that in the first dataset, SVR architectures gave the best result for most training set length (3 out of 4 times). Furthermore considering each (TSL, TDL) setting separately, similar deduction can be made with SVR outperforming 12 times out of 20. However performance obtained by NN and GPR are relatively similar (see Table 2). For the second dataset, we observe that SVR and GPR have very similar performance obtaining the best scores respectively 5 and 7 times out of 12. In the third dataset, both NN and SVR gave similar resulting to be sthe best performing techniques respectively 8 and 11 times out of 20, with GPR yielding the poorest performance. Thus, SVR seems to provide consistently the best performances. GPRs and, closely, NNs proved to be the next more reliable in terms of performance showing mixed behaviour when operating with a limited knowledge. Surprisingly, RC models, even if outperforming MLR, seems less reliable on average when compared to more classic approaches.\nIn this case, we would also consider the significant variance due to the random ”training” of the reservoir connections and the limited time that RC networks need to be designed and trained. Actually, the adoption of an RC technique simplifies modelling in that no selection of optimal observation window is needed, thereby exploiting the inherently dynamic wiring of the reservoir neurons. Moreover, redundancy in the reservoir dimension allows for enhanced flexibility. It\nallows, specifically, the same architecture to operate efficiently with different scenarios showing different time constants or with a single scenario where different dynamics are simultaneously involved. In RC, adaptation take place in the training phase involving the final multi-linear stage by selecting the weight coefficients associated to the different components of the RC time expansion stage. In fact, looking at minimum MAE estimates from RC approach at each TSL, it appears that RC come closer to the performance obtained by NN, SVR and GPR at the longest TDL, outperforming them at least once (third dataset, 5.6 hrs long training set).\nExcept under the conditions TDL=30sec, TSl=2.8 hrs for the third dataset, MLR showed the poorest relative performance.\nIn order to consider the viability of implementing the above prediction algorithms on-board the devices considered, we have to evaluate our results from the computational and storage complexity point of view. From the previous sections, we know that the main challenges for the methodologies evaluated in this work are input dimensionality and training set length. Ideally we would like to obtain sufficient performances with limited input dimensionality (i.e. small number of sensing units and small window of observation) and limited training set length. Figures 2, 3 and 4 show the performance relationship with the TSL at the identified optimal TDL for the three datasets. Figures 5, 6 and 7 show the best performance obtained by the machine learning techniques for all the TDL with respect to the TSL for the three studied datasets.\nApart from confirming the relative performance among the different techniques, it is quite evident that, as already reported in literature, larger training sets generate better performance than very small one, the latter being unable to represent the real span of the multivariate manifold of sensor responses. However, far from being linear, this relationship usually shows that after a specific threshold, no significant improvement is attained by using larger training set. It is therefore important to identify this threshold value, in order to optimize the length of the training set and ultimately the cost of the calibration. Table 3 shows the best performing hyper-parameters set at optimal TDL and TSL. Comparing the architectures obtaining the best performances for the different ML methodologies, our results show that SVRs hold a significant space complexity. Considering 3 weeks long training set of the SNAQ data, to achieve its best performance will require the storage of 10k × d floating point precision values, where d is the dimension of the input space. Since the number of support vectors both affects computational and storage complexity of the model, this can discourage their use as an on-board computational intelligence component for concentration estimation. It is worth noting that a high number of support vectors or, better still, high SVs to total number of training samples ratio, are quite common when operating performance-optimal parameter selection schemes. This is generally due to the peculiar choice of parameters. Considering figures 8 and 9, depicting UCSD dataset (third dataset) results, we note how a significant reduction of the number of SV (up to one half) can be achieved at a small performance cost by tuning the C value. Nonetheless, the number of SV remains very high, suggesting the complex nonlinear nature of the calibration problem. Conversely, its corresponding optimal shallow neural network, being only bound to the number of hidden neurons and inputs dimensionality, needs, in our settings, a fraction of the number of the parameters (weights) to be stored. This confirms the Tapped Delay Neural Networks (TDNNs) as a very efficient methodology from the point of view of the learnt knowledge representation. Meanwhile, RC networks are bound by the number of hidden interconnected neurons in the reservoir. Together with input and output dimension, it generates the dimension of the input weights, output weights and internal interconnection weights matrices. In our scenario, they do not appear as a competitive choice. In fact, their optimal hyperparameters selection\nneeds considerably more storage space and computational capabilities than their optimal NN counterparts. However, they have shown better storage efficiency than optimal SVR architectures in most of the cases considered in this work. Gaussian processes, like all other lazy learning machine learning tools, pose significant computational and storage issues when dealing with large datasets. Unlike SVRs, they need to store all the training set samples and use them for prediction computations, at least in their original formulation. Under this condition, the model can be used only when the number of available training samples is not high (<< 1000 samples). In our model setup, specifically in the ENEA Pirelli dataset, GP use when using only 24 training samples, can be favourable also from the performance point of view. In some cases, the loss of performance can be justified by the limited computational and storage needs of a less than optimal methodology. Specifically, capability limits of the target microcontroller node or scalability issues when cloud computing is concerned, can be particularly relevant and may lead to such a choice. Most of the time, in arriving at the best option, engineers will have to take into account the trade-off between performance and complexity. In our view, considering the overall calibration scenario characterized by low dimensional feature sets and training set that ranges from hundreds to thousands samples, shallow neural networks appears to be the best option for on board integration. Actually, if we analyze the worst case for NNs, i.e. when TDL = 60 (at TSL = 5.6hrs) for the UCSD dataset, we observe that the difference between NN and SVR is negligible taking into account the computational complexity. In fact, the best estimation obtained with SVR involves the use of 10072 support vectors, while NN required the storage of 2883 weights.\nS n aQ\nd at\nas et\nML technique\n(Train - Val - Test) Partition\nMean Absolute Error Test (STD)\n(Number of samples) Tapped Delay Length (min) 0.33 1 3 4 5\nNN\n1440 - 10080 - 40285 1.62 (0.08) 1.57 (0.13) 1.47 (0.11) 1.54 (0.14) 1.53 (0.14) 10080 - 10080 - 31645 1.26 (0.05) 1.19 (0.06) 1.17 (0.04) 1.16 (0.04) 1.16 (0.04) 20160 - 10080 - 21565 1.39 (0.04) 1.32 (0.03) 1.18 (0.03) 1.17 (0.03) 1.18 (0.02) 30240 - 10080 - 11485 1.38 (0.04) 1.24 (0.03) 1.10 (0.02) 1.10 (0.02) 1.10 (0.03)\nSVR\n1440 - 10080 - 40285 1.41 1.34 1.26 1.29 1.28 10080 - 10080 - 31645 1.51 1.30 1.25 1.25 1.16 20160 - 10080 - 21565 1.31 1.15 1.05 1.05 1.05 30240 - 10080 - 11485 1.32 1.18 1.10 1.07 1.07\nGPR\n1440 - 10080 - 40285 3.34 (0) 2.61 (0.05) 2.25 (0.17) 2.18 (0.20) 2.19 (0.25) 10080 - 10080 - 31645 1.55 (0.02) 1.30 (0.02) 1.20 (0.02) 1.20 (0.01) 1.22 (0.01) 20160 - 10080 - 21565 1.40 (0.01) 1.22 (0.05) 1.10 (0.03) 1.10 (0.03) 1.10 (0.03) 30240 - 10080 - 11485 1.33 (0.01) 1.17 (0.004) 1.06 (0.002) 1.06 (0.003) 1.06 (0.002)\nMLR\n1440 - 10080 - 40285 1.81 1.66 1.63 1.65 1.67 10080 - 10080 - 31645 1.62 1.47 1.40 1.40 1.40 20160 - 10080 - 21565 1.55 1.40 1.30 1.30 1.30 30240 - 10080 - 11485 1.58 1.48 1.39 1.38 1.38\nRC\n1440 - 10080 - 40285 3.02 (0.31)/2.33 10080 - 10080 - 31645 2.74 (0.95)/1.44 20160 - 10080 - 21565 1.55 (0.76)/1.16 30240 - 10080 - 11485 1.25 (0.03)/1.20\nE N\nE A\nP ir\nel li\nd at\nas et\n(Number of samples) Tapped Delay Length (hours) 1 3 5\nNN\n24 - 168 - 7482 0.90 (0.10) 0.96 (0.43) 1.22 (0.37) 168 - 168 - 7338 0.83 (0.11) 0.86 (0.19) 0.79 (0.14) 336 - 168 - 7170 0.71 (0.10) 0.79 (0.13) 0.69 (0.09) 504 - 168 - 7002 0.61 (0.06) 0.76 (0.12) 0.71 (0.12)\nSVR\n24 - 168 - 7482 0.67 0.92 1.17 168 - 168 - 7338 0.56 0.69 0.53 336 - 168 - 7170 0.50 0.54 0.56 504 - 168 - 7002 0.64 0.53 0.54\nGPR\n24 - 168 - 7482 0.62 (0) 0.80 (0.02) 0.67 (0.02) 168 - 168 - 7338 0.95 (0) 0.75 (0.09) 0.78 (0.02) 336 - 168 - 7170 0.61(0.05) 0.50 (0) 0.75 ( 0) 504 - 168 - 7002 0.55 (0) 0.47 (0) 0.52 (0)\nMLR\n24 - 168 - 7482 1.79 - - 168 - 168 - 7338 1.82 1.97 1.70 336 - 168 - 7170 1.24 1.43 1.36 504 - 168 - 7002 1.09 1.33 1.30\nRC\n24 - 168 - 7482 1.42 (0.28)/0.94 168 - 168 - 7338 1.37 (0.21)/1.09 336 - 168 - 7170 1.17 (0.25)/0.88 504 - 168 - 7002 0.96 (0.14)/0.79\nU C\nS D\nd at\nas et\n(Number of samples) Tapped Delay Length (sec) 1 5 10 30 60\nNN\n1440 - 10080 - 30562 165.73 (57.32) 100.71 (45.68) 101.46 (24.96) 93.14 (20.76) 144.13 (66.68) 10080 - 10080 - 21922 65.20 (12.86) 52.73 (6.50) 55.06 (7.10) 61.67 (10.15) 70.77 (5.94) 20160 - 10080 - 11842 44.58 (5.24) 40.79 (1.41) 41.19 (3.01) 42.95 (2.96) 47.25 (4.16) 30240 - 10080 - 1762 64.98 (2.68) 61.10 (3.12) 60.44 (3.29) 60.33 (3.15) 56.93 (2.81)\nSVR\n1440 - 10080 - 30562 128.81 119.48 123.12 135.40 125.95 10080 - 10080 - 21922 55.76 64.41 61.46 74.37 76.41 20160 - 10080 - 11842 39.43 41.85 39.10 41.75 47.67 30240 - 10080 - 1762 57.97 53.19 49.59 53.52 54.79\nGPR\n1440 - 10080 - 30562 136.92 (0.20) 133.27 (0.07) 132.18 (0.02) 130.76 (0) 128.57 (0) 10080 - 10080 - 21922 129.95 (0) 125.87 (0) 121.39 (0) 106.55 (0) 120.68 (0) 20160 - 10080 - 11842 89.92(0) 91.69 (0) 87.59 (0) 91.23 (0) 90.74 (0) 30240 - 10080 - 1762 67.38 (0.17) 64.15 (0.12) 56.27 (0.09) 61.05 (0) 59.04 (0.03)\nMLR\n1440 - 10080 - 30562 - - - - 10080 - 10080 - 21922 184.50 166.19 123.73 61.51 142.24 20160 - 10080 - 11842 106.72 59.77 51.58 47.21 49.60 30240 - 10080 - 1762 95.35 79.54 74.13 72.29 73.64\nRC\n1440 - 10080 - 30562 147.35 (39.76)/115.10 10080 - 10080 - 21922 108.51 (15.15)/75.06 20160 - 10080 - 11842 55.60 (6.38)/46.22 30240 - 10080 - 1762 70.13 (3.64)/62.55\nTable 2: Listing of the results obtained by all the selected ML methodologies for all the considered datasets at different TDLs and four different training set lengths. Bold values indicates best performances for each (TDL,TSL) settings. \"-\" indicates out of scale values.\nBest Hyperparameters Best ML technique Tapped Delay Length\n3 min\nS n aQ\nSVR (30240 samples) γ = 25\nC = 213\n= 0.7\nsv = 13095\nNN (30240 samples) Epochs = 900\nhn = 15\nGPR (30240 samples) σ = 0.2439\nKF = squaredexp\nRC (30240 samples) ρ = 0.1 input_scaling = 0.1 output_dim = 250\n1 hour\nE N\nE A\nP ir\nel li\nSVR (504 samples) γ = 24\nC = 29\n= 0.2\nsv = 356\nNN (504 samples) Epochs = 400\nhn = 5\nGPR (504 samples) σ = 0.0136\nKF = Matern32\nRC (504 samples) ρ = 0.9 input_scaling = 1e− 9 output_dim = 200\n10 sec\nU C\nS D\nSVR (20160 samples) γ = 25\nC = 215\n= 1.3\nsv = 14447\nNN (20160 samples) Epochs = 600\nhn = 3\nGPR (20160 samples) σ = 91.6378\nKF = matern32\nRC (20160 samples) ρ = 0.3 input_scaling = 0.01 output_dim = 100\nTable 3: Best parameters provided by the considered methodologies, for the three datasets at the best performing tapped delay length.\nIn Fig. 8 and Fig. 9 we show the trends of MAE while changing the SVR hyperparameters values."
    }, {
      "heading" : "4.1 Dynamic performances",
      "text" : "This section describes our results in view of the systems overall response to rapid changes of gas concentrations, to further analyze the outcome of the use of dynamic calibration algorithms. In particular, we are interested in following what happens at different rate of concentration change. To this effect, we have plotted the MAE performance indicator against the approximated value of first derivative of reference concentrations for the first two datasets (NO2 species for SNAQ and CO species for ENEA Pirelli). For the third dataset (UCSD), where reference concentrations set points changes abruptly, we have plotted the response to these concentration changes allowing us to compare the static and dynamic approaches responses to rapid transients. Figures 13 and 14 the MAE behaviour for different ML algorithms in the two -datasets (SNAQ and ENEA Pirelli) for the test datasets. We can clearly see that the dynamic GPR results are far better than the static GPR all along the derivative axis . In contrast, no improvement was observed with dynamic SVRs relatively to the static SVR (Fig. 14) due to the absence of sensors dynamic related information in the ENEA Pirelli dataset. This analysis helps clarify that the amelioration is not due to time series prediction capabilities of tapped delay architectures. In figures 15 and 16 we show how TDNN output responds significantly faster to abrupt transients, occurring in the UCSD dataset, with respect to raw sensor data, while figure 17 depicts TDNN similar advantage with respect to the static NN algorithm . These results also show that this may sometimes occur at the cost of a limited observed over elongation. The enhanced error performances are, hence, not due to noise suppression capabilities of tapped delay architectures but to an improved responsiveness. Those figures, in facts, confirms at a wider level what already shown in Esposito et al. only for NNs [36] using a single dataset. In that work, authors firstly shown that the MAE grows along with the speed with which the concentration changes but dynamic calibration algorithms can reduce the error notwithstanding the rapidity of change. Quite often, the faster the concentration transient, the better the improvement made. From the results of the three datasets, we infer that dynamic algorithms improve global performances primarily by reducing the error associated with rapid transients in static algorithms."
    }, {
      "heading" : "5 Conclusions",
      "text" : "Recently, chemical multi-sensor devices are increasingly designed for pervasive or mobile air quality deployments, often requiring computational intelligence to effectively solve their complex calibration problem. In this work, we have assessed and compared, for the first time, the performances of multiple machine learning approaches in a comprehensive set of continuous and open sampling scenarios. Five of the best performing machine learning approaches in the recent chemical sensing literature, along with their dynamic implementations were reviewed and assessed. The tests were carried out by using three different datasets spanning a significant variety of conditions designed to challenge the technological limits of chemical sensors eventuallly determining the absolute performance levels. From the dynamic point of view, the datasets included smooth or challenging abrupt concentration variations scenarios. Each of the sensors, used in the concerned devices, had its own dynamic behaviour versus the target gas as shown in their datasheet reported properties. In fact, data are sampled or averaged at different time resolution ranging from 1hr to 1/100th of second. These datasets were generated using devices that employed different sensor technologies while being exposed to typical outdoor and indoor pollutants, in uncontrolled field measurement as well as in laboratory tests, with unknown or known interferents at high concentrations. The utilisation of external test sets, the extensive model selection and performance computing approach (encompassing the use of hundreds or thousands of hyperparameters combinations), helped to guarantee a fair comparison among the techniques, ensuring statistical consistency to the entire framework. The obtained results, to our best knowledge, are comparable to, or ameliorate, the state of the art performances obtained for solid state based field calibrated air quality analyzers.\nThe most relevant finding has been the consistency with which dynamic machine learning approaches surpasses their respective static counterparts that rely only on instantaneous sensor responses. This was observed in the two datasets with fast sampling period, suggesting a relevant impact of the slow sensor dynamic on the performances. Instead, none of the dynamic techniques shown a significant advantage when dealing with ENEA Pirelli dataset which features a 1hr averaging of sensor responses. Interestingly, the minimum effective length of the tapped delay line was found to be very similar among the different techniques. This confirms that these architectures are ideal for analysing, tracking and above all, correcting the intrinsic slow dynamic of the sensors, given a sufficient and optimal observation window length. Results also suggest to take into account the slowest sensor dynamic when designing tapped delay length in order to obtain a concise but effective representation. In both the field datasets, our results also indicated that a training set length of more than one week is required for this non-adaptive approaches to build a sufficient knowledge of the sensor array model. In terms of basic machine learning techniques, SVRs was shown to have the best performance in most scenarios irrespective of the timeframe, the sensor technology or the length of the tapped delay line. However, despite not showing the best performance in many cases, plain shallow neural networks (NN) provided more compact and low computational/storage impact models for a usually very small performance cost. Non linear techniques shown a significant performance advantage over plain MLR being also able to use less training samples for obtaining the same performance levels. RC showed average performance levels needing small computational efforts for training at the cost of a very redundant model. Eventually, these results strongly suggest the use of dynamic approaches for on-line processing of chemical sensor data instead of the traditional static approach, especially when a network or mobile deployment is concerned. We observed that of all the models considered in this work, shallow neural networks confirms an interesting suitability for this task. Notwithstanding being one of the oldest methodology, they clearly prove to be one of the first choice that will be recommended for the next generation of intelligent, pervasive or wearable sensing systems with on board pollutant concentration estimation capabilities."
    }, {
      "heading" : "6 Acknowledgments",
      "text" : "The authors want to thank the UCSD team for having shared their dataset used in this work. Equally, We want to thank the authors of [41] and [29] for having shared, respectively, the basic framework and specific code used for developing our own Reservoir Computing implementation."
    }, {
      "heading" : "7 Author Contributions",
      "text" : "S. De Vito and E. Esposito devised the work and designed the experimental optimization and testing framework. Together with M. Salvato they implemented the machine learning components and their optimization and performance assessment code. O. Popoola and R. Jones recorded, preprocessed and reviewed the inception of the SNAQ dataset. F. Formisano reviewed the final code and managed the parallel code execution. S. De Vito, E. Esposito and O. Popoola wrote the paper. S. De Vito and G. Di Francia supervised the entire work."
    } ],
    "references" : [ {
      "title" : "Principles of Air Pollution Meteorology",
      "author" : [ "T.J. Lyons", "W.D. Scott" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1990
    }, {
      "title" : "Chemical principles of environmental pollution",
      "author" : [ "B. Alloway", "D.C. Ayres" ],
      "venue" : "CRC press,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1997
    }, {
      "title" : "Accounting for meteorological effects in measuring urban ozone levels and trends",
      "author" : [ "P. Bloomfield", "J.A. Royle", "L.J. Steinberg", "Q. Yang" ],
      "venue" : "Atmospheric Environment,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1996
    }, {
      "title" : "Assessment of air quality microsensors versus reference methods: The eunetair joint exercise",
      "author" : [ "C. Borrego", "A.M. Costa", "J. Ginja", "M. Amorim", "M. Coutinho", "K. Karatzas", "Th. Sioumis", "N. Katsifarakis", "K. Konstantinidis", "S. De Vito", "E. Esposito", "P. Smith", "N. AndrÃĺ", "P. Gerard", "L.A. Francis", "N. Castell", "P. Schneider", "M. Viana", "M.C. Minguillon", "W. Reimringer", "R.P. Otjes", "O. von Sicard", "R. Pohle", "B. Elen", "D. Suriano", "V. Pfister", "M. Prato", "S. Dipinto", "M.Penza" ],
      "venue" : "Atmospheric Environment,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2016
    }, {
      "title" : "Different strategies for the identification of gas sensing systems",
      "author" : [ "S. Marco", "A. Pardo", "F.A.M. Davide Davide", "C. Di Natale", "A. D’Amico", "A. Hierlemann", "J. Mitrovics", "M. Schweizer", "U. Weimar", "W. Göpel" ],
      "venue" : "Sensors and Actuators B: Chemical,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1996
    }, {
      "title" : "NO2 and NOx urban pollution monitoring with on-field calibrated electronic nose by automatic bayesian regularization",
      "author" : [ "S. De Vito", "M. Piga", "L. Martinotto", "G. Di Francia. CO" ],
      "venue" : "Sensors and Actuators B: Chemical,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2009
    }, {
      "title" : "Electronic nose: current status and future trends",
      "author" : [ "F. Röck", "N. Barsan", "U. Weimar" ],
      "venue" : "Chemical reviews,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2008
    }, {
      "title" : "Signal and data processing for machine olfaction and chemical sensing: a review",
      "author" : [ "S. Marco", "A. Gutierrez-Galvez" ],
      "venue" : "IEEE Sensors Journal,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2012
    }, {
      "title" : "An adaptive classification model based on the artificial immune system for chemical sensor drift mitigation",
      "author" : [ "E. Martinelli", "G. Magna", "S. De Vito", "R. Di Fuccio", "G. Di Francia", "A. Vergara", "C. Di Natale" ],
      "venue" : "Sensors and Actuators B: Chemical,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "Calibration transfer and drift counteraction in chemical sensor arrays using direct standardization",
      "author" : [ "J. Fonollosa", "L. Fernández", "A. Gutiérrez-Gálvez", "R. Huerta", "S. Marco" ],
      "venue" : "Sensors and Actuators B: Chemical,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2016
    }, {
      "title" : "Calibration transfer in temperature modulated gas sensor arrays",
      "author" : [ "L. Fernández", "S. Guney", "A. Gutiérrez-Gálvez", "S. Marco" ],
      "venue" : "Sensors and Actuators B: Chemical,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2016
    }, {
      "title" : "Cooperative 3d air quality assessment with wireless chemical sensing networks",
      "author" : [ "S. De Vito", "G. Fattoruso", "R. Liguoro", "A. Oliviero", "E. Massera", "C. Sansone", "V. Casola", "G. Di Francia" ],
      "venue" : "Procedia Engineering,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2011
    }, {
      "title" : "Electronic noses for environmental monitoring applications",
      "author" : [ "L. Capelli", "S. Sironi", "R. Del Rosso" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1997
    }, {
      "title" : "Gas distribution in unventilated indoor environments inspected by a mobile robot",
      "author" : [ "M. Wandel", "A. Lilienthal", "T. Duckett", "U. Weimar", "A. Zell" ],
      "venue" : "Proceedings of the IEEE International Conference on Advanced Robotics (ICAR",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2003
    }, {
      "title" : "Fog computing and its role in the internet of things",
      "author" : [ "F. Bonomi", "R. Milito", "J. Zhu", "S. Addepalli" ],
      "venue" : "In Proceedings of the first edition of the MCC workshop on Mobile cloud computing,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2012
    }, {
      "title" : "Pattern analysis for machine olfaction: a review",
      "author" : [ "R. Gutierrez-Osuna" ],
      "venue" : null,
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2002
    }, {
      "title" : "Chaos based neural network optimization for concentration estimation of indoor air contaminants by an electronic nose",
      "author" : [ "L. Zhang", "F. Tian", "S. Liu", "J. Guo", "B. Hu", "Q. Ye", "L. Dang", "X. Peng", "C. Kadri", "J. Feng" ],
      "venue" : "Sensors and Actuators A: physical,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2013
    }, {
      "title" : "Multisensor microsystem for contaminants in air",
      "author" : [ "P. Althainz", "J. Goschnick", "S. Ehrmann", "H.J. Ache" ],
      "venue" : "Sensors and Actuators B: Chemical,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1996
    }, {
      "title" : "Calibration of a multivariate gas sensing device for atmospheric pollution measurement",
      "author" : [ "M. Kamionka", "P. Breuil", "C. Pijolat" ],
      "venue" : "Sensors and Actuators B: Chemical,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2006
    }, {
      "title" : "Odour classification system for continuous monitoring applications",
      "author" : [ "M. Trincavelli", "S. Coradeschi", "A. Loutfi" ],
      "venue" : "Sensors and Actuators B: Chemical,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2009
    }, {
      "title" : "Remote sensing of gas/odor source location and concentration distribution using mobile system",
      "author" : [ "H. Ishida", "T. Nakamoto", "T. Moriizumi" ],
      "venue" : "Sensors and Actuators B: Chemical,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 1998
    }, {
      "title" : "Approach for quantification of metal oxide type semiconductor gas sensors used for ambient air quality monitoring",
      "author" : [ "N. Masson", "R. Piedrahita", "M. Hannigan" ],
      "venue" : "Sensors and Actuators B: Chemical,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2015
    }, {
      "title" : "Reservoir computing compensates slow response of chemosensor arrays exposed to fast varying gas concentrations in continuous monitoring",
      "author" : [ "J. Fonollosa", "S. Sheik", "R. Huerta", "S. Marco" ],
      "venue" : "Sensors and Actuators B: Chemical,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2015
    }, {
      "title" : "On field calibration of an electronic nose for benzene estimation in an urban pollution monitoring scenario",
      "author" : [ "S. De Vito", "E. Massera", "M. Piga", "L. Martinotto", "G. Di Francia" ],
      "venue" : "Sensors and Actuators B: Chemical,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2008
    }, {
      "title" : "Bonavitacola. Field calibration of a cluster of low-cost available sensors for air quality monitoring. part a: Ozone and nitrogen dioxide",
      "author" : [ "L. Spinelle", "M. Gerboles", "M.G. Villani", "M. Aleixandre" ],
      "venue" : "Sensors and Actuators B: Chemical,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2015
    }, {
      "title" : "Development of a normalized multi-sensors system for low cost on-line atmospheric pollution detection",
      "author" : [ "Z. Al Barakeh", "P. Breuil", "N. Redon", "C. Pijolat", "N. Locoge", "J.P. Viricelle" ],
      "venue" : "Sensors and Actuators B: Chemical,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2016
    }, {
      "title" : "A wireless electronic nose system using a fe2o3 gas sensing array and least squares support vector regression",
      "author" : [ "K. Song", "Q. Wang", "Q. Liu", "H. Zhang", "Y. Cheng" ],
      "venue" : "Sensors (Basel, Switzerland),",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2011
    }, {
      "title" : "On time series features and kernels for machine olfaction",
      "author" : [ "S. Vembu", "A. Vergara", "M.K. Muezzinoglu", "R. Huerta" ],
      "venue" : "Sensors and Actuators B: Chemical,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2012
    }, {
      "title" : "Continuous prediction in chemoresistive gas sensors using reservoir computing",
      "author" : [ "S. Sheik", "S. Marco", "R. Huerta", "J. Fonollosa" ],
      "venue" : "Procedia Engineering,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2014
    }, {
      "title" : "Dynamic neural network architectures for on field stochastic calibration of indicative low cost air quality sensing systems",
      "author" : [ "E. Esposito", "S. De Vito", "M. Salvato", "V. Bright", "R.L. Jones", "O. Popoola" ],
      "venue" : "Sensors and Actuators B: Chemical,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2016
    }, {
      "title" : "Probabilistic gas quantification with mox sensors in open sampling systemsâĂŤa gaussian process approach",
      "author" : [ "J.G. Monroy", "A. Lilienthal", "J.L. Blanco", "J. Gonzalez-Jimenez", "M. Trincavelli" ],
      "venue" : "Sensors and Actuators B: Chemical,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2013
    }, {
      "title" : "Calibration of mox gas sensors in open sampling systems based on gaussian processes",
      "author" : [ "J.G. Monroy", "A. Lilienthal", "J.L. Blanco", "J. Gonzalez-Jimenez", "M. Trincavelli" ],
      "venue" : "In Sensors,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2012
    }, {
      "title" : "Gas concentration estimation in ternary mixtures with room temperature operating sensor array using tapped delay architectures",
      "author" : [ "S. De Vito", "A. Castaldo", "F. Loffredo", "E. Massera", "T. Polichetti", "I. Nasti", "P. Vacca", "L. Quercia", "G. Di Francia" ],
      "venue" : "Sensors and Actuators B: Chemical,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2007
    }, {
      "title" : "A time delay neural network for estimation of gas concentrations in a mixture",
      "author" : [ "M. Pardo", "G. Faglia", "G. Sberveglieri", "M. Corte", "F. Masulli", "M. Riani" ],
      "venue" : "Sensors and Actuators B: Chemical,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2000
    }, {
      "title" : "Modular learning architectures for large-scale sequential processing",
      "author" : [ "D. Verstraeten", "B. Schrauwen", "S. Dieleman", "P. Brakel", "P. Buteneers", "D. Pecevski. Oger" ],
      "venue" : null,
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2012
    }, {
      "title" : "A portable low-cost high density sensor network for air quality at london heathrow airport",
      "author" : [ "O. Popoola", "I. Mead", "V. Bright", "R. North", "G. Stewart", "P. Kayes", "R. Jones" ],
      "venue" : "AGU",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "As the AQMS are based on low-cost technologies, it will then possible to solve the sparsity problem that negatively affect the current monitoring strategies [2] [3] [4].",
      "startOffset" : 157,
      "endOffset" : 160
    }, {
      "referenceID" : 1,
      "context" : "As the AQMS are based on low-cost technologies, it will then possible to solve the sparsity problem that negatively affect the current monitoring strategies [2] [3] [4].",
      "startOffset" : 161,
      "endOffset" : 164
    }, {
      "referenceID" : 2,
      "context" : "As the AQMS are based on low-cost technologies, it will then possible to solve the sparsity problem that negatively affect the current monitoring strategies [2] [3] [4].",
      "startOffset" : 165,
      "endOffset" : 168
    }, {
      "referenceID" : 3,
      "context" : "To this regard, the Aveiro intercomparison exercise have set a a valuable point of reference evaluating the actual field performance of several multisensors, based on different classes of sensors, targeted to multiple pollutants and comparing their performances with EU DQOs [10].",
      "startOffset" : 275,
      "endOffset" : 279
    }, {
      "referenceID" : 4,
      "context" : "Specifically, chemical microsensors devices are, in general, subjected to interferent gases that modify their response to the target gas [11].",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 5,
      "context" : "For this reason, any attempt to rely on a univariate calibration procedure, neglecting interferents influence, is prone to failure [12].",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 6,
      "context" : "Besides, chemical microsensors response generally changes in time due to several effects including poisoning and environmental variables sensitivity [13].",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 7,
      "context" : "As a consequence, long term stability is a significant concern given the need to limit recalibration and maintenance burden on a pervasive network of possibly hundreds of AQMS [14].",
      "startOffset" : 176,
      "endOffset" : 180
    }, {
      "referenceID" : 8,
      "context" : "Adaptive/semi-supervised calibration schemes could represent one of the possible solutions to measurement drift issue [15].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 9,
      "context" : "This hampers the use of a single calibration function, thus requiring an ad hoc calibration procedure for each chemical multisensor device or, alternatively, the development of calibration transfer strategies [16], [17].",
      "startOffset" : 209,
      "endOffset" : 213
    }, {
      "referenceID" : 10,
      "context" : "This hampers the use of a single calibration function, thus requiring an ad hoc calibration procedure for each chemical multisensor device or, alternatively, the development of calibration transfer strategies [16], [17].",
      "startOffset" : 215,
      "endOffset" : 219
    }, {
      "referenceID" : 11,
      "context" : "A significant research effort is also needed to further develop our chemical sensor data fusion capabilities, in particular, to precisely reconstruct a 3D picture of pollution in both indoor or outdoor [18],[19],[20].",
      "startOffset" : 202,
      "endOffset" : 206
    }, {
      "referenceID" : 12,
      "context" : "A significant research effort is also needed to further develop our chemical sensor data fusion capabilities, in particular, to precisely reconstruct a 3D picture of pollution in both indoor or outdoor [18],[19],[20].",
      "startOffset" : 207,
      "endOffset" : 211
    }, {
      "referenceID" : 13,
      "context" : "A significant research effort is also needed to further develop our chemical sensor data fusion capabilities, in particular, to precisely reconstruct a 3D picture of pollution in both indoor or outdoor [18],[19],[20].",
      "startOffset" : 212,
      "endOffset" : 216
    }, {
      "referenceID" : 14,
      "context" : "On these basis, the emerging ”fog” and ”edge” computing frameworks are also pushing low semantic extraction computations towards the very edge of a sensing or control network [21].",
      "startOffset" : 175,
      "endOffset" : 179
    }, {
      "referenceID" : 7,
      "context" : "Several algorithms have been proposed during the last decade to implement the needed quantitative calibration of the chemical multisensor device but currently no effective comparison have been performed [14, 22, 11].",
      "startOffset" : 203,
      "endOffset" : 215
    }, {
      "referenceID" : 15,
      "context" : "Several algorithms have been proposed during the last decade to implement the needed quantitative calibration of the chemical multisensor device but currently no effective comparison have been performed [14, 22, 11].",
      "startOffset" : 203,
      "endOffset" : 215
    }, {
      "referenceID" : 4,
      "context" : "Several algorithms have been proposed during the last decade to implement the needed quantitative calibration of the chemical multisensor device but currently no effective comparison have been performed [14, 22, 11].",
      "startOffset" : 203,
      "endOffset" : 215
    }, {
      "referenceID" : 16,
      "context" : "The limited performance obtained by this strategy has led to the development of multivariate calibration strategies relying on the use of complex synthetic gas mixtures [23, 24] or on field recorded data [25] to cope with specificity and stability problems.",
      "startOffset" : 169,
      "endOffset" : 177
    }, {
      "referenceID" : 17,
      "context" : "The limited performance obtained by this strategy has led to the development of multivariate calibration strategies relying on the use of complex synthetic gas mixtures [23, 24] or on field recorded data [25] to cope with specificity and stability problems.",
      "startOffset" : 169,
      "endOffset" : 177
    }, {
      "referenceID" : 18,
      "context" : "The limited performance obtained by this strategy has led to the development of multivariate calibration strategies relying on the use of complex synthetic gas mixtures [23, 24] or on field recorded data [25] to cope with specificity and stability problems.",
      "startOffset" : 204,
      "endOffset" : 208
    }, {
      "referenceID" : 19,
      "context" : "Furthermore, most of the recent systems operate in open sampling scenarios in which sensors rarely reach a steady state [26].",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 20,
      "context" : "[27], for example, explicitly proposed the use of steady state responses as approximates of the real sensors responses in field deployments.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "The model can be calibrated with laboratory based sensor response recordings and tuned with on field recorded data to compensate for temperature interference [28].",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 22,
      "context" : "The last has the advantage of implicitly learning a dynamic model of the multisensor device, thereby reducing the effects on the performance of the slow sensors dynamic [29].",
      "startOffset" : 169,
      "endOffset" : 173
    }, {
      "referenceID" : 8,
      "context" : "Advanced learning techniques like semi-supervised learning proved to be promising in reducing the number of calibration samples needed as well as improving robustness to drift by complementing learning from un-labeled samples with the classic supervised approach [15].",
      "startOffset" : 263,
      "endOffset" : 267
    }, {
      "referenceID" : 23,
      "context" : ", for example, have shown promising results using hourly averaged sensors data from field measurements; this work showed the possibility to let them learn a nonlinear multiple regression model for benzene concentration estimation [30].",
      "startOffset" : 230,
      "endOffset" : 234
    }, {
      "referenceID" : 5,
      "context" : "The effects of the changes of pollutants joint concentrations distribution, which can be due on seasonal effects, on the performances were also highlighted [12].",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 24,
      "context" : ", have confirmed that shallow neural networks, can significantly outperform linear univariate and multivariate models, thus highlighting the possibility to exploit multivariate information to reach EU set DQOs [31].",
      "startOffset" : 210,
      "endOffset" : 214
    }, {
      "referenceID" : 25,
      "context" : "[32] proposed a fuzzy logic based strategy for differentiating among different type of air pollution and estimating a pollution index using a neural network.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 26,
      "context" : "[33] used a least square SVM for calibrating a wireless chemical multi-sensors system which targeted explosive gases (methane, hydrogen) concentration estimations.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 27,
      "context" : "[34] proposed the use of time series kernel based SVMs for enhancing the identification capabilities of a small network of pervasive open sampling multi-sensors systems.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "[29], [35], investigated the use of reservoir computing strategies.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 28,
      "context" : "[29], [35], investigated the use of reservoir computing strategies.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 29,
      "context" : "[36], investigated the performance of the tapped delay approach with real world data, recorded with open sampling systems.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 30,
      "context" : "described an interesting probabilistic quantification approach based on the well-known Gaussian processes (GP) framework [37].",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 31,
      "context" : "In their first work [38], instead of attempting to describe sensors dynamics, they propose the prediction of the uncertainty caused by slow sensors operation in an open sampling scenario.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 32,
      "context" : "Neural networks have been used in this scenario since the last decade [39, 40].",
      "startOffset" : 70,
      "endOffset" : 78
    }, {
      "referenceID" : 33,
      "context" : "Neural networks have been used in this scenario since the last decade [39, 40].",
      "startOffset" : 70,
      "endOffset" : 78
    }, {
      "referenceID" : 22,
      "context" : "In this work we foster the use the Reservoir computing approach, originating from the echostate network paradigm and introduced in chemical multisensors field by [29].",
      "startOffset" : 162,
      "endOffset" : 166
    }, {
      "referenceID" : 34,
      "context" : "we used the open-source Python library Oger [41] for the implementation of the algorithms developed to compute gas concentration estimations.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 35,
      "context" : "The comparison was carried out taking into account three different datasets: the first was from a specific deployment of multi-sensory devices (SNAQ systems, see [42] for detailed description of the deployment), developed by the Centre for Atmospheric Sciences (CAS), Chemistry Department, University of Cambridge (UK).",
      "startOffset" : 162,
      "endOffset" : 166
    }, {
      "referenceID" : 23,
      "context" : "[30].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 1,
      "context" : "In particular, HNN varied in the [3, 5, 7, 10, 15, 20] set while EN ∈ [100, 200, 300, 400, 500, 600, 900].",
      "startOffset" : 33,
      "endOffset" : 54
    }, {
      "referenceID" : 3,
      "context" : "In particular, HNN varied in the [3, 5, 7, 10, 15, 20] set while EN ∈ [100, 200, 300, 400, 500, 600, 900].",
      "startOffset" : 33,
      "endOffset" : 54
    }, {
      "referenceID" : 8,
      "context" : "In particular, HNN varied in the [3, 5, 7, 10, 15, 20] set while EN ∈ [100, 200, 300, 400, 500, 600, 900].",
      "startOffset" : 33,
      "endOffset" : 54
    }, {
      "referenceID" : 13,
      "context" : "In particular, HNN varied in the [3, 5, 7, 10, 15, 20] set while EN ∈ [100, 200, 300, 400, 500, 600, 900].",
      "startOffset" : 33,
      "endOffset" : 54
    }, {
      "referenceID" : 3,
      "context" : "• RU ∈ [10, 20, 30, 50, 100, 150, 200, 250].",
      "startOffset" : 7,
      "endOffset" : 43
    }, {
      "referenceID" : 13,
      "context" : "• RU ∈ [10, 20, 30, 50, 100, 150, 200, 250].",
      "startOffset" : 7,
      "endOffset" : 43
    }, {
      "referenceID" : 23,
      "context" : "• RU ∈ [10, 20, 30, 50, 100, 150, 200, 250].",
      "startOffset" : 7,
      "endOffset" : 43
    }, {
      "referenceID" : 29,
      "context" : "only for NNs [36] using a single dataset.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 34,
      "context" : "Equally, We want to thank the authors of [41] and [29] for having shared, respectively, the basic framework and specific code used for developing our own Reservoir Computing implementation.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 22,
      "context" : "Equally, We want to thank the authors of [41] and [29] for having shared, respectively, the basic framework and specific code used for developing our own Reservoir Computing implementation.",
      "startOffset" : 50,
      "endOffset" : 54
    } ],
    "year" : 2017,
    "abstractText" : "Chemical multisensor devices need calibration algorithms to estimate gas concentrations. Their possible adoption as indicative air quality measurements devices poses new challenges due to the need to operate in continuous monitoring modes in uncontrolled environments. Several issues, including slow dynamics, continue to affect their real world performances. At the same time, the need for estimating pollutant concentrations on board the devices, especially for wearables and IoT deployments, is becoming highly desirable. In this framework, several calibration approaches have been proposed and tested on a variety of proprietary devices and datasets; still, no thorough comparison is available to researchers. This work attempts a benchmarking of the most promising calibration algorithms according to recent literature with a focus on machine learning approaches. We test the techniques against absolute and dynamic performances, generalization capabilities and computational/storage needs using three different datasets sharing continuous monitoring operation methodology. Our results can guide researchers and engineers in the choice of optimal strategy. They show that non-linear multivariate techniques yield reproducible results, outperforming linear approaches. Specifically, the Support Vector Regression method consistently shows good performances in all the considered scenarios. We highlight the enhanced suitability of shallow neural networks in a trade-off between performance and computational/storage needs. We confirm, on a much wider basis, the advantages of dynamic approaches with respect to static ones that only rely on instantaneous sensor array response. The latter have been shown to be best choice whenever prompt and precise response is needed.",
    "creator" : "LaTeX with hyperref package"
  }
}