{"conference": "ICLR 2017 conference submission", "title": "Cooperative Training of Descriptor and Generator Networks", "abstract": "This paper studies the cooperative training of two probabilistic models of signals such as images. Both models are parametrized by convolutional neural networks (ConvNets). The first network is a descriptor network, which is an exponential family model or an energy-based model, whose feature statistics or energy function are defined by a bottom-up ConvNet, which maps the observed signal to the feature statistics. The second network is a generator network, which is a non-linear version of factor analysis. It is defined by a  top-down ConvNet, which maps the latent factors to the observed signal. The maximum likelihood training algorithms of both the descriptor net and the generator net are in the form of alternating back-propagation, and both algorithms involve Langevin sampling. We observe that the two training algorithms can cooperate with each other by jump-starting each other\u2019s Langevin sampling, and they can be seamlessly interwoven into a CoopNets algorithm that can train both nets simultaneously.", "reviews": [{"is_meta_review": true, "comments": "This paper proposed a new joint training scheme for two probabilistic models of signals (e.g. images) which are both deep neural network based and are termed generator and descriptor networks.  In the new scheme, termed cooperative training, the two networks train together and assist each other: the generator network provides samples that work as initial samples for the descriptor network, and the descriptor network updates those samples to help guide training of the generator network.\n\nThis is an interesting approach for coupling the training of these two models.  The paper however is quite weak on the empirical studies.  In particular:\n- The training datasets are tiny, from sets of 1 image to 5-6.  What is the reason for not using larger sets?  I think the small datasets are leading to over training and are really masking the true value of the proposed cooperative training approach.\n- For most of the experiments presented in the paper it is hard to assess the specific value brought by the proposed cooperative training approach because baseline results are missing.  There are comparisons provided for face completion experiments - but even there comparisons with descriptor or generator network trained separately or with other deep auto-encoders are missing.  Thus it is hard to conclude if and how much gain is obtained by cooperative training over say individually training the descriptor and generator networks.\n\nAnother comment is that in the \u201crelated work\u201d section, I think relation with variational auto encoders (Kingma and Welling 2013) should be included.\n\nDespite limitations mentioned above, I think the ideas presented in the paper are intuitively appealing and worth discussing at ICLR.  Paper would be considerably strengthened by adding more relevant baselines and addressing the training data size issues.", "IS_META_REVIEW": true}, {"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "While the paper may have an interesting theoretical contribution, it seems to greatly suffer from problems in the presentation: the basic motivation is of the system is hardly mentioned in the introduction, and the conclusion does not explain much either. I think the paper should be rewritten, and, as some of the reviewers point out, the experiments strengthened before it can be accepted for publication.\n \n (I appreciate the last-minute revisions by the authors, but I think it really came too late, 14th/21st/23rd Jan, to be taken seriously into account in the review process.)", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"TITLE": "Second Revision", "OTHER_KEYS": "Yang Lu", "comments": "We have added three new results: \n\n(1) Synthesis results by GAN for comparison. (Figure 6(b))\n(2) Synthesis results by algorithm G alone for comparison. (Figure 6(a))\n(3) Synthesis results on 224x224 resolution. (Figure 8)", "IS_META_REVIEW": false, "DATE": "21 Jan 2017", "is_meta_review": false}, {"TITLE": "Reply to the Reviewers", "OTHER_KEYS": "Yang Lu", "comments": "Dear Reviewers, \n\nThank you for reviewing our paper and thank you for your comments! \n\nWe have uploaded a revision for your consideration. \n\nBecause the reviewers questioned the small training sizes in our experiments on textures and objects, we have opted to replace these experiments by a new experiment on 14 categories from standard datasets such as ImageNet and MIT place, where each training set consists of 1000 images randomly sampled from the category. Please see the experiment section as well as the appendix for the synthesis results. These are all we have got, without cheery picking. As can be seen, our method can generate meaningful and varied images. We haven\u2019t had time to tune the code. In fact, we had to recruit a new author (Ms. Ruiqi Gao) to help us run the code due to our various time constraints. With more careful tuning (including increasing image resolution), we expect to further improve the quality of synthesis. \n\nAbout the comparison with separate training method by either Algorithm D for descriptor or Algorithm G for generator individually, the separate training methods currently cannot produce synthesis results that are comparable to those produced by the cooperative training. This illustrates the advantage of cooperative training over separate training. In fact, the main motivation for this work is to overcome the difficulty with separate training by cooperative training.  \n\nWe have added a quantitative comparison with GAN for the face completion experiment, because our method is intended as an alternative to GAN.  Our original code was written in MatConvNet. We moved to TensorFlow in order to use existing code of GAN. We then rewrote Algorithm G in TensorFlow for image completion. GAN did not do well in this experiment. We are still checking and tuning our code to improve GAN performance. \n\nWe want to emphasize that we are treating the following two issues separately:\n\n(1) Train-test split and quantitative evaluation of generalizability. \n(2) Image synthesis judged qualitatively. \n\nWhile the face completion experiment is intended to address (1), the synthesis experiment is intended to address (2).  In fact, the generator network captures people\u2019s imagination mainly because of (2) (at least this is the case with ourselves), and some GAN papers are more qualitative than quantitative. \n\nWe will continue to work on experiments, to further address the questions raised by the reviewers and to continue to strengthen the quantitative side. \n\nWe have also made some minor changes to incorporate the reviewers\u2019 suggestions on wording and additional references. \n\nAs to the energy function, in particular, f(Y; W), for the descriptor, it is defined by a bottom-up ConvNet that maps the image Y to a score (very much like a discriminator), and we give the details of this ConvNet in the experiment section. We feel we made this clear in the original version. \n\nAs to equation (8), we have expanded the derivation. Equations (16) and (17) are about finite step Langevin dynamics. \n\nFinally please allow us to make some general comments regarding our paper. Our paper addresses the core issue of this conference, i.e., learning representations in the form of probabilistic generative models. There are two types of such papers: \n\n(1) Build on the successes of GAN. \n(2) Explore new connections and new routes. \n\nWe believe that papers in these two categories should be judged differently. Our paper belongs to category (2). It explores the connection between undirected model (descriptor) and directed model (generator). It also explores the connection between MCMC sampling (descriptor) and ancestral sampling (generator). Furthermore, it explores the new ground where two models interact with each other via synthesized data. We have also tried hard to gain a theoretical understanding of our method in appendix. \n\nThere have been a lot of papers in category (1) recently. We hope that the conference will be more open to the relatively fewer papers in category (2). In fact we are heartened that all three reviewers find our work interesting, and we can continue to improve our experiments. \n\nThanks for your consideration, and thanks for your comments that have helped us improve our work. \n", "IS_META_REVIEW": false, "DATE": "14 Jan 2017", "is_meta_review": false}, {"TITLE": "-", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "IS_META_REVIEW": false, "DATE": "20 Dec 2016", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Official Review", "is_meta_review": false, "comments": "The authors proposes an interesting idea of connecting the energy-based model (descriptor) and \nthe generator network to help each other. The samples from the generator are used as the initialization \nof the descriptor inference. And the revised samples from the descriptor is in turn used to update\nthe generator as the target image. \n\nThe proposed idea is interesting. However, I think the main flaw is that the advantages of having that \narchitecture are not convincingly demonstrated in the experiments. For example, readers will expect \nquantative analysis on how initializing with the samples from the generator helps? Also, the only \nquantative experiment on the reconstruction is also compared to quite old models. Considering that \nthe model is quite close to the model of Kim & Bengio 2016, readers would also expect a comparison \nto that model. \n\n** Minor\n- I'm wondering if the analysis on the convergence is sound when considering the fact that samples \nfrom SGLD are biased samples (with fixed step size). \n- Can you explain a bit more on how you get Eqn 8? when p(x|y) is also dependent on W_G?\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Interesting idea, but improperly evaluated", "is_meta_review": false, "comments": "This paper introduces CoopNets, an algorithm which trains a Deep-Energy Model (DEM, the \u201cdescriptor\u201d) with the help of an auxiliary directed bayes net, e.g. \u201cthe generator\u201d. The descriptor is trained via standard maximum likelihood, with Langevin MCMC for sampling. The generator is trained to generate likely samples under the DEM in a single, feed-forward ancestral sampling step. It can thus be used to shortcut expensive MCMC sampling, hence the reference to \u201ccooperative training\u201d.\n\nThe above idea is interesting and novel, but unfortunately is not sufficiently validated by the experimental results. First and foremost, two out of the three experiments do not feature a train /test split, and ignore standard training and evaluation protocols for texture generation (see [R1]). Datasets are also much too small. As such these experiments only seem to confirm the ability of the model to overfit. On the third in-painting tasks, baselines are almost non-existent: no VAEs, RBMs, DEM, etc which makes it difficult to evaluate the benefits of the proposed approach.\n\nIn a future revision, I would also encourage the authors to answer the following questions experimentally. What is the impact of the missing rejection step in Langevin MCMC (train with, without ?). What is the impact of the generator on the burn-in process of the Markov chain (show sample auto-correlation). How bad is approximation of training the generator from ({\\tilde{Y}, \\hat{X}) instead of ({\\tilde{Y}, \\tilde{X}) ? Run comparative experiments.\n\nThe paper would also greatly benefit from a rewrite focusing on clarity, instead of hyperbole (\u201cpioneering work\u201d in reference to closely related, but non-peer reviewed work) and prose (\u201ctale of two nets\u201d). For example, the authors fail to specify the exact form of the energy function: this seems like a glaring omission.\n\nPROS:\n+ Interesting and novel idea\nCONS:\n- Improper experimental protocols\n- Missing baselines\n- Missing diagnostic experiments\n\n[R1] Heess, N., Williams, C. K. I., and Hinton, G. E. (2009). Learning generative texture models with extended fields of-experts.", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "No Title", "is_meta_review": false, "comments": "This paper proposed a new joint training scheme for two probabilistic models of signals (e.g. images) which are both deep neural network based and are termed generator and descriptor networks.  In the new scheme, termed cooperative training, the two networks train together and assist each other: the generator network provides samples that work as initial samples for the descriptor network, and the descriptor network updates those samples to help guide training of the generator network.\n\nThis is an interesting approach for coupling the training of these two models.  The paper however is quite weak on the empirical studies.  In particular:\n- The training datasets are tiny, from sets of 1 image to 5-6.  What is the reason for not using larger sets?  I think the small datasets are leading to over training and are really masking the true value of the proposed cooperative training approach.\n- For most of the experiments presented in the paper it is hard to assess the specific value brought by the proposed cooperative training approach because baseline results are missing.  There are comparisons provided for face completion experiments - but even there comparisons with descriptor or generator network trained separately or with other deep auto-encoders are missing.  Thus it is hard to conclude if and how much gain is obtained by cooperative training over say individually training the descriptor and generator networks.\n\nAnother comment is that in the \u201crelated work\u201d section, I think relation with variational auto encoders (Kingma and Welling 2013) should be included.\n\nDespite limitations mentioned above, I think the ideas presented in the paper are intuitively appealing and worth discussing at ICLR.  Paper would be considerably strengthened by adding more relevant baselines and addressing the training data size issues.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Question about Langevin dynamics used to compute expectations", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_META_REVIEW": false, "DATE": "09 Dec 2016", "is_meta_review": false}, {"TITLE": "Train / test split", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "is_meta_review": false}], "SCORE": 6, "authors": "Jianwen Xie, Yang Lu, Ruiqi Gao, Song-Chun Zhu, Ying Nian Wu", "KEYWORDS": "Cooperative training of the descriptor and generator networks by coupling two maximum likelihood learning algorithms.", "accepted": false, "id": ""}
