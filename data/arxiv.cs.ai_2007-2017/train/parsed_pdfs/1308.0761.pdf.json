{
  "name" : "1308.0761.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "SAT@home", "biclop.rambler@yandex.ru,", "zaikin.icc@gmail.com", "SAT@home", "SAT@home", "Einstein@home," ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n30 8.\n07 61\nv1 [\ncs .A\nI] 4\nA ug\n2 01\nKeywords: volunteer computing, BOINC, partitioning, Monte Carlo method, predictive function, tabu search, A5/1, Bivium, SAT@home"
    }, {
      "heading" : "1 Introduction",
      "text" : "In recent years, solving large scale computational problems via volunteer computing projects gained a lot of popularity. Nowadays there are about 70 active projects, the majority of them are based on the BOINC platform [1]. Total performance of all BOINC projects is more than 7 petaflops. The most important results obtained in volunteer computing projects include the discovery of new pulsars in the Einstein@home, and of large prime numbers of a special kind in the PrimeGrid project.\nVolunteer computing is a type of distributed computing. Actually a volunteer computing project is a desktop grid constructed from PCs of private persons called volunteers. It is important to note that volunteers contribute resources of their computers for free, so they assume no obligation to the organizers of the project. Therefore, a list of active project participants (and consequently a structure of a desktop grid) can vary greatly during the computational experiment. PCs of volunteers receive tasks from server, process them and send results back\nto the server. In volunteer projects tasks should be solved independently because volunteer PCs can communicate only with project server, moreover do it rarely and irregularly. In general, for a project to work effectively it should contain the following components: stable 24/7 server, internet site with the goal of the project clearly pointed out and a number of client applications for various computing platforms.\nIt is well-known that a lot of important combinatorial problems (for example from areas of formal verification, planning or bioinformatics) can be effectively reduced to SAT [2]. Despite quite significant progress in the development of SAT solvers there remain hard SAT instances that cannot be solved without the use of large amounts of computational resources of various types. Therefore, in our opinion, it is essential to use volunteer computing for solving hard SAT instances. We develop and maintain a volunteer computing project SAT@home [31] specially designed for solving SAT problems via partitioning approach.\nIn volunteer computing projects, excluding the projects with ambitious goals like SETI@home, it is very important to know how much time it will take to solve a particular problem. It is considered to be normal if a time estimation involves months or even years. Knowledge about this time provides volunteers with an additional motivation by showing how effectively they progress in solving the problem.\nFurther we present a Monte Carlo method of estimating time to solve SAT problems in distributed computing environments. For a given solver and a fixed partitioning of the original SAT problem this method statistically estimates values of several qualitative parameters of the chosen partitioning. One of these parameters corresponds to the time required to solve the considered SAT problem. To automatically search for a partitioning with minimal estimation of time we use a tabu search based algorithm. Our method was used to estimate the time required for solving of several hard SAT instances in the SAT@home project.\nA brief outline of the paper is given below. In the next section we present a Monte Carlo method that makes it possible to estimate time required for solving of SAT problems. Section 3 contains some implementation details and the results of computational experiments that show practical applicability of the proposed method. In Section 4 we briefly describe the volunteer computing project SAT@home and present some of the results obtained in this project. In Section 5 we consider related works."
    }, {
      "heading" : "2 Monte Carlo method for estimating time of solving of SAT problem via partitioning approach",
      "text" : "In [15,16,17,18] various approaches to partitioning SAT problems were studied. Further we will use the notation of [15]. Consider an arbitrary SAT-problem for CNF C over a set of Boolean variables X . Partitioning of C is a set of formulae\nC · Fi, i ∈ {1, . . . , S} ,\nsuch that for any i, j : i 6= j, formula C · Fi · Fj is unsatisfiable and\nC ≡ C · F1 ∨ . . . ∨ C · FS .\nWhen one has a partitioning of the original SAT problem, satisfiability problems for C · Fj , j ∈ {1, . . . , S}, can be solved independently in a distributed computing environment. There exist various partitioning techniques. For example, one can construct {Fj}Sj=1 using a scattering procedure [16], a guiding path solver [34] or a lookahead solver [13], [17]. Unfortunately, in general case for these partitioning methods it is not possible to estimate the total time required for solving the original SAT problem.\nHowever, a partitioning method that makes it possible to construct such estimations was used in a number of papers about solving cryptanalysis problems via SAT approach [10], [22], [29], [30]. According to this method, from the set of variables X of an original CNF C we choose a subset X̃ and consider a family consisting of all the CNFs that are produced from C by substituting all the 2|X̃| truth assignments of variables from X̃. It is clear, that in this case the formula F1 ∨ F2,∨ . . . ,∨FS is a DNF in which S = 2|X̃|, and at the first glance it may look like a significant drawback of the approach. On the other hand one can solve N , N << 2|X̃| SAT problems for CNFs randomly chosen from the family considered, calculate the average time of their solving and, based on this information, estimate the total time required for solving the original SAT problem. Below we show that this method can be formally justified using the Monte Carlo approach. We also propose an algorithm that searches for a partitioning with a minimal estimated time and show practical applicability of this procedure to the inversion problems of several cryptographic functions."
    }, {
      "heading" : "2.1 Monte Carlo approach to statistical estimation of quality of partitioning of SAT problem",
      "text" : "Consider a SAT problem for an arbitrary CNF formula C = C(X) over the set of Boolean variablesX = {x1, . . . , xn}. We refer to an arbitrary X̃ = {xi1 , . . . , xid}, X̃ ⊆ X , {i1, . . . , id} ⊆ {1, . . . , n}, as a decomposition set for the SAT problem considered. Further a CNF formula obtained as a result of substituting a truth assignment xi1 = α1, . . . , xid = αd to C is denoted as C [ X̃/(α1, . . . , αd) ] . A set of CNFs\n∆ ( C, X̃ ) = { C [ X̃/(α1, . . . , αd) ]}\n(α1,...,αd)∈{0,1}d\nis called a decomposition family produced by X̃. It is easy to see that in accordance with the above ∆ ( C, X̃ ) is a partitioning of C.\nConsider some algorithm A solving SAT. In the remainder of the paper we presume that A is complete, i.e. its runtime is finite for an arbitrary input. We also presume that A is a deterministic algorithm that does not involve randomization. We need the latter condition to use the Monte Carlo approach correctly.\nWe denote an amount of time required by A to solve all the CNFs from\n∆ ( C, X̃ )\nas tA\n( C, X̃ ) . Below we mainly concentrate on estimation problem\nfor tA\n( C, X̃ ) .\nLet’s define a uniform distribution on the set {0, 1}d. With each randomly chosen vector (α1, . . . , αd) from {0, 1}d we associate a value\nξA (α1, . . . , αd) ,\nthat is equal to the time required by algorithm A to solve SAT for formula C [ X̃/(α1, . . . , αd) ] . Therefore, a random variable\nξA\n( C, X̃ )\n= {ξA (α1, . . . , αd)}(α1,...,αd)∈{0,1}d\nwith some probability distribution is defined. Due to the completeness of A, vari-\nable ξA\n( C, X̃ ) has a finite expected value E [\nξA\n( C, X̃ )] and a finite variance\nVar (\nξA\n( C, X̃ )) . It is important, that since we presume that A is a determin-\nistic algorithm, then N independent observations of values of ξA\n( C, X̃ ) can be\nconsidered as a single observation of N independent random variables with the\nsame distribution as ξA\n( C, X̃ ) .\nIt is not difficult to prove that\ntA\n( C, X̃ ) = 2d · E [\nξA\n( C, X̃ )] . (1)\nBelow we refer to X̃ ∈ 2X with a minimal value tA ( C, X̃ ) as an optimal de-\ncomposition set.\nTo estimate the value of E [\nξA\n( C, X̃ )] we will use the Monte Carlo method\n[20], [24]. According to this method, in order to approximately calculate the expected value E[ξ] of an arbitrary random variable ξ a probabilistic experiment is used, that consists of N independent observations of values of ξ. Let ξ1, . . . , ξN be results of the corresponding observations. They can be considered as a single observation of N independent random variables with the same distribution, i.e. the following equalities hold:\nE[ξ] = E[ξ1] = . . . = E[ξN ],Var(ξ) = Var(ξ1) = . . . = Var(ξN ).\nIf E[ξ] and Var(ξ) are both finite then from Central Limit Theorem [11] we have the main formula of the Monte Carlo method\nPr\n\n\n\n∣ ∣ ∣ ∣ ∣ ∣ 1 N · N ∑\nj=1\nξj − E[ξ]\n∣ ∣ ∣ ∣ ∣ ∣ < δγ · σ√ N    = γ. (2)\nHere σ = √\nVar(ξ) stands for a standard deviation, γ — for a confidence level, γ = Φ(δγ), where Φ(·) is the normal cumulative distribution function. It means that under the considered assumptions the value\nξ̄ = 1\nN ·\nN ∑\nj=1\nξj\nis a good approximation of E[ξ], when the number of observations N is large enough. For any given N the quality of this approximation depends on the value of Var(ξ). In practice to estimate Var(ξ) an unbiased sample variance\ns2 = 1 N − 1 · N ∑\nj=1\n( ξj − ξ̄ )2\n(3)\nis used. In this case instead of (2) a following formula is applied [32]\nPr\n\n\n\n∣ ∣ ∣ ∣ ∣ ∣ 1 N · N ∑\nj=1\nξj − E[ξ]\n∣ ∣ ∣ ∣ ∣ ∣ < tγ,N−1 · s√ N    = γ, (4)\nwhere tγ,N−1 is a quantile of a Student’s distribution with N − 1 degrees of freedom that corresponds to the confidence level γ . If for example γ = 0, 999 and N ≥ 10000 then tγ,N−1 ≈ 3.29.\nIn our case it is important to note that N can be significantly less than 2d. It means that the preprocessing stage can be used to estimate the total time, required for processing the whole decomposition family ∆ ( C, X̃ ) .\nSo the process of approximate calculating of value (1) for a given X̃ is as follows. We randomly choose N truth assignments of variables from X̃ and denote this set as:\nΘ ( X̃ ) = {(\nα11, . . . , α 1 d\n) , . . . , ( αN1 , . . . , α N d )} . (5)\nConsider random variables\nξj = ξA\n(\nαj1, . . . , α j d\n)\n, j = 1, . . . , N,\nand calculate the value\nFA,C\n( X̃ ) = 2d ·\n\n\n1\nN ·\nN ∑\nj=1\nξj\n\n .\nBy the above if N is large enough then the value FA,C\n( X̃ ) can be considered\nas a good approximation of (1). Below we refer to function FA,C (·) as a predictive function. Note that values of the predictive function can be calculated using a usual PC or a computing cluster, given a qualitative random number generator. Therefore, instead of searching for an optimal decomposition set one can search for decomposition set with minimal value of predictive function."
    }, {
      "heading" : "2.2 Algorithm for predictive function minimization",
      "text" : "Despite quite a natural formulation, the problem described has some specific features.\n1. The value of FA,C\n( X̃ ) cannot be effectively calculated for an arbitrary\nX̃ since it is easy to construct such small X̃ that the time required for\ncalculating FA,C\n( X̃ ) is comparable with the time required for solving the\noriginal SAT problem.\n2. The value of FA,C\n( X̃ ) represents the reaction of computing environment to\nthe corresponding decomposition set. Therefore, methods relying on the analytical properties of an objective function cannot be applied to the problem of minimizing FA,C (·).\nBecause of these features the most natural minimization strategy for FA,C (·) is a strategy of “successive improvements”. It implies that at the first step we\nshould construct an initial decomposition set X̃0 for which the value FA,C\n(\nX̃0\n)\ncan be calculated in a short time. After this, we try to improve this value by observing the neighbourhood of a point corresponding to X̃0 in some search space ℜ . Thus, the process of minimization consists in moving from point to point looking for the one with minimal FA,C (·).\nIn some minimization algorithms it is allowed to calculate a value of an objective function in the same point of a search space more than once. It is feasible for problems where the objective function can be calculated easily in any point of ℜ. Due to the reasons mentioned above, in our case it is undesirable. Ideally the calculation of FA,C (·) in an arbitrary point of ℜ shouldn’t be performed more than once. In the algorithm described below we keep all points for which the value of FA,C (·) is already calculated. It naturally corresponds to the basic idea of tabu search (TS) [12].\nLet’s start from defining the search space. Note that an arbitrary set X̃ ∈ 2X can be described using a Boolean vector\nχ ( X̃ )\n= χ = (χ1, . . . , χn) , χi =\n{\n1, xi ∈ X̃ 0, xi /∈ X̃ , i = 1, . . . , n. (6)\nThen it is convenient to define the search space ℜ as an n-dimensional Boolean hypercube En = {0, 1}n. For an arbitrary point χ ∈ En a neighbourhood Nρ (χ) of radius ρ is defined as a set of vectors χ′ from En such that\ndistH (χ ′, χ) ≤ ρ,\nwhere distH (χ1, χ2) stands for the Hamming distance between χ1 and χ2. Punctured neighbourhood of point χ is a set\nN ∗ρ = Nρ (χ) \\{χ}.\nFurther by F (χ) we denote FA,C\n( X̃ ) , where χ = χ ( X̃ ) is in accordance with\n(6). We consider the problem of search for a minimum of the function F (·) over En.\nA simple local search [25] stops after finding a local extremum. There are various techniques that make it possible to escape from such points. In accordance with the main TS principle we should move from the local extremum to some point that is not included in a current tabu list T . It may occur that the value of the objective function in a new point is worse than its best known value. After we move to a new point a local search stage is launched in a punctured neighbourhood of this point.\nTypes of constraints in the tabu list and ways of their usage differ from problem to problem and are determined based on a problem’s individual features. In our approach it is convenient to keep all the points we have checked in the form of two lists of constraints: L1 and L2. List L1 stores points χ ∈ En such that for all χ′ ∈ Nρ (χ) we have already calculated F (χ′). In list L2 we keep points χ ∈ En such that F (χ) have been calculated and there exists χ′ ∈ Nρ (χ) for which we haven’t calculated F (χ′) yet. To reflect this information every point in L2 is represented by two vectors: a vector χ ∈ En and a Boolean vector θ (χ) of length ρ ∑\ni=1\n(\nn i\n)\n. Vector θ (χ) stores information about points from Nρ (χ): components of θ (χ) equal to 1 correspond to the points in which the calculation of F (·) has already been performed (other components are equal to 0). Vector θ (χ) is referred to as a neighbourhood vector of χ.\nBelow we give a brief outline of our algorithm. The algorithm is split into iterations. Denote iteration with number t ≥ 0 as I (t) and the best known value of F (·) obtained at this iteration as Ψt . Also, at the start of every iteration we have a current point χct that is obtained as a result of the previous iteration. The algorithm starts from some point χ0 in which the value of predictive function can be calculated fast. We will discuss possible ways of choosing χ0 later. At the start of I (0) we assume that χc0 = χ0, Ψ0 = F (χ0), L2 = {χ0}, L1 = ∅. Further we describe the transition from I (t), t ≥ 0, to I (t+ 1).\nAt the beginning of I (t) we know point χct and its neighbourhood vector θ (χct). We start the local search stage at N ∗ρ (χct). In particular we consequently check points χ′ ∈ N ∗ρ (χct) that correspond to zero components of θ (χct). We calculate value F (χ′) and add χ′ to L2. Every time when we add a new point χ′ to L2 we have to modify the constraints in this list: for all χ\n′′ ∈ L2 such that distH (χ\n′, χ′′) ≤ ρ we set a component of θ (χ′′), corresponding to χ′, to be 1. Neighbourhood vector θ (χ′) is modified accordingly. If for some point from L2 its neighbourhood vector consists only of 1s then this point is removed from L2 and added to L1. If for all χ\n′ ∈ N ∗ρ (χct) : F (χ′) ≥ Ψt−1 then Ψt = Ψt−1, and as χct+1 we choose a point from L2 according to some heuristic, iteration I (t) ends and iteration I (t+ 1) starts. If F (χ′) < Ψt−1 and θ (χ\n′) contains at least one 0 then Ψt = F (χ ′), χct+1 = χ ′, iteration I (t) ends and iteration I (t+ 1) starts. If F (χ′) < Ψt−1 but θ (χ ′) contains only 1s then Ψt = F (χ ′), as χct+1 we choose\na point from L2 according to some heuristic, iteration I (t) ends and iteration I (t+ 1) starts.\nThe algorithm stops if either list L2 becomes empty, i.e. all the points from this list are moved to L1, or the time limit is exceeded. It is easy to see that during the work of the algorithm a calculation of the value of the predictive function in an arbitrary point of En is performed at most once. A decomposition set that corresponds to the best known value of the predictive function at the moment when algorithm stops we denote as X̃∗.\nIn computational experiments (see Section 3) we used ρ = 1 and a following heuristic to choose a current point from L2: it is randomly chosen from the points with the minimal Hamming distance to the point with the best known value of the predictive function."
    }, {
      "heading" : "2.3 Additional improvements of the predictive function minimization algorithm",
      "text" : "Here we present a technique that makes it possible to significantly speed up the algorithm proposed.\nIt is easy to see that the majority of time required to compute FA,C\n( X̃ ) is\nspent on calculation of values ξj = ξA\n(\nαj1, . . . , α j d\n)\n, j = 1, . . . , N . However, the\ncalculation of FA,C\n( X̃ ) can be organized as a following iterative process:\nF 1A,C\n( X̃ ) = 2d\nN · ξ1, F jA,C\n( X̃ )\n= F j−1A,C\n( X̃ ) + 2d\nN · ξj , j = 2, . . . , N. (7)\nThus,\nFA,C\n( X̃ )\n= FNA,C\n( X̃ ) .\nIt should be noted that the order of summation in (7) is insignificant. Assume that we need to calculate the value of FA,C (·) in some point χ ( X̃ ′ ) and Ψ is our current best known value. Suppose that for some k < N values ξi1 , . . . , ξik , {i1, . . . , ik} ⊂ {1, . . . , N}, have already been obtained and inequality\n2d N ·\nk ∑\nr=1\nξir > Ψ\nholds. Then it is clear that FA,C\n( X̃ ′ ) > Ψ . In this situation we can interrupt\nthe process of calculation of FA,C\n( X̃ ′ ) and move to the next point of the search\nspace. We note that in this case the correctness of the algorithm described is not affected.\nAn ability of the algorithm proposed to construct a good decomposition set in relatively short amount of time greatly depends on the choice of X̃0. As we\nalready mentioned earlier we should choose X̃0 in such a way that FA,C\n(\nX̃0\n)\nis\ncalculated fast. In a general case we can always assume X̃0 = X . However, for many SAT problems it is possible to choose X̃0 ⊂ X such that ∣ ∣ ∣X̃0 ∣ ∣\n∣ << |X | and, nevertheless, the value FA,C ( X̃0 ) can be computed effectively. In particular for\na SAT problem that encodes the inversion problem of a cryptographic function we can choose X̃0 as a corresponding Strong Unit Propagation Backdoor Set (SUPBS, [19]). In this case we also can search for X̃∗ only among the subsets of X̃0 , X̃0 ⊂ X , decreasing the power of the search space to 2|X̃0|."
    }, {
      "heading" : "3 Implementation and computational experiments",
      "text" : "The algorithm described in Section 2 was implemented as a parallel program pdsat that uses the MPI library. One MPI process of pdsat is assigned to be a master process, others — to be slave processes. For each new point X̃ = {xi1 , . . . , xid} of the search space the master process constructs a set of vectors Θ ( X̃ ) ⊆ {0, 1}d, ∣ ∣ ∣Θ ( X̃ )∣ ∣ ∣ = N (see (5)), using Mersenne twister pseudorandom number generator. After receiving (\nαji1 , . . . , α j id\n) ∈ Θ ( X̃ )\n, j ∈ {1, . . . , N}, a slave process starts solving the SAT problem for CNF C [ X̃/ (\nαji1 , . . . , α j id\n)]\n.\nBelow the set { C [ X̃/ (\nαji1 , . . . , α j id\n)]}N\nj=1 is called a sample for a decomposition\nset X̃. The interruption technique described in Subsection 2.3 was implemented in pdsat. The master process tracks the total time spent on processing the set Θ ( X̃ ) by all the slave processes. If it decides that, according to (7), the value of\nthe predictive function for X̃ will exceed its best known value then the master process interrupts the processing Θ ( X̃ ) by sending asynchronous messages to\nthe slave processes. A sequential SAT-solver underlies every slave process. In our experiments we used MiniSat-C 1.14.1 and MiniSat 2.2 [9]. Let’s present the results of computational experiments on constructing decomposition sets related to the SAT problems that encode the inversion of widely known cryptographic functions — A5/1 and Bivium. In two experiments described further pdsat was taking 80 cores of a computing cluster. In both cases pdsat stopped because of reaching the timeout of 4 days. For each new point of the search space pdsat processed a sample of 10 000 CNFs (N = 10000 ).\nThe A5/1 keystream generator consists of 3 LFSRs (linear feedback shift register [23]) that are shifted asynchronously. This generator was described in details in [3]. Cryptanalysis of the A5/1 generator consists in finding the initial contents of LFSRs (64 bits) based on the known keystream fragment.\nUsually if the cryptanalysis is considered as a SAT problem then it is called a logical cryptanalysis [21]. Logical cryptanalysis of the A5/1 generator with first 144 bits of the keystream known was described in [28] where a decomposition set\nof 31 variables was found manually, guided by the features of the A5/1 algorithm. LFSRs cells corresponding to the variables from this set are marked with grey in the scheme of the A5/1 generator on the left-hand side of Fig. 1. For further convenience we enumerate cells of the A5/1 registers using continuous numbering (and do the same for Bivium later).\nOn the right-hand side of Fig. 1 we present a decomposition set for logical cryptanalysis of A5/1 that was found automatically by pdsat. As an X̃0 pdsat was given a SUPBS of a CNF encoding the A5/1 cryptanalysis problem (64 variables corresponding to the initial state of A5/1 registers). X̃∗ was constructed as a subset of X̃0.\nBest results for the problem of logical cryptanalysis of the A5/1 generator were obtained using MiniSat-C 1.14.1 solver with light modifications. Description of these modifications was presented in [28]. Table 1 presents some qualitative parameters of decomposition sets from Fig. 1. Min. time, Max. time and Avg. time stand for the minimal, maximal and average time (in seconds) taken to solve SAT problems for CNFs from the sample, s2 stands for the unbiased sample variance (see (3)), F (·) — for the value of predictive function. Values in Table 1 are calculated for one core of Intel Xeon E5450 processor.\nDespite the fact that values of the predictive function for the decomposition sets from Fig. 1 are quite close, the value of s2 for the set found by pdsat is significantly less. It means that for this decomposition set the obtained time estimation is more precise according to (4).\nFurther we consider the logical cryptanalysis of the Bivium cipher [7]. In Bivium two shifted registers of a special kind are used, first consisting of 93 cells and second consisting of 84 cells. Logical cryptanalysis of Bivium is considered in the following formulation (that was earlier studied in [10], [22], [30]): based on the known fragment of the keystream one should find 177 bits that correspond to the internal state of the Bivium registers at the start of the keystream generation. In the experiments presented below we considered first 200 bits of the keystream (similar to [10]).\nThe authors of [10], [22], [30] presented several variants of decomposition sets. We compared the decomposition set found by pdsat with the results of [10] since in that work decomposition sets are presented explicitly and those experiments are easy to reproduce. The best decomposition set among the ones presented in [10] consists of 45 variables and is shown in Fig. 2. In our experiment as an X̃0 pdsat was given a SUPBS of a CNF encoding the Bivium cryptanalysis problem (177 variables). The decomposition set found by pdsat is shown in Fig. 3.\nFor the problem of logical cryptanalysis of Bivium we used the MiniSat 2.2 solver. In Table 2 qualitative parameters of the decomposition sets from figures 2 and 3 are presented.\nIn Table 3 we present additional information about traversal of the search space during the computational experiments that clearly corroborates the efficiency of the interruption technique described in Subsection 2.3.\nIt should be noted that in both computational experiments pdsat was provided only with SUPBS of SAT problems. Nevertheless, it managed to find the decomposition sets that are comparable to or better than the ones obtained by manually analyzing the features of the corresponding keystream generators."
    }, {
      "heading" : "4 Solving hard SAT instances in the volunteer computing project SAT@home",
      "text" : "We use the pdsat program described in Section 3 to plan computational experiments in the volunteer computing project SAT@home. SAT@home was launched on the 29th of September 2011 [31]. It uses computing resources provided by volunteer PCs to solve hard combinatorial problems that can be effectively reduced to SAT. The project was implemented using the BOINC platform. An experiment that consisted in solving 10 inversion problems of the generator A5/1 was successfully finished in SAT@home on the 7th of May 2012. It should be noted that we considered only instances that cannot be solved using the known rainbow tables [26].\nIn that experiment we used the decomposition set from [28]. The computing application was based on a modified version of MiniSat-C 1.14.1 (see [28]).\nFirst 114 bits of the keystream that correspond to one keystream burst of the GSM protocol were analyzed. On average in order to solve one problem of logical cryptanalysis of A5/1 SAT@home processed about 1 billion SAT problems.\nSince May 2012 SAT@home is occupied in searching for systems of orthogonal Latin squares. During this time we found several pairs of orthogonal diagonal Latin squares of order 10 that are different from the ones published in [6].\nCharacteristics of the SAT@home project as of 8 of February 2013 are (according to BOINCstats1):\n– 2367 active PCs (active PC in volunteer computing is a PC that sent at least one result in last 30 days) about 80% of them use Microsoft Windows OSes; – 1299 active users (active user is a user that has at least one active PC); – versions of the client application: Windows/x86, Linux/x86, Linux/x64; – average real performance: 2,9 teraflops, maximal performance: 6,3 teraflops.\nThe dynamics of the real performance of SAT@home can be seen at the SAT@home performance page2.\nIt should be noted that the estimation for the A5/1 cryptanalysis (see Section 3) obtained with the use of pdsat is close to the average real time spent by SAT@home to solve corresponding SAT problems. With respect to the estimation from Section 3 logical cryptanalysis of Bivium cipher would take about 6 years in SAT@home with its current performance."
    }, {
      "heading" : "5 Related Work",
      "text" : "Topics related to organization of SAT solving in distributed environments were considered in many papers, for example in [5], [8], [13], [15], [16], [34].\nIn [15,16,17,18] various approaches to partitioning SAT problems were studied. Detailed analysis of a number of problems regarding partitioning approach was presented in [15]. Also, in [15] special efficiency functions were introduced to evaluate the quality of a SAT problem partitioning. In our paper, we used predictive functions that are based on different principles.\nThe authors of [17] proposed to use lookahead heuristics [14] to construct SAT problem partitionings. This idea (with significant additions) was implemented in [13]. During the process of the original problem solving, the distributed solver from [13] processed hundreds of thousands of SAT instances that correspond to cubes of partitioning that were generated by a lookahead solver on the preprocessing stage.\nIn [27] a desktop grid for solving SAT which used conflict clauses exchange via a peer-to-peer protocol was described. Apparently, [4] became the first paper about the use of a desktop grid based on the BOINC platform for solving SAT. Unfortunately, it did not evolve into a full-fledged volunteer computing project.\n1 http://boincstats.com/ 2 http://sat.isa.ru/pdsat/performance.php\nThe first work that used SAT-solvers for cryptanalysis was [21]. The authors of [10], [22], [29], [30] presented some estimations of the time required for logical cryptanalysis of the Bivium cipher, obtained using the ideas underlying the Monte-Carlo method. The main novelty of our approach lies in the fact that we consider the process of construction of the decomposition set with good qualitative parameters as a process of optimisation of predictive function in a special search space.\nThe most effective method of cryptanalysis of the A5/1 generator is the rainbow method implemented in A5/1 Cracking project. Publicly available rainbow tables [26] made it possible to successfully determine the initial state of A5/1 registers based on 8 known bursts of the keystream with probability about 88% in several seconds on a usual PC. It means that ∼12% of the key space is not covered by these tables. In the SAT@home project we were searching for keys from these 12% of the key space only.\nExtensive bibliography regarding the use of SAT solvers in searching for combinatorial designs (for example, orthogonal Latin squares) is presented in [33]."
    }, {
      "heading" : "6 Conclusions",
      "text" : "In this paper, we proposed a method for estimating time to solve SAT in distributed computing environments. It uses the Monte Carlo method to statistically estimate the quality of partitioning of the original SAT problem. To search for a partitioning with good quality a special tabu search algorithm was used. The proposed method was used to obtain an approximate time required to do logical cryptanalysis of the well-known ciphers A5/1 and Bivium. Ten problems of logical cryptanalysis of the A5/1 generator, that could not be solved using the known rainbow tables [26], were successfully solved in the volunteer computing project SAT@home [31] that was developed and is maintained by the authors.\nAcknowledgments. Authors thank Stepan Kochemazov for numerous valuable comments that allowed us to significantly improve the quality of the paper, Alexey Ignatiev for constructive feedback and helpful discussions, Mikhail Posypkin and Nikolay Khrapov for their help in developing and administering of the SAT@home project, Karsten Nohl for detailing the rainbow method used in the A5/1 Cracking Project. Also we express our gratitude to all the users participating in the SAT@home project for their dedication and enthusiasm. This work was supported by Russian Foundation for Basic Research, grant 11-07-00377a. Oleg Zaikin also acknowledges support from President of Russian Federation grant for young scientists SP-1855.2012.5."
    } ],
    "references" : [ {
      "title" : "BOINC: A System for Public-Resource Computing and Storage",
      "author" : [ "D.P. Anderson" ],
      "venue" : "Buyya, R. (ed.) GRID. pp. 4–10. IEEE Computer Society",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Handbook of Satisfiability, Frontiers in Artificial Intelligence and Applications, vol",
      "author" : [ "A. Biere", "M. Heule", "H. van Maaren", "Walsh", "T. (eds." ],
      "venue" : "185. IOS Press",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Real Time Cryptanalysis of A5/1 on a PC",
      "author" : [ "A. Biryukov", "A. Shamir", "D. Wagner" ],
      "venue" : "Schneier, B. (ed.) FSE. LNCS, vol. 1978, pp. 1–18. Springer",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "SAT Over BOINC: An Application-Independent Volunteer Grid Project",
      "author" : [ "M. Black", "G. Bard" ],
      "venue" : "Jha, S., gentschen Felde, N., Buyya, R., Fedak, G. (eds.) GRID. pp. 226–227. IEEE",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Parallel propositional satisfiability checking with distributed dynamic learning",
      "author" : [ "W. Blochinger", "C. Sinz", "W. Küchlin" ],
      "venue" : "Parallel Computing 29(7), 969–994",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Completion of the spectrum of orthogonal diagonal latin squares",
      "author" : [ "J.W. Brown", "F. Cherry", "L. Most", "M. Most", "E. Parker", "W. Wallis" ],
      "venue" : "Lect. Notes Pure Appl. Math. 139, 43–49",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Trivium: A stream cipher construction inspired by block cipher design principles",
      "author" : [ "C.D. Cannière" ],
      "venue" : "Katsikas, S.K., Lopez, J., Backes, M., Gritzalis, S., Preneel, B. (eds.) ISC. LNCS, vol. 4176, pp. 171–186. Springer",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "GridSAT: a system for solving satisfiability problems using a computational grid",
      "author" : [ "W. Chrabakh", "R. Wolski" ],
      "venue" : "Parallel Computing 32(9), 660–687",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "An Extensible SAT-solver",
      "author" : [ "N. Eén", "N. Sörensson" ],
      "venue" : "Giunchiglia, E., Tacchella, A. (eds.) SAT. LNCS, vol. 2919, pp. 502–518. Springer",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Attacking Bivium Using SAT Solvers",
      "author" : [ "T. Eibach", "E. Pilz", "G. Völkel" ],
      "venue" : "Büning, H.K., Zhao, X. (eds.) SAT. LNCS, vol. 4996, pp. 63–76. Springer",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "An introduction to probability theory and its applications",
      "author" : [ "W. Feller" ],
      "venue" : "Vol. II. Second edition, John Wiley & Sons Inc., New York",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1971
    }, {
      "title" : "Tabu Search",
      "author" : [ "F. Glover", "M. Laguna" ],
      "venue" : "Kluwer Academic Publishers, Norwell, MA, USA",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Cube and Conquer: Guiding CDCL SAT Solvers by Lookaheads",
      "author" : [ "M. Heule", "O. Kullmann", "S. Wieringa", "A. Biere" ],
      "venue" : "Eder, K., Lourenço, J., Shehory, O. (eds.) Haifa Verification Conference. LNCS, vol. 7261, pp. 50–65. Springer",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Grid Based Propositional Satisfiability Solving",
      "author" : [ "A.E.J. Hyvärinen" ],
      "venue" : "Ph.D. thesis, Aalto University",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A Distribution Method for Solving SAT in Grids",
      "author" : [ "A.E.J. Hyvärinen", "T.A. Junttila", "I. Niemelä" ],
      "venue" : "Biere, A., Gomes, C.P. (eds.) SAT. LNCS, vol. 4121, pp. 430–435. Springer",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Partitioning SAT Instances for Distributed Solving",
      "author" : [ "A.E.J. Hyvärinen", "T.A. Junttila", "I. Niemelä" ],
      "venue" : "Fermüller, C.G., Voronkov, A. (eds.) LPAR (Yogyakarta). LNCS, vol. 6397, pp. 372–386. Springer",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Grid-Based SAT Solving with Iterative Partitioning and Clause Learning",
      "author" : [ "A.E.J. Hyvärinen", "T.A. Junttila", "I. Niemelä" ],
      "venue" : "Lee, J.H.M. (ed.) CP. LNCS, vol. 6876, pp. 385–399. Springer",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Limitations of restricted branching in clause learning",
      "author" : [ "M. Järvisalo", "T.A. Junttila" ],
      "venue" : "Constraints 14(3), 325–356",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Monte Carlo methods",
      "author" : [ "M.H. Kalos", "P.A. Whitlock" ],
      "venue" : "Wiley, New York, NY",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "Logical Cryptanalysis as a SAT Problem",
      "author" : [ "F. Massacci", "L. Marraro" ],
      "venue" : "J. Autom. Reasoning 24(1/2), 165–203",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Attacking Bivium with MiniSat",
      "author" : [ "C. Mcdonald", "C. Charnes", "J. Pieprzyk" ],
      "venue" : "Tech. Rep. 2007/040, ECRYPT Stream Cipher Project",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Handbook of Applied Cryptography",
      "author" : [ "A. Menezes", "P.C. van Oorschot", "S.A. Vanstone" ],
      "venue" : "CRC Press",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "The Monte Carlo Method",
      "author" : [ "N. Metropolis", "S. Ulam" ],
      "venue" : "J. Amer. statistical assoc. 44(247), 335–341",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 1949
    }, {
      "title" : "Combinatorial Optimization: Algorithms and Complexity",
      "author" : [ "C.H. Papadimitriou", "K. Steiglitz" ],
      "venue" : "Prentice-Hall",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 1982
    }, {
      "title" : "Parallel SAT Solving on Peer-to-Peer Desktop Grids",
      "author" : [ "S. Schulz", "W. Blochinger" ],
      "venue" : "J. Grid Comput. 8(3), 443–471",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Parallel Logical Cryptanalysis of the Generator A5/1 in BNB-Grid System",
      "author" : [ "A. Semenov", "O. Zaikin", "D. Bespalov", "M. Posypkin" ],
      "venue" : "Malyshkin, V. (ed.) PaCT. LNCS, vol. 6873, pp. 473–483. Springer",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Grain of salt – an automated way to test stream ciphers through sat solvers",
      "author" : [ "M. Soos" ],
      "venue" : "Tools’10: Proceedings of the Workshop on Tools for Cryptanalysis 2010. pp. 131–144. Ecrypt II",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Extending SAT Solvers to Cryptographic Problems",
      "author" : [ "M. Soos", "K. Nohl", "C. Castelluccia" ],
      "venue" : "Kullmann, O. (ed.) SAT. LNCS, vol. 5584, pp. 244–257. Springer",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Mathematical Statistics",
      "author" : [ "S.S. Wilks" ],
      "venue" : "J. Wiley and Sons",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 1962
    }, {
      "title" : "PSATO: a Distributed Propositional Prover and its Application to Quasigroup Problems",
      "author" : [ "H. Zhang", "M.P. Bonacina", "J. Hsiang" ],
      "venue" : "J. Symb. Comput. 21(4), 543–560",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 1996
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Nowadays there are about 70 active projects, the majority of them are based on the BOINC platform [1].",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 1,
      "context" : "It is well-known that a lot of important combinatorial problems (for example from areas of formal verification, planning or bioinformatics) can be effectively reduced to SAT [2].",
      "startOffset" : 174,
      "endOffset" : 177
    }, {
      "referenceID" : 13,
      "context" : "2 Monte Carlo method for estimating time of solving of SAT problem via partitioning approach In [15,16,17,18] various approaches to partitioning SAT problems were studied.",
      "startOffset" : 96,
      "endOffset" : 109
    }, {
      "referenceID" : 14,
      "context" : "2 Monte Carlo method for estimating time of solving of SAT problem via partitioning approach In [15,16,17,18] various approaches to partitioning SAT problems were studied.",
      "startOffset" : 96,
      "endOffset" : 109
    }, {
      "referenceID" : 15,
      "context" : "2 Monte Carlo method for estimating time of solving of SAT problem via partitioning approach In [15,16,17,18] various approaches to partitioning SAT problems were studied.",
      "startOffset" : 96,
      "endOffset" : 109
    }, {
      "referenceID" : 16,
      "context" : "2 Monte Carlo method for estimating time of solving of SAT problem via partitioning approach In [15,16,17,18] various approaches to partitioning SAT problems were studied.",
      "startOffset" : 96,
      "endOffset" : 109
    }, {
      "referenceID" : 13,
      "context" : "Further we will use the notation of [15].",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 14,
      "context" : "For example, one can construct {Fj}Sj=1 using a scattering procedure [16], a guiding path solver [34] or a lookahead solver [13], [17].",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 29,
      "context" : "For example, one can construct {Fj}Sj=1 using a scattering procedure [16], a guiding path solver [34] or a lookahead solver [13], [17].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 12,
      "context" : "For example, one can construct {Fj}Sj=1 using a scattering procedure [16], a guiding path solver [34] or a lookahead solver [13], [17].",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 15,
      "context" : "For example, one can construct {Fj}Sj=1 using a scattering procedure [16], a guiding path solver [34] or a lookahead solver [13], [17].",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 9,
      "context" : "However, a partitioning method that makes it possible to construct such estimations was used in a number of papers about solving cryptanalysis problems via SAT approach [10], [22], [29], [30].",
      "startOffset" : 169,
      "endOffset" : 173
    }, {
      "referenceID" : 20,
      "context" : "However, a partitioning method that makes it possible to construct such estimations was used in a number of papers about solving cryptanalysis problems via SAT approach [10], [22], [29], [30].",
      "startOffset" : 175,
      "endOffset" : 179
    }, {
      "referenceID" : 26,
      "context" : "However, a partitioning method that makes it possible to construct such estimations was used in a number of papers about solving cryptanalysis problems via SAT approach [10], [22], [29], [30].",
      "startOffset" : 181,
      "endOffset" : 185
    }, {
      "referenceID" : 27,
      "context" : "However, a partitioning method that makes it possible to construct such estimations was used in a number of papers about solving cryptanalysis problems via SAT approach [10], [22], [29], [30].",
      "startOffset" : 187,
      "endOffset" : 191
    }, {
      "referenceID" : 18,
      "context" : "we will use the Monte Carlo method [20], [24].",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 22,
      "context" : "we will use the Monte Carlo method [20], [24].",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 10,
      "context" : "If E[ξ] and Var(ξ) are both finite then from Central Limit Theorem [11] we have the main formula of the Monte Carlo method Pr ",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 28,
      "context" : "In this case instead of (2) a following formula is applied [32] Pr ",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 11,
      "context" : "It naturally corresponds to the basic idea of tabu search (TS) [12].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 23,
      "context" : "A simple local search [25] stops after finding a local extremum.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 17,
      "context" : "In particular for a SAT problem that encodes the inversion problem of a cryptographic function we can choose X̃0 as a corresponding Strong Unit Propagation Backdoor Set (SUPBS, [19]).",
      "startOffset" : 177,
      "endOffset" : 181
    }, {
      "referenceID" : 8,
      "context" : "2 [9].",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 21,
      "context" : "The A5/1 keystream generator consists of 3 LFSRs (linear feedback shift register [23]) that are shifted asynchronously.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 2,
      "context" : "This generator was described in details in [3].",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 19,
      "context" : "Usually if the cryptanalysis is considered as a SAT problem then it is called a logical cryptanalysis [21].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 25,
      "context" : "Logical cryptanalysis of the A5/1 generator with first 144 bits of the keystream known was described in [28] where a decomposition set",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 25,
      "context" : "Decomposition sets for logical cryptanalysis of A5/1: the one from [28] (lefthand side) and the one found by pdsat (right-hand side)",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 25,
      "context" : "Description of these modifications was presented in [28].",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 25,
      "context" : "time s F (·) from [28] 0.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 6,
      "context" : "Further we consider the logical cryptanalysis of the Bivium cipher [7].",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 9,
      "context" : "Logical cryptanalysis of Bivium is considered in the following formulation (that was earlier studied in [10], [22], [30]): based on the known fragment of the keystream one should find 177 bits that correspond to the internal state of the Bivium registers at the start of the keystream generation.",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 20,
      "context" : "Logical cryptanalysis of Bivium is considered in the following formulation (that was earlier studied in [10], [22], [30]): based on the known fragment of the keystream one should find 177 bits that correspond to the internal state of the Bivium registers at the start of the keystream generation.",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 27,
      "context" : "Logical cryptanalysis of Bivium is considered in the following formulation (that was earlier studied in [10], [22], [30]): based on the known fragment of the keystream one should find 177 bits that correspond to the internal state of the Bivium registers at the start of the keystream generation.",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 9,
      "context" : "In the experiments presented below we considered first 200 bits of the keystream (similar to [10]).",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 9,
      "context" : "The authors of [10], [22], [30] presented several variants of decomposition sets.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 20,
      "context" : "The authors of [10], [22], [30] presented several variants of decomposition sets.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 27,
      "context" : "The authors of [10], [22], [30] presented several variants of decomposition sets.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 9,
      "context" : "We compared the decomposition set found by pdsat with the results of [10] since in that work decomposition sets are presented explicitly and those experiments are easy to reproduce.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 9,
      "context" : "The best decomposition set among the ones presented in [10] consists of 45 variables and is shown in Fig.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 9,
      "context" : "Decomposition set of 45 variables for logical cryptanalysis of Bivium from [10]",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 9,
      "context" : "time s F (·) from [10] 0.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 25,
      "context" : "In that experiment we used the decomposition set from [28].",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 25,
      "context" : "1 (see [28]).",
      "startOffset" : 7,
      "endOffset" : 11
    } ],
    "year" : 2013,
    "abstractText" : "This paper proposes a method to estimate the total time required to solve SAT in distributed environments via partitioning approach. It is based on the observation that for some simple forms of problem partitioning one can use the Monte Carlo approach to estimate the time required to solve an original problem. The method proposed is based on an algorithm for searching for partitioning with an optimal solving time estimation. We applied this method to estimate the time required to perform logical cryptanalysis of the widely known stream ciphers A5/1 and Bivium. The paper also describes a volunteer computing project SAT@home aimed at solving hard combinatorial problems reduced to SAT. In this project during several months there were solved 10 problems of logical cryptanalysis of the A5/1 cipher that could not be solved using known rainbow tables.",
    "creator" : "LaTeX with hyperref package"
  }
}