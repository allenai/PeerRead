{
  "name" : "1606.05934.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Adapting ELM to Time Series Classification: A Novel Diversified Top-k Shapelets Extraction Method",
    "authors" : [ "Qiuyan Yan", "Qifa Sun", "Xinming Yan" ],
    "emails" : [ "yanqy@cumt.edu.cn" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Extreme Learning Machine (ELM for short) was originally developed based on single hidden-layer feed forward neural networks (SLFNs) [1]. Compared with the conventional learning machines, it is of extremely fast learning capacity and good generalization capability. Thus, ELM, with its variants [2,3], has been widely applied in many fields [4,5]. The results indicate that ELM produces comparable or better classification accuracies with reduced training time and implementation complexity compared to artificial neural networks methods and support vector machine methods.\nUnfortunately, as a black-box method, ELM fails to measure up to the task of time series data classification by itself. A possible solution to this issue is\nar X\niv :1\n60 6.\n05 93\n4v 1\n[ cs\n.L G\n] 2\n0 Ju\nto improve the interpretability of ELM by feature selection. If a set of selected features improve the classification accuracy much more than original feature sets, it is reasonable to interpret the results by them. Nevertheless, selection the most representative and interpretative feature can improve the interpretability of ELM and make ELM more adapt to time series classification. In the context of feature selection in time series data analysis, most of the current methods adopt such a framework that ranks subsequences according to their individual discriminative power to the target class and then selects top-k ranked subsequences[6]. These methods have some common drawbacks: (1) the selected features are not the most representative and interpretative, (2) many redundant features are selected, and (3) the number of selected feature is arbitrarily specified by a parameter k .\nIn this paper, a novel method is proposed to improve ELM representative and interpretative ability by extraction diversified top-k shapelets features.Shapelets was introduced as a primitive for time series data mining [7] and was utilized in classifying time series data [8,9]. The original shapelet based classifier embeds the shapelet discovery algorithm in a decision tree, and uses information gain to assess the quality of candidates. Shapelets transformed classification methods [10,11] were proposed to separate the processing of shapelets selection and classification. The k shapelets are selected in an offline manner, and which can not only improve the affectivity and efficiency of classification, but also introduce a common feature attraction method which can be used in all typical time series classification algorithms. Nevertheless, shapelets based classification methods have been widely discussed and used in many real applications [12,13,14].\nThe most challenge is that there are large quantities of redundant shapelets in candidates which decreasing the accuracy of classification and the parameter k is hard to determine. Some works [11,15] detect this problem and use clustering or pruning methods to remove the redundant,but still exist redundant shapelets, also, the k value is determined from experiments. This paper make the following contributions: First, in order to get rid of the similar and redundant shapelets in candidate set, two conceptions including similar shapelets and diversified top-k shapelets are presented. Based on these conceptions, a method of construction diversify shapelets graph is proposed. Second, a diversified top-k shapelets query method is presented to find top-k representative shapeletes of each class. Third, we propose an diversified top-k shapelets transformed ELM algorithm which can automatically determine the parameter k and transform data using the determined k shapelets. The experimental results show that the proposed approach significantly improves the interpretability and performance of ELM."
    }, {
      "heading" : "2 Preliminary",
      "text" : "Extreme Learning Machine (ELM) is a generalized single hidden-layer feedforward network. In ELM, the hidden layer node parameters are mathematically calculated instead of being iteratively tuned; thus, it provides good generalization performance at thousands of times faster speed than traditional popular learning algorithms for feedforward neural networks.\nSuppose there are N arbitrary distinct training instances (xi, ti), where xi = [xi1, xi2, ..., xin]\nT ∈ Rn, and ti = [ti1, ti2, ..., tin]T ∈ Rm,standard SLFNs with Ñ hidden nodes and activation function g(x) are mathematically modeled as\nÑ∑ i=1 βigi(xj) = Ñ∑ i=1 βigi(wi · xj + bi) = oj , j = 1, ..., N, (1)\nwhere wi = [wi1, wi2, ..., win] T is the weight vector connecting the ith hidden node and the input nodes, βi = [βi1, βi2, ..., βim] T is the weight vector connecting the ith hidden nodes and the output nodes, and bi is the threshold of the ith hidden node.If a SLFN with Ñ hidden nodes with activation function g(x) can approximate these N samples with zero error, it then implies that there exist βi, wi and bi, such that:\nÑ∑ i=1 βiG(wi · xj + bi) = tj , j = 1, . . . , N, (2)\nThe above N equations can be written compactly as\nHβ = T, (3)\nwhere H(w1, . . . , wÑ , b1, . . . , bÑ , x1, . . . , xN )\n=  g(w1 · x1 + b1) . . . G(wÑ · x1 + bÑ )... . . . ... g(w1 · xN + b1) · · · G(wÑ · xN + bÑ )  N×Ñ\nβ =  βT1 . . . . . .\nβT Ñ  Ñ×m and T =  tT1 . . . . . . tTN  N×m\nH is named as hidden layer output matrix of the network, where with respect to inputs x1, x2, . . . , xN and its jth row represents the output vector of the hidden layer with respect to input xj .\nELM differs from other training algorithms in that the hidden node parameters wi and bi are not tuned during training, but are instead assigned with random values according to any continuous samplings distribution. Eq. (3) then becomes a linear system and the output weight β are estimates as Eq. (4).\n_ β = H†T, (4)\nwhere H† is the Moore-Penrose generalized inverse of the hidden layer output matrix H."
    }, {
      "heading" : "3 Diversified Top-k Shapelets Transformed ELM",
      "text" : "In this section, we discuss three parts of our work (1) construction the diversity graph of shapelets candidates, (2) querying diversified top-k shapelets, and (3) transforming the data based on diversified top-k shapelets and applying in ELM. The following contents will discuss above three contribution separately.\nBefore removing the redundant shapelets, we firstly need to get the shapelets candidates set. The original shapelets extraction algoritm is time consuming and complexity is O(n2m4), n is the number of time series in the data set, m is the length of each time series. In order to improve the efficiency of shapelets based classification method, we follow the method proposed in [9], which transformed the data sets through SAX method and decreased the time complexity to O(nm2)."
    }, {
      "heading" : "3.1 Construction the diversity graph of shapelets candidates",
      "text" : "Considerable works have focused on the diversified top-k query, but they almost applying on a typical circumstance. In our work, we use the diversity graph[16] to find a general method to extract diversified top-k shapelets.\nGiven I is a shapelets candidate sets, I = {s1, . . . sn}, and n is the number of I. The question is how to measure the similarity of two shapelets and how to define the diversified top-k shapelets. So we first give the two definitions.\nDefinition 1: Similar shapelets. Given two shapelets si and sj which represent the same class, 1 ≤ i, j ≤ n, i 6= j and n is the number of shapelets candidates. The optimal split point of si and sj are < si, di > and < sj , dj >, the split threshold are di and dj . We say si and sj are similar shapelets when they satisfy dis(si, sj) ≤ min(di, dj). We denote the similar shapelets as si ≈ sj .\nDefinition 2: Diversified top-k Shapelets. Given a shapelets candidates set I = {s1, . . . sn}, and an integer k where 1 ≤ k ≤ |I|. The diversified topk shapelets query results of I, denoted as DivTopk(I), is a list of results that satisfy the following three conditions.\n1) DivTopk(I) ⊆ I, |DivTopk(I)| ≤ k 2) For any two results si ∈ I and sj ∈ I and si 6= sj , if si ≈ sj , then\n{si, sj} 6⊂DivTopk(I). 3) ∑ si∈I score(si) is maximized.\nWe give a diversity shapelet graph example of ChlorineConcentration dataset as in Fig. 1.There are ten shapelets candidates as shown in fig1-a and the diversity graph of these ten candidates from algorithm 1 are shown in fig1-b.The black, red and green subsequence are the top-3 shapelets and can get the best classification accuracy.Next section we will explain how to get the diversified top-k shapelets on the diversify graph."
    }, {
      "heading" : "3.2 Diversified Top-k Shapelets Extraction",
      "text" : "Traditional top-k query only returns the objects with largest k score, however, diversified top-k query concerns not only the score value but also the similarity\nAlgorithm 1 conShapeletGraph(allShapelets)\ninput: shapelets candidates allShapelets output: diverisity shapelets Graph\n1: Graph = φ 2: sort(allShapelets) 3: for i=1 to |allShapelets| do 4: Graph.add(allShapelets[i]) 5: end for 6: for j=1 to |allShapelets| do 7: for k=1 to |allShapelets| do 8: if (allShapelets[j] ≈ allShapelets[k]) then 9: Graph[j].add(Graph[k])\n10: Graph[k].add(Graph[j]) 11: end if 12: end for 13: end for 14: return Graph\nof each object and remove all the redundant objects from results. According to [16], find top-k results falling into two categories: incremental manner and bounding manner. We noticed that the bounding manner first satisfied the k value, but in each step it may not add the largest score vertex. In our problem, we must maintain the largest information gain shapelets in order to have the best classification accuracy. So we calculate the diversified top-k shapelets via an incremental manner. The detailed search procedure is shown as in algorithm2."
    }, {
      "heading" : "3.3 Diversified Top-k Shapelets Transform for ELM",
      "text" : "After getting diversified top-k shapelets, we can use these shapelets to transform data before ELM classification. For each instance of data Ti , the subsequence distance is computed between Ti and Sj , Sj is a shapelet in Top-k shapelets.The resulting k distances are used to form a new instance of transformed data, where each attribute corresponds to the distance from each shapelet to the original time series.\nAlgorithm 2 DivTopkShapelets (Graph, k)\ninput: diverisity Graph, k value output: k number of shapelets\n1: kShapelets = Ø, n = |V(Graph)| 2: kShapelets.add(v1) 3: while(| kShapelets |< k) 4: for i=2 to n do 5: if (Graph[i] ∩ kShapelets = Ø) then 6: kShapelets.add(vi) 7: end if 8: end for 9: return kShapelets\nIn order to get the best classification accuracy and also to get rid of the independence on the parameter k, we set k in an interval of [1, κ ] where κ is an empirical optimal value, according to our experiments(see section 4.1), which is set to 9,then we use the ELM to learning training data and evaluate each diversified top-k shapelets candidate. The k value with the largest prediction accuracy is selected.\nWhen using data split into training data and testing data, the shapelets extraction and k determination is carried out only on the training data to avoid bias. The optimal diversified top-k shapelets are then used to transform each instance of the testing data.The details are as in following algorithm 3.\nAlgorithm 3 DivShapELM(Graph, κ)\ninput: κ value output: ELM classification results\n1: for i=1 to κ do 2: kShapelets = DivTopkShapelet(Graph,k) 3: output = Ø 4: for ts= 1 to |Dt| do 5: transformed = Ø 6: for s = 1 to |kshapelets| do 7: dist = subsequenceDist(ts,s) 8: transformed.add(dist) 9: end for\n10: output.add(transformed) 11: end for 12: using ELM evaluate output 13: kShapelets = the output with highest accuracy on ELM 14: end for 15: using kShapelets transform testing data 16: return ELM classification results"
    }, {
      "heading" : "4 Experiments",
      "text" : "To evaluate our proposed methods, we selected 15 data sets from the UCR time series repository (listed in Table 1).We use a simple train/test split and all reported results are testing accuracy.All shapelets candidates seletction, top-k diversified shapelets extraction and classifier construction is done on the training set. All experiments are implemented in Java within the Weka framework."
    }, {
      "heading" : "4.1 Determination of shapelets length and κ",
      "text" : "There are two parameters min and max in the procedure of shapelets candidates generation. The two parameters determine the length of shapelets candidates which can influence finding the best representative shapelets. Followed [11], we set min-length and max-length of subsequences to generate shapeless are m/11 and m/2 separately, m is the length of each time series.\nFirst, in order to explain how the k value influences accuracy of classification, we test the average accuracy of six classifier on fifteen data sets with the varying k value. As shown in Fig. 2, with the increasing of k value, average classification accuracy first increases and then becomes stable when k is 9. Accordingly, we set the κ value as 9 and use this value in the following experiments."
    }, {
      "heading" : "4.2 Representation of Optimal Shapelets Sets",
      "text" : "In this section, we want to get a visual overlook at what was the optimal shapelets indeed. Because reference[15] has verified that ShapeletSelection can remove more redundant shapelets than other similar methods, we only compared the optimal shapelets sets between DivTopkShapelets and ShapeletSelection when the two algorithms all have the best classification,as shown in Fig. 3. The optimal shapelets sets were acquired from ShapeletSelection when k=8 (Fig.3-a), and from DivTopkShapelet when k=2(Fig.3-b)."
    }, {
      "heading" : "4.3 Accuracy Comparison",
      "text" : "In this section, we select six traditional time series classification algorithms including C4.5, 1NN, Naive Bays(NaB), BayesianNetwork(BaN), RandomForest(RaF) and RotationForest(RoF) to compare the accuracy with our proposed methods.\nAccuracy comparison with Traditional Classification Firstly, we directly use selected classification algorithms to classify the datasets .Secondly,we use DivTopkShapelets(set k=9)to extract optimal shapeletes sets and transform data, then classify transformed data sets with six selected classification algorithms.Results are presented in table 1, the column captions with classifier name plus S(see C4.5(S)) means DivTopkShapelets transformed classification results. From the table we can see that compared to traditional classification algorithms, DivTopkShapelets transformed classification methods can improve accuracy of 9 out of 15 datasets. For all six classification algorithms, average accuracy are improved. Especially for NaiveBays, DivTopkShapelets improves 13 data sets accuracy.\nAccuracy comparison with ShapeletSelection Algorithm We compared the relative accuracy of DivTopkShapelets with the most similar method ShapeletSelection as shown in table2. From results we can draw the conclusion that compared with ShapeletSelection, DivTopkShapelets transformed classification method can improve accuracy of 9 out of 15 datasets. On Adiac dataset, DivTopkShapelets has the best performance, the average accuracy improved 20.08%. For classifiers, DivTopkShapelets increase 1NN classifier most with the accuracy improved 6.06%.\nAccuracy Comparison with ELM In this section,we compare the classification accuracy between DivShapELM and traditional ELM and average accuracy of six traditional classification algorithms(column named as avg T). As shown in table 3,DivShapELM can obvious improve traditional ELM accuracy.\nIn 13 out of 15 datasets, the accuracy of ELM is improved and is close to or even better than average accuracy of traditional classification algorithms. Especially, in Trace dataset, DivShapELM has the accuracy of 99.20, better then ELM 39.40%."
    }, {
      "heading" : "4.4 Runtime comparison",
      "text" : "DivShapELM has three extra pre-procedures: shapelets candidate selection, diversified shapelets selection and data transform. Once the transformed data\nare prepared, the rest procedure is a usual classification process. Table 4 gives the extra time and classification time of DivShapELM and ELM. The time cost of diversified shapelets selection are varied with data sets, but which can be conducted in an offline manner. Because DivShapELM can transform a dataset from n ×m length to a Rn×k matrix(k m), the runtime of DivShapELM can be reduced larglely. As shown in table 4, DivShapELM has the less classification time on 12 out of 15 datasets."
    }, {
      "heading" : "5 Conclusion and Future Work",
      "text" : "In this paper,we proposed a novel method to adapt ELM to time series classification by extraction diversified top-k shapelets. Our work includes three parts: (1) we introduce two conceptions of similar shapelets and diversified top-k shapelets, based on these conceptions, a method of construction diversity shapelets graph is presented, (2) we propose a diversified top-k shapelets extraction method, named as DivTopkShaplete, to find out all of the most representative and interpretative features of each class, and (3) we put forward a shapelets transformed ELM algorithm, named as DivShapELM, which automatically determine k value and get the diversified top-k shapelets to improve performance of ELM. The experiments results show that DivShapELM can improve the efficiency and interpretative of ELM . Also, we experimentally verify that DivTopkShaplete is an excellent feature extraction method which can improve the accuracy of traditional time series classification algorithms.\nFor future work, we plan to leverage multi-view feature representations [17,18,19,20,21,22,23] to achieve the performance improvement."
    } ],
    "references" : [ {
      "title" : "Extreme learning machine: a new learning scheme of feedforward neural networks",
      "author" : [ "Huang G-B", "Zhu Q-Y", "Siew C-K" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2004
    }, {
      "title" : "Weighted extreme learning machine for imbalance",
      "author" : [ "W. Zong", "G.-B. Huang", "Y. Chen" ],
      "venue" : "learning. Neurocomputing,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2013
    }, {
      "title" : "Self-adaptive evolutionary extreme learning machine",
      "author" : [ "J. Cao", "Z. Lin", "G.-B. Huang" ],
      "venue" : "Neural Processing Letters,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2012
    }, {
      "title" : "BETAWARE: a machine-learning tool to detect and predict transmembrane beta barrel proteins in Prokaryotes",
      "author" : [ "C. Savojardo", "P. Fariselli", "R. Casadio" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2013
    }, {
      "title" : "Projective Feature Learning for 3D Shapes with Multi-View Depth Images.Pacific Graphics,24(7):111,2015",
      "author" : [ "Z. Xie", "K. Xu", "W. Shan", "L. Liu", "Y. Xiong", "H. Huang" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2015
    }, {
      "title" : "Improving ELM-based microarray data classification by diversified sequence features selection",
      "author" : [ "Yuhai Zhao", "Guoren Wang", "Ying Yin", "Yuan Li", "Zhanghui Wang" ],
      "venue" : "Neural Comput&Applic,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2016
    }, {
      "title" : "Time series shapelets: a new primitive for data mining",
      "author" : [ "L Ye", "E. Keogh" ],
      "venue" : "Proc 15th ACM SIGKDD : 947-956,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2009
    }, {
      "title" : "Logical-shapelets: an expressive primitive for time series classification",
      "author" : [ "A Mueen", "E Keogh", "N. Young" ],
      "venue" : "In:Proc 17th ACM SIGKDD : 1154-1162,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "Fast shapelets: A scalable algorithm for discovering time series shapelets",
      "author" : [ "T Rakthanmanon", "E. Keogh" ],
      "venue" : "In:Proc 13th SDM,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2013
    }, {
      "title" : "A shapelet transform for time series classification",
      "author" : [ "J Lines", "M Davis L", "J Hills" ],
      "venue" : "In:Proc 18th ACM SIGKDD :289-297,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2012
    }, {
      "title" : "Classification of time series by shapelet transformation",
      "author" : [ "J Hills", "J Lines", "E Baranauskas" ],
      "venue" : "Data Mining and Knowledge",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2014
    }, {
      "title" : "Clustering time series using unsupervised-shapelets",
      "author" : [ "J Zakaria", "A Mueen", "E. Keogh" ],
      "venue" : "In Proc. 12th ICDM : 785-794,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2012
    }, {
      "title" : "Extracting Interpretable Features for Early Classification on Time Series",
      "author" : [ "Z Xing", "J Pei", "Y Philip S" ],
      "venue" : "In Proc.11th SDM :247-258,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2011
    }, {
      "title" : "Efficient pattern-based time series classification on gpu",
      "author" : [ "W Chang K", "B Deka", "W Hwu W M" ],
      "venue" : "In Proc.12th ICDM :131-140,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2012
    }, {
      "title" : "Shapelet pruning and shapelet coverage for time series classification",
      "author" : [ "JD Yuan", "ZH Wang", "M. Han" ],
      "venue" : "Journal of Software,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2015
    }, {
      "title" : "Diversifying top-k results",
      "author" : [ "L Qin", "X Yu J", "L. Chang" ],
      "venue" : "Proceedings of the VLDB Endowment,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2012
    }, {
      "title" : "Robust Subspace Clustering for Multi-View Data by Exploiting Correlation Consensus",
      "author" : [ "Y Wang", "X Lin", "L Wu" ],
      "venue" : "IEEE Transactions on Image Processing,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2015
    }, {
      "title" : "Unsupervised Metric Fusion Over Multiview Data by Graph Random Walk-Based Cross-View Diffusion",
      "author" : [ "Y Wang", "W Zhang", "L Wu", "X Lin", "X. Zhao" ],
      "venue" : "IEEE Transactions on Neural Networks and Learning Systems,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2015
    }, {
      "title" : "Effective Multi-Query Expansions: Robust Landmark Retrieval",
      "author" : [ "Y Wang", "X Lin", "L Wu", "W. Zhang" ],
      "venue" : "ACM Multimedia:",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "LBMCH: Learning Bridging Mapping for Cross-modal Hashing",
      "author" : [ "Y Wang", "X Lin", "L Wu", "W Zhang", "Q. Zhang" ],
      "venue" : "ACM SIGIR:",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2015
    }, {
      "title" : "Towards metric fusion on multi-view data: a cross-view based graph random walk approach",
      "author" : [ "Y Wang", "X Lin", "Q. Zhang" ],
      "venue" : "ACM CIKM : 805-810,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2013
    }, {
      "title" : "Efficient image and tag co-ranking: a bregman divergence optimization method",
      "author" : [ "L Wu", "Y Wang", "J Shepherd" ],
      "venue" : "ACM Multimedia:",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2013
    }, {
      "title" : "Exploiting Correlation Consensus: Towards Subspace Clustering for Multi-modal Data",
      "author" : [ "Y Wang", "X Lin", "L Wu", "W Zhang", "Q. Zhang" ],
      "venue" : "ACM Multimedia:",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Extreme Learning Machine (ELM for short) was originally developed based on single hidden-layer feed forward neural networks (SLFNs) [1].",
      "startOffset" : 132,
      "endOffset" : 135
    }, {
      "referenceID" : 1,
      "context" : "Thus, ELM, with its variants [2,3], has been widely applied in many fields [4,5].",
      "startOffset" : 29,
      "endOffset" : 34
    }, {
      "referenceID" : 2,
      "context" : "Thus, ELM, with its variants [2,3], has been widely applied in many fields [4,5].",
      "startOffset" : 29,
      "endOffset" : 34
    }, {
      "referenceID" : 3,
      "context" : "Thus, ELM, with its variants [2,3], has been widely applied in many fields [4,5].",
      "startOffset" : 75,
      "endOffset" : 80
    }, {
      "referenceID" : 4,
      "context" : "Thus, ELM, with its variants [2,3], has been widely applied in many fields [4,5].",
      "startOffset" : 75,
      "endOffset" : 80
    }, {
      "referenceID" : 5,
      "context" : "In the context of feature selection in time series data analysis, most of the current methods adopt such a framework that ranks subsequences according to their individual discriminative power to the target class and then selects top-k ranked subsequences[6].",
      "startOffset" : 254,
      "endOffset" : 257
    }, {
      "referenceID" : 6,
      "context" : "Shapelets was introduced as a primitive for time series data mining [7] and was utilized in classifying time series data [8,9].",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 7,
      "context" : "Shapelets was introduced as a primitive for time series data mining [7] and was utilized in classifying time series data [8,9].",
      "startOffset" : 121,
      "endOffset" : 126
    }, {
      "referenceID" : 8,
      "context" : "Shapelets was introduced as a primitive for time series data mining [7] and was utilized in classifying time series data [8,9].",
      "startOffset" : 121,
      "endOffset" : 126
    }, {
      "referenceID" : 9,
      "context" : "Shapelets transformed classification methods [10,11] were proposed to separate the processing of shapelets selection and classification.",
      "startOffset" : 45,
      "endOffset" : 52
    }, {
      "referenceID" : 10,
      "context" : "Shapelets transformed classification methods [10,11] were proposed to separate the processing of shapelets selection and classification.",
      "startOffset" : 45,
      "endOffset" : 52
    }, {
      "referenceID" : 11,
      "context" : "Nevertheless, shapelets based classification methods have been widely discussed and used in many real applications [12,13,14].",
      "startOffset" : 115,
      "endOffset" : 125
    }, {
      "referenceID" : 12,
      "context" : "Nevertheless, shapelets based classification methods have been widely discussed and used in many real applications [12,13,14].",
      "startOffset" : 115,
      "endOffset" : 125
    }, {
      "referenceID" : 13,
      "context" : "Nevertheless, shapelets based classification methods have been widely discussed and used in many real applications [12,13,14].",
      "startOffset" : 115,
      "endOffset" : 125
    }, {
      "referenceID" : 10,
      "context" : "Some works [11,15] detect this problem and use clustering or pruning methods to remove the redundant,but still exist redundant shapelets, also, the k value is determined from experiments.",
      "startOffset" : 11,
      "endOffset" : 18
    }, {
      "referenceID" : 14,
      "context" : "Some works [11,15] detect this problem and use clustering or pruning methods to remove the redundant,but still exist redundant shapelets, also, the k value is determined from experiments.",
      "startOffset" : 11,
      "endOffset" : 18
    }, {
      "referenceID" : 8,
      "context" : "In order to improve the efficiency of shapelets based classification method, we follow the method proposed in [9], which transformed the data sets through SAX method and decreased the time complexity to O(nm).",
      "startOffset" : 110,
      "endOffset" : 113
    }, {
      "referenceID" : 15,
      "context" : "In our work, we use the diversity graph[16] to find a general method to extract diversified top-k shapelets.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 15,
      "context" : "According to [16], find top-k results falling into two categories: incremental manner and bounding manner.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 10,
      "context" : "Followed [11], we set min-length and max-length of subsequences to generate shapeless are m/11 and m/2 separately, m is the length of each time series.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 14,
      "context" : "Because reference[15] has verified that ShapeletSelection can remove more redundant shapelets than other similar methods, we only compared the optimal shapelets sets between DivTopkShapelets and ShapeletSelection when the two algorithms all have the best classification,as shown in Fig.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 16,
      "context" : "For future work, we plan to leverage multi-view feature representations [17,18,19,20,21,22,23] to achieve the performance improvement.",
      "startOffset" : 72,
      "endOffset" : 94
    }, {
      "referenceID" : 17,
      "context" : "For future work, we plan to leverage multi-view feature representations [17,18,19,20,21,22,23] to achieve the performance improvement.",
      "startOffset" : 72,
      "endOffset" : 94
    }, {
      "referenceID" : 18,
      "context" : "For future work, we plan to leverage multi-view feature representations [17,18,19,20,21,22,23] to achieve the performance improvement.",
      "startOffset" : 72,
      "endOffset" : 94
    }, {
      "referenceID" : 19,
      "context" : "For future work, we plan to leverage multi-view feature representations [17,18,19,20,21,22,23] to achieve the performance improvement.",
      "startOffset" : 72,
      "endOffset" : 94
    }, {
      "referenceID" : 20,
      "context" : "For future work, we plan to leverage multi-view feature representations [17,18,19,20,21,22,23] to achieve the performance improvement.",
      "startOffset" : 72,
      "endOffset" : 94
    }, {
      "referenceID" : 21,
      "context" : "For future work, we plan to leverage multi-view feature representations [17,18,19,20,21,22,23] to achieve the performance improvement.",
      "startOffset" : 72,
      "endOffset" : 94
    }, {
      "referenceID" : 22,
      "context" : "For future work, we plan to leverage multi-view feature representations [17,18,19,20,21,22,23] to achieve the performance improvement.",
      "startOffset" : 72,
      "endOffset" : 94
    } ],
    "year" : 2016,
    "abstractText" : "ELM (Extreme Learning Machine) is a single hidden layer feed-forward network, where the weights between input and hidden layer are initialized randomly. ELM is efficient due to its utilization of the analytical approach to compute weights between hidden and output layer. However, ELM still fails to output the semantic classification outcome. To address such limitation, in this paper, we propose a diversified top-k shapelets transform framework, where the shapelets are the subsequences i.e., the best representative and interpretative features of each class. As we identified, the most challenge problems are how to extract the best k shapelets in original candidate sets and how to automatically determine the k value. Specifically, we first define the similar shapelets and diversified top-k shapelets to construct diversity shapelets graph. Then, a novel diversity graph based top-k shapelets extraction algorithm named as DivTopkshapelets is proposed to search top-k diversified shapelets. Finally, we propose a shapelets transformed ELM algorithm named as DivShapELM to automatically determine the k value, which is further utilized for time series classification. The experimental results over public data sets demonstrate that the proposed approach significantly outperforms traditional ELM algorithm in terms of effectiveness and efficiency.",
    "creator" : "LaTeX with hyperref package"
  }
}