{
  "name" : "1511.06335.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Junyuan Xie" ],
    "emails" : [ "jxie@cs.washington.edu", "rbg@fb.com", "ali@cs.washington.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Clustering is central to many data-driven application domains and has been studied extensively in terms of distance functions and grouping algorithms. Relatively little work has focused on learning representations for clustering. In this paper, we propose Deep Embedded Clustering (DEC), a method that simultaneously learns feature representations and cluster assignments using deep neural networks. DEC learns a mapping from the data space to a lower-dimensional feature space in which it iteratively optimizes a clustering objective. Our experimental evaluations on image and text corpora show significant improvement over state-of-theart methods."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Clustering, an essential data analysis and visualization tool, has been studied extensively in unsupervised machine learning from different perspectives: What defines a cluster? What is the right distance metric? How to efficiently group instances into clusters? How to validate clusters? And so on. Numerous different distance functions and embedding methods have been explored in the literature. Relatively little work has focused on the unsupervised learning of the feature space in which to perform clustering.\nA notion of distance or dissimilarity is central to data clustering algorithms. Distance, in turn, relies on representing the data in some feature space. The k-means clustering algorithm (MacQueen et al., 1967), for example, uses the Euclidean distance between points in a given feature space, which for images might be raw pixels or gradient-orientation histograms. The choice of feature space is customarily left as an implementation detail for the end-user to determine. Yet it is clear that this choice is crucial; for all but the simplest image datasets, clustering with Euclidean distance on raw pixels is completely ineffective. In this paper, we revisit cluster analysis and ask: Can the feature space and cluster memberships be solved for jointly?\nWe take inspiration from recent work on deep learning for computer vision (Krizhevsky et al., 2012; Girshick et al., 2014; Zeiler & Fergus, 2014; Long et al., 2014), where clear gains on benchmark tasks have resulted from learning better features. These improvements, however, were obtained with supervised learning, whereas our goal is unsupervised data clustering. To this end, we define a parameterized non-linear mapping from the data space X to a lower-dimensional feature space Z, where we optimize a clustering objective. Unlike previous work, which operates on the data space or a shallow linear embedded space, we use stochastic gradient descent (SGD) via backpropagation on a clustering objective to learn the mapping, which is parameterized by a deep neural network. We refer to the clustering algorithm as Deep Embedded Clustering, or DEC.\nOptimizing DEC is challenging. We want to simultaneously solve for cluster assignment and the underlying feature representation. However, unlike in supervised learning, we cannot train our deep network with labeled data. Instead we propose to iteratively refine clusters with an auxiliary target\nar X\niv :1\n51 1.\n06 33\n5v 1\n[ cs\n.L G\n] 1\n9 N\nov 2\ndistribution derived from the current soft cluster assignment. This process gradually improves the clustering as well as the feature representation.\nOur experiments show significant improvements over state-of-the-art clustering methods in terms of both accuracy and running time on image and textual datasets. We evaluate DEC on MNIST (LeCun et al., 1998), STL (Coates et al., 2011), and REUTERS (Lewis et al., 2004), and compare it with standard and state-of-the-art clustering methods (Nie et al., 2011; Yang et al., 2010). In addition, our experiments show that DEC is significantly less sensitive to the choice of hyperparameters compared to state-of-the-art methods. This robustness is an important property of our clustering algorithm since, when applied to real data, supervision is not available for hyperparameter cross-validation.\nOur contributions are: (a) joint optimization of deep embedding and clustering; (b) a novel iterative refinement via soft assignment; and (c) state-of-the-art clustering results in terms of clustering accuracy and speed. Our Caffe-based (Jia et al., 2014) implementation of DEC will be available at https://github.com/piiswrong/dec."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "Clustering has been extensively studied in machine learning in terms of feature selection (Boutsidis et al., 2009; Liu & Yu, 2005; Alelyani et al., 2013), distance functions (Xing et al., 2002; Xiang et al., 2008), grouping methods (MacQueen et al., 1967; Von Luxburg, 2007; Li et al., 2004), and cluster validation (Halkidi et al., 2001). Space does not allow for a comprehensive literature study and we refer readers to (Aggarwal & Reddy, 2013) for a survey.\nOne branch of popular methods for clustering are k-means (MacQueen et al., 1967), and Gaussian Mixture Models (GMM) (Bishop, 2006). These methods are fast and applicable to a wide range of problems. However, their distance metrics are limited to the original data space and they tend to be ineffective when input dimensionality is high (Steinbach et al., 2004).\nSeveral variants of k-means have been proposed to address issues with higher-dimensional input spaces. De la Torre & Kanade (2006); Ye et al. (2008) perform joint dimensionality reduction and clustering by first clustering the data with k-means and then projecting the data into a lower dimensions where the inter-cluster variance is maximized. This process is repeated in EM-style iterations until convergence. However, this framework is limited to linear embedding; our method employs deep neural networks to perform non-linear embedding that is necessary for more complex real-world data.\nSpectral clustering and its variants have gained popularity recently (Von Luxburg, 2007). They allow more flexible distance metrics and generally perform better than k-means. Combining spectral clustering and embedding has been explored in Yang et al. (2010); Nie et al. (2011). Tian et al. (2014) proposes an algorithm based on spectral clustering, but replaces eigenvalue decomposition with deep autoencoder, which improves performance but further increases memory consumption.\nMost spectral clustering algorithms need to compute the full graph Laplacian matrix and therefore have quadratic or super quadratic complexities in the number of data points. This means they need specialized machines with large memory for any dataset larger than a few tens of thousands of points. In order to scale spectral clustering to large datasets, approximate algorithms were invented to trade off performance for speed (Yan et al., 2009). Our method, however, is linear in the number of data points and scales gracefully to big data.\nMinimizing the Kullback-Leibler (KL) divergence between a data distribution and an embedded distribution has been used for data visualization and dimensionality reduction (van der Maaten & Hinton, 2008). T-SNE, for instance, is a non-parametric algorithm in this school and a parametric variant of t-SNE (van der Maaten, 2009) uses deep neural network to parametrize the embedding. The complexity of t-SNE isO(n2), where n is the number of data points, but it can be approximated in O(n log n) (van Der Maaten, 2014).\nWe take inspiration from parametric t-SNE. Instead of minimizing KL divergence to produce an embedding that is faithful to distances in the original data space, we define a centroid-based probability distribution and minimize its KL divergence to an auxiliary target distribution to simultaneously improve clustering assignment and feature representation. A centroid-based method also has the benefit of reducing complexity to O(nk), where k is the number of centroids."
    }, {
      "heading" : "3 DEEP EMBEDDED CLUSTERING",
      "text" : "Consider the problem of clustering a set of n points {xi ∈ X}ni=1 into k clusters, each represented by a centroid µj , j = 1, . . . , k. Instead of clustering directly in the data spaceX , we propose to first transform the data with a non-linear mapping fθ : X → Z, where θ corresponds to learnable parameters and Z is the feature space. The dimensionality of Z is typically much smaller than X in order to avoid the “curse of dimensionality” (Bellman, 1961). To parametrize fθ, deep neural networks (DNNs) are a natural choice due to their theoretical function approximation properties (Hornik, 1991) and their demonstrated feature learning capabilities (Bengio et al., 2013).\nThe proposed algorithm (DEC) clusters data by simultaneously learning a set of k cluster centers {µj ∈ Z}kj=1 in the feature space Z and the parameters θ of the DNN that maps data points into Z. DEC has two phases: (1) parameter initialization with a deep autoencoder (Vincent et al., 2010) and (2) parameter optimization (i.e., clustering), where we iterate between computing an auxiliary target distribution and minimizing Kullback–Leibler (KL) divergence to it. We start by describing phase (2) (parameter optimization / clustering) given an initial estimate of θ and {µj}kj=1."
    }, {
      "heading" : "3.1 CLUSTERING WITH KL DIVERGENCE",
      "text" : "Given an initial estimate of the deep mapping (fθ) and the initial cluster centroids ({µj}kj=1), we propose to improve the clustering using an unsupervised algorithm that alternates between two steps. In the first step, we compute a soft assignment between the embedded points and the cluster centroids. In the second step, we train the deep mapping and refine the clusters by learning from current high confidence assignments using an auxiliary target distribution. This process is repeated until convergence criteria is met."
    }, {
      "heading" : "3.1.1 SOFT ASSIGNMENT",
      "text" : "Following van der Maaten & Hinton (2008) we use the Student’s t-distribution as a kernel to measure the similarity between embedded point zi and centroid µj :\nqij = (1 + ‖zi − µj‖2/α)− α+1 2∑\nj′(1 + ‖zi − µj′‖2/α)− α+1 2\n, (1)\nwhere zi = fθ(xi) ∈ Z corresponds to xi ∈ X after embedding, α are the degrees of freedom of the Student’s t-distribution and qij can be interpreted as the probability of assigning sample i to cluster j (i.e., a soft assignment). Since we cannot cross-validate α on a validation set in the unsupervised setting, and learning it is superfluous (van der Maaten, 2009), we let α = 1 for all experiments."
    }, {
      "heading" : "3.1.2 KL DIVERGENCE MINIMIZATION",
      "text" : "We propose to iteratively refine the clusters by learning from their high confidence assignments with the help of an auxiliary target distribution. Specifically, our model is trained by matching the soft assignment to the target distribution. To this end, we define our objective as a KL divergence loss between the soft assignments qi and the auxiliary distribution pi as follows:\nL = KL(P‖Q) = ∑ i ∑ j pij log pij qij . (2)\nThe choice of target distribution P is crucial for DEC’s performance. A naive approach would be setting each pi to a one-hot distribution (to the nearest centroid) for data points above a confidence threshold and ignore the rest. However, because qi are soft assignments, it is more natural and flexible to use probabilistic labels. Specifically, we would like our target distribution to have the following properties: (1) strengthen predictions (i.e., improve cluster purity), (2) put more emphasis on data points assigned with high confidence, and (3) normalize loss contribution of each centroid to prevent large clusters from distorting the hidden feature space.\nIn our experiments, we compute p by first raising q to the second power and then normalizing by frequency per cluster:\npij = q2ij/fj∑ j′ q 2 ij′/fj′ , (3)\nUnder review as a conference paper at ICLR 2016\ni n p u t r e c o n s t r u c t i o n f e a t u r e\nwhere fj = ∑ i qij are soft cluster frequencies. Please refer to section 5.1 for discussions on empirical properties of L and P ."
    }, {
      "heading" : "3.1.3 OPTIMIZATION",
      "text" : "We jointly optimize the cluster centers {µj}j and DNN parameters θ using Stochastic Gradient Descent (SGD) with momentum. The gradients of L with respect to feature-space embedding of each data point zi and each cluster centroid µj are computed as:\n∂L ∂zi = α+ 1 α ∑ j (1 + ‖zi − µj‖2 α )−1(pij − qij)(zi − µj), (4)\n∂L ∂µj = −α+ 1 α ∑ i (1 + ‖zi − µj‖2 α )−1(pij − qij)(zi − µj). (5)\nThe gradients ∂L/∂zi are then passed down to the DNN and used in standard backpropagation to compute the DNN’s parameter gradient ∂L/∂θ. For the purpose of discovering cluster assignments, we stop our procedure when less than tol% of points change cluster assignment between two consecutive iterations."
    }, {
      "heading" : "3.2 PARAMETER INITIALIZATION",
      "text" : "In order to perform clustering with DEC, we need initial estimates of the DNN parameters θ and the cluster centroids {µj}j . We initialize the network layer by layer. Each layer is a denoising autoencoder trained to reconstruct the previous layer’s output after random corruption (Vincent et al., 2010). A denoising autoencoder is a two layer neural network defined as:\nx̃ ∼ Dropout(x) (6) h = g1(W1x̃+ b1) (7)\nh̃ ∼ Dropout(h) (8) y = g2(W2h̃+ b2) (9)\nwhere Dropout(·) (Srivastava et al., 2014) is a stochastic mapping that randomly sets a portion of its input dimensions to 0, g1 and g2 are activation functions for encoding and decoding layer respectively, and θ = {W1, b1,W2, b2} are model parameters. Training is performed by minimizing the least-squares loss ‖x− y‖22. After training of one layer, we use its output h as the input to train the next layer. We use rectified linear units (ReLUs) (Nair & Hinton, 2010) in all encoder/decoder pairs, except for g2 of the first pair (it needs to reconstruct input data that may have positive and negative values, such as zero-mean images) and g1 of the last pair (so the final data embedding retains full information (Vincent et al., 2010)).\nAfter greedy layer-wise training, we concatenate all encoder layers followed by all decoder layers, in reverse layer-wise training order, to form a deep autoencoder and then finetune it to minimize\nreconstruction loss. The final result of the pretraining stage is a multilayer deep autoencoder with a bottleneck coding layer in the middle. We then discard the decoder layers and use the encoder layers as our initial mapping between the data space and the feature space, as shown in Fig. 1.\nTo initialize the cluster centers, we pass the data through the initialized DNN to get embedded data points and then perform standard k-means clustering in the feature space Z to obtain k initial centroids {µj}kj=1."
    }, {
      "heading" : "4 EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "4.1 DATASETS",
      "text" : "We evaluate the proposed method (DEC) on one text dataset and two image datasets and compare it against other algorithms including k-means, LDGMI (Yang et al., 2010), and SEC (Nie et al., 2011). LDGMI and SEC are spectral clustering based algorithms that use Laplacian matrix and various transformations to improve clustering performance. Empirical evidence reported in Yang et al. (2010); Nie et al. (2011) shows that LDMGI and SEC outperform traditional spectral clustering methods on a wide range of datasets. We show qualitative and quantitative results that demonstrate the benefit of DEC compared to LDGMI and SEC.\nIn order to study the performance and generality of different algorithms, we perform experiment on two image datasets and one text data set:\n• MNIST: The MNIST dataset consists of 70000 handwritten digits of 28-by-28 pixel size. The digits are centered and size-normalized (LeCun et al., 1998).\n• STL-10: This is a dataset of 96-by-96 color images. There are 10 classes with 1300 examples each. It also contains 100000 unlabeled images of the same resolution (Coates et al., 2011). We also used the unlabeled set when training our autoencoders. Similar to Doersch et al. (2012), we concatenated HOG feature and a 8-by-8 color map to use as input to all algorithms.\n• REUTERS: Reuters contains about 810000 English news stories labeled with a category tree (Lewis et al., 2004). We used the four root categories: corporate/industrial, government/social, markets, and economics as labels and further pruned all documents that are labeled by multiple root categories to get 685071 articles. We then computed tf-idf features on the 2000 most frequently occurring word stems. Since some algorithms does not scale to the full Reuters dataset, we also sampled a random subset of 10000 examples, which we call REUTERS-10k, for comparison purposes.\nA summary of dataset statistics is shown in Table 1. For all algorithms, we normalize all datasets so that 1d‖xi‖ 2 2 is approximately 1, where d is the dimensionality of the data space point xi ∈ X ."
    }, {
      "heading" : "4.2 EVALUATION METRIC",
      "text" : "We use the standard unsupervised evaluation metric and protocols for evaluations and comparisons to other algorithms Yang et al. (2010). For all algorithms we set the number of clusters to the number of ground-truth categories and evaluate performance with unsupervised clustering accuracy (ACC ):\nACC = max m\n∑n i=1 1{li = m(ci)}\nn , (10)\nwhere li is the ground-truth label, ci is the cluster assignment produced by the algorithm, and m ranges over all possible one-to-one mappings between clusters and labels.\nIntuitively this metric takes a cluster assignment from an unsupervised algorithm and a ground truth assignment and then finds the best matching between them. The best mapping can be efficiently computed by the Hungarian algorithm (Kuhn, 1955)."
    }, {
      "heading" : "4.3 IMPLEMENTATION",
      "text" : "Determining hyperparameters by cross-validation on a validation set is not an option in unsupervised clustering. Thus we use commonly used parameters for DNNs and avoid dataset specific tuning as much as possible. Specifically, inspired by van der Maaten (2009), we set network dimensions to d–500–500–2000–10 for all datasets, where d is the data-space dimension, which varies between datasets. All layers are densely (fully) connected.\nDuring greedy layer-wise pretraining we initialize the weights to random numbers drawn from a zero-mean Gaussian distribution with a standard deviation of 0.01. Each layer is pretrained for 50000 iterations with a dropout rate of 20%. The entire deep autoencoder is further finetuned for 100000 iterations without dropout. For both layer-wise pretraining and end-to-end finetuning of the autoencoder the minibatch size is set to 256, starting learning rate is set to 0.1, which is divided by 10 every 20000 iterations, and weight decay is set to 0. All of the above parameters are set to achieve a reasonably good reconstruction loss and are held constant across all datasets. Dataset-specific settings of these parameters might improve performance on each dataset, but we refrain from this type of unrealistic parameter tuning. In the entropy minimization phase, we run k-means with 20 restarts to initialize the centroids and then backpropagate with a constant learning rate of 0.01. The convergence threshold is set to tol = 0.1%. Our implementation is based on Python and Caffe (Jia et al., 2014) and will be publicly released.\nFor all baseline algorithms, we perform 20 random restarts when initializing centroids and pick the result with the best objective value. For a fair comparison with previous work (Yang et al., 2010), we vary one hyperparameter for each algorithm over 9 possible choices and report the best accuracy in Table 2 and the range of accuracies in Fig. 2. For LDGMI and SEC, we use the same parameter and range as in their corresponding papers. For our proposed algorithm, we vary λ, the parameter that controls annealing speed, over 2i × 10, i = 0, 1, ..., 8. Since k-means do not have tunable hyperparameters, we simply run them 9 times. GMM performs similar to k-means so we\nonly report k-means. Traditional spectral clustering performs worse than LDGMI and SEC so we only report the latter (Yang et al., 2010; Nie et al., 2011)."
    }, {
      "heading" : "4.4 EXPERIMENT RESULTS",
      "text" : "We evaluate the performance of our algorithm both quantitatively and qualitatively. In Table 2, we report the best performance, over 9 hyperparameter settings, of each algorithm. Note that DEC outperforms all other methods, sometimes with a significant margin. To demonstrate the effectiveness of end-to-end training, we also show the results from freezing the mapping during clustering. We find that “DEC w/o backprop” generally performs worse than DEC.\nIn order to investigate the effect of hyperparameters, we plot the accuracy of each method under all 9 settings (Fig. 2). We observe that DEC is more consistent across hyperparameter ranges compared to LDGMI and SEC. For DEC, hyperparameter λ = 40 gives near optimal performance on all dataset whereas for other algorithms the optimal hyperparameter varies widely. Moreover, DEC can process the entire REUTERS dataset in half an hour with GPU acceleration while the second best algorithms, LDGMI and SEC, would need months of computation time and terabytes of memory. We, indeed, could not run these methods on the full REUTERS dataset and report N/A in Table 2 (GPU adaptation of these methods is non-trivial).\nIn Fig. 3 we show 10 top scoring images from each cluster in MNIST and STL. Each row corresponds to a cluster and images are sorted from left to right based on their distance to the cluster center. We observe that for MNIST, DEC’s cluster assignment corresponds to natural clusters very well, with the exception of confusing 4 and 9, while for STL, DEC is mostly correct with airplanes, trucks and cars, but spends part of its attention on poses instead of categories when it comes to animal classes."
    }, {
      "heading" : "5 DISCUSSION",
      "text" : ""
    }, {
      "heading" : "5.1 ASSUMPTIONS AND OBJECTIVE",
      "text" : "The underlying assumption of DEC is that the initial classifier’s high confidence predictions are mostly correct. To verify that this assumption holds for our task and that our choice of P has the desired properties, we plot the magnitude of the gradient of L with respect to each embedded point, |∂L/∂zi|, against its similarity, qij , to a randomly chosen MNIST cluster j (Fig. 4). We observe points that are closer to the cluster center (large qij) contribute more to the gradient. We also show the raw images of 10 data points at each 10 percentile sorted by qij . Instances with higher\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7\nqij\n0.02\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n|L x i |\nsimilarity are more canonical examples of “5”. As confidence decreases, instances become more ambiguous and eventually turn into a mislabeled “8” suggesting the soundness of our assumption."
    }, {
      "heading" : "5.2 CONTRIBUTION OF ITERATIVE OPTIMIZATION",
      "text" : "In Fig. 5 we visualize the progression of the embedded representation of a random subset of MNIST during training. For visualization we use t-SNE (van der Maaten & Hinton, 2008) applied to the embedded points zi. It is clear that the clusters are becoming increasingly well separated. Fig. 5 (f) shows how accuracy correspondingly improves over SGD epochs."
    }, {
      "heading" : "5.3 CONTRIBUTION OF AUTOENCODER INITIALIZATION",
      "text" : "To better understand the contribution of each component, we show the performance of all algorithms with autoencoder features in Table 3. We observe that SEC and LDMGI’s performance do not change significantly with autoencoder feature, while k-means improved but is still below DEC. This demonstrates the power of deep embedding and benefit of fine-tuning with the proposed KL divergence based objective."
    }, {
      "heading" : "5.4 PERFORMANCE ON IMBALANCED DATA",
      "text" : "In order to study the effect of imbalanced data, we sample subsets of MNIST with various retention rates and the results are shown in Table 4. For minimum retention rate rmin, data points of class 0 will be kept with probability rmin and class 9 with probability 1, with the other classes linearly in between. As a result the largest cluster will be 1/rmin times as large as the smallest one. From the table we can see that DEC is robust against cluster size variation. We also observe that KL divergence minimization (DEC) consistently improves clustering accuracy after autoencoder and k-means initialization (shown as AE+k-means)."
    }, {
      "heading" : "6 CONCLUSION",
      "text" : "This paper presents Deep Embedded Clustering, or DEC—an algorithm that clusters a set of data points in a jointly optimized feature space. DEC works by iteratively optimizing a KL divergence based clustering objective with a self-training target distribution. Our method can be viewed as an unsupervised extension of semisupervised self-training. Our framework provide a way to learn a representation specialized for clustering without groundtruth cluster membership labels.\nEmpirical studies demonstrate the strength of our proposed algorithm. DEC offers improved performance as well as robustness with respect to hyperparameter settings, which is particularly important in unsupervised tasks since cross-validation is not possible. DEC also has the virtue of linear complexity in the number of data points which allows it to scale to large datasets."
    } ],
    "references" : [ {
      "title" : "Data clustering: algorithms and applications",
      "author" : [ "Aggarwal", "Charu C", "Reddy", "Chandan K" ],
      "venue" : "CRC Press,",
      "citeRegEx" : "Aggarwal et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Aggarwal et al\\.",
      "year" : 2013
    }, {
      "title" : "Feature selection for clustering: A review",
      "author" : [ "Alelyani", "Salem", "Tang", "Jiliang", "Liu", "Huan" ],
      "venue" : "Data Clustering: Algorithms and Applications,",
      "citeRegEx" : "Alelyani et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Alelyani et al\\.",
      "year" : 2013
    }, {
      "title" : "Adaptive Control Processes: A Guided Tour",
      "author" : [ "R. Bellman" ],
      "venue" : null,
      "citeRegEx" : "Bellman,? \\Q1961\\E",
      "shortCiteRegEx" : "Bellman",
      "year" : 1961
    }, {
      "title" : "Representation learning: A review and new perspectives",
      "author" : [ "Bengio", "Yoshua", "Courville", "Aaron", "Vincent", "Pascal" ],
      "venue" : null,
      "citeRegEx" : "Bengio et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2013
    }, {
      "title" : "Pattern recognition and machine learning",
      "author" : [ "Bishop", "Christopher M" ],
      "venue" : "springer New York,",
      "citeRegEx" : "Bishop and M.,? \\Q2006\\E",
      "shortCiteRegEx" : "Bishop and M.",
      "year" : 2006
    }, {
      "title" : "Unsupervised feature selection for the k-means clustering problem",
      "author" : [ "Boutsidis", "Christos", "Drineas", "Petros", "Mahoney", "Michael W" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Boutsidis et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Boutsidis et al\\.",
      "year" : 2009
    }, {
      "title" : "An analysis of single-layer networks in unsupervised feature learning",
      "author" : [ "Coates", "Adam", "Ng", "Andrew Y", "Lee", "Honglak" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Coates et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Coates et al\\.",
      "year" : 2011
    }, {
      "title" : "Discriminative cluster analysis",
      "author" : [ "De la Torre", "Fernando", "Kanade", "Takeo" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Torre et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Torre et al\\.",
      "year" : 2006
    }, {
      "title" : "What makes paris look like paris",
      "author" : [ "Doersch", "Carl", "Singh", "Saurabh", "Gupta", "Abhinav", "Sivic", "Josef", "Efros", "Alexei" ],
      "venue" : "ACM Transactions on Graphics,",
      "citeRegEx" : "Doersch et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Doersch et al\\.",
      "year" : 2012
    }, {
      "title" : "Rich feature hierarchies for accurate object detection and semantic segmentation",
      "author" : [ "Girshick", "Ross", "Donahue", "Jeff", "Darrell", "Trevor", "Malik", "Jitendra" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Girshick et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Girshick et al\\.",
      "year" : 2014
    }, {
      "title" : "On clustering validation techniques",
      "author" : [ "Halkidi", "Maria", "Batistakis", "Yannis", "Vazirgiannis", "Michalis" ],
      "venue" : "Journal of Intelligent Information Systems,",
      "citeRegEx" : "Halkidi et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Halkidi et al\\.",
      "year" : 2001
    }, {
      "title" : "Approximation capabilities of multilayer feedforward networks",
      "author" : [ "Hornik", "Kurt" ],
      "venue" : "Neural networks,",
      "citeRegEx" : "Hornik and Kurt.,? \\Q1991\\E",
      "shortCiteRegEx" : "Hornik and Kurt.",
      "year" : 1991
    }, {
      "title" : "Caffe: Convolutional architecture for fast feature embedding",
      "author" : [ "Jia", "Yangqing", "Shelhamer", "Evan", "Donahue", "Jeff", "Karayev", "Sergey", "Long", "Jonathan", "Girshick", "Ross", "Guadarrama", "Sergio", "Darrell", "Trevor" ],
      "venue" : "arXiv preprint arXiv:1408.5093,",
      "citeRegEx" : "Jia et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Jia et al\\.",
      "year" : 2014
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "The hungarian method for the assignment problem",
      "author" : [ "Kuhn", "Harold W" ],
      "venue" : "Naval research logistics quarterly,",
      "citeRegEx" : "Kuhn and W.,? \\Q1955\\E",
      "shortCiteRegEx" : "Kuhn and W.",
      "year" : 1955
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "LeCun", "Yann", "Bottou", "Léon", "Bengio", "Yoshua", "Haffner", "Patrick" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "Rcv1: A new benchmark collection for text categorization research",
      "author" : [ "Lewis", "David D", "Yang", "Yiming", "Rose", "Tony G", "Li", "Fan" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2004
    }, {
      "title" : "Entropy-based criterion in categorical clustering",
      "author" : [ "Li", "Tao", "Ma", "Sheng", "Ogihara", "Mitsunori" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Li et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2004
    }, {
      "title" : "Toward integrating feature selection algorithms for classification and clustering",
      "author" : [ "Liu", "Huan", "Yu", "Lei" ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering,",
      "citeRegEx" : "Liu et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2005
    }, {
      "title" : "Fully convolutional networks for semantic segmentation",
      "author" : [ "Long", "Jonathan", "Shelhamer", "Evan", "Darrell", "Trevor" ],
      "venue" : "arXiv preprint arXiv:1411.4038,",
      "citeRegEx" : "Long et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Long et al\\.",
      "year" : 2014
    }, {
      "title" : "Some methods for classification and analysis of multivariate observations",
      "author" : [ "MacQueen", "James" ],
      "venue" : "In Proceedings of the fifth Berkeley symposium on mathematical statistics and probability,",
      "citeRegEx" : "MacQueen and James,? \\Q1967\\E",
      "shortCiteRegEx" : "MacQueen and James",
      "year" : 1967
    }, {
      "title" : "Rectified linear units improve restricted boltzmann machines",
      "author" : [ "Nair", "Vinod", "Hinton", "Geoffrey E" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Nair et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Nair et al\\.",
      "year" : 2010
    }, {
      "title" : "Spectral embedded clustering: A framework for in-sample and out-of-sample spectral clustering",
      "author" : [ "Nie", "Feiping", "Zeng", "Zinan", "Tsang", "Ivor W", "Xu", "Dong", "Zhang", "Changshui" ],
      "venue" : "IEEE Transactions on Neural Networks,",
      "citeRegEx" : "Nie et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Nie et al\\.",
      "year" : 2011
    }, {
      "title" : "Dropout: A simple way to prevent neural networks from overfitting",
      "author" : [ "Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan" ],
      "venue" : null,
      "citeRegEx" : "Srivastava et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2014
    }, {
      "title" : "The challenges of clustering high dimensional data",
      "author" : [ "Steinbach", "Michael", "Ertöz", "Levent", "Kumar", "Vipin" ],
      "venue" : "In New Directions in Statistical Physics,",
      "citeRegEx" : "Steinbach et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Steinbach et al\\.",
      "year" : 2004
    }, {
      "title" : "Learning deep representations for graph clustering",
      "author" : [ "Tian", "Fei", "Gao", "Bin", "Cui", "Qing", "Chen", "Enhong", "Liu", "Tie-Yan" ],
      "venue" : "In AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Tian et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Tian et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning a parametric embedding by preserving local structure",
      "author" : [ "van der Maaten", "Laurens" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Maaten and Laurens.,? \\Q2009\\E",
      "shortCiteRegEx" : "Maaten and Laurens.",
      "year" : 2009
    }, {
      "title" : "Accelerating t-SNE using tree-based algorithms",
      "author" : [ "van Der Maaten", "Laurens" ],
      "venue" : null,
      "citeRegEx" : "Maaten and Laurens.,? \\Q2014\\E",
      "shortCiteRegEx" : "Maaten and Laurens.",
      "year" : 2014
    }, {
      "title" : "Visualizing data using t-SNE",
      "author" : [ "van der Maaten", "Laurens", "Hinton", "Geoffrey" ],
      "venue" : null,
      "citeRegEx" : "Maaten et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Maaten et al\\.",
      "year" : 2008
    }, {
      "title" : "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion",
      "author" : [ "Vincent", "Pascal", "Larochelle", "Hugo", "Lajoie", "Isabelle", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine" ],
      "venue" : null,
      "citeRegEx" : "Vincent et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Vincent et al\\.",
      "year" : 2010
    }, {
      "title" : "A tutorial on spectral clustering",
      "author" : [ "Von Luxburg", "Ulrike" ],
      "venue" : "Statistics and computing,",
      "citeRegEx" : "Luxburg and Ulrike.,? \\Q2007\\E",
      "shortCiteRegEx" : "Luxburg and Ulrike.",
      "year" : 2007
    }, {
      "title" : "Learning a mahalanobis distance metric for data clustering and classification",
      "author" : [ "Xiang", "Shiming", "Nie", "Feiping", "Zhang", "Changshui" ],
      "venue" : "Pattern Recognition,",
      "citeRegEx" : "Xiang et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Xiang et al\\.",
      "year" : 2008
    }, {
      "title" : "Distance metric learning with application to clustering with side-information",
      "author" : [ "Xing", "Eric P", "Jordan", "Michael I", "Russell", "Stuart", "Ng", "Andrew Y" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Xing et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Xing et al\\.",
      "year" : 2002
    }, {
      "title" : "Fast approximate spectral clustering",
      "author" : [ "Yan", "Donghui", "Huang", "Ling", "Jordan", "Michael I" ],
      "venue" : "In ACM SIGKDD,",
      "citeRegEx" : "Yan et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Yan et al\\.",
      "year" : 2009
    }, {
      "title" : "Image clustering using local discriminant models and global integration",
      "author" : [ "Yang", "Yi", "Xu", "Dong", "Nie", "Feiping", "Yan", "Shuicheng", "Zhuang", "Yueting" ],
      "venue" : "IEEE Transactions on Image Processing,",
      "citeRegEx" : "Yang et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2010
    }, {
      "title" : "Discriminative k-means for clustering",
      "author" : [ "Ye", "Jieping", "Zhao", "Zheng", "Wu", "Mingrui" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Ye et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Ye et al\\.",
      "year" : 2008
    }, {
      "title" : "Visualizing and understanding convolutional networks",
      "author" : [ "Zeiler", "Matthew D", "Fergus", "Rob" ],
      "venue" : "In ECCV",
      "citeRegEx" : "Zeiler et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zeiler et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "In this paper, we revisit cluster analysis and ask: Can the feature space and cluster memberships be solved for jointly? We take inspiration from recent work on deep learning for computer vision (Krizhevsky et al., 2012; Girshick et al., 2014; Zeiler & Fergus, 2014; Long et al., 2014), where clear gains on benchmark tasks have resulted from learning better features.",
      "startOffset" : 195,
      "endOffset" : 285
    }, {
      "referenceID" : 9,
      "context" : "In this paper, we revisit cluster analysis and ask: Can the feature space and cluster memberships be solved for jointly? We take inspiration from recent work on deep learning for computer vision (Krizhevsky et al., 2012; Girshick et al., 2014; Zeiler & Fergus, 2014; Long et al., 2014), where clear gains on benchmark tasks have resulted from learning better features.",
      "startOffset" : 195,
      "endOffset" : 285
    }, {
      "referenceID" : 19,
      "context" : "In this paper, we revisit cluster analysis and ask: Can the feature space and cluster memberships be solved for jointly? We take inspiration from recent work on deep learning for computer vision (Krizhevsky et al., 2012; Girshick et al., 2014; Zeiler & Fergus, 2014; Long et al., 2014), where clear gains on benchmark tasks have resulted from learning better features.",
      "startOffset" : 195,
      "endOffset" : 285
    }, {
      "referenceID" : 15,
      "context" : "We evaluate DEC on MNIST (LeCun et al., 1998), STL (Coates et al.",
      "startOffset" : 25,
      "endOffset" : 45
    }, {
      "referenceID" : 6,
      "context" : ", 1998), STL (Coates et al., 2011), and REUTERS (Lewis et al.",
      "startOffset" : 13,
      "endOffset" : 34
    }, {
      "referenceID" : 16,
      "context" : ", 2011), and REUTERS (Lewis et al., 2004), and compare it with standard and state-of-the-art clustering methods (Nie et al.",
      "startOffset" : 21,
      "endOffset" : 41
    }, {
      "referenceID" : 22,
      "context" : ", 2004), and compare it with standard and state-of-the-art clustering methods (Nie et al., 2011; Yang et al., 2010).",
      "startOffset" : 78,
      "endOffset" : 115
    }, {
      "referenceID" : 34,
      "context" : ", 2004), and compare it with standard and state-of-the-art clustering methods (Nie et al., 2011; Yang et al., 2010).",
      "startOffset" : 78,
      "endOffset" : 115
    }, {
      "referenceID" : 12,
      "context" : "Our Caffe-based (Jia et al., 2014) implementation of DEC will be available at https://github.",
      "startOffset" : 16,
      "endOffset" : 34
    }, {
      "referenceID" : 5,
      "context" : "Clustering has been extensively studied in machine learning in terms of feature selection (Boutsidis et al., 2009; Liu & Yu, 2005; Alelyani et al., 2013), distance functions (Xing et al.",
      "startOffset" : 90,
      "endOffset" : 153
    }, {
      "referenceID" : 1,
      "context" : "Clustering has been extensively studied in machine learning in terms of feature selection (Boutsidis et al., 2009; Liu & Yu, 2005; Alelyani et al., 2013), distance functions (Xing et al.",
      "startOffset" : 90,
      "endOffset" : 153
    }, {
      "referenceID" : 32,
      "context" : ", 2013), distance functions (Xing et al., 2002; Xiang et al., 2008), grouping methods (MacQueen et al.",
      "startOffset" : 28,
      "endOffset" : 67
    }, {
      "referenceID" : 31,
      "context" : ", 2013), distance functions (Xing et al., 2002; Xiang et al., 2008), grouping methods (MacQueen et al.",
      "startOffset" : 28,
      "endOffset" : 67
    }, {
      "referenceID" : 17,
      "context" : ", 2008), grouping methods (MacQueen et al., 1967; Von Luxburg, 2007; Li et al., 2004), and cluster validation (Halkidi et al.",
      "startOffset" : 26,
      "endOffset" : 85
    }, {
      "referenceID" : 10,
      "context" : ", 2004), and cluster validation (Halkidi et al., 2001).",
      "startOffset" : 32,
      "endOffset" : 54
    }, {
      "referenceID" : 24,
      "context" : "However, their distance metrics are limited to the original data space and they tend to be ineffective when input dimensionality is high (Steinbach et al., 2004).",
      "startOffset" : 137,
      "endOffset" : 161
    }, {
      "referenceID" : 33,
      "context" : "In order to scale spectral clustering to large datasets, approximate algorithms were invented to trade off performance for speed (Yan et al., 2009).",
      "startOffset" : 129,
      "endOffset" : 147
    }, {
      "referenceID" : 1,
      "context" : ", 2009; Liu & Yu, 2005; Alelyani et al., 2013), distance functions (Xing et al., 2002; Xiang et al., 2008), grouping methods (MacQueen et al., 1967; Von Luxburg, 2007; Li et al., 2004), and cluster validation (Halkidi et al., 2001). Space does not allow for a comprehensive literature study and we refer readers to (Aggarwal & Reddy, 2013) for a survey. One branch of popular methods for clustering are k-means (MacQueen et al., 1967), and Gaussian Mixture Models (GMM) (Bishop, 2006). These methods are fast and applicable to a wide range of problems. However, their distance metrics are limited to the original data space and they tend to be ineffective when input dimensionality is high (Steinbach et al., 2004). Several variants of k-means have been proposed to address issues with higher-dimensional input spaces. De la Torre & Kanade (2006); Ye et al.",
      "startOffset" : 24,
      "endOffset" : 847
    }, {
      "referenceID" : 1,
      "context" : ", 2009; Liu & Yu, 2005; Alelyani et al., 2013), distance functions (Xing et al., 2002; Xiang et al., 2008), grouping methods (MacQueen et al., 1967; Von Luxburg, 2007; Li et al., 2004), and cluster validation (Halkidi et al., 2001). Space does not allow for a comprehensive literature study and we refer readers to (Aggarwal & Reddy, 2013) for a survey. One branch of popular methods for clustering are k-means (MacQueen et al., 1967), and Gaussian Mixture Models (GMM) (Bishop, 2006). These methods are fast and applicable to a wide range of problems. However, their distance metrics are limited to the original data space and they tend to be ineffective when input dimensionality is high (Steinbach et al., 2004). Several variants of k-means have been proposed to address issues with higher-dimensional input spaces. De la Torre & Kanade (2006); Ye et al. (2008) perform joint dimensionality reduction and clustering by first clustering the data with k-means and then projecting the data into a lower dimensions where the inter-cluster variance is maximized.",
      "startOffset" : 24,
      "endOffset" : 865
    }, {
      "referenceID" : 1,
      "context" : ", 2009; Liu & Yu, 2005; Alelyani et al., 2013), distance functions (Xing et al., 2002; Xiang et al., 2008), grouping methods (MacQueen et al., 1967; Von Luxburg, 2007; Li et al., 2004), and cluster validation (Halkidi et al., 2001). Space does not allow for a comprehensive literature study and we refer readers to (Aggarwal & Reddy, 2013) for a survey. One branch of popular methods for clustering are k-means (MacQueen et al., 1967), and Gaussian Mixture Models (GMM) (Bishop, 2006). These methods are fast and applicable to a wide range of problems. However, their distance metrics are limited to the original data space and they tend to be ineffective when input dimensionality is high (Steinbach et al., 2004). Several variants of k-means have been proposed to address issues with higher-dimensional input spaces. De la Torre & Kanade (2006); Ye et al. (2008) perform joint dimensionality reduction and clustering by first clustering the data with k-means and then projecting the data into a lower dimensions where the inter-cluster variance is maximized. This process is repeated in EM-style iterations until convergence. However, this framework is limited to linear embedding; our method employs deep neural networks to perform non-linear embedding that is necessary for more complex real-world data. Spectral clustering and its variants have gained popularity recently (Von Luxburg, 2007). They allow more flexible distance metrics and generally perform better than k-means. Combining spectral clustering and embedding has been explored in Yang et al. (2010); Nie et al.",
      "startOffset" : 24,
      "endOffset" : 1567
    }, {
      "referenceID" : 1,
      "context" : ", 2009; Liu & Yu, 2005; Alelyani et al., 2013), distance functions (Xing et al., 2002; Xiang et al., 2008), grouping methods (MacQueen et al., 1967; Von Luxburg, 2007; Li et al., 2004), and cluster validation (Halkidi et al., 2001). Space does not allow for a comprehensive literature study and we refer readers to (Aggarwal & Reddy, 2013) for a survey. One branch of popular methods for clustering are k-means (MacQueen et al., 1967), and Gaussian Mixture Models (GMM) (Bishop, 2006). These methods are fast and applicable to a wide range of problems. However, their distance metrics are limited to the original data space and they tend to be ineffective when input dimensionality is high (Steinbach et al., 2004). Several variants of k-means have been proposed to address issues with higher-dimensional input spaces. De la Torre & Kanade (2006); Ye et al. (2008) perform joint dimensionality reduction and clustering by first clustering the data with k-means and then projecting the data into a lower dimensions where the inter-cluster variance is maximized. This process is repeated in EM-style iterations until convergence. However, this framework is limited to linear embedding; our method employs deep neural networks to perform non-linear embedding that is necessary for more complex real-world data. Spectral clustering and its variants have gained popularity recently (Von Luxburg, 2007). They allow more flexible distance metrics and generally perform better than k-means. Combining spectral clustering and embedding has been explored in Yang et al. (2010); Nie et al. (2011). Tian et al.",
      "startOffset" : 24,
      "endOffset" : 1586
    }, {
      "referenceID" : 1,
      "context" : ", 2009; Liu & Yu, 2005; Alelyani et al., 2013), distance functions (Xing et al., 2002; Xiang et al., 2008), grouping methods (MacQueen et al., 1967; Von Luxburg, 2007; Li et al., 2004), and cluster validation (Halkidi et al., 2001). Space does not allow for a comprehensive literature study and we refer readers to (Aggarwal & Reddy, 2013) for a survey. One branch of popular methods for clustering are k-means (MacQueen et al., 1967), and Gaussian Mixture Models (GMM) (Bishop, 2006). These methods are fast and applicable to a wide range of problems. However, their distance metrics are limited to the original data space and they tend to be ineffective when input dimensionality is high (Steinbach et al., 2004). Several variants of k-means have been proposed to address issues with higher-dimensional input spaces. De la Torre & Kanade (2006); Ye et al. (2008) perform joint dimensionality reduction and clustering by first clustering the data with k-means and then projecting the data into a lower dimensions where the inter-cluster variance is maximized. This process is repeated in EM-style iterations until convergence. However, this framework is limited to linear embedding; our method employs deep neural networks to perform non-linear embedding that is necessary for more complex real-world data. Spectral clustering and its variants have gained popularity recently (Von Luxburg, 2007). They allow more flexible distance metrics and generally perform better than k-means. Combining spectral clustering and embedding has been explored in Yang et al. (2010); Nie et al. (2011). Tian et al. (2014) proposes an algorithm based on spectral clustering, but replaces eigenvalue decomposition with deep autoencoder, which improves performance but further increases memory consumption.",
      "startOffset" : 24,
      "endOffset" : 1606
    }, {
      "referenceID" : 2,
      "context" : "The dimensionality of Z is typically much smaller than X in order to avoid the “curse of dimensionality” (Bellman, 1961).",
      "startOffset" : 105,
      "endOffset" : 120
    }, {
      "referenceID" : 3,
      "context" : "To parametrize fθ, deep neural networks (DNNs) are a natural choice due to their theoretical function approximation properties (Hornik, 1991) and their demonstrated feature learning capabilities (Bengio et al., 2013).",
      "startOffset" : 195,
      "endOffset" : 216
    }, {
      "referenceID" : 29,
      "context" : "DEC has two phases: (1) parameter initialization with a deep autoencoder (Vincent et al., 2010) and (2) parameter optimization (i.",
      "startOffset" : 73,
      "endOffset" : 95
    }, {
      "referenceID" : 29,
      "context" : "Each layer is a denoising autoencoder trained to reconstruct the previous layer’s output after random corruption (Vincent et al., 2010).",
      "startOffset" : 113,
      "endOffset" : 135
    }, {
      "referenceID" : 23,
      "context" : "A denoising autoencoder is a two layer neural network defined as: x̃ ∼ Dropout(x) (6) h = g1(W1x̃+ b1) (7) h̃ ∼ Dropout(h) (8) y = g2(W2h̃+ b2) (9) where Dropout(·) (Srivastava et al., 2014) is a stochastic mapping that randomly sets a portion of its input dimensions to 0, g1 and g2 are activation functions for encoding and decoding layer respectively, and θ = {W1, b1,W2, b2} are model parameters.",
      "startOffset" : 165,
      "endOffset" : 190
    }, {
      "referenceID" : 29,
      "context" : "We use rectified linear units (ReLUs) (Nair & Hinton, 2010) in all encoder/decoder pairs, except for g2 of the first pair (it needs to reconstruct input data that may have positive and negative values, such as zero-mean images) and g1 of the last pair (so the final data embedding retains full information (Vincent et al., 2010)).",
      "startOffset" : 306,
      "endOffset" : 328
    }, {
      "referenceID" : 14,
      "context" : "Dataset # Points # classes Dimension % of largest class MNIST LeCun et al. (1998) 70000 10 784 11% STL-10 Coates et al.",
      "startOffset" : 62,
      "endOffset" : 82
    }, {
      "referenceID" : 6,
      "context" : "(1998) 70000 10 784 11% STL-10 Coates et al. (2011) 13000 10 1428 10% REUTERS-10K 10000 4 2000 43% REUTERS Lewis et al.",
      "startOffset" : 31,
      "endOffset" : 52
    }, {
      "referenceID" : 6,
      "context" : "(1998) 70000 10 784 11% STL-10 Coates et al. (2011) 13000 10 1428 10% REUTERS-10K 10000 4 2000 43% REUTERS Lewis et al. (2004) 685071 4 2000 43%",
      "startOffset" : 31,
      "endOffset" : 127
    }, {
      "referenceID" : 34,
      "context" : "We evaluate the proposed method (DEC) on one text dataset and two image datasets and compare it against other algorithms including k-means, LDGMI (Yang et al., 2010), and SEC (Nie et al.",
      "startOffset" : 146,
      "endOffset" : 165
    }, {
      "referenceID" : 22,
      "context" : ", 2010), and SEC (Nie et al., 2011).",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 22,
      "context" : ", 2010), and SEC (Nie et al., 2011). LDGMI and SEC are spectral clustering based algorithms that use Laplacian matrix and various transformations to improve clustering performance. Empirical evidence reported in Yang et al. (2010); Nie et al.",
      "startOffset" : 18,
      "endOffset" : 231
    }, {
      "referenceID" : 22,
      "context" : ", 2010), and SEC (Nie et al., 2011). LDGMI and SEC are spectral clustering based algorithms that use Laplacian matrix and various transformations to improve clustering performance. Empirical evidence reported in Yang et al. (2010); Nie et al. (2011) shows that LDMGI and SEC outperform traditional spectral clustering methods on a wide range of datasets.",
      "startOffset" : 18,
      "endOffset" : 250
    }, {
      "referenceID" : 15,
      "context" : "The digits are centered and size-normalized (LeCun et al., 1998).",
      "startOffset" : 44,
      "endOffset" : 64
    }, {
      "referenceID" : 6,
      "context" : "It also contains 100000 unlabeled images of the same resolution (Coates et al., 2011).",
      "startOffset" : 64,
      "endOffset" : 85
    }, {
      "referenceID" : 16,
      "context" : "• REUTERS: Reuters contains about 810000 English news stories labeled with a category tree (Lewis et al., 2004).",
      "startOffset" : 91,
      "endOffset" : 111
    }, {
      "referenceID" : 6,
      "context" : "It also contains 100000 unlabeled images of the same resolution (Coates et al., 2011). We also used the unlabeled set when training our autoencoders. Similar to Doersch et al. (2012), we concatenated HOG feature and a 8-by-8 color map to use as input to all algorithms.",
      "startOffset" : 65,
      "endOffset" : 183
    }, {
      "referenceID" : 34,
      "context" : "We use the standard unsupervised evaluation metric and protocols for evaluations and comparisons to other algorithms Yang et al. (2010). For all algorithms we set the number of clusters to the number of ground-truth categories and evaluate performance with unsupervised clustering accuracy (ACC ):",
      "startOffset" : 117,
      "endOffset" : 136
    }, {
      "referenceID" : 12,
      "context" : "Our implementation is based on Python and Caffe (Jia et al., 2014) and will be publicly released.",
      "startOffset" : 48,
      "endOffset" : 66
    }, {
      "referenceID" : 34,
      "context" : "For a fair comparison with previous work (Yang et al., 2010), we vary one hyperparameter for each algorithm over 9 possible choices and report the best accuracy in Table 2 and the range of accuracies in Fig.",
      "startOffset" : 41,
      "endOffset" : 60
    }, {
      "referenceID" : 34,
      "context" : "Traditional spectral clustering performs worse than LDGMI and SEC so we only report the latter (Yang et al., 2010; Nie et al., 2011).",
      "startOffset" : 95,
      "endOffset" : 132
    }, {
      "referenceID" : 22,
      "context" : "Traditional spectral clustering performs worse than LDGMI and SEC so we only report the latter (Yang et al., 2010; Nie et al., 2011).",
      "startOffset" : 95,
      "endOffset" : 132
    } ],
    "year" : 2017,
    "abstractText" : "Clustering is central to many data-driven application domains and has been studied extensively in terms of distance functions and grouping algorithms. Relatively little work has focused on learning representations for clustering. In this paper, we propose Deep Embedded Clustering (DEC), a method that simultaneously learns feature representations and cluster assignments using deep neural networks. DEC learns a mapping from the data space to a lower-dimensional feature space in which it iteratively optimizes a clustering objective. Our experimental evaluations on image and text corpora show significant improvement over state-of-theart methods.",
    "creator" : "LaTeX with hyperref package"
  }
}