{
  "name" : "1603.01670.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Network Morphism",
    "authors" : [ "Tao Wei", "Changhu Wang", "Rong Rui", "Chang Wen Chen" ],
    "emails" : [ "TAOWEI@BUFFALO.EDU", "CHW@MICROSOFT.COM", "YONGRUI@MICROSOFT.COM", "CHENCW@BUFFALO.EDU" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Deep convolutional neural networks (DCNNs) have achieved state-of-the-art results on diverse computer vision tasks such as image classification (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; Szegedy et al., 2014), object detection (Girshick et al., 2014; Girshick, 2015; Ren et al.,\n†Tao Wei performed this work while being an intern at Microsoft Research Asia.\nParent Network\nChild Network\nFigure 1: Illustration of network morphism. The child network is expected to inherit the entire knowledge from the parent network with the network function preserved. A variety of morphing types are illustrated. The change of segment AC represents the depth morphing: s → s + t; the inflated node r involves width and kernel size morphing; a subnet is embedded in segment CD, which is subnet morphing. Complex network morphism can also be achieved with a combination of these basic morphing operations.\n2015), and semantic segmentation (Long et al., 2014). However, training such a network is very time-consuming. It usually takes weeks or even months to train an effective deep network, let alone the exploration of diverse network settings. It is very much desired for these well-trained networks to be directly adopted for other related applications with minimum retraining.\nTo accomplish such an ideal goal, we need to systematically study how to morph a well-trained neural network to a new one with its network function completely preserved. We call such operations network morphism. Upon completion of such morphism, the child network shall not only inherit the entire knowledge from the parent network, but also be capable of growing into a more powerful one in much shortened training time as the process continues on. This is\nar X\niv :1\n60 3.\n01 67\n0v 1\n[ cs\n.L G\n] 5\nM ar\n2 01\nfundamentally different from existing work related to network knowledge transferring, which either tries to mimic a parent network’s outputs (Bucilu et al., 2006; Ba & Caruana, 2014; Romero et al., 2014), or pre-trains to facilitate the convergence and/or adapt to new datasets with possible total change in network function (Simonyan & Zisserman, 2014; Oquab et al., 2014).\nMathematically, a morphism is a structure-preserving map from one mathematical structure to another (Weisstein, 2002). In the context of neural networks, network morphism refers to a parameter-transferring map from a parent network to a child network that preserves its function and outputs. Although network morphism generally does not impose constraints on the architecture of the child network, we limit the investigation of network morphism to the expanding mode, which intuitively means that the child network is deeper and/or wider than its parent network. Fig. 1 illustrates the concept of network morphism, where a variety of morphing types are demonstrated including depth morphing, width morphing, kernel size morphing, and subnet morphing. In this work, we derive network morphism equations for a successful morphing operation to follow, based on which novel network morphism algorithms can be developed for all these morphing types. The proposed algorithms work for both classic multi-layer perceptron models and convolutional neural networks. Since in the proposed network morphism it is required that the output is unchanged, a complex morphing can be decomposed into basic morphing steps, and thus can be solved easily.\nDepth morphing is an important morphing type, since current top-notch neural networks are going deeper and deeper (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; Szegedy et al., 2014; He et al., 2015a). One heuristic approach is to embed an identity mapping layer into the parent network, which is referred as IdMorph. IdMorph is explored by a recent work (Chen et al., 2015), but is potentially problematic due to the sparsity of the identity layer, and might fail sometimes (He et al., 2015a). To overcome the issues associated with IdMorph, we introduce several practices for the morphism operation to follow, and propose a deconvolution-based algorithm for network depth morphing. This algorithm is able to asymptotically fill in all parameters with non-zero elements. In its worst case, the non-zero occupying rate of the proposed algorithm is still higher than IdMorph for an order of magnitude.\nAnother challenge the proposed network morphism will face is the dealing of the non-linearity in a neural network. Even the simple IdMorph method fails in this case, because it only works for idempotent functions1. In this work, to\n1An idempotent function ϕ is defined to satisfy ϕ ◦ ϕ = ϕ. This condition passes the ReLU function but fails on most of other commonly used activation functions, such as Sigmoid and TanH.\ndeal with the non-linearity, we introduce the concept of parametric-activation function family, which is defined as an adjoint function family for arbitrary non-linear activation function. It can reduce the non-linear operation to a linear one with a parameter that can be learned. Therefore, the network morphism of any continuous non-linear activation neurons can be solved.\nTo the best of our knowledge, this is the first work about network morphism, except the recent work (Chen et al., 2015) that introduces the IdMorph. We conduct extensive experiments to show the effectiveness of the proposed network morphism learning scheme on widely used benchmark datasets for both classic and convolutional neural networks. The effectiveness of basic morphing operations are also verified. Furthermore, we show that the proposed network morphism is able to internally regularize the network, that typically leads to an improved performance. Finally, we also successfully morph the well-known 16- layered VGG net (Simonyan & Zisserman, 2014) to a better performing model, with only 115 of the training time comparing against the training from scratch."
    }, {
      "heading" : "2. Related Work",
      "text" : "We briefly introduce recent work related to network morphism and identify the differences from this work.\nMimic Learning. A series of work trying to mimic the teacher network with a student network have been developed, which usually need learning from scratch. For example, (Bucilu et al., 2006) tried to train a lighter network by mimicking an ensemble network. (Ba & Caruana, 2014) extended this idea, and used a shallower but wider network to mimic a deep and wide network. In (Romero et al., 2014), the authors adopted a deeper but narrower network to mimic a deep and wide network. The proposed network morphism scheme is different from these algorithms, since instead of mimicking, its goal is to make the child network directly inherit the intact knowledge (network function) from the parent network. This allows network morphism to achieve the same performance. That is why the networks are called parent and child, instead of teacher and student. Another major difference is that the child network is not learned from scratch.\nPre-training and Transfer Learning. Pre-training (Simonyan & Zisserman, 2014) is a strategy proposed to facilitate the convergence of very deep neural networks, and transfer learning2 (Simonyan & Zisserman, 2014; Oquab et al., 2014) is introduced to overcome the overfitting problem when training large neural networks on relatively small\n2Although transfer learning in its own concept is very general, here it is referred as a technique used for DCNNs to pre-train the model on one dataset and then adapt to another.\ndatasets. They both re-initialize the last few layers of the parent network with the other layers remaining the same (or refined in a lighter way). Their difference is that pretraining continues to train the child network on the same dataset, while transfer learning continues on a new one. However, these two strategies totally alter the parameters in the last few layers, as well as the network function.\nNet2Net. Net2Net is a recent work proposed in (Chen et al., 2015). Although it targets at the same problem, there are several major differences between network morphism and Net2Net. First, the solution of Net2Net is still restricted to the IdMorph approach, while NetMorph is the first to make it possible to embed non-identity layers. Second, Net2Net’s operations only work for idempotent activation functions, while NetMorph is the first to handle arbitrary non-linear activation functions. Third, Net2Net’s discussion is limited to width and depth changes, while NetMorph studies a variety of morphing types, including depth, width, kernel size, and subnet changes. Fourth, Net2Net needs to separately consider depth and width changes, while NetMorph is able to simultaneously conduct depth, width, and kernel size morphing in a single operation."
    }, {
      "heading" : "3. Network Morphism",
      "text" : "We shall first discuss the depth morphing in the linear case, which actually also involves with width and kernel size morphing. Then we shall describe how to deal with the non-linearities in the neural networks. Finally, we shall present the stand-alone versions for width morphing and kernel size morphing, followed by the subnet morphing."
    }, {
      "heading" : "3.1. Network Morphism: Linear Case",
      "text" : "Let us start from the simplest case of a classic neural network. We first drop all the non-linear activation functions and consider a neural network only connected with fully connected layers.\nAs shown in Fig. 2, in the parent network, two hidden layers Bl−1 and Bl+1 are connected via the weight matrix G:\nBl+1 = G ·Bl−1, (1)\nwhere Bl−1 ∈ RCl−1 , Bl+1 ∈ RCl+1 , G ∈ RCl+1×Cl−1 , Cl−1 and Cl+1 are the feature dimensions of Bl−1 and Bl+1. For network morphism, we shall insert a new hidden layer Bl, so that the child network satisfies:\nBl+1 = Fl+1 ·Bl = Fl+1 · (Fl ·Bl−1) = G ·Bl−1, (2)\nwhere Bl ∈ RCl , Fl ∈ RCl×Cl−1 , and Fl+1 ∈ RCl+1×Cl . It is obvious that network morphism for classic neural networks is equivalent to a matrix decomposition problem:\nG = Fl+1 · Fl. (3)\nNext, we consider the case of a deep convolutional neural network (DCNN). For a DCNN, the build-up blocks are convolutional layers rather than fully connected layers. Thus, we call the hidden layers as blobs, and weight matrices as filters. For a 2D DCNN, the blob B∗ is a 3D tensor of shape (C∗, H∗,W∗), where C∗, H∗, and W∗ represent the number of channels, height and width of B∗. The filters G, Fl, and Fl+1 are 4D tensors of shapes (Cl+1, Cl−1,K,K), (Cl, Cl−1,K1,K1), and (Cl+1, Cl,K2,K2), where K, K1, K2 are convolutional kernel sizes.\nThe convolutional operation in a DCNN can be defined in a multi-channel way:\nBl(cl) = ∑ cl−1 Bl−1(cl−1) ∗ Fl(cl, cl−1), (4)\nwhere ∗ is the convolution operation defined in a traditional way. It is easy to derive that the filters Fl, Fl+1 and G shall satisfy the following equation:\nG̃(cl+1, cl−1) = ∑ cl Fl(cl, cl−1) ∗ Fl+1(cl+1, cl), (5)\nwhere G̃ is a zero-padded version of G whose effective kernel size (receptive field) is K̃ = K1 + K2 − 1 ≥ K. If K̃ = K, we will have G̃ = G.\nMathematically, inner products are equivalent to multichannel convolutions with kernel sizes of 1 × 1. Thus, Equation (3) is equivalent to Equation (5) with K = K1 = K2 = 1. Hence, we are able to unify them into one equation:\nG̃ = Fl+1 ~ Fl, (6)\nwhere ~ is a non-communicative operator that can either be an inner product or a multi-channel convolution. We\ncall Equation (6) as the network morphism equation (for depth in the linear case).\nAlthough Equation (6) is primarily derived for depth morphing (G morphs into Fl and Fl+1), it also involves network width (the choice of Cl), and kernel sizes (the choice of K1 and K2). Thus, it will be called network morphism equation for short for the remaining of this paper.\nThe problem of network depth morphing is formally formulated as follows:\nInput: G of shape (Cl+1, Cl−1,K,K); Cl, K1, K2.\nOutput: Fl of shape (Cl, Cl−1,K1,K1), Fl+1 of shape (Cl+1, Cl,K2,K2) that satisfies Equation (6)."
    }, {
      "heading" : "3.2. Network Morphism Algorithms: Linear Case",
      "text" : "In this section, we introduce two algorithms to solve for the network morphism equation (6).\nSince the solutions to Equation (6) might not be unique, we shall make the morphism operation to follow the desired practices that: 1) the parameters will contain as many nonzero elements as possible, and 2) the parameters will need to be in a consistent scale. These two practices are widely adopted in existing work, since random initialization instead of zero filling for non-convex optimization problems is preferred (Bishop, 2006), and the scale of the initializations is critical for the convergence and good performance of deep neural networks (Glorot & Bengio, 2010; He et al., 2015b).\nNext, we introduce two algorithms based on deconvolution to solve the network morphism equation (6), i.e., 1) general network morphism, and 2) practical network morphism. The former one fills in all the parameters with nonzero elements under certain condition, while the latter one does not depend on such a condition but can only asymptotically fill in all parameters with non-zero elements."
    }, {
      "heading" : "3.2.1. GENERAL NETWORK MORPHISM",
      "text" : "This algorithm is proposed to solve Equation (6) under certain condition. As shown in Algorithm 1, it initializes convolution kernels Fl and Fl+1 of the child network with random noises. Then we iteratively solve Fl+1 and Fl by fixing the other. For each iteration, Fl or Fl+1 is solved by deconvolution. Hence the overall loss is always decreasing and is expected to converge. However, it is not guaranteed that the loss in Algorithm 1 will always converge to 0.\nWe claim that if the parameter number of either Fl or Fl+1 is no less than G̃, Algorithm 1 shall converge to 0. Claim 1. If the following condition is satisfied, the loss in\nAlgorithm 1 General Network Morphism Input: G of shape (Cl+1, Cl−1,K,K); Cl, K1, K2 Output: Fl of shape (Cl, Cl−1,K1,K1), Fl+1 of shape (Cl+1, Cl,K2,K2) Initialize Fl and Fl+1 with random noise. Expand G to G̃ with kernel size K̃ = K1 + K2 − 1 by padding zeros. repeat\nFix Fl, and calculate Fl+1 = deconv(G̃, Fl) Fix Fl+1, and calculate Fl = deconv(G̃, Fl+1) Calculate loss l = ‖G̃− conv(Fl, Fl+1)‖2\nuntil l = 0 or maxIter is reached Normalize Fl and Fl+1 with equal standard variance.\nAlgorithm 2 Practical Network Morphism Input: G of shape (Cl+1, Cl−1,K,K); Cl, K1, K2 Output: Fl of shape (Cl, Cl−1,K1,K1), Fl+1 of shape (Cl+1, Cl,K2,K2) /* For simplicity, we illustrate this algorithm for the case ‘Fl expands G’ */ Kr2 = K2 repeat\nRun Algorithm 1 with maxIter set to 1: l, Fl, F rl+1 = NETMORPHGENERAL(G;Cl,K1,Kr2) Kr2 = K r 2 − 1 until l = 0 Expand F rl+1 to Fl+1 with kernel size K2 by padding zeros. Normalize Fl and Fl+1 with equal standard variance.\nAlgorithm 1 shall converge to 0 (in one step):\nmin(ClCl−1K 2 1 , Cl+1ClK 2 2 ) ≥ Cl+1Cl−1(K1 + K2 − 1)2.\n(7) The three items in condition (7) are the parameter numbers of Fl ,Fl+1, and G̃, respectively.\nIt is easy to check the correctness of Condition (7), as a multi-channel convolution can be written as the multiplication of two matrices. Condition (7) claims that we have more unknowns than constraints, and hence it is an undetermined linear system. Since random matrices are rarely inconsistent (with probability 0), the solutions of the undetermined linear system always exist."
    }, {
      "heading" : "3.2.2. PRACTICAL NETWORK MORPHISM",
      "text" : "Next, we propose a variant of Algorithm 1 that can solve Equation (6) with a sacrifice in the non-sparse practice. This algorithm reduces the zero-converging condition to that the parameter number of either Fl or Fl+1 is no less than G, instead of G̃. Since we focus on network morphism in an expanding mode, we can assume that this condition\nis self-justified, namely, either Fl expands G, or Fl+1 expands G. Thus, we can claim that this algorithm solves the network morphism equation (6). As described in Algorithm 2, for the case that Fl expands G, starting from Kr2 = K2, we iteratively call Algorithm 1 and shrink the size of Kr2 until the loss converges to 0. This iteration shall terminate as we are able to guarantee that if Kr2 = 1, the loss is 0. For the other case that Fl+1 expands G, the algorithm is similar.\nThe sacrifice of the non-sparse practice in Algorithm 2 is illustrated in Fig. 3. In its worst case, it might not be able to fill in all parameters with non-zero elements, but still fill asymptotically. This figure compares the non-zero element occupations for IdMorph and NetMorph. We assume Cl+1 = O(Cl) , O(C). In the best case (c), NetMorph is able to occupy all the elements by non-zeros, with an order of O(C2K2). And in the worst case (b), it has an order of O(C2) non-zero elements. Generally, NetMorph lies in between the best case and worst case. IdMorph (a) only has an order of O(C) non-zeros elements. Thus the nonzero occupying rate of NetMorph is higher than IdMorph for at least one order of magnitude. In practice, we shall also have C K, and thus NetMorph can asymptotically fill in all parameters with non-zero elements."
    }, {
      "heading" : "3.3. Network Morphism: Non-linear Case",
      "text" : "In the proposed network morphism it is also required to deal with the non-linearities in a neural network. In general, it is not trivial to replace the layer Bl+1 = ϕ(G ~ Bl+1) with two layers Bl+1 = ϕ(Fl+1 ~ ϕ(Fl ~ Bl−1)), where ϕ represents the non-linear activation function.\nFor an idempotent activation function satisfying ϕ◦ϕ = ϕ, the IdMorph scheme in Net2Net (Chen et al., 2015) is to set Fl+1 = I , and Fl = G, where I represents the identity mapping. Then we have\nϕ(I ~ϕ(G~Bl−1) = ϕ ◦ϕ(G~Bl+1) = ϕ(G~Bl+1). (8)\nHowever, although IdMorph works for the ReLU activation function, it cannot be applied to other commonly used activation functions, such as Sigmoid and TanH, since the idempotent condition is not satisfied.\nTo handle arbitrary continuous non-linear activation functions, we propose to define the concept of P(arametric)activation function family. A family of P-activation functions for an activation function ϕ, can be defined to be any continuous function family that maps ϕ to the linear identity transform ϕid : x 7→ x. The P-activation function family for ϕ might not be uniquely defined. We define the canonical form for P-activation function family as follows:\nP -ϕ , {ϕa}|a∈[0,1] = {(1− a) ·ϕ+ a ·ϕid}|a∈[0,1], (9)\nwhere a is the parameter to control the shape morphing of the activation function. We have ϕ0 = ϕ, and ϕ1 =\nϕid. The concept of P-activation function family extends PReLU (He et al., 2015b), and the definition of PReLU coincides with the canonical form of P-activation function family for the ReLU non-linear activation unit.\nThe idea of leveraging P-activation function family for network morphism is shown in Fig. 4. As shown, it is safe to add the non-linear activations indicated by the green boxes, but we need to make sure that the yellow box is equivalent to a linear activation initially. This linear activation shall grow into a non-linear one once the value of a has been learned. Formally, we need to replace the layer Bl+1 = ϕ(G ~ Bl+1) with two layers Bl+1 = ϕ(Fl+1 ~ ϕa(Fl ~ Bl−1)). If we set a = 1, the morphing shall be successful as long as the network morphing equation (6) is satisfied:\nϕ(Fl+1 ~ ϕ a(Fl ~Bl−1)) = ϕ(Fl+1 ~ Fl ~Bl−1) (10)\n= ϕ(G~Bl−1). (11)\nThe value of a shall be learned when we continue to train the model."
    }, {
      "heading" : "3.4. Stand-alone Width and Kernel Size Morphing",
      "text" : "As mentioned, the network morphism equation (6) involves network depth, width, and kernel size morphing. Therefore, we can conduct width and kernel size morphing by introducing an extra depth morphing via Algorithm 2.\nSometimes, we need to pay attention to stand-alone network width and kernel size morphing operations. In this section, we introduce solutions for these situations."
    }, {
      "heading" : "3.4.1. WIDTH MORPHING",
      "text" : "For width morphing, we assume Bl−1, Bl, Bl+1 are all parent network layers, and the target is to expand the width (channel size) of Bl from Cl to C̃l, C̃l ≥ Cl. For the parent network, we have\nBl(cl) = ∑ cl−1 Bl−1(cl−1) ∗ Fl(cl, cl−1), (12)\nBl+1(cl+1) = ∑ cl Bl(cl) ∗ Fl+1(cl+1, cl). (13)\nFor the child network, Bl+1 should be kept unchanged:\nBl+1(cl+1) = ∑ c̃l Bl(c̃l) ∗ F̃l+1(cl+1, c̃l) (14)\n= ∑ cl Bl(cl) ∗ Fl+1(cl+1, cl) + ∑ c̄l Bl(c̄l) ∗ F̃l+1(cl+1, c̄l),\n(15)\nwhere c̃l and cl are the indices of the channels of the child network blob B̃l and parent network blob Bl. c̄l is the index of the complement c̃l\\cl. Thus, we only need to satisfy:\n0 = ∑ c̄l Bl(c̄l) ∗ F̃l+1(cl+1, c̄l) (16)\n= ∑ c̄l Bl−1(cl−1) ∗ F̃l(c̄l, cl−1) ∗ F̃l+1(cl+1, c̄l), (17)\nor simply,\nF̃l(c̄l, cl−1) ∗ F̃l+1(cl+1, c̄l) = 0. (18)\nIt is obvious that we can either set F̃l(c̄l, cl−1) or F̃l+1(cl+1, c̄l) to 0, and the other can be set arbitrarily. Following the non-sparse practice, we set the one with less parameters to 0, and the other one to random noises. The zeros and random noises in F̃l and F̃l+1 may be clustered together. To break this unwanted behavior, we perform a random permutation on c̃l, which will not change Bl+1."
    }, {
      "heading" : "3.4.2. KERNEL SIZE MORPHING",
      "text" : "For kernel size morphing, we propose a heuristic yet effective solution. Suppose that a convolutional layer l has kernel size of Kl, and we want to expand it to K̃l. When the filters of layer l are padded with (K̃l −Kl)/2 zeros on each side, the same operation shall also apply for the blobs. As shown in Fig. 5, the resulting blobs are of the same shape and also with the same values."
    }, {
      "heading" : "3.5. Subnet Morphing",
      "text" : "Modern networks are going deeper and deeper. It is challenging to manually design tens of or even hundreds of layers. One elegant strategy is to first design a subnet template, and then construct the network by these subnets. Two typical examples are the mlpconv layer for Network in Network (NiN) (Lin et al., 2013) and the inception layer for GoogLeNet (Szegedy et al., 2014), as shown in Fig. 6(a).\nWe study the problem of subnet morphing in this section, that is, network morphism from a minimal number (typically one) of layers in the parent network to a subnet in the child network. One commonly used subnet is the stacked sequential subnet as shown in Fig. 6(c). An exmaple is the inception layer for GoogLeNet with a four-way stacking of sequential subnets.\nWe first describe the morphing operation for the sequential subnet, based on which its stacked version is then obtained.\nSequential subnet morphing is to morph from a single layer to multiple sequential layers, as illustrated in Fig. 6(b). Similar to Equation (6), one can derive the network morphism equation for sequential subnets from a single layer to P + 1 layers:\nG̃(cl+P , cl−1) = ∑\ncl,··· ,cl+P−1\nFl(cl, cl−1) ∗ · · · ∗ Fl+P (cl+P , cl+P−1),\n(19) where G̃ is a zero-padded version of G. Its effective kernel size is K̃ = ∑ p=0,··· ,P Kl+p − P , and Kl is the kernel size of layer l. Similar to Algorithm 1, subnet morphing equation (19) can be solved by iteratively optimizing the parameters for one layer with the parameters for the other layers fixed. We can also develop a practical version of the algorithm that can solve for Equation (19), which is similar to Algorithm 2. The algorithm details are omitted here.\nFor stacked sequential subnet morphing, we can follow the work flow illustrated as Fig. 6(c). First, a single layer in the parent network is split into multiple paths. The split {Gi}\nis set to satisfy n∑\ni=1\nGi = G, (20)\nin which the simplest case is Gi = 1nG. Then, for each path, a sequential subnet morphing can be conducted. In Fig. 6(c), we illustrate an n-way stacked sequential subnet morphing, with the second path morphed into two layers."
    }, {
      "heading" : "4. Experimental Results",
      "text" : "In this section, we conduct experiments on three datasets (MNIST, CIFAR10, and ImageNet) to show the effectiveness of the proposed network morphism scheme, on 1) different morphing operations, 2) both the classic and convolutional neural networks, and 3) both the idempotent activations (ReLU) and non-idempotent activations (TanH)."
    }, {
      "heading" : "4.1. Network Morphism for Classic Neural Networks",
      "text" : "The first experiment is conducted on the MNIST dataset (LeCun et al., 1998). MNIST is a standard dataset for handwritten digit recognition, with 50,000 training images and 10,000 testing images. In this section, instead of using state-of-the-art DCNN solutions (LeCun et al., 1998; Chang & Chen, 2015), we adopt the simple softmax regression model as the parent network to evaluate the effectiveness of network morphism on classic networks. The grayscale 28×28 digit images were flattened as a 784 dimension feature vector as input. The parent model achieved 92.29% accuracy, which is considered as the baseline. Then, we morphed this model into a multiple layer perception (MLP) model by adding a PReLU or PTanH hidden layer with the number of hidden neurons h = 50. Fig. 7(a) shows the performance curves of NetMorph3 and Net2Net after morphing. We can see that, for the PReLU activation, NetMorph works much better than Net2Net. NetMorph continues to\n3In the experiments, we use NetMorph to represent the proposed network morphism algorithm.\nimprove the performance from 92% to 97%, while Net2Net improves only to 94%. We also show the curve of NetMorph with the non-idempotent activation PTanH in Fig. 7(b). The curve for Net2Net is unavailable since it cannot handle non-idempotent activations."
    }, {
      "heading" : "4.2. Depth Morphing, Subnet Morphing, and Internal Regularization for DCNN",
      "text" : "Extensive experiments were conducted on the CIFAR10 dataset (Krizhevsky & Hinton, 2009) to verify the network morphism scheme for convolutional neural networks. CIFAR10 is an image recognition database composed of 32 × 32 color images. It contains 50,000 training images and 10,000 testing images for ten object categories. The baseline network we adopted is the Caffe (Jia et al., 2014) cifar10_quick model with an accuracy of 78.15%.\nIn the following, we use the unified notation cifar_ddd to represent a network architecture of three subnets, in which each digit d is the number of convolutional layers in the corresponding subnet. The detailed architecture of each subnet is described by (<kernel_size>:<num_output>,...). Following this notation, the cifar10_quick model, which has three convolutional layers and two fully con-\nnected layers, can be denoted as cifar_111, with its architecture described with (5:32)(5:32)(5:64). Additionally, we also use [<subnet>] to indicate the grouping of layers in a subnet, and x<times> to represent for the repetition of layers or subnets. Hence, cifar10_quick can also be denoted as (5:32)x2(5:64) or [(5:32)]x2[(5:64)]. Note that in this notation, the fully connected layers are ignored.\nFig. 8 shows the comparison results between NetMorph and Net2Net, in the morphing sequence of cifar_111→211→222→2222→3333. The detailed network architectures of these networks are shown in Table 1. In this table, some layers are morphed by adding a 1x1 convolutional layer with channel size four times larger. This is a good practice adopted in the design of current state-of-the-art network (He et al., 2015a). Algorithm 2 is leveraged for the morphing. From Fig. 8(a) and (b), we can see the superiority of NetMorph over Net2Net. NetMorph improves the performance from 78.15% to 82.06%, then to 82.43%, while Net2Net from 78.15% to 81.21%, then to 81.99%. The relatively inferior performance of Net2Net may be caused by the IdMorph in Net2Net involving too many zero elements on the embedded layer, while non-zero elements are also not in a consistent scale with existing parameters. We also verified this by comparing the histograms of the embedded filters (after morphing) for both methods. The parameter scale for NetMorph fits a normal distribution with a relatively large standard derivation, while that of Net2Net shows two peaks around 0 and 0.5.\nFig. 8(c) illustrates the performance of NetMorph for subnet morphing. The architecture is morphed from cifar_222 to cifar_2222. As can be seen, NetMorph achieves additional performance improvement from 82.43% to 83.14%. Fig. 8(d) illustrates for the morphing from cifar_2222 to cifar_3333, and the performance is further improved to around 84%.\nFinally, we compare NetMorph with the model directly trained from scratch (denoted as Raw) in Fig. 8. It can be seen that NetMorph consistently achieves a better accuracy\nthan Raw. As the network goes deeper, the gap becomes larger. We interpret this phenomena as the internal regularization ability of NetMorph. In NetMorph, the parameters are learned in multiple phases rather than all at once. Deep neural networks usually involve a large amount of parameters, and overfit to the training data can occur easily. For NetMorph, the parameters learned have been placed in a good position in the parameter space. We only need to explore for a relatively small region rather than the whole parameter space. Thus, the NetMorph learning process shall result in a more regularized network to achieve better performance."
    }, {
      "heading" : "4.3. Kernel Size Morphing and Width Morphing",
      "text" : "In this section we evaluate kernel size morphing and width morphing in the sequence of cifar_base→ksize→width. The detailed network architectures are shown in Table 1. The baseline network (cifar_base) is a narrower version of cifar_222 with an accuracy of 81.48%.\nFig. 9(a) shows the curve of kernel size morphing, which expands the kernel size of the second layer in each subnet from 1 to 3 (cifar_ksize). This results in a performance of 82.81%, which is 1.33% higher than the parent network. We further double the number of channels (width) for the first layer in each subnet (cifar_width).\nFig. 9(b) shows the results of NetMorph and Net2Net. As can be seen, NetMorph is slightly better. It improves the performance to 83.09% while Net2Net dropped a little to 82.70%.\nFor width morphing, NetMorph works for arbitrary continuous non-linear activation functions, while Net2Net only for piece-wise linear ones. We also conducted width morphing directly from the parent network for TanH neurons, which results in about 4% accuracy improvement."
    }, {
      "heading" : "4.4. Experiment on ImageNet",
      "text" : "We also conduct experiments on the ImageNet dataset (Russakovsky et al., 2014) with 1,000 object categories. The models were trained on 1.28 million training images and tested on 50,000 validation images. The top-1 and top5 accuracies for both 1-view and 10-view are reported.\nThe proposed experiments is based on the VGG16 net, which was actually trained with multi-scales (Simonyan & Zisserman, 2014). Because the Caffe (Jia et al., 2014) implementation favors single-scale, for a fair comparison, we first de-multiscale this model by continuing to train it on the ImageNet dataset with the images resized to 256 × 256. This process caused about 1% performance drop. This coincides with Table 3 in (Simonyan & Zisserman, 2014) for model D. In this paper, we adopt the de-multiscaled version of the VGG16 net as the parent network to morph. The morphing operation we adopt is to add a convolutional layer at the end of the first three subsets for each. The detailed network architecture is shown in Table 2. We continue to train the child network after morphing, and the final model is denoted as\nVGG16(NetMorph). The results are shown in Table 3. We can see that, VGG16(NetMorph) not only outperforms its parent network, i.e, VGG16(baseline), but also outperforms the multi-scale version, i.e, VGG16(multi-scale). Since VGG16(NetMorph) is a 19-layer network, we also list the VGG19 net in Table 3 for comparison. As can be seen, VGG16(NetMorph) also outperforms VGG19 in a large margin. Note that VGG16(NetMorph) and VGG19 have different architectures, as shown in Table 2. Therefore, the proposed NetMorph scheme not only can help improve the performance, but also is an effective network architecture explorer. Further, we are able to add more layers into VGG16(NetMorph), and a better performing model shall be expected.\nWe compare the training time cost for the NetMorph learning scheme and the training from scratch. VGG16 was trained for around 2~3 months for a single GPU time (Simonyan & Zisserman, 2014), which does not include the pre-training time on a 11-layered network. For a deeper network, the training time shall increase. While for the 19- layered network VGG16(NetMorph), the whole morphing and training process was finished within 5 days, resulting in around 15x speedup."
    }, {
      "heading" : "5. Conclusions",
      "text" : "In this paper, we have introduced the systematic study about network morphism. The proposed scheme is able to morph a well-trained parent network to a new child network, with the network function completely preserved. The child network has the potential to grow into a more powerful one in a short time. We introduced diverse morphing operations, and developed novel morphing algorithms based on the morphism equations we have derived. The non-linearity of a neural network has been carefully addressed, and the proposed algorithms enable the morphing of any continuous non-linear activation neurons. Extensive experiments have been carried out to demonstrate the effectiveness of the proposed network morphism scheme."
    } ],
    "references" : [ {
      "title" : "Do deep nets really need to be deep",
      "author" : [ "Ba", "Jimmy", "Caruana", "Rich" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Ba et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ba et al\\.",
      "year" : 2014
    }, {
      "title" : "Batchnormalized maxout network in network",
      "author" : [ "Chang", "Jia-Ren", "Chen", "Yong-Sheng" ],
      "venue" : "arXiv preprint arXiv:1511.02583,",
      "citeRegEx" : "Chang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chang et al\\.",
      "year" : 2015
    }, {
      "title" : "Net2net: Accelerating learning via knowledge transfer",
      "author" : [ "Chen", "Tianqi", "Goodfellow", "Ian", "Shlens", "Jonathon" ],
      "venue" : "arXiv preprint arXiv:1511.05641,",
      "citeRegEx" : "Chen et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "Rich feature hierarchies for accurate object detection and semantic segmentation",
      "author" : [ "Girshick", "Ross", "Donahue", "Jeff", "Darrell", "Trevor", "Malik", "Jagannath" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Girshick et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Girshick et al\\.",
      "year" : 2014
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "Glorot", "Xavier", "Bengio", "Yoshua" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Glorot et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Glorot et al\\.",
      "year" : 2010
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian" ],
      "venue" : "arXiv preprint arXiv:1512.03385,",
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification",
      "author" : [ "He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian" ],
      "venue" : "arXiv preprint arXiv:1502.01852,",
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning multiple layers of features from tiny",
      "author" : [ "Krizhevsky", "Alex", "Hinton", "Geoffrey" ],
      "venue" : null,
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2009
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "LeCun", "Yann", "Bottou", "Léon", "Bengio", "Yoshua", "Haffner", "Patrick" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "Fully convolutional networks for semantic segmentation",
      "author" : [ "Long", "Jonathan", "Shelhamer", "Evan", "Darrell", "Trevor" ],
      "venue" : "arXiv preprint arXiv:1411.4038,",
      "citeRegEx" : "Long et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Long et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning and transferring mid-level image representations using convolutional neural networks",
      "author" : [ "Oquab", "Maxime", "Bottou", "Leon", "Laptev", "Ivan", "Sivic", "Josef" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Oquab et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Oquab et al\\.",
      "year" : 2014
    }, {
      "title" : "Faster r-cnn: Towards real-time object detection with region proposal networks",
      "author" : [ "Ren", "Shaoqing", "He", "Kaiming", "Girshick", "Ross", "Sun", "Jian" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Ren et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2015
    }, {
      "title" : "Fitnets: Hints for thin deep nets",
      "author" : [ "Romero", "Adriana", "Ballas", "Nicolas", "Kahou", "Samira Ebrahimi", "Chassang", "Antoine", "Gatta", "Carlo", "Bengio", "Yoshua" ],
      "venue" : "arXiv preprint arXiv:1412.6550,",
      "citeRegEx" : "Romero et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Romero et al\\.",
      "year" : 2014
    }, {
      "title" : "Imagenet large scale visual recognition challenge",
      "author" : [ "Russakovsky", "Olga", "Deng", "Jia", "Su", "Hao", "Krause", "Jonathan", "Satheesh", "Sanjeev", "Ma", "Sean", "Huang", "Zhiheng", "Karpathy", "Andrej", "Khosla", "Aditya", "Bernstein", "Michael" ],
      "venue" : "International Journal of Computer Vision,",
      "citeRegEx" : "Russakovsky et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Russakovsky et al\\.",
      "year" : 2014
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "Simonyan", "Karen", "Zisserman", "Andrew" ],
      "venue" : "arXiv preprint arXiv:1409.1556,",
      "citeRegEx" : "Simonyan et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Simonyan et al\\.",
      "year" : 2014
    }, {
      "title" : "Going deeper with convolutions",
      "author" : [ "Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew" ],
      "venue" : "arXiv preprint arXiv:1409.4842,",
      "citeRegEx" : "Szegedy et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "Deep convolutional neural networks (DCNNs) have achieved state-of-the-art results on diverse computer vision tasks such as image classification (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; Szegedy et al., 2014), object detection (Girshick et al.",
      "startOffset" : 144,
      "endOffset" : 219
    }, {
      "referenceID" : 16,
      "context" : "Deep convolutional neural networks (DCNNs) have achieved state-of-the-art results on diverse computer vision tasks such as image classification (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; Szegedy et al., 2014), object detection (Girshick et al.",
      "startOffset" : 144,
      "endOffset" : 219
    }, {
      "referenceID" : 10,
      "context" : "2015), and semantic segmentation (Long et al., 2014).",
      "startOffset" : 33,
      "endOffset" : 52
    }, {
      "referenceID" : 13,
      "context" : "fundamentally different from existing work related to network knowledge transferring, which either tries to mimic a parent network’s outputs (Bucilu et al., 2006; Ba & Caruana, 2014; Romero et al., 2014), or pre-trains to facilitate the convergence and/or adapt to new datasets with possible total change in network function (Simonyan & Zisserman, 2014; Oquab et al.",
      "startOffset" : 141,
      "endOffset" : 203
    }, {
      "referenceID" : 11,
      "context" : ", 2014), or pre-trains to facilitate the convergence and/or adapt to new datasets with possible total change in network function (Simonyan & Zisserman, 2014; Oquab et al., 2014).",
      "startOffset" : 129,
      "endOffset" : 177
    }, {
      "referenceID" : 8,
      "context" : "Depth morphing is an important morphing type, since current top-notch neural networks are going deeper and deeper (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; Szegedy et al., 2014; He et al., 2015a).",
      "startOffset" : 114,
      "endOffset" : 207
    }, {
      "referenceID" : 16,
      "context" : "Depth morphing is an important morphing type, since current top-notch neural networks are going deeper and deeper (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; Szegedy et al., 2014; He et al., 2015a).",
      "startOffset" : 114,
      "endOffset" : 207
    }, {
      "referenceID" : 2,
      "context" : "IdMorph is explored by a recent work (Chen et al., 2015), but is potentially problematic due to the sparsity of the identity layer, and might fail sometimes (He et al.",
      "startOffset" : 37,
      "endOffset" : 56
    }, {
      "referenceID" : 2,
      "context" : "To the best of our knowledge, this is the first work about network morphism, except the recent work (Chen et al., 2015) that introduces the IdMorph.",
      "startOffset" : 100,
      "endOffset" : 119
    }, {
      "referenceID" : 13,
      "context" : "In (Romero et al., 2014), the authors adopted a deeper but narrower network to mimic a deep and wide network.",
      "startOffset" : 3,
      "endOffset" : 24
    }, {
      "referenceID" : 11,
      "context" : "Pre-training (Simonyan & Zisserman, 2014) is a strategy proposed to facilitate the convergence of very deep neural networks, and transfer learning2 (Simonyan & Zisserman, 2014; Oquab et al., 2014) is introduced to overcome the overfitting problem when training large neural networks on relatively small Although transfer learning in its own concept is very general, here it is referred as a technique used for DCNNs to pre-train the model on one dataset and then adapt to another.",
      "startOffset" : 148,
      "endOffset" : 196
    }, {
      "referenceID" : 2,
      "context" : "Net2Net is a recent work proposed in (Chen et al., 2015).",
      "startOffset" : 37,
      "endOffset" : 56
    }, {
      "referenceID" : 2,
      "context" : "For an idempotent activation function satisfying φ◦φ = φ, the IdMorph scheme in Net2Net (Chen et al., 2015) is to set Fl+1 = I , and Fl = G, where I represents the identity mapping.",
      "startOffset" : 88,
      "endOffset" : 107
    }, {
      "referenceID" : 16,
      "context" : ", 2013) and the inception layer for GoogLeNet (Szegedy et al., 2014), as shown in Fig.",
      "startOffset" : 46,
      "endOffset" : 68
    }, {
      "referenceID" : 9,
      "context" : "The first experiment is conducted on the MNIST dataset (LeCun et al., 1998).",
      "startOffset" : 55,
      "endOffset" : 75
    }, {
      "referenceID" : 9,
      "context" : "In this section, instead of using state-of-the-art DCNN solutions (LeCun et al., 1998; Chang & Chen, 2015), we adopt the simple softmax regression model as the parent network to evaluate the effectiveness of network morphism on classic networks.",
      "startOffset" : 66,
      "endOffset" : 106
    }, {
      "referenceID" : 14,
      "context" : "We also conduct experiments on the ImageNet dataset (Russakovsky et al., 2014) with 1,000 object categories.",
      "startOffset" : 52,
      "endOffset" : 78
    } ],
    "year" : 2017,
    "abstractText" : "We present in this paper a systematic study on how to morph a well-trained neural network to a new one so that its network function can be completely preserved. We define this as network morphism in this research. After morphing a parent network, the child network is expected to inherit the knowledge from its parent network and also has the potential to continue growing into a more powerful one with much shortened training time. The first requirement for this network morphism is its ability to handle diverse morphing types of networks, including changes of depth, width, kernel size, and even subnet. To meet this requirement, we first introduce the network morphism equations, and then develop novel morphing algorithms for all these morphing types for both classic and convolutional neural networks. The second requirement for this network morphism is its ability to deal with nonlinearity in a network. We propose a family of parametric-activation functions to facilitate the morphing of any continuous non-linear activation neurons. Experimental results on benchmark datasets and typical neural networks demonstrate the effectiveness of the proposed network morphism scheme.",
    "creator" : "LaTeX with hyperref package"
  }
}