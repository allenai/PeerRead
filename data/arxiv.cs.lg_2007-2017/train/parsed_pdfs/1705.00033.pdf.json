{
  "name" : "1705.00033.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Random Forest Ensemble of Support Vector Regression Models for Solar Power Forecasting",
    "authors" : [ "Mohamed Abuella" ],
    "emails" : [ "mabuella@uncc.edu", "b.chowdhury@uncc.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Note: This is a pre-print of the full paper that published in Innovative Smart Grid Technologies, North America Conference, 2017, which can be referenced as below:\nM. Abuella and B. Chowdhury, “Random Forest Ensemble of Support Vector Regression for Solar Power Forecasting,” in Proceedings of Innovative Smart Grid\nTechnologies, North America Conference, 2017.\nresources, two off-the-shelf machine learning tools are deployed to forecast the solar power output of a solar photovoltaic system. The support vector machines generate the forecasts and the random forest acts as an ensemble learning method to combine the forecasts. The common ensemble technique in wind and solar power forecasting is the blending of meteorological data from several sources. In this study though, the present and the past solar power forecasts from several models, as well as the associated meteorological data, are incorporated into the random forest to combine and improve the accuracy of the day-ahead solar power forecasts. The performance of the combined model is evaluated over the entire year and compared with other combining techniques.\nKeywords—Ensemble learning, post-processing, random forest,\nsolar power, support vector regression.\nI. INTRODUCTION\nThe wind and solar energy resources have created operational challenges for the electric power grid due to the uncertainty involved in their output in the short term. The intermittency of these resources may adversely affect the operation of the electric grid when the penetration levels of these variable generations are high. Thus, wherever the variable generation resources are used, it becomes highly desirable to maintain higher than normal operating reserves and efficient energy storage systems to manage the power balance in the system. The operating reserves that use fossil fuel generating units should be kept to a minimum in order to get the maximum benefit from the deployment of the renewable resources. Therefore, the forecast of these variable generations becomes a vital tool in the operation of the power systems and electricity markets [1].\nAs in wind power forecasting, the solar power also consists of a variety of methods based on the time horizon being forecasted, the data available to the forecaster and the particular application of the forecast. The methods are broadly categorized according to the time horizon in which they generally show value. Methods that are common in solar power forecasting include Numerical Weather Prediction (NWP) and Model Output Statistics (MOS) to produce forecasts, as well as hybrid techniques that combine ensemble forecasts and Statistical Learning Methods [2]. Applying machine learning\ntechniques directly to historical time series of solar photovoltaic (PV) production associated with NWP outcomes have placed among the top models in the recent global competition of energy forecasting, GEFCom2014 [3]. Just to name a few of the machine learning tools, the artificial neural networks (ANN) and support vector regression (SVR), gradient boosting (GB), random forest (RF), etc. are believed to be the most common.\nHybrid models of two or more statistical and physical techniques are often combined to capture complex interactions and provide useful insights and better forecasts. In ref. [4], the authors implement a hybrid model that consists of ARMA and ANN to forecast the solar irradiance by NWP data for 5 locations of a Mediterranean climate. They found the proposed model outperforms the naïve persistence model and improvement with respect to its core techniques as well. The study reported in ref. [5] presents the benefits of combining the data of solar irradiance that is derived from a satellite with ground-measured data to improve the intraday forecasts in the range up to six hours ahead. In ref [6], the authors combine satellite images with ANN outcomes to forecast the solar irradiance of leading time up to two hours for two sites in California.\nIn ref. [7], several statistical combining methods are used to combine multiple linear regression models for load forecasting, and the authors conclude that the regression combining technique is the best. While ref. [8] uses several statistical models to forecast the hourly PV electricity production for the next day at some power plants in France, the random forest (RF) has shown a superior performance. Ref. [9] also found the random forest has the best performance among others to predict the daily solar irradiance variability of four sites with different climatic conditions in Australia. The authors of [10] used random forest with other models as well to forecast the solar power in GEF2014 competition. They indicate that the random forest and the gradient boosting technique are the most accurate models. In the aforementioned studies, RF is not implemented there as a combining method; it is a standalone forecasting model that depends on its own trees.\nThe commonly used ensemble technique in wind and solar power forecasting is to blend the weather data from several sources. As in ref. [11], the authors compare several data-\nNote: This is a pre-print of the full paper that published in Innovative Smart Grid Technologies, North America Conference, 2017, which can be referenced as below:\nM. Abuella and B. Chowdhury, “Random Forest Ensemble of Support Vector Regression for Solar Power Forecasting,” in Proceedings of Innovative Smart Grid\nTechnologies, North America Conference, 2017.\ndriven models using input data from two NWPs and building two artificial hybrid and stochastic ensemble models based on ANN, the model that combines multiple models outperforms the rest of the models. It points out that the ensemble is enhanced by including forecasts with similar accuracy, but generated from NWP data of higher variance and different data-driven techniques. Ref. [12] uses Ensemble Prediction System (EPS) to produce weather scenarios by running multiple initials to quantify the uncertainty and then produce the probabilistic solar power forecasts of sites in Italy. Ref. [13] applies a physical post processing and ANN to improve the solar irradiance forecasts of one and two days ahead.\nThe majority of the existing research literature on combining forecasting methods of solar forecasts do not include the previously generated forecasts to boost the model performance. It can be useful to add these past models’ outcomes as well into the ensemble learning methods. The research team from the National Renewable Energy Laboratory (NREL) and IBM Thomas Watson Research Center [17] deployed and tested several machine-learning techniques to blend three NWPs outcomes. They conclude that the ensemble approaches that take into account the diversity and the state parameters of the models provide lower errors in the solar irradiance forecasting. Although these studies forecast the solar irradiance at different sites in the U.S., the time period is limited since they do not investigate the performance of the different seasons over the entire year.\nUsing RF as a combining method of other models in solar power is scarce. This paper adopts RF to combine the forecasts for a PV system and investigates the performance throughout a complete year. The rest of the paper is organized as follows: Section II describes the ensemble method that is used to build and combine the SVR models. Section III discusses the methodology of combining the solar power forecasts. Section IV presents the results and the evaluation of the model. Section V provides the conclusions.\nII. ENSEMBLE LEARNING\nThe algorithms that use decision trees can be useful to combine the different models’ outcomes efficiently. This ensemble approach combines all the outputs from variant models besides the features, such as the weather data that allow the ensemble method to find the associative rules to determine the best output. For instance, if the weather is sunny, then model A performs best, and its output should be selected; otherwise model B should be selected, and so on. This ensemble learning approach has shown very promising results in numerous machine learning benchmarks. For more details on this topic, i.e., ensemble learning and its techniques as bagging, boosting, stacking, and Bayesian averaging, the interested reader may refer to [16], The general diagram of combining the models is shown in Fig.1."
    }, {
      "heading" : "A. Random Forest",
      "text" : "Since the classification and regression trees (CART) use the bagging principle of the ensemble learning, and they are built by the same data, these trees are sometimes correlated and statistically dependent. Consequently, to make the trees more variant and uncorrelated, Breiman [17] proposed that each split\nof the bagged tree should be grown by a random number of features and observation samples. Hence, this method is called the random forest (RF).\nFig.1. General diagram of combining different models\nThree parameters are required to be set in RF, the number of trees B (forest size), m the number of predictors out of p available variables (features) that are randomly chosen to be used for each split, and the minimum number nmin of observations per node (leaf size).\nThe random forest building algorithm [16] has three major steps.\n• Create B sample datasets of size N from the training data; these sample datasets can be replaced and overlapped.\n• For each sample dataset, grow a random forest tree Tb, by repeating the following steps for each terminal node, until the minimum node size nmin is reached:\nI. Select m predictors at random from the p variables.\nII. Pick the best predictor among the m selected predictors for the split-point.\nIII. Split this point (node) into two daughter nodes by setting certain decision rules.\n• Finally, find the ensemble of the trees {Tb}1 B, where B\nis the number of trees in the random forest.\nThe prediction of a given point x of the response variable can be obtained by averaging the individual tree’s outputs:\n\uD835\uDC53\uD835\uDC45\uD835\uDC39 = 1\n\uD835\uDC35 ∑ \uD835\uDC47\uD835\uDC4F(\uD835\uDC65)\n\uD835\uDC35\n\uD835\uDC4F=1\n(1)\nThe ensemble learning algorithm repeatedly assembles the input data to create regression trees that best fit the relationship between the features and the output. This process of decorrelation of the trees makes the random forest outcomes less variable, and hence more reliable.\nIII. MODELING"
    }, {
      "heading" : "A. Data Description",
      "text" : "The solar power system is in Australia and has a latitude 35°16'30\"S, longitude 149°06'49\"E, altitude 595m. The panel type is Solarfun SF160-24-1M195, consisting of 8 panels, its nominal power of 1560W, and panel orientation 38° clockwise from the north, with panel tilt of 36°. The weather forecasts data and the available measured solar power data starts from April 2012 to May 2014, as shown in Fig.2.\nModel B\nModel A\nModel C\nModel N\nMethod of\nCombining The\nModels\nCombined Forecasts Individual forecasting\nmodels\nNote: This is a pre-print of the full paper that published in Innovative Smart Grid Technologies, North America Conference, 2017, which can be referenced as below:\nM. Abuella and B. Chowdhury, “Random Forest Ensemble of Support Vector Regression for Solar Power Forecasting,” in Proceedings of Innovative Smart Grid\nTechnologies, North America Conference, 2017.\n(a) (b)\nFig.2. (a) The weather data and their numeral codes, (b) available data size\nNormalizing the data is of paramount importance since the scale used for the values for each variable might be different. The best practice is to normalize the data and transform all the values to a common scale, as shown in equation (2).\nXScaled = a + [x − min(X)]\n[max(X) − min(X)] ∗ {b − a} (2)\nWhere x is a sample from data variable X, {a, b} is the desired range of the normalized data, such as {0, 1}, and X (min, max)=the minimum and maximum values of the observed data.\nThere is also a standardization technique, especially when the variance of the data is high, which is making the data to have a zero mean and a unit standard deviation, as follows:\nXstandardized = [x − mean(X)]\nstd(\uD835\uDC4B) (3)"
    }, {
      "heading" : "B. Model Building",
      "text" : "The different models are achieved by using different parameterization within the same model, i.e., support vector regression models (SVRs). Refer to [18] for more details about this model. The models are built as in Fig.3, where the available data is divided into two sets, one dataset consisting of all 26 months and another dataset consisting of the most recent 12 months only, as shown in Fig.2.a. Then, each of these datasets is used to build 12 SVR models based on two normalization techniques as in Equations (2) and (3), after that two different SVR’s hyper-parameters of C and Gamma are also chosen, and different combination of weather variables are used to produce 12 models from each two datasets to get the total of 24 SVR models.\nFig.3. Construction of 12 SVR models from a dataset. Another 12 SVRs from the other dataset. The total number of SVR models is 24\nThe forecast methodology is shown in Fig.4.a. The forecasting day should be excluded from the data, while the rest of the available data is used to train the SVR models. The super learner is the random forest where the available weather and the previous forecasts are blended together to find the associative rules to achieve better solar power forecasts for the next day, as shown in Fig.4.b, and they should be as close as possible to the observed solar power that minimize the forecast errors of that day.\n(a) (b)\nFig.4. Demonstrates (a) the 24 hours ahead forecasting and (b) the combining methodology scheme for May 31st.\nRF does not need cross-validation to estimate the parameters because it has a built-in estimation of accuracy. The parameter selection is carried out by the wrapping strategy or a greedy search for the best evaluation results among the available training data, the parameters search of RF are shown in Fig.5, [number of trees B=100, samples m=6 (i.e.,18/3), leaf size nmin =5]. It is worth mentioning that a change with a reasonable range of these parameters does not affect the RF performance significantly. Thus, the robustness and flexibility of RF are the main advantages of this ensemble method.\n(a) (b) (c)\nFig.5. The search for random forest parameters. (a) The forest size, (b) the features number at each node, (c) the leaf size (minimum number of samples at each node).\nIV. RESULTS AND EVALUATION\nThe following metrics are used to evaluate the accuracy of the forecasts and the model performance: graphs, Root Mean Square Error (RMSE) as calculated by (4), and a comparison with other combining methods. For comparison purposes, the simple average combining method (Avg.), and the best model (Model 4) out of the 24 SVRs are adopted to evaluate the performance of the combined forecasts. Also, the improvement or the skill factor is used to compare the performance of the combined forecasts with respect to the other methods as in (5).\n\uD835\uDC45\uD835\uDC40\uD835\uDC46\uD835\uDC38 = √ 1\n\uD835\uDC5B ∑(\uD835\uDC4C\uD835\uDC56 − \uD835̂\uDC4C\uD835\uDC56)\n2\n\uD835\uDC5B\n\uD835\uDC56=1\n(4)\nwhere \uD835̂\uDC4C is the forecast of the solar power and Y is the observed value of the solar power. \uD835̂\uDC4C and Y are normalized to the nominal installed capacity of the solar power system, \uD835\uDC5B is the number of hours - it can be day hours or month hours. The objective is to minimize the RMSE for all forecasting hours to\nNo. Month Year\n1 April 2012 2 May 2012 3 June 2012 4 July 2012 5 August 2012 6 September 2012 7 October 2012 8 November 2012 9 December 2012 10 January 2013 11 February 2013 12 March 2013 13 April 2013 14 May 2013 15 June 2013 16 July 2013 17 August 2013 18 September 2013 19 October 2013 20 November 2013 21 December 2013 22 January 2014 23 February 2014 24 March 2014 25 April 2014 26 May 2014\nNormalized (A)\nC=16\nGamma=1\nOriginal 14 Inputs\nNew Inputs\nHeat index & wind\nWithout Cloud Cover Inputs\nC=10\nGamma=0.8\nOriginal 14 Inputs\nNew Inputs\nHeat index & wind\nWithout Cloud Cover Inputs\nNormalized (B)\nC=16\nGamma=1\nOriginal 14 Inputs\nNew Inputs\nHeat index & wind\nWithout Cloud Cover Inputs\nC=10\nGamma=0.8\nOriginal 14 Inputs\nNew Inputs\nHeat index & wind\nWithout Cloud Cover Inputs\nDay Month\n1 : June : July : : : : : : : April 30 May\n31\n00:00\n:\n23:00\nWeather Data\n: : : : : : : : : : : : : : : : : : : : : :\nPV Power\n: :\n(P as\nt O\nb se\nrv at\nio n s)\n: : : : : :\nForecasts (Model’s Outcomes)\nàat 00:00\nDay Month\n1 : June : July : : : : : : : April 30 May\n31\n00:00\n:\n23:00\nWeather Data\n: : : : : : : : : : : : : : : : : : : : : :\nModels’ Outcomes\n: : : : : : : : : : : : : : : : : : : : : : : :\nForecasts\nPV Power\n: :\n(P as\nt O\nb se\nrv at\nio n\ns)\n: : : : : :\nCombined Forecasts\n0 200 400 600 800 1000 0.062\n0.063\n0.064\n0.065\n0.066\n0.067\n0.068\nNumber of Trees\nR M\nS E\n0 5 10 15 20 0.063\n0.064\n0.065\n0.066\n0.067\n0.068\n0.069\nNumber of Features\nR M\nS E\n0 20 40 60 80 100 0.063\n0.0635\n0.064\n0.0645\n0.065\n0.0655\n0.066\nMinimum Number of Samples at Each Node\nR M\nS E\nNote: This is a pre-print of the full paper that published in Innovative Smart Grid Technologies, North America Conference, 2017, which can be referenced as below:\nM. Abuella and B. Chowdhury, “Random Forest Ensemble of Support Vector Regression for Solar Power Forecasting,” in Proceedings of Innovative Smart Grid\nTechnologies, North America Conference, 2017.\nyield more accurate forecasts. If the training and testing of the model are carried out for just the daylight hours while filtering out the night hours (which have zero solar power generation), the RMSE should also be determined for these daylight hours only without including the night hours.\n\uD835\uDC3C\uD835\uDC5A\uD835\uDC5D\uD835\uDC5F\uD835\uDC5C\uD835\uDC63\uD835\uDC52\uD835\uDC5A\uD835\uDC52\uD835\uDC5B\uD835\uDC61 \uD835\uDC5F\uD835\uDC4E\uD835\uDC61\uD835\uDC52 (%) =\n(\uD835\uDC42\uD835\uDC61ℎ\uD835\uDC52\uD835\uDC5F \uD835\uDC5A\uD835\uDC52\uD835\uDC61ℎ\uD835\uDC5C\uD835\uDC51 − \uD835\uDC38\uD835\uDC5B\uD835\uDC60\uD835\uDC52\uD835\uDC5A\uD835\uDC4F\uD835\uDC59\uD835\uDC52 \uD835\uDC40\uD835\uDC52\uD835\uDC61ℎ\uD835\uDC5C\uD835\uDC51)\n\uD835\uDC42\uD835\uDC61ℎ\uD835\uDC52\uD835\uDC5F \uD835\uDC40\uD835\uDC52\uD835\uDC61ℎ\uD835\uDC5C\uD835\uDC51 ∗ 100\n(5)\nThe best model (model-4) is made of normalization technique (A), SVR’s hyper-parameters C=10 gamma =8, and the original 14 weather inputs by using a dataset of all available months.\nBest Model (4) = All-months dataset + Normalize (A)\n+ Parm10_08 + Orig14ins (6)\nIn the graphical illustration, the month of October is chosen for comparison of all SVRs models with other combined models as shown in Fig.6. For the aggregation of daily RMSEs over the whole month, it is obvious the ensemble forecasts (black line) has a lower RMSE than SVR models and the simple average combining method (dark blue line). However, in a few days, the best SVR model (model-4) or the average method could be more accurate, as in 6th, 7th, 16th, 28th, and 29th days, when they have a lower RMSE.\nThe fluctuation in the daily RMSE can be seen among the 24 SVR models, and it’s a normal trend in the forecasting models. However, the ensemble method produces more accurate combined forecasts and the performance of this combining model becomes more stable than in the individual SVR models.\nFig.6. Daily RMSE of different models and combined forecasts, October\nTo get a broader evaluation of the combined forecasts performance, the comparison is conducted with the best model (model 4) and the simple average method over a complete year, as shown in TABLE I. It is clear that the ensemble method has lower monthly RMSEs. The improvement rate of the ensemble method over the other methods is calculated as in (5). In some months such as October, the ensemble method has an improvement rate of 18% and 28% over the best model and the average method respectively. For two months where the improvement rate is negative, such as in March the simple average and the best model are better than the ensemble, and also in June, the best model has the lowest monthly RMSE.\nIn general, the aggregated mean (i.e., the statistical average) of the monthly RMSEs indicates that the combined forecasts from the ensemble method have the most accurate forecasts (RMSEmean=0.0725), while, the total improvement of the ensemble method over the average and the best model is 9% and 5 % respectively.\nNotice that the monthly RMSE is calculated for all hours of the month where n in (4) is not the day hour, but rather the month hours. Thus, it could be 744, or any other number of hours depending on the month.\nThe Random Forest is a nonlinear model and a black-box and difficult to analyze. In order to get an idea about the performance of the model, a statistical analysis is conducted. Firstly, the importance estimation of the RF inputs (features) is found. This would be available since one of the algorithm steps is to estimate the best features to split the nodes, as explained in Section II.A. October and March are chosen for this analysis, because in these months, the largest change in the ensemble method performance occurs. The features are the weather variables and the 24 SVRs’ outcomes. As shown in Fig.7 and Fig.8, the features do not have the similar pattern of importance; thereby, in the data training by RF, they would give different results and different performance.\nSecondly, a statistical analysis by finding the standard deviation and the correlation between the SVR models’ outcomes is conducted over the full year. Fig.9 presents the histograms of this statistical metrics.\n(a) (b)\nFig.7. The estimation of weather features importance by Random Forest, (a) October, (b) March\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 0.02\n0.04\n0.06\n0.08\n0.1\n0.12\n0.14\n0.16\n0.18\n0.2\n0.22\nDays\nR M\nS E\nThe RMSE of the Models\nM1 M2 M3 M4 M5 M6 M7 M8 M9 M10 M11 M12 M13 M14 M15 M16 M17 M18 M19 M20 M21 M22 M23 M24 Combined Forecasts Average Forecasts\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n1.4\n1.6\nImportance of Features, October\nWeather Features\nIm p\no rt\na n\nc e\nS c o\nre\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n1.4\nImportance of Features, March\nWeather Features\nIm p\no rt\na n\nc e\nS c o\nre\nNote: This is a pre-print of the full paper that published in Innovative Smart Grid Technologies, North America Conference, 2017, which can be referenced as below:\nM. Abuella and B. Chowdhury, “Random Forest Ensemble of Support Vector Regression for Solar Power Forecasting,” in Proceedings of Innovative Smart Grid\nTechnologies, North America Conference, 2017.\n(a) (b)\nFig.8. The estimation of models’ outcomes importance by Random Forest, (a) October, (b) March\nFor October, the standard deviation is high, and hence the\ncorrelation of the SVRs’ outcomes, and thus the ensemble\nmethod, has the best performance. However, in the last\nmonths - March, April, and May, the standard deviation is low\nand the correlation is high, and the performance of the ensemble method in these months is not as good as the other\nmonths, as indicated in TABLE I.\nFig.9. The standard deviation and the correlation of different models\nV. CONCLUSION\nCombining the forecasts by the random forest leads to more accurate forecasts throughout the year. These combined forecasts are produced from an intelligent weighting approach that takes into account the weather situations, the past forecast of the forecasting models (24 SVRs), and their different temporal horizons - these all are used as associative rules in the ensemble method to yield accurate forecasts and a stable performance. The simple average as a combining method is not enough to get better forecasts since it does not capture or represent all the varieties in the weather data, and hence the solar power forecast. The forecasting errors are inherited mostly from the NWP model errors, and some of the errors resulting from the technical degrading of the physical systems (PV modules efficiency, orientation, etc.). Adding the past generated forecasts increases the accuracy of the combined forecasts.\nREFERENCES\n[1] J. M. Morales, A. J. Conejo, H. Madsen, P. Pinson, and M. Zugno,\n“Integrating renewables in electricity markets - Operational problems,” Springer, vol. 205, p. 429, 2014.\n[2] A. Tuohy, J. Zack, S. E. Haupt, J. Sharp, M. Ahlstrom, S. Dise, E. Grimit, C. Mohrlen, M. Lange, M. G. Casado, J. Black, M. Marquis, and C.\nCollier, “Solar Forecasting: Methods, Challenges, and Performance,” IEEE Power Energy Mag., vol. 13, no. 6, pp. 50–59, 2015.\n[3] T. Hong, P. Pinson, S. Fan, H. Zareipour, A. Troccoli, and R. J.\nHyndman, “Probabilistic energy forecasting: Global Energy Forecasting Competition 2014 and beyond,” Int. J. Forecast., 2016.\n[4] C. Voyant, M. Muselli, C. Paoli, and M.-L. Nivet, “Numerical weather prediction (NWP) and hybrid ARMA/ANN model to predict global radiation,” Energy, vol. 39, no. 1, pp. 341–355, 2012.\n[5] L. M. Aguiar, B. Pereira, P. Lauret, F. D’\\iaz, and M. David, “Combining solar irradiance measurements, satellite-derived data and a numerical\nweather prediction model to improve intra-day solar forecasting,” Renew. Energy, vol. 97, pp. 599–610, 2016.\n[6] R. Marquez, H. T. C. Pedro, and C. F. M. Coimbra, “Hybrid solar\nforecasting method uses satellite imaging and ground telemetry as inputs to ANNs,” Sol. Energy, vol. 92, pp. 176–188, 2013.\n[7] J. Liu, “Combining sister load forecasts,” MSc Thesis, University of North Carolina at Charlotte, 2015.\n[8] M. Zamo, O. Mestre, P. Arbogast, and O. Pannekoucke, “A benchmark of statistical regression methods for short-term forecasting of photovoltaic\nelectricity production, part I: Deterministic forecast of hourly production,” Sol. Energy, vol. 105, pp. 792–803, 2014.\n[9] J. Huang, A. Troccoli, and P. Coppin, “An analytical comparison of four\napproaches to modelling the daily variability of solar irradiance using meteorological records,” Renew. Energy, vol. 72, no. October, pp. 195– 202, 2014.\n[10] A. A. Mohammed, W. Yaqub, and Z. Aung, “Probabilistic Forecasting of\nSolar Power: An Ensemble Learning Approach,” in Intelligent Decision Technologies, Springer, 2015, pp. 449–458.\n[11] M. Pierro, F. Bucci, M. De Felice, E. Maggioni, D. Moser, A. Perotto, F.\nSpada, and C. Cornaro, “Multi-Model Ensemble for day ahead prediction of photovoltaic power generation,” Sol. Energy, vol. 134, pp. 132–146, 2016.\n[12] S. Sperati, S. Alessandrini, and L. Delle Monache, “An application of the\nECMWF Ensemble Prediction System for short-term solar power forecasting,” Sol. Energy, vol. 133, pp. 437–450, 2016.\n[13] M. Pierro, F. Bucci, C. Cornaro, E. Maggioni, A. Perotto, M. Pravettoni, and F. Spada, “Model output statistics cascade to improve day ahead solar irradiance forecast,” Sol. Energy, vol. 117, pp. 99–113, 2015.\n[14] W. Cheung, J. Zhang, A. Florita, B.-M. Hodge, S. Lu, H. F. Hamann, Q.\nSun, and B. Lehman, “Ensemble Solar Forecasting Statistical Quantification and Sensitivity Analysis,” 2015.\n[15] S. Lu, Y. Hwang, I. Khabibrakhmanov, F. J. Marianno, X. Shao, J.\nZhang, B.-M. Hodge, and H. F. Hamann, “Machine learning based multiphysical-model blending for enhancing renewable energy forecastimprovement via situation dependent error correction,” in Control Conference (ECC), 2015 European, 2015, pp. 283–290.\n[16] T. Hastie, R. Tibshirani, J. Friedman, and others, The elements of statistical learning, 2 Edition. Springer-Verlag New York, 2009.\n[17] L. Breiman, “Random forests,” Mach. Learn., vol. 45, no. 1, pp. 5–32, 2001.\n[18] M. Abuella and B. Chowdhury, “Solar Power Forecasting Using Support\nVector Regression,” in Proceedings of the American Society for Engineering Management 2016 International Annual Conference, 2016.\nMohamed Abuella (S’14) received his Bachelor of Technology degree from College of industrial Technology, Misurata, Libya in 2008, and M.S. degree from Southern Illinois University Carbondale in 2012. He is currently a PhD student in the Department of Electrical and Computer Engineering at University of North Carolina at Charlotte. His research interest is in planning and operations of electrical power systems.\nBadrul Chowdhury (S’83–M’87–SM’93) received the B.S. degree from the Bangladesh University of Engineering and Technology, Dhaka, Bangladesh, in 1981, and the M.S. and Ph.D. degrees from the Virginia Polytechnic Institute and State University, Blacksburg, VA, USA, in 1983 and 1987, respectively, all in electrical engineering. He is currently a Professor with the Department of Electrical and Computer Engineering with joint appointment with the Department of Systems Engineering and Engineering Management, University of North Carolina at Charlotte, Charlotte, NC, USA. His current research interests include power system modeling, analysis and control, and renewable and distributed energy resource modeling and integration in smart grids. Dr. Chowdhury is a Member of Tau Beta Pi and Phi Kappa Phi.\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nImportance of outcomes, October\nModels’ Outocmes\nIm p\no rt\na n c e\nS c o\nre\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nImportance of outcomes, March\nModels’ Outocmes\nIm p\no rt\na n c e\nS c o\nre\n0.950\n0.955\n0.960\n0.965\n0.970\n0.975\n0.980\n0.985\n0.990\n0.995\n1.000\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\n0.014\n0.016\n0.018\nC o\nrr el\na ti\no n\nS td\n. D\nev .\nStd. Dev. and Correlation of Models Std.Dev.\nCorrelation"
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "To mitigate the uncertainty of variable renewable<lb>resources, two off-the-shelf machine learning tools are deployed<lb>to forecast the solar power output of a solar photovoltaic system.<lb>The support vector machines generate the forecasts and the<lb>random forest acts as an ensemble learning method to combine<lb>the forecasts. The common ensemble technique in wind and solar<lb>power forecasting is the blending of meteorological data from<lb>several sources. In this study though, the present and the past<lb>solar power forecasts from several models, as well as the<lb>associated meteorological data, are incorporated into the random<lb>forest to combine and improve the accuracy of the day-ahead<lb>solar power forecasts. The performance of the combined model is<lb>evaluated over the entire year and compared with other<lb>combining techniques. Keywords—Ensemble learning, post-processing, random forest,<lb>solar power, support vector regression.",
    "creator" : "Microsoft® Word 2016"
  }
}