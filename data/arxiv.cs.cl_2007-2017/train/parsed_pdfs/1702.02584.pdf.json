{
  "name" : "1702.02584.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Convolutional Neural Network for Humor Recognition",
    "authors" : [ "Lei Chen", "Chong Min Lee" ],
    "emails" : [ "LChen@ets.org", "CLee001@ets.org" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The ability to make effective presentations has been found to be linked with success at school and in the workplace. Humor plays important roles in successful public speaking, e.g., helping to reduce public speaking anxiety, which was treated as the most prevalent type of social phobia, generating shared amusement to boost persuasive power, and serving as a means to attract attention and reduce tension (Xu, 2016).\nAutomatically simulating an audience’s reactions to humor will not only be useful for presentation training, but also improve other conversational systems by giving machines more empathetic power. The present study reports our efforts in recognizing utterances that cause laughter in presentations. These include building a corpus\nfrom TED talks and using Convolutional Neural Networks (CNNs) in the recognition.\nThe remainder of the paper is organized as follows: Section 2 briefly reviews the previous related research; Section 3 describes the corpus we collected from TED talks; Section 4 describes the text classification methods; Section 5 reports on our experiments; and finally Section 6 discusses the findings of our study and plans for future work."
    }, {
      "heading" : "2 Previous Research",
      "text" : "Humor recognition refers to the task of deciding whether a sentence/spoken-utterance expresses a certain degree of humor. In most of the previous studies (Mihalcea and Strapparava, 2005; Purandare and Litman, 2006; Yang et al., 2015), humor recognition was modeled as a binary classification task\nIn the seminal work (Mihalcea and Strapparava, 2005), a corpus of 16,000 “one-liners” was created using daily joke websites to collect humorous instances while using formal writing resources (e.g., news titles) to obtain non-humorous instances. Three humor-specific stylistic features, including alliteration, antonymy, and adult slang were utilized together with content-based features to build classifiers. In a recent work (Yang et al., 2015), a new corpus was constructed from a Pun of the Day website. It systematically explained and computed latent semantic structure features based on the following four aspects: (a) Incongruity, (b) Ambiguity, (c) Interpersonal Effect, and (d) Phonetic Style. In addition, Word2Vec (Mikolov et al., 2013) distributed representations were utilized in the model building.\nBeyond lexical cues from text inputs, other research has also utilized speakers’ acoustic cues (Purandare and Litman, 2006; Bertero and Fung, 2016b). These studies have typically used\nar X\niv :1\n70 2.\n02 58\n4v 1\n[ cs\n.C L\n] 8\nF eb\n2 01\n7\naudio tracks from TV shows and their corresponding captions in order to categorize characters’ speaking turns as humorous or non-humorous. Utterances prior to canned laughter that was manually inserted into the shows were treated as humorous, while other utterances were treated as negative cases.\nConvolutional Neural Networks (CNNs) have recently been successfully used in several text categorization tasks (e.g., review rating, sentiment recognition, and question type recognition). Kim (2014); Johnson and Zhang (2015); Zhang and Wallace (2015) suggested that using a simple CNN setup, which entails one layer of convolution on top of word embedding vectors, achieves excellent results on multiple tasks. Deep learning is rapidly being applied to computational humor research (Bertero and Fung, 2016b,a). In Bertero and Fung (2016b), CNN was found to be the best model that uses both acoustic and lexical cues for humor recognition. By using Long Short Time Memory (LSTM) cells (Hochreiter and Schmidhuber, 1997), Bertero and Fung (2016a) showed that Recurrent Neural Networks (RNNs) perform better on modeling sequential information than Conditional Random Fields (CRFs) (Lafferty et al., 2001).\nFrom the brief review, we can find that the limited number of previously created corpora only cover one-line puns or jokes and conversations from TV comedy shows. There is a great need for an open corpus that can support investigating humor in presentations.1 CNN-based text categorization methods have been applied for humor recognition (e.g., in (Bertero and Fung, 2016b)) but with limitations: (a) a rigorous comparison with the state-of-the-art conventional method examined in Yang et al. (2015) is missing; (b) CNN’s performance in the previous research is not quite clear2; and (c) some important techniques that can improve CNN performance (e.g., using variedsized filters and dropout regularization (Hinton et al., 2012)) were missing. Therefore, the present study is meant to address these limitations.\n1While we were working on this paper, we found a recent Master’s thesis (Acosta, 2016) that also conducted research on detecting laughter on the TED transcriptions. However, that study only explored conventional text classification approaches.\n2Though CNN works best when using both lexical and acoustic cues, it did not outperform the Logistical Regression (LR) model when using text inputs exclusively."
    }, {
      "heading" : "3 TED Talk Data",
      "text" : "TED Talks3 are recordings from TED conferences and other special TED programs. In the present study, we focused on the transcripts of the talks. Most transcripts of the talks contain the markup ‘(Laughter)’, which represents where audiences laughed aloud during the talks. This special markup was used to determine utterance labels.\nWe collected 1,192 TED Talk transcripts4. An example transcription is given in Figure 1. The collected transcripts were split into sentences using the Stanford CoreNLP tool (Manning et al., 2014). In this study, sentences containing or immediately followed by ‘(Laughter)’ were used as humorous sentences, as shown in Figure 1; all other sentences were defined as non-humorous sentences. Following (Mihalcea and Strapparava, 2005; Yang et al., 2015), we selected the same sizes (n = 4726) of humorous and non-humorous sentences. To minimize possible topic shifts between positive and negative instances, for each positive instance, we picked up one negative instance nearby (the context window was 7 sentences in this study). For example, in Figure 1, a negative instance (corresponding to ‘sent-2’) was selected from the nearby sentences ranging from ‘sent-7’ and ‘sent+7’."
    }, {
      "heading" : "4 Methods",
      "text" : ""
    }, {
      "heading" : "4.1 Conventional Model",
      "text" : "Following Yang et al. (2015), we applied Random Forest (Breiman, 2001) to do humor recognition by using the following two groups of features. The first group are latent semantic structural features covering the following 4 categories5: Incongruity (2), Ambiguity (6), Interpersonal Effect (4), and Phonetic Pattern (4). The second group are semantic distance features, including the humor label classes from 5 sentences in the training set that are closest to this sentence (found by using a k-Nearest Neighbors (kNN) method), and each sentence’s averaged Word2Vec representations (n = 300).\n3http://www.ted.com 4The transcripts were collected on 7/9/2015. 5The number in parenthesis indicates how many features\nare in that category\nsent-7 . . . . . . . . . No-Humorous He has no memory of the past, no knowledge of the future, and he only cares about two things: easy and fun. sent-1 Now, in the animal world, that works fine. Humorous If you’re a dog and you spend your whole life doing nothing other than easy and fun things, you’re a huge success! (Laughter) sent+1 And to the Monkey, humans are just another animal species. . . . . . . sent+7 . . .\nFigure 1: An excerpt from TED talk “Tim Urban: Inside the mind of a master procrastinator” (http: //bit.ly/2l1P3RJ)\nFigure 2: CNN network architecture"
    }, {
      "heading" : "4.2 CNN model",
      "text" : "Our CNN-based text classification’s setup follows Kim (2014). Figure 2 depicts the model’s details. From the left side’s input texts to the right side’s prediction labels, different shapes of tensors flow through the entire network for solving the classification task in an end-to-end mode.\nFirstly, input tokenized text strings were converted to a 2D tensor with shape (L × d), where L represents sentences’ maximum length while d represents the word-embedding dimension. In this study, we utilized the Word2Vec (Mikolov et al., 2013) embedding vectors (d = 300) that were trained from 100 billion words of Google News. Next, the embedding matrix was fed into a 1D convolution network with multiple filters. To cover varied reception fields, the size of the filters changed from fw−1, fw, and fw+1. For each filter size, fn filters were utilized. Then, max pooling, which stands for finding the largest value from a vector, was applied to each feature map (total 3 × fn feature maps) output by the 1D convolution. Finally, maximum values from all of 3 × fn filters were formed as a flattened vector to go through a fully connected (FC) layer to predict two possible labels (Humor vs. Non-Humor).\nNote that for 1D convolution and FC layer’s input, we applied ‘dropout’ (Hinton et al., 2012) regularization, which entails randomly setting a proportion of network weights to be zero during model training, to overcome overfitting. By using cross-entropy as the learning metric, the whole sequential network (all weights and bias) could be optimized by using any SGD optimization, e.g., Adam (Kingma and Ba, 2014), Adadelta (Zeiler, 2012), and so on."
    }, {
      "heading" : "5 Experiments",
      "text" : "We used two corpora: the TED Talk corpus (denoted as TED) and the Pun of the Day corpus6 (denoted as Pun). Note that we normalized words in the Pun data to lowercase to avoid avoid a possibly elevated result caused by a special pattern: in the original format, all negative instances started with capital letters. The Pun data allows us to verify that our implementation is consistent with the work reported in Yang et al. (2015).\nIn our experiment, we firstly divided each corpus into two parts. The smaller part (the Held-Out Partition) was used for tweaking various hyper-\n6The authors of Yang et al. (2015) kindly shared their data with us. We would like to thank them for their generosity.\nparameters used in text classifiers. The larger portion (the CV Partition) was then formulated as a 10-fold cross-validation setup for obtaining a stable and comprehensive model evaluation result. Note that, with a goal of building a speakerindependent humor detector, when partitioning our TED data set, we always kept all of utterances of a single a talk within the same partition. Therefore, in our experimental setup, we always evaluated “unseen” utterances from the talks that had not been used in the training stage. To our knowledge, this is the first time that such a strict experimental setup has been used in recognizing humor in conversations, and it makes the humor recognition task on the TED data quite challenging.\nWhen building conventional models, we developed our own feature extraction scripts and used the SKLL7 python package for tweaking and training Random Forest models. When implementing CNN, we used the Keras8 Python package.9 Regarding hyper-parameter tweaking, we utilized the Tree Parzen Estimation (TPE) method as detailed in Bergstra et al. (2012). After running 200 iterations of tweaking, we ended up with the following selection: fw is 6 (entailing that the various filter sizes are (5, 6, 7)), fn is 100, dropout1 is 0.7 and dropout2 is 0.35, optimization uses Adam (Kingma and Ba, 2014). When training the CNN model, we randomly selected 10% of the training data as the validation set for using early stopping to avoid overfitting.\n7https://github.com/ EducationalTestingService/skll\n8https://github.com/fchollet/keras 9The implementation will be released with the paper\nOn the Pun data, the CNN model shows consistent improved performance over the conventional model, as suggested in Yang et al. (2015). In particular, precision has been greatly increased from 0.762 to 0.864. On the TED data, we also observed that the CNN model helps to increase precision (from 0.515 to 0.582) and accuracy (from 52.0% to 58.9%). The empirical evaluation results suggests that the CNN-based model has an advantage on the humor recognition task. In addition, focusing on the system development time, generating and implementing those features in the conventional model would take days or even weeks. However, the CNN model automatically learns its optimal feature representation and can adjust the features automatically across data sets. This makes the CNN model quite versatile for supporting different tasks and data domains. Compared with the humor recognition results on the Pun data, the results on the TED data is still quite low, and more research is needed to fully handle humor in authentic presentations."
    }, {
      "heading" : "6 Discussion",
      "text" : "For the purpose of monitoring how well speakers can use humor during their presentations, we have created a corpus from TED talks. Compared to the existing (albeit limited) corpora for humor recognition research, ours has the following advantages: (a) it was collected from authentic talks, rather than from TV shows performed by professional actors based on scripts; (b) it contains about 100 times more speakers compared to the limited number of actors in existing corpora. We compared two types of leading text-based humor recognition methods: a conventional classifier (e.g., random forest) based on human-engineered features vs. an end-to-end CNN method, which relies on its inherent representation learning. We found that the CNN method has better performance. More importantly, the representation learning of the CNN method makes it very efficient when facing new data sets.\nStemming from the present study, we envision that more research is worth pursuing: (a) for presentations, cues from other modalities such as audio or video will be included, similar to Bertero and Fung (2016b); (b) context information from multiple utterances will be modeled by using sequential modeling methods."
    } ],
    "references" : [ {
      "title" : "Laff-O-Tron: Laugh Prediction in TED Talks",
      "author" : [ "Andrew D. Acosta." ],
      "venue" : "Master’s thesis, California Polytechnic State University, San Luis Obispo, CA.",
      "citeRegEx" : "Acosta.,? 2016",
      "shortCiteRegEx" : "Acosta.",
      "year" : 2016
    }, {
      "title" : "Making a Science of Model Search",
      "author" : [ "J. Bergstra", "D. Yamins", "D.D. Cox." ],
      "venue" : "ArXiv .",
      "citeRegEx" : "Bergstra et al\\.,? 2012",
      "shortCiteRegEx" : "Bergstra et al\\.",
      "year" : 2012
    }, {
      "title" : "A long short-term memory framework for predicting humor in dialogues",
      "author" : [ "D Bertero", "P Fung." ],
      "venue" : "Proceedings of NAACL-HLT .",
      "citeRegEx" : "Bertero and Fung.,? 2016a",
      "shortCiteRegEx" : "Bertero and Fung.",
      "year" : 2016
    }, {
      "title" : "Deep learning of audio and language features for humor prediction",
      "author" : [ "D Bertero", "P Fung." ],
      "venue" : "International Conference on Language Resources and Evaluation (LREC).",
      "citeRegEx" : "Bertero and Fung.,? 2016b",
      "shortCiteRegEx" : "Bertero and Fung.",
      "year" : 2016
    }, {
      "title" : "Random forests",
      "author" : [ "L Breiman." ],
      "venue" : "Machine learning 45(1):5–32.",
      "citeRegEx" : "Breiman.,? 2001",
      "shortCiteRegEx" : "Breiman.",
      "year" : 2001
    }, {
      "title" : "Improving neural networks by preventing coadaptation of feature detectors",
      "author" : [ "GE Hinton", "N Srivastava", "A Krizhevsky." ],
      "venue" : "arXiv preprint arXiv: .",
      "citeRegEx" : "Hinton et al\\.,? 2012",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2012
    }, {
      "title" : "Semi-supervised convolutional neural networks for text categorization via region embedding",
      "author" : [ "R Johnson", "T Zhang." ],
      "venue" : "Advances in neural information processing .",
      "citeRegEx" : "Johnson and Zhang.,? 2015",
      "shortCiteRegEx" : "Johnson and Zhang.",
      "year" : 2015
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Y Kim." ],
      "venue" : "Proc. EMNLP. Doha, Qatar, pages 1746–1751.",
      "citeRegEx" : "Kim.,? 2014",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "D Kingma", "J Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980 .",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "J Lafferty", "A McCallum", "F Pereira." ],
      "venue" : "In Proceedings of the eighteenth international conference on machine learning, ICML.",
      "citeRegEx" : "Lafferty et al\\.,? 2001",
      "shortCiteRegEx" : "Lafferty et al\\.",
      "year" : 2001
    }, {
      "title" : "The Stanford CoreNLP natural language processing toolkit",
      "author" : [ "Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky." ],
      "venue" : "Association for Computational Linguistics (ACL) System Demonstrations.",
      "citeRegEx" : "Manning et al\\.,? 2014",
      "shortCiteRegEx" : "Manning et al\\.",
      "year" : 2014
    }, {
      "title" : "Making computers laugh: Investigations in automatic humor recognition",
      "author" : [ "Rada Mihalcea", "Carlo Strapparava." ],
      "venue" : "Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing. As-",
      "citeRegEx" : "Mihalcea and Strapparava.,? 2005",
      "shortCiteRegEx" : "Mihalcea and Strapparava.",
      "year" : 2005
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "T Mikolov", "I Sutskever", "K Chen." ],
      "venue" : "Advances in neural information processing systems .",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Humor: Prosody analysis and automatic recognition for F*R*I*E*N*D*S",
      "author" : [ "Amruta Purandare", "Diane J. Litman." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Purandare and Litman.,? 2006",
      "shortCiteRegEx" : "Purandare and Litman.",
      "year" : 2006
    }, {
      "title" : "Laughing Matters: Humor Strategies in Public Speaking",
      "author" : [ "Z Xu." ],
      "venue" : "Asian Social Science 12(1):117.",
      "citeRegEx" : "Xu.,? 2016",
      "shortCiteRegEx" : "Xu.",
      "year" : 2016
    }, {
      "title" : "Humor recognition and humor anchor extraction",
      "author" : [ "Diyi Yang", "Alon Lavie", "Chris Dyer", "Eduard Hovy." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Lisbon,",
      "citeRegEx" : "Yang et al\\.,? 2015",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2015
    }, {
      "title" : "ADADELTA: an adaptive learning rate method",
      "author" : [ "MD Zeiler." ],
      "venue" : "arXiv preprint arXiv:1212.5701 .",
      "citeRegEx" : "Zeiler.,? 2012",
      "shortCiteRegEx" : "Zeiler.",
      "year" : 2012
    }, {
      "title" : "A sensitivity analysis of (and practitioners’ guide to) convolutional neural networks for sentence classification",
      "author" : [ "Y Zhang", "B Wallace." ],
      "venue" : "arXiv:1510.03820.",
      "citeRegEx" : "Zhang and Wallace.,? 2015",
      "shortCiteRegEx" : "Zhang and Wallace.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : ", helping to reduce public speaking anxiety, which was treated as the most prevalent type of social phobia, generating shared amusement to boost persuasive power, and serving as a means to attract attention and reduce tension (Xu, 2016).",
      "startOffset" : 226,
      "endOffset" : 236
    }, {
      "referenceID" : 11,
      "context" : "In most of the previous studies (Mihalcea and Strapparava, 2005; Purandare and Litman, 2006; Yang et al., 2015), humor recognition was modeled as a binary classification task",
      "startOffset" : 32,
      "endOffset" : 111
    }, {
      "referenceID" : 13,
      "context" : "In most of the previous studies (Mihalcea and Strapparava, 2005; Purandare and Litman, 2006; Yang et al., 2015), humor recognition was modeled as a binary classification task",
      "startOffset" : 32,
      "endOffset" : 111
    }, {
      "referenceID" : 15,
      "context" : "In most of the previous studies (Mihalcea and Strapparava, 2005; Purandare and Litman, 2006; Yang et al., 2015), humor recognition was modeled as a binary classification task",
      "startOffset" : 32,
      "endOffset" : 111
    }, {
      "referenceID" : 11,
      "context" : "In the seminal work (Mihalcea and Strapparava, 2005), a corpus of 16,000 “one-liners” was",
      "startOffset" : 20,
      "endOffset" : 52
    }, {
      "referenceID" : 15,
      "context" : "In a recent work (Yang et al., 2015), a new corpus was constructed from a Pun of the Day website.",
      "startOffset" : 17,
      "endOffset" : 36
    }, {
      "referenceID" : 12,
      "context" : "In addition, Word2Vec (Mikolov et al., 2013) distributed representations were utilized in the model building.",
      "startOffset" : 22,
      "endOffset" : 44
    }, {
      "referenceID" : 13,
      "context" : "Beyond lexical cues from text inputs, other research has also utilized speakers’ acoustic cues (Purandare and Litman, 2006; Bertero and Fung, 2016b).",
      "startOffset" : 95,
      "endOffset" : 148
    }, {
      "referenceID" : 3,
      "context" : "Beyond lexical cues from text inputs, other research has also utilized speakers’ acoustic cues (Purandare and Litman, 2006; Bertero and Fung, 2016b).",
      "startOffset" : 95,
      "endOffset" : 148
    }, {
      "referenceID" : 6,
      "context" : "Kim (2014); Johnson and Zhang (2015); Zhang and Wallace (2015) suggested that using a simple CNN setup, which entails one layer of convolu-",
      "startOffset" : 0,
      "endOffset" : 11
    }, {
      "referenceID" : 6,
      "context" : "Kim (2014); Johnson and Zhang (2015); Zhang and Wallace (2015) suggested that using a simple CNN setup, which entails one layer of convolu-",
      "startOffset" : 12,
      "endOffset" : 37
    }, {
      "referenceID" : 6,
      "context" : "Kim (2014); Johnson and Zhang (2015); Zhang and Wallace (2015) suggested that using a simple CNN setup, which entails one layer of convolu-",
      "startOffset" : 12,
      "endOffset" : 63
    }, {
      "referenceID" : 2,
      "context" : "Deep learning is rapidly being applied to computational humor research (Bertero and Fung, 2016b,a). In Bertero and Fung (2016b), CNN was found to be the best model that uses both acoustic and lexical cues",
      "startOffset" : 72,
      "endOffset" : 128
    }, {
      "referenceID" : 2,
      "context" : "By using Long Short Time Memory (LSTM) cells (Hochreiter and Schmidhuber, 1997), Bertero and Fung (2016a) showed that Recurrent Neural Networks (RNNs) perform better on modeling sequential information than Con-",
      "startOffset" : 81,
      "endOffset" : 106
    }, {
      "referenceID" : 9,
      "context" : "ditional Random Fields (CRFs) (Lafferty et al., 2001).",
      "startOffset" : 30,
      "endOffset" : 53
    }, {
      "referenceID" : 3,
      "context" : ", in (Bertero and Fung, 2016b)) but with limitations: (a) a rigorous comparison with the state-of-the-art conventional method examined in Yang et al.",
      "startOffset" : 5,
      "endOffset" : 30
    }, {
      "referenceID" : 5,
      "context" : ", using variedsized filters and dropout regularization (Hinton et al., 2012)) were missing.",
      "startOffset" : 55,
      "endOffset" : 76
    }, {
      "referenceID" : 2,
      "context" : ", in (Bertero and Fung, 2016b)) but with limitations: (a) a rigorous comparison with the state-of-the-art conventional method examined in Yang et al. (2015) is missing; (b) CNN’s performance in the previous research is not quite clear2; and (c) some important techniques that can improve CNN performance (e.",
      "startOffset" : 6,
      "endOffset" : 157
    }, {
      "referenceID" : 0,
      "context" : "While we were working on this paper, we found a recent Master’s thesis (Acosta, 2016) that also conducted research on detecting laughter on the TED transcriptions.",
      "startOffset" : 71,
      "endOffset" : 85
    }, {
      "referenceID" : 10,
      "context" : "The collected transcripts were split into sentences using the Stanford CoreNLP tool (Manning et al., 2014).",
      "startOffset" : 84,
      "endOffset" : 106
    }, {
      "referenceID" : 11,
      "context" : "Following (Mihalcea and Strapparava, 2005; Yang et al., 2015), we selected the same sizes (n = 4726) of humorous and non-humorous",
      "startOffset" : 10,
      "endOffset" : 61
    }, {
      "referenceID" : 15,
      "context" : "Following (Mihalcea and Strapparava, 2005; Yang et al., 2015), we selected the same sizes (n = 4726) of humorous and non-humorous",
      "startOffset" : 10,
      "endOffset" : 61
    }, {
      "referenceID" : 4,
      "context" : "(2015), we applied Random Forest (Breiman, 2001) to do humor recognition by using the following two groups of features.",
      "startOffset" : 33,
      "endOffset" : 48
    }, {
      "referenceID" : 14,
      "context" : "Following Yang et al. (2015), we applied Random Forest (Breiman, 2001) to do humor recognition by using the following two groups of features.",
      "startOffset" : 10,
      "endOffset" : 29
    }, {
      "referenceID" : 7,
      "context" : "Our CNN-based text classification’s setup follows Kim (2014). Figure 2 depicts the model’s details.",
      "startOffset" : 50,
      "endOffset" : 61
    }, {
      "referenceID" : 12,
      "context" : "In this study, we utilized the Word2Vec (Mikolov et al., 2013) embedding vectors (d = 300) that were trained from 100 billion words of Google News.",
      "startOffset" : 40,
      "endOffset" : 62
    }, {
      "referenceID" : 5,
      "context" : "Note that for 1D convolution and FC layer’s input, we applied ‘dropout’ (Hinton et al., 2012) regularization, which entails randomly setting a",
      "startOffset" : 72,
      "endOffset" : 93
    }, {
      "referenceID" : 8,
      "context" : "Adam (Kingma and Ba, 2014), Adadelta (Zeiler, 2012), and so on.",
      "startOffset" : 5,
      "endOffset" : 26
    }, {
      "referenceID" : 16,
      "context" : "Adam (Kingma and Ba, 2014), Adadelta (Zeiler, 2012), and so on.",
      "startOffset" : 37,
      "endOffset" : 51
    }, {
      "referenceID" : 15,
      "context" : "The Pun data allows us to verify that our implementation is consistent with the work reported in Yang et al. (2015).",
      "startOffset" : 97,
      "endOffset" : 116
    }, {
      "referenceID" : 15,
      "context" : "The authors of Yang et al. (2015) kindly shared their data with us.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 8,
      "context" : "35, optimization uses Adam (Kingma and Ba, 2014).",
      "startOffset" : 27,
      "endOffset" : 48
    }, {
      "referenceID" : 1,
      "context" : "9 Regarding hyper-parameter tweaking, we utilized the Tree Parzen Estimation (TPE) method as detailed in Bergstra et al. (2012). After running 200 iterations of tweaking, we ended up with the following selection: fw is 6 (entailing that the various filter sizes are (5, 6, 7)), fn is 100, dropout1 is 0.",
      "startOffset" : 105,
      "endOffset" : 128
    }, {
      "referenceID" : 15,
      "context" : "tent improved performance over the conventional model, as suggested in Yang et al. (2015). In particular, precision has been greatly increased from 0.",
      "startOffset" : 71,
      "endOffset" : 90
    }, {
      "referenceID" : 2,
      "context" : "Stemming from the present study, we envision that more research is worth pursuing: (a) for presentations, cues from other modalities such as audio or video will be included, similar to Bertero and Fung (2016b); (b) context information from multiple utterances will be modeled by using sequential modeling methods.",
      "startOffset" : 185,
      "endOffset" : 210
    } ],
    "year" : 2017,
    "abstractText" : "For the purpose of automatically evaluating speakers’ humor usage, we build a presentation corpus containing humorous utterances based on TED talks. Compared to previous data resources supporting humor recognition research, ours has several advantages, including (a) both positive and negative instances coming from a homogeneous data set, (b) containing a large number of speakers, and (c) being open. Focusing on using lexical cues for humor recognition, we systematically compare a newly emerging text classification method based on Convolutional Neural Networks (CNNs) with a well-established conventional method using linguistic knowledge. The CNN method shows its advantages on both higher recognition accuracies and being able to learn essential features auto-",
    "creator" : "LaTeX with hyperref package"
  }
}