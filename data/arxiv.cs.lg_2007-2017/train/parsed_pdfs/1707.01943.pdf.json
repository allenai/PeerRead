{
  "name" : "1707.01943.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A causal framework for explaining the predictions of black-box sequence-to-sequence models",
    "authors" : [ "David Alvarez-Melis" ],
    "emails" : [ "tommi}@csail.mit.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We interpret the predictions of any blackbox structured input-structured output model around a specific input-output pair. Our method returns an “explanation” consisting of groups of input-output tokens that are causally related. These dependencies are inferred by querying the black-box model with perturbed inputs, generating a graph over tokens from the responses, and solving a partitioning problem to select the most relevant components. We focus the general approach on sequence-tosequence problems, adopting a variational autoencoder to yield meaningful input perturbations. We test our method across several NLP sequence generation tasks."
    }, {
      "heading" : "1 Introduction",
      "text" : "Interpretability is often the first casualty when adopting complex predictors. This is particularly true for structured prediction methods at the core of many natural language processing tasks such as machine translation (MT). For example, deep learning models for NLP involve a large number of parameters and complex architectures, making them practically black-box systems. While such systems achieve state-of-the-art results in MT (Bahdanau et al., 2014), summarization (Rush et al., 2015) and speech recognition (Chan et al., 2015), they remain largely uninterpretable, although attention mechanisms (Bahdanau et al., 2014) can shed some light on how they operate.\nStronger forms of interpretability could offer several advantages, from trust in model predic-\ntions, error analysis, to model refinement. For example, critical medical decisions are increasingly being assisted by complex predictions that should lend themselves to easy verification by human experts. Without understanding how inputs get mapped to the outputs, it is also challenging to diagnose the source of potential errors. A slightly less obvious application concerns model improvement (Ribeiro et al., 2016) where interpretability can be used to detect biases in the methods.\nInterpretability has been approached primarily from two main angles: model interpretability, i.e., making the architecture itself interpretable, and prediction interpretability, i.e., explaining particular predictions of the model (cf. (Lei et al., 2016)). Requiring the model itself to be transparent is often too restrictive and challenging to achieve. Indeed, prediction interpretability can be more easily sought a posteriori for black-box systems including neural networks.\nIn this work, we propose a novel approach to prediction interpretability with only oracle access to the model generating the prediction. Following (Ribeiro et al., 2016), we turn the local behavior of the model around the given input into an interpretable representation of its operation. In contrast to previous approaches, we consider structured prediction where both inputs and outputs are combinatorial objects, and our explanation consists of a summary of operation rather than a simpler prediction method.\nOur method returns an “explanation” consisting of sets of input and output tokens that are causally related under the black-box model. Causal dependencies arise from analyzing perturbed versions of inputs that are passed through the black-\nar X\niv :1\n70 7.\n01 94\n3v 2\n[ cs\n.L G\n] 2\n5 Ju\nl 2 01\n7\nbox model. Although such perturbations might be available in limited cases, we generate them automatically. For sentences, we adopt a variational autoencoder to produce semantically related sentence variations. The resulting inferred causal dependencies (interval estimates) form a dense bipartite graph over tokens from which explanations can be derived as robust min-cut k-partitions.\nWe demonstrate quantitatively that our method can recover known dependencies. As a starting point, we show that a grapheme-to-phoneme dictionary can be largely recovered if given to the method as a black-box model. We then show that the explanations provided by our method closely resemble the attention scores used by a neural machine translation system. Moreover, we illustrate how our summaries can be used to gain insights and detect biases in translation systems. Our main contributions are:\n• We propose a general framework for explaining structured black-box models\n• For sequential data, we propose a variational autoencoder for controlled generation of input perturbations required for causal analysis\n• We evaluate the explanations produced by our framework on various sequence-tosequence prediction tasks, showing they can recover known associations and provide insights into the workings of complex systems."
    }, {
      "heading" : "2 Related Work",
      "text" : "There is a wide body of work spanning various fields centered around the notion of “interpretability”. This term, however, is underdetermined, so the goals, methods and formalisms of these approaches are often non-overlapping (Lipton, 2016). In the context of machine learning, perhaps the most visible line of work on interpretability focuses on medical applications (Caruana et al., 2015), where trust can be a decisive factor on whether a model is used or not. With the ever-growing success and popularity of deep learning methods for image processing, recent work has addressed interpretability in this setting, usually requiring access to the method’s activations and gradients (Selvaraju et al., 2016), or directly modeling how influence propagates (Bach\net al., 2015). For a broad overview of interpretability in machine learning, we refer the reader to the recent survey by Doshi-Velez and Kim (2017).\nMost similar to this work are the approaches of Lei et al. (2016) and Ribeiro et al. (2016). The former proposes a model that justifies its predictions in terms of fragments of the input. This approach formulates explanation generation as part of the learning problem, and, as most previous work, only deals with the case where predictions are scalar or categorical. On the other hand, Ribeiro et al. (2016) propose a framework for explaining the predictions of black-box classifiers by means of locally-faithful interpretable models. They focus on sparse linear models as explanations, and rely on local perturbations of the instance to explain. Their model assumes the input directly admits a fixed size interpretable representation in euclidean space, so their framework operates directly on this vector-valued representation.\nOur method differs from—and can be thought of as generalizing—these approaches in two fundamental aspects. First, our framework considers both inputs and outputs to be structured objects thus extending beyond the classification setting. This requires rethinking the notion of explanation to adapt it to variable-size combinatorial objects. Second, while our approach shares the locality and model-agnostic view of Ribeiro et al. (2016), generating perturbed versions of structured objects is a challenging task by itself. We propose a solution to this problem in the case of sequence-tosequence learning."
    }, {
      "heading" : "3 Interpreting structured prediction",
      "text" : "Explaining predictions in the structured inputstructured output setting poses various challenges. As opposed to scalar or categorical prediction, structured predictions vary in size and complexity. Thus, one must decide not only how to explain the prediction, but also what parts of it to explain. Intuitively, the “size” of an explanation should grow with the size of the input and output. A good explanation would ideally also decompose into cognitive chunks (Doshi-Velez and Kim, 2017): basic units of explanation which are a priori bounded in size. Thus, we seek a framework that naturally\ndecomposes an explanation into (potentially several) explaining components, each of which justifies, from the perspective of the black-box model, parts of the output relative to the parts of the input.\nFormally, suppose we have a black-box model F : X → Y that maps a structured input x ∈ X to a structured output y ∈ Y . We make no assumptions on the spaces X ,Y , except that their elements admit a feature-set representation x = {x1, x2, . . . , xn}, y = {y1, y2, . . . , ym}. Thus, x and y can be sequences, graphs or images. We refer to the elements xi and yj as units or “tokens” due to our motivating application of sentences, though everything in this work holds for other combinatorial objects.\nFor a given input output pair (x,y), we are interested in obtaining an explanation of y in terms of x. Following (Ribeiro et al., 2016), we seek explanations via interpretable representations that are both i) locally faithful, in the sense that they approximate how the model behaves in the vicinity of x, and ii) model agnostic, that is, that do not require any knowledge ofF . For example, we would like to identify whether token xi is a likely cause for the occurrence of yj in the output when the input context is x. Our assumption is that we can summarize the behavior of F around x in terms of a weighted bipartite graph G = (Vx ∪ Vy, E), where the nodes Vx and Vy correspond to the elements in x and y, respectively, and the weight of each edge Eij corresponds to the influence of the occurrence of token xi on the appearance of yj . The bipartite graph representation suggests naturally that the explanation be given in terms of explaining components. We can formalize these components as subgraphs Gk = (V kx ∪ V ky , Ek), where the elements in V kx are likely causes for the elements in V ky . Thus, we define an explanation of y as a collection of such components: Ex→y = {G1, . . . , Gk}.\nOur approach formalizes this framework through a pipeline (sketched in Figure 1) consisting of three main components, described in detail in the following section: a perturbation model for exercising F locally, a causal inference model for inferring associations between inputs and predictions, and a selection step for partitioning and selecting the most relevant sets of associations.\nWe refer to this framework as a structured-output causal rationalizer (SOCRAT).\nA note on alignment models When the inputs and outputs are sequences such as sentences, one might envision using an alignment model, such as those used in MT, to provide an explanation. This differs from our approach in several respects. Specifically, we focus on explaining the behavior of the “black box” mapping F only locally, around the current input context, not globally. Any global alignment model would require access to substantial parallel data to train and would have varying coverage of the local context around the specific example of interest. Any global model would likely also suffer from misspecification in relation to F . A more related approach to ours would be an alignment model trained locally based on the same perturbed sentences and associated outputs that we generate."
    }, {
      "heading" : "4 Building blocks",
      "text" : ""
    }, {
      "heading" : "4.1 Perturbation Model",
      "text" : "The first step in our approach consists of obtaining perturbed versions of the input: semantically similar to the original but with potential changes in elements and their order. This is a major challenge with any structured inputs. We propose to do this using a variational autoencoder (VAE) (Kingma and Welling, 2014; Rezende et al., 2014). VAEs have been successfully used with fixed dimensional inputs such as images (Rezende and Mohamed, 2015; Sønderby et al., 2016) and recently also adapted to generating sentences from continuous representations (Bowman et al., 2016). The goal is to introduce the perturbation in the continuous latent representation rather than directly on the structured inputs.\nA VAE is composed of a probabilistic encoder ENC : X → Rd and a decoder DEC : Rd → X . The encoder defines a distribution over latent codes q(z|x), typically by means of a twostep procedure that first maps x 7→ (µ,σ) and then samples z from a gaussian distribution with these parameters. We can leverage this stochasticity to obtain perturbed versions of the input\nby sampling repeatedly from this distribution, and then mapping these back to the original space using the decoder. The training regime for the VAE ensures approximately that a small perturbation of the hidden representation maintains similar semantic content while introducing small changes in the decoded surface form. We emphasize that the approach would likely fail with an ordinary autoencoder where small changes in the latent representation can result in large changes in the decoded output. In practice, we ensure diversity of perturbations by scaling the variance term σ and sampling points z̃ and different resolutions. We provide further details of this procedure in the Appendix. Naturally, we can train this perturbation model in advance on (unlabeled) data from the input domain X , and then use it as a subroutine in our method. After this process is complete, we have N pairs of perturbed input-output pairs: {(x̃i, ỹi)}Ni=1 which exercise the mapping F around semantically similar inputs."
    }, {
      "heading" : "4.2 Causal model",
      "text" : "The second step consists of using the perturbed input-output pairs {(x̃i, ỹi)}Ni=1 to infer causal dependencies between the original input and output tokens. A naive approach would consider 2x2 contingency tables representing presence/absence of input/output tokens together with a test statistic for assessing their dependence. Instead, we incorporate all input tokens simultaneously to predict the occurrence of a single output token via logistic regression. The quality of these dependency estimators will depend on the frequency with which each input and output token occurs in the perturbations. Thus, we are interested in obtaining uncertainty estimates for these predictions, which can be naturally done with a Bayesian approach to logistic regression. Let φx(x̃) ∈ {0, 1}|x| be a binary vector encoding the presence of the original tokens\nx1, . . . , xn from x in the perturbed version x̃. For each target token yj ∈ y, we estimate a model:\nP (yj ∈ ỹ | x̃) = σ(θTj φx(x̃)) (1) where σ(z) = (1 + exp(−z))−1. We use a Gaussian approximation for the logarithm of the logistic function together with the prior p(θ) = N (θ0,H−10 ) (Murphy, 2012). Since in our case all tokens are guaranteed to occur at least once (we include the original example pair as part of the set), we use θ0 = α1,H0 = βI, with α, β > 0. Upon completion of this step, we have dependency coefficients between all original input and output tokens {θij}, along with their uncertainty estimates."
    }, {
      "heading" : "4.3 Explanation Selection",
      "text" : "The last step in our interpretability framework consists of selecting a set explanations for (x,y). The steps so far yield a dense bipartite graph between the input and output tokens. Unless |x| and |y| are small, this graph itself may not be sufficiently interpretable. We are interested in selecting relevant components of this dependency graph, i.e., partition the vertex set of G into disjoint subsets so as to minimize the weight of omitted edges (i.e. the k-cut value of the partition).\nGraph partitioning is a well studied NPcomplete problem (Garey et al., 1976). The usual setting assumes deterministic edge weights, but in our case we are interested in incorporating the uncertainty of the dependency estimates—resulting from their finite sample estimation—into the partitioning problem. For this, we rely on the approach of Fan et al. (2012) designed for interval estimates of edge weights. At a high level, this is a robust optimization formulation which seeks to minimize worst case cut values, and can be cast as a Mixed Integer Programming (MIP) problem. Specifically, for a bipartite graph G = (U, V,E)\nAlgorithm 1 Structured-output causal rationalizer 1: procedure SOCRAT(x,y, F ) 2: (µ,σ)← ENCODE(x) 3: for i = 1 to N do 4: z̃i ← SAMPLE(µ,σ)  Perturbation Model.5: x̃i ← DECODE(z̃i)\n6: ỹi ← F (x̃i) 7: end for 8: G ← CAUSAL(x,y, {x̃i, ỹi}Ni=1) 9: Ex 7→y ← BIPARTITION(G)\n10: Ex 7→y ← SORT(Ex 7→y) . By cut capacity 11: return Ex7→y 12: end procedure\nwith edge weights given as uncertainty intervals θij ± θ̂ij , the partitioning problem is given by\nmin (xuik,x v jk,yij)∈Y n∑ i=1 m∑ j=1 θijyij+\nmax S:S⊆V,|S|≤Γ (it,jt)∈V \\S ∑ (i,j)∈S θ̂ijyij + (Γ− bΓc)θ̂it,jtyit,jt\n(2)\nwhere xuik, x v jk are binary variables indicating subset belonging for elements of U and V respectively, yij are binary auxiliary variables indicating whether i and j are in different partitions, and Y is a set of constraints that ensure the K-partition is valid. Γ is a parameter in [0, |V |] which adjusts the robustness of the partition (the number of deviations from the mean edge values). See the Appendix for further explanation of this objective.\nIf |x| and |y| are small, the number of clusters K will also be small, so we can simply return all the partitions (i.e. the explanation chunks) Ekx→y := (V k x ∪ V ky ). However, when K is large, one might wish to entertain only the κ most relevant explanations. The graph partitioning framework provides us with a natural way to score the importance of each chunk. Intuitively, subgraphs that have few high-valued edges connecting them to other parts of the graph (i.e. low cut-capacity) can be thought of as self-contained explanations, and thus more relevant for interpretability. We can therefore define the importance score an atom as:\nimportance(Ekx→y) := − ∑\n(i,j)∈Xk\nθij (3)\nwhere Xk is the cut-set implied by Ekx→y:\nXk = {(i, j) ∈ E | i ∈ Ekx→y, j ∈ V \\ Ekx→y} The full interpretability method is succinctly expressed in Algorithm 1."
    }, {
      "heading" : "5 Experimental Framework",
      "text" : ""
    }, {
      "heading" : "5.1 Training and optimization",
      "text" : "For the experiments involving sentence inputs, we train in advance the VAE described in Section 4.1. We use symmetric encoder-decoders consisting of recurrent neural networks with an intermediate variational layer. In our case, however, we use L stacked RNN’s on both sides, and a stacked variational layer. Training variational autoencoders for text is notoriously hard. In addition to dropout and KLD annealing (Bowman et al., 2016), we found that slowly scaling the variance sampled from the normal distribution from 0 to 1 made training much more stable.\nFor the partitioning step we compare the robust formulation described above with two classical approaches to bipartite graph partitioning which do not take uncertainty into account: the coclustering method of Dhillon (2001) and the biclustering method of Kluger et al. (2003). For these two, we use off-the-shelf implementations,1 while we solve the MIP problem version of (2) with the optimization library gurobi.2"
    }, {
      "heading" : "5.2 Recovering simple mappings",
      "text" : "Before using our interpretability framework in real tasks where quantitative evaluation of explanations is challenging, we test it in a simplified setting where the “black-box” is simple and fully known. A reasonable minimum expectation on our method is that it should be able to infer many of these simple dependencies. For this purpose, we use the CMU Dictionary of word pronunciations,3 which is based on the ARPAbet symbol set and consists of about 130K word-to-phoneme pairs. Phonemes are expressed as tokens of 1 to 3 characters. An example entry in this dictionary is the pair vowels 7→ V AW1 AH0 L Z. Though the mapping is simple, it is not one-toone (a group of characters can correspond to a single phoneme) nor deterministic (the same character can map to different phonemes depending on the context). Thus, it provides a reasonable testbed\n1http://scikit-learn.org/stable/modules/biclustering.html 2http://www.gurobi.com/ 3www.speech.cs.cmu.edu/cgi-bin/cmudict\nfor our method. The setting is as follows: given an input-output pair from the cmudict “black-box”, we use our method to infer dependencies between characters in the input and phonemes in the output. Since locality in this context is morphological instead of semantic, we produce perturbations selecting n words randomly from the intersection of the cmudict vocabulary and the set of words with edit distance at most 2 from the original word.\nTo evaluate the inferred dependencies, we randomly selected 100 key-value pairs from the dictionary and manually labeled them with characterto-phoneme alignments. Even though our framework is not geared to produce pairwise alignments, it should nevertheless be able to recover them to a certain extent. To provide a point of reference, we compare against a (strong) baseline that is tailored to such a task: a state-of-theart unsupervised word alignment method based on Monte Carlo inference (Tiedemann and Östling, 2016). The results in Figure 2 show that the version of our method that uses the uncertainty clustering performs remarkably close to the alignment system, with an alignment error rate only ten points above an oracle version of this system that was trained on the full arpabet dictionary (dashed line). The raw and partitioned explanations provided by our method for an example input-output pair are shown in Table 1, where the edge widths correspond to the estimated strength of dependency. Throughout this work we display the nodes in the same lexical order of the inputs/outputs to facilitate reading, even if that makes the explanation chunks less visibly discernible. Instead, we sometimes provide an additional (sorted) heatplot\nof dependency values to show these partitions."
    }, {
      "heading" : "5.3 Machine Translation",
      "text" : "In our second set of experiments we evaluate our explanation model in a relevant and popular sequence-to-sequence task: machine translation. As black-boxes, we use three different methods for translating English into German: (i) Azure’s Machine Translation system, (ii) a Neural MT model, and (iii) a human (native speaker of German). We provide details on all three systems in the Appendix. We translate the same English sentences with all three methods, and explain their predictions using SOCRAT. To be able to generate sentences with similar language and structure as those used to train the two automatic systems, we use the monolingual English side of the WMT14 dataset to train the variational autoencoder described in Section 4.1. For every explanation instance, we sample S = 100 perturbations and use the blackboxes to translate them. In all cases, we use the same default SOCRAT configurations, including the robust partitioning method.\nIn Figure 3, we show the explanations provided by our method for the predictions of each of the three systems on the input sentence “Students said they looked forward to his class”. Although the three black-boxes all provided different translations, the explanations show a mostly consistent clustering around the two phrases in the sentence, and in all three cases the cluster with the highest cut value (i.e. the most relevant explanative chunk) is the one containing the subject. Interestingly, the\ndependency coefficients are overall higher for the human than for the other systems, suggesting more coherence in the translations (potentially because the human translated sentences in context, while the two automatic systems carry over no information from one example to the next).\nThe NMT system, as opposed to the other two, is not truly a black-box. We can open the box to get a glimpse on the true dependencies on the inputs used by the system at prediction time (the attention weights) and compare them to the explanation graph. The attention matrix, however, is dense and not normalized over target tokens, so it is not directly comparable to our dependency scores. Nevertheless, we can partition it with the coclustering method described in Section 4.3 to enforce group structure and make it easier to compare. Figure 4 shows the attention matrix and the explanation for an example sentence of the test set. Their overall cluster structure agrees, though our method shows conservatism with respect to the dependencies of the function words (to, for). Interestingly, our method is able to figure out that the <unk> token was likely produced by the word “appeals”, as shown by the explanation graph.\nIt must be emphasized that although we dis-\nplay attention scores in various experiments in this work, we do so only for qualitative evaluation purposes. Our model-agnostic framework can be used on top of models that do not use attention mechanisms or for which this information is hard to extract. Even in cases where it is available, the explanation provided by SOCRAT might be complementary or even preferable to attention scores because: (a) being normalized on both directions (as opposed to only over source tokens) and partitioned, it is often more interpretable than a dense attention matrix, and (b) it can be retrieved chunkby-chunk in decreasing order of relevance, which is especially important when explaining large inputs and/or outputs."
    }, {
      "heading" : "5.4 A (mediocre) dialogue system",
      "text" : "So far we have used our method to explain (mostly) correct predictions of meaningful models. But we can use it to gain insights into the workings of flawed black-box systems too. To test this, we train a simple dialogue system on the OpenSubtitle corpus (Tiedemann, 2009), consisting of ∼14M two-step movie dialogues. As before, we use a sequence-to-sequence model with attention, but now we constrain the quality of the model, using only two layers, hidden state dimension of 1000 and no hyper-parameter tuning.\nInput Prediction\nWhat do you mean it doesn’t matter? I don’t know Perhaps have we met before? I don’t think so Can I get you two a cocktail? No, thanks.\nTable 2: “Good” dialogue system predictions.\ndon't\nit\nknow.\nWhat do\nI\ndoesn't matter?meanyou\nI d o n ' t k n o w .\n?\nmatter\nt\n'\ndoesn\nit\nmean\nyou\ndo\nWhat\n0.15\n0.30\n0.45\n0.60\n0.75\nFigure 5: Explanation with S = 50 (left) and attention (right) for the first prediction in Table 2.\nAlthough most of the predictions of this model are short and repetitive (Yes/No/<unk> answers), some of them are seemingly meaningful, and might—if observed in isolation—lead one to believe the system is much better than it actually is. For example, the predictions in Table 2 suggest a complex use of the input to generate the output. To better understand this model, we rationalize its predictions using SOCRAT. The explanation graph for one such “good” prediction, shown in Figure 5, suggests that there is little influence of anything except the tokens What and you on the output. Thus, our method suggests that this model is using only partial information of the input and has probably memorized the connection between question words and responses. This is confirmed upon inspecting the model’s attention scores for this prediction (same figure, right pane)."
    }, {
      "heading" : "5.5 Bias detection in parallel corpora",
      "text" : "Natural language processing methods that derive semantics from large corpora have been shown to incorporate biases present in the data, such as archaic stereotypes of male/female occupations (Caliskan et al., 2017) and sexist adjective associations (Bolukbasi et al., 2016). Thus, there is interest in methods that can detect and address those biases. For our last set of experiments, we use our approach to diagnose and explain biased translations of MT systems, first on a simplistic but verifiable synthetic setting, where we inject\nT u p e u x p e n s e r q u i l e s t b o n q u e t u < u n k >\ngood\nis\nthis\nthink\nmight\nyou\n,\nHowever\nV o u s p o u r r i e z p e n s e r q u e c e l a e s t b o n n e\ngood\nis\nthis\nthink\nmight\nyou\n0.15\n0.30\n0.45\n0.60\nFigure 7: Attention scores on similar sentences by the biased translator.\na pre-specified spurious association into an otherwise normal parallel training corpus, and then on an industrial-quality black-box system.\nWe simulate a biased corpus as follows. Starting from the WMT14 English-French dataset, we identify French sentences written in the informal register (e.g. containing the singular second person tu) and prepend their English translation with the word However. We obtain about 6K examples this way, after which we add an additional 1M examples that do not contain the word however on the English side. The purpose of this is to attempt to induce a (false) association between this adverb and the informal register in French. We then train a sequence-to-sequence model on this polluted data, and we use it to translate adversariallychosen sentences containing the contaminating token. For example, given the input sentence “However, you might think this is good”, the method predicts the translation “Tu peux penser qu ’ il est bon que tu <unk>”, which, albeit far from perfect, seems reasonable. However, using SOCRAT to explain this prediction (cf. Figure 6) raises a red flag: there is an inexplicable strong dependency between the function word however and tokens in the output associated with the informal register (tu, peux), and a lack of dependency between the second tu and the source-side pronoun you. The model’s attention for this prediction (shown in Figure 7, left) confirms that it has picked up this spurious association. Indeed, translating the English sentence now without the prepended adverb\nresults in a switch to the formal register, as shown in the second plot in Figure 7.\nAlthough somewhat contrived, this synthetic setting works as a litmus test to show that our method is able to detect known artificial biases from a model’s predictions. We now move to a real setting, where we investigate biases in the predictions of an industrial-quality translation system. We use Azure’s MT service to translate into French various simple sentences that lack gender specification in English, but which require genderdeclined words in the output. We choose sentences containing occupations and adjectives previously shown to exhibit gender biases in linguistic corpora (Bolukbasi et al., 2016). After observing the choice of gender in the translation, we use SOCRAT to explain the output.\nIn line with previous results, we observe that this translation model exhibits a concerning preference for the masculine grammatical gender in sentences containing occupations such as doctor, professor or adjectives such as smart, talented, while choosing the feminine gender for charming, compassionate subjects who are dancers or nurses. The explanation graphs for two such examples, shown in Figure 8 (left and center), suggest strong associations between the genderneutral but stereotype-prone source tokens (nurse, doctor, charming) and the gender-carrying target tokens (i.e. the feminine-declined cette, danseuse, charmante in the first sentence and the masculine ce, médecin, talenteux in the second). While it is not unusual to observe interactions between multiple source and target tokens, the strength of dependence in some of these pairs (charming→danseuse, doctor→ce) is unexplained from a grammatical point of view. For comparison, the third example—a sentence in the plural form that\ndoes not involve choice of grammatical gender in French—shows comparatively much weaker associations across words in different parts of the sentence."
    }, {
      "heading" : "6 Discussion",
      "text" : "Our model-agnostic framework for prediction interpretability with structured data can produce reasonable, coherent, and often insightful explanations. The results on the machine translation task demonstrate how such a method yields a partial view into the inner workings of a black-box system. Lastly, the results of the last two experiments also suggest potential for improving existing systems, by questioning seemingly correct predictions and explaining those that are not.\nThe method admits several possible modifications. Although we focused on sequence-tosequence tasks, SOCRAT generalizes to other settings where inputs and outputs can be expressed as sets of features. An interesting application would be to infer dependencies between textual and image features in image-to-text prediction (e.g. image captioning). Also, we used a VAE-based sampling for object perturbations but other approaches are possible depending on the nature of the domain or data."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank the anonymous reviewers for their helpful suggestions regarding presentation and additional experiments, and Dr. Chantal Melis for valuable feedback. DAM gratefully acknowledges support from a CONACYT fellowship and the MIT-QCRI collaboration."
    }, {
      "heading" : "A Formulation of graph partitioning with uncertainty",
      "text" : "The bipartite version of the graph partitioning problem with edge uncertainty considered by Fan et al. (2012) has the following form. Assume we want to partition U and V into K subsets each, say {Ui} and {Vj}, with each Ui having cardinality in [cumin, c u max] and each Vj in [c v min, c v max]. Let xuik be the binary indicator of ui ∈ Uk, and analogously for xvjk and vj . In addition, we let yij be a binary variable which takes value 1 when ui, vj are in different corresponding subsets (i.e. ui ∈ Uk, vj ∈ Vk′ and k 6= k′). We can express the constraints of the problem as:\nY =  K∑ k=1 xvik = 1 ∀i K∑ k=1 xujk = 1 ∀j cumin ≤ N∑ i=1 xvik ≤ cumax ∀i cvmin ≤ N∑ i=1 xujk ≤ cvmax ∀j − yij − xvik + xujk ≤ 0 ∀i, j, k − yij + xvik − xujk ≤ 0 ∀i, j, k xvik, x u jk, yij ∈ {0, 1}, ∀i, j, k (4) (5) (6) (7) (8) (9) (10)\nConstraints (4) and (5) enforce the fact that each si and tj can belong to only one subset, (6) and (7) limit the size of the Uk and Bk to the specified ranges. On the other hand, (8) and (9) encode the definition of yij : if yij = 0 then xuik = x v jk for every k. A deterministic version of the bipartite graph partitioning problem which ignores edge uncertainty can be formulated as:\nmin (xuik,x v ik,yij)∈Y N∑ i=1 M∑ j=1 wijyij (11)\nThe robust version of this problem proposed by Fan et al. (2012) incorporates edge uncertainty by adding the following term to the objective:\nmax S:S⊆V,|S|≤Γ (it,jt)∈J\\S ∑ (i,j)∈S âijyij + (Γ− bΓc)ŵit,jtyit,jt\n(12)\nwhere Γ is a parameter in [0, |V |] which adjusts the robustness of the partition against the conservatism of the solution. This term essentially computes the maximal variance of a single cut (S, V \\ S) of size |Γ|. Thus, larger values of this parameter put more on the edge variance, at the cost of a more complex optimization problem. As shown by Fan et al. (2012) the objective can be brought back to a linear form by dualizing the term (12), resulting in the following formulation\nmin M∑ i=1 M∑ j=1 wijyij + Γp0 + ∑ (i,j)∈J pij\ns.t. p0 + pij − âijyij ≥ 0, (i, j) ∈ J pij ≥ 0, (i, j) ∈ J p0 ≥ 0\n(xuik, y v jk, yij) ∈ Y,\n(13)\nThis is a mixed integer programming (MIP) problem, which can be solved with specialized packages, such as GUROBI."
    }, {
      "heading" : "B Details on optimization and training",
      "text" : "Solving the mixed integer programming problem (13) to optimality can be prohibitive for large graphs. Since we are not interested in the exact value of the partition cost, we can settle for an approximate solution by relaxing the optimality gap tolerance. We observed that relaxing the absolute gap tolerance from the Gurobi default of 10−12 to 10−4 resulted in little minimal change in the solutions and a decrease in solve time of orders of magnitude. We added a run-time limit of 2 minutes for the optimization, though in all our experiments when never observed this limit being reached."
    }, {
      "heading" : "C Details on the variational autoencoder",
      "text" : "For all experiments in Sections 5.3 through 5.5 we use the same variational autoencoder: a network with three layer-GRU encoder and decoder and a stacked three layer variational autoencoder connecting the last hidden state of the encoder and the first hidden state of the decoder. We use a dimension 500 for the hidden states of the GRUs and\n400 for the latent states z. We train it on a 10M sentence subset of the English side of the WMT14 translation task, with KLD and variance annealing, as described in the main text. We train for one full epoch with no KLD penalty and no noise term (i.e. decoding directly from the mean vectormu), and start variance annealing on the second epoch and KLD annealing on the 8th epoch. We train for 50 epochs, freezing the KLD annealing when the validation set perplexity deteriorates by more than a pre-specified threshold.\nAlgorithm 2 Variational autoencoder perturbation model for sequence-to-sequence prediction\n1: procedure PERTURB(x) 2: (µ,σ) = ENCODE(x) 3: for i = 1 to N do 4: z̃i ∼ N (µ, diag(ασ)) 5: x̃i ← DECODE(z̃i) 6: end for 7: return {(x̃i)}Ni=1 8: end procedure\nOnce trained, the variational autoencoder is used as a subroutine of SOCRAT to generate perturbations as described in Algorithm 2. Given an input sentence x, we use the encoder to obtain approximate posterior parameters (µ,σ), and then repeatedly sample latent representations from the a gaussian distribution with these parameters. The scaling parameter α constrains the locality of the space from which examples are drawn, by scaling the variance of the encoded representation’s approximate posterior distribution. Larger values of α encourage samples to deviate further away from the mean encoding of the input µ, and thus more likely to result in diverse samples, at the cost of potentially less semantic coherence with the original input x. In Table 3 we show example sentences\ngenerated by this perturbation model on two input sentences from the WMT14 dataset with increasing scaling value α."
    }, {
      "heading" : "D Black-box system specifications",
      "text" : "The three systems used in the machine translation task in Section 5.3 are described below.\nAzure’s MT Service Via REST API calls to Microsoft’s Translator Text service provided as part of Azure’s cloud services.\nNeural MT System A sequence-to-sequence model with attention trained with the OpenNMT library (Klein et al., 2017) on the WMT15 English-German translation task dataset. A pretrained model was obtained from http://www. opennmt.net/Models/. It has two layers, hidden state dimension 500 and was trained for 13 epochs.\nA human A native German speaker, fluent in English, was given the perturbed English sentences and asked to translate them to German in one go. No additional instructions or context were provided, except that in cases where the source sentence is not directly translatable as is, it should be translated word-to-word to the extent possible. The human’s German and English language models were trained for 28 and 16 years, respectively."
    } ],
    "references" : [ {
      "title" : "On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation",
      "author" : [ "Sebastian Bach", "Alexander Binder", "Grégoire Montavon", "Frederick Klauschen", "Klaus-Robert Müller", "Wojciech Samek." ],
      "venue" : "PLoS One,",
      "citeRegEx" : "Bach et al\\.,? 2015",
      "shortCiteRegEx" : "Bach et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural Machine Translation By Jointly Learning To Align and Translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "Iclr 2015, pages 1–15.",
      "citeRegEx" : "Bahdanau et al\\.,? 2014",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings",
      "author" : [ "Tolga Bolukbasi", "Kai-Wei Chang", "James Zou", "Venkatesh Saligrama", "Adam Kalai." ],
      "venue" : "NIPS, (Nips):4349—-4357.",
      "citeRegEx" : "Bolukbasi et al\\.,? 2016",
      "shortCiteRegEx" : "Bolukbasi et al\\.",
      "year" : 2016
    }, {
      "title" : "Generating Sentences from a Continuous Space",
      "author" : [ "Samuel R. Bowman", "Luke Vilnis", "Oriol Vinyals", "Andrew M. Dai", "Rafal Jozefowicz", "Samy Bengio." ],
      "venue" : "Iclr, pages 1–13.",
      "citeRegEx" : "Bowman et al\\.,? 2016",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2016
    }, {
      "title" : "Semantics derived automatically from language corpora contain human-like biases",
      "author" : [ "Aylin Caliskan", "Joanna J Bryson", "Arvind Narayanan." ],
      "venue" : "Science (80-. )., 356(6334):183–186.",
      "citeRegEx" : "Caliskan et al\\.,? 2017",
      "shortCiteRegEx" : "Caliskan et al\\.",
      "year" : 2017
    }, {
      "title" : "Intelligible Models for HealthCare : Predicting Pneumonia Risk and Hospital 30-day Readmission",
      "author" : [ "Rich Caruana", "Yin Lou", "Johannes Gehrke", "Paul Koch", "Marc Sturm", "Noemie Elhadad." ],
      "venue" : "Proc. 21th ACM SIGKDD Int. Conf. Knowl. Discov. Data Min.",
      "citeRegEx" : "Caruana et al\\.,? 2015",
      "shortCiteRegEx" : "Caruana et al\\.",
      "year" : 2015
    }, {
      "title" : "Listen, attend and spell",
      "author" : [ "William Chan", "Navdeep Jaitly", "Quoc V. Le", "Oriol Vinyals." ],
      "venue" : "arXiv Prepr., pages 1–16.",
      "citeRegEx" : "Chan et al\\.,? 2015",
      "shortCiteRegEx" : "Chan et al\\.",
      "year" : 2015
    }, {
      "title" : "Co-clustering documents and words using Bipartite spectral graph partitioning",
      "author" : [ "Inderjit s. Dhillon." ],
      "venue" : "Proc 7th ACM SIGKDD Conf, pages 269–274.",
      "citeRegEx" : "Dhillon.,? 2001",
      "shortCiteRegEx" : "Dhillon.",
      "year" : 2001
    }, {
      "title" : "A Roadmap for a Rigorous Science of Interpretability",
      "author" : [ "Finale Doshi-Velez", "Been Kim." ],
      "venue" : "ArXiv eprints, (Ml):1–12.",
      "citeRegEx" : "Doshi.Velez and Kim.,? 2017",
      "shortCiteRegEx" : "Doshi.Velez and Kim.",
      "year" : 2017
    }, {
      "title" : "Robust optimization of graph partitioning involving interval uncertainty",
      "author" : [ "Neng Fan", "Qipeng P. Zheng", "Panos M. Pardalos." ],
      "venue" : "Theor. Comput. Sci., volume 447, pages 53–61.",
      "citeRegEx" : "Fan et al\\.,? 2012",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2012
    }, {
      "title" : "Some simplified NP-complete graph problems",
      "author" : [ "M.R. Garey", "D.S. Johnson", "L. Stockmeyer." ],
      "venue" : "Theor. Comput. Sci., 1(3):237–267.",
      "citeRegEx" : "Garey et al\\.,? 1976",
      "shortCiteRegEx" : "Garey et al\\.",
      "year" : 1976
    }, {
      "title" : "AutoEncoding Variational Bayes",
      "author" : [ "Diederik P Kingma", "Max Welling." ],
      "venue" : "Iclr, (Ml):1–14.",
      "citeRegEx" : "Kingma and Welling.,? 2014",
      "shortCiteRegEx" : "Kingma and Welling.",
      "year" : 2014
    }, {
      "title" : "OpenNMT: Open-Source Toolkit for Neural Machine Translation",
      "author" : [ "G. Klein", "Y. Kim", "Y. Deng", "J. Senellert", "A.M. Rush." ],
      "venue" : "ArXiv e-prints.",
      "citeRegEx" : "Klein et al\\.,? 2017",
      "shortCiteRegEx" : "Klein et al\\.",
      "year" : 2017
    }, {
      "title" : "Spectral biclustering of microarray data: Coclustering genes and conditions",
      "author" : [ "Yuval Kluger", "Ronen Basri", "Joseph T. Chang", "Mark Gerstein" ],
      "venue" : null,
      "citeRegEx" : "Kluger et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Kluger et al\\.",
      "year" : 2003
    }, {
      "title" : "Rationalizing Neural Predictions",
      "author" : [ "Tao Lei", "Regina Barzilay", "Tommi Jaakkola." ],
      "venue" : "EMNLP 2016, Proc. 2016 Conf. Empir. Methods Nat. Lang. Process., pages 107–117.",
      "citeRegEx" : "Lei et al\\.,? 2016",
      "shortCiteRegEx" : "Lei et al\\.",
      "year" : 2016
    }, {
      "title" : "The Mythos of Model Interpretability",
      "author" : [ "Zachary C Lipton." ],
      "venue" : "ICML Work. Hum. Interpret. Mach. Learn., (Whi).",
      "citeRegEx" : "Lipton.,? 2016",
      "shortCiteRegEx" : "Lipton.",
      "year" : 2016
    }, {
      "title" : "Machine Learning: A Probabilistic Perspective",
      "author" : [ "Kevin P. Murphy" ],
      "venue" : null,
      "citeRegEx" : "Murphy.,? \\Q2012\\E",
      "shortCiteRegEx" : "Murphy.",
      "year" : 2012
    }, {
      "title" : "Stochastic backpropagation and approximate inference in deep generative models",
      "author" : [ "D J Rezende", "S Mohamed", "D Wierstra." ],
      "venue" : "Proc. 31st . . . , 32:1278–1286.",
      "citeRegEx" : "Rezende et al\\.,? 2014",
      "shortCiteRegEx" : "Rezende et al\\.",
      "year" : 2014
    }, {
      "title" : "Variational Inference with Normalizing Flows",
      "author" : [ "Danilo Jimenez Rezende", "Shakir Mohamed." ],
      "venue" : "Proc. 32nd Int. Conf. Mach. Learn., 37:1530–1538.",
      "citeRegEx" : "Rezende and Mohamed.,? 2015",
      "shortCiteRegEx" : "Rezende and Mohamed.",
      "year" : 2015
    }, {
      "title" : "Why Should I Trust You?\": Explaining the Predictions of Any Classifier",
      "author" : [ "Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin." ],
      "venue" : "Proc. 22Nd ACM SIGKDD Int. Conf. Knowl. Discov. Data Min., KDD ’16, pages 1135–1144, New York, NY,",
      "citeRegEx" : "Ribeiro et al\\.,? 2016",
      "shortCiteRegEx" : "Ribeiro et al\\.",
      "year" : 2016
    }, {
      "title" : "A Neural Attention Model for Abstractive Sentence Summarization",
      "author" : [ "Alexander M Rush", "Sumit Chopra", "Jason Weston." ],
      "venue" : "Proc. Conf. Empir. Methods Nat. Lang. Process., (September):379–389.",
      "citeRegEx" : "Rush et al\\.,? 2015",
      "shortCiteRegEx" : "Rush et al\\.",
      "year" : 2015
    }, {
      "title" : "Grad-CAM: Why did you say that? Visual Explanations from Deep Networks via Gradient-based Localization",
      "author" : [ "Ramprasaath R. Selvaraju", "Abhishek Das", "Ramakrishna Vedantam", "Michael Cogswell", "Devi Parikh", "Dhruv Batra." ],
      "venue" : "(Nips):1–5.",
      "citeRegEx" : "Selvaraju et al\\.,? 2016",
      "shortCiteRegEx" : "Selvaraju et al\\.",
      "year" : 2016
    }, {
      "title" : "Ladder Variational Autoencoders",
      "author" : [ "Casper Kaae Sønderby", "Tapani Raiko", "Lars Maaløe", "Søren Kaae Sønderby", "Ole Winther." ],
      "venue" : "NIPS, (Nips).",
      "citeRegEx" : "Sønderby et al\\.,? 2016",
      "shortCiteRegEx" : "Sønderby et al\\.",
      "year" : 2016
    }, {
      "title" : "News from OPUS - A Collection of Multilingual Parallel Corpora with Tools and Interfaces",
      "author" : [ "Jörg Tiedemann." ],
      "venue" : "N. Nicolov, G. Bontcheva, G. Angelova, and R. Mitkov, editors, Recent Adv. Nat. Lang. Process., pages 237—-248. John Benjamins,",
      "citeRegEx" : "Tiedemann.,? 2009",
      "shortCiteRegEx" : "Tiedemann.",
      "year" : 2009
    }, {
      "title" : "Efficient Word Alignment with Markov Chain Monte Carlo",
      "author" : [ "Jörg Tiedemann", "Robert Östling." ],
      "venue" : "Prague Bull. Math. Linguist., (106):125–146.",
      "citeRegEx" : "Tiedemann and Östling.,? 2016",
      "shortCiteRegEx" : "Tiedemann and Östling.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "While such systems achieve state-of-the-art results in MT (Bahdanau et al., 2014), summarization (Rush et al.",
      "startOffset" : 58,
      "endOffset" : 81
    }, {
      "referenceID" : 20,
      "context" : ", 2014), summarization (Rush et al., 2015) and speech recognition (Chan et al.",
      "startOffset" : 23,
      "endOffset" : 42
    }, {
      "referenceID" : 6,
      "context" : ", 2015) and speech recognition (Chan et al., 2015), they remain largely uninterpretable, although attention mechanisms (Bahdanau et al.",
      "startOffset" : 31,
      "endOffset" : 50
    }, {
      "referenceID" : 1,
      "context" : ", 2015), they remain largely uninterpretable, although attention mechanisms (Bahdanau et al., 2014) can shed some light on how they operate.",
      "startOffset" : 76,
      "endOffset" : 99
    }, {
      "referenceID" : 19,
      "context" : "A slightly less obvious application concerns model improvement (Ribeiro et al., 2016) where interpretability can be used to detect biases in the methods.",
      "startOffset" : 63,
      "endOffset" : 85
    }, {
      "referenceID" : 14,
      "context" : "(Lei et al., 2016)).",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 19,
      "context" : "Following (Ribeiro et al., 2016), we turn the local behavior of the model around the given input into an interpretable representation of its operation.",
      "startOffset" : 10,
      "endOffset" : 32
    }, {
      "referenceID" : 15,
      "context" : "This term, however, is underdetermined, so the goals, methods and formalisms of these approaches are often non-overlapping (Lipton, 2016).",
      "startOffset" : 123,
      "endOffset" : 137
    }, {
      "referenceID" : 5,
      "context" : "In the context of machine learning, perhaps the most visible line of work on interpretability focuses on medical applications (Caruana et al., 2015), where trust can be a decisive factor on whether a model is used or not.",
      "startOffset" : 126,
      "endOffset" : 148
    }, {
      "referenceID" : 21,
      "context" : "With the ever-growing success and popularity of deep learning methods for image processing, recent work has addressed interpretability in this setting, usually requiring access to the method’s activations and gradients (Selvaraju et al., 2016), or directly modeling how influence propagates (Bach et al.",
      "startOffset" : 219,
      "endOffset" : 243
    }, {
      "referenceID" : 0,
      "context" : ", 2016), or directly modeling how influence propagates (Bach et al., 2015).",
      "startOffset" : 55,
      "endOffset" : 74
    }, {
      "referenceID" : 0,
      "context" : ", 2016), or directly modeling how influence propagates (Bach et al., 2015). For a broad overview of interpretability in machine learning, we refer the reader to the recent survey by Doshi-Velez and Kim (2017).",
      "startOffset" : 56,
      "endOffset" : 209
    }, {
      "referenceID" : 14,
      "context" : "Most similar to this work are the approaches of Lei et al. (2016) and Ribeiro et al.",
      "startOffset" : 48,
      "endOffset" : 66
    }, {
      "referenceID" : 14,
      "context" : "Most similar to this work are the approaches of Lei et al. (2016) and Ribeiro et al. (2016). The former proposes a model that justifies its predictions in terms of fragments of the input.",
      "startOffset" : 48,
      "endOffset" : 92
    }, {
      "referenceID" : 14,
      "context" : "Most similar to this work are the approaches of Lei et al. (2016) and Ribeiro et al. (2016). The former proposes a model that justifies its predictions in terms of fragments of the input. This approach formulates explanation generation as part of the learning problem, and, as most previous work, only deals with the case where predictions are scalar or categorical. On the other hand, Ribeiro et al. (2016) propose a framework for explaining the predictions of black-box classifiers by means of locally-faithful interpretable models.",
      "startOffset" : 48,
      "endOffset" : 408
    }, {
      "referenceID" : 19,
      "context" : "Second, while our approach shares the locality and model-agnostic view of Ribeiro et al. (2016), generating perturbed versions of structured objects is a challenging task by itself.",
      "startOffset" : 74,
      "endOffset" : 96
    }, {
      "referenceID" : 8,
      "context" : "A good explanation would ideally also decompose into cognitive chunks (Doshi-Velez and Kim, 2017): basic units of explanation which are a priori bounded in size.",
      "startOffset" : 70,
      "endOffset" : 97
    }, {
      "referenceID" : 19,
      "context" : "Following (Ribeiro et al., 2016), we seek explanations via interpretable representations that are both i) locally faithful, in the sense that they approximate how the model behaves in the vicinity of x, and ii) model agnostic, that is, that do not require any knowledge ofF .",
      "startOffset" : 10,
      "endOffset" : 32
    }, {
      "referenceID" : 11,
      "context" : "We propose to do this using a variational autoencoder (VAE) (Kingma and Welling, 2014; Rezende et al., 2014).",
      "startOffset" : 60,
      "endOffset" : 108
    }, {
      "referenceID" : 17,
      "context" : "We propose to do this using a variational autoencoder (VAE) (Kingma and Welling, 2014; Rezende et al., 2014).",
      "startOffset" : 60,
      "endOffset" : 108
    }, {
      "referenceID" : 18,
      "context" : "VAEs have been successfully used with fixed dimensional inputs such as images (Rezende and Mohamed, 2015; Sønderby et al., 2016) and recently also adapted to generating sentences from continuous representations (Bowman et al.",
      "startOffset" : 78,
      "endOffset" : 128
    }, {
      "referenceID" : 22,
      "context" : "VAEs have been successfully used with fixed dimensional inputs such as images (Rezende and Mohamed, 2015; Sønderby et al., 2016) and recently also adapted to generating sentences from continuous representations (Bowman et al.",
      "startOffset" : 78,
      "endOffset" : 128
    }, {
      "referenceID" : 3,
      "context" : ", 2016) and recently also adapted to generating sentences from continuous representations (Bowman et al., 2016).",
      "startOffset" : 90,
      "endOffset" : 111
    }, {
      "referenceID" : 16,
      "context" : "We use a Gaussian approximation for the logarithm of the logistic function together with the prior p(θ) = N (θ0,H 0 ) (Murphy, 2012).",
      "startOffset" : 118,
      "endOffset" : 132
    }, {
      "referenceID" : 10,
      "context" : "Graph partitioning is a well studied NPcomplete problem (Garey et al., 1976).",
      "startOffset" : 56,
      "endOffset" : 76
    }, {
      "referenceID" : 9,
      "context" : "For this, we rely on the approach of Fan et al. (2012) designed for interval estimates of edge weights.",
      "startOffset" : 37,
      "endOffset" : 55
    }, {
      "referenceID" : 3,
      "context" : "In addition to dropout and KLD annealing (Bowman et al., 2016), we found that slowly scaling the variance sampled from the normal distribution from 0 to 1 made training much more stable.",
      "startOffset" : 41,
      "endOffset" : 62
    }, {
      "referenceID" : 7,
      "context" : "For the partitioning step we compare the robust formulation described above with two classical approaches to bipartite graph partitioning which do not take uncertainty into account: the coclustering method of Dhillon (2001) and the biclustering method of Kluger et al.",
      "startOffset" : 209,
      "endOffset" : 224
    }, {
      "referenceID" : 7,
      "context" : "For the partitioning step we compare the robust formulation described above with two classical approaches to bipartite graph partitioning which do not take uncertainty into account: the coclustering method of Dhillon (2001) and the biclustering method of Kluger et al. (2003). For these two, we use off-the-shelf implementations,1 while we solve the MIP problem version of (2) with the optimization library gurobi.",
      "startOffset" : 209,
      "endOffset" : 276
    }, {
      "referenceID" : 24,
      "context" : "To provide a point of reference, we compare against a (strong) baseline that is tailored to such a task: a state-of-theart unsupervised word alignment method based on Monte Carlo inference (Tiedemann and Östling, 2016).",
      "startOffset" : 189,
      "endOffset" : 218
    }, {
      "referenceID" : 23,
      "context" : "To test this, we train a simple dialogue system on the OpenSubtitle corpus (Tiedemann, 2009), consisting of ∼14M two-step movie dialogues.",
      "startOffset" : 75,
      "endOffset" : 92
    }, {
      "referenceID" : 4,
      "context" : "Natural language processing methods that derive semantics from large corpora have been shown to incorporate biases present in the data, such as archaic stereotypes of male/female occupations (Caliskan et al., 2017) and sexist adjective associations (Bolukbasi et al.",
      "startOffset" : 191,
      "endOffset" : 214
    }, {
      "referenceID" : 2,
      "context" : ", 2017) and sexist adjective associations (Bolukbasi et al., 2016).",
      "startOffset" : 42,
      "endOffset" : 66
    }, {
      "referenceID" : 2,
      "context" : "We choose sentences containing occupations and adjectives previously shown to exhibit gender biases in linguistic corpora (Bolukbasi et al., 2016).",
      "startOffset" : 122,
      "endOffset" : 146
    } ],
    "year" : 2017,
    "abstractText" : "We interpret the predictions of any blackbox structured input-structured output model around a specific input-output pair. Our method returns an “explanation” consisting of groups of input-output tokens that are causally related. These dependencies are inferred by querying the black-box model with perturbed inputs, generating a graph over tokens from the responses, and solving a partitioning problem to select the most relevant components. We focus the general approach on sequence-tosequence problems, adopting a variational autoencoder to yield meaningful input perturbations. We test our method across several NLP sequence generation tasks.",
    "creator" : "LaTeX with hyperref package"
  }
}