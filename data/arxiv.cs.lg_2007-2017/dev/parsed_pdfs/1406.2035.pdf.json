{
  "name" : "1406.2035.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning Word Representations with Hierarchical Sparse Coding",
    "authors" : [ "Dani Yogatama", "Manaal Faruqui", "Chris Dyer", "Noah A. Smith" ],
    "emails" : [ "dyogatama@cs.cmu.edu", "mfaruqui@cs.cmu.edu", "cdyer@cs.cmu.edu", "nasmith@cs.cmu.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "When applying machine learning to text, the classic categorical representation of words as indices of a vocabulary fails to capture syntactic and semantic similarities that are easily discoverable in data (e.g., pretty, beautiful, and lovely have similar meanings, opposite to unattractive, ugly, and repulsive). In contrast, recent approaches to word representation learning apply neural networks to obtain dense, low-dimensional, continuous embeddings of words [18, 3, 7, 13, 15].\nIn this work, we propose an alternative approach based on decomposition of a high-dimensional matrix capturing surface statistics of association between a word and its “contexts” with sparse coding. As in past work, contexts are words that occur nearby in running text [26]. Learning is performed by minimizing a reconstruction loss function to find the best factorization of the input matrix.\nThe key novelty in our method is to govern the relationships among dimensions of the learned word vectors, introducing a hierarchical organization imposed through a structured penalty known as the group lasso [29]. The idea of regulating the order in which variables enter a model was first proposed by Zhao et al. [30], and it has since been shown useful for other applications [8]. Our approach is motivated by coarse-to-fine organization of words’ meanings often found in the field of lexical semantics (see §2.2 for a detailed description), which mirrors evidence for distributed nature of hierarchical concepts in the brain [21].\nOn standard evaluation tasks—word similarity ranking, analogies, sentence completion, and sentiment analysis—we find that our method outperforms or is competitive with the best published neural network representations."
    }, {
      "heading" : "2 Model",
      "text" : ""
    }, {
      "heading" : "2.1 Background and Notation",
      "text" : "The observable representation of word v is taken to be a vector xv ∈ RC of cooccurrence statistics with C different contexts. Most commonly, each context is a possible neighboring word within a fixed\nar X\niv :1\n40 6.\n20 35\nv1 [\ncs .C\nL ]\n8 J\nun 2\nwindow.1 Following many others, we let xv,c be the pointwise mutual information (PMI) between the occurrence of context word c within a five-word window of an occurrence of word v [26, 19, 4].\nIn sparse coding, the goal is to represent each input vector x ∈ RC as a sparse linear combination of basis vectors. Given a stacked input matrix X ∈ RC×V , where V is the number of words, we seek to minimize:\narg min D∈D,A\n‖X−DA‖22 + λΩ(A), (1)\nwhere D ∈ RC×M is the dictionary of basis vectors, D is the set of matrices with unit `2 columns, A ∈ RM×V is the code matrix, λ is a regularization hyperparameter, and Ω is the regularizer. Here, we use the squared loss for the reconstruction error, but other loss functions could also be used [11]. Note that it is not necessary, although typical, for M to be less than C (when M > C, it is often called an overcomplete representation). The most common regularizer is the `1 penalty, which results in sparse codes [11]. While structured regularizers are associated with sparsity as well (e.g., the group lasso encourages group sparsity), our motivation is to use Ω to encourage a coarse-to-fine organization of latent dimensions of the learned representations of words."
    }, {
      "heading" : "2.2 Structured Regularization for Word Representations",
      "text" : "For Ω(A), we design a forest-structured regularizer that encourages the model to use some dimensions in the code space before using other dimensions. Consider the trees in Figure 1. In this example, there are 13 variables in each tree, and 26 variables in total (i.e., M = 26), each corresponding to a latent dimension for one particular word. These trees describe the order in which variables “enter the model” (i.e., take nonzero values). In general, a node may take a nonzero value only if its ancestors also do. For example, nodes 3 and 4 may only be nonzero if nodes 1 and 2 are also nonzero. Our regularizer for column v of A, denoted by av (in this example, av ∈ R26), for the trees in Figure 1 is:\nΩ(av) =\n26∑\ni=1\n‖〈av,i, av,Descendants(i)〉‖2\nwhere Descendants(i) returns the (possibly empty) set of descendants of node i. Jenatton et al. [8] proposed a related penalty with only one tree for learning image and document representations.\nLet us analyze why organizing the code space this way is helpful in learning better word representations. Recall that the goal is to have a good dictionary D and code matrix A. We apply the structured penalty to each column of A. When we use the same structured penalty in these columns, we encode an additional shared constraint that the dimensions of av that correspond to top level nodes should focus on “general” contexts that are present in most words. In our case, this corresponds to contexts with extreme PMI values for most words, since they are the ones that incur the largest losses. As we go down the trees, more word-specific contexts can then be captured. As a result, we have better organization across words when learning their representations, which also translates to a more structured dictionary D. Contrast this with the case when we use unstructured regularizers that penalize each dimension of A independently (e.g., lasso). In this case, each dimension of av has more flexibility to pay attention to any contexts (the only constraint that we encode is that the cardinality of the model should be small). We hypothesize that this is less appropriate for learning word representations, since the model has excessive freedom when learning A on noisy PMI values, which translates to poor D.\nThe intuitive motivation for our regularizer comes from the field of lexical semantics, which often seeks to capture the relationships between words’ meanings in hierarchically-organized lexicons. The best-known example is WordNet [16]. Words with the same (or close) meanings are grouped together (e.g., professor and prof are synonyms), and fine-grained meaning groups (“synsets”) are nested under coarse-grained ones (e.g., professor is a hyponym of academic). Our hierarchical sparse coding approach is still several steps away from inducing such a lexicon, but it seeks to employ the dimensions of a distributed word representation scheme in a similar coarse-to-fine way.\n1Others include: global context [7], multilingual context [4], geographic context [1], brain activation data [5], and second-order context [22]."
    }, {
      "heading" : "2.3 Learning",
      "text" : "Learning is accomplished by minimizing the function in Eq. 1, with the group lasso regularization function described in §2.2. The function is not convex with respect to D and A, but it is convex with respect to each when the other is fixed. Alternating minimization routines have been shown to work reasonably well in practice for such problems [10], but they are too expensive here due to (i) the size of X ∈ RC×V (C and V are each on the order of 105) and (ii) the many overlapping groups in the structured regularizer Ω(A).\nOur solution is based on the online dictionary learning method of Mairal et al. [12]. For T iterations, we (i) sample a mini-batch of words and (in parallel) solve for each one’s a using the alternating directions method of multipliers, shown to work well for overlapping group lasso problems [20, 28];2 then (ii) update D using the block coordinate descent algorithm of Mairal et al. [12]. Finally, we parallelize solving for all columns of A, which are separable once D is fixed. See details in Algorithm 1.\nAlgorithm 1 Learning word representations with the forest regularizer. Input: input matrix X, regularization constant λ\nInitialize D0 randomly F0 = 0, G0 = 0 for t = 1, . . . , T do\nSample word indices Vt for v ∈ Vt do\nav = arg mina∈RM ‖xv −Dt−1a‖22 + λΩ(a) I ADMM, in parallel end for η = 1− |Vt|1+t|Vt| Ft = ηFt−1 + ∑ v∈Vt ava > v\nGt = ηGt−1 + ∑ v∈Vt xva > v Dt = arg minD∈D 1 t ( 1 2Tr(D > t−1Dt−1Ft)− Tr(D>t−1Gt) ) I block coordinate descent end for for v = 1, . . . , V do av = arg mina∈RM ‖xv −DTa‖22 + λΩ(a) I ADMM in parallel end for"
    }, {
      "heading" : "3 Experiments",
      "text" : "We present a controlled comparison of the forest regularizer against several strong baseline word representations learned on a fixed dataset, across several tasks. In §3.4 we compare to publicly available word vectors trained on different data.\n2Since our groups form tree structures, other methods such as FISTA [2] could also be used."
    }, {
      "heading" : "3.1 Setup and Baselines",
      "text" : "We use the WMT-2011 English news corpus as our training data.3 The corpus contains about 15 million sentences and 370 million words. The size of our vocabulary is 180,834.4\nIn our experiments, we use forests similar to those in Figure 1 to organize the latent word space. Note that the example has 26 nodes (2 trees). We choose to evaluate performance with M = 52 (4 trees) and M = 520 (40 trees).5 We denote the sparse coding method with regular `1 penalty by SC, and our method with structured regularization (§2.2) by FOREST. We set λ = 0.1 and use a mini-batch of size 20,000 in both cases.\nWe compare with the following baseline methods:\n• Turney and Pantel [26]: principal component analysis (PCA) by truncated singular value decomposition on X>. Note that this is also the same as minimizing the squared reconstruction loss in Eq. 1 without any penalty on A. • Mikolov et al. [13]: a recursive neural network (RNN) language model. We obtain an\nimplementation from http://rnnlm.org/. • Mnih and Teh [18]: a log bilinear model that predicts a word given its context, trained using\nnoise-contrastive estimation (NCE, [6]). We use our own implementation for this model. • Mikolov et al. [15]: a log bilinear model that predicts a word given its context (continuous\nbag of words, CBOW), trained using negative sampling [14]. We obtain an implementation from https://code.google.com/p/word2vec/. • Mikolov et al. [15]: a log bilinear model that predicts context words given a target word\n(skip gram, SG), trained using negative sampling [14]. We obtain an implementation from https://code.google.com/p/word2vec/.\nOur focus here is on comparisons of model architectures. For a fair comparison, we train all competing methods on the same corpus using a context window of five words (left and right). For the baseline methods, we use default settings in the provided implementations (or papers, when implementations are not available and we reimplement their methods). We also trained the last two baseline methods with hierarchical softmax using a binary Huffman tree instead of negative sampling; consistent with Mikolov et al. [14], we found that negative sampling performs better and relegate hierarchical softmax results to supplementary materials."
    }, {
      "heading" : "3.2 Evaluation",
      "text" : "We evaluate on the following benchmark tasks.\nWord similarity The first task evaluates how well the representations capture word similarity. For example beautiful and lovely should be closer in distance than beautiful and unattractive. We evaluate on a suite of word similarity datasets, subsets of which have been considered in past work (more details are given in supplementary materials). Following standard practices, for each competing model, we compute cosine distances between word pairs in word similarity datasets, then rank and report Spearman’s rank correlation coefficient [24] between the model’s rankings and human rankings.\nSyntactic and semantic analogies The second evaluation dataset is two analogy tasks proposed by Mikolov et al. [15]. These questions evaluate syntactic and semantic relations between words. There are 10,675 syntactic questions (e.g., walking : walked :: swimming : swam) and 8,869 semantic questions (e.g., Athens : Greece :: Oslo :: Norway). In each question, one word is missing, and the task is to correctly predict the missing word. We use the vector offset method [15] that computes the vector b = aAthens − aGreece + aOslo. We only consider a question to be answered correctly if the returned vector (b) has the highest cosine similarity to the correct answer (in this example, aNorway).\n3http://www.statmt.org/wmt11/ 4 We replace words with frequency less than 10 with #rare# and numbers with #number#. 5In preliminary experiments we explored binary tree structures and found they did not work as well; we leave\na more extensive exploration of structures to future work.\nSentence completion The third evaluation task is the Microsoft Research sentence completion challenge [31]. In this task, the goal it to choose from a set of five candidate words which one best completes a sentence. For example: Was she his {client, musings, discomfiture, choice, opportunity} , his friend , or his mistress?, (client is the correct answer). We choose the candidate with the highest average similarity to every other word in the sentence.6\nSentiment analysis The last evaluation task is sentence-level sentiment analysis. We use the movie reviews dataset from Socher et al. [23]. The dataset consists of 6,920 sentences for training, 872 sentences for development, and 1,821 sentences for testing. We train `2-regularized logistic regression to predict binary sentiment, tuning the regularization strength on development data. We represent each example (sentence) as an M dimensional vector constructed by taking the average of word representations of words appearing in that sentence.\nThe analogy, sentence completion, and sentiment analysis tasks are evaluated on prediction accuracy."
    }, {
      "heading" : "3.3 Results",
      "text" : "Table 1 shows results on all evaluation tasks for M = 52 and M = 520. In the similarity ranking and sentiment analysis tasks, our method performed the best in both low and high dimensional embeddings. In the sentence completion challenge, our method performed best in the high-dimensional case and second-best in the low-dimensional case. While it lags behind other methods on the analogies tasks, its second- or third-place ranking is competitive. Importantly, FOREST outperforms unstructured sparse coding (SC) on every task."
    }, {
      "heading" : "3.4 Other Comparisons",
      "text" : "In Table 2, we compare with four baseline methods for which pre-trained 50-dimensional word representations are available:\n• Collobert et al. [3]: a neural network language model trained on Wikipedia data for 2 months (CW).7\n• Huang et al. [7]: a neural network model that uses additional global document context (RNN-DC).8\n6We note that unlike matrix decomposition based approaches, some of the neural network based models can directly compute the scores of context words given a possible answer [15]. We choose to use average similarities for a fair comparison of the representations.\n7http://ronan.collobert.com/senna/ 8 http://www.socher.org/index.php/Main/ImprovingWordRepresentationsViaGlobalContextAndMultipleWordPrototypes/\nThese methods were all trained on different corpora, so they have different vocabularies that do not always include all of the words found in the tasks. We estimate performance on the items for which prediction is possible, and show the count for each method in Table 2. This comparison should be interpreted cautiously since many experimental variables are conflated; nonetheless, FOREST performs strongly."
    }, {
      "heading" : "3.5 Discussion",
      "text" : "Our method produces sparse word representations with exact zeros. We observe that the sparse coding method without a structured regularizer produces sparser representations, but it performs worse on our evaluation tasks, indicating that it zeroes out meaningful dimensions. For FOREST with M = 52 and M = 520, the average numbers of nonzero entries are 91% and 85% respectively. While our word representations are not extremely sparse, this makes intuitive sense since we try to represent about 180,000 contexts in only 52 (520) dimensions (we also did not tune λ). As we increase M , we get sparser representations.\nWe visualize our M = 52 word representations (FOREST) related to animals (10 words) and countries (10 words). We show the coefficient patterns for these words in Figure 2. We can see that in both cases, there are dimensions where the coefficient signs (positive or negative) agree for all 10 words (they are mostly on the right and left sides of the plots). Note that the dimensions where all the coefficients agree are not the same in animals and countries. The larger magnitude of the vectors for more abstract concepts (animal, animals, country, countries) is suggestive of neural imaging studies that have found evidence of more global activation patterns for processing superordinate terms [21].\nIn Figure 3, we show tree visualizations of coefficients of word representations for animal, horse, and elephant. We show one tree for M = 52 (there are four trees in total, but other trees exhibit similar patterns). Coefficients that differ in sign mostly correspond to leaf nodes, validating our motivation that top level nodes should focus more on “general” contexts (for which they should be roughly similar for animal, horse, and elephant) and leaf nodes focus on word-specific contexts. One of the leaf nodes for animal is driven to zero, suggesting that more abstract concepts require fewer dimensions to explain.\nFor FOREST and SG with M = 520, we project the learned word representations into two dimensions using the t-SNE tool [27] from http://homepage.tudelft.nl/19j49/t-SNE.html. We\n9http://metaoptimize.com/projects/wordreprs/ [25] 10Obtained from http://www.cs.cmu.edu/˜bmurphy/NNSE/. 11We found that NNSE trained using our contexts performed very poorly; see supplementary materials.\nshow projections of words related to the concept “good” vs. “bad” in Figure 4.12 See supplementary materials for “man” vs. “woman,” as well as 2-dimensional projections of NCE."
    }, {
      "heading" : "3.6 Runtime complexity",
      "text" : "Recall that we denote the number of input vectors (i.e., words) by V , the dimension of input vectors (i.e., contexts) byC, and the dimension of the code space byM . In our method, learning the dictionary is reasonably fast (we set T = 10). At every time step t, we need to perform a minimization of av.13 This is simply a regularized regression problem in RM with C instances, with M on the order of tens or hundreds in our case. Updating F and G involve cheap operations, namely vector multiplications and matrix additions. The minimization of D is perhaps the most computationally expensive part of each dictionary learning iteration.14 Since we use block coordinate descent [12],\n12Since t-SNE is a non convex method, we run it 10 times and choose the plots with the lowest t-SNE error. 13We use a mini batch of size 20,000, but we can do the minimization of each mini batch example in parallel. 14The minimization of D is a constrained optimization problem in RC×M , where C is on the order of\nhundreds of thousands.\nwe sequentially update the columns of D (there are M columns). The expensive part of this update is a dense matrix and dense vector multiplication, but this is also parallelizable, so the overall update is still time-efficient. Similar to Mairal et al. [12], we only perform one pass through the columns of D.\nThe final minimization of the entire matrix A can be carried out in parallel for each v ∈ V , so we have V regularized regression problems. However, since V is on the order of hundreds of thousands, we find that this step is the most expensive part in practice. For M = 52, the dictionary learning step took about 30 minutes (64 cores) and the overall learning procedure took approximately 2 hours (640 cores). For M = 520, the dictionary learning step took about 1.5 hours (64 cores) and the overall learning procedure took approximately 20 hours (640 cores). We do not optimize our code and we will be able to take advantage of further advances in dictionary learning and sparse optimization to reduce the runtime (e.g., using a parameter server similar to Kumar et al. [9]). For comparison, the SG model took about 1.5 hours and 5 hours for M = 52 and M = 520 using a highly optimized implementation from the author’s website (with no parallelization).\nExisting learning algorithms for neural network based models maximize the log likelihood of the corpus using gradient based methods, which typically involve taking derivatives w.r.t. all tokens in the corpus. Contrast this with our method that operates on the type level, i.e., it only uses tokens in the corpus to get cooccurrence counts to construct the input matrix X. The size of X only depends on the number of word types in the corpus. Once we construct X, the time complexity of learning is similar regardless of the size of the corpus (assuming we have a closed vocabulary)."
    }, {
      "heading" : "4 Conclusion",
      "text" : "We introduced a new method for learning word representations based on hierarchical sparse coding. The regularizer encourages hierarchical organization of the latent dimensions of vector-space word embeddings. We showed that our method outperforms state-of-the-art methods on word similarity ranking, sentence completion challenge, and sentiment analysis tasks."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The authors thank Sam Thomson, Bryan R. Routledge, Jesse Dodge, and Fei Liu for helpful feedback on an earlier draft of this paper. This work was supported by computing resources provided by a grant from the Pittsburgh Supecomputing Center and the National Science Foundation through grant IIS-1352440."
    }, {
      "heading" : "1 Additional Results",
      "text" : "In Table 1, we compare FOREST with three additional baselines:\n• Murphy et al. [9]: a word representation trained using non-negative sparse embedding (NNSE) on our corpus. Similar to the authors, we use an NNSE implementation from http://spams-devel.gforge.inria.fr/ [6]. • Mikolov et al. [7]: a log bilinear model that predicts a word given its context, trained using\nhierarchical softmax with a binary Huffman tree (continuous bag of words, CBOW-HS). We use an implementation from https://code.google.com/p/word2vec/. • Mikolov et al. [7]: a log bilinear model that predicts context words given a target word,\ntrained using hierarchical softmax with a binary Huffman tree (skip gram, SG-HS). We use an implementation from https://code.google.com/p/word2vec/.\nWe train these models on our corpus using the same setup as experiments in our paper.\nTable 1: Summary of results for non-negative sparse embedding (NNSE), continuous bag-of-words and skip gram models trained with hierarchical softmax (CBOW-HS and SG-HS). Higher number is better (higher correlation coefficient or higher accuracy).\nM Task NNSE CBOW-HS SG-HS FOREST\n52 Word similarity 0.04 0.38 0.47 0.52 Syntactic analogies 0.10 19.50 24.87 24.38 Semantic analogies 0.01 5.31 14.77 9.86 Sentence completion 0.01 22.51 28.78 28.88 Sentiment analysis 61.12 68.92 71.72 75.51\n520 Word similarity 0.05 0.50 0.57 0.66 Syntactic analogies 0.81 46.00 50.40 48.00 Semantic analogies 0.57 8.00 31.05 41.33 Sentence completion 22.81 25.80 27.79 28.88 Sentiment analysis 67.05 78.50 79.57 81.90"
    }, {
      "heading" : "2 Additional Two Dimensional Projections",
      "text" : "For FOREST, SG, and NCE with M = 520, we project the learned word representations into two dimensions using the t-SNE tool [12] from http://homepage.tudelft.nl/19j49/t-SNE. html. We show projections of words related to the concept “good” vs. “bad” and “man” vs. “woman” in Figure 1.\nar X\niv :1\n40 6.\n20 35\nv1 [\ncs .C\nL ]\n8 J\nun 2\n01 4"
    }, {
      "heading" : "3 List of Word Similarity Datasets",
      "text" : "We use the following word similarity datasets in our experiments:\n• Finkelstein et al. [3]: WordSimilarity dataset (353 pairs). • Agirre et al. [1]: a subset of WordSimilarity dataset for evaluating similarity (203 pairs). • Agirre et al. [1]: a subset of WordSimilarity dataset for evaluating relatedness (252 pairs). • Miller and Charles [8]: semantic similarity dataset (30 pairs) • Rubenstein and Goodenough [11]: contains only nouns (65 pairs) • Luong et al. [5]: rare words (2,034 pairs) • Bruni et al. [2]: frequent words (3,000 pairs)\n• Radinsky et al. [10]: MTurk-287 dataset (287 pairs) • Halawi and Dror [4]: MTurk-771 dataset (771 pairs) • Yang and Powers [13]: contains only verbs (130 pairs)"
    } ],
    "references" : [ {
      "title" : "A study on similarity and relatedness using distributional and wordnet-based approaches",
      "author" : [ "E. Agirre", "E. Alfonseca", "K. Hall", "J. Kravalova", "M. Pasca", "A. Soroa" ],
      "venue" : "In Proc. of NAACL-HLT",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2009
    }, {
      "title" : "Distributional semantics in technicolor",
      "author" : [ "E. Bruni", "G. Boleda", "M. Baroni", "Tran", "N.-K" ],
      "venue" : "In Proc. of ACL",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2012
    }, {
      "title" : "Placing search in context: The concept revisited",
      "author" : [ "L. Finkelstein", "E. Gabrilovich", "Y. Matias", "E. Rivlin", "Z. Solan", "G. Wolfman", "E. Ruppin" ],
      "venue" : "ACM Transactions on Information Systems,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2002
    }, {
      "title" : "The word relatedness mturk-771 test collection",
      "author" : [ "G. Halawi", "G. Dror" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2014
    }, {
      "title" : "Better word representations with recursive neural networks for morphology",
      "author" : [ "Luong", "M.-T", "R. Socher", "C.D. Manning" ],
      "venue" : "In Proc. of CONLL",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "Online learning for matrix factorization and sparse coding",
      "author" : [ "J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2010
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "T. Mikolov", "K. Chen", "G. Corrado", "J. Dean" ],
      "venue" : "In Proc. of Workshop at ICLR",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2013
    }, {
      "title" : "Contextual correlates of semantic similarity",
      "author" : [ "G.A. Miller", "W.G. Charles" ],
      "venue" : "Language and Cognitive Processes,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1991
    }, {
      "title" : "Learning effective and interpretable semantic models using non-negative sparse embedding",
      "author" : [ "B. Murphy", "P. Talukdar", "T. Mitchell" ],
      "venue" : "In Proc. of COLING",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2012
    }, {
      "title" : "A word at a time: Computing word relatedness using temporal semantic analysis",
      "author" : [ "K. Radinsky", "E. Agichtein", "E. Gabrilovich", "S. Markovitch" ],
      "venue" : "In Proc. of WWW",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2011
    }, {
      "title" : "Contextual correlates of synonymy",
      "author" : [ "H. Rubenstein", "J.B. Goodenough" ],
      "venue" : "Communications of the ACM,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1965
    }, {
      "title" : "Visualizing data using t-sne",
      "author" : [ "L. van der Maaten", "G. Hinton" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2008
    }, {
      "title" : "Verb similarity on the taxonomy of wordnet",
      "author" : [ "D. Yang", "D.M.W. Powers" ],
      "venue" : "In Proc. of GWC",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "In contrast, recent approaches to word representation learning apply neural networks to obtain dense, low-dimensional, continuous embeddings of words [18, 3, 7, 13, 15].",
      "startOffset" : 150,
      "endOffset" : 168
    }, {
      "referenceID" : 6,
      "context" : "In contrast, recent approaches to word representation learning apply neural networks to obtain dense, low-dimensional, continuous embeddings of words [18, 3, 7, 13, 15].",
      "startOffset" : 150,
      "endOffset" : 168
    }, {
      "referenceID" : 12,
      "context" : "In contrast, recent approaches to word representation learning apply neural networks to obtain dense, low-dimensional, continuous embeddings of words [18, 3, 7, 13, 15].",
      "startOffset" : 150,
      "endOffset" : 168
    }, {
      "referenceID" : 7,
      "context" : "[30], and it has since been shown useful for other applications [8].",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 3,
      "context" : "1 Following many others, we let xv,c be the pointwise mutual information (PMI) between the occurrence of context word c within a five-word window of an occurrence of word v [26, 19, 4].",
      "startOffset" : 173,
      "endOffset" : 184
    }, {
      "referenceID" : 10,
      "context" : "Here, we use the squared loss for the reconstruction error, but other loss functions could also be used [11].",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 10,
      "context" : "The most common regularizer is the `1 penalty, which results in sparse codes [11].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 7,
      "context" : "[8] proposed a related penalty with only one tree for learning image and document representations.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "Others include: global context [7], multilingual context [4], geographic context [1], brain activation data [5], and second-order context [22].",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 3,
      "context" : "Others include: global context [7], multilingual context [4], geographic context [1], brain activation data [5], and second-order context [22].",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 0,
      "context" : "Others include: global context [7], multilingual context [4], geographic context [1], brain activation data [5], and second-order context [22].",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 4,
      "context" : "Others include: global context [7], multilingual context [4], geographic context [1], brain activation data [5], and second-order context [22].",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 9,
      "context" : "Alternating minimization routines have been shown to work reasonably well in practice for such problems [10], but they are too expensive here due to (i) the size of X ∈ RC×V (C and V are each on the order of 10) and (ii) the many overlapping groups in the structured regularizer Ω(A).",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 11,
      "context" : "[12].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[12].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 1,
      "context" : "Since our groups form tree structures, other methods such as FISTA [2] could also be used.",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 12,
      "context" : "[13]: a recursive neural network (RNN) language model.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 5,
      "context" : "• Mnih and Teh [18]: a log bilinear model that predicts a word given its context, trained using noise-contrastive estimation (NCE, [6]).",
      "startOffset" : 131,
      "endOffset" : 134
    }, {
      "referenceID" : 2,
      "context" : "[3]: a neural network language model trained on Wikipedia data for 2 months (CW).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[7]: a neural network model that uses additional global document context (RNN-DC).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 11,
      "context" : "14 Since we use block coordinate descent [12], Since t-SNE is a non convex method, we run it 10 times and choose the plots with the lowest t-SNE error.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 11,
      "context" : "[12], we only perform one pass through the columns of D.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 8,
      "context" : "[9]).",
      "startOffset" : 0,
      "endOffset" : 3
    } ],
    "year" : 2017,
    "abstractText" : "We propose a new method for learning word representations using hierarchical regularization in sparse coding inspired by the linguistic study of word meanings. We apply an efficient online and distributed learning method. Experiments on various benchmark tasks—word similarity ranking, analogies, sentence completion, and sentiment analysis—demonstrate that the method outperforms or is competitive with state-of-the-art neural network representations. Our word representations are available at http://www.ark.cs.cmu.edu/dyogatam/wordvecs/",
    "creator" : "LaTeX with hyperref package"
  }
}