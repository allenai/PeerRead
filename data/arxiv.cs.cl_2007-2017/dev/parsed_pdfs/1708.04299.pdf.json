{
  "name" : "1708.04299.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Emotion Detection on TV Show Transcripts with Sequence-based Convolutional Neural Networks",
    "authors" : [ "Sayyed M. Zahiri", "Jinho D. Choi" ],
    "emails" : [ "sayyed.zahiri@emory.edu", "jinho.choi@emory.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Human emotions have been widely studied in the realm of psychological and behavioral sciences as well as computer science (Strapparava and Mihalcea, 2008). A wide variety of researches have been conducted in detecting emotions from facial expressions and audio waves (Yu et al., 2001; Zeng et al., 2006; Lucey et al., 2010). The recent advent of natural language processing and machine learning has made the task of emotion detection on text possible, yet since emotions are not necessarily conveyed on text, quantifying different types of emotions using only text is generally challenging.\nAnother challenging aspect about this task is due to the lack of annotated datasets. There are few publicly available datasets (Strapparava and Mihalcea, 2007; Alm, 2008; Mohammad and Bravo-Marquez,\n2017; Buechel and Hahn, 2017). However, in order to further explore the feasibility of text-based emotion detection on dialogue, a more comprehensive dataset would be desired. This paper presents a new corpus that comprises transcripts of the TV show, Friends, where each utterance is annotated with one of the seven emotions: sad, mad, scared, powerful, peaceful, joyful, and neutral. Several annotation tasks are conducted through crowdsourcing for the maintenance of a high quality dataset. Dialogues from these transcripts include disfluency, slangs, metaphors, humors, etc., which make this task even more challenging. To the best of our knowledge, this is the largest text-based corpus providing finegrained emotions for such long sequences of consecutive utterances in multiparty dialogue.\nConvolutional neural networks (CNN) have been popular for several tasks on document classification. One of the major advantages of CNN is found in its capability of extensive feature extraction through deep-layered convolutions. Nonetheless, CNN are often not used for sequence modeling (Waibel et al., 1989; LeCun et al., 1995; Gehring et al., 2017) because their basic architecture does not take previous sequence information into account. One common approach to alleviate this issue is using recurrent neural networks (Sutskever et al., 2014; Liu et al., 2016). However, RNNs typically perform slower and require more training data to avoid overfitting. To exploit the sequence information embedded in our corpus yet to employ the advantages of CNN, sequenced-based CNN (SCNN) are proposed along with attention mechanisms, which guide CNN to fuse features form the current state with features from the previous states. The contributions of this research are summarized as follows:\n• We create a new corpus providing fine-grained emotion annotation on dialogue and give thorough corpus analytics (Section 3) .\nar X\niv :1\n70 8.\n04 29\n9v 1\n[ cs\n.C L\n] 1\n4 A\nug 2\n01 7\n• We introduce several sequence-based convolution neural network models with attention to facilitate sequential dependencies among utterances. (Section 4).\n• We give both quantitative and qualitative analysis that show the advantages of SCNN over the basic CNN as well as the advances in the attention mechanisms (Section 5)."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Text-based Emotion Detection",
      "text" : "Text-based emotion detection is on its early stage in natural language processing although it has recently drawn lots of attention. There are three common methods researchers have employed for detecting emotions from text: keyword-based, learningbased and hybrids of those two. In the first method, classification is done by aid of emotional keywords. Strapparava et al. (2004) categorized emotions by mapping keywords in sentences into lexical representation of affective concepts. Chaumartin (2007) did emotion detection on news headlines. The performance of these keyword-based approaches had been rather unsatisfactory due to the fact that the semantics of the keywords heavily depend on the contexts, and it is significantly affected by the absence of those keywords (Shaheen et al., 2014)\nTwo types of machine learning approaches have been used for the second method, supervised approaches where training examples are used to classify documents into emotional categories, and unsupervised approaches where statistical measures are used to capture the semantic dependencies between words and infer their relevant emotional categories. Chaffar and Inkpen (2011) detected emotions from several corpora collected from blog, news headline, and fairy tale using a supervised approach. Douiji et al. (2016) used YouTube comments and developed an unsupervised learning algorithm to detect emotions from the comments. Their approach gave\ncomparative performance to supervised approaches such as Chaffar and Inkpen (2011) that support vector machines were employed for statistical learning.\nThe hybrid method attempts to take advantages from both the keyword-based and learning methods. Seol et al. (2008) used an ensemble of a keywordbased approach and knowledge-based artificial neural networks to classify emotions in drama, novel, and public web diary. Hybrid approaches generally perform well although their architectures tend to be complicated for the replication."
    }, {
      "heading" : "2.2 Sequence Modeling on Dialogue",
      "text" : "The tasks of state tracking and dialogue act classification are ongoing fields of research similar to our task. Shi et al. (2016) proposed multi-channel CNN for cross-language dialogue tracking. Dufour et al. (2016) introduced a topic model that considered all information included in sub-dialogues to track the dialogue states introduced by the DSTC5 challenge (Kim et al., 2016). Stolcke et al. (2000) proposed a statistical approach to model dialogue act on human-to-human telephone conversations."
    }, {
      "heading" : "2.3 Attention in Neural Networks",
      "text" : "Attention mechanism has been widely employed in the field of computer vision (Mnih et al., 2014; Xu et al., 2015) and recently become popular in natural language processing as well. In particular, incorporating attention mechanism has achieved the stateof-the-art performance in machine translation and question answering tasks (Bahdanau et al., 2014; Hermann et al., 2015; dos Santos et al., 2016). Our attention mechanism is distinguished from the previous work which perform attention on two statistic embeddings whereas our approach puts attention on static and dynamically generated embeddings."
    }, {
      "heading" : "3 Corpus",
      "text" : "The Character Mining project provides transcripts of the TV show, Friends; transcripts from all sea-\nsons of the show are publicly available in JSON.1 Each season consists of episodes, each episode contains scenes, each scene includes utterances, where each utterance gives the speaker information. For this research, we take transcripts from the first four seasons and create a corpus by adding another layer of annotation with emotions. As a result, our corpus comprises 97 episodes, 897 scenes, and 12,606 utterances, where each utterance is annotated with one of the seven emotions borrowed from the six primary emotions in the Willcox (1982)’s feeling wheel, sad, mad, scared, powerful, peaceful, joyful, and a default emotion of neutral. Table 1 describes a scene containing seven utterances and their corresponding annotation from crowdsourcing."
    }, {
      "heading" : "3.1 Crowdsourcing",
      "text" : "Pioneered by Snow et al. (2008), crowdsourcing has been widely used for the creation of many corpora in natural language processing. Our annotation tasks are conducted on the Amazon Mechanical Turk. Each MTurk HIT shows a scene, where each utterance in the scene is annotated by four crowd workers who are asked to choose the most relevant emotion associated with that utterance. To assign a suitable budget for each HIT, the corpus is divided into four batches, where all scenes in each batch are restricted to the [5, 10), [11, 15), [15, 20), [20, 25] number of utterances and are budgeted to 10, 13, 17, 20 cents per HIT, respectively. Each HIT takes about 2.5 minutes on average, and the entire annotation costs about $680. The annotation quality for 20% of each HIT is checked manually and those HITs with poor quality are re-annotated."
    }, {
      "heading" : "3.2 Inter-Annotator Agreement",
      "text" : "Two kinds of measurements are used to evaluate the inter-annotator agreement (Table 3). First, Cohen’s kappa is used to measure the agreement between two annotators whereas Fleiss’ kappa is used for three and four annotators. Second, the partial agreement (an agreement between any pair of annotators) is measured to illustrate the improvement\n1nlp.mathcs.emory.edu/character-mining\nfrom a fewer to a greater number of crowd workers. Among all kinds of annotator groups, the kappa scores around 14% are achieved. Such low scores are rather expected because emotion detection is highly subjective so that annotators often judge different emotions that are all acceptable for the same utterance. This may also be attributed to the limitation of our dataset; a higher kappa score could be achieved if the annotators were provided with a multimodal dataset (e.g., text, speech, image).\nWhile the kappa scores are not so much compelling, the partial agreement scores show more promising results. The impact of a greater number of annotators is clearly illustrated in this measurement; over 70% of the annotation have no agreement with only two annotators, whereas 85% of the annotation find some agreement with four annotators. This implies that it is possible to improve the annotation quality by adding more annotators.\nIt is worth mentioning that crowd workers were asked to choose from 37 emotions for the first 25% of the annotation, which comprised 36 secondary emotions from Willcox (1982) and the neutral emotion. However, vast disagreements were observed for this annotation, resulting Cohen’s kappa score of 0.8%. Thus, we proceeded with the seven emotions described above, hoping to go back and complete the annotation for fine-grained emotions later."
    }, {
      "heading" : "3.3 Voting and Ranking",
      "text" : "We propose a voting/ranking scheme that allows to assign appropriate labels to the utterances with disagreed annotation. Given the quadruple annotation, we first divide the dataset into five-folds (Table 4):\nFor the first three folds, the annotation coming from the majority vote is considered the gold label (a1).\nThe least absolute error (LAE) is then measured for each annotator by comparing one’s annotation to the gold labels. For the last two folds, annotation generated by the annotator with the minimum LAE is chosen as gold, which is reasonable since those annotators generally produce higher quality annotation. With this scheme, 75.5% of the dataset can be deterministically assigned with gold labels from voting and the rest can be assigned by ranking."
    }, {
      "heading" : "3.4 Analysis",
      "text" : "Table 5 shows the distribution of all emotions in our corpus. The two most dominant emotions, neutral and joyful, together yield over 50% of the dataset, which does not seem to be balanced although it is understandable given the nature of this show, that is a comedy. However, when the coarse-grained emotions are considered, positive, negative, and neutral, they yield about 40%, 30%, and 30% respectively, which gives a more balanced distribution. The last column shows the ratio of annotation that all four annotators agree. Only around 1% of the annotation shows complete agreement for peaceful and powerful, which is reasonable since these emotions often get confused with neutral."
    }, {
      "heading" : "3 Emotions 7 Emotions Ratio 4-Agree",
      "text" : "Figure 1 illustrates how emotions of the six main characters are progressing and getting affected by\nother utterances as time elapses within a scene. It is clear that the emotion of the current speaker is often affected by the same speaker’s previous emotions as well as previous emotions of the other speakers participating in the dialogue.\nFigure 2 shows the confusion matrix with respect to the annotation (dis)agreement. Rows correspond to the labels obtained using the voting scheme (Section 3.3), and columns represent the emotions selected by each of four annotators. The two dominant emotions, neutral and joyful, cause the most confusion for annotators whereas the minor emotions such as sad, powerful, or peaceful show good agreement on the diagonal."
    }, {
      "heading" : "3.5 Comparison",
      "text" : "The ISEAR databank consists of 7,666 statements and six emotions,2 which were gathered from a research conducted by psychologists on 3,000 participants. The SemEval’07 Task 14 dataset was created from news headlines; it contained 250 sentences annotated with six emotions (Strapparava and Mihalcea, 2007). The WASSA’17 Task 1 dataset was collected from 7,097 tweets and labeled with four emotions (Mohammad and Bravo-Marquez, 2017). The participants of this shared task were expected to develop a model to detect the intensity of emotions in tweets. Our corpus is larger than most of the other text-based corpora and the only kind providing emotion annotation on dialogue sequences conveyed by consecutive utterances.\n2emotion-research.net/toolbox/ toolboxdatabase.2006-10-13.2581092615"
    }, {
      "heading" : "4 Sequence-Based Convolutional Neural Networks (SCNN)",
      "text" : "A unique aspect about our corpus is that it preserves the original sequence of utterances given each dialogue such that it allows to tackle this problem as a sequence classification task. This section introduces sequence-based CNN models that utilize the emotion sequence from the previous utterances for detecting the emotion of the current utterance. Additionally, attention mechanisms are suggested for better optimization of these SCNN models."
    }, {
      "heading" : "4.1 Sequence Unification: Concatenation",
      "text" : "We present a Sequence-based CNN that leverages the sequence information to improve classification. Figure 3 describes our first SCNN model. The input to SCNN is an embedding matrixX ∈ Rt×m, where t is the maximum number of tokens in any utterance and m is the embedding size. Each row in x represents a token in the utterance. At each time step, region sizes of ρ ∈ {1, 2, 3, ..., r}, r ∈ N are considered. Each region has the f -number of filters. As a result of applying convolution and max-pooling, a univariate feature vector ~h ∈ Rr·f is generated for the current utterance.\nIn the next step, the dense feature vectors from the current utterance and k-1 previous utterances within the same dialogue get concatenated columnwise. As a result of this concatenation, the vector ~l ∈ Rr·f ·k is created. Then, 1-D convolution is applied to ~l to obtain the vector ~q ∈ R(r·f ·k−F )/S+1, where S is the stride and F is the receptive field of the 1-D convolution. As a result of this operation, features extracted from the current utterance gets fused with the features associated with the previous utterances. In the final step, softmax is applied for the classification of seven emotions."
    }, {
      "heading" : "4.2 Sequence Unification: Convolution",
      "text" : "Let us refer the model in Section 4.1 to SCNNc. In the second proposed model, referred to SCNNv (Fig. 4), two separate 2-D convolutions, Conv1 and Conv2, are utilized for sequence unification. The input to Conv1 is the same embedding matrix X . The input to Conv2 is another matrix Y ∈ Rk×r·f , which is a row-wise concatenation of the dense vectors generated from the Conv1 of the current utterance as well as k-1 previous utterances. Conv2 has region sizes of β ∈ {1, .., b}, b ∈ N with the dnumber of filters for each region size. The output of Conv2 is the vector ~v ∈ Rd·b. Conv1 and Conv2 are conceptually identical although they have different region sizes, filters and other hyper-parameters.\nIn the next step, the outputs of two convolutions get concatenated column-wise to create the vector ~w ∈ R(r·f+d·b), which is fed into an one-dimensional CNN to create the vector ~q ∈ R(r·f+d·b−F )/S+1, that is the fused version of the feature vectors. Finally, the vector ~q is passed to a softmax layer for classification. Note that the intuition behind Conv2 is to capture features from the emotion sequence as Conv1 captures features from n-grams.\nInput Layer\nConvolution and Pooling\nAttention Matrix\nUtterance Embedding"
    }, {
      "heading" : "4.3 Attention Mechanism",
      "text" : "We also equipped the SCNNc and SCNNv models with an attention mechanism. This mechanism allows the SCNN models to learn what part of the features should be attended more. Essentially, this attention model is a weighted arithmetic sum of current utterance’s feature vector where the weights are chosen based on the relevance of each element of that feature vector given the unified feature vectors from the previous utterances.\nFigure 5 depicts attention on SCNNc, SCNNac . In this model, the current feature vector ~h and the k-1 previous feature vectors get concatenated rowwise to create Z ∈ Rk×r·f . An attention matrix A ∈ Rr·f×k is applied to the current feature vector. The weights of this attention matrix are learned given the past feature vectors. The result of this operation is ~u = ~h × A × Z, ~u ∈ R1×rf . Finally, 1-D convolution and softmax are applied to ~u.\nFigure 6 shows another attention model, called\nSCNNav , based on SCNNv. In this model, an attention vector ~a with trainable weights is applied to the outputs of Conv1 (~h) and Conv2 (~v). The output of the attention vector is a vector ~u = (hT × ~a × vT )T ,~a ∈ R1×d·b. Finally, 1-D convolution and softmax are applied to ~a to complete the classification task. The multiplication sign in Figures 5 and 6 are the matrix multiplication operator and the T sign refers to the transpose operation.\nGenerally the inputs to attention mechanism have a fixed size whereas in our model one of the input comes from dynamically generated embedding of previous hidden layers and the other input is the dense representation of the current utterance."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Corpus",
      "text" : "Our corpus is split into training, development, and evaluation sets that include 77, 11, and 9 episodes, respectively. Although episodes are randomly assigned, all utterances from the same episode are kept in the same set to preserve the sequence information. Further attempts are made to maintain similar ratios for each emotion across different sets. Table 6 shows the distributions of the datasets."
    }, {
      "heading" : "5.2 Preprocessing",
      "text" : "In this work we utilize Word2vec word embedding model introduced by Mikolov et al. (2013). The word embedding is trained separately using Friends\nTV show transcripts, Amazon reviews, New York times and Wall street journal articles. In this research we use word embedding size of 200."
    }, {
      "heading" : "5.3 Models",
      "text" : "We report the results of the following four models: SCNNc , SCNNac , SCNNv and SCNNav . For each of the mentioned models, we collect the results using n ∈ {1, ..., 5} previous utterances. For a better comparison we also report the results achieved from base CNN (Kim, 2014) and RNN-CNN. RNNCNN is our replication of the model proposed by Donahue et al. (2015) to fuse time-series information for visual recognition and description. RNNCNN that we employ is comprised of a Long Short Term Memory (LSTM); the input to LSTM is the feature vectors generated by CNN for all the utterances in a scene and we train RNN-CNN to tune all the hyper-parameters."
    }, {
      "heading" : "5.4 Results",
      "text" : "Table 7 summarizes the overall performance of our proposed sequence-based CNN models on the development set. First column of the table indicates the number of previous utterances included in our model. We report both accuracy and F1score for evaluating the performance of our models. F1-score, by considering false positives and false negatives is generally a better way of measuring the performance on a corpus with unbalanced distributed classes. In SCNNc and SCNNac , by including previous three utterances, and in SCNNv and SCNNav , by considering previous five utterances the best results are achieved. We also ran our experiments with considering more than five utterances, however no significant improvement was observed.\nTable 8 summarizes the overall performance of our models on the evaluation set. To compare our proposed models to some other baseline models, we also include performances of CNN and RNN-CNN. The accuracies and the F1-scores are reported for the cases of 7 and 3 emotions (Table 5),\nwhere the latter case is comparable to the typical sentiment analysis task. For the models listed in table 8, we choose the sequence numbers that had the best performances on the development set (3 for SCNNc and SCNNac , 5 for SCNNv and SCNN a v). From tables 7 and 8, we can see that SCNNac outperformed all other listed models.\nTo fuse the generated dense feature vectors we applied different combinations of regularized feature fusion networks similar to the networks employed in Wu et al. (2015) and Bodla et al. (2017). However, for our task, none of these fusion networks performed better than the 1-D convolutional layer we utilized. It worth to be mention that, in the first time step of our both attentive models, the two inputs to the attention matrix/vector are two identical vectors (first utterance’s feature vector) which mean that the main impact of attention mechanism starts from the second time step."
    }, {
      "heading" : "5.5 Analysis",
      "text" : "Figure 7 shows the confusion matrix of gold labels and prediction for our best model, SCNNac . Mostly all of the emotions get confused the most with neutral. Peaceful has the highest rate of confusion with neutral; 30% of total number of examples in this class are confused with neutral. Whereas, joyful and powerful have the least confusion rates with neutral (13.8% and 20.4% respectively).\nTo further explore the effect of sequence number, we divided the evaluation set into four batches based on the number of utterances in the scenes as described in section 3.1. After examining the performances of SCNNc and SCNNac on four batches, we noticed these two models by considering previous three sequences, performed better (roughly 4% boost in F1-scores) on the first two batches (i.e. scenes containing [5,15) utterances) compared to other models such as base CNN. It seems, in very long scenes which usually contain more speakers and transitions between the speakers, our proposed models did not significantly outperform base CNN.\nDuring our experiments we observed that RNNCNN did not have a compelling performance. Generally, complicated models with higher number of hyper parameters require a larger corpus for tuning the hyper parameters. Given the relatively small size of our proposed corpus for such a model, we noticed that RNN-CNN over fitted rapidly at the very early epochs. It was basically well-tuned to detect two most dominate classes. Also, SCNNv, did not outperform base CNN model although it did outperform base CNN after including attention mechanism (SCNNav). We believe that size of our corpus could be inadequate to train this model which has more hyper-parameters than SCNNc.\nFigure 8 depicts the heat-map representation of the vector created from multiplication of current utterance’s feature vector with the attention matrix A, in SCNNac . The heat-map includes the first eight consecutive utterances (rows of the heat-map) of\na scene. Each row shows importance of the current utterance (color-mapped with blue) as well as previous three utterances (last three columns of the heat-map) at that particular time step. At each time step, the current utterance has the largest value which means our model attends more to the current utterance compared to previous ones. Attention matrix learns to assign the weights to previous utterances given the weight assigned to the current utterance.\nFor instance, utterance number 7 and 8 attend more to their previous two utterances (which have similar emotions). Utterance number 5 attends less to utterance 2 as the second utterance has a positive emotion. Similarly, utterance 2 attends more to its previous utterance as they both have a positive emotion. Generally, in the cases where the emotion of current utterance is neutral, the weights assigned to its previous utterances are relatively small and mostly similar."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this work, we introduced a new corpus for emotion detection task which was gathered from the spoken dialogues. We also proposed attentive SCNN models which incorporated the existing sequence information. The experimental results showed that our proposed models outperformed the base CNN. As annotating emotions from text are usually subjective, in future, we plan to assign more annotators to improve the quality of the current corpus. Also, to fully evaluate the performances of our proposed models we intend to implement different combinations of attention mechanisms and expand the size of our corpus by annotating more seasons of Friends TV show."
    } ],
    "references" : [ {
      "title" : "Affect in Text and Speech",
      "author" : [ "Cecilia Ovesdotter Alm." ],
      "venue" : "Ph.D. thesis, University of Illinois at Urbana-Champaign.",
      "citeRegEx" : "Alm.,? 2008",
      "shortCiteRegEx" : "Alm.",
      "year" : 2008
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1409.0473 .",
      "citeRegEx" : "Bahdanau et al\\.,? 2014",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep heterogeneous feature fusion for template-based face recognition",
      "author" : [ "Navaneeth Bodla", "Jingxiao Zheng", "Hongyu Xu", "JunCheng Chen", "Carlos Castillo", "Rama Chellappa." ],
      "venue" : "Applications of Computer Vision (WACV), 2017 IEEE Winter Con-",
      "citeRegEx" : "Bodla et al\\.,? 2017",
      "shortCiteRegEx" : "Bodla et al\\.",
      "year" : 2017
    }, {
      "title" : "Emobank: Studying the impact of annotation perspective and representation format on dimensional emotion analysis",
      "author" : [ "Sven Buechel", "Udo Hahn." ],
      "venue" : "Proceedings of the 15th Conference of the European Chapter of the Association for Computational",
      "citeRegEx" : "Buechel and Hahn.,? 2017",
      "shortCiteRegEx" : "Buechel and Hahn.",
      "year" : 2017
    }, {
      "title" : "Using a heterogeneous dataset for emotion analysis in text",
      "author" : [ "Soumaya Chaffar", "Diana Inkpen." ],
      "venue" : "Canadian Conference on Artificial Intelligence. Springer, pages 62–67.",
      "citeRegEx" : "Chaffar and Inkpen.,? 2011",
      "shortCiteRegEx" : "Chaffar and Inkpen.",
      "year" : 2011
    }, {
      "title" : "Upar7: A knowledge-based system for headline sentiment tagging",
      "author" : [ "François-Régis Chaumartin." ],
      "venue" : "Proceedings of the 4th International Workshop on Semantic Evaluations. Association for Computational Linguistics, pages 422–425.",
      "citeRegEx" : "Chaumartin.,? 2007",
      "shortCiteRegEx" : "Chaumartin.",
      "year" : 2007
    }, {
      "title" : "Long-term recurrent convolutional networks for visual recognition and description",
      "author" : [ "Jeffrey Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Kate Saenko", "Trevor Darrell." ],
      "venue" : "Proceedings of the IEEE",
      "citeRegEx" : "Donahue et al\\.,? 2015",
      "shortCiteRegEx" : "Donahue et al\\.",
      "year" : 2015
    }, {
      "title" : "Attentive pooling networks",
      "author" : [ "Cıcero Nogueira dos Santos", "Ming Tan", "Bing Xiang", "Bowen Zhou." ],
      "venue" : "CoRR, abs/1602.03609 .",
      "citeRegEx" : "Santos et al\\.,? 2016",
      "shortCiteRegEx" : "Santos et al\\.",
      "year" : 2016
    }, {
      "title" : "Using youtube comments for text-based emotion recognition",
      "author" : [ "Yasmina Douiji", "Hajar Mousannif", "Hassan Al Moatassime." ],
      "venue" : "Procedia Computer Science 83:292–299.",
      "citeRegEx" : "Douiji et al\\.,? 2016",
      "shortCiteRegEx" : "Douiji et al\\.",
      "year" : 2016
    }, {
      "title" : "Tracking dialog states using an authortopic based representation",
      "author" : [ "Richard Dufour", "Mohamed Morchid", "Titouan Parcollet." ],
      "venue" : "Spoken Language Technology Workshop (SLT), 2016 IEEE. IEEE, pages 544–551.",
      "citeRegEx" : "Dufour et al\\.,? 2016",
      "shortCiteRegEx" : "Dufour et al\\.",
      "year" : 2016
    }, {
      "title" : "Convolutional sequence to sequence learning",
      "author" : [ "Jonas Gehring", "Michael Auli", "David Grangier", "Denis Yarats", "Yann N Dauphin." ],
      "venue" : "arXiv preprint arXiv:1705.03122 .",
      "citeRegEx" : "Gehring et al\\.,? 2017",
      "shortCiteRegEx" : "Gehring et al\\.",
      "year" : 2017
    }, {
      "title" : "Teaching machines to read and comprehend",
      "author" : [ "Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom." ],
      "venue" : "Advances in Neural Information Processing Systems. pages 1693–1701.",
      "citeRegEx" : "Hermann et al\\.,? 2015",
      "shortCiteRegEx" : "Hermann et al\\.",
      "year" : 2015
    }, {
      "title" : "The fifth dialog state tracking challenge",
      "author" : [ "Seokhwan Kim", "Luis Fernando D’Haro", "Rafael E Banchs", "Jason D Williams", "Matthew Henderson", "Koichiro Yoshino" ],
      "venue" : "In Proceedings of the 2016 IEEE Workshop on Spoken Language Technology (SLT)",
      "citeRegEx" : "Kim et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2016
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim." ],
      "venue" : "arXiv preprint arXiv:1408.5882 .",
      "citeRegEx" : "Kim.,? 2014",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks 3361(10):1995",
      "author" : [ "Yann LeCun", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "LeCun and Bengio,? \\Q1995\\E",
      "shortCiteRegEx" : "LeCun and Bengio",
      "year" : 1995
    }, {
      "title" : "Recurrent neural network for text classification with multi-task learning",
      "author" : [ "Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang." ],
      "venue" : "arXiv preprint arXiv:1605.05101 .",
      "citeRegEx" : "Liu et al\\.,? 2016",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2016
    }, {
      "title" : "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression",
      "author" : [ "Patrick Lucey", "Jeffrey F Cohn", "Takeo Kanade", "Jason Saragih", "Zara Ambadar", "Iain Matthews." ],
      "venue" : "Computer Vision and Pattern Recog-",
      "citeRegEx" : "Lucey et al\\.,? 2010",
      "shortCiteRegEx" : "Lucey et al\\.",
      "year" : 2010
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean." ],
      "venue" : "Advances in neural information processing systems. pages 3111–3119.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Recurrent models of visual attention",
      "author" : [ "Volodymyr Mnih", "Nicolas Heess", "Alex Graves" ],
      "venue" : "In Advances in neural information processing systems",
      "citeRegEx" : "Mnih et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2014
    }, {
      "title" : "WASSA-2017 shared task on emotion intensity",
      "author" : [ "Saif M. Mohammad", "Felipe Bravo-Marquez." ],
      "venue" : "Proceedings of the Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis (WASSA). Copenhagen, Denmark.",
      "citeRegEx" : "Mohammad and Bravo.Marquez.,? 2017",
      "shortCiteRegEx" : "Mohammad and Bravo.Marquez.",
      "year" : 2017
    }, {
      "title" : "Emotion recognition from text using knowledge-based ann",
      "author" : [ "Young-Soo Seol", "Dong-Joo Kim", "Han-Woo Kim." ],
      "venue" : "Proceedings of ITCCSCC. pages 1569–1572.",
      "citeRegEx" : "Seol et al\\.,? 2008",
      "shortCiteRegEx" : "Seol et al\\.",
      "year" : 2008
    }, {
      "title" : "Emotion recognition from text based on automatically generated rules",
      "author" : [ "Shadi Shaheen", "Wassim El-Hajj", "Hazem Hajj", "Shady Elbassuoni." ],
      "venue" : "Data Mining Workshop (ICDMW), 2014 IEEE International Conference on. IEEE, pages 383–392.",
      "citeRegEx" : "Shaheen et al\\.,? 2014",
      "shortCiteRegEx" : "Shaheen et al\\.",
      "year" : 2014
    }, {
      "title" : "A multichannel convolutional neural network for cross-language",
      "author" : [ "Hongjie Shi", "Takashi Ushio", "Mitsuru Endo", "Katsuyoshi Yamagami", "Noriaki Horii" ],
      "venue" : null,
      "citeRegEx" : "Shi et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2016
    }, {
      "title" : "Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks. In Proceedings of the conference on empirical methods in natural language",
      "author" : [ "Rion Snow", "Brendan O’Connor", "Daniel Jurafsky", "Andrew Y Ng" ],
      "venue" : null,
      "citeRegEx" : "Snow et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Snow et al\\.",
      "year" : 2008
    }, {
      "title" : "Dialogue act modeling for automatic tagging and recognition",
      "author" : [ "Andreas Stolcke", "Klaus Ries", "Noah Coccaro", "Elizabeth Shriberg", "Rebecca Bates", "Daniel Jurafsky", "Paul Taylor", "Rachel Martin", "Carol Van Ess-Dykema", "Marie Meteer" ],
      "venue" : null,
      "citeRegEx" : "Stolcke et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Stolcke et al\\.",
      "year" : 2000
    }, {
      "title" : "Semeval2007 task 14: Affective text",
      "author" : [ "Carlo Strapparava", "Rada Mihalcea." ],
      "venue" : "Proceedings of the 4th International Workshop on Semantic Evaluations. Association for Computational Linguistics, pages 70–74.",
      "citeRegEx" : "Strapparava and Mihalcea.,? 2007",
      "shortCiteRegEx" : "Strapparava and Mihalcea.",
      "year" : 2007
    }, {
      "title" : "Learning to identify emotions in text",
      "author" : [ "Carlo Strapparava", "Rada Mihalcea." ],
      "venue" : "Proceedings of the 2008 ACM symposium on Applied computing. ACM, pages 1556–1560.",
      "citeRegEx" : "Strapparava and Mihalcea.,? 2008",
      "shortCiteRegEx" : "Strapparava and Mihalcea.",
      "year" : 2008
    }, {
      "title" : "Wordnet affect: an affective extension of wordnet",
      "author" : [ "Carlo Strapparava", "Alessandro Valitutti" ],
      "venue" : "In LREC. Citeseer,",
      "citeRegEx" : "Strapparava and Valitutti,? \\Q2004\\E",
      "shortCiteRegEx" : "Strapparava and Valitutti",
      "year" : 2004
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le." ],
      "venue" : "Advances in neural information processing systems. pages 3104–3112.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Phoneme recognition using time-delay neural networks",
      "author" : [ "Alex Waibel", "Toshiyuki Hanazawa", "Geoffrey Hinton", "Kiyohiro Shikano", "Kevin J Lang." ],
      "venue" : "IEEE transactions on acoustics, speech, and signal processing 37(3):328–339.",
      "citeRegEx" : "Waibel et al\\.,? 1989",
      "shortCiteRegEx" : "Waibel et al\\.",
      "year" : 1989
    }, {
      "title" : "The feeling wheel: A tool for expanding awareness of emotions and increasing spontaneity and intimacy",
      "author" : [ "Gloria Willcox." ],
      "venue" : "Transactional Analysis Journal 12(4):274–276.",
      "citeRegEx" : "Willcox.,? 1982",
      "shortCiteRegEx" : "Willcox.",
      "year" : 1982
    }, {
      "title" : "Modeling spatial-temporal clues in a hybrid deep learning framework for video classification",
      "author" : [ "Zuxuan Wu", "Xi Wang", "Yu-Gang Jiang", "Hao Ye", "Xiangyang Xue." ],
      "venue" : "Proceedings of the 23rd ACM international conference on Multimedia. ACM, pages",
      "citeRegEx" : "Wu et al\\.,? 2015",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2015
    }, {
      "title" : "Show, attend and tell: Neural image caption generation with visual attention (2015)",
      "author" : [ "K Xu", "J Ba", "R Kiros", "A Courville", "R Salakhutdinov", "R Zemel", "Y Bengio." ],
      "venue" : "arxiv preprint. arXiv preprint arXiv:1502.03044 .",
      "citeRegEx" : "Xu et al\\.,? 2015",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    }, {
      "title" : "Emotion detection from speech",
      "author" : [ "Feng Yu", "Eric Chang", "Ying-Qing Xu", "HeungYeung Shum" ],
      "venue" : null,
      "citeRegEx" : "Yu et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2001
    }, {
      "title" : "Spontaneous emotional facial expression detection",
      "author" : [ "Zhihong Zeng", "Yun Fu", "Glenn I Roisman", "Zhen Wen", "Yuxiao Hu", "Thomas S Huang." ],
      "venue" : "Journal of multimedia 1(5):1–8.",
      "citeRegEx" : "Zeng et al\\.,? 2006",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 26,
      "context" : "Human emotions have been widely studied in the realm of psychological and behavioral sciences as well as computer science (Strapparava and Mihalcea, 2008).",
      "startOffset" : 122,
      "endOffset" : 154
    }, {
      "referenceID" : 33,
      "context" : "A wide variety of researches have been conducted in detecting emotions from facial expressions and audio waves (Yu et al., 2001; Zeng et al., 2006; Lucey et al., 2010).",
      "startOffset" : 111,
      "endOffset" : 167
    }, {
      "referenceID" : 34,
      "context" : "A wide variety of researches have been conducted in detecting emotions from facial expressions and audio waves (Yu et al., 2001; Zeng et al., 2006; Lucey et al., 2010).",
      "startOffset" : 111,
      "endOffset" : 167
    }, {
      "referenceID" : 16,
      "context" : "A wide variety of researches have been conducted in detecting emotions from facial expressions and audio waves (Yu et al., 2001; Zeng et al., 2006; Lucey et al., 2010).",
      "startOffset" : 111,
      "endOffset" : 167
    }, {
      "referenceID" : 25,
      "context" : "There are few publicly available datasets (Strapparava and Mihalcea, 2007; Alm, 2008; Mohammad and Bravo-Marquez, 2017; Buechel and Hahn, 2017).",
      "startOffset" : 42,
      "endOffset" : 143
    }, {
      "referenceID" : 0,
      "context" : "There are few publicly available datasets (Strapparava and Mihalcea, 2007; Alm, 2008; Mohammad and Bravo-Marquez, 2017; Buechel and Hahn, 2017).",
      "startOffset" : 42,
      "endOffset" : 143
    }, {
      "referenceID" : 19,
      "context" : "There are few publicly available datasets (Strapparava and Mihalcea, 2007; Alm, 2008; Mohammad and Bravo-Marquez, 2017; Buechel and Hahn, 2017).",
      "startOffset" : 42,
      "endOffset" : 143
    }, {
      "referenceID" : 3,
      "context" : "There are few publicly available datasets (Strapparava and Mihalcea, 2007; Alm, 2008; Mohammad and Bravo-Marquez, 2017; Buechel and Hahn, 2017).",
      "startOffset" : 42,
      "endOffset" : 143
    }, {
      "referenceID" : 29,
      "context" : "Nonetheless, CNN are often not used for sequence modeling (Waibel et al., 1989; LeCun et al., 1995; Gehring et al., 2017) because their basic architecture does not take previous sequence information into account.",
      "startOffset" : 58,
      "endOffset" : 121
    }, {
      "referenceID" : 10,
      "context" : "Nonetheless, CNN are often not used for sequence modeling (Waibel et al., 1989; LeCun et al., 1995; Gehring et al., 2017) because their basic architecture does not take previous sequence information into account.",
      "startOffset" : 58,
      "endOffset" : 121
    }, {
      "referenceID" : 28,
      "context" : "One common approach to alleviate this issue is using recurrent neural networks (Sutskever et al., 2014; Liu et al., 2016).",
      "startOffset" : 79,
      "endOffset" : 121
    }, {
      "referenceID" : 15,
      "context" : "One common approach to alleviate this issue is using recurrent neural networks (Sutskever et al., 2014; Liu et al., 2016).",
      "startOffset" : 79,
      "endOffset" : 121
    }, {
      "referenceID" : 21,
      "context" : "The performance of these keyword-based approaches had been rather unsatisfactory due to the fact that the semantics of the keywords heavily depend on the contexts, and it is significantly affected by the absence of those keywords (Shaheen et al., 2014)",
      "startOffset" : 230,
      "endOffset" : 252
    }, {
      "referenceID" : 5,
      "context" : "Chaumartin (2007) did emotion detection on news headlines.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 4,
      "context" : "Chaffar and Inkpen (2011) detected emotions from several corpora collected from blog, news headline, and fairy tale using a supervised approach.",
      "startOffset" : 0,
      "endOffset" : 26
    }, {
      "referenceID" : 4,
      "context" : "Chaffar and Inkpen (2011) detected emotions from several corpora collected from blog, news headline, and fairy tale using a supervised approach. Douiji et al. (2016) used YouTube comments and developed an unsupervised learning algorithm to detect emotions from the comments.",
      "startOffset" : 0,
      "endOffset" : 166
    }, {
      "referenceID" : 4,
      "context" : "Chaffar and Inkpen (2011) detected emotions from several corpora collected from blog, news headline, and fairy tale using a supervised approach. Douiji et al. (2016) used YouTube comments and developed an unsupervised learning algorithm to detect emotions from the comments. Their approach gave comparative performance to supervised approaches such as Chaffar and Inkpen (2011) that support vector machines were employed for statistical learning.",
      "startOffset" : 0,
      "endOffset" : 378
    }, {
      "referenceID" : 20,
      "context" : "Seol et al. (2008) used an ensemble of a keywordbased approach and knowledge-based artificial neural networks to classify emotions in drama, novel, and public web diary.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 12,
      "context" : "(2016) introduced a topic model that considered all information included in sub-dialogues to track the dialogue states introduced by the DSTC5 challenge (Kim et al., 2016).",
      "startOffset" : 153,
      "endOffset" : 171
    }, {
      "referenceID" : 19,
      "context" : "Shi et al. (2016) proposed multi-channel CNN for cross-language dialogue tracking.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 9,
      "context" : "Dufour et al. (2016) introduced a topic model that considered all information included in sub-dialogues to track the dialogue states introduced by the DSTC5 challenge (Kim et al.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 9,
      "context" : "Dufour et al. (2016) introduced a topic model that considered all information included in sub-dialogues to track the dialogue states introduced by the DSTC5 challenge (Kim et al., 2016). Stolcke et al. (2000) proposed a statistical approach to model dialogue act on human-to-human telephone conversations.",
      "startOffset" : 0,
      "endOffset" : 209
    }, {
      "referenceID" : 18,
      "context" : "Attention mechanism has been widely employed in the field of computer vision (Mnih et al., 2014; Xu et al., 2015) and recently become popular in natural language processing as well.",
      "startOffset" : 77,
      "endOffset" : 113
    }, {
      "referenceID" : 32,
      "context" : "Attention mechanism has been widely employed in the field of computer vision (Mnih et al., 2014; Xu et al., 2015) and recently become popular in natural language processing as well.",
      "startOffset" : 77,
      "endOffset" : 113
    }, {
      "referenceID" : 1,
      "context" : "In particular, incorporating attention mechanism has achieved the stateof-the-art performance in machine translation and question answering tasks (Bahdanau et al., 2014; Hermann et al., 2015; dos Santos et al., 2016).",
      "startOffset" : 146,
      "endOffset" : 216
    }, {
      "referenceID" : 11,
      "context" : "In particular, incorporating attention mechanism has achieved the stateof-the-art performance in machine translation and question answering tasks (Bahdanau et al., 2014; Hermann et al., 2015; dos Santos et al., 2016).",
      "startOffset" : 146,
      "endOffset" : 216
    }, {
      "referenceID" : 30,
      "context" : "As a result, our corpus comprises 97 episodes, 897 scenes, and 12,606 utterances, where each utterance is annotated with one of the seven emotions borrowed from the six primary emotions in the Willcox (1982)’s feeling wheel, sad, mad, scared, powerful, peaceful, joyful, and a default emotion of neutral.",
      "startOffset" : 193,
      "endOffset" : 208
    }, {
      "referenceID" : 23,
      "context" : "Pioneered by Snow et al. (2008), crowdsourcing has been widely used for the creation of many corpora in natural language processing.",
      "startOffset" : 13,
      "endOffset" : 32
    }, {
      "referenceID" : 30,
      "context" : "It is worth mentioning that crowd workers were asked to choose from 37 emotions for the first 25% of the annotation, which comprised 36 secondary emotions from Willcox (1982) and the neutral emotion.",
      "startOffset" : 160,
      "endOffset" : 175
    }, {
      "referenceID" : 25,
      "context" : "The SemEval’07 Task 14 dataset was created from news headlines; it contained 250 sentences annotated with six emotions (Strapparava and Mihalcea, 2007).",
      "startOffset" : 119,
      "endOffset" : 151
    }, {
      "referenceID" : 19,
      "context" : "The WASSA’17 Task 1 dataset was collected from 7,097 tweets and labeled with four emotions (Mohammad and Bravo-Marquez, 2017).",
      "startOffset" : 91,
      "endOffset" : 125
    }, {
      "referenceID" : 17,
      "context" : "In this work we utilize Word2vec word embedding model introduced by Mikolov et al. (2013). The word embedding is trained separately using Friends",
      "startOffset" : 68,
      "endOffset" : 90
    }, {
      "referenceID" : 13,
      "context" : "For a better comparison we also report the results achieved from base CNN (Kim, 2014) and RNN-CNN.",
      "startOffset" : 74,
      "endOffset" : 85
    }, {
      "referenceID" : 6,
      "context" : "RNNCNN is our replication of the model proposed by Donahue et al. (2015) to fuse time-series information for visual recognition and description.",
      "startOffset" : 51,
      "endOffset" : 73
    }, {
      "referenceID" : 30,
      "context" : "To fuse the generated dense feature vectors we applied different combinations of regularized feature fusion networks similar to the networks employed in Wu et al. (2015) and Bodla et al.",
      "startOffset" : 153,
      "endOffset" : 170
    }, {
      "referenceID" : 2,
      "context" : "(2015) and Bodla et al. (2017). However, for our task, none of these fusion networks performed better than the 1-D convolutional layer we utilized.",
      "startOffset" : 11,
      "endOffset" : 31
    } ],
    "year" : 2017,
    "abstractText" : "While there have been significant advances in detecting emotions from speech and image recognition, emotion detection on text is still under-explored and remained as an active research field. This paper introduces a corpus for text-based emotion detection on multiparty dialogue as well as deep neural models that outperform the existing approaches for document classification. We first present a new corpus that provides annotation of seven emotions on consecutive utterances in dialogues extracted from the show, Friends. We then suggest four types of sequence-based convolutional neural network models with attention that leverage the sequence information encapsulated in dialogue. Our best model shows the accuracies of 37.9% and 54% for fineand coarsegrained emotions, respectively. Given the difficulty of this task, this is promising.",
    "creator" : "LaTeX with hyperref package"
  }
}