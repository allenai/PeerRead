{
  "name" : "1705.08091.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Local Monotonic Attention Mechanism for End-to-End Speech Recognition",
    "authors" : [ "Andros Tjandra", "Sakriani Sakti", "Satoshi Nakamura" ],
    "emails" : [ "andros.tjandra.ai6@is.naist.jp", "ssakti@is.naist.jp", "s-nakamura@is.naist.jp" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms: speech recognition, encoder-decoder neural network, global and local attention mechanism"
    }, {
      "heading" : "1 Introduction",
      "text" : "End-to-end training is a newly emerging approach to sequence-to-sequence mapping tasks, that allows the model to directly learn the mapping between variablelength representation of different modalities (i.e., text-to-text sequence [1, 2], speech-to-text sequence [3, 4], image-to-text sequence [5], etc). In the context of a speech recognition task, it replaces the conventional fashion that consists of several sub-components that are trained and tuned separately, which include a front-end feature extractor, a acoustic modeling, a pronunciation lexicon, and a language model, into a single integrated neural network that is jointly optimized at the same time to directly map from a speech sequence to a text transcription.\nar X\niv :1\n70 5.\n08 09\n1v 1\n[ cs\n.C L\n] 2\n3 M\nOne popular approaches in the end-to-end mapping tasks of different modalities is based on encoder-decoder architecture. The earlier version of an encoderdecoder model is built with only two different components [2, 6]: (1) an encoder that processes the source sequence and encodes them into a fixed-length vector; and (2) a decoder that produces the target sequence based on information from fixed-length vector given by encoder. Both the encoder and decoder are jointly trained to maximize the probability of a correct target sequence given a source sequence. This architecture has been applied in many applications such as machine translation [2, 6], image captioning [7], and so on.\nHowever, such architecture encounters difficulties, especially for coping with long sequences. Because in order to generate the correct target sequence, the decoder solely depends only on the last hidden state of the encoder. In other words, the network needs to compress all of the information contained in the source sequence into a single fixed-length vector. Cho et al. [8] demonstrated a decrease in the performance of the encoder-decoder model associated with an increase in the length of the input sentence sequence. Therefore, Bahdanau et al. [1] introduced attention mechanism to address these issues. Instead of relying on a fixed-length vector, the decoder is assisted by the attention module to get the related context from the encoder sides, depends on the current decoder states.\nMost attention-based encoder-decoder model used today has a “global” property [1, 9]. Every time the decoder needs to predict the output given the previous output, it must compute a weighted summarization of the whole input sequence generated by the encoder states. This global property allows the decoder to address any parts of the source sequence at each step of the output generation and provides advantages in some cases like machine translation tasks. Specifically, when the source and the target languages have different grammatical structures and the last part of the target sequence may depend on the first part of the source sequence.\nHowever, although the global attention mechanism has often improved performance in some tasks, it is very computationally expensive. For a case that requires mapping between long sequences, it often produces misalignment. Furthermore, it does not fit with monotonous or left-to-right natures in speech recognition tasks. In other words, to produces a transcription given the speech signal, the attention module needs to focus on the audio’s specific timing and always move in one direction from the start to the end of the audio. Therefore, the attention needs two important characteristics to address these problems: local and monotonicity properties. The local property helps our attention module focus on certain parts from the speech that the decoder wants to transcribe, and the monotonicity property strictly generates alignment left-to-right from beginning to the end of the speech.\nIn this paper, we propose a novel attention module that has local and monotonicity properties. We explore various ways to control these properties on the attention-based encoder-decoder model. Experimental results demonstrate that an encoder-decoder based ASR with local monotonic attention significantly improved performance and reduced the computational complexity more than one\nthat used the standard global attention architecture."
    }, {
      "heading" : "2 Related Work",
      "text" : "Humans do not generally process all of the information that they encounter at once. Selective attention, which is a critical property in human perception, allows attention to be focused on particular information while filtering out a range of other information. The biological structure of the eye and the eye movement mechanism is one part of visual selective attention that provides the ability to focus attention selectively on parts of the visual space to acquire information when and where it is needed [10]. In the case of the cocktail party effect, humans can selectively focus their attentive hearing on a single speaker among various conversation and background noise sources [11].\nThe attention mechanism in deep learning has been studied for many years. But, only recently have attention mechanisms made their way into the sequenceto-sequence deep learning architectures that were proposed to solve machine translation tasks. Such mechanisms provide a model with the ability to jointly align and translate [1]. With the attention-based model, the encoder-decoder model significantly improved the performance on machine translation [1, 9] and has successfully been applied to ASR tasks [3, 4].\nHowever, as we mentioned above, most of those attention mechanism are based on “global” property, where the attention module tries to match the current hidden states with all the states from the encoder sides. This approach is inefficient and computationally expensive on longer source sequences. A “local attention” was recently introduced by Luong et al. [9] which provided the capability to only focus small subset of the encoder sides. Chorowski et al. [3] also proposed a soft constraint to encourage monotonicity by invoking a penalty based on the current alignment and previous alignments. However, the methods still did not guarantee a monotonicity movement of the attention.\nTo the best of our knowledge, only few studies have explored about local and monotonicity properties on an attention-based model. This work presents a novel attention module with locality and monotonicity properties. Our proposed mechanism strictly enforces monotonicity and locality properties in their alignment by explicitly modeling them in mathematical equations. We also explore various ways to control both properties and evaluate the impact of each variations on our proposed model."
    }, {
      "heading" : "3 Attention-based Encoder Decoder Neural Net-",
      "text" : "work for Speech Recognition\nThe encoder-decoder model is a neural network that directly models conditional probability p(y|x), where x = [x1, ..., xS ] is the source sequence with length S and y = [y1, ..., yT ] is the target sequence with length T . Figure 1 shows the overall structure of the attention-based encoder-decoder model that consists of\nencoder, decoder and attention modules.\nThe encoder task processes input sequence x and outputs representative information he = [he1, ..., h e S ] for the decoder. The attention module is an extension scheme for assisting the decoder to find relevant information on the encoder side based on the current decoder hidden states [1, 9]. Usually, attention modules produces context information ct at the time t based on the encoder and\ndecoder hidden states:\nct = S∑ s=1 at(s) ∗ hes (1)\nat(s) = Align(h e s, h d t )\n= exp(Score(hes, h d t ))∑S\ns=1 exp(Score(h e s, h d t ))\n(2)\nThere are several variations for score functions:\nScore(hes, h d t ) =  〈hes, hdt 〉, dot product heᵀs Wsh d t , bilinear\nV ᵀs tanh(Ws[h e s, h d t ]), MLP\n(3)\nwhere Score : (RM×RN )→ R, M is the number of hidden units for encoder and N is the number of hidden units for decoder. Finally, the decoder task, which predicts the target sequence probability at time t based on previous output and context information ct can be formulated:\nlog p(y|x) = T∑ t=1 log p(yt|y<t, ct) (4)\nMost common input x for speech recognition tasks is a sequence of feature vectors like Mel-spectral filterbank and/or MFCC. Therefore, x ∈ RS×D where D is the number of features and S is the total frame length for an utterance. Output y, which is a speech transcription sequence, can be either phoneme or grapheme (character) sequence."
    }, {
      "heading" : "4 Locality and Monotonicity Properties",
      "text" : "In the previous section, we explained the standard global attention-based encoderdecoder model. However, such a mechanism cannot explicitly control the area and focus attention given previous information. This problem happens because it applies the scoring function into all the encoder states and normalizes them with a softmax function. Another problem is we cannot explicitly enforce the probability mass generated by the current attention modules that are always moving incrementally to the end of the source sequence. In this section, we discuss and explain how to model the locality and monotonicity properties on the attention module.\nFigure 2 illustrates the overall mechanism of our proposed local monotonic\nattention, and details are described blow.\n1. Monotonicity-based Prediction of Central Position First, we define how to predict the next central position of the alignment illustrated in Part (1) of Figure 2. Assume we have source sequence with length S, which is encoded by the stack of Bi-LSTM (see Figure 1) into S encoded states he = [he1, ..., h e S ]. At time t, we want to decode the t-th\ntarget output given the source sequence, previous output yt−1, and current decoder hidden states hdt ∈ RN . In standard approaches, we use hidden states hdt to predict ∆pt with a multilayer perceptron (MLP). ∆pt is used to determine how far we move the center of the alignment compared to previous center pt−1.\nIn this paper, we propose two different formulations for estimating ∆pt to ensure a forward or monotonicity movement:\n• Constrained position prediction : We limit maximum range from ∆pt with hyperparameter Cmax with the following equation:\n∆pt = Cmax ∗ sigmoid(V ᵀp tanh(Wphdt )) (5)\nHere we can control how far our next center of alignment position pt relies on our datasets and guarantee 0 ≤ ∆pt ≤ Cmax. However, it requires us to handle hyperparameter Cmax. • Unconstrained position prediction : Compared to a previous formulation, since we do not limit the maximum range of ∆pt, here we can ignore hyperparameter Cmax and use exponential (exp) function instead of sigmoid. We can also use another function (e.g softplus) as long as the function satisfy f : R→ R+0 and the result of ∆pt ≥ 0. We formulate unconstrained position prediction with following equation:\n∆pt = exp(V ᵀ p tanh(Wph d t )) (6)\nHere Vp ∈ RK×1, Wp ∈ RK×N , N is the number of decoder hidden units and K is the number of hidden projection layer units. We omit the bias for simplicity. Both equations guarantee monotonicity properties since ∀t ∈ [1..T ], pt ≥ (pt−1 + ∆pt). Additionally, we also used scaling variable λt to scale the unnormalized Gaussian distribution that depends on ht. We calculated λt with following equation:\nλt = exp(V ᵀ λ tanh(Wph d t )) (7)\nwhere Vλ ∈ RK×1. In our initial experiments, we discovered that we improved our model performance by scaling with λt for each time-step. The main objective of this step is to generate a scaled Gaussian distribution aNt :\naNt (s) = λt ∗ exp ( − (s− pt) 2\n2σ2\n) . (8)\nwhere pt is the mean and σ is the standard deviation, both of which are used to calculate the weighted sum from the encoder states to generate context vector ct later. In this paper, we treat σ as a hyperparameter.\n2. Locality-based Alignment Generation After calculating new position pt, we generate locality-based alignment, as shown in Part (2) of Figure 2. Based on predicted position pt, we follow Luong et al. [9] to generate alignment aSt only within [pt − 2σ, pt + 2σ]:\naSt (s) = Align(h e s, h d t ),∀s ∈ [pt − 2σ, pt + 2σ]. (9)\nSince pt is a real number and the indexes for the encoder states are integers, we convert pt into an integer with floor operation. With this action, we can reduce the decoding computational complexity O(T ∗S) into O(T ∗σ) where σ S and σ is constant, and our attention model fulfill locality properties.\n3. Context Calculation In the last step, we calculate context ct with alignments a N t and a S t , as\nshown in Part (3) of Figure 2:\nct = (pt+2σ)∑ s=(pt−2σ) ( aNt (s) ∗ aSt (s) ) ∗ hes (10)\nContext ct and current hidden state h d t will later be utilized for calculating current output yt.\nOverall, we can rephrase the first step as generating “prior” probabilities aNt based on the previous pt−1 position and the current decoder states. Then the second step task generates “likelihood” probabilities aSt by measuring the relevance of our encoder states with the current decoder states. In the third step, we combine our “prior” and “likelihood” probability into an unnormalized “posterior” probability at and calculate expected context ct."
    }, {
      "heading" : "5 Experiment",
      "text" : ""
    }, {
      "heading" : "5.1 Speech Data",
      "text" : "We conducted our experiments on the TIMIT 1 [12] dataset with the same setup for training, development, and test sets as defined in the Kaldi s5 recipe [13]. The training set contains 3696 sentences from 462 speakers. We also used another sets of 50 speakers for the development set and the test set contains 192 utterances, 8 each from 24 speakers. For every experiment, we used 40-dimensional fbank with delta and acceleration (total 120-dimension feature vector) extracted from the Kaldi toolkit. The input features were normalized by subtracting the mean and divided by the standard deviation from the training set. For our decoder target, we re-mapped the original target phoneme set from 61 into 39 phoneme class plus the end of sequence mark (eos).\n1https://catalog.ldc.upenn.edu/ldc93s1"
    }, {
      "heading" : "5.2 Model Architectures",
      "text" : "On the encoder sides, we projected our input features with a linear layer with 512 hidden units followed by tanh activation function. We used three bidirectional LSTMs (Bi-LSTM) for our encoder with 256 hidden units for each LSTM (total 512 hidden units for Bi-LSTM). To reduce the computational time, we used hierarchical subsampling [14, 15], applied it to the top two Bi-LSTM layers, and reduced their length by a factor of 4.\nOn the decoder sides, we used a 64-dimensional embedding matrix to transform the input phonemes into a continuous vector, followed by two unidirectional LSTMs with 512 hidden units. For every local monotonic model, we used a MLP with 128 hidden units to generate ∆pt and λt. Hyperparameter σ was set to 1.5, and Cmax for constrained position prediction (see Eq. 5) was set to 5. Both hyperparameters were empirically selected and generally gave consistent results across various settings in our proposed model. For our scorer module, we used bilinear and MLP scorers (see Eq 3) with 128 hidden units. We used an Adam [16] optimizer with a learning rate of 0.0005.\nIn the recognition phase, we generated transcriptions with best-1 (greedy) search from the decoder. We did not use any language model in this work. All of our models were implemented on the Chainer framework [17].\nFor comparison, we evaluated our proposed model with the standard global attention-based encoder-decoder model as the baseline. Most of the configurations follow the above descriptions, except the baseline model that does not have a MLP for generating ∆pt and λt."
    }, {
      "heading" : "6 Result and Discussion",
      "text" : "Table 1 summarizes our experiments on our proposed local attention models and compares them to the baseline model using several possible scenarios."
    }, {
      "heading" : "6.1 Constrained vs Unconstrained Position Prediction",
      "text" : "Considering the use of constrained and unconstrained position prediction ∆pt, our results show that the model with the unconstrained position prediction (exp) model gives better results than one based on the constrained position prediction (sigmoid) model on both MLP and bilinear scorers. We conclude that it is more beneficial to use the unconstrained position prediction formulation since it gives better performance and we do not need to handle the additional hyperparameter Cmax."
    }, {
      "heading" : "6.2 Alignment Scorer vs Non-Scorer",
      "text" : "Next we investigate the importance of the scorer module by comparing the results between a model with and without it. Our results reveal that, by only relying on Gaussian alignment aNt and set a S t = 1, our model performance’s was worse than one that used both the scorer and Gaussian alignment. This might be because the scorer modules are able to correct the details from the Gaussian\nalignment based on the relevance of the encoder states in the current decoder states. Thus, we conclude that alignment with the scorer is essential for our proposed models."
    }, {
      "heading" : "6.3 Overall comparison to the baseline",
      "text" : "Overall, our proposed encoder-decoder model with local monotonic attention significantly improved the performance and reduced the computational complexity in comparison with one that used standard global attention mechanism (we cannot compare directly with [3] since its pretrained with HMM state alignment). The best performance achieved by our proposed model with unconstrained position prediction and bilinear scorer, and provided 12.2% relative error rate reduction to our baseline."
    }, {
      "heading" : "7 Conclusion",
      "text" : "This paper demonstrated a novel attention mechanism for encoder decode model that ensures monotonicity and locality properties. We explored various ways to control these properties, including monotonicity-based position prediction and locality-based alignment generation. The results reveal our proposed encoderdecoder model with local monotonic attention significantly improved the performance and reduced the computational complexity more than one that used standard global attention architecture. In the future, we will further investigated our proposed approach in a large-vocabulary speech recognition task."
    }, {
      "heading" : "8 Acknowledgements",
      "text" : "Part of this work was supported by JSPS KAKENHI Grant Numbers JP17H00747 and JP17K00237."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "D. Bahdanau", "K. Cho", "Y. Bengio" ],
      "venue" : "arXiv preprint arXiv:1409.0473, 2014.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Sequence-to-Sequence learning with neural networks",
      "author" : [ "I. Sutskever", "O. Vinyals", "Q.V. Le" ],
      "venue" : "Advances in neural information processing systems, 2014, pp. 3104–3112.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "End-to-end continuous speech recognition using attention-based recurrent NN: First results",
      "author" : [ "J. Chorowski", "D. Bahdanau", "K. Cho", "Y. Bengio" ],
      "venue" : "arXiv preprint arXiv:1412.1602, 2014.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition",
      "author" : [ "W. Chan", "N. Jaitly", "Q. Le", "O. Vinyals" ],
      "venue" : "Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. IEEE, 2016, pp. 4960–4964.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Show, attend and tell: Neural image caption generation with visual attention",
      "author" : [ "K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A.C. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio" ],
      "venue" : "Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, 2015, pp. 2048–2057.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
      "author" : [ "K. Cho", "B. Van Merriënboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio" ],
      "venue" : "arXiv preprint arXiv:1406.1078, 2014.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Deep visual-semantic alignments for generating image descriptions",
      "author" : [ "A. Karpathy", "L. Fei-Fei" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 3128–3137.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "On the properties of neural machine translation: Encoder–decoder approaches",
      "author" : [ "K. Cho", "B. van Merriënboer", "D. Bahdanau", "Y. Bengio" ],
      "venue" : "Syntax, Semantics and Structure in Statistical Translation, p. 103, 2014.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Effective approaches to attention-based neural machine translation",
      "author" : [ "M.-T. Luong", "H. Pham", "C.D. Manning" ],
      "venue" : "arXiv preprint arXiv:1508.04025, 2015.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "The dynamic representation of scenes",
      "author" : [ "R.A. Rensink" ],
      "venue" : "Visual cognition, vol. 7, no. 1-3, pp. 17–42, 2000. 11",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Some experiments on the recognition of speech, with one and with two ears",
      "author" : [ "E.C. Cherry" ],
      "venue" : "The Journal of the acoustical society of America, vol. 25, no. 5, pp. 975–979, 1953.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1953
    }, {
      "title" : "Darpa TIMIT acoustic-phonetic continous speech corpus cd-rom. NIST speech disc 1-1.1",
      "author" : [ "J.S. Garofolo", "L.F. Lamel", "W.M. Fisher", "J.G. Fiscus", "D.S. Pallett" ],
      "venue" : "NASA STI/Recon technical report n, vol. 93, 1993.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "The Kaldi speech recognition toolkit",
      "author" : [ "D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz", "J. Silovsky", "G. Stemmer", "K. Vesely" ],
      "venue" : "IEEE 2011 Workshop on Automatic Speech Recognition and Understanding. IEEE Signal Processing Society, Dec. 2011, iEEE Catalog No.: CFP11SRW-USB.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Supervised sequence labelling",
      "author" : [ "A. Graves" ],
      "venue" : "Supervised Sequence Labelling with Recurrent Neural Networks. Springer, 2012, pp. 5–13.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Endto-end attention-based large vocabulary speech recognition",
      "author" : [ "D. Bahdanau", "J. Chorowski", "D. Serdyuk", "P. Brakel", "Y. Bengio" ],
      "venue" : "Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. IEEE, 2016, pp. 4945–4949.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "D. Kingma", "J. Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980, 2014.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Chainer: a nextgeneration open source framework for deep learning",
      "author" : [ "S. Tokui", "K. Oono", "S. Hido", "J. Clayton" ],
      "venue" : "Proceedings of Workshop on Machine Learning Systems (LearningSys) in The Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS), 2015. [Online]. Available: http://learningsys.org/papers/ LearningSys 2015 paper 33.pdf",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Regularizing neural networks by penalizing confident output distributions",
      "author" : [ "G. Pereyra", "G. Tucker", "J. Chorowski", "L. Kaiser", "G. Hinton" ],
      "venue" : "arXiv preprint arXiv:1701.06548, 2017.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Learning online alignments with continuous rewards policy gradient",
      "author" : [ "Y. Luo", "C.-C. Chiu", "N. Jaitly", "I. Sutskever" ],
      "venue" : "arXiv preprint arXiv:1608.01281, 2016. 12",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : ", text-to-text sequence [1, 2], speech-to-text sequence [3, 4], image-to-text sequence [5], etc).",
      "startOffset" : 24,
      "endOffset" : 30
    }, {
      "referenceID" : 1,
      "context" : ", text-to-text sequence [1, 2], speech-to-text sequence [3, 4], image-to-text sequence [5], etc).",
      "startOffset" : 24,
      "endOffset" : 30
    }, {
      "referenceID" : 2,
      "context" : ", text-to-text sequence [1, 2], speech-to-text sequence [3, 4], image-to-text sequence [5], etc).",
      "startOffset" : 56,
      "endOffset" : 62
    }, {
      "referenceID" : 3,
      "context" : ", text-to-text sequence [1, 2], speech-to-text sequence [3, 4], image-to-text sequence [5], etc).",
      "startOffset" : 56,
      "endOffset" : 62
    }, {
      "referenceID" : 4,
      "context" : ", text-to-text sequence [1, 2], speech-to-text sequence [3, 4], image-to-text sequence [5], etc).",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 1,
      "context" : "The earlier version of an encoderdecoder model is built with only two different components [2, 6]: (1) an encoder that processes the source sequence and encodes them into a fixed-length vector; and (2) a decoder that produces the target sequence based on information from fixed-length vector given by encoder.",
      "startOffset" : 91,
      "endOffset" : 97
    }, {
      "referenceID" : 5,
      "context" : "The earlier version of an encoderdecoder model is built with only two different components [2, 6]: (1) an encoder that processes the source sequence and encodes them into a fixed-length vector; and (2) a decoder that produces the target sequence based on information from fixed-length vector given by encoder.",
      "startOffset" : 91,
      "endOffset" : 97
    }, {
      "referenceID" : 1,
      "context" : "This architecture has been applied in many applications such as machine translation [2, 6], image captioning [7], and so on.",
      "startOffset" : 84,
      "endOffset" : 90
    }, {
      "referenceID" : 5,
      "context" : "This architecture has been applied in many applications such as machine translation [2, 6], image captioning [7], and so on.",
      "startOffset" : 84,
      "endOffset" : 90
    }, {
      "referenceID" : 6,
      "context" : "This architecture has been applied in many applications such as machine translation [2, 6], image captioning [7], and so on.",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 7,
      "context" : "[8] demonstrated a decrease in the performance of the encoder-decoder model associated with an increase in the length of the input sentence sequence.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "[1] introduced attention mechanism to address these issues.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "Most attention-based encoder-decoder model used today has a “global” property [1, 9].",
      "startOffset" : 78,
      "endOffset" : 84
    }, {
      "referenceID" : 8,
      "context" : "Most attention-based encoder-decoder model used today has a “global” property [1, 9].",
      "startOffset" : 78,
      "endOffset" : 84
    }, {
      "referenceID" : 9,
      "context" : "The biological structure of the eye and the eye movement mechanism is one part of visual selective attention that provides the ability to focus attention selectively on parts of the visual space to acquire information when and where it is needed [10].",
      "startOffset" : 246,
      "endOffset" : 250
    }, {
      "referenceID" : 10,
      "context" : "In the case of the cocktail party effect, humans can selectively focus their attentive hearing on a single speaker among various conversation and background noise sources [11].",
      "startOffset" : 171,
      "endOffset" : 175
    }, {
      "referenceID" : 0,
      "context" : "Such mechanisms provide a model with the ability to jointly align and translate [1].",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 0,
      "context" : "With the attention-based model, the encoder-decoder model significantly improved the performance on machine translation [1, 9] and has successfully been applied to ASR tasks [3, 4].",
      "startOffset" : 120,
      "endOffset" : 126
    }, {
      "referenceID" : 8,
      "context" : "With the attention-based model, the encoder-decoder model significantly improved the performance on machine translation [1, 9] and has successfully been applied to ASR tasks [3, 4].",
      "startOffset" : 120,
      "endOffset" : 126
    }, {
      "referenceID" : 2,
      "context" : "With the attention-based model, the encoder-decoder model significantly improved the performance on machine translation [1, 9] and has successfully been applied to ASR tasks [3, 4].",
      "startOffset" : 174,
      "endOffset" : 180
    }, {
      "referenceID" : 3,
      "context" : "With the attention-based model, the encoder-decoder model significantly improved the performance on machine translation [1, 9] and has successfully been applied to ASR tasks [3, 4].",
      "startOffset" : 174,
      "endOffset" : 180
    }, {
      "referenceID" : 8,
      "context" : "[9] which provided the capability to only focus small subset of the encoder sides.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[3] also proposed a soft constraint to encourage monotonicity by invoking a penalty based on the current alignment and previous alignments.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "The attention module is an extension scheme for assisting the decoder to find relevant information on the encoder side based on the current decoder hidden states [1, 9].",
      "startOffset" : 162,
      "endOffset" : 168
    }, {
      "referenceID" : 8,
      "context" : "The attention module is an extension scheme for assisting the decoder to find relevant information on the encoder side based on the current decoder hidden states [1, 9].",
      "startOffset" : 162,
      "endOffset" : 168
    }, {
      "referenceID" : 8,
      "context" : "[9] to generate alignment at only within [pt − 2σ, pt + 2σ]: at (s) = Align(h e s, h d t ),∀s ∈ [pt − 2σ, pt + 2σ].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 11,
      "context" : "1 Speech Data We conducted our experiments on the TIMIT 1 [12] dataset with the same setup for training, development, and test sets as defined in the Kaldi s5 recipe [13].",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 12,
      "context" : "1 Speech Data We conducted our experiments on the TIMIT 1 [12] dataset with the same setup for training, development, and test sets as defined in the Kaldi s5 recipe [13].",
      "startOffset" : 166,
      "endOffset" : 170
    }, {
      "referenceID" : 13,
      "context" : "To reduce the computational time, we used hierarchical subsampling [14, 15], applied it to the top two Bi-LSTM layers, and reduced their length by a factor of 4.",
      "startOffset" : 67,
      "endOffset" : 75
    }, {
      "referenceID" : 14,
      "context" : "To reduce the computational time, we used hierarchical subsampling [14, 15], applied it to the top two Bi-LSTM layers, and reduced their length by a factor of 4.",
      "startOffset" : 67,
      "endOffset" : 75
    }, {
      "referenceID" : 15,
      "context" : "We used an Adam [16] optimizer with a learning rate of 0.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 16,
      "context" : "All of our models were implemented on the Chainer framework [17].",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 2,
      "context" : "Model Test PER (%) Global Attention Model (Baseline) Att Enc-Dec (pretrained with HMM align)[3] 18.",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 17,
      "context" : "6 Att Enc-Dec [18] 23.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 18,
      "context" : "2 Att Enc-Dec [19] 24.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 2,
      "context" : "Overall, our proposed encoder-decoder model with local monotonic attention significantly improved the performance and reduced the computational complexity in comparison with one that used standard global attention mechanism (we cannot compare directly with [3] since its pretrained with HMM state alignment).",
      "startOffset" : 257,
      "endOffset" : 260
    } ],
    "year" : 2017,
    "abstractText" : "Recently, sequence-to-sequence model by using encoder-decoder neural network has gained popularity for automatic speech recognition (ASR). The architecture commonly uses an attentional mechanism which allows the model to learn alignments between source speech sequence and target text sequence. Most attentional mechanisms used today is based on a global attention property which requires a computation of a weighted summarization of the whole input sequence generated by encoder states. However, it is computationally expensive and often produces misalignment on the longer input sequence. Furthermore, it does not fit with monotonous or left-to-right nature in speech recognition task. In this paper, we propose a novel attention mechanism that has local and monotonic properties. Various ways to control those properties are also explored. Experimental results demonstrate that encoder-decoder based ASR with local monotonic attention could achieve significant performance improvements and reduce the computational complexity in comparison with the one that used the standard global attention architecture.",
    "creator" : "LaTeX with hyperref package"
  }
}