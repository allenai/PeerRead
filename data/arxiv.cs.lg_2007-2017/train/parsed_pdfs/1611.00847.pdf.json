{
  "name" : "1611.00847.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "DEEP CONVOLUTIONAL NEURAL NETWORK DESIGN PATTERNS",
    "authors" : [ "Leslie N. Smith", "Nicholay Topin" ],
    "emails" : [ "leslie.smith@nrl.navy.mil", "ntopin1@umbc.edu" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Recently, there has been a large assortment of articles on new neural network architectures, especially regarding Residual Networks, such as He et al. (2015; 2016); Larsson et al. (2016); Zhang et al. (2016); Huang et al. (2016b). This has motivated us to take a high-level look at these architectures as a potential source for universal principles of design. This is especially relevant because many inexperienced practitioners are currently looking to apply deep learning to various new applications. A lack of guidance causes novice deep learning practitioners to ignore the most recent research and pick Alexnet (or some such standard architecture) regardless of its appropriateness for their application.\nThis abundance of research is also an opportunity to determine elements that provide benefits in specific contexts. We ask some fundamental questions: Do universal principles of deep network design exist? Can these principles be mined from the collective knowledge on deep learning? Which architectural choices work best in any given context? Which architectures or parts of architectures seem elegant?\nDesign patterns were first described by Christopher Alexander (Alexander (1979)) in regards to the architectures of buildings and towns. Alexander wrote of a timeless quality in architecture that “lives” and this quality is enabled by building based on universal principles. The basis of design patterns is that they resolve a conflict of forces in a given context and lead to an equilibrium analogous to the ecological balance in nature. Design patterns are both highly specific, making them clear to follow, and flexible so they can be adapted to different environments and situations. Inspired by Alexander’s work, the “gang of four” (Gamma et al. (1995)) applied the concept of design patterns to the architecture of object-oriented software. This classic computer science book describes 23 patterns that resolve issues prevalent in software design, such as “requirements always change”. We were inspired by these previous works on architectures to articulate possible design patterns for neural network architectures.\nDesign patterns provide universal guiding principles, and here we take the first steps to defining design patterns for neural network architectures. Overall, it is an enormous task to define design principles for all neural networks and all applications, so we limit this paper to convolutional neural networks (CNN) and their canonical application of image classification. However, we recognize\nar X\niv :1\n61 1.\n00 84\n7v 1\n[ cs\n.L G\n] 2\nN ov\n2 01\n6\nthat architectures must depend upon the application with our first design pattern, Design Pattern 1: Architectural Structure follows the Application, but we leave the details to future work. In addition, these principles allowed us to discover some gaps in the existing research and to articulate novel architectural features, such as freeze-drop-path (see Section 4.1). The rules of thumb articulated here might be valuable for both the experienced and novice practitioners. Moreso, we truly hope that this preliminary work serves as a stepping stone for others to discover and share other deep learning design patterns."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "There has been a lot of research studying neural network architectures, but we are unaware of a recent survey of the field. Unfortunately, we cannot do justice to this body of work, so we instead focus on recent innovations in convolutional neural networks architectures and, in particular, on the family of Residual Network variants (He et al., 2015). We start with the Network In Network (Lin et al., 2013), which describes a hierarchical network with a small network design repeatedly embedded in the overall architecture. Szegedy et al. (2015a) incorporated this idea into their Inception architecture. Later, these authors proposed modifications to the original inception design (Szegedy et al., 2015b). A similar concept was contained in the multi-scale convolution architecture (Liao & Carneiro, 2015). In the meantime, Batch Normalization (Ioffe & Szegedy, 2015) was presented as a unit within the network that makes training the network faster and easier.\nPrior to the introduction of residual networks there were a few papers suggesting skip connections. Skip connections were proposed by Raiko et al. (2012). Srivastava, et al. introduced Highway Networks (Srivastava et al., 2015), which use a gating mechanism to decide whether to combine the input with the layer outputs, and showed how this allowed the training of very deep networks. The DropIn technique (Smith et al., 2015; 2016) describes both a gradual change and a stochastic method in allowing the input to skip layers in order to train very deep networks. The concept of stochastic depth via a drop-path method was introduced by Huang et al. (2016b).\nResidual Networks were introduced by He et al. (2015), which described their network that won the ImageNet Challenge. They were able to extend the depth of a network from tens to hundreds of layers and in doing so, improve the network’s performance. The authors followed up with another paper (He et al., 2016) where they investigate why identity mappings help and report results for a network with more than a thousand layers. The research community took notice of this architecture and several modifications to the original design were soon proposed.\nThe Inception-v4 paper (Szegedy et al., 2016) describes the impact of residual connections on their Inception architecture and compared these results with the results from an updated Inception design. The Resnet in Resnet paper (Targ et al., 2016) suggests an architecture with connections in a layer. Veit et al. (2016) provided an understanding of Residual Networks as an ensemble of relatively shallow networks. These authors illustrated how these residual connections allow the input to follow an exponential number of paths through the architecture. At the same time, the FractalNet paper (Larsson et al., 2016) demonstrated training deep networks with a symmetrically repeating architectural pattern. As described later, we found the symmetry introduced in this paper intriguing. In a similar vein, Convolutional Neural Fabrics (Saxena & Verbeek, 2016) introduces a three dimensional network, where the usual depth through the network is only the first dimension.\nWide Residual Networks (Zagoruyko & Komodakis, 2016) demonstrates that simultaneously increasing both depth and width leads to improved performance. In Swapout (Singh et al., 2016), each layer can be dropped, skipped, used normally, or combined with a residual. Deeply Fused Nets (Wang et al., 2016) proposes networks with multiple paths. In the Weighted Residual Networks paper (Shen & Zeng, 2016), the authors recommend a weighting factor for the output from the convolutional layers, which only gradually introduces the trainable layers. Convolutional Residual Memory Networks (Moniz & Pal, 2016) proposes an architecture that combines a convolutional residual network with an LSTM memory mechanism. For Residual of Residual Networks (Zhang et al., 2016), the authors propose adding a hierarchy of skip connections where the input can skip a layer, a module, or any number of modules. DenseNets (Huang et al., 2016a) introduces a network where each module is densely connected; that is, the output from a layer is input to all of the other layers in the module. In the Multi-Residual paper (Abdi & Nahavandi, 2016), the authors propose\nexpanding a residual block width-wise to contain multiple convolutional paths. As described in the Appendix A, there is a close relationship between many of these residual networks."
    }, {
      "heading" : "3 DESIGN PATTERNS",
      "text" : "To the best of our knowledge, there has been little written to provide guidance and understanding on appropriate architectural choices. The book ”Neural Networks: Tricks of the Trade” (Orr & Müller, 2003) contains recommendations for network models but without reference to the vast amount of research in the past few years. The closest to this work is probably Szegedy et al. (2015b) where the authors describe a few design principles based on their experiences.\nWe reviewed the literature specifically to extract commonalities and boil their designs down to fundamental elements that might be considered design patterns. It seems clear to us that in reviewing the literature some design choices seem elegant while others are less so. In this section, we discuss some of these elements. Here, we will first describe a few high level design patterns and then propose some more detailed ones."
    }, {
      "heading" : "3.1 HIGH LEVEL ARCHITECTURE DESIGN",
      "text" : "Several researchers have pointed out that the winners of the ImageNet Challenge (Russakovsky et al., 2015) have successively used deeper networks (as seen in, Krizhevsky et al. (2012), Szegedy et al. (2015a), Simonyan & Zisserman (2014), He et al. (2015)). It is also apparent from the ImageNet Challenge that multiplying the number of paths through the network is a recent trend; a trend that is apparent when looking from Alexnet to Inception to ResNets. For example, Veit et al. (2016) show that ResNets can be considered to be an exponential ensemble of networks with different lengths. This leads to Design Pattern 2: Proliferate Paths. One does this by including a multiplicity of branches in the architecture. Recent examples include FractalNet (Larsson et al. 2016), Xception (Chollet 2016), and Decision Forest Convolutional Networks (Ioannou et al. 2016). We even will go so far as to predict that the winner of this year’s ImageNet Challenge will do so by increasing the number of branches in their architecture rather than continuing to increase the depth.\nScientists have embraced simplicity/parsimony for centuries. Simplicity was exemplified in the paper ”Striving for Simplicity” (Springenberg et al. 2014) by achieving state-of-the-art results with fewer types of units. We add this as Design Pattern 3: Strive for Simplicity; to use fewer types of layers in order to keep the network as simple as possible. We also noted a special degree of elegance in the FractalNet (Larsson et al. 2016) design, which we attributed to the symmetry of its structure. Architectural symmetry is typically considered a sign of beauty and quality, so we add it here as Design Pattern 4: Increase Symmetry. In addition to its symmetry, FractalNets also adheres to the Proliferate Paths design pattern so it is the baseline of our experiments in Section 4.\nExamination of trade-offs in order to understand the relevant forces is an essential element of design patterns. One fundamental trade-off is to maximize representational power versus compression of redundant and non-discriminating information. It is universal in all convolutional neural networks, from the data to the final convolutional layer, that the activations are downsampled and the number of channels increased. As is exemplified in Deep Pyramidal Residual Networks (Han et al. (2016)), which leads to Design Pattern 5: Pyramid Shape where there should be an overall smooth downsampling throughout the architecture and the downsampling should be combined with an increase in the number of channels.\nAnother important trade-off in deep learning is training accuracy versus the ability of the network to generalize to non-seen cases. The ability to generalize is an important virtue of deep neural networks. A way to improve generalization is by Design Pattern 6: Cover the Problem Space with the training data (Ratner et al. 2016, Hu et al. 2016, Wong et al. 2016, Johnson-Roberson et al. 2016). This allows training accuracy improvements to directly improve test accuracy. In addition, regularization is commonly used to improve generalization. Regularization includes methods such as dropout (Srivastava et al. 2014a) and drop-path (Huang et al. 2016b). As noted by Srivastava et al. 2014b, dropout improves generalization by injecting noise in the architecture. We believe regularization techniques and prudent noise injection during training to improve generalization (Srivastava et al. 2014b, Gulcehre et al. 2016) belong to Design Pattern 7: Over-training. Over-training includes any training method where the network is trained on a harder problem than necessary so that\nthe performance in the easier inference situation is improved. In addition to regularization methods, this includes the use of noisy data (Rasmus et al. 2015, Krause et al. 2015, Pezeshki et al. 2015)."
    }, {
      "heading" : "3.2 DETAILED ARCHITECTURE DESIGN",
      "text" : "A common thread throughout many of the more successful architectures is to make the job required of each layer easier. Use of very deep networks is such an example since any single layer only needs to incrementally modify the input. This partially explains the success of residual networks, since in very deep networks, a layer’s output is likely similar to the input, hence adding the input to the layer’s output makes the layer’s job easier. Also, this is part of the motivation behind design pattern Proliferate Paths but this concept of making each layer’s job easy extends beyond that. An example of Design Pattern 8: Incremental Feature Construction is to use short skip lengths in ResNets. A recent paper (Alain & Bengio (2016)) demonstrated that using an identity skip length of 64 in a network of depth 128 led to the first portion of the network not training and consisting of dead weights, which should be avoided.\nAnother way to make a layer’s job easier is with Design Pattern 9: Normalize layer inputs. Normalization of layer inputs has been shown to improve training and accuracy but the underlying reasons are not clear (Ioffe & Szegedy 2015, Ba et al. 2016, Salimans & Kingma 2016). The Batch Normalization (Ioffe & Szegedy 2015) paper attributes the benefits to handling internal covariate shift, while the authors of streaming normalization (Liao et al. 2016) express that it might be otherwise. We feel that normalization puts all the input samples on more equal footing as if they we scaled via a units conversion, which allows back-propagation to train more effectively.\nSome research, such as Wide ResNets (Zagoruyko & Komodakis 2016), has shown that increasing the number of channels improves performance but there are additional costs with the extra channels. The input data for many of the benchmark datasets in the literature have 3 channels (i.e., RGB). It is almost universal that the output from the first layer of a CNN increases the number of channels Design Pattern 11: Input Transition. A few examples this increase in channels/outputs at the first layer for ImageNet are AlexNet (96), Inception (32), VGG (224), and ResNets (64). Intuitively it makes sense to increase the number of channels from 3 in the first layer as it allows the input data to be examined many ways but it is not clear how many filters to use. However, another trade-off is that of cost versus accuracy. Costs includes the number of parameters in the network, which is directly reflected in greater computational and storage costs of training and inference. Increasing the number of channels increases costs, which leads to Design Pattern 10: Available resources guide Network Depth. In addition to doubling the number of outputs when downsampling (see design pattern 13), pick the depth of the first layer based on memory, computational resources, and desired accuracy. Deep learning is computationally expensive and each practitioner must balance these costs against their application’s performance."
    }, {
      "heading" : "3.2.1 JOINING BRANCHES: CONCATENATION VERSUS SUMMATION/MEAN VERSUS MAXOUT",
      "text" : "When there are multiple branches, three methods have been used to combine the outputs; concatenation, summation (or mean), or Maxout. It seems that different researchers have their favorites and there hasn’t been any motivation for using one in preference to another. In this Section, we propose some easy rules for deciding how to combine branches.\nSummation is one of the most common ways of combining outputs. Summation/mean splits the work of approximation among the branches and leads to Design Pattern 12: Summation Joining. Summation is the preferred joining mechanism for residual networks because it allows the network to compute corrective terms (i.e., residuals) rather than the entire signal. The difference between sum and fractal-join (mean) is best understood by considering drop-path (Huang et al. 2016b). In a residual network where the input skip connection is always present, summation causes the convolutional layers to learn the residual (the difference from the input). On the other hand, in networks with several branches where any branch can be dropped, such as FractalNet (Larsson et al. (2016)), using the mean is preferable as it keeps the output smooth as branches are randomly dropped.\nSome researchers seem to prefer concatenation ( e.g., Szegedy et al. (2015a)). We believe that concatenation is most useful for increasing the number of outputs when pooling, which leads to Design Pattern 13: Down-sampling Transition. That is, when down-sampling by pooling or using a stride greater than 1, a good way to combine branches is to concatenate the output channels,\nhence smoothly accomplishing both joining and an increase in the number of channels that typically accompanies down-sampling.\nMaxout has been used for competition, as in locally competitive networks (Srivastava et al. 2014b) and competitive multi-scale networks Liao & Carneiro (2015). Maxout chooses only one of the activations and leads to Design Pattern 14: MaxOut for Competition. This is in contrast to summation or mean where the activations are “cooperating”; here there is a “competition” with only one “winner”. For example, when each branch is composed of different sized kernels, maxout is useful to incorporate scale invariance, which is analogous to translation invariance enabled by max pooling.\nWe believe that all these joining mechanism are appropriate in a single network, rather than the typical situation where only one is used throughout."
    }, {
      "heading" : "4 EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "4.1 ARCHITECTURAL INNOVATIONS",
      "text" : "While the main focus of this paper is to elucidate fundamental design principles, doing so helped us to discover a few architectural innovations. In this section we will describe these innovations.\nFirst, we recommended combining summation/mean, concatenation, and maxout joining mechanisms with differing roles within a single architecture. Next, Design Pattern 2 to proliferate branches led us to modify the large scale sequential pattern of modules in the FractalNet architecture. Instead of lining up the modules for maximum depth, we arranges the modules in a fractal pattern as shown in 1b, which we named a Fractal of FractalNet (FoF) network. This architecture exchanges depth for a greater number of paths."
    }, {
      "heading" : "4.1.1 FREEZE-DROP-PATH AND STAGEWISE BOOSTING NETWORKS (SBN)",
      "text" : "Drop-path was introduced by Huang et al. (2016b). It works by randomly removing branches during an iteration of training, as though that path doesn’t exist in the network. Symmetry considerations led us to an opposite method that we named freeze-path. Instead of removing a branch from the network during training, we freeze the weights, as though the learning rate was set to zero. A similar idea has been proposed for recurrent neural networks (Krueger et al. 2016).\nThe combined usefulness of drop-path and freeze-path, which we name freeze-drop-path, is best explained in the non-stochastic case. Figure 1 shows an example of a fractal of FractalNet architecture. Let’s say we start training only the leftmost branch and use drop-path on all all of the other branches. This branch should train quickly since it has only a relatively few parameters compared to the entire network. Next we freeze the weights in that branch and allow the next branch on the right to be active. The leftmost branch is providing a good function approximation and the next branch is working on the corrective term. Since the next branch contains more layers than the previous and the corrective term should be easier to approximate than the original function, this branch should allow the network to attain greater accuracy. One can continue this process from left to right to train the entire network. We used freeze-drop-path as the final join in the FoF architecture in Figure 1b and named it Stagewise Boosting Networks (SBN) because they are analogous to stagewise boosting (Friedman et al. 2001). Boosting neural networks are not new (Schwenk & Bengio 2000) but this architecture is new. In Section B we will discuss the implementation we tested."
    }, {
      "heading" : "4.1.2 TAYLOR SERIES NETWORKS (TSN)",
      "text" : "Taylor series is a classic and well known function approximation method. The Taylor series expansion is f(x+ h) = f(x) + hf ′(x) + h2f ′′(x)/2 + ... (1) Since neural networks are also function approximators, it is a short leap from SBNs to consider the branches of that network as terms in a Taylor series expansion. This implies squaring the second branch before the summation joining unit, analogous to the second order term in the Taylor expansion. Similarly, the third branch would be cubed. We call this “Taylor Series Networks” (TSN) and there is precedence for polynomial networks (Livni et al. 2014) and multiplication in networks (e.g. Lin et al. 2015 in the literature. The implementation details of this TSN is similar to the SBN and discussed in the Appendix."
    }, {
      "heading" : "4.2 RESULTS",
      "text" : "The experiments in this section are primarily to empirically validate the architectural innovations described above but not to fully test them. We leave a more complete evaluation to future work.\nTable 1 compares the final test accuracy results for CIFAR-10 and CIFAR-100 in a number of experiments. An accuracy is computed as the mean of the last 6 test accuracies computed over the last 3,000 iterations (out of 100,000 iterations) of the training. The results from the original FractalNet (Larsson et al. 2016) are given in the first row of the table and we use as our baseline. Figure 2 compares the test accuracy results of the original FractalNet architectures to architectures with a few modifications advocated by design patterns. The first modification is to use concatenation instead of fractal-joins at all the downsampling locations in the networks. The results for both CIFAR-10 (2a) and CIFAR-100 (2b indicate that the results are equivalent when concatenation is used instead\nof fractal-joins at all the downsampling locations in the networks. The second experiment was to change the kernel sizes in the first module from 3x3 such that the left most column used a kernel size of 7x7, the second column 5x5, and the third used 3x3. The fractal-join for module one was replaced with Maxout. Here the results in Figure 2 are a bit worse, indicating that the more cooperative fractal-join (mean/summation) with 3x3 kernels has better performance than the competitive Maxout with multiple scales. Figure 2 also illustrates an experiment with replacing max pooling with average pooling throughout that architecture, which changes the training profile. Here the training plateaus and quickly lags behind the original FractalNet, but ends with a better final performance. This implies that average pooling can significantly reduce the length of the training and will be examined in future work. In addition, this provides some evidence that “cooperative” average pooling might be preferable to “competitive” max pooling.\nTable 1 and Figure 3 next compare the final test accuracy results for the architectural innovations described in Section 4.1. The final accuracy results shown in this table are a bit worse than the FractalNet baseline, but it is clear from both Figures 3a and 3b that the new architectures train more quickly than FractalNet. The FoF architecture ends with a similar final test accuracy as FractalNet\nbut the SBN and TSN architectures (which use freeze-drop-path) lag behind when the learning rate is dropped. This is more apparent for CIFAR-100 than for CIFAR-10, which implies that these architectures might be better suited for applications with larger numbers of classes. However, we are leaving the exploration of more suitable applications for these new architectures for future work."
    }, {
      "heading" : "5 CONCLUSION",
      "text" : "In this paper we describe convolutional neural network architectural design patterns that we discovered by studying the plethora of new architectures in recent deep learning papers. We hope these design patterns will be useful to both experienced practitioners looking to push the state-of-the-art and novice practitioners looking to apply deep learning to new applications. There exists a large expanse of potential follow up work, some of which we have indicated here as future work. Our effort here is primarily limited to Residual Networks for classification, but we hope this preliminary work will inspire others to follow up with new architectural design patterns for Recurrent Neural Networks, Deep Reinforcement Learning architectures, and beyond."
    }, {
      "heading" : "A RELATIONSHIPS BETWEEN RESIDUAL ARCHITECTURES",
      "text" : "The architectures mentioned in Section 2 commonly combine outputs from two or more layers using concatenation along the depth axis, element-wise summation, and element-wise average. We show that the latter two are special cases of the former with weight-sharing enforced. Likewise, we show that skip connections can be considered as introducing additional layers into a network that share parameters with existing layers. In this way, any of the residual network variants can be reformulated into a standard form where many of the variants are equivalent.\nA filter has three dimensions: two spatial dimensions, along which convolution occurs, and a third dimension, depth. Each input channel corresponds to a different depth for each filter of a layer. As a result, a filter can be considered to consist of “slices,” each of which is convolved over one input channel. The results of these convolutions are then added together, along with a bias, to produce a single output channel. The output channels of multiple filters are concatenated to produce the output of a single layer. When the outputs of several layers are concatenated, the behavior is similar to that of a single layer. However, instead of each filter having the same spatial dimensions, stride, and padding, each filter may have a different structure. As far as the function within a network, though, the two cases are the same. In fact, a standard layer, one where all filters have the same shape, can be considered a special case of concatenating outputs of multiple layer types.\nIf summation is used instead of concatenation, the network can be considered to perform concatenation but enforce weight-sharing in the following layer. The results of first summing several channels element-wise and then convolving a filter slice over the output yields the same result as convolving the slice over the channels and then performing an element-wise summation afterwards. Therefore, enforcing weight-sharing such that the filter slices applied to the nth channels of all inputs share weights results in behavior identical to summation, but in a form similar to concatenation, which highlights the relationship between the two. When Batch Normalization (BN) (Ioffe & Szegedy 2015 is used, as is the current standard practice, performing an average is essentially identical to performing a summation, since BN scales the output. Therefore, scaling the input by a constant (i.e., averaging instead of a summation) is rendered irrelevant. The details of architecture-specific manipulations of summations and averages is described further in Section 3.2.1.\nDue to the ability to express depth-wise concatenation, element-wise sum, and element-wise mean as variants of each other, architectural features of recent works can be combined within a single network, regardless of choice of combining operation. However, this is not to say that concatenation has the most expressivity and is therefore strictly better than the others. Summation allows networks to divide up the network’s task. Also, there is a trade-off between the number of parameters and the expressivity of a layer; summation uses weight-sharing to significantly reduce the number of parameters within a layer at the expense of some amount of expressivity.\nDifferent architectures can further be expressed in a similar fashion through changes in the connections themselves. A densely connected series of layers can be “pruned” to resemble any desired architecture with skip connections through zeroing specific filter slices. This operation removes the dependency of the output on a specific input channel; if this is done for all channels from a given layer, the connection between the two layers is severed. Likewise, densely connected layers can be turned into linearly connected layers while preserving the layer dependencies: a skip connection can be passed through the intermediate layers. A new filter can be introduced for each input channel passing through, where the filter performs the identity operation for the given input channel. All existing filters in the intermediate layers can have zeroed slices for this input so as to not introduce new dependencies. In this way, arbitrarily connected layers can be turned into a standard form.\nWe certainly do not recommend this representation for actual experimentation as it introduces fixed parameters. We merely describe it to illustrate the relationship between different architectures. This representation illustrates how skip connections effectively enforce specific weights in intermediate layers. Though this restriction reduces expressivity, the number of stored weights is reduced, the number of computations performed is decreased, and the network may be more easily trainable. However, it is not enough to introduce arbitrary restrictions to gain these benefits. In the following sections, we describe the patterns we have observed in how to properly reduce network complexity.\nB IMPLEMENTATION DETAILS\nOur implementations are in Caffe (Jia et al. 2014; downloaded October 9, 2016) using CUDA 8.0. These experiments were run on a 64 node cluster with 8 Nvidia Titan Black GPUs, 128 GB memory, and dual Intel Xenon E5-2620 v2 CPUs per node. We used the CIFAR-10 and CIFAR-100 datasets (Krizhevsky & Hinton 2009 for our classification tests. These datasets consist of 60,000 32x32 colour images (50,000 for training and 10,000 for testing) in 10 or 100 classes, respectively. Our Caffe code and prototxt files will be made publicly available upon this paper’s acceptance to ICLR.\nB.1 ARCHITECTURES\nWe started with the FractalNet implementation 1 as our baseline and it is described in Larsson et al. 2016. We used the three column module as shown in Figure 1a. In some of our experiments, we replaced the fractal-join with concatenation at the downsampling locations. In other experiments, we modified the kernel sizes and combined the branches with MaxOut. A FractalNet module is shown in Figure 1a and the architecture consists of five sequential modules.\nOur fractal of FractalNet architecture uses the same module but has an overall fractal design as in Figure 1a rather than the original sequential one. We limited our investigation to this one realization and left the study of other (possibly more complex) designs for future work. We followed the FractalNet implementation in regards to dropout where the dropout rate for a module were 0%, 10%, 20%, or 30%, depending on the depth of the module in the architecture. This choice for dropout rates were not found by experimentation and better values are possible. The local drop-path rate in the fractal-joins were fixed at 15%.\nFreeze-drop-path introduces four new parameters. The first is as to whether the active branch is chosen stochastically or deterministically. If it is stochastically, a random number is generated and the active branch is chosen based on which interval it falls in (intervals will be described shortly). If it is deterministically, a parameter is set by the user as to the number of iterations in one cycle through all the branches (we called this parameter num iter per cycle). In our Caffe implementation, the bottom input specified first is assigned as branch 1, the next is branch 2, then branch 3, etc. The next parameter indicates the proportion of iterations each branch should be active relative to all the other branches. The first type of interval uses the square of the branch number (i.e., 1, 4, 9, 16, ...) to assign the interval length for that branch to be active. This gives the more update iterations to the higher numbered branches. The next type gives the same amount of iterations to each branch. Our experiments showed that the first interval typed works better (as we expected) and was used to obtained the results in Section 4.\nThe Stagewise Boosting Network’s (SBN) architecture is the same as the fractal of FractalNet architecture except that branches 2 and 3 are combined with a fractal-join and then combined with branch 1 in a freeze-drop-path join. The reason for this came out of our first experiments; if branches 2 and 3 were separate, the performance deteriorated when branch 2 was frozen and branch 3 was active. In hindsight, this is due to the weights in the branch 2 path that are also in branch 3’s path being modified by the training. The Taylor series network is the same architecture as SBN with the addition of squaring the combined activations before the freeze-drop-path join.\nFor all of our experiments, we trained for 400 epochs. Since the training used 8 GPUs and each GPU had a batchsize of 25, this amounted to 100,000 iterations. We adopted the same learning rate as the FractalNet implementation, which started at 0.002 and dropped the learning rate by a factor of 10 at epochs 200, 300, and 350.\n1https://github.com/gustavla/fractalnet/tree/master/caffe"
    } ],
    "references" : [ {
      "title" : "Understanding intermediate layers using linear classifier probes",
      "author" : [ "Guillaume Alain", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1610.01644,",
      "citeRegEx" : "Alain and Bengio.,? \\Q2016\\E",
      "shortCiteRegEx" : "Alain and Bengio.",
      "year" : 2016
    }, {
      "title" : "The timeless way of building, volume 1",
      "author" : [ "Christopher Alexander" ],
      "venue" : null,
      "citeRegEx" : "Alexander.,? \\Q1979\\E",
      "shortCiteRegEx" : "Alexander.",
      "year" : 1979
    }, {
      "title" : "Xception: Deep learning with depthwise separable convolution",
      "author" : [ "François Chollet" ],
      "venue" : "arXiv preprint arXiv:1610.02357,",
      "citeRegEx" : "Chollet.,? \\Q2016\\E",
      "shortCiteRegEx" : "Chollet.",
      "year" : 2016
    }, {
      "title" : "The elements of statistical learning, volume 1. Springer series in statistics",
      "author" : [ "Jerome Friedman", "Trevor Hastie", "Robert Tibshirani" ],
      "venue" : null,
      "citeRegEx" : "Friedman et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Friedman et al\\.",
      "year" : 2001
    }, {
      "title" : "Design patterns: elements of reusable object-oriented software",
      "author" : [ "Erich Gamma", "Richard Helm", "Ralph Johnson", "John Vlissides" ],
      "venue" : "Pearson Education India,",
      "citeRegEx" : "Gamma et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Gamma et al\\.",
      "year" : 1995
    }, {
      "title" : "Noisy activation functions",
      "author" : [ "Caglar Gulcehre", "Marcin Moczulski", "Misha Denil", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1603.00391,",
      "citeRegEx" : "Gulcehre et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gulcehre et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep pyramidal residual networks",
      "author" : [ "Dongyoon Han", "Jiwhan Kim", "Junmo Kim" ],
      "venue" : "arXiv preprint arXiv:1610.02915,",
      "citeRegEx" : "Han et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "arXiv preprint arXiv:1512.03385,",
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Identity mappings in deep residual networks",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "arXiv preprint arXiv:1603.05027,",
      "citeRegEx" : "He et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Frankenstein: Learning deep face representations using small data",
      "author" : [ "Guosheng Hu", "Xiaojiang Peng", "Yongxin Yang", "Timothy Hospedales", "Jakob Verbeek" ],
      "venue" : "arXiv preprint arXiv:1603.06470,",
      "citeRegEx" : "Hu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2016
    }, {
      "title" : "Densely connected convolutional networks",
      "author" : [ "Gao Huang", "Zhuang Liu", "Kilian Q Weinberger" ],
      "venue" : "arXiv preprint arXiv:1608.06993,",
      "citeRegEx" : "Huang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep networks with stochastic depth",
      "author" : [ "Gao Huang", "Yu Sun", "Zhuang Liu", "Daniel Sedra", "Kilian Weinberger" ],
      "venue" : "arXiv preprint arXiv:1603.09382,",
      "citeRegEx" : "Huang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2016
    }, {
      "title" : "Decision forests, convolutional networks and the models in-between",
      "author" : [ "Yani Ioannou", "Duncan Robertson", "Darko Zikic", "Peter Kontschieder", "Jamie Shotton", "Matthew Brown", "Antonio Criminisi" ],
      "venue" : "arXiv preprint arXiv:1603.01250,",
      "citeRegEx" : "Ioannou et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ioannou et al\\.",
      "year" : 2016
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "Sergey Ioffe", "Christian Szegedy" ],
      "venue" : "arXiv preprint arXiv:1502.03167,",
      "citeRegEx" : "Ioffe and Szegedy.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ioffe and Szegedy.",
      "year" : 2015
    }, {
      "title" : "Caffe: Convolutional architecture for fast feature embedding",
      "author" : [ "Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell" ],
      "venue" : "In Proceedings of the 22nd ACM international conference on Multimedia,",
      "citeRegEx" : "Jia et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Jia et al\\.",
      "year" : 2014
    }, {
      "title" : "Driving in the matrix: Can virtual worlds replace human-generated annotations for real world tasks",
      "author" : [ "Matthew Johnson-Roberson", "Charles Barto", "Rounak Mehta", "Sharath Nittur Sridhar", "Ram Vasudevan" ],
      "venue" : null,
      "citeRegEx" : "Johnson.Roberson et al\\.,? \\Q1983\\E",
      "shortCiteRegEx" : "Johnson.Roberson et al\\.",
      "year" : 1983
    }, {
      "title" : "The unreasonable effectiveness of noisy data for fine-grained recognition",
      "author" : [ "Jonathan Krause", "Benjamin Sapp", "Andrew Howard", "Howard Zhou", "Alexander Toshev", "Tom Duerig", "James Philbin", "Li Fei-Fei" ],
      "venue" : "arXiv preprint arXiv:1511.06789,",
      "citeRegEx" : "Krause et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Krause et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning multiple layers of features from tiny images",
      "author" : [ "Alex Krizhevsky", "Geoffrey Hinton" ],
      "venue" : null,
      "citeRegEx" : "Krizhevsky and Hinton.,? \\Q2009\\E",
      "shortCiteRegEx" : "Krizhevsky and Hinton.",
      "year" : 2009
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton" ],
      "venue" : null,
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Zoneout: Regularizing rnns by randomly preserving hidden activations",
      "author" : [ "David Krueger", "Tegan Maharaj", "János Kramár", "Mohammad Pezeshki", "Nicolas Ballas", "Nan Rosemary Ke", "Anirudh Goyal", "Yoshua Bengio", "Hugo Larochelle", "Aaron Courville" ],
      "venue" : "arXiv preprint arXiv:1606.01305,",
      "citeRegEx" : "Krueger et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Krueger et al\\.",
      "year" : 2016
    }, {
      "title" : "Fractalnet: Ultra-deep neural networks without residuals",
      "author" : [ "Gustav Larsson", "Michael Maire", "Gregory Shakhnarovich" ],
      "venue" : "arXiv preprint arXiv:1605.07648,",
      "citeRegEx" : "Larsson et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Larsson et al\\.",
      "year" : 2016
    }, {
      "title" : "Streaming normalization: Towards simpler and more biologically-plausible normalizations for online and recurrent learning",
      "author" : [ "Qianli Liao", "Kenji Kawaguchi", "Tomaso Poggio" ],
      "venue" : "arXiv preprint arXiv:1610.06160,",
      "citeRegEx" : "Liao et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Liao et al\\.",
      "year" : 2016
    }, {
      "title" : "Competitive multi-scale convolution",
      "author" : [ "Zhibin Liao", "Gustavo Carneiro" ],
      "venue" : "arXiv preprint arXiv:1511.05635,",
      "citeRegEx" : "Liao and Carneiro.,? \\Q2015\\E",
      "shortCiteRegEx" : "Liao and Carneiro.",
      "year" : 2015
    }, {
      "title" : "Bilinear cnn models for fine-grained visual recognition",
      "author" : [ "Tsung-Yu Lin", "Aruni RoyChowdhury", "Subhransu Maji" ],
      "venue" : "In Proceedings of the IEEE International Conference on Computer Vision, pp",
      "citeRegEx" : "Lin et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2015
    }, {
      "title" : "On the computational efficiency of training neural networks",
      "author" : [ "Roi Livni", "Shai Shalev-Shwartz", "Ohad Shamir" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Livni et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Livni et al\\.",
      "year" : 2014
    }, {
      "title" : "Convolutional residual memory networks",
      "author" : [ "Joel Moniz", "Christopher Pal" ],
      "venue" : "arXiv preprint arXiv:1606.05262,",
      "citeRegEx" : "Moniz and Pal.,? \\Q2016\\E",
      "shortCiteRegEx" : "Moniz and Pal.",
      "year" : 2016
    }, {
      "title" : "Neural networks: tricks of the trade",
      "author" : [ "Genevieve B Orr", "Klaus-Robert Müller" ],
      "venue" : null,
      "citeRegEx" : "Orr and Müller.,? \\Q2003\\E",
      "shortCiteRegEx" : "Orr and Müller.",
      "year" : 2003
    }, {
      "title" : "Deconstructing the ladder network architecture",
      "author" : [ "Mohammad Pezeshki", "Linxi Fan", "Philemon Brakel", "Aaron Courville", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1511.06430,",
      "citeRegEx" : "Pezeshki et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Pezeshki et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep learning made easier by linear transformations in perceptrons",
      "author" : [ "Tapani Raiko", "Harri Valpola", "Yann LeCun" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Raiko et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Raiko et al\\.",
      "year" : 2012
    }, {
      "title" : "Semisupervised learning with ladder networks",
      "author" : [ "Antti Rasmus", "Mathias Berglund", "Mikko Honkala", "Harri Valpola", "Tapani Raiko" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Rasmus et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Rasmus et al\\.",
      "year" : 2015
    }, {
      "title" : "Data programming: Creating large training sets, quickly",
      "author" : [ "Alexander Ratner", "Christopher De Sa", "Sen Wu", "Daniel Selsam", "Christopher Ré" ],
      "venue" : "arXiv preprint arXiv:1605.07723,",
      "citeRegEx" : "Ratner et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ratner et al\\.",
      "year" : 2016
    }, {
      "title" : "Imagenet large scale visual recognition challenge",
      "author" : [ "Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein" ],
      "venue" : "International Journal of Computer Vision,",
      "citeRegEx" : "Russakovsky et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Russakovsky et al\\.",
      "year" : 2015
    }, {
      "title" : "Weight normalization: A simple reparameterization to accelerate training of deep neural networks",
      "author" : [ "Tim Salimans", "Diederik P Kingma" ],
      "venue" : "arXiv preprint arXiv:1602.07868,",
      "citeRegEx" : "Salimans and Kingma.,? \\Q2016\\E",
      "shortCiteRegEx" : "Salimans and Kingma.",
      "year" : 2016
    }, {
      "title" : "Convolutional neural fabrics",
      "author" : [ "Shreyas Saxena", "Jakob Verbeek" ],
      "venue" : "arXiv preprint arXiv:1606.02492,",
      "citeRegEx" : "Saxena and Verbeek.,? \\Q2016\\E",
      "shortCiteRegEx" : "Saxena and Verbeek.",
      "year" : 2016
    }, {
      "title" : "Boosting neural networks",
      "author" : [ "Holger Schwenk", "Yoshua Bengio" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Schwenk and Bengio.,? \\Q2000\\E",
      "shortCiteRegEx" : "Schwenk and Bengio.",
      "year" : 2000
    }, {
      "title" : "Weighted residuals for very deep networks",
      "author" : [ "Falong Shen", "Gang Zeng" ],
      "venue" : "arXiv preprint arXiv:1605.08831,",
      "citeRegEx" : "Shen and Zeng.,? \\Q2016\\E",
      "shortCiteRegEx" : "Shen and Zeng.",
      "year" : 2016
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "Karen Simonyan", "Andrew Zisserman" ],
      "venue" : "arXiv preprint arXiv:1409.1556,",
      "citeRegEx" : "Simonyan and Zisserman.,? \\Q2014\\E",
      "shortCiteRegEx" : "Simonyan and Zisserman.",
      "year" : 2014
    }, {
      "title" : "Swapout: Learning an ensemble of deep architectures",
      "author" : [ "Saurabh Singh", "Derek Hoiem", "David Forsyth" ],
      "venue" : "arXiv preprint arXiv:1605.06465,",
      "citeRegEx" : "Singh et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Singh et al\\.",
      "year" : 2016
    }, {
      "title" : "Gradual dropin of layers to train very deep neural networks",
      "author" : [ "Leslie N Smith", "Emily M Hand", "Timothy Doster" ],
      "venue" : "arXiv preprint arXiv:1511.06951,",
      "citeRegEx" : "Smith et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Smith et al\\.",
      "year" : 2015
    }, {
      "title" : "Gradual dropin of layers to train very deep neural networks",
      "author" : [ "Leslie N Smith", "Emily M Hand", "Timothy Doster" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Smith et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Smith et al\\.",
      "year" : 2016
    }, {
      "title" : "Striving for simplicity: The all convolutional net",
      "author" : [ "Jost Tobias Springenberg", "Alexey Dosovitskiy", "Thomas Brox", "Martin Riedmiller" ],
      "venue" : "arXiv preprint arXiv:1412.6806,",
      "citeRegEx" : "Springenberg et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Springenberg et al\\.",
      "year" : 2014
    }, {
      "title" : "Dropout: a simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Srivastava et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2014
    }, {
      "title" : "Training very deep networks",
      "author" : [ "Rupesh K Srivastava", "Klaus Greff", "Jürgen Schmidhuber" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Srivastava et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2015
    }, {
      "title" : "Understanding locally competitive networks",
      "author" : [ "Rupesh Kumar Srivastava", "Jonathan Masci", "Faustino Gomez", "Jürgen Schmidhuber" ],
      "venue" : "arXiv preprint arXiv:1410.1165,",
      "citeRegEx" : "Srivastava et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2014
    }, {
      "title" : "Going deeper with convolutions",
      "author" : [ "Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Szegedy et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2015
    }, {
      "title" : "Rethinking the inception architecture for computer vision",
      "author" : [ "Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jonathon Shlens", "Zbigniew Wojna" ],
      "venue" : "arXiv preprint arXiv:1512.00567,",
      "citeRegEx" : "Szegedy et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2015
    }, {
      "title" : "Inception-v4, inception-resnet and the impact of residual connections on learning",
      "author" : [ "Christian Szegedy", "Sergey Ioffe", "Vincent Vanhoucke" ],
      "venue" : "arXiv preprint arXiv:1602.07261,",
      "citeRegEx" : "Szegedy et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2016
    }, {
      "title" : "Resnet in resnet: Generalizing residual architectures",
      "author" : [ "Sasha Targ", "Diogo Almeida", "Kevin Lyman" ],
      "venue" : "arXiv preprint arXiv:1603.08029,",
      "citeRegEx" : "Targ et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Targ et al\\.",
      "year" : 2016
    }, {
      "title" : "Residual networks are exponential ensembles of relatively shallow networks",
      "author" : [ "Andreas Veit", "Michael Wilber", "Serge Belongie" ],
      "venue" : "arXiv preprint arXiv:1605.06431,",
      "citeRegEx" : "Veit et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Veit et al\\.",
      "year" : 2016
    }, {
      "title" : "Under review as a conference paper at ICLR",
      "author" : [ "Sebastien C Wong", "Adam Gatt", "Victor Stamatescu", "Mark D McDonnell" ],
      "venue" : null,
      "citeRegEx" : "Wong et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Wong et al\\.",
      "year" : 2017
    }, {
      "title" : "Residual networks",
      "author" : [ "Ke Zhang", "Miao Sun", "Tony X Han", "Xingfang Yuan", "Liru Guo", "Tao Liu" ],
      "venue" : null,
      "citeRegEx" : "Zhang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "Recently, there has been a large assortment of articles on new neural network architectures, especially regarding Residual Networks, such as He et al. (2015; 2016); Larsson et al. (2016); Zhang et al.",
      "startOffset" : 141,
      "endOffset" : 187
    }, {
      "referenceID" : 5,
      "context" : "Recently, there has been a large assortment of articles on new neural network architectures, especially regarding Residual Networks, such as He et al. (2015; 2016); Larsson et al. (2016); Zhang et al. (2016); Huang et al.",
      "startOffset" : 141,
      "endOffset" : 208
    }, {
      "referenceID" : 5,
      "context" : "Recently, there has been a large assortment of articles on new neural network architectures, especially regarding Residual Networks, such as He et al. (2015; 2016); Larsson et al. (2016); Zhang et al. (2016); Huang et al. (2016b). This has motivated us to take a high-level look at these architectures as a potential source for universal principles of design.",
      "startOffset" : 141,
      "endOffset" : 230
    }, {
      "referenceID" : 1,
      "context" : "We ask some fundamental questions: Do universal principles of deep network design exist? Can these principles be mined from the collective knowledge on deep learning? Which architectural choices work best in any given context? Which architectures or parts of architectures seem elegant? Design patterns were first described by Christopher Alexander (Alexander (1979)) in regards to the architectures of buildings and towns.",
      "startOffset" : 339,
      "endOffset" : 367
    }, {
      "referenceID" : 1,
      "context" : "We ask some fundamental questions: Do universal principles of deep network design exist? Can these principles be mined from the collective knowledge on deep learning? Which architectural choices work best in any given context? Which architectures or parts of architectures seem elegant? Design patterns were first described by Christopher Alexander (Alexander (1979)) in regards to the architectures of buildings and towns. Alexander wrote of a timeless quality in architecture that “lives” and this quality is enabled by building based on universal principles. The basis of design patterns is that they resolve a conflict of forces in a given context and lead to an equilibrium analogous to the ecological balance in nature. Design patterns are both highly specific, making them clear to follow, and flexible so they can be adapted to different environments and situations. Inspired by Alexander’s work, the “gang of four” (Gamma et al. (1995)) applied the concept of design patterns to the architecture of object-oriented software.",
      "startOffset" : 339,
      "endOffset" : 945
    }, {
      "referenceID" : 7,
      "context" : "Unfortunately, we cannot do justice to this body of work, so we instead focus on recent innovations in convolutional neural networks architectures and, in particular, on the family of Residual Network variants (He et al., 2015).",
      "startOffset" : 210,
      "endOffset" : 227
    }, {
      "referenceID" : 42,
      "context" : "introduced Highway Networks (Srivastava et al., 2015), which use a gating mechanism to decide whether to combine the input with the layer outputs, and showed how this allowed the training of very deep networks.",
      "startOffset" : 28,
      "endOffset" : 53
    }, {
      "referenceID" : 38,
      "context" : "The DropIn technique (Smith et al., 2015; 2016) describes both a gradual change and a stochastic method in allowing the input to skip layers in order to train very deep networks.",
      "startOffset" : 21,
      "endOffset" : 47
    }, {
      "referenceID" : 8,
      "context" : "The authors followed up with another paper (He et al., 2016) where they investigate why identity mappings help and report results for a network with more than a thousand layers.",
      "startOffset" : 43,
      "endOffset" : 60
    }, {
      "referenceID" : 46,
      "context" : "The Inception-v4 paper (Szegedy et al., 2016) describes the impact of residual connections on their Inception architecture and compared these results with the results from an updated Inception design.",
      "startOffset" : 23,
      "endOffset" : 45
    }, {
      "referenceID" : 47,
      "context" : "The Resnet in Resnet paper (Targ et al., 2016) suggests an architecture with connections in a layer.",
      "startOffset" : 27,
      "endOffset" : 46
    }, {
      "referenceID" : 20,
      "context" : "At the same time, the FractalNet paper (Larsson et al., 2016) demonstrated training deep networks with a symmetrically repeating architectural pattern.",
      "startOffset" : 39,
      "endOffset" : 61
    }, {
      "referenceID" : 37,
      "context" : "In Swapout (Singh et al., 2016), each layer can be dropped, skipped, used normally, or combined with a residual.",
      "startOffset" : 11,
      "endOffset" : 31
    }, {
      "referenceID" : 50,
      "context" : "For Residual of Residual Networks (Zhang et al., 2016), the authors propose adding a hierarchy of skip connections where the input can skip a layer, a module, or any number of modules.",
      "startOffset" : 34,
      "endOffset" : 54
    }, {
      "referenceID" : 7,
      "context" : "Unfortunately, we cannot do justice to this body of work, so we instead focus on recent innovations in convolutional neural networks architectures and, in particular, on the family of Residual Network variants (He et al., 2015). We start with the Network In Network (Lin et al., 2013), which describes a hierarchical network with a small network design repeatedly embedded in the overall architecture. Szegedy et al. (2015a) incorporated this idea into their Inception architecture.",
      "startOffset" : 211,
      "endOffset" : 425
    }, {
      "referenceID" : 7,
      "context" : "Unfortunately, we cannot do justice to this body of work, so we instead focus on recent innovations in convolutional neural networks architectures and, in particular, on the family of Residual Network variants (He et al., 2015). We start with the Network In Network (Lin et al., 2013), which describes a hierarchical network with a small network design repeatedly embedded in the overall architecture. Szegedy et al. (2015a) incorporated this idea into their Inception architecture. Later, these authors proposed modifications to the original inception design (Szegedy et al., 2015b). A similar concept was contained in the multi-scale convolution architecture (Liao & Carneiro, 2015). In the meantime, Batch Normalization (Ioffe & Szegedy, 2015) was presented as a unit within the network that makes training the network faster and easier. Prior to the introduction of residual networks there were a few papers suggesting skip connections. Skip connections were proposed by Raiko et al. (2012). Srivastava, et al.",
      "startOffset" : 211,
      "endOffset" : 995
    }, {
      "referenceID" : 7,
      "context" : "Unfortunately, we cannot do justice to this body of work, so we instead focus on recent innovations in convolutional neural networks architectures and, in particular, on the family of Residual Network variants (He et al., 2015). We start with the Network In Network (Lin et al., 2013), which describes a hierarchical network with a small network design repeatedly embedded in the overall architecture. Szegedy et al. (2015a) incorporated this idea into their Inception architecture. Later, these authors proposed modifications to the original inception design (Szegedy et al., 2015b). A similar concept was contained in the multi-scale convolution architecture (Liao & Carneiro, 2015). In the meantime, Batch Normalization (Ioffe & Szegedy, 2015) was presented as a unit within the network that makes training the network faster and easier. Prior to the introduction of residual networks there were a few papers suggesting skip connections. Skip connections were proposed by Raiko et al. (2012). Srivastava, et al. introduced Highway Networks (Srivastava et al., 2015), which use a gating mechanism to decide whether to combine the input with the layer outputs, and showed how this allowed the training of very deep networks. The DropIn technique (Smith et al., 2015; 2016) describes both a gradual change and a stochastic method in allowing the input to skip layers in order to train very deep networks. The concept of stochastic depth via a drop-path method was introduced by Huang et al. (2016b). Residual Networks were introduced by He et al.",
      "startOffset" : 211,
      "endOffset" : 1499
    }, {
      "referenceID" : 7,
      "context" : "Unfortunately, we cannot do justice to this body of work, so we instead focus on recent innovations in convolutional neural networks architectures and, in particular, on the family of Residual Network variants (He et al., 2015). We start with the Network In Network (Lin et al., 2013), which describes a hierarchical network with a small network design repeatedly embedded in the overall architecture. Szegedy et al. (2015a) incorporated this idea into their Inception architecture. Later, these authors proposed modifications to the original inception design (Szegedy et al., 2015b). A similar concept was contained in the multi-scale convolution architecture (Liao & Carneiro, 2015). In the meantime, Batch Normalization (Ioffe & Szegedy, 2015) was presented as a unit within the network that makes training the network faster and easier. Prior to the introduction of residual networks there were a few papers suggesting skip connections. Skip connections were proposed by Raiko et al. (2012). Srivastava, et al. introduced Highway Networks (Srivastava et al., 2015), which use a gating mechanism to decide whether to combine the input with the layer outputs, and showed how this allowed the training of very deep networks. The DropIn technique (Smith et al., 2015; 2016) describes both a gradual change and a stochastic method in allowing the input to skip layers in order to train very deep networks. The concept of stochastic depth via a drop-path method was introduced by Huang et al. (2016b). Residual Networks were introduced by He et al. (2015), which described their network that won the ImageNet Challenge.",
      "startOffset" : 211,
      "endOffset" : 1554
    }, {
      "referenceID" : 7,
      "context" : "Unfortunately, we cannot do justice to this body of work, so we instead focus on recent innovations in convolutional neural networks architectures and, in particular, on the family of Residual Network variants (He et al., 2015). We start with the Network In Network (Lin et al., 2013), which describes a hierarchical network with a small network design repeatedly embedded in the overall architecture. Szegedy et al. (2015a) incorporated this idea into their Inception architecture. Later, these authors proposed modifications to the original inception design (Szegedy et al., 2015b). A similar concept was contained in the multi-scale convolution architecture (Liao & Carneiro, 2015). In the meantime, Batch Normalization (Ioffe & Szegedy, 2015) was presented as a unit within the network that makes training the network faster and easier. Prior to the introduction of residual networks there were a few papers suggesting skip connections. Skip connections were proposed by Raiko et al. (2012). Srivastava, et al. introduced Highway Networks (Srivastava et al., 2015), which use a gating mechanism to decide whether to combine the input with the layer outputs, and showed how this allowed the training of very deep networks. The DropIn technique (Smith et al., 2015; 2016) describes both a gradual change and a stochastic method in allowing the input to skip layers in order to train very deep networks. The concept of stochastic depth via a drop-path method was introduced by Huang et al. (2016b). Residual Networks were introduced by He et al. (2015), which described their network that won the ImageNet Challenge. They were able to extend the depth of a network from tens to hundreds of layers and in doing so, improve the network’s performance. The authors followed up with another paper (He et al., 2016) where they investigate why identity mappings help and report results for a network with more than a thousand layers. The research community took notice of this architecture and several modifications to the original design were soon proposed. The Inception-v4 paper (Szegedy et al., 2016) describes the impact of residual connections on their Inception architecture and compared these results with the results from an updated Inception design. The Resnet in Resnet paper (Targ et al., 2016) suggests an architecture with connections in a layer. Veit et al. (2016) provided an understanding of Residual Networks as an ensemble of relatively shallow networks.",
      "startOffset" : 211,
      "endOffset" : 2374
    }, {
      "referenceID" : 44,
      "context" : "The closest to this work is probably Szegedy et al. (2015b) where the authors describe a few design principles based on their experiences.",
      "startOffset" : 37,
      "endOffset" : 60
    }, {
      "referenceID" : 31,
      "context" : "Several researchers have pointed out that the winners of the ImageNet Challenge (Russakovsky et al., 2015) have successively used deeper networks (as seen in, Krizhevsky et al.",
      "startOffset" : 80,
      "endOffset" : 106
    }, {
      "referenceID" : 20,
      "context" : "Recent examples include FractalNet (Larsson et al. 2016), Xception (Chollet 2016), and Decision Forest Convolutional Networks (Ioannou et al.",
      "startOffset" : 35,
      "endOffset" : 56
    }, {
      "referenceID" : 12,
      "context" : "2016), Xception (Chollet 2016), and Decision Forest Convolutional Networks (Ioannou et al. 2016).",
      "startOffset" : 75,
      "endOffset" : 96
    }, {
      "referenceID" : 40,
      "context" : "Simplicity was exemplified in the paper ”Striving for Simplicity” (Springenberg et al. 2014) by achieving state-of-the-art results with fewer types of units.",
      "startOffset" : 66,
      "endOffset" : 92
    }, {
      "referenceID" : 20,
      "context" : "We also noted a special degree of elegance in the FractalNet (Larsson et al. 2016) design, which we attributed to the symmetry of its structure.",
      "startOffset" : 61,
      "endOffset" : 82
    }, {
      "referenceID" : 8,
      "context" : ", 2015) have successively used deeper networks (as seen in, Krizhevsky et al. (2012), Szegedy et al.",
      "startOffset" : 60,
      "endOffset" : 85
    }, {
      "referenceID" : 8,
      "context" : ", 2015) have successively used deeper networks (as seen in, Krizhevsky et al. (2012), Szegedy et al. (2015a), Simonyan & Zisserman (2014), He et al.",
      "startOffset" : 60,
      "endOffset" : 109
    }, {
      "referenceID" : 8,
      "context" : ", 2015) have successively used deeper networks (as seen in, Krizhevsky et al. (2012), Szegedy et al. (2015a), Simonyan & Zisserman (2014), He et al.",
      "startOffset" : 60,
      "endOffset" : 138
    }, {
      "referenceID" : 4,
      "context" : "(2015a), Simonyan & Zisserman (2014), He et al. (2015)).",
      "startOffset" : 38,
      "endOffset" : 55
    }, {
      "referenceID" : 4,
      "context" : "(2015a), Simonyan & Zisserman (2014), He et al. (2015)). It is also apparent from the ImageNet Challenge that multiplying the number of paths through the network is a recent trend; a trend that is apparent when looking from Alexnet to Inception to ResNets. For example, Veit et al. (2016) show that ResNets can be considered to be an exponential ensemble of networks with different lengths.",
      "startOffset" : 38,
      "endOffset" : 289
    }, {
      "referenceID" : 2,
      "context" : "2016), Xception (Chollet 2016), and Decision Forest Convolutional Networks (Ioannou et al. 2016). We even will go so far as to predict that the winner of this year’s ImageNet Challenge will do so by increasing the number of branches in their architecture rather than continuing to increase the depth. Scientists have embraced simplicity/parsimony for centuries. Simplicity was exemplified in the paper ”Striving for Simplicity” (Springenberg et al. 2014) by achieving state-of-the-art results with fewer types of units. We add this as Design Pattern 3: Strive for Simplicity; to use fewer types of layers in order to keep the network as simple as possible. We also noted a special degree of elegance in the FractalNet (Larsson et al. 2016) design, which we attributed to the symmetry of its structure. Architectural symmetry is typically considered a sign of beauty and quality, so we add it here as Design Pattern 4: Increase Symmetry. In addition to its symmetry, FractalNets also adheres to the Proliferate Paths design pattern so it is the baseline of our experiments in Section 4. Examination of trade-offs in order to understand the relevant forces is an essential element of design patterns. One fundamental trade-off is to maximize representational power versus compression of redundant and non-discriminating information. It is universal in all convolutional neural networks, from the data to the final convolutional layer, that the activations are downsampled and the number of channels increased. As is exemplified in Deep Pyramidal Residual Networks (Han et al. (2016)), which leads to Design Pattern 5: Pyramid Shape where there should be an overall smooth downsampling throughout the architecture and the downsampling should be combined with an increase in the number of channels.",
      "startOffset" : 17,
      "endOffset" : 1581
    }, {
      "referenceID" : 21,
      "context" : "The Batch Normalization (Ioffe & Szegedy 2015) paper attributes the benefits to handling internal covariate shift, while the authors of streaming normalization (Liao et al. 2016) express that it might be otherwise.",
      "startOffset" : 160,
      "endOffset" : 178
    }, {
      "referenceID" : 10,
      "context" : "The difference between sum and fractal-join (mean) is best understood by considering drop-path (Huang et al. 2016b). In a residual network where the input skip connection is always present, summation causes the convolutional layers to learn the residual (the difference from the input). On the other hand, in networks with several branches where any branch can be dropped, such as FractalNet (Larsson et al. (2016)), using the mean is preferable as it keeps the output smooth as branches are randomly dropped.",
      "startOffset" : 96,
      "endOffset" : 415
    }, {
      "referenceID" : 10,
      "context" : "The difference between sum and fractal-join (mean) is best understood by considering drop-path (Huang et al. 2016b). In a residual network where the input skip connection is always present, summation causes the convolutional layers to learn the residual (the difference from the input). On the other hand, in networks with several branches where any branch can be dropped, such as FractalNet (Larsson et al. (2016)), using the mean is preferable as it keeps the output smooth as branches are randomly dropped. Some researchers seem to prefer concatenation ( e.g., Szegedy et al. (2015a)).",
      "startOffset" : 96,
      "endOffset" : 587
    }, {
      "referenceID" : 41,
      "context" : "Maxout has been used for competition, as in locally competitive networks (Srivastava et al. 2014b) and competitive multi-scale networks Liao & Carneiro (2015). Maxout chooses only one of the activations and leads to Design Pattern 14: MaxOut for Competition.",
      "startOffset" : 74,
      "endOffset" : 159
    }, {
      "referenceID" : 19,
      "context" : "A similar idea has been proposed for recurrent neural networks (Krueger et al. 2016).",
      "startOffset" : 63,
      "endOffset" : 84
    }, {
      "referenceID" : 3,
      "context" : "We used freeze-drop-path as the final join in the FoF architecture in Figure 1b and named it Stagewise Boosting Networks (SBN) because they are analogous to stagewise boosting (Friedman et al. 2001).",
      "startOffset" : 176,
      "endOffset" : 198
    }, {
      "referenceID" : 9,
      "context" : "Drop-path was introduced by Huang et al. (2016b). It works by randomly removing branches during an iteration of training, as though that path doesn’t exist in the network.",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 24,
      "context" : "We call this “Taylor Series Networks” (TSN) and there is precedence for polynomial networks (Livni et al. 2014) and multiplication in networks (e.",
      "startOffset" : 92,
      "endOffset" : 111
    }, {
      "referenceID" : 20,
      "context" : "The results from the original FractalNet (Larsson et al. 2016) are given in the first row of the table and we use as our baseline.",
      "startOffset" : 41,
      "endOffset" : 62
    } ],
    "year" : 2016,
    "abstractText" : "Recent research in the deep learning field has produced a plethora of new architectures. At the same time, a growing number of groups are applying deep learning to new applications and problems. Many of these groups might be composed of inexperienced deep learning practitioners who are baffled by the dizzying array of architecture choices and therefore use an older architecture, such as Alexnet. Here, we are attempting to bridge this gap by mining the collective knowledge contained in recent deep learning research to discover underlying principles for designing neural network architectures. In addition, we describe several architectural innovations, including Fractal of FractalNet, Stagewise Boosting Networks, and Taylor Series Networks (our Caffe code and prototxt files will be made publicly available upon acceptance to ICLR). We hope others are inspired to build on this preliminary work.",
    "creator" : "LaTeX with hyperref package"
  }
}