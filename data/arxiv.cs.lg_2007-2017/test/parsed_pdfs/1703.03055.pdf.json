{
  "name" : "1703.03055.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Interpretable Structure-Evolving LSTM",
    "authors" : [ "Xiaodan Liang", "Liang Lin", "Xiaohui Shen", "Jiashi Feng", "Shuicheng Yan", "Eric P. Xing" ],
    "emails" : [ "xiaodan1@cs.cmu.edu,", "linliang@ieee.org,", "xshen@adobe.com,", "elefjia@nus.edu.sg,", "eleyans@nus.edu.sg,", "epxing@cs.cmu.edu" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Recently, there has been a surge of interest in developing various kinds of Long Short-Term Memory (LSTM) neural networks for modeling complex dependencies within sequential and multi-dimensional data, due to their advan-\ntage in a wide range of applications such as speech recognition [9], image generation [27], image-to-caption generation [31] and multi-dimensional image processing [14].\nDespite the remarkable success, existing LSTM models such as chain-structured [9] [31], tree-structured LSTM models [35, 24] and graph-structured LSTM [16] can only process data with pre-fixed structures in terms of their internal information propagation route. They are therefore limited in dealing with the data containing complex multilevel correlations. For example, the structure of human social network is inherently hierarchical, where each individual is a member of several communities, ranging from small (e.g., families, friends) to large (e.g., organizations such as schools and businesses). Semantic object parsing in an image, for another example, can benefit from modeling the contextual dependencies among regions in different levels, where the lower-level graph representation on small regions (e.g., superpixels) can preserve the local and fine object boundaries while the higher-level graph on larger coherent regions captures more semantic interactions. Thus, in order to well abstract multi-level representations of such data, it is desirable to integrate the data structure evolving with LSTM parameter learning.\nIn this work, we seek a general and interpretable framework for representing the data via LSTM networks over the dynamically learned multi-level data structures, in which hierarchical intrinsic representations are simultaneously learned from the data along with encoding the longterm dependencies via LSTM units. Since numerous important problems can be framed as learning from graph data (tree-structure can be treated as one specific graph), our structure-evolving directly investigates the hierarchical representation learning over the initial arbitrary graph structures. However, learning dynamic hierarchical graphs is much more challenging than the convenient hierarchical convolution neural networks due to the arbitrary number of nodes, orderless node layouts and diverse probabilistic graph edges. To learn intermediate interpretable graph structures of the data and alleviate the over-fitting problem, we design a stochastic algorithm to sample the graph\n1\nar X\niv :1\n70 3.\n03 05\n5v 1\n[ cs\n.C V\n] 8\nM ar\n2 01\nstructure (i.e., the grouping of graph nodes) in each LSTM layer and gradually build the multi-level graph representations in a bottom-up manner. We thus name our model as the structure-evolving LSTM. Compared with existing LSTM structures with pre-fixed chain [9] [31], tree [35, 24] or graph topologies [16], the structure-evolving LSTM has the capability of modeling long-range interactions using the dynamically evolved hierarchical graph topologies to capture the multi-level inherent correlations embedded in the data.\nAs illustrated in Fig. 1, the structure-evolving LSTM gradually evolves the multi-level graph representations through a stochastic and bottom-up node merging process, starting with an initial graph in which each node indicates a data element and every two neighboring nodes are linked by an edge. To enable learn the interpretable hierarchical representation, we propose to progressively merge different graph nodes guided by the global advantage reward at each step. The new graph that is composed by the merged graph nodes and updated graph edges is thus generated by a stochastic policy that ensures not only the less overhead graph transition from the previous graph to the new graph and the advantage discriminative capability brought by the new graph.\nSpecifically, for two connected nodes, their merging probability is estimated from the adaptive forget gate outputs in the LSTM unit, indicating how likely the two nodes tend to be merged into a clique (i.e., a node at the higher level graph). Then the graph structure is generated by designing a Metropolis-Hasting algorithm [2, 26]. Specifically, this algorithm stochastically merging some graph nodes by sampling their merging probabilities, and produces a new graph structure (i.e., a set of partitioned cliques). This structure is further examined and determined according to a global reward defined as an acceptance probability. Under such a stochastic sampling paradigm, the acceptance probability involves two terms: i) a state transition probability (i.e., a product of the merging probabilities); ii) a posterior probability representing the compatibility of the\ngenerated graph structure with task-specific observations. Intuitively, this global reward thus encourages the structureevolving step that better not leads to a hugh graph shift (i.e., only very few edges are merges) and also can help boost the target-specific performance.\nOnce a new level of graph structure is evolved, the LSTM layer broadcasts information along the generated graph topology following a stochastic updating scheme, in order to enable global reasoning on all nodes. In turn, the updated LSTM gate outputs induce the merging probability of graph nodes for the subsequent graph structure evolving. Instead of being influenced equally by all of its neighboring nodes in each LSTM unit, our model learns the adaptive forget gates for each neighboring node when updating the hidden states of a certain node. Such an adaptive scheme has advantage in conveying semantically meaningful interactions between two graph nodes. The network parameters are then updated by back-propagation in an end-to-end way.\nWe leverage the structure-evolving LSTM model to address the fundamental semantic object parsing task and experimentally show that structure-evolving LSTM outperforms other state-of-the-art LSTM structures on three object parsing datasets."
    }, {
      "heading" : "2. Related Works",
      "text" : "Long Short-Term Memory (LSTM) recurrent networks have been first introduced to address the sequential prediction tasks [10, 23, 31, 13], and then extended to multidimensional image processing tasks [4, 25] such as image generation [27, 25], person detection [22], scene labeling [3] and object parsing [17]. It can keep long-term memory by training proper gating weights and has practically showed the effectiveness on a range of problems [4, 3]. For image processing, in each LSTM unit, the prediction of each pixel is designed to affected by a fixed factorization (e.g., 2 or 8 neighboring pixels [14][8][17] or diagonal neighborhood [27][25]). Recently, Tree-LSTM [24] introduces the structure with tree-structured topologies for predicting semantic representations of sentences. Graph\nLSTM [16] has been proposed to propagate information on a basic pre-defined graph topology to capture diverse natural visual correlations (e.g., local boundaries and homogeneous regions). However, the complex patterns in different modalities often embed hierarchal structures, representing different levels of correlations between nodes. Different from using pre-defined data structures in previous LSTMs [14, 8, 17, 16, 25], the proposed structureevolving LSTM targets on automatically learning the hierarchical graph representations by evolving from an initial graph structure. In this way, the intrinsic multi-level semantic abstractions can be learned and then used to boost the multi-scale reasoning by LSTM units.\nThe structure-evolving LSTM (dynamically evolving multi-level graphs) is superior to the most related Graph LSTM [16] (a pre-fixed single-level graph) in two aspects: 1) Structure-evolving LSTM learns more powerful representations as it progressively exploits hierarchical information along stacked LSTM layers; 2) at its later layers, the structure-evolving LSTM captures the inherent structure of the desired output benefiting from the higher-level graph topologies. These superiorities bring significant improvements on several semantic parsing datasets, which gives apple-to-apple comparison with [16]. Our work aims to develop a new and general principled graph evolving based learning method to learn more powerful Graph LSTM or other RNN models. Devising new Graph-LSTM unit is not within the scope of this paper. We use Graph-LSTM as a running example which by no means implies our method is limited to Graph LSTM."
    }, {
      "heading" : "3. Structure-evolving LSTM",
      "text" : "Fig. 1 illustrates the proposed structure-evolving LSTM network architecture. Suppose that the initialized graph for the data is denoted as G(0) =< V (0), E(0) >, where V (0) and E(0) are the corresponding graph nodes (e.g., data elements) and edges. Each node v0i ∈ V (0), {i ∈ 1, · · · , N0}\nis represented by the deep features f (0)i learned from the underlying CNN model with D dimensions. Based on the LSTM gate outputs and the graph G(t) in the previous t-th LSTM layer, structure-evolving LSTM then learns a higherlevel graph structure G(t+1) =< V (t+1), E(t+1) > for the information propagation in the (t + 1)-th LSTM layer. Learning new graph structures and updating LSTM parameters are thus alternatively performed and the network parameters are trained in an end-to-end way."
    }, {
      "heading" : "3.1. Basic LSTM Units",
      "text" : "Given the dynamically constructed graph structure G(t), the t-th structure-evolving LSTM layer determines the states of each node vti that comprise the hidden states h t i and the memory states mti of each node, and the edge probability ptij of two nodes for evolving a new graph structure. The state of each node is influenced by its previous states and the states of connected graph nodes in order to propagate information to all nodes. Thus the inputs to LSTM units consist of the input states f ti of the node v t i , its previous hidden states h(t−1)i and memory states m (t−1) i , and the hidden and memory states of its neighboring nodes vtj , j ∈ NG(t)(i). Note that there is a flexibility in the order of node updating in the structure-evolving LSTM layers. Following [16], we randomly specify the node updating sequence to propagate information to all nodes in order to increase the model diversity during learning the LSTM network parameters.\nOur structure-evolving LSTM follows the Graph LSTM units [16] to generate hidden and memory cells, and then show how to inject the edge merging probabilities of the nodes into the LSTM units. We thus first introduce the generation of hidden and memory cells to make this paper more self-contained. When operating on a specific node vti , some of its neighboring nodes have already been updated while others may not. We therefore use a visit flag qtj to indicate whether the graph node vtj has been updated, where q t j is set as 1 if updated and otherwise 0. We then use the updated hidden states htj for the visited nodes and the previous states\nht−1j for the unvisited nodes. Note that the nodes in the graph may have an arbitrary number of neighboring nodes. LetNG(t)(i) denote the number of neighboring graph nodes for the node i. To obtain a fixed feature dimension for the inputs of the Graph LSTM unit during network training, the hidden states h̄t−1i used for computing the LSTM gates of the node vti are obtained by averaging the hidden states of neighboring nodes, computed as:\nh̄t−1i =\n∑ j∈N\nG(t) (i)(1(q\nt j = 1)h t j + 1(q t j = 0)h t−1 j ) |NG(t)(i)| . (1)\nStructure-evolving LSTM. The structure-evolving LSTM consists of five gates: the input gate gu, the forget gate gf , the adaptive forget gate ḡf , the memory gate gc, the output gate go and the edge gate p. The 1 is an indicator function. The W e indicates the recurrent edge gate weight parameters. The Wu,W f ,W c,W o are the recurrent gate weight matrices specified for input features while Uu, Uf , U c, Uo are those for hidden states of each node. Uun, Ufn, U cn, Uon are the weight parameters specified for states of neighboring nodes. The structureevolving LSTM unit specifies different forget gates for different neighboring nodes by functioning the input states of the current node with their hidden states, defined as ḡfij , j ∈ NG(t)(i). It results in the different influences of neighboring nodes on the updated memory states mt+1i and hidden states ht+1i . The merging probability pij of each pair of graph nodes < i, j >∈ E(t) is calculated by weighting the adaptive forget gates ḡfij with the weight matrix W e. Intuitively, adaptive forget gates are to identify the distinguished correlations of different node pairs, e.g. some nodes have stronger correlations than others. The merging probability for each pair is thus estimated from adaptive forget gates for graph evolving. The new hidden states, memory states and edge gates (i.e., merging probabilities of each connected pair of nodes) in the graphG(t) can be calculated as follows:\ngui =δ(W uf ti + U uht−1i + U unh̄t−1i + b u), ḡfij =δ(W f f ti + U fnht−1j + b f ), gfi =δ(W f f ti + U fht−1i + b f ),\ngoi =δ(W of ti + U oht−1i + U onh̄t−1i + b o), gci = tanh(W cf ti + U cht−1i + U cnh̄t−1i + b c),\nmi,t =\n∑ j∈NG(i) (1(qj = 1)ḡ f ij m t j + 1(qj = 0)ḡ f ij m t−1 j ) |NG(t)(i)| + gfi m t−1 i + g u i gci ,\nhti = tanh(g o i mti)\nptij =δ(W eḡfij).\n(2)\nHere δ is a logistic sigmoid function, and indicates a point-wise product. Let W,U denote the concatenation of all weight matrices and {Zj,t}j∈NG(i) represent all the related information of neighboring nodes. This mechanism acts as a memory system, where the information can be written into the memory states and sequentially recorded by each graph node, which is then used to communicate with the hidden states of subsequent graph nodes and previous LSTM layer. And the merging probabilities {pij}, < i, j >∈ E(t) can be conveniently learned and used for generating the new higher-level graph structure G(t+1) in the (t+1)-th layer, detailed in Section 3.2. During training, the merging probabilities of graph edges are supervised by approximating to the final graph structure for a specific task, such as the connections of final semantic regions for image parsing. The back propagation is used to train all the weight metrics."
    }, {
      "heading" : "3.2. Interpretable Structure Evolving",
      "text" : "Given the graph structure G(t) =< V (t), E(t) > and all merging probabilities {pij}, < i, j >∈ E(t), the higherlevel graph structure G(t+1) can be evolved by stochastically merging some graph nodes and examined with an acceptance probability, as shown in Fig. 2. Specifically, a new graph node G(t+1) is constructed by merging some graph nodes with the merging probability. As there is no deterministic graph transition path from an initial graph to the final one, it is intractable to enumerate all possible G(t+1) for evaluation within the large search space. We thus use a stochastic mechanism rather than a deterministic one to find a good graph transition. Such a stochastic searching scheme is also effective in alleviating the risk of getting trapped in a bad local optimum. To find a better graph transition between two graphs G(t) and G(t+1), the acceptance rate of the transition from the graph from G(t) to graph G(t+1) is defined by a Metropolis-Hastings method [2, 26]:\nα(G(t) → G(t+1)) = min(1,\nq(G(t+1)→G (t) )\nq(G(t)→G(t+1)) P (G(t+1)|I; W,U) P (G(t)|I; W,U) ). (3)\nwhere q(G(t+1)→G (t) ) and q(G(t)→G (t+1)\n) denote the graph state transition probability from one graph to another, and P (G(t+1)|I; W,U) and P (G(t)|I; W,U) denote the posterior probability of the graph structure G(t+1) and G(t), respectively. Typically, P (G(t)|I; W,U) is assumed to follow a Gibbs distribution 1 Z exp(−L(F (I,G\n(t),W,U), Y )), where Z is the partition function, F (I,G(t),W,U) is the network prediction, Y is the task-specific target and L(·) is the corresponding loss function. For example, Y can be the segmentation groundtruth and L(·) is the pixel-wise cross-entropy loss\nfor the image parsing task. The model is more likely to accept a new graph structure G(t+1) that can bring more significant performance improvement indicated by P (G(t+1)|I;W,U) P (G(t)|I;W,U) . The graph state transition probability ratio is computed by:\nq(G(t+1)→G (t) ) q(G(t)→G(t+1)) ∝ ∏ <i,j>∈E(t+1)(1− (1− ptij))∏ <i,j>∈E(t)(1− (1− ptij))\n= ∏\n<i,j>∈E(t)\\E(t+1) ptij .\n(4)\nThe state transition probability is thus calculated by multiplying all merging probabilities of eliminated edges in G(t). It implies that the graph nodes with larger merging probabilities {ptij} of G(t) are more encouraged to be merged in G(t+1). During testing, the acceptance rate is only determined by the graph state transition probability in Eqn. 4. To enable finish the graph structure exploration within a specified time schedule in each step, we can empirically set the upper bound for the sampling trials, say 50 in our experiments.\nIn the (t + 1)-th structure-evolving LSTM layer, the information propagation is performed on all nodes with a stochastic node updating sequence along the new graph topology G(t+1) =< V (t+1), E(t+1) >. The input states f t+1i for each node v t+1 i ∈ V t+1 are produced by averaging those of all corresponding merged nodes in G(t). Similarly, the hidden and memory states of vt+1i are averaged and used for further updating. The weight matrices of the structureevolving LSTM units are shared for all stacked layers with generated hierarchical graph representations, which helps improve the capability of the network parameters in sensing multi-level semantic abstractions. The final loss for training structure-evolving LSTM includes the final task-related\nprediction loss and the loss on the predicted merging probabilities for all layers."
    }, {
      "heading" : "4. Experiments",
      "text" : "The proposed structure-evolving LSTM aims to provide a principled framework to dynamically learn the hierarchal data structures, which is applicable for kinds of tasks (e.g., nature language understanding and image content understanding). However, among all these applications, the semantic object parsing task that requires to produce the pixel-wise labeling by considering the complex interactions between different pixels, superpixels or parts, is a perfect match to better evaluate the structure generation capability of our structure-evolving LSTM. Our dynamically evolved hierarchical graph structures can effectively capture the multi-level and diverse contextual dependencies. We thus evaluate the effectiveness of the proposed structureevolving LSTM model on the semantic object parsing task (i.e., segmenting an object in the image into its semantic parts) where exploiting multi-level graph representations for the image content is very natural and useful for the final parsing result."
    }, {
      "heading" : "4.1. Semantic Object Parsing Task",
      "text" : "We take the object parsing task as our application scenario, which aims to generate pixel-wise semantic part segmentation for each image, as shown in Fig. 3. The initial graph G(0) is constructed on superpixels that are obtained through image over-segmentation using SLIC [1] following [16]. Each superpixel indicates one graph node and each graph edge connects two spatially neighboring superpixel nodes. The input image first passes through a stack of convolutional layers to generatt convolutional feature maps. The input features f0i of each graph node vi are computed by averaging the convolutional features of all\nthe pixels belonging to the same superpixel node vi. Five structure-evolving LSTM layers are then stacked to learn multi-level graph representations by stochastically grouping some nodes into a large node with the coherent semantic meanings through a bottom-up process.\nTo make sure that the number of the input states for the first LSTM layer is compatible with that of the following layers, the dimensions of hidden and memory states in all LSTM layers are set the same as the feature dimension of the last convolutional layer before the LSTM stack. After that, one prediction layer with several 1× 1 convolution filters produces confidence maps for all labels. During training, we use the groundtruth semantic edge map defined over all the superpixels to supervise the prediction of merging probabilities of all the edges in each LSTM layer. Specifically, the ground-truth merging probability of two graph nodes is set as 1 only if they belong to the same semantic label. L2-norm loss is employed for the back-propagation. The cross-entropy loss is employed on all the predictions layers to produce the final parsing result."
    }, {
      "heading" : "4.2. Datasets and Implementation Details",
      "text" : "Dataset: We validate the effectiveness of the structureevolving LSTM on three challenging image parsing datasets. The PASCAL-Person-part dataset [7] concentrates on the human part segmentation on images from PASCAL VOC 2010. Its semantic labels consist of Head, Torso, Upper/Lower Arms, Upper/Lower Legs, and one background class. 1,716 images are used for training and 1,817 for testing. The Horse-Cow parsing dataset is a part segmentation benchmark introduced in [28]. It includes 294 training images and 227 testing images and each pixel is labeled as\nhead, leg, tail or body. The third task, human parsing aims to predict every pixel with 18 labels: face, sunglass, hat, scarf, hair, upper-clothes, left-arm, right-arm, belt, pants, left-leg, right-leg, skirt, left-shoe, right-shoe, bag, dress and null. Originally, 7,700 images are included in the ATR dataset [15], with 6,000 for training, 1,000 for testing and 700 for validation. 10,000 images are further collected by [18] to cover images with more challenging poses and clothes variations.\nEvaluation metric: The standard intersection over union (IOU) criterion and pixel-wise accuracy are adopted for evaluation on PASCAL-Person-Part dataset and HorseCow parsing dataset, following [7]. We use the same evaluation metrics as in [15, 18] for evaluation on the human parsing dataset, including accuracy, average precision, average recall, and average F-1 score.\nNetwork architecture: For fair comparison with [5, 30, 6], our network is based on the publicly available model, DeepLab-CRF-LargeFOV” [5] for the PASCAL-PersonPart and Horse-Cow parsing dataset, which slightly modifies VGG-16 net [21] to FCN [20]. Co-CNN” structure [18] is used to compare with [15, 18] on one human parsing datasets for fair comparison.\nTraining: The SLIC over-segmentation method [1] generates 1,000 superpixels on average for each image. The learning rate of the newly added layers over pre-trained models is initialized as 0.001 and that of other previously learned layers is initialized as 0.0001. All weight matrices used in the structure-evolving LSTM units are randomly initialized from a uniform distribution of [-0.1, 0.1]. We only use five LSTM layers for all models since only slight improvements are observed by using more LSTM\nlayers, which also consumes more computation resources. The weights of all convolutional layers are initialized with Gaussian distribution with standard deviation 0.001. We train all the models using stochastic gradient descent with a batch size of 1 image, momentum of 0.9, and weight decay of 0.0005. We fine-tune the networks on DeepLabCRF-LargeFOV” and train the networks based on CoCNN” from scratch for roughly 60 epochs. The structureevolving LSTM is implemented by extending the Caffe framework [12]. All networks are trained on a single NVIDIA GeForce GTX TITAN X GPU with 12GB memory. In the testing stage, extracting superpixels takes 0.5s and our method takes 1.3s per image in total."
    }, {
      "heading" : "4.3. Results and Comparisons",
      "text" : "Comparisons with State-of-the-art Methods. We report the result comparisons with recent state-of-the-art methods on PASCAL-Person-part dataset, Horse-Cow parsing dataset and ATR dataset in Table 1, Table 4, Table 5, respectively. The proposed structure-evolving LSTM structure substantially outperforms these baselines in terms of most of the metrics, especially for small semantic parts. This superior performance achieved by the structureevolving LSTM demonstrates the effectiveness of capturing multi-scale context by propagating information on the gen-\nerated graph structures. Comparisons with Existing LSTM Structures. Table 1 gives the performance comparison among different LSTM structures, including Row LSTM [27], Diagonal BiLSTM [27], LG-LSTM [17], Grid LSTM [14] and Graph LSTM [16], which use the same network architecture and number of LSTM layers. In particular, Row LSTM, Diagonal BiLSTM, LG-LSTM, Grid LSTM and LG-LSTM use the fixed locally factorized topology for all images while Graph LSTM propagates information on the fixed superpixel graph. It can be seen that exploiting the multi-level graph representations for different LSTM layers leads to over 3.41% improvement than the pre-defined LSTM structures on average IoU.\nDiscussion on Using Stochastic Policy. Note that the structure-evolving LSTM stochastically merges some graph nodes and employs an acceptance rate to determine whether a new graph structure should be accepted. An alternative way is deterministically merging some graph nodes by hardthresholding, that is, two nodes are merged only if their merging probability is larger than a fixed threshold T . In our experiment, three thresholds (i.e., 0.5, 0.7,0.9) are tested in Table 1. Using a smaller threshold (e.g., 0.5) is more likely to obtain more aggressive graph transitions by merging more nodes while a larger threshold would prevent the graph from changing its structure. It is shown that using 0.7\nthreshold in the deterministic policy obtains the best performance, which is still inferior to the proposed stochastic policy. Additionally, we find that only slight performance differences are obtained after running the feed-forward prediction using the structure-evolving LSTM for ten times, which verifies the robustness of the structure-evolving LSTM.\nComparisons with Using All Pre-defined Graph Structures. An optional strategy to capture multi-scale context is to utilize pre-computed multi-scale superpixel maps as the intermediate graph structures, reported as Graph LSTM (multi-scale superpixel maps)” in Table 1. Five predefined graph structures in LSTM layers can be constructed by five superpixel maps with 1000, 800, 600, 256 400, 200 superpixels, respectively. These superpixel numbers are consistent with the averaged node number of our learned graph structures for all training images. The superiority of “Structure-evolving LSTM” demonstrates that exploiting adaptive graph structures makes the structure more consistent with the high-level semantic representation instead of just relying on the bottom-up oversegmentation.\nDiscussion on Predictions with Different Levels of Graphs. The performance of using different numbers of the structure-evolving LSTM layers is reported in Table 2. It demonstrates that exploiting more levels of graph structures makes the network parameters learn different levels of semantic abstraction, leading to better parsing results, whereas the previous LSTM model [16] reported that no performance gain is achieved with more than two LSTM layers. Note that the parsing prediction is produced by each LSTM layer and these predictions are element-wisely summed to generate the final result. The individual parsing performance by using each graph structure is reported in Table 3. The higher-level graph structure may wrongly merge\nbottom-up graph nodes, which thus may lead to the deteriorated performance. However, combining all predictions from all the structure-evolving LSTM layers can largely boost the prediction benefited from incorporating the multiscale semantical context.\nVisualization. The qualitative comparisons of parsing results on ATR dataset and the graph structures exploited by structure-evolving LSTM layers are visualized in Fig. 4. The structure-evolving LSTM outputs more reasonable results for confusing labels (e.g., skirt and dress) by effectively exploiting multi-scale context with the generated multi-level graph structures."
    }, {
      "heading" : "5. Conclusion",
      "text" : "We presented a novel interpretable structure-evolving Graph LSTM which simultaneously learns multi-level graph representations for the data and LSTM network parameters in an end-to-end way. While following the line of graph-based RNNs, our work significantly improves the way of network learning by allowing the underlying multi-level graph structures to evolve along with the parameter learning. The network can thus learn representations to better fit the hidden structure of the data. Moreover, we propose a principled approach to evolve graph structures stochastically, which is not straightforward and could have potential impact on the application of graph-based RNNs in multiple domains. We have demonstrated its effectiveness on the object parsing task for an image. In future, the structure-evolving LSTM can be extended to enable the reversible graph transition (e.g., splitting some merged nodes) during the LSTM network optimization. We will also evaluate its performance on the tasks of other modalities, such as the social networks."
    } ],
    "references" : [ {
      "title" : "Slic superpixels",
      "author" : [ "R. Achanta", "A. Shaji", "K. Smith", "A. Lucchi", "P. Fua", "S. Süsstrunk" ],
      "venue" : "Technical report,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Graph partition by swendsen-wang cuts",
      "author" : [ "A. Barbu", "S. Zhu" ],
      "venue" : "Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on, pages 320–327,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Scene Labeling with LSTM Recurrent Neural Networks",
      "author" : [ "W. Byeon", "T.M. Breuel", "F. Raue", "M. Liwicki" ],
      "venue" : "CVPR, pages 3547–3555,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Texture classification using 2d lstm networks",
      "author" : [ "W. Byeon", "M. Liwicki", "T.M. Breuel" ],
      "venue" : "ICPR, pages 1144–1149,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs",
      "author" : [ "L.-C. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille" ],
      "venue" : "ICLR,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Attention to scale: Scale-aware semantic image segmentation",
      "author" : [ "L.-C. Chen", "Y. Yang", "J. Wang", "W. Xu", "A.L. Yuille" ],
      "venue" : "CVPR,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Detect what you can: Detecting and representing objects using holistic models and body parts",
      "author" : [ "X. Chen", "R. Mottaghi", "X. Liu", "S. Fidler", "R. Urtasun" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2014
    }, {
      "title" : "Multidimensional recurrent neural networks",
      "author" : [ "A. Graves", "S. Fernandez", "J. Schmidhuber" ],
      "venue" : "ICANN,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Speech recognition with deep recurrent neural networks",
      "author" : [ "A. Graves", "A.-r. Mohamed", "G. Hinton" ],
      "venue" : "In ICASSP, pages 6645–6649,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2013
    }, {
      "title" : "Offline handwriting recognition with multidimensional recurrent neural networks",
      "author" : [ "A. Graves", "J. Schmidhuber" ],
      "venue" : "NIPS, pages 545–552,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Caffe: Convolutional architecture for fast feature embedding",
      "author" : [ "Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell" ],
      "venue" : "ACM Multimedia,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "An empirical exploration of recurrent network architectures",
      "author" : [ "R. Józefowicz", "W. Zaremba", "I. Sutskever" ],
      "venue" : "ICML, pages 2342–2350,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Grid long short-term memory",
      "author" : [ "N. Kalchbrenner", "I. Danihelka", "A. Graves" ],
      "venue" : "arXiv preprint arXiv:1507.01526,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Deep human parsing with active template regression",
      "author" : [ "X. Liang", "S. Liu", "X. Shen", "J. Yang", "L. Liu", "J. Dong", "L. Lin", "S. Yan" ],
      "venue" : "TPAMI,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Semantic object parsing with graph lstm",
      "author" : [ "X. Liang", "X. Shen", "J. Feng", "L. Lin", "S. Yan" ],
      "venue" : "ECCV,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Semantic object parsing with local-global long short-term memory",
      "author" : [ "X. Liang", "X. Shen", "D. Xiang", "J. Feng", "L. Lin", "S. Yan" ],
      "venue" : "CVPR,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Human parsing with contextualized convolutional neural network",
      "author" : [ "X. Liang", "C. Xu", "X. Shen", "J. Yang", "S. Liu", "J. Tang", "L. Lin", "S. Yan" ],
      "venue" : "ICCV,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Matching-CNN Meets KNN: Quasi- Parametric Human Parsing",
      "author" : [ "S. Liu", "X. Liang", "L. Liu", "X. Shen", "J. Yang", "C. Xu", "L. Lin", "X. Cao", "S. Yan" ],
      "venue" : "CVPR,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Fully convolutional networks for semantic segmentation",
      "author" : [ "J. Long", "E. Shelhamer", "T. Darrell" ],
      "venue" : "arXiv preprint arXiv:1411.4038,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "K. Simonyan", "A. Zisserman" ],
      "venue" : "arXiv preprint arXiv:1409.1556,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "End-to-end people detection in crowded scenes",
      "author" : [ "R. Stewart", "M. Andriluka" ],
      "venue" : "NIPS,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "I. Sutskever", "O. Vinyals", "Q.V. Le" ],
      "venue" : "NIPS, pages 3104–3112,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Improved semantic representations from tree-structured long short-term memory networks",
      "author" : [ "K.S. Tai", "R. Socher", "C.D. Manning" ],
      "venue" : "arXiv preprint arXiv:1503.00075,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Generative image modeling using spatial lstms",
      "author" : [ "L. Theis", "M. Bethge" ],
      "venue" : "arXiv preprint arXiv:1506.03478,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Image segmentation by data-driven markov chain monte carlo",
      "author" : [ "Z. Tu", "S.-C. Zhu" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 24(5):657–673,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Pixel recurrent neural networks",
      "author" : [ "A. van den Oord", "N. Kalchbrenner", "K. Kavukcuoglu" ],
      "venue" : "ICML, 2016",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2016
    }, {
      "title" : "Semantic part segmentation using compositional model combining shape and appearance",
      "author" : [ "J. Wang", "A. Yuille" ],
      "venue" : "CVPR,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Joint object and part segmentation using deep learned potentials",
      "author" : [ "P. Wang", "X. Shen", "Z. Lin", "S. Cohen", "B. Price", "A. Yuille" ],
      "venue" : "ICCV,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Zoom better to see clearer: Huamn part segmentation with auto zoom net",
      "author" : [ "F. Xia", "P. Wang", "L.-C. Chen", "A.L. Yuille" ],
      "venue" : "arXiv preprint arXiv:1511.06881,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Show, attend and tell: Neural image caption generation with visual attention",
      "author" : [ "K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A.C. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio" ],
      "venue" : "ICML, pages 2048–2057,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Paper doll parsing: Retrieving similar styles to parse clothing items",
      "author" : [ "K. Yamaguchi", "M. Kiapour", "T. Berg" ],
      "venue" : "ICCV,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Parsing clothing in fashion photographs",
      "author" : [ "K. Yamaguchi", "M. Kiapour", "L. Ortiz", "T. Berg" ],
      "venue" : "CVPR,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Conditional random fields as recurrent neural networks",
      "author" : [ "S. Zheng", "S. Jayasumana", "B. Romera-Paredes", "V. Vineet", "Z. Su", "D. Du", "C. Huang", "P. Torr" ],
      "venue" : "ICCV,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Long short-term memory over tree structures",
      "author" : [ "X. Zhu", "P. Sobhani", "H. Guo" ],
      "venue" : "arXiv preprint arXiv:1503.04881,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "Recently, there has been a surge of interest in developing various kinds of Long Short-Term Memory (LSTM) neural networks for modeling complex dependencies within sequential and multi-dimensional data, due to their advantage in a wide range of applications such as speech recognition [9], image generation [27], image-to-caption generation [31] and multi-dimensional image processing [14].",
      "startOffset" : 284,
      "endOffset" : 287
    }, {
      "referenceID" : 25,
      "context" : "Recently, there has been a surge of interest in developing various kinds of Long Short-Term Memory (LSTM) neural networks for modeling complex dependencies within sequential and multi-dimensional data, due to their advantage in a wide range of applications such as speech recognition [9], image generation [27], image-to-caption generation [31] and multi-dimensional image processing [14].",
      "startOffset" : 306,
      "endOffset" : 310
    }, {
      "referenceID" : 29,
      "context" : "Recently, there has been a surge of interest in developing various kinds of Long Short-Term Memory (LSTM) neural networks for modeling complex dependencies within sequential and multi-dimensional data, due to their advantage in a wide range of applications such as speech recognition [9], image generation [27], image-to-caption generation [31] and multi-dimensional image processing [14].",
      "startOffset" : 340,
      "endOffset" : 344
    }, {
      "referenceID" : 12,
      "context" : "Recently, there has been a surge of interest in developing various kinds of Long Short-Term Memory (LSTM) neural networks for modeling complex dependencies within sequential and multi-dimensional data, due to their advantage in a wide range of applications such as speech recognition [9], image generation [27], image-to-caption generation [31] and multi-dimensional image processing [14].",
      "startOffset" : 384,
      "endOffset" : 388
    }, {
      "referenceID" : 8,
      "context" : "Despite the remarkable success, existing LSTM models such as chain-structured [9] [31], tree-structured LSTM models [35, 24] and graph-structured LSTM [16] can only process data with pre-fixed structures in terms of their internal information propagation route.",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 29,
      "context" : "Despite the remarkable success, existing LSTM models such as chain-structured [9] [31], tree-structured LSTM models [35, 24] and graph-structured LSTM [16] can only process data with pre-fixed structures in terms of their internal information propagation route.",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 33,
      "context" : "Despite the remarkable success, existing LSTM models such as chain-structured [9] [31], tree-structured LSTM models [35, 24] and graph-structured LSTM [16] can only process data with pre-fixed structures in terms of their internal information propagation route.",
      "startOffset" : 116,
      "endOffset" : 124
    }, {
      "referenceID" : 22,
      "context" : "Despite the remarkable success, existing LSTM models such as chain-structured [9] [31], tree-structured LSTM models [35, 24] and graph-structured LSTM [16] can only process data with pre-fixed structures in terms of their internal information propagation route.",
      "startOffset" : 116,
      "endOffset" : 124
    }, {
      "referenceID" : 14,
      "context" : "Despite the remarkable success, existing LSTM models such as chain-structured [9] [31], tree-structured LSTM models [35, 24] and graph-structured LSTM [16] can only process data with pre-fixed structures in terms of their internal information propagation route.",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 8,
      "context" : "Compared with existing LSTM structures with pre-fixed chain [9] [31], tree [35, 24] or graph topologies [16], the structure-evolving LSTM has the capability of modeling long-range interactions using the dynamically evolved hierarchical graph topologies to capture the multi-level inherent correlations embedded in the data.",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 29,
      "context" : "Compared with existing LSTM structures with pre-fixed chain [9] [31], tree [35, 24] or graph topologies [16], the structure-evolving LSTM has the capability of modeling long-range interactions using the dynamically evolved hierarchical graph topologies to capture the multi-level inherent correlations embedded in the data.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 33,
      "context" : "Compared with existing LSTM structures with pre-fixed chain [9] [31], tree [35, 24] or graph topologies [16], the structure-evolving LSTM has the capability of modeling long-range interactions using the dynamically evolved hierarchical graph topologies to capture the multi-level inherent correlations embedded in the data.",
      "startOffset" : 75,
      "endOffset" : 83
    }, {
      "referenceID" : 22,
      "context" : "Compared with existing LSTM structures with pre-fixed chain [9] [31], tree [35, 24] or graph topologies [16], the structure-evolving LSTM has the capability of modeling long-range interactions using the dynamically evolved hierarchical graph topologies to capture the multi-level inherent correlations embedded in the data.",
      "startOffset" : 75,
      "endOffset" : 83
    }, {
      "referenceID" : 14,
      "context" : "Compared with existing LSTM structures with pre-fixed chain [9] [31], tree [35, 24] or graph topologies [16], the structure-evolving LSTM has the capability of modeling long-range interactions using the dynamically evolved hierarchical graph topologies to capture the multi-level inherent correlations embedded in the data.",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 1,
      "context" : "Then the graph structure is generated by designing a Metropolis-Hasting algorithm [2, 26].",
      "startOffset" : 82,
      "endOffset" : 89
    }, {
      "referenceID" : 24,
      "context" : "Then the graph structure is generated by designing a Metropolis-Hasting algorithm [2, 26].",
      "startOffset" : 82,
      "endOffset" : 89
    }, {
      "referenceID" : 9,
      "context" : "Long Short-Term Memory (LSTM) recurrent networks have been first introduced to address the sequential prediction tasks [10, 23, 31, 13], and then extended to multidimensional image processing tasks [4, 25] such as image generation [27, 25], person detection [22], scene labeling [3] and object parsing [17].",
      "startOffset" : 119,
      "endOffset" : 135
    }, {
      "referenceID" : 21,
      "context" : "Long Short-Term Memory (LSTM) recurrent networks have been first introduced to address the sequential prediction tasks [10, 23, 31, 13], and then extended to multidimensional image processing tasks [4, 25] such as image generation [27, 25], person detection [22], scene labeling [3] and object parsing [17].",
      "startOffset" : 119,
      "endOffset" : 135
    }, {
      "referenceID" : 29,
      "context" : "Long Short-Term Memory (LSTM) recurrent networks have been first introduced to address the sequential prediction tasks [10, 23, 31, 13], and then extended to multidimensional image processing tasks [4, 25] such as image generation [27, 25], person detection [22], scene labeling [3] and object parsing [17].",
      "startOffset" : 119,
      "endOffset" : 135
    }, {
      "referenceID" : 11,
      "context" : "Long Short-Term Memory (LSTM) recurrent networks have been first introduced to address the sequential prediction tasks [10, 23, 31, 13], and then extended to multidimensional image processing tasks [4, 25] such as image generation [27, 25], person detection [22], scene labeling [3] and object parsing [17].",
      "startOffset" : 119,
      "endOffset" : 135
    }, {
      "referenceID" : 3,
      "context" : "Long Short-Term Memory (LSTM) recurrent networks have been first introduced to address the sequential prediction tasks [10, 23, 31, 13], and then extended to multidimensional image processing tasks [4, 25] such as image generation [27, 25], person detection [22], scene labeling [3] and object parsing [17].",
      "startOffset" : 198,
      "endOffset" : 205
    }, {
      "referenceID" : 23,
      "context" : "Long Short-Term Memory (LSTM) recurrent networks have been first introduced to address the sequential prediction tasks [10, 23, 31, 13], and then extended to multidimensional image processing tasks [4, 25] such as image generation [27, 25], person detection [22], scene labeling [3] and object parsing [17].",
      "startOffset" : 198,
      "endOffset" : 205
    }, {
      "referenceID" : 25,
      "context" : "Long Short-Term Memory (LSTM) recurrent networks have been first introduced to address the sequential prediction tasks [10, 23, 31, 13], and then extended to multidimensional image processing tasks [4, 25] such as image generation [27, 25], person detection [22], scene labeling [3] and object parsing [17].",
      "startOffset" : 231,
      "endOffset" : 239
    }, {
      "referenceID" : 23,
      "context" : "Long Short-Term Memory (LSTM) recurrent networks have been first introduced to address the sequential prediction tasks [10, 23, 31, 13], and then extended to multidimensional image processing tasks [4, 25] such as image generation [27, 25], person detection [22], scene labeling [3] and object parsing [17].",
      "startOffset" : 231,
      "endOffset" : 239
    }, {
      "referenceID" : 20,
      "context" : "Long Short-Term Memory (LSTM) recurrent networks have been first introduced to address the sequential prediction tasks [10, 23, 31, 13], and then extended to multidimensional image processing tasks [4, 25] such as image generation [27, 25], person detection [22], scene labeling [3] and object parsing [17].",
      "startOffset" : 258,
      "endOffset" : 262
    }, {
      "referenceID" : 2,
      "context" : "Long Short-Term Memory (LSTM) recurrent networks have been first introduced to address the sequential prediction tasks [10, 23, 31, 13], and then extended to multidimensional image processing tasks [4, 25] such as image generation [27, 25], person detection [22], scene labeling [3] and object parsing [17].",
      "startOffset" : 279,
      "endOffset" : 282
    }, {
      "referenceID" : 15,
      "context" : "Long Short-Term Memory (LSTM) recurrent networks have been first introduced to address the sequential prediction tasks [10, 23, 31, 13], and then extended to multidimensional image processing tasks [4, 25] such as image generation [27, 25], person detection [22], scene labeling [3] and object parsing [17].",
      "startOffset" : 302,
      "endOffset" : 306
    }, {
      "referenceID" : 3,
      "context" : "It can keep long-term memory by training proper gating weights and has practically showed the effectiveness on a range of problems [4, 3].",
      "startOffset" : 131,
      "endOffset" : 137
    }, {
      "referenceID" : 2,
      "context" : "It can keep long-term memory by training proper gating weights and has practically showed the effectiveness on a range of problems [4, 3].",
      "startOffset" : 131,
      "endOffset" : 137
    }, {
      "referenceID" : 12,
      "context" : ", 2 or 8 neighboring pixels [14][8][17] or diagonal neighborhood [27][25]).",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 7,
      "context" : ", 2 or 8 neighboring pixels [14][8][17] or diagonal neighborhood [27][25]).",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 15,
      "context" : ", 2 or 8 neighboring pixels [14][8][17] or diagonal neighborhood [27][25]).",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 25,
      "context" : ", 2 or 8 neighboring pixels [14][8][17] or diagonal neighborhood [27][25]).",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 23,
      "context" : ", 2 or 8 neighboring pixels [14][8][17] or diagonal neighborhood [27][25]).",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 22,
      "context" : "Recently, Tree-LSTM [24] introduces the structure with tree-structured topologies for predicting semantic representations of sentences.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 14,
      "context" : "LSTM [16] has been proposed to propagate information on a basic pre-defined graph topology to capture diverse natural visual correlations (e.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 12,
      "context" : "Different from using pre-defined data structures in previous LSTMs [14, 8, 17, 16, 25], the proposed structureevolving LSTM targets on automatically learning the hierarchical graph representations by evolving from an initial graph structure.",
      "startOffset" : 67,
      "endOffset" : 86
    }, {
      "referenceID" : 7,
      "context" : "Different from using pre-defined data structures in previous LSTMs [14, 8, 17, 16, 25], the proposed structureevolving LSTM targets on automatically learning the hierarchical graph representations by evolving from an initial graph structure.",
      "startOffset" : 67,
      "endOffset" : 86
    }, {
      "referenceID" : 15,
      "context" : "Different from using pre-defined data structures in previous LSTMs [14, 8, 17, 16, 25], the proposed structureevolving LSTM targets on automatically learning the hierarchical graph representations by evolving from an initial graph structure.",
      "startOffset" : 67,
      "endOffset" : 86
    }, {
      "referenceID" : 14,
      "context" : "Different from using pre-defined data structures in previous LSTMs [14, 8, 17, 16, 25], the proposed structureevolving LSTM targets on automatically learning the hierarchical graph representations by evolving from an initial graph structure.",
      "startOffset" : 67,
      "endOffset" : 86
    }, {
      "referenceID" : 23,
      "context" : "Different from using pre-defined data structures in previous LSTMs [14, 8, 17, 16, 25], the proposed structureevolving LSTM targets on automatically learning the hierarchical graph representations by evolving from an initial graph structure.",
      "startOffset" : 67,
      "endOffset" : 86
    }, {
      "referenceID" : 14,
      "context" : "The structure-evolving LSTM (dynamically evolving multi-level graphs) is superior to the most related Graph LSTM [16] (a pre-fixed single-level graph) in two aspects: 1) Structure-evolving LSTM learns more powerful representations as it progressively exploits hierarchical information along stacked LSTM layers; 2) at its later layers, the structure-evolving LSTM captures the inherent structure of the desired output benefiting from the higher-level graph topologies.",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 14,
      "context" : "These superiorities bring significant improvements on several semantic parsing datasets, which gives apple-to-apple comparison with [16].",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 14,
      "context" : "Following [16], we randomly specify the node updating sequence to propagate information to all nodes in order to increase the model diversity during learning the LSTM network parameters.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 14,
      "context" : "Our structure-evolving LSTM follows the Graph LSTM units [16] to generate hidden and memory cells, and then show how to inject the edge merging probabilities of the nodes into the LSTM units.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 1,
      "context" : "To find a better graph transition between two graphs G and G, the acceptance rate of the transition from the graph from G to graph G is defined by a Metropolis-Hastings method [2, 26]:",
      "startOffset" : 176,
      "endOffset" : 183
    }, {
      "referenceID" : 24,
      "context" : "To find a better graph transition between two graphs G and G, the acceptance rate of the transition from the graph from G to graph G is defined by a Metropolis-Hastings method [2, 26]:",
      "startOffset" : 176,
      "endOffset" : 183
    }, {
      "referenceID" : 0,
      "context" : "The initial graph G is constructed on superpixels that are obtained through image over-segmentation using SLIC [1] following [16].",
      "startOffset" : 111,
      "endOffset" : 114
    }, {
      "referenceID" : 14,
      "context" : "The initial graph G is constructed on superpixels that are obtained through image over-segmentation using SLIC [1] following [16].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 6,
      "context" : "Comparison of semantic object parsing performance with several state-of-the-art methods on the PASCAL-Person-Part dataset [7] and with other variants of the structure-evolving LSTM model, including using different LSTM structures, the extracted multi-scale superpixel maps and a deterministic policy with different thresholds for the graph transition, respectively.",
      "startOffset" : 122,
      "endOffset" : 125
    }, {
      "referenceID" : 4,
      "context" : "DeepLab-LargeFOV [5] 78.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 4,
      "context" : "78 DeepLab-LargeFOV-CRF [5] 80.",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 28,
      "context" : "95 HAZN [30] 80.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 5,
      "context" : "11 Attention [6] - - - - - - - 56.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 12,
      "context" : "Grid LSTM [14] 81.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 25,
      "context" : "90 Row LSTM [27] 82.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 25,
      "context" : "80 Diagonal BiLSTM [27] 82.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 15,
      "context" : "62 LG-LSTM [17] 82.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 14,
      "context" : "97 Graph LSTM [16] 82.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 14,
      "context" : "Graph LSTM (multi-scale superpixel maps) [16] 83.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 6,
      "context" : "The PASCAL-Person-part dataset [7] concentrates on the human part segmentation on images from PASCAL VOC 2010.",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 26,
      "context" : "The Horse-Cow parsing dataset is a part segmentation benchmark introduced in [28].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 13,
      "context" : "Originally, 7,700 images are included in the ATR dataset [15], with 6,000 for training, 1,000 for testing and 700 for validation.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 16,
      "context" : "10,000 images are further collected by [18] to cover images with more challenging poses and clothes variations.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 6,
      "context" : "Evaluation metric: The standard intersection over union (IOU) criterion and pixel-wise accuracy are adopted for evaluation on PASCAL-Person-Part dataset and HorseCow parsing dataset, following [7].",
      "startOffset" : 193,
      "endOffset" : 196
    }, {
      "referenceID" : 13,
      "context" : "We use the same evaluation metrics as in [15, 18] for evaluation on the human parsing dataset, including accuracy, average precision, average recall, and average F-1 score.",
      "startOffset" : 41,
      "endOffset" : 49
    }, {
      "referenceID" : 16,
      "context" : "We use the same evaluation metrics as in [15, 18] for evaluation on the human parsing dataset, including accuracy, average precision, average recall, and average F-1 score.",
      "startOffset" : 41,
      "endOffset" : 49
    }, {
      "referenceID" : 4,
      "context" : "Network architecture: For fair comparison with [5, 30, 6], our network is based on the publicly available model, DeepLab-CRF-LargeFOV” [5] for the PASCAL-PersonPart and Horse-Cow parsing dataset, which slightly modifies VGG-16 net [21] to FCN [20].",
      "startOffset" : 47,
      "endOffset" : 57
    }, {
      "referenceID" : 28,
      "context" : "Network architecture: For fair comparison with [5, 30, 6], our network is based on the publicly available model, DeepLab-CRF-LargeFOV” [5] for the PASCAL-PersonPart and Horse-Cow parsing dataset, which slightly modifies VGG-16 net [21] to FCN [20].",
      "startOffset" : 47,
      "endOffset" : 57
    }, {
      "referenceID" : 5,
      "context" : "Network architecture: For fair comparison with [5, 30, 6], our network is based on the publicly available model, DeepLab-CRF-LargeFOV” [5] for the PASCAL-PersonPart and Horse-Cow parsing dataset, which slightly modifies VGG-16 net [21] to FCN [20].",
      "startOffset" : 47,
      "endOffset" : 57
    }, {
      "referenceID" : 4,
      "context" : "Network architecture: For fair comparison with [5, 30, 6], our network is based on the publicly available model, DeepLab-CRF-LargeFOV” [5] for the PASCAL-PersonPart and Horse-Cow parsing dataset, which slightly modifies VGG-16 net [21] to FCN [20].",
      "startOffset" : 135,
      "endOffset" : 138
    }, {
      "referenceID" : 19,
      "context" : "Network architecture: For fair comparison with [5, 30, 6], our network is based on the publicly available model, DeepLab-CRF-LargeFOV” [5] for the PASCAL-PersonPart and Horse-Cow parsing dataset, which slightly modifies VGG-16 net [21] to FCN [20].",
      "startOffset" : 231,
      "endOffset" : 235
    }, {
      "referenceID" : 18,
      "context" : "Network architecture: For fair comparison with [5, 30, 6], our network is based on the publicly available model, DeepLab-CRF-LargeFOV” [5] for the PASCAL-PersonPart and Horse-Cow parsing dataset, which slightly modifies VGG-16 net [21] to FCN [20].",
      "startOffset" : 243,
      "endOffset" : 247
    }, {
      "referenceID" : 16,
      "context" : "Co-CNN” structure [18] is used to compare with [15, 18] on one human parsing datasets for fair comparison.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 13,
      "context" : "Co-CNN” structure [18] is used to compare with [15, 18] on one human parsing datasets for fair comparison.",
      "startOffset" : 47,
      "endOffset" : 55
    }, {
      "referenceID" : 16,
      "context" : "Co-CNN” structure [18] is used to compare with [15, 18] on one human parsing datasets for fair comparison.",
      "startOffset" : 47,
      "endOffset" : 55
    }, {
      "referenceID" : 0,
      "context" : "Training: The SLIC over-segmentation method [1] generates 1,000 superpixels on average for each image.",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 26,
      "context" : "Comparison of object parsing performance with five stateof-the-art methods over the Horse-Cow object parsing dataset [28].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 26,
      "context" : "SPS [28] 79.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 27,
      "context" : "18 Joint [29] 87.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 15,
      "context" : "49 LG-LSTM [17] 89.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 28,
      "context" : "92 HAZN [30] 90.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 14,
      "context" : "16 Graph LSTM [16] 91.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 26,
      "context" : "SPS [28] 78.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 27,
      "context" : "43 Joint [29] 85.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 15,
      "context" : "00 LG-LSTM [17] 89.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 28,
      "context" : "43 HAZN [30] 90.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 14,
      "context" : "94 Graph LSTM [16] 91.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 10,
      "context" : "The structureevolving LSTM is implemented by extending the Caffe framework [12].",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 13,
      "context" : "Performance comparison with state-of-the-art methods when evaluating on ATR dataset [15].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 16,
      "context" : "Following [18], we also take the additional 10,000 images in [18] as extra training images, denoted as “Ours (more data)”.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 16,
      "context" : "Following [18], we also take the additional 10,000 images in [18] as extra training images, denoted as “Ours (more data)”.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 31,
      "context" : "[33] 84.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 30,
      "context" : "80 PaperDoll [32] 88.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 17,
      "context" : "76 M-CNN [19] 89.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 13,
      "context" : "81 ATR [15] 91.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 16,
      "context" : "38 Co-CNN [18] 95.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 16,
      "context" : "95 Co-CNN (more) [18] 96.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 15,
      "context" : "14 LG-LSTM [17] 96.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 15,
      "context" : "97 LG-LSTM (more) [17] 96.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 32,
      "context" : "12 CRFasRNN (more) [34] 96.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 25,
      "context" : "Table 1 gives the performance comparison among different LSTM structures, including Row LSTM [27], Diagonal BiLSTM [27], LG-LSTM [17], Grid LSTM [14] and Graph LSTM [16], which use the same network architecture and number of LSTM layers.",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 25,
      "context" : "Table 1 gives the performance comparison among different LSTM structures, including Row LSTM [27], Diagonal BiLSTM [27], LG-LSTM [17], Grid LSTM [14] and Graph LSTM [16], which use the same network architecture and number of LSTM layers.",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 15,
      "context" : "Table 1 gives the performance comparison among different LSTM structures, including Row LSTM [27], Diagonal BiLSTM [27], LG-LSTM [17], Grid LSTM [14] and Graph LSTM [16], which use the same network architecture and number of LSTM layers.",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 12,
      "context" : "Table 1 gives the performance comparison among different LSTM structures, including Row LSTM [27], Diagonal BiLSTM [27], LG-LSTM [17], Grid LSTM [14] and Graph LSTM [16], which use the same network architecture and number of LSTM layers.",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 14,
      "context" : "Table 1 gives the performance comparison among different LSTM structures, including Row LSTM [27], Diagonal BiLSTM [27], LG-LSTM [17], Grid LSTM [14] and Graph LSTM [16], which use the same network architecture and number of LSTM layers.",
      "startOffset" : 165,
      "endOffset" : 169
    }, {
      "referenceID" : 14,
      "context" : "It demonstrates that exploiting more levels of graph structures makes the network parameters learn different levels of semantic abstraction, leading to better parsing results, whereas the previous LSTM model [16] reported that no performance gain is achieved with more than two LSTM layers.",
      "startOffset" : 208,
      "endOffset" : 212
    } ],
    "year" : 2017,
    "abstractText" : "This paper develops a general framework for learning interpretable data representation via Long Short-Term Memory (LSTM) recurrent neural networks over hierarchal graph structures. Instead of learning LSTM models over the pre-fixed structures, we propose to further learn the intermediate interpretable multi-level graph structures in a progressive and stochastic way from data during the LSTM network optimization. We thus call this model the structure-evolving LSTM. In particular, starting with an initial element-level graph representation where each node is a small data element, the structure-evolving LSTM gradually evolves the multi-level graph representations by stochastically merging the graph nodes with high compatibilities along the stacked LSTM layers. In each LSTM layer, we estimate the compatibility of two connected nodes from their corresponding LSTM gate outputs, which is used to generate a merging probability. The candidate graph structures are accordingly generated where the nodes are grouped into cliques with their merging probabilities. We then produce the new graph structure with a Metropolis-Hasting algorithm, which alleviates the risk of getting stuck in local optimums by stochastic sampling with an acceptance probability. Once a graph structure is accepted, a higher-level graph is then constructed by taking the partitioned cliques as its nodes. During the evolving process, representation becomes more abstracted in higher-levels where redundant information is filtered out, allowing more efficient propagation of long-range data dependencies. We evaluate the effectiveness of structure-evolving LSTM in the application of semantic object parsing and demonstrate its advantage over state-of-the-art LSTM models on standard benchmarks.",
    "creator" : "LaTeX with hyperref package"
  }
}