{
  "name" : "1708.04806.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "New Ideas for Brain Modelling 4",
    "authors" : [ "Kieran Greer" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "it considers the neural binding structure of an earlier paper. To help with this, the paper describes some new methods in the areas of image processing and high-level behaviour simulation. The work is all based on earlier research by the author and the new additions are intended to fit in with the overall design. For image processing, a grid-like structure is used with ‘full linking’. Each cell in the classifier grid stores a list of all other cells it gets associated with and this is used as the learned image that new input is compared to. For the behaviour metric, a new prediction equation is suggested, as part of a simulation, that uses feedback and history to dynamically determine its course of action. While the new methods are from widely different topics, both can be compared with the binary-analog type of interface that is the main focus of the paper. It is suggested that the simplest of linking between a tree and ensemble can explain neural binding and variable signal strengths.\nKeywords: image comparison, binary-analog, clustering, neural, cognitive."
    }, {
      "heading" : "1 Introduction",
      "text" : "This paper continues the research that considers a new cognitive model, last updated in [8]. In particular, it considers figure 4 of that paper and how it might be useful in practice. To help with this, the paper describes some new methods in the areas of image processing and high-level behaviour simulation. The work is all based on earlier research by the author and the new\nadditions are intended to fit in with the overall design. For image processing, a grid-like structure is used with ‘full linking’, if you like. Each cell in the classifier grid stores a list of all other cells it gets associated with and this is used as the learned image that new input is compared with. For the behaviour metric, a new prediction equation is suggested, as part of a simulation, that uses feedback and history to dynamically determine its current state and course of action. While the new methods are from widely different topics, both can be compared with the binary-analog type of interface that is the main focus of the paper. It is suggested that the simplest of linking between a tree and ensemble can explain neural binding and variable signal strengths.\nThe rest of the paper is organised as follows: section 2 introduces an image processing method and compares it with the proposed neural binding structure. Section 3 describes some related work. Section 4 re-visits the behaviour metric of an earlier paper and proposes a new predictive equation for that. This can also be compared to the neural binding structure, with the idea of developing a general philosophy there. Section 5 re-visits the concept base and concept trees, and updates both with the new algorithms and methods. Section 6 gives some justification for the binary-analog neural structure in terms of how it might be used, while section 7 gives some conclusions to the work."
    }, {
      "heading" : "2 Image Processing",
      "text" : "A first investigation into the new theory can be attempted with image recognition. Images are represented by a 2-D grid, with a black cell meaning that a pixel is present and a white cell meaning that it is empty. A classifier uses the same grid-like structure, where all of the cells link to each other. The author has used this structure before in [8] and it is a type of entropy classifier. It attempts to reduce the error overall and is not so concerned with minimising individual associations. The paper [7] describes a classifier that is maybe more visual in nature. It also uses a complete linking method, where each cell has its own set of weights for each output category. With the image classifier, each cell stores a count of every other cell it gets associated with, when averaging this can determine what cells are most similar to each other. Figure 1 is an example of\nthe clustering technique. If the LHS grid is the first image to be mapped, then for cell A1, the other black cells are recorded as shown, with a count of 1. The count would then be incremented each time a cell is recorded again. The idea of linking everything this way has now been used 3 times.\nUsing image processing as an example, a set of hand-written numbers [21] was selected as the test data. Each number was trained on a separate classifier, where each cell would store the other related cells of each binary input image. The counts could then be averaged to produce the weight values. To use the system, a new binary image would be presented to each of the trained classifiers. For the pixels in the input image, the weighted values of the related classifier cells can be retrieved. These would also be linked to other cells, maybe not in the input image. For example, if the counts are as shown and a new image is presented that contains pixels in cells A1, A2 and A3 – then the classifier would return these cells plus B1 and C1. The success score is then the percentage of retrieved weighted cells in the classifier that are in the input image compared to the number that are not in the image. For this example, 3 cells are in the input while 2 cells are not, leading to a success score of 1.5. The weight value of the cell can also be considered for a threshold, but all cells are returned here because they all have the same count value of 1. The idea is probably auto-associative. It may also be self-organising, but each number is trained on a separate classifier first.\nThe test results are not terribly good and to improve it, some level of scaling would be required. A smaller image can be represented by part of a larger image, no matter what the larger image is. An average success score of only 46% was achieved with this basic version, with a best score for a number of 89% and a worst score of 15%. It was interesting that the classifier would try to return a picture that was more a reflection of itself. So regardless of what the input was, for example, the number 1 classifier would try to return an image that looked like a ‘1’. If the input image was a ‘4’ however, then maybe the number 4 classifier would return a more accurate comparison and therefore win the matching competition."
    }, {
      "heading" : "3 Related Work",
      "text" : "The author’s own papers that are quoted [7]-[14] are all relevant to the research of this paper. The Concept Tree [11] of section 5 has previously been compared to Markov Models [5] and is very similar to them, including the construction rule that can be implicit in a Markov Model. A new feedback loop is described in that section, when the Markov Model comparison also looks like a Finite State Machine. The paper [22] is a survey of these and notes that there are many different types that obey different rules. While the Concept Tree is a static structure for storing knowledge; with the addition of control structures like feedback, it is possible to consider it for state changes as well. An earlier paper on control theory [23] posts some interesting equations that are maybe similar to ones in this paper. Equation 1 there, for example, looks like the image success score ratio and equation 7 is a likelihood ratio test that is also trying to maximise inside some type of sequence. The behaviour metric of section 4 does not have a lot of new theory in this paper. The earlier paper [12] notes some references, including [1][15][18] and [19]. One interesting aspect of this paper is the new predictive equation. It uses a feedback mechanism that appears to be similar to one that was part of another research project and even in the project code1. So, I am grateful that I was able to work on that for a short while, as it may have made me aware of the process.\n1 Cognitive Algorithm by Boris Kazachenko, http://www.cognitivealgorithm.info/."
    }, {
      "heading" : "3.1 Cognitive Models",
      "text" : "Hawkins and Blakeslee [16] describe how a region of the cortex might work (p. 57) and they note an input signal being voted on by a higher level, where one higher level pattern set will win and switch off the other sets. They also state explicitly that the higher level is voting to ‘fit’ its label better than the other patterns. It may be trying to return its own image as the input signal and the best match there with the input signal should win. The theory that they state is that a region learns when it may be important and then it can become partially active, as part of a memory or prediction. So, this is a type of reasoning, to play over previous scenarios, even when they have not happened in the current situation yet. That can then maybe be reinforced further by specific instance values, making it the real decision. The following quote is also interesting:\n‘Every moment in your waking life, each region of your neocortex is comparing a set of expected columns driven from above with the set of observed columns driven from below. Where the two sets intersect is what we perceive. If we had perfect input from below and perfect predictions, then the set of perceived columns would always be contained in the set of predicted columns. We often don't have such agreement. The method of combining partial prediction with partial input resolves ambiguous input, it fills in missing pieces of information, and it decides between alternative views.’\nThe paper [20] introduces the idea of temporal synchrony and synchronised oscillatory activity as important for multisensory perception. The image processing of section 2 has already been tried in [3], where they tested the full dataset. Their results were better overall, with maybe 55% accuracy. As stated however, the tests here are only initial results and it would be expected that some improvement would be possible, especially if the images can be scaled."
    }, {
      "heading" : "3.2 Neural Binding",
      "text" : "There is quite a lot of research and philosophy into the idea of neural binding. At its most basic, it means ‘how do neural ensembles that fire together be understood to represent the said concept’. For example, questions like ‘why don’t we confuse a red circle and a blue square with a blue circle and a red square’ [4] need to be answered. It includes the idea of consciousness and how the brain is able to be coherent, but while there are lots of theories, there are not a lot of very specific results for the binding mechanism itself. Some cognitive models for the real brain include temporal logic or predicate calculus rules [4] to explain how variables can bind with each other and reasoning can be obtained. This includes the flow of information in both directions and so the basic circuits of this and earlier papers would not be too extravagant. The paper [2] describes a theory that is quite similar. They call the framework the Specialized Neural Regions for Global Efficiency (SNRGE) framework. The specializations associated with different brain areas represent computational trade-offs that are inherent in the neurobiological implementation of cognitive processes. That is, the trade-offs are a direct consequence of what computational processes can be easily implemented in the underlying biology. The specializations correspond anatomically to the hippocampus (HC), the prefrontal cortex (PFC), and all of neocortex that is posterior to prefrontal cortex (posterior cortex, PC). Essentially, prefrontal cortex and the hippocampus appear to serve as memory areas that dynamically and interactively support the computation that is being performed by posterior brain areas. The PC stores overlapping distributed representations used to encode semantic and perceptual information. The HC stores sparse, pattern separated representations used to rapidly encode ('bind') entire patterns of information across cortex while minimizing interference. The FC stores isolated stripes (columns) of neurons capable of sustained firing (i.e., active maintenance or working memory). They argue against temporal synchrony, because of the ‘red circle blue square’ question and prefer to argue for coarse-coded distributed representations (CCDR) ([17] and others) instead."
    }, {
      "heading" : "4 Cognitive Behaviour",
      "text" : "An earlier paper introduced a set of equations that were based on the collective behaviour research of [6]. They proposed a set of characteristics for modelling the stigmergic behaviour of very simple animals, such as ants. They proposed to use coordination, cooperation, deliberation and collaboration, as follows:\n• Coordination – is the appropriate organisation in space and time of the tasks required to solve\na specific problem.\n• Cooperation – occurs when individuals achieve together a task that could not be done by a\nsingle one.\n• Deliberation – refers to mechanisms that occur when a colony faces several opportunities.\nThese mechanisms result in a collective choice for at least one of the opportunities.\n• Collaboration – means that different activities are performed simultaneously by groups of\nspecialised individuals.\nThey note that these are not mutually exclusive, but rather contribute together to accomplish the various collective tasks of the colony. This led to a set of equations by the author [12] for modelling these types of entity. The model is actually behaviour-based not entity-based, where the entity instances are then made up of a set of the pre-defined behaviours, with the following characteristics:\n1. Individual agent characteristics: Relate to an agent as an individual:\n1.1. Ability: this defines how well the behaviour is able to execute the required action. 1.2. Flexibility: this defines how well an agent performing a behaviour can adapt or change to\na different behaviour if the situation requires it to. This can be seen as the ability to make that decision individually. The collective capabilities described next can then be seen as the ability to be flexible after an environment response.\n2. Collaborative agent characteristics: These relate to the agent working in a team environment:\n2.1. Coordination: this defines how well the agent can coordinate its actions with those of\nother agents. This is again a behaviour selection, related to flexibility, but this variable measures the group aspect of the attribute after interaction with other agents.\n2.2. Cooperation: this defines how well an agent performing an action can cooperate with\nother agents also involved in that action. How well can the selected behaviours work together?\n2.3. Communication: this defines how well the agents can communicate with each other. This\nis defined as an input signal and an output signal for each behaviour type. Behaviours could require local or remote communication, for example.\nThe metric is quite well balanced, with approximately half of the evaluation going to the individual capabilities and half going to the group capabilities."
    }, {
      "heading" : "4.1 Problem Modelling",
      "text" : "It is possible to specify a problem with all of the related agents and actions that are part of the solution space. The modelling is based around the behaviour types that are used to solve the problem, where the same type definitions can be used, both to model the problem and also to simulate its execution. The agents are defined by agent types, where an agent type can perform a particular set of behaviours. So, if the same behaviour type is to be performed at different levels of success; for static values, this would require different behaviour definitions, or for dynamic ones the value can change through an equation."
    }, {
      "heading" : "4.2 Behaviour Equations",
      "text" : "The problem is therefore modelled as sets of agents that can each perform a set of behaviours. The Problem Success Likelihood (PSL) is the summed result of the behaviour scores and estimates how well the problem can be solved. The top part of the PSL value, shown in equation 1, evaluates the average agent complexity (ECs), as just described. When modelling, this is measured for all of the behaviour type instances (Bs) that are part of the problem behaviour set (PBS). This can be no larger than the optimal problem complexity (PC) value of 1.0. The problem complexity is a factor of how intelligent the agents need to be to solve it. Because the evaluations are all normalised, in a static specification, the maximum value that the problem complexity can be is\n1.0. If all behaviours are perfect, they will also only sum to 1 as well. The problem success likelihood, can therefore be defined as follows:\nPSL = Eq. 1"
    }, {
      "heading" : "4.2.1 Individual Parameters",
      "text" : "The individual capabilities of a behaviour can be modelled as follows:\nECs = Eq. 2\nIs = Eq. 3\nThe agent or entity complexity (ECs) for behaviour s is a factor of its ability to perform the related behaviour attributes of intelligence and collective capabilities. The agent intelligence (Is) is a factor of its ability (BAs) and flexibility (BFs) capabilities for the specified behaviour."
    }, {
      "heading" : "4.2.2 Team Work",
      "text" : "The collective or team work capabilities (COLs) of a behaviour are modelled as follows:\nCOLs = Eq. 4\nCOMs = Eq. 5\nThe collective capabilities of the agent performing the behaviour are its ability to cooperate with other agents (COPs), coordinate its actions with them (CORs) and also communicate this (COMs),\nnormalised. The communication capabilities of the agent for the behaviour s include its ability to send a signal to another agent (SOs) and also its ability to receive a signal from another agent (SIs)."
    }, {
      "heading" : "4.3 Prediction Operation for the Metric",
      "text" : "When simulating the problem, the Problem Complexity value can change. The success likelihood then becomes an individual evaluation, based on its knowledge and understanding of the environment. This can be defined by the subset of behaviours the agent has either performed or has interacted with from other agents. For a more intelligent agent, the memory or history of earlier events can lead to a prediction operation that can reason over the earlier events, before selecting a behaviour to use in the current situation. It could be a deliberation function that is fed the history of earlier and/or possible choices, before selecting the most appropriate one.\nFor example, Equation 3 of section 4.2.1 defines agent intelligence as a combination of ability and flexibility. The idea is the ability to perform the intended behaviour but also flexibility to change or adapt the individual behaviour depending on some response. Ideally, an agent would score high in both and a modelling scenario that uses static values would be able to demonstrate this. If instead, running the agents in a simulation, it may be more interesting to let them change their behaviours dynamically, but again constrained by the pre-defined environment. This leads to the idea of a prediction metric that is influenced by what the agent can do and also what it did in the past. The current situation is the most important and so the decision there has the largest weight. The prediction could then include decreasing values for earlier related events. These can be factored as a count of earlier events times a factor for the time when they occurred. If the behaviour was not repeated, then maybe something went wrong, such as an unfavourable response. These responses, including for the current situation, would change the state of the agent into what it then has to deal with. The equation would be something like:\nPr = (ECS1 + (∑ \uD835\uDC53(n, ECm, R, t)\uD835\uDC40\uD835\uDC5A=0 / M)) / 2 Eq. 6\nWhere: ECs1 is the currently selected individual behaviour complexity, ECm is any previously selected individual behaviour complexity, for any related scenario, n is number of times in memory that the previous event occurred, R is the response or impact of the event, t is the last time the event occurred, M is the total number of behaviour-response pairs stored in memory, f is some function evaluation over the variable set, maybe n(EC x R) / t.\nWhen the agent selects a new behaviour, it is expecting a positive response. After a reply from the environment, the individual behaviour plus the response is fed back into the equation to get a new amount. If this is less than expected – ‘current Pr plus new EC’, then the response has been a negative one and possibly a different behaviour should be selected. This type of process can repeat, with bad responses being flagged and not selected again, for example, until a decision is made, maybe a new stable state is reached. As the equation calculates, it also feeds back its current state to update its evaluation for the next selection. The responses are therefore even more responsible for changing the agent state, where the behaviour selection, using the entity complexity, is an individual one, maybe based more on knowledge. So again, there is a hint of a simpler evaluation, which is the knowledge-based decision, balancing itself with the more complex decision, after the response is also factored in."
    }, {
      "heading" : "4.4 Intelligent Simulation Equations",
      "text" : "A more intelligent version of the metric for a simulation might therefore look as follows:\nPSL = \uD835\uDC43\uD835\uDC5F\n\uD835\uDC43\uD835\uDC36 Eq. 7\nWhere the predictive equation can replace the entity complexity and both of the flexibility parts. The prediction is modelled as in equation 6, by the agent decision plus its reaction to any response. A dynamic problem complexity can be measured as in equation 8 and is essentially the fraction of all behaviours that the agent knows about. Depending on how this is measured, a multiplication might be more appropriate for the PSL. However, if I only ‘know’ about 1 behaviour, for example, then based on my own knowledge, I can solve that more easily than if I have to deal with several known behaviours.\nPC = (∑ ECn\uD835\uDC41\uD835\uDC5B=0 + ∑ \uD835\uDC45\uD835\uDC5A \uD835\uDC40 \uD835\uDC5A=0 ) / PBS\nEq. 8\nThe collective capabilities are thus reduced to cooperation and communication with other entities, where coordination is moved to the predictive part.\nCOLs = \uD835\uDC36\uD835\uDC42\uD835\uDC43\uD835\uDC60+\uD835\uDC36\uD835\uDC42\uD835\uDC40\uD835\uDC60\n2 Eq. 9"
    }, {
      "heading" : "4.5 Testing",
      "text" : "The behaviour metric was tested in [12]. There has not been an opportunity to test the new predictive equation or its feedback algorithm and so this paper presents the theory of it only and notes the relation of the theory to the other work. However, a worked example described next, should help to show how it would work in practice."
    }, {
      "heading" : "4.5.1 Worked Example",
      "text" : "Consider the following scenario: an agent finds itself in a situation S1. The agent is modelled with behaviours B1 to B5 and the world is modelled with behaviours B1 to B10. The agent has encountered the same scenario S1 before and attempted the following behaviour set with related events, to deal with it:\nEvent B3 was tried at time t3 and resulted in a response R8. Event B4 was tried at time t2 and resulted in response R6.\nBased on this, behaviour B4 is selected again, leading to the equation:\nPr = B4s1 + (1xB4 × R6/2) + (1xB3 × R8/3)\nThe scene is an interactive one, with another agent able to reply. The other agent also knows the scenario and replies with a behaviour B10. This is found to be unfavourable for the agent and reduces its overall evaluation through the equation:\nPr = (B4S1 x R10S1) + (1xB4 × R6/t2) + (1xB3 × R8/t3), where R10S1 is negative.\nTherefore, the prediction reduces and the agent is required to try again. It now has knowledge of the reply B10 and also knows not to use behaviour B4 if it doesn’t have to. Therefore, a new response based on its new history could lead to:\nPr = B2S1 + (1 x B4S1 x R10S1/t2) + (1xB4 × R6/t3) + (1xB3 × R8/t4)\nThe reply by the environment, B10 again, is not as unfavourable now and so the prediction increases. Based on other criteria, the behaviour can be played again, or a satisfactory situation may have been achieved. In either case, to resolve this situation, two behaviours were tried and both were fed back into the evaluation function. The first one even counted as part of the history for selecting the second one."
    }, {
      "heading" : "5 Concept Base and Concept Trees",
      "text" : "The paper [14] described the idea of a ‘Concept Base’ that was later developed into a ‘Concept Tree’ [11][10]. In the paper, the concept base uses stigmergy or basic reinforcement learning and stores concepts with optional instance values, of the form:\n[A1, Va1] [B1, Vb1] [C1, Vc1] … are attributes of X"
    }, {
      "heading" : "In this equation, A, B or C is the concept type, where in this case it could be a behaviour type. Va1,",
      "text" : "Vb1, etc. are then related value instances associated with the concept type. For example, A is associated with B, is associated with C, when certain values are also present and X is the more complex or encompassing concept that they would all belong to. The concept base is intended to store information as flat chain structures. Each global concept X stores a set of values related to it, for example, the food items in a recipe. The paper also showed that some clustering of the base concepts can be achieved if time is included. That would produce a narrowing tree, from the base set to the global concept and it would be experience-based. A more knowledge-based approach is to create ‘concept trees’ that have a more common base node extending through several paths to branch nodes. So it is interesting that the knowledge-based approach, that might also be considered more constructive, can extend the tree outwards, while the experience-based approach might narrow the tree. The two approaches can be complementary that way. If the time feedback is considered to be only 1 degree of freedom, then an additional response feedback will provide a much more variable reply and so that should allow for a richer structure to be formed. The response would add experience, where an equation for the global concept could then look like:\nf(n, BA, R, t), f(n, BB, R, t), f(n, BC, R, t), … are attributes of X\nIn this case, the response R can be a transitional state between two behaviours. It can also be positive or negative, where a negative response can feed-back to a lower-level behaviour. Therefore, definite structure can be added to the initial behaviour set, even as part of the symbolic neural network [14]. In this case, the response values can define an even more precise structure and low-level reasoning can be automatically built into the tree. It becomes fixed, but\nit can be triggered in different ways and so depending on what other parts of the system are active, they can change the flow of information in that tree part. This is illustrated in Figure 2. In this figure, the sensory input is from below and the tree represents a more cognitive layer. When the brain goes to reason over what behaviour to choose, the inhibitor can send negative feedback into the region to disable some options. Interesting if the symbolic neural network can also obey the simple counting rule of the concept tree.\nIn the figure, the cat prefers to sit on the mat. It would therefore firstly choose that branch, but in this case the mat ‘environment’ is wet and so the cat receives negative feedback. This could be represented structurally as an inhibitor feedback from a ‘wet’ value to the ‘sat’ neuron. The sat neuron therefore chooses to fire somewhere else and knows about the grass option. If the grass environment is OK, then the cat can sit on it instead. With continual reinforcement from the top of the tree, the mat branch is available again in the future and so if the environment\nchanges and gives a more positive feedback, it can subsequently be selected instead. Sets of keys might still be required, for example, the sat neuron might be switched off for the mat path, but still open to the grass path. The figure also starts to look more like a state machine, although the inhibitor is not really a state change but more of a correction."
    }, {
      "heading" : "6 Binary-Analog Interface in the Cognitive Model",
      "text" : "The image-processing model of section 2 fits quite well with the cognitive model of the earlier research and the neuron binding, figure 4 in [8], in particular. The sensory input of the ensemble mass can be the binary image input and the tree structure can represent a slightly more sophisticated representation, possibly with the inclusion of analogue values. Any node can be a base tree node, when it also has the maximum importance there. It might also link with other trees, but maybe more at the periphery, when it would represent a less significant value. Therefore, a matching process with the tree layer can be more graded. The behaviour metric of section 4 does not fit quite as obviously. The sensory input must still come from below and it would probably have to represent the environment. So, the agent firstly experiences that environment before deciding on a plan. The behaviour selection and reasoning process must therefore occur afterwards, using the higher-level brain regions as well. There is still the more structured tree part however, that sits between the sensory inputs and the cortex processes, so what additional information would it store? For one thing, it can simply process the sensory input, to cross-reference it and convert it into a more analogue signal. If the base concept is more important and all nodes in the region can be both base nodes in one tree but branches in others, then the cross-referencing would produce a more coherent understanding.\nThe question like ‘why don’t we confuse a red circle and a blue square with a blue circle and a red square’ [4] could be answered if red or blue are base concepts in one tree and circle or square in another and they then also cross-reference each other. If a node is at the base of a tree, then it must be actively firing more times than leaf nodes, for example, to active the whole tree. If the tree is then accepted as part of a circuit, then this neuron will receive positive feedback, which\nmay be recognised more because of the greater firing effort. Leaf nodes would also have to be relevant to complete a circuit, but they may also be peripheral to a main concept if they have to fire much less frequently. One of the leaf nodes could also be the base of another tree, when both trees could be active at the same time and would relate to the same larger concept. So possibly, ‘red’, ‘blue’, ‘circle’ and ‘square’ can all be base nodes of trees and also branch nodes in other trees. This is illustrated in Figure 3. The concepts of red, blue, circle and square are all base concepts learned by the system and also have cross-referenced branches in other trees. Some neurons can have 1000 branches or more2. If the senses send signals about red and circle, for example, then the two central trees can complete a circuit, even if the concepts exist in other places as well. For reasoning then, there would also be influences from the higher-level processes that may be required to provide input, to activate all neurons, or for synchronisation.\nIt is therefore possible to give concepts graded strengths and also any arbitrary mix of the learned base set. This is already part of the concept trees research [11], where a leaf node in one tree links to a base node in another tree. Or to put it another way, if a base node branches to link with another tree, it represents itself in any other tree. The concept tree may be too semantic for the\n2 Prof K. Arai, SAI’14.\nlevel being considered here and their base concepts would not be ‘anchored’ because there is an idea that concept trees can re-join with each other. So possibly, the path description for Figure 3 is only 1 or 2 levels deep – the base node and its same branches, but it may help to explain coherence inside of the pattern ensembles. As semantics need to be processed however, the concept trees may occur further up the processing levels. Interestingly though, this linking process would fit with the lowest level of the original cognitive model [13]."
    }, {
      "heading" : "7 Conclusions",
      "text" : "This paper has mostly looked at the figure 4 ‘binary-analog’ structure of [8] and considered where it might be useful. This has led to some interesting results, both in the areas of image processing and higher-level reasoning. It has also helped when thinking about the temporal synchrony problems of neural binding and how separate parts can be re-combined into a more coherent whole. The behaviour metric has been updated to include a predictive part that may be used as part of a simulation. This allows the metric to be more intelligent and maybe helps to clarify the flexibility attributes. A behaviour decision is based on the agent’s current state, its abilities and also its memory. Negative feedback can be modelled as inhibitors, where the inhibitors can be used as real structural entities and not just to suppress a signal. They can be related to a specific response, but still be dynamic, because context will determine when they fire.\nIf the model of this and earlier papers is used, then (sub)concepts can in fact be represented individually, with lots of cross-linking representing the different contexts. With so many neurons in the brain, depending on how a scenario is broken down, why could it not accommodate this? For the pattern ensembles, base nodes that link as branches in other trees can simply represent themselves. This is a simplification of concept trees, where symbolically or conceptually, the node representation is only 1 or 2 levels deep. If there is a tree structure however, then there can be deeper paths that can relate to graded signal strengths, or even allow for different connection patterns over the same ensemble. Also key is the higher-level reinforcement, through reasoning,\nthat completes the circuits. Another rule could be that extending tree structures are knowledgebased and narrowing ones are experience-based. Although, as suggested in [8], some sort of residual effect could create an extending tree structure from experience, even though it would still be a process of removing nodes and links. So, it is still possible to fit the ideas together into a common model, even if it uses a lot of standard and compatible structures."
    } ],
    "references" : [ {
      "title" : "Acquiring Visibly Intelligent Behavior with Example- Guided Neuroevolution",
      "author" : [ "B.D. Bryant", "R. Miikkulainen" ],
      "venue" : "In Proceedings of the Twenty-Second National Conference on Artificial Intelligence",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2007
    }, {
      "title" : "Neural Mechanisms of Binding in the Hippocampus and Neocortex: Insights from Computational Models",
      "author" : [ "D.M. Cer", "R.C. O'Reilly" ],
      "venue" : "Handbook of binding and memory: Perspectives from cognitive neuroscience,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2006
    }, {
      "title" : "Character recognition in natural images",
      "author" : [ "T.E. de Campos", "B.R. Babu", "M. Varma" ],
      "venue" : "In Proceedings of the International Conference on Computer Vision Theory and Applications (VISAPP),",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2009
    }, {
      "title" : "The Neural Binding Problem(s)",
      "author" : [ "J. Feldman" ],
      "venue" : "Cognitive neurodynamics,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2013
    }, {
      "title" : "Markov models for pattern recognition: from theory to applications",
      "author" : [ "G.A. Fink" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2014
    }, {
      "title" : "The biological principles of swarm intelligence",
      "author" : [ "S. Garnier", "J. Gautrais", "G. Theraulaz" ],
      "venue" : "Swarm Intelligence",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2007
    }, {
      "title" : "A Single-Pass Classifier for Categorical Data, Special Issue on: IJCSysE Recent Advances in Evolutionary and Natural Computing Practice and Applications",
      "author" : [ "K. Greer" ],
      "venue" : "Int. J. Computational Systems Engineering, Inderscience,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2017
    }, {
      "title" : "New Ideas for Brain Modelling 3, available on arXiv at https://arxiv.org/abs/1612.00369",
      "author" : [ "K. Greer" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2016
    }, {
      "title" : "New Ideas for Brain Modelling, IOSR",
      "author" : [ "K. Greer" ],
      "venue" : "Journal of Engineering (IOSRJEN),",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2016
    }, {
      "title" : "Adding Context to Concept Trees, available on arXiv at http://arxiv.org/abs/1606.05597",
      "author" : [ "K. Greer" ],
      "venue" : "DCS",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2016
    }, {
      "title" : "Concept Trees: Building Dynamic Concepts from Semi-Structured Data using Nature-Inspired Methods",
      "author" : [ "K. Greer" ],
      "venue" : "Studies in Fuzziness and Soft Computing, Springer- Verlag, Germany,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2014
    }, {
      "title" : "A Metric for Modelling and Measuring Complex Behavioural Systems, IOSR",
      "author" : [ "K. Greer" ],
      "venue" : "Journal of Engineering (IOSRJEN),",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2013
    }, {
      "title" : "Turing: Then, Now and Still Key, in: X-S. Yang (eds.), Artificial Intelligence, Evolutionary Computation and Metaheuristics (AIECM) - Turing 2012",
      "author" : [ "K. Greer" ],
      "venue" : "Studies in Computational Intelligence,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "Symbolic Neural Networks for Clustering Higher-Level Concepts",
      "author" : [ "K. Greer" ],
      "venue" : "NAUN International Journal of Computers, Issue 3,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2011
    }, {
      "title" : "Benchmarks, Test Beds, Controlled Experimentation, and the Design of Agent Architectures, AI Magazine",
      "author" : [ "S. Hanks", "M.E. Pollack", "P.R. Cohen" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1993
    }, {
      "title" : "Tutorial on agent-based modelling and simulation, Journal of Simulation, Operational Research Society Ltd. 2010;4:151–162",
      "author" : [ "C.M. Macal", "M.J. North" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2010
    }, {
      "title" : "Introducing the TILEWORLD: Experimentally Evaluating Agent Architectures",
      "author" : [ "M. Pollack", "M. Ringuette" ],
      "venue" : "In Proceedings of the Eighth National Conference on Artificial Intelligence,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1990
    }, {
      "title" : "Crossmodal binding through neural coherence: implications for multisensory processing",
      "author" : [ "D. Senkowski", "T.R. Schneider", "J.J. Foxe", "A.K. Engel" ],
      "venue" : "Trends in Neurosciences,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2008
    }, {
      "title" : "Probabilistic Finite-State Machines-Part I",
      "author" : [ "E. Vidal", "F. Thollard", "C. de la Higuera", "F. Casacuberta", "R.C. Carrasco" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2005
    }, {
      "title" : "Some aspects of the theory of statistical control schemes",
      "author" : [ "E. Yashchin" ],
      "venue" : "IBM J. Res. Develop.,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1987
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "This paper continues the research that considers a new cognitive model, last updated in [8].",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 7,
      "context" : "The author has used this structure before in [8] and it is a type of entropy classifier.",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 6,
      "context" : "The paper [7] describes a classifier that is maybe more visual in nature.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 6,
      "context" : "The author’s own papers that are quoted [7]-[14] are all relevant to the research of this paper.",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 13,
      "context" : "The author’s own papers that are quoted [7]-[14] are all relevant to the research of this paper.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 10,
      "context" : "The Concept Tree [11] of section 5 has previously been compared to Markov Models [5] and is very similar to them, including the construction rule that can be implicit in a Markov Model.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 4,
      "context" : "The Concept Tree [11] of section 5 has previously been compared to Markov Models [5] and is very similar to them, including the construction rule that can be implicit in a Markov Model.",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 18,
      "context" : "The paper [22] is a survey of these and notes that there are many different types that obey different rules.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 19,
      "context" : "An earlier paper on control theory [23] posts some interesting equations that are maybe similar to ones in this paper.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 11,
      "context" : "The earlier paper [12] notes some references, including [1][15][18] and [19].",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : "The earlier paper [12] notes some references, including [1][15][18] and [19].",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 14,
      "context" : "The earlier paper [12] notes some references, including [1][15][18] and [19].",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 15,
      "context" : "The earlier paper [12] notes some references, including [1][15][18] and [19].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 16,
      "context" : "The earlier paper [12] notes some references, including [1][15][18] and [19].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 17,
      "context" : "The paper [20] introduces the idea of temporal synchrony and synchronised oscillatory activity as important for multisensory perception.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 2,
      "context" : "The image processing of section 2 has already been tried in [3], where they tested the full dataset.",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 3,
      "context" : "For example, questions like ‘why don’t we confuse a red circle and a blue square with a blue circle and a red square’ [4] need to be answered.",
      "startOffset" : 118,
      "endOffset" : 121
    }, {
      "referenceID" : 3,
      "context" : "Some cognitive models for the real brain include temporal logic or predicate calculus rules [4] to explain how variables can bind with each other and reasoning can be obtained.",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 1,
      "context" : "The paper [2] describes a theory that is quite similar.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 5,
      "context" : "An earlier paper introduced a set of equations that were based on the collective behaviour research of [6].",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 11,
      "context" : "This led to a set of equations by the author [12] for modelling these types of entity.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 11,
      "context" : "The behaviour metric was tested in [12].",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 13,
      "context" : "The paper [14] described the idea of a ‘Concept Base’ that was later developed into a ‘Concept Tree’ [11][10].",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 10,
      "context" : "The paper [14] described the idea of a ‘Concept Base’ that was later developed into a ‘Concept Tree’ [11][10].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 9,
      "context" : "The paper [14] described the idea of a ‘Concept Base’ that was later developed into a ‘Concept Tree’ [11][10].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 13,
      "context" : "Therefore, definite structure can be added to the initial behaviour set, even as part of the symbolic neural network [14].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 7,
      "context" : "The image-processing model of section 2 fits quite well with the cognitive model of the earlier research and the neuron binding, figure 4 in [8], in particular.",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 3,
      "context" : "The question like ‘why don’t we confuse a red circle and a blue square with a blue circle and a red square’ [4] could be answered if red or blue are base concepts in one tree and circle or square in another and they then also cross-reference each other.",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 10,
      "context" : "This is already part of the concept trees research [11], where a leaf node in one tree links to a base node in another tree.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 12,
      "context" : "Interestingly though, this linking process would fit with the lowest level of the original cognitive model [13].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 7,
      "context" : "This paper has mostly looked at the figure 4 ‘binary-analog’ structure of [8] and considered where it might be useful.",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 7,
      "context" : "Although, as suggested in [8], some sort of residual effect could create an extending tree structure from experience, even though it would still be a process of removing nodes and links.",
      "startOffset" : 26,
      "endOffset" : 29
    } ],
    "year" : 2017,
    "abstractText" : "This paper continues the research that considers a new cognitive model. In particular, it considers the neural binding structure of an earlier paper. To help with this, the paper describes some new methods in the areas of image processing and high-level behaviour simulation. The work is all based on earlier research by the author and the new additions are intended to fit in with the overall design. For image processing, a grid-like structure is used with ‘full linking’. Each cell in the classifier grid stores a list of all other cells it gets associated with and this is used as the learned image that new input is compared to. For the behaviour metric, a new prediction equation is suggested, as part of a simulation, that uses feedback and history to dynamically determine its course of action. While the new methods are from widely different topics, both can be compared with the binary-analog type of interface that is the main focus of the paper. It is suggested that the simplest of linking between a tree and ensemble can explain neural binding and variable signal strengths.",
    "creator" : "Microsoft® Word 2016"
  }
}