{
  "name" : "1611.06777.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Effective Deterministic Initialization for k-Means-Like Methods via Local Density Peaks Searching",
    "authors" : [ "Fengfu Li", "Hong Qiao", "Bo Zhang" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms—clustering, k-means, k-medoids, local density peaks searching, deterministic initialization\nF"
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "C LUSTERING methods are important techniques for ex-ploratory data analysis with wide applications ranging from data mining [1], vector quantization [2], dimension reduction [3], to manifold learning [4]. The aim of these methods is to partition data points into clusters so that data in the same cluster are similar to each other while data in different clusters are dissimilar. The approaches to achieve this aim include methods based on density estimation such as DBSCAN [5], mean-shift clustering [6] and clustering by fast search and find of density peaks [7], methods that recursively find nested clusters [8], and partitional methods based on minimizing objective functions such as k-means [9], k-medoids [10], the EM algorithm [11] and ISODATA [12]. For more information about clustering methods, see [1], [9], [13], [14].\nAmong partitional clustering methods, the k-means algorithm is probably the most popular and most widely\n• F. Li and B. Zhang are with LSEC and Institute of Applied Mathematics, AMSS, Chinese Academy of Sciences, Beijing 100190, China (email: b.zhang@amt.ac.cn).\n• H. Qiao is with State Key Lab of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China and the CAS Center for Excellence in Brain Science and Intelligence Technology (CEBSIT), Shanghai 200031, China (email: hong.qiao@ia.ac.cn).\nstudied one [15]. Given a set of m data points X in the Euclidean space with dimensionality p (thus X ∈ Rp×m) and the number of clusters, k, the partitional clustering problem aims to determine a set of k data points in Rp so as to minimize the mean squared distance from each data point to its nearest center. The k-means algorithm solves this problem by a simple iterative scheme for finding a local minimal solution. It is of many advantages, including conceptually simple and easy to implement, linear time complexity and being guaranteed to converge to a local minima (see, e.g. [16], [17], [18], [19]). However, it also has the following four main drawbacks:\n1) it requires the user to provide the cluster number k in advance; 2) it is highly sensitive to the selection of initial seeds due to its gradient descent nature; 3) it is sensitive to outliers due to the use of the squared Euclidean distance; 4) it is limited to detecting compact, hyperspherical clusters that are well separated.\nFig. 1(a)-(d) shows the above issues on toy data sets. Many approaches have been proposed to deal with these issues individually. The first issue can be partially remedied by extending the k-means algorithm with the estimation of the number of clusters. x-means [20] is one of the first such attempts that use the splitting and merging rules for the number of centers to increase and decrease as the algorithm\nar X\niv :1\n61 1.\n06 77\n7v 1\n[ cs\n.L G\n] 2\n1 N\nov 2\n01 6\n2 (a) improper k (b) improper seeds Best centers Estimated Centers (c) outliers effect (d) non-spherical distribution\n(e) estimated k (f) carefully selected seeds\nNormal data Detected outliers\n(g) outliers detection/removal (h) manifold-based clustering\nFig. 1. Illustration of the drawbacks of k-means ((a)-(d)) and the solution of our methods ((e)-(h)). First row shows the following cases for which k-means fails: (a) k is unreasonably given, (b) the initial seeds (marked by blue ◦) are improperly selected, (c) the effect of outliers is strong, and (d) the data distribution is non-spherical. Second row shows our solutions: (e) an appropriate k is automatically estimated, (f) the initial seeds that are geometrically close to the center of the clusters are selected, (g) outliers are detected and removed before clustering, and (h) the manifold-based distance is used as the dissimilarity measure instead of the squared Euclidean distance to deal with manifold-distributed clusters.\nproceeds. g-means [21] works similarly as x-means except it assumes that the clusters are generated from the Gaussian distributions. dip-means [22], however, only assumes each cluster to admit a unimodal distribution and verifies this by Hartigans’ dip test [23].\nTo address the second issue of k-means, a general approach is to run k-means repeatedly for many times and then to choose the model with the smallest mean square error. However, this can be time-consuming when k is relatively large. To overcome the adverse effect of randomly selecting the initial seeds, many of adaptive initialization methods have been proposed. The k-means++ algorithm [24] aims to avoid poor quality data partitioning during the restarts and achieves O(log k)-competitive results with the optimal clustering. The Min-Max k-means algorithm [25] deals with the initialization problem of k-means by alternating the objective function to be weighted by the variance of each cluster. Methods such as PCA-Part and Var-Part [26] use a deterministic approach based on PCA and a variance of data to hierarchically split the data set into k parts where initial seeds are selected. For many other deterministic initialization methods, see [18], [19], [27] and the references quoted there.\nThe third drawback of k-means, that is, its sensitivity to outliers, can be addressed by using more robust proximity measure [28], such as the Mahalanobis distance and the L1 distance rather than the Euclidean distance. Another remedy for this issue is to detect the outliers and then remove them before the clustering begins. The outlier removal clustering algorithm [29] uses this idea and achieves a better performance than the original k-means method when dealing with overlapping clusters. Other outliers-removing cluster algorithms can be found in [30]and the references quoted there.\nThe last drawback of k-means that we are concerned with is its inability to separate non-spherical clusters. This issue can be partially remedied by using the Mahalanobis distance to detect hyperellipsoidal clusters. However, it is difficult to optimize the objective function of k-means with non-Euclidean distances. k-medoids [10], as a variant of kmeans, overcomes this difficulty by restricting the centers to be the data samples themselves. It can be solved effectively (but slowly) by data partitioning around medoids (PAM) [31], or efficiently (but approximately optimally) by CLARA [31]. In addition, the k-medoids algorithm makes it possible to deal with manifold-distributed data by using neighborhood-based (dis)similarity measures. However, kmedoids also has the first two drawbacks of k-means.\nIn this paper, we propose a novel method named Local Density Peaks Searching (LDPS) to estimate the number of clusters and to select high quality initial seeds for both k-means and k-medoids. A novel measure named local distinctiveness index (LDI) is proposed to characterize how distinctive a data point is compared with its neighbors. The larger the LDI is, the more locally distinguishable the data point is. Based on the LDI, we characterize the local density peaks by high local densities and high LDIs. A score function is then given with the two measures to quantitatively evaluate the potential of a data point to be a local density peak. Data points with high scores are founded and regarded as local density peaks. By counting the number of local density peaks, a reasonable number of clusters can be obtained for further clustering with k-means or k-medoids. In addition, the local density peaks can also be served as good initial seeds for the k-means or k-medoids clustering algorithm. As a result, the first two drawbacks of k-means or k-medoids are thus remedied.\nIn analogy with the searching of local density peaks, we\n3 characterize outliers with low local densities but high LDIs. Another score function to quantitatively evaluate the potential of a data point to be an outlier is given. Based on the scores, outliers can be effectively detected. To minimize the effect of outliers, we remove them before clustering begins. Thus, the third issue of k-means is remedied. Fig. 1 (e)-(h) shows the clustering results of our methods compared with the original k-means algorithm.\nThe remainder of the paper is organized as follows. Section 2 briefly reviews some related works. In Section 3, we give a step by step introduction of our initialization framework for k-means and k-medoids. Two novel clustering algorithms, called LDPS-means and LDPS-medoids, are proposed in Section 4. They are based on the LDPS algorithm together with the k-means and k-medoids clustering algorithms. Section 5 gives a theoretical analysis on the performance of the proposed methods. Experiments on both synthetic and real data sets are conducted in Section 6 to evaluate the effectiveness of the proposed methods. Final conclusions and discussions are given in Section 7."
    }, {
      "heading" : "2 RELATED WORKS",
      "text" : "2.1 The k-means algorithm\nGiven a data set X = {x(i) ∈ Rp|i = 1, 2, . . . ,m}, the k-means algorithm aims to minimize the Sum of Squared Error (SSE) with k cluster centers C = {c(j) ∈ Rp|j = 1, 2, . . . , k}:\nC∗ = arg min C SSE(X|C) = m∑ i=1 k∑ j=1 sij d(x (i), c(j)) (1)\nwhere S = {sij |i = 1, 2, . . . ,m; j = 1, 2, . . . , k} is the assignment index set, sij = 1 if x(i) is assigned to the jth cluster and sij = 0 if otherwise, d(x, c) is a dissimilarity measure defined by the Euclidean distance d(x, c) = ‖x − c‖22. To solve (1), the k-means algorithm starts from a set of k randomly selected initial seeds C(0) and iteratively updates the assignment index set S with\nsij = 1, if j = arg min1≤l≤k d(x (i), c(l));\n0, otherwise (2)\nand the cluster centers C with\nc(j) = 1\nmj m∑ i=1 sijx (i). (3)\nHere, mj = ∑ i sij is the number of points that are assigned to the jth cluster. The update procedure stops when S has no change or SSE changes very little between the current and previous iterations. Finally, the algorithm is guaranteed to converge [16] at a quadratic rate [17] to a local minima of the SSE, denoted as SSE∗.\n2.2 Variants of the k-means algorithm\n2.2.1 k-medoids k-medoids [10] has the same objective function (1) as kmeans. However, it selects data samples as centers (also called medoids), and the pairwise dissimilarity measure\nd(x, c) is no longer restricted to the square of the Euclidean distance. This leads to a slightly different but more general procedure in updating the centers. Specifically, if we denote the indices of the points in the jth cluster as I(j)={i|sij = 1}, then k-medoids updates the center indices as\nIcj = arg min i∈I(j) ∑ l∈I(j) d(x(l),x(i)) (4)\nwhere Icj denotes the index of the jth center, that is, c (j) = x(I c j ). The assignments in k-medoids is updated in the same way as in k-means. Compared with k-means, k-medoids is capable to deal with diverse data distributions due to its better flexibility of choosing dissimilarity measures. For example, k-means fails to discover the underlying manifold structure of the manifold-distributed data, while k-medoids may be able to do so with manifold-based dissimilarity measures.\n2.2.2 x-means and dip-means\nx-means [20] is an extension of k-means with the estimation of the number k of clusters. It uses the splitting and merging rules for the number of centers to increase and decrease as the algorithm proceeds. During the process, the Bayesian Information Criterion (BIC) [32] is applied to score the modal. The BIC score is defined as follows:\nBIC(M |X) = L(X|M)− l 2 log(m),\nwhere L(X|M) is the log-likelihood of the data set X according to the modal M and l = k(p+1) is the number of parameters in the modal M with the dimensionality p and k cluster centers. x-means chooses the modal with the best BIC score on the data. The BIC criterion works well only for the case where there are a plenty of data and well-separated spherical clusters.\ndip-means [22] is another extension of k-means with the estimation of the number of clusters. It assumes that each cluster admits a unimodal distribution which is verified by Hartigans’ dip test. The dip test is applied in each split candidate cluster with a score function\nscorej =\n{ 1 |vj | ∑ x∈vj dip(F (x) α ), |vj | mj ≥ vthd\n0 , otherwise.\nHere, vj is the set of split viewers, α is a statistic significant level for the dip test. The candidate with the maximum score is split in each iteration. It works well when the data set has various structural types. However, it would underestimate k when the clusters are closely adjacent.\n2.2.3 k-means++\nk-means++ is a popular variant of k-means with adaptive initialization. It randomly selects the first center and then sequentially chooses x ∈X to be the jth (2 ≤ j ≤ k) center with probability D(x)2/( ∑ x′ D(x\n′)2), where D(x) is the minimum distance from x to the closest center that we have already chosen. The capability of the method is reported to be O(log k)-competitive with the optimal clustering.\n4 Data Points; k(optional) Kernel Density Estimation (Sec. 3.1)\nLocal\nDistinctiveness\nIndex\n(Sec. 3.1)\nLocal Density\nPeaks Searching\n(Sec. 3.2)\nOutliers Detection\n(Sec. 3.5)\nk Given ?\nSelecting\nInitial Seeds\n(Sec. 3.4)\nOutliers Removal\n(Sec. 3.5)\nEstimating k\n(Sec. 3.3)\nN\nY Updating\nAssignments\n(Sec. 2.1 or 2.2.1)\nStop ?\nUpdating Centers (Sec. 2.1 or 2.2.1)\nN\nY Optimal Centers;\nOutliers\nInput\nI: Initialization II: Clustering\nOutput\nFig. 2. The clustering framework as a combination of the local density and local distinctiveness index based initialization and the k-means type clustering. The number k of clusters is an optional input. The main part of the framework is the initialization stage which serves as estimating k, selecting initial seeds and detecting/removing outliers for the clustering stage. The clustering stage assists to find an optimized solution. Note that Y=yes, N=no."
    }, {
      "heading" : "2.3 Clustering by fast search and find of density peaks",
      "text" : "Clustering by fast search and find of density peaks (CFSFDP) [7] is a novel clustering method. It characterizes a cluster center with a higher density than its neighbors and with a relatively large distance from other points with high densities. The density ρi of x(i) is defined as\nρi = m∑ j=1 χ(dij − dc) (5)\nwhere χ(z) = 1 if z < 0 and χ(z) = 0 if otherwise, dij is the distance between x(i) and x(j) and dc is a cutoff distance. Intuitively, ρi equals to the number of the points whose distance from x(i) is less than dc. Another measure δgi , which we call the global distinctiveness index (GDI), is the minimum distance between x(i) and any other points with high densities:\nδgi = min j:ρj>ρi dij . (6)\nFor the point with the peak density, its GDI is defined as δgi = maxj dij . Only those points with relatively high local densities and high GDIs are considered as cluster centers. By combining the local density and GDI, a possible way of choosing the cluster centers is to define\nγi = ρi · δgi\nand then to choose the points with high values of γ to be the centers. After the cluster centers are determined, each of the remaining points is assigned to the cluster if it is the nearest neighbor of the cluster center with a higher density.\nThough the CFSFDP method is simple, it is powerful to distinguish clusters with distinct shapes. In addition, it is insensitive to outliers due to a cluster halo detection procedure. However, CFSFDP also has some disadvantages, such as: 1) it does not give a quantitative measure of how to choose the cluster centers automatically, 2) its assignment step is not clear compared with the k-means and k-medoids algorithms, 3) it is sensitive to the parameter dc, and 4) it cannot serve as an initialization method for k-means or k-medoids when the number k of the clusters is given in advance."
    }, {
      "heading" : "3 LOCAL DENSITY AND LDI BASED INITIALIZATION",
      "text" : "FOR k-MEANS-LIKE METHODS\nIn this section, we propose an initialization framework for the k-means and k-medoids algorithms. Fig. 2 illustrates the overall diagram for the clustering framework, where Part I is the initialization framework. We first introduce two basic measures: one is the local density and the other is the local distinctiveness index (LDI). Based on these two measures, we propose a local density peaks searching algorithm to find the local density peaks, which can be used to estimate the number k of clusters as well as to serve as the initial seeds for k-means or k-medoids. In addition, outliers can be detected and removed with the help of the measures. Below is a detailed description of the proposed method."
    }, {
      "heading" : "3.1 Local density and local distinctiveness index",
      "text" : "Local density characterizes the density distribution of a data set. We use the kernel density estimation (KDE) [33] to compute the local densities. Suppose the samples X are generated from a random distribution with a unknown density f . Then KDE of f at the point x(i) is given by\nρ i\n= f̂(x(i)) = 1\nmh m∑ j=1 K( dij h ), (7)\nwhere h is a smoothing parameter called bandwidth, dij is the distance between x(i) and x(j), and K(z) (z ∈ R) is a kernel function satisfying that 1) K(z) ≥ 0, 2) ∫ K(z)dz = 1 and 3) K(−z) = K(z). A popular choice for the kernel function is the standard Gaussian kernel:\nK(z) = 1√ 2π exp(−z 2 2 ). (8)\nIt is known that the Gaussian kernel is smooth. In addition, compared with the uniform kernel K(z) = [1/(b− a)]I{a≤z≤b} which is used in [7] (see (5)), the Gaussian kernel has a relatively higher value when |z| is small and thus keeps more local information near zero. In what follows, we will use the Gaussian kernel.\nBased on the local density, we propose a new measure called the local distinctiveness index (LDI) to evaluate the distinctiveness of the point x(i) compared with its r-neighbors\n5 x1 0 0.2 0.4 0.6 0.8 1 x 2 0 0.2 0.4 0.6 0.8 1 2 13 10 15 34 9 14 7 8 11 12 5 6 1\n(a) The R15 data set.\nLocal Density 0 3 6 9 12 15\nG lo b a l D is ti n ct iv en es s In d ex\n0\n0.1\n0.2\n0.3\n0.4\n2 4\n5\n8\n3\n15 10 14\n1\n7\n12\n6\n11 9 13\n(b) ρ-δg graph of R15.\nNormalized Local Density 0 0.2 0.4 0.6 0.8 1\nL o ca l D is ti n ct iv en es s In d ex\n0\n0.2\n0.4\n0.6\n0.8\n1 1 2\n4 93 1114 5\n15 12 6 810 713\n(c) ρ-δl graph of R15.\nFig. 3. Comparison of GDI and LDI on the R15 data set. (a) The R15 data set which owns 15 clusters. Points marked with (red) ”+” own highest local densities within their neighborhoods. (b) The dominant effect of GDI. Although the 15 labeled points have similar highest local densities in their neighborhoods, their GDIs vary a lot from 0.1 to 0.4. The GDIs of points 1, 2, 4, 6, and 8 are dominated by points 3, 5 and 7 which own relatively larger densities than the aforementioned points. (c) The LDI wiped out the dominant effect by choosing proper r; where r = 0.1d∗. Now, labeled points almost all have largest LDIs. As a result, they are quantitatively more distinctive than the unlabeled points.\nNr(x (i)) = {x(j) | 0 < dij ≤ r}. We first define the local dominating index set (LDIS):\nLDIS(x(i)) = {j|x(j) ∈ Nr(x(i)) and ρj > ρi}.\nIntuitively, LDIS(x(i)) indicates which of the points in the r-neighbors of x(i) dominates x(i) with the local density measure. Based on LDIS, we can define LDI as follows:\nδli = 1 if LDIS(x (i)) = ∅,\nmin j∈LDIS(x(i))\ndij/r elsewise. (9)\nwhere ∅ denotes the empty set. With the definition (9), δl lies in (0, 1]. The point x(i) has the biggest LDI if its LDIS is empty, which means that x(i) is not dominated by any other point, that is, either Nr(x(i)) is empty or ρj ≤ ρi for any j ∈ Nr(x(i)). For any other point, its LDI is computed as the minimal distance between the point and the dominating points, divided by the local parameter r. When r is set to be larger than d∗ = maxi,j dij , the LDI will degenerate to the GDI since Nr(x(i)) = X for any x(i). Thus, LDI is a generalization of GDI. However, LDI characterizes the local property of the data distribution, but GDI does not give us any local information of the data distribution. Fig. 3 shows the difference between GDI and LDI. The GDI of the point with the highest density is defined as the global maximum distance between the point and all other points, and GDIs of the other points are defined by their maximum distance to the points with higher densities. Thus, even though two points have similar highest local densities within their neighborhoods, their GDIs may have a big difference. We call this phenomenon as the dominant effect of GDI (see Fig. 3(a)-(b)). Fortunately, the dominant effect of GDI can be eliminated by using LDI with the appropriate choice of the parameter r since LDI of a point is only affected by the points within its r-neighborhood (see Fig. 3(c)). Thus, LDI will be quantitatively more distinctive than GDI when the number of clusters is large."
    }, {
      "heading" : "3.2 Local density peaks searching",
      "text" : "In the CFSFDP algorithm, a density peak is characterized by both a higher density than its neighbors and a relatively\nlarge distance from the points with higher densities. We use the similar idea to search for the local density peaks by assuming that a local density peak should have a high local density and a large local distinctiveness index. Fig. 4 gives an intuitive explanation of how this works.\nTo find the local density peaks, we introduce a quantitative measure γc to evaluate the potential of a point to be a local density peak. γc is defined as\nγci =\n( 1− 1\n2 (1− ρi)2 −\n1 2 (1− δli)2\n)2\n= 1− (1− ρi)(1− δli)︸ ︷︷ ︸ I −1 2 (ρi − δli)︸ ︷︷ ︸ II 2  2 . (10)\nHere, ρi = ρi/max i ρi is the normalized local density.\nBy definition (10), γc lies in [0, 1] and is an increasing function of ρ and δl. Further, the term I in the second equation in the definition (10) is used for selecting the high local density and high LDI points. For instance, the point with a high density (ρ > 0.9) and a high LDI (δl > 0.9) will have the γc value being close to 1. On the contrary, the point with a low local density (ρ < 0.1) and a low local LDI (δl < 0.1) will have the γc value being close to 0. The term II in the second equation in the definition (10) is to balance the influence of the local density and LDI. As a consequence, the points with balanced local densities and LDIs are preferred. For example, γc|ρ=0.5,δl=0.6 equals to 0.64, which is much greater than γc|ρ=1,δl=0.1 that equals to 0.36.\nWe now analyze the local density and LDI distribution in Fig. 4. First, the point which lies in the centroid of a cluster will have the highest local density and LDI and thus is regarded as a local density peak. Secondly, points which are close to the centroid will have high local densities and low LDIs since their LDISs all include the local density peak, and their distances to the local density peak are small. Finally, points which are far away from the centroid will have relatively low local densities. Quantitatively, the local density peaks in a local area will have high γc values, but the other points will have relatively much smaller γc values by the definition (10). Thus, there should have a big gap in\n6 1 2 3 4\n5\n6\n7\n(local density peak) high density, high LDI\nhigh densities, low LDIs\nlow densities, low LDIs\nlow densities, high LDIs\nFig. 4. Local density and LDI distribution. Point 1 is the unique density peak which has the highest local density and highest LDI among all of the points. Points 2 and 3 have relatively large densities, but their LDIs are very small since their LDISs include Point 1, and their distances to Point 1 are very small. Points 4-7 all have relatively low densities, but their LDIs are different. Points 4 and 5 are relatively close to the center and thus their LDIs are small. Points 6 and 7, however, are far away from the cluster, and as a result, their LDIs are relatively high.\nterms of the γc value between the local density peaks and the other points around the local density peakes. By observing the gap, the local density peaks can be found. Based on the above discussion we propose the Local Density Peaks Searching (LDPS) algorithm which is stated in Algorithm 1.\nAlgorithm 1 The LDPS Algorithm Input: dissimilarity matrix D, bandwidth h and local pa-\nrameter r Output: Ildp and τ∗\n1: Compute local densities ρ with (7) 2: Compute local distinctiveness index δl by (9) 3: Compute γc with ρ and δl by (10) 4: Sort γc with the descending order:\n[γcs, Ics] = sort(γc | ”descend order”)\n5: Compute the gaps τ (negative difference of γcs):\nτi = −(∆γcs)i = γcsi − γcsi+1 6: Observe the biggest gap and decide the number of the\nlocal density peaks:\nk = arg max i τi\n7: Search for the local density peaks with indices:\nI ldp = k ∪ i=1 {Icsi }\n8: Compute the maximum gap of the γc value between the local density peaks and the other points:\nτ∗ = τk\n9: return Ildp and τ∗\nIn Algorithm 1, γcs is the sorted vector of γc with the descending order, Ics is the sorted index, that is, γcsi = γ c Icsi\n, and ∆ is the numerical difference operator:\n∆f(zj) = f(zj+1)− f(zj)\nzj+1 − zj ."
    }, {
      "heading" : "3.3 Estimating k via local density peaks searching",
      "text" : "k-means and k-medoids require the number k of the clusters as an input. However, it is not always easy to determine\nthe best value of k [20]. Thus, learning k is a fundamental issue for the clustering algorithms. Here, we use the LDPS algorithm for estimating k which is equal to the number of the local density peaks.\nτ∗ in Algorithm 1 is the minimum gap of the γc value between the selected local density peaks and the other points and thus treated as a measure of how distinctive the local density peaks are. The bigger the value of τ∗ is, the better the estimated k will be. If the resulting τ∗ is too small, the procedure for estimating k will fail. In this case, we set the estimated k as −1.\nCompared with x-means and dip-means which are incremental methods that use the splitting/merging rules to estimate k, our method does not have to split the data set into subsets and is thus fast and more stable. Further, CFSFDP uses the two-dimensional ρ-δg decision graph (see Fig. 3(b)) together with manual help to select the cluster centers, while our method estimates k quantitatively and automatically without any manual help."
    }, {
      "heading" : "3.4 Selecting initial seeds with local density peaks",
      "text" : "Choosing appropriate initial seeds for the cluster centers is a very important initialization step and plays an essential role for k-means and k-medoids to work properly. Here, we assume that we have already known the true number k of the clusters (either given by the user or estimated by using the LDPS algorithm). Let us denote by k∗ the true number of the clusters used for the clustering algorithms.\nIf the true number k∗ of the clusters is not given in advance by the user, we use the estimated k to be k∗. In addition, we take the local density peaks {x(i)|i ∈ I ldp} obtained by the LDPS algorithm to be the initial seeds. In fact, we select the first k∗ elements with the leading γc values as the initial seeds, that is,\nIs = k∗\n∪ i=1 {Icsi }, (11)\nwhere Is is the indices of the initial seeds. Geometrically, the initial seeds found by (11) will have relatively high local densities as well as high LDIs. Thus, they avoid being selected as outliers (due to the high local densities) and avoid lying too close to each other (due to the high GDIs). As a result, these initial seeds can lead to very fast convergence of the k-means algorithm when the clusters are separable. This advantage will be verified by the experiments in Section 6."
    }, {
      "heading" : "3.5 Outliers detection and removal",
      "text" : "Outliers detection and removal can be very useful for kmeans to work stably. Here, we develop a simple algorithm to detect and remove the outliers, based on the local density and LDI. First, we define the γo value as follows:\nγoi =\n( 1− 1\n2 ρ2i −\n1 2 (1− δli)2\n)2 . (12)\nThis definition is very similar to that of γc except that γo is a decreasing function of ρ and γc is increasing with ρ increasing. The points with low densities but high LDIs will get high γo values and are thus regarded as outliers.\n7 Secondly, we use a threshold of γo, denoted by γot , to detect the outliers with the principle that γo of the outliers should be greater than γot . For example, if we set γ o t = 0.95, then the points with ρ < 0.1 and δl > 0.8 will have the γo values being greater than 0.95. Thus, they are treated as outliers. The set of outliers and the set of the corresponding indices are denoted as Xo and Io, respectively. The other points with higher densities or lower LDIs will get relatively smaller values of γo and therefore will be treated as normal samples.\nFinally, we remove the outliers from the data set X before k-means proceeds, that is, setting X=X\\Xo={x(i)|1 ≤ i ≤ m and i /∈ Io}."
    }, {
      "heading" : "3.6 Model selection for the LDPS algorithm",
      "text" : "The accuracy of the estimation of k obtained by the LDPS algorithm depends heavily on the bandwidth h for the local density estimation and the neighborhood size r for the computation of LDI. Denote by θ = (h, r) the (normalized) parameters, where h = h/d∗ and r = r/d∗. Fig. 5(a) shows the results of the LDPS algorithm with different parameters θ on the R15 data set. As seen in Fig. 5(a), the estimated k is equal to the ground truth k∗ only when the parameters θ are properly selected."
    }, {
      "heading" : "3.6.1 Parameters choosing by grid search",
      "text" : "In many real applications, we do not know beforehand what the true number k∗ of the clusters is. Therefore, we need to define certain criteria to evaluate the estimated number k of clusters and do model selection to optimize the criteria.\nHere, we utilize τ∗ as a criterion to assess how good the estimated k will be. As discussed in Section 3.3, τ∗ indicates the maximum gap of the γc value between the selected local density peaks and the other points. Mathematically, it can be written as a function of the dissimilarity matrix D with parameters θ (see Algorithm 1). The parameters that maximize τ∗ will result in the most distinctive local density peaks. Thus, we choose the parameters by solving the optimization problem:\nθ̂ = arg max θ\nτ∗(D;θ). (13)\nFig. 5(b) shows the τ∗ value on the R15 data set with respect to different θ, where the dissimilarity measure is the square of the Euclidean distance.\nThere are no explicit solutions for the optimization problem (13). A practical way of solving this problem approximately will be the grid search method [34], in which various pairs of the (h, r) values are tried and the one that results in the maximum τ∗ is picked. Due to the local property of the density estimation and LDI, h and r are generally set to lie in a small range such as (0, 0.2] and [0.05, 0.5], respectively. Take the R15 data set as an example, we equally split h and r into 10 fractions, respectively and then use a grid search procedure to maximize τ∗. The maximum gap we get is τ∗ = 0.53 with θ̂ = (0.02, 0.1)."
    }, {
      "heading" : "4 LDPS-MEANS AND LDPS-MEDOIDS",
      "text" : "In the previous section, we proposed the LDPS algorithm for initializing k-means and k-medoids. For k-means, the input dissimilarity matrix D is the square of the Euclidean distance. For k-medoids, any kind of dissimilarity matrixD can be used as input. In view of this difference, they use different procedures for updating the cluster centers.\nIn this section, we propose two novel clustering algorithms, LDPS-means (Algorithm 2) and LDPS-medoids (Algorithm 3), as a combination of the LDPS initialization algorithm (Algorithm 1) with the clustering procedures of k-means and k-medoids, respectively. Their clustering framework is implemented as in Fig. 2.\nAlgorithm 2 The LDPS-means Algorithm Input: X , k∗(optional), h, r, γot Output: C∗, Xo, and τ∗\n1: Perform the LDPS algorithm to get ρ, δl, Ildp and τ 2: if k∗ is not given then 3: Estimate k with k = |Ildp|, and set k∗ = k 4: end if 5: Select k∗ initial seeds with indices Is by (11); 6: Compute τ∗:\nτ∗ = τk∗\n7: Detect outliers Xo with γot 8: Remove the outliers: X = X\\Xo 9: while not converging do\n10: Compute assignments S by (2) 11: Update the centers C by (3) 12: end while 13: return C∗ = C, Xo and τ∗\nLDPS-means is a powerful method to deal with spherically distributed data. However, it is unable to separate nonspherically distributed clusters. LDPS-medoids can deal with this issue by choosing appropriate dissimilarity measures. In the next subsection, we will discuss how to choose an appropriate dissimilarity measure for the LDPS-medoids algorithm."
    }, {
      "heading" : "4.1 Dissimilarity measures for LDPS-medoids",
      "text" : "A dissimilarity measure is the inverse of a similarity measure [35], which is a real-word function that quantifies the similarity between two objects. It can be viewed as a kind of distance without satisfying the distance axioms. It assesses the dissimilarity between data samples, and the larger it is, the less similar the samples are.\n8 Algorithm 3 The LDPS-medoids Algorithm Input: D, k∗(optional), h, r, γot Output: Ic∗, Io and τ∗\n1: Perform the LDPS algorithm to get ρ, δl, Ildp and τ 2: if k∗ is not given then 3: Estimate k with k = |Ildp|, and set k∗ = k 4: end if 5: Select k∗ initial seeds with indices Is by (11); 6: Compute τ∗:\nτ∗ = τk∗\n7: Detect outliers with indexes Io 8: Remove the outliers: D = (dij)i,j /∈Io 9: while not converging do\n10: Compute assignments S by (2) 11: Update the medoids by (4) 12: end while 13: return Ic∗ = Ic, Io, and τ∗\nChoosing appropriate dissimilarity measures for clustering methods is very crucial and task-specific. The (square of) Euclidean distance is the most commonly used dissimilarity measure and suitable to deal with spherically distributed data. The Mahalanobis distance is a generalization of the Euclidean distance and can deal with hyper-ellipsoidal clusters. If the dissimilarity measure is the L1 distance, k-medoids will get the same result as k-median [36]. For manifold distributed data, the best choice for dissimilarity measures would be the manifold distance [37] which is usually approximated by the graph distance based on the - neighborhood or the t-nearest-neighborhood (t-nn). Graphbased k-means [38] uses this measure. For images, one of the most effective similarity measures may be the complex wavelet structural similarity (CW-SSIM) index [39], which is robust to small rotations and translations of images. In [40], a combination of the manifold assumption and the CW-SSIM index is used for constructing a new manifold distance named geometric CW-SSIM distance, which shows a superior performance for visual object categorization tasks. Other cases include the cosine similarity which is commonly used in information retrieval, defined on vectors arising from the bag of words modal. In many machine learning applications, kernel functions such as the radial basis function (RBF) kernel can be viewed as similarity functions.\nIn the section on experiments, whenever manifold-based dissimilarity measures are needed, we always use t-nn as the neighborhood constructor for approximating the manifold distance. t is generally set to be a small value such as 3, 5 and 8."
    }, {
      "heading" : "5 PERFORMANCE ANALYSIS",
      "text" : "In this section, we analyze the performance of the local density and LDI based clustering methods. To simplify the analysis, we make the following assumptions:\n1) the clusters are spherical-distributed, 2) each cluster has a constant number (m0) of data points, 3) the clusters are non-overlapping and can be separated by k-means with appropriate initial seeds.\nWe use both the final SSE and the number of iterations (updates) as criteria to assess the performance of k-means and LDPS-means. Under the above assumptions, we have the following results.\nTheorem 1. Under Assumptions 1)-3) above, the average number of repeats that k-means needs to achieve the competitive performance of LDPS-means is O(ek).\nHere, competitive means both good final SSE and less number of iterations. See Appendix for the proof.\nWe now analyze the time complexity of k-means to achieve the competitive performance of LDPS-means. The time complexity of LDPS-means is O(m2p + mkp) = O(m2p). The time complexity of k-means to achieve the competitive performance of LDPS-means is O(E(#repeats) · mkp). This is summarized in the following theorem.\nTheorem 2. Under Assumptions 1)-3) above, the time complexity of k-means to achieve the competitive performance of LDPS-means is O(ekmkp). The relative time complexity of kmeans to achieve the competitive performance of LDPS-means is O(ek/m0).\nNote that k and m0 do not depend on each other. Thus, compared with k-means, LDPS-means is superior in time complexity when k is much larger compared with log(m0).\nThe above theorems are also true for k-medoids and LDPS-medoids, and in this case, the assumption 1) is not needed."
    }, {
      "heading" : "6 EXPERIMENTS",
      "text" : "In this section, we conduct experiments to evaluate the effectiveness of the proposed methods. The experiments mainly consist of two parts: one is for evaluating the performance of the LDPS algorithm in estimating k and the other is the clustering performances of LDPS-means and LDPS-medoids obtained by the deterministic LDPS initialization algorithm (Algorithm 1). We also evaluate the effect of the outliers detection and removal procedure on the performance of the clustering algorithm.\nAll experiments are conducted on a single PC with Intel i7-4770 CPU (4 Cores) and 16G RAM."
    }, {
      "heading" : "6.1 The compared methods",
      "text" : "In the first part on estimating k, we compare the LDPS method with x-means, dip-means and CFSFDP on the estimation of the cluster number k. The x-means is parameterfree. For dip-means, we set the significance level α = 0 for the dip test and the voting percentage vthd = 1% as in [22]. For CFSFDP, we follow the suggestion in [7] to choose dc so that the average number of neighbors is around 1% to 2% of the total number of data points in the data set. Formula (13) is used to estimate the parameters in the LDPS algorithm. We denote LDPS with the square of the Euclidean distance and the manifold-based dissimilarity measure as LDPS(E) and LDPS(M), respectively.\nIn the clustering part, we compare the clustering performance of LDPS-means and LDPS-medoids with k-means and k-medoids, respectively.\n9"
    }, {
      "heading" : "6.2 The data sets",
      "text" : ""
    }, {
      "heading" : "6.2.1 Overview of the data sets",
      "text" : "We use both synthetic data sets and real world data sets for evaluation. Four different kinds of synthetic data sets are used: the A-sets [41] have different numbers of clusters, k, the S-sets [42] have different data distributions, the Dimsets vary with the dimensionality p and the Shape-sets [7] are of different shapes. They can be download from the clustering datasets website 1. We made certain modifications on the S-sets and Dim-sets since these two sets are easy to be separated. More details can be found in Section 6.4. The real world data sets include Handwritten Pendigits [43], Coil-20 [44], Coil-100 [45] and Olivetti Face Database [46]."
    }, {
      "heading" : "6.2.2 Attribute normalization",
      "text" : "In clustering tasks, attribute normalization is an important preprocessing step to prevent the attributes with large ranges from dominating the calculation of the distance. In addition, it helps to get more accurate numerical computations. In our experiments, the attributes are generally normalized into the interval [0, 1] using the min-max normalization. Specifically,\n(x (i) j )normalized =\nx (i) j − xminj\nxmaxj − xminj\nwhere x(i)j is the jth attribute of the data point x (i), xminj = minl x (l) j and x max j = maxl x (l) j . For gray images whose pixel values lying in [0, 255],we simply normalize the pixels by dividing 255."
    }, {
      "heading" : "6.3 Performance criteria",
      "text" : "For estimating k, we use the simple criterion that the better performance is achieved when the estimated k is close to the ground truth k∗.\n1. http://cs.joensuu.fi/sipu/datasets/\nFor the task of clustering the synthetic data sets, three criteria are used to evaluate the performance of initial seeds: 1) the total CPU time cost to achieve SSE∗, 2) the number of repeats (#repe) needed to achieve SSE∗ and 3) the number of assignment iterations (#iter) when SSE∗ is achieved in the repeat. We first run LDPS-means to get an upper bound for SSE∗, denoted as SSE∗0. We then run k-means repeatedly and sequentially up to 1000 times and record the minimal SSE∗, which is denote by SSE∗k. During this process, once SSE ∗ k is smaller than SSE∗0, we record the current number of repeats as #repe and the number of iterations in this repeat as #iter. Otherwise, #repe and #iter are recorded when the minimal SSE∗k is achieved within the whole 1000 repeats. The same strategy is used to record the results of k-means++, whose minimal SSE∗ is denoted as SSE∗k++. Records of CPU time, #iter, #repe, SSE∗k and SSE ∗ k++ are averaged over the 1000 duplicate tests to reduce the randomness. On the real world data sets, we consider the unsupervised classification task [47]. Three criteria are used to evaluate the clustering performance of the comparing algorithms by comparing the learned categories with the true categories. First, each learned category is associated with the true category that accounts for the largest number of the training cases in the learned category and thus the error rate (re) can be computed. The second criterion is the rate of true association (rt), which is the fraction of pairs of images from the same true category that were correctly placed in the same learned category. The last criteria is the rate of false association (rf ), which is the fraction of pairs of images from different true categories that were erroneously placed in the same learned category. The better clustering performance is characterized with a lower value for re and rf but a higher value for rt. To fairly compare the performance of LDPSmeans with k-means, we make their running time to be the same by control the number of repeats of k-means. The same strategy is applied for LDPS-medoids and k-medoids. The results of k-means and k-medoids are recorded when SSE∗\n10\nin the repeat is the smallest one among the repeats."
    }, {
      "heading" : "6.4 Experiments on synthetic data sets",
      "text" : "We use four kinds of synthetic data sets. The A-sets contains three two-dimensional sets A1, A2 and A3 with different numbers of circular clusters (k∗ = 20, 35, 50). Each cluster has 150 data points. We generate a new set A0 by selecting five clusters from A1 with the labels 1− 5.\nThe S-sets S1 to S4 are composed of the data points sampled from two-dimensional Gaussian clusters N(µ,Σ), with 100 data points in each cluster. Their centers are the\nsame as that of the min-max normalized A3 set. We set Σ = σ · I2, with σ being 0.002, 0.004, 0.006 and 0.008, respectively, where I2 is the identity matrix in R2.\nWe also generate four Dim-sets Dp with the dimensionality p = 3, 6, 9, 12. The Dim-sets are Gaussian clusters that distribute in multi-dimensional spaces. Each of them has 50 clusters with 100 data samples in each cluster. The first two-dimensional projection of their cluster centers are the same as that of the min-max normalized A3 set. The axes in the other dimensions of the cluster centers are randomly distributed. Their covariances are set to be σ · Ip with σ = 0.001, 0.004, 0.007, 0.01, respectively, where Ip is the\n11\nidentity matrix in Rp. The Shape-sets consist of 8 sets with different shapes (see Fig. 6). Six of them are the Flame set (k∗=2), the Spiral set (k∗=3), the Compound set (k∗=6), the Aggregation set (k∗=7), the R15 set (k∗=15) and the D31 set (k∗=31). We generate two new Shape-sets, the Crescent shape set (k∗ = 2) and the Path-based set (k∗ = 3)."
    }, {
      "heading" : "6.4.1 Performance on the estimation of k",
      "text" : "The results of the estimated k are summarized in TABLE 1. From this table it is seen that x-means fails to split on most of the data sets but it gets the correct result for the D12 set. dip-means gets better results than x-means in most of the cases, but it underestimated k for most of the data sets. In particular, dip-means fails to detect any valid cluster in D6, D9 and D12 due to the relatively high dimensionality. CFSFDP gets better results than dip-means on most of the data sets. Though CFSFDP underestimated k for the sets A3, S3, S4 and all the Dim-sets, it gets a very good estimation of k (very close to the true cluster number k∗) on the Shape-sets. Note that CFSFDP fails on the Flame set, the Compound set and the Aggregation set. This is slightly different from the results reported in [7]. It should be pointed out that the results reported in [7] can be achieved with a very careful selection of parameters and with a prior knowledge on the data distribution, which we did not consider in this paper.\nLDPS(E) works very well on most of the data sets. Compared with CFSFDP, lpds-means obtained the correct k on A3, S3 and S4 and a very close k to k∗ on the Dim-sets. Compared with the other comparing methods, LDPS(M) obtained the best results due to its use of LDI and manifold-based dissimilarity measure. Compared with LDPS(E), LDPS(M) shows its superiority in learning k when dealing with the Shape-sets.\nBased on the above analysis, we summarize the ability of the comparing methods for estimating k on the synthetic data sets in TABLE 4."
    }, {
      "heading" : "6.4.2 Clustering performance",
      "text" : "We first compare the clustering performance of LDPS-means with k-means and k-means++ on the A-sets to verify the result of Theorem 1. The experimental results are listed in TABLE 2. As shown in TABLE 2, the clustering performance of LDPS-means is getting much better as k increases. On the sets A2 and A3, LDPS-means outperforms k-means and k-means++ greatly. This is consistent with Theorem 1.\nWe then conduct experiments on S-sets and Dim-sets to show the capability of LDPS-means in separating clusters with a varying complexity of data distributions and\na varying dimensionality, respectively. The experimental results are listed in TABLE 3. From the table it is seen that, compared with k-means and k-means++, LDPS-means takes much less time and much less number of iterations to achieve SSE∗0. Note that SSE ∗ 0 is smaller than SSE ∗ k and SSE∗k++ on most of the data sets. Finally, the variants of k-means (including k-means, kmeans++ and LDPS-means) fail on most of the Shape-sets. However, using LDPS-medoids with the manifold-based dissimilarity measure can get satisfactory results. Fig. 6 shows the clustering results on the Shape-sets by LDPSmedoids with an appropriate t and the estimated parameters θ = (h, r)."
    }, {
      "heading" : "6.5 Experiments on Handwritten Pendigits",
      "text" : "We now carry out experiments on the real world data set, Handwritten Pendigits, to evaluate the performance of LDPS-means and LDPS-medoids on general purpose clustering. This data set can be download from the UCI Machine Learning repository2. The Handwritten Pendigits data set contains totally 10992 data points with 16-dimensional features. Each of them represents a digit from 0− 9 written by a human subject. The data set consists of a training data set PDtr10 and a testing data set PD te 10 with 7494 and 3498 samples, respectively. Apart from the full data set, we also consider three subsets that contain the digits {1,3,5} (PDtr3 and PDte3 ), {0,2,4,6,7} (PDtr5 and PDte5 ), and {0,1,2,3,4,5,6,7} (PDtr8 and PDte8 ). On these sets, the manifold distance is approximated by the graph distance, which is the shortest distance on the graph constructed by 5-nn."
    }, {
      "heading" : "6.5.1 Performance on the estimation of k",
      "text" : "TABLE 5 presents the results of estimating k on the Handwritten Pendigits. x-means fails on all of those data sets. dip-means also fails on all of those data sets though it gets the closest k to the true cluster number on the PDtr10 set compared with all of the other comparing methods. CFSFDP(E) and CFSFDP(M) get an underestimated k at most of the cases. Compared with the CFSFDP methods, the LDPS methods get the correct k on most of the data sets owing to LDI. LDPS(M) gets better results than those obtained by LDPS(E) on the PDtr10 and PD te 10 sets due to the use of the manifold distance."
    }, {
      "heading" : "6.5.2 Clustering performance",
      "text" : "We now compare the unsupervised object classification performance of LDPS-mean and LDPS-medoids with k-means and k-medoids. The results are shown in TABLE 6. As seen in the table, LDPS-means gets better results than k-means does on most of the data sets except for the rtrue criterion on PDtr10, PD te 8 and PD te 10, where LDPS(E) fails to estimate the correct k. k-medoids gets better results than the other comparing methods due to the use of the manifold distance as the dissimilarity measure. However, LDPS-medoids gets the best results on all the data sets with all the criteria.\n2. http://archive.ics.uci.edu/ml/\n12"
    }, {
      "heading" : "6.6 Experiments on Coil-20",
      "text" : "We now consider the real world data set, Coil-20 [44], which is used for the task of unsupervised object clustering. The data set Coil-20 contains 20 objects, and each object contains 72 images taken 5 degree apart as the object rotated on a turntable. We compress each image into 32× 32 pixels with 256 grey levels per pixel. To compare the performance of the\ncomparing methods with different numbers of categories, we select three subsets of Coil-20: Coil-5 (objects 1,3,5,7 and 9), Coil-10 (objects with even number), Coil-15 (objects except 3,7,11,15 and 19). Fig. 7 shows some examples of the twenty objects.\nWe use the CW-SSIM index and t-nn to construct the\n13\nTABLE 10 Clustering performance comparison on the large Coil-sets, where re is the error rate, rt stands for the rate of true association which is the fraction of pairs of images from the same true category that were correctly placed in the same learned category, and rf is the rate of false association which is the fraction of pairs of images from different true categories that were erroneously placed in the same learned category.\nData Sets k-means LDPS-means k-medoids LDPS-medoids re rt rf re rt rf re rt rf re rt rf\nCoil-25 0.323 0.674 0.023 0.372 0.665 0.031 0.226 0.801 0.022 0.180 0.843 0.016 Coil-50 0.371 0.624 0.016 0.406 0.643 0.016 0.325 0.741 0.024 0.199 0.829 0.010 Coil-75 0.429 0.572 0.010 0.484 0.581 0.013 0.341 0.698 0.017 0.239 0.764 0.007 Coil-100 0.437 0.551 0.009 0.472 0.507 0.008 0.415 0.657 0.019 0.271 0.749 0.010\nFig. 7. Example objects of Coil-20. Their labels are the same as their order, sorting from left to right and then up to down.\nmanifold distance. First, the CW-SSIM index is performed on the images to get the structural similarity between the objects. Then, we construct the graph distance using the 3- nn neighborhood based on the structural similarity. Finally, the graph distance is served as an approximation to the manifold distance between the objects."
    }, {
      "heading" : "6.6.1 Performance on the estimation of k",
      "text" : "The estimated cluster number k of Coil-20 and its subsets is presented in TABLE 7. x-means again fails to get a reasonable estimation of k for these sets. dip-means, CFSFDP(E) and LDPS(E) also fails to get a meaningful number of clusters since the Euclidean distance can not properly measure the dissimilarity between these objects. Compared with these four method, CFSFDP(M) gets betters results though it underestimated k on Coil-15 and Coil-20. LDPS(M) obtained reasonable estimations of k for most of the data sets. It underestimates k on Coil-20 since the objects 3, 6 and 19 are very similar (see Fig. 7); they are clustered to the same category ”cars” by the LDPS(M) algorithm. Similarly, the objects 15 and 17 have very similar shapes and are thus clustered to the same category."
    }, {
      "heading" : "6.6.2 Clustering performance",
      "text" : "TABLE 8 shows the clustering results on the Coil-sets. Unlike the results on the Pendigits sets, LDPS-means has got a worse result than k-means did on these sets. This may be because LDPS(E) failed to estimate the proper k on the Coilsets. LDPS-medoids, on the contrary, learnt the proper k on these sets and selected the initial seeds with high quality. As a result, the clustering performance of LDPS-medoids is much better than that of all the other comparing methods on the Coil-sets."
    }, {
      "heading" : "6.7 Experiments on Coil-100",
      "text" : "Coil-100 is the third real world data set we considered. Unlike Coil-20 which has a small number of categories, the true cluster number k∗ of Coil-100 is very large. Thus, it is used for the task of unsupervised object clustering with a large number of categories. The Coil-100 data set contains\n100 categories of objects consisting of 7200 color images, and each object has 72 images taken 5 degree apart as the object rotated on a turntable. The color images are converted into gray images and resized to 32 × 32 pixels. We select three subsets from Coil-100, which are Coil-25 (objects 1, 5, 9, · · · , 93 and 97), Coil-50 (objects with even number) and Coil-75 (Coil-25 + Coil-50).\nThe manifold distance is approximated using the graph distance with the CW-SSIM index and the 4-nn neighborhood."
    }, {
      "heading" : "6.7.1 Performance on the estimation of k",
      "text" : "Since the number of clusters of Coil-100 is large, the density of certain local density peaks can be easily dominated by the largest density. Thus, it is very hard to get a balanced local density distribution for the local density peaks. To deal with this difficulty the local density is normalized as follows:\nρ = ( ρ\nρ∗ )1/4. (14)\nThe main purpose of the local density normalization (14) is to enlarge the relatively small local densities. In this case, ρ also lies in the range of (0, 1] and is in the same increasing order as the original ρ.\nTABLE 9 shows the results of the estimated cluster number k on the large Coil-sets. The LDPS methods use the new normalized local density ρ (defined in (14)) as the local density value. Surprisingly, LDPS(M) has learnt almost the identical number of clusters as k∗ with the new normalized local density ρ and the manifold distance. CFSFDP(M), however, underestimated the cluster number k on all of the data sets. The other comparing methods all fail to learn a reasonable cluster number k.\nTo obtain a more careful comparison of the ability to learn k with LDPS(M) and CFSFDP(M) on Coil-100, we select the first k∗ categories (k∗ = 10, 20, · · · , 100) from Coil-100 as k∗ subsets. The k∗-k curves are shown in Fig. 8. As k∗ is getting bigger, the estimated k by CFSFDP(M) is getting farther away from k∗; however, the k learned by LDPS(M) is getting much closer to the ground truth k∗ with very little changes. Thus, our method is much more effective in learning k than CFSFDP when the ground truth k∗ is very large. This is consist with the analysis in Section 3.1."
    }, {
      "heading" : "6.7.2 Clustering performance",
      "text" : "The new normalized local density ρ, defined in (14), is also used for this task. Based on new normalized local density ρ, the clustering results are shown in TABLE 10. LDPSmedoids again gets the best clustering results compared\n14\nTABLE 11 Results of the estimated k of the comparing methods on the Oliv.-sets.\nData Sets x-means dip-means CFSFDP (E) CFSFDP (M) LDPS (E) LDPS (M) Oliv.-10 18 1 4 10 4 10 Oliv.-20 33 1 1 18 5 20 Oliv.-30 45 1 1 21 −1 30 Oliv.-40 56 1 1 28 −1 36\nTABLE 12 Clustering performance comparison of the comparing methods on the Oliv.-sets, where re is the error rate, rt stands for the rate of true association which is the fraction of pairs of images from the same true category that were correctly placed in the same learned category, and rf is the rate of false association which is the fraction of pairs of images from different true categories that were erroneously placed in the same learned category.\nData Sets k-means LDPS-means k-medoids LDPS-medoids re rt rf re rt rf re rt rf re rt rf\nOliv.-10 0.240 0.804 0.052 0.200 0.849 0.041 0.110 0.896 0.022 0.070 0.907 0.016 Oliv.-20 0.415 0.492 0.042 0.395 0.511 0.043 0.250 0.729 0.031 0.245 0.740 0.032 Oliv.-30 0.427 0.458 0.027 0.403 0.529 0.025 0.280 0.717 0.020 0.197 0.764 0.013 Oliv.-40 0.455 0.465 0.026 0.465 0.482 0.025 0.275 0.688 0.021 0.213 0.739 0.010\nwith the other comparing methods. In addition, with k∗ increasing, the relative performance of LDPS-means is getting better compared with k-medoids. This is consist with the conclusion in Theorem 2."
    }, {
      "heading" : "6.8 Experiments on Olivetti Face Database",
      "text" : "The last real world data set is the Olivetti Face Database, served for the task of unsupervised face clustering. The Olivetti Face Database is formerly the ORL Database of Faces, which consists of 400 face images from 40 individuals. The images are taken at different times, with varying lighting, facial expressions and facial details. The size of each image is 64× 64 pixels. Again, we select three subsets: Oliv.-10 (faces 2, 6, 10, · · · , 34 and 38), Oliv.-20 (faces with the odd number), and Oliv.-30 (Oliv.-10 + Oliv.-20). The whole Olivetti Face Databse is denoted as Oliv.-40. Fig. 9 presents some example faces of the Olivetti Face Database.\nThe manifold distance of the Oliv.-sets is approximated by using the graph distance with the CW-SSIM index and the 3-nn neighborhood."
    }, {
      "heading" : "6.8.1 Performance on the estimation of k",
      "text" : "It is a hard task to estimate k for the Oliv.-sets due to the limited samples in each category (m0 = 10) and high dimensionality (p = 4096) but a relatively large number of\nFig. 9. The first two categories of Oliv.-40. Images of the first person vary with the angle to the camera, and images of the second person vary greatly with the facial expressions.\nclusters (total k∗ = 40). TABLE 11 listed the estimated k of the comparing methods on the Oliv.-sets. x-means, dipmeans, CFSFDP(E) and LDPS(E) all fail to learn a reasonable k. The estimated k of Oliv.-40 by CFSFDP with the CWSSIM index is around 30 [7], much close to k∗ compared with the previous methods. CFSFDP(M) learns a reasonable k for Oliv.-10 and Oliv.-20, but badly underestimated k for Oliv.-30 and Oliv.-40. LDPS(M) gets the consistent cluster number with the ground truth for the first three Oliv.-sets. Though it underestimated the number of clusters on Oliv.40, its estimated result is much closer to the ground truth than that obtained by the other comparing methods."
    }, {
      "heading" : "6.8.2 Clustering performance",
      "text" : "TABLE 12 shows the clustering results on the Oliv.-sets. kmeans and LDPS-means obtained bad results on the last three Oliv.-sets due to their use of the Euclidean distance as the dissimilarity measure. With the help of the manifold distance, k-medoids and LDPS-medoids obtained much better results than k-means and LDPS-means did. LDPS-medoids outperforms k-medoids with the properly selected initial seeds. In addition, when setting k∗ to be 42 for Oliv.-40, LDPS-medoids gets re = 18.5%, rt = 74.0% and rf = 0.9%. This improves re by 15.9% decreasing, rt by 8.8% increasing and rf by 25% decreasing over the results reported in [7], where re = 22.0%, rt = 68% and rf = 1.2%."
    }, {
      "heading" : "7 CONCLUSION AND FUTURE WORKS",
      "text" : "In this paper, we proposed a novel method, the LDPS algorithm, to learn the appropriate number of clusters and to select deterministically the initial seeds of clusters with\n15\nhigh quality for the k-means-like methods. In addition, two novel methods, LDPS-means and LDPS-medoids, have also been proposed as a combination of the LDPS initialization algorithm and the k-means-like clustering algorithm. Performance analysis and experimental results have demonstrated that our methods have the following advantages:\n1) The LDPS algorithm can learn a reasonable number k of clusters for data sets with balanced samples in each category. In addition, it can deal with a variety of data distributions with an appropriate dissimilarity measure. 2) The initial seeds selected by LDPS-means/LDPSmedoids are geometrically close to the centers of the clusters. As a result, our methods can achieve a very good SSE∗, which is sometimes competitive with that achieved by k-means/k-medoids with thousands of repeats. In addition, the number of iterations in the clustering stage of our methods is generally much less than that needed by the other methods that select the initial seeds randomly. 3) Our methods give superior results in dealing with data sets with very large true cluster number k∗, compared with k-means/k-medoids. This is mainly due to the local distinctiveness index introduced in this paper. 4) LDPS-medoids gives a superior performance on the unsupervised object clustering tasks. This is mainly due to the use of the LDPS algorithm for deterministic initialization and the manifold distance (based on the CW-SSIM index and the t-nn neighborhood) as the dissimilarity measure.\nDespite the above advantages, our methods have also some limitations. First, the time complexity of our methods is relatively higher compared with the original k-means algorithm. Thus, our methods can not deal with very large data sets. Secondly, difficulty may occur in parameters estimation when dealing with unbalanced data sets. In addition, we did not give detailed discussion on the outliers issue. These issues will be considered in the future."
    }, {
      "heading" : "APPENDIX A PROOF OF THEOREM 1",
      "text" : "To prove the theorem, we need two basic results in mathematical analysis [48].\nLemma 1. lim n→∞\n(n!) 1 n\nn = 1 e .\nLemma 2. lim n→∞ (1 + 1n ) n = e.\nProof of Theorem 1: First, local density peaks would geometrically near the center of the clusters as analyzed in sec. 3.2 under conditions 1)-3). With the local density peaks as initial seeds, LDPS-means could separate the clusters with only O(1) iterations. Second, the k-means achieves the competitive performance when the initial seeds are selected with each cluster a seed. The statistical event that the kmeans could achieve this in one repeat forms a Bernoulli distribution [49], with probability\nP (success) cond. 2)= mk0/\n( m\nk\n) .\nIn addition, the distribution of P (success) between two different repeats are independent and identical. As a consequence, the expected number of repeats (#repeats) to achieve the competitive performance is,\nE(#repeats) = 1\nP (success) =\n( m\nk\n) /mk0 . (15)\nFinally, by the lemmas and conditions, we get( m\nk\n) =\nm!\n(m− k)! · k! lem. 1 = O ( (m/e)m ((m− k)/e)m−k · (k/e)k ) = O ( ( m\nm− k )m−k · (m k )k )\ncond. 2) = O ( (1 +\n1\nm0 − 1 )(m0−1)k ·mk0 ) lem. 2 = O ( ek ·mk0 ) (16)\nThus, with (15) and (16) we get E(#repeats) = O(ek)."
    } ],
    "references" : [ {
      "title" : "A survey of clustering data mining techniques",
      "author" : [ "P. Berkhin" ],
      "venue" : "Group. Multidimens. Data. Springer, 2006, pp. 25–71.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "The importance of encoding versus training with sparse coding and vector quantization",
      "author" : [ "A. Coates", "A.Y. Ng" ],
      "venue" : "Proc. 28th ICML, 2011, pp. 921–928.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Randomized dimensionality reduction for k-means clustering",
      "author" : [ "C. Boutsidis", "A. Zouzias", "M.W. Mahoney", "P. Drineas" ],
      "venue" : "IEEE Trans. Inf. Theory, vol. 61, no. 2, pp. 1045–1062, 2015.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Learning manifolds with k-means and k-flats",
      "author" : [ "G. Canas", "T. Poggio", "L. Rosasco" ],
      "venue" : "Proc. NIPS, 2012, pp. 2465–2473.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A density-based algorithm for discovering clusters in large spatial databases with noise",
      "author" : [ "M. Ester", "H.-P. Kriegel", "J. Sander", "X. Xu" ],
      "venue" : "KDD, vol. 96, no. 34, 1996, pp. 226–231.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Semi-supervised kernel mean shift clustering",
      "author" : [ "S. Anand", "S. Mittal", "O. Tuzel", "P. Meer" ],
      "venue" : "IEEE Trans. Pattern Anal. Math. Intell., vol. 36, no. 6, pp. 1201–1215, 2014.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Clustering by fast search and find of density peaks",
      "author" : [ "A. Rodriguez", "A. Laio" ],
      "venue" : "Sci., vol. 344, no. 6191, pp. 1492–1496, 2014.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Algorithms for hierarchical clustering: an overview",
      "author" : [ "F. Murtagh", "P. Contreras" ],
      "venue" : "Data Min. and Knowl. Discov., vol. 2, no. 1, pp. 86–97, 2012.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Data clustering: 50 years beyond k-means",
      "author" : [ "A.K. Jain" ],
      "venue" : "Pattern Recogn. Letters, vol. 31, no. 8, pp. 651–666, 2010.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A simple and fast algorithm for kmedoids clustering",
      "author" : [ "H.-S. Park", "C.-H. Jun" ],
      "venue" : "Expert Syst. with Applicat., vol. 36, no. 2, pp. 3336–3341, 2009.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "The expectation-maximization algorithm",
      "author" : [ "T.K. Moon" ],
      "venue" : "IEEE Signal Process. Mag., vol. 13, no. 6, pp. 47–60, 1996.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Feature selection based on sensitivity analysis of fuzzy isodata",
      "author" : [ "Q. Liu", "Z. Zhao", "Y.-X. Li", "Y. Li" ],
      "venue" : "Neurocomput., vol. 85, pp. 29–37, 2012.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Data clustering: a review",
      "author" : [ "A.K. Jain", "M.N. Murty", "P.J. Flynn" ],
      "venue" : "ACM comput. surveys, vol. 31, no. 3, pp. 264–323, 1999.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Survey of clustering algorithms",
      "author" : [ "R. Xu", "D. Wunsch" ],
      "venue" : "IEEE Trans. Neural Netw., vol. 16, no. 3, pp. 645–678, 2005.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Top 10 algorithms in data mining",
      "author" : [ "X. Wu", "V. Kumar", "J.R. Quinlan" ],
      "venue" : "Knowl. and Inf. Syst., vol. 14, no. 1, pp. 1–37, 2008.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "K-means-type algorithms: a generalized convergence theorem and characterization of local optimality",
      "author" : [ "S.Z. Selim", "M.A. Ismail" ],
      "venue" : "IEEE Trans. Pattern Anal. Math. Intell., no. 1, pp. 81–87, 1984.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1984
    }, {
      "title" : "Convergence properties of the k-means algorithms",
      "author" : [ "L. Bottou", "Y. Bengio" ],
      "venue" : "Proc. NIPS, pp. 585–592, 1995.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "A comparative study of efficient initialization methods for the k-means clustering algorithm",
      "author" : [ "M.E. Celebi", "H.A. Kingravi", "P.A. Vela" ],
      "venue" : "Expert Systems with Applications, vol. 40, no. 1, pp. 200– 210, 2013.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Linear, deterministic, and order-invariant initialization methods for the k-means clustering algorithm",
      "author" : [ "M.E. Celebi", "H.A. Kingravi" ],
      "venue" : "Partitional Clustering Algorithms. Springer, 2015, pp. 79–98.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "X-means: Extending k-means with efficient estimation of the number of clusters.",
      "author" : [ "D. Pelleg", "A.W. Moore" ],
      "venue" : "in Proc. ICML,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2000
    }, {
      "title" : "Learning the k in k-means",
      "author" : [ "G. Hamerly", "C. Elkan" ],
      "venue" : "Proc. NIPS, 2004, pp. 281–288.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Dip-means: an incremental clustering method for estimating the number of clusters",
      "author" : [ "A. Kalogeratos", "A. Likas" ],
      "venue" : "Proc. NIPS, 2012, pp. 2393–2401.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "The dip test of unimodality",
      "author" : [ "J.A. Hartigan", "P. Hartigan" ],
      "venue" : "The Ann. of Stat., pp. 70–84, 1985.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1985
    }, {
      "title" : "k-means++: The advantages of careful seeding",
      "author" : [ "D. Arthur", "S. Vassilvitskii" ],
      "venue" : "Proc. 8th ann. ACM-SIAM symposium on Discrete alg. Society for Industrial and Applied Mathematics, 2007, pp. 1027–1035.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "The minmax k-means clustering algorithm",
      "author" : [ "G. Tzortzis", "A. Likas" ],
      "venue" : "Pattern Recogn., vol. 47, no. 7, pp. 2505–2516, 2014.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "In search of deterministic methods for initializing k-means and gaussian mixture clustering",
      "author" : [ "T. Su", "J.G. Dy" ],
      "venue" : "Intelligent Data Analysis, vol. 11, no. 4, pp. 319–338, 2007.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "An initialization method for the k-means algorithm using neighborhood model",
      "author" : [ "F. Cao", "J. Liang", "G. Jiang" ],
      "venue" : "Computers & Mathematics with Applications, vol. 58, no. 3, pp. 474–483, 2009.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A survey of outlier detection methodologies",
      "author" : [ "V.J. Hodge", "J. Austin" ],
      "venue" : "Art. Intell. Review, vol. 22, no. 2, pp. 85–126, 2004.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Improving k-means by outlier removal",
      "author" : [ "V. Hautamäki", "S. Cherednichenko", "I. Kärkkäinen", "T. Kinnunen", "P. Fränti" ],
      "venue" : "Image Anal. Springer, 2005, pp. 978–987.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Robust clustering by pruning outliers",
      "author" : [ "J.-S. Zhang", "Y.-W. Leung" ],
      "venue" : "IEEE Trans. Syst., Man, Cybern. B, vol. 33, no. 6, pp. 983–998, 2003.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Finding groups in data: an introduction to cluster analysis",
      "author" : [ "L. Kaufman", "P.J. Rousseeuw" ],
      "venue" : null,
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2009
    }, {
      "title" : "A reference bayesian test for nested hypotheses and its relationship to the schwarz criterion",
      "author" : [ "R.E. Kass", "L. Wasserman" ],
      "venue" : "J. of the Amer. Stat. Associat., vol. 90, no. 431, pp. 928–934, 1995.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Density estimation",
      "author" : [ "S.J. Sheather" ],
      "venue" : "Stat. Sci., vol. 19, no. 4, pp. 588–597, 2004.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "A practical guide to support vector classification",
      "author" : [ "C.-W. Hsu", "C.-C. Chang", "C.-J. Lin" ],
      "venue" : "2003.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Similarity measures",
      "author" : [ "S. Santini", "R. Jain" ],
      "venue" : "IEEE Trans. Pattern Anal. Math. Intell., vol. 21, no. 9, pp. 871–883, 1999.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Local search heuristics for k-median and facility location problems",
      "author" : [ "V. Arya", "N. Garg", "R. Khandekar", "A. Meyerson", "K. Munagala", "V. Pandit" ],
      "venue" : "SIAM J. on Comput., vol. 33, no. 3, pp. 544–562, 2004.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "The manifold ways of perception",
      "author" : [ "H.S. Seung", "D.D. Lee" ],
      "venue" : "Sci., vol. 290, no. 5500, pp. 2268–2269, 2000.",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "A novel graph-based k-means for nonlinear manifold clustering and representative selection",
      "author" : [ "E. Tu", "L. Cao", "J. Yang", "N. Kasabov" ],
      "venue" : "Neurocomput., vol. 143, pp. 109–122, 2014.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Complex wavelet structural similarity: A new image similarity  16 index",
      "author" : [ "M.P. Sampat", "Z. Wang", "S. Gupta", "A.C. Bovik", "M.K. Markey" ],
      "venue" : "IEEE Trans. Image Process., vol. 18, no. 11, pp. 2385–2401, 2009.",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A new manifold distance for visual object categorization",
      "author" : [ "F. Li", "X. Huang", "H. Qiao", "B. Zhang" ],
      "venue" : "Proc. 12th World Congress on Intelligent Control and Automation, 2016, pp. 2232–2236.",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Dynamic local search algorithm for the clustering problem",
      "author" : [ "I. Kärkkäinen", "P. Fränti" ],
      "venue" : "Research Report A. University of Joensuu, 2002.",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Iterative shrinking method for clustering problems",
      "author" : [ "P. Fränti", "O. Virmajoki" ],
      "venue" : "Pattern Recogn., vol. 39, no. 5, pp. 761–775, 2006.",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Methods of combining multiple classifiers based on different representations for pen-based handwritten digit recognition",
      "author" : [ "F. Alimoglu", "E. Alpaydin" ],
      "venue" : "Proc. of the 5th TAINN. Citeseer, 1996.",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Columbia object image library (coil-20)",
      "author" : [ "S.A. Nene", "S.K. Nayar", "H. Murase" ],
      "venue" : "CUCS-005-96, Tech. Rep., Feb. 1996.",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Columbia object image library (coil-100)",
      "author" : [ "S. Nayar", "S.A. Nene", "H. Murase" ],
      "venue" : "Department of Comp. Science, Columbia University, Tech. Rep. CUCS-006-96, 1996.",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Parameterisation of a stochastic model for human face identification",
      "author" : [ "F. Samaria", "A. Harter" ],
      "venue" : "Proc. 2rd WACV, 1994, pp. 138– 142.",
      "citeRegEx" : "46",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Non-metric affinity propagation for unsupervised image categorization",
      "author" : [ "D. Dueck", "B.J. Frey" ],
      "venue" : "Proc. ICCV, 2007, pp. 1– 8.",
      "citeRegEx" : "47",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Principles of mathematical analysis",
      "author" : [ "W. Rudin" ],
      "venue" : "New York: McGraw- Hill Science,",
      "citeRegEx" : "48",
      "shortCiteRegEx" : "48",
      "year" : 1964
    }, {
      "title" : "Probability, random variables, and stochastic processes",
      "author" : [ "A. Papoulis", "S.U. Pillai" ],
      "venue" : "Tata McGraw-Hill Education,",
      "citeRegEx" : "49",
      "shortCiteRegEx" : "49",
      "year" : 2002
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "C LUSTERING methods are important techniques for exploratory data analysis with wide applications ranging from data mining [1], vector quantization [2], dimension reduction [3], to manifold learning [4].",
      "startOffset" : 123,
      "endOffset" : 126
    }, {
      "referenceID" : 1,
      "context" : "C LUSTERING methods are important techniques for exploratory data analysis with wide applications ranging from data mining [1], vector quantization [2], dimension reduction [3], to manifold learning [4].",
      "startOffset" : 148,
      "endOffset" : 151
    }, {
      "referenceID" : 2,
      "context" : "C LUSTERING methods are important techniques for exploratory data analysis with wide applications ranging from data mining [1], vector quantization [2], dimension reduction [3], to manifold learning [4].",
      "startOffset" : 173,
      "endOffset" : 176
    }, {
      "referenceID" : 3,
      "context" : "C LUSTERING methods are important techniques for exploratory data analysis with wide applications ranging from data mining [1], vector quantization [2], dimension reduction [3], to manifold learning [4].",
      "startOffset" : 199,
      "endOffset" : 202
    }, {
      "referenceID" : 4,
      "context" : "The approaches to achieve this aim include methods based on density estimation such as DBSCAN [5], mean-shift clustering [6] and clustering by fast search and find of density peaks [7], methods that recursively find nested clusters [8], and partitional methods based on minimizing objective functions such as k-means [9], k-medoids [10], the EM algorithm [11] and ISODATA [12].",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 5,
      "context" : "The approaches to achieve this aim include methods based on density estimation such as DBSCAN [5], mean-shift clustering [6] and clustering by fast search and find of density peaks [7], methods that recursively find nested clusters [8], and partitional methods based on minimizing objective functions such as k-means [9], k-medoids [10], the EM algorithm [11] and ISODATA [12].",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 6,
      "context" : "The approaches to achieve this aim include methods based on density estimation such as DBSCAN [5], mean-shift clustering [6] and clustering by fast search and find of density peaks [7], methods that recursively find nested clusters [8], and partitional methods based on minimizing objective functions such as k-means [9], k-medoids [10], the EM algorithm [11] and ISODATA [12].",
      "startOffset" : 181,
      "endOffset" : 184
    }, {
      "referenceID" : 7,
      "context" : "The approaches to achieve this aim include methods based on density estimation such as DBSCAN [5], mean-shift clustering [6] and clustering by fast search and find of density peaks [7], methods that recursively find nested clusters [8], and partitional methods based on minimizing objective functions such as k-means [9], k-medoids [10], the EM algorithm [11] and ISODATA [12].",
      "startOffset" : 232,
      "endOffset" : 235
    }, {
      "referenceID" : 8,
      "context" : "The approaches to achieve this aim include methods based on density estimation such as DBSCAN [5], mean-shift clustering [6] and clustering by fast search and find of density peaks [7], methods that recursively find nested clusters [8], and partitional methods based on minimizing objective functions such as k-means [9], k-medoids [10], the EM algorithm [11] and ISODATA [12].",
      "startOffset" : 317,
      "endOffset" : 320
    }, {
      "referenceID" : 9,
      "context" : "The approaches to achieve this aim include methods based on density estimation such as DBSCAN [5], mean-shift clustering [6] and clustering by fast search and find of density peaks [7], methods that recursively find nested clusters [8], and partitional methods based on minimizing objective functions such as k-means [9], k-medoids [10], the EM algorithm [11] and ISODATA [12].",
      "startOffset" : 332,
      "endOffset" : 336
    }, {
      "referenceID" : 10,
      "context" : "The approaches to achieve this aim include methods based on density estimation such as DBSCAN [5], mean-shift clustering [6] and clustering by fast search and find of density peaks [7], methods that recursively find nested clusters [8], and partitional methods based on minimizing objective functions such as k-means [9], k-medoids [10], the EM algorithm [11] and ISODATA [12].",
      "startOffset" : 355,
      "endOffset" : 359
    }, {
      "referenceID" : 11,
      "context" : "The approaches to achieve this aim include methods based on density estimation such as DBSCAN [5], mean-shift clustering [6] and clustering by fast search and find of density peaks [7], methods that recursively find nested clusters [8], and partitional methods based on minimizing objective functions such as k-means [9], k-medoids [10], the EM algorithm [11] and ISODATA [12].",
      "startOffset" : 372,
      "endOffset" : 376
    }, {
      "referenceID" : 0,
      "context" : "For more information about clustering methods, see [1], [9], [13], [14].",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 8,
      "context" : "For more information about clustering methods, see [1], [9], [13], [14].",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 12,
      "context" : "For more information about clustering methods, see [1], [9], [13], [14].",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 13,
      "context" : "For more information about clustering methods, see [1], [9], [13], [14].",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 14,
      "context" : "studied one [15].",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 15,
      "context" : "[16], [17], [18], [19]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[16], [17], [18], [19]).",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 17,
      "context" : "[16], [17], [18], [19]).",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 18,
      "context" : "[16], [17], [18], [19]).",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 19,
      "context" : "x-means [20] is one of the first such attempts that use the splitting and merging rules for the number of centers to increase and decrease as the algorithm ar X iv :1 61 1.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 20,
      "context" : "g-means [21] works similarly as x-means except it assumes that the clusters are generated from the Gaussian distributions.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 21,
      "context" : "dip-means [22], however, only assumes each cluster to admit a unimodal distribution and verifies this by Hartigans’ dip test [23].",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 22,
      "context" : "dip-means [22], however, only assumes each cluster to admit a unimodal distribution and verifies this by Hartigans’ dip test [23].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 23,
      "context" : "The k-means++ algorithm [24] aims to avoid poor quality data partitioning during the restarts and achieves O(log k)-competitive results with the optimal clustering.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 24,
      "context" : "The Min-Max k-means algorithm [25] deals with the initialization problem of k-means by alternating the objective function to be weighted by the variance of each cluster.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 25,
      "context" : "Methods such as PCA-Part and Var-Part [26] use a deterministic approach based on PCA and a variance of data to hierarchically split the data set into k parts where initial seeds are selected.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 17,
      "context" : "For many other deterministic initialization methods, see [18], [19], [27] and the references quoted there.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 18,
      "context" : "For many other deterministic initialization methods, see [18], [19], [27] and the references quoted there.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 26,
      "context" : "For many other deterministic initialization methods, see [18], [19], [27] and the references quoted there.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 27,
      "context" : "The third drawback of k-means, that is, its sensitivity to outliers, can be addressed by using more robust proximity measure [28], such as the Mahalanobis distance and the L1 distance rather than the Euclidean distance.",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 28,
      "context" : "The outlier removal clustering algorithm [29] uses this idea and achieves a better performance than the original k-means method when dealing with overlapping clusters.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 29,
      "context" : "Other outliers-removing cluster algorithms can be found in [30]and the references quoted there.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 9,
      "context" : "k-medoids [10], as a variant of kmeans, overcomes this difficulty by restricting the centers to be the data samples themselves.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 30,
      "context" : "It can be solved effectively (but slowly) by data partitioning around medoids (PAM) [31], or efficiently (but approximately optimally) by CLARA [31].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 30,
      "context" : "It can be solved effectively (but slowly) by data partitioning around medoids (PAM) [31], or efficiently (but approximately optimally) by CLARA [31].",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 15,
      "context" : "Finally, the algorithm is guaranteed to converge [16] at a quadratic rate [17] to a local minima of the SSE, denoted as SSE∗.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 16,
      "context" : "Finally, the algorithm is guaranteed to converge [16] at a quadratic rate [17] to a local minima of the SSE, denoted as SSE∗.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 9,
      "context" : "k-medoids [10] has the same objective function (1) as kmeans.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 19,
      "context" : "x-means [20] is an extension of k-means with the estimation of the number k of clusters.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 31,
      "context" : "During the process, the Bayesian Information Criterion (BIC) [32] is applied to score the modal.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 21,
      "context" : "dip-means [22] is another extension of k-means with the estimation of the number of clusters.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 6,
      "context" : "Clustering by fast search and find of density peaks (CFSFDP) [7] is a novel clustering method.",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 32,
      "context" : "We use the kernel density estimation (KDE) [33] to compute the local densities.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 6,
      "context" : "In addition, compared with the uniform kernel K(z) = [1/(b− a)]I{a≤z≤b} which is used in [7] (see (5)), the Gaussian kernel has a relatively higher value when |z| is small and thus keeps more local information near zero.",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 0,
      "context" : "By definition (10), γ lies in [0, 1] and is an increasing function of ρ and δ.",
      "startOffset" : 30,
      "endOffset" : 36
    }, {
      "referenceID" : 19,
      "context" : "However, it is not always easy to determine the best value of k [20].",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 33,
      "context" : "A practical way of solving this problem approximately will be the grid search method [34], in which various pairs of the (h, r) values are tried and the one that results in the maximum τ∗ is picked.",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 34,
      "context" : "A dissimilarity measure is the inverse of a similarity measure [35], which is a real-word function that quantifies the similarity between two objects.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 35,
      "context" : "If the dissimilarity measure is the L1 distance, k-medoids will get the same result as k-median [36].",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 36,
      "context" : "For manifold distributed data, the best choice for dissimilarity measures would be the manifold distance [37] which is usually approximated by the graph distance based on the neighborhood or the t-nearest-neighborhood (t-nn).",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 37,
      "context" : "Graphbased k-means [38] uses this measure.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 38,
      "context" : "For images, one of the most effective similarity measures may be the complex wavelet structural similarity (CW-SSIM) index [39], which is robust to small rotations and translations of images.",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 39,
      "context" : "In [40], a combination of the manifold assumption and the CW-SSIM index is used for constructing a new manifold distance named geometric CW-SSIM distance, which shows a superior performance for visual object categorization tasks.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 21,
      "context" : "For dip-means, we set the significance level α = 0 for the dip test and the voting percentage vthd = 1% as in [22].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 6,
      "context" : "For CFSFDP, we follow the suggestion in [7] to choose dc so that the average number of neighbors is around 1% to 2% of the total number of data points in the data set.",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 40,
      "context" : "Four different kinds of synthetic data sets are used: the A-sets [41] have different numbers of clusters, k, the S-sets [42] have different data distributions, the Dimsets vary with the dimensionality p and the Shape-sets [7] are of different shapes.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 41,
      "context" : "Four different kinds of synthetic data sets are used: the A-sets [41] have different numbers of clusters, k, the S-sets [42] have different data distributions, the Dimsets vary with the dimensionality p and the Shape-sets [7] are of different shapes.",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 6,
      "context" : "Four different kinds of synthetic data sets are used: the A-sets [41] have different numbers of clusters, k, the S-sets [42] have different data distributions, the Dimsets vary with the dimensionality p and the Shape-sets [7] are of different shapes.",
      "startOffset" : 222,
      "endOffset" : 225
    }, {
      "referenceID" : 42,
      "context" : "The real world data sets include Handwritten Pendigits [43], Coil-20 [44], Coil-100 [45] and Olivetti Face Database [46].",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 43,
      "context" : "The real world data sets include Handwritten Pendigits [43], Coil-20 [44], Coil-100 [45] and Olivetti Face Database [46].",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 44,
      "context" : "The real world data sets include Handwritten Pendigits [43], Coil-20 [44], Coil-100 [45] and Olivetti Face Database [46].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 45,
      "context" : "The real world data sets include Handwritten Pendigits [43], Coil-20 [44], Coil-100 [45] and Olivetti Face Database [46].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 0,
      "context" : "In our experiments, the attributes are generally normalized into the interval [0, 1] using the min-max normalization.",
      "startOffset" : 78,
      "endOffset" : 84
    }, {
      "referenceID" : 46,
      "context" : "On the real world data sets, we consider the unsupervised classification task [47].",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 6,
      "context" : "This is slightly different from the results reported in [7].",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 6,
      "context" : "It should be pointed out that the results reported in [7] can be achieved with a very careful selection of parameters and with a prior knowledge on the data distribution, which we did not consider in this paper.",
      "startOffset" : 54,
      "endOffset" : 57
    }, {
      "referenceID" : 43,
      "context" : "We now consider the real world data set, Coil-20 [44], which is used for the task of unsupervised object clustering.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 6,
      "context" : "-40 by CFSFDP with the CWSSIM index is around 30 [7], much close to k∗ compared with the previous methods.",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 6,
      "context" : "8% increasing and rf by 25% decreasing over the results reported in [7], where re = 22.",
      "startOffset" : 68,
      "endOffset" : 71
    } ],
    "year" : 2016,
    "abstractText" : "The k-means algorithm is a widely used clustering method in pattern recognition and machine learning due to its simplicity to implement and low time complexity. However, it has the following main drawbacks: 1) the number of clusters, k, needs to be provided by the user in advance, 2) it can easily reach local minima with randomly selected initial centers, 3) it is sensitive to outliers, and 4) it can only deal with well separated hyperspherical clusters. In this paper, we propose a Local Density Peaks Searching (LDPS) initialization framework to address these issues. The LDPS framework includes two basic components: one of them is the local density that characterizes the density distribution of a data set, and the other is the local distinctiveness index (LDI) which we introduce to characterize how distinctive a data point is compared with its neighbors. Based on these two components, we search for the local density peaks which are characterized with high local densities and high LDIs to deal with the first two drawbacks of k-means. Moreover, we detect outliers characterized with low local densities but high LDIs, and exclude them out before clustering begins. Finally, we apply the LDPS initialization framework to k-medoids, which is a variant of k-means and chooses data samples as centers, with diverse similarity measures other than the Euclidean distance to fix the last drawback of k-means. Combining the LDPS initialization framework with k-means and k-medoids, we obtain two novel clustering methods called LDPS-means and LDPS-medoids, respectively. Experiments on synthetic data sets verify the effectiveness of the proposed methods, especially when the ground truth of the cluster number k is large. Further, experiments on several real world data sets, Handwritten Pendigits, Coil-20, Coil-100 and Olivetti Face Database, illustrate that our methods give a superior performance than the analogous approaches on both estimating k and unsupervised object categorization.",
    "creator" : "LaTeX with hyperref package"
  }
}