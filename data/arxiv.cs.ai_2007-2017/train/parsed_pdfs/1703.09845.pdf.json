{
  "name" : "1703.09845.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Bringing Salary Transparency to the World: Computing Robust Compensation Insights via LinkedIn Salary",
    "authors" : [ "Krishnaram Kenthapadi", "Stuart Ambler", "Liang Zhang", "Deepak Agarwal" ],
    "emails" : [ "dagarwal)@linkedin.com" ],
    "sections" : [ {
      "heading" : "1. INTRODUCTION",
      "text" : "Online professional social networks such as LinkedIn have empowered job seekers to discover and assess career opportunities, and job providers to discover and assess potential candidates. For most job seekers, salary (or broadly, compensation) is a crucial consideration in choosing a new job opportunity1. At the same time, job seekers face challenges in learning the compensation associated with different jobs, given the sensitive nature of compensation data and the dearth of reliable sources containing compensation data. The recently launched LinkedIn Salary product2 has been designed to realize the vision of helping the world’s professionals optimize their earning potential through salary transparency, and thereby make more informed career decisions.\nWith its structured information including the work experience, educational history, and skills associated with over 460 million users, LinkedIn is in a unique position to collect compensation data from its users at scale and provide rich, robust insights covering different aspects of compensation, while preserving user privacy. For instance, we can provide insights on the distribution of base salary, bonus, equity, and other types of compensation for a given profession, how they vary based on factors such as location, experience, education, company size, and industry, and which locations, industries, or companies pay the most.\n1More candidates (74%) want to see salary compared to any other feature in a job posting, according to a survey of over 5000 job seekers in US and Canada [4]. Job seekers value compensation the most when looking for new opportunities, according to a US survey of 2305 adults [5]. 2https://www.linkedin.com/salary\n.\nIn addition to helping job seekers understand their economic value in the marketplace, the compensation data has the potential to help us better understand the monetary dimensions of the Economic Graph [28] (which includes companies, industries, regions, jobs, skills, educational institutions, etc.).\nThe availability of compensation insights along dimensions such as gender, ethnicity, and other demographic factors can lead to greater transparency, shedding light on the extent of compensation disparity, and thereby help stakeholders including employers, employees, and policy makers to take steps to address pay inequality.\nFurther, products such as LinkedIn Salary can help bring greater efficiency in the labor marketplace by reducing asymmetry of compensation knowledge, and by serving as market-perfecting tools for workers and employers [21]. Such tools have the potential to help students make good career choices, taking expected compensation into account, and to encourage workers to learn skills needed for obtaining well paying jobs, thereby helping reduce the skills gap.\nWe describe the overall design and architecture of the modeling system underlying LinkedIn’s Salary product. We focus on unique challenges such as the simultaneous need for user privacy, product coverage, and robust, reliable compensation insights, and describe how we addressed them using mechanisms such as outlier detection and Bayesian hierarchical smoothing. We perform extensive evaluation with nearly one year of anonymized compensation data collected from over one million LinkedIn users, thereby demonstrating the efficacy of the statistical models. We also highlight the lessons learned through the production deployment of our system."
    }, {
      "heading" : "2. PROBLEM SETTING",
      "text" : "In the publicly launched LinkedIn Salary product [25], users can explore compensation insights by searching for different titles and locations (Figure 1). For a given title and location, we present the quantiles (10th and 90th percentiles, median) and histograms for base salary, bonus, and other types of compensation. We also present more granular insights on how the pay varies based on factors such as location, experience, education, company size, and industry, and which locations, industries, or companies pay the most.\nThe compensation insights shown in the product are based on anonymized compensation data that we have been collecting from LinkedIn users. We designed a give-to-get model based data collection process as follows. First, cohorts (such as User Experience Designers in San Francisco Bay Area) with a sufficient number of LinkedIn users are selected. Within each cohort, emails are sent to a random subset of users, requesting them to submit their compensation data (in return for aggregated compensation insights later). Once we collect sufficient data, we get back to the responding users with the compensation insights, and also reach out to the remaining\nar X\niv :1\n70 3.\n09 84\n5v 1\n[ cs\n.S I]\n2 9\nM ar\n2 01\n7\nusers in those cohorts, promising corresponding insights immediately upon submission of their compensation data.\nConsidering the sensitive nature of compensation data and the desire for preserving privacy of users, we designed our system with the goal of not being able to attribute the compensation data to an individual user. Our methodology for achieving this goal through a combination of techniques such as encryption, access control, anonymization, aggregation, and thresholding is described in [13]. We next highlight the key data mining and machine learning challenges for the salary modeling system."
    }, {
      "heading" : "2.1 Modeling Challenges",
      "text" : "Modeling on anonymized data: Due to the privacy requirements, the salary modeling system has access only to cohort level data containing anonymized compensation submissions (e.g., salaries for UX Designers in San Francisco Bay Area), limited to those cohorts having at least a minimum number of entries. Each cohort represents a combination selected from title, country, region, company, years of experience, and so forth, and contains anonymized compensation entries obtained from individuals belonging to the combination. Within a cohort, each individual entry consists of values for different compensation types such as base salary, annual bonus, sign-on bonus, commission, annual monetary value of vested stocks, and tips, and is available without associated user name, id, or any attributes other than those that define the cohort. Consequently, our modeling choices are limited since we have access only to the anonymized data, and cannot, for instance, build prediction models that make use of more discriminating features not available due to anonymization.\nEvaluation: In contrast to several other user-facing products such as movie and job recommendations, we face unique evaluation and data quality challenges. Users themselves may not have a good perception of the true compensation range, and hence it is not feasible to perform online A/B testing to compare the compensation insights generated by different models. Further, there are very few reliable and easily available ground truth datasets in the compensation domain, and even when available (e.g., BLS OES dataset), mapping such datasets to LinkedIn’s taxonomy is inevitably noisy.\nOutlier Detection: As the quality of the insights depends on the quality of submitted data, detecting and pruning potential outlier entries is crucial. Such entries could arise due to either mistakes/misunderstandings during submission, or intentional falsification (such as someone attempting to game the system). We needed a solution to this problem that would work even during the early stages of data collection, when this problem was more challenging, and there may not be sufficient data across say, related cohorts.\nRobustness and Stability: While some cohorts may each have a large sample size, a large number of cohorts typically contain very few (< 20) data points each. Given the desire to have data for as many cohorts as possible, we need to ensure that the compensation insights are robust and stable even when there is data sparsity. That is, for such cohorts, the insights should be reliable, and not too sensitive to the addition of a new entry. A related challenge is whether we can reliably infer the insights for cohorts with no data at all.\nOur problem can thus be stated as follows: How do we design the salary modeling system to meet the immediate and future needs of LinkedIn Salary and other LinkedIn products? How do we compute robust, reliable compensation insights based on anonymized compensation data (for preserving privacy of users), while addressing the product requirements such as coverage? We address these questions in §3 and §4 respectively."
    }, {
      "heading" : "3. LINKEDIN SALARY MODELING SYSTEM DESIGN AND ARCHITECTURE",
      "text" : "We describe the overall design and architecture of the salary modeling system deployed as part of the recently launched LinkedIn Salary product. Our system consists of an online component that uses a service oriented architecture for retrieving compensation insights corresponding to the query from the user facing product, and an offline component for processing anonymized compensation data and generating compensation insights. Figure 2 presents the key components of our system, divided into three groups: Online, Run Regularly, and Run As Needed. Explanations of the text in the figure are in italics in the remainder of the section."
    }, {
      "heading" : "3.1 Online System for Retrieving",
      "text" : "Compensation Insights\n3.1.1 LinkedIn Salary Platform\nThe REST Server provides compensation insights on request by instances of REST Client. The REST API [18] allows retrieval of individual insights, or lists of them. For each cohort, an insight includes, when data is available, the quantiles (10th and 90th percentiles, median), and histograms for base salary, bonus, and other compensation types. For robustness of the insights in the face of small numbers of submissions and changes as data is collected, we report quantiles such as 10th and 90th percentiles and median, rather than absolute range and mean.\n3.1.2 LinkedIn Salary Use Case For an eligible user, compensation insights are obtained via a\nREST Client from the REST Server implementing the Salary Platform REST API. These are then presented as part of LinkedIn Salary product. Based on the product and business needs, the eligibility can be defined in terms of criteria such as whether the user has submitted his/her compensation data within the last one year (give-to-get model), or whether the user has a premium membership.\nOur Salary Platform has four service APIs to give the information needed for LinkedIn Salary insight page: (1) a “criteria” finder to obtain the core compensation insight for a cohort, (2) a “facets” finder to provide information on cohorts with insights for filters such as industry and years of experience, (3) a “relatedInsights” finder to obtain compensation insights for related titles, regions, etc., and (4) a “topInsights” finder to obtain compensation insights by top paying industries, companies, etc. These finders were carefully designed to be extensible as the product needs evolve over time. For instance, although we had originally designed the “criteria” finder to provide insights during the compensation collection stage, we were able to reuse and extend it for LinkedIn Salary and Job search use cases.\n3.1.3 Job Search Use Cases The compensation insights can be very valuable for enhancing\nother LinkedIn products. For example, we would like to present the compensation information as part of a webpage providing the description of a job posting, as part of the job search results page, and as a facet to filter jobs based on salary range. Suppose the compensation insights are requested for a combination such as 〈title, company, location〉, e.g., 〈Software engineer, Slack Technologies, SF Bay Area〉 for presentation under a corresponding job posting webpage. We may not have sufficient data or confidence to return insights at this granularity. Hence, the Salary Platform REST Server generates possible generalizations such as 〈Software engineer, Internet industry, SF Bay Area〉 or 〈Software engineer, SF Bay Area〉, and probes the Insights store for the corresponding precomputed insights."
    }, {
      "heading" : "3.2 Offline System for Computing",
      "text" : "Compensation Insights\nThe insights are generated using an offline workflow, that consumes the Anonymized Submissions data (corresponding to cohorts having at least a minimum number of entries) on HDFS, and then pushes the results to the Insights and Lists Voldemort key-value stores [26] for use by the REST Server. Analytical Data is generated at several points for business analysis, modeling research, development, debugging, and testing. We can also use previously generated compensation insights data for sanity checks on data quality, by checking differences in the size or number of cohorts, or in their insights.\nThis offline workflow consists of two groups, Run Regularly for the hadoop flow that runs more than once a day, and Run As Needed\nfor hadoop and other flows that run, for instance, when externally supplied data changes.\n3.2.1 Hadoop Flow Run Regularly This consists of components that Process Outliers (see §4.1),\nusing several methods to detect questionable submissions and remove or modify them, Aggregate Submissions to obtain 10th percentiles (“low end”), medians, 90th percentiles (“high end”), and histograms of various kinds of compensation for each cohort, Smooth Small Cohorts (see §4.2) that uses Bayesian smoothing to help with some of the problems caused by cohorts with small numbers of submissions, Delete & Add Insights to remove certain insights and add others such as from trusted reliable sources other than user submissions, and Make Lists to generate lists of insights or their keys.\n3.2.2 Flows Run As Needed These consist of components such as those that Ingest External\nData (see §4.1.1) to map external notions of title and region to their LinkedIn counterparts, Train Regression Models (see §4.2.3) to train models used for smoothing and prediction, Choose Smoothing Parameters (see §5.3.3) to optimize tuning parameters for Bayesian smoothing, Make Similar Titles to create for each title, a list of similar ones, and its analog Make Similar Regions."
    }, {
      "heading" : "4. STATISTICAL MODELING FOR",
      "text" : "COMPENSATION INSIGHTS"
    }, {
      "heading" : "4.1 Outlier Detection",
      "text" : "An important goal for the LinkedIn Salary product is accuracy of reported results. This is difficult in general to evaluate, since there are few reliable public datasets containing comparable information. We use user-submitted data as the main basis of reported results, so concerns about the accuracy of the submissions naturally come up. Even if they were completely accurate, there would still be issues of selection bias; but accuracy of submissions cannot simply be assumed. Mistakes and misunderstandings in submission entry do occur, and there is the possibility of falsification.\nAs part of the salary modeling offline workflow, anonymized submissions are treated with three outlier detection methods to remove or otherwise reduce the impact of spurious data. The first method uses sanity-check limits such as the federal minimum wage (lower limit) and a reasonable upper bound (e.g., $2M /year in US) for base salary, excluding submissions with compensation outside the limits. The second method uses limits derived from US Bureau of Labor Statistics (BLS) Occupational Employment Statistics (OES) data [2], though the many-to-many mapping of BLS to LinkedIn titles and regions makes this less than perfect. The third method is based on traditional box-and-whisker methods, classifying some submissions as outliers based solely on the anonymized submissions in each cohort remaining after the first two methods.\n4.1.1 External Dataset based Outlier Detection We have mapped the BLS OES compensation dataset, to LinkedIn\ntitles and regions, for outlier detection for US base salary data. To map from 840 BLS occupations (defined by the Standard Occupational Classification (SOC) system) to about 25K LinkedIn standardized (canonical) titles, first we expand the SOC codes to O*NET alternate titles (www.onetcenter.org), then apply LinkedIn standardization software which maps an arbitrary title to a canonical title. The mapping from BLS to LinkedIn regions is done using zipcodes. In general, one BLS SOC code corresponds to more than one LinkedIn standardized title, and more than one BLS region corresponds to one LinkedIn region. Thus, we have a many to many\nmapping. For each LinkedIn (title, region) combination, we obtain all BLS (SOC code, region) rows that map to it, compute the associated box-and-whisker compensation range limits, and aggregate these limits to derive one lower and one upper bound.\nWe obtained the limits for 6.5K standardized titles (out of 25K total) and about 300 US region codes, resulting in 1.5M LinkedIn 〈title, region〉 pairs. Submissions with base salary outside these limits are not used to compute compensation insights.\n4.1.2 Outlier Detection based on User Submitted Data\nThe outlier detection based solely on user submitted data is done via a box-and-whisker method [3]3, applied separately to each compensation type.\nThe box-and-whisker method is as follows. For each compensation type for each cohort (of any granularity, not just title-countryregion), we compute Q1 and Q3, the first and third quartiles respectively, the interquartile range, IQR = Q3 − Q1, then compute the lower limit as Q1 − 1.5 · IQR, and the upper limit as Q3+2 ·IQR. We chose (and tuned) the different factors 1.5 and 2 to reflect the typically skewed compensation distributions observed in our dataset.\nSubmissions with base salary outside the calculated ranges are excluded from consideration in the compensation insights. Other compensation type data in the submissions are instead clipped to the range of the limits. We do not want to prune the entire entry since the base salary is valid, and given that, do not want to remove outlying values of other compensation types, since that would have the effect of making them zero for the total compensation calculation.\nIf the number of valid (non-outlier) entries of a given compensation type in a given cohort falls below a threshold, or if the fraction of box-and-whisker outliers is too high, we exclude the entire set of entries for that compensation type for that cohort. In that case, when the compensation type is base salary, we exclude the entire cohort."
    }, {
      "heading" : "4.2 Bayesian Hierarchical Smoothing",
      "text" : "There is an inherent trade-off between the quality of compensation insights and the product coverage. The higher the threshold for the number of samples for the cohort, the more accurate the empirical estimates are. The lower the threshold for the number of samples, the larger the coverage. Since it is critical for the product to have a good coverage of insights for many different cohorts, obtaining accurate estimates of insights (e.g., percentiles) for cohorts with very few user input samples turns out to be a key challenge. Due to the sparsity of the data, empirical estimates of the 10th or 90th percentile, or even the median, are not reliable when a cohort contains very little data. For example, the empirical estimate of the 10th percentile of a cohort’s compensation with only 10 points is the minimum of the 10, which is known to be a very inaccurate estimate.\nWe next describe a Bayesian hierarchical smoothing methodology to obtain accurate estimates of compensation insights for cohorts with very little data. Specifically, for cohorts with large enough sample sizes (i.e., number of samples greater than or equal to a threshold h), we consider it safe to use the empirical estimates for median and percentiles. On the other hand, for cohorts with sample sizes less than h, we first assume that the compensation follows a log-normal distribution [24] (see §5.3.1 for validation of the assumption), then exploit the rich hierarchical structure amongst 3http://www.itl.nist.gov/div898/handbook/ prc/section1/prc16.htm\nthe cohorts, and “borrow strength” from the ancestral cohorts that have sufficient data to derive cohort estimates. For example, by successively relaxing the conditions, we can associate the cohort “UX designers in SF Bay Area in Internet industry with 10+ years of experience” with larger cohorts such as “UX designers in SF Bay Area in Internet industry”, “UX designers in SF Bay Area with 10+ years of experience”, “UX designers in SF Bay Area”, and so forth, and pick the “best” ancestral cohort using statistical methods (§4.2.1). After the best ancestor is selected, we treat the data collected from the ancestral cohort as the prior to apply a Bayesian smoothing methodology, and obtain the posterior of the parameters of the log-normal distribution for the cohort of interest (§4.2.2). We also describe how to handle the “root” cohorts (a combination of job title and region in our product) using a prior from a regression model in §4.2.3.\n4.2.1 Finding the Best Ancestral Cohort The set of all possible cohorts forms a natural hierarchy, where\neach cohort in the hierarchy can have multiple ancestors, as in the example of “UX designers in SF Bay Area in Internet industry with 10+ years of experience” given above. We describe our statistical approach to find the best ancestral cohort among all the ancestors of the cohort of interest in the hierarchy. The ancestor can later be used to provide the prior information to estimate the posterior of parameters for the distribution of compensation insights for the cohort. We note that for ancestors with sample size greater than h, the empirical estimates of parameters are used to obtain the prior; otherwise, the posterior estimates of the parameters for the ancestor will be used as the prior of the cohort of interest.\nWe denote the cohort of interest as y, and the observed logtransformed compensation data that belong to y as yi, i = 1, . . . , n, with empirical average ȳ. Let P = {z(1), . . . , z(K)} be the set of y’s ancestral cohorts. Our objective is to pick the “best” ancestral cohort from P , based on how well the data in y matches the data in the ancestor statistically.\nAssume for cohort z(j), the compensation data follows a lognormal distribution, with mean µj and variance σ2j after the log transformation. We can pick the best z(J) out of the K ancestors of y, by the following criteria (maximizing log-likelihood, or equivalently minimizing negative log-likelihood, assuming each yi to be independently drawn at random from the distribution of z(j)), using the estimates of µj and σ2j for each z (j), j = 1, . . . ,K:\nJ = arg min j n2 log(2πσ2j ) + n∑ i=1 (yi − µj)2\n2σ2j  (1) We then use the corresponding µJ and σ2J that provides the maximum of the log-likelihood as the prior for the smoothing of the compensation percentiles for cohort y (see §4.2.2 for details). We also note that in Equation (1), if the number of samples for z(j) is greater than h, we compute µj and σ2j from the empirical estimates from its data directly. Otherwise, µj and σ2j would be estimated by the posterior of cohort z(j) smoothed by the prior from z(j)’s ancestors, again following §4.2.2.\n4.2.2 Bayesian Hierarchical Smoothing For cohort y, suppose we pick cohort z as the best ancestor fol-\nlowing §4.2.1. Denote µ and σ2 as the estimated mean and variance for z. Also, denote number of samples in cohort z as m. We can assume the following model for the data yi, i = 1, . . . , n, with yi being the log of each compensation sample in cohort y:\nyi ∼ N(ν, τ2), i = 1, . . . , n (2)\nwhere we assume ν and τ2 have the conjugate prior as follows:\nν|τ2 ∼ N(µ, τ 2\nn0 ), (3)\nτ̃ = 1/τ2 ∼ Gamma(η/σ2, η), (4)\nwhere n0 = m/δ, and δ and η are hyper-parameters that can be tuned through cross-validation. We note that the prior mean of ν is the same as the best ancestor’s mean µ, and for the prior distribution of τ̃ , 1/σ2 is the prior mean (also from the best ancestor) and 1/(ησ2) is the prior variance.\nNow we derive the posterior p(ν, τ̃ |y1, . . . , yn). Let ȳ = ∑ i yi/n\nbe the empirical average of observations in y. First, the joint posterior,\np(ν, τ̃ |y1, . . . , yn) ∝ p(y1, . . . , yn|ν, τ̃)p(ν|τ̃)p(τ̃). (5)\nThe marginal posterior, p(τ̃ |y1, . . . , yn) ∝ ∫ p(y1, . . . , yn|ν, τ̃)p(ν|τ̃)p(τ̃)dν\n∼ Gamma (n\n2 +\nη\nσ2 ,\nη + 1\n2 ∑ i (yi − ȳ)2 + nn0 2(n+ n0) (ȳ − µ)2\n) .\n(6)\nHence the posterior mean of τ̃ ,\nˆ̃τ = n 2 + η σ2\nη + 1 2 ∑ i (yi − ȳ)2 + nn02(n+n0) (ȳ − µ) 2 . (7)\nFor simplicity, we plug in τ̂2 = 1/ˆ̃τ as the estimate of τ2 for the rest of the calculation. Given τ̃ , the posterior of ν is\np(ν|τ̃ , y1, . . . , yn) ∝ p(y1, . . . , yn|ν, τ̃)p(ν|τ̃)\n∼ N( n n+ n0 ȳ + n0 n+ n0 µ,\n1\n(n+ n0)τ̃ )\n(8)\nTherefore, for any new observation ynew,\nE[ynew|y1, . . . , yn] = E[ν|τ̃ , y1, . . . , yn] = n\nn+ n0 ȳ+ n0 n+ n0 µ,\n(9)\nV ar[ynew|y1, . . . , yn] = E[V ar[ynew|y1, . . . , yn, ν]] + V ar[E[ynew|y1, . . . , yn, ν]]\n= τ̂2 + V ar[ν|y1, . . . , yn] = (1 + 1\nn+ n0 )τ̂2 (10)\nTo summarize, the median, 10th percentile, and the 90th percentile of the compensation for cohort y can be estimated as follows:\n• Input: Data for cohort y as y1, . . . , yn; For best ancestral cohort z of y, let z’s sample size bem, mean be µ and variance be σ2. Compute the empirical average, ȳ of y1, . . . , yn.\n• Tuning parameters: δ and η, which can be optimized via cross-validation (see §5.3.3).\n• Let n0 = m/δ.\n• Estimate the posterior mean of τ2 as\nτ̂2 =\nη + 1 2 ∑ i (yi − ȳ)2 + nn02(n+n0) (ȳ − µ) 2\nn 2 + η σ2\n.\n• For a new observation added to the population of existing observations of y, denoted as ynew, the mean and variance of ynew (post log-transformation) are\nµ̂ = E[ynew|y1, . . . , yn] = n\nn+ n0 ȳ + n0 n+ n0 µ,\nσ̂2 = V ar[ynew|y1, . . . , yn] = (1 + 1\nn+ n0 )τ̂2.\nSince normal distribution is symmetric, the median of log(compensation) is set to µ̂, 10th percentile to µ̂−1.282·σ̂, and 90th percentile to µ̂+ 1.282 · σ̂.\n• Obtain the final estimates by taking exponential transformation of these three quantities.\n4.2.3 Smoothing for Root Cohorts We note that there can be cases where the root cohorts in the\nhierarchy do not have enough samples to estimate percentiles empirically. For LinkedIn Salary product specifically, we consider a root cohort as a function of a job title and a geographical region, and model the compensation for these cohorts using a number of features. Simply using parent cohorts such as title only or region only as the prior might not be good enough, as there is a lot of heterogeneity for compensation of the same title for different regions (e.g., New York vs. Fresno), and that of the same region for different titles as well (e.g., Software engineers vs. Nurses). Hence, for these cohorts, we build a feature based regression model to serve as the prior for Bayesian smoothing.\nSuppose there are P root cohorts, with a combined total of N samples. Denote the value of ith sample for cohort p, after applying log-transformation as uip, and the vector of all responses asU . Let the feature vector for cohort p bexp, which can include features such as title, country, region, skills related to the title, average compensation data for the region determined from external sources, and so forth, and the entire design matrix be X . The regression model can then be expressed as\nuip ∼ N(x′pβ, γ2), (11)\nwhere β has a Gaussian prior\nβ ∼ N(0, λI), (12)\nand λ is a L2 penalty tuning parameter for the regression model. The estimate of β is given by\nβ̂ = (X ′X + λI)−1X ′U , (13)\nand the estimate of γ2 is given by\nγ̂2 =\n∑ i ∑ p (uip − x′pβ̂)2\nN . (14)\nFor any root cohort, to obtain its posterior of ν and τ̃ following Equation (5), the parameters µ and σ2 for the prior in Equations (3) and (4) can be specified as: µ = x′pβ̂, and σ2 = γ̂2. We set the “sample size”m to be equal to the smoothing sample size threshold h, with the interpretation that a regression model prior is given the same weight as an ancestral cohort with h samples."
    }, {
      "heading" : "5. EXPERIMENTS",
      "text" : "We next present an evaluation of our system for computing compensation insights as part of LinkedIn Salary, focusing on the statistical models. We study the impact of the outlier detection methods and evaluate the effectiveness of the regression model and Bayesian\nstatistical smoothing. We also investigate the performance of the statistical smoothing techniques for different segments and under different parameter settings."
    }, {
      "heading" : "5.1 Experimental Setup",
      "text" : "As described earlier, we implemented our statistical models as part of the offline system for computing compensation insights, and deployed in production as part of LinkedIn Salary product. Our offline system for computing compensation insights is implemented in a distributed computing environment using Hadoop.\nWe performed our experiments over nearly one year of anonymized compensation data collected from more than one million LinkedIn users, across three countries (USA, Canada, and UK). Due to the privacy requirements, we have access only to cohort level data containing anonymized compensation submissions (e.g., salaries for Software Engineers in San Francisco Bay Area), limited to those cohorts having at least a minimum number of entries. Each cohort consists of a set of anonymized compensation data corresponding to individuals, and each individual entry consists of values for different compensation types. There are approximately 54K titlecountry-region cohorts, and 24K title-country-region-company-cohorts, for which we have insights.\nAs discussed in §2.1, we cannot compare the performance of different models using online A/B testing, since users may not necessarily have a good perception of the true compensation range. We also face additional evaluation challenges since there are very few reliable and easily available ground truth datasets in the compensation domain (e.g., BLS OES dataset), and it is challenging to map such datasets to LinkedIn’s taxonomy due to the noise introduced."
    }, {
      "heading" : "5.2 Studying Impact of Spurious Added Data",
      "text" : "Because of the lack of reliable ground truth data, our focus is on studying the impact of adding spurious data, then winnowing it by the box-and-whisker method (§4.1.2). We take the actual compensation data submitted, after it has gone through all the outlier detection stages, and consider it as valid. This constitutes about 80% of the submissions in cohorts. We limit this data to title-countryregion cohorts, to US data of yearly base salary, and to cohorts with at least 20 valid entries, the last so that statistical smoothing does not interfere with interpretation. This leaves about 10K cohorts for the study.\nFor each cohort, we record selected quantiles (10th percentile, median, and 90th percentile), then perturb the cohort in three ways by addition of spurious, synthetic salary entries numbering certain fractions (5% through 35% in steps of 5%) of the original numbers of entries. The three methods of perturbation are:\n• Add the spurious data at the federal minimum wage, $15080.\n• Add the spurious data at $2 million. • Add the spurious data uniformly at random between the 10th\nand 90th percentiles, the low and high ends.\nFor each method, we examine the following, averaged over all the cohorts used in the test.\n• The fraction of the added entries that are removed as outliers; a perfect score is 1. • The fraction of the original entries that are removed as out-\nliers; a perfect score is 0. • The fractional changes in the selected quantiles. This is the\nmost important of these measures in practice, because these are what we report to users.\nFor all the data added between low and high ends, less than 1% of the added or original entries were removed. The reported quantiles changed by less than 5%.\nFor added high entries, about 100% were removed as outliers until the amount of the added data was sufficiently large: at 30%, 92% were removed, and at 35%, none. Less than 1% of the original data were removed as outliers. The selected quantiles changed by less than 1% until at 30% added, the median increased by 1% and the high end increased by 180%, and at 35%, the low end increased by 6%, median by 13%, and high end by 2300%.\nFor added low entries, note that the minimum wage is in general closer to the low end of the original data, than $2 million is to the high end. One might therefore suspect that outlier detection by the box-and-whisker method of these added entries, would be less successful than for added high entries. Little of the original data were eliminated as outliers; see Table 1 for other results.\nOverall, the box-and-whisker outlier detection, aside from very large amounts of spurious data or when the low end changed from the addition of more than 5% spurious minimum wage data, did at least reasonably and often quite well."
    }, {
      "heading" : "5.3 Evaluating Regression Model and Bayesian Statistical Smoothing",
      "text" : "We first selected the set of base salaries reported by LinkedIn users in US, resulting in about 800K entries. We limited to only title-country-region cohorts so that each user submission is reflected exactly once. We used this dataset for validating the log-normal distribution assumption and for training and evaluating the regression model.\n5.3.1 Validation of Log-Normal Assumption We sampled 10K entries at random from the above dataset, and\ngenerated Q-Q plots in both original and logarithmic scale. From Figure 3, we can visually observe that the logarithmic scale is a better fit (closer to a straight line), suggesting that the data on the whole can be fit to a log-normal distribution. Hence, for training the regression model as well as applying statistical smoothing, we use the compensation data in the logarithmic scale. However, we remark that this assumption is an approximation since the observed salaries for an individual cohort may not follow a log-normal distribution.\n5.3.2 Evaluating Regression Model for Root Cohorts We trained and validated log-linear regression models for esti-\nmating compensation for each title-country-region cohort. A total of 24 models were trained corresponding to three countries and eight compensation types. We performed evaluation for all 24 combinations, but present results only for base and total compensation models in US. For the (log) base salary model, we observed\nthat the mean cross-validation error (0.085) is comparable to the mean empirical variance within each cohort (0.080). The corresponding numbers for the total compensation model were 0.097 and 0.092 respectively. Since we used only cohort-level features and not features that could discriminate samples within a cohort (due to anonymization), we do not expect the mean error to be less than the mean variance within each cohort. Thus, these results suggest that the trained models are performing well.\nWe next computed the coefficient of determination, R2 for the (log) base salary model in two different ways. First, we computed over the entire dataset, where each row represents the anonymized (log) base salary reported by some user. There are as many rows for each title-country-region cohort as the number of users that provided salary for that combination. We computed R2 = 1 − the residual sum of squares\nthe total sum of squares = 0.58. The same value (after rounding) was observed for the total compensation as well. Due to anonymization however, intuitively, our model attempts to predict the mean log base salary for a cohort, but not the base salary for each user. Hence, we also computed a variant ofR2 over the aggregated dataset, with one row per title-country-region cohort. The empirical mean log base salary for each cohort is treated as the observed value, and the predicted log base salary is treated as the predicted value. The coefficient computed in this manner was significantly higher: 0.88 for both base and total compensation. We observed similar results for all 24 models.\n5.3.3 Optimization of Statistical Smoothing Parameters\nWe next describe how we chose the optimal values of the following parameters used as part of statistical smoothing: (1) δ, which is a discounting factor for the effect of the ancestral cohort, and\n(2) η, which is a parameter used in the prior Gamma distribution for choosing the variance. The key idea is to maximize the likelihood of observing a hold-out set, with respect to the posterior distribution computed with each choice of smoothing parameters. We partitioned the set S of all observed entries, restricted to cohorts with small sample size (< 20), where we applied smoothing, into a training set, Strain and a hold-out test set, Stest of 10% of the data. We performed this over all possible cohorts (title-country-region, title-country-region-company, etc.), randomly partitioning each cohort, ensuring that at least one entry is present in the hold-out set. For different choices of smoothing parameters, we computed the posterior distribution for each cohort based on the training set, and computed the (combined) likelihood of observing the entries in the hold-out set (assuming independence across entries, which is an approximation). We selected the parameters that maximized this likelihood. Algorithm 1 provides a formal description.\nAlgorithm 1 Optimizing smoothing parameters 1: Input: The set, S of all observed entries, partioned into the\ntraining set, Strain and the hold-out set, Stest; The candidate set, ∆ of choices for δ; The candidate set, E of choices for η. 2: Output: Optimal parameters, δ∗, η∗. 3: for all δ ∈ ∆ do 4: for all η ∈ E do 5: Compute smoothed posterior log-normal distribution,\nDposterior(c) for each cohort, c based on only Strain.\n6: LL[δ, η] := ∑ s∈Stest log(pDposterior(c(s))(s)), where c(s) denotes the cohort containing entry s. 7: Output (δ∗, η∗) = arg maxδ∈∆,η∈E LL[δ, η].\nWe performed grid search with ∆ = [1, 1000] and E = {0.01 · 2r|r ∈ [0, 11]}, and computed the optimal parameters overall as well as for different segments such as cohorts containing a company, cohorts containing an industry, and so on. In all cases, the likelihood was maximized well inside this range (that is, not at the extremes for either δ or η). We observed that (δ∗ = 5, η∗ = 0.32) was optimal overall, while for cohorts containing a company, (δ∗ = 250, η∗ = 0.04), implying larger discount for ancestral cohort. This observation agrees with our intuition that the salaries at say, title-country-region-company level could deviate significantly from the corresponding title-country-region level (which is likely to be selected as the best ancestral cohort), and hence we would like to give relatively lesser weight to the ancestral cohort in such cases.\n5.3.4 Evaluating Statistical Smoothing: Goodness-of-fit Analysis\nIn addition to parameter optimization, Algorithm 1 also helps us study the “goodness-of-fit” of the observed data in the hold-out set, with no smoothing vs. some extent of smoothing. Here, we compare the likelihood of observing each cohort in the hold-out set with respect to (1) a distribution derived just from the observed entries in the corresponding cohort in the training set, vs. (2) the smoothed posterior distribution that also takes ancestral cohort into account. As δ → ∞, the weight given to the ancestral cohort tends to zero, and hence the smoothed distribution converges to a (log-normal) distribution derived from just the observed entries. However, in §5.3.3, we observed that a finite value of δ (= 5 overall), rather than δ →∞ (corresponding to the right extreme, 1000 in our computation), leads to the maximum likelihood or the best goodnessof-fit. A similar observation holds even when limited to different\nsegments such as cohorts containing a company. We conclude that combining ancestor and cohort data with statistical smoothing results in better goodness-of-fit for the observed data in the hold-out set, compared to using ancestor or cohort alone.\nAn intuitive explanation is that statistical smoothing provides better stability and robustness of insights. Inferring a distribution based on just the observed entries in the training set for a cohort and using it to fit the corresponding hold-out set is not as robust as using the smoothed distribution to fit the hold-out set, especially when the cohort contains very few entries. However, this evaluation approach intrinsically requires a distributional assumption for computing the likelihood, which can be thought of as a limitation since (1) any such assumption could be empirically validated on the whole (§5.3.1), but may not hold for an individual cohort, and (2) the log-normal distribution assumption is used as part of the smoothing methodology, and hence can be viewed as “favoring” it. Consequently, we decided to evaluate using a non-parametric approach that does not require any distributional assumption.\n5.3.5 Evaluating Statistical Smoothing: Quantile Coverage Test\nWe next performed a quantile coverage test, wherein we measured what fraction of a hold-out set lies between two quantiles (e.g., 10th and 90th percentiles), computed based on the training set (1) empirically without smoothing, and (2) after applying smoothing. For 0 < α < β < 1, an ideal quantile computation method should satisfy the following properties (averaged across all cohorts): (1)α fraction of the hold-out data points in a cohort should be lower than the corresponding α-quantile, (2) 1−β fraction of the hold-out data points in a cohort should exceed the corresponding β-quantile, and (3) β − α fraction is in between, where the quantiles are computed for each cohort based on the training set entries. Since 10th and 90th percentiles are shown to the users in the product, we set α = 0.1, β = 0.9. We used the optimal parameters from §5.3.3 for computing 10th and 90th percentiles using smoothing method.\nWe partitioned the set of observed entries (limited to cohorts with small sample size (< 20), where we applied smoothing) into a training set and a hold-out set as before. For each cohort, we computed 10th and 90th percentiles using the training set data points using each method (empirical vs. smoothed), and measured what fraction of the hold-out data points belong to this range. We then computed the mean fraction across all cohorts (and also for different segments of interest).\nWe observed that the fractions computed using smoothed percentiles are significantly better than those computed using empirical percentiles. The mean fraction of the hold-out data between 10th and 90th percentiles is 85% with smoothing (close to the ideal of 80%), but only 54% with empirical approach. The remaining hold-out data is roughly evenly split below 10th percentile and above 90th percentile. We observed similar results for various segments such as cohorts containing a company, an industry, or a degree.\nWe also investigated the effect of the cohort size. Intuitively, we expect closer-to-ideal results for larger cohorts, but more deviation for very small cohorts. Comparing cohorts with at least 5 entries to those with just 3 or 4 entries, the above mean fraction deviates only slightly away from the 80% ideal (83% vs. 86%), with smoothing applied. However, the deviation is significantly worse with empirical approach (71% vs. 39%), agreeing with our intuition that computing empirical percentiles based on very few entries is unreliable.\nWhile such a quantile coverage test by itself cannot imply that an approach is effective, the goodness-of-fit and coverage-based eval-\nuations together establish that statistical smoothing leads to significantly better and robust compensation insights. Compensation domain knowledge experts also confirmed this conclusion. By employing statistical smoothing, we were able to reduce the threshold used for displaying compensation insights in LinkedIn Salary product, thereby achieving significant increase in product coverage, while simultaneously preserving the quality of the insights."
    }, {
      "heading" : "6. LESSONS LEARNED IN PRACTICE",
      "text" : "We next present the challenges encountered and the lessons learned through the production deployment of both our salary modeling offline workflow and REST service as part of the overall LinkedIn Salary system for more than a year, initially during the compensation collection and later as part of the publicly launched product. We performed several iterative deployments of the offline workflow and the REST service, always maintaining a sandbox version (for testing) and a production version of our system. The service was deployed to multiple servers across different data centers to ensure better fault-tolerance and latency.\nOne of the greatest challenges has been the lack of good public “ground truth” datasets. In the United States, the Internal Revenue Service has fairly complete salary data, but it is not public. The Bureau of Labor Statistics makes available aggregate data, and but that brings with it the challenge of mapping to LinkedIn taxonomy. Some state governments, for example California, make available government employee salaries, but government jobs differ from private-sector jobs.\nWe did simulations sampling smaller from larger cohorts to get an idea of how much variation in the reported quantiles might be expected if we decreased the sample size threshold. This analysis helped us understand the tradeoffs between increasing coverage and ensuring robust insights, and guided the product team decisions on whether to reduce the threshold. In fact, this analysis helped motivate the need for applying Bayesian smoothing, after which we were able to further reduce the threshold while retaining robustness of insights. The choice of smoothing threshold (h = 20) was determined by similar tradeoffs. While applying smoothing is desirable for even larger sized cohorts from the perspective of robustness, a practical limitation is that the smoothed histograms have to be computed based on a parametrized (log-normal) distribution, resulting in all smoothed histograms having identical shape (truncated log-normal distribution from 10th to 90th percentiles). As the empirical histogram was considered more valuable from the product perspective, we decided not to display any histogram for smoothed cohorts, and chose a relatively low threshold of 20 to ensure adequate coverage for cohorts with histograms."
    }, {
      "heading" : "7. RELATED WORK",
      "text" : "Salary Information Products: There are several commercial services offering compensation information. For example, Glassdoor [6] offers a comparable service, while PayScale [8] collects individual salary submissions, offers free reports for detailed matches, and sells compensation information to companies. The US Bureau of Labor Statistics [7] publishes a variety of statistics on pay and benefits.\nPrivacy: Preserving user privacy is crucial when collecting compensation data. At first, we wanted to make use of rigorous privacy techniques such as differential privacy [14, 15]. However, we soon realized that these are not applicable in our context for the following reasons: (1) the amount of noise to be added to the quantiles, histograms, and other insights would be very large (thereby depriving the compensation insights of their reliability and usefulness), since the worst case sensitivity of these functions to any one user’s\ncompensation data could be large, and (2) the insights need to be provided on a continual basis with the arrival of new data points. Although there is theoretical work on applying differential privacy under continual observations [12, 16], we have not come across any practical implementations or applications of these techniques. We also explored approaches similar to recent work at Google [17] and Apple [19] on privacy-preserving data collection at scale. These approaches are (or seem to be) built on the concept of randomized response [27] and require response from typically hundreds of thousands of users for the results to be useful. In contrast, even the larger of our cohorts contain only a few thousand data points, and hence these approaches are not applicable in our setting.\nSurvey Techniques: There is extensive work on traditional statistical survey techniques [20, 23], as well on newer areas such as web survey methodology [11]. See [1] for a survey of non-response bias challenges, and [10] for an overview of selection bias.\nStatistical Smoothing: The idea of Bayesian hierarchical statistical smoothing originates from the smoothing of sparse events (e.g., CTR) in the context of computational advertising [9, 29], where a natural hierarchy for the combination of ads category and publisher category is used for an (ad, publisher) pair with very little data to borrow strength from its ancestor nodes. However, that is an entirely different context and the models used are hence different."
    }, {
      "heading" : "8. CONCLUSIONS AND FUTURE WORK",
      "text" : "We studied the problem of computing robust, reliable compensation insights based on anonymized compensation data collected from LinkedIn users. We presented the design and architecture of the modeling system underlying LinkedIn’s Salary product, which was launched recently towards the goals of bringing greater transparency and helping professionals make more informed career decisions. We highlighted unique challenges such as modeling on anonymized data, lack of good evaluation datasets or measures, and the simultaneous need for user privacy, data quality, and product coverage, and described how we addressed them using mechanisms such as outlier detection and Bayesian hierarchical smoothing. We showed the effectiveness of our models through extensive experiments on anonymized data from over one million users. We also discussed the design decisions and tradeoffs while building our system, and the lessons learned from more than one year of production deployment.\nThe availability of compensation data, combined with other datasets, opens several research possibilities to better understand and improve the efficiency of career marketplace (as discussed in §1). There are also several directions to extend this work, which we are currently pursuing. We would like to improve quality of insights and product coverage via better data collection and processing, including inference of insights for cohorts with no data, improvement of the statistical smoothing methodology, better estimation of variance in the regression models, and creation of intermediary levels such as company clusters between companies and industries. We also plan to improve outlier detection both at the user-level (using user profile and behavioral features, during submission), and at the cohort level (e.g., using the medcouple measure of skewness [22]). Another direction is to use other datasets (e.g., position transition graphs, salaries extracted from job postings) to detect/correct inconsistencies in the insights across cohorts. Finally, mechanisms can be explored to quantify and address different types of biases such as sample selection bias and response bias. For example, models could be developed to predict response propensity based on user profile and behavioral attributes, which could then be used to compensate for response bias through techniques such as inverse probability weighting."
    }, {
      "heading" : "9. ACKNOWLEDGMENTS",
      "text" : "The authors would like to thank all other members of LinkedIn Salary team for their collaboration for deploying our system as part of the launched product, and Stephanie Chou, Ahsan Chudhary, Tim Converse, Tushar Dalvi, Anthony Duerr, David Freeman, Joseph Florencio, Souvik Ghosh, David Hardtke, Parul Jain, Prateek Janardhan, Santosh Kumar Kancha, Ryan Sandler, Cory Scott, Ganesh Venkataraman, Ya Xu, and Lu Zheng for insightful feedback and discussions."
    }, {
      "heading" : "10. REFERENCES",
      "text" : "[1] Encyclopedia of Social Measurement, volume 2, chapter Non-Response Bias.\nAcademic Press, 2005. [2] BLS Handbook of Methods, chapter 3, Occupational Employment Statistics.\nU.S. Bureau of Labor Statistics, 2008. http://www.bls.gov/opub/hom/pdf/homch3.pdf.\n[3] NIST/SEMATECH e-Handbook of Statistical Methods. National Institute of Standards and Technology, U.S. Department of Commerce, 2013. [4] How to rethink the candidate experience and make better hires. CareerBuilder’s Candidate Behavior Study, 2016. [5] Job seeker nation study. Jobvite, 2016. [6] Glassdoor introduces salary estimates in job listings, February 2017.\nhttps://www.glassdoor.com/press/ glassdoor-introduces-salary-estimates-job-listings-reveals-unfilled-jobs-272-billion/.\n[7] Overview of BLS statistics on pay and benefits, February 2017. https://www.bls.gov/bls/wages.htm. [8] Payscale data & methodology, February 2017. http://www.payscale. com/docs/default-source/pdf/data_one_pager.pdf. [9] D. Agarwal, R. Agrawal, R. Khanna, and N. Kota. Estimating rates of rare events with multiple hierarchies through scalable log-linear models. In KDD, 2010. [10] J. Bethlehem. Selection bias in web surveys. International Statistical Review, 78(2), 2010. [11] M. Callegaro, K. L. Manfreda, and V. Vehovar. Web survey methodology. Sage, 2015. [12] T.-H. H. Chan, E. Shi, and D. Song. Private and continual release of statistics. ACM Transactions on Information and System Security, 14(3), 2011. [13] A. Duerr and S. K. Kancha. Bringing salary transparency to the world. LinkedIn Engineering Blog, 2016. https://engineering.linkedin.com/blog/2016/10/ bringing-salary-transparency-to-the-world. [14] C. Dwork, K. Kenthapadi, F. McSherry, I. Mironov, and M. Naor. Our data, ourselves: Privacy via distributed noise generation. In EUROCRYPT, 2006. [15] C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data analysis. In TCC, 2006. [16] C. Dwork, M. Naor, T. Pitassi, and G. N. Rothblum. Differential privacy under continual observation. In STOC, 2010. [17] Ú. Erlingsson, V. Pihur, and A. Korolova. RAPPOR: Randomized aggregatable privacy-preserving ordinal response. In CCS, 2014. [18] R. T. Fielding. Architectural styles and the design of network-based software architectures. PhD thesis, University of California, Irvine, 2000. [19] A. Greenberg. Apple’s ‘differential privacy’ is about collecting your data – but not your data. Wired, June 2016. [20] R. M. Groves, F. J. Fowler Jr, M. P. Couper, J. M. Lepkowski, E. Singer, and R. Tourangeau. Survey methodology. John Wiley & Sons, 2011. [21] S. Harris. How to make the job market work like a supermarket. LinkedIn Pulse, 2016. https://www.linkedin.com/pulse/ how-make-job-market-work-like-supermarket-seth-harris. [22] M. Hubert and E. Vandervieren. An adjusted boxplot for skewed distributions. Computational statistics & data analysis, 52(12), 2008. [23] R. J. Jessen. Statistical survey techniques. John Wiley & Sons, 1978. [24] M. Pinkovskiy and X. Sala-i Martin. Parametric estimations of the world\ndistribution of income, 2009. Working Paper No. 15433, National Bureau of Economic Research.\n[25] R. Sandler. Introducing “LinkedIn Salary”: Unlock your earning potential. LinkedIn Blog, 2016. https://blog.linkedin.com/2016/11/02/ introducing-linkedin-salary-unlock-your-earning-potential. [26] R. Sumbaly, J. Kreps, L. Gao, A. Feinberg, C. Soman, and S. Shah. Serving large-scale batch computed data with project Voldemort. In FAST, 2012. [27] S. L. Warner. Randomized response: A survey technique for eliminating evasive answer bias. Journal of the American Statistical Association, 60(309), 1965. [28] J. Weiner. The future of LinkedIn and the Economic Graph. LinkedIn Pulse, 2012. [29] L. Zhang and D. Agarwal. Fast computation of posterior mode in multi-level hierarchical models. In NIPS, 2009."
    } ],
    "references" : [ {
      "title" : "Estimating rates of rare events with multiple hierarchies through scalable log-linear models",
      "author" : [ "D. Agarwal", "R. Agrawal", "R. Khanna", "N. Kota" ],
      "venue" : "KDD",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Selection bias in web surveys",
      "author" : [ "J. Bethlehem" ],
      "venue" : "International Statistical Review, 78(2)",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Web survey methodology",
      "author" : [ "M. Callegaro", "K.L. Manfreda", "V. Vehovar" ],
      "venue" : "Sage",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Private and continual release of statistics",
      "author" : [ "T.-H.H. Chan", "E. Shi", "D. Song" ],
      "venue" : "ACM Transactions on Information and System Security, 14(3)",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Bringing salary transparency to the world",
      "author" : [ "A. Duerr", "S.K. Kancha" ],
      "venue" : "LinkedIn Engineering Blog",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Our data",
      "author" : [ "C. Dwork", "K. Kenthapadi", "F. McSherry", "I. Mironov", "M. Naor" ],
      "venue" : "ourselves: Privacy via distributed noise generation. In EUROCRYPT",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Calibrating noise to sensitivity in private data analysis",
      "author" : [ "C. Dwork", "F. McSherry", "K. Nissim", "A. Smith" ],
      "venue" : "TCC",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Differential privacy under continual observation",
      "author" : [ "C. Dwork", "M. Naor", "T. Pitassi", "G.N. Rothblum" ],
      "venue" : "STOC",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "RAPPOR: Randomized aggregatable privacy-preserving ordinal response",
      "author" : [ "Ú. Erlingsson", "V. Pihur", "A. Korolova" ],
      "venue" : "CCS",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Architectural styles and the design of network-based software architectures",
      "author" : [ "R.T. Fielding" ],
      "venue" : "PhD thesis, University of California, Irvine",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Apple’s ‘differential privacy’ is about collecting your data – but not your data",
      "author" : [ "A. Greenberg" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2016
    }, {
      "title" : "Survey methodology",
      "author" : [ "R.M. Groves", "F.J. Fowler Jr", "M.P. Couper", "J.M. Lepkowski", "E. Singer", "R. Tourangeau" ],
      "venue" : "John Wiley & Sons",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "How to make the job market work like a supermarket",
      "author" : [ "S. Harris" ],
      "venue" : "LinkedIn Pulse",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "An adjusted boxplot for skewed distributions",
      "author" : [ "M. Hubert", "E. Vandervieren" ],
      "venue" : "Computational statistics & data analysis, 52(12)",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Statistical survey techniques",
      "author" : [ "R.J. Jessen" ],
      "venue" : "John Wiley & Sons",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1978
    }, {
      "title" : "Parametric estimations of the world distribution of income",
      "author" : [ "M. Pinkovskiy", "X. Sala-i Martin" ],
      "venue" : "Working Paper No",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2009
    }, {
      "title" : "Introducing “LinkedIn Salary”: Unlock your earning potential",
      "author" : [ "R. Sandler" ],
      "venue" : "LinkedIn Blog",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Serving large-scale batch computed data with project Voldemort",
      "author" : [ "R. Sumbaly", "J. Kreps", "L. Gao", "A. Feinberg", "C. Soman", "S. Shah" ],
      "venue" : "FAST",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Randomized response: A survey technique for eliminating evasive answer bias",
      "author" : [ "S.L. Warner" ],
      "venue" : "Journal of the American Statistical Association, 60(309)",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 1965
    }, {
      "title" : "The future of LinkedIn and the Economic Graph",
      "author" : [ "J. Weiner" ],
      "venue" : "LinkedIn Pulse",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Fast computation of posterior mode in multi-level hierarchical models",
      "author" : [ "L. Zhang", "D. Agarwal" ],
      "venue" : "NIPS",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : "In addition to helping job seekers understand their economic value in the marketplace, the compensation data has the potential to help us better understand the monetary dimensions of the Economic Graph [28] (which includes companies, industries, regions, jobs, skills, educational institutions, etc.",
      "startOffset" : 202,
      "endOffset" : 206
    }, {
      "referenceID" : 12,
      "context" : "Further, products such as LinkedIn Salary can help bring greater efficiency in the labor marketplace by reducing asymmetry of compensation knowledge, and by serving as market-perfecting tools for workers and employers [21].",
      "startOffset" : 218,
      "endOffset" : 222
    }, {
      "referenceID" : 16,
      "context" : "In the publicly launched LinkedIn Salary product [25], users can explore compensation insights by searching for different titles and locations (Figure 1).",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 4,
      "context" : "Our methodology for achieving this goal through a combination of techniques such as encryption, access control, anonymization, aggregation, and thresholding is described in [13].",
      "startOffset" : 173,
      "endOffset" : 177
    }, {
      "referenceID" : 9,
      "context" : "The REST API [18] allows retrieval of individual insights, or lists of them.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 17,
      "context" : "The insights are generated using an offline workflow, that consumes the Anonymized Submissions data (corresponding to cohorts having at least a minimum number of entries) on HDFS, and then pushes the results to the Insights and Lists Voldemort key-value stores [26] for use by the REST Server.",
      "startOffset" : 261,
      "endOffset" : 265
    }, {
      "referenceID" : 15,
      "context" : "On the other hand, for cohorts with sample sizes less than h, we first assume that the compensation follows a log-normal distribution [24] (see §5.",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 2,
      "context" : "01 · 2|r ∈ [0, 11]}, and computed the optimal parameters overall as well as for different segments such as cohorts containing a company, cohorts containing an industry, and so on.",
      "startOffset" : 11,
      "endOffset" : 18
    }, {
      "referenceID" : 5,
      "context" : "At first, we wanted to make use of rigorous privacy techniques such as differential privacy [14, 15].",
      "startOffset" : 92,
      "endOffset" : 100
    }, {
      "referenceID" : 6,
      "context" : "At first, we wanted to make use of rigorous privacy techniques such as differential privacy [14, 15].",
      "startOffset" : 92,
      "endOffset" : 100
    }, {
      "referenceID" : 3,
      "context" : "Although there is theoretical work on applying differential privacy under continual observations [12, 16], we have not come across any practical implementations or applications of these techniques.",
      "startOffset" : 97,
      "endOffset" : 105
    }, {
      "referenceID" : 7,
      "context" : "Although there is theoretical work on applying differential privacy under continual observations [12, 16], we have not come across any practical implementations or applications of these techniques.",
      "startOffset" : 97,
      "endOffset" : 105
    }, {
      "referenceID" : 8,
      "context" : "We also explored approaches similar to recent work at Google [17] and Apple [19] on privacy-preserving data collection at scale.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 10,
      "context" : "We also explored approaches similar to recent work at Google [17] and Apple [19] on privacy-preserving data collection at scale.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 18,
      "context" : "These approaches are (or seem to be) built on the concept of randomized response [27] and require response from typically hundreds of thousands of users for the results to be useful.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 11,
      "context" : "Survey Techniques: There is extensive work on traditional statistical survey techniques [20, 23], as well on newer areas such as web survey methodology [11].",
      "startOffset" : 88,
      "endOffset" : 96
    }, {
      "referenceID" : 14,
      "context" : "Survey Techniques: There is extensive work on traditional statistical survey techniques [20, 23], as well on newer areas such as web survey methodology [11].",
      "startOffset" : 88,
      "endOffset" : 96
    }, {
      "referenceID" : 2,
      "context" : "Survey Techniques: There is extensive work on traditional statistical survey techniques [20, 23], as well on newer areas such as web survey methodology [11].",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 1,
      "context" : "See [1] for a survey of non-response bias challenges, and [10] for an overview of selection bias.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 0,
      "context" : ", CTR) in the context of computational advertising [9, 29], where a natural hierarchy for the combination of ads category and publisher category is used for an (ad, publisher) pair with very little data to borrow strength from its ancestor nodes.",
      "startOffset" : 51,
      "endOffset" : 58
    }, {
      "referenceID" : 20,
      "context" : ", CTR) in the context of computational advertising [9, 29], where a natural hierarchy for the combination of ads category and publisher category is used for an (ad, publisher) pair with very little data to borrow strength from its ancestor nodes.",
      "startOffset" : 51,
      "endOffset" : 58
    }, {
      "referenceID" : 13,
      "context" : ", using the medcouple measure of skewness [22]).",
      "startOffset" : 42,
      "endOffset" : 46
    } ],
    "year" : 2017,
    "abstractText" : "The recently launched LinkedIn Salary product has been designed to realize the vision of helping the world’s professionals optimize their earning potential through salary transparency. We describe the overall design and architecture of the salary modeling system underlying this product. We focus on the unique data mining challenges in designing and implementing the system, and describe the modeling components such as outlier detection and Bayesian hierarchical smoothing that help to compute and present robust compensation insights to users. We report on extensive evaluation with nearly one year of anonymized compensation data collected from over one million LinkedIn users, thereby demonstrating the efficacy of the statistical models. We also highlight the lessons learned through the deployment of our system at LinkedIn.",
    "creator" : "LaTeX with hyperref package"
  }
}