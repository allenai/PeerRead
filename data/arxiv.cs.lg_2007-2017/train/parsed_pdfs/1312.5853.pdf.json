{
  "name" : "1312.5853.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Multi-GPU Training of ConvNets",
    "authors" : [ "Omry Yadan", "Keith Adams", "Yaniv Taigman" ],
    "emails" : [ "ranzato}@fb.com" ],
    "sections" : [ {
      "heading" : "Multi-GPU Training of ConvNets",
      "text" : ""
    }, {
      "heading" : "Omry Yadan Keith Adams Yaniv Taigman Marc’Aurelio Ranzato",
      "text" : ""
    }, {
      "heading" : "Facebook AI Group",
      "text" : "{omry, kma, yaniv, ranzato}@fb.com\nConvolutional neural networks [3] have proven useful in many domains, including computer vision [1, 4, 5], audio processing [6, 7] and natural language processing [8]. These powerful models come at great cost in training time, however. Currently, long training periods make experimentation difficult and time consuming.\nIn this work, we consider a standard architecture [1] trained on the Imagenet dataset [2] for classification and investigate methods to speed convergence by parallelizing training across multiple GPUs. In this work, we used up to 4 NVIDIA TITAN GPUs with 6GB of RAM. While our experiments are performed on a single server, our GPUs have disjoint memory spaces, and just as in the distributed setting, communication overheads are an important consideration. Unlike previous work [9, 10, 11], we do not aim to improve the underlying optimization algorithm. Instead, we isolate the impact of parallelism, while using standard supervised back-propagation and synchronous mini-batch stochastic gradient descent.\nWe consider two basic approaches: data and model parallelism [9]. In data parallelism, the minibatch is split across several GPUs as shown in fig. 3. Each GPU is responsible for computing gradients with respect to all model parameters, but does so using a subset of the samples in the mini-batch. This is the most straightforward parallelization method, but it requires considerable communication between GPUs, since each GPU must communicate both gradients and parameter values on every update step. Also, each GPU must use a large number of samples to effectively utilize the highly parallel device; thus, the mini-batch size effectively gets multiplied by the number of GPUs, hampering convergence. In our implementation, we find a speed-up of 1.5 times moving from 1 to 2 GPUs in the data parallel framework (when using 2 GPUs, each gets assigned a minibatch of size 128). This experiment used the same architecture, network set up and dataset described in Krizhevsky et al. [1].\nModel parallelism, on the other hand, consists of splitting an individual network’s computation across multiple GPUs [9, 12]. An example is shown in fig. 4. For instance, a convolutional layer with N filters can be run on two GPUs, each of which convolves its input with N/2 filters. In their seminal work, Krizhevsky et al. [1] further customized the architecture of the network to better leverage model parallelism: the architecture consists of two “columns” each allocated on one GPU.\nar X\niv :1\n31 2.\n58 53\nv4 [\ncs .L\nG ]\n1 8\nFe b\n20 14\nColumns have cross connections only at one intermediate layer and at the very top fully connected layers. While model parallelism is more difficult to implement, it has two potential advantages relative to data parallelism. First, it may requires less communication bandwidth when the cross connections involve small intermediate feature maps. Second, it allows the instantiation of models that are too big for a single GPUs memory. Our implementation of model parallelism, replicating prior work by Krizhevsky et al. [1], achieved a speed-up of 1.6 times moving from 1 to 2 GPUs.\nData and model parallelism can also be hybridized, as shown in fig. 5. We consider using 4 GPUs in three possible configurations: all model parallelism, all data parallelism, and a hybrid of model and data parallelism. In our experiment, we find that the hybrid approach yields the fastest convergence. The hybrid approach on 4 GPUs achieves a speed-up of 2.2 times compared to 1 GPU. More detailed results are shown in tab. 1. The convergence curves comparing the most interesting configurations are shown in fig. 1.\nIn general, not all configurations could be explored. For instance, on a single NVIDIA TITAN GPU with 6GB of RAM we are unable to fit mini-batches larger than 256 samples. On the other hand, we find mini-batches of 64 or fewer samples under-utilize the GPUs cores. This can be seen in tab. 1 which reports the timing of the data parallelism approach using 4 GPUs.\nThese preliminary results show promising speed-up factors by employing a hybrid parallelization strategy. In the future, we plan to extend this work to parallelization across servers by combining data and model parallelism with recent advances in asynchronous optimization methods and local learning algorithms."
    } ],
    "references" : [ {
      "title" : "ImageNet classification with deep convolutional neural networks",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey Hinton" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2012
    }, {
      "title" : "Imagenet: a large-scale hierarchical image database",
      "author" : [ "J. Deng", "W. Dong", "R. Socher", "L.J. Li", "K. Li", "L. Fei-Fei" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2009
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1998
    }, {
      "title" : "Deep neural networks for object detection",
      "author" : [ "C. Szegedy", "A. Toshev", "D. Erhan" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2013
    }, {
      "title" : "Learning hierarchical features for scene labeling",
      "author" : [ "C. Farabet", "L. Couprie", "C. amd Najman", "Y. LeCun" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "Applying convolutional neural networks concepts to hybrid nn-hmm model for speech recognition",
      "author" : [ "O. Abdel-Hamid", "A.R. Mohamed", "H. Jiang", "G. Penn" ],
      "venue" : "In ICASSP,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2012
    }, {
      "title" : "Improvements to deep convolutional neural networks for lvcsr",
      "author" : [ "T. Sainath", "A.R. Kingsbury", "Mohamed", "G. Dahl", "G. Saon", "H. Soltau", "T. Beran", "A. Aravkin", "B. Ramabhadran" ],
      "venue" : "In ASRU,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2013
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "Large scale distributed deep networks",
      "author" : [ "J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "Q. Le", "M. Mao", "M. Ranzato", "A. Senior", "P. Tucker", "K. Yang", "A. Ng" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2012
    }, {
      "title" : "Asynchronous stochastic gradient descent for dnn training",
      "author" : [ "S. Zhang", "C. Zhang", "Z. You", "R. Zheng", "B. Xu" ],
      "venue" : "In ICASSP,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2013
    }, {
      "title" : "Pipelined back-propagation for context-dependent deep neural networks",
      "author" : [ "X. Chen", "A. Eversole", "G. Li", "D. Yu", "F. Seide" ],
      "venue" : "In Interspeech,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2012
    }, {
      "title" : "Deep learning with cots hpc",
      "author" : [ "A. Coates", "B. Huval", "T. Wang", "D.J. Wu", "A.Y. Ng", "B. Catanzaro" ],
      "venue" : "In ICML,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Convolutional neural networks [3] have proven useful in many domains, including computer vision [1, 4, 5], audio processing [6, 7] and natural language processing [8].",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 0,
      "context" : "Convolutional neural networks [3] have proven useful in many domains, including computer vision [1, 4, 5], audio processing [6, 7] and natural language processing [8].",
      "startOffset" : 96,
      "endOffset" : 105
    }, {
      "referenceID" : 3,
      "context" : "Convolutional neural networks [3] have proven useful in many domains, including computer vision [1, 4, 5], audio processing [6, 7] and natural language processing [8].",
      "startOffset" : 96,
      "endOffset" : 105
    }, {
      "referenceID" : 4,
      "context" : "Convolutional neural networks [3] have proven useful in many domains, including computer vision [1, 4, 5], audio processing [6, 7] and natural language processing [8].",
      "startOffset" : 96,
      "endOffset" : 105
    }, {
      "referenceID" : 5,
      "context" : "Convolutional neural networks [3] have proven useful in many domains, including computer vision [1, 4, 5], audio processing [6, 7] and natural language processing [8].",
      "startOffset" : 124,
      "endOffset" : 130
    }, {
      "referenceID" : 6,
      "context" : "Convolutional neural networks [3] have proven useful in many domains, including computer vision [1, 4, 5], audio processing [6, 7] and natural language processing [8].",
      "startOffset" : 124,
      "endOffset" : 130
    }, {
      "referenceID" : 7,
      "context" : "Convolutional neural networks [3] have proven useful in many domains, including computer vision [1, 4, 5], audio processing [6, 7] and natural language processing [8].",
      "startOffset" : 163,
      "endOffset" : 166
    }, {
      "referenceID" : 0,
      "context" : "In this work, we consider a standard architecture [1] trained on the Imagenet dataset [2] for classification and investigate methods to speed convergence by parallelizing training across multiple GPUs.",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 1,
      "context" : "In this work, we consider a standard architecture [1] trained on the Imagenet dataset [2] for classification and investigate methods to speed convergence by parallelizing training across multiple GPUs.",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 8,
      "context" : "Unlike previous work [9, 10, 11], we do not aim to improve the underlying optimization algorithm.",
      "startOffset" : 21,
      "endOffset" : 32
    }, {
      "referenceID" : 9,
      "context" : "Unlike previous work [9, 10, 11], we do not aim to improve the underlying optimization algorithm.",
      "startOffset" : 21,
      "endOffset" : 32
    }, {
      "referenceID" : 10,
      "context" : "Unlike previous work [9, 10, 11], we do not aim to improve the underlying optimization algorithm.",
      "startOffset" : 21,
      "endOffset" : 32
    }, {
      "referenceID" : 8,
      "context" : "We consider two basic approaches: data and model parallelism [9].",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 0,
      "context" : "[1].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "Model parallelism, on the other hand, consists of splitting an individual network’s computation across multiple GPUs [9, 12].",
      "startOffset" : 117,
      "endOffset" : 124
    }, {
      "referenceID" : 11,
      "context" : "Model parallelism, on the other hand, consists of splitting an individual network’s computation across multiple GPUs [9, 12].",
      "startOffset" : 117,
      "endOffset" : 124
    }, {
      "referenceID" : 0,
      "context" : "[1] further customized the architecture of the network to better leverage model parallelism: the architecture consists of two “columns” each allocated on one GPU.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "[1] (with the only exception of the mini-batch size).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "The task is classification on the ImageNet 2012 datasset [2].",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 0,
      "context" : "[1], achieved a speed-up of 1.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "[1].",
      "startOffset" : 0,
      "endOffset" : 3
    } ],
    "year" : 2014,
    "abstractText" : "In this work, we consider a standard architecture [1] trained on the Imagenet dataset [2] for classification and investigate methods to speed convergence by parallelizing training across multiple GPUs. In this work, we used up to 4 NVIDIA TITAN GPUs with 6GB of RAM. While our experiments are performed on a single server, our GPUs have disjoint memory spaces, and just as in the distributed setting, communication overheads are an important consideration. Unlike previous work [9, 10, 11], we do not aim to improve the underlying optimization algorithm. Instead, we isolate the impact of parallelism, while using standard supervised back-propagation and synchronous mini-batch stochastic gradient descent.",
    "creator" : "LaTeX with hyperref package"
  }
}