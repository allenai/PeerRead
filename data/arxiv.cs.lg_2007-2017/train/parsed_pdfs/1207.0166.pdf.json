{
  "name" : "1207.0166.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On Multilabel Classification and Ranking with Partial Feedback, Ver. 3",
    "authors" : [ "Claudio Gentile" ],
    "emails" : [ "claudio.gentile@uninsubria.it", "francesco@orabona.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Consider a book recommendation system. Given a customer’s profile, the system recommends a few possible books to the user by means of, e.g., a limited number of banners placed at different positions on a webpage. The system’s goal is to select books that the user likes and possibly purchases. Typical feedback in such systems is the actual action of the user or, in particular, what books he has bought/preferred, if any. The system cannot observe what would have been the user’s actions had other books got recommended, or had the same book ads been placed in a different order within the webpage.\nSuch problems are collectively referred to as learning with partial feedback. As opposed to the full information case, where the system (the learning algorithm) knows the outcome of each possible response (e.g., the user’s action for each and every possible book recommendation placed in the largest banner ad), in the partial feedback setting, the system only observes the response to very limited options and, specifically, the option that was actually recommended.\nar X\niv :1\n20 7.\n01 66\nv3 [\ncs .L\nIn this and many other examples of this sort, it is reasonable to assume that recommended options are not given the same treatment by the system, e.g., large banners which are displayed on top of the page should somehow be more committing as a recommendation than smaller ones placed elsewhere. Moreover, it is often plausible to interpret the user feedback as a preference (if any) restricted to the displayed alternatives.\nIn this paper, we consider instantiations of this problem in the multilabel and learning-to-rank settings. Learning proceeds in rounds, in each time step t the algorithm receives an instance xt and outputs an ordered subset Ŷt of labels from a finite set of possible labels [K] = {1, 2, . . . , K}. Restrictions might apply to the size of Ŷt (due, e.g., to the number of available slots in the webpage). The set Ŷt corresponds to the aforementioned recommendations, and is intended to approximate the true set of preferences associated with xt. However, the latter set is never observed. In its stead, the algorithm receives Yt ∩ Ŷt, where Yt ⊆ [K] is a noisy version of the true set of user preferences on xt. When we are restricted to |Ŷt| = 1 for all t, this becomes a multiclass classification problem with bandit feedback – see below."
    }, {
      "heading" : "1.1 Related work",
      "text" : "This paper lies at the intersection between online learning with partial feedback and multilabel classification/ranking. Both fields include a substantial amount of work, so we can hardly do it justice here. In the sequel, we outline some of the main contributions in the two fields, with an emphasis on those we believe are the most related to this paper.\nA well-known tool for facing the problem of partial feedback in online learning is to trade off exploration and exploitation through upper confidence bounds. This technique has been introduced by [28], and can by now be considered a standard tool. In the so-called bandit setting with contextual information (sometimes called bandits with side information or bandits with covariates, e.g., [3, 12, 15, 11, 27], and references therein) an online algorithm receives at each time step a context (typically, in the form of a feature vector x) and is compelled to select an action (e.g., a label), whose goodness is quantified by a predefined loss function. Full information about the loss function (one that would perhaps allow to minimizes the total loss over the contexts seen so far) is not available. The specifics of the interaction model determines which pieces of loss will be observed by the algorithm, e.g., the actual value of the loss on the chosen action, some information on more profitable directions on the action space, noisy versions thereof, etc. The overall goal is to compete against classes of functions that map contexts to (expected) losses in a regret sense, that is, to obtain sublinear cumulative regret bounds.\nAll these algorithms share the common need to somehow trade off an exploratory attitude for gathering loss information on unchosen directions of the context-action space, and an exploitatory attitude for choosing actions that are deemed best according to the available data. For instance, [3, 12, 15, 1] work in a finite action space where the mappings context-to-loss for each action are linear (or generalized linear, as [15]’s) functions of the features. They all obtain T 1/2-like regret bounds, where T is the time horizon. This is extended by [27], where the loss function is modeled as a sample from a Gaussian process over the joint context-action space. We are using a similar (generalized) linear modeling here. An earlier (but somehow more general) setting that models such mappings by VC-classes is considered by [29], where a T 2/3 regret bound has been proven under i.i.d. assumptions. Linear multiclass classification problems with bandit feedback\nare considered by, e.g., [25, 11, 20], where either T 2/3 or T 1/2 or even logarithmic regret bounds are proven, depending on the noise model and the underlying loss functions.\nAll the above papers do not consider structured action spaces, where the learner is afforded to select sets of actions, which is more suitable to multilabel and ranking problems. Along these lines are [19, 37, 26, 35, 33, 2]. The general problem of online minimization of a submodular loss function under both full and bandit information without covariates is considered by [19], achieving a regret T 2/3 in the bandit case. [37] consider the problem of online learning of assignments, where at each round an algorithm is requested to assign positions (e.g., rankings) to sets of items (e.g., ads) with given constraints on the set of items that can be placed in each position. Their problem shares similar motivations as ours but, again, the bandit version of their algorithm does not explicitly take side information into account, and leads to a T 2/3 regret bound. Another paper with similar goals but a different mathematical model is by [26], where the aim is to learn a suitable ordering (an “ordered slate”) of the available actions. Among other things, the authors prove a T 1/2 regret bound in the bandit setting with a multiplicative weight updating scheme. Yet, no contextual information is incorporated. [35] motivate the ability of selecting sets of actions by a problem of diverse retrieval in large document collections which are meant to live in a general metric space. In contrast to our paper, that approach does not lead to strong regret guarantees for specific (e.g., smooth) loss functions. [33] use a simple linear model for the hidden utility function of users interacting with a web system and providing partial feedback in any form that allows the system to make significant progress in learning this function (this is called an α-informative feedback by the authors). Under these assumptions, a regret bound of T 1/2 is again provided that depends on the degree of informativeness of the feedback, as measured by the progress made during the learning process. It is experimentally argued that this feedback is typically made available by a user that clicks on relevant URLs out of a list presented by a search engine. Despite the neatness of the argument, no formal effort is put into relating this information to the context information at hand or, more generally, to the way data are generated. The recent paper [2] investigates classes of graphical models for contextual bandit settings that afford richer interaction between contexts and actions leading again to a T 2/3 regret bound.\nFinally, a very interesting recent work that came to our attention at the time of writing this extended version of our conference paper [18] is [5]. In that paper, the authors provide sufficient conditions that insure rates of the form T 1/2 in partial monitoring games with side information. Partial monitoring is an attempt to formalize through a unifying language the partial information settings where the algorithm is observing only partial information about the loss of its action, in the form of some kind of feedback or “signal”. The results presented by [5] do not seem to conveniently extend to the structured action space setting we are interested in (or, if they do, we do not see it in the current version of their paper). Moreover, being very general in scope, that paper is missing a tight dependence of the regret bound on the number of available actions, which can be very large in structured action spaces.\nThe literature on multilabel learning and learning to rank is overwhelming. The wide attention this literature attracts is often motivated by its web-search-engine or recommender-system applications, and many of the papers are experimental in nature. Relevant references include [38, 17, 14], along with references therein. Moreover, when dealing with multilabel, the typical assumption is full supervision, an important concern being modeling correlations among classes. In contrast to that, the specific setting we are considering here need not face such a modeling [14]. The more\nrecent work [39] reduces any online algorithm working on pairwise loss functions (like a ranking loss) to a batch algorithm with generalization bound guarantees. But, again, only fully supervised settings are considered. Other related references are [22, 16], where learning is by pairs of examples. Yet, these approaches need i.i.d. assumptions on the data, and typically deliver batch learning procedures.\nTo summarize, whereas we are technically closer to the linear modeling approaches by [3, 12, 13, 11, 15, 1, 27, 5], from a motivational standpoint we are perhaps closest to [37, 26, 33]."
    }, {
      "heading" : "1.2 Our results",
      "text" : "We investigate the multilabel and learning-to-rank problems in a partial feedback scenario with contextual information, where we assume a probabilistic linear model over the labels, although the contexts can be chosen by an adaptive adversary. We consider two families of loss functions, one is a cost-sensitive multilabel loss that generalizes the standard Hamming loss in several respects, the other is a kind of (unnormalized) ranking loss. In both cases, the learning algorithm is maintaining a (generalized) linear predictor for the probability that a given label occurs, the ranking being produced by upper confidence-corrected estimated probabilities. In such settings, we prove T 1/2 log T cumulative regret bounds, which are essentially optimal (up to log factors) in some cases. A distinguishing feature of our user feedback model is that, unlike previous papers (e.g., [19, 37, 1, 27]), we are not assuming the algorithm is observing a noisy version of the risk function on the currently selected action. In fact, when a generalized linear model is adopted, the mapping context-to-risk turns out to be nonconvex in the parameter space. Furthermore, when operating on structured action spaces this more traditional form of bandit model does not seem appropriate to capture the typical user preference feedback. Our approach is based on having the loss decouple from the label generating model, the user feedback being a noisy version of the gradient of a surrogate convex loss associated with the model itself. As a consequence, the algorithm is not directly dealing with the original loss when making exploration. In this sense, we are more similar to the multiclass bandit algorithm by [11]. Yet, our work is a substantial departure from [11]’s in that we lift their machinery to nontrivial structured action spaces, and we do so by means of generalized linear models. On one hand, these extensions pose several extra technical challenges; on the other, they provide additional modeling power and practical advantage.\nThough the emphasis is on theoretical results, we also validate our algorithms on two realworld multilabel datasets w.r.t. a number of loss functions, showing good comparative performance against simple multilabel/ranking baselines that operate with full information."
    }, {
      "heading" : "1.3 Structure of the paper",
      "text" : "The paper is organized as follows. In Section 2 we introduce our learning model, our first loss function, the label generation model, and some preliminary results and notation used throughout the rest of the paper. In Section 3 we describe our partial feedback algorithm working under the loss function introduced in Section 2, along with the associated regret analysis. In Section 4 we show that a very similar machinery applies to ranking with partial feedback, where the loss function is a kind of pairwise ranking loss (with partial feedback). Similar regret bounds are then presented that work under additional modeling restrictions. In Section 5 we provide our experimental evidence\ncomparing our method with its immediate full information counterpart. Section 6 gives proof ideas and technical details. The paper is concluded with Section 7, where possible directions for future research are mentioned."
    }, {
      "heading" : "2 Model and preliminaries",
      "text" : "We consider a setting where the algorithm receives at time t the side information vector xt ∈ Rd, is allowed to output a (possibly ordered) subset1 Ŷt ⊆ [K] of the set of possible labels, then the subset of labels Yt ⊆ [K] associated with xt is generated, and the algorithm gets as feedback Ŷt∩Yt. The loss suffered by the algorithm may take into account several things: the distance between Yt and Ŷt (both viewed as sets), as well as the cost for playing Ŷt. The cost c(Ŷt) associated with Ŷt might be given by the sum of costs suffered on each class i ∈ Ŷt, where we possibly take into account the order in which i occurs within Ŷt (viewed as an ordered list of labels). Specifically, given constant a ∈ [0, 1] and costs c = {c(i, s), i = 1, . . . , s, s ∈ [K]}, such that 1 ≥ c(1, s) ≥ c(2, s) ≥ . . . c(s, s) ≥ 0, for all s ∈ [K], we consider the loss function\n`a,c(Yt, Ŷt) = a |Yt \\ Ŷt|+ (1− a) ∑\ni∈Ŷt\\Yt\nc(ji, |Ŷt|),\nwhere ji is the position of class i in Ŷt, and c(ji, ·) depends on Ŷt only through its size |Ŷt|. In the above, the first term accounts for the false negative mistakes, hence there is no specific ordering of labels therein. The second term collects the loss contribution provided by all false positive classes, taking into account through the costs c(ji, |Ŷt|) the order in which labels occur in Ŷt. The constant a serves as weighting the relative importance of false positive vs. false negative mistakes2. As a specific example, suppose that K = 10, the costs c(i, s) are given by c(i, s) = (s − i + 1)/s, i = 1, . . . , s, the algorithm plays Ŷt = (4, 3, 6), but Yt is {1, 3, 8}. In this case, |Yt \\ Ŷt| = 2, and∑\ni∈Ŷt\\Yt c(ji, |Ŷt|) = 3/3 + 1/3, i.e., the cost for mistakingly playing class 4 in the top slot of Ŷt is more damaging than mistakingly playing class 6 in the third slot. In the special case when all costs are unitary, there is no longer need to view Ŷt as an ordered collection, and the above loss reduces to a standard Hamming-like loss between sets Yt and Ŷt, i.e., a |Yt \\ Ŷt|+ (1− a) |Ŷt \\ Yt|. Notice that the partial feedback Ŷt∩Yt allows the algorithm to know which of the chosen classes in Ŷt are good or bad (and to what extent, because of the selected ordering within Ŷt). Yet, the algorithm does not observe the value of `a,c(Yt, Ŷt) bacause Yt \\ Ŷt remains hidden.\nThe reader should also observe the asymmetry between the label set Ŷt produced by the algorithm and the true label set Yt: The algorithm predicts an ordered set of labels, but the true set of labels is unordered. In fact, it is often the case in, e.g., recommender system practice, that the user feedback does not contain preference information in the form of an ordered set of items. Still, in such systems we would like to get back to the user with an appropriate ranking over the items.\nWorking with the above loss function makes the algorithm’s output Ŷt become a ranked list of classes, where ranking is restricted to the deemed relevant classes only. In this sense, the above problem can be seen as a partial information version of the multilabel ranking problem (see [17],\n1 An ordered subset is like a list with no repeated items. 2Notice that a is not redundant here, since the costs c(i, s) have been normalized to [0,1].\nand references therein). In a standard multilabel ranking problem a classifier has to provide for any given instance xt, both a separation between relevant and irrelevant classes and a ranking of the classes within the two sets (or, perhaps, over the whole set of classes, as long as ranking is consistent with the relevance separation). In our setting, instead, ranking applies to the selected classes only, but the information gathered by the algorithm while training is partial. That is, only a relevance feedback among the selected classes is observed (the set Yt ∩ Ŷt), but no supervised ranking information (e.g., in the form of pairwise preferences) is provided to the algorithm within this set. Alternatively, we can think of a ranking framework where restrictions on the size of Ŷt are set by an exogenous (and possibly time-varying) parameter of the problem, and the algorithm is required to provide a ranking complying with these restrictions.\nAnother important concern we would like to address with our loss function `a,c is to avoid combinatorial explosions due to the exponential number of possible choices for Ŷt. As we shall see below, this is guaranteed by the chosen structure for costs c(i, s). Another loss function providing similar guarantees (though with additional modeling restrictions) is the (pairwise) ranking loss considered in Section 4, where more on the connection to the ranking setting with partial feedback is given.\nThe problem arises as to which noise model we should adopt so as to encompass significant real-world settings while at the same time affording efficient implementation of the resulting algorithms. For any subset Yt ⊆ [K], we let (y1,t, . . . , yK,t) ∈ {0, 1}K be the corresponding indicator vector. Then it is easy to see that\n`a,c(Yt, Ŷt) = a ∑ i/∈Ŷt yi,t + (1− a) ∑ i∈Ŷt c(ji, |Ŷt|) (1− yi,t)\n= a K∑ i=1 yi,t + (1− a) ∑ i∈Ŷt ( c(ji, |Ŷt|)− ( a 1−a + c(ji, |Ŷt|) ) yi,t ) .\nMoreover, because the first sum does not depend on Ŷt, for the sake of optimizing over Ŷt (but also for the sake of defining the regret RT – see below) we can equivalently define\n`a,c(Yt, Ŷt) = (1− a) ∑ i∈Ŷt ( c(ji, |Ŷt|)− ( a 1−a + c(ji, |Ŷt|) ) yi,t ) . (1)\nLet Pt(·) be a shorthand for the conditional probability Pt(· |xt), where the side information vector xt can in principle be generated by an adaptive adversary as a function of the past. Then\nPt(y1,t, . . . , yK,t) = P(y1,t, . . . , yK,t |xt),\nwhere the marginals Pt(yi,t = 1) satisfy3\nPt(yi,t = 1) = g(−u>i xt)\ng(u>i xt) + g(−u>i xt) , i = 1, . . . , K, (2)\n3 The reader familiar with generalized linear models will recognize the derivative of the function p(∆) = g(−∆)\ng(∆)+g(−∆) as the (inverse) link function of the associated canonical exponential family of distributions [30].\nfor some K vectors u1, . . . ,uK ∈ Rd and some (known) function g : D ⊆ R → R+. The model is well defined if u>i x ∈ D for all i and all x ∈ Rd chosen by the adversary. We assume for the sake of simplicity that ||xt|| = 1 for all t. Notice that here the variables yi,t need not be conditionally independent. We are only definining a family of allowed joint distributions Pt(y1,t, . . . , yK,t) through the properties of their marginals Pt(yi,t). A classical result in the theory of copulas [34] makes one derive all allowed joint distributions starting from the corresponding one-dimensional marginals.\nThe function g above will be instantiated to the negative derivative of a suitable convex and nonincreasing loss function L which our algorithm will be based upon. For instance, if L is the square loss L(∆) = (1 −∆)2/2, then g(∆) = 1 −∆, resulting in Pt(yi,t = 1) = (1 + u>i xt)/2, under the assumptionD = [−1, 1]. If L is the logistic loss L(∆) = ln(1+e−∆), then g(∆) = 1\ne∆+1 ,\nand Pt(yi,t = 1) = eu > i xt/(eu > i xt +1), with domainD = R. Observe that in both cases Pt(yi,t = 1) is an increasing function of u>i xt. This will be true in general. Set for brevity ∆i,t = u>i xt. Taking into account (1), this model allows us to write the (conditional) expected loss of the algorithm playing Ŷt as\nEt[`a,c(Yt, Ŷt)] = (1− a) ∑ i∈Ŷt ( c(ji, |Ŷt|)− ( a 1−a + c(ji, |Ŷt|) ) pi,t ) , (3)\nwhere we introduced the shorthands\npi,t = p(∆i,t), p(∆) = g(−∆)\ng(∆) + g(−∆) ,\nand the expectation Et in (3) is w.r.t. the generation of labels Yt, conditioned on both xt, and all previous x and Y .\nA key aspect of this formalization is that the Bayes optimal ordered subset\nY ∗t = argminY=(j1,j2,...,j|Y |)⊆[K]Et[`a,c(Yt, Y )]\ncan be computed efficiently when knowing ∆1,t, . . . ,∆K,t. This is handled by the next lemma. In words, this lemma says that, in order to minimize (3), it suffices to try out all possible sizes s = 0, 1, . . . , K for Y ∗t and, for each such value, determine the sequence Y ∗ s,t that minimizes (3) over all sequences of size s. In turn, Y ∗s,t can be computed just by sorting classes i ∈ [K] in decreasing order of pi,t, sequence Y ∗s,t being given by the first s classes in this sorted list.\nLemma 1. With the notation introduced so far, let pi1,t ≥ pi2,t ≥ . . . piK ,t be the sequence of pi,t sorted in nonincreasing order. Then we have that\nY ∗t = argmins=0,1,...KEt[`a,c(Yt, Y ∗s,t)] ,\nwhere Y ∗s,t = (i1, i2, . . . , is), and Y ∗ 0,t = ∅.\nProof: First observe that, for any given size s, the sequence Y ∗s,t must contain the s top-ranked classes in the sorted order of pi,t. This is because, for any candidate sequence Ys = {j1, j2, . . . , js}, we have Et[`a,c(Y ∗t , Ys)] = (1− a) ∑ i∈Ys ( c(ji, s)− ( a 1−a + c(ji, s) ) pi,t ) . If there exists i ∈ Ys which is not among the s-top ranked ones, then we could replace class i in position ji within Ys with class k /∈ Ys such that pk,t > pi,t obtaining a smaller loss.\nNext, we show that the optimal ordering within Y ∗s,t is precisely ruled by the nonicreasing order of pi,t. By the sake of contradiction, assume there are i and k in Y ∗s,t such that i preceeds k in Y ∗ s,t but pk,t > pi,t. Specifically, let i be in position j1 and k be in position j2 with j1 < j2 and such that c(j1, s) > c(j2, s). Then, disregarding the common (1 − a)-factor, switching the two classes within Y ∗s,t yields an expected loss difference of\nc(j1, s)− ( a 1−a + c(j1, s) ) pi,t + c(j2, s)− ( a 1−a + c(j2, s) ) pk,t\n− ( c(j1, s)− ( a 1−a + c(j1, s) ) pk,t ) − ( c(j2, s)− ( a 1−a + c(j2, s) ) pi,t ) = (pk,t − pi,t) (c(j1, s)− c(j2, s)) > 0 ,\nsince pk,t > pi,t and c(j1, s) > c(j2, s). Hence switching would get a smaller loss which leads as a consequence to Y ∗s,t = (i1, i2, . . . , is).\nNotice the way costs c(i, s) influence the Bayes optimal computation. We see from (3) that placing class i within Ŷt in position ji is beneficial (i.e., it leads to a reduction of loss) if and only if pi,t > c(ji, |Ŷt|)/( a1−a + c(ji, |Ŷt|)). Hence, the higher is the slot ij in Ŷt the larger should be pi,t in order for this inclusion to be convenient.4\nIt is Y ∗t above that we interpret as the true set of user preferences on xt. We would like to compete against Y ∗t in a cumulative regret sense, i.e., we would like to bound\nRT = T∑ t=1 Et[`a,c(Yt, Ŷt)]− Et[`a,c(Yt, Y ∗t )]\nwith high probability. We use a similar but largely more general analysis than [11]’s to devise an online second-order descent algorithm whose updating rule makes the comparison vector U = (u1, . . . ,uK) ∈ RdK defined through (2) be Bayes optimal w.r.t. a surrogate convex loss L(·) such that g(∆) = −L′(∆). Observe that the expected loss function defined in (3) is, generally speaking, nonconvex in the margins ∆i,t (consider, for instance the logistic case g(∆) = 1e∆+1 ). Thus, we cannot directly minimize this expected loss."
    }, {
      "heading" : "3 Algorithm and regret bounds",
      "text" : "In Figure 1 is our bandit algorithm for (ordered) multiple labels. The algorithm is based on replacing the unknown model vectors u1, . . . ,uK with prototype vectors w′1,t, . . . ,w ′ K,t, being w ′ i,t the time-t approximation to ui, satisying similar constraints we set for the ui vectors. For the sake of brevity, we let ∆̂′i,t = x > t w ′ i,t, and ∆i,t = u > i xt, i ∈ [K].\n4 Notice that this depends on the actual size of Ŷt, so we cannot decompose this problem into K independent problems. The decomposition does occur if the costs c(i, s) are constants, independent of i and s, the criterion for inclusion becoming pi,t ≥ θ, for some constant threshold θ.\nThe algorithm uses ∆̂′i,t as proxies for the underlying ∆i,t according to the (upper confidence)\napproximation scheme ∆i,t ≈ [∆̂′i,t + i,t]D, where i,t ≥ 0 is a suitable upper-confidence level for class i at time t, and [·]D denotes the clipping-to-D operation: If D = [−R,R], then\n[x]D =  R if x > R x if −R ≤ x ≤ R −R if x < −R .\nThe algorithm’s prediction at time t has the same form as the computation of the Bayes optimal sequence Y ∗t , where we replace the true (and unknown) pi,t = p(∆i,t) with the corresponding upper confidence proxy\np̂i,t = p([∆̂ ′ i,t + i,t]D) .\nbeing\nŶt = argminY=(j1,j2,...j|Y |)⊆[K] (∑ i∈Y ( c(ji, |Y |)− ( a 1−a + c(ji, |Y |) ) p̂i,t )) .\nComputing Ŷt above can be done by mimicking the computation of the Bayes optimal ordered subset Y ∗t (just replace pi,t by p̂i,t). From a computational viewpoint, this essentially amounts to sorting classes i ∈ [K] in decreasing value of p̂i,t, i.e., order of K logK running time per prediction. Thus the algorithm is producing a ranked list of relevant classes based on upper-confidence-corrected scores p̂i,t. Class i is deemed relevant and ranked high among the relevant ones when either ∆̂′i,t is a good approximation to ∆i,t and pi,t is large, or when the algorithm is not very confident on its own approximation about i (that is, the upper confidence level i,t is large).\nThe algorithm in Figure 1 receives in input the loss parameters a and c(i, s), the model function g(·) and the associated margin domain D = [−R,R], and maintains both K positive definite matrices Ai,t of dimension d (initially set to the d × d identity matrix), and K weight vector wi,t ∈ Rd (initially set to the zero vector). At each time step t, upon receiving the d-dimensional instance vector xt the algorithm uses the weight vectors wi,t to compute the prediction vectors w′i,t. These vectors can easily be seen as the result of projecting wi,t onto interval D = [−R,R] w.r.t. the distance function di,t−1, i.e.,\nw′i,t = argminw∈Rd :w>xt∈D di,t−1(w,wi,t), i ∈ [K],\nwhere di,t(u,w) = (u−w)>Ai,t (u−w) .\nVectors w′i,t are then used to produce prediction values ∆̂ ′ i,t involved in the upper-confidence calculation of the predicted ordered subset Ŷt ⊆ [K]. Next, the feedback Yt ∩ Ŷt is observed, and the algorithm in Figure 1 promotes all classes i ∈ Yt∩ Ŷt (sign si,t = 1), demotes all classes i ∈ Ŷt \\Yt (sign si,t = −1), and leaves all remaining classes i /∈ Ŷt unchanged (sign si,t = 0). Promotion of class i on xt implies that if the new vector xt+1 is close to xt then i will be ranked higher on xt+1. The update w′i,t → wi,t+1 is based on the gradients ∇i,t of a loss function L(·) satisfying L′(∆) = −g(∆). On the other hand, the update Ai,t−1 → Ai,t uses the rank one matrix5 xtx>t .\n5 The rank-one update is based on xtx>t rather than∇i,t∇>i,t, as in, e.g., [21]. This is due to technical reasons that will be made clear in Section 6. This feature tells this algorithm slightly apart from the Online Newton step algorithm [21], which is the starting point of our analysis.\nIn both the update of w′i,t and the one involving Ai,t−1, the reader should observe the role played by the signs si,t. Finally, the constants c′L and c ′′ L occurring in the expression for 2 i,t are related to smoothness properties of L(·), as explained in the next theorem.6\nTheorem 2. Let L : D = [−R,R] ⊆ R → R+ be a C2(D) convex and nonincreasing function of its argument, (u1, . . . ,uK) ∈ RdK be defined in (2) with g(∆) = −L′(∆) for all ∆ ∈ D, and such that ‖ui‖ ≤ U for all i ∈ [K]. Assume there are positive constants cL, c′L and c′′L such that:\ni. L ′(∆)L′′(−∆)+L′′(∆)L′(−∆)\n(L′(∆)+L′(−∆))2 ≥ −cL,\nii. (L′(∆))2 ≤ c′L,\niii. L′′(∆) ≥ c′′L simultaneously hold for all ∆ ∈ D. Then the cumulative regret RT of the algorithm in Figure 1 satisfies, with probability at least 1− δ,\nRT = O ( (1− a) cLK √ T C d ln ( 1 + T\nd\n)) ,\nwhere\nC = O ( U2 +\nd c′L (c′′L) 2 ln\n( 1 + T\nd\n) + ( c′L\n(c′′L) 2 + L(−R) c′′L\n) ln KT\nδ\n) .\nIt is easy to see that when L(·) is the square loss L(∆) = (1 − ∆)2/2 and D = [−1, 1], we have cL = 1/2, c′L = 4 and c ′′ L = 1; when L(·) is the logistic loss L(∆) = ln(1 + e−∆) and D = [−R,R], we have cL = 1/4, c′L ≤ 1 and c′′L = 12(1+cosh(R)) , where cosh(x) = ex+e−x 2 .\nThe following remarks are in order at this point.\nRemark 1. A drawback of Theorem 2 is that, in order to properly set the upper confidence levels i,t, we assume prior knowledge of the norm upper bound U . Because this information is often unavailable, we present here a simple modification to the algorithm that copes with this limitation. We change the definition of 2i,t in Figure 1 to\n2i,t = max { x>A−1i,t−1x ( 2 d c′L (c′′L) 2 ln ( 1 + t− 1 d ) + 12 c′′L ( c′L c′′L + 3L(−R) ) ln K(t+ 4) δ ) , 4R2 } .\nthat is, we substitute U2 by d c ′ L\n(c′′L) 2 ln\n( 1 + t−1\nd\n) , and cap the maximal value of 2i,t to 4R\n2. This immediately leads to the following result.7\nTheorem 3. With the same assumptions and notation as in Theorem 2, if we replace 2i,t as explained above we have that, with probability at least 1− δ, RT satisfies\nRT = O ( (1− a) cLK √ T C d ln ( 1 + T\nd\n) + (1− a) cLK Rd ( exp ( (c′′L) 2 U2\nc′L d\n) − 1 )) .\n6 The proof is given in Section 6. 7 The proof is deferred to Section 6.\nRemark 2. From a computational standpoint, the most demanding operation in Figure 1 is computing the upper confidence levels i,t involving the inverse matrices A−1i,t−1, i ∈ [K]. This can be done incrementally inO(K d2) time per round, which makes it hardly practical if both d andK are large. In practice (as explained, e.g., by [11]), one can use a version of the algorithm which maintains diagonal matrices Ai,t instead of full ones. All the steps remain the same except Step 5 of Algorithm 1 where one defines the rth diagonal element of matrix Ai,t as (Ai,t)r,r = (Ai,t−1)r,r +x2r,t, being xt = (x1,t, x2,t, . . . , xr,t, . . . , xK,t)>. The resulting running time per round (including prediction and update) becomes O(dK + K logK). In fact, when a limitation on the size of Ŷt is given, the running time may be further reduced, see Remark 3."
    }, {
      "heading" : "4 On ranking with partial feedback",
      "text" : "As Lemma 1 points out, when the cost values c(i, s) in the loss function `a,c are stricly decreasing i.e., c(1, s) > c(2, s) > . . . > c(s, s), for all s ∈ [K], then the Bayes optimal ordered sequence Y ∗t on xt is unique can be obtained by sorting classes in decreasing values of pi,t, and then decide on a cutoff point8 induced by the loss parameters, so as to tell relevant classes apart from irrelevant ones. In turn, because p(∆) = g(−∆)\ng(∆)+g(−∆) is increasing in ∆, this ordering corresponds to sorting classes in decreasing values of ∆i,t. Now, if parameter a in `a,c is very close9 to 1, then |Y ∗t | = K, and the algorithm itself will produce ordered subsets Ŷt such that |Ŷt| = K. Moreover, it does so by receiving full feedback on the relevant classes at time t (since Yt ∩ Ŷt = Yt). As is customary (e.g., [14]), one can view any multilabel assignment Y = (y1, . . . , yK) ∈ {0, 1}K as a ranking among the K classes in the most natural way: i preceeds j if and only if yi > yj . The (unnormalized) ranking loss function `rank(Y, f) between the multilabel Y and a ranking function f : Rd → RK , representing degrees of class relevance sorted in a decreasing order fj1(xt) ≥ fj2(xt) ≥ . . . ≥ fjK (xt) ≥ 0, counts the number of class pairs that disagree in the two rankings:\n`rank(Y, f) = ∑\ni,j∈[K] : yi>yj\n( {fi(xt) < fj(xt)}+ 12 {fi(xt) = fj(xt)} ) ,\nwhere {. . .} is the indicator function of the predicate at argument. As pointed out by [14], the ranking function f(xt) = (p1,t, . . . , pK,t) is also Bayes optimal w.r.t. `rank(Y, f), no matter if the class labels yi are conditionally independent or not. Hence we can use the algorithm in Figure 1 with a close to 1 for tackling ranking problems derived from multilabel ones, when the measure of choice is `rank and the feedback is full.\nWe now consider a partial information version of the above ranking problem. Suppose that at each time t, the environment discloses both xt and a maximal size St for the ordered subset Ŷt = (j1, j2, . . . , j|Ŷt|) (both xt and St can be chosen adaptively by an adversary). Here St might be the number of available slots in a webpage or the number of URLs returned by a search engine in response to query xt. Then it is plausible to compete in a regret sense against the best time-t offline ranking of the form\nf ∗(xt) = f ∗(xt;St) = (f ∗ 1 (xt), f ∗ 2 (xt), . . . , f ∗ K(xt)),\n8 This is called the zero point by [17]. 9 If a = 1, the algorithm only cares about false negative mistakes, the best strategy being always predicting\nŶt = [K]. Unsurprisingly, this yields zero regret in both Theorems 2 and 3.\nwhere the number of strictly positive f ∗i (xt) values is at most St. Further, the ranking loss could be reasonably restricted to count the number of class pairs disagreeing within Ŷt plus a quantity related to the number of false negative mistakes. If Ŷt is the sequence of length St associated with ranking function f , we consider the loss function `p−rank,t (“partial information `rank at time t”)\n`p−rank,t(Y, f) = ∑\ni,j∈Ŷt : yi>yj\n( {fi(xt) < fj(xt)}+ 12 {fi(xt) = fj(xt)} ) + St |Yt \\ Ŷt| .\nIn this loss function, the factor St multiplying |Yt \\ Ŷt| serves as balancing the contribution of the double sum ∑ i,j∈Ŷt : yi>yj with the contribution of false negative mistakes |Yt\\Ŷt|. For convenience, we will interchangeably use the notations `p−rank,t(Y, f) and `p−rank,t(Y, Ŷt), whenever it is clear from the surrounding context that Ŷt is the sequence corresponding to f .\nThe next lemma10 is the ranking counterpart to Lemma 1. It shows that the Bayes optimal ranking for `p−rank,t is given by\nf ∗(xt;St) = (p ′ 1,t, p ′ 2,t, . . . , p ′ K,t),\nwhere p′j,t = pj,t if pj,t is among the St largest values in the sequence (p1,t, . . . , pK,t), and 0 otherwise. That is, f ∗(xt;St) is the function that ranks classes according to decreasing values of pi,t and cuts off exactly at position St. In order for this result to go through we need to restrict model (2) to the case of conditionally independent classes, i.e., to the case when\nPt(y1,t, . . . , yK,t) = ∏ i∈[K] pi,t . (4)\nThis is in striking contrast to the full information setting, where the Bayes optimal ranking only depends on the marginal distribution values pi,t [14]. Due to the interaction between the two terms in the definition of `p−rank,t, the Bayes optimal ranking for `p−rank,t turns out to depend on both marginal and pairwise correlation values of the joint class distribution. This would force us to maintain O(K2) upper confidence values i,j , one for each pair (i, j), i < j, leading to an extra computational burder which can also become prohibitive when the number of classes K is large.\nLemma 4. With the notation introduced so far, let the joint distribution Pt(y1,t, . . . , yK,t) factorize as in (4). Then f ∗(xt;St) introduced above satisfies\nf ∗(xt;St) = argminY=(i1,i2,...ih) ,h≤StEt[`p−rank,t(Yt, Y )] .\nIf we add to the argmin of our algorithm (Step 3 in Figure 1) the further constraint |Y | ≤ St (notice that the resulting computation is still about sorting classes according to decreasing values of p̂i,t), we are defining a partial information ranking algorithm that ranks classes according to decreasing values of p̂i,t up to position St (i.e., |Ŷt| = St). Let f̂(xt, St) be the resulting ranking. We can then define the cumulative regret RT w.r.t. `p−rank,t as\nRT = T∑ t=1 Et[`p−rank,t(Yt, f̂(xt, St))]− Et[`p−rank,t(Yt, f ∗(xt, St)], (5)\n10 We postpone the lengthy proof to Section 6.\nthat is, the amount to which the conditional `p−rank,t-risk of f̂(xt, St) exceeds the one of the Bayes optimal ranking f ∗(xt;St), cumulated over time.\nWe have the following ranking counterpart to Theorem 2.\nTheorem 5. With the same assumptions and notation as in Theorem 2, combined with the independence assumption (4), let the cumulative regret RT w.r.t. `p−rank,t be defined as in (5). Then, with probability at least 1− δ, we have that the algorithm in Figure 1 working with a→ 1 and strictly decreasing cost values c(i, s) (i.e., the one computing in round t the ranking function f̂(xt, St)) achieves\nRT = O ( cL √ S K T C d ln ( 1 + T\nd\n)) ,\nwhere S = maxt=1,...,T St.\nThe proof (see Section 6) is very similar to the one of Theorem 2. This suggests that, to some extent, we are decoupling the label generating model from the loss function ` under consideration.\nRemark 3. As is typical in many multilabel classification settings, the number of classes K can either be very large or have an inner structure (e.g., a hierarchical or DAG-like structure). It is often the case that in such a large label space, many classes are relatively rare. This has lead researchers to consider methods that are specifically taylored to leverage the label sparsity of the chosen classifier (e.g., [23] and references therein) and/or the specific structure of the set of labels (e.g., [9, 6], and references therein). Though our algorithm is not designed to exploit the label structure, we would like to stress that the restriction |Ŷt| ≤ St ≤ S in Theorem 5 allows us to replace the linear dependence on the total number of classes K (which is often much larger than S) by √ SK. It is very easy to see that this restriction would bring similar benefits to Theorem 2.\nThe above restriction is not only beneficial from a “statistical” point of view, but also from a computational one. In fact, as is by now standard, algorithms like the one in Figure 1 can easily be cast in dual variables (i.e., in a RKHS). This comes with at least two consequences:\n1. We can depart from the (generalized) linear modeling assumption (2), and allow for more general nonlinear dependences of pi,t on the input vectors xt.\n2. We can maintain a dual variable representation for margins ∆̂′i,t and quadratic forms x > t A −1 i,t−1xt,\nso that computing each one of them takesO(N2i,t−1) inner products, whereNi,t is the number of times class i has been updated up to time t, each inner product being O(d). Now, each of the (at most St ≤ S) updates is O(N2i,t−1). Hence, the overall running time in round t is coarsely overapproximated by O(d ∑ i∈[K] N 2 i,T + K logK). From ∑ i∈[K] Ni,T ≤ ST , we see that when S is small compared to K, then Ni,t−1 tends to be small as well. For instance, if S ≤ √ K this leads to a running time per round of the form SdT 2, which can be smaller than Kd2 mentioned in Remark 2.\nFinally, observe that one can also combine Theorem 5 with the argument contained in Remark 1."
    }, {
      "heading" : "5 Experiments",
      "text" : "The experiments we report here are meant to validate the exploration-exploitation tradeoff implemented by our algorithm under different conditions (restricted vs. nonrestricted number of classes),\nloss measures (`a,c, `rank,t, and Hamming loss) and model/parameter settings (L = square loss, L = logistic loss, with varying R). Datasets. We used two multilabel datasets. The first one, called Mediamill, was introduced in a video annotation challenge [36]. It comprises 30,993 training samples and 12,914 test ones. The number of features d is 120, and the number of classes K is 101. The second dataset is Sony CSL Paris [31], made up of 16,452 train samples and 16,519 test samples, each sample being described by d = 98 features. The number of classes K is 632. In both cases, feature vectors have been normalized to unit L2 norm. Parameter setting and loss measures. We used the algorithm in Figure 1 with two different loss functions, the square loss and the logistic loss, and varied the parameter R for the latter. The setting of the cost function c(i, s) depends on the task at hand, and for this preliminary experiments we decided to evaluate two possible settings only. The first one, denoted by “decreasing c” is c(i, s) = s−i+1\ns , i = 1, . . . , s, the second one, denoted by “constant c”, is c(i, s) = 1, for all i and\ns. In all experiments, the a parameter was set to 0.5, so that `a,c with constant c reduces to half the Hamming loss. In the decreasing c scenario, we evaluated the performance of the algorithm on the loss `a,c that the algorithm is minimizing, but also its ability to produce meaningful (partial) rankings through `rank,t. On the constant c setting, we evaluated the Hamming loss. As is typical of multilabel problems, the label density, i.e., the average fraction of labels associated with the examples, is quite small. For instance, on Mediamill this is 4.3%. Hence, it is clearly beneficial to impose an upper bound S on |Ŷt|. For the constant c and ranking loss experiments we tried out different values of S, and reported the final performance. Baseline. As baseline, we considered a full information version of Algorithm 1 using the square loss, that receives after each prediction the full array of true labels Yt for each sample. We call this algorithm OBR (Online Binary Relevance), because it is a natural online adaptation of the binary relevance algorithm, widely used as a baseline in the multilabel literature. Comparing to OBR stresses the effectiveness of the exploration/exploitation rule above and beyond the details of underlying generalized linear predictor. OBR was used to produce subsets (as in the Hamming loss case), and restricted rankings (as in the case of `rank,t). Results. Our results are summarized in Figures 2 and 3. The algorithms have been trained by sweeping only once over the training data. Though preliminary in nature, these experiments allow us to draw a few conclusions. Our results for the avarage `a,c(Yt, Ŷt) with decreasing c are contained in the two left plots. We can see that the performance is improving over time on both datasets, as predicted by Theorem 2. In the middle plots are the final cumulative Hamming losses with constant c divided by the number of training samples, as a function of S. Similar plots are on the right with the final average ranking losses `rank,t divided by S. In both cases we see that there is an optimal value of S that allows to balance the exploration and the exploitation of the algorithm. Moreover the performance of our algorithm is always pretty close to the performance of OBR, even if our algorithm is receiving only partial feedback. In many experiments the square loss seems to give better results. Exception is the ranking loss on the Mediamill dataset (Figure 3, right)."
    }, {
      "heading" : "6 Technical details",
      "text" : "This section contains all proofs missing from the main text, along with ancillary results and comments.\nThe algorithm in Figure 1 works by updating through the gradients ∇i,t of a modular marginbased loss function ∑K i=1 L(w > i x) associated with the label generation model (2), i.e., associated with function g, so as to make the parameters (u1, . . . ,uK) ∈ RdK therein achieve the Bayes optimality condition\n(u1, . . . ,uK) = arg min w1,...,wK :w > i xt∈D\nEt [ K∑ i=1 L(si,tw > i xt) ] , (6)\nwhere Et[·] above is over the generation of Yt in producing the sign value si,t ∈ {−1, 0,+1}, conditioned on the past (in particular, conditioned on Ŷt). The requirement in (6) is akin to the classical construction of proper scoring rules in the statistical literature (e.g., [32]).\nThe above is combined with the ability of the algorithm to guarantee the high probability convergence of the prototype vectors w′i,t to the corresponding ui (Lemma 10). The rate of convergence is ruled by the fact that the associated upper confidence values i,t shrink to zero as 1√t when t grows large. In order for this convergence to take place, it is important to insure that the algorithm is observing informative feedback (either “correct”, i.e., si,t = 1, or “mistaken”, i.e., si,t = −1) for each class i contained in the selected Ŷt. This in turn implies regret bounds for both `a,c (Lemma 8) and `rank,t (Lemma 9).\nThe following lemma faces the problem of hand-crafting a convenient loss function L(·) such that (6) holds.\nLemma 6. Let w1, . . . ,wK ∈ RdK be arbitrary weight vectors such that w>i xt ∈ D, i ∈ [K], (u1, . . . ,uK) ∈ RdK be defined in (2), si,t be the updating signs computed by the algorithm at the end (Step 5) of time t, L : D = [−R,R] ⊆ R → R+ be a convex and differentiable function of its argument, with g(∆) = −L′(∆). Then for any t we have\nEt [ K∑ i=1 L(si,tw > i xt) ] ≥ Et [ K∑ i=1 L(si,t u > i xt) ] ,\ni.e., (6) holds.\nProof: Let us introduce the shorthands ∆i = u>i xt, ∆̂i = w>i,txt, si = si,t, and pi = P(yi,t = 1 |xt) = L ′(−∆i) L′(∆i)+L′(−∆i) . Moreover, let Pt(·) be an abbreviation for the conditional probability P(· | (y1,x1), . . . , (yt−1,xt−1),xt). Recalling the way si,t is constructed (Figure 1), we can write\nEt [ K∑ i=1 L(si,t ∆̂i) ] = ∑ i∈Ŷt ( Pt(si,t = 1)L(∆̂i) + Pt(si,t = −1)L(−∆̂i) ) + (K − |Ŷt|)L(0)\n= ∑ i∈Ŷt ( pi L(∆̂i) + (1− pi)L(−∆̂i) ) + (K − |Ŷt|)L(0) ,\nFor similar reasons,\nEt [ K∑ i=1 L(si,t ∆i) ] = ∑ i∈Ŷt (pi L(∆i) + (1− pi)L(−∆i)) + (K − |Ŷt|)L(0) .\nSince L(·) is convex, so is Et [∑K i=1 L(si,t ∆̂i) ] when viewed as a function of the ∆̂i. We have\nthat ∂ Et[ ∑K i=1 L(si,t ∆̂i)] ∂∆̂i = 0 if and only if for all i ∈ Ŷt we have that ∆̂i satisfies\npi = L′(−∆̂i)\nL′(∆̂i) + L′(−∆̂i) .\nSince pi = L′(−∆i) L′(∆i)+L′(−∆i) , we have that Et [∑K i=1 L(si,t ∆̂i) ]\nis minimized when ∆̂i = ∆i for all i ∈ [K]. The claimed result immediately follows.\nLet now V art(·) be a shorthand for V ar(· | (y1,x1), . . . , (yt−1,xt−1),xt). The following lemma shows that under additional assumptions on the loss L(·), we are afforded to bound the variance of a difference of losses L(·) by the expectation of this difference. This will be key to proving the fast rates of convergence contained in the subsequent Lemma 10.\nLemma 7. Let (w′1,t, . . . ,w′K,t) ∈ RdK be the weight vectors computed by the algorithm in Figure 1 at the beginning (Step 2) of time t, si,t be the updating signs computed at the end (Step 5) of time t, and (u1, . . . ,uK) ∈ RdK be the comparison vectors defined through (2). Let L : D = [−R,R] ⊆\nR → R+ be a C2(D) convex function of its argument, with g(∆) = −L′(∆) and such that there are positive constants c′L and c ′′ L with (L\n′(∆))2 ≤ c′L and L′′(∆) ≥ c′′L for all ∆ ∈ D. Then for any i ∈ Ŷt\n0 ≤ V art ( L(si,t x > t w ′ i,t)− L(si,t u>i xt) ) ≤ 2c ′ L c′′L Et [ L(si,t x > t w ′ i,t)− L(si,t u>i xt) ] .\nProof: Let us introduce the shorthands ∆i = x>t ui, ∆̂i = x>t w′i,t, si = si,t, and recall that pi = P(yi,t = 1 |xt) = L ′(−∆i) L′(∆i)+L′(−∆i) . Then, for any i ∈ [K],\nV art ( L(si,t x > t w ′ i,t)− L(si,t u>i xt) ) ≤ Et (( L(si ∆̂i)− L(si ∆i) )2) ≤ c′L (∆̂i −∆i)2 . (7)\nMoreover, for any i ∈ Ŷt we can write Et [ L(si ∆̂i)− L(si ∆i) ] = pi (L(∆̂i)− L(∆i)) + (1− pi) (L(−∆̂i)− L(−∆i))\n≥ pi ( L′(∆i)(∆̂i −∆i) +\nc′′L 2\n(∆̂i −∆i)2 )\n+ (1− pi) ( L′(−∆i)(∆i − ∆̂) +\nc′′L 2\n(∆̂i −∆i)2 )\n= pi c′′L 2 (∆̂i −∆i)2 + (1− pi) c′′L 2 (∆̂i −∆i)2 = c′′L 2 (∆̂i −∆i)2, (8)\nwhere the second equality uses the definition of pi. Combining (7) with (8) gives the desired bound. We continue by showing a one-step regret bound for our original loss `a,c. The precise connection to loss L(·) will be established with the help of a later lemma (Lemma 10).\nLemma 8. LetL : D = [−R,R] ⊆ R → R+ be a convex, twice differentiable, and nonincreasing function of its argument. Let (u1, . . . ,uK) ∈ RdK be defined in (2) with g(∆) = −L′(∆) for all ∆ ∈ D. Let also cL be a positive constant such that\nL′(∆)L′′(−∆) + L′′(∆)L′(−∆) (L′(∆) + L′(−∆))2 ≥ −cL\nholds for all ∆ ∈ D. Finally, let ∆i,t denote u>i xt, and ∆̂′i,t denote x>t w′i,t, where w′i,t is the i-the weight vector computed by the algorithm at the beginning (Step 2) of time t. If time t is such that |∆i,t − ∆̂′i,t| ≤ i,t for all i ∈ [K], then\nEt[`a,c(Yt, Ŷt)]− Et[`a,c(Yt, Y ∗t )] ≤ 2 (1− a) cL ∑ i∈Ŷt i,t .\nProof: Recall the shorthand notation p(∆) = g(−∆) g(∆)+g(−∆) . We can write\nEt[`a,c(Yt, Ŷt)]− Et[`a,c(Yt, Y ∗t )] = (1− a) ∑ i∈Ŷt ( c(ĵi, |Ŷt|)− ( a 1−a + c(ĵi, |Ŷt|) ) p(∆i,t) ) − (1− a)\n∑ i∈Y ∗t ( c(j∗i , |Y ∗t |)− ( a 1−a + c(j ∗ i , |Y ∗t |) ) p(∆i,t) ) ,\nwhere ĵi denotes the position of class i in Ŷt and j∗i is the position of class i in Y ∗ t . Now,\np′(∆) = −g′(−∆) g(∆)− g′(∆) g(−∆) (g(∆) + g(−∆))2 = −L′(∆)L′′(−∆)− L′(−∆)L′′(∆) (L′(∆) + L′(−∆))2 ≥ 0\nsince g(∆) = −L′(∆), and L(·) is convex and nonincreasing. Hence p(∆) is itself a nondecreasing function of ∆. Moreover, the extra condition on L involving L′ and L′′ is a Lipschitz condition on p(∆) via a uniform bound on p′(∆). Hence, from |∆i,t − ∆̂′i,t| ≤ i,t and the definition of Ŷt we can write\nEt[`a,c(Yt, Ŷt)]− Et[`a,c(Yt, Y ∗t )] ≤ (1− a) ∑ i∈Ŷt ( c(ĵi, |Ŷt|)− ( a 1−a + c(ĵi, |Ŷt|) ) p([∆̂′i,t − i,t]D) ) − (1− a)\n∑ i∈Y ∗t ( c(j∗i , |Y ∗t |)− ( a 1−a + c(j ∗ i , |Y ∗t |) ) p([∆̂′i,t + i,t]D) ) ≤ (1− a)\n∑ i∈Ŷt ( c(ĵi, |Ŷt|)− ( a 1−a + c(ĵi, |Ŷt|) ) p([∆̂′i,t − i,t]D) ) − (1− a)\n∑ i∈Ŷt ( c(ĵi, |Ŷt|)− ( a 1−a + c(ĵi, |Ŷt|) ) p([∆̂′i,t + i,t]D) ) = (1− a)\n∑ i∈Ŷt ( c(ĵi, |Ŷt|) ( p([∆̂′i,t + i,t]D)− p([∆̂′i,t − i,t]D) )) ≤ 2 (1− a) cL\n∑ i∈Ŷt i,t ,\nthe last inequality deriving from c(i, s) ≤ 1 for all i ≤ s ≤ K, and\np([∆̂′i,t + i,t]D)− p([∆̂′i,t − i,t]D) ≤ cL ( [∆̂′i,t + i,t]D − [∆̂′i,t − i,t]D ) ≤ 2 cL i,t.\nNow, we first give a proof of Lemma 4, and then provide a one step regret for the partial information ranking loss. Proof: [Lemma 4] Recall the notation Pt(·) = P(· |xt), and pi,t = p(∆i,t) = g(−∆i,t)g(∆i,t)+g(−∆i,t) . For notational convenience, in this proof we drop subscript t from pi,t, St, yi,t, Ŷt, and `p−rank,t. A simple adaptation of [14] (proof of Theorem 1 therein) shows that for a generic sequence â =\n(â1, . . . , âK) with at most S nonzero values âi and associated set of indices Ŷ , one has\nEt[`p−rank(Yt, â)] = ∑\ni,j∈Ŷ , i<j\n(r̂i,j + r̂j,i) + S ∑ i∈[K] pi − ∑ i∈Ŷ pi  where\nr̂i,j = r̂i,j(â) = Pt(yi > yj) ( {âi < âj}+ 12 {âi = âj} ) .\nMoreover, if p∗ denotes the sequence made up of at most S nonzero values taken from {pi , i ∈ [K]}, where i ranges again in Ŷ , we have\nEt[`p−rank(Yt, p∗)] = ∑\ni,j∈Ŷ , i<j\n(ri,j + rj,i) + S ∑ i∈[K] pi − ∑ i∈Ŷ pi  with\nri,j = ri,j(p ∗) = Pt(yi > yj) ( {pi < pj}+ 12 {pi = pj} ) .\nHence Et[`p−rank(Yt, â)]− Et[`p−rank(Yt, p∗)] = ∑ i,j∈Ŷ , i<j (r̂i,j − ri,j + r̂j,i − rj,i) .\nSince Pt(yi > yj)− Pt(yj > yi) = Pt(yi = 1)− Pt(yj = 1) = pi − pj,\na simple (but lengthy) case analysis reveals that\nr̂i,j − ri,j + r̂j,i − rj,i =  1 2 (pi − pj) If âi < âj, pi = pj or âi = âj, pi > pj 1 2\n(pj − pi) If âi = âj, pi < pj or âi > âj, pi = pj pi − pj If âi < âj, pi > pj pj − pi If âi > âj, pi < pj .\nNotice that the above quantity is always nonnegative, and is strictly positive if the pi are all different. The nonnegativity implies that whatever set of indices Ŷ we select, the best way to sort them within Ŷ in order to minimize Et[`p−rank(Yt, ·)] is by following the ordering of the corresponding pi.\nWe are left to show that the best choice for Ŷ is to collect the S largest11 values in {pi , i ∈ [K]}. To this effect, consider again Et[`p−rank(Yt, p∗)] = Et[`p−rank(Yt, Ŷ )], and introduce the shorthand pi,j = pi pj = pi−Pt(yi > yj). Disregarding the term S ∑ i∈[K] pi, which is independent of Ŷ , we\n11 It is at this point that we need the conditional independence assumption over the classes.\ncan write Et[`p−rank(Yt, Ŷ )] = ∑\ni,j∈Ŷ , i<j\nPt(yi > yj) ( {pi < pj}+ 12 {pi = pj} ) +\n∑ i,j∈Ŷ , i<j Pt(yj > yi) ( {pj < pi}+ 12 {pj = pi} ) − S ∑ i∈Ŷ pi\n= ∑\ni,j∈Ŷ , i<j\n(pi − pi,j){pi < pj}+ (pi − pi,j)12 {pi = pj}\n+ ∑\ni,j∈Ŷ , i<j (pj − pi,j){pj < pi}+ (pj − pi,j)12 {pj = pi} − S ∑ i∈Ŷ pi\n= ∑\ni,j∈Ŷ , i<j (pi − pj){pi < pj}+ 12 (pi − pj) {pi = pj}+ pj − pi,j − S ∑ i∈Ŷ pi\n= ∑\ni,j∈Ŷ , i<j (min{pi, pj} − pipj)− S ∑ i∈Ŷ pi\nwhich can be finally seen to be equal to\n− ∑ i∈Ŷ (S + 1− ĵi) pi − ∑ i,j∈Ŷ , i<j pi pj , (9)\nwhere ĵi is the position of class i within Ŷt in decreasing order of pi. Now, rename the indices in Ŷ as 1, 2, . . . , S, in such a way that p1 > p2 > . . . > pS (so that ĵi = i), and consider the way to increase (9) by adding to Ŷ item k /∈ Ŷ such that pS > pk and removing from Ŷ the item in position `. Denote the resulting sequence by Ŷ ′. From (9), it is not hard to see that\nEt[`p−rank(Yt, Ŷ )]− Et[`p−rank(Yt, Ŷ ′)]\n= (`− 1) p` + S∑\ni=`+1 pi − `−1∑ i=1 pi p` − S∑ i=`+1 p` pi − (S − 1) pk + S∑ i=1,i 6=` pi pk − S(p` − pk)\n= (`− 1) p` + S∑\ni=`+1\npi − (p` − pk) S∑\ni=1,i 6=`\npi − (S − 1) pk − S(p` − pk)\n≤ (S − 1) p` − (p` − pk) S∑\ni=1,i 6=`\npi − (S − 1) pk − S(p` − pk)\n= (pk − p`) ( 1 +\nS∑ i=1,i 6=` pi\n) (10)\nwhich is smaller than zero since, by assumption, p` > pk. Reversing the direction, if we maintain a sequence Ŷ of size S, we can always reduce (9) by removing its the last element and replacing it with a larger element outside the sequence. We continue until no element outside the current\nsequence exists which is larger than the smallest one in the sequence. Clearly, we end up collecting the S largest elements in {pi , i ∈ [K]}.\nFinally, from (9) it is very clear that removing an element from a sequence Ŷ with length h ≤ S can only increase the value of (9). Since this holds for an arbitrary Ŷ , and an arbitrary h ≤ S this shows, that no matter which set Ŷ we start off from, we always converge to the same set containing exaclty the S largest elements in {pi , i ∈ [K]}. This concludes the proof.\nLemma 9. Under the same assumptions and notation as in Lemma 8, combined with the independence assumption (4), let the Algorithm in Figure 1 be working with a→ 1 and strictly decreasing cost values c(i, s), i.e., the algorithm is computing in round t the ranking function f̂(xt;St) defined in Section 4. Let w′i,t be the i-th weight vector computed by this algorithm at the beginning (Step 2) of time t. If time t is such that |∆i,t − ∆̂′i,t| ≤ i,t for all i ∈ [K], then\nEt[`rank,t(Yt, f̂(xt;St)]− Et[`rank,t(Yt, f ∗(xt;St)] ≤ 4St cL ∑ i∈Ŷt i,t .\nProof: We use the same notation as in the proof of Lemma 4, where â is now Ŷt, the sequence produced by ranking f̂(xt;St) operating on p̂i,t. Denote by Y ∗t the sequences determined by f ∗(xt;St), and let ĵi and j∗i be the position of class i in decreasing order of pi,t within Ŷt and Y ∗t , respectively.\nProceeding as in Lemma 8 and recalling (9) we can write\nEt[`p−rank,t(Yt, f̂(xt;St))]− Et[`p−rank,t(Yt, f ∗(xt;St)] = ∑ i∈Y ∗t (St + 1− j∗i ) pi + ∑ i,j∈Y ∗t , i<j pi pj − ∑ i∈Ŷt (St + 1− ĵi) pi − ∑ i,j∈Ŷt, i<j pi pj\n≤ ∑ i∈Y ∗t (St + 1− j∗i ) p([∆̂′i,t + i,t]D) + ∑ i,j∈Y ∗t , i<j p([∆̂′i,t + i,t]D) p([∆̂ ′ j,t + j,t]D)\n− ∑ i∈Ŷt (St + 1− ĵi) p([∆̂′i,t − i,t]D)− ∑ i,j∈Ŷt, i<j p([∆̂′i,t − i,t]D) p([∆̂′j,t − j,t]D)\n≤ ∑ i∈Ŷt (St + 1− ĵi) ( p([∆̂′i,t + i,t]D)− p([∆̂′i,t − i,t]D) ) +\n∑ i,j∈Ŷt, i<j ( p([∆̂′i,t + i,t]D) p([∆̂ ′ j,t + j,t]D)− p([∆̂′i,t − i,t]D) p([∆̂′j,t − j,t]D) ) ≤ 2StcL\n∑ i∈Ŷt i,t + ∑ i,j∈Ŷt, i<j 2cL ( i,t + j,t)\n= 2St cL ∑ i∈Ŷt i,t + 2 (St − 1) cL ∑ i∈Ŷt i,t\n< 4St cL ∑ i∈Ŷt i,t ,\nas claimed.\nLemma 10. Let L : D = [−R,R] ⊆ R → R+ be a C2(D) convex and nonincreasing function of its argument, (u1, . . . ,uK) ∈ RdK be defined in (2) with g(∆) = −L′(∆) for all ∆ ∈ D, and such that ‖ui‖ ≤ U for all i ∈ [K]. Assume there are positive constants c′L and c′′L with (L′(∆))2 ≤ c′L and L′′(∆) ≥ c′′L for all ∆ ∈ D. With the notation introduced in Figure 1, we have that\n(x>w′i,t − u>i x)2 ≤ x>A−1i,t−1x ( U2 +\nd c′L (c′′L) 2 ln\n( 1 +\nt− 1 d\n) + 12\nc′′L ( c′L c′′L + 3L(−R) ) ln K(t+ 4) δ ) holds with probability at least 1 − δ for any δ < 1/e, uniformly over i ∈ [K], t = 1, 2, . . . , and x ∈ Rd.\nProof: For any given class i, the time-t update rule w′i,t → wi,t+1 → w′i,t+1 in Figure 1 allows us to start off from [21] (proof of Theorem 2 therein), from which one can extract the following inequality\ndi,t−1(ui,w ′ i,t)\n≤ U2 + 1 (c′′L) 2 t−1∑ k=1 ri,k − 2 c′′L t−1∑ k=1 ( ∇>i,k(w′i,k − ui)− c′′L 2 ( si,k x > k (w ′ i,k − ui) )2) , (11)\nwhere we set ri,k = ∇>i,k A−1i,k ∇i,k. Using the lower bound on the second derivative of L we have\nL(si,k x > kw ′ i,k)− L(si,k u>i xk)\n≤ L′(si,k x>kw′i,k)(si,kx>kw′i,k − si,k u>i xk)− c′′L 2 (si,k x > kw ′ i,k − si,k u>i xk)2 = ∇>i,k(w′i,k − ui)− c′′L 2 ( si,k x > k (w ′ i,k − ui) )2 .\nPlugging back into (11) yields\ndi,t−1(ui,w ′ i,t) ≤ U2 +\n1\n(c′′L) 2 t−1∑ k=1 ri,k − 2 c′′L t−1∑ k=1 ( L(si,k x > kw ′ i,k)− L(si,k u>i xk) ) (12)\nWe now borrow a proof technique from [13] (see also [11, 1] and references therein). Define\nLi,k = L(si,k x > kw ′ i,k)− L(si,k u>i xk)\nand L′i,k = Ek[Li,k] − Li,k. Notice that the sequence of random variables L′i,1, L′i,2, . . . , forms a martingale difference sequence such that, for any i ∈ Ŷk:\ni. Ek[Li,k] ≥ 0, by Lemma 7;\nii. |L′i,k| ≤ 2L(−R), since L(·) is nonincreasing over D, and si,k x>kw′i,k, si,k u>i xk ∈ D;\niii. V ark(L′i,k) = V ark(Li,k) ≤ 2c′L c′′L Ek[Li,k] (again, because of Lemma 7).\nOn the other hand, when i /∈ Ŷk then si,k = 0, and the above three properties are trivally satisfied. Under the above conditions, we are in a position to apply any fast concentration result for bounded martingale difference sequences. For instance, setting for brevity B = B(t, δ) = 3 ln K(t+4)\nδ , a\nresult contained in [24] allows us derive the inequality\nt−1∑ k=1 Ek[Li,k]− t−1∑ k=1 Li,k ≥ max  √√√√8c′L c′′L B t−1∑ k=1 Ek[Li,k], 6L(−R)B  , that holds with probability at most δ\nKt(t+1) for any t ≥ 1. We use the inequality\n√ cb ≤ 1\n2 (c + b)\nwith c = 4c ′ L\nc′′L B, and b = 2\n∑t−1 k=1 Ek[Li,k], and simplify. This gives\n− t−1∑ k=1 Li,k ≤ ( 2c′L c′′L + 6L(−R) ) B\nwith probability at least 1− δ Kt(t+1) . Using the Cauchy-Schwarz inequality\n(x>w′i,t − u>i x)2 ≤ x>A−1i,t−1 x di,t−1(ui,w′i,t)\nholding for any x ∈ Rd, and replacing back into (12) allows us to conclude that\n(x>w′i,t − u>i x)2 ≤ x>A−1i,t−1x\n( U2 + 1\n(c′′L) 2 t−1∑ k=1 ri,k + 12 c′′L ( c′L c′′L + 3L(−R) ) ln K(t+ 4) δ ) (13)\nholds with probability at least 1− δ Kt(t+1) , uniformly over x ∈ Rd. The bounds on ∑t−1 k=1 ri,k can be obtained in a standard way. Applying known inequalities [4, 8, 10, 7, 21, 13], and using the fact that∇i,k = L′(si,k x>kw′i,k) si,kxk we have12\nt−1∑ k=1 ri,k = t−1∑ k=1 |si,j| (L′(si,k x>kw′i,k))2 x>k A−1i,kxk\n≤ c′L t−1∑ k=1 |si,k|x>k A−1i,kxk\n≤ c′L t−1∑ k=1 ln |Ai,k| |Ai,k−1| = c′L ln |Ai,t−1| |Ai,0|\n≤ d c′L ln (\n1 + t− 1 d\n) .\n12 It is in this chain of inequalities that we exploit the rank-one update ofAi,t−1 based on xtx>t rather than∇i,t∇>i,t. Notice that using the latter (as in the worst-case analysis by [21]), does not guarantee a significant progress in the positive definiteness of Ai,t. This is due to the presence of the multiplicative factor g(si,t∆̂′i,t) (Step 5 in Figure 1) which can be arbitrarily small.\nCombining as in (13) and stratifying over t = 1, 2, . . ., and i ∈ [K] concludes the proof. We are now ready to put all pieces together. Proof: [Theorem 2] From Lemma 8 and Lemma 10, we see that with probability at least 1− δ,\nRT ≤ 2 (1− a) cL T∑ t=1 ∑ i∈Ŷt i,t , (14)\nwhen 2i,t is the one given in Figure 1. We continue by proving a pointwise upper bound on the sum in the RHS. More in detail, we will find an upper bound on ∑T t=1 ∑ i∈Ŷt 2 i,t, and then derive a resulting upper bound on the RHS of (14). From Lemma 10 and the update rule (Step 5) of the algorithm we can write\n2i,t ≤ C x>t A−1i,t−1xt\n= C x>t (Ai,t−1 + |si,t|xtx>t )−1xt\n1− |si,t|x>t (Ai,t−1 + |si,t|xtx>t )−1xt\n= C x>t A −1 i,t xt\n1− |si,t|x>t (Ai,t−1 + |si,t|xtx>t )−1xt\n≤ C x>t A −1 i,t xt\n1− |si,t|x>t (A0 + |si,t|xtx>t )−1xt\n= C x>t A −1 i,t xt\n1− 1 2\n= 2C x>t A −1 i,t xt .\nHence, if we set ri,t = x>t A −1 i,t xt and proceed as in the proof of Lemma 10, we end up with the upper bound ∑T\nt=1 2 i,t ≤ 2C d ln\n( 1 + T\nd\n) , holding for all i ∈ [K]. Denoting by M the quantity\n2C d ln ( 1 + T\nd\n) , we conclude from (14) that\nRT ≤ 2 (1− a) cL max ∑ i∈[K] T∑ t=1 i,t ∣∣∣ T∑ t=1 2i,t ≤M, i ∈ [K]  = 2 (1− a) cLK√T M , as claimed. Proof: [Theorem 3] As we said, we change the definition of 2i,t in the Algorithm in Figure 1 to\n2i,t =\nmax { x>A−1i,t−1x ( 2 d c′L (c′′L) 2 ln ( 1 + t− 1 d ) + 12 c′′L ( c′L c′′L + 3L(−R) ) ln K(t+ 4) δ ) , 4R2 } .\nFirst, notice that the 4R2 cap seamlessly applies, since (x>w′i,t − u>i x)2 in Lemma 10 is bounded by 4R2 anyway. With this modification, we have that Theorem 2 only holds for t such that d c ′ L\n(c′′L) 2 ln\n( 1 + t−1\nd\n) ≥ U2, i.e., for t ≥ d ( exp ( (c′′L) 2 U2\nc′L d\n) − 1 ) + 1, while for t <\nd ( exp ( (c′′L) 2 U2\nc′L d\n) − 1 ) + 1 we have in the worst-case scenario the maximum amount of regret\nat each step. From Lemma 8 we see that this maximum amount (the cap on 2i,t is needed here) can be bounded by 4 (1− a) cL |Ŷt|R ≤ 4 (1− a) cLK R.\nProof: [Theorem 5] We start from the one step-regret delivered by Lemma 9, and proceed as in the proof of Theorem 2. This yields\nRT ≤ 4 cL T∑ t=1 St ∑ i∈Ŷt i,t\n≤ 4S cL T∑ t=1 ∑ i∈Ŷt i,t\n≤ 4S cL T∑ t=1 ∑ i∈[K] i,t\n= 4S cL ∑ i∈[K] T∑ t=1 i,t ,\nwith probability at least 1− δ, where 2i,t is the one given in Figure 1. Let M be as in the proof of Theorem 2. If Ni,T denotes the total number of times class i occurs in Ŷt, we have that ∑T t=1 2 i,t ≤\nM , implying ∑T t=1 i,t ≤ √ Ni,T M for all i ∈ [K]. Moreover, ∑ i∈[K] Ni,T ≤ ST . Hence\nRT ≤ 4S cL ∑ i∈K] √ Ni,T M ≤ 4 cL √ M SK T ,\nas claimed."
    }, {
      "heading" : "7 Conclusions",
      "text" : "We have used generalized linear models to formalize the exploration-exploitation tradeoff in a multilabel/ranking setting with partial feedback, providing T 1/2-like regret bounds under semiadversarial settings. Our analysis decouples the multilabel/ranking loss at hand from the labelgeneration model. Thanks to the usage of calibrated score values p̂i,t, our algorithm is capable of automatically inferring where to split the ranking between relevant and nonrelevant classes [17], the split being clearly induced by the loss parameters in `a,c. We are planning on using more general label models that explicitly capture label correlations to be applied to other loss functions (e.g., F-measure, 0/1, average precision, etc.). We are also planning on carrying out a more thorough experimental comparison, especially to full information multilabel methods that take such correlations into account. Finally, we are currenty working on extending our framework to structured output tasks, like (multilabel) hierarchical classification."
    } ],
    "references" : [ {
      "title" : "Improved algorithms for linear stochastic bandits",
      "author" : [ "Y. Abbasi-Yadkori", "D. Pal", "C. Szepesvári" ],
      "venue" : "In Proc. of the 25th NIPS,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2011
    }, {
      "title" : "Graphical models for bandit problems",
      "author" : [ "K. Amin", "M. Kearns", "U. Syed" ],
      "venue" : "In Proc. of UAI,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2011
    }, {
      "title" : "Using confidence bounds for exploitation-exploration",
      "author" : [ "P. Auer" ],
      "venue" : "trade-offs. JMLR,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2003
    }, {
      "title" : "Relative loss bounds for online density estimation with the exponential family of distributions",
      "author" : [ "K.S. Azoury", "M.K. Warmuth" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2001
    }, {
      "title" : "Partial monitoring with side information",
      "author" : [ "G. Bartók", "C. Szepesvári" ],
      "venue" : "In Proc. 23rd Alt,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2012
    }, {
      "title" : "Multi-label classification on tree- and dag-structured hierarchies",
      "author" : [ "W. Bi", "J. Kwok" ],
      "venue" : "In Proc. 28th ICML,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2011
    }, {
      "title" : "Learning noisy linear classifiers via adaptive and selective sampling",
      "author" : [ "G. Cavallanti", "N. Cesa-Bianchi", "C. Gentile" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2011
    }, {
      "title" : "A second-order perceptron algorithm",
      "author" : [ "N. Cesa-Bianchi", "A. Conconi", "C. Gentile" ],
      "venue" : "In Proc. of the 15th Annual Conference on Computational Learning Theory (COLT",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2002
    }, {
      "title" : "Incremental algorithms for hierarchical classification",
      "author" : [ "N. Cesa-Bianchi", "C. Gentile", "L. Zaniboni" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2006
    }, {
      "title" : "Robust bounds for classification via selective sampling",
      "author" : [ "N. Cesa-Bianchi", "C. Gentile", "F. Orabona" ],
      "venue" : "In Proc. of the 26th International Conference on Machine Learning",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2009
    }, {
      "title" : "Multiclass classification with bandit feedback using adaptive regularization",
      "author" : [ "K. Crammer", "C. Gentile" ],
      "venue" : "In Proc. of the 29th International Conference on Machine Learning",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2011
    }, {
      "title" : "Stochastic linear optimization under bandit feedback",
      "author" : [ "V. Dani", "T. Hayes", "S. Kakade" ],
      "venue" : "In Proc. of the 21th annual conference on Learning Theory (COLT",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2008
    }, {
      "title" : "Selective sampling and active learning from single and multiple teachers",
      "author" : [ "O. Dekel", "C. Gentile", "K. Sridharan" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "On label dependence and loss minimization in multi-label classification",
      "author" : [ "K. Dembczynski", "W. Waegeman", "W. Cheng", "E. Hullermeier" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2012
    }, {
      "title" : "Parametric bandits: The generalized linear case",
      "author" : [ "S. Filippi", "O. Cappé", "A. Garivier", "C. Szepesvári" ],
      "venue" : "In Proc. NIPS,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2010
    }, {
      "title" : "An efficient boosting algorithm for combining preferences",
      "author" : [ "Y. Freund", "R.D. Iyer", "R.E. Schapire", "Y. Singer" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2003
    }, {
      "title" : "Multilabel classification via calibrated label ranking",
      "author" : [ "J. Furnkranz", "E. Hullermeier", "E. Loza Menca", "K. Brinker" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2008
    }, {
      "title" : "On multilabel classification and ranking with partial feedback",
      "author" : [ "C. Gentile", "F. Orabona" ],
      "venue" : "In Proc. NIPS 2012,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2012
    }, {
      "title" : "Online submodular minimization",
      "author" : [ "E. Hazan", "S. Kale" ],
      "venue" : "In Proc. NIPS 22,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2009
    }, {
      "title" : "Newtron: an efficient bandit algorithm for online multiclass prediction",
      "author" : [ "E. Hazan", "S. Kale" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2011
    }, {
      "title" : "Logarithmic regret algorithms for online convex optimization",
      "author" : [ "E. Hazan", "A. Agarwal", "S. Kale" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2007
    }, {
      "title" : "Large margin rank boundaries for ordinal regression",
      "author" : [ "R. Herbrich", "T. Graepel", "K. Obermayer" ],
      "venue" : "In Advances in Large Margin Classifiers,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2000
    }, {
      "title" : "Multi-label prediction via compressed sensing",
      "author" : [ "D. Hsu", "S. Kakade", "J. Langford", "T. Zhang" ],
      "venue" : "In Proc. 23rd NIPS,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2009
    }, {
      "title" : "On the generalization ability of online strongly convex programming",
      "author" : [ "S. Kakade", "A. Tewari" ],
      "venue" : "In Nips,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2008
    }, {
      "title" : "Efficient bandit algorithms for online multiclass prediction",
      "author" : [ "S. Kakade", "S. Shalev-Shwartz", "A. Tewari" ],
      "venue" : "In Proc. 25th ICML,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2008
    }, {
      "title" : "Non-stochastic bandit slate problems",
      "author" : [ "S. Kale", "L. Reyzin", "R. Schapire" ],
      "venue" : "In 24th NIPS,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2010
    }, {
      "title" : "Contextual gaussian process bandit optimization",
      "author" : [ "A. Krause", "C.S. Ong" ],
      "venue" : "In 25th NIPS,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2011
    }, {
      "title" : "Asymptotically efficient adaptive allocation rules",
      "author" : [ "T.H. Lai", "H. Robbins" ],
      "venue" : "Adv. Appl. Math.,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 1985
    }, {
      "title" : "The epoch-greedy algorithm for contextual multi-armed bandits",
      "author" : [ "J. Langford", "T. Zhang" ],
      "venue" : "Nips",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2007
    }, {
      "title" : "Generalized linear models",
      "author" : [ "P. McCullagh", "J.A. Nelder" ],
      "venue" : null,
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 1989
    }, {
      "title" : "Improving multilabel analysis of music titles: A large-scale validation of the correction approach",
      "author" : [ "F. Pachet", "P. Roy" ],
      "venue" : "IEEE Trans. on Audio, Speech, and Lang. Proc.,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2009
    }, {
      "title" : "Elicitation of personal probabilities and expectations",
      "author" : [ "L.J. Savage" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 1973
    }, {
      "title" : "Online structured prediction via coactive learning",
      "author" : [ "P. Shivaswamy", "T. Joachims" ],
      "venue" : "In Proc. 29th ICML,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2012
    }, {
      "title" : "Fonctions de rpartition n dimensions et leurs marges",
      "author" : [ "A. Sklar" ],
      "venue" : "Publ. Inst. Statist. Univ. Paris,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 1959
    }, {
      "title" : "Learning optimally diverse rankings over large document collections",
      "author" : [ "A. Slivkins", "F. Radlinski", "S. Gollapudi" ],
      "venue" : "In Proc. of the 27th ICML,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2010
    }, {
      "title" : "The challenge problem for automated detection of 101 semantic concepts in multimedia",
      "author" : [ "C.G.M. Snoek", "M. Worring", "J.C. van Gemert", "J.-M. Geusebroek", "A.W.M. Smeulders" ],
      "venue" : "In Proc. of the 14th ACM international conference on Multimedia,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2006
    }, {
      "title" : "Online learning of assignments",
      "author" : [ "M. Streeter", "D. Golovin", "A. Krause" ],
      "venue" : "In Proc. of the 23rd NIPS,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2009
    }, {
      "title" : "Random k-labelsets for multilabel classification",
      "author" : [ "G. Tsoumakas", "I. Katakis", "I. Vlahavas" ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2011
    }, {
      "title" : "Generalization bounds for online learning algorithms with pairwise loss functions",
      "author" : [ "Y. Wang", "R. Khardon", "D. Pechyony", "R. Jones" ],
      "venue" : "In Proc. of the 25th Conference on Learning Theory (COLT),",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 27,
      "context" : "This technique has been introduced by [28], and can by now be considered a standard tool.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 2,
      "context" : ", [3, 12, 15, 11, 27], and references therein) an online algorithm receives at each time step a context (typically, in the form of a feature vector x) and is compelled to select an action (e.",
      "startOffset" : 2,
      "endOffset" : 21
    }, {
      "referenceID" : 11,
      "context" : ", [3, 12, 15, 11, 27], and references therein) an online algorithm receives at each time step a context (typically, in the form of a feature vector x) and is compelled to select an action (e.",
      "startOffset" : 2,
      "endOffset" : 21
    }, {
      "referenceID" : 14,
      "context" : ", [3, 12, 15, 11, 27], and references therein) an online algorithm receives at each time step a context (typically, in the form of a feature vector x) and is compelled to select an action (e.",
      "startOffset" : 2,
      "endOffset" : 21
    }, {
      "referenceID" : 10,
      "context" : ", [3, 12, 15, 11, 27], and references therein) an online algorithm receives at each time step a context (typically, in the form of a feature vector x) and is compelled to select an action (e.",
      "startOffset" : 2,
      "endOffset" : 21
    }, {
      "referenceID" : 26,
      "context" : ", [3, 12, 15, 11, 27], and references therein) an online algorithm receives at each time step a context (typically, in the form of a feature vector x) and is compelled to select an action (e.",
      "startOffset" : 2,
      "endOffset" : 21
    }, {
      "referenceID" : 2,
      "context" : "For instance, [3, 12, 15, 1] work in a finite action space where the mappings context-to-loss for each action are linear (or generalized linear, as [15]’s) functions of the features.",
      "startOffset" : 14,
      "endOffset" : 28
    }, {
      "referenceID" : 11,
      "context" : "For instance, [3, 12, 15, 1] work in a finite action space where the mappings context-to-loss for each action are linear (or generalized linear, as [15]’s) functions of the features.",
      "startOffset" : 14,
      "endOffset" : 28
    }, {
      "referenceID" : 14,
      "context" : "For instance, [3, 12, 15, 1] work in a finite action space where the mappings context-to-loss for each action are linear (or generalized linear, as [15]’s) functions of the features.",
      "startOffset" : 14,
      "endOffset" : 28
    }, {
      "referenceID" : 0,
      "context" : "For instance, [3, 12, 15, 1] work in a finite action space where the mappings context-to-loss for each action are linear (or generalized linear, as [15]’s) functions of the features.",
      "startOffset" : 14,
      "endOffset" : 28
    }, {
      "referenceID" : 14,
      "context" : "For instance, [3, 12, 15, 1] work in a finite action space where the mappings context-to-loss for each action are linear (or generalized linear, as [15]’s) functions of the features.",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 26,
      "context" : "This is extended by [27], where the loss function is modeled as a sample from a Gaussian process over the joint context-action space.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 28,
      "context" : "An earlier (but somehow more general) setting that models such mappings by VC-classes is considered by [29], where a T 2/3 regret bound has been proven under i.",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 24,
      "context" : ", [25, 11, 20], where either T 2/3 or T 1/2 or even logarithmic regret bounds are proven, depending on the noise model and the underlying loss functions.",
      "startOffset" : 2,
      "endOffset" : 14
    }, {
      "referenceID" : 10,
      "context" : ", [25, 11, 20], where either T 2/3 or T 1/2 or even logarithmic regret bounds are proven, depending on the noise model and the underlying loss functions.",
      "startOffset" : 2,
      "endOffset" : 14
    }, {
      "referenceID" : 19,
      "context" : ", [25, 11, 20], where either T 2/3 or T 1/2 or even logarithmic regret bounds are proven, depending on the noise model and the underlying loss functions.",
      "startOffset" : 2,
      "endOffset" : 14
    }, {
      "referenceID" : 18,
      "context" : "Along these lines are [19, 37, 26, 35, 33, 2].",
      "startOffset" : 22,
      "endOffset" : 45
    }, {
      "referenceID" : 36,
      "context" : "Along these lines are [19, 37, 26, 35, 33, 2].",
      "startOffset" : 22,
      "endOffset" : 45
    }, {
      "referenceID" : 25,
      "context" : "Along these lines are [19, 37, 26, 35, 33, 2].",
      "startOffset" : 22,
      "endOffset" : 45
    }, {
      "referenceID" : 34,
      "context" : "Along these lines are [19, 37, 26, 35, 33, 2].",
      "startOffset" : 22,
      "endOffset" : 45
    }, {
      "referenceID" : 32,
      "context" : "Along these lines are [19, 37, 26, 35, 33, 2].",
      "startOffset" : 22,
      "endOffset" : 45
    }, {
      "referenceID" : 1,
      "context" : "Along these lines are [19, 37, 26, 35, 33, 2].",
      "startOffset" : 22,
      "endOffset" : 45
    }, {
      "referenceID" : 18,
      "context" : "The general problem of online minimization of a submodular loss function under both full and bandit information without covariates is considered by [19], achieving a regret T 2/3 in the bandit case.",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 36,
      "context" : "[37] consider the problem of online learning of assignments, where at each round an algorithm is requested to assign positions (e.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 25,
      "context" : "Another paper with similar goals but a different mathematical model is by [26], where the aim is to learn a suitable ordering (an “ordered slate”) of the available actions.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 34,
      "context" : "[35] motivate the ability of selecting sets of actions by a problem of diverse retrieval in large document collections which are meant to live in a general metric space.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 32,
      "context" : "[33] use a simple linear model for the hidden utility function of users interacting with a web system and providing partial feedback in any form that allows the system to make significant progress in learning this function (this is called an α-informative feedback by the authors).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 1,
      "context" : "The recent paper [2] investigates classes of graphical models for contextual bandit settings that afford richer interaction between contexts and actions leading again to a T 2/3 regret bound.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 17,
      "context" : "Finally, a very interesting recent work that came to our attention at the time of writing this extended version of our conference paper [18] is [5].",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 4,
      "context" : "Finally, a very interesting recent work that came to our attention at the time of writing this extended version of our conference paper [18] is [5].",
      "startOffset" : 144,
      "endOffset" : 147
    }, {
      "referenceID" : 4,
      "context" : "The results presented by [5] do not seem to conveniently extend to the structured action space setting we are interested in (or, if they do, we do not see it in the current version of their paper).",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 37,
      "context" : "Relevant references include [38, 17, 14], along with references therein.",
      "startOffset" : 28,
      "endOffset" : 40
    }, {
      "referenceID" : 16,
      "context" : "Relevant references include [38, 17, 14], along with references therein.",
      "startOffset" : 28,
      "endOffset" : 40
    }, {
      "referenceID" : 13,
      "context" : "Relevant references include [38, 17, 14], along with references therein.",
      "startOffset" : 28,
      "endOffset" : 40
    }, {
      "referenceID" : 13,
      "context" : "In contrast to that, the specific setting we are considering here need not face such a modeling [14].",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 38,
      "context" : "recent work [39] reduces any online algorithm working on pairwise loss functions (like a ranking loss) to a batch algorithm with generalization bound guarantees.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 21,
      "context" : "Other related references are [22, 16], where learning is by pairs of examples.",
      "startOffset" : 29,
      "endOffset" : 37
    }, {
      "referenceID" : 15,
      "context" : "Other related references are [22, 16], where learning is by pairs of examples.",
      "startOffset" : 29,
      "endOffset" : 37
    }, {
      "referenceID" : 2,
      "context" : "To summarize, whereas we are technically closer to the linear modeling approaches by [3, 12, 13, 11, 15, 1, 27, 5], from a motivational standpoint we are perhaps closest to [37, 26, 33].",
      "startOffset" : 85,
      "endOffset" : 114
    }, {
      "referenceID" : 11,
      "context" : "To summarize, whereas we are technically closer to the linear modeling approaches by [3, 12, 13, 11, 15, 1, 27, 5], from a motivational standpoint we are perhaps closest to [37, 26, 33].",
      "startOffset" : 85,
      "endOffset" : 114
    }, {
      "referenceID" : 12,
      "context" : "To summarize, whereas we are technically closer to the linear modeling approaches by [3, 12, 13, 11, 15, 1, 27, 5], from a motivational standpoint we are perhaps closest to [37, 26, 33].",
      "startOffset" : 85,
      "endOffset" : 114
    }, {
      "referenceID" : 10,
      "context" : "To summarize, whereas we are technically closer to the linear modeling approaches by [3, 12, 13, 11, 15, 1, 27, 5], from a motivational standpoint we are perhaps closest to [37, 26, 33].",
      "startOffset" : 85,
      "endOffset" : 114
    }, {
      "referenceID" : 14,
      "context" : "To summarize, whereas we are technically closer to the linear modeling approaches by [3, 12, 13, 11, 15, 1, 27, 5], from a motivational standpoint we are perhaps closest to [37, 26, 33].",
      "startOffset" : 85,
      "endOffset" : 114
    }, {
      "referenceID" : 0,
      "context" : "To summarize, whereas we are technically closer to the linear modeling approaches by [3, 12, 13, 11, 15, 1, 27, 5], from a motivational standpoint we are perhaps closest to [37, 26, 33].",
      "startOffset" : 85,
      "endOffset" : 114
    }, {
      "referenceID" : 26,
      "context" : "To summarize, whereas we are technically closer to the linear modeling approaches by [3, 12, 13, 11, 15, 1, 27, 5], from a motivational standpoint we are perhaps closest to [37, 26, 33].",
      "startOffset" : 85,
      "endOffset" : 114
    }, {
      "referenceID" : 4,
      "context" : "To summarize, whereas we are technically closer to the linear modeling approaches by [3, 12, 13, 11, 15, 1, 27, 5], from a motivational standpoint we are perhaps closest to [37, 26, 33].",
      "startOffset" : 85,
      "endOffset" : 114
    }, {
      "referenceID" : 36,
      "context" : "To summarize, whereas we are technically closer to the linear modeling approaches by [3, 12, 13, 11, 15, 1, 27, 5], from a motivational standpoint we are perhaps closest to [37, 26, 33].",
      "startOffset" : 173,
      "endOffset" : 185
    }, {
      "referenceID" : 25,
      "context" : "To summarize, whereas we are technically closer to the linear modeling approaches by [3, 12, 13, 11, 15, 1, 27, 5], from a motivational standpoint we are perhaps closest to [37, 26, 33].",
      "startOffset" : 173,
      "endOffset" : 185
    }, {
      "referenceID" : 32,
      "context" : "To summarize, whereas we are technically closer to the linear modeling approaches by [3, 12, 13, 11, 15, 1, 27, 5], from a motivational standpoint we are perhaps closest to [37, 26, 33].",
      "startOffset" : 173,
      "endOffset" : 185
    }, {
      "referenceID" : 18,
      "context" : ", [19, 37, 1, 27]), we are not assuming the algorithm is observing a noisy version of the risk function on the currently selected action.",
      "startOffset" : 2,
      "endOffset" : 17
    }, {
      "referenceID" : 36,
      "context" : ", [19, 37, 1, 27]), we are not assuming the algorithm is observing a noisy version of the risk function on the currently selected action.",
      "startOffset" : 2,
      "endOffset" : 17
    }, {
      "referenceID" : 0,
      "context" : ", [19, 37, 1, 27]), we are not assuming the algorithm is observing a noisy version of the risk function on the currently selected action.",
      "startOffset" : 2,
      "endOffset" : 17
    }, {
      "referenceID" : 26,
      "context" : ", [19, 37, 1, 27]), we are not assuming the algorithm is observing a noisy version of the risk function on the currently selected action.",
      "startOffset" : 2,
      "endOffset" : 17
    }, {
      "referenceID" : 10,
      "context" : "In this sense, we are more similar to the multiclass bandit algorithm by [11].",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 10,
      "context" : "Yet, our work is a substantial departure from [11]’s in that we lift their machinery to nontrivial structured action spaces, and we do so by means of generalized linear models.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 0,
      "context" : "Specifically, given constant a ∈ [0, 1] and costs c = {c(i, s), i = 1, .",
      "startOffset" : 33,
      "endOffset" : 39
    }, {
      "referenceID" : 16,
      "context" : "In this sense, the above problem can be seen as a partial information version of the multilabel ranking problem (see [17], 1 An ordered subset is like a list with no repeated items.",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 0,
      "context" : "2Notice that a is not redundant here, since the costs c(i, s) have been normalized to [0,1].",
      "startOffset" : 86,
      "endOffset" : 91
    }, {
      "referenceID" : 29,
      "context" : "3 The reader familiar with generalized linear models will recognize the derivative of the function p(∆) = g(−∆) g(∆)+g(−∆) as the (inverse) link function of the associated canonical exponential family of distributions [30].",
      "startOffset" : 218,
      "endOffset" : 222
    }, {
      "referenceID" : 33,
      "context" : "A classical result in the theory of copulas [34] makes one derive all allowed joint distributions starting from the corresponding one-dimensional marginals.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 10,
      "context" : "We use a similar but largely more general analysis than [11]’s to devise an online second-order descent algorithm whose updating rule makes the comparison vector U = (u1, .",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 0,
      "context" : "Parameters: loss parameters a ∈ [0, 1], cost values c(i, s), interval D = [−R,R], function g : D → R, confidence level δ ∈ [0, 1].",
      "startOffset" : 32,
      "endOffset" : 38
    }, {
      "referenceID" : 0,
      "context" : "Parameters: loss parameters a ∈ [0, 1], cost values c(i, s), interval D = [−R,R], function g : D → R, confidence level δ ∈ [0, 1].",
      "startOffset" : 123,
      "endOffset" : 129
    }, {
      "referenceID" : 20,
      "context" : ", [21].",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 20,
      "context" : "This feature tells this algorithm slightly apart from the Online Newton step algorithm [21], which is the starting point of our analysis.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 10,
      "context" : ", by [11]), one can use a version of the algorithm which maintains diagonal matrices Ai,t instead of full ones.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 13,
      "context" : ", [14]), one can view any multilabel assignment Y = (y1, .",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 13,
      "context" : "As pointed out by [14], the ranking function f(xt) = (p1,t, .",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 16,
      "context" : ", f ∗ K(xt)), 8 This is called the zero point by [17].",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 13,
      "context" : "This is in striking contrast to the full information setting, where the Bayes optimal ranking only depends on the marginal distribution values pi,t [14].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 22,
      "context" : ", [23] and references therein) and/or the specific structure of the set of labels (e.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 8,
      "context" : ", [9, 6], and references therein).",
      "startOffset" : 2,
      "endOffset" : 8
    }, {
      "referenceID" : 5,
      "context" : ", [9, 6], and references therein).",
      "startOffset" : 2,
      "endOffset" : 8
    }, {
      "referenceID" : 35,
      "context" : "The first one, called Mediamill, was introduced in a video annotation challenge [36].",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 30,
      "context" : "The second dataset is Sony CSL Paris [31], made up of 16,452 train samples and 16,519 test samples, each sample being described by d = 98 features.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 31,
      "context" : ", [32]).",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 13,
      "context" : "A simple adaptation of [14] (proof of Theorem 1 therein) shows that for a generic sequence â =",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 20,
      "context" : "Proof: For any given class i, the time-t update rule wi,t → wi,t+1 → wi,t+1 in Figure 1 allows us to start off from [21] (proof of Theorem 2 therein), from which one can extract the following inequality di,t−1(ui,w ′ i,t) ≤ U + 1 (c′′ L) 2 t−1 ∑",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 12,
      "context" : "We now borrow a proof technique from [13] (see also [11, 1] and references therein).",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 10,
      "context" : "We now borrow a proof technique from [13] (see also [11, 1] and references therein).",
      "startOffset" : 52,
      "endOffset" : 59
    }, {
      "referenceID" : 0,
      "context" : "We now borrow a proof technique from [13] (see also [11, 1] and references therein).",
      "startOffset" : 52,
      "endOffset" : 59
    }, {
      "referenceID" : 23,
      "context" : "For instance, setting for brevity B = B(t, δ) = 3 ln K(t+4) δ , a result contained in [24] allows us derive the inequality",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 3,
      "context" : "Applying known inequalities [4, 8, 10, 7, 21, 13], and using the fact that∇i,k = L(si,k xkwi,k) si,kxk we have12 t−1 ∑",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 7,
      "context" : "Applying known inequalities [4, 8, 10, 7, 21, 13], and using the fact that∇i,k = L(si,k xkwi,k) si,kxk we have12 t−1 ∑",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 9,
      "context" : "Applying known inequalities [4, 8, 10, 7, 21, 13], and using the fact that∇i,k = L(si,k xkwi,k) si,kxk we have12 t−1 ∑",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 6,
      "context" : "Applying known inequalities [4, 8, 10, 7, 21, 13], and using the fact that∇i,k = L(si,k xkwi,k) si,kxk we have12 t−1 ∑",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 20,
      "context" : "Applying known inequalities [4, 8, 10, 7, 21, 13], and using the fact that∇i,k = L(si,k xkwi,k) si,kxk we have12 t−1 ∑",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 12,
      "context" : "Applying known inequalities [4, 8, 10, 7, 21, 13], and using the fact that∇i,k = L(si,k xkwi,k) si,kxk we have12 t−1 ∑",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 20,
      "context" : "Notice that using the latter (as in the worst-case analysis by [21]), does not guarantee a significant progress in the positive definiteness of Ai,t.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 16,
      "context" : "Thanks to the usage of calibrated score values p̂i,t, our algorithm is capable of automatically inferring where to split the ranking between relevant and nonrelevant classes [17], the split being clearly induced by the loss parameters in `a,c.",
      "startOffset" : 174,
      "endOffset" : 178
    } ],
    "year" : 2013,
    "abstractText" : "We present a novel multilabel/ranking algorithm working in partial information settings. The algorithm is based on 2nd-order descent methods, and relies on upper-confidence bounds to trade-off exploration and exploitation. We analyze this algorithm in a partial adversarial setting, where covariates can be adversarial, but multilabel probabilities are ruled by (generalized) linear models. We show O(T 1/2 log T ) regret bounds, which improve in several ways on the existing results. We test the effectiveness of our upper-confidence scheme by contrasting against full-information baselines on real-world multilabel datasets, often obtaining comparable performance.",
    "creator" : "LaTeX with hyperref package"
  }
}