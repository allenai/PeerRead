{
  "name" : "1511.00792.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Sayantan Dasgupta" ],
    "emails" : [ "sayantad@uci.edu" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Recommendation systems came into spotlight with the introduction of Netflix one million challenge. A number of recommendation algorithms have been developed using features extracted from the description or content of the items. However, these content based algorithms typically fail to capture the user opinion on different items. Collaborative filtering, on the other hand, relies on user feedback in some form or the other, and can provide with a better insight of user opinions. Most of the collaborative filtering systems built till date have presumed the availability of explicit feedback from users, either in the form of ratings, or texts, or even through ’like’ tags. This, however, is neither always the case, may nor even be necessary. Moreover, rating provided by a user is usually subjective, and may depend upon the circumstance, or the mood of the user. Incorporating these effects can prove immensely challenging.\nThe selection of different items by a user can reflect his opinion with a great efficacy. The records of these selections are usually available as user logs, and for most of the applications enormous quantities of such logs are available. Examples of such Implicit recommendation systems are music recommendation, personalised search result recommendation or personalised news recommendation; for example, in a personalised news recommender system Das et al. (2007), we always know the articles a user read or liked from his click logs, but will never come to know the articles he did not like (Figure 1). Please note that the feedback available in the form of like or unlike tag is an explicit binary feedback, and must not be confused with implicit feedback systems, where we have only the knowledge of what the users liked before.\nMajority of the literature of recommender systems focus on cases where explicit feedback are available.There have been a few previous attempts to build recommendation systems based on implicit feedback, e.g. in personalized ranking of search results Lin et al. (2005) or personalized news recommendation Das et al. (2007). These recommendation algorithms usually rely on Probabilistic Latent Semantic Algorithm (PLSI) Hofmann (2004). PLSI deploys EM algorithm for training, and therefore suffers from local maxima problem. These recommendation systems often do not give\nar X\niv :1\n51 1.\n00 79\n2v 1\n[ cs\n.L G\n] 3\nN ov\n2 01\n5\noptimal performance. Also, most of the literature on implicit feedback, such as Lin et al. (2005) and Das et al. (2007) are published using proprietary datasets, and lack of publicly available dataset has also been a major impediment for academic contribution in this segment.\nHu et al. (2008) proposes a weighted matrix factorization (WRMF) for implicit feedback recommendation. The algorithm scans through the entire dataset for every iteration until convergence, and it may prove computationally very expensive for large amount of user logs stored across multiple nodes in a distributed ecosystem. Bayesian Personalized Ranking (BPR) inRendle et al. (2009) uses stochastic approach to train from a small batch size, and reduces the computation time significantly. These two methods are shown to outperform others such as similarity or neighbourhood based methods in Rendle et al. (2009), and hence we limit our discussion on BPR and WRMF in this article.\nThere have been recent developments on clustering algorithms based on Method of Moments (MoM), also referred to as Spectral Methods in the literature. Unlike traditional clustering algorithms which rely on EM and similar algorithms to maximize the likelihood of the data, MoM tries to learn the parameters from higher order moments of the data. The method can be proven to be globally convergent using PAC style proof (see Anandkumar et al. (2014)), and has been successfully applied for Hidden Markov Model in Hsu et al. (2012) and Song et al. (2010), for Topic Modeling in Anandkumar et al. (2012), for various Natural Language Processing applications in Cohen et al. (2014), Dhillon et al. (2012) and for Spectral Experts of Linear Regression in Chaganty & Liang (2013).\nHere we propose a generative model for implicit feedback recommendation system based on Method of Moments. We derive the model in the next section, and then show its competitive performance based on a few publicly available dataset which were released very recently."
    }, {
      "heading" : "2 LATENT VARIABLE MODEL FOR METHOD OF MOMENTS",
      "text" : "In this section we outline the generative latent variable model for Method of Moments, and derive the equations to extract the parameters of the model. We also show that the method yields unique model parameters.\nLet us assume that there are U users and D items, and the latent variable h can assume K states. For any user x ∈ {x1, x2 . . . xU} we first choose a latent state of h from the discrete distribution P (h|x), then we choose an item y ∈ {y1, y2 . . . yD} from the discrete distribution P (y|h). The generative process is as follows,\nh ∼ Discrete(P (h|x)) y ∼ Discrete(P (y|h)) (1)\nLet us denote the probability of the latent variable h assuming the state k ∈ 1 . . .K as,\nπk = P (h = k) (2)\nAlgorithm 1 Method of Moments for Parameter Extraction\nInput: Sparse Moments M̂2 ∈ RD×D and M̂3 ∈ RD×D×D Output: P (y|h) and P (h|x)\n1. Compute a singular value decomposition of M̂2 as M̂2 = UΣV >, and estimate the whitening matrix Ŵ = UΣ−1/2 so that Ŵ>M̂2Ŵ = I 2. Multiply M̂3 with Ŵ from all three directions to produce the tensor M̂3(Ŵ , Ŵ , Ŵ ) ∈ RK×K×K . Compute eigenvalues {λk}Kk=1 and eigenvectors {vk}Kk=1 of M̂3(Ŵ , Ŵ , Ŵ ) through power iteration followed by deflation 3. Estimate the columns of O as ˆ̄µk = λkŴ †vk, where Ŵ † = Ŵ (Ŵ>Ŵ ) −1\n, and π̂k = λ−2k , ∀k ∈ 1, 2 . . .K\n4. Assign Ô = [ˆ̄µ1| ˆ̄µ2| . . . | ˆ̄µK ] and π̂ = [π̂1π̂2 . . . π̂K ]> 5. Estimate P (y|h = k) = Ôyk∑ y Ôyk ,∀k ∈ 1 . . .K, y ∈ 1 . . . D,\nP (h = k|x) = π̂k ∏\ny∈Yx Ôyk∑K k=1 π̂k ∏ y∈Yx Ôyk ∀k ∈ 1 . . .K, x ∈ 1 . . . U\nLet us define µ̄k ∈ RD as the probability vector of all the items conditional to the latent state k ∈ 1 . . .K, i.e.\nµ̄k = P (y|h = k) (3)\nLet the matrix O ∈ RD×K denote the conditional probabilities for the items, i.e. Oi,k = P (yi|h = k). Then O = [µ̄1|µ̄2| . . . |µ̄K ]. We assume that the matrix O is of full rank, and the columns of O are fully identifiable. The aim of our algorithm is to estimate the matrix O as well as the vector π.\nFollowing the generative model in equation 1, if we choose three items u, v, w from the list of items associated to any user, then, P (u|h), P (v|h) and P (w|h) are conditionally independent given h. Let us note that P (u = yi), P (v = yi) and P (w = yi) all are synonymous with P [yi], since all of them represents the probability of the item yi appearing in the list of any user in the entire dataset. Therefore, P (u = yi|h = k) = P (yi|h = k) = Oi,k = [µ̄k]i, and the same holds for P (v = yi|h = k) and P (w = yi|h = k). The probability of any item yi appearing in the list of a user is, therefore,\nP [yi] = P (u = yi) = K∑ k=1 P (u = yi|h = k)P (h = k) = K∑ k=1 Oi,kπk ∀i ∈ 1 . . . D (4)\nHence the probability of each individual item across the entire dataset can be expressed as,\nM1 = P [y1,2...D] > = Oπ = [µ̄1|µ̄2| . . . |µ̄K ] · [π1, π2 . . . πk]> = K∑ k=1 πkµ̄k (5)\nThe probability of any two items yi and yj occurring together in the item list of any user can be defined as,\nP [yi, yj ] = P (u = yi, v = yj)\n= K∑ k=1 P (u = yi, v = yj |h = k)P (h = k)\n= K∑ k=1 P (u = yi|h = k)P (v = yj |h = k)πk\n= K∑ k=1 πk[µ̄k]i[µ̄k]j ∀i = 1 . . . D,∀j = 1 . . . D (6)\nDefining M2 as the pairwise probability matrix, with [M2]i,j = P [yi, yj ], we can express it as,\nM2 = K∑ k=1 πkµ̄kµ̄ > k = K∑ k=1 πkµ̄k ⊗ µ̄k (7)\nSimilarly, the probability of any three items yi, yj and yl occurring together in the item list of any user can be defined as,\nP [yi, yj , yl] = P (u = yi, v = yj , w = yl)\n= K∑ k=1 P (u = yi, v = yj , w = yl|h = k)P (h = k)\n= K∑ k=1 P (u = yi|h = k)P (v = yj |h = k)P (w = yl|h = k)P (h = k)\n= K∑ k=1 P (u = yi|h = k)P (v = yj |h = k)P (w = yl|h = k)πk\n= K∑ k=1 πk[µ̄k]i[µ̄k]j [µ̄k]l ∀i = 1 . . . D,∀j = 1 . . . D,∀l = 1 . . . D (8)\nTherefore, the tensor M3 defined as the third order probability moment, with [M3]i,j,l = P [yi, yj , yl], can be represented as,\nM3 = K∑ k=1 πkµ̄k ⊗ µ̄k ⊗ µ̄k (9)\nThe algorithm first computes a matrix W ∈ RD×K for M2, such that W>M2W = I . This process is similar to whitening for independent component analysis (ICA), with the co-variance matrix being replaced by the co-occurrence probability matrix in our case. The whitening is usually done through singular value decomposition. Let us note that,\nW>M2W = W >( K∑\nk=1\nπkµ̄kµ̄ > k\n) W = K∑ k=1 (√ πkW >µ̄k )(√ πkW >µ̄k )> = K∑ k=1 µ̃kµ̃ > k = I (10)\nHence µ̃k = √ πkW\n>µ̄k are orthonormal vectors. Multiplying M3 from all three dimensions by W , we get\nM̃3 = M3(W,W,W ) = K∑ k=1 πk(W >µ̄k)⊗ (W>µ̄k)⊗ (W>µ̄k) = K∑ k=1 1 √ πk µ̃k ⊗ µ̃k ⊗ µ̃k (11)\nHence the robust eigenvectors of M̃3 are µ̃k, whereas the eigenvalues are πk.\nWe use the tensor decomposition algorithm from Anandkumar et al. (2014), which computes the eigen-vectors of M̃3 through power iteration followed by deflation (Algorithm 2). The space complexity of the algorithm isO(K3+DK), whereas time complexity isO((U+D)K3+NK), where N is the total number of < user, item > pairs in the dataset with a non-zero entry. Since the space complexity is independent of N , the algorithm has the capability to scale across a large amount of dataset.\nWe create an estimation of the sparse moments M2 and M3 by counting the occurrence, pairwise occurrence and three way occurrence of the items across the selections made by all the users in the dataset, and normalizing by the total number of occurrence in each case. Let us denote the estimates as M̂2 and M̂3. Then the method follows as Algorithm 1.\nAlgorithm 2 Power Iteration for Eigen-decomposition of a Third Order Symmetric Tensor\nInput: Symmetric Tensor T ∈ RK×K×K , dimension K and tolerance Output: The set of eigenvalues {λk}Kk=1 and eigenvectors {vk}Kk=1 of T for k = 1 to K do\nInitialize θ at random from a unit sphere in RK repeat\nAssign θ0 ← θ Assign θ ← T (·,θ,θ)‖T (·,θ,θ)‖\nuntil ‖θ − θ0‖ < Assign vk ← θ, λk ← T (·,θ,θ)‖T (·,θ,θ)‖ Deflate the tensor as T ← T − λk(vk ⊗ vk ⊗ vk)\nend for\nOnce we have Ô and π̂, the user personalization probabilities P (h = k|x) can be estimated as,\nP (h = k|x) = P (h = k) ∏ y∈Yx P (y|h = k)∑K\nk=1 P (h = k) ∏ y∈Yx P (y|h = k)\n= π̂k ∏ y∈Yx Ôyk∑K\nk=1 π̂k ∏ y∈Yx Ôyk\n(12)\nwhere Yx is the list of items selected by the user x in the training set. The power iterations for computing the eigenvalues of a symmetric tensor is described in Algorithm 2. The product T (·, θ, θ) represents the vector resulting from product of the tensor T with the vector θ along any two axes. Although the dimensions of M2 and M3 are D2 and D3 respectively, in practice, these two quantities are extremely sparse. They can be extracted by one or two passes through the dataset. Once πk and µ̄k are extracted, it requires one more pass through the entire dataset to compute the user probabilities (P (h|x)). Finally, the probability of a user x̃ selecting an item ỹ can be computed as the following equation, and the items with highest probability are usually recommended for the user x̃.\nP (ỹ|x̃) = K∑ k=1 P (ỹ|h = k)P (h = k|x̃) (13)\nCorollary. The parameters {πk}Kk=1 and {µ̄k}Kk=1 are unique given the training set X Proof. The probability column vectors µ̄k = 1√πk ( W> )† µ̃k consist of three terms, W , πk and µ̃k. πk and µ̃k are the eigenvalues and eigenvectors of the tensor M̃3 = M3(W,W,W ). Since M2 andM3 are fixed given the dataset X , πk and µ̃k also depend on W . The whitening matrix W is unique only upto a factor of Q ∈ RK×K , such that Q>Q = I . Let us assume the whitening matrix W whitens M2, then any matrix W ′ = WQ also whitens M2, since\nW ′>M2W ′ = Q> ( W>M2W ) Q = Q>Q = I (14)\nNow, let us assume that we obtain the matrix W ′ as the whitening matrix, then similar to Equation 10,\nW ′>M2W ′ = Q>W> ( K∑ k=1 πkµ̄kµ̄ > k ) WQ\n= K∑ k=1 (√ πkQ >W>µ̄k ) (√ πkQ >W>µ̄k )> =\nK∑ k=1 µ̃′kµ̃ ′> k = I\n(15)\nHence the new orthonormal vectors are µ̃′k, which vary from µ̃k by a factor of Q >. Furthermore,\nM̃ ′3 = M3(WQ,WQ,WQ)\n= K∑ k=1 πk(Q >W>µ̄k)⊗ (Q>W>µ̄k)⊗ (Q>W>µ̄k)\n= K∑ k=1 1 √ πk µ̃′k ⊗ µ̃′k ⊗ µ̃′k\n(16)\nHence, the eigenvalues of M̃ ′3 remain same as those of M̃3, whereas the eigenvectors change to µ̃′k = Q >µ̃k. {µ̄k}Kk=1 can be computed back from {µ̃′k}Kk=1 as,\nµ̄k = 1 √ πk\n( W ′> )† µ̃′k =\n1 √ πk W ′(W ′>W ′)−1µ̃′k (17)\nTherefore,\nµ̄k = 1 √ πk WQ(Q>W>WQ)−1Q>µ̃k = 1 √ πk WQQ−1(W>W )−1Q−>Q>µ̃k\n= 1 √ πk W (W>W )−1µ̃k = 1 √ πk\n( W> )† µ̃k\n(18)\nHence, {µ̄k}Kk=1 are invariant of Q, and are unique as long as the eigenvector decomposition of the tensor M̃3 is robust enough to produce unique eigenvectors. Any matrix W that can whiten M2 is sufficient to extract the parameters of the model.\nSince the parameters {πk}Kk=1 and {µ̄k}Kk=1 are unique, the matrix O is unique, and therefore, from Equation 12, the user personalization parameters P (h = k|x) are also unique. Hence, the parameters extracted by method of moments are unique, and invariant to the whitening process."
    }, {
      "heading" : "3 EXPERIMENTAL RESULTS",
      "text" : "We show the implementation of our model on three publicly available datasets, so that the results can be reproduced whenever necessary. The different attributes of datasets are described in Table 1. Treating the dataset as a User×Item matrix, we can define the density of non-zero elements as,\nDensity = Number of < User, Item > Tuples Number of Users× Number of Items\n(19)\nWe use K = 50 for all the models in our experiments; WRMF runs out of memory for higher values of K. We use the implementation of BPR and WRMF from MyMediaLite library available at http://www.mymedialite.net/ , developed by the authors of Rendle et al. (2009). For every dataset, we compute the Precision@τ , Recall@τ , and Mean Average Precision (MAP@τ ) for τ ∈ {5, 10, 20, 40, 60, 80, 100, 200, 300, 400, 500}. The Precision-Recall curves as well as MAP@τ is shown in Figure 2. We also show the computation time averaged over 10 executions for each method in Figure 2. We carry out our experiments on Unix Platform on a single machine with Intel i5 Processor (2.4GHz) and 8GB memory, and no multi-threading or other performance enhancement technique is used in the code."
    }, {
      "heading" : "3.1 TA-FENG DATASET",
      "text" : "Ta-Feng dataset consists of online grocery purchase records for the months of January, February, November and December in 2001.We combine the records of January and November resulting in a training set consisting of around 24,000 users and 21,000 products, and around 470,000 sales records. The records of February and December are combined to form the test set. BPR achieves the highest MAP of all, but MoM yields the best Precision-Recall curve with slightly worse MAP than BPR. We also compare our result for Precison@5 with that obtained using Unified Boltzman Machine (UBM) in Gunawardana & Meek (2009) in Table 2."
    }, {
      "heading" : "3.2 MILLION SONG DATASET",
      "text" : "Million Song dataset contains the logs of 1 million users listening to 385,000 song tracks with 48 million observations. Here, we use a subset of the data consisting of 100,000 users and around 165,000 song tracks with around 1.45 million observations released in Kaggle. The MoM yields the best MAP, as well as the best Precision-Recall for lower values of τ , and it trains in similar time as BPR.\nWe also compare our result for MAP@500 with those of content based Music Recommendation using Deep Convolutional Neural Network (DCNN) in Van den Oord et al. (2013) for 9330 most popular songs with 20,000 users in Table 3. Our method is far less computationally expensive than Deep Convolutional Neural Network for large datasets."
    }, {
      "heading" : "3.3 YANDEX SEARCH LOG DATASET",
      "text" : "Yandex dataset got released in Kaggle very recently, and it contains the search logs of 27 days for 5.7 million users and 70.3 million URLs. However, only 12 million URLs have at least one click. We have selected a subset of 150,000 users and 624,000 URLs, and used the data of first 14 days as the training set, and the last 13 days as the test set. The WRMF performs best on Yandex dataset, however, it takes around 10x time to train compared to MoM or BPR. MoM, on the other hand, does reasonably well but only takes a fraction of the time taken by WRMF."
    }, {
      "heading" : "4 CONCLUSION AND FUTURE WORKS",
      "text" : "Here we have introduced a generative model based on Method of Moments for implicit feedback recommendations, and established its competitive performance on three datasets. BPR yields the best results for the Ta-Feng dataset, however, its performance degrades severely as the size and the\nsparsity of the data increases. WRMF, on the other hand, performs worse for Ta-Feng dataset, but the performance improves as the dataset becomes larger and sparser. However, its computation time is significantly higher than the other two algorithms, and for the Yandex dataset, it is almost 10x slower compared to BPR or MoM. MoM gives competitive performance for all the datasets, yet taking similar computation time as BPR. The competitive computational aspects of MoM may be even more evident when implemented on larger amounts of data stored across multiple nodes in a distributed ecosystem. Although WRMF is scalable, it scans through the entire dataset for every iteration, and after each iteration it has to synchronize across all the nodes in the distributed system. This results in a very high communication overhead across the nodes. MoM, on the other hand, can extract the moments M2 and M3 in one or two passes through the entire dataset. Although certain parts of our algorithm, such as whitening of M2, may require parallelization, it can be parallelized across much smaller number of nodes than required for the storage of the entire dataset. We wish to perform a competitive study of these algorithms implemented in distributed ecosystems in future.\nThere has been efforts to model Bayesian Non-parametrics using Method of Moments; Tung & Smola (2014) suggests a spectral method to draw inference in Indian Buffet Process. However, it does not highlight the scalability of the method. We also want to incorporate Bayesian Nonparametrics in Recommendation algorithms using Method of Moments in future. Also, we want to extend Method of Moments for rating prediction in explicit feedback recommendation systems."
    } ],
    "references" : [ {
      "title" : "A spectral algorithm for latent dirichlet allocation",
      "author" : [ "Anandkumar", "Anima", "Liu", "Yi-kai", "Hsu", "Daniel J", "Foster", "Dean P", "Kakade", "Sham M" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Anandkumar et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Anandkumar et al\\.",
      "year" : 2012
    }, {
      "title" : "Tensor decompositions for learning latent variable models",
      "author" : [ "Anandkumar", "Animashree", "Ge", "Rong", "Hsu", "Daniel", "Kakade", "Sham M", "Telgarsky", "Matus" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Anandkumar et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Anandkumar et al\\.",
      "year" : 2014
    }, {
      "title" : "Spectral experts for estimating mixtures of linear regressions",
      "author" : [ "Chaganty", "Arun Tejasvi", "Liang", "Percy" ],
      "venue" : "arXiv preprint arXiv:1306.3729,",
      "citeRegEx" : "Chaganty et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Chaganty et al\\.",
      "year" : 2013
    }, {
      "title" : "Spectral learning of latent-variable pcfgs: algorithms and sample complexity",
      "author" : [ "Cohen", "Shay B", "Stratos", "Karl", "Collins", "Michael", "Foster", "Dean P", "Ungar", "Lyle H" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Cohen et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cohen et al\\.",
      "year" : 2014
    }, {
      "title" : "Google news personalization: scalable online collaborative filtering",
      "author" : [ "Das", "Abhinandan S", "Datar", "Mayur", "Garg", "Ashutosh", "Rajaram", "Shyam" ],
      "venue" : "In Proceedings of the 16th international conference on World Wide Web,",
      "citeRegEx" : "Das et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Das et al\\.",
      "year" : 2007
    }, {
      "title" : "Spectral dependency parsing with latent variables",
      "author" : [ "Dhillon", "Paramveer S", "Rodu", "Jordan", "Collins", "Michael", "Foster", "Dean P", "Ungar", "Lyle H" ],
      "venue" : "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,",
      "citeRegEx" : "Dhillon et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Dhillon et al\\.",
      "year" : 2012
    }, {
      "title" : "A unified approach to building hybrid recommender systems",
      "author" : [ "Gunawardana", "Asela", "Meek", "Christopher" ],
      "venue" : "In Proceedings of the Third ACM Conference on Recommender Systems,",
      "citeRegEx" : "Gunawardana et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Gunawardana et al\\.",
      "year" : 2009
    }, {
      "title" : "Latent semantic models for collaborative filtering",
      "author" : [ "Hofmann", "Thomas" ],
      "venue" : "ACM Trans. Inf. Syst.,",
      "citeRegEx" : "Hofmann and Thomas.,? \\Q2004\\E",
      "shortCiteRegEx" : "Hofmann and Thomas.",
      "year" : 2004
    }, {
      "title" : "A spectral algorithm for learning hidden markov models",
      "author" : [ "Hsu", "Daniel", "Kakade", "Sham M", "Zhang", "Tong" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Hsu et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hsu et al\\.",
      "year" : 2012
    }, {
      "title" : "Collaborative filtering for implicit feedback datasets",
      "author" : [ "Hu", "Yifan", "Koren", "Yehuda", "Volinsky", "Chris" ],
      "venue" : "In Proceedings of the 2008 Eighth IEEE International Conference on Data Mining,",
      "citeRegEx" : "Hu et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2008
    }, {
      "title" : "Shifted power method for computing tensor eigenpairs",
      "author" : [ "Kolda", "Tamara G", "Mayo", "Jackson R" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications,",
      "citeRegEx" : "Kolda et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Kolda et al\\.",
      "year" : 2011
    }, {
      "title" : "Using probabilistic latent semantic analysis for personalized web search",
      "author" : [ "Lin", "Chenxi", "Xue", "Gui-Rong", "Zeng", "Hua-Jun", "Yu", "Yong" ],
      "venue" : "In Web Technologies Research and Development-APWeb",
      "citeRegEx" : "Lin et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2005
    }, {
      "title" : "Bpr: Bayesian personalized ranking from implicit feedback",
      "author" : [ "Rendle", "Steffen", "Freudenthaler", "Christoph", "Gantner", "Zeno", "Schmidt-Thieme", "Lars" ],
      "venue" : "In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Rendle et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Rendle et al\\.",
      "year" : 2009
    }, {
      "title" : "Hilbert space embeddings of hidden markov models",
      "author" : [ "Song", "Le", "Boots", "Byron", "Siddiqi", "Sajid M", "Gordon", "Geoffrey J", "Smola", "Alex J" ],
      "venue" : "In Proceedings of the 27th international conference on machine learning",
      "citeRegEx" : "Song et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2010
    }, {
      "title" : "Spectral methods for indian buffet process inference",
      "author" : [ "Tung", "Hsiao-Yu", "Smola", "Alex J" ],
      "venue" : "In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems",
      "citeRegEx" : "Tung et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Tung et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep content-based music recommendation",
      "author" : [ "Van den Oord", "Aaron", "Dieleman", "Sander", "Schrauwen", "Benjamin" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Oord et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Oord et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Examples of such Implicit recommendation systems are music recommendation, personalised search result recommendation or personalised news recommendation; for example, in a personalised news recommender system Das et al. (2007), we always know the articles a user read or liked from his click logs, but will never come to know the articles he did not like (Figure 1).",
      "startOffset" : 209,
      "endOffset" : 227
    }, {
      "referenceID" : 4,
      "context" : "Examples of such Implicit recommendation systems are music recommendation, personalised search result recommendation or personalised news recommendation; for example, in a personalised news recommender system Das et al. (2007), we always know the articles a user read or liked from his click logs, but will never come to know the articles he did not like (Figure 1). Please note that the feedback available in the form of like or unlike tag is an explicit binary feedback, and must not be confused with implicit feedback systems, where we have only the knowledge of what the users liked before. Majority of the literature of recommender systems focus on cases where explicit feedback are available.There have been a few previous attempts to build recommendation systems based on implicit feedback, e.g. in personalized ranking of search results Lin et al. (2005) or personalized news recommendation Das et al.",
      "startOffset" : 209,
      "endOffset" : 863
    }, {
      "referenceID" : 4,
      "context" : "Examples of such Implicit recommendation systems are music recommendation, personalised search result recommendation or personalised news recommendation; for example, in a personalised news recommender system Das et al. (2007), we always know the articles a user read or liked from his click logs, but will never come to know the articles he did not like (Figure 1). Please note that the feedback available in the form of like or unlike tag is an explicit binary feedback, and must not be confused with implicit feedback systems, where we have only the knowledge of what the users liked before. Majority of the literature of recommender systems focus on cases where explicit feedback are available.There have been a few previous attempts to build recommendation systems based on implicit feedback, e.g. in personalized ranking of search results Lin et al. (2005) or personalized news recommendation Das et al. (2007). These recommendation algorithms usually rely on Probabilistic Latent Semantic Algorithm (PLSI) Hofmann (2004).",
      "startOffset" : 209,
      "endOffset" : 917
    }, {
      "referenceID" : 4,
      "context" : "Examples of such Implicit recommendation systems are music recommendation, personalised search result recommendation or personalised news recommendation; for example, in a personalised news recommender system Das et al. (2007), we always know the articles a user read or liked from his click logs, but will never come to know the articles he did not like (Figure 1). Please note that the feedback available in the form of like or unlike tag is an explicit binary feedback, and must not be confused with implicit feedback systems, where we have only the knowledge of what the users liked before. Majority of the literature of recommender systems focus on cases where explicit feedback are available.There have been a few previous attempts to build recommendation systems based on implicit feedback, e.g. in personalized ranking of search results Lin et al. (2005) or personalized news recommendation Das et al. (2007). These recommendation algorithms usually rely on Probabilistic Latent Semantic Algorithm (PLSI) Hofmann (2004). PLSI deploys EM algorithm for training, and therefore suffers from local maxima problem.",
      "startOffset" : 209,
      "endOffset" : 1028
    }, {
      "referenceID" : 4,
      "context" : "Also, most of the literature on implicit feedback, such as Lin et al. (2005) and Das et al.",
      "startOffset" : 59,
      "endOffset" : 77
    }, {
      "referenceID" : 1,
      "context" : "(2005) and Das et al. (2007) are published using proprietary datasets, and lack of publicly available dataset has also been a major impediment for academic contribution in this segment.",
      "startOffset" : 11,
      "endOffset" : 29
    }, {
      "referenceID" : 1,
      "context" : "(2005) and Das et al. (2007) are published using proprietary datasets, and lack of publicly available dataset has also been a major impediment for academic contribution in this segment. Hu et al. (2008) proposes a weighted matrix factorization (WRMF) for implicit feedback recommendation.",
      "startOffset" : 11,
      "endOffset" : 203
    }, {
      "referenceID" : 1,
      "context" : "(2005) and Das et al. (2007) are published using proprietary datasets, and lack of publicly available dataset has also been a major impediment for academic contribution in this segment. Hu et al. (2008) proposes a weighted matrix factorization (WRMF) for implicit feedback recommendation. The algorithm scans through the entire dataset for every iteration until convergence, and it may prove computationally very expensive for large amount of user logs stored across multiple nodes in a distributed ecosystem. Bayesian Personalized Ranking (BPR) inRendle et al. (2009) uses stochastic approach to train from a small batch size, and reduces the computation time significantly.",
      "startOffset" : 11,
      "endOffset" : 569
    }, {
      "referenceID" : 1,
      "context" : "(2005) and Das et al. (2007) are published using proprietary datasets, and lack of publicly available dataset has also been a major impediment for academic contribution in this segment. Hu et al. (2008) proposes a weighted matrix factorization (WRMF) for implicit feedback recommendation. The algorithm scans through the entire dataset for every iteration until convergence, and it may prove computationally very expensive for large amount of user logs stored across multiple nodes in a distributed ecosystem. Bayesian Personalized Ranking (BPR) inRendle et al. (2009) uses stochastic approach to train from a small batch size, and reduces the computation time significantly. These two methods are shown to outperform others such as similarity or neighbourhood based methods in Rendle et al. (2009), and hence we limit our discussion on BPR and WRMF in this article.",
      "startOffset" : 11,
      "endOffset" : 799
    }, {
      "referenceID" : 0,
      "context" : "The method can be proven to be globally convergent using PAC style proof (see Anandkumar et al. (2014)), and has been successfully applied for Hidden Markov Model in Hsu et al.",
      "startOffset" : 78,
      "endOffset" : 103
    }, {
      "referenceID" : 0,
      "context" : "The method can be proven to be globally convergent using PAC style proof (see Anandkumar et al. (2014)), and has been successfully applied for Hidden Markov Model in Hsu et al. (2012) and Song et al.",
      "startOffset" : 78,
      "endOffset" : 184
    }, {
      "referenceID" : 0,
      "context" : "The method can be proven to be globally convergent using PAC style proof (see Anandkumar et al. (2014)), and has been successfully applied for Hidden Markov Model in Hsu et al. (2012) and Song et al. (2010), for Topic Modeling in Anandkumar et al.",
      "startOffset" : 78,
      "endOffset" : 207
    }, {
      "referenceID" : 0,
      "context" : "The method can be proven to be globally convergent using PAC style proof (see Anandkumar et al. (2014)), and has been successfully applied for Hidden Markov Model in Hsu et al. (2012) and Song et al. (2010), for Topic Modeling in Anandkumar et al. (2012), for various Natural Language Processing applications in Cohen et al.",
      "startOffset" : 78,
      "endOffset" : 255
    }, {
      "referenceID" : 0,
      "context" : "The method can be proven to be globally convergent using PAC style proof (see Anandkumar et al. (2014)), and has been successfully applied for Hidden Markov Model in Hsu et al. (2012) and Song et al. (2010), for Topic Modeling in Anandkumar et al. (2012), for various Natural Language Processing applications in Cohen et al. (2014), Dhillon et al.",
      "startOffset" : 78,
      "endOffset" : 332
    }, {
      "referenceID" : 0,
      "context" : "The method can be proven to be globally convergent using PAC style proof (see Anandkumar et al. (2014)), and has been successfully applied for Hidden Markov Model in Hsu et al. (2012) and Song et al. (2010), for Topic Modeling in Anandkumar et al. (2012), for various Natural Language Processing applications in Cohen et al. (2014), Dhillon et al. (2012) and for Spectral Experts of Linear Regression in Chaganty & Liang (2013).",
      "startOffset" : 78,
      "endOffset" : 355
    }, {
      "referenceID" : 0,
      "context" : "The method can be proven to be globally convergent using PAC style proof (see Anandkumar et al. (2014)), and has been successfully applied for Hidden Markov Model in Hsu et al. (2012) and Song et al. (2010), for Topic Modeling in Anandkumar et al. (2012), for various Natural Language Processing applications in Cohen et al. (2014), Dhillon et al. (2012) and for Spectral Experts of Linear Regression in Chaganty & Liang (2013). Here we propose a generative model for implicit feedback recommendation system based on Method of Moments.",
      "startOffset" : 78,
      "endOffset" : 428
    }, {
      "referenceID" : 0,
      "context" : "We use the tensor decomposition algorithm from Anandkumar et al. (2014), which computes the eigen-vectors of M̃3 through power iteration followed by deflation (Algorithm 2).",
      "startOffset" : 47,
      "endOffset" : 72
    }, {
      "referenceID" : 12,
      "context" : "net/ , developed by the authors of Rendle et al. (2009). For every dataset, we compute the Precision@τ , Recall@τ , and Mean Average Precision (MAP@τ ) for τ ∈ {5, 10, 20, 40, 60, 80, 100, 200, 300, 400, 500}.",
      "startOffset" : 35,
      "endOffset" : 56
    }, {
      "referenceID" : 12,
      "context" : "net/ , developed by the authors of Rendle et al. (2009). For every dataset, we compute the Precision@τ , Recall@τ , and Mean Average Precision (MAP@τ ) for τ ∈ {5, 10, 20, 40, 60, 80, 100, 200, 300, 400, 500}. The Precision-Recall curves as well as MAP@τ is shown in Figure 2. We also show the computation time averaged over 10 executions for each method in Figure 2. We carry out our experiments on Unix Platform on a single machine with Intel i5 Processor (2.4GHz) and 8GB memory, and no multi-threading or other performance enhancement technique is used in the code. 3.1 TA-FENG DATASET Ta-Feng dataset consists of online grocery purchase records for the months of January, February, November and December in 2001.We combine the records of January and November resulting in a training set consisting of around 24,000 users and 21,000 products, and around 470,000 sales records. The records of February and December are combined to form the test set. BPR achieves the highest MAP of all, but MoM yields the best Precision-Recall curve with slightly worse MAP than BPR. We also compare our result for Precison@5 with that obtained using Unified Boltzman Machine (UBM) in Gunawardana & Meek (2009) in Table 2.",
      "startOffset" : 35,
      "endOffset" : 1198
    }, {
      "referenceID" : 15,
      "context" : "We also compare our result for MAP@500 with those of content based Music Recommendation using Deep Convolutional Neural Network (DCNN) in Van den Oord et al. (2013) for 9330 most popular songs with 20,000 users in Table 3.",
      "startOffset" : 146,
      "endOffset" : 165
    } ],
    "year" : 2017,
    "abstractText" : "Building recommendation algorithms is one of the most challenging tasks in Machine Learning. Although there has been significant progress in building recommendation systems when explicit feedback is available from the users in the form of rating or text, most of the applications do not receive such feedback. Here we consider the recommendation task where the available data is the record of the items selected by different users over time for subscription or purchase. This is known as implicit feedback recommendation. Such data are usually available as large amount of user logs stored over massively distributed storage systems such as Hadoop. Therefore it is essential to have a highly scalable algorithm to build a recommender system for such applications. Here we propose a probabilistic algorithm that takes only two to three passes through the entire dataset to extract the model parameters during the training phase. We demonstrate the competitive performance of our algorithm in several empirical measures as well as the computation time in comparison with the existing algorithms on various publicly available datasets.",
    "creator" : "LaTeX with hyperref package"
  }
}