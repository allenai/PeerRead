{
  "name" : "1206.6411.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "On the Difficulty of Nearest Neighbor Search",
    "authors" : [ "Junfeng He", "Sanjiv Kumar", "Shih-Fu Chang" ],
    "emails" : [ "jh2700@columbia.edu", "sanjivk@google.com", "sfchang@ee.columbia.edu" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Finding nearest neighbors is a key step in many machine learning algorithms such as spectral clustering, manifold learning and semi-supervised learning. Appearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nRapidly increasing data in many domains such as the Web is posing new challenges on how to efficiently retrieve nearest neighbors of a query from massive databases. Fortunately, in most applications, it is sufficient to return approximate nearest neighbors of a query, which allows efficeint scalable search.\nA large number of approximate Nearest Neighbor (NN) search techniques have been proposed in the last decade including hashing and tree-based methods, to name a few, (Datar et al., 2004; Liu et al., 2004; Weiss et al., 2008). However, the performance of all these techniques depends heavily on the data set characteristics. In fact, as a fundamental question, one would like to know how difficult is (approximate) NN search in a given data set. And more broadly, which data characteristics of the dataset affect the ”difficulty” and how? The term ”difficulty” here has two different but related meanings: in the context of NN search problem (independent of indexing methods), ”difficulty” represents ”meaningfulness”, i.e., for a query, how differentiable is its NN point compared to other points? In the context of approximate NN search methods like tree or hashing based indexing methods, ”difficulty” represents ”complexity”, i.e., what is the time and space complexity to guarantee to find the NN point (with a high probability)? These questions have not been paid much attention in the literature.\nIn terms of ”meaningfulness” of NN search problem in a given dataset, most of the existing works have focused on the effect of one data property: dimensionality, that too in an asymptotic sense, showing that NN search will be meaningless when the number of dimensions goes to infinity (Beyer et al., 1999; Aggarwal et al., 2001; Francois et al., 2007). First, non-asymptotic analysis has not been discussed, i.e., when the number of dimensions is finite. Moreover, the effect of other crucial properties has not been stud-\nied, for instance, the sparsity of data vectors. Since in many applications, high-dimensional vectors tend to be sparse, it is important to study the two data properties e.g., dimensionality and sparsity together, along with other factors such as database size and distance metric.\nIn terms of the complexity of approximate NN search methods like Locality Sensitive Hashing (LSH), some general bounds for (Gionis et al., 1999; Indyk & Motwani, 1998) have been presented. However, it has not been studied how the complexity of approximate NN search methods is affected by the difficulty of NN search problem on the dataset, and moreover, by various data properties like dimension, sparsity, etc.\nThe main contributions of this paper are: 1. We introduces a new concrete measure Relative Contrast for the meaningfulness/difficulty of nearest neighbor search problem in a given data set (independent of indexing methods). Unlike previous works that only provide asymptotic discussions for one or two data properties, we derive an explicitly computable function to estimate relative contrast in non-asymptotic case. It for the first time enables us to analyze how the difficulty of nearest neighbor search is affected by different data properties simultaneously, such as dimensionality, sparsity, database size, along with the norm of Lp distance metric , for a given data set. (Sec. 2)\n2. We provide a theoretical analysis on how the difficulty measure ”relative contrast” determines the complexity of LSH, a popular approximate NN search method. This is the first work to relate the complexity of approximate NN search methods to the difficulty measure of a given dataset, allowing us to analyze how the complexity is affected by various data properties simultaneously. For practitioners’ benefits, relative contrast also provides insights on how to choose parameters e.g., the number of hash tables of LSH, and a principled explanation of why PCA-based methods perform well in practice. (Sec. 3)\n3. We reveal the relationship between relative contrast and previous studies on measuring NN search difficulty, and show that most existing works can be derived as special asymptotic cases for dense vectors of the proposed relative contrast. (Sec. 4)\n2. Relative Contrast (Cr)\nSuppose we are given a data set X containing n ddim points, X = {xi, i = 1, . . . , n}, and a query q\nwhere xi, q ∈ Rd are i.i.d samples from an unknown distribution p(x). Further, let D(·, ·) be the distance function for the d-dimensional data. We focus on Lp distances in this paper: D(x, q)=( ∑\nj |xj−qj |p)1/p."
    }, {
      "heading" : "2.1. Definition",
      "text" : "Suppose Dqmin = min i=1,...n D(xi, q) is the distance to the nearest database sample1, and Dqmean = Ex[D(x, q)] is the expected distance of a random database sample from the query q. We define the relative contrast for the data set X for a query q as : Cqr = Dqmean Dq\nmin . It is\na very intuitive measure of separability of the nearest neighbor of q from the rest of the database points. Now, taking expectations with respect to queries, the relative contrast for the dataset X is given as,\nCr = Eq[D\nq mean]\nEq[D q min] = Dmean Dmin\n(1)\nIntuitively, Cr captures the notion of difficulty of NN search in X. Smaller the Cr, more difficult the search. If Cr is close to 1, then on average a query q will have almost the same distance to its nearest neighbor as that to a random point in X. This will imply that NN search in database X is not very meaningful.\nIn the following sections, we derive relative contrast as a function of various important data characteristics."
    }, {
      "heading" : "2.2. Estimation",
      "text" : "Suppose xj and qj are the jth dimensions of vectors x and q. Let’s define,\nRj = Eq[|xj − qj |p], R = d ∑\nj=1\nRj . (2)\nBoth Rj and R are random variables (because x j is a random variable). Suppose each Rj has finite mean and variance denoted as µj = E[Rj ], σ 2 j = var[Rj ]. Then, the mean and variance of R are given as,\nµ =\nd ∑\nj=1\nµj , σ 2 ≤\nd ∑\nj=1\nσ2j .\nHere, if dimensions are independent then σ2 = ∑ j σ 2 j . Without the loss of generality, we can scale the data such that the new mean µ′ is 1. The variance of the scaled data, called normalized variance will be:\nσ′ 2 =\nσ2 µ2 . (3)\n1Without loss of generality, we assume that the query is distinct from the database samples, i.e., Dqmin 6= 0.\nThe normalized variance gives the spread of the distances from query to random points in the database with the mean distance fixed at 1. If the spread is small, it is harder to separate the nearest neighbor from the rest of the points. Next, we estimate the relative contrast for a given dataset as follows.\nTheorem 2.1 If {Rj, j=1,...d} are independent and satisfy Lindeberg’s condition2, the relative contrast can be approximated as,\nCr = Dmean Dmin ≈ 1 [1 + φ−1( 1n + φ( −1 σ′ ))σ ′] 1 p\n(4)\nwhere φ is the c.d.f of standard Gaussian, n is the number of database samples, σ′ is normalized standard deviation, and p is the distance metric norm.\nProof: Since Rj are independent and satisfy Lindeberg’s condition, from central limit theorem, R will be distributed as Gaussian for large enough d with mean µ = ∑\nj µj and variance σ 2 =\n∑\nj σ 2 j . Normalizing the\ndata by dividing by µ, the new mean is µ′ = 1, and new variance is σ′2 as defined in (3). Now, the probability that R ≤ α for any 0 ≤ α ≤ 1 is given as\nP (R ≤ α) ≈ φ(α− 1 σ′ )− φ(0− 1 σ′ ), (5)\nwhere φ is the c.d.f of standard Gaussian, and the second term in RHS is the correction factor since R is always nonnegative.\nLet’s denote the number of samples for which R ≤ α as N(α). Clearly, N(α) follows Binomial distribution with probability of success given in (5):\nP (N(α) = k) =\n(\nn k\n)\n(P (R ≤ α))k(1−P (R ≤ α))n−k.\nHence the expected number of database points, N̄(α) that satisfy R ≤ α can be computed as\nN̄(α)=E[N(α)]=nP (R ≤ α) = n(φ(α− 1 σ′ )−φ(−1 σ′ )).\nRecall Dmin is the expected distance to the nearest neighbor and Rmin ≈ Dpmin.3 Thus, N̄(D p min) ≈ N̄(Rmin) = 1. Hence,\nDmin ≈ (N̄−1(1)) 1 p ≈ [1 + φ−1( 1 n + φ( −1 σ′ ))σ′] 1 p (6)\n2Lindeberg’s condition is a sufficient condition for central limit theorem to be applicable even when variables are not identically distributed. Intuitively speaking, the Linderberg condition guarantees that no Rj dominates R.\n3The approximation becomes exact when metric L1 is considered. For other norms (e.g., p = 2), bounds on Dmin can be further derived.\nMoreover, after normalization, R follows a Gaussian distribution with mean 1. So, Rmean = 1, and Dmean ≈ R 1 p mean = 1. Thus, the relative contrast can be approximated as:\nCr = Dmean Dmin ≈ 1 [1 + φ−1( 1n + φ( −1 σ′ ))σ ′] 1 p\nwhich completes the proof.\nRange of Cr: Note that when n is large enough φ(−1σ′ ) ≤ 1n+φ(−1σ′ ) ≤ 12 , so 0 ≤ 1+φ−1( 1n+φ(−1σ′ ))σ′ ≤ 1 and hence Cr is always ≥ 1. And moreover, when σ′ → 0, φ(−1σ′ ) → 0, and Cr → 1. Generalization 1: The concept of relative contrast can be extended easily to the k-nearest neighbor setting by defining Ckr = Dmean Dknn , where Dknn is the expected distance to the kth nearest neighbor. Using N̄(Dpknn) ≈ N̄(Rknn) = k, and following similar arguments as above, one can easily show that\nCkr = Dmean Dknn ≈ 1 [1 + φ−1( kn + φ( −1 σ′ ))σ ′] 1 p\n(7)"
    }, {
      "heading" : "2.3. Effect of normalized variance σ′ on Cr",
      "text" : "From (4), relative contrast is a function of database size n, normalized variance σ′2, and distance metric norm p. Here, σ′ is a function of data characteristics such as dimensionality and sparsity. Figure 1 shows how Cr changes with σ\n′ according to (4) when n is varied from 100 to 100M , and 0 < σ′ < 0.2 (Note that σ′ is usually very small for high dimensional data, e.g., far smaller than 0.1). It is clear that smaller σ′ leads to smaller relative contrast, i.e., more difficult nearest neighbor search.\nIn the above plots, p was fixed to be 1 but other values yield similar results. An interesting thing to note is that as the database size n increases, relative contrast increases. In other words, nearest neighbor search is more meaningful for a larger database.4 However, this effect is not very pronounced for smaller values of σ′."
    }, {
      "heading" : "2.4. Data Properties vs σ′",
      "text" : "Since we already know the relationship between Cr and σ′, by analyzing how data properties affect σ′, we will find out how data properties affect Cr, i.e., the difficulty of NN search. Though many data properties can be studied, in this work we focus on sparsity\n4It should not be confused with computational ease since computationally search costs more in larger databases.\n(a very important property in many domains involving, say, text, images and videos), together with other properties like data dimension and metric.\nSuppose, the jth dimensions of vectors x and q are distributed the same way as a random variable Vj . But each dimension has only sj probability of having a non-zero value where 0 < sj ≤ 1. Denote mj,p as the p-th moment of |Vj |, and m′j,p as the p-th moment of |Vj1 − Vj2|, where Vj1 and Vj2 are independently distributed as Vj .\nTheorem 2.2 If dimensions are independent,\nσ′2 =\n∑d\nj=1 s2jm ′ j,2p+2(1−sj)sjmj,2p−µ2j ( ∑d\nj=1 µj)2\nwhere µj = s 2 jm ′ j,p + 2(1 − sj)sjmj,p. Moreover, if dimensions are i.i.d.,\nσ′ = 1\nd1/2\n√\ns[(m′2p − 2m2p)s+ 2m2p] s2[(m′p − 2mp)s+ 2mp]2 − 1. (8)\nProof: Please see the supplementary material (He, 2012).\nFor some distributions, mp and m ′ p have a closed form representation. For example, if every dimension follows uniform distribution U(0, 1), then pth moment is quite easy in this case: mp = 1 (p+1) ,m ′ p = 2 p+1 − 2p+2 . However, if mp and m ′ p do not have a closed form representation, one can always generate samples according to the distribution, and estimate mp and m ′ p empirically."
    }, {
      "heading" : "2.5. Data Properties vs Relative Contrast Cr",
      "text" : "We now summarize how different database properties and distance metric affect relative contrast.\nData Dimensionality (d): From (8), it is easy to see that larger d will lead to smaller σ′. Moreover, from (4), smaller σ′ implies smaller relative contrast Cr, making NN search less meaningful. This indicates the\nwell-known phenomenon of distance concentration in high dimensional spaces. However, when dimensions are not independent, thankfully, the rate at which distances start concentrating slows down.\nData Sparsity (s): From (8), we can see that σ′ =\n1 d1/2\n√\n(m′ 2p−2m2p)+ 2m2p s\n[(m′p−2mp)s+2mp]2 − 1. If m′p − 2mp ≥ 0, when s\nbecomes smaller (i.e., data vectors have fewer non-zero elements), σ′ gets larger, and so does the relative contrast. Another interesting case is when p → 0+, i.e., L0 or zero-one distance. In this case, mp = m ′ p = 1, and from (8) σ′ = 1 d1/2 √ (1−s)2 1−(1−s)2 , which increases monotonically as s decreases. However, for general cases, it is not easy to theoretically prove how σ′ will change when s gets smaller. But in experiments, we have always found that smaller s will lead to larger σ′. In other words, when data vectors become more sparse, NN search becomes easier. That raises another interesting question: What is the effective dimensionality of sparse vectors? One may be tempted to use d · s as the intrinsic dimensionality. But as we will show in the experimental section, this is generally not the case and relative contrast provides an empirical approach to finding intrinsic dimensionality of high-dimensional sparse vectors.\nDatabase Size (n): From (4), keeping σ′ fixed, Cr increases monotonically with n. Hence, NN search is more meaningful in larger databases. Actually, when n→∞, irrespective of σ′, 1+φ−1( 1n +φ(−1σ′ ))σ′ → 0, and Cr → ∞. Thus, when the database size is large enough, one doesn’t need to worry about the meaningfulness of NN search irrespective of the dimensionality. However, unfortunately when dimensionality is high, Cr increases very slowly with n, making the gains not very pronounced in practice. This is the same phenomenon noticed in Fig. 1 for small values of σ′.\nDistance Metric Norm (p): Since p appears in both (4) and (8), it makes analysis of relative contrast with respect to p not as straightforward. In the special case when data vectors are dense (i.e., s = 1), and each dimension is i.i.d with uniform distribution, one can show that smaller p leads to bigger contrast."
    }, {
      "heading" : "2.6. Validation of Relative Contrast",
      "text" : "To verify the form of relative contrast derived in Sec. 2, we conducted experiments with both synthetic and real-world datasets, which are summarized below."
    }, {
      "heading" : "2.6.1. Synthetic Data",
      "text" : "We generated synthetic data by assuming each dimension to be i.i.d from uniform distribution U [0, 1]. Fig.\n0 500 1000 1500 2000 1\n1.5\n2\n2.5\ndimension d\nC on\ntr as\nt c r\ns=0.5,p=1,Empirical s=0.5,p=1,Predicted s=1,p=1,Empirical s=1,p=1,Predicted\n0 0.5 1 1\n2\n3\n4\nsparsity s\nC on\ntr as\nt c r\nd=500,p=1,Empirical d=500,p=1,Predicted d=1000,p=1,Empirical d=1000,p=1,Predicted\n(a) (b)\n0 1 2 3 4 1\n2\n3\n4\n5\n6\nL p\nC on\ntr as\nt c r\nd = 60,s=0.5,Empirical d = 60,s=0.5,Predicted\n1000 3000 10000 30000 100000 1\n1.5\n2\n2.5\ndatabase size n\nC on\ntr as\nt c r\nd = 30,s=1,p=1,Empirical d = 30,s=1,p=1,Predicted d = 60,s=1,p=1,Empirical d = 60,s=1,p=1,Predicted\n(c) (d)\nFigure 2. Experiments with synthetic data on how relative contrast changes with different database characteristics. Graphs are best viewed with color.\n2 compares the predicted (theoretical) relative contrast with the empirical one. The solid curves show the predicted contrast computed using (4), where the normalized variance σ′ is estimated using (8). The dotted curves show the empirical contrast, directly computed according to the definition in (1) from the data by averaging the results over one hundred queries. For most of the cases, the predicted and empirical contrasts have similar values.\nFig. 2 (a) confirms that as dimensionality increases, relative contrast decreases, thus making the nearest neighbor search harder. Moreover, except for very small d, the prediction is close to the empirical contrast verifying the theory. It is not surprising that predictions are not very accurate for small d since the central limit theorem(CLT) is not applicable in that case. It is interesting to note that (4) also predicts the rate at which contrast changes with d, unlike the previous works (Beyer et al., 1999; Aggarwal et al., 2001) which only show that NN search becomes impossible when dimensionality goes to infinity.\nFig. 2 (b) shows how data sparsity affects the contrast for two different choices of d. The main observation is that as s increases (denser vectors), contrast decreases, making nearest neighbor search harder. In other words, lesser the number of non-zero dimensions for a fixed d, easier the search. In fact, the search remains well-behaved even in high-dimensional datasets if data is sparse. The prediction is quite accurate in comparison to the empirical one except when s.d is small and hence CLT does not apply any more. As a note of caution, one should not regard s.d as the intrinsic dimensionality of the data, since a dataset with dense vectors of dimension s.d usually has different\ncontrast than the d-dim s-sparse data set.\nThe effects of two other characteristics i.e., Lp distance metric for different p and database size n are shown in Figs. 2 (c) and (d), respectively. The effect of these parameters on relative contrast is milder than that of d and s. For large d, the contrast drops quickly and it becomes hard to visualize the effects of p and n. So, here we show these plots for smaller values of d. From Fig. 2 (c) it is clear that for norms less than 1, contrast is the highest (Note that we have an approximation for p > 1 in Theorem 2.1, which causes the bias of predicted Cr for p = 3, 4). This observation matches the conclusion from (Aggarwal et al., 2001) for dense vectors. Fig. 2 (d) shows that as the database size increases, it becomes more meaningful to do nearest neighbor search. But as the dimensionality is increased (from 30 to 60 in the plot), the rate of increase of contrast with n decreases. For very high dimensional data, the effect of n is very small."
    }, {
      "heading" : "2.6.2. Real-world Data",
      "text" : "Next, we conducted experiments with four real-world datasets commonly used in computer vision applications: sift, gist, color and image. The details of these sets are given in Table 1. The sift and gist sets contain 128-dim and 384-dim vectors, which are mostly dense. On the other hand, both color and image datasets are very high dimensional as well as sparse. Color data set contains color histogram of images while the image data set contains bag-of-words representation of local features in images.\nWhile deriving the form of relative contrast in Sec. 2, we assumed that dimensions were independent. However, this assumption may not be true for real-world data. One way to adress this problem would be to assume that the dimensions become independent after embedding the data in an appropriate low-dimensional space. In these experiments, we define effective dimensionality de as the number of dimensions necessary to preserve 85% variance of the data5. The effective dimensionality for different datasets is shown in Table\n5For large databases, one can use a small subset to estimate the covariance matrix.\n1. Table 2 compares the empirical and predicted relative contrasts for different datasets. Since our theory is based on the law of large numbers, the prediction is more accurate on image and gist data sets as their effective dimensions are large enough. For the color data, de is too small (just 22) and hence the prediction of relative contrast shows more bias for this set.\nOne interesting outcome of these experiments is that our analysis provides an alternative way of finding intrinsic dimensionality of the data which can be further used by various nearest neighbor search methods. The traditional method of finding intrinsic dimensionality using data variance suffers from the assumption of linearity of the low-dimensional space and the arbitrary choice of threshold on variance. On the other hand, nonlinear methods are computationally prohibitive for large datasets. In the relative contrast based method, for a given dataset, one can sweep over different values of d′ where 0 < d′ < d, and find the one which gives the least discrepancy between the predicted and empirical contrasts averaged over different p. For large datasets, one can use a smaller sample and a few queries to estimate the empirical contrast. Using this procedure, the intrinsic dimensionality for the four datasets turns out to be: sift - 41, gist - 75, color - 41, image - 70. For the two sparse datasets (color and image), it indicates the dimensionality of equivalent low-dimensional dense vector space. It is interesting to note that intrinsic dimensionality is not equal to d · s for the two sparse datasets as discussed before. For image dataset, it is much smaller than d·s indicating high correlations in non-zero entries of the data vectors."
    }, {
      "heading" : "3. Relative Contrast and Hashing",
      "text" : ""
    }, {
      "heading" : "3.1. Relative Contrast and LSH",
      "text" : "LSH are commonly used in many practical large-scale search systems due to their efficiency and ability to deal with high-dimensional data. In each hash table, every data point x is converted into codes by using a\nseries of k hash functions hj(x), j = 1, · · · , k. Each hash function is designed to satisfy the locality condition i.e., neighboring points have the same hashed value with high probability and vice-a-versa. A commonly used hash function in LSH is h(x) = ⌊wT x+bt ⌋, where w is a vector with entries sampled from a pstable distribution, and b is uniformly distributed as U [0, t] (Datar et al., 2004). We now provide the following theorems to show how relative contrast (Cr) affects the complexity of LSH.\nTheorem 3.1 LSH can find the exact nearest neighbor with probability 1 − δ by returning O(log 1δng(Cr)) candidate points, where g(Cr) is a function monotonically decreasing with Cr.\nProof: Please see the supplementary material. Corollary 3.2 LSH can find the exact nearest neighbor with a probability at least 1 − δ with a time complexity O(d log 1δn g(Cr) log n) and space complexity O(log 1δn (1+g(Cr)) + nd). l, the number of hash tables needed, is l = O(log 1δn g(Cr)).\nProof: Please see the supplementary material.\nThe above theorems imply that when Cr is larger, g(Cr) will be smaller, thus, among the datasets of same size, to get the same recall of the true nearest neighbor, the dataset with higher relative contrast Cr will have better time and space complexity, return less number of candidates for reranking, and need fewer number of hash tables, or in one word, be easier for approximate NN search with LSH.\nNote that our theory shares some similarity to the results in (Gionis et al., 1999) about the complexity of LSH, however, it has several unique properties. First, our theory is about finding exact NN (with a probability guarantee), not finding approximate NN (with a probability guarantee) like in previous works. Moreover, we have related the complexity of LSH to relative contrast Cr, enabling us to analyze how the complexity of LSH is affected by various data properties of the dataset simultaneously. To the best of our knowledge, our work is the first one on this important topic.\nTo verify the effect of relative contrast on LSH, we conducted experiments on three real-world datasets.\nIn Fig. 3, performance of LSH for L1 distance (i.e., p = 1) is given on three datasets: sift, gist and color. From Table 2, for p = 1, Cr for the three datasets is in this order: sift(4.78) > color(3.19) > gist (1.83). From Fig. 3 (a), we can see that for several settings of number of bits and number of tables, the number of returned points needed to get the same nearest neighbor recall for the three sets follows sift < color < gist, as predicted by Theorem 3.1. Moreover, from Fig. 3 (b),\nthe number of hash tables needed to get the same recall follows sift < color < gist, as predicted by Corollary 3.2. We have tried experiments with k = 12, 16..., 40 and observe the same trend, but only show results for k = 32 due to space limit.\nThe above experiments used the typical framework of hash table lookup. Another popular way to retrieve neighbors in code space is via hamming ranking. When using a k-bit code, points that are within hamming distance r to the query are returned as candidates. In Figure 4, we show the recall of nearest neighbor for two different values of k. Similar to the case of hash table lookup experiments, the number of returned points needed to get the same recall follows sift < color < gist. This follows the same order as suggested by relative contrast. The interesting thing is that color has much higher dimensionality than gist, but its sparsity helps in achieving better relative contrast and hence better search performance."
    }, {
      "heading" : "3.2. Relative Contrast and PCA hashing",
      "text" : "Hashing methods that use PCA as a heuristics often achieve quite good performance in practice (Weiss et al., 2008; Gong & Lazebnik, 2011). In this section, we show PCA hashing is actually seeking projections that maximize relative contrast in each projection with L2 distance under some assumptions. A\ncommonly used hash function in PCA-based hashing methods is\nh(x) = sgn(wTx+ b) (9)\nwhere w is heuristically picked as a PCA direction, and b is a threshold which is usually chosen as E[wTx]. Assuming the data to be zero-centered, i.e., E[x] = 0, leads to b = 0. Since q and x are assumed to be i.i.d samples from some unknown p(x), E[q] = 0 as well.\nFor a query q, denote xq,NN as q’s NN in the database. Denote SNN = Eq[(q − xq,NN )(q − xq,NN )T ], and ΣX = (1/n) ∑ i xix T i . The following theorem shows that maximizing relative contrast will lead us to PCA hashing under some assumptions.\nTheorem 3.3 For linear hashing as (9), to find projection vector w to maximize relative contrast, we\nshould find ŵ = argmax w wTΣXw wTSNNw . If we further assume that the nearest neighbors are isotropic, i.e., SNN = αI, we will get ŵ = argmax\nw wTΣXw, i.e.,\nPCA hashing.\nProof: Please see the supplementary material.\nIf we do not assume nearest neighbors to be isotropic, we can empirically compute SNN from a few samples. And then we can find projection vectors w in (9) as\nŵ = argmax w wTΣXw wTSNNw , which are the generalized eigenvectors of ΣX and SNN . This will often obtain better results than PCA hashing. We provide one example in Figure 5, in which, ”MRC” represents the method we described as above, and ”PCA”, ”LSH”, ”SH” are PCA hashing, Locality Sensitive Hashing, and Spectral Hashing (Weiss et al., 2008) respectively."
    }, {
      "heading" : "4. Related Works",
      "text" : ""
    }, {
      "heading" : "4.1. Previous Works",
      "text" : "Some of the influential works on analyzing NN search difficulty are (Beyer et al., 1999) and (Francois et al.,\n2007), whose main results are shown in Theorem 4.1 and 4.2.\nTheorem 4.1 (Beyer et al., 1999) Denote Dqmax = max\ni=1,...n D(xi, q) and D\nq min = min\ni=1,...n D(xi, q). If\nlim d→∞\nvar( D(xi,q) p\nE[D(xi,q)p] ) → 0, then for every ǫ ≥ 0,\nlim d→∞\nP [Dqmax ≤ (1 + ǫ)Dqmin] = 1.\nTheorem 4.2 (Francois et al., 2007) If every dimension of the data is i.i.d., when d → ∞,√\nV ar(||xi−q||p) E(||xi−q||p) ≈ 1√ d 1 p σj µj , where σj = V ar(||xji −qj ||pp) and µj = E(||xji −qj ||pp) are the variance and mean on each dimension."
    }, {
      "heading" : "4.2. Relations Between Our Analysis and Previous Works",
      "text" : "Relation to Beyer’s Work Note that if the distance function D(xi, q) in Beyer’s work is Lp distance, then var( D(xi,q) p\nE[D(xi,q)p] ) = σ\n2\nµ2 =\n(σ′)2. When σ′ → 0(d → ∞), Beyer’s work shows that Dqmax ≈ Dqmin, and our theory shows Cr → 1, or equivalently Dmean → Dmin. So we will get the same conclusion: when d → ∞, NN search is not very ”meaningful” , because we can not differentiate the nearest neighbor from other points. However, Beyer’s theory works for the worst case (i.e., compare NN point to the worst point with maximum distance), while ours works for the average case.\nRelation to Francois’s Work In Theorem 4.2, a measurement called ”relative vari-\nance”, defined as √ V ar(||xi−q||p) E(||xi−q||p) , is discussed, which is a modification of the condition var( D(xi,q) p\nE[D(xi,q)p] ) in\nBeyer’s work. If √ V ar(||xi−q||p) E(||xi−q||p) → 0 , NN search will become meaningless. The following theory reveals the relationship between relative variance and relative contrast.\nTheorem 4.3 In (4), if σ′ → 0 (e.g., d → ∞), Cr ≈ 1\n1+φ−1( 1n ) 1 p 1\nd1/2 σj µj\n.\nProof: Please see the supplementary material.\nFrom Theorem 4.3, we see when σ′ → 0 (e.g., d → ∞), the relative contrast monotonically depends on 1 p 1 d1/2 σj µj , which equals to ”relative variance” as in Theorem 4.2.\nTo summarize, most of the known analysis can be derived as special asymptotic cases (when σ′ → 0, e.g., d → ∞) of the proposed measure with the focus on only one or two data properties."
    }, {
      "heading" : "5. Conclusion and Future Work",
      "text" : "In this work, we introduced a new measure called relative contrast to describe the difficulty of nearest neighbor search in a data set. The proposed measure can be used to evaluate the influence of several crucial data characteristics such as dimensionality, sparsity, and database size simultaneously in arbitrary normed metric spaces. Furthermore, we show how relative contrast determines the difficulty of ANN search with LSH and provides guidance for better parameter settings. In the future, we would like to relax the independence assumption used in the theory of relative contrast, and also study how relative contrast affects the complexity of other approximate NN search methods besides LSH. Moreover, we will explore a better but harder definition of Cr = Eq[ Dqmean Dq\nmin ]."
    } ],
    "references" : [ {
      "title" : "On the surprising behavior of distance metrics in high dimensional space",
      "author" : [ "C. Aggarwal", "A. Hinneburg", "D. Keim" ],
      "venue" : null,
      "citeRegEx" : "Aggarwal et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Aggarwal et al\\.",
      "year" : 2001
    }, {
      "title" : "Locality-sensitive hashing scheme based on pstable distributions",
      "author" : [ "M. Datar", "N. Immorlica", "P. Indyk", "V.S. Mirrokni" ],
      "venue" : "In SOGC,",
      "citeRegEx" : "Datar et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Datar et al\\.",
      "year" : 2004
    }, {
      "title" : "The concentration of fractional distances",
      "author" : [ "D. Francois", "V. Wertz", "M. Verleysen" ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering,",
      "citeRegEx" : "Francois et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Francois et al\\.",
      "year" : 2007
    }, {
      "title" : "Similarity search in high dimensions via hashing",
      "author" : [ "A. Gionis", "P. Indyk", "R. Motwani" ],
      "venue" : "In VLDB,",
      "citeRegEx" : "Gionis et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Gionis et al\\.",
      "year" : 1999
    }, {
      "title" : "Iterative quantization: A procrustean approach to learning binary codes",
      "author" : [ "Y. Gong", "S. Lazebnik" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Gong and Lazebnik,? \\Q2011\\E",
      "shortCiteRegEx" : "Gong and Lazebnik",
      "year" : 2011
    }, {
      "title" : "Supplementary material for ”on the difficulty of nearest neighbor search",
      "author" : [ "J. He", "et. al" ],
      "venue" : null,
      "citeRegEx" : "He and al.,? \\Q2012\\E",
      "shortCiteRegEx" : "He and al.",
      "year" : 2012
    }, {
      "title" : "An investigation of practical approximate nearest neighbor algorithms",
      "author" : [ "T. Liu", "A.W. Moore", "A. Gray", "K. Yang" ],
      "venue" : null,
      "citeRegEx" : "Liu et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "A large number of approximate Nearest Neighbor (NN) search techniques have been proposed in the last decade including hashing and tree-based methods, to name a few, (Datar et al., 2004; Liu et al., 2004; Weiss et al., 2008).",
      "startOffset" : 165,
      "endOffset" : 223
    }, {
      "referenceID" : 6,
      "context" : "A large number of approximate Nearest Neighbor (NN) search techniques have been proposed in the last decade including hashing and tree-based methods, to name a few, (Datar et al., 2004; Liu et al., 2004; Weiss et al., 2008).",
      "startOffset" : 165,
      "endOffset" : 223
    }, {
      "referenceID" : 0,
      "context" : "In terms of ”meaningfulness” of NN search problem in a given dataset, most of the existing works have focused on the effect of one data property: dimensionality, that too in an asymptotic sense, showing that NN search will be meaningless when the number of dimensions goes to infinity (Beyer et al., 1999; Aggarwal et al., 2001; Francois et al., 2007).",
      "startOffset" : 285,
      "endOffset" : 351
    }, {
      "referenceID" : 2,
      "context" : "In terms of ”meaningfulness” of NN search problem in a given dataset, most of the existing works have focused on the effect of one data property: dimensionality, that too in an asymptotic sense, showing that NN search will be meaningless when the number of dimensions goes to infinity (Beyer et al., 1999; Aggarwal et al., 2001; Francois et al., 2007).",
      "startOffset" : 285,
      "endOffset" : 351
    }, {
      "referenceID" : 3,
      "context" : "In terms of the complexity of approximate NN search methods like Locality Sensitive Hashing (LSH), some general bounds for (Gionis et al., 1999; Indyk & Motwani, 1998) have been presented.",
      "startOffset" : 123,
      "endOffset" : 167
    }, {
      "referenceID" : 0,
      "context" : "It is interesting to note that (4) also predicts the rate at which contrast changes with d, unlike the previous works (Beyer et al., 1999; Aggarwal et al., 2001) which only show that NN search becomes impossible when dimensionality goes to infinity.",
      "startOffset" : 118,
      "endOffset" : 161
    }, {
      "referenceID" : 0,
      "context" : "This observation matches the conclusion from (Aggarwal et al., 2001) for dense vectors.",
      "startOffset" : 45,
      "endOffset" : 68
    }, {
      "referenceID" : 1,
      "context" : "A commonly used hash function in LSH is h(x) = ⌊wT x+b t ⌋, where w is a vector with entries sampled from a pstable distribution, and b is uniformly distributed as U [0, t] (Datar et al., 2004).",
      "startOffset" : 173,
      "endOffset" : 193
    }, {
      "referenceID" : 3,
      "context" : "Note that our theory shares some similarity to the results in (Gionis et al., 1999) about the complexity of LSH, however, it has several unique properties.",
      "startOffset" : 62,
      "endOffset" : 83
    }, {
      "referenceID" : 2,
      "context" : "2 (Francois et al., 2007) If every dimension of the data is i.",
      "startOffset" : 2,
      "endOffset" : 25
    } ],
    "year" : 2012,
    "abstractText" : "Fast approximate nearest neighbor(NN) search in large databases is becoming popular. Several powerful learning-based formulations have been proposed recently. However, not much attention has been paid to a more fundamental question: how difficult is (approximate) nearest neighbor search in a given data set? And which data properties affect the difficulty of nearest neighbor search and how? This paper introduces the first concrete measure called Relative Contrast that can be used to evaluate the influence of several crucial data characteristics such as dimensionality, sparsity, and database size simultaneously in arbitrary normed metric spaces. Moreover, we present a theoretical analysis to prove how the difficulty measure (relative contrast) determines/affects the complexity of Local Sensitive Hashing, a popular approximate NN search method. Relative contrast also provides an explanation for a family of heuristic hashing algorithms with good practical performance based on PCA. Finally, we show that most of the previous works in measuring NN search meaningfulness/difficulty can be derived as special asymptotic cases for dense vectors of the proposed measure.",
    "creator" : "LaTeX with hyperref package"
  }
}