{
  "name" : "1606.09197.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Model-Free Trajectory Optimization for Reinforcement Learning",
    "authors" : [ "Riad Akrour", "Abbas Abdolmaleki", "Hany Abdulsamad", "Gerhard Neumann" ],
    "emails" : [ "AKROUR@IAS.TU-DARMSTADT.DE", "ABBAS.A@UA.PT", "ABDULSAMAD@IAS.TU-DARMSTADT.DE", "NEUMANN@IAS.TU-DARMSTADT.DE" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Trajectory Optimization methods based on stochastic optimal control (Todorov, 2006; Theodorou et al., 2009; Todorov & Tassa, 2009) have been very successful in learning high dimensional controls in complex settings such as end-to-end control of physical systems (Levine & Abbeel, 2014). These methods are based on a time-dependent linearization of the dynamics model around the mean trajectory in order to obtain a closed form update of the policy as a Linear-Quadratic Regulator (LQR). This linearization is then repeated locally for the new policy at every iteration. However, the linearization of the dynamics might induce a bias and impede the algorithm from converging to the optimal policy. To circumvent this limitation, we propose in this paper a novel model-free trajectory optimization algo-\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\nrithm (MOTO) couched in the approximate policy iteration framework. At each iteration, a Q-Function is estimated locally around the current trajectory distribution using a time-dependent quadratic function. Afterward, the policy is updated according to a new information theoretic formulation that bounds a KL-divergence in closed form.\nMOTO is well suited for high dimensional state space and continuous action control problems. The policy is represented by a time-dependent stochastic linear-feedback controller which is updated by a Q-Function propagated backward in time. We extend the work of (Abdolmaleki et al., 2015), which was proposed in the domain of stochastic search (having no notion of state space nor of sequential decisions), to that of sequential decision making and show that our policy class can be updated under a KL-constraint in closed form, when the learned Q-Function is a quadratic function of the state and action space. In order to maximize sample efficiency we rely on importance sampling to reuse transition samples from policies of all time-steps and all previous iterations in a principled way. MOTO is able to solve complex control problems despite the simplicity of the Q-Function thanks to two key properties: i) the learned Q-Function is fitted to samples of the current policy, which ensures that the function is valid locally and ii) the closed form update of the policy ensures that the KL-constraint is satisfied exactly irrespective of the number of samples or the non-linearity of the dynamics, which ensures that the Q-Function is used locally.\nThe experimental section demonstrates that on tasks with highly non-linear dynamics MOTO outperforms similar methods that rely on a linearization of these dynamics. Additionally, it is shown on a simulated Robot Table Tennis Task that MOTO is able to scale to high dimensional tasks while keeping the sample complexity relatively low; amenable to a direct application to a physical system.\nar X\niv :1\n60 6.\n09 19\n7v 1\n[ cs\n.L G\n] 2\n9 Ju\nn 20"
    }, {
      "heading" : "2. Notations",
      "text" : "Consider an undiscounted finite-horizon Markov Decision Process (MDP) of horizon T with state space S ⊂ Rds and action space A ⊂ Rda . The transition function p(st+1|st, at), which gives the probability (density) of transitioning to state st+1 upon the execution of action at in st, is assumed to be time-independent; while there are T time-dependent reward functions rt : S×A 7→ R. A policy π is defined by a set of time-dependent density functions πt, where πt(a|s) is the probability of executing action a in state s at time-step t. The goal is to find the optimal policy π∗ = {π∗1 , . . . , π∗T } maximizing the policy return J(π) = IEs1,a1,... [∑T t=1 rt(st, at) ] , where the expectation is taken w.r.t. all the random variables st and at such that s1 ∼ ρ1 follows the distribution of the initial state, at ∼ πt(.|st) and st+1 ∼ p(st+1|st, at).\nAs is common in Policy Search (Deisenroth et al., 2013), MOTO operates on a restricted class of parametrized policies πθ, θ ∈ Rdθ and is an iterative algorithm comprising two main steps, policy evaluation and policy update. Throughout this paper, we will assume that each timedependent policy is parametrized by θt = {Kt, kt,Σt} such that πθt is of linear-Gaussian form πθt(a|s) = N (Kts + kt,Σt), where the gain matrix Kt is a da × ds matrix, the bias term kt is a da dimensional column vector and the covariance matrix Σt which controls the exploration of the policy is of dimension da × da; yielding a total number of parameters across all time-steps of dθ = T (dads + 1 2da(da + 3)).\nThe policy at iteration i of the algorithm is denoted by πi and following standard definitions, the QFunction of πi at time-step t is denoted by Qit(s, a) = IEst,at,... [∑T t′=t rt′(st′ , at′) ] with (st, at) = (s, a) and at′ ∼ πit′(.|st′),∀t′ > t. While the V-Function is given by V it (s) = IEa∼πt(.|s) [Q π t (s, a)] and the Advantage Function byAit(s, a) = Q i t(s, a)−V it (s). Furthermore, the state distribution at time-step t of policy πi is denoted by ρit(s). In order to keep notations simple, the time-step (resp. the iteration number) is occasionally dropped when all the elements of an equation apply similarly to all of them."
    }, {
      "heading" : "3. Information-Theoretic Policy Update",
      "text" : "MOTO alternates between policy evaluation and policy update. At each iteration i, the policy evaluation step generates a set of M rollouts1 from the policy πi in order to estimate from samples the Q-Function Q̃i (Sec. 4.1) and the state distribution ρ̃i (Sec. 4.3). Using these quantities, an information-theoretic policy update is derived at each time-\n1A rollout is a Monte Carlo simulation of a trajectory according to ρ1, π and p or the execution of π on a physical system.\nstep as a solution of a constrained optimization problem to compute the new policy πi+1."
    }, {
      "heading" : "3.1. Optimization Problem",
      "text" : "The goal of the policy update is to return a new policy πi+1 that maximizes the Q-Function Q̃i in expectation under the state distribution p̃i of the previous policy πi. In order to limit policy oscillation between iterations, the KL w.r.t. πi is upper bounded. The use of the KL divergence to define the step-size of the policy update has already been successfully applied in prior work (Peters et al., 2010; Levine & Abbeel, 2014; Schulman et al., 2015). Additionally, we lower bound the entropy of πi+1 in order to better control the reduction of exploration. The optimization problem to obtain πi+1 is thus given by the following non-linear program:\nmaximize π\n∫ ∫ ρ̃it(s)π(a|s)Q̃it(s, a)dads, (1)\nsubject to IEs∼ρ̃it(s) [ KL ( π(.|s)|πit(.|s) )] ≤ , (2)\nIEs∼ρ̃it(s) [H (π(.|s))] ≥ β. (3)\nThe KL between two distributions p and q is given by KL(p|q) = ∫ p(x) log p(x)q(x)dx while the entropyH is given\nby H = − ∫ p(x) log p(x)dx. The step-size is a hyperparameter of the algorithm kept constant throughout the iterations while β is set according to the entropy of πit, β = IEs∼ρ̃it(s) [ H ( πit(.|s) )] − β0 and β0 is a constant entropy reduction hyper-parameter.\nEq. (1) indicates that πi+1t maximizes Q̃ i t in expectation under its own action distribution and the state distribution of πit. Eq. (2) bounds the average change in the policy to the step-size while Eq. (3) controls the explorationexploitation trade-off and ensures that the exploration in the action space (which is directly linked to the entropy of the policy) is not reduced too quickly. A similar constraint was introduced in the stochastic search domain by (Abdolmaleki et al., 2015), and was shown to avoid premature convergence. However, this constraint is even more crucial in our setting because of the inherent non-stationarity of the objective function being optimized at each iteration. The cause for the non-stationarity of the objective optimized at time-step t in the policy update is twofold: i) updates of policies πt′ with time-step t′ > t will modify in the next iteration of the algorithm Q̃t as a function of s and a and hence the optimization landscape as a function of the policy parameters, ii) updates of policies with timestep t′ < t will induce a change in the state distribution ρt. If the policy had unlimited expressiveness, the optimal solution of Eq. (1) would be to choose arg maxa Q̃t irrespective of ρt. However, due to the restricted class of the policy, any change in ρt will likely change the optimization landscape including the position of the optimal policy pa-\nrameter. Hence, Eq. (3) ensures that exploration in action space is maintained as the optimization landscape evolves and avoids premature convergence."
    }, {
      "heading" : "3.2. Closed Form Update",
      "text" : "Using the method of Lagrange multipliers, the solution of the optimization problem in section 3.1 is given by\nπ′t(a|s) ∝ πt(a|s)η ∗/(η∗+ω∗) exp\n( Q̃t(s, a)\nη∗ + ω∗\n) , (4)\nwith η∗ and ω∗ being the optimal Lagrange multipliers related to the KL and entropy constraints respectively. Assuming that Q̃t(s, a) is of quadratic form in a and s\nQ̃t(s, a) = 1\n2 aTQaaa+ a TQass+ a T qa + q(s), (5)\nwith q(s) grouping all terms of Q̃t(s, a) that do not depend2 on a, then π′t(a|s) is again of linear-Gaussian form\nπ′t(a|s) = N (a|FLs+ Ff, F (η∗ + ω∗)),\nsuch that the gain matrix, bias and covariance matrix of π′t are function of matrices F and L and vector f where\nF = (η∗Σ−1t −Qaa)−1, L = η∗Σ−1t Kt +Qas, f = η∗Σ−1t kt + qa.\nNote that, for ηΣ−1t − Qaa to be invertible, either Qt(s, .) needs to be concave is a (i.e. Qaa is negative semidefinite), or η needs to be large enough (and for a given Qaa such an η always exists). Efficient algorithms for learning model parameters with a specific semidefinite shape are available (Bhojanapalli et al., 2015). However, as it is desired for the new policy π′t to have a small KL divergence w.r.t. πt, the resulting η was always large enough in our experiments and F well defined, without imposing additional constraint on the shape of Qaa."
    }, {
      "heading" : "3.3. Dual Minimization",
      "text" : "The Lagrangian multipliers η and ω are obtained by minimizing the convex dual function\ngt(η, ω) = η − ωβ + (η + ω) ∫ ρ̃t(s)\nlog (∫ π(a|s)η/(η+ω) exp ( Q̃t(s, a)/(η + ω) ) da ) ds.\nThe dual function further simplifies thanks to the quadratic form of Q̃t(s, a) and by additionally assuming normality of\n2Constant terms and terms depending on s but not a won’t appear in the policy update. As such, and albeit we only refer in this paper to Qt(s, a), the Advantage Function At(s, a) can be used interchangeably in lieu of Qt(s, a) for updating the policy.\nthe state distribution ρ̃t(s) = N (s|µs,Σs) to the function gt(η, ω) = η − ωβ + µTs Mµs + tr(ΣsM) + µTs m+m0, which can be efficiently optimized by gradient descent to obtain η∗ and ω∗. The full expression of the dual function, including the definition of M , m and m0 in addition to the partial derivatives ∂gt(η,ω)∂η and ∂gt(η,ω) ∂ω are given in the supplementary material."
    }, {
      "heading" : "4. Sample Efficient Policy Evaluation",
      "text" : "The KL constraint introduced in the policy update gives rise to a non-linear optimization problem. This problem can still be solved in closed form for the class of linearGaussian policies, if the learned function Q̃it is quadratic in s and a. The first subsection introduces the main supervised learning problem solved during the policy evaluation for learning Q̃it while the remaining subsections discuss how to improve its sample efficiency."
    }, {
      "heading" : "4.1. The Supervised Learning Problem",
      "text" : "In the remainder of the section, we will be interested in finding the parameter w of a linear model Q̃it = 〈w, φ(s, a)〉, where the feature function φ contains a bias and all the linear and quadratic terms of s and a, yielding 1 + (da + ds)(da + ds + 3)/2 parameters. Q̃it can subsequently be written as in Eq. (9) by extractingQaa, Qas and qa from w.\nAt each iteration i, M rollouts are performed following πi. Let us initially assume that Q̃it is learned only from samples Dit = {s [k] t , a [k] t , s [k] t+1; k = 1..M} gathered by the execution of the M rollouts. The parameter w of Q̃it is learned by regularized linear least square regression\nw = arg min w\n1\nM M∑ k=1 ( 〈w, φ(s[k]t , a [k] t )〉−\nQ̂it(s [k] t , a [k] t ) )2 + λwTw. (6)\nThe target value Q̂it(s [k], a[k]) is a noisy estimate of the true value Qit(s [k] t , a [k] t ). We will distinguish two cases for obtaining the estimate Q̂it(s [k] t , a [k] t ).\nMonte-Carlo Estimate. This estimate is obtained by summing the future rewards for each trajectory k, yielding Q̂it(s [k] t , a [k] t ) = ∑T t′=t rt′(s [k] t′ , a [k] t′ ). This estimator is known to have no bias but high variance. The variance can be reduced by averaging over multiple rollouts, assuming we can reset to states s[k]t . However, such an assumption would severely limit the applicability of the algorithm on physical systems.\nDynamic Programming. In order to reduce the variance, this estimate exploits the V-Function to reduce the noise\nof the expected rewards of time-steps t′ > t through the following identity\nQ̂it(s [k] t , a [k] t ) = rt(s [k] t , a [k] t ) + V̂ i t+1(s [k] t+1), (7)\nthat is unbiased if V̂ it+1 is. However, we will use for V̂ i t+1 a V-Function Ṽ it+1 learned recursively in time. This might introduce a bias which will accumulate as t goes to 1. Fortunately, Ṽ is not restricted by our algorithm to be of a particular class as it does not appear in the policy update. Hence, the bias can be reduced by increasing the complexity of the function approximator class. Nonetheless, in this paper, a quadratic function will also be used for the V-Function which worked well in our experiments.\nThe V-Function is learned by first assuming that Ṽ iT+1 is the zero function3, then recursively in time, the function Ṽ it+1 and the transition samples in Dit are used to fit the parametric function Ṽ it by minimizing the squared loss∑M k=1 ( Q̂it(s [k] t , a [k] t )− Ṽ it (s [k] t ) )2 .\nIn addition to reducing the variance of the estimate Q̂it(s [k] t , a [k] t ), the choice of learning a V-Function is further justified by the increased possibility of reusing sample transitions from all time-steps and previous iterations."
    }, {
      "heading" : "4.2. Sample Reuse",
      "text" : "In order to improve the sample efficiency of our approach, we will reuse samples from different time-steps and iterations using importance sampling. Let the expected loss which Q̃it minimizes under the assumption of an infinite number of samples be\nw = arg min w\nIE[`it(s, a, s ′;w)],\nwhere the loss `it is the inner term within the sum in Eq. (6); the estimate Q̂it(s [k] t , a [k] t ) is taken as in Eq. (7) and the expectation is with respect to the current state s ∼ ρit, the action a ∼ πit(.|s) and the next state s′ ∼ p(.|s, a).\nReusing samples from different time-steps. To use transition samples from all time-steps when learning Q̃it, we rely on importance sampling, where the importance weight (IW) is given by the ratio between the state-action probability of the current time-step zit(s, a) = ρ i t(s)π i t(a|s) divided by the time-independent state-action probability of πi given by zi(s, a) = 1T ∑T t=1 z i t(s, a). The expected loss minimized by Q̃it becomes\nmin w IE\n[ zit(s, a)\nzi(s, a) `it(s, a, s\n′;w) | (s, a) ∼ zi(s, a) ] . (8)\n3Alternatively one could assume the presence of a final reward RT+1(sT+1), as is usually formulated in control tasks (Bertsekas, 1995), to which V iT+1 could be initialized to.\nSince the transition probabilities are not time-dependent they cancel out from the IW. Upon the computation of the IW, weighted least square regression is used to minimize an empirical estimate of (8) for the dataset Di = ∪Tt=1Dit. Note that the (nominator of the) IW needs to be recomputed at every time-step for all samples (s, a) ∈ Di. Additionally, if the rewards are time-dependent, the estimate Q̂it(s [k] t , a [k] t ) in Eq. (7) needs to be recomputed with the current time-dependent reward, assuming the reward function is known.\nReusing samples from previous iterations. Following a similar reasoning, at a given time-step t, samples from previous iterations can be reused for learning Q̃it. In this case, we have access to the samples of the state-action distribution z1:it (s, a) ∝ ∑i j=1 z j t (s, a). The computation of z 1:i t requires the storage of all previous policies and state distributions. Thus, we will in practice limit ourselves to the K last iterations.\nFinally, both forms of sample reuse will be combined for learning Q̃it under the complete dataset up to iteration i, D1:i = ∪ij=1Dj using weighted least square regression where the IW are given by zit(s, a)/z\n1:i(s, a) with z1:i(s, a) ∝ ∑T t=1 z 1:i t (s, a)."
    }, {
      "heading" : "4.3. Estimating the State Distribution",
      "text" : "To compute the IW, the state distribution at every time-step ρit needs to be estimated. Since M rollouts are sampled for every policy πi only M state samples are available for the estimation of ρit, necessitating again the reuse of previous samples to cope with higher dimensional control tasks.\nForward propagation of the state distribution. The first investigated solution for the estimation of the state distribution is the propagation of the estimate ρ̃it forward in time. Starting from ρ̃i1 which is identical for all iterations, importance sampling is used to learn ρ̃it+1 with t > 1 from samples (st, at, st+1) ∈ D1:it by weighted maximum-likelihood; where each sample st+1 is weighted by zit(st, at)/z 1:i t (st, at). And the computation of this IW only depends on the previously estimated state distribution ρ̃it. In practice however, the estimate ρ̃ i t might entail errors despite the use of all samples from past iterations, which are propagated forward leading to a degeneracy of the number of effective samples in latter time-steps.\nState distribution of a mixture policy. The second considered solution for the estimation of ρ̃it is based on the intuition that due to to the KL constraint during the policy update, successive policies are close to each other and state samples from previous iterations can thus be reused in a simpler manner. Specifically, ρ̃it will be learned from samples of the mixture policy π1:i ∝ ∑i j=1 γ\ni−jπj which selects a policy from previous iterations with an exponen-\nAlgorithm 1 Model-Free Trajectory Optimization (MOTO)\nInput: Initial policy π0, number of trajectories per iteration M, step-size and entropy reduction rate β0 Output: Policy πN for i = 0 to N − 1 do\nSample M trajectories from πi for t = T to 1 do Estimate state distribution ρ̃it (Sec. 4.3) Compute IW for all (s, a, s′) ∈ D1:i (Sec. 4.2) Estimate the Q-Function Q̃it (Sec. 4.1) Optimize: (η∗, ω∗) = arg min git(η, ω) (Sec. 3.3) Update πi+1t using η ∗, ω∗,ρ̃it and Q̃ i t (Sec. 3.2)\nend for end for\ntially decaying (w.r.t. to the iteration number) probability and executes it for a whole rollout. In practice, the decay factor γ is selected according to the dimensionality of the problem, the number of samples per iterations M and the KL upper bound (intuitively, a smaller yields closer policies and henceforth more reusable samples). The estimated state distribution ρ̃it is learned as a Gaussian distribution by weighted maximum likelihood from samples ofD1:it where a sample of iteration j is weighted by γi−j .\nMOTO is summarized in Alg. 1. The innermost loop is split between policy evaluation (Sec. 4) and policy update (Sec. 3). For every time-step t, once the state distribution ρ̃it is estimated, the IWs of all the transition samples are computed and used to learn the Q-Function (and the VFunction using the same IWs, if dynamic programming is used when estimating the Q-Function), concluding the policy evaluation part. Subsequently, the components of the quadratic model Q̃it that depend on the action are extracted and used to find the optimal dual parameters η∗ and ω∗ that are respectively related to the KL and the entropy constraint, by minimizing the convex dual function git using gradient descent. The policy update then directly proceeds using the aforementioned quantities to yield a new policy πt+1 and the process is iterated.\nIn addition to the simplification of the policy update, the rationale behind the use of a local quadratic approximation forQit is twofold: i) since Q i t is only optimized locally (because of the KL constraint), a quadratic model would potentially contain as much information as a Hessian matrix in a second order gradient descent setting ii) If Q̃t in Eq. (4) is an arbitrarily complex model then it is common that π′t, of linear-Gaussian form, is fit by weighted maximumlikelihood (Deisenroth et al., 2013); it is clear though from Eq. (4) that however complex Q̃t(s, a) is, if both πt and π′t are of linear-Gaussian form then there exist a quadratic model that would result in the same policy update. Additionally, note that Q̃t is not used when learning Q̃t−1 (sec.\n4.1) and hence the bias introduced by Q̃t will not propagate back. For these reasons, we believe that choosing a more complex class for Q̃t than that of quadratic functions might not necessarily lead to an improvement of the resulting policy, for the class of linear-Gaussian policies."
    }, {
      "heading" : "5. Related Work",
      "text" : "In the Approximate Policy Iteration scheme (Szepesvari, 2010), policy updates can potentially decrease the expected reward leading to policy oscillations (Wagner, 2011), unless the updated policy is ’close’ enough to the previous one (Kakade & Langford, 2002). (Pirotta et al., 2013b) refines the lower bound proposed in (Kakade & Langford, 2002) yielding more aggressive updates, but both approaches only considered discrete action spaces. (Pirotta et al., 2013a) provides an extension to continuous domains but only for single dimensional actions.\nWhen the action space is continuous, which is typical in e.g. robotic applications, using a stochastic policy and updating it under a KL constraint to ensure ’closeness’ of successive policies has shown several empirical successes (Daniel et al., 2012; Levine & Koltun, 2014; Schulman et al., 2015). However, only an empirical sample estimate of the objective function is generally optimized (Peters et al., 2010; Schulman et al., 2015), which typically requires a high number of samples and precludes it from a direct application to physical systems. The sample complexity can be reduced when a model of the dynamics is available (Levine & Koltun, 2014) or learned (Levine & Abbeel, 2014). In the latter work, empirical evidence suggests that good policies can be learned on high dimensional continuous state-action spaces with only a few hundred episodes. The counter part being that time-dependent dynamics are assumed to be linear, which might be a simplifying assumption in practice. Learning more sophisticated models using for example Gaussian Processes was experimented by (Deisenroth & Rasmussen, 2011) and (Pan & Theodorou, 2014) in the Policy Search and Trajectory Optimization context, but it is still considered to be a challenging task, see (Deisenroth et al., 2013), chapter 3.\nMost trajectory optimization methods are based on stochastic optimal control. These methods linearize the system dynamics and update the policy in closed form as an LQG. Instances of such algorithms are for example iLQG (Todorov, 2006), DDP (Jacobson & Mayne, 1970; Theodorou et al., 2010), AICO (Toussaint, 2009) and the trajectory optimization algorithm used in the GPS algorithm (Levine & Abbeel, 2014). These methods face issues in maintaining the stability of the policy update step. DDP, iLQG and AICO regularize the update by introducing a damping term in the matrix inversion step, while GPS uses a KL bound on successive trajectory distributions. These methods share\nthe same assumptions as MOTO for ρit and π i t respectively considered to be of Gaussian and linear-Gaussian form. However, the additional linearization of the system dynamics boils down to assuming p(st, at, st+1) – and by extension the whole trajectory – to be normally distributed."
    }, {
      "heading" : "6. Experimental Validation",
      "text" : "MOTO is experimentally validated on a set of multi-link swing-up tasks and on a robot table tennis task. The experimental section aims at analyzing the proposed algorithm from four different angles: i) the quality of the returned policy comparatively to state-of-the-art trajectory optimization algorithm, ii) the effectiveness of the proposed variance reduction and sample reuse schemes, iii) the contribution of the added entropy constraint during policy updates in finding better local optima and iv) the ability of the algorithm to scale to higher dimensional problems."
    }, {
      "heading" : "6.1. Multi-link Swing-up Tasks",
      "text" : "A set of swing-up tasks involving a multi-link pole with respectively two and four joints is considered in this section. The set of tasks includes several variants with different torque and joint limits, introducing additional nonlinearities in the dynamics and resulting in more challenging control problems for trajectory optimization algorithms based on linearizing the dynamics. The state space consists of the joint positions and joint velocities while the control actions are the motor torques. In all the tasks, the reward function is split between an action cost and a state cost. The action cost is constant throughout the time-steps while the state cost is time-dependent and is equal to zero for all but the 20 last time-steps. During this period, a quadratic cost penalizes the state for not being the null vector, i.e. having zero velocity and reaching the upright position. Examples of successful swing-ups learned by MOTO for the double and quadruple link pole are depicted in Fig. 1.a and 2.a respectively. MOTO is compared to the trajectory optimization algorithm proposed in (Levine & Abbeel, 2014), that we will refer to as GPS4.\nGPS and MOTO both use a time-dependent linearGaussian policy. In order to learn the linear model of the system dynamics, GPS reuses samples from different timesteps by learning a Gaussian mixture model on all the samples and use this model to learn a joint Gaussian distribution p(st, at, st+1). When comparing MOTO to GPS, and in order to isolate the choice of learning a linear model of the dynamics or remain model-free, we give to both algorithm a high number of samples (200 and 400 rollouts\n4This is a slight abuse of notation as the GPS algorithm of (Levine & Abbeel, 2014) additionally feeds the optimized trajectory to an upper level policy. However, in this paper, we are only interested in the trajectory optimization part.\nper iteration for the double and quadruple link respectively) and bypass any kind of sample reuse.\nFig. 1.b compares GPS to two configurations of MOTO on the double link swing up task. The same initial policy and step-size are used by both algorithm. However, we found that GPS performs better with a smaller initial variance, as otherwise actions quickly hit the torque limits making the dynamics modeling harder. Fig. 1.b shows that even if the dynamics of the system are not linear, GPS still manages to improve the policy return, and eventually finds a swing-up policy. The two configurations of MOTO have an entropy reduction constant β0 of .1 and .5. The effect of the entropy constraint is similar to the one observed in the stochastic search domain by (Abdolmaleki et al., 2015). Specifically, a smaller entropy reduction constant β0 results in an initially slower convergence but ultimately leads to higher quality policies. In this particular task, MOTO with β0 = .1 manages to slightly outperform GPS.\nNext, GPS and MOTO are compared on the quadruple link swing-up task. We found this task to be significantly more challenging than the double link and to increase the difficulty further, soft joint limits are introduced on the three last joints in the following way: whenever a joint angle exceeds in absolute value the threshold of 23π, the desired torque of the policy is ignored in favor of a linear-feedback controller that aims at pushing back the joint angle within the constrained range. As a result, Fig. 2.b shows that GPS can barely improve its average return (with the torque limits set to 25, as in the double link task.) while MOTO performs significantly better. Finally, the torque limits are reduced even further but MOTO still manages to find a swing-up policy as demonstrated by Fig. 2.a.\nIn the last set of comparisons, the importance of each of the components of MOTO is assessed on the double link experiment (torque limit set to 10). The number of rollouts per iteration is reduced to M = 20. Fig. 1.c shows that: i) the entropy constraint provides an improvement on the quality of the policy in the last iterations in exchange of a slower initial progress, ii) importance sampling greatly helps in speeding-up the convergence and iii) the MonteCarlo estimate of Q̂ti is not adequate for such a small number of rollouts per iterations, which is further exacerbated by the fact that sample reuse of transitions from different time-steps is not possible with the Monte-Carlo estimate.\nFinally, Fig. 2.c aims at finding where the balance lies between performing a small number of rollouts per iterations with small policy updates and vice versa. To do so, we start with an initial M = 20 and subsequently divide this number by two until M = 5. In each case, the entropy reduction constant is set such that, for a similar number of rollouts, the entropy is reduced by the same amount, while we choose γ′ = γM/M ′ to yield again a similar sample de-\ncay after the same number of rollouts have been performed. Tuning , however, was more complicated and we tested several values on non-overlapping ranges for each M and selected the best one. Fig. 2.c shows that, on the double link swing-up task, a better sample efficiency is achieved with a smaller M . However, the improvement becomes negligible from M = 10 to M = 5. We also noticed a sharp decrease in the number of effective samples when M goes to 1. In this limiting case, the complexity of z1:i increases with the increase of the number of policies composing it, and might become a poor representation of the data-set. Fitting a state-action distribution that is more representative of the data might be the subject of future work, in order to increase the sample efficiency of the algorithm, which is crucial when applied to physical systems."
    }, {
      "heading" : "6.2. Robot Table Tennis Task",
      "text" : "The considered robot table tennis task consists of a simulated robotic arm mounted on a floating base, having a racket attached to the end effector that has to return using\na forehand stroke incoming balls to the opposite side of the table (Fig. 4). The arm has 9 degrees of freedom comprising the six joints of the arm and the three linear joints of the base allowing (small) 3D movement. Together with the joint velocities and the 3D position of the incoming ball, the resulting state space is of dimension ds = 21 and the action space is of dimension da = 9 and consists of direct torque commands.\nWe use the analytical player of (Mülling et al., 2011) to generate a single forehand stroke, which is subsequently used to learn from demonstration the initial policy π1. The\nanalytical player consists of a waiting phase (keeping the arm still), a preparation phase, a hitting phase and a return phase, which resets the arm to the waiting position of the arm. Only the preparation and hitting phase will be replaced by a learned policy. The total control time for the two learned phases comprises 300 time-steps at 500hz, although for the MOTO algorithm we subsequently divide the control frequency by a factor of 10, average the torque commands of every 10 time-steps and use these quantities as the initial bias for each of the 30 policies; while the gain matrices are initially set to zero. By doing so, the initial policy will capture the basic template of a forehand stroke but does not contain any correlation between the torque commands and the state entries such as the ball position.\nThree settings of the task are considered, a noiseless case where the ball is launched with the same initial velocity, a varying context setting where the initial velocity is sampled uniformly within a fixed range and the noisy bounce setting where a Gaussian noise is added to both the x and y velocities of the ball upon bouncing on the table, to simulate the effect of a spin.\nWe compare MOTO to the REPS policy search algorithm (Peters et al., 2010) and the stochastic search algorithm MORE (Abdolmaleki et al., 2015) that share a related information-theoretic update. Both algorithms will optimize the parameters of a Dynamical Movement Primitive (DMP) (Ijspeert & Schaal, 2003). A DMP is a non-linear attractor system commonly used in robotics. The DMP is initialized from the same single trajectory and the two algorithms will optimize the goal joint positions and velocities of the attractor system. Note that the DMP generates a trajectory of states, which will be tracked by a linear controller using the inverse dynamics. While MOTO will directly output the torque commands and do not rely on this the inverse dynamics model.\nFig. 3.a and 3.c show that MOTO converges faster than\nREPS and to a smaller extent to MORE in both the noiseless and the varying context setting. This is somewhat surprising since MOTO with its time-dependent linear policy have a much higher number of parameters to optimize than the 18 parameters of the DMP’s attractor. However, the resulting policy in both cases is slightly less good than that of MORE and REPS. Note that for the varying context setting, we used a contextual variant of REPS that learns a mapping from the initial ball velocity to the DMP’s parameters. MORE, on the other hand couldn’t be compared in this setting. Finally, Fig. 3.b shows that our policy is successfully capable of adapting to noise at ball bounce, while the other methods fail to do so since the trajectory of the DMP is not updated once generated."
    }, {
      "heading" : "7. Conclusion",
      "text" : "We proposed in this paper a new trajectory optimization algorithm that unlike other state-of-the-art algorithms does not rely on a linearization of the dynamics. Yet, we are able to derive an efficient policy update by locally fitting a Q-Function, and outperform state-of-the-art trajectory optimization methods on the more challenging tasks. One of the main addition that would ease the transition from simulation to physical systems concerns the safety of the exploration. On a more technical side, the use of a more complex function for the estimation of the V-Function could be investigated to allow for a more refined bias-variance tradeoff."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The research leading to these results was partially funded by the DFG Project LearnRobotS under the SPP 1527 Autonomous Learning."
    }, {
      "heading" : "8. Dual Function",
      "text" : "Recall the quadratic form of the Q-Function Q̃t(s, a) in the action a and state s\nQ̃t(s, a) = 1\n2 aTQaaa+ a TQass+ a T qa + q(s). (9)\nThe new policy π′t(a|s) solution of the constrained maximization problem is again of linear-Gaussian form and given by\nπ′t(a|s) = N (a|FLs+ Ff, F (η∗ + ω∗)),\nsuch that the gain matrix, bias and covariance matrix of π′t are function of matrices F and L and vector f where\nF = (η∗Σ−1t −Qaa)−1, L = η∗Σ−1t Kt +Qas, f = η∗Σ−1t kt + qa.\nWith η∗ and ω∗ the optimal Lagrange multipliers related to the KL and entropy constraints, obtained by minimizing the dual function\ngt(η, ω) = η − ωβ + (η + ω) ∫ ρ̃t(s)\nlog (∫ π(a|s)η/(η+ω) exp ( Q̃t(s, a)/(η + ω) )) ds.\nFrom the quadratic form of Q̃t(s, a) and by additionally assuming that the state distribution is approximated by ρ̃t(s) = N (s|µs,Σs), the dual function simplifies to\ngt(η, ω) = η − ωβ + µTs Mµs + tr(ΣsM) + µTs m+m0.\nWhere M , m and m0 are defined by\nM = 1\n2\n( LTFL− ηKTt Σ−1t Kt ) , m = LTFf−ηKTt Σ−1t kt,\nm0 = 1\n2 (fTFf − ηkTt Σ−1t kt − η log |2πΣt|\n+ (η + ω) log |2π(η + ω)F |).\nThe convex dual function gt can be efficiently minimized by gradient descent and the policy update is performed upon the computation of η∗ and ω∗. The gradient w.r.t. η and ω is given by5\n∂gt(η, ω)\n∂η = cst + lin + quad\ncst = − 1 2 (kt − Ff)T Σ−1t (kt − Ff)− 1 2 [log |2πΣt|\n− log |2π(η + ω)F |+ (η + ω)tr(Σ−1t F )− da].\nlin = ((Kt − FL)µs)T Σ−1t (Ff − kt). quad = µTs (Kt + FL) TΣ−1t (Kt + FL)µs\n+ tr(Σs(Kt + FL)TΣ−1t (Kt + FL))\n∂gt(η, ω) ∂ω = − β + 1 2 (da + log |2π(η + ω)F |).\n5cst, lin, quad, F , L and f all depend on η and ω. We dropped the dependency from the notations for compactness. da is the dimensionality of the action."
    } ],
    "references" : [ {
      "title" : "Dynamic Porgramming and optimal control",
      "author" : [ "Bertsekas", "Dimitri P" ],
      "venue" : "Athena Scientific,",
      "citeRegEx" : "Bertsekas and P.,? \\Q1995\\E",
      "shortCiteRegEx" : "Bertsekas and P.",
      "year" : 1995
    }, {
      "title" : "Dropping convexity for faster semi-definite optimization",
      "author" : [ "Bhojanapalli", "Srinadh", "Kyrillidis", "Anastasios T", "Sanghavi", "Sujay" ],
      "venue" : "CoRR, abs/1509.03917,",
      "citeRegEx" : "Bhojanapalli et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bhojanapalli et al\\.",
      "year" : 2015
    }, {
      "title" : "Hierarchical Relative Entropy Policy Search",
      "author" : [ "C. Daniel", "G. Neumann", "J. Peters" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics (AISTATS),",
      "citeRegEx" : "Daniel et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Daniel et al\\.",
      "year" : 2012
    }, {
      "title" : "PILCO: A ModelBased and Data-Efficient Approach to Policy Search",
      "author" : [ "M. Deisenroth", "C. Rasmussen" ],
      "venue" : "In 28th International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Deisenroth and Rasmussen,? \\Q2011\\E",
      "shortCiteRegEx" : "Deisenroth and Rasmussen",
      "year" : 2011
    }, {
      "title" : "A Survey on Policy Search for Robotics",
      "author" : [ "M.P. Deisenroth", "G. Neumann", "J. Peters" ],
      "venue" : "Foundations and Trends in Robotics,",
      "citeRegEx" : "Deisenroth et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Deisenroth et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning Attractor Landscapes for Learning Motor Primitives",
      "author" : [ "A. Ijspeert", "S. Schaal" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Ijspeert and Schaal,? \\Q2003\\E",
      "shortCiteRegEx" : "Ijspeert and Schaal",
      "year" : 2003
    }, {
      "title" : "Differential dynamic programming. Modern analytic and computational methods in science and mathematics",
      "author" : [ "Jacobson", "David H", "Mayne", "David Q" ],
      "venue" : null,
      "citeRegEx" : "Jacobson et al\\.,? \\Q1970\\E",
      "shortCiteRegEx" : "Jacobson et al\\.",
      "year" : 1970
    }, {
      "title" : "Approximately optimal approximate reinforcement learning",
      "author" : [ "Kakade", "Sham", "Langford", "John" ],
      "venue" : "In Machine Learning, Proceedings of the Nineteenth International Conference (ICML 2002),",
      "citeRegEx" : "Kakade et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Kakade et al\\.",
      "year" : 2002
    }, {
      "title" : "Learning complex neural network policies with trajectory optimization",
      "author" : [ "Levine", "Sergey", "Koltun", "Vladlen" ],
      "venue" : "In Proceedings of the 31th International Conference on Machine Learning,",
      "citeRegEx" : "Levine et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Levine et al\\.",
      "year" : 2014
    }, {
      "title" : "A Biomimetic Approach to Robot Table Tennis",
      "author" : [ "K. Mülling", "J. Kober", "J. Peters" ],
      "venue" : "Adaptive Behavior Journal,",
      "citeRegEx" : "Mülling et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Mülling et al\\.",
      "year" : 2011
    }, {
      "title" : "Probabilistic differential dynamic programming",
      "author" : [ "Pan", "Yunpeng", "Theodorou", "Evangelos" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Pan et al\\.,? \\Q1915\\E",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 1915
    }, {
      "title" : "Relative Entropy Policy Search",
      "author" : [ "J. Peters", "K. Mülling", "Y. Altun" ],
      "venue" : "In Proceedings of the 24th National Conference on Artificial Intelligence (AAAI). AAAI Press,",
      "citeRegEx" : "Peters et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2010
    }, {
      "title" : "Trust region policy optimization",
      "author" : [ "Schulman", "John", "Levine", "Sergey", "Abbeel", "Pieter", "Jordan", "Michael I", "Moritz", "Philipp" ],
      "venue" : "In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015,",
      "citeRegEx" : "Schulman et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Schulman et al\\.",
      "year" : 2015
    }, {
      "title" : "Algorithms for Reinforcement Learning",
      "author" : [ "Szepesvari", "Csaba" ],
      "venue" : null,
      "citeRegEx" : "Szepesvari and Csaba.,? \\Q2010\\E",
      "shortCiteRegEx" : "Szepesvari and Csaba.",
      "year" : 2010
    }, {
      "title" : "Stochastic Differential Dynamic Programming",
      "author" : [ "E. Theodorou", "Y. Tassa", "E. Todorov" ],
      "venue" : "In Proceedings of the 29th American Control Conference,",
      "citeRegEx" : "Theodorou et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Theodorou et al\\.",
      "year" : 2010
    }, {
      "title" : "Path Integral Stochastic Optimal Control for Rigid Body Dynamics. In ieee international symposium on approximate dynamic programming and reinforcement learning",
      "author" : [ "Theodorou", "Evangelos A", "J. Buchli", "S. Schaal" ],
      "venue" : null,
      "citeRegEx" : "Theodorou et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Theodorou et al\\.",
      "year" : 2009
    }, {
      "title" : "Optimal control theory",
      "author" : [ "Todorov", "Emanuel" ],
      "venue" : "Bayesian Brain,",
      "citeRegEx" : "Todorov and Emanuel.,? \\Q2006\\E",
      "shortCiteRegEx" : "Todorov and Emanuel.",
      "year" : 2006
    }, {
      "title" : "Iterative local dynamic programming",
      "author" : [ "Todorov", "Emanuel", "Tassa", "Yuval" ],
      "venue" : "In IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning,",
      "citeRegEx" : "Todorov et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Todorov et al\\.",
      "year" : 2009
    }, {
      "title" : "Robot Trajectory Optimization using Approximate Inference",
      "author" : [ "M. Toussaint" ],
      "venue" : "In Proceedings of the 26th International Conference on Machine Learning,",
      "citeRegEx" : "Toussaint,? \\Q2009\\E",
      "shortCiteRegEx" : "Toussaint",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "Trajectory Optimization methods based on stochastic optimal control (Todorov, 2006; Theodorou et al., 2009; Todorov & Tassa, 2009) have been very successful in learning high dimensional controls in complex settings such as end-to-end control of physical systems (Levine & Abbeel, 2014).",
      "startOffset" : 68,
      "endOffset" : 130
    }, {
      "referenceID" : 4,
      "context" : "As is common in Policy Search (Deisenroth et al., 2013), MOTO operates on a restricted class of parametrized policies πθ, θ ∈ Rθ and is an iterative algorithm comprising two main steps, policy evaluation and policy update.",
      "startOffset" : 30,
      "endOffset" : 55
    }, {
      "referenceID" : 11,
      "context" : "The use of the KL divergence to define the step-size of the policy update has already been successfully applied in prior work (Peters et al., 2010; Levine & Abbeel, 2014; Schulman et al., 2015).",
      "startOffset" : 126,
      "endOffset" : 193
    }, {
      "referenceID" : 12,
      "context" : "The use of the KL divergence to define the step-size of the policy update has already been successfully applied in prior work (Peters et al., 2010; Levine & Abbeel, 2014; Schulman et al., 2015).",
      "startOffset" : 126,
      "endOffset" : 193
    }, {
      "referenceID" : 1,
      "context" : "Efficient algorithms for learning model parameters with a specific semidefinite shape are available (Bhojanapalli et al., 2015).",
      "startOffset" : 100,
      "endOffset" : 127
    }, {
      "referenceID" : 4,
      "context" : "(4) is an arbitrarily complex model then it is common that π′ t, of linear-Gaussian form, is fit by weighted maximumlikelihood (Deisenroth et al., 2013); it is clear though from Eq.",
      "startOffset" : 127,
      "endOffset" : 152
    }, {
      "referenceID" : 2,
      "context" : "robotic applications, using a stochastic policy and updating it under a KL constraint to ensure ’closeness’ of successive policies has shown several empirical successes (Daniel et al., 2012; Levine & Koltun, 2014; Schulman et al., 2015).",
      "startOffset" : 169,
      "endOffset" : 236
    }, {
      "referenceID" : 12,
      "context" : "robotic applications, using a stochastic policy and updating it under a KL constraint to ensure ’closeness’ of successive policies has shown several empirical successes (Daniel et al., 2012; Levine & Koltun, 2014; Schulman et al., 2015).",
      "startOffset" : 169,
      "endOffset" : 236
    }, {
      "referenceID" : 11,
      "context" : "However, only an empirical sample estimate of the objective function is generally optimized (Peters et al., 2010; Schulman et al., 2015), which typically requires a high number of samples and precludes it from a direct application to physical systems.",
      "startOffset" : 92,
      "endOffset" : 136
    }, {
      "referenceID" : 12,
      "context" : "However, only an empirical sample estimate of the objective function is generally optimized (Peters et al., 2010; Schulman et al., 2015), which typically requires a high number of samples and precludes it from a direct application to physical systems.",
      "startOffset" : 92,
      "endOffset" : 136
    }, {
      "referenceID" : 4,
      "context" : "Learning more sophisticated models using for example Gaussian Processes was experimented by (Deisenroth & Rasmussen, 2011) and (Pan & Theodorou, 2014) in the Policy Search and Trajectory Optimization context, but it is still considered to be a challenging task, see (Deisenroth et al., 2013), chapter 3.",
      "startOffset" : 266,
      "endOffset" : 291
    }, {
      "referenceID" : 14,
      "context" : "Instances of such algorithms are for example iLQG (Todorov, 2006), DDP (Jacobson & Mayne, 1970; Theodorou et al., 2010), AICO (Toussaint, 2009) and the trajectory optimization algorithm used in the GPS algorithm (Levine & Abbeel, 2014).",
      "startOffset" : 71,
      "endOffset" : 119
    }, {
      "referenceID" : 18,
      "context" : ", 2010), AICO (Toussaint, 2009) and the trajectory optimization algorithm used in the GPS algorithm (Levine & Abbeel, 2014).",
      "startOffset" : 14,
      "endOffset" : 31
    }, {
      "referenceID" : 9,
      "context" : "We use the analytical player of (Mülling et al., 2011) to generate a single forehand stroke, which is subsequently used to learn from demonstration the initial policy π.",
      "startOffset" : 32,
      "endOffset" : 54
    }, {
      "referenceID" : 11,
      "context" : "We compare MOTO to the REPS policy search algorithm (Peters et al., 2010) and the stochastic search algorithm MORE (Abdolmaleki et al.",
      "startOffset" : 52,
      "endOffset" : 73
    } ],
    "year" : 2016,
    "abstractText" : "Many of the recent Trajectory Optimization algorithms alternate between local approximation of the dynamics and conservative policy update. However, linearly approximating the dynamics in order to derive the new policy can bias the update and prevent convergence to the optimal policy. In this article, we propose a new model-free algorithm that backpropagates a local quadratic time-dependent Q-Function, allowing the derivation of the policy update in closed form. Our policy update ensures exact KL-constraint satisfaction without simplifying assumptions on the system dynamics demonstrating improved performance in comparison to related Trajectory Optimization algorithms linearizing the dynamics.",
    "creator" : "LaTeX with hyperref package"
  }
}