{
  "name" : "1511.07275.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Wojciech Zaremba", "Tomas Mikolov", "Armand Joulin", "Rob Fergus" ],
    "emails" : [ "woj.zaremba@gmail.com", "tmikolov@fb.com", "ajoulin@fb.com", "robfergus@fb.com" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Many every day tasks require a multi-step interaction with the world. For example, picking an apple from a tree requires visual localization of the apple; extending the arm and then fine muscle control, guided by visual feedback, to pluck it from the tree. While each individual procedure is not complex, the task nevertheless requires careful sequencing of operations across both visual and motor systems.\nThis paper explores how machines can learn algorithms involving a similar compositional structure. Since our emphasis is on learning the correct sequence of operations, we consider the domain of arithmetic where the operations themselves are very simple. For example, although learning to add two digits is straightforward, solving addition of two multi-digit numbers requires precise coordination of this operation with movement over the sequence and recording of the carry. We explore a variety of algorithms in this domain, including complex tasks involving addition and multiplication.\nOur approach formalizes the notion of a central controller that interacts with the world via a set of interfaces, appropriate to the task at hand. The controller is a neural network model which must learn to control the interfaces, via a set of discrete actions (e.g. “move input tape left”, “read”, “write symbol to output tape”, “write nothing this time step” ) to produce the correct output for given input patterns. Specifically, we train the controller from large sets of examples of input and output patterns using reinforcement learning. Our reward signal is sparse, only being received when the model emits the correct symbol on the output tape.\nWe consider two separate settings. In the first, we provide supervision in the form of ground truth actions. In the second, we train only with input-output pairs (i.e. no supervision over actions). While we are able to solve all the tasks in the latter case, the supervised setting provides insights about the model limitations and an upper bound on the performance. We evaluate our model on sequences far longer than those present during training. Surprisingly, we find that controllers with even modest capacity to recall previous states can easily overfit the short training sequences and not generalize to the test examples, even if the correct actions are provided. Even with an appropriate controller, off-the-shelf Q-learning fails on the majority of our tasks. We therefore introduce a series of modifications that dramatically improve performance. These include: (i) a novel dynamic discount term that makes the reward invariant to the sequence length; (ii) an extra penalty that aids generalization and (iii) the deployment of Watkins Q-lambda [Sutton & Barto (1998)].\nWe would like to direct the reader to the video accompanying this paper (https://youtu.be/GVe6kfJnRAw). This gives a concise overview of our approach and complements the following explanations. Full source code for this work can be found at https://github.com/wojzaremba/algorithm-learning.\n§Work done while the author was at Facebook AI Research.\nar X\niv :1\n51 1.\n07 27\n5v 1\n[ cs\n.A I]\n2 3\nN ov\n2 01\n5"
    }, {
      "heading" : "2 MODEL",
      "text" : "Our model consists of an RNN-based controller that accesses the environment through a series of pre-defined interfaces. Each interface has a specific structure and set of actions it can perform. The interfaces are manually selected according to the task (see Section 3). The controller is the only part of the system that learns and has no prior knowledge of how the interfaces operate. Thus the controller must learn the sequence of actions over the various interfaces that allow it to solve a task. We make use of three different interfaces:\nInput Tape: This provides access to the input data symbols stored on an “infinite” 1-D tape. A read head accesses a single character at a time through the read action. The head can be moved via the left and right actions.\nInput Grid: This is a 2D version of the input tape where the read head can now be moved by actions up, down, left and right.\nOutput Tape: This is similar to the input tape, except that the head now writes a single symbol at a time to the tape, as provided the controller. The vocabulary includes a no-operation symbol (NOP) enabling the controller to defer output if it desires. During training, the written and target symbols are compared using a cross-entropy loss. This provides a differentiable learning signal that is used in addition to the sparse reward signal provided by the Q-learning.\nFig. 1(a) shows examples of the input tape and grid interfaces. Fig. 1(b) gives an overview of our controller–interface abstraction and Fig. 1(c) shows an example of this on the addition task (for two time steps).\nFor the controller, we explore several recurrent neural network architectures: two different sizes of 1-layer LSTM [Hochreiter & Schmidhuber (1997)], a gated-recurrent unit (GRU)[Cho et al. (2014)] and a vanilla feed-forward network. Note that RNN-based models are able to remember previous network state, unlike the the feed-forward network. This is important because some tasks explicitly require some form of memory, e.g. the carry in addition.\nThe simple algorithms we consider (see Section 3) have deterministic solutions that can be expressed as a finite state automata. Thus during training we hope the controller will implicitly learn the correct automata from the training samples, since this would ensure generalization to sequences of arbitrary length. On some tasks like reverse, we observe a higher-order form of over-fitting: the model learns to solve the training tasks correctly and generalizes successfully to test sequences of the same length (thus is not over-fitting in the standard sense). However, when presented with longer test sequences the model fails completely. This suggests that the model has converged to an incorrect local minima, one corresponding to an alternate automata which have an implicit awareness of the sequence length of which they were trained. See Fig. 4 for an example of this on the reverse task. Note that this behavior results from the controller, not the learning scheme, since it is present in both the supervised (Section 5) and Q-learning settings (Section 6). These experiments show the need\nto carefully adjust the controller capacity to prevent it learning any dependencies on the length of training sequences, yet ensuring it has enough state to implement the algorithm in question.\nAs illustrated in Fig. 1(c), the controller passes two signals to the output tape: a discrete action (move left, move right, write something) and a symbol from the vocabulary. This symbol is produced by taking the max from the softmax output on the top of the controller. In training, two different signals are computed from this: (i) a cross-entropy loss is used to compare the softmax output to the target symbol and (ii) a discrete 1/0 reward if the symbol is correct/incorrect. The first signal gives a continuous gradient to update the controller parameters via backpropagation. Leveraging the reward requires reinforcement learning, since many actions might occur before a symbol is written to the output tape. Thus the action output of the controller is trained with reinforcement learning and the symbol output is trained by backpropagation."
    }, {
      "heading" : "3 TASKS",
      "text" : "We consider six different tasks: copy, reverse, walk, multi-digit addition, 3 number addition and single digit multiplication. The input interface for copy and reverse is an input tape, but an input grid for the others. All tasks use an output tape interface. Unless otherwise stated, all arithmetic operations use base 10. Examples of the six tasks are shown in Fig. 2.\nCopy: This task involves copying the symbols from the input tape to the output tape. Although simple, the model still has to learn the correspondence between input and output symbols, as well as executing the move right action on the input tape.\nReverse: Here the goal is to reverse a sequence of symbols on the input tape. We provide a special character “r” to indicate the end of the sequence. The model must learn to move right multiple times until it hits the “r” symbol, then move to the left, copying the symbols to the output tape.\nWalk: The goal is to copy symbols, according to the directions given by an arrow symbol. The controller starts by moving to the right (suppressing prediction) until reaching one of the symbols ↑, ↓,←. Then it should change it’s direction accordingly, and copy all symbols encountered to the output tape.\nAddition: The goal is to add two multi-digit sequences, provided on an input grid. The sequences are provided in two adjacent rows, with their right edges aligned. The initial position of the read head is the last digit of the top number (i.e. upper-right corner). The model has to: (i) memorize an addition table for pairs of digits; (ii) learn how to move over the input grid and (iii) discover the concept of a carry.\n3 Number Addition: As for the addition task, but now three numbers are to be added. This is more challenging as the reward signal is less frequent (since more correct actions must be completed before a correct output digit can be produced). Also the carry now can take on three states (0, 1 and 2), compared with two for the 2 number addition task.\nSingle Digit Multiplication: This involves multiplying a single digit with a long multi-digit number. It is of similar complexity to the 2 number addition task, except that the carry can take on more values ∈ [0, 8]."
    }, {
      "heading" : "4 RELATED WORK",
      "text" : "A variety of recent work has explored the learning of simple algorithms. Many of them are different embodiments of the controller-interface abstraction formalized in our model. The Neural Turing Machine (NTM) [Graves et al. (2014)] uses a modified LSTM [Hochreiter & Schmidhuber (1997)] as the controller, and has three inferences: sequential input, delayed output and a differentiable memory. The model is able to learn simple algorithms including copying and sorting. The Stack RNN [Joulin & Mikolov (2015)] has an RNN controller and three interfaces: sequential input, a stack memory and sequential output. The learning of simple binary patterns and regular expressions is demonstrated. A closely related work to this is [Das et al. (1992)], which was recently extended in the Neural DeQue [Grefenstette et al. (2015)] to use a list instead. End-to-End Memory Networks [Sukhbaatar et al. (2015)] use a feed-forward network as the controller and interfaces consisting of a soft-attention input, plus a delayed output (by a fixed number of “hops”). The model is applied to simple Q&A tasks, some of which involve logical reasoning. In contrast, our model automatically determines when to produce output and uses more general interfaces.\nHowever, most of these approaches use continuous interfaces that permit training via backpropagation of gradients. Our approach differs in that it uses discrete interfaces thus is more challenging to train since as we must rely on reinforcement learning instead. A notable exception is the Reinforcement Learning Neural Turing Machine (RLNTM) [Zaremba & Sutskever (2015)] which is a version of the NTM with discrete interfaces. The Stack-RNN [Joulin & Mikolov (2015)] also uses a discrete search procedure for its interfaces but it is unclear how this would scale to larger problems.\nThe problem of learning algorithms has its origins in the field of program induction [Nordin (1997); Liang et al. (2013); Wineberg & Oppacher (1994); Solomonoff (1964)]. In this domain, the model has to infer the source code of a program that solves a given problem. This is a similar goal to ours, but in quite a different setting. I.e. we do produce a computer program, but rather a neural net that can operate with interfaces such as tapes, so implements the program without being humanreadable. A more relevant work is [Schmidhuber (2004)] which learns an algorithms for the Hanoi tower problem, using a simple form of program induction and incremental learning components. Genetic algorithms [Holland (1992); Goldberg (1989)] also can be considered a form of program induction, but are based on a random search strategy rather than a learned one.\nSimilar to [Mnih et al. (2013)], we train the controller to approximate the Q-function. However, we introduce several modifications on top of the classical Q-learning. First, we use Watkins Q(λ) [Watkins (1989); Sutton & Barto (1998)]. This helps to overcome a non-stationary environment. We are unaware of any prior work that uses Watkins Q(λ) for this purpose. Second, we reparametrized Q function, to become invariant to the sequence length. Finally, we penalize ||Q(s, •)||, which might help to remove positive bias [Hasselt (2010)]."
    }, {
      "heading" : "5 SUPERVISED EXPERIMENTS",
      "text" : "To understand the behavior of our model and to provide an upper bound on performance, we train our model in a supervised setting, i.e. where the ground truth actions are provided. Note that the controller must still learn which symbol to output. But this now can be done purely with backpropagation since the actions are known.\nTo facilitate comparisons of difficulty between tasks, we use a common measure of complexity, corresponding to the number of time steps required to solve each task (using the ground truth actions∗). For instance, a reserve task involving a sequence of length 10 requires 20 time-steps (10 steps to move to the “r” and 10 steps to move back to the start). The conversion factors between sequence lengths and complexity are as follows: copy=1; reverse=2; walk=1; addition=2; 3 row addition=3 and single digit multiplication=1.\nFor each task, we train a separate model, starting with sequences of complexity 6 and incrementing by 4 once it achieves 100% accuracy on held-out examples of the current length. Training stops once the model successfully generalizes to examples of complexity 1000. Three different cores for the controllers are explored: (i) a 200 unit, 1-layer LSTM; (iii) a 200 unit, 1-layer GRU model and ∗In practice, multiple solutions can exist (see Appendix A), thus the measure is approximate.\n(iii) a 200 unit, 1-layer feed-forward network. An additional linear layer is placed on top of these model that maps the hidden state to either action for a given interface, or the target symbol.\nIn Fig. 3 we show the accuracy of the different controllers on the six tasks for test instances of increasing complexity, up to 20, 000 time-steps. The simple feed-forward controller generalizes perfectly on the copy, reverse and walk tasks but completely fails on the remaining ones, due to a lack of required memory†. The RNN-based controllers succeed to varying degrees, although some variability in performance is observed.\nFurther insight can be obtained by examining the internal state of the controller. To do this, we compute the autocorrelation matrix‡ A of the network state over time when the model is processing a reverse task example of length 35, having been trained on sequences of length 10 or shorter. For this problem there should be two distinct states: move right until “r” is reached and then move left to the start. Fig. 1 plots A for models with three different controllers. The larger the controller capacity, the less similar the states are within the two phases of execution, showing how it has not captured the correct algorithm. The figure also shows the confidence in the two actions over time. In the case of the high capacity models, the initial confidence in the move left action is high, but this drops off after moving along the sequence. This is because the controller has learned during training that it should change direction after at most 10 steps. Consequently, the unexpectedly long test sequence makes it unsure of what the correct action is. By contrast, the simple feed-forward controller does not show this behavior since it is stateless, thus has no capacity to know where it is within a sequence. The equivalent automata is shown in Fig. 4(a), while Fig. 4(b) shows the incorrect time-dependent automata learned by the over-expressive RNN-based controllers. We note that this argument is empirically supported by our results in Table 2, as well as related work such as [Graves et al. (2014)] and [Joulin & Mikolov (2015)] which found limited capacity controllers to be most effective. For example, in the latter case, the counting and memorization tasks used controllers with just 40 and 100 units respectively."
    }, {
      "heading" : "6 Q-LEARNING",
      "text" : "In the previous section, we assumed that the optimal controller actions were given during training. This meant only the output symbols need to be predicted and these could be learned via backpropagation. We now consider the setting where the actions are also learned, to test the true capabilities of the models to learn simple algorithms from pairs of input and output sequences.\nWe use Q-learning, a standard reinforcement learning algorithm to learn a sequence of discrete actions that solves a problem. A function Q, the estimated sum of future rewards, is updated during\n†Ammending the interfaces to allow both reading and writing on the same interface would provide a mechanism for long-term memory, even with a feed-forward controller. But then the same lack of generalization issues (encountered with more powerful controllers) would become an issue. ‡Let hi be the controller state at time i, then the autocorrelation Ai,j between time-steps i and j is given by Ai,j = 〈hi−E,hj−E〉\nσ2 , i, j = 1, . . . , T where E = ∑T k=1 hk T , σ2 = ∑T k=1〈hk−E,hk−E〉 T\n. T is the number of time steps (i.e. complexity).\ntraining according to: Qt+1(s, a) = Qt(s, a)− α [ Qt(s, a)− ( R(s′) + γmax\na Qn(s\n′, a) )]\n(1)\nTaking the action a in state s causes a transition to state s′, which in our case is deterministic. R(s′) is the reward experienced in the state s′. The discount factor is γ and α is the learning rate. The another commonly considered quantity is V (s) = maxaQ(s, a). V is called the value function, and V (s) is the expected sum of future rewards starting from the state s. Moreover, Q∗ and V ∗ are function values for the optimal policy.\nOur controller receives a reward of 1 every time it correctly predicts a digit (and 0 otherwise). Since the overall solution to the task requires all digits to be correct, we terminate a training episode as soon as an incorrect prediction is made. This learning environment is non-stationary, since even if the model initially picks the right actions, the symbol prediction is unlikely to be correct, so the model receives no reward. But further on in training, when the symbol prediction is more reliable, the correct action will be rewarded§. This is important because reinforcement learning algorithms assume stationarity of the environment, which is not true in our case. Learning in non-stationary environments is not well understood and there are no definitive methods to deal with it. However, empirically we find that this non-stationarity can be partially addressed by the use of Watkins Q(λ) [Watkins (1989)], as detailed in Section 6.2. §If we were to use reinforcement to train the symbol output as well as the actions, then the environment would be stationary. However, this would mean ignoring the reliable signal available from direct backpropagation of the symbol output."
    }, {
      "heading" : "6.1 DYNAMIC DISCOUNT",
      "text" : "The purpose of the reinforcement learning is to learn a policy that yields the highest sum of the future rewards. Q-learning does it indirectly by learning a Q-function. The optimal policy can be extracted by taking argmax over Q(s, •). Note that shifting or scaling Q induces the same policy. We propose to dynamically rescale Q so (i) it is independent of the length of the episode and (ii) Q is within a small range, making it easier to predict.\nWe define Q̂ to be our reparametrization. Q̂(s, a) should be roughly in range [0, 1], and it should correspond to how close we are to V ∗(s). Q could be decomposed multiplicatively as Q(s, a) = Q̂(s, a)V ∗(s). However, in practice, we do not have access to V ∗(s), thus instead we use an estimate of future rewards based on the total number of digits left in the sequence. Since every correct prediction yields a reward of 1, the optimal policy should achieve sum of future rewards equal to the number of remaining symbols to predict. The number of remaining symbols to predict is known and we denote it by V̂ (s). Note that this is a form of supervision, albeit a weak one.\nTherefore, we normalize the Q-function by the remaining sum of rewards left in the task:\nQ̂(s, a) := Q(s, a)\nV̂ (s)\nWe assume that s transitions to s′, and we re-write the Q-learning update equations:\nQ̂(s, a) = R(s′)\nV̂ (s) + γmax a\nV̂ (s′) V̂ (s) Q̂(s′, a)\nQ̂t+1(s, a) = Q̂t(s, a)− α [ Q̂t(s, a)− (R(s′) V̂ (s) + γmax a V̂ (s′) V̂ (s) Q̂t(s ′, a) )]\nNote that V̂ (s) ≥ V̂ (s′), with equality if no digit was predicted at the current time-step. As the episode progresses, the discount factor V̂ (s ′)\nV̂ (s) decreases, making the model greedier. At the end of\nthe sequence, the discount drops to 12 .\n6.2 WATKINS Q(λ)\nThe update to Q(s, a) in Eqn. 1 comes from two parts: the observed reward R(s′) and the estimated future reward Q(s′, a). In our setting, there are two factors that make the former far more reliable than the latter: (i) rewards are deterministic and (ii) the non-stationarity (induced by the ongoing learning of the symbol output by backpropagation) means that estimates of Q(s, a) are unreliable as environment evolves. Consequently, the single action recurrence used in Eqn. 1 can be improved upon when on-policy actions are chosen. More precisely, let at, at+1, . . . , at+T be consecutive actions induced by Q:\nat+i = argmax a Q(st+i, a)\nst+i at+i−−−→ st+i+1\nThen the optimal Q∗ follows the following recursive equation:\nQ∗(st, at) = T∑ i=1 γi−1R(st+i) + γ T max a Q∗(st+n+1, a)\nand the update rule corresponding to Eqn. 1 becomes:\nQt+1(st, at) = Qt(st, at)− α [ Qt(st, at)− ( T∑ i=1 γi−1R(st+i) + γ T max a Qt(st+n+1, a) )] This is a special form of Watkins Q(λ) [Watkins (1989)] where λ = 1. The classical applications of Watkins Q(λ) suggest choosing a small λ, which trades-off estimates based on various numbers of future rewards. λ = 0 rolls back to the classical Q-learning. Due to reliability of our rewards, we found λ = 1 to be better than λ < 1, however this needs further study.\nNote that this unrolling of rewards can only take place until a non-greedy action is taken. When using an -greedy policy, this means we would expect to be able to unroll −1 steps, on average. For the value of = 0.05 used in our experiments, this corresponds to 20 steps on average.\n6.3 PENALTY ON Q-FUNCTION\nAfter reparameterizing the Q-function to Q̂ (Section 6.1), the optimal Q̂∗(s, a) should be 1 for the correct action and zero otherwise. To encourage our estimate Q̂(s, a) to converge to this, we introduce a penalty that “pushes down” on incorrect actions: κ‖ ∑ a Q̂(s, a) − 1‖2. This has the effect of introducing a margin between correct and incorrect actions, greatly improving generalization. We commence training with κ = 0 and make it non-zero once good accuracy is reached on short samples (introducing it from the outset hurts learning)."
    }, {
      "heading" : "6.4 REINFORCEMENT LEARNING EXPERIMENTS",
      "text" : "We apply our enhancements to the six tasks in a series of experiments designed to examine the contribution of each of them. Unless otherwise specified, the controller is a 1-layer GRU model with 200 units. This was selected on the basis of its mean performance across the six tasks in the supervised setting (see Section 5). As the performance of reinforcement learning methods tend to be highly stochastic, we repeat each experiment 10 times with a different random seed. Each model is trained using 3× 107 characters which takes ∼ 4 hrs. A model is considered to have successfully solved the task if it able to give a perfect answer to 50 test instances, each 100 digits in length. The GRU model is trained with a batch size of 20, a learning rate of α = 0.1, using the same initialization as [Glorot & Bengio (2010)] but multiplied by 2. All tasks are trained with the same curriculum used in the supervised experiments (and in [Joulin & Mikolov (2015)]), whereby the sequences are initially of complexity 6 (corresponding to 2 or 3 digits, depending on the task) and once 100% accuracy is achieved, increased by 4 until the model is able to solve validation sequences of length 100.\nFor 3-row addition, a more elaborate curriculum was needed which started with examples that did not involve a carry and contained many zero. The test distribution was unaffected. Some examples: 1 2 2 ; 2 0 2 ; 8 3 3 3 3 7 ; 3 2 0 6 9 1 3 1 3 1 2 8 0 8 3 ; 8 0 1 8 5 2 0 2 1 1 3 1 4 0 7 0 5 4 3 1 3 2 7 5 0 7 1 .\nWe show results for various combinations of terms in Table 2. The experiments demonstrate that standard Q-learning fails on most of our tasks (first six columns). Each of our additions (dynamic discount, Watkins Q(λ) and penalty term) give significant improvements. When all three are used our model is able to succeed at all tasks, providing the appropriate curriculum and controller are used. For the reverse and walk tasks, the default GRU controller failed completely. However, using a feed-forward controller instead enabled the model to succeed, when dynamic discount and Watkins Q(λ) was used. As noted above, the 3-row addition required a more careful curriculum before the model was able to learn successfully. Increasing the capacity of the controller (columns 2-4) hurts performance, echoing Fig. 1. The last two columns of Table 2 show results on test sequences of length 1000. Except for multiplication, the models still generalized successfully.\nFig. 5 shows accuracy as a function of test example complexity for standard Q-learning and our enhanced version. The difference is performance is clear. At very high complexity, corresponding to 1000’s of digits, the accuracy starts to drop on the more complicated tasks. We note that these trends are essentially the same as those observed in the supervised setting (Fig. 3), suggesting that Q-learning is not to blame. Instead, the inability of the controller to learn an automata seems to be the cause. Potential solutions to this might include (i) noise injection, (ii) discretization of state, or (iii) a state error correction mechanism. However, this issue, the inability of RNN to perfectly represent an automata can be examined separately from the setting where actions have to be learnt (i.e. in the supervised domain).\nFurther results can be found in the appendices. For the addition task, our model was able to discover multiple correct solutions, each with a different movement pattern over the input tape (see Appendix A). Table 3 in Appendix B sheds light on the trade-off between errors in actions and errors in symbol prediction by varying the base used in the arithmetic operations and hence the size of the\ntarget vocabulary. Appendix C explores the use of non-integer rewards. Surprisingly, this slows down training, relative to the 0/1 reward structure."
    }, {
      "heading" : "7 DISCUSSION",
      "text" : "We have explored the ability of neural network models to learn algorithms for simple arithmetic operations. Through experiments with supervision and reinforcement learning, we have shown that they are able to do this successfully, albeit with caveats. Q-learning was shown to work as well as the supervised case. But, disappointingly, we were not able to find a single controller that could solve all tasks. We found that for some tasks, generalization ability was sensitive to the memory capacity of the controller: too little and it would be unable to solve more complex tasks that rely on carrying state across time; too much at the resulting model would overfit the length of the training sequences. Finding automatic methods to control model capacity would seem to be important in developing robust models for this type of learning problem."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "We wish to thank Jason Weston, Marc’Aurelio Ranzato and Przemysław Mazur for useful discussions, and comments. We also thank Christopher Olah for LSTM figures that have been used in the paper and the accompanying video."
    }, {
      "heading" : "APPENDIX A: DIFFERENT SOLUTIONS TO ADDITION TASK",
      "text" : "On examination of the models learned on the addition task, we notice that three different solutions were discovered. While they all give the correct answer, they differ in their actions over the input grid, as shown in Fig. 6."
    }, {
      "heading" : "APPENDIX B:REWARD FREQUENCY VS REWARD RELIABILITY",
      "text" : "We explore how learning time varies as the size of the target vocabulary is varied. This trades off reward frequency and reliability. For small vocabularies, the reward occurs more often but is less reliable since the chance of the wrong action sequence yielding the correct result is relatively high (and vice-versa for for larger vocabularies). For copying and reverse tasks, altering the vocabulary size just alters the variety of symbols on the tape. However, for the arithmetic operations this involves a change of base, which influences the task in a more complex way. For instance, addition in base 4 requires the memorization of digit-to-digit addition table of size 16 instead of 100 for the base 10. Table 3 shows the median training time as a function of vocabulary size. The results suggest that an infrequent but reliable reward is preferred to a frequent but noisy one."
    }, {
      "heading" : "APPENDIX C: REWARD STRUCTURE",
      "text" : "Reward in reinforcement learning systems drives the learning process. In our setting we control the rewards, deciding when, and how much to give. We now examine various kinds of rewards and their influence on the learning time of our system.\nOur vanilla setting gives a reward of 1 for every correct prediction, and reward 0 for every incorrect one. We refer to this setting as “0/1 reward”. We consider two other settings in addition to this, both of which rely on the probabilities of the correct prediction. Let y be the target symbol and pi = p(y = i), i ∈ [0, 9] be the probability of predicting label i. In setting “Discretized reward”, we sort pi. That gives us an order on indices a1, a2, . . . , a10, i.e. pa1 ≥ pa2 ≥ pa3 · · · ≥ pa10 . “Discretized reward” yields reward 1 iff a1 ≡ y, reward 12 iff a2 ≡ y, and reward 13 iff a3 ≡ y. Otherwise, environment gives a reward 0. In the “Continuous\nreward” setting, a reward of py is given for every prediction. One could also consider reward log(py), however this quantity is unbounded, and further processing might be necessary to make it work.\nTable 4 gives results for the three different reward structures, showing training time for the five tasks (training is stopped once the model generalizes to test sequences of length 100). One might expect that a continuous reward would convey more information than a discrete one, thus result in faster training. However, the results do not support this hypothesis, as training seems harder with continuous reward than a discrete one. We hypothesize, that the continuous reward makes environment less stationary, which might make Q-learning less efficient, although this needs further verification."
    } ],
    "references" : [ {
      "title" : "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "author" : [ "Cho", "Kyunghyun", "van Merrienboer", "Bart", "Gulcehre", "Caglar", "Bougares", "Fethi", "Schwenk", "Holger", "Bengio", "Yoshua" ],
      "venue" : "arXiv preprint arXiv:1406.1078,",
      "citeRegEx" : "Cho et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning context-free grammars: Capabilities and limitations of a recurrent neural network with an external stack memory",
      "author" : [ "Das", "Sreerupa", "Giles", "C Lee", "Sun", "Guo-Zheng" ],
      "venue" : "Proceedings of The Fourteenth Annual Conference of Cognitive Science Society,",
      "citeRegEx" : "Das et al\\.,? \\Q1992\\E",
      "shortCiteRegEx" : "Das et al\\.",
      "year" : 1992
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "Glorot", "Xavier", "Bengio", "Yoshua" ],
      "venue" : "In International conference on artificial intelligence and statistics,",
      "citeRegEx" : "Glorot et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Glorot et al\\.",
      "year" : 2010
    }, {
      "title" : "Genetic Algorithms in Search, Optimization and Machine Learning",
      "author" : [ "Goldberg", "David E" ],
      "venue" : "AddisonWesley Longman Publishing Co., Inc.,",
      "citeRegEx" : "Goldberg and E.,? \\Q1989\\E",
      "shortCiteRegEx" : "Goldberg and E.",
      "year" : 1989
    }, {
      "title" : "Neural turing machines",
      "author" : [ "Graves", "Alex", "Wayne", "Greg", "Danihelka", "Ivo" ],
      "venue" : "arXiv preprint arXiv:1410.5401,",
      "citeRegEx" : "Graves et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning to transduce with unbounded memory",
      "author" : [ "Grefenstette", "Edward", "Hermann", "Karl Moritz", "Suleyman", "Mustafa", "Blunsom", "Phil" ],
      "venue" : "arXiv preprint arXiv:1506.02516,",
      "citeRegEx" : "Grefenstette et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Grefenstette et al\\.",
      "year" : 2015
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Hochreiter", "Sepp", "Schmidhuber", "Jürgen" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Hochreiter et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter et al\\.",
      "year" : 1997
    }, {
      "title" : "Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control and Artificial Intelligence",
      "author" : [ "Holland", "John H" ],
      "venue" : null,
      "citeRegEx" : "Holland and H.,? \\Q1992\\E",
      "shortCiteRegEx" : "Holland and H.",
      "year" : 1992
    }, {
      "title" : "Inferring algorithmic patterns with stack-augmented recurrent nets",
      "author" : [ "Joulin", "Armand", "Mikolov", "Tomas" ],
      "venue" : "arXiv preprint arXiv:1503.01007,",
      "citeRegEx" : "Joulin et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Joulin et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning dependency-based compositional semantics",
      "author" : [ "Liang", "Percy", "Jordan", "Michael I", "Klein", "Dan" ],
      "venue" : "Computational Linguistics,",
      "citeRegEx" : "Liang et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2013
    }, {
      "title" : "Playing atari with deep reinforcement learning",
      "author" : [ "Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Silver", "David", "Graves", "Alex", "Antonoglou", "Ioannis", "Wierstra", "Daan", "Riedmiller", "Martin" ],
      "venue" : "arXiv preprint arXiv:1312.5602,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2013
    }, {
      "title" : "Evolutionary program induction of binary machine code and its applications",
      "author" : [ "Nordin", "Peter" ],
      "venue" : "Krehl Munster,",
      "citeRegEx" : "Nordin and Peter.,? \\Q1997\\E",
      "shortCiteRegEx" : "Nordin and Peter.",
      "year" : 1997
    }, {
      "title" : "Optimal ordered problem solver",
      "author" : [ "Schmidhuber", "Jürgen" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Schmidhuber and Jürgen.,? \\Q2004\\E",
      "shortCiteRegEx" : "Schmidhuber and Jürgen.",
      "year" : 2004
    }, {
      "title" : "A formal theory of inductive inference",
      "author" : [ "Solomonoff", "Ray J" ],
      "venue" : "Part I. Information and control,",
      "citeRegEx" : "Solomonoff and J.,? \\Q1964\\E",
      "shortCiteRegEx" : "Solomonoff and J.",
      "year" : 1964
    }, {
      "title" : "Weakly supervised memory networks",
      "author" : [ "Sukhbaatar", "Sainbayar", "Szlam", "Arthur", "Weston", "Jason", "Fergus", "Rob" ],
      "venue" : "arXiv preprint arXiv:1503.08895,",
      "citeRegEx" : "Sukhbaatar et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sukhbaatar et al\\.",
      "year" : 2015
    }, {
      "title" : "Reinforcement learning: An introduction, volume 1",
      "author" : [ "Sutton", "Richard S", "Barto", "Andrew G" ],
      "venue" : "MIT press Cambridge,",
      "citeRegEx" : "Sutton et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 1998
    }, {
      "title" : "Learning from Delayed Rewards",
      "author" : [ "Watkins", "Chris" ],
      "venue" : "PhD thesis, Cambrdige University,",
      "citeRegEx" : "Watkins and Chris.,? \\Q1989\\E",
      "shortCiteRegEx" : "Watkins and Chris.",
      "year" : 1989
    }, {
      "title" : "A representation scheme to perform program induction in a canonical genetic algorithm. In Parallel Problem Solving from NaturePPSN",
      "author" : [ "Wineberg", "Mark", "Oppacher", "Franz" ],
      "venue" : null,
      "citeRegEx" : "Wineberg et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Wineberg et al\\.",
      "year" : 1994
    }, {
      "title" : "Reinforcement learning neural turing machines",
      "author" : [ "Zaremba", "Wojciech", "Sutskever", "Ilya" ],
      "venue" : "arXiv preprint arXiv:1505.00521,",
      "citeRegEx" : "Zaremba et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zaremba et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "For the controller, we explore several recurrent neural network architectures: two different sizes of 1-layer LSTM [Hochreiter & Schmidhuber (1997)], a gated-recurrent unit (GRU)[Cho et al. (2014)] and a vanilla feed-forward network.",
      "startOffset" : 179,
      "endOffset" : 197
    }, {
      "referenceID" : 3,
      "context" : "The Neural Turing Machine (NTM) [Graves et al. (2014)] uses a modified LSTM [Hochreiter & Schmidhuber (1997)] as the controller, and has three inferences: sequential input, delayed output and a differentiable memory.",
      "startOffset" : 33,
      "endOffset" : 54
    }, {
      "referenceID" : 3,
      "context" : "The Neural Turing Machine (NTM) [Graves et al. (2014)] uses a modified LSTM [Hochreiter & Schmidhuber (1997)] as the controller, and has three inferences: sequential input, delayed output and a differentiable memory.",
      "startOffset" : 33,
      "endOffset" : 109
    }, {
      "referenceID" : 3,
      "context" : "The Neural Turing Machine (NTM) [Graves et al. (2014)] uses a modified LSTM [Hochreiter & Schmidhuber (1997)] as the controller, and has three inferences: sequential input, delayed output and a differentiable memory. The model is able to learn simple algorithms including copying and sorting. The Stack RNN [Joulin & Mikolov (2015)] has an RNN controller and three interfaces: sequential input, a stack memory and sequential output.",
      "startOffset" : 33,
      "endOffset" : 332
    }, {
      "referenceID" : 1,
      "context" : "A closely related work to this is [Das et al. (1992)], which was recently extended in the Neural DeQue [Grefenstette et al.",
      "startOffset" : 35,
      "endOffset" : 53
    }, {
      "referenceID" : 1,
      "context" : "A closely related work to this is [Das et al. (1992)], which was recently extended in the Neural DeQue [Grefenstette et al. (2015)] to use a list instead.",
      "startOffset" : 35,
      "endOffset" : 131
    }, {
      "referenceID" : 1,
      "context" : "A closely related work to this is [Das et al. (1992)], which was recently extended in the Neural DeQue [Grefenstette et al. (2015)] to use a list instead. End-to-End Memory Networks [Sukhbaatar et al. (2015)] use a feed-forward network as the controller and interfaces consisting of a soft-attention input, plus a delayed output (by a fixed number of “hops”).",
      "startOffset" : 35,
      "endOffset" : 208
    }, {
      "referenceID" : 1,
      "context" : "A closely related work to this is [Das et al. (1992)], which was recently extended in the Neural DeQue [Grefenstette et al. (2015)] to use a list instead. End-to-End Memory Networks [Sukhbaatar et al. (2015)] use a feed-forward network as the controller and interfaces consisting of a soft-attention input, plus a delayed output (by a fixed number of “hops”). The model is applied to simple Q&A tasks, some of which involve logical reasoning. In contrast, our model automatically determines when to produce output and uses more general interfaces. However, most of these approaches use continuous interfaces that permit training via backpropagation of gradients. Our approach differs in that it uses discrete interfaces thus is more challenging to train since as we must rely on reinforcement learning instead. A notable exception is the Reinforcement Learning Neural Turing Machine (RLNTM) [Zaremba & Sutskever (2015)] which is a version of the NTM with discrete interfaces.",
      "startOffset" : 35,
      "endOffset" : 919
    }, {
      "referenceID" : 1,
      "context" : "A closely related work to this is [Das et al. (1992)], which was recently extended in the Neural DeQue [Grefenstette et al. (2015)] to use a list instead. End-to-End Memory Networks [Sukhbaatar et al. (2015)] use a feed-forward network as the controller and interfaces consisting of a soft-attention input, plus a delayed output (by a fixed number of “hops”). The model is applied to simple Q&A tasks, some of which involve logical reasoning. In contrast, our model automatically determines when to produce output and uses more general interfaces. However, most of these approaches use continuous interfaces that permit training via backpropagation of gradients. Our approach differs in that it uses discrete interfaces thus is more challenging to train since as we must rely on reinforcement learning instead. A notable exception is the Reinforcement Learning Neural Turing Machine (RLNTM) [Zaremba & Sutskever (2015)] which is a version of the NTM with discrete interfaces. The Stack-RNN [Joulin & Mikolov (2015)] also uses a discrete search procedure for its interfaces but it is unclear how this would scale to larger problems.",
      "startOffset" : 35,
      "endOffset" : 1015
    }, {
      "referenceID" : 1,
      "context" : "A closely related work to this is [Das et al. (1992)], which was recently extended in the Neural DeQue [Grefenstette et al. (2015)] to use a list instead. End-to-End Memory Networks [Sukhbaatar et al. (2015)] use a feed-forward network as the controller and interfaces consisting of a soft-attention input, plus a delayed output (by a fixed number of “hops”). The model is applied to simple Q&A tasks, some of which involve logical reasoning. In contrast, our model automatically determines when to produce output and uses more general interfaces. However, most of these approaches use continuous interfaces that permit training via backpropagation of gradients. Our approach differs in that it uses discrete interfaces thus is more challenging to train since as we must rely on reinforcement learning instead. A notable exception is the Reinforcement Learning Neural Turing Machine (RLNTM) [Zaremba & Sutskever (2015)] which is a version of the NTM with discrete interfaces. The Stack-RNN [Joulin & Mikolov (2015)] also uses a discrete search procedure for its interfaces but it is unclear how this would scale to larger problems. The problem of learning algorithms has its origins in the field of program induction [Nordin (1997); Liang et al.",
      "startOffset" : 35,
      "endOffset" : 1232
    }, {
      "referenceID" : 1,
      "context" : "A closely related work to this is [Das et al. (1992)], which was recently extended in the Neural DeQue [Grefenstette et al. (2015)] to use a list instead. End-to-End Memory Networks [Sukhbaatar et al. (2015)] use a feed-forward network as the controller and interfaces consisting of a soft-attention input, plus a delayed output (by a fixed number of “hops”). The model is applied to simple Q&A tasks, some of which involve logical reasoning. In contrast, our model automatically determines when to produce output and uses more general interfaces. However, most of these approaches use continuous interfaces that permit training via backpropagation of gradients. Our approach differs in that it uses discrete interfaces thus is more challenging to train since as we must rely on reinforcement learning instead. A notable exception is the Reinforcement Learning Neural Turing Machine (RLNTM) [Zaremba & Sutskever (2015)] which is a version of the NTM with discrete interfaces. The Stack-RNN [Joulin & Mikolov (2015)] also uses a discrete search procedure for its interfaces but it is unclear how this would scale to larger problems. The problem of learning algorithms has its origins in the field of program induction [Nordin (1997); Liang et al. (2013); Wineberg & Oppacher (1994); Solomonoff (1964)].",
      "startOffset" : 35,
      "endOffset" : 1253
    }, {
      "referenceID" : 1,
      "context" : "A closely related work to this is [Das et al. (1992)], which was recently extended in the Neural DeQue [Grefenstette et al. (2015)] to use a list instead. End-to-End Memory Networks [Sukhbaatar et al. (2015)] use a feed-forward network as the controller and interfaces consisting of a soft-attention input, plus a delayed output (by a fixed number of “hops”). The model is applied to simple Q&A tasks, some of which involve logical reasoning. In contrast, our model automatically determines when to produce output and uses more general interfaces. However, most of these approaches use continuous interfaces that permit training via backpropagation of gradients. Our approach differs in that it uses discrete interfaces thus is more challenging to train since as we must rely on reinforcement learning instead. A notable exception is the Reinforcement Learning Neural Turing Machine (RLNTM) [Zaremba & Sutskever (2015)] which is a version of the NTM with discrete interfaces. The Stack-RNN [Joulin & Mikolov (2015)] also uses a discrete search procedure for its interfaces but it is unclear how this would scale to larger problems. The problem of learning algorithms has its origins in the field of program induction [Nordin (1997); Liang et al. (2013); Wineberg & Oppacher (1994); Solomonoff (1964)].",
      "startOffset" : 35,
      "endOffset" : 1281
    }, {
      "referenceID" : 1,
      "context" : "A closely related work to this is [Das et al. (1992)], which was recently extended in the Neural DeQue [Grefenstette et al. (2015)] to use a list instead. End-to-End Memory Networks [Sukhbaatar et al. (2015)] use a feed-forward network as the controller and interfaces consisting of a soft-attention input, plus a delayed output (by a fixed number of “hops”). The model is applied to simple Q&A tasks, some of which involve logical reasoning. In contrast, our model automatically determines when to produce output and uses more general interfaces. However, most of these approaches use continuous interfaces that permit training via backpropagation of gradients. Our approach differs in that it uses discrete interfaces thus is more challenging to train since as we must rely on reinforcement learning instead. A notable exception is the Reinforcement Learning Neural Turing Machine (RLNTM) [Zaremba & Sutskever (2015)] which is a version of the NTM with discrete interfaces. The Stack-RNN [Joulin & Mikolov (2015)] also uses a discrete search procedure for its interfaces but it is unclear how this would scale to larger problems. The problem of learning algorithms has its origins in the field of program induction [Nordin (1997); Liang et al. (2013); Wineberg & Oppacher (1994); Solomonoff (1964)].",
      "startOffset" : 35,
      "endOffset" : 1300
    }, {
      "referenceID" : 1,
      "context" : "A closely related work to this is [Das et al. (1992)], which was recently extended in the Neural DeQue [Grefenstette et al. (2015)] to use a list instead. End-to-End Memory Networks [Sukhbaatar et al. (2015)] use a feed-forward network as the controller and interfaces consisting of a soft-attention input, plus a delayed output (by a fixed number of “hops”). The model is applied to simple Q&A tasks, some of which involve logical reasoning. In contrast, our model automatically determines when to produce output and uses more general interfaces. However, most of these approaches use continuous interfaces that permit training via backpropagation of gradients. Our approach differs in that it uses discrete interfaces thus is more challenging to train since as we must rely on reinforcement learning instead. A notable exception is the Reinforcement Learning Neural Turing Machine (RLNTM) [Zaremba & Sutskever (2015)] which is a version of the NTM with discrete interfaces. The Stack-RNN [Joulin & Mikolov (2015)] also uses a discrete search procedure for its interfaces but it is unclear how this would scale to larger problems. The problem of learning algorithms has its origins in the field of program induction [Nordin (1997); Liang et al. (2013); Wineberg & Oppacher (1994); Solomonoff (1964)]. In this domain, the model has to infer the source code of a program that solves a given problem. This is a similar goal to ours, but in quite a different setting. I.e. we do produce a computer program, but rather a neural net that can operate with interfaces such as tapes, so implements the program without being humanreadable. A more relevant work is [Schmidhuber (2004)] which learns an algorithms for the Hanoi tower problem, using a simple form of program induction and incremental learning components.",
      "startOffset" : 35,
      "endOffset" : 1675
    }, {
      "referenceID" : 1,
      "context" : "A closely related work to this is [Das et al. (1992)], which was recently extended in the Neural DeQue [Grefenstette et al. (2015)] to use a list instead. End-to-End Memory Networks [Sukhbaatar et al. (2015)] use a feed-forward network as the controller and interfaces consisting of a soft-attention input, plus a delayed output (by a fixed number of “hops”). The model is applied to simple Q&A tasks, some of which involve logical reasoning. In contrast, our model automatically determines when to produce output and uses more general interfaces. However, most of these approaches use continuous interfaces that permit training via backpropagation of gradients. Our approach differs in that it uses discrete interfaces thus is more challenging to train since as we must rely on reinforcement learning instead. A notable exception is the Reinforcement Learning Neural Turing Machine (RLNTM) [Zaremba & Sutskever (2015)] which is a version of the NTM with discrete interfaces. The Stack-RNN [Joulin & Mikolov (2015)] also uses a discrete search procedure for its interfaces but it is unclear how this would scale to larger problems. The problem of learning algorithms has its origins in the field of program induction [Nordin (1997); Liang et al. (2013); Wineberg & Oppacher (1994); Solomonoff (1964)]. In this domain, the model has to infer the source code of a program that solves a given problem. This is a similar goal to ours, but in quite a different setting. I.e. we do produce a computer program, but rather a neural net that can operate with interfaces such as tapes, so implements the program without being humanreadable. A more relevant work is [Schmidhuber (2004)] which learns an algorithms for the Hanoi tower problem, using a simple form of program induction and incremental learning components. Genetic algorithms [Holland (1992); Goldberg (1989)] also can be considered a form of program induction, but are based on a random search strategy rather than a learned one.",
      "startOffset" : 35,
      "endOffset" : 1845
    }, {
      "referenceID" : 1,
      "context" : "A closely related work to this is [Das et al. (1992)], which was recently extended in the Neural DeQue [Grefenstette et al. (2015)] to use a list instead. End-to-End Memory Networks [Sukhbaatar et al. (2015)] use a feed-forward network as the controller and interfaces consisting of a soft-attention input, plus a delayed output (by a fixed number of “hops”). The model is applied to simple Q&A tasks, some of which involve logical reasoning. In contrast, our model automatically determines when to produce output and uses more general interfaces. However, most of these approaches use continuous interfaces that permit training via backpropagation of gradients. Our approach differs in that it uses discrete interfaces thus is more challenging to train since as we must rely on reinforcement learning instead. A notable exception is the Reinforcement Learning Neural Turing Machine (RLNTM) [Zaremba & Sutskever (2015)] which is a version of the NTM with discrete interfaces. The Stack-RNN [Joulin & Mikolov (2015)] also uses a discrete search procedure for its interfaces but it is unclear how this would scale to larger problems. The problem of learning algorithms has its origins in the field of program induction [Nordin (1997); Liang et al. (2013); Wineberg & Oppacher (1994); Solomonoff (1964)]. In this domain, the model has to infer the source code of a program that solves a given problem. This is a similar goal to ours, but in quite a different setting. I.e. we do produce a computer program, but rather a neural net that can operate with interfaces such as tapes, so implements the program without being humanreadable. A more relevant work is [Schmidhuber (2004)] which learns an algorithms for the Hanoi tower problem, using a simple form of program induction and incremental learning components. Genetic algorithms [Holland (1992); Goldberg (1989)] also can be considered a form of program induction, but are based on a random search strategy rather than a learned one.",
      "startOffset" : 35,
      "endOffset" : 1862
    }, {
      "referenceID" : 1,
      "context" : "A closely related work to this is [Das et al. (1992)], which was recently extended in the Neural DeQue [Grefenstette et al. (2015)] to use a list instead. End-to-End Memory Networks [Sukhbaatar et al. (2015)] use a feed-forward network as the controller and interfaces consisting of a soft-attention input, plus a delayed output (by a fixed number of “hops”). The model is applied to simple Q&A tasks, some of which involve logical reasoning. In contrast, our model automatically determines when to produce output and uses more general interfaces. However, most of these approaches use continuous interfaces that permit training via backpropagation of gradients. Our approach differs in that it uses discrete interfaces thus is more challenging to train since as we must rely on reinforcement learning instead. A notable exception is the Reinforcement Learning Neural Turing Machine (RLNTM) [Zaremba & Sutskever (2015)] which is a version of the NTM with discrete interfaces. The Stack-RNN [Joulin & Mikolov (2015)] also uses a discrete search procedure for its interfaces but it is unclear how this would scale to larger problems. The problem of learning algorithms has its origins in the field of program induction [Nordin (1997); Liang et al. (2013); Wineberg & Oppacher (1994); Solomonoff (1964)]. In this domain, the model has to infer the source code of a program that solves a given problem. This is a similar goal to ours, but in quite a different setting. I.e. we do produce a computer program, but rather a neural net that can operate with interfaces such as tapes, so implements the program without being humanreadable. A more relevant work is [Schmidhuber (2004)] which learns an algorithms for the Hanoi tower problem, using a simple form of program induction and incremental learning components. Genetic algorithms [Holland (1992); Goldberg (1989)] also can be considered a form of program induction, but are based on a random search strategy rather than a learned one. Similar to [Mnih et al. (2013)], we train the controller to approximate the Q-function.",
      "startOffset" : 35,
      "endOffset" : 2015
    }, {
      "referenceID" : 1,
      "context" : "A closely related work to this is [Das et al. (1992)], which was recently extended in the Neural DeQue [Grefenstette et al. (2015)] to use a list instead. End-to-End Memory Networks [Sukhbaatar et al. (2015)] use a feed-forward network as the controller and interfaces consisting of a soft-attention input, plus a delayed output (by a fixed number of “hops”). The model is applied to simple Q&A tasks, some of which involve logical reasoning. In contrast, our model automatically determines when to produce output and uses more general interfaces. However, most of these approaches use continuous interfaces that permit training via backpropagation of gradients. Our approach differs in that it uses discrete interfaces thus is more challenging to train since as we must rely on reinforcement learning instead. A notable exception is the Reinforcement Learning Neural Turing Machine (RLNTM) [Zaremba & Sutskever (2015)] which is a version of the NTM with discrete interfaces. The Stack-RNN [Joulin & Mikolov (2015)] also uses a discrete search procedure for its interfaces but it is unclear how this would scale to larger problems. The problem of learning algorithms has its origins in the field of program induction [Nordin (1997); Liang et al. (2013); Wineberg & Oppacher (1994); Solomonoff (1964)]. In this domain, the model has to infer the source code of a program that solves a given problem. This is a similar goal to ours, but in quite a different setting. I.e. we do produce a computer program, but rather a neural net that can operate with interfaces such as tapes, so implements the program without being humanreadable. A more relevant work is [Schmidhuber (2004)] which learns an algorithms for the Hanoi tower problem, using a simple form of program induction and incremental learning components. Genetic algorithms [Holland (1992); Goldberg (1989)] also can be considered a form of program induction, but are based on a random search strategy rather than a learned one. Similar to [Mnih et al. (2013)], we train the controller to approximate the Q-function. However, we introduce several modifications on top of the classical Q-learning. First, we use Watkins Q(λ) [Watkins (1989); Sutton & Barto (1998)].",
      "startOffset" : 35,
      "endOffset" : 2195
    }, {
      "referenceID" : 1,
      "context" : "A closely related work to this is [Das et al. (1992)], which was recently extended in the Neural DeQue [Grefenstette et al. (2015)] to use a list instead. End-to-End Memory Networks [Sukhbaatar et al. (2015)] use a feed-forward network as the controller and interfaces consisting of a soft-attention input, plus a delayed output (by a fixed number of “hops”). The model is applied to simple Q&A tasks, some of which involve logical reasoning. In contrast, our model automatically determines when to produce output and uses more general interfaces. However, most of these approaches use continuous interfaces that permit training via backpropagation of gradients. Our approach differs in that it uses discrete interfaces thus is more challenging to train since as we must rely on reinforcement learning instead. A notable exception is the Reinforcement Learning Neural Turing Machine (RLNTM) [Zaremba & Sutskever (2015)] which is a version of the NTM with discrete interfaces. The Stack-RNN [Joulin & Mikolov (2015)] also uses a discrete search procedure for its interfaces but it is unclear how this would scale to larger problems. The problem of learning algorithms has its origins in the field of program induction [Nordin (1997); Liang et al. (2013); Wineberg & Oppacher (1994); Solomonoff (1964)]. In this domain, the model has to infer the source code of a program that solves a given problem. This is a similar goal to ours, but in quite a different setting. I.e. we do produce a computer program, but rather a neural net that can operate with interfaces such as tapes, so implements the program without being humanreadable. A more relevant work is [Schmidhuber (2004)] which learns an algorithms for the Hanoi tower problem, using a simple form of program induction and incremental learning components. Genetic algorithms [Holland (1992); Goldberg (1989)] also can be considered a form of program induction, but are based on a random search strategy rather than a learned one. Similar to [Mnih et al. (2013)], we train the controller to approximate the Q-function. However, we introduce several modifications on top of the classical Q-learning. First, we use Watkins Q(λ) [Watkins (1989); Sutton & Barto (1998)].",
      "startOffset" : 35,
      "endOffset" : 2218
    }, {
      "referenceID" : 1,
      "context" : "A closely related work to this is [Das et al. (1992)], which was recently extended in the Neural DeQue [Grefenstette et al. (2015)] to use a list instead. End-to-End Memory Networks [Sukhbaatar et al. (2015)] use a feed-forward network as the controller and interfaces consisting of a soft-attention input, plus a delayed output (by a fixed number of “hops”). The model is applied to simple Q&A tasks, some of which involve logical reasoning. In contrast, our model automatically determines when to produce output and uses more general interfaces. However, most of these approaches use continuous interfaces that permit training via backpropagation of gradients. Our approach differs in that it uses discrete interfaces thus is more challenging to train since as we must rely on reinforcement learning instead. A notable exception is the Reinforcement Learning Neural Turing Machine (RLNTM) [Zaremba & Sutskever (2015)] which is a version of the NTM with discrete interfaces. The Stack-RNN [Joulin & Mikolov (2015)] also uses a discrete search procedure for its interfaces but it is unclear how this would scale to larger problems. The problem of learning algorithms has its origins in the field of program induction [Nordin (1997); Liang et al. (2013); Wineberg & Oppacher (1994); Solomonoff (1964)]. In this domain, the model has to infer the source code of a program that solves a given problem. This is a similar goal to ours, but in quite a different setting. I.e. we do produce a computer program, but rather a neural net that can operate with interfaces such as tapes, so implements the program without being humanreadable. A more relevant work is [Schmidhuber (2004)] which learns an algorithms for the Hanoi tower problem, using a simple form of program induction and incremental learning components. Genetic algorithms [Holland (1992); Goldberg (1989)] also can be considered a form of program induction, but are based on a random search strategy rather than a learned one. Similar to [Mnih et al. (2013)], we train the controller to approximate the Q-function. However, we introduce several modifications on top of the classical Q-learning. First, we use Watkins Q(λ) [Watkins (1989); Sutton & Barto (1998)]. This helps to overcome a non-stationary environment. We are unaware of any prior work that uses Watkins Q(λ) for this purpose. Second, we reparametrized Q function, to become invariant to the sequence length. Finally, we penalize ||Q(s, •)||, which might help to remove positive bias [Hasselt (2010)].",
      "startOffset" : 35,
      "endOffset" : 2520
    }, {
      "referenceID" : 4,
      "context" : "We note that this argument is empirically supported by our results in Table 2, as well as related work such as [Graves et al. (2014)] and [Joulin & Mikolov (2015)] which found limited capacity controllers to be most effective.",
      "startOffset" : 112,
      "endOffset" : 133
    }, {
      "referenceID" : 4,
      "context" : "We note that this argument is empirically supported by our results in Table 2, as well as related work such as [Graves et al. (2014)] and [Joulin & Mikolov (2015)] which found limited capacity controllers to be most effective.",
      "startOffset" : 112,
      "endOffset" : 163
    } ],
    "year" : 2017,
    "abstractText" : "We present an approach for learning simple algorithms such as copying, multidigit addition and single digit multiplication directly from examples. Our framework consists of a set of interfaces, accessed by a controller. Typical interfaces are 1-D tapes or 2-D grids that hold the input and output data. For the controller, we explore a range of neural network-based models which vary in their ability to abstract the underlying algorithm from training instances and generalize to test examples with many thousands of digits. The controller is trained using Q-learning with several enhancements and we show that the bottleneck is in the capabilities of the controller rather than in the search incurred by Q-learning.",
    "creator" : "LaTeX with hyperref package"
  }
}