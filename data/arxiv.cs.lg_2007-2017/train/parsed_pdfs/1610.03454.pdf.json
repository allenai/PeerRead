{
  "name" : "1610.03454.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Deep Variational Canonical Correlation Analysis",
    "authors" : [ "Weiran Wang", "Honglak Lee", "Karen Livescu" ],
    "emails" : [ "weiranwang@ttic.edu", "honglak@eecs.umich.edu", "klivescu@ttic.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In the multi-view representation learning setting, we have multiple views/measurements of the same underlying signal, and the goal is to learn useful features of each view using complementary information contained in the views. The intuition underlying this setting is that the learned features can help uncover the common sources of variation in the views, which can be helpful for exploratory analysis or for downstream tasks.\nA classical approach in this setting is canonical correlation analysis (CCA, Hotelling, 1936) and its nonlinear extensions, including the kernel extension (KCCA, Lai and Fyfe, 2000; Akaho, 2001; Melzer et al., 2001; Bach and Jordan, 2002) and the deep neural network (DNN) extension (DCCA, Andrew et al., 2013; Wang et al., 2015b). CCA projects two random vectors x ∈ Rdx and y ∈ Rdy into a lower-dimensional subspace so that the projections are maximally correlated. There is a probabilistic latent variable model interpretation of linear CCA (Bach and Jordan, 2005) as shown in Figure 1. Assume that x and y are linear functions of some lower-dimensional random variable z ∈ Rdz , where dz ≤ min(dx, dy). When the prior distribution of the latent variable p(z) and the conditional distributions p(x|z) and p(y|z) are all Gaussian, Bach and Jordan (2005) showed that E[z|x] (resp. E[z|y]) lives in the same space as the linear CCA projection for x (resp. y).\nThis generative interpretation of CCA is often lost in nonlinear extensions of CCA. For example, in deep CCA, to extend CCA to nonlinear mappings with greater representation power, one extracts nonlinear features from the original inputs of each view using two DNNs, f for x and g for y, so that the canonical correlation of the DNN outputs (measured by a linear CCA) is maximized. Formally, given a dataset of N pairs of observations (x1,y1), . . . , (xN ,yN ) of the random vectors (x,y), DCCA optimizes the following objective\nmax Wf ,Wg,U,V\n1 N tr ( U>f(X)g(Y)>V ) (1)\ns.t. U> ( f(X)f(X)> ) U = V> ( g(Y)g(Y)> ) V = NI,\nar X\niv :1\n61 0.\n03 45\n4v 1\n[ cs\n.L G\n] 1\n1 O\nct 2\nwhere f(X) = [f(x1), . . . , f(xN )] and g(Y) = [g(y1), . . . ,g(yN )], and Wf denotes all weight parameters of the DNN f (and similarly for g).\nDCCA has achieved good performance in the multi-view representation learning setting across different domains (Wang et al., 2015b,a; Lu et al., 2015; Yan and Mikolajczyk, 2015). However, a disadvantage of DCCA is that it directly looks for DNNs that can map inputs into the low-dimensional space, without a model for generating samples from the latent space. Although Wang et al. (2015b)’s deep canonically correlated autoencoders (DCCAE) model optimizes the combination of the autoencoder objective (reconstruction errors) and the canonical correlation objective, the authors found that in practice, the canonical correlation term tends to dominate the reconstruction error terms in the DCCAE objective when tuning performance for a downstream task (especially when the inputs are noisy), and as a result the inputs are not reconstructed well. At the same time, optimization of the DCCA and DCCAE objectives is challenging due to the constraints that couple all training samples together.\nThe main contribution of this paper is the proposal of a new deep multi-view learning model named variational CCA (VCCA), which extends the latent variable model interpretation of linear CCA to nonlinear observation models parameterized by deep neural networks (DNNs). The marginal data likelihood as well as inference are intractable under this model. Inspired by variational autoencoders (VAE, Kingma and Welling, 2014), we parameterize the posterior distribution of the latent variables with another DNN, and derive a variational lower bound of the data likelihood as the objective of VCCA, which is further approximated by Monte Carlo sampling. With the reparameterization trick, sampling for the Monte Carlo approximation is trivial and all DNN weights in VCCA can be optimized jointly via stochastic gradient descent, using unbiased gradient estimates from small minibatches. Interestingly, VCCA is related to multiview autoencoders (Ngiam et al., 2011), with the key distinctions of additional regularization on the posterior distribution and the sampling procedure at the bottleneck layer.\nWe also propose a variant of VCCA called VCCA-private that can, in addition to the “common variables” underlying both views, extract the “private variables” within each view. We demonstrate that VCCA-private is able to disentangle the shared and private information for multi-view data without hard supervision. Last but not least, as generative models, VCCA and VCCA-private enable us to obtain high-quality samples for the input of each view."
    }, {
      "heading" : "2 Variational CCA",
      "text" : "The probabilistic latent variable model of CCA (Bach and Jordan, 2005) defines the following joint distribution over the random variables (x,y):\np(x,y, z) = p(z)p(x|z)p(y|z), p(x,y) = ∫ p(x,y, z)dz. (2)\nThe assumption underlying this model is that, conditioned on the latent variables z ∈ Rdz , the two views x and y are independent. However, linear observation models (p(x|z) and p(y|z) as shown in Figure 1) have limited representation power. In this paper, we consider nonlinear observation models pθ(x|z;θx) and pθ(y|z;θy), parameterized by θx and θy respectively, which can be the collections of weights of DNNs. In\nthis case, the marginal likelihood pθ(x,y) does not have a closed form. In addition, the inference problem pθ(z|x)—the problem of inferring the latent variables given one of the views—is also intractable.\nInspired by Kingma and Welling (2014)’s work on variational autoencoders (VAE), we approximate pθ(z|x) with the conditional density qφ(z|x;φz), where φz is the collection of parameters of another DNN.1 We can derive a lower bound on the marginal data likelihood using qφ(z|x) as follows\nlog pθ(x,y) = log pθ(x,y) ∫ qφ(z|x)dz = ∫ log pθ(x,y)qφ(z|x)dz\n= ∫ qφ(z|x) ( log\nqφ(z|x) pθ(z|x,y) + log pθ(x,y, z) qφ(z|x)\n) dz\n= DKL(qφ(z|x)||pθ(z|x,y)) + Eqφ(z|x) [ log pθ(x,y, z)\nqφ(z|x) ] ≥ Eqφ(z|x) [ log pθ(x,y, z)\nqφ(z|x) ] =: L(x,y;θ,φ) (3)\nwhere we used the fact that KL divergence is nonnegative in the last step. As a result, L(x,y;θ,φ) is a lower bound on the data log-likelihood logθ p(x,y). Substituting (2) into (3), we have\nL(x,y;θ,φ) = ∫ qφ(z|x) ( log p(z)\nqφ(z|x) + log pθ(x|z) + log pθ(y|z)\n) dz\n= −DKL(qφ(z|x)||p(z)) + Eqφ(z|x) [log pθ(x|z) + log pθ(y|z)] . (4)\nVCCA maximizes this variational lower bound on the data likelihood on the training set:\nmax θ,φ\n1\nN N∑ i=1 L(xi,yi;θ,φ). (5)\nThe first term in (4) measures the KL-divergence between the approximate posterior distribution and the prior distribution of the latent variables z. When the parameterization qφ(z|x) is chosen properly, this term can be computed exactly in closed form. As a concrete example, let the variational approximate posterior be a multivariate Gaussian with diagonal covariance. That is, for a sample pair (xi,yi), we have\nlog qφ(zi|xi) = logN (zi;µi,Σi), Σi = diag ( σ2i1, . . . , σ 2 idz ) , (6)\nwhere the mean µi and covariance Σi are outputs of an encoding DNN f (and thus [µi,Σi] = f(xi;φz) are deterministic nonlinear functions of xi). In this case, we have\nDKL(qφ(zi|xi)||p(zi)) = − 1\n2 dz∑ j=1 ( 1 + log σ2ij − σ2ij − µ2ij ) .\nThe second term of (4) corresponds to the expected complete data likelihood under the approximate posterior distribution. Though still intractable, this term can be approximated by Monte Carlo sampling. In particular, we draw L samples z (l) i ∼ qφ(zi|xi):\nz (l) i = µi + Σi (l), where (l) ∼ N (0, I), for l = 1, . . . , L, (7)\nand have\nEqφ(zi|xi) [log pθ(xi|zi) + log pθ(yi|zi)] ≈ 1\nL L∑ l=1 log pθ ( xi|z(l)i ) + log pθ ( yi|z(l)i ) . (8)\n1For notational simplicity, we denote by θ the collection of parameters associated with the model probabilities pθ(·), and φ the collection of parameters associated with the variational approximate probabilities qφ(·), and often omit specific parameters inside the probabilities.\nNotice that we parameterized qφ(zi|xi) above to obtain the VCCA objective; this is useful when the first view is available for downstream tasks, in which case we can directly apply qφ(zi|xi) to obtain its projection (as features). One could also derive an objective by parameterizing the approximate posterior qφ(zi|yi), or both of them at the same time. We give a sketch of VCCA in Figure 2.\nConnection to multi-view autoencoder (MVAE) If we use the following Gaussian observation models\nlog pθ(x|z) = logN (gx(z;θx), I), log pθ(y|z) = logN (gy(z;θy), I),\nwe observe that log pθ ( xi|z(l)i ) and log pθ ( yi|z(l)i ) measure the reconstruction errors of each view’s inputs from samples z (l) i using the two DNNs gx and gy respectively. In this case, maximizing L(x,y;θ,φ) is equivalent to\nmin θ,φ\n1\nN N∑ i=1 DKL(qφ(zi|xi)||p(zi)) + 1 2NL N∑ i=1 L∑ l=1 ∥∥∥xi − gx (z(l)i ;θx)∥∥∥2 + ∥∥∥yi − gy (z(l)i ;θy)∥∥∥2 (9) s.t. z\n(l) i = µi + Σi (l), where (l) ∼ N (0, I), l = 1, . . . , L.\nNow, consider the case of Σi → 0, for i = 1, . . . , N , and we have z(l)i → µi which is a deterministic function of x (and there is no need for sampling). In the limit, the second term of (9) reduces to\n1\n2N N∑ i=1 ‖xi − gx(f(xi;φz);θx)‖ 2 + ‖yi − gy(f(xi;φz);θy)‖ 2 ,\nwhich is the objective of the multi-view autoencoder (Ngiam et al., 2011). Note, however, that Σi → 0 is prevented by the VCCA objective as it results in a large penalty in DKL(qφ(zi|xi)||p(zi)). Compared with the objective of the multi-view autoencoder, in the VCCA objective we are creating L different “noisy” versions of the latent representation and enforce that these versions reconstruct the original inputs well. And the “noise” distribution (the variances Σi) are also learned and regularized by the KL divergence DKL(qφ(zi|xi)||p(zi)). Using the VCCA objective, we expect to learn different representations from those of feed-forward multi-view autoencoders, due to these regularization effects."
    }, {
      "heading" : "2.1 Extracting private variables",
      "text" : "So far, VCCA aims at extracting only the latent variables z that are common to both views. A potential disadvantage of this model is that it assumes the common variables are sufficient by themselves to generate the views, which can be too restrictive in practice. Consider the example of audio and articulatory measurements as two views for speech. Although the transcription is a common variable behind the views, it combines with the physical environment and the vocal tract anatomy to generate the individual views. In other words, there might be large variations in the input space that can not be explained by the common variables, making the objective (4) hard to optimize. It may then be beneficial to explicitly model the private variables within each view.\nWe therefore propose a new probabilistic graphical model shown in Figure 3, that we refer to as VCCAprivate. We introduce two sets of hidden variables hx ∈ Rdhx and hy ∈ Rdhy to explain the aspects of x and y not captured by the common variables z. Under this model, the data likelihood is defined by\npθ(x,y, z,hx,hy) = p(z)p(hx)p(hy)pθ(x|z,hx;θx)pθ(y|z,hy;θy), (10)\npθ(x,y) = ∫ ∫ ∫ pθ(x,y, z,hx,hy)dz dhx dhy.\nTo obtain tractable inference, we introduce the following factored variational posterior\nqφ(z,hx,hy|x,y) = qφ(z|x;φz)qφ(hx|x;φx)qφ(hy|y;φy), (11)\nwhere each factor is parameterized by a different DNN. Similar to VCCA, we can derive a variational lower bound of the data likelihood for VCCA-private as\nlog pθ(x,y) ≥ ∫ ∫ ∫ qφ(z,hx,hy|x,y) log pθ(x,y, z,hx,hy)\nqφ(z,hx,hy|x,y) dz dhx dhy\n= ∫ ∫ ∫ qφ(z,hx,hy|x,y) [ log p(z)\nqφ(z|x) + log\np(hx)\nqφ(hx|x) + log\np(hy)\nqφ(hy|y) + log pθ(x|z,hx) + log pθ(y|z,hy) ] dz dhx dhy\n= −DKL(qφ(z|x)||p(z))−DKL(qφ(hx|x)||p(hx))−DKL(qφ(hy|y)||p(hy))\n+ ∫ ∫ qφ(z|x)qφ(hx|x) log pθ(x|z,hx)dz dhx + ∫ ∫ qφ(z|x)qφ(hy|y) log pθ(y|z,hy)dz dhy\n=: Lprivate(x,y;θ,φ). (12)\nSimilar to VCCA, the last two terms of (13) can be approximated by Monte Carlo sampling. For example, we draw samples of z and hx from their corresponding approximate posteriors, and concatenate their samples as inputs to the DNN parameterizing pθ(x|z,hx). In this paper, we use simple Gaussian prior distributions for the private variables, i.e., hx ∼ N (0, I) and hy ∼ N (0, I). We leave to future work to examine the effect of more sophisticated prior distributions for the latent variables.\nVCCA-private maximizes this lower bound on the training set, i.e.,\nmax θ,φ\n1\nN N∑ i=1 Lprivate(xi,yi;θ,φ). (13)\nOptimization The objectives (5) and (13) decouple over the training samples and can be trained efficiently using stochastic gradient descent. Enabled by the reparameterization trick, unbiased gradient estimates are\nobtained by Monte Carlo sampling and the standard backpropagation procedure on minibatches of training samples. We apply the ADAM algorithm (Kingma and Ba, 2015) for optimizing our objectives."
    }, {
      "heading" : "3 Related work",
      "text" : "Recently, there has been much interest in unsupervised deep generative models (Kingma and Welling, 2014; Rezende et al., 2014; Goodfellow et al., 2014; Gregor et al., 2015; Makhzani et al., 2016; Burda et al., 2016; Alain et al., 2016). A common motivation behind these models is that, with the expressive power of DNNs, the generative models can capture rich distributions for complex inputs. Additionally, if we are able to generate realistic samples from the learned distribution, we can infer that we have discovered the underlying structure of the data, which may allow us to reduce the sample complexity for learning for downstream tasks. However, previous models have mostly focused on single-view data. Here we focus on the multi-view setting where multiple views of the data are present for feature extraction but only one view is available at test time (in downstream tasks).\nOur work is also related to the deep multi-view probabilistic models based on restricted Boltzmann machines (Srivastava and Salakhutdinov, 2014; Sohn et al., 2014). We note that these are undirected graphical models for which both inference and learning are difficult, and one typically resorts to carefully designed variational approximation and Gibbs sampling procedures for training such models. In contrast, our models only require sampling from simple, standard distributions (such as Gaussians), and all parameters can be learned end-to-end by standard stochastic gradient methods. Therefore, our models are more scalable than the previous multi-view probabilistic models.\nOn the other hand, there is a rich literature in modeling multi-view data using the same or similar graphical models behind VCCA/VCCA-private (Wang, 2007; Jia et al., 2010; Salzmann et al., 2010; Virtanen et al., 2011; Memisevic et al., 2012; Klami et al., 2013). Our methods differ from previous work in parameterizing the probability distributions using DNNs. This makes the model more powerful, while still having tractable objectives and efficient end-to-end training using the local reparameterization technique. We note that, unlike earlier work on probabilistic models of linear CCA (Bach and Jordan 2005), VCCA does not optimize the same criterion, nor produce the same solution, as any linear or nonlinear CCA. However, we retain the terminology in order to clarify the connection with earlier work on probabilistic models for CCA, which we are extending with DNN models for the observations and for the variational posterior distribution approximation."
    }, {
      "heading" : "4 Experimental results",
      "text" : ""
    }, {
      "heading" : "4.1 Noisy MNIST dataset",
      "text" : "We first demonstrate our algorithms on the noisy MNIST dataset used by Wang et al. (2015b). The dataset is generated using the MNIST dataset (LeCun et al., 1998), which consists of 28× 28 grayscale digit images, with 60K/10K images for training/testing. We first linearly rescale the pixel values to the range [0, 1]. Then, we randomly rotate the images at angles uniformly sampled from [−π/4, π/4] and the resulting images are used as view 1 inputs. For each view 1 image, we randomly select an image of the same identity (0-9) from the original dataset, add independent random noise uniformly sampled from [0, 1] to each pixel, and truncate the pixel final values to [0, 1] to obtain the corresponding view 2 sample. The original training set is further split into training/tuning sets of size 50K/10K. The data generation process ensures that the digit identity is the only common variable underlying both views. To evaluate the amount of class information extracted by different methods, after unsupervised learning of latent representations, we reveal the labels and train a linear SVM on the projected view 1 training data (using the one-versus-all scheme), and use it to classify the projected test set. This experiment simulates the typical usage of multi-view learning method, which is to extract useful representation for downstream discriminative tasks.\nNote that this synthetic dataset perfectly satisfies the multi-view assumption that the two views are independent given the class label, so the latent representation should contain precisely the class information. This was indeed achieved by CCA-based and contrastive loss-based multi-view approaches. In Figure 5, we show 2D t-SNE (van der Maaten and Hinton, 2008) visualizations the original view 1 inputs and view 1 projections by various deep multi-view methods, including multi-view autoencoder (MVAE, Ngiam et al., 2011), deep CCA (DCCA, Andrew et al., 2013), deep canonically correlated autoencoders (DCCAE, Wang et al., 2015b), and the constrastive loss of Hermann and Blunsom (2014).\nWe use DNNs with 3 hidden layers of 1024 ReLU units each to parameterize the conditional distributions: qφ(z|x), pθ(x|z), pθ(y|z) in VCCA, and additionally qφ(hx|x) and qφ(hy|y) in VCCA-private. The capacities of these networks are the same as those of their counterparts in DCCA and DCCAE from Wang et al. (2015b). The reconstruction networks pθ(x|z) or pθ(x|z,hx) model each pixel of x as an independent Bernoulli variable and parameterize its mean (using a sigmoid activation); pθ(y|z) and pθ(y|z,hy) model y with diagonal Gaussians and parameterize the mean (using a sigmoid activation) and standard deviation for each pixel dimension. We tune the dimensionality dz over {10, 20, 30, 40, 50}, and fix dhx = dhy = 30 for\nInputs MVAE Contrastive loss\nVCCA-private. We select the hyperparameter combination that yields the best SVM classification accuracy on the projected tuning set, and report the corresponding accuracy on the projected test set.\nThe effect of dropout We add dropout (Srivastava et al., 2014) to all intermediate layers and the input layers and find it to be very useful in our models, with most of the gain coming from dropout applied to the samples of z, hx and hy. This is because dropout encourages each latent dimension to reconstruct the inputs well in the absence of other dimensions, and therefore avoids learning co-adapted features. Intuitively, in VCCA-private dropout also helps to prevent the degenerate situation where the pathways x→ hx → x and y → hy → y achieve good reconstruction while ignoring z (e.g., by setting it to a constant). We use the same dropout rate for all layers and tune it over {0, 0.1, 0.2, 0.3, 0.4}.\nWe show the 2D t-SNE embeddings of the common variables z learned by VCCA and VCCA-private on test set in Figure 6 and Figure 7 respectively. We observe that in general, VCCA/VCCA-private tend to separate the classes in the projection well; dropout significantly improves the performance of both VCCA and VCCA-private, with the latter slightly outperforming the former. While such class separation can also be achieved by DCCA/contrastive loss as well, these methods can not naturally generate samples in the input space. On the other hand, such separation is not achieved by multi-view autoencoders.\nThe effect of private variables on reconstructions We show sample reconstructions (mean and standard deviation) by VCCA for the view 2 images from the test set in Figure 8 (rows 2 and 3). We observe that for each input, the mean reconstruction of yi by VCCA is a prototypical image of the same digit, regardless of the individual style in yi. This is to be expected, as yi contains an arbitrary image of the same digit as xi, and the variation in background noise in yi does not appear in xi and can not be reflected in qφ(z|x); thus the best way for pθ(y|z) to model yi is to output a prototypical image of that class to achieve on average small reconstruction error. On the other hand, since yi contains little rotation of the digits, this variation is suppressed to a large extent in qφ(z|x) (it is no longer the major variation in z as in the original inputs).\nWe show sample reconstructions by VCCA-private for the same set of view 2 images in Figure 8 (rows\n4 and 5). With the help of private variables hy (as part of the input to pθ(y|z,hy)), the model does a much better job in reconstructing the styles of y. And by disentangling the private variables from the shared variables, qφ(z|x) achieves even better class separation than VCCA does. We also note that the the standard deviation of the reconstruction is low within the digit and high outside the digit, implying that pθ(y|z,hy) is able to separate the background noise from the digit image.\nDisentanglement of private/shared variables In Figure 9 we provide the 2D t-SNE embeddings of the shared variables z (top row) and the private variables hx (bottom row) learned by VCCA-private. In the embedding of hx, digits with different identities but the same rotation are mapped close together, and the rotation varies smoothly from left to right, confirming that the private variables contain little class information but mainly style information.\nFinally, we give the test error rates of linear SVMs applied to the features learned with different models in Table 1. Having the advantages as a generative model, VCCA-private is comparable in performance to the best previous performance with DCCAE."
    }, {
      "heading" : "4.2 XRMB speech-articulation dataset",
      "text" : "We now apply deep multi-view methods to the task of learning acoustic features for automatic speech recognition. Our dataset is the Wisconsin X-ray microbeam (XRMB) corpus (Westbury, 1994), which contains simultaneously recorded speech and articulatory measurements from 47 American English speakers.\nWe follow the setup of Wang et al. (2015a,b) and use the learned features for speaker-independent phonetic recognition.2 The two input views are standard 39D acoustic features (13 mel frequency cepstral coefficients (MFCCs) along with their first and second derivatives) and 16D articulatory features (horizontal/vertical displacement of 8 pellets attached to several parts of the vocal tract), each then concatenated over a 7-frame window around each frame to incorporate context information. As in previous work, the XRMB speakers are split into disjoint sets of 35/8/2/2 speakers for feature learning/recognizer training/tuning/testing. The 35 speakers for feature learning are fixed; the remaining 12 are used in a 6-fold experiment (recognizer training on 8 speakers, tuning on 2 speakers, and testing on the remaining 2 speakers). Each speaker has roughly 50K frames. We remove the per-speaker mean and variance of the articulatory measurements for each training speaker, and remove the mean of the acoustic measurements for each utterance, in order to reduce the speaker variation for better recognition performance. All learned feature types are used in a “tandem” speech recognition approach (Hermansky et al., 2000), i.e., they are appended to the original 39D features and used in a standard hidden Markov model (HMM)-based recognizer with Gaussian mixture observation distributions.\nEach method uses up to 3 ReLU hidden layers, each of 1024 units, for the projection mappings. For VCCA/VCCA-private, we use isotropic Gaussian observation models for each view, with the standard deviation tuned by grid search. The mean phone error rates (PER) over 6 folds obtained with different features are\n2As in Wang and Livescu (2016), we use the Kaldi speech recognition toolkit (Povey et al., 2011) for feature extraction and recognition with hidden Markov models. Our results therefore do not match those in Wang et al. (2015a,b) (who instead used the HTK toolkit (Young et al., 1999)) for the same types of features, but the relative merits of different types of features are consistent.\ngiven in Table 2. Our methods achieve competitive performance in comparison to previous deep multi-view methods."
    }, {
      "heading" : "5 Conclusions",
      "text" : "We have proposed variational canonical correlation analysis (VCCA), a deep generative method for multiview representation learning. Our method embodies a natural idea for multi-view learning: the multiple views can be generated from a small set of shared latent variables. VCCA is parameterized by DNNs and can be trained efficiently by backpropagation, and is therefore scalable. We have also shown that, by modeling the private variables that are specific to each view, the VCCA-private variant can potentially disentangle shared/common variables better and provide higher-quality reconstructions/samples.\nIn the future, we will explore other prior distributions such as mixtures of Gaussians or discrete random variables, which may enforce the clustering effect in the latent space and in turn work better for discriminative downstream tasks. We will also explore other observation models, including replacing the auto-encoder objective with that of adversarial networks (Goodfellow et al., 2014; Makhzani et al., 2016; Chen et al., 2016), which may have stronger empirical performance. Another direction is to explicitly incorporate the structure of the inputs, such as the sequence structure of speech and text inputs and the spatial structure of images."
    } ],
    "references" : [ {
      "title" : "A kernel method for canonical correlation analysis",
      "author" : [ "Shotaro Akaho" ],
      "venue" : "In Proceedings of the International Meeting of the Psychometric Society (IMPS2001),",
      "citeRegEx" : "Akaho.,? \\Q2001\\E",
      "shortCiteRegEx" : "Akaho.",
      "year" : 2001
    }, {
      "title" : "GSNs: Generative stochastic networks",
      "author" : [ "Guillaume Alain", "Yoshua Bengio", "Li Yao", "Jason Yosinski", "Eric Thibodeau-Laufer", "Saizheng Zhang", "Pascal Vincent" ],
      "venue" : "Information and Inference,",
      "citeRegEx" : "Alain et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Alain et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep canonical correlation analysis",
      "author" : [ "Galen Andrew", "Raman Arora", "Jeff Bilmes", "Karen Livescu" ],
      "venue" : "In Proc. of the 30th Int. Conf. Machine Learning (ICML",
      "citeRegEx" : "Andrew et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Andrew et al\\.",
      "year" : 2013
    }, {
      "title" : "Kernel independent component analysis",
      "author" : [ "Francis R. Bach", "Michael I. Jordan" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Bach and Jordan.,? \\Q2002\\E",
      "shortCiteRegEx" : "Bach and Jordan.",
      "year" : 2002
    }, {
      "title" : "A probabilistic interpretation of canonical correlation analysis",
      "author" : [ "Francis R. Bach", "Michael I. Jordan" ],
      "venue" : "Technical Report 688,",
      "citeRegEx" : "Bach and Jordan.,? \\Q2005\\E",
      "shortCiteRegEx" : "Bach and Jordan.",
      "year" : 2005
    }, {
      "title" : "Importance weighted autoencoders",
      "author" : [ "Yuri Burda", "Roger Grosse", "Ruslan Salakhutdinov" ],
      "venue" : null,
      "citeRegEx" : "Burda et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Burda et al\\.",
      "year" : 2016
    }, {
      "title" : "InfoGAN: Interpretable representation learning by information maximizing generative adversarial nets",
      "author" : [ "Xi Chen", "Yan Duan", "Rein Houthooft", "John Schulman", "Ilya Sutskever", "Pieter Abbeel" ],
      "venue" : "[cs.LG],",
      "citeRegEx" : "Chen et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Generative adversarial nets. In Advances in Neural Information Processing Systems (NIPS), pages 2672–2680",
      "author" : [ "Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "DRAW: A recurrent neural network for image generation",
      "author" : [ "Karol Gregor", "Ivo Danihelka", "Alex Graves", "Danilo Jimenez Rezende", "Daan Wierstra" ],
      "venue" : "In Proc. of the 32st Int. Conf. Machine Learning (ICML",
      "citeRegEx" : "Gregor et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gregor et al\\.",
      "year" : 2015
    }, {
      "title" : "Multilingual distributed representations without word alignment",
      "author" : [ "Karl Moritz Hermann", "Phil Blunsom" ],
      "venue" : "In Proc. of the 2nd Int. Conf. Learning Representations (ICLR 2014),",
      "citeRegEx" : "Hermann and Blunsom.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hermann and Blunsom.",
      "year" : 2014
    }, {
      "title" : "Tandem connectionist feature extraction for conventional HMM systems",
      "author" : [ "Hynek Hermansky", "Daniel P.W. Ellis", "Sangita Sharma" ],
      "venue" : "In Proc. of the IEEE Int. Conf. Acoustics, Speech and Sig. Proc",
      "citeRegEx" : "Hermansky et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Hermansky et al\\.",
      "year" : 2000
    }, {
      "title" : "Relations between two sets of variates",
      "author" : [ "Harold Hotelling" ],
      "venue" : "Biometrika, 28(3/4):321–377,",
      "citeRegEx" : "Hotelling.,? \\Q1936\\E",
      "shortCiteRegEx" : "Hotelling.",
      "year" : 1936
    }, {
      "title" : "Factorized latent spaces with structured sparsity. In Advances in Neural Information Processing Systems (NIPS), pages 982–990",
      "author" : [ "Yangqing Jia", "Mathieu Salzmann", "Trevor Darrell" ],
      "venue" : null,
      "citeRegEx" : "Jia et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Jia et al\\.",
      "year" : 2010
    }, {
      "title" : "ADAM: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "In Proc. of the 3rd Int. Conf. Learning Representations (ICLR 2015),",
      "citeRegEx" : "Kingma and Ba.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Auto-encoding variational Bayes",
      "author" : [ "Diederik P. Kingma", "Max Welling" ],
      "venue" : "[stat.ML],",
      "citeRegEx" : "Kingma and Welling.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma and Welling.",
      "year" : 2014
    }, {
      "title" : "Bayesian canonical correlation analysis",
      "author" : [ "Arto Klami", "Seppo Virtanen", "Samuel Kaski" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Klami et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Klami et al\\.",
      "year" : 2013
    }, {
      "title" : "Kernel and nonlinear canonical correlation analysis",
      "author" : [ "P.L. Lai", "C. Fyfe" ],
      "venue" : "Int. J. Neural Syst.,",
      "citeRegEx" : "Lai and Fyfe.,? \\Q2000\\E",
      "shortCiteRegEx" : "Lai and Fyfe.",
      "year" : 2000
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Yann LeCun", "Léon Bottou", "Yoshua Bengio", "Patrick Haffner" ],
      "venue" : "Proc. IEEE,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "Deep multilingual correlation for improved word embeddings. In The 2015 Conference of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL-HLT",
      "author" : [ "Ang Lu", "Weiran Wang", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu" ],
      "venue" : null,
      "citeRegEx" : "Lu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2015
    }, {
      "title" : "Adversarial autoencoders",
      "author" : [ "Alireza Makhzani", "Jonathon Shlens", "Navdeep Jaitly", "Ian Goodfellow" ],
      "venue" : "In Proc. of the 4th Int. Conf. Learning Representations (ICLR",
      "citeRegEx" : "Makhzani et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Makhzani et al\\.",
      "year" : 2016
    }, {
      "title" : "Nonlinear feature extraction using generalized canonical correlation analysis",
      "author" : [ "Thomas Melzer", "Michael Reiter", "Horst Bischof" ],
      "venue" : "In Proc. of the 11th Int. Conf. Artificial Neural Networks",
      "citeRegEx" : "Melzer et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Melzer et al\\.",
      "year" : 2001
    }, {
      "title" : "Shared kernel information embedding for discriminative inference",
      "author" : [ "Roland Memisevic", "Leonid Sigal", "David J. Fleet" ],
      "venue" : "IEEE Trans. Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Memisevic et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Memisevic et al\\.",
      "year" : 2012
    }, {
      "title" : "Multimodal deep learning",
      "author" : [ "Jiquan Ngiam", "Aditya Khosla", "Mingyu Kim", "Juhan Nam", "Honglak Lee", "Andrew Ng" ],
      "venue" : "In Proc. of the 28th Int. Conf. Machine Learning (ICML",
      "citeRegEx" : "Ngiam et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Ngiam et al\\.",
      "year" : 2011
    }, {
      "title" : "The Kaldi speech recognition toolkit",
      "author" : [ "Daniel Povey", "Arnab Ghoshal", "Gilles Boulianne", "Lukas Burget", "Ondrej Glembek", "Nagendra Goel", "Mirko Hannemann", "Petr Motlicek", "Yanmin Qian", "Petr Schwarz", "Jan Silovsky", "Georg Stemmer", "Karel Vesely" ],
      "venue" : "In Proc. of the 2011 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU",
      "citeRegEx" : "Povey et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Povey et al\\.",
      "year" : 2011
    }, {
      "title" : "Stochastic backpropagation and approximate inference in deep generative models",
      "author" : [ "Danilo Jimenez Rezende", "Shakir Mohamed", "Daan Wierstra" ],
      "venue" : "In Proc. of the 31st Int. Conf. Machine Learning (ICML",
      "citeRegEx" : "Rezende et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Rezende et al\\.",
      "year" : 2014
    }, {
      "title" : "Factorized orthogonal latent spaces",
      "author" : [ "Mathieu Salzmann", "Carl Henrik Ek", "Raquel Urtasun", "Trevor Darrell" ],
      "venue" : "In Proc. of the 13th Int. Workshop on Artificial Intelligence and Statistics (AISTATS",
      "citeRegEx" : "Salzmann et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Salzmann et al\\.",
      "year" : 2010
    }, {
      "title" : "Improved multimodal deep learning with variation of information",
      "author" : [ "Kihyuk Sohn", "Wenling Shang", "Honglak Lee" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Sohn et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sohn et al\\.",
      "year" : 2014
    }, {
      "title" : "Multimodal learning with deep boltzmann machines",
      "author" : [ "Nitish Srivastava", "Ruslan Salakhutdinov" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Srivastava and Salakhutdinov.,? \\Q2014\\E",
      "shortCiteRegEx" : "Srivastava and Salakhutdinov.",
      "year" : 2014
    }, {
      "title" : "Dropout: A simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey E. Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R. Salakhutdinov" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Srivastava et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2014
    }, {
      "title" : "Visualizing data using t-SNE",
      "author" : [ "Laurens J.P. van der Maaten", "Geoffrey E. Hinton" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Maaten and Hinton.,? \\Q2008\\E",
      "shortCiteRegEx" : "Maaten and Hinton.",
      "year" : 2008
    }, {
      "title" : "Bayesian CCA via group sparsity",
      "author" : [ "Seppo Virtanen", "Arto Klami", "Samuel Kaski" ],
      "venue" : "In Proc. of the 28th Int. Conf. Machine Learning (ICML",
      "citeRegEx" : "Virtanen et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Virtanen et al\\.",
      "year" : 2011
    }, {
      "title" : "Variational Bayesian approach to canonical correlation analysis",
      "author" : [ "Chong Wang" ],
      "venue" : "IEEE Trans. Neural Networks,",
      "citeRegEx" : "Wang.,? \\Q2007\\E",
      "shortCiteRegEx" : "Wang.",
      "year" : 2007
    }, {
      "title" : "Large-scale approximate kernel canonical correlation analysis",
      "author" : [ "Weiran Wang", "Karen Livescu" ],
      "venue" : "In Proc. of the 4th Int. Conf. Learning Representations (ICLR 2016),",
      "citeRegEx" : "Wang and Livescu.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wang and Livescu.",
      "year" : 2016
    }, {
      "title" : "Unsupervised learning of acoustic features via deep canonical correlation analysis",
      "author" : [ "Weiran Wang", "Raman Arora", "Karen Livescu", "Jeff Bilmes" ],
      "venue" : "In Proc. of the IEEE Int. Conf. Acoustics, Speech and Sig. Proc. (ICASSP’15),",
      "citeRegEx" : "Wang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2015
    }, {
      "title" : "On deep multi-view representation learning",
      "author" : [ "Weiran Wang", "Raman Arora", "Karen Livescu", "Jeff Bilmes" ],
      "venue" : "In Proc. of the 32st Int. Conf. Machine Learning (ICML",
      "citeRegEx" : "Wang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2015
    }, {
      "title" : "X-Ray Microbeam Speech Production",
      "author" : [ "John R. Westbury" ],
      "venue" : "Database User’s Handbook Version",
      "citeRegEx" : "Westbury.,? \\Q1994\\E",
      "shortCiteRegEx" : "Westbury.",
      "year" : 1994
    }, {
      "title" : "Deep correlation for matching images and text",
      "author" : [ "Fei Yan", "Krystian Mikolajczyk" ],
      "venue" : "In Proc. of the 2015 IEEE Computer Society Conf. Computer Vision and Pattern Recognition",
      "citeRegEx" : "Yan and Mikolajczyk.,? \\Q2015\\E",
      "shortCiteRegEx" : "Yan and Mikolajczyk.",
      "year" : 2015
    }, {
      "title" : "The HTK book version 2.2",
      "author" : [ "Steve J. Young", "Dan Kernshaw", "Julian Odell", "Dave Ollason", "Valtcho Valtchev", "Phil Woodland" ],
      "venue" : "Technical report, Entropic,",
      "citeRegEx" : "Young et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Young et al\\.",
      "year" : 1999
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Abstract We present deep variational canonical correlation analysis (VCCA), a deep multi-view learning model that extends the latent variable model interpretation of linear CCA (Bach and Jordan, 2005) to nonlinear observation models parameterized by deep neural networks (DNNs).",
      "startOffset" : 177,
      "endOffset" : 200
    }, {
      "referenceID" : 22,
      "context" : "Interestingly, the resulting model resembles that of multi-view autoencoders (Ngiam et al., 2011), with the key distinction of an additional sampling procedure at the bottleneck layer.",
      "startOffset" : 77,
      "endOffset" : 97
    }, {
      "referenceID" : 0,
      "context" : "A classical approach in this setting is canonical correlation analysis (CCA, Hotelling, 1936) and its nonlinear extensions, including the kernel extension (KCCA, Lai and Fyfe, 2000; Akaho, 2001; Melzer et al., 2001; Bach and Jordan, 2002) and the deep neural network (DNN) extension (DCCA, Andrew et al.",
      "startOffset" : 155,
      "endOffset" : 238
    }, {
      "referenceID" : 20,
      "context" : "A classical approach in this setting is canonical correlation analysis (CCA, Hotelling, 1936) and its nonlinear extensions, including the kernel extension (KCCA, Lai and Fyfe, 2000; Akaho, 2001; Melzer et al., 2001; Bach and Jordan, 2002) and the deep neural network (DNN) extension (DCCA, Andrew et al.",
      "startOffset" : 155,
      "endOffset" : 238
    }, {
      "referenceID" : 3,
      "context" : "A classical approach in this setting is canonical correlation analysis (CCA, Hotelling, 1936) and its nonlinear extensions, including the kernel extension (KCCA, Lai and Fyfe, 2000; Akaho, 2001; Melzer et al., 2001; Bach and Jordan, 2002) and the deep neural network (DNN) extension (DCCA, Andrew et al.",
      "startOffset" : 155,
      "endOffset" : 238
    }, {
      "referenceID" : 4,
      "context" : "There is a probabilistic latent variable model interpretation of linear CCA (Bach and Jordan, 2005) as shown in Figure 1.",
      "startOffset" : 76,
      "endOffset" : 99
    }, {
      "referenceID" : 0,
      "context" : "A classical approach in this setting is canonical correlation analysis (CCA, Hotelling, 1936) and its nonlinear extensions, including the kernel extension (KCCA, Lai and Fyfe, 2000; Akaho, 2001; Melzer et al., 2001; Bach and Jordan, 2002) and the deep neural network (DNN) extension (DCCA, Andrew et al., 2013; Wang et al., 2015b). CCA projects two random vectors x ∈ Rx and y ∈ Ry into a lower-dimensional subspace so that the projections are maximally correlated. There is a probabilistic latent variable model interpretation of linear CCA (Bach and Jordan, 2005) as shown in Figure 1. Assume that x and y are linear functions of some lower-dimensional random variable z ∈ Rz , where dz ≤ min(dx, dy). When the prior distribution of the latent variable p(z) and the conditional distributions p(x|z) and p(y|z) are all Gaussian, Bach and Jordan (2005) showed that E[z|x] (resp.",
      "startOffset" : 182,
      "endOffset" : 853
    }, {
      "referenceID" : 4,
      "context" : "Figure 1: Probabilistic interpretation of CCA (Bach and Jordan, 2005).",
      "startOffset" : 46,
      "endOffset" : 69
    }, {
      "referenceID" : 18,
      "context" : "DCCA has achieved good performance in the multi-view representation learning setting across different domains (Wang et al., 2015b,a; Lu et al., 2015; Yan and Mikolajczyk, 2015).",
      "startOffset" : 110,
      "endOffset" : 176
    }, {
      "referenceID" : 36,
      "context" : "DCCA has achieved good performance in the multi-view representation learning setting across different domains (Wang et al., 2015b,a; Lu et al., 2015; Yan and Mikolajczyk, 2015).",
      "startOffset" : 110,
      "endOffset" : 176
    }, {
      "referenceID" : 22,
      "context" : "Interestingly, VCCA is related to multiview autoencoders (Ngiam et al., 2011), with the key distinctions of additional regularization on the posterior distribution and the sampling procedure at the bottleneck layer.",
      "startOffset" : 57,
      "endOffset" : 77
    }, {
      "referenceID" : 17,
      "context" : ", 2015b,a; Lu et al., 2015; Yan and Mikolajczyk, 2015). However, a disadvantage of DCCA is that it directly looks for DNNs that can map inputs into the low-dimensional space, without a model for generating samples from the latent space. Although Wang et al. (2015b)’s deep canonically correlated autoencoders (DCCAE) model optimizes the combination of the autoencoder objective (reconstruction errors) and the canonical correlation objective, the authors found that in practice, the canonical correlation term tends to dominate the reconstruction error terms in the DCCAE objective when tuning performance for a downstream task (especially when the inputs are noisy), and as a result the inputs are not reconstructed well.",
      "startOffset" : 11,
      "endOffset" : 266
    }, {
      "referenceID" : 4,
      "context" : "The probabilistic latent variable model of CCA (Bach and Jordan, 2005) defines the following joint distribution over the random variables (x,y):",
      "startOffset" : 47,
      "endOffset" : 70
    }, {
      "referenceID" : 14,
      "context" : "Inspired by Kingma and Welling (2014)’s work on variational autoencoders (VAE), we approximate pθ(z|x) with the conditional density qφ(z|x;φz), where φz is the collection of parameters of another DNN.",
      "startOffset" : 12,
      "endOffset" : 38
    }, {
      "referenceID" : 22,
      "context" : "which is the objective of the multi-view autoencoder (Ngiam et al., 2011).",
      "startOffset" : 53,
      "endOffset" : 73
    }, {
      "referenceID" : 13,
      "context" : "We apply the ADAM algorithm (Kingma and Ba, 2015) for optimizing our objectives.",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 14,
      "context" : "Recently, there has been much interest in unsupervised deep generative models (Kingma and Welling, 2014; Rezende et al., 2014; Goodfellow et al., 2014; Gregor et al., 2015; Makhzani et al., 2016; Burda et al., 2016; Alain et al., 2016).",
      "startOffset" : 78,
      "endOffset" : 235
    }, {
      "referenceID" : 24,
      "context" : "Recently, there has been much interest in unsupervised deep generative models (Kingma and Welling, 2014; Rezende et al., 2014; Goodfellow et al., 2014; Gregor et al., 2015; Makhzani et al., 2016; Burda et al., 2016; Alain et al., 2016).",
      "startOffset" : 78,
      "endOffset" : 235
    }, {
      "referenceID" : 7,
      "context" : "Recently, there has been much interest in unsupervised deep generative models (Kingma and Welling, 2014; Rezende et al., 2014; Goodfellow et al., 2014; Gregor et al., 2015; Makhzani et al., 2016; Burda et al., 2016; Alain et al., 2016).",
      "startOffset" : 78,
      "endOffset" : 235
    }, {
      "referenceID" : 8,
      "context" : "Recently, there has been much interest in unsupervised deep generative models (Kingma and Welling, 2014; Rezende et al., 2014; Goodfellow et al., 2014; Gregor et al., 2015; Makhzani et al., 2016; Burda et al., 2016; Alain et al., 2016).",
      "startOffset" : 78,
      "endOffset" : 235
    }, {
      "referenceID" : 19,
      "context" : "Recently, there has been much interest in unsupervised deep generative models (Kingma and Welling, 2014; Rezende et al., 2014; Goodfellow et al., 2014; Gregor et al., 2015; Makhzani et al., 2016; Burda et al., 2016; Alain et al., 2016).",
      "startOffset" : 78,
      "endOffset" : 235
    }, {
      "referenceID" : 5,
      "context" : "Recently, there has been much interest in unsupervised deep generative models (Kingma and Welling, 2014; Rezende et al., 2014; Goodfellow et al., 2014; Gregor et al., 2015; Makhzani et al., 2016; Burda et al., 2016; Alain et al., 2016).",
      "startOffset" : 78,
      "endOffset" : 235
    }, {
      "referenceID" : 1,
      "context" : "Recently, there has been much interest in unsupervised deep generative models (Kingma and Welling, 2014; Rezende et al., 2014; Goodfellow et al., 2014; Gregor et al., 2015; Makhzani et al., 2016; Burda et al., 2016; Alain et al., 2016).",
      "startOffset" : 78,
      "endOffset" : 235
    }, {
      "referenceID" : 27,
      "context" : "Our work is also related to the deep multi-view probabilistic models based on restricted Boltzmann machines (Srivastava and Salakhutdinov, 2014; Sohn et al., 2014).",
      "startOffset" : 108,
      "endOffset" : 163
    }, {
      "referenceID" : 26,
      "context" : "Our work is also related to the deep multi-view probabilistic models based on restricted Boltzmann machines (Srivastava and Salakhutdinov, 2014; Sohn et al., 2014).",
      "startOffset" : 108,
      "endOffset" : 163
    }, {
      "referenceID" : 31,
      "context" : "On the other hand, there is a rich literature in modeling multi-view data using the same or similar graphical models behind VCCA/VCCA-private (Wang, 2007; Jia et al., 2010; Salzmann et al., 2010; Virtanen et al., 2011; Memisevic et al., 2012; Klami et al., 2013).",
      "startOffset" : 142,
      "endOffset" : 262
    }, {
      "referenceID" : 12,
      "context" : "On the other hand, there is a rich literature in modeling multi-view data using the same or similar graphical models behind VCCA/VCCA-private (Wang, 2007; Jia et al., 2010; Salzmann et al., 2010; Virtanen et al., 2011; Memisevic et al., 2012; Klami et al., 2013).",
      "startOffset" : 142,
      "endOffset" : 262
    }, {
      "referenceID" : 25,
      "context" : "On the other hand, there is a rich literature in modeling multi-view data using the same or similar graphical models behind VCCA/VCCA-private (Wang, 2007; Jia et al., 2010; Salzmann et al., 2010; Virtanen et al., 2011; Memisevic et al., 2012; Klami et al., 2013).",
      "startOffset" : 142,
      "endOffset" : 262
    }, {
      "referenceID" : 30,
      "context" : "On the other hand, there is a rich literature in modeling multi-view data using the same or similar graphical models behind VCCA/VCCA-private (Wang, 2007; Jia et al., 2010; Salzmann et al., 2010; Virtanen et al., 2011; Memisevic et al., 2012; Klami et al., 2013).",
      "startOffset" : 142,
      "endOffset" : 262
    }, {
      "referenceID" : 21,
      "context" : "On the other hand, there is a rich literature in modeling multi-view data using the same or similar graphical models behind VCCA/VCCA-private (Wang, 2007; Jia et al., 2010; Salzmann et al., 2010; Virtanen et al., 2011; Memisevic et al., 2012; Klami et al., 2013).",
      "startOffset" : 142,
      "endOffset" : 262
    }, {
      "referenceID" : 15,
      "context" : "On the other hand, there is a rich literature in modeling multi-view data using the same or similar graphical models behind VCCA/VCCA-private (Wang, 2007; Jia et al., 2010; Salzmann et al., 2010; Virtanen et al., 2011; Memisevic et al., 2012; Klami et al., 2013).",
      "startOffset" : 142,
      "endOffset" : 262
    }, {
      "referenceID" : 17,
      "context" : "The dataset is generated using the MNIST dataset (LeCun et al., 1998), which consists of 28× 28 grayscale digit images, with 60K/10K images for training/testing.",
      "startOffset" : 49,
      "endOffset" : 69
    }, {
      "referenceID" : 26,
      "context" : "1 Noisy MNIST dataset We first demonstrate our algorithms on the noisy MNIST dataset used by Wang et al. (2015b). The dataset is generated using the MNIST dataset (LeCun et al.",
      "startOffset" : 93,
      "endOffset" : 113
    }, {
      "referenceID" : 2,
      "context" : ", 2011), deep CCA (DCCA, Andrew et al., 2013), deep canonically correlated autoencoders (DCCAE, Wang et al., 2015b), and the constrastive loss of Hermann and Blunsom (2014). We use DNNs with 3 hidden layers of 1024 ReLU units each to parameterize the conditional distributions: qφ(z|x), pθ(x|z), pθ(y|z) in VCCA, and additionally qφ(hx|x) and qφ(hy|y) in VCCA-private.",
      "startOffset" : 25,
      "endOffset" : 173
    }, {
      "referenceID" : 2,
      "context" : ", 2011), deep CCA (DCCA, Andrew et al., 2013), deep canonically correlated autoencoders (DCCAE, Wang et al., 2015b), and the constrastive loss of Hermann and Blunsom (2014). We use DNNs with 3 hidden layers of 1024 ReLU units each to parameterize the conditional distributions: qφ(z|x), pθ(x|z), pθ(y|z) in VCCA, and additionally qφ(hx|x) and qφ(hy|y) in VCCA-private. The capacities of these networks are the same as those of their counterparts in DCCA and DCCAE from Wang et al. (2015b). The reconstruction networks pθ(x|z) or pθ(x|z,hx) model each pixel of x as an independent Bernoulli variable and parameterize its mean (using a sigmoid activation); pθ(y|z) and pθ(y|z,hy) model y with diagonal Gaussians and parameterize the mean (using a sigmoid activation) and standard deviation for each pixel dimension.",
      "startOffset" : 25,
      "endOffset" : 489
    }, {
      "referenceID" : 28,
      "context" : "The effect of dropout We add dropout (Srivastava et al., 2014) to all intermediate layers and the input layers and find it to be very useful in our models, with most of the gain coming from dropout applied to the samples of z, hx and hy.",
      "startOffset" : 37,
      "endOffset" : 62
    }, {
      "referenceID" : 35,
      "context" : "Our dataset is the Wisconsin X-ray microbeam (XRMB) corpus (Westbury, 1994), which contains simultaneously recorded speech and articulatory measurements from 47 American English speakers.",
      "startOffset" : 59,
      "endOffset" : 75
    }, {
      "referenceID" : 10,
      "context" : "All learned feature types are used in a “tandem” speech recognition approach (Hermansky et al., 2000), i.",
      "startOffset" : 77,
      "endOffset" : 101
    }, {
      "referenceID" : 23,
      "context" : "The mean phone error rates (PER) over 6 folds obtained with different features are 2As in Wang and Livescu (2016), we use the Kaldi speech recognition toolkit (Povey et al., 2011) for feature extraction and recognition with hidden Markov models.",
      "startOffset" : 159,
      "endOffset" : 179
    }, {
      "referenceID" : 37,
      "context" : "(2015a,b) (who instead used the HTK toolkit (Young et al., 1999)) for the same types of features, but the relative merits of different types of features are consistent.",
      "startOffset" : 44,
      "endOffset" : 64
    }, {
      "referenceID" : 10,
      "context" : "All learned feature types are used in a “tandem” speech recognition approach (Hermansky et al., 2000), i.e., they are appended to the original 39D features and used in a standard hidden Markov model (HMM)-based recognizer with Gaussian mixture observation distributions. Each method uses up to 3 ReLU hidden layers, each of 1024 units, for the projection mappings. For VCCA/VCCA-private, we use isotropic Gaussian observation models for each view, with the standard deviation tuned by grid search. The mean phone error rates (PER) over 6 folds obtained with different features are 2As in Wang and Livescu (2016), we use the Kaldi speech recognition toolkit (Povey et al.",
      "startOffset" : 78,
      "endOffset" : 612
    }, {
      "referenceID" : 7,
      "context" : "We will also explore other observation models, including replacing the auto-encoder objective with that of adversarial networks (Goodfellow et al., 2014; Makhzani et al., 2016; Chen et al., 2016), which may have stronger empirical performance.",
      "startOffset" : 128,
      "endOffset" : 195
    }, {
      "referenceID" : 19,
      "context" : "We will also explore other observation models, including replacing the auto-encoder objective with that of adversarial networks (Goodfellow et al., 2014; Makhzani et al., 2016; Chen et al., 2016), which may have stronger empirical performance.",
      "startOffset" : 128,
      "endOffset" : 195
    }, {
      "referenceID" : 6,
      "context" : "We will also explore other observation models, including replacing the auto-encoder objective with that of adversarial networks (Goodfellow et al., 2014; Makhzani et al., 2016; Chen et al., 2016), which may have stronger empirical performance.",
      "startOffset" : 128,
      "endOffset" : 195
    } ],
    "year" : 2017,
    "abstractText" : "We present deep variational canonical correlation analysis (VCCA), a deep multi-view learning model that extends the latent variable model interpretation of linear CCA (Bach and Jordan, 2005) to nonlinear observation models parameterized by deep neural networks (DNNs). Marginal data likelihood as well as inference are intractable under this model. We derive a variational lower bound of the data likelihood by parameterizing the posterior density of the latent variables with another DNN, and approximate the lower bound via Monte Carlo sampling. Interestingly, the resulting model resembles that of multi-view autoencoders (Ngiam et al., 2011), with the key distinction of an additional sampling procedure at the bottleneck layer. We also propose a variant of VCCA called VCCA-private which can, in addition to the “common variables” underlying both views, extract the “private variables” within each view. We demonstrate that VCCA-private is able to disentangle the shared and private information for multi-view data without hard supervision.",
    "creator" : "LaTeX with hyperref package"
  }
}