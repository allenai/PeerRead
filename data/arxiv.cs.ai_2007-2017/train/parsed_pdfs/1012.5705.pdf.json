{
  "name" : "1012.5705.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Looking for plausibility",
    "authors" : [ "Wan Ahmad Tajuddin", "Wan Abdullah" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "In the interpretation of experimental data, one is actually looking for plausible explanations. We look for a measure of plausibility, with which we can compare different possible explanations, and which can be combined when there are different sets of data. This is contrasted to the conventional measure for probabilities as well as to the proposed measure of possibilities. We define what characteristics this measure of plausibility should have.\nIn getting to the conception of this measure, we explore the relation of plausibility to abductive reasoning, and to Bayesian probabilities. We also compare with the Dempster-Schaefer theory of evidence, which also has its own definition for plausibility. Abduction can be associated with biconditionality in inference rules, and this provides a platform to relate to the Collins-Michalski theory of plausibility. Finally, using a formalism for wiring logic onto Hopfield neural networks, we ask if this is relevant in obtaining this measure."
    }, {
      "heading" : "INTRODUCTION",
      "text" : "Traditionally, uncertainty in propositions is handled through the concept of probability with its statistical interpretation. It is however, a bit strange when such an approach which is based on counting events, is applied to hypotheses, like explanatory propositions. For example, in the interpretation of experimental data, one is actually looking for plausible explanations to describe the observed data. Whilst probability may be seen to underlie data measurement, a statistical picture for explanations may be viable only in a multiple-world universe.\nWe thus seek a measure for plausibility, with which we can compare different possible explanations. Furthermore, this measure of plausibility should allow appropriate combination when different explanations and/or sets of data are combined."
    }, {
      "heading" : "UNCERTAINTY MEASURES",
      "text" : "Probabilities are calculated from the numbers of events satisfying respective propositions from a total set and are normalized. Thus probabilities for alternative explanations are exclusive, and the more\nalternatives there are, the less are the values for probabilities in general. Plausibility, on the other hand, should not behave as such.. The plausibility of a proposition should not depend on the plausibility of an alternative proposition, much less reduced by it.\nIn logical conjunctions, probabilities are combined by multiplication, and in disjunctions, by addition, when the atoms are exclusive:\nP(X∧Y) = P(X)×P(Y) P(X∨Y) = P(X)+P(Y)-P(X∧Y)\nThis does not necessarily follow for plausibilities; in fact, for example, intuitively it does not seem correct that the plausibility of (X or Y) is the sum of the individual plausibilities of X and Y.\nPlausibility seems closer to the notion of possibility [1] (based on fuzzy sets, thus rather similar to fuzzy logic [2]) when combinations are concerned. Disjunctions of possibilities return the maximum values, while conjunctions of possibilities involve the minimum values:\npos(X∧Y) ≤ min(pos(X),pos(Y)) pos(X∨Y) = max(pos(X),pos(Y))\nOther than probabilities and possibilities, other uncertainty measures (sometimes with similar names) have been proposed by, for example Dempster and Shafer [3], further studied below, and others."
    }, {
      "heading" : "PLAUSIBILITY",
      "text" : "Let us now define what characteristics the measure of plausibility should have.\n• We can have a normalized scale from 0, which would denote something not at all plausible, to 1 denoting something fully plausible. Let us denote plausibility by 'pl':\npl(X) = 0 : X is not all plausible pl(X) = 1 : X is fully plausible\n• Plausibilities are not exclusive. For example, even when X and Y are not logically compatible, yet X can be fully plausible at the same time that Y is fully plausible:\npl(X) + pl(Y) > 1 though X ∩ Y = ∅\n• Like possibility, and unlike probability, plausibility is not self-dual. That is, pl(X) ≠ 1-pl(¬X).\nIn another words, when something is not at all plausible, it does not mean that it is fully plausible that that something is untrue. Let us include negative values for plausibility to enable the representation of negations, and require that\npl(¬X) = - pl(X). This means that now\npl(X) ∈ [-1,1], giving values for fully plausibly false on the one end, and fully plausibly true on the other, with nonplausibility (‘non-relevance’) in the middle.\nA Wiktionary definition of plausibility is “seemingly or apparently valid, likely, or acceptable”. This suggests that plausibility may be related to abduction."
    }, {
      "heading" : "ABDUCTIVE REASONING",
      "text" : "Abduction (see e.g. [4]), first described by Peirce more than a hundred years ago, is the process of arriving at the premise which would 'explain' some situation. Given that the set of propositions C formally follows from the set of propositions A subject to the set of logical rules T, then the derivation of C from T and A is deduction, that of T from C and A is induction, and that of A from T and C is abduction. Note that the abduced set is sufficient, but not necessary for C to follow from T, and may be one of many alternatives; in abduction one also usually looks for the most natural explanation in the form of the most economical one.\nGiven a set of true propositions, then, an abduced proposition is a plausible proposition. Plausibility then is a measure of how good is the proposition in explaining the available facts. It can be related to how much of the requirement for sufficiency has been satisfied."
    }, {
      "heading" : "BAYESIAN PROBABILITIES",
      "text" : "When propositions have probabilistic values, then abduction is related to conditional probabilities. Bayes' theorem states that the probability for θ given x,\nP(θ |x) = P(x |θ ) P(θ ) / P(x) depends on the conditional probability, or likelihood, of x given θ, multiplied by the prior probability for θ. Abduction being the 'reverse' of deduction is parallel to the conditional probability being the 'reverse' of likelihood.\nConditional probabilities then present a possible model for plausibilities. Combinations of plausibilities in inferences can copy those in Bayesian networks [5]. However, the determination of prior probabilities pose a problem for calculating conditional probabilities."
    }, {
      "heading" : "DEMPSTER-SHAFER THEORY",
      "text" : "The Dempster-Shafer theory of evidence [3] has its own definition for plausibility which is based on some belief function. Plausibility for a set of propositions is the sum of “belief masses” of other sets of propositions which intersect with this set, while belief is that for those which are subsets, and they bound the probability:\nbel(X) ≤ P(X) ≤ pl(X) and are related,\npl(X) = 1 - bel(¬X). The theory prescribes a method for combining belief masses from different assignments, along the lines of Bayesian theory. To obtain the plausibility of a combined set of propositions, one has to recalculate using the primary definition.\nThe assignment of belief masses is problematic here, and also, their association with probabilities is misleading. Furthermore, plausibility being the upper bound for belief suggests a rather subjective definition."
    }, {
      "heading" : "BICONDITIONALITY AND COLLINS-MICHALSKI",
      "text" : "A theory for plausible reasoning has been proposed by Collins and Michalski [6]. It has several types of inferences, namely mutual implication, mutual dependency, generalization, specialization, and similarity/analogy. A drawback is that it has a restrictively large number of parameters to model uncertainty.\nThe inferences existing in Collins-Michalski theory are mostly explained by biconditionality, wherein implications are taken to also include some degree of equivalences [7]. This relates to abduction, as abduction coincides with deduction when implications are replaced by equivalences. In the probabilistic picture, this is like equating the conditional probability to the likelihood,\nP(θ |x) = P(x |θ ) Of course this is not rational; after all, this is plausible reasoning. However, this supports the notion that plausibility has an abductive basis."
    }, {
      "heading" : "CONNECTIONIST",
      "text" : "It has been shown [8,9] that logic can be hardwired onto a Hopfield neural network [10]. The Hopfield network minimizes a Lyapunov or “energy” function related to the synaptic strengths [11]. By writing an expression yielding a value proportional to unsatisfied clauses, the network then searches for an optimum logical interpretation of the propositional atoms, represented by neurons. This then gives the values for the synapses. In some sense, the logic is contained in the synapses.\nFrom the synaptic strengths then, we can make inferences about the logical clauses [12]. This may be helpful in our search for a measure of plausibility."
    }, {
      "heading" : "PLAUSIBILITY, PLAUSIBLY",
      "text" : "Finally, we can make some preliminary moves in the direction of plausibility. Taking an abductive interpretation, the more of the set of existing ‘measured’ propositions C a hypothetical proposition Ai forces, the more plausible Ai should be. Perhaps then the plausibility of Ai corresponds to the fraction of propositions in C that is forced correct (both, either true or false) by Ai. If Ai does not have any effect on C then Ai should have plausibility 0, and if forces all of the known propositions C to be correct, then Ai have plausibility 1.\nWhat is of interest is the combination of plausibilities. Is it possible to have simple combining rules for conjunctions and disjunctions, for example? For propositions Ai and Aj each forcing disjoint subsets of C and having no joint effects on the correctness of any subset, then simply\npl(Ai∧ Aj) = pl(Ai) + pl(Aj). In the case of the existence of some common proposition Ci forced by both Ai and Aj, i.e. the rule base T contains\nAi → Ci Aj → Ci\nthen the value is less, as also implicated by the upper limit of 1 on the value of plausibilities, while in the case that Ai and Aj have a joint effect on Ci, e.g.\nAi∧ Aj → Ci\nthen the value is more. In general, plausibility of conjunctions can be complicated. It may simplify somewhat for singleton C’s, whence the measure would involve some kind of probability, and these phenomena would be averaged out.\nHaving plausibility related to fraction of forced correctness is compatible with our previous definition of negative plausibilities. For disjoint hypotheses Ai and Aj, using de Morgan’s theorem, the plausibility for a disjunction is similarly given,\npl(Ai∨Aj) = pl(Ai) + pl(Aj), a bit counter-intuitively. Proceeding, thus\npl(Ai → Aj) = pl(Aj) - pl(Ai). Interestingly, this has some similarity to synaptic weight assignment in connectionist logic previously discussed where whilst some weights are increased with the addition of a clause, some are decreased.\nUsing hardwired logic on connectionist networks, and forcing the correct values for the neurons in C, and that for a hypothesis, the final network state yields the least value for the energy function, which corresponds to the number of clauses unsatisfied. This intuitively should also represent the number of errors in C, allowing an estimation of the plausibility of the hypothesis. For this, synaptic weights, or equivalently, clauses in T need to be known explicitly; however, this can be obtained [12] through Hebbian learning."
    }, {
      "heading" : "CONCLUSIONS",
      "text" : "In summary, we have explored the concept of plausibility and attempted to characterize a measure for it. In particular, we have taken an abductive basis, and conjecture that a hypothesis is more plausible if it forces more of the measured truth. For simple cases we can write down the rules for combinations. We also indicated how plausibility can be measured using a logic neural network.\nThis exploratory proposal for plausibility needs to be further investigated, especially for the non-trivial cases of combinations. It is hoped that combination of plausibilities would help in the interpretation of combinations of measurements in experimental data."
    }, {
      "heading" : "13 (1989) 1-49.",
      "text" : "[7] W. A. T. Wan Abdullah, “Biconditionality, Analogy, Induction and Predicate Logic”, Malaysian J. Comput. Sci. 3 (1987) 21-28.\n[8] W. A. T. Wan Abdullah, “Neural Network Logic”, in O. Benhar, C. Bosio, P. del Giudice & E. Tabet (eds.), Neural Networks: From Biology to High Energy Physics, ETS Editrice, Pisa, 1991, pp. 135-142.\n[9] W. A. T. Wan Abdullah, “Logic Programming on a Neural Network”, J. of Intelligent Systems 7 (1992) 513-519.\n[10] J. J. Hopfield, “Neural networks and physical systems with emergent collective computational abilities”, Proc. Natl. Acad. Sci. USA 79 (1982) 2554-2558.\n[11] J. J. Hopfield & D. W. Tank, ““Neural” Computation of Decisions in Optimization Problems”, Biol. Cybern. 52 (1985) 141-152.\n[12] W. A. T. Wan Abdullah, “The Logic of Neural Networks”, Phys. Lett. 176A (1993) 202-206."
    } ],
    "references" : [ {
      "title" : "Fuzzy Sets as the Basis for a Theory of Possibility",
      "author" : [ "L. Zadeh" ],
      "venue" : "Fuzzy Sets and Systems 1 ",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1978
    }, {
      "title" : "Fuzzy sets",
      "author" : [ "L.A. Zadeh" ],
      "venue" : "Information and Control 8 ",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1965
    }, {
      "title" : "A Mathematical Theory of Evidence",
      "author" : [ "G. Shafer" ],
      "venue" : "Princeton Univ. Press, Princeton NJ",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1976
    }, {
      "title" : "Abduction",
      "author" : [ "L. Magnani" ],
      "venue" : "Reason and Science: Processes of Discovery and Explanation, Kluwer, New York",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Probabilistic Inferences in Bayesian Networks",
      "author" : [ "J. Ding" ],
      "venue" : "e-print arXiv:1011.0935",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Biconditionality",
      "author" : [ "W.A.T. Wan Abdullah" ],
      "venue" : "Analogy, Induction and Predicate Logic”, Malaysian J. Comput. Sci. 3 ",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "Neural Network Logic",
      "author" : [ "W.A.T. Wan Abdullah" ],
      "venue" : "O. Benhar, C. Bosio, P. del Giudice & E. Tabet (eds.), Neural Networks: From Biology to High Energy Physics, ETS Editrice, Pisa",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Logic Programming on a Neural Network",
      "author" : [ "W.A.T. Wan Abdullah" ],
      "venue" : "J. of Intelligent Systems 7 ",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Neural networks and physical systems with emergent collective computational abilities",
      "author" : [ "J.J. Hopfield" ],
      "venue" : "Proc. Natl. Acad. Sci. USA 79 ",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1982
    }, {
      "title" : "Neural” Computation of Decisions in Optimization Problems",
      "author" : [ "J.J. Hopfield", "D.W. Tank" ],
      "venue" : "Biol. Cybern",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1985
    }, {
      "title" : "The Logic of Neural Networks",
      "author" : [ "W.A.T. Wan Abdullah" ],
      "venue" : "Phys. Lett. 176A ",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1993
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Plausibility seems closer to the notion of possibility [1] (based on fuzzy sets, thus rather similar to fuzzy logic [2]) when combinations are concerned.",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 1,
      "context" : "Plausibility seems closer to the notion of possibility [1] (based on fuzzy sets, thus rather similar to fuzzy logic [2]) when combinations are concerned.",
      "startOffset" : 116,
      "endOffset" : 119
    }, {
      "referenceID" : 2,
      "context" : "Other than probabilities and possibilities, other uncertainty measures (sometimes with similar names) have been proposed by, for example Dempster and Shafer [3], further studied below, and others.",
      "startOffset" : 157,
      "endOffset" : 160
    }, {
      "referenceID" : 0,
      "context" : "This means that now pl(X) ∈ [-1,1], giving values for fully plausibly false on the one end, and fully plausibly true on the other, with nonplausibility (‘non-relevance’) in the middle.",
      "startOffset" : 28,
      "endOffset" : 34
    }, {
      "referenceID" : 3,
      "context" : "[4]), first described by Peirce more than a hundred years ago, is the process of arriving at the premise which would 'explain' some situation.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "Combinations of plausibilities in inferences can copy those in Bayesian networks [5].",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 2,
      "context" : "The Dempster-Shafer theory of evidence [3] has its own definition for plausibility which is based on some belief function.",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 5,
      "context" : "The inferences existing in Collins-Michalski theory are mostly explained by biconditionality, wherein implications are taken to also include some degree of equivalences [7].",
      "startOffset" : 169,
      "endOffset" : 172
    }, {
      "referenceID" : 6,
      "context" : "It has been shown [8,9] that logic can be hardwired onto a Hopfield neural network [10].",
      "startOffset" : 18,
      "endOffset" : 23
    }, {
      "referenceID" : 7,
      "context" : "It has been shown [8,9] that logic can be hardwired onto a Hopfield neural network [10].",
      "startOffset" : 18,
      "endOffset" : 23
    }, {
      "referenceID" : 8,
      "context" : "It has been shown [8,9] that logic can be hardwired onto a Hopfield neural network [10].",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 9,
      "context" : "The Hopfield network minimizes a Lyapunov or “energy” function related to the synaptic strengths [11].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 10,
      "context" : "From the synaptic strengths then, we can make inferences about the logical clauses [12].",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 10,
      "context" : "For this, synaptic weights, or equivalently, clauses in T need to be known explicitly; however, this can be obtained [12] through Hebbian learning.",
      "startOffset" : 117,
      "endOffset" : 121
    } ],
    "year" : 2010,
    "abstractText" : "In the interpretation of experimental data, one is actually looking for plausible explanations. We look for a measure of plausibility, with which we can compare different possible explanations, and which can be combined when there are different sets of data. This is contrasted to the conventional measure for probabilities as well as to the proposed measure of possibilities. We define what characteristics this measure of plausibility should have. In getting to the conception of this measure, we explore the relation of plausibility to abductive reasoning, and to Bayesian probabilities. We also compare with the Dempster-Schaefer theory of evidence, which also has its own definition for plausibility. Abduction can be associated with biconditionality in inference rules, and this provides a platform to relate to the Collins-Michalski theory of plausibility. Finally, using a formalism for wiring logic onto Hopfield neural networks, we ask if this is relevant in obtaining this measure.",
    "creator" : "Writer"
  }
}