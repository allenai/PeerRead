{
  "name" : "1610.04317.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Approximate Counting, the Lovász Local Lemma and Inference in Graphical Models",
    "authors" : [ "Ankur Moitra" ],
    "emails" : [ "moitra@mit.edu." ],
    "sections" : [ {
      "heading" : null,
      "text" : "Moreover our algorithm extends straightforwardly to approximate sampling, which shows that under Lovász Local Lemma-like conditions it is not only possible to find a satisfying assignment, it is also possible to generate one approximately uniformly at random from the set of all satisfying assignments. Our approach is a significant departure from earlier techniques in approximate counting, and is based on a framework to bootstrap an oracle for computing marginal probabilities on individual variables. Finally, we give an application of our results to show that it is algorithmically possible to sample from the posterior distribution in an interesting class of graphical models.\n∗Massachusetts Institute of Technology. Department of Mathematics and the Computer Science and Artificial Intelligence Lab. Email: moitra@mit.edu. This work was supported in part by NSF CAREER Award CCF-1453261, NSF Large CCF-1565235, an Alfred P. Sloan Fellowship, an Edmund F. Kelley Research Award, a Google Research Award and the MIT NEC Corporation.\nar X\niv :1\n61 0.\n04 31\n7v 1\n[ cs\n.D S]\n1 4\nO ct"
    }, {
      "heading" : "1 Introduction",
      "text" : ""
    }, {
      "heading" : "1.1 Background",
      "text" : "In this paper we introduce a new approach for approximately counting in bounded degree systems with higher-order constraints. For example, if we are given a CNF formula Φ with n variables and m clauses with the property that each clause contains at least k variables and each variable belongs to at most d clauses we ask:\nQuestion 1.1. How does k need to relate to d for there to be algorithms to estimate the number of satisfying assignments to Φ within a (1± 1/nc) multiplicative factor?\nIn the case of a monotone CNF formula where no variable appears negated, the problem is equivalent to the following: Suppose we are given a hypergraph on n nodes and m hyperedges with the property that each hyperedge contains at least k nodes and each node belongs to at most d hyper edges. How does k need to relate to d in order to be able to approximately compute the number of independent sets. Here an independent set is a subset of nodes for which there is no induced hyperedge. Bordewich, Dyer and Karpinski [5] gave an MCMC algorithm for approximating the number of hypergraph independent sets (equivalently, the number of satisfying assignments in a monotone CNF formula) that succeeds whenever k ≥ d+2. Bezakova et al. [4] gave a deterministic algorithm that succeeds whenever k ≥ d ≥ 200 and proved that when d ≥ 5 · 2k/2 it is NP -hard to approximate the number of hypergraph independent sets even within an exponential factor.\nMore broadly, there is a rich literature on approximate counting problems. In a seminal work, Weitz [26] gave an algorithm to approximately count in the hardcore model with parameter λ in graphs of degree at most d whenever\nλ ≤ (d− 1) d−1\n(d− 2)d\nAnd in another seminal work, Sly [24] showed a matching hardness result which was later improved in various respects by Sly and Sun [25] and Galanis, S̆tefankovic̆ and Vigoda [10]. These results show that approximate counting is algorithmically possible if and only if there is spatial mixing. Moreover, Weitz’s result can be thought of as a comparison theorem that spatial mixing holds on a bounded degree graph if and only if it holds on an infinite tree with the same degree bound. There have been a number of attempts to generalize these results to hypergraphs, many of which follow the approach of defining analogues of the self-avoiding walk trees used in Weitz’s algorithm [26]. However what makes hypergraph versions of these problems more challenging is that spatial mixing fails, even on trees. And we can see that there are exponential gaps between the upper and lower bounds, since the algorithms above require k to be linear in d while the lower bounds only rule out k ≤ 2 log d−O(1).\nWe can take another vantage point to study these problems. Bounded degree CNF formulae are also one of the principal objects of study in the Lovász Local Lemma [9] which is a celebrated result in combinatorics that guarantees when k ≥ log d + O(1) that Φ has at least one satisfying assignment. The original proof of the Lovász Local Lemma was nonconstructive and did not yield a polynomial time algorithm for finding such an assignment, even though it was guaranteed to exist. Beck [3] gave an algorithm followed by a parallel\nversion due to Alon [2] that can find a satisfying assignment whenever k ≥ 8 log d + O(1). And in a celebrated recent result, Moser and Tardos [20] gave an algorithm matching exactly the existential result. This was followed by a number of works giving constructive proofs of various other settings and generalizations of the Lovász Local Lemma [13, 1, 15, 18]. However these works leave open the following question:\nQuestion 1.2. Under the conditions of the Lovász Local Lemma (i.e. when k is logarithmic in d) is it possible to approximately sample from the uniform distribution on satisfying assignments?\nApproximate counting and approximate sampling problems are well-known to be related. When the problem is self-reducible, they are in fact algorithmically equivalent [16, 22]. However in our setting the problem is not self-reducible because as we fix variables we could violate the assumption that k is at least logarithmic in d. It is natural to hope that under the conditions of the Lovász Local Lemma, that there is an algorithm for approximate sampling that matches the limits of the existential and now algorithmic results. However the hardness results of Bezákova et al. [4] imply that we need at least another factor of two, and that it is NP -hard to approximately count when k ≤ 2 log d−O(1).\nIn fact, there is another connection between the Lovász Local Lemma and approximate counting. Scott and Sokal [25] showed that given the dependency graph of events in the local lemma, the best lower bound on the probability of an event guaranteed to exist by the Lovász Local Lemma (i.e. the fraction of satisfying assignments) is exactly the solution to some counting problem. Harvey, Srivastava and Vondrák [14] recently adapted techniques of Weitz to complex polydisks and gave an algorithm for approximately computing this lower bound. This yields a lower bound on the fraction of satisfying assignments, however the actual number could be exponentially larger."
    }, {
      "heading" : "1.2 Our Results",
      "text" : "Our main result is an algorithm to approximately count the number of solutions when k is at least logarithmic in d. In what follows, let c, k and d be constants. We prove1:\nTheorem 1.3 (informal). Suppose Φ is a CNF formula with at least k variables per clause and at most d clauses containing any one variable. For any k ≥ 20 log d there is a deterministic polynomial time algorithm for approximating the number of satisfying assignments to Φ within a multiplicative (1 ± 1/nc) factor. Moreover there is a randomized polynomial time algorithm to sample from a distribution that is 1/nc-close in total variation distance to the uniform distribution on satisfying assignments.\nThis algorithm closes an exponential gap between the known upper [5, 4] and lower [4] bounds. It also shows that under Lovász Local Lemma-like conditions not only is it possible to efficiently find a satisfying assignment, it is possible to find a random one. Moreover our\n1We have not made an attempt to optimize the constant in this theorem. It remains an interesting question to improve the constant, in hopes of approaching or even finding a sharp phase transition corresponding to where approximate counting is easy and where it is hard. For counting independent sets in hypergraphs, this will necessarily be a different threshold than the threshold where the Gibbs measure is unique [4].\napproach is a significant departure from earlier techniques based either on path coupling [5] or adapting Weitz’s approach to non-binary models and hypergraphs [11, 21, 23, 19, 4]. The results above appear in Theorem 6.3 and Theorem 6.5.\nOur approach starts from a thought experiment about what we could do if we had access to a very powerful oracle that could answer questions about the marginal distributions of individual variables under the uniform distribution on satisfying assignments. We use this oracle and properties of the Lovász Local Lemma (namely, bounds it gives on the marginal distribution of individual variables) to construct a coupling between two random satisfying assignments so that both agree outside some logarithmic sized component. If we knew the distribution on what logarithmic sized component this coupling procedure produces, we could brute force and find the ratio of the number of satisfying assignments with x = T to the number with x = F to compute marginals at x. However the distribution of what component the coupling produces depends intimately on the powerful oracle we have assumed that we have access to.\nInstead, we abstract the coupling procedure as a random root-to-leaf path in a tree that represents the state of the coupling. We show that at the leaves of this tree, there is a way to fractionally charge assignments where x = T against assignments where x = F . Crucially, doing so requires only brute-force search on a logarithmic sized component. Finally, we show that there is a polynomial sized linear program to find a flow through the tree that produces an approximately valid way to fractionally charge assignments with x = T against ones with x = F , and that any such solution certifies the correct marginal distribution. From these steps, we have thus bootstrapped an oracle for answering queries about the marginal distribution. Our main results then follow from utilizing this oracle. In settings where the problem is self-reducible [22] it is well-known how to go from knowing the marginal to approximate counting and sampling. In our setting, the problem is not self-reducible because setting variables could result in clauses becoming too small in which case k would not be large enough as a function of d. We are able to get around this by using the Lovász Local Lemma once more to find a safe ordering in which to set the variables."
    }, {
      "heading" : "1.3 Further Applications",
      "text" : "Our algorithms have an interesting application in graphical models. Directed graphical models are a rich language for describing distributions by the conditional relationships of their variables. However very little is known algorithmically about learning them or performing basic tasks such as inference [7, 8]. In most settings, these problems are computationally hard. However we can study an interesting class of directed graphical models which we call cause networks. See Figure 1.\nDefinition 1.4. In a cause network there is a collection of hidden variables x1, x2, ...xn that are chosen independently to be T or F with equal probability. There is a collection of m observed variables each of which is either an OR or an AND of several variables or their negations.\nOur goal is: Given a random sample x1, x2, ...xn from the model where we observe the truth value of each of the m clauses, to sample from the posterior distribution on the hidden variables. This generalizes graphical models such as the symptom-disease network where\nthe hidden variables represent diseases that a patient may have, and the clauses represent observed symptoms. We will require the following regularity condition on our observations:\nDefinition 1.5. A collection of observations is regular if for every observed variable, the corresponding clause is adjacent to (i.e. shares a variable with) at most k/6 OR clauses that are false and at most k/6 AND clauses that are true.\nNow, as an immediate corollary we have:\nCorollary 1.6. Given a cause network where each observed variable depends on at least k hidden variables, each hidden variable affects at most d observed variables and k ≥ 30 log d, there is a polynomial time algorithm for sampling from the posterior distribution for any regular collection of observations.\nThis is a rare setting where there is an algorithm to solve an inference problem in graphical models but (i) the underlying graph does not have bounded treewidth and (ii) correlation decay fails. We believe that our techniques may eventually be applicable to settings where the observed variables are noisy functions of the hidden variables and where the hidden variables are not distributed uniformly."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "In this paper, we will be interested in approximately counting the number of satisfying assignments to a CNF formula. For example, we could be given:\nΦ = (x1 ∨ x3 ∨ x5) ∧ (x2 ∨ x3 ∨ x8) ∧ ... ∧ (x4 ∨ x5 ∨ x9)\nLet’s fix some parameters. We will assume that there are n variables and there are m clauses each of which is an OR of at least k distinct variables. Finally, we will require a degree bound that each variable appears in at most d clauses. We will be interested in the relationships between k and d that allow us to approximately count the number of satisfying assignments in polynomial time.\nThe celebrated Lovász Local Lemma tells us conditions on k and d where we are guaranteed that there is at least one satisfying assignment:\nTheorem 2.1. [9] If e(d+ 1)2−k ≤ 1 then Φ has at least one satisfying assignment.\nMoser and Tardos [20] gave an algorithm to find a satisfying assignment under these same conditions . However the assignment that their randomized algorithm finds is not uniform from the set of all satisfying assignments. Our goal is to solve to be able to both approximately count and uniformly sample when k is logarithmic in d.\nThere are many more related results, but we will not review them all here. Instead we state a version of the asymmetric local lemma given in [12] which gives us some control on the uniform distribution on assignments. Let C be the collection of clauses in Φ. Let Pr[·] denote the uniform distribution all assignments – i.e. uniform on {T, F}n. Finally, for a clause b let Γ(b) denote all the clauses that intersect b. We can abuse notation and for any event a that depends on some set of the variables, let Γ(a) denote all the clauses that contain any of the variables on which a depends.\nTheorem 2.2. Suppose there is an assignment x : C → (0, 1) such that for all c ∈ C we have Pr[c is unsatisfied] ≤ x(c) ∏\nb∈Γ(c)\n( 1− x(b) ) then there is at least one satisfying assignment. Moreover the uniform distribution D on satisfying assignments satisfies that for any event a\nPrD[a] ≤ Pr[a] ∏\nb∈Γ(a)\n( 1− x(b) )−1 Notice that this inequality is one-sided, as it ought to be. After all if we take b to be some clause, and a to be the event that b is not satisfied then we know that PrD[a] = 0 even though Pr[a] is nonzero. However what this theorem does tell us is that the marginal distribution of D on any variable is close to uniform. We will establish a quantitative version of this statement in the following corollary:\nCorollary 2.3. Suppose that ed62−k ≤ 1. Then for every variable xi, we have 1\n2 − 2 d5 ≤ PrD[xi = T ] ≤ 1 2 + 2 d5\nProof. Set x(c) = 1 d6\nfor each clause c, and consider the event a that xi = T . Now invoking Theorem 2.2 we calculate:\nPrD[xi = T ] ≤ Pr[xi = T ] ∏\nb∈Γ(a)\n( 1− x(b) )−1 ≤ (1 2 )( 1− 1 d6 )−1 ≤ 1 2 + 2 d5\nwhere the last inequality follows because (1− 1 d6\n)−d ≤ e 2 d5 ≤ 1 + 4\nd5 . An identical calculation\nworks for the event xi = F . All that remains is to check that the condition in Theorem 2.2 holds, which is a standard calculation: If c is a clause then\nPr[c is unsatisfied] ≤ ( 1 d6 )( 1− 1 d6 )d The left hand side is at most 2−k because each clause has at least k distinct variables, and the right hand side is at least ( 1\nd6 )(1 e ). Rearranging completes the proof.\nNotice that k is still only logarithmic in d but with a larger constant, and by increasing this constant we get some useful facts about the marginals of the uniform distribution on satisfying assignments."
    }, {
      "heading" : "3 A Coupling Procedure",
      "text" : ""
    }, {
      "heading" : "3.1 Marked Variables",
      "text" : "Now we are almost ready to define a coupling procedure. The basic strategy that we will employ is to start from either x = T and x = F , and then sample from the corresponding marginal distribution on satisfying assignments. If we sample a variable y next, then Corollary 2.3 tells us that regardless of whether x = T or x = F , each clause has at least k − 1 variables remaining and so the marginal distribution on y is still close to uniform.\nThus we will try to couple the conditional distributions, when starting from x = T or x = F as well as we can, to show that the marginal distribution on variables that are all at least some distance ∆ away must converge in total variation distance. There is, however, an important catch that motivates the need for a fix. Imagine that we continue in this fashion, sampling variables from the appropriate conditional distribution. We can reach a situation where a clause c has all of its variables except y set and yet the clause is still unsatisfied. The marginal distribution on y is no longer close to uniform. Hence, reaching small clauses is problematic because then we cannot say much about the marginal distribution on the remaining variables and it would be difficult to construct a good coupling.\nInstead, our strategy is to use the Lovász Local Lemma once more, but to decide on a set of variables in advance which we call marked.\nLemma 3.1. Set c0 = e ( 1 2 )( 1 6 )2. Suppose that 2e(d+ 1)c−k0 ≤ 1. Then there is an assignment\nM : {xi}ni=1 → {marked, unmarked}\nsuch that for every clause c, it has at least k 3 marked and at least k 3 unmarked variables.\nProof. We will choose each variable to be marked or unmarked with equal probability, and independently. Consider the m bad events, one for each clause c, that c does not have enough marked or enough unmarked variables. Then we have\nPr[c is bad] ≤ 2e−( 1 2 )( 1 6 )2k = 2c−k0\nwhich follows from the Chernoff bound. Now we can appeal to the Lovász Local Lemma to get the desired conclusion.\nOnly the variables that are marked will be allowed to be set to either T or F by the coupling procedure. The above lemma guarantees that every clause c always has enough remaining variables that can make it true that the marginal distribution on any marked variable always is close to uniform."
    }, {
      "heading" : "3.2 Factorizing Formulas",
      "text" : "Now fix a variable x. We will build up two partial assignments, and will use the notation\nA1(x) = T and A2(x) = F\nto indicate that the first partial assignment sets x to T , and the second one sets x to F . Furthermore we will refer to the conditional distribution that is uniform on all satisfying assignments consistent with the decisions made so far in A1 and D1. Similarly we will refer to the other conditional distribution as D2. Note that these distributions are updated as more variables are set.\nWe can now state our goal. Suppose we have partial assignments A1 and A2. Then we will want to write ΦA1 = ΦI1 ∧ ΦO1 where ΦA1 is the subformula we get after making the assignments in A1 and simplifying – i.e. removing variables that are F , and deleting clauses that already have a variable set to T . Similarly we will want to write\nΦA2 = ΦI2 ∧ ΦO2\nFinally, we want the following conditions to be met:\n(1) ΦO1 = ΦO2(:= ΦO)\n(2) ΦI1 and ΦO share no variables, and similarly for ΦI2 and ΦO\nThe crucial point is that if we can find partial assignments A1 and A2 where ΦA1 and ΦA2 meet the above conditions, then the conditional distribution on all variables in ΦO is exactly the same. We will use the notation\nD1 ∣∣∣ vars(ΦO)\nto denote the conditional distribution of D1 projected onto just the variables in ΦO. Then we have:\nLemma 3.2. If the above factorization conditions are met, then D1 ∣∣∣ vars(ΦO) = D2 ∣∣∣ vars(ΦO)\nProof. From the assumption that ΦA1 = ΦI1∧ΦO and because ΦI1 and ΦO share no variables, it means that there are no clauses that contain variables from both the subformulas ΦI1 and ΦO. Any such clause would prevent us from writing the formula ΦA1 in such a factorized form. Thus the distribution D1 is simply the cross product of the uniform distributions on satisfying assignments to ΦI1 and ΦO. An identical statement holds for D1 which completes the proof.\nNote that meeting the factorization conditions does not mean that the number of satisfying assignments to ΦA1 and ΦA2 are the same."
    }, {
      "heading" : "3.3 Factorization via Coupling",
      "text" : "Our goal in this subsection is to give a coupling procedure to generate partial assignments A1 and A2 starting from x = T and x = F respectively, that result in a factorized formula. In fact, we will set exactly the same set S of variables in both, although not all variables will be set to the same value in the two partial assignments and this set S will also be random.\nThere are two important constraints that we will impose on how we construct the partial assignments, that will make it somewhat more tricky. First, suppose we have only set the variable x and next we choose to set the variable y in both A1 and A2. We will want that the distribution on how we set y in the coupling procedure in A1 to match the conditional distribution D1 and similarly for A2. Now suppose we terminate with some set S having been set. We can continue sampling the variables in S̄ from D1, and we are now guaranteed that the full assignment we generate is uniform from the set of assignments with x = T . An identical statement holds when starting with x = F . Second, we will want that with very high probability, the coupling procedure terminates with not too many variables in the formula ΦI1 or ΦI2 . Finally, we will assume that we are given access to a powerful oracle:\nDefinition 3.3. We will call the following a conditional distribution oracle: Given a CNF formula Φ, a partial assignment A and a variable y it can answer with the probability that y = T in a uniformly random satisfying assignment that is also consistent with A\nSuch an oracle is obviously very powerful, and it is well known that if we had access to it we could compute the number of satisfying assignments to Φ exactly with a polynomial number of queries. However one should think of the coupling procedure as a though experiment, which will be useful in an indirect way to build up towards our algorithm for approximate counting.\nNotice that a clause c can only trigger the WHILE loop at most once. If it ends up in Case # 1 then it is deleted from the formula. If it ends up in Case # 2 then all its variables are included in VI and once a variable is included in VI it is never removed. Thus the procedure clearly terminates. Our first step is to show that when it does, the formula factorizes. Let CI be the set of remaining clauses which have all of their variables in VI . Similarly let CO be the set of remaining clauses which have all of their variables in VO. Then set Φ′I = ∧c∈CIc and let ΦI1 and ΦI2 be the simplification of Φ ′ I with respect to the partial assignments A1 and A2. Similarly set Φ′O = ∧c∈COc and let ΦO1 and ΦO2 be the simplification of Φ ′ O with respect to the partial assignments A1 and A2.\nClaim 3.4. All variables with different truth assignments in A1 and A2 are in VI .\nProof. A variable is set in response to it being contained in some clause c that triggers the WHILE loop. Any such variable is moved into VI in both Case # 1 and Case # 2.\nNow we have an immediate corollary that helps us towards proving that we have found partial assignments for which Φ factorizes:\nAlgorithm 1 Coupling Procedure, Input: Monotone CNF Φ, variable x and conditional distribution oracle F\n1. Using Lemma 3.1, label variables as marked or unmarked\n2. Initialize A1(x) = T and A2(x) = F 3. Initialize VI = {x} and VO = {xi}ni=1 \\ {x} 4. While there is a clause c with variables in both VI and VO\n5. Sequentially sample its marked variables (if any) from D1 and D2, using F to construct best coupling\n6. Case # 1: c is satisfied by variables already set in both A1 and A2 7. Let S be the variables in c that have different truth values in A1 and A2. 8. Update VI ← VI ∪ S, VO ← VO \\ S 9. Delete c\n10. Case # 2: c is not satisfied by variables already set in either A1 or A2 11. Let S be all variables in c (marked or unmarked)\n12. Update VI ← VI ∪ S, VO ← VO \\ S 13. End\nCorollary 3.5. ΦO1 = ΦO2\nProof. Recall that ΦO1 and ΦO2 come from simplifying Φ ′ O (which contains only variables in VO) according to A1 and A2. From Claim 3.4, we know that A1 and A2 are the same restricted to VO and thus we get the same formula in both cases.\nNow that we know they are equal, we can define ΦO = ΦO1 = ΦO2 . What remains is to show that the subformulas we have are actually factorizations of the original formula Φ:\nLemma 3.6. ΦA1 = ΦI1 ∧ ΦO and ΦA2 = ΦI2 ∧ ΦO\nProof. When the WHILE loop terminates, every clause c in the original formula Φ either has all of its variables in VI or in VO, or was deleted because it already contains at least one variable in both A1 and A2 that satisfies it (although it need not be the same variable). Hence every clause in Φ that is not already satisfied in both A1 and A2 shows up in Φ′I ∧Φ′O. Some clauses that are already satisfied in both may show up as well. In any case, this completes the proof because the remaining operation just simplifies the formulas according to the partial assignments."
    }, {
      "heading" : "3.4 How Quickly Does the Coupling Procedure Terminate?",
      "text" : "What remains is to bound the probability that the number of variables included in VI is at most t. First we need an elementary definition:\nDefinition 3.7. When a variable xi is given different truth assignments in A1 and A2, we call it a type 1 error. When a clause c has all of its marked variables set in both A1 and A2, but in at least one of them is not yet satisfied, we call it a type 2 error.\nNote that it is possible for a variable to participate in both a type 1 and type 2 error. In any case, these are the only reasons that a variable is included in VI in an execution of the coupling procedure:\nObservation 1. All variables in VI are included either due to a type 1 error or a type 2 error, or both.\nNow our approach to showing that VI contains not too many variables with high probability is to show that if it did, there would be a large collection of disjoint errors. First we construct a useful graph underlying the process:\nDefinition 3.8. Let G be the graph on vertices VI where we connect variables if and only if they appear in the same clause together (any clause from the original formula Φ).\nThe crucial property is that it is connected:\nObservation 2. G is connected\nProof. This property holds by induction. Assume that at the start of the WHILE loop, the property holds. Then at the end of the loop, any variable xi added to VI must have been contained in a clause c that at the outset had one of its variables in VI . This completes the proof.\nNow by Observation 1, for every variable in VI we can blame it on either a type 1 or a type 2 error. Both of these types of errors are unlikely. But for each variable, charging it to an error is problematic because of overlaps in the events. In particular, suppose we have two variables xi and xj that are both included in VI . It could be that both variables are in the same clause c which resulted in a type 2 error, in which case we could only charge one of the variables to it. This turns out not to be a major issue.\nThe more challenging type of overlap is when two clauses c and c′ both experience type 2 errors and overlap. In isolation, each clause would be unlikely to experience a type 2 error. But it could be that c and c′ share all but one of their marked variables, in which case once we know that c experiences a type 2 error, then c′ has a reasonable chance of experiencing one as well. We will get around this issue by building what we call a 3-tree. This approach is inspired by Noga Alon’s parallel algorithmic local lemma [2] where he uses a 2, 3-tree.\nDefinition 3.9. We call a graph T on subset of VI a 3-tree if each vertex is distance at least 3 from all the others, and when we add edges between vertices at distance exactly 3 the tree is connected.\nNext we show that G contains a large 3-tree:\nLemma 3.10. Any maximal 3-tree contains at least |VI | (d+1)dk vertices.\nProof. Consider a maximal 3-tree T . We claim that every vertex xi ∈ VI must be distance at most 2 from some xj in T . If not, then we could take the shortest path from xi to T and move along it, and at some point we would encounter a vertex that is also not in T whose distance from T is exactly 3, at which point we could add it, contradicting T ’s maximality. Now for every xi in T , we remove from consideration at most (d + 1)dk other variables (all those at distance at most 2 from xi in G). This completes the proof.\nNow we can indeed charge every variable in T to a disjoint error:\nClaim 3.11. If two variables xi and xj in T are the result of type 2 errors for c and c ′, then\nvars(ci) ∩ vars(cj) = ∅\nProof. For the sake of contradiction, suppose that vars(ci)∩vars(cj) 6= ∅. Then since c and c′ experience type 2 errors, all of their variables are included in VI . This gives a length 2 path from xi to xj in G, which if they were both included in T , would contradict the assumption that T is a 3-tree.\nWe are now ready to prove the main theorem of this section:\nTheorem 3.12. Suppose that d ≥ k ≥ 20 log d. Then\nPr[|VI | ≥ (d+ 1)dkt] ≤ (3 d )t Proof. Suppose that |VI | ≥ (d+ 1)dkt. Then by Lemma 3.10 we can find a 3-tree T with at least t vertices. The probability of any particular 3-tree on t vertices can be bounded by:( 2\nd5 + d (3 7 )k/3)t This is because by Observation 1 each vertex is caused by either a type 1 or type 2 error (or both). Moreover by Claim 3.11 the clauses that cause the type 2 errors for each vertex in T are disjoint. Now the first term in the expression above is the probability of a type 1 error, which follows from Corollary 2.3. The second term follows from the fact that each variable is contained in at most d clauses each of which could cause a type 2 error, from Lemma 3.1 which implies that each clause has at least k/3 marked variables, and again from Corollary 2.3 which implies that for any variable xi, the minimum of the probability it is set to T or to T in either A1 or A2 is conservatively at least 3/7 (because we chose the best coupling).\nNow it is well-known (see [17, 2]) that the number of trees of size t in a graph of degree at most D is at most (eD)t. Moreover if we connect pairs of vertices in G that are distance exactly 3 from each other, then we get a new graph H whose maximum degree is at most D = d3k. Thus putting it all together we have that the probability that |VI | > (d + 1)dkt can be bounded by (2k\nd2 + d4k (3 7 )k/3)t ≤ (3 d )t where the last inequality follows from the constraint d ≥ k ≥ 20 log d.\nThus we can conclude that with high probability, the number of variables in VI is at most logarithmic. We can now brute-force search over all assignments to count the number of satisfying assignments to either ΦI1 or ΦI2 . The trouble is that we do not have access to the marginal probabilities, so we cannot actually execute the coupling procedure. We will need to circumvent this issue next."
    }, {
      "heading" : "4 Implications of the Coupling Procedure",
      "text" : "In this section, we give an abstraction that allows us to think about the coupling procedure as a randomly chosen root-to-leaf path in a certain tree whose nodes represent states. First, we make an elementary observation that will be useful in discussing how this tree is constructed. Recall that the coupling procedure chooses any clause that contains variables in both VI and VO and then samples all marked variables in it. We will assume without loss of generality that the choices it makes are done in lexicographic order. So if the clauses in Φ are ordered arbitrarily as c1, c2, ...., cm and the variables are ordered as x1, x2, ..., xn when executing the WHILE loop, if it has a choice of more than one clause it chooses among them the clause ci with the lowest subscript i. Similarly, given a choice of which marked variable to sample next, it chooses among them the xj with the lowest subscript j.\nThe important point is that now we can think of a state associated with the coupling procedure, which we will denote by σ.\nDefinition 4.1. The state σ of the coupling procedure specifies the following:\n1. The set of remaining clauses C ′ – i.e. that have not yet been deleted\n2. The partition of the variables into VI and VO\n3. The set S of variables whose values have been set, along with their values in both A1 and A2\n4. The current clause c∗ being operated on in the while loop, if any\nWe will assume that the set M of marked variables is fixed once and for all. Now the transition rules are that if c∗ has any marked variables that are unset, it chooses the lexicographically first and sets it. And when c∗ has no remaining marked variables to set, it updates C ′, VI and VO according to whether it falls into Case # 1 or Case #2 and sets the current clause to empty. Finally, if the current clause is empty then it chooses the lexicographically first clause from C ′ which has at least one variable in each of VI and VO to be c∗.\nFinally, we can define the next variable operation:\nDefinition 4.2. Let R : Σ → {xi}ni=1 ∪ {∅} × Σ be the function that takes in a state σ, transitions to the next state σ′ that sets some variable y and outputs (y, σ′).\nNote that some states σ do not immediately set a variable – e.g. if the next operation is to choose the next clause, or update C ′, VI and VO. These latter transitions are deterministic, so we let σ′ be the end resulting state and y be the variable that it sets. Now we can define the stochastic decision tree underlying the coupling procedure:\nAlgorithm 2 Decision Tree Sampling, Input: Monotone CNF Φ, stochastic decision tree S\n1. Choose a random root-to-leaf path in S\n2. Choose a uniformly random assignment A1 consistent with A1 3. Choose a uniformly random assignment A2 consistent with A2 4. Output A1 with probability q = PrD[x = T ], and otherwise output A2\nDefinition 4.3. Given a conditional distribution oracle F , the function R and a stopping threshold s, the associated stochastic decision tree is the following:\n(1) The root node corresponds to the state where only x is set, A1(x) = T , A2(x) = F , VI = {x} and VO = {xi}ni=1 \\ {x}.\n(2) Each node has either zero or four descendants. If the current node corresponds to state σ, let (y, σ′) = R(σ). Then if y = ∅ or if |VI | = s there are no descendants and the current node is a leaf corresponding to the termination of the coupling procedure or |VI | being to large. Otherwise the four descendants correspond to the four choices for how to set y in A1 and A2, and are marked with the state σ′′ which incorporates their respective choices into σ′.\n(3) Moreover the probability on an edge from a state σ′ to a state σ′′ where y has been set as A1(y) = T and A2(y) = T is equal to\nmin(D1(y),D2(y))\nand the transition to the state where A1(y) = F and A2(y) = F has probability\nmin(1−D1(y), 1−D2(y))\nFinally if D1(y) > D2(y) then the transition to A1(y) = T and A2(y) = F is non-zero and is assigned all the remaining probability. Otherwise the transition to A1(y) = F and A2(y) = T is non-zero and is assigned all the remaining probability.\nNow we can use the stochastic decision tree to give an alternative procedure to sample a uniformly random satisfying assignment of Φ. We will refer to the process of starting from the root, and choosing a descendant with the corresponding transition probability, until a leaf node is reached as “choosing a random root-to-leaf path”.\nClaim 4.4. The decision tree sampling procedure outputs a uniformly random satisfying assignment of Φ.\nProof. We could alternatively think of the decision tree sampling procedure as deciding on A1 or A2 with probability q vs. 1− q at the outset. Then if we choose A1, and we only keep track of the choices made for A1, marginally these correspond to sequentially sampling the assignment of variables from D1. And when we reach a leaf node in S we can interpret the\nremaining choices to A1 as sampling all unset variables from D1. Thus the output in this case is a uniformly random satisfying assignment with x = T . An identical statement holds for when we choose A2, and because we decided between them at the outset with the correct probability, this completes the proof of the claim.\nNow let σ be the state of a leaf node u and let A1 and A2 be the resulting partial assignments. Let p1 be the product of certain probabilities along the root-to-leaf path. In particular, suppose along the path there is a transition with y being set. Let q1 be the probability of the transition to (A1(y),A2(y)) – i.e. along the branch that it actually went down. And let q2 be the probability of the transition to (A1(y),A2(y)) – i.e. where y is set the same in A1 but is set to the opposite value as it was in A2. We let p1 be the product of all q1\nq1+q2 over all such decision on the root-to-leaf path.\nLemma 4.5. Let A be an assignment that agrees with A1. Then for the Decision Tree Sampling procedure\nPr [ terminates at leaf u ∣∣∣outputs assignment A] = p1 Proof. The idea behind this proof is to think of the random choice of which of the four descendants to transition to as being broken down into two separate random choices where we first choose A1(y) and then we choose A2(y). See Figure 2. Now we can make the random choices in the Decision Tree Sampling procedure in an entirely different order. Instead of choosing the transition in the first layer, then the second layer and so on, we instead make all of the choices in the odd layers. Moreover at each leaf, we choose which assignment consistent with A1 we would output. This is the first phase. Next we choose whether to output the assignment consistent with A1 or with A2. Finally, we make all the choices in the even layers which fixes the root-to-leaf path and then we choose an assignment consistent with A2. This is the second phase.\nThe key point is that once the output A is fixed, all of the choices in the first phase are determined, because every time a variable y is set it must agree with its setting in A. Moreover each leaf node must choose A for its assignment consistent with A1. And finally, we know that the sampling procedure must output the assignment consistent with A1 because A agrees with A1 and not A2 (because they differ on how they set x). Thus conditioned on outputting A the only random choices left are those in the second phase. Now the lemma follows because the probability of reaching leaf node u is exactly the probability along the path of all of the even layer choices, which is how we defined p1.\nWe can define p2 in an analogous way to how we defined p1 (i.e. as the product of certain probabilities along the root-to-leaf path), and the lemma above shows that p2 is exactly the probability of all the decisions made along the root-to-leaf path conditioned on the output being A where A agrees with A2.\nThe key lemma is the following:\nLemma 4.6. Let N1 be the number of satisfying assignments consistent with A1 and let N2 be the number of satisfying assignments consistent with A2. Then\np1N1 p2N2 = q 1− q\nProof. Let u be a leaf node. Consider a random variable Zu that when we run the decision tree sampling procedure is non-zero if and only if we end at u. Moreover let Zu = (1− q) if an assignment with x = T is output, and Zu = −q if an assignment with x = F is output. Then clearly E[Zu] = 0. Now alternatively we can write:\nE[Zu] = E A\n[E[Zu|A is output]]\nwhere A is a uniformly random satisfying assignment of Φ, precisely because of Lemma 4.4. Let N be the total number of such assignments. Then\nE A [E[Zu|A]] = (N1 N ) (p1)(1− q) + (N2 N ) (p2)(−q)\nThis follows because the only assignments A that can be output at u must be consistent with either A1 or A2. Note that these are disjoint events because in one of them x = T while in the other x = F . Then once we know that A is consistent with A1 (which happens with probability N1\nN ) the probability for the decisions made in A2 being such that we reach\nu is exactly p1, as this was how it was defined. The final term in the product of three terms is just the value of Zu. An identical argument justifies the second term. Now using the fact that the above expression evaluates to zero and rearranging completes the proof."
    }, {
      "heading" : "5 Certifying the Marginal Distribution",
      "text" : ""
    }, {
      "heading" : "5.1 One-Sided Stochastic Decision Trees",
      "text" : "The stochastic decision tree that we defined in the previous section is a natural representation of the trajectory of the coupling procedure. However it has an important drawback that we will remedy here. Its crucial property is captured in Lemma 4.6 which gives a relation between\n(1) pi – the conditional probability of an assignment consistent with Ai reaching u and\n(2) Ni – the number of assignments consistent with Ai for i = 1, 2. However p1 is the product of various ratios of probabilities along the root-to-leaf path. This means that if we think of the transition probabilities as variables, the constraint imposed by Lemma 4.6 is far from linear2.\nIn this section, we will transform a stochastic decision tree into two separate trees, that we call one-sided stochastic decision trees. These will have the property that the constraint imposed by Lemma 4.6 will be linear in the unknown probabilities that we think of as variables. Ultimately we will show that any such pair can (1) certify that a given value q is within an additive inverse polynomial factor of PrD[x = T ] and (2) can be constructed in polynomial time through linear programming. First we explain the transformation from a stochastic decision tree to a one-sided stochastic decision tree. We will then formally define its properties and what we require of it.\nNow suppose we are given a stochastic decision tree S. Let’s construct the one-sided stochastic decision tree S1 that represents the trajectory of the partial assignment A1. When we start from the starting state σ (see Definition 4.1), the four descendants of it in S will now be four grand-children. It’s immediate descendants will be two nodes u and u′, one representing the choice A1(y) = T and one representing A1(y) = F , where y is the next variable set (see Definition 4.2). The two children of σ in S that correspond to A1(y) = T will now be the children of u and the other two children will now be the children of u′. We will continue in this way so that alternate layers represent nodes present in S and new nodes.\nThis alone does not change much the semantics of the trajectory. All we are doing is breaking up the decision of which of the four children to proceed to, into two separate decisions. The first decision is based on just A1 and the second is based on A2. However we will change the semantics of what probabilities we associate with different transitions. For starters, we will work with total probabilities. So the total probability incoming into the starting node is 1. Let’s see how this works inductively. Let’s now suppose that σ represents the state of some node in S (not necessarily the starting state) and u and u′ are its descendants in S1. Then if the total probability into σ in S1 is z, we place z along both the edges to u and to u′. This is because the decision tree is now from the perspective of A1, who perhaps has already chosen his assignment uniformly at random from those with x = T but has not set all of those values in A1. Hence his decision is not a random variable, since given the option of transition to u or u′ he must go to whichever one is consistent with his hidden values.\nHowever from this perspective, the choices corresponding to A2 are random because he has no knowledge of the assignment that the other player is working with. If we have z total probability coming into u, then the total probability into its two descendants will be ( q1\nq1+q2 )z\nand ( q2 q1+q2 )z respectively, where q1 and q2 were the probabilities on the transitions in S into the two corresponding descendants. In particular, if q1 is the probability of setting A1(y) = T and A2(y) = T and q2 is the probability of setting A1(y) = T and A2(y) = F then ( q1q1+q2 )z is the total probability on the transition from u to the descendant where A2(y) = T and ( q2 q1+q2\n)z is the total probability on the transition from u to the descendant where A2(y) = F . 2What’s worse is that the contribution of a particular decision to p1 and p2 is a multiplication by one of two ratios of probabilities, which have different denominators. For reasons that we will not digress into, this makes it challenging to encode the total probability p1 as a flow in a tree.\nNote that from Corollary 2.3 we have that( q2 q1 + q2 ) z ≤ ( 6 d5 ) z\nThis is an important property that we will make crucial use of later. Notice that it is a linear constraint in the total probability. Now we are ready to define a one-sided stochastic decision tree, which closely mirrors Definition 4.3.\nDefinition 5.1. Given the function R and a stopping threshold s, the associated one-sided stochastic decision tree for A1 is the following:\n(1) The root node corresponds to the state where only x is set, A1(x) = T , A2(x) = F , VI = {x} and VO = {xi}ni=1 \\ {x}.\n(2) Each node has either two descendants and four grand-descendants or zero descendants. If the current node a corresponds to state σ, let (y, σ′) = R(σ). Then if y = ∅ or if |VI | = s there are no descendants and the current node is a leaf corresponding to the termination of the coupling procedure or |VI | being to large. Otherwise the two descendants correspond to the two choices for how to set y in A1. Each of their two descendants correspond to the two choices for how to set y in A2. Each granddescendant is marked with the state σ′ which incorporates their respective choices.\n(3) Let z be the total probability into a. Then the total probability into each descendant is z. Moreover let the total probability into the grand-descendants with states A1(y) = T and A2(y) = T and A1(y) = T and A2(y) = F be z1 and z2 respectively. Then z1 and z2 are nonnegative, sum to z and satisfy z2 ≤ ( 6d5 )z. Similarly, let the total probability into the grand-descendants with states A1(y) = F and A2(y) = F and A1(y) = F and A2(y) = T be z3 and z4 respectively. Then z3 and z4 are nonnegative, sum to z and satisfy z4 ≤ ( 6d5 )z.\nThe one-sided stochastic decision tree for A2 is defined analogously, in the obvious way. Finally we record an elementary fact:\nClaim 5.2. There is a perfect matching between the root-to-leaf paths in S1 and S2, so that any pair of assignments A1 and A2 that takes a root-to-leaf path p in S1, must also take the root-to-leaf path in S2 to which p is matched.\nProof. Recall that the odd levels in S1 and S2 correspond to the nodes in S. Therefore from a root-to-leaf path p in S1 we can construct the root-to-leaf path in S, which in turn uniquely defines a root-to-leaf path in S2 (because it specifies which nodes are visited in odd layers, and all paths end on a node in an odd layer)."
    }, {
      "heading" : "5.2 An Algorithm for Finding a Valid S1 and S2",
      "text" : "We are now ready to prove one of the two main theorems of this section:\nTheorem 5.3. Let q = PrD[x = T ] and q ′ ≤ q ≤ q′′. Then there are two one-sided stochastic decision trees S1 and S2 that for any pair of matched root-to-leaf paths terminating in u and u′ respectively satisfy ( q′\n1− q′ ) p2N2 ≤ p1N1 ≤ ( q′′ 1− q′′ ) p2N2\nwhere N1 and N2 are number of satisfying assignments consistent with A1 and A2 respectively, and p1 and p2 are the total probability into u and u\n′ respectively. Moreover given q′ and q′′ that satisfy q′ ≤ q ≤ q′′ there is an algorithm to construct two one-sided stochastic decision trees S1 and S2 that satisfy the above condition on all matched leaf nodes corresponding to a termination of the coupling procedure, which runs in time polynomial in m and 4s where s is the stopping size.\nProof. The first part of the theorem follows from the transformation we gave from a stochastic decision tree to two one-sided stochastic decision trees. Then Claim 5.2 combined with Lemma 4.6 implies q\n1−q = p1N1 p2N2\n, which then necessarily satisfies q ′\n1−q′ ≤ p1N1 p2N2 ≤ q′′ 1−q′′ . Rear-\nranging completes the proof of the first part. To prove the second part of the theorem, notice that if s is the stopping size, then the number of leaf nodes in S1 and in S2 is bounded by 4 s. At each leaf node that corresponds to a termination of the coupling procedure, from Lemma 3.6 we can compute the ratio of N1 to N2 as the ratio of the number of satisfying assignments to ΦI1 to the number of satisfying assignments to ΦI2 . This can be done in polynomial in m and 2\ns time by brute-force. Finally, the constraints in Definition 5.1 are all linear in the variables that represent total probability (if we treat 6\nd5 , q\n′ 1−q′ , q′′ 1−q′′ and all ratios N1 N2\nas given constants). Thus we can find a valid choice of the total probability variables by linear programming. This completes the proof of the second part.\nRecall that we will be able to choose s = O(d2k log n) and Theorem 3.12 will imply that at most an inverse polynomial fraction of the distribution fails to couple. Thus the algorithm above runs in polynomial time for any constants d and k. What remains is to show that any valid choice of total probabilities certifies that q′ ≤ PrD[x = T ] ≤ q′′."
    }, {
      "heading" : "5.3 A Fractional Matching to Certify q",
      "text" : "We are now ready to prove the second main theorem of this section. We will show that having any two one-sided stochastic decision trees that meet the constraints on the leaves imposed by Theorem 5.3 is enough to certify that PrD[x = T ] is approximately between q ′ and q′′. This result will rest on two facts. Fix any assignment A. Then either\n(1) The assignment has too many clauses that restricted to marked variables are all F or\n(2) The total probability of A reaching a leaf node u where the coupling procedure failed to terminate before reaching size s is at most O( 1\nnc ).\nTheorem 5.4. Suppose that d ≥ k ≥ 20 log d. Then any two one-sided stochastic decision trees S1 and S2 that meet the constraints on the leaves imposed by Theorem 5.3 and satisfy s = 10cd2k log n imply that\nq′ −O ( 1 nc ) ≤ PrD[x = T ] ≤ q′′ +O ( 1 nc )\nThe proof of this theorem will use many of the same tools that appeared in the proof of Theorem 3.12, since in essence we are performing a one-sided charging argument.\nProof. The proof will proceed by constructing a complete bipartite graph H = (U, V,E) and finding a fractional approximate matching as follows. The nodes in U represent the satisfying assignments of Φ with x = T . The nodes in V represent the satisfying assignments of Φ with x = F . Moreover all but a O( 1\nnc ) fraction of the nodes on the left will send between\n1 − q′′ − O( 1 nc ) and 1 − q′ + O( 1 nc ) flow along their outgoing edges. Finally all but a O( 1 nc\n) fraction of the nodes on the right will receive between q′ −O( 1\nnc ) and q′′ +O( 1 nc ) flow along\ntheir incoming edges. First notice that any assignment A (say with x = T ) is mapped by S1 to a distribution over leaf nodes, some of which correspond to a coupling and some of which correspond to a failure to couple before reaching size s. Now consider matched pairs of leaf nodes (according to Claim 5.2) that correspond to a coupling. Let p1 and p2 be the total probability of the leaf nodes in S1 and S2 respectively. Let N1 and N2 be the total number of assignments that are consistent with A1 and A2, and let N1 and N2 be the corresponding sets of assignments. From the assumption that( q′\n1− q′ ) p2N2 ≤ p1N1 ≤ ( q′′ 1− q′′ ) p2N2\nand the intermediate value theorem it follows that there is a q′ ≤ q∗ ≤ q′′ which satisfies( q′∗ 1− q∗ ) p2N2 = p1N1\nHence there is a flow that sends exactly (1− q∗)p1 units of flow out of each node in N1 and which each node in N2 receives exactly q∗p2 units of flow.\nIf every leaf node corresponding to a coupling, we would indeed have the fractional matching we are looking for, just by summing these flows over all leaf nodes. What remains is to handle the leaf nodes that do not correspond to the coupling terminating before size s. Consider any such leaf node u in S1 and the corresponding leaf node v in S2. From Lemma 3.10 we have that there is a 3-tree T of size at least 10c log n. For each node in T , from Claim 3.11 we have there are at least 10c log n disjoint type 1 or type 2 errors.\nCase # 1: Suppose that there are at least 5c log n disjoint type 1 errors. Fix the 3-tree T , and look at all root-to-leaf paths that are consistent with just the type 1 errors. Then the sum of their total probabilities is at most( 6\nd12 )5c logn This follows because the constraint that z2 ≤ ( 6d12 )z (and similarly for z4) in Definition 5.1 implies that for each path we can factor out the above term corresponding to just the decisions where there are type 1 errors. The remaining probabilities are conditional distributions on the paths (after having taken into account the type 1 errors) and sum to at most one. Finally the total number of 3-trees of size 10c log n is at most (ed3k)10c logn. Thus for any assignment\nA, if we ignore what happens to it when it ends up at a leaf node which did not couple and which has at least 5c log n disjoint type 1 errors, in total we have ignored at most(6e\nd\n)5c logn ≤ 1/nc\nof its probability. Case # 2: Suppose that there are at least 5c log n disjoint type 2 errors. Each type 2 error can be blamed on either A1 or A2 or both (e.g. it could be that the clause c might only have all of its marked variables set to F in A1). Let’s suppose that the assignment A contributes at least 5/2c log n disjoint type 2 errors. In this case we will completely ignore A in the constraints imposed by our flow. How many such assignments can there be? The probability of getting any such assignment is bounded by(\ned3k )10c logn((3\n7\n)k/3)5/2c logn ≤ 1/nc\nThus if we ignore the flow constraints for all such assignments, we will be ignoring at most a 1/nc fraction of the nodes in U and the nodes in V . The only remaining case is when the assignment A ends up at a leaf node u that has at least 5c log n disjoint type 2 errors, but it contributes less than 5/2c log n itself. For each type 2 error that it does not contribute to, it contributes to another type 1 error. The only minor complication is that the node responsible might not be in the 3-tree T . However it is distance at most 1 from the 3-tree because it is contained in a clause that results in type 2 error that does contain a node in T . Now by an analogous reasoning as in Case #1 above, if we fix the pattern of these type 1 errors – i.e. we fix the 3-tree and the extra nodes at distance 1 from it that contribute the missing type 1 errors – the sum of the total probability of all consistent root-to-leaf paths is at most ( 6\nd12 )5c logn Now the number of patterns can be bounded by (ed4k)10c logn, which accounts for the inclusion of extra nodes that are not in T . Once again, for such an assignment A if we ignore what happens to it when it ends up at a leaf node which did not couple and which has at least 5c log n disjoint type 2 but it contributes less than 5/2c log n itself, in total we have ignored at most (6e\nd\n)5c logn ≤ 1/nc\nof its probability. Now returning to the beginning of the proof and letting N1 and N2 be the total number of satisfying assignments with x = T and x = F respectively. We have that the flow in the bipartite graph implies\n(1− q′′)N1 −O ( 1 nc ) ≤ flowoutU = flowinV ≤ q′′N2 +O ( 1 nc ) and the further condition\nq′N2 −O ( 1 nc ) ≤ flowinV = flowoutU ≤ (1− q′)N1 +O ( 1 nc ) which gives q ′\n1−q′−O( 1 nc ) ≤ q 1−q ≤\nq′′\n1−q′′ +O( 1 nc ) which completes the proof of the theorem."
    }, {
      "heading" : "6 Applications",
      "text" : "Here we show how to use our algorithm for computing marginal probabilities when k is logarithmic in d for approximate counting and sampling from the uniform distribution on satisfying assignments."
    }, {
      "heading" : "6.1 Approximate Counting",
      "text" : "First, we show how to use an algorithm for computing marginal probabilities to do approximate counting in a monotone CNF, where no variable is negated. This approach is standard, and appears in [4].\nCorollary 6.1. Suppose we are given a monotone CNF formula Φ on n variables with at least k variables per clause and at most d clauses containing any one variable with d ≥ k ≥ 20 log d. Let OPT be the number of satisfying assignments. Then there is an algorithm that outputs a quantity count that satisfies(\n1− 1 nc\n) OPT ≤ count ≤ (1 + 1\nnc\n) OPT\nand runs in time polynomial in m and ncd 2k.\nProof. First, we fix an ordering of the variables x1, x2, ...xn and a sequence of formulas Φ1,Φ2, ...Φn. Let Φ1 = Φ and let Φi be the subformula we get when substituting x1 = T, x2 = T, ...xi−1 = T into Φ and simplifying. Notice that each such formula is a monotone CNF and inherits the properties we need from Φ. In particular, each clause has at least k variables because the only clauses left in Φi (i.e. not already satisfied) are the ones which have all of their variables unset. Also, each variable belongs to at most d clauses because we have only removed variables and clauses.\nThus we can appeal to Theorem 5.3 and Theorem 5.4 to compute for each variable xi the quantity pi , PrDi [xi = T ] to within an additive 1/2n\nc+1 where Di is the uniform distribution on satisfying assignments to Φi. Let our estimate be qi. Since pi ≥ 1/2 we have that qi and pi are also multiplicatively close, with (1±1/nc+1). Now if we take the product of the pi’s we get a telescoping product which computes the ratio of the number of satisfying assignments with all variables set to T divided by the number of satisfying assignments to Φ. Moreover each pi ≥ 1/2. Thus we conclude\ncount , n∏\ni=1\n( 1 qi ) = ( 1± 1 nc ) OPT\nwhich completes the proof.\nThe above approach heavily used monotonicity to ensure that no clause becomes too small (i.e. contains few variables, but is still unsatisfied). This is a similarly issue to what happened with the coupling procedure, which necessitating using marked and unmarked variables, the latter being variables that are never set and are used to make sure no clause becomes too small. We can take a similar approach here. In what follows we will no longer assume Φ is monotone.\nLemma 6.2. Set c0 = max(e ( 1 2 )( 1 6 )2 , 3/4). Suppose that e(d + 1)c−k0 ≤ 1. Then there is a partial assignment A so that every clause is satisfied and each clause has at least k/3 unset variables. Moreover there is a randomized algorithm to find such a partial assignment that runs in time polynomial in m, n, k and d. Alternatively there is a deterministic algorithm that runs in time polynomial in m and nO(d 2).\nProof. We will choose independent for each variable to set it to T with probability 1/4, to set it to F with probability 1/4 and to leave it unset with probability 1/2. Now consider the m bad events, one for each clause c, that c is either unsatisfied or has not enough unset variables (or both). Then we have\nPr[c is bad] ≤ e−( 1 2 )( 1 6 )2k + (3\n4\n)k ≤ 2c−k0\nwhere the first term follows from the Chernoff bound and represents the probability that there are not enough unset variables, and the second term is the probability that the clause is unsatisfied. Once again we can appeal to the Lovász Local Lemma to show the existence. Finally we can use the algorithm of Moser and Tardos [20] to find such a partial assignment in randomized polynomial time. Moreover Moser and Tardos [20] also give a deterministic algorithm that runs in time polynomial in m and nO(d 2).\nTheorem 6.3. Suppose we are given a CNF formula Φ on n variables with at least k variables per clause and at most d clauses containing any one variable with d ≥ k ≥ 20 log d. Let OPT be the number of satisfying assignments. Then there is a deterministic algorithm that outputs a quantity count that satisfies(\n1− 1 nc\n) OPT ≤ count ≤ (1 + 1\nnc\n) OPT\nand runs in time polynomial in m and ncd 2k.\nProof. Our proof follows the same basic outline as in Corollary 6.1. First we (deterministically) find a partial assignment that meets Lemma 6.2 and let x1, x2, ....xt be an ordering of the set variables. We define Φ1,Φ2, ...Φt in the same way as the subformula we get by substituting in the assignments for x1, x2, ...xi−1 and simplifying to get Φi. Again let qi be our estimate for the marginal probabilities.\nThe key point is that Φt+1 would be empty, because all clauses are satisfied. Moreover each clause that appears in any formula Φi for 1 ≤ i ≤ t has at least k/3 variables because it has at least that many unset variables in the partial assignment. Moreover we can now output\ncount , 2n−t n∏\ni=1\n( 1 qi ) = ( 1± 1 nc ) OPT\nbecause Φt+1 has exactly 2 n−t satisfying assignments (every choice of the unset variables) and we have used the same telescoping product, but now to compute the ratio of the number of satisfying assignments to Φt+1 divided by the number of satisfying assignments to Φ.\nAlgorithm 3 Sampling Procedure, Input: CNF Φ, oracle F for approximating marginals of variables\n1. Using Lemma 3.1, label variables as marked or unmarked\n2. While there is a marked variable x that is unset\n3. Sample x using F\n4. Initialize VI = {x} and VO to be all unset variables (x is already set) 5. While there is a clause c with variables in both VI and VO\n6. Sequentially sample its marked variables (if any) using F\n7. Case # 1: c is satisfied\n8. Delete c\n9. Case # 2: c is unsatisfied\n10. Let S be all variables in c (marked or unmarked)\n11. Update VI ← VI ∪ S, VO ← VO \\ S 12. End\n13. End\n14. For each connected component of the remaining clauses\n15. Enumerate and uniformly choose a satisfying assignment of the unset variables\n16. End"
    }, {
      "heading" : "6.2 Approximate Sampling",
      "text" : "Here we give an algorithm to generate an assignment approximately uniformly from the set of all satisfying assignments. Again, the complication is that our oracle for approximating the marginals works only if k is at least logarithmic in d so we need some care in the order we choose to sample variables. First we give the algorithm:\nFirst, we prove that the output is close to uniform.\nLemma 6.4. If the oracle F outputs a marginal probability that is 1/nc+1 close to the true marginal distribution for each variable queried, then the output of the Sampling Procedure is a random assignment whose distribution is 1/nc-close in total variation distance to the uniform distribution on all satisfying assignments.\nProof. The proof of this lemma is in two parts. First, imagine we were instead given access to an oracle G that answered each query for a marginal distribution with the exact value. Then each variable set using the oracle is chosen from the correct marginal distribution. And in the last step, the set of satisfying assignments is a cross-product of the satisfying assignments for each component. Thus the procedure would output a uniformly random assignment from the set of all satisfying assignments. Second, since at most n variables are queried, we have that with probability at least 1 − 1/nc all of the random decision of the\nprocedure would be the same if we had given it answers from G instead of from F . This now completes the proof.\nThe key step in the analysis of this algorithm rests on showing that with high probability each connected component is of logarithmic size.\nTheorem 6.5. Suppose we are given a CNF formula Φ on n variables with at least k variables per clause and at most d clauses containing any one variable with d ≤ k ≤ 20 log d. There is an algorithm that outputs a random assignment whose distribution is 1/nc-close in total variation distance to the uniform distribution on all satisfying assignments. Moreover the algorithm runs in time polynomial in m and ncd 2k.\nProof. The proof of this theorem uses many ideas from the coupling procedure as analyzed in Section 3. Let Φ′ be the formula at the start of some iteration of the inner WHILE loop. Then at the end of the inner WHILE loop, using Lemma 3.6 we can write:\nΦ′ = Φ′I ∧ Φ′O\nwhere Φ′I is a formula on the variables in VI and Φ ′ O is a formula on the variables in VO. In particular, no clause has variables in both because the inner WHILE loop terminated. Now we can appeal to the analysis in Theorem 3.12 which gives a with high probability bound on the size of VI . The analysis presented in its proof is nominally for a different procedure, the Coupling Procedure, but the inner WHILE loop of the Sampling Procedure is identical except for the fact that there are no type 1 errors because we are building up just one assignment. Thus\nPr[|VI | ≥ (d+ 1)dkt] ≤ (3 d )t The inner WHILE loop is run at most n times and so if we choose t ≥ c log n we get that with probability at least 1− 1/nc no component has size larger than (d + 1)dkc log n. Now the brute force search in the last step can be implemented in time polynomial in m and ncd 2k, which combined with Lemma 6.4 completes the proof.\nWe can also now prove Corollary 1.6.\nProof. Recall, we are given a cause network and the truth assignment of each observed variable. First we do some preprocessing. If an observed variable is an OR of several hidden variables or their negation, and the observed variable is set to F we know the assignment of each hidden variable on which it depends. Similarly, if an observed variable is an AND and it is set to T again we know the assignment of each of its variables. For all the remaining observed variables, we know there is exactly one configuration of its variables that is prohibited so each yields a clause in a CNF formula Φ. Moreover each clause depends on at least 2k/3 variables whose truth value has not been set because the collection of observations is regular. Finally each variable is contained in at most d clauses. The posterior distribution on the remaining hidden variables (whose value has not already been set) is uniform on the set of satisfying assignments to Φ and thus we can appeal to Theorem 6.5 to complete the proof."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We are indebted to Elchanan Mossel and David Rolnick for many helpful discussions at an earlier stage of this work."
    } ],
    "references" : [ {
      "title" : "Random walks that find perfect objects and the Lovász local lemma",
      "author" : [ "D. Achlioptas", "F. Iliopoulos" ],
      "venue" : "In FOCS, page 494–503,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2014
    }, {
      "title" : "A parallel algorithmic version of the local lemma",
      "author" : [ "N. Alon" ],
      "venue" : "In Random Structures and Algorithms,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1991
    }, {
      "title" : "An algorithmic approach to the Lovász local lemma",
      "author" : [ "J. Beck" ],
      "venue" : "Random Structure and Algorithms,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1991
    }, {
      "title" : "S̆tefankovic̆. Approximation via correlation decay when strong spatial mixing fails",
      "author" : [ "I. Bezákova", "A. Galanis", "L.A. Goldberg", "D.H. Guo" ],
      "venue" : "In ArXiv:1510.09193,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2016
    }, {
      "title" : "Stopping times, metrics and approximate counting",
      "author" : [ "M. Bordewich", "M. Dyer", "M. Karpinski" ],
      "venue" : "In ICALP,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2006
    }, {
      "title" : "Reconstruction of Markov random fields from samples: Some observations and algorithms",
      "author" : [ "G. Bresler", "E. Mossel", "A. Sly" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "Approximating probabilistic inference in Bayesian belief networks is NP -hard",
      "author" : [ "P. Dagum", "M. Luby" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1993
    }, {
      "title" : "An optimal approximation algorithm for Bayesian inference",
      "author" : [ "P. Dagum", "M. Luby" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1997
    }, {
      "title" : "Problems and results on 3-chromatic hypergraphs and some related questions",
      "author" : [ "P. Erdös", "L. Lovász" ],
      "venue" : "In Infinite and Finite Sets,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1975
    }, {
      "title" : "Inapproximability of the partition function for the antiferromagnetic Ising and hard-core models",
      "author" : [ "A. Galanis", "D. S̆tefankovic", "E. Vigoda" ],
      "venue" : "Combinatorics, Probability and Computing,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2016
    }, {
      "title" : "Correlation decay and deterministic FPTAS for counting colorings of a graph",
      "author" : [ "D. Gamarnik", "D. Katz" ],
      "venue" : "Journal of Discrete Algorithms,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2012
    }, {
      "title" : "New constructive aspects of the Lovász Local Lemma",
      "author" : [ "B. Haeupler", "B. Saha", "A. Srinivasan" ],
      "venue" : "In Journal of the ACM,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2011
    }, {
      "title" : "A constructive algorithm for the Lovász local lemma on permutations",
      "author" : [ "D. Harris", "A. Srinivasan" ],
      "venue" : "In SODA,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Computing the independence polynomial in Shearer’s region for the LLL",
      "author" : [ "N. Harvey", "P. Srivastava", "J. Vondrák" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2016
    }, {
      "title" : "An algorithmic proof of the Lovász local lemma via resampling oracles",
      "author" : [ "N. Harvey", "J. Vondrák" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2015
    }, {
      "title" : "Random generation of combinatorial structures from a uniform distribution",
      "author" : [ "M. Jerrum", "L. Valiant", "V. Vazirani" ],
      "venue" : "Theoretical Computer Science,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1986
    }, {
      "title" : "The Art of Computer Programming, Vol I. Addison Wesley, London, page 396 (exercise",
      "author" : [ "D. Knuth" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1969
    }, {
      "title" : "Commutativity in the algorithmic Lovász local lemma",
      "author" : [ "V. Kolmogorov" ],
      "venue" : "In FOCS 2016,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2016
    }, {
      "title" : "FPTAS for counting monotone CNF",
      "author" : [ "J. Liu", "P. Lu" ],
      "venue" : "In SODA, pages 1531–1548,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "A constructive proof of the Lovász Local Lemma",
      "author" : [ "R. Moser", "G. Tardos" ],
      "venue" : "In Journal of the ACM,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2010
    }, {
      "title" : "The correlation decay (CD) tree and strong spatial mixing in multi-spin systems",
      "author" : [ "C. Nair", "P. Tetali" ],
      "venue" : null,
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2007
    }, {
      "title" : "Approximately counting, uniform generation and rapidly mixing markov chains",
      "author" : [ "A. Sinclair", "M. Jerrum" ],
      "venue" : "Information and Computation,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1989
    }, {
      "title" : "Approximation algorithms for two-state anti-ferromagnetic spin systems",
      "author" : [ "A. Sinclair", "P. Srivastava", "M. Thurley" ],
      "venue" : "Journal of Statistical Physics,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2014
    }, {
      "title" : "Computational transition at the uniqueness threshold",
      "author" : [ "A. Sly" ],
      "venue" : "In FOCS, pages 287–296,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2010
    }, {
      "title" : "Counting in two-spin models on d-regular graphs",
      "author" : [ "A. Sly", "N. Sun" ],
      "venue" : "Annals of Probability,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2014
    }, {
      "title" : "Counting independent sets up to the tree threshold",
      "author" : [ "D. Weitz" ],
      "venue" : "In STOC,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Bordewich, Dyer and Karpinski [5] gave an MCMC algorithm for approximating the number of hypergraph independent sets (equivalently, the number of satisfying assignments in a monotone CNF formula) that succeeds whenever k ≥ d+2.",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 3,
      "context" : "[4] gave a deterministic algorithm that succeeds whenever k ≥ d ≥ 200 and proved that when d ≥ 5 · 2 it is NP -hard to approximate the number of hypergraph independent sets even within an exponential factor.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 25,
      "context" : "In a seminal work, Weitz [26] gave an algorithm to approximately count in the hardcore model with parameter λ in graphs of degree at most d whenever",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 23,
      "context" : "And in another seminal work, Sly [24] showed a matching hardness result which was later improved in various respects by Sly and Sun [25] and Galanis, S̆tefankovic̆ and Vigoda [10].",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 24,
      "context" : "And in another seminal work, Sly [24] showed a matching hardness result which was later improved in various respects by Sly and Sun [25] and Galanis, S̆tefankovic̆ and Vigoda [10].",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 9,
      "context" : "And in another seminal work, Sly [24] showed a matching hardness result which was later improved in various respects by Sly and Sun [25] and Galanis, S̆tefankovic̆ and Vigoda [10].",
      "startOffset" : 175,
      "endOffset" : 179
    }, {
      "referenceID" : 25,
      "context" : "There have been a number of attempts to generalize these results to hypergraphs, many of which follow the approach of defining analogues of the self-avoiding walk trees used in Weitz’s algorithm [26].",
      "startOffset" : 195,
      "endOffset" : 199
    }, {
      "referenceID" : 8,
      "context" : "Bounded degree CNF formulae are also one of the principal objects of study in the Lovász Local Lemma [9] which is a celebrated result in combinatorics that guarantees when k ≥ log d + O(1) that Φ has at least one satisfying assignment.",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 2,
      "context" : "Beck [3] gave an algorithm followed by a parallel",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 1,
      "context" : "version due to Alon [2] that can find a satisfying assignment whenever k ≥ 8 log d + O(1).",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 19,
      "context" : "And in a celebrated recent result, Moser and Tardos [20] gave an algorithm matching exactly the existential result.",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 12,
      "context" : "This was followed by a number of works giving constructive proofs of various other settings and generalizations of the Lovász Local Lemma [13, 1, 15, 18].",
      "startOffset" : 138,
      "endOffset" : 153
    }, {
      "referenceID" : 0,
      "context" : "This was followed by a number of works giving constructive proofs of various other settings and generalizations of the Lovász Local Lemma [13, 1, 15, 18].",
      "startOffset" : 138,
      "endOffset" : 153
    }, {
      "referenceID" : 14,
      "context" : "This was followed by a number of works giving constructive proofs of various other settings and generalizations of the Lovász Local Lemma [13, 1, 15, 18].",
      "startOffset" : 138,
      "endOffset" : 153
    }, {
      "referenceID" : 17,
      "context" : "This was followed by a number of works giving constructive proofs of various other settings and generalizations of the Lovász Local Lemma [13, 1, 15, 18].",
      "startOffset" : 138,
      "endOffset" : 153
    }, {
      "referenceID" : 15,
      "context" : "When the problem is self-reducible, they are in fact algorithmically equivalent [16, 22].",
      "startOffset" : 80,
      "endOffset" : 88
    }, {
      "referenceID" : 21,
      "context" : "When the problem is self-reducible, they are in fact algorithmically equivalent [16, 22].",
      "startOffset" : 80,
      "endOffset" : 88
    }, {
      "referenceID" : 3,
      "context" : "[4] imply that we need at least another factor of two, and that it is NP -hard to approximately count when k ≤ 2 log d−O(1).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 24,
      "context" : "Scott and Sokal [25] showed that given the dependency graph of events in the local lemma, the best lower bound on the probability of an event guaranteed to exist by the Lovász Local Lemma (i.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 13,
      "context" : "Harvey, Srivastava and Vondrák [14] recently adapted techniques of Weitz to complex polydisks and gave an algorithm for approximately computing this lower bound.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 4,
      "context" : "This algorithm closes an exponential gap between the known upper [5, 4] and lower [4] bounds.",
      "startOffset" : 65,
      "endOffset" : 71
    }, {
      "referenceID" : 3,
      "context" : "This algorithm closes an exponential gap between the known upper [5, 4] and lower [4] bounds.",
      "startOffset" : 65,
      "endOffset" : 71
    }, {
      "referenceID" : 3,
      "context" : "This algorithm closes an exponential gap between the known upper [5, 4] and lower [4] bounds.",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 3,
      "context" : "For counting independent sets in hypergraphs, this will necessarily be a different threshold than the threshold where the Gibbs measure is unique [4].",
      "startOffset" : 146,
      "endOffset" : 149
    }, {
      "referenceID" : 4,
      "context" : "approach is a significant departure from earlier techniques based either on path coupling [5] or adapting Weitz’s approach to non-binary models and hypergraphs [11, 21, 23, 19, 4].",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 10,
      "context" : "approach is a significant departure from earlier techniques based either on path coupling [5] or adapting Weitz’s approach to non-binary models and hypergraphs [11, 21, 23, 19, 4].",
      "startOffset" : 160,
      "endOffset" : 179
    }, {
      "referenceID" : 20,
      "context" : "approach is a significant departure from earlier techniques based either on path coupling [5] or adapting Weitz’s approach to non-binary models and hypergraphs [11, 21, 23, 19, 4].",
      "startOffset" : 160,
      "endOffset" : 179
    }, {
      "referenceID" : 22,
      "context" : "approach is a significant departure from earlier techniques based either on path coupling [5] or adapting Weitz’s approach to non-binary models and hypergraphs [11, 21, 23, 19, 4].",
      "startOffset" : 160,
      "endOffset" : 179
    }, {
      "referenceID" : 18,
      "context" : "approach is a significant departure from earlier techniques based either on path coupling [5] or adapting Weitz’s approach to non-binary models and hypergraphs [11, 21, 23, 19, 4].",
      "startOffset" : 160,
      "endOffset" : 179
    }, {
      "referenceID" : 3,
      "context" : "approach is a significant departure from earlier techniques based either on path coupling [5] or adapting Weitz’s approach to non-binary models and hypergraphs [11, 21, 23, 19, 4].",
      "startOffset" : 160,
      "endOffset" : 179
    }, {
      "referenceID" : 21,
      "context" : "In settings where the problem is self-reducible [22] it is well-known how to go from knowing the marginal to approximate counting and sampling.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 6,
      "context" : "However very little is known algorithmically about learning them or performing basic tasks such as inference [7, 8].",
      "startOffset" : 109,
      "endOffset" : 115
    }, {
      "referenceID" : 7,
      "context" : "However very little is known algorithmically about learning them or performing basic tasks such as inference [7, 8].",
      "startOffset" : 109,
      "endOffset" : 115
    }, {
      "referenceID" : 8,
      "context" : "[9] If e(d+ 1)2−k ≤ 1 then Φ has at least one satisfying assignment.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 19,
      "context" : "Moser and Tardos [20] gave an algorithm to find a satisfying assignment under these same conditions .",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 11,
      "context" : "Instead we state a version of the asymmetric local lemma given in [12] which gives us some control on the uniform distribution on assignments.",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 1,
      "context" : "This approach is inspired by Noga Alon’s parallel algorithmic local lemma [2] where he uses a 2, 3-tree.",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 16,
      "context" : "Now it is well-known (see [17, 2]) that the number of trees of size t in a graph of degree at most D is at most (eD).",
      "startOffset" : 26,
      "endOffset" : 33
    }, {
      "referenceID" : 1,
      "context" : "Now it is well-known (see [17, 2]) that the number of trees of size t in a graph of degree at most D is at most (eD).",
      "startOffset" : 26,
      "endOffset" : 33
    }, {
      "referenceID" : 3,
      "context" : "This approach is standard, and appears in [4].",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 19,
      "context" : "Finally we can use the algorithm of Moser and Tardos [20] to find such a partial assignment in randomized polynomial time.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 19,
      "context" : "Moreover Moser and Tardos [20] also give a deterministic algorithm that runs in time polynomial in m and n 2).",
      "startOffset" : 26,
      "endOffset" : 30
    } ],
    "year" : 2017,
    "abstractText" : "In this paper we introduce a new approach for approximately counting in bounded degree systems with higher-order constraints. Our main result is an algorithm to approximately count the number of solutions to a CNF formula Φ with at least k variables per clause and degree at most d when k is logarithmic in d. This closes an exponential gap between the known upper and lower bounds. Moreover our algorithm extends straightforwardly to approximate sampling, which shows that under Lovász Local Lemma-like conditions it is not only possible to find a satisfying assignment, it is also possible to generate one approximately uniformly at random from the set of all satisfying assignments. Our approach is a significant departure from earlier techniques in approximate counting, and is based on a framework to bootstrap an oracle for computing marginal probabilities on individual variables. Finally, we give an application of our results to show that it is algorithmically possible to sample from the posterior distribution in an interesting class of graphical models. ∗Massachusetts Institute of Technology. Department of Mathematics and the Computer Science and Artificial Intelligence Lab. Email: moitra@mit.edu. This work was supported in part by NSF CAREER Award CCF-1453261, NSF Large CCF-1565235, an Alfred P. Sloan Fellowship, an Edmund F. Kelley Research Award, a Google Research Award and the MIT NEC Corporation. ar X iv :1 61 0. 04 31 7v 1 [ cs .D S] 1 4 O ct 2 01 6",
    "creator" : "LaTeX with hyperref package"
  }
}