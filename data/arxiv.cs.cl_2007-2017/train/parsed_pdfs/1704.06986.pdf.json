{
  "name" : "1704.06986.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning to Create and Reuse Words in Open-Vocabulary Neural Language Modeling",
    "authors" : [ "Kazuya Kawakami", "Chris Dyer", "Phil Blunsom" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 4.\n06 98\n6v 1\n[ cs\n.C L\n] 2\n3 A\npr 2\n01 7\naccount for one of the most characteristic statistical facts of natural language: the frequent creation and reuse of new word types. Although character-level language models offer a partial solution in that they can create word types not attested in the training corpus, they do not capture the “bursty” distribution of such words. In this paper, we augment a hierarchical LSTM language model that generates sequences of word tokens character by character with a caching mechanism that learns to reuse previously generated words. To validate our model we construct a new open-vocabulary language modeling corpus (the Multilingual Wikipedia Corpus; MWC) from comparable Wikipedia articles in 7 typologically diverse languages and demonstrate the effectiveness of our model across this range of languages."
    }, {
      "heading" : "1 Introduction",
      "text" : "Language modeling is an important problem in natural language processing with many practical applications (translation, speech recognition, spelling autocorrection, etc.). Recent advances in neural networks provide strong representational power to language models with distributed representations and unbounded dependencies based on recurrent networks (RNNs). However, most language models operate by generating words by sampling from a closed vocabulary which is composed of the most frequent words in a corpus. Rare tokens are typically replaced by a special token, called the unknown word token, 〈UNK〉. Although fixed-vocabulary language models have some important practical applications and are appealing\nmodels for study, they fail to capture two empirical facts about the distribution of words in natural languages. First, vocabularies keep growing as the number of documents in a corpus grows: new words are constantly being created (Heaps, 1978). Second, rare and newly created words often occur in “bursts”, i.e., once a new or rare word has been used once in a document, it is often repeated (Church and Gale, 1995; Church, 2000).\nThe open-vocabulary problem can be solved by dispensing with word-level models in favor of models that predict sentences as sequences of characters (Sutskever et al., 2011; Chung et al., 2017). Character-based models are quite successful at learning what (new) word forms look like (e.g., they learn a language’s orthographic conventions that tell us that sustinated is a plausible English word and bzoxqir is not) and, when based on models that learn long-range dependencies such as RNNs, they can also be good models of how words fit together to form sentences.\nHowever, existing character-sequence models have no explicit mechanism for modeling the fact that once a rare word is used, it is likely to be used again. In this paper, we propose an extension to character-level language models that enables them to reuse previously generated tokens (§2). Our starting point is a hierarchical LSTM that has been previously used for modeling sentences (word by word) in a conversation (Sordoni et al., 2015), except here we model words (character by character) in a sentence. To this model, we add a caching mechanism similar to recent proposals for caching that have been advocated for closed-vocabulary models (Merity et al., 2017; Grave et al., 2017). As word tokens are generated, they are placed in an LRU cache, and, at each time step the model decides whether to copy a previously generated word from the cache or to generate it from scratch, character by character. The decision of whether\nto use the cache or not is a latent variable that is marginalised during learning and inference. In summary, our model has three properties: it creates new words, it accounts for their burstiness using a cache, and, being based on LSTM s over word representations, it can model long range dependencies.\nTo evaluate our model, we perform ablation experiments with variants of our model without the cache or hierarchical structure. In addition to standard English data sets (PTB and WikiText-2), we introduce a new multilingual data set: the Multilingual Wikipedia Corpus (MWC), which is constructed from comparable articles from Wikipedia in 7 typologically diverse languages (§3) and show the effectiveness of our model in all languages (§4). By looking at the posterior probabilities of the generation mechanism (language model vs. cache) on held-out data, we find that the cache is used to generate “bursty” word types such as proper names, while numbers and generic content words are generated preferentially from the language model (§5)."
    }, {
      "heading" : "2 Model",
      "text" : "In this section, we describe our hierarchical character language model with a word cache. As is typical for RNN language models, our model uses the chain rule to decompose the problem into incremental predictions of the next word conditioned on the history:\np(w) =\n|w|∏\nt=1\np(wt | w<t).\nWe make two modifications to the traditional\nRNN language model, which we describe in turn. First, we begin with a cache-less model we call the hierarchical character language model (HCLM; §2.1) which generates words as a sequence of characters and constructs a “word embedding” by encoding a character sequence with an LSTM (Ling et al., 2015). However, like conventional closed-vocabulary, word-based models, it is based on an LSTM that conditions on words represented by fixed-length vectors.1\n1TheHCLM is an adaptation of the hierarchical recurrent encoder-decoder of Sordoni et al. (2015) which was used to model dialog as a sequence of actions sentences which are themselves sequences of words. The original model was proposed to compose words into query sequences but we use it to compose characters into word sequences.\nThe HCLM has no mechanism to reuse words that it has previously generated, so new forms will only be repeated with very low probability. However, since the HCLM is not merely generating sentences as a sequence of characters, but also segmenting them into words, we may add a wordbased cache to which we add words keyed by the hidden state being used to generate them (§2.2). This cache mechanism is similar to the model proposed by Merity et al. (2017).\nNotation. Our model assigns probabilities to sequences of wordsw = w1, . . . , w|w|, where |w| is the length, and where each word wi is represented by a sequence of characters ci = ci,1, . . . , ci,|ci| of length |ci|."
    }, {
      "heading" : "2.1 Hierarchical Character-level Language",
      "text" : "Model (HCLM)\nThis hierarchical model satisfies our linguistic intuition that written language has (at least) two different units, characters and words.\nThe HCLM consists of four components, three LSTMs (Hochreiter and Schmidhuber, 1997): a character encoder, a word-level context encoder, and a character decoder (denoted LSTMenc, LSTMctx, and LSTMdec, respectively), and a softmax output layer over the character vocabulary. Fig. 1 illustrates an unrolled HCLM. Suppose the model reads word wt−1 and predicts the next word wt. First, the model reads the character sequence representing the word wt−1 = ct−1,1, . . . , ct−1,|ct−1| where |ct−1| is the length of the word generated at time t − 1 in characters. Each character is represented as a vector vct−1,1 , . . . ,vct−1,|ct−1| and fed into the encoder LSTMenc . The final hidden state of the encoder LSTMenc is used as the vector representation of the previously generated word wt−1,\nh enc t = LSTMenc(vct−1,1 , . . . ,vct−1,|ct|).\nThen all the vector representations of words (vw1 , . . . ,vw|w|) are processed with a context LSTMctx . Each of the hidden states of the context LSTMctx are considered representations of the history of the word sequence.\nh ctx t = LSTMctx(h enc 1 , . . . ,h enc t )\nFinally, the initial state of the decoder LSTM is set to be hctxt and the decoder LSTM reads a vector representation of the start symbol v〈S〉 and\np(Pokémon) = λtplm(Pokémon) + (1− λt)pptr(Pokémon)\ngenerates the next word wt+1 character by character. To predict the j-th character in wt, the decoder LSTM reads vector representations of the previous characters in the word, conditioned on the context vector hctxt and a start symbol.\nh dec t,j = LSTMdec(vct,1 , . . . ,vct,j−1 ,h ctx t ,v〈S〉).\nThe character generation probability is defined by a softmax layer for the corresponding hidden representation of the decoder LSTM .\np(ct,j | w<t, ct,<j) = softmax(Wdech dec t,j + bdec)\nThus, a word generation probability from\nHCLM is defined as follows.\nplm(wt | w<t) =\n|ct|∏\nj=1\np(ct,j | w<t, ct,<j)"
    }, {
      "heading" : "2.2 Continuous cache component",
      "text" : "The cache component is an external memory structure which store K elements of recent history. Similarly to the memory structure used in Grave et al. (2017), a word is added to a key-value memory after each generation of wt. The key at position i ∈ [1,K] is ki and its value mi. The memory slot is chosen as follows: if the wt exists already in the memory, its key is updated (discussed below). Otherwise, if the memory is not full, an empty slot is chosen or the least recently\nused slot is overwritten. When writing a new word to memory, the key is the RNN representation that was used to generate the word (ht) and the value is the word itself (wt). In the case when the word already exists in the cache at some position i, the ki is updated to be the arithmetic average of ht and the existing ki.\nTo define the copy probability from the cache at time t, a distribution over copy sites is defined using the attention mechanism of Bahdanau et al. (2015). To do so, we construct a query vector (rt) from the RNN’s current hidden state ht,\nrt = tanh(Wqht + bq),\nthen, for each element i of the cache, a ‘copy score,’ ui,t is computed,\nui,t = v T tanh(Wuki + rt).\nFinally, the probability of generating a word via the copying mechanism is:\npmem(i | ht) = softmaxi(ut)\npptr(wt | ht) = pmem(i | ht)[mi = wt],\nwhere [mi = wt] is 1 if the ith value in memory is wt and 0 otherwise. Since pmem defines a distribution of slots in the cache, pptr translates it into word space."
    }, {
      "heading" : "2.3 Character-level Neural Cache Language Model",
      "text" : "The word probability p(wt | w<t) is defined as a mixture of the following two probabilities. The first one is a language model probability, plm(wt | w<t) and the other is pointer probability , pptr(wt | w<t). The final probability p(wt | w<t) is\nλtplm(wt | w<t) + (1− λt)pptr(wt | w<t),\nwhere λt is computed by a multi-layer perceptron with two non-linear transformations using ht as its input, followed by a transformation by the logistic sigmoid function:\nγt = MLP(ht), λt = 1\n1− e−γt .\nWe remark that Grave et al. (2017) use a clever trick to estimate the probability, λt of drawing from the LM by augmenting their (closed) vocabulary with a special symbol indicating that a copy should be used. This enables word types that are highly predictive in context to compete with the probability of a copy event. However, since we are working with an open vocabulary, this strategy is unavailable in our model, so we use the MLP formulation."
    }, {
      "heading" : "2.4 Training objective",
      "text" : "The model parameters as well as the character projection parameters are jointly trained by maximizing the following log likelihood of the observed characters in the training corpus,\nL = − ∑\nlog p(wt | w<t)."
    }, {
      "heading" : "3 Datasets",
      "text" : "We evaluate our model on a range of datasets, employing preexisting benchmarks for comparison to previous published results, and a new multilingual corpus which specifically tests our model’s performance across a range of typological settings."
    }, {
      "heading" : "3.1 Penn Tree Bank (PTB)",
      "text" : "We evaluate our model on the Penn Tree Bank. For fair comparison with previous works, we followed the standard preprocessing method used by Mikolov et al. (2010). In the standard preprocessing, tokenization is applied, words are lowercased, and punctuation is removed. Also, less frequent words are replaced by unknown an token\n(UNK),2 constraining the word vocabulary size to be 10k. Because of this preprocessing, we do not expect this dataset to benefit from the modeling innovations we have introduced in the paper. Fig.1 summarizes the corpus statistics."
    }, {
      "heading" : "3.2 WikiText-2",
      "text" : "Merity et al. (2017) proposed the WikiText-2 Corpus as a new benchmark dataset.3 They pointed out that the preprocessed PTB is unrealistic for real language use in terms of word distribution. Since the vocabulary size is fixed to 10k, the word frequency does not exhibit a long tail. The wikiText-2 corpus is constructed from 720 articles. They provided two versions. The version for word level language modeling was preprocessed by discarding infrequent words. But, for character-level models, they provided raw documents without any removal of word or character types or lowercasing, but with tokenization. We make one change to this corpus: since Wikipedia articles make extensive use of characters from other languages; we replaced character types that occur fewer than 25 times were replaced with a dummy character (this plays the role of the 〈UNK〉 token in the character vocabulary). Tab. 2 summarizes the corpus statistics."
    }, {
      "heading" : "3.3 Multilingual Wikipedia Corpus (MWC)",
      "text" : "Languages differ in what word formation processes they have. For character-level modeling it is therefore interesting to compare a model’s performance across languages. Since there is at present no standard multilingual language modeling dataset, we created a new dataset, the Mul-\n2When the unknown token is used in character-level model, it is treated as if it were a normal word (i.e. UNK is the sequence U, N, and K). This is somewhat surprising modeling choice, but it has become conventional (Chung et al., 2017).\n3 http://metamind.io/research/the-wikitext-long-term-\ntilingual Wikipedia Corpus (MWC), a corpus of the same Wikipedia articles in 7 languages which manifest a range of morphological typologies. The MWC contains English (EN), French (FR), Spanish (ES), German (DE), Russian (RU), Czech (CS), and Finnish (FI).\nTo attempt to control for topic divergences across languages, every language’s data consists of the same articles. Although these are only comparable (rather than true translations), this ensures that the corpus has a stable topic profile across languages.4\nConstruction & Preprocessing We constructed the MWC similarly to the WikiText-2 corpus. Articles were selected from Wikipedia in the 7 target languages. To keep the topic distribution to be approximately the same across the corpora, we extracted articles about entities which explained in all the languages. We extracted articles which exist in all languages and each consist of more than 1,000 words, for a total of 797 articles. These cross-lingual articles are, of course, not usually translations, but they tend to be comparable. This filtering ensures that the topic profile in each language is similar. Each language corpus is approximately the same size as the WikiText-2 corpus.\nWikipedia markup was removed with WikiExtractor,5 to obtain plain text. We used the same thresholds to remove rare characters in the WikiText-2 corpus. No tokenization or other normalization (e.g., lowercasing) was done.\nStatistics After the preprocessing described above, we randomly sampled 360 articles. The articles are split into 300, 30, 30 sets and the first 300 articles are used for training and the rest are used\n4The Multilingual Wikipedia Corpus (MWC) is available for download from http://k-kawakami.com/research/mwc\n5 https://github.com/attardi/wikiextractor\nfor dev and test respectively. Table 3 summarizes the corpus statistics.\nAdditionally, we show in Fig. 2 the distribution of frequencies of OOV word types (relative to the training set) in the dev+test portions of the corpus, which shows a power-law distribution, which is expected for the burstiness of rare words found in prior work. Curves look similar for all languages (see Appendix A)."
    }, {
      "heading" : "4 Experiments",
      "text" : "We now turn to a series of experiments to show the value of our hierarchical character-level cache language model. For each dataset we trained the model with LSTM units. To compare our results with a strong baseline, we also train a model without the cache.\nModel Configuration For HCLM and HCLM with cache models, We used 600 dimensions for the character embeddings and the LSTMs have 600 hidden units for all the experiments. This keeps the model complexity to be approximately the same as previous works which used an LSTM with 1000 dimension. Our baseline LSTM have 1000 dimensions for embeddings and reccurence weights.\nFor the cache model, we used cache size 100 in every experiment. All the parameters including character projection parameters are randomly sampled from uniform distribution from −0.08 to 0.08. The initial hidden and memory state of LSTMenc and LSTMctx are initialized with zero. Mini-batches of size 25 are used for PTB experiments and 10 for WikiText-2, due to memory limitations. The sequences were truncated with 35\nwords. Then the words are decomposed to characters and fed into the model. A Dropout rate of 0.5 was used for all but the recurrent connections.\nLearning The models were trained with the Adam update rule (Kingma and Ba, 2015) with a learning rate of 0.002. The maximum norm of the gradients was clipped at 10.\nEvaluation We evaluated our models with bitsper-character (bpc) a standard evaluation metric for character-level language models. Following the definition in Graves (2013), bits-per-character is the average value of − log2 p(wt | w<t) over the whole test set,\nbpc = − 1\n|c| log2 p(w),\nwhere |c| is the length of the corpus in characters."
    }, {
      "heading" : "4.1 Results",
      "text" : "PTB Tab. 4 summarizes results on the PTB dataset.6 Our baseline HCLM model achieved 1.276 bpc which is better performance than the LSTM with Zoneout regularization (Krueger et al., 2017). And HCLM with cache outperformed the baseline model with 1.247 bpc and achieved competitive results with state-of-the-art models with regularization on recurrence weights, which was not used in our experiments.\nExpressed in terms of per-word perplexity (i.e., rather than normalizing by the length of the corpus in characters, we normalize by words and exponentiate), the test perplexity on HCLMwith cache is 94.79. The performance of the unregularized 2-layer LSTM with 1000 hidden units on wordlevel PTB dataset is 114.5 and the same model with dropout achieved 87.0. Considering the fact\n6Models designated with a * have more layers and more parameters.\nthat our character-level models are dealing with an open vocabulary without unknown tokens, the results are promising.\nWikiText-2 Tab. 5 summarizes results on the WikiText-2 dataset. Our baseline, LSTM achieved 1.803 bpc and HCLM model achieved 1.670 bpc. The HCLM with cache outperformed the baseline models and achieved 1.500 bpc. The word level perplexity is 227.30, which is quite high compared to the reported word level baseline result 100.9 with LSTM with ZoneOut and Variational Dropout regularization (Merity et al., 2017). However, the character-level model is dealing with 76,136 types in training set and 5.87% OOV rate where the word level models only use 33,278 types without OOV in test set. The improvement rate over the HCLM baseline is 10.2% which is much higher than the improvement rate obtained in the PTB experiment.\nMultilingual Wikipedia Corpus (MWC) Tab. 6 summarizes results on the MWC dataset. Similarly to WikiText-2 experiments, LSTM is strong baseline. We observe that the cache mechanism improve performance in every languages. In English, HCLM with cache achieved 1.538 bpc where the baseline is 1.622 bpc. It is 5.2% improvement. For other languages, the improvement rates were 2.7%, 3.2%, 3.7%, 2.5%, 4.7%, 2.7% in FR, DE, ES, CS, FI, RU respectively. The best improvement rate was obtained in Finnish."
    }, {
      "heading" : "5 Analysis",
      "text" : "In this section, we analyse the behavior of proposed model qualitatively. To analyse the model, we compute the following posterior probability which tell whether the model used the cache given a word and its preceding context. Let zt be a random variable that says whether to use the cache or the LM to generate the word at time t. We would like to know, given the text w, whether the cache was used at time t. This can be computed as follows:\np(zt | w) = p(zt, wt | ht, cachet)\np(wt | ht, cachet)\n= (1− λt)pptr(wt | ht, cachet)\np(wt | ht, cachet) ,\nwhere cachet is the state of the cache at time t. We report the average posterior probability of cache generation excluding the first occurrence of w, p(z | w).\nTab. 7 shows the words in the WikiText-2 test set that occur more than 1 time that are most/least likely to be generated from cache and character language model (words that occur only one time cannot be cache-generated). We see that the model uses the cache for proper nouns: Lesnar, Gore, etc., as well as very frequent words which always stored somewhere in the cache such as singletoken punctuation, the, and of. In contrast, the model uses the language model to generate numbers (which tend not to be repeated): 300, 770 and basic content words: sounds, however, unable,\netc. This pattern is similar to the pattern found in empirical distribution of frequencies of rare words observed in prior wors (Church and Gale, 1995; Church, 2000), which suggests our model is learning to use the cache to account for bursts of rare words.\nTo look more closely at rare words, we also investigate how the model handles words that occurred between 2 and 100 times in the test set, but fewer than 5 times in the training set. Fig. 3 is a scatter plot of p(z | w) vs the empirical frequency in the test set. As expected, more frequently repeated words types are increasingly likely to be drawn from the cache, but less frequent words show a range of cache generation probabilities.\nTab. 8 shows word types with the highest and lowest average p(z | w) that occur fewer than 5 times in the training corpus. The pattern here is similar to the unfiltered list: proper nouns are extremely likely to have been cache-generated, whereas numbers and generic (albeit infrequent) content words are less likely to have been."
    }, {
      "heading" : "6 Discussion",
      "text" : "Our results show that theHCLM outperforms a basic LSTM. With the addition of the caching mechanism, the HCLM becomes consistently more powerful than both the baseline HCLM and the LSTM. This is true even on the PTB, which has no rare or OOV words in its test set (because of preprocessing), by caching repetitive common\nwords such as the. In true open-vocabulary settings (i.e., WikiText-2 and MWC), the improvements are much more pronounced, as expected.\nComputational complexity. In comparison with word-level models, our model has to read and generate each word character by character, and it also requires a softmax over the entire memory at every time step. However, the computation is still linear in terms of the length of the sequence, and the softmax over the memory cells and character vocabulary are much smaller than word-level vocabulary. On the other hand, since the recurrent states are updated once per character (rather than per word) in our model, the distribution of operations is quite different. Depending on the hardware support for these operations (repeated updates of recurrent states\nvs. softmaxes), our model may be faster or slower. However, our model will have fewer parameters than a word-based model since most of the parameters in such models live in the word projection layers, and we use LSTMs in place of these.\nNon-English languages. For non-English languages, the pattern is largely similar for nonEnglish languages. This is not surprising since morphological processes may generate forms that are related to existing forms, but these still have slight variations. Thus, they must be generated by the language model component (rather than from the cache). Still, the cache demonstrates consistent value in these languages.\nFinally, our analysis of the cache on English\ndoes show that it is being used to model word reuse, particularly of proper names, but also of frequent words. While empirical analysis of rare word distributions predicts that names would be reused, the fact that cache is used to model frequent words suggests that effective models of language should have a means to generate common words as units. Finally, our model disfavors copying numbers from the cache, even when they are available. This suggests that it has learnt that numbers are not generally repeated (in contrast to names)."
    }, {
      "heading" : "7 Related Work",
      "text" : "Caching language models were proposed to account for burstiness by Kuhn and De Mori (1990), and recently, this idea has been incorporated to augment neural language models with a caching mechanism (Merity et al., 2017; Grave et al., 2017).\nOpen vocabulary neural language models have been widely explored (Sutskever et al., 2011; Mikolov et al., 2012; Graves, 2013, inter alia). Attempts to make them more aware of wordlevel dynamics, using models similar to our hierarchical formulation, have also been proposed (Chung et al., 2017).\nThe only models that are open vocabulary language modeling together with a caching mechanism are the nonparametric Bayesian language models based on hierarchical Pitman–Yor processes which generate a lexicon of word types using a character model, and then generate a text using these (Teh, 2006; Goldwater et al., 2009; Chahuneau et al., 2013). These, however, do not use distributed representations on RNNs to capture long-range dependencies."
    }, {
      "heading" : "8 Conclusion",
      "text" : "In this paper, we proposed a character-level language model with an adaptive cache which selectively assign word probability from past history or character-level decoding. And we empirically show that our model efficiently model the word sequences and achieved better perplexity in every standard dataset. To further validate the performance of our model on different languages, we collected multilingual wikipedia corpus for 7 typologically diverse languages. We also show that our model performs better than character-level models by modeling burstiness of words in local context.\nThe model proposed in this paper assumes the observation of word segmentation. Thus, the model is not directly applicable to languages, such as Chinese and Japanese, where word segments are not explicitly observable. We will investigate a model which can marginalise word segmentation as latent variables in the future work."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We thank the three anonymous reviewers for their valuable feedback. The third author acknowledges the support of the EPSRC and nvidia Corporation."
    }, {
      "heading" : "A Corpus Statistics",
      "text" : "Fig. 4 show distribution of frequencies of OOV word types in 6 languages."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "Proc. ICLR.",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Knowledge-rich morphological priors for bayesian language models",
      "author" : [ "Victor Chahuneau", "Noah A. Smith", "Chris Dyer." ],
      "venue" : "Proc. NAACL.",
      "citeRegEx" : "Chahuneau et al\\.,? 2013",
      "shortCiteRegEx" : "Chahuneau et al\\.",
      "year" : 2013
    }, {
      "title" : "Hierarchical multiscale recurrent neural networks",
      "author" : [ "Junyoung Chung", "Sungjin Ahn", "Yoshua Bengio." ],
      "venue" : "Proc. ICLR.",
      "citeRegEx" : "Chung et al\\.,? 2017",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2017
    }, {
      "title" : "Empirical estimates of adaptation: the chance of two Noriegas is closer to p/2 than p",
      "author" : [ "KennethWChurch." ],
      "venue" : "Proc. COLING.",
      "citeRegEx" : "KennethWChurch.,? 2000",
      "shortCiteRegEx" : "KennethWChurch.",
      "year" : 2000
    }, {
      "title" : "Poisson mixtures",
      "author" : [ "KennethWChurch andWilliam AGale." ],
      "venue" : "Natural Language Engineering 1(2):163– 190.",
      "citeRegEx" : "AGale.,? 1995",
      "shortCiteRegEx" : "AGale.",
      "year" : 1995
    }, {
      "title" : "Recurrent batch normalization",
      "author" : [ "Tim Cooijmans", "Nicolas Ballas", "César Laurent", "Çağlar Gülçehre", "Aaron Courville." ],
      "venue" : "Proc. ICLR.",
      "citeRegEx" : "Cooijmans et al\\.,? 2017",
      "shortCiteRegEx" : "Cooijmans et al\\.",
      "year" : 2017
    }, {
      "title" : "A Bayesian framework for word segmentation: Exploring the effects of context",
      "author" : [ "Sharon Goldwater", "Thomas L Griffiths", "Mark Johnson." ],
      "venue" : "Cognition 112(1):21–54.",
      "citeRegEx" : "Goldwater et al\\.,? 2009",
      "shortCiteRegEx" : "Goldwater et al\\.",
      "year" : 2009
    }, {
      "title" : "Improving neural languagemodels with a continuous cache",
      "author" : [ "Edouard Grave", "Armand Joulin", "Nicolas Usunier." ],
      "venue" : "Proc. ICLR.",
      "citeRegEx" : "Grave et al\\.,? 2017",
      "shortCiteRegEx" : "Grave et al\\.",
      "year" : 2017
    }, {
      "title" : "Generating sequences with recurrent neural networks",
      "author" : [ "Alex Graves." ],
      "venue" : "arXiv preprint arXiv:1308.0850 .",
      "citeRegEx" : "Graves.,? 2013",
      "shortCiteRegEx" : "Graves.",
      "year" : 2013
    }, {
      "title" : "Hypernetworks",
      "author" : [ "David Ha", "Andrew Dai", "Quoc V Le." ],
      "venue" : "Proc. ICLR.",
      "citeRegEx" : "Ha et al\\.,? 2017",
      "shortCiteRegEx" : "Ha et al\\.",
      "year" : 2017
    }, {
      "title" : "Information retrieval: Computational and theoretical aspects",
      "author" : [ "Harold Stanley Heaps." ],
      "venue" : "Academic Press, Inc.",
      "citeRegEx" : "Heaps.,? 1978",
      "shortCiteRegEx" : "Heaps.",
      "year" : 1978
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation 9(8):1735– 1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba." ],
      "venue" : "Proc. ICLR.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "A clockwork RNN",
      "author" : [ "Jan Koutnik", "Klaus Greff", "Faustino Gomez", "Juergen Schmidhuber." ],
      "venue" : "Proc. ICML.",
      "citeRegEx" : "Koutnik et al\\.,? 2014",
      "shortCiteRegEx" : "Koutnik et al\\.",
      "year" : 2014
    }, {
      "title" : "Zoneout: Regularizing rnns by randomly preserving hidden activations",
      "author" : [ "David Krueger", "TeganMaharaj", "János Kramár", "Mohammad Pezeshki", "Nicolas Ballas", "Nan Rosemary Ke", "Anirudh Goyal", "Yoshua Bengio", "Hugo Larochelle", "Aaron Courville" ],
      "venue" : null,
      "citeRegEx" : "Krueger et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Krueger et al\\.",
      "year" : 2017
    }, {
      "title" : "A cachebased natural language model for speech recognition",
      "author" : [ "Roland Kuhn", "Renato De Mori." ],
      "venue" : "IEEE transactions on pattern analysis and machine intelligence 12(6):570–583.",
      "citeRegEx" : "Kuhn and Mori.,? 1990",
      "shortCiteRegEx" : "Kuhn and Mori.",
      "year" : 1990
    }, {
      "title" : "Finding function in form: Compositional character models for open vocabulary word representation",
      "author" : [ "Wang Ling", "Tiago Luís", "Luís Marujo", "Ramón Fernandez Astudillo", "Silvio Amir", "Chris Dyer", "Alan W Black", "Isabel Trancoso." ],
      "venue" : "Proc. EMNLP.",
      "citeRegEx" : "Ling et al\\.,? 2015",
      "shortCiteRegEx" : "Ling et al\\.",
      "year" : 2015
    }, {
      "title" : "Pointer sentinel mixturemodels",
      "author" : [ "Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher." ],
      "venue" : "Proc. ICLR.",
      "citeRegEx" : "Merity et al\\.,? 2017",
      "shortCiteRegEx" : "Merity et al\\.",
      "year" : 2017
    }, {
      "title" : "Recurrent neural network based language model",
      "author" : [ "Tomas Mikolov", "Martin Karafiát", "Lukas Burget", "Jan Cernockỳ", "Sanjeev Khudanpur." ],
      "venue" : "Proc. Interspeech.",
      "citeRegEx" : "Mikolov et al\\.,? 2010",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2010
    }, {
      "title" : "Subword language modeling with neural networks",
      "author" : [ "Tomáš Mikolov", "Ilya Sutskever", "Anoop Deoras", "HaiSon Le", "Stefan Kombrink", "Jan Cernocky." ],
      "venue" : "preprint (http://www. fit. vutbr. cz/imikolov/rnnlm/char. pdf) .",
      "citeRegEx" : "Mikolov et al\\.,? 2012",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2012
    }, {
      "title" : "Recurrent dropout without memory loss",
      "author" : [ "Stanislau Semeniuta", "Aliaksei Severyn", "Erhardt Barth." ],
      "venue" : "Proc. COLING.",
      "citeRegEx" : "Semeniuta et al\\.,? 2016",
      "shortCiteRegEx" : "Semeniuta et al\\.",
      "year" : 2016
    }, {
      "title" : "A hierarchical recurrent encoderdecoder for generative context-aware query suggestion",
      "author" : [ "Alessandro Sordoni", "Yoshua Bengio", "Hossein Vahabi", "Christina Lioma", "Jakob Grue Simonsen", "JianYun Nie." ],
      "venue" : "Proc. CIKM.",
      "citeRegEx" : "Sordoni et al\\.,? 2015",
      "shortCiteRegEx" : "Sordoni et al\\.",
      "year" : 2015
    }, {
      "title" : "Generating text with recurrent neural networks",
      "author" : [ "Ilya Sutskever", "James Martens", "Geoffrey E Hinton." ],
      "venue" : "Proc. ICML.",
      "citeRegEx" : "Sutskever et al\\.,? 2011",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2011
    }, {
      "title" : "A hierarchical Bayesian language model based on Pitman-Yor processes",
      "author" : [ "Yee Whye Teh." ],
      "venue" : "Proc. ACL.",
      "citeRegEx" : "Teh.,? 2006",
      "shortCiteRegEx" : "Teh.",
      "year" : 2006
    }, {
      "title" : "On multiplicative integration with recurrent neural networks",
      "author" : [ "YuhuaiWu", "Saizheng Zhang", "Ying Zhang", "Yoshua Bengio", "Ruslan R Salakhutdinov." ],
      "venue" : "Proc. NIPS.",
      "citeRegEx" : "YuhuaiWu et al\\.,? 2016",
      "shortCiteRegEx" : "YuhuaiWu et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural architecture search with reinforcement learning",
      "author" : [ "Barret Zoph", "Quoc V Le." ],
      "venue" : "arXiv preprint arXiv:1611.01578 .",
      "citeRegEx" : "Zoph and Le.,? 2016",
      "shortCiteRegEx" : "Zoph and Le.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "First, vocabularies keep growing as the number of documents in a corpus grows: new words are constantly being created (Heaps, 1978).",
      "startOffset" : 118,
      "endOffset" : 131
    }, {
      "referenceID" : 22,
      "context" : "The open-vocabulary problem can be solved by dispensing with word-level models in favor of models that predict sentences as sequences of characters (Sutskever et al., 2011; Chung et al., 2017).",
      "startOffset" : 148,
      "endOffset" : 192
    }, {
      "referenceID" : 2,
      "context" : "The open-vocabulary problem can be solved by dispensing with word-level models in favor of models that predict sentences as sequences of characters (Sutskever et al., 2011; Chung et al., 2017).",
      "startOffset" : 148,
      "endOffset" : 192
    }, {
      "referenceID" : 21,
      "context" : "Our starting point is a hierarchical LSTM that has been previously used for modeling sentences (word by word) in a conversation (Sordoni et al., 2015), except here we model words (character by character) in a sentence.",
      "startOffset" : 128,
      "endOffset" : 150
    }, {
      "referenceID" : 17,
      "context" : "To this model, we add a caching mechanism similar to recent proposals for caching that have been advocated for closed-vocabulary models (Merity et al., 2017; Grave et al., 2017).",
      "startOffset" : 136,
      "endOffset" : 177
    }, {
      "referenceID" : 7,
      "context" : "To this model, we add a caching mechanism similar to recent proposals for caching that have been advocated for closed-vocabulary models (Merity et al., 2017; Grave et al., 2017).",
      "startOffset" : 136,
      "endOffset" : 177
    }, {
      "referenceID" : 16,
      "context" : "1) which generates words as a sequence of characters and constructs a “word embedding” by encoding a character sequence with an LSTM (Ling et al., 2015).",
      "startOffset" : 133,
      "endOffset" : 152
    }, {
      "referenceID" : 20,
      "context" : "TheHCLM is an adaptation of the hierarchical recurrent encoder-decoder of Sordoni et al. (2015) which was used to model dialog as a sequence of actions sentences which are themselves sequences of words.",
      "startOffset" : 74,
      "endOffset" : 96
    }, {
      "referenceID" : 17,
      "context" : "This cache mechanism is similar to the model proposed by Merity et al. (2017).",
      "startOffset" : 57,
      "endOffset" : 78
    }, {
      "referenceID" : 11,
      "context" : "The HCLM consists of four components, three LSTMs (Hochreiter and Schmidhuber, 1997): a character encoder, a word-level context encoder, and a character decoder (denoted LSTMenc, LSTMctx, and LSTMdec, respectively), and a softmax output layer over the character vocabulary.",
      "startOffset" : 50,
      "endOffset" : 84
    }, {
      "referenceID" : 6,
      "context" : "Similarly to the memory structure used in Grave et al. (2017), a word is added to a key-value memory after each generation of wt.",
      "startOffset" : 42,
      "endOffset" : 62
    }, {
      "referenceID" : 0,
      "context" : "To define the copy probability from the cache at time t, a distribution over copy sites is defined using the attention mechanism of Bahdanau et al. (2015). To do so, we construct a query vector (rt) from the RNN’s current hidden state ht,",
      "startOffset" : 132,
      "endOffset" : 155
    }, {
      "referenceID" : 7,
      "context" : "We remark that Grave et al. (2017) use a clever trick to estimate the probability, λt of drawing from the LM by augmenting their (closed) vocabulary with a special symbol indicating that a copy should be used.",
      "startOffset" : 15,
      "endOffset" : 35
    }, {
      "referenceID" : 18,
      "context" : "For fair comparison with previous works, we followed the standard preprocessing method used by Mikolov et al. (2010). In the standard preprocessing, tokenization is applied, words are lowercased, and punctuation is removed.",
      "startOffset" : 95,
      "endOffset" : 117
    }, {
      "referenceID" : 2,
      "context" : "This is somewhat surprising modeling choice, but it has become conventional (Chung et al., 2017).",
      "startOffset" : 76,
      "endOffset" : 96
    }, {
      "referenceID" : 12,
      "context" : "Learning The models were trained with the Adam update rule (Kingma and Ba, 2015) with a learning rate of 0.",
      "startOffset" : 59,
      "endOffset" : 80
    }, {
      "referenceID" : 8,
      "context" : "Following the definition in Graves (2013), bits-per-character is the average value of − log2 p(wt | w<t) over the whole test set,",
      "startOffset" : 28,
      "endOffset" : 42
    }, {
      "referenceID" : 14,
      "context" : "276 bpc which is better performance than the LSTM with Zoneout regularization (Krueger et al., 2017).",
      "startOffset" : 78,
      "endOffset" : 100
    }, {
      "referenceID" : 13,
      "context" : "CW-RNN (Koutnik et al., 2014) - 1.",
      "startOffset" : 7,
      "endOffset" : 29
    }, {
      "referenceID" : 19,
      "context" : "46 HF-MRNN (Mikolov et al., 2012) - 1.",
      "startOffset" : 11,
      "endOffset" : 33
    }, {
      "referenceID" : 19,
      "context" : "39 ME n-gram (Mikolov et al., 2012) - 1.",
      "startOffset" : 13,
      "endOffset" : 35
    }, {
      "referenceID" : 5,
      "context" : "37 RBN (Cooijmans et al., 2017) 1.",
      "startOffset" : 7,
      "endOffset" : 31
    }, {
      "referenceID" : 20,
      "context" : "32 Recurrent Dropout (Semeniuta et al., 2016) 1.",
      "startOffset" : 21,
      "endOffset" : 45
    }, {
      "referenceID" : 14,
      "context" : "301 Zoneout (Krueger et al., 2017) 1.",
      "startOffset" : 12,
      "endOffset" : 34
    }, {
      "referenceID" : 2,
      "context" : "297 HM-LSTM (Chung et al., 2017) - 1.",
      "startOffset" : 12,
      "endOffset" : 32
    }, {
      "referenceID" : 9,
      "context" : "27 HyperNetwork (Ha et al., 2017) 1.",
      "startOffset" : 16,
      "endOffset" : 33
    }, {
      "referenceID" : 9,
      "context" : "265 LayerNorm HyperNetwork (Ha et al., 2017) 1.",
      "startOffset" : 27,
      "endOffset" : 44
    }, {
      "referenceID" : 9,
      "context" : "250 2-LayerNorm HyperLSTM (Ha et al., 2017)* - 1.",
      "startOffset" : 26,
      "endOffset" : 43
    }, {
      "referenceID" : 25,
      "context" : "219 2-Layer with New Cell (Zoph and Le, 2016)* - 1.",
      "startOffset" : 26,
      "endOffset" : 45
    }, {
      "referenceID" : 17,
      "context" : "9 with LSTM with ZoneOut and Variational Dropout regularization (Merity et al., 2017).",
      "startOffset" : 64,
      "endOffset" : 85
    }, {
      "referenceID" : 17,
      "context" : "Caching language models were proposed to account for burstiness by Kuhn and De Mori (1990), and recently, this idea has been incorporated to augment neural language models with a caching mechanism (Merity et al., 2017; Grave et al., 2017).",
      "startOffset" : 197,
      "endOffset" : 238
    }, {
      "referenceID" : 7,
      "context" : "Caching language models were proposed to account for burstiness by Kuhn and De Mori (1990), and recently, this idea has been incorporated to augment neural language models with a caching mechanism (Merity et al., 2017; Grave et al., 2017).",
      "startOffset" : 197,
      "endOffset" : 238
    }, {
      "referenceID" : 2,
      "context" : "Attempts to make them more aware of wordlevel dynamics, using models similar to our hierarchical formulation, have also been proposed (Chung et al., 2017).",
      "startOffset" : 134,
      "endOffset" : 154
    }, {
      "referenceID" : 23,
      "context" : "The only models that are open vocabulary language modeling together with a caching mechanism are the nonparametric Bayesian language models based on hierarchical Pitman–Yor processes which generate a lexicon of word types using a character model, and then generate a text using these (Teh, 2006; Goldwater et al., 2009; Chahuneau et al., 2013).",
      "startOffset" : 284,
      "endOffset" : 343
    }, {
      "referenceID" : 6,
      "context" : "The only models that are open vocabulary language modeling together with a caching mechanism are the nonparametric Bayesian language models based on hierarchical Pitman–Yor processes which generate a lexicon of word types using a character model, and then generate a text using these (Teh, 2006; Goldwater et al., 2009; Chahuneau et al., 2013).",
      "startOffset" : 284,
      "endOffset" : 343
    }, {
      "referenceID" : 1,
      "context" : "The only models that are open vocabulary language modeling together with a caching mechanism are the nonparametric Bayesian language models based on hierarchical Pitman–Yor processes which generate a lexicon of word types using a character model, and then generate a text using these (Teh, 2006; Goldwater et al., 2009; Chahuneau et al., 2013).",
      "startOffset" : 284,
      "endOffset" : 343
    } ],
    "year" : 2017,
    "abstractText" : "Fixed-vocabulary language models fail to account for one of the most characteristic statistical facts of natural language: the frequent creation and reuse of new word types. Although character-level language models offer a partial solution in that they can create word types not attested in the training corpus, they do not capture the “bursty” distribution of such words. In this paper, we augment a hierarchical LSTM language model that generates sequences of word tokens character by character with a caching mechanism that learns to reuse previously generated words. To validate our model we construct a new open-vocabulary language modeling corpus (the Multilingual Wikipedia Corpus; MWC) from comparable Wikipedia articles in 7 typologically diverse languages and demonstrate the effectiveness of our model across this range of languages.",
    "creator" : "LaTeX with hyperref package"
  }
}