{
  "name" : "1510.07211.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On End-to-End Program Generation from User Intention by Deep Neural Networks",
    "authors" : [ "Lili Mou", "Rui Men", "Ge Li", "Lu Zhang", "Zhi Jin" ],
    "emails" : [ "doublepower.mou@gmail.com,", "menruimr@gmail.com,", "lige@sei.pku.edu.cn", "zhanglu@sei.pku.edu.cn", "zhijin@sei.pku.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Categories and Subject Descriptors I.2.2 [Artificial Intelligence]: Automatic Programming— Program synthesis\nGeneral Terms Algorithms\nKeywords Deep learning, Recurrent network, Program generation"
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "Imagine a following scenario in software engineering: There exists abundant high-quality source code, well commented and documented, in large software repositories. A very powerful machine (e.g., deep neural network) learns the mapping from natural language of problem descriptions to source code. During development, users express their intention by natural language (similar to some in the repository); the learning machine automatically output the desired code as the solution.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.\nA more compelling feature is that the above process works in an “end-to-end” manner, which requires little, if any, human knowledge, and is completely language independent— the only thing needed is to represent sentences and programs as characters. The learning machine automatically reads a natural language sentence character-by-character to capture user intention, and then generates code in a similar fashion. As learning machines differ from code retrieval systems, the generated code is different from any existing code, being more flexible but maybe also vulnerable. However, the code should be (almost) correct: It satisfies the syntax, and implements the desired functionality. The code is usable with a little post-editing.\nSuch scenario of automatic program generation has long been the dream of software engineering (SE), and is closely related to a variety of SE tasks, e.g., algorithm discovery, programming assistance [6]. However, traditional approaches are typically weak in terms of automation and abstraction. For example, Manna et al. propose deductive approaches [15], Flener et al. inductive approaches [5]; these methods require human-designed specifications. Program generation by genetic programming [20, 7] can automatically search the space of candidate programs (inefficiently), but carefully chosen mutation or crossover operations should also be provided. Natural language programming, emerged in the past decade, is much like “pseudo-compiling,” where the natural language is of low-level abstraction [12, 4].\nNowadays, software artifacts, including code and documentation, have become “big data” (e.g., Github, SourceForge). Provided sufficient training data of code with corresponding comments and documents, it is possible in principle to train a generative model of programs based on natural language. At the meantime, the natural language processing (NLP) community is witnessing significant breakthroughs and amazing results in various tasks including question answering [13], machine translation [19], or image-caption generation [21]. These cross-disciplinary advances bring new opportunities for automatic program generation.\nIn this paper, we investigate, by a case study in Section 2, the feasibility of generating executable, functionally coherent code by recurrent neural networks (RNNs); empirical analysis reveals the mechanism how RNNs could accomplish the goal. We also envision several scenarios where such techniques may help real-world SE tasks, and address long-term research challenges (Section 3). Although we concede there still remains a long way before end-to-end program generation can be used in SE practice, we believe it would become a reality in future decades.\nar X\niv :1\n51 0.\n07 21\n1v 1\n[ cs\n.S E\n] 2\n5 O\nct 2\n01 5"
    }, {
      "heading" : "2. A CASE STUDY",
      "text" : ""
    }, {
      "heading" : "2.1 The Model of Recurrent Networks",
      "text" : "Among a variety of machine learning methods, the deep neural network (also known as deep learning) is among recent groundbreaking advances, featured by its ability of learning highly complicated features automatically [14].\nFor end-to-end program generation, we prefer the recurrent neural networks (RNNs), which are suitable for modeling time-series data (e.g., a sequence of characters) by its iterative nature. An RNN typically keeps one or a few hidden layers, changing over each (discrete) time step according to input data. This process is delineated in Figure 1.\nTheoretical analysis shows that recurrent neural networks are equivalent to Turing machines [9]. However, training RNNs in early years was difficult because of the gradient blowup or vanishing problem [18]. Long short term memory (LSTM) units [8], or gated units [2] are designed to balance between retaining the previous state and memorizing new information at the current time step, making RNNs much easier to train.\nOn this basis, Sutskever et al. design an RNN model for sequence to sequence generation [19]. The idea is to first read an input sequence, ended with a special symbol, <eos> (end of sequence), depicted by Figure 1a. For output, the RNN applies a softmax layer at each time step, predicting the probability that each symbol1 may occur at the current step; the symbol with the highest probability is chosen, and fed to the network as input at the next time step. This process is done iteratively until the special symbol, <eos>, is generated by the network (Figure 1b).\nSuch RNN architecture can be applied to sequences of different granularities, e.g., word-level, sub-word level, etc. In particular, character-level RNN generative models have unexpectedly achieved remarkable and somewhat amazing performance. Successful applications include generating texts, music, or even Linux-like C code [10]. Empirical studies show that RNNs are particularly good at modeling syntax aspects, e.g., parenthesis pairing, indentation, etc [11]. It works much like a push-down automata, but seems less capable of capturing semantics—the Linux-like code generated, for example, is plausible, but cannot be compiled and lacks coherence in functionality.\nWe are therefore curious whether RNNs can generate executable, functionally coherent source code, which is an essence to benefit real-world software engineering tasks.\nTo accomplish this goal, we leverage a dataset from a pedagogical programming online judge (OJ) system,2 intended for the undergraduate course, Introduction to Computing. The OJ system comprises different programming problems. Students submit their source code to a specific problem, and the OJ system judges its validity automatically (via running). We notice that programs corresponding to a specific problem have exactly the same functionality, which makes the dataset particularly suitable, as a first trial, for training neural networks to generate functionally coherent programs.\nWe fed the network with 4 different programing problems, each containing more than 500 different source code samples. After preprocessing, a program was preceded by a\n1A symbol may refer to a word or a character according to the granularity in a certain application. 2http://programming.grids.cn\nbrief comment, e.g., “find the maximum and second maximum numbers,” serving as the input sequence (Figure 1a).3 What follows is a program that solves the particular problem, serving as the output sequence (Figure 1b). Figures 2b and 2c further illustrate two training samples of the aforementioned programming problem."
    }, {
      "heading" : "2.2 Result and Analysis",
      "text" : "Figure 2a is a sample code generated by RNN. Through a quick analysis, we find that the code is almost executable: with a little post-correction of 4 characters among ∼280, the program is compilable and functionally correct.\nWe would answer a very fundamental question: Is RNN generating code by simply memorizing a particular training sample? If this were the case, RNN would just work in a copy-and-paste fashion, which degrades the problem to a trivial case.\nBy examining the training data, we observe that there does NOT exist a same program in the training set, which rules out the possibility that RNN works by exact memorizing. We further use ccfinder4 to detect most similar code in the training set. Two are shown in Figure 2, and the results are particularly interesting. We provide our explanation regarding several aspects of a program as follows.\n• Structure. Figure 2b shows the most similar code in structure. The generated code implements the same algorithm—scanning the array twice to find the maximum and second maximum numbers respectively. Notice, however, the two structures (abstract syntax trees, say) are not exactly the same as there are differences in variable definitions. A more interesting detail is that RNN has recognized“i<n”and“i<=n-1”are equivalent in the for loop, and that it does not follow exactly the sample code (b) in the training set but remains correct.\n• Variable IDs. The training sample with the most similar variable IDs is shown in Figure 2c. Our generated code uses the same ID, a, for the array, and max1, max2 to cache the two wanted numbers; but later, the structure diverges. Nevertheless, our network is aware of the variable IDs it has generated, and remains coherent until the very end of the program.\n3 Entire dataset and configurations are available on our website. http://sites.google.com/site/rnngenprogram 4http://www.ccfinder.net/\nFigure 2: (a) Code generated by RNN. The code is almost correct except 4 wrong characters (among ∼280 characters in total), highlighted in the figure. (b) Code with the most similar structure in the training set, detected by ccfinder. (c) Code with the most similar identifiers in the training set, also detected by ccfinder. Note that we preserve all indents, spaces and line feeds. The 4 errors are (1) The identifier “x” should be “n”; (2) “max” should be “max2”; (3) “==” should be “<”; (4) return type should be void.\n• Style. We find no particular training samples having the same code style in terms of indents, line feeds, etc. It makes much sense because the training programs are written by junior programmers, who may not follow standard style convention, and thus the network has no idea about the “right” style. However, as all training samples are “correct” programs, our network has little difficulty in learning the syntax of C programs as the generated code can almost be compiled.\nThrough the above analysis, we gain a basic idea on how RNN is able to generate programs. The RNN first recognizes the brief comment, “find the maximum and second maximum numbers,” which precedes the code as input. We would like to point out that, in this experiment, the RNN does not understand the meaning of this sentence; but via reading the brief comment, the RNN switches its hidden states to generate code of the functionality in need. For each functionality, RNN is aware of different aspects of a possible program, including structures, IDs, etc. When generating, it chooses the most likely character conditioned on the previous characters, also conditioned on the input. In particular, the RNN does have the ability to mix different structures and IDs but remain (almost) coherent."
    }, {
      "heading" : "3. PROSPECTIVES & ROAD MAP",
      "text" : "While simple and preliminary, our case study and analysis provide illuminating pictures on end-to-end program generation with deep neural networks. We point out several scenarios where deep learning can benefit real-world SE practice, which are also research topics in long-term studies.\n• Understanding changeable user intention. The current case study shows RNN’s ability of recognizing certain\nintention and generating corresponding code. In SE practice, however, we are oftentimes facing changeable requirement from users. To address the problem, a direct extension is to train a parametric code generator with arguments (e.g., file names, protocols) implicitly expressed using natural language. To tackle a more challenging prospective, we might first train a network to generate different “primitive” code snippets, and then “glue” them together. For instance, if a network has learned to write code of finding the maximum number, and also of finding the minimum number, then it shall be possible to generate these two snippets subsequently if it reads an instruction “find the maximum and minimum numbers.”\n• Incorporating multiple sources of user intention. When developing software, programmers usually find their code dependent to context (e.g., previously defined variables, existing API call sequences) in addition to the functionality in need. In such scenarios, we might train a network to fill missing blocks of code. While we admit that code completion in general could hardly make any sense, we think this problem is mostly realistic in some task-specific scenarios. For example, a typical way of reading a txt file in Java involves creating FileReader, BufferedReader, reading lines in the file, closing the file, and also catching exceptions. Such standard pipelines might be generated automatically by neural networks, provided context code.\nDespite the promising future of using RNNs to generate source code, efforts shall be made from multiple disciplines including SE, NLP and machine learning communities. Most important questions in the SE community are defining user’s intention and providing datasets for training. How can we\nspecify the functionality that we want to generate? How can we specify the arguments of a function? How can we collect the dataset which is not only large and informative enough for training, but also clean enough for not including too much noise? These are among the open questions. The NLP and machine learning communities, on the other hand, are continuously improving neural architectures. Attentionbased networks [3, 21], for example, are proposed recently to mitigate the problem of long input sequences that cannot be composed into a fixed-size vector. More studies are still needed in terms of understanding the memory capacity of RNNs, generating data with more coherent semantics, or even revising generated data, etc.\nWe concede that using RNNs to generate programs differs significantly from writing programs by humans. It appears unrealistic currently to train any learning machine, including deep neural networks, to fully understand either natural languages or programming languages. However, supported by existing evidence in the literature and the case study in this paper, we deem end-to-end program generation shall be possible in the future."
    }, {
      "heading" : "4. RELATED WORK IN DEEP LEARNING FOR PROGRAM ANALYSIS",
      "text" : "Recent years have witnessed the birth of program analysis based on deep neural networks. Our previous work learns programs’ vector representations, serving as a pretraining phrase in deep learning [17]; we also propose treebased convolutional neural networks to classify programs by functionality and detect source code of certain patterns [16]. Zaremba et al. use RNNs to estimate the output of a restricted python program [22]. Allamanis et al. leverage vector representations to suggest method names [1]. All the above models are discriminative, by which we mean the tasks can be viewed as a classification problem. Karpathy et al. train an RNN-based language model on C code, which maximizes the joint probability of a program [11]. Different from the above studies, this paper investigates whether neural models can synthesize executable, functionally coherent programs, which demands more need in matching users’ intention and capturing internal structures of source code."
    }, {
      "heading" : "5. CONCLUSIVE REMARKS",
      "text" : "In this paper, we trained a recurrent neural network (RNN) to generate (almost) executable, functionally coherent source code. Our initial work has demonstrated the possibility of automatic end-to-end program generation. Through analyzing the RNN’s mechanism, we envisioned several scenarios where such techniques can be applied in software engineering tasks in future decades. We call for studies from multiple disciplines to further address this new research direction."
    }, {
      "heading" : "6. REFERENCES",
      "text" : "[1] M. Allamanis, E. Barr, C. Bird, and C. Sutton.\nSuggesting accurate method and class names. In ESEC/FSE, 2015.\n[2] K. Cho, B. van Merriënboer, D. Bahdanau, and Y. Bengio. On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint, 1409.1259, 2014.\n[3] J. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio. Attention-based models for speech recognition. arXiv preprint, 1506.07503, 2015.\n[4] A. Cozzie and S. T. King. Macho: Writing programs with natural language and examples. Technical report, University of Illinois at Urbana-Champaign, 2012.\n[5] P. Flener and D. Partridge. Inductive programming. Automated Softw. Engineering, 8(2):131–137, 2001.\n[6] S. Gulwani. Dimensions in program synthesis. In Proc. ACM SIGPLAN Symposium on Principles and Practice of Declarative Programming, 2010.\n[7] T. Helmuth and L. Spector. General program synthesis benchmark suite. In Proc. Genetic and Evol. Comput. Conf. ACM, 2015.\n[8] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735–1780, 1997.\n[9] H. Hyötyniemi. Turing machines are recurrent neural networks. Proc. STeP, 1996.\n[10] A. Karpathy. The unreasonable effectiveness of recurrent neural networks. http://karpathy.github.io/ 2015/05/21/rnn-effectiveness/, 2015.\n[11] A. Karpathy, J. Johnson, and F. Li. Visualizing and understanding recurrent networks. arXiv preprint, 1506.02078, 2015.\n[12] R. Knöll and M. Mezini. Pegasus: First steps toward a naturalistic programming language. In OOPSLA, 2006.\n[13] A. Kumar, O. Irsoy, J. Su, et al. Ask me anything: Dynamic memory networks for natural language processing. arXiv preprint, 1506.07285, 2015.\n[14] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521(7553):436–444, 2015.\n[15] Z. Manna and R. Waldinger. A deductive approach to program synthesis. ACM Trans. Programming Languages and Syst., 2(1):90–121, 1980.\n[16] L. Mou, G. Li, Z. Jin, L. Zhang, and T. Wang. TBCNN: A tree-based convolutional neural network for programming language processing. AAAI Workshop, 2015.\n[17] L. Mou, G. Li, Y. Liu, H. Peng, Z. Jin, Y. Xu, and L. Zhang. Building program vector representations for deep learning. arXiv preprint, 1409.3358, 2014.\n[18] R. Pascanu, T. Mikolov, and Y. Bengio. On the difficulty of training recurrent neural networks. arXiv preprint, 1211.5063, 2012.\n[19] I. Sutskever, O. Vinyals, and Q. Le. Sequence to sequence learning with neural networks. In NIPS, 2014.\n[20] W. Weimer, T. Nguyen, C. Le Goues, and S. Forrest. Automatically finding patches using genetic programming. In ICSE, 2009.\n[21] K. Xu et al. Show, attend and tell: Neural image caption generation with visual attention. arXiv preprint.\n[22] W. Zaremba and I. Sutskever. Learning to execute. arXiv preprint, 1410.4615, 2014."
    } ],
    "references" : [ {
      "title" : "Suggesting accurate method and class names",
      "author" : [ "M. Allamanis", "E. Barr", "C. Bird", "C. Sutton" ],
      "venue" : "ESEC/FSE",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "B",
      "author" : [ "K. Cho" ],
      "venue" : "van Merriënboer, D. Bahdanau, and Y. Bengio. On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint, 1409.1259",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Attention-based models for speech recognition",
      "author" : [ "J. Chorowski", "D. Bahdanau", "D. Serdyuk", "K. Cho", "Y. Bengio" ],
      "venue" : "arXiv preprint, 1506.07503",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Macho: Writing programs with natural language and examples",
      "author" : [ "A. Cozzie", "S.T. King" ],
      "venue" : "Technical report, University of Illinois at Urbana-Champaign",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Inductive programming",
      "author" : [ "P. Flener", "D. Partridge" ],
      "venue" : "Automated Softw. Engineering, 8(2):131–137",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Dimensions in program synthesis",
      "author" : [ "S. Gulwani" ],
      "venue" : "Proc. ACM SIGPLAN Symposium on Principles and Practice of Declarative Programming",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "General program synthesis benchmark suite",
      "author" : [ "T. Helmuth", "L. Spector" ],
      "venue" : "Proc. Genetic and Evol. Comput. Conf. ACM",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Long short-term memory",
      "author" : [ "S. Hochreiter", "J. Schmidhuber" ],
      "venue" : "Neural Comput., 9(8):1735–1780",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Turing machines are recurrent neural networks",
      "author" : [ "H. Hyötyniemi" ],
      "venue" : "Proc. STeP",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "The unreasonable effectiveness of recurrent neural networks",
      "author" : [ "A. Karpathy" ],
      "venue" : "http://karpathy.github.io/ 2015/05/21/rnn-effectiveness/",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Visualizing and understanding recurrent networks",
      "author" : [ "A. Karpathy", "J. Johnson", "F. Li" ],
      "venue" : "arXiv preprint, 1506.02078",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Pegasus: First steps toward a naturalistic programming language",
      "author" : [ "R. Knöll", "M. Mezini" ],
      "venue" : "OOPSLA",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "et al",
      "author" : [ "A. Kumar", "O. Irsoy", "J. Su" ],
      "venue" : "Ask me anything: Dynamic memory networks for natural language processing. arXiv preprint, 1506.07285",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Deep learning",
      "author" : [ "Y. LeCun", "Y. Bengio", "G. Hinton" ],
      "venue" : "Nature, 521(7553):436–444",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A deductive approach to program synthesis",
      "author" : [ "Z. Manna", "R. Waldinger" ],
      "venue" : "ACM Trans. Programming Languages and Syst., 2(1):90–121",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1980
    }, {
      "title" : "TBCNN: A tree-based convolutional neural network for programming language processing",
      "author" : [ "L. Mou", "G. Li", "Z. Jin", "L. Zhang", "T. Wang" ],
      "venue" : "AAAI Workshop",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Building program vector representations for deep learning",
      "author" : [ "L. Mou", "G. Li", "Y. Liu", "H. Peng", "Z. Jin", "Y. Xu", "L. Zhang" ],
      "venue" : "arXiv preprint, 1409.3358",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "On the difficulty of training recurrent neural networks",
      "author" : [ "R. Pascanu", "T. Mikolov", "Y. Bengio" ],
      "venue" : "arXiv preprint, 1211.5063",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "I. Sutskever", "O. Vinyals", "Q. Le" ],
      "venue" : "NIPS",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Automatically finding patches using genetic programming",
      "author" : [ "W. Weimer", "T. Nguyen", "C. Le Goues", "S. Forrest" ],
      "venue" : "ICSE",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Learning to execute",
      "author" : [ "W. Zaremba", "I. Sutskever" ],
      "venue" : "arXiv preprint, 1410.4615",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : ", algorithm discovery, programming assistance [6].",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 14,
      "context" : "propose deductive approaches [15], Flener et al.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 4,
      "context" : "inductive approaches [5]; these methods require human-designed specifications.",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 19,
      "context" : "Program generation by genetic programming [20, 7] can automatically search the space of candidate programs (inefficiently), but carefully chosen mutation or crossover operations should also be provided.",
      "startOffset" : 42,
      "endOffset" : 49
    }, {
      "referenceID" : 6,
      "context" : "Program generation by genetic programming [20, 7] can automatically search the space of candidate programs (inefficiently), but carefully chosen mutation or crossover operations should also be provided.",
      "startOffset" : 42,
      "endOffset" : 49
    }, {
      "referenceID" : 11,
      "context" : "Natural language programming, emerged in the past decade, is much like “pseudo-compiling,” where the natural language is of low-level abstraction [12, 4].",
      "startOffset" : 146,
      "endOffset" : 153
    }, {
      "referenceID" : 3,
      "context" : "Natural language programming, emerged in the past decade, is much like “pseudo-compiling,” where the natural language is of low-level abstraction [12, 4].",
      "startOffset" : 146,
      "endOffset" : 153
    }, {
      "referenceID" : 12,
      "context" : "At the meantime, the natural language processing (NLP) community is witnessing significant breakthroughs and amazing results in various tasks including question answering [13], machine translation [19], or image-caption generation [21].",
      "startOffset" : 171,
      "endOffset" : 175
    }, {
      "referenceID" : 18,
      "context" : "At the meantime, the natural language processing (NLP) community is witnessing significant breakthroughs and amazing results in various tasks including question answering [13], machine translation [19], or image-caption generation [21].",
      "startOffset" : 197,
      "endOffset" : 201
    }, {
      "referenceID" : 13,
      "context" : "Among a variety of machine learning methods, the deep neural network (also known as deep learning) is among recent groundbreaking advances, featured by its ability of learning highly complicated features automatically [14].",
      "startOffset" : 218,
      "endOffset" : 222
    }, {
      "referenceID" : 8,
      "context" : "Theoretical analysis shows that recurrent neural networks are equivalent to Turing machines [9].",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 17,
      "context" : "However, training RNNs in early years was difficult because of the gradient blowup or vanishing problem [18].",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 7,
      "context" : "Long short term memory (LSTM) units [8], or gated units [2] are designed to balance between retaining the previous state and memorizing new information at the current time step, making RNNs much easier to train.",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 1,
      "context" : "Long short term memory (LSTM) units [8], or gated units [2] are designed to balance between retaining the previous state and memorizing new information at the current time step, making RNNs much easier to train.",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 18,
      "context" : "design an RNN model for sequence to sequence generation [19].",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 9,
      "context" : "Successful applications include generating texts, music, or even Linux-like C code [10].",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 10,
      "context" : ", parenthesis pairing, indentation, etc [11].",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 18,
      "context" : "Figure 1: A sequence to sequence recurrent neural network, adapted from [19].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 2,
      "context" : "Attentionbased networks [3, 21], for example, are proposed recently to mitigate the problem of long input sequences that cannot be composed into a fixed-size vector.",
      "startOffset" : 24,
      "endOffset" : 31
    }, {
      "referenceID" : 16,
      "context" : "Our previous work learns programs’ vector representations, serving as a pretraining phrase in deep learning [17]; we also propose treebased convolutional neural networks to classify programs by functionality and detect source code of certain patterns [16].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 15,
      "context" : "Our previous work learns programs’ vector representations, serving as a pretraining phrase in deep learning [17]; we also propose treebased convolutional neural networks to classify programs by functionality and detect source code of certain patterns [16].",
      "startOffset" : 251,
      "endOffset" : 255
    }, {
      "referenceID" : 20,
      "context" : "use RNNs to estimate the output of a restricted python program [22].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 0,
      "context" : "leverage vector representations to suggest method names [1].",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 10,
      "context" : "train an RNN-based language model on C code, which maximizes the joint probability of a program [11].",
      "startOffset" : 96,
      "endOffset" : 100
    } ],
    "year" : 2015,
    "abstractText" : "This paper envisions an end-to-end program generation scenario using recurrent neural networks (RNNs): Users can express their intention in natural language; an RNN then automatically generates corresponding code in a characterby-character fashion. We demonstrate its feasibility through a case study and empirical analysis. To fully make such technique useful in practice, we also point out several crossdisciplinary challenges, including modeling user intention, providing datasets, improving model architectures, etc. Although much long-term research shall be addressed in this new field, we believe end-to-end program generation would become a reality in future decades, and we are looking forward to its practice.",
    "creator" : "LaTeX with hyperref package"
  }
}