{
  "name" : "1602.08128.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 2.\n08 12\n8v 1\n[ cs\n.S D\n] 2\n5 Fe\nb 20\n16\nKeywords: Mispronunciation, PCA"
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "With the advent of technology, many Computer Assisted Language Learning (CALL) and Computer Assisted Pronunciation Training (CAPT) tools are available in the market to help people learn a new language. Learning a new language can be a very difficult. Without proper feedback, it can be very frustrating. Most CALL tools focus on teaching new words or sentences using a listen-and-repeat procedure. These tools use modern automatic speech recognition (ASR) algorithms and provide visual aids such as spectrograms and waveforms as feedback. However, these tools are not geared towards identifying specific mispronunciations and consequently are a poor substitute for a human instructor. The work discussed in this paper focuses on mispronunciation detection.\nIn the last two decades, a significant amount of research has been carried out in the field of mispronunciation detection. The majority of the mispronunciation detection studies1–7 have extensively used ASRs based on statistical models, such as Hidden Markov Models (HMM). However, as Garćıa-Moral et al.8 point out, for HMM based systems “large databases are required to warrant relevant and statistically reliable results”. For mispronunciation detection, these systems have to train different models for correct pronunciations and mispronunciations. Recent works by Wang et al.,3 and Harrison et al.5 have shown improvement in detection accuracy, but these systems had to use extended recognition network by making use of cross-language phonological rules, i.e. rules that dictate how a learner’s first language affects his/her pronunciation of a second language. The extended recognition networks require additional models to be trained in order to account for the cross-language rules. The use of large database and a large number of models makes HMM-based systems computationally complex and costly. Furthermore, performance often suffers when the training set is limited.\nIn this paper, we present a mispronunciation detection system based on Principal Component Analysis (PCA) and show it is computationally efficient compared to HMM’s and performs well with limited training data. The approach used in this paper for detecting mispronunciation is to classify pronunciations, at both the word-level and syllable-level, as either acceptable or unacceptable.\nFurther author information: (Send correspondence to Zhenhao Ge) Zhenhao Ge: E-mail: zge@purdue.edu, Telephone: 1 317 457 9348 Sudhendu R. Sharma: E-mail: sharmasr@purdue.edu, Telephone: 1 405 334 9992"
    }, {
      "heading" : "2. PCA METHOD FOR PATTEN RECOGNITION",
      "text" : "PCA is a well-known method that has been successfully employed in high-dimensional recognition problems. One of the popular applications of PCA is for face recognition which has been studied extensively over the past 20 years.9–13 The fundamental idea behind PCA is encoding the most relevant information that distinguishes one pattern from another and eliminating dimensions with less information to reduce the computational cost.14\nThe mispronunciation detection work in this paper shares much in common with face recognition. Mispronunciation detection can be thought of as a pattern recognition problem, just like face recognition. Furthermore, Mel-Frequency Cepstral Coefficients (MFCCs) and spectrograms, which are the features that we are investigating, are also of high dimension. The similarities between mispronunciation detection and face recognition motivated our incorporating PCA-based face recognition techniques into our work. In the next section, we first review the application of PCA in face recognition, and then discuss the procedures of the PCA-based mispronunciation detection."
    }, {
      "heading" : "2.1 Review of Face Recognition using PCA",
      "text" : "The PCA approach for face recognition requires all data (face images), training data Dtrain, and testing data Dtest from multiple classes to have the same size (say N1 ×N2). The method tends to work best when the faces in all the images are located at the same position within the image. These data matrices are then vectorized to N1N2-dimensional column vectors. When implementing a PCA-based detection/classification system, like face recognition, given M vectorized training faces Γ1,Γ2, . . . ,ΓM (M ≪ N1N2) as the whole set of Dtrain, let Ψ = 1 M ∑M i=1 Γi be the mean face and let Φi = Γi−Ψ be the mean-shifted faces which form A = [Φ1,Φ2, ...,ΦM ]. C = AAT is the covariance matrix of A, and can supply up to M eigenvectors to represent the eigenspace of A. Each face in A is a linear combination of these eigenvectors. If selecting the most significant M ′ eigenvectors to form a sub-eigenspace of training data U = [u1, u2, . . . , uM ′ ] (N1N2 × M\n′), the representation of Φi in the M ′-dimensional eigenspace is Ωi = U\nTΦi and the projection of Φi in the eigenspace is Φ̂i = UΩi. The dimension of data is reduced from N1N2 to M ′ while preserving the most relevant information to distinguish faces.\nAfter the eigenspace U for the training data A is computed, the following two steps are used for face recognition:13\n1. Face detection: compute the Euclidean distance from test image Φj to its projection Φ̂j onto the eigenspace U ,\nedfes = ‖Φj − Φ̂j‖ , (1)\nwhere edfes is called “distance from eigenspace”. If edfes < Td, where Td is a detection threshold, the test image is verified to be a face and step 2 below is used for face classification.\n2. Face classification: compute the minimum Euclidean distance between Ωj and Ωk in Equation (2), where Ωj is the eigenspace representation of the test face Φj and Ωk is the averaged eigenspace representation of faces in class k, k = 1, 2, . . . ,K, where K is the number of classes.\nedies = min ‖Ωj − Ωk‖ k = 1, 2, . . . ,K (2)\nwhere edies is called “distance within eigenspace”. If edies < Tc, where Tc is the classification threshold, then the test image is classified as a face belonging to that class; otherwise, a new class of faces is claimed. Figure 1 illustrates the difference between edfes and edies."
    }, {
      "heading" : "2.2 Procedure for Mispronunciation Detection Using PCA",
      "text" : "In this work, the training and testing data consist of two disjointed classes of speech samples. The first class consists of samples from native speakers, all of which have correct pronunciations. The second class contains samples from non-native speakers with possible mispronunciations. Given an input test sample, the procedure used for mispronunciation detection can be divided into 3 steps:\nFigure 1: A simplified version of eigenspace to illustrate edfes and edies\n1. Word verification: compute edfes from the test sample Φj to the eigenspace UAll constructed from all samples in DAll.train, where DAll.train includes all native DN.train and non-native DNN.train samples. If edfes < Td, where Td is the threshold for word verification, then the test sample is considered “verified” and we proceed to step 2. Otherwise the test sample is “rejected” and the detection process stops.\n2. Native/non-native (N/NN) classification: compute edfes from the test sample Φj to the eigenspace UN constructed only from DN.train. If edfes < Tc, then classify the test sample as native and stop. Otherwise, classify it as non-native and proceed to step 3.\n3. Syllable-level mispronunciation detection: Divide the non-native test samples into syllables Φjk, k = 1, 2, . . . ,K, where K is the number of syllables for that test sample. Similarly divide native samples DN.train into syllables and call the k\nth syllable DNk.train. For each test syllable Φjk, compute its edfes to the eigenspace UNk constructed from k\nth syllable of native samples DNk.train. If edfes > Tk, then classify the test syllable as mispronounced, otherwise classify it as correctly pronounced.\nWord verification is very similar to face detection, since both cases use the distance metric edfes of the eigenspace UAll which is constructed from all classes of samples. However, N/NN classification is different from face classification. The main difference is that in face classification edies is used as the metric to determine the class of the “test” face, where as in mispronunciation detection we use edfes as the metric to do the N/NN classification. In face classfication, the various classes of faces are well defined. If edies of a “test face” lies within the predefined threshold of some class k, then it is classified as face k. Otherwise, it is claimed to be a “new face” that does not match any of the faces. The “new face” scenario for face classification is shown in Figure 2a, where Ωj is the “test” face. However, in N/NN classification, there are only two classes and any region outside the native class belongs to the non-native class. So the misclassification shown in Figure 2b will occur if edies is used to determine the class of Ωj . In Figure 2b, Ωj is in the non-native class but it is closer to native class. Thus, instead of using distance edies within eigenspace UAll, N/NN classification uses edfes to measure the distance from the eigenspace UN, which is a sub-eigenspace of UAll.\nThe syllable-level mispronunciation detection is an extension of the N/NN classification applied to syllables. Each “test syllable” is treated like a “test word” in step 2, and mispronunciation can be detected in the syllables using the same N/NN classification approach."
    }, {
      "heading" : "3. SYSTEM DESIGN AND IMPLEMENTATION",
      "text" : "Having reviewed the application of PCA in face recognition and how it can be applied to mispronunciation detection, the following section will discuss the implementation of PCA-based mispronunciation detection in detail, including database construction, data pre-processing, feature selection, eigenspace training, and detection threshold optimization."
    }, {
      "heading" : "3.1 Database Construction",
      "text" : "Although the algorithm is not language specific, Spanish was chosen for this work. The database used in this work is relatively small and contains only 10 Spanish words listed in Table 1. These words were selected by language experts to cover a variety of common mispronunciations exhibited by American speakers learning Spanish. There were 13 male speakers, 7 of them native speakers and 6 non-native. Each speaker repeated each of the 10 words 5 times.\nBecause of the limited size of the database, the Leave-One-Out method was used for training and testing and is further discussed in Section 3.4."
    }, {
      "heading" : "3.2 Data Pre-processing and Feature Extraction",
      "text" : "The PCA method requires centralized and uniform-size input features for training and testing. For centralization, periods of silence before and after the actual speech segment were removed using voiced/unvoiced detection. To make all samples uniform in size, the samples were time-scale modified to match the average duration of the training data. Furthermore, to improve the detection performance, the background noise was suppressed and the amplitude of each sample was normalized to unity.\nSpectrograms and MFCCs were chosen as input features to the detection system. These features were computed for frames with window size 25ms (using a Hamming window) and 15 ms overlap between frames. The spectrogram feature space is 50-dimensional with each dimension spanning 320 Hz to 16 KHz. The MFCC feature space was 13-dimensional representing the first 13 Mel-scale cepstral coefficients."
    }, {
      "heading" : "3.3 Eigenspace Training and Detection Threshold Optimization",
      "text" : "As discussed in Section 2.2, these three steps are: (a) word verification; (b) N/NN classification; and (c) syllablelevel mispronunciation detection. The detection system runs on a word-by-word basis. For each word, Wi∗ , i∗ = 1, 2, ..., 10, each step (a), (b), and (c) undergoes the following 3 phases of training.\n• Phase 1: Train to determine eigenspace U .\n• Phase 2: Compute two sets of distances e1 dfes and e2 dfes , where e1 dfes corresponds to the distances from “class 1” samples to the eigenspace U and e2\ndfes corresponds to the distance from “class 2” samples to U .\n• Phase 3: Find an optimal detection threshold T that separates these two sets of distances.\nEven though the 3 training phases are the same for the 3 steps (a), (b), and (c), each step has a different trained eigenspace, uses different class 1 and class 2 data, and generates different thresholds. Table 2 summarizes the difference in the 3 training phases of the 3 steps. In Table 2, the data used in training to obtain the eigenspace (2nd row) and to compute e1\ndfes (3rd row) are slightly different. Even though they share the same notation in\nthe table, they represent two disjoint subsets within the class 1 data. Furthermore, different eigenspaces are trained using the Leave-One-Out approach. In this approach, samples corresponding to one speaker are left out. The samples from remaining speakers are used to train the eigenspace, and the samples from the left out speaker are used to find e1\ndfes . This is repeated for all speakers belonging to class 1. As mentioned, this approach\nleads to several trained eigenspaces. The e2 dies distances are then computed for class 2 data to those different trained eigenspaces and averaged for each speaker in class 2. The distributions formed by e1\ndfes and e2 dfes , an\nexample shown in Figure 3, are then used to find the optimal threshold. By convention,15 the dimension of the eigenspace in each step is determined by selecting eigenvectors that represent 80% variance of the total principle components. For example, the dimension of the eigenspace used to determine edfes in Figure 3 is 18.\nThe main goal of the training process is to find the “optimal threshold” to detect mispronunciations in the test samples. In each step, since the test samples come from two classes of data which are supposed to be separated, the optimal threshold T should be the one that separates these two classes best. This is done using Bayes rule by finding the optimal threshold to separate two sets of distances e1\ndfes for class 1 and e2 dfes for class 2\ndata. Let random variables X1 and X2 represent e 1 dfes and e2 dfes and assume they are both Gaussian distributed with prior probabilities P (ω1) and P (ω2), where ω1,2 denote the classes of these two groups. The classification error can be computed by Equation (3) using Bayes rule\nP (error) =\n∫ ∞\nx∗ p(x | ω1)P (ω1)dx+ ∫ x∗ −∞ p(x | ω2)P (ω2)dx (3)\nwhere the optimal threshold x∗ or T can be found by computing the discriminant function g(x) at g(x) = 0 where\ng(x) = p(x | ω1)P (ω1)− p(x | ω2)P (ω2) (4)\nFigure 3 illustrates the numerical distribution of e1 dfes (top line) and e2 dfes (bottom line) as an example of word verification step for the word tres. The distances are averaged at 5 repetitions for each speaker. Figure 4 plots the corresponding theoretical Gaussian distribution in order to obtain a more reliable optimal threshold. MFCCs were used as the feature set here. Note that since there are 9 words from class 2 and only 1 word from class 1, distance e1\ndfes has been repeated 9 times to make them comparable to the e2 dfes . The optimal threshold\nTd can be found by computing the minimum error rate of the theoretical distribution (Gaussian distribution assumed) shown in Figure 4. By Bayes rule, the threshold is optimal when it provides the theoretical minimum classification error P (error) (0.030% in this case). The P (error) computed using spectrograms is similar but a little higher (0.447%).\n0 20 40 60 80 100 120 60\n80\n100\n120\n140\n160\n180\n200\n220\n240\n# of samples (13 speakers x 9 words)\ne d fe\ns\nNumerical Distribution of e dfes in system design (step 1)\npca(18), MFCC, p(error): 0.030%\ntres (target) other words (non−target)\nFigure 3: Numerical distribution of class 1 and class 2 data in word verification (target word: tres, feature: MFCCs )\n80 100 120 140 160 180 200 220 240 0\n0.005\n0.01\n0.015\n0.02\n0.025\n0.03\n← x* = 113.5342\nTheoretical Distribution of e dfes in system design (step 1)\npca(18), MFCC, p(error): 0.030%\ne dfes (x)\np( x|\nw i)*\np( w\ni)\ntres (target) other words (non−target)\nFigure 4: Theoretical distribution of class 1 and class 2 data in word verification (target word: tres, feature: MFCCs )\nFigure 5 illustrates the native/non-native distribution of data in N/NN classification step for word pala. The P (error) obtained using MFCCs (Figure 6) and spectrograms are 0.001% and 11.65% respectively. Experimental results shows that MFCCs are much better than spectrograms in separating data."
    }, {
      "heading" : "4. SYSTEM TESTING AND RESULTS",
      "text" : "After eigenspace training and detection threshold optimization, the mispronunciation detection system is built up. The following section presents the system testing results in each step."
    }, {
      "heading" : "4.1 Leave-One-Out Training and Testing",
      "text" : "Because of the relatively small size of the database, the Leave-One-Out (LOO) method is used for training and testing. Traditionally in LOO, all but one sample is used in training and the left out sample is used for testing. In our case, samples belonging to one speaker (i.e. all 5 repetitions) are left out for testing and all samples belonging to all other speakers are used in system training, which includes the 3 phases discussed in the Section 3.3: eigenspace construction (U), 2-class distance measurement (e1\ndfes ,e2 dfes ), and detection threshold optimization\n(Td, Tc and Tk).\nFor example, in the word verification step, the goal is to verify whether or not the “test word” is the target word Wi∗ . The number of samples available for training and testing in this step is 650 (10 different words × 13 speakers × 5 repetitions). Using the LOO method, 5 repetitions of word Wi∗ from one speaker are left out for\n1 2 3 4 5 6 60\n70\n80\n90\n100\n110\n120\n130\n# of speakers, 6 native (N) and 6 non−native (NN)\ne d fe\ns Numerical Distribution of e dfes in system design (step 2) pca(11), MFCC, SM004, p(error): 0.001% pala native (class1) pala non−native (class2)\nFigure 5: Numerical distribution of edfes of the class 1 and class 2 data in N/NN classification (target word: pala, feature: MFCCs )\ntesting and the remaining 645 samples are used to train the system and obtain the optimal threshold Td. This is repeated for each speaker. At the end, there are 130 trained system/threshold combinations and 5 test samples for each system to be validated.\nIn the native/non-native classification, the number of samples available for training and testing is 65 (13 speakers × 5 repetitions of the target word). Five repetitions from one speaker are left out for testing and the remaining 60 samples are used to train the system and obtain the optimal threshold Tc. At the end, there are 13 system/threshold combinations and 5 test samples for each system to be validated."
    }, {
      "heading" : "4.2 Results of Word Verification and N/NN Classification",
      "text" : "Compared with the theoretical error rate in Equation (3), the performance of the mispronunciation detection system is measured by the numerical error rate Pe\nPe = Ne1 +Ne2 N1 +N2 , (5)\nwhere N1, N2 are the number of test samples from class 1 and 2, and Ne1, Ne2 are the number of misclassified samples from each class.\nIn word verification, the error rate Pe is always below 3% and 7% for MFCCs and spectrograms respectively. In N/NN classification, the performance based on HMMs using MFCCs is also compared along with the PCA method. Figure 7 and 8 show the results of the Leave-One-Out method applied to the word aire using PCA and HMMs. The 13 columns in the figure represent the 13 speakers of the word aire. For each column (speaker), there are 5 samples, which are compared against the threshold. The bottom line corresponds to the 7 native speakers and the top line corresponds to the 6 non-native speakers. The thresholds during each LOO iteration vary slightly because of small differences in the training database during each trial. These variations diminish as the database size increases. For the PCA method using MFCCs, there is 1 sample from both native speaker 4 and 6 in Figure 7 that are slightly above the threshold and misclassified as non-native and all the rest from both classes are correctly classified. A similar situation is illustrated in Figure 8 with slightly higher Pe. Complete word verification and N/NN classification results of all 10 words are provided in Table 3.\nIn Table 3, MFCCs are shown to perform much better than spectrograms using PCA, especially in step 2, N/NN classification. The PCA method performs slightly better on average than HMMs in this data-limited case. However, general parameters of HMMs are used and they are not optimized for this specific system. With respect to the computational cost, training an eigenspace is almost 103 times faster than training HMMs (5 ms\nvs. 5 s), while in sample testing, the PCA method is also 60-80 times faster (about 4 ms vs. 250 ms) than HMMs depending on the length of the word.\n10 20 30 40 50 60 1500\n2000\n2500\n3000\n3500\n# of test samples, 35 native (N) and 30 non−native (NN)\nNumerical Distribution of negative log−likelihood in system test (step 2) hmm(20), MFCC, p(error): 6.154%\naire native (class1) aire non−native (class2) optimal threshold\nFigure 8: Numerical distribution of negative loglikelihood of the test samples in N/NN classification using HMM (target word: aire, feature: MFCCs )"
    }, {
      "heading" : "4.3 Results of Syllable-Level Mispronunciation Detection",
      "text" : "For syllable-level mispronunciation detection, an approach similar to the N/NN classification is followed, except that it is done at a syllable level. Though in real applications, only samples that are classified as non-native in the N/NN classification would be processed by the 3rd step, to make testing results comparable and unbiased, all test samples including native samples are also used here for evaluation.\nThere is a major difference between this step and the previous two steps. This is because the assumption that two classes of data in eigenspace training and edfes computing are separated is no longer valid, namely, some syllables may be pronounced well enough that can not be used to distinguish native and non-native. If this is the case, the threshold obtained using Bayes rules is biased and moves towards the native class, which dramatically increases the classification error rate Pe. Thus, Pe cannot be used to measure the performance of mispronunciation detection. Instead, it serves as the similarity measurement of the syllables pronounced by both native and non-native classes. By dividing the total error rate Pe into False Negative Rate (FNR) and False Positive Rate (FPR) in Equation (6), it is easy to find that with a relatively low FNR, higher FPR indicates better pronunciation of the syllable, while lower FPR shows the problematic syllable that one should pay attention to.\nFNR = Ne1\nN1 and FPR =\nNe2 N2 (6)\nFigure 9 and 10 illustrate two examples where “good” (FPR = 80%) and “bad” (FPR = 23.3%) mispronunciations are detected. Table 4 shows FNR/FPR for each syllable and highlights all FPR ≤ 30%. From Table 4, common mispronunciation problems16 like vowel reduction, aspiration, linkage and stress are successfully detected in the test database. Some of them are listed below:\n• Vowel Reduction: /pa/ in pala; /den/ in accidente; /ie/ in arie\n• Aspiration: /pa/ in pala; /puer/ in puertorriquena\n• Linkage: /tr/ in tres; /cons/ in construccion; /na/ in puertorriquena\n• Stress: /ci/ in accidente; /cons/ and /cion/ in construccion\n10 20 30 40 50 60 30\n40\n50\n60\n70\n80\n90\n100\n110\n# of test samples, 35 native (N) and 30 non−native (NN)\ne d fe\ns\nNumerical Distribution of e dfes in system test (step 3)\npca(7), MFCC, Syllable: 1, FNR: 2.857%, FPR: 80.000%\njamaica, s(1), native (class1) jamaica, s(1), non−native (class2) optimal threshold\nFigure 9: Numerical distribution of edfes of the test samples in mispronunciation classification using PCA (target syllable: jamaica, /ja/, feature: MFCCs )"
    }, {
      "heading" : "5. CONCLUSION AND FUTURE WORK",
      "text" : "The testing results in Section 4 show that PCA can be a computationally efficient approach to detect mispronunciations, even when the training and testing database is limited. MFCCs were shown to outperform spectrograms. Compared with HMMs, the PCA method is much faster and achieves comparable results for the database used in this paper.\nFor future work, the threshold in each step should be optimized on a larger database to improve robustness. This is especially important for optimizing the threshold in the syllable-level detection, since subtle differences that reside in syllables require more data to differentiate.\nBecause of the constraints of the PCA method in data separation (discriminant information may stay in the less significant components), further investigation on the distributions of multiple types of mispronunciations is warranted. Furthermore, hybrid methods based on PCA, LDA (Linear Discriminant Analysis) and ICA (Independent Component Analysis) should also be considered."
    } ],
    "references" : [ {
      "title" : "Recognition and pronunciation scoring for language learning,” in [Proc",
      "author" : [ "H. Franco", "V. Abrash", "K. Precoda", "H. Bratt", "R. Rao", "J. Butzberger", "R. Rossier", "F. Cesari", "“The sri eduspeak system" ],
      "venue" : "of InSTIL ], 123–128",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Automatic detection of mispronunciation for language instruction,",
      "author" : [ "O. Ronen", "L. Neumeyer", "H. Franco" ],
      "venue" : "in [Proc. of Eurospeech ],",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1997
    }, {
      "title" : "Mispronunciation detection based on cross-language phonological comparisons,",
      "author" : [ "L. Wang", "X. Feng", "H.M. Meng" ],
      "venue" : "[IEEE IET International Conference on Audio, Language and Image Processing ],",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2008
    }, {
      "title" : "Improving mispronunciation detection and diagnosis of learners’ speech with context-sensitive phonological rules based on language transfer,",
      "author" : [ "A. Harrison", "W. Lau", "H. Meng", "L. Wang" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2008
    }, {
      "title" : "Implementation of an extended recognition network for mispronunciation detection and diagnosis in computer-assisted pronunciation training,",
      "author" : [ "A. Harrison", "W. Lo", "X. Qian", "H. Ming" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2009
    }, {
      "title" : "A new method for mispronunciation detection using support vector machine based on pronunciation space models,",
      "author" : [ "S. Wei", "G. Hu", "Y. Hu", "Wang", "R.-H" ],
      "venue" : "Speech Communication 51(10),",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2009
    }, {
      "title" : "Capturing l2 segmental mispronunciations with joint-sequence models in computer-aided pronunciation training (CAPT),",
      "author" : [ "X. Qian", "H. Meng", "F. Soong" ],
      "venue" : "in [International Symposium on Chinese Spoken Language Processing (ISCSLP) ],",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2010
    }, {
      "title" : "Data balancing for efficient training of hybrid ANN/HMM automatic speech recognition systems,",
      "author" : [ "A.I. Garćıa-Moral", "R. Solera-Ureña", "C. Peláez-Moreno", "F.D. de Maŕıa" ],
      "venue" : "IEEE Transactions on Audio, Speech & Language Processing",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "A low-dimensional procedure for the characterization of human faces,",
      "author" : [ "L. Sirovich", "M. Kirby" ],
      "venue" : "The Journal of the Optical Society of America",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1987
    }, {
      "title" : "Application of the karhunen-loeve procedure for the characterization of human faces,",
      "author" : [ "M. Kirby", "L. Sirovich" ],
      "venue" : "IEEE Trans. on Pattern Analysis and Machine Intelligence",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1990
    }, {
      "title" : "Recognition using class specific linear projection,” IEEE Trans",
      "author" : [ "P. Belhumeur", "J. Hespanha", "D. Kriegman", "“Eigenfaces vs. fisherfaces" ],
      "venue" : "on Pattern Analysis and Machine Intelligence 19(7), 771–720",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Face recognition using curvelet based pca,",
      "author" : [ "T. Mandal", "Q.M.J.Wu" ],
      "venue" : "in [Pattern Recognition,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2008
    }, {
      "title" : "Eigenfaces for recognition,",
      "author" : [ "M. Turk", "A. Pentland" ],
      "venue" : "Journal of Cognitive Neuroscience",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1991
    }, {
      "title" : "A tutorial on principal component analysis,",
      "author" : [ "J. Shlens" ],
      "venue" : "tech. rep., Institute for Nonlinear Science,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2005
    }, {
      "title" : "Principal variance component analysis,",
      "author" : [ "S.A. Batch" ],
      "venue" : "http://www.niehs.nih.gov/research/resources/software/pvca",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2010
    }, {
      "title" : "The Sounds of Spanish: Analysis and Application with special reference to American English",
      "author" : [ "R.M. Hammond" ],
      "venue" : "Cascadilla",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2001
    } ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "This paper presents a method for detecting mispronunciations with the aim of improving Computer Assisted Language Learning (CALL) tools used by foreign language learners. The algorithm is based on Principle Component Analysis (PCA). It is hierarchical with each successive step refining the estimate to classify the test word as being either mispronounced or correct. Preprocessing before detection, like normalization and time-scale modification, is implemented to guarantee uniformity of the feature vectors input to the detection system. The performance using various features including spectrograms and Mel-Frequency Cepstral Coefficients (MFCCs) are compared and evaluated. Best results were obtained using MFCCs, achieving up to 99% accuracy in word verification and 93% in native/non-native classification. Compared with Hidden Markov Models (HMMs) which are used pervasively in recognition application, this particular approach is computational efficient and effective when training data is limited.",
    "creator" : "LaTeX with hyperref package"
  }
}