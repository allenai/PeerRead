{
  "name" : "1611.10338.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "SLA Violation Prediction In Cloud Computing: A Machine Learning Perspective",
    "authors" : [ "Reyhane Askari Hemmat", "Abdelhakim Hafid" ],
    "emails" : [ "reyhane.askari.hemmat@umontreal.ca", "ahafid@iro.umontreal.ca" ],
    "sections" : [ {
      "heading" : null,
      "text" : "I. INTRODUCTION The usage of cloud systems have been so prevalent that it is hard to picture to use many services and applications without cloud computing. Cloud computing reduces the maintenance costs of the service and also allows users to access on demand services without being involved in technical implementation details. The relationship between a cloud provider and a customer is governed with a Service Level Agreement (SLA) that is established to define the level of the service and its associated costs. SLA usually contains specific parameters and a minimum level of quality for each element of the service that is negotiated between the provider and the customer [1].\nAn SLA is an important part of each contract because a provider would like to allocate the least amount of resources for each customer to reduce the cost of its server infrastructure. At the same time, the provider needs to avoid having penalties due to failure of providing the agreed service. The failure of providing a service is called an SLA violation. On the other hand, the customer would like to receive the service on demand and without any interruptions. Despite these high availability rates, violations do happen in real world and have caused both the provider and the customer heavy costs.\nAccording to [2], SLA management has six phases: SLA contract definition, basic schema with the Quality of Service (QoS) parameters, SLA negotiation, SLA monitoring, SLA violation detection and SLA enforcement. An essential part of SLA monitoring is to be able to predict violations enabling\nproviders to reallocate the resources accordingly before occurrence of violations. Moreover, from a customer’s point of view, a trusted provider can be chosen based on the provider’s future violations.\nIn this paper, through a set of experiments, we identify best performing Machine Learning models which are able to predict SLA violations on a real world dataset. It is worth mentioning that due to skewness of real world datasets1, it is a challenging problem to predict violations. We report our results on a subset of the Google Cloud Cluster trace dataset [3]. The results presented in Section VII show that the best performance is achieved using the Random Forest [4] method with an accuracy of 99.88%.\nThe remainder of this paper is organized as follows. Section II describes the previous works on SLA violation prediction. Section II describes our dataset which was based on Google Cluster Trace published on 2011. Section IV defines SLA violation and how we can find them in the dataset. Section V explains the two models that we used for the prediction task. Section VI describes our experimental setup including how we overcome data skewness and what kind of error metrics are used. In section VII we discuss the our results and finally, Section VIII concludes the paper and presents future work."
    }, {
      "heading" : "II. RELATED WORKS",
      "text" : "Many models have been proposed in recent years to tackle SLA management problems. Imran et al. [5] use a map-reduce model to detect violations and find the most probable causes of SLA violations using Holt-Winters forecasting. In [6], the authors proposed an SLA aware resource scheduling model that determines how to allocate coming requests without explicit prediction of violations. Similarly, in [7] , Mohammed et al. proposed an SLA-based trust model for cloud that selects the provider based on a selection scheme; in this scheme, although no violation is predicted, the customers are grouped according to business needs and the most trusted cloud provider is selected based on the customer’s non-functional requirements.\nIn a similar work for predicting SLA violations in composite services, in [8] the authors propose a regression machine\n1For example, Google Cloud Cluster has 99.90% availability versus 0.10% violations.\nar X\niv :1\n61 1.\n10 33\n8v 1\n[ cs\n.D C\n] 3\n0 N\nov 2\n01 6\nlearning model; the regression model is implemented using the WEKA framework which cannot be scaled to real world environments where the scale of the dataset is much bigger compared to the one which is used in our paper.\nIn [9], the authors proposed a model for predicting host load using real data of Google Compute Cluster. A Bayesian model is used to predict the mean load over long-term and consecutive future time intervals. Although the load could be predicted, there is no detection of SLA violations in the paper.\nThe authors in [10] propose a provisioning method that monitors and also predicts future loads. Based on the predicted loads, an autonomous elasticity controls the number of allocated virtual machine to a job. While no explicit detection of violations are predicted, the number of delayed requests was lowered by a factor of three.\nThe authors in [11], use unsupervised learning to cluster the resource usage and duration of services to avoid violations of Google Cluster trace dataset. If a violation happens inside a cluster of services, the other services inside the cluster will be assigned resources to avoid the violation. This helps in violation avoidance in the cluster but there is no specific prediction of SLA violations for each service.\nThe authors in [12] use a Naive Bayes model to predict SLA violations. Despite its good performance, the dataset is generated using simulation. It contains 40% violations and neglects the fact that in real world, violations are very rare (0.1%)."
    }, {
      "heading" : "III. DATASET",
      "text" : "The dataset that we report our results on contains 29-day trace of Google’s Cloud Compute. For security reasons, part of the trace has been omitted or obfuscated. For example, the values for CPU, disk and memory have been rescaled by dividing each value by their corresponding largest value in the trace. Also the names of the users’ applications have been hashed. The trace has six separate tables: Job Events, Task Events, Task Usage, Machine Events, Machine Attributes, and Task Constraints. The entity relationship diagram of the database is shown in Figure III.\nUser’s application submits its required resources as jobs to the cluster and each job is consists of several tasks. The state transition diagram of jobs and tasks is depicted in Figure 2. The Job Events table traces the event cycle of the jobs that were submitted to the cluster. The tasks inside each job are tracked in Tasks Events table. Each task is then assigned to a specific machine. Machine Events table shows removal or addition of a machine to the cluster or update of its resources. Machine attributes table shows the attributes of each machine such as kernel version, clock speed and presence of an external IP address[3]. Tasks can have constraints on machine attributes which are recorded in the Tasks Constraints table.\nInformation such as requested CPU, requested memory, requested disk space, scheduling class and priority of the task are all recorded inside tasks events table. The Task Usage table contains the actual usage of resources for each task. It contains information such as assigned memory and memory usage.\nFigure 3 illustrates the mechanism of resource allocation in Google’s Cluster. It shows the state of the cluster at 500 random snapshots. We define a snapshot as a moment in time when the total requested resources is calculated. Similarly, available or allocated resources are calculated at each snapshot. In Figure 3 the total requested memory, assigned memory, memory usage and available memory of the cluster at each snapshot is calculated using the Task Events, Task Usage and Machine Events tables. It is the nature of cloud to allocate less resources than requested resources and even accept more requests than its available resources. Figure 3 shows that at all\nof the 500 snapshots of the cluster, the requested memory to the cluster is much higher than the actual usage of memory.\nAnother important characteristic of our data set is its skewness. The task of violation detection can be simply considered as a classification problem where we want to predict whether at a specific time in the future the provider will have violation or not. Since the availability rate is very high (97.8%) and violations do not happen most of the time, the engine have the tendency of always predicting the violation as false. There has been some methods in machine learning to handle such data such as over sampling and under sampling; also it has been recommenced to use generative models[13]."
    }, {
      "heading" : "IV. SLA VIOLATION DEFINITION",
      "text" : "In order to identify SLA violations we need to have specific details of QoS parameters and Service Level Objectives (SLOs). SLOs are quantitative parameters of an SLA such as availability, throughput and response time. Although we do not have access to the details of SLA for this dataset, we can find violations in the availability of the service using the trace.\nFigure 2 shows the state transition diagram for jobs and tasks in the trace. We define a violation in the availability of the service when a task is evicted and never re-scheduled after that. According to the documentation of the trace [3], eviction of a task is due to ”overcommiting of the scheduler or because the machine on which it was running became unusable (e.g. taken offline for repairs), or because a disk holding the tasks data was lost.” Thus, all the tasks that were evicted and never re-scheduled were detected as cases of violations. The percentage of not evicted tasks to the total tasks submitted to the cluster is 97.8%. Thus, the cluster has only 2.2% violations. Our goal is to use this data and predict future violations. Features such as the amount of requested CPU, disk and memory of the violated tasks and also the available resources at the time of request can be studied to predict future violations."
    }, {
      "heading" : "V. PREDICTION MODELS",
      "text" : "We formulate SLA violation detection as follows: Given a set of features extracted from traces of the cluster, what is the probability of failure2 of a submitted task.\nTo work towards this goal, we establish a classification model and use two algorithms as the core classifier: Naive Bayes models and Random Forest Model."
    }, {
      "heading" : "A. Naive Bayes Models",
      "text" : "From a probabilistic point of view, the conditional probability of class k among K different classes given a vector representation of n distinct features x = {x1, ..., xn} can be written as P (Ck|x). According to the Bayes theorem [14], the above probability can be reformulated as follows:\nP (Ck|x) = P (x|Ck)P (Ck)\nP (x) , (1)\nin which P (Ck|x) is called the posterior meaning our updated knowledge conditioned on the observed data. Two probabilities P (x|Ck) and P (Ck) are called the likelihood and the prior respectively.\nIn a classification setup, the denominator P (x) is constant. In practice, training such a Bayesian classifier amounts to maximizing the nominator for the target class and minimizing it for the other classes. The nominator is the joint probability of features and classes P (Ck, x1, ..., xn) which according to the chain rule, can be reformulated as follows:\nP (Ck, x1, ..., xn) =P (Ck)∗ P (x1|Ck)∗ P (x2|x1, Ck)∗ P (x3|x2, x1, Ck)∗ ...\nP (xn|xn−1, ..., x1, Ck).\nConsequently, since P (Ck|x1, ..., xn) ∝ P (Ck, x1, ..., xn), a classifier can be defined as follows:\nĉ = argmax k∈{1,...,K} P (Ck, x1, ..., xn).\nIn practice, for large number of features, n, it is challenging to train such a classifier. One of the simple, yet effective probabilistic classifiers is known as Naive Bayes. Naive Bayes algorithm has an assumption that given the class label Ck, all the features {x1, ..., xn} are independent of each other. The adjective naive comes from the fact that the assumption of class conditional independence is simplistic. A graphical illustration of the this classifier is shown in Figure 4."
    }, {
      "heading" : "B. Random Forest Model",
      "text" : "Decision Tree is the main building block of the Random Forest model. As a result, we first briefly introduce Decision Tree and then we explain the Random Forest.\n2failure is defined in the section IV.\n1) Decision Tree: Decision Tree is a family of scalable classifiers that enjoys the advantage of human-interpretable results. Formally, a classification decision tree is a tree in which each leaf represents a target class, each internal node represents a condition, and each branch corresponds to the outcome of the condition in the parent node.\nAs a simple example, consider a set of features {Outlook,Humidity,Wind} and the target is PlayTennis which takes values of “Yes” or “No”. A trained decision tree is shown in Figure 5.\n2) Decision Tree Learning: Construction of a decision tree amounts to finding the appropriate conditions as nodes and ordering them from root to the leaves. Conditions are a test on one of the features of the given datapoint. Among different types of tests, we use Gini Impurity on each feature as the criterion that splits nodes to their children. Gini impurity measures the probability of being wrongly classified for a random datapoint, if the classification is based on the distribution of the targets.\n3) Random Forrest: From a geometrical point of view, a decision tree leads to a hierarchical portioning over the feature space. Starting from the top most node in the tree, each node divides the feature space into two or more partitions. Consequently, as the tree gets deeper, more complicated partitioning is done. However, in the case of over-fitting, the partitioned space is over complicated that yields to small error on the training data while a relatively larger error on the test data.\nOne of the successful ways to overcome the issue of overfitting is Random Forrest. Random Forrest amounts to using\nan “ensemble” of decision trees and aggregating their results in order to get a more robust prediction.\nEach of the decision trees in random forest is constructed on a subset of the data that is achieved by sampling with replacement from the original dataset. For each decision tree, a different random subset of features is used. In the final stage and aggregation bagging used."
    }, {
      "heading" : "VI. EXPERIMENTAL SETUP",
      "text" : "In our experiments, we fed the historical data to Naive Bayes and Random Forest machine learning models to predict future violations. The task of prediction is modeled as a classification task where we have two classes. Class zero (violation=0) is the case of unviolated tasks and class one (violation=1) is the case of violated tasks. Since the availability rate is very high (97.8%), our classes are highly unbalanced. This is known as skewness in dataset which makes the classification task very hard because the classifier will always have the tendency to predict the dominant class."
    }, {
      "heading" : "A. Overcoming Data Skewness",
      "text" : "As depicted in Figure 6, we have experimented several models to re-sample the database and create a more balanced dataset: Random over-sampling, Random Under-sampling, SMOTE (Synthetic Minority Over-sampling Technique) [15], Near Miss 1, 2 and 3, One-sided Selection, and Neighborhood Cleaning Rule.\n1) Random Over-sampling: Both Random Over-sampling and Random Under-sampling are baseline methods to combat the skewness of the dataset and balance the class distribution. In case of over-sampling, we randomly duplicate the samples of the minority class to have roughly the same number of datapoints in both classes.\n2) Random Under-sampling: Random Under-sampling is the case of randomly deleting data points from the dominant class until both classes have roughly the same size. This method might delete the datapoints in the decision boundary that are important in the process of decision making.\n3) SMOTE [15]: is a re-sampling method that generates new synthetic datapoints of the minority class using interpolation between the current datapoints. This may cause adding new datapoints in the space of the majority class.\n4) Tomek links [16]: is an under sampling method. Tomek links removes the borderline and noisy datapoints. It takes two samples, Ei and Ej and computes d(Ei, Ej) as their distance. A (Ei, Ej) is a Tomek link if there is no sample Ek that d(Ei, Ek) < d(Ei, Ej) or d(Ej , Ek) < d(Ei, Ej). After finding the links, the datapoints from the dominant class are be removed.\n5) One-sided Selection [17]: uses the combination of Tomek links and CNN (Condensed Nearest Neighbor Rule) to find the safe samples and removes the unsafe samples from the majority class. Tomek links removes the samples near the border line and CNN removes the samples that are far from the border line.\n6) Neighborhood Cleaning Rule [18]: removes some datapoints from the majority class. It finds the three nearest neighbors of each datapoint; if its neighbors belong to the majority class and the datapoint is from the minority class, then the neighbors will be removed. It the neighbors belong to the minority class and the datapoint is from the majority class,\n7) NearMiss [19]: NearMiss 1, 2 and 3 algorithms are under-sampling methods. NearMiss 1 chooses the datapoints from the majority class whose average distance to three closest datapoints in the minority class is the smallest. NearMiss 2 chooses the majority class datapoints whose average to all datapoints in the minority class is the smallest. NearMiss 3 selects a given number of majority class datapoints for each datapoint in the minority class.\n8) SMOTE-Tomek links [20]: Since SMOTE over-sampling might lead to over-fitting and Tomek links under sampling\nmight remove important datapoints, the ensemble of these two methods provide better results. In SMOTE-Tomek links, we first over-sample the minority class with SMOTE and then under-sample both the majority and minority classes producing a more balanced dataset.\n9) SMOTE-ENN [21]: is also the ensemble of SMOTE and ENN. SMOTE is used as the over-sampler for minority class and then ENN provides data cleaning for both classes."
    }, {
      "heading" : "B. Error Metrics",
      "text" : "In order to measure the performance of the models, error metrics are required. To show that the results are not biased and will roughly remain the same with new data, the dataset is randomly split into two sets; training and test. The training set is fed to a machine learning model. The trained model is then used to predict violations on the test set which was intact during the training and its true target values are known. This approach helps us select a model which will have good performance on unseen data.\nTo split the dataset into train and test sets, 3-fold cross validation is used. The dataset is randomly split into three partitions and the prediction model is trained three times. During each training, two-third of the dataset is used as the training set and fed to the model and one-third as the test set. The aggregated results of the three runs on the model will be reported as the final result.\nIn this paper, five different error metrics have been used to measure the performance of the models: Accuracy, Receiver Operating Characteristic (ROC) curves, Precision, Recall and Fβ score. Let us first define the Confusion Matrix [22] which will help to define the above-mentioned metrics.\nAs shown in Figure 8, a Confusion Matrix contains four values to describe the performance of a classification model: false positive, false negative, true positive, and true negative. False positive (resp. negative) is the number of mistakenly classified examples that are classified as 1 (resp. 0) where the actual targets are 0 (resp. 1). Similarly, true positive (resp. negative) is the number of correctly classified examples that are classified as 1 (resp. 0). Generally speaking, the first term (false or true) indicates if the classification result matches with the actual target. The second term (negative or positive) indicates the prediction of the classifier. Based on the confusion matrix, each of the metrics above are defined as follows:\naccuracy = True positives + True negatives\n# of all examples , (2)\nprecision = True positives\n# of all positive predictions , (3)\nrecall = True positives\n# of all positive examples , (4)\nFβ = (1 + β 2) . precision . recall (β2 . precision) + recall . (5)\nFinally, Receiver operating characteristic (ROC) curve is a visualization of the performance of a binary classifier as its\ndiscrimination threshold changes. The discrimination threshold is the cut-off applied on the predicted probability of a test example that assigns it to a particular class. In an ROC curve, true positive rate and false positive rate are plotted on vertical and horizontal axis respectively."
    }, {
      "heading" : "VII. RESULTS AND DISCUSSION",
      "text" : "Table I in Appendix A shows the results for different models and different sampling methods. A summary of the results showing F1 score is presented in Figure 6. The ROC curves of the Random Forest Classifier are also shown in Figure 7.\nIt is worth mentioning why error metrics other than accuracy are used. In skewed datasets, accuracy can not be a good error metric to find the best performing classifier. Two classes are available: 0.2% of the samples are represented as violated class and 0.98% of the samples are represented as unviolated.\nConsider a classifier that predicts there will be no violations. It has an accuracy of 0.98% but Precision and Recall of zero. Thus, precision, recall and fβ score will help us find the better performing algorithm.\nThe best performing model in terms of F1 score is the random forest classification algorithm on the dataset re-sampled using the SMOTE + ENN method. According to the trained model, the five features shown in Figure 9 are sorted based on their contributions in classification task.\nOur intuition is that the Random Forest has better performance because tree based classifiers are less sensitive to class distributions. Thus, even with no re-sampling technique it has an acceptable performance (accuracy = 0.97% and f1 = 0.79). On the other hand, Naive Bayes classifiers are highly biased with class distribution and do not have any acceptable results without re-sampling techniques.\nAmong re-sampling methods, ensemble methods such as SMOTE-ENN, SMOTE-Tomek links and SMOTE-Borderline 1,2 had better results. SMOTE-ENN has better performance and our intuition is that because ENN removes more examples than the Tomek-Links, it provides more in depth data cleaning rule and removes any sample whose three nearest neighbors is miss-classified, which helps with better re-sampling the dataset."
    }, {
      "heading" : "VIII. CONCLUSION AND FUTURE WORKS",
      "text" : "The paper systematically compares the performance of two machine learning classification models on the task of SLA violation prediction. As discussed, in such a classification task, the data is skewed meaning that the number of violated tasks are much less than the number of unviolated ones. Consequently, the paper also explores several methods of handling unbalanced data. Results in section VII suggest that, the random forest algorithm performs the best when SMOTE + ENN is used as the over-sampling method. Among other proposed models for tasks of SLA violation prediction or avoidance, our models are trained on a real world dataset which introduces new challenges that have been neglected in previous works, to the best of our knowledge. It is worth mentioning that the random forest model is not a black-box and the trained model is human-interpretable as the results suggests that mem_requested is the most important feature in predicting violations. Moreover, thanks to the relatively high speed of random forest, it can be used real-time.\nDespite the impressive results achieved by random forest, one drawback of random forest is that it is not trivial to update the knowledge representation of the model based on the new coming examples. One of the future works might be to explore other models that can be easily updated when receiving more training data. Another remaining question in the area of SLA violation avoidance is how to take advantage of the prediction of classifier in order to avoid violation."
    }, {
      "heading" : "APPENDIX A",
      "text" : "The full table of the results is presented in the following table."
    } ],
    "references" : [ {
      "title" : "Mechanisms for SLA provisioning in cloud-based service providers.",
      "author" : [ "Casalicchio", "Emiliano", "Luca Silvestri" ],
      "venue" : "Computer Networks",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2013
    }, {
      "title" : "Service level agreements in virtualised service platforms.",
      "author" : [ "Gallizo", "Georgina" ],
      "venue" : "eChallenges",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2009
    }, {
      "title" : "J",
      "author" : [ "C. Reiss" ],
      "venue" : "Wilkes and J. L. Hellerstein, ”Google cluster-usage traces: format+ schema”",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Elements of Statistical Learning Ed",
      "author" : [ "T. Hastie", "R. Tibshirani", "J. Friedman" ],
      "venue" : "2”, p592-593, Springer",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Diagnosing cloud performance anomalies using large time series dataset analysis.",
      "author" : [ "Jehangiri", "Ali Imran" ],
      "venue" : "IEEE 7th International Conference on Cloud Computing",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2014
    }, {
      "title" : "SLA-aware Resource Scheduling for Cloud Storage, 2014",
      "author" : [ "Zhihao Yao", "Ioannis Papapanagiotou", "RobertD. Callaway" ],
      "venue" : "IEEE 3rd International Conference on Cloud Networking (Cloud- Net)",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2014
    }, {
      "title" : "Sla-based trust model for cloud computing.",
      "author" : [ "Alhamad", "Mohammed", "Tharam Dillon", "Elizabeth Chang" ],
      "venue" : "Network-Based Information Systems (NBiS),",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2010
    }, {
      "title" : "Runtime prediction of service level agreement violations for composite services.",
      "author" : [ "Leitner", "Philipp" ],
      "venue" : "Service-Oriented Computing. IC- SOC/ServiceWave 2009 Workshops",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2010
    }, {
      "title" : "Host load prediction in a Google compute cloud with a Bayesian model.",
      "author" : [ "Di", "Sheng", "Derrick Kondo", "Walfredo Cirne" ],
      "venue" : "Proceedings of the International Conference on High Performance Computing,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2012
    }, {
      "title" : "Efficient provisioning of bursty scientific workloads on the cloud using adaptive elasticity control.",
      "author" : [ "Ali-Eldin", "Ahmed" ],
      "venue" : "Proceedings of the 3rd workshop on Scientific Cloud Computing Date. ACM,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2012
    }, {
      "title" : "Service clustering for autonomic clouds using random forest.",
      "author" : [ "Uriarte", "Rafael Brundo", "Sotirios Tsaftaris", "Francesco Tiezzi" ],
      "venue" : "Cluster, Cloud and Grid Computing (CCGrid),",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2015
    }, {
      "title" : "Bayesian network, and probabilistic ontology driven trust model for sla management of cloud services.",
      "author" : [ "Jules", "Obed", "Abdelhakim Hafid", "Mohamed Adel Serhani" ],
      "venue" : "Cloud Networking (CloudNet),",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2014
    }, {
      "title" : "A multiple resampling method for learning from imbalanced data sets.",
      "author" : [ "Estabrooks", "Andrew", "Taeho Jo", "Nathalie Japkowicz" ],
      "venue" : "Computational intelligence",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2004
    }, {
      "title" : "Statistical learning theory",
      "author" : [ "Vapnik", "Vladimir Naumovich", "Vlamimir Vapnik" ],
      "venue" : "New York: Wiley,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1998
    }, {
      "title" : "SMOTE: synthetic minority over-sampling technique.",
      "author" : [ "Chawla", "Nitesh V" ],
      "venue" : "Journal of artificial intelligence research",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2002
    }, {
      "title" : "Two Modifications of CNN",
      "author" : [ "I. Tomek" ],
      "venue" : "IEEE Transactions on Systems Man and Communications",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1976
    }, {
      "title" : "Addressing the curse of imbalanced training sets: one-sided selection.",
      "author" : [ "Kubat", "Miroslav", "Stan Matwin" ],
      "venue" : "ICML. Vol",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1997
    }, {
      "title" : "Improving identification of difficult small classes by balancing class distribution.",
      "author" : [ "Laurikkala", "Jorma" ],
      "venue" : "Conference on Artificial Intelligence in Medicine in Europe",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2001
    }, {
      "title" : "kNN approach to unbalanced data distributions: a case study involving information extraction.",
      "author" : [ "Mani", "Inderjeet", "I. Zhang" ],
      "venue" : "Proceedings of workshop on learning from imbalanced datasets",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2003
    }, {
      "title" : "Balancing Training Data for Automated Annotation of Keywords: a Case Study.",
      "author" : [ "Batista", "Gustavo EAPA", "Ana LC Bazzan", "Maria Carolina Monard" ],
      "venue" : "WOB",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2003
    }, {
      "title" : "A study of the behavior of several methods for balancing machine learning training data.",
      "author" : [ "Batista", "Gustavo EAPA", "Ronaldo C. Prati", "Maria Carolina Monard" ],
      "venue" : "ACM Sigkdd Explorations Newsletter",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "SLA usually contains specific parameters and a minimum level of quality for each element of the service that is negotiated between the provider and the customer [1].",
      "startOffset" : 161,
      "endOffset" : 164
    }, {
      "referenceID" : 1,
      "context" : "According to [2], SLA management has six phases: SLA contract definition, basic schema with the Quality of Service (QoS) parameters, SLA negotiation, SLA monitoring, SLA violation detection and SLA enforcement.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 2,
      "context" : "We report our results on a subset of the Google Cloud Cluster trace dataset [3].",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 3,
      "context" : "The results presented in Section VII show that the best performance is achieved using the Random Forest [4] method with an accuracy of 99.",
      "startOffset" : 104,
      "endOffset" : 107
    }, {
      "referenceID" : 4,
      "context" : "[5] use a map-reduce model to detect violations and find the most probable causes of SLA violations using Holt-Winters forecasting.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "In [6], the authors proposed an SLA aware resource scheduling model that determines how to allocate coming requests without explicit prediction of violations.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 6,
      "context" : "Similarly, in [7] , Mohammed et al.",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 7,
      "context" : "In a similar work for predicting SLA violations in composite services, in [8] the authors propose a regression machine",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 8,
      "context" : "In [9], the authors proposed a model for predicting host load using real data of Google Compute Cluster.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 9,
      "context" : "The authors in [10] propose a provisioning method that monitors and also predicts future loads.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 10,
      "context" : "The authors in [11], use unsupervised learning to cluster the resource usage and duration of services to avoid violations of Google Cluster trace dataset.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 11,
      "context" : "The authors in [12] use a Naive Bayes model to predict SLA violations.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 2,
      "context" : "Machine attributes table shows the attributes of each machine such as kernel version, clock speed and presence of an external IP address[3].",
      "startOffset" : 136,
      "endOffset" : 139
    }, {
      "referenceID" : 2,
      "context" : "The state transition diagram of a task on Google Cluster machines[3].",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 12,
      "context" : "There has been some methods in machine learning to handle such data such as over sampling and under sampling; also it has been recommenced to use generative models[13].",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 2,
      "context" : "According to the documentation of the trace [3], eviction of a task is due to ”overcommiting of the scheduler or because the machine on which it was running became unusable (e.",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 13,
      "context" : "According to the Bayes theorem [14], the above probability can be reformulated as follows:",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 14,
      "context" : "SMOTE (Synthetic Minority Over-sampling Technique) [15],",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 14,
      "context" : "3) SMOTE [15]: is a re-sampling method that generates new synthetic datapoints of the minority class using interpolation between the current datapoints.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 15,
      "context" : "4) Tomek links [16]: is an under sampling method.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 16,
      "context" : "5) One-sided Selection [17]: uses the combination of Tomek links and CNN (Condensed Nearest Neighbor Rule) to find the safe samples and removes the unsafe samples from the majority class.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 17,
      "context" : "6) Neighborhood Cleaning Rule [18]: removes some datapoints from the majority class.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 18,
      "context" : "7) NearMiss [19]: NearMiss 1, 2 and 3 algorithms are under-sampling methods.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 19,
      "context" : "8) SMOTE-Tomek links [20]: Since SMOTE over-sampling might lead to over-fitting and Tomek links under sampling might remove important datapoints, the ensemble of these two methods provide better results.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 20,
      "context" : "9) SMOTE-ENN [21]: is also the ensemble of SMOTE and ENN.",
      "startOffset" : 13,
      "endOffset" : 17
    } ],
    "year" : 2016,
    "abstractText" : "Service level agreement (SLA) is an essential part of cloud systems to ensure maximum availability of services for customers. With a violation of SLA, the provider has to pay penalties. Thus, being able to predict SLA violations favors both the customers and the providers. In this paper, we explore two machine learning models: Naive Bayes and Random Forest Classifiers to predict SLA violations. Since SLA violations are a rare event in the real world (∼ 0.2%), the classification task becomes more challenging. In order to overcome these challenges, we use several re-sampling methods such as Random Over and Under Sampling, SMOTH, NearMiss (1,2,3), One-sided Selection, Neighborhood Cleaning Rule, etc. to re-balance the dataset. We use the Google Cloud Cluster trace as the dataset to examine these different methods. We find that random forests with SMOTE-ENN re-sampling have the best performance among other methods with the accuracy of 0.9988% and F1 score of 0.9980.",
    "creator" : "LaTeX with hyperref package"
  }
}