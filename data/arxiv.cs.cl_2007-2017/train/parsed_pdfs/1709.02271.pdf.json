{
  "name" : "1709.02271.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Leveraging Discourse Information Effectively for Authorship Attribution∗",
    "authors" : [ "Su Wang", "Elisa Ferracane", "Raymond J. Mooney" ],
    "emails" : [ "shrekwang@utexas.edu,", "elisa@ferracane.com", "mooney@cs.utexas.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Authorship attribution (AA) is the task of identifying the author of a text, given a set of authorlabeled training texts. This task typically makes use of stylometric cues at the surface lexical and syntactic level (Stamatatos et al., 2015), although Feng and Hirst (2014) and Feng (2015) go beyond the sentence level, showing that discourse information can help. However, they achieve limited performance gains and lack an in-depth analysis of discourse featurization techniques. More recently, convolutional neural networks (CNNs) have demonstrated considerable success on AA relying only on character-level n-grams (Ruder et al., 2016; Shrestha et al., 2017). The strength of these models is evidenced by findings that traditional stylometric features such as word n-grams and POS-tags do not improve, and can sometimes even hurt performance (Ruder et al., 2016; Sari et al., 2017). However, none of these CNN models make use of discourse.\nOur work builds upon these prior studies by exploring an effective method to (i) featurize the\n∗The first two authors make equal contribution.\ndiscourse information, and (ii) integrate discourse features into the best text classifier (i.e., CNNbased models), in the expectation of achieving state-of-the-art results in AA.\nFeng and Hirst (2014) (henceforth F&H14) made the first comprehensive attempt at using discourse information for AA. They employ an entity-grid model, an approach introduced by Barzilay and Lapata (2008) for the task of ordering sentences. This model tracks how the grammatical relations of salient entities (e.g., subj, obj, etc.) change between pairs of sentences in a document, thus capturing a form of discourse coherence. The grid is summarized into a vector of transition probabilities. However, because the model only records the transition between two consecutive sentences at a time, the coherence is local. Feng (2015) (henceforth F15) further extends the entity-grid model by replacing grammatical relations with discourse relations from Rhetorical Structure Theory (Mann and Thompson, 1988, RST). Their study uses a linear-kernel SVM to perform pairwise author classifications, where a non-discourse model captures lexical and syntactic features. They find that adding the entitygrid with grammatical relations enhances the nondiscourse model by almost 1% in accuracy, and using RST relations provides an improvement of 3%. The study, however, works with only one small dataset and their models produce overall unremarkable performance (∼85%). Ji and Smith (2017) propose an advanced Recursive Neural Network (RecNN) architecture to work with RST in the more general area of text categorization and present impressive results. However, we suspect that the massive number of parameters of RecNNs would likely cause overfitting when working with smaller datasets, as is often the case in AA tasks.\nIn our paper, we opt for a state-of-the-art character bigram CNN classifier (Shrestha et al.,\nar X\niv :1\n70 9.\n02 27\n1v 1\n[ cs\n.C L\n] 7\nS ep\n2 01\n7\n2017), and investigate various ways in which the discourse information can be featurized and integrated into the CNN. Specifically,\n• Featurization. We attempt to capture a more global discourse coherence by modeling the entire sequence of relations in a document for every salient entity, instead of only the relations between pairs of sentences.\n• Feature integration. Using a neural network architecture allows us to explore embedding the relations from the entity-grid model1, rather than only exploiting a vector of relation probabilities.\nWe explore these questions using two approaches to represent salient entities: grammatical relations, and RST discourse relations. We apply these models to datasets of varying sizes and genres, and find that adding any discourse information improves AA consistently on longer documents, but has mixed results on shorter documents. Further, embedding the discourse features in a parallel CNN at the input end yields better performance than concatenating them to the output layer as a feature vector (Section 3). The global featurization is more effective than the local one. We also show that SVMs, which can only use discourse probability vectors, neither produce a competitive performance (even with fine-tuning), nor generalize in using the discourse information effectively.\n1Nguyen and Joty (2017) are the first to propose applying embeddings in modeling local coherence (for the coherence judgment task). Our methods roughly subsume theirs, which correspond to our GR CNN2-DE (global) model (Section 3). This scheme did not come out on top in our AA tasks."
    }, {
      "heading" : "2 Background",
      "text" : "Entity-grid model. Typical lexical features for AA are relatively superficial and restricted to within the same sentence. F&H14 hypothesize that discourse features beyond the sentence level also help authorship attribution. In particular, they propose an author has a particular style for representing entities across a discourse. Their work is based on the entity-grid model of Barzilay and Lapata (2008) (henceforth B&L).\nThe entity-grid model tracks the grammatical relation (subj, obj, etc.) that salient entities take on throughout a document as a way to capture local coherence . A salient entity is defined as a noun phrase that co-occurs at least twice in a document. Extensive literature has shown that subject and object relations are a strong signal for salience and it follows from the Centering Theory that you want to avoid rough shifts in the center (Grosz et al., 1995; Strube and Hahn, 1999). B&L thus focus on whether a salient entity is a subject (s), object (o), other (x), or is not present (-) in a given sentence, as illustrated in Table 1. Every sentence in a document is encoded with the grammatical relation of all the salient entities, resulting in a grid similar to Table 3.\nThe local coherence of a document is then defined on the basis of local entity transitions. A local entity transition is the sequence of grammatical relations that an entity can assume across n consecutive sentences, resulting in {s,o,x,-}n possible transitions. Following B&L, F&H14 consider sequences of length n=2, that is, transitions between two consecutive sentences, resulting in 42=16 pos-\nsible transitions. The probability for each transition is then calculated as the frequency of the transition divided by the total number of transitions. This step results in a single probability vector for every document, as illustrated in Table 2.\nB&L apply this model to a sentence ordering task, where the more coherent option, as evidenced by its transition probabilities, was chosen. In authorship attribution, texts are however assumed to already be coherent. F&H14 instead hypothesize that an author unconsciously employs the same methods for describing entities as the discourse unfolds, resulting in discernible transition probability patterns across multiple of their texts. Indeed, F&H14 find that adding the B&L vectors increases the accuracy of AA by almost 1% over a baseline lexico-syntactic model.\nRST discourse relations. F15 extends the notion of tracking salient entities to RST. Instead of using grammatical relations in the grid, RST discourse relations are specified. An RST discourse relation defines the relationship between two or more elementary discourse units (EDUs), which are spans of text that typically correspond to syntactic clauses. In a relation, an EDU can function as a nucleus (e.g., result.N) or as a satellite (e.g., summary.S). All the relations in a document then form a tree as in Figure 1.2\nF15 finds that RST relations are more effective for AA than grammatical relations. In our paper, we populate the entity-grid in the same way as F15’s “Shallow RST-style” encoding, but use finegrained instead of coarse-grained RST relations, and do not distinguish between intra-sentential\n2For reasons of space, only the first sentence of the excerpt is illustrated.\nand multi-sentential RST relations, or salient and non-salient entities. We explore various featurization techniques using the coding scheme.\nCNN model. Shrestha et al. (2017) propose a convolutional neural network formulation for AA tasks (detailed in Section 3). They report stateof-the-art performance on a corpus of Twitter data (Schwartz et al., 2013), and compare their models with alternative architectures proposed in the literature: (i) SCH: an SVM that also uses character n-grams, among other stylometric features (Schwartz et al., 2013); (ii) LSTM-2: an LSTM trained on bigrams (Tai et al., 2015); (iii) CHAR: a Logistic Regression model that takes character n-grams (Stamatatos, 2009); (iv) CNN-W: a CNN trained on word embeddings (Kalchbrenner et al., 2014). The authors show that the model CNN23 produces the best performance overall. Ruder et al. (2016) apply character n-gram CNNs to a wide range of datasets, providing strong empirical evidence that the architecture generalizes well. Further, they find that including word n-grams in addition to character n-grams reduces performance, which is in agreement with Sari et al. (2017)’s findings."
    }, {
      "heading" : "3 Models",
      "text" : "Building on Shrestha et al. (2017)’s work, we employ their character-bigram CNN (CNN2)4, and\n3Shrestha et al. (2017) test two variants of CNN models: CNN1/CNN2 for unigram/bigram character CNNs respectively.\n4Our preliminary experiments found that using character n-gram orders higher than 2 performed worse, likely due to the increased number of features and overfitting.\npropose two extensions which utilize discourse information: (i) CNN2 enhanced with relation probability vectors (CNN2-PV), and (ii) CNN2 enhanced with discourse embeddings (CNN2-DE). The CNN2-PV allows us to conduct a comparison with F&H14 and F15, which also use relation probability vectors.\nCNN2. CNN2 is the baseline model with no discourse features. Illustrated in Figure 2 (center), it consists of (i) an embedding layer, (ii) a convolution layer, (iii) a max-pooling layer, and (iv) a softmax layer. We briefly sketch the processing procedure and refer the reader to (Shrestha et al., 2017, Section 2) for mathematical details.\nThe network takes a sequence of character bigrams x = 〈x1, . . . , xl〉 as input, and outputs a multinomial φ over class labels as the prediction. The model first looks up the embedding matrix to produce a sequence of embeddings for x (i.e., the matrix C), then pushes the embedding sequence through convolutional filters of three bigram-window sizes w = 3, 4, 5, each yielding m feature maps. We then apply the max-over-time pooling (Collobert et al., 2011) to the feature maps from each filter, and concatenate the resulting vectors to obtain a single vector y, which then goes through the softmax layer to produce predictions.\nCNN2-PV. This model (Figure 2, left+center) featurizes discourse information into a vector of relation probabilities. In order to derive the discourse features, an entity grid is constructed by feeding the document through an NLP pipeline5 to identify salient entities. Two\n5Using neural coreference resolver, dependency parser in\nflavors of discourse features are created by populating the entity grid with either (i) grammatical relations (GR) or (ii) RST discourse relations6 (RST). The GR features are represented as grammatical relation transitions derived from the entity grid, e.g., 〈sx,xs,ss,...〉. The RST features are represented as RST discourse relations with their nuclearity, e.g., 〈definition.N,attribution.S,...〉. The probability vectors are then distributions over relation types. For GR, the vector is a distribution over all the entity role transitions, i.e., 〈p(sx), p(xs), p(ss), . . .〉 (see Table 2). For RST, the vector is a distribution over all the RST discourse relations, i.e., 〈p(definition.N), p(attribution.S), . . .〉 Denoting a feature as such with y′′, we construct the pooling vector y for the char-bigrams, and concatenate y′′ to y before feeding the resulting vector to the softmax layer.\nCNN2-DE. In this model (Figure 2, center+right), we embed discourse features in high-dimensional space (similar to char-bigram embeddings). Let z = 〈z1, . . . , zl′〉 be a sequence of discourse features7, we treat it in a similar fashion to the charbigram sequence x, i.e. feeding it through a “parallel” convolutional net (Figure 2 right). The operation results in a pooling vector y′. We concatenate y′ to the pooling vector y (which is constructed from x) then feed [y;y′] to the softmax layer for the final prediction.\nStanford Core NLP (Clark and Manning, 2016). 6Using RST Parser from Ji and Eisenstein (2014). 7The sequence comes in two variants, depending on the featurization technique, see Section 4.2."
    }, {
      "heading" : "4 Experiments and Results",
      "text" : "We begin by introducing the datasets (Section 4.1), followed by detailing the featurization methods (Section 4.2), the experiments (Section 4.3), and finally reporting results (Section 4.4)."
    }, {
      "heading" : "4.1 Datasets",
      "text" : "The statistics for the three datasets used in the experiments are summarized in Table 4.\nnovel-9. This dataset was compiled by F&H14: a collection of 19 novels by 9 nineteenth century British and American authors in the Project Gutenberg. To compare to F&H14, we apply the same resampling method (F&H14, Section 4.2) to correct the imbalance in authors by oversampling the texts of less-represented authors.\nnovel-50. This dataset extends novel-9, compiling the works of 50 randomly selected authors of the same period. For each author, we randomly select 5 novels for a total 250 novels.\nIMDB62. IMDB62 consists of 62K movie reviews from 62 users (1,000 each) from the Internet Movie dataset, compiled by Seroussi et al. (2011). Unlike the novel datasets, the reviews are considerably shorter, with a mean of 349 words per text."
    }, {
      "heading" : "4.2 Featurization",
      "text" : "As described in Section 2, in both the GR and RST variants, from each input entry we start by obtaining an entity grid.\nCNN2-PV. We collect the probabilities of entity role transitions (in GR) or discourse relations (in RST) for the entries. Each entry corresponds to a probability distribution vector.\nCNN2-DE. We employ two schema for creating discourse feature sequences from an entity grid. While we always read the grid by column (by a salient entity), we vary whether we track the entity across a number of sentences (n rows at a time) or across the entire document (one entire column at a\ntime), denoted as local and global reading respectively.\nFor the GR discourse features, in the case of local reading, we process the entity roles one sentence pair at a time (Figure 3, left). For example, in processing the pair (s1, s2), we find the first non-empty role r11 for entity E1 in s1. If E1 also has a non-empty role r21 in the s2, we collect the entity role transition r11r21. We then proceed to the following entity E2, until we process all the entities in the grid and move to the next sentence pair. For the global reading, we instead read the entity roles by traversing one column of the entire document at a time (Figure 3, right). The entity roles in all the sentences are read for one entity: we collect transitions for all the non-empty roles (e.g., so, but not s-).\nFor the RST discourse features, we process nonempty discourse relations also through either local or global reading. In the local reading, we read all the discourse relations in a sentence (a row) then move on to the next sentence.8 In the global reading, we read in discourse relations for one entity at a time. This results in sequences of discourse relations for the input entries."
    }, {
      "heading" : "4.3 Experiments",
      "text" : "Baseline-dataset experiments. All the baselinedataset experiments are evaluated on novel-9. As a comparison to previous work (F15), we evaluate our models using a pairwise classification task with GR discourse features. In her model, novels are partitioned into 1000-word chunks, and the model is evaluated with accuracy.9 Surpassing F15’s SVM model by a large margin, we then further evaluate the more difficult multi-class task, i.e., all-class prediction simultaneously, with both GR and RST discourse features and the more robust F1 evaluation. In this multi-class task, we implement two SVMs to extend F15’s SVM models:\n8We do not check the next sentences as in GR, because the discourse relations in one cell of the entity grid typically already capture relations beyond the sentence level.\n9Averaged over all the author-author pair experiments.\n(i) SVM2: a linear-kernel SVM which takes charbigrams as input, as our CNNs, and (ii) SVM2PV: an updated SVM2 which takes also probability vector features.\nFurther, we are interested in finding a performance threshold on the minimally-required input text length for discourse information to “kick in”. To this end, we chunk the novels into different sizes: 200-2000 words, at 200-word intervals, and evaluate our CNNs in the multi-class condition.\nGeneralization-dataset experiments. To confirm that our models generalize, we pick the best models from the baseline-dataset experiments and evaluate on the novel-50 and IMDB62 datasets. For novel-50, the chunking size applied is 2000- word as per the baseline-dataset experiment results, and for IMDB62, texts are not chunked (i.e., we feed the models with the original reviews directly). For model comparison, we also run the SVMs (i.e., SVM2 and SVM2-PV) used in the baseline-dataset experiment. All the experiments conducted here are multi-class classification with macro-averaged F1 evaluation.\nModel configurations. Following F15, we perform 5-fold cross-validation. The embedding sizes are tuned on novel-9 (multi-class condition): 50 for char-bigrams; 20 for discourse features. The learning rate is 0.001 using the Adam Optimizer (Kingma and Ba, 2014). For all models, we apply dropout regularization of 0.75 (Srivastava et al., 2014), and run 50 epochs (batch size 32). The SVMs in the baseline-dataset experiments use default settings, following F15. For the SVMs in the generalization-dataset experiments, we tuned the hyperparameters on novel-9 with a grid search, and found the optimal setting as: stopping condition tol is 1e-5, at a max-iteration of 1,500."
    }, {
      "heading" : "4.4 Results",
      "text" : "Baseline-dataset experiments. The results of the baseline-dataset experiments are reported in Table 5, 6 and Figure 4. In Table 5, Baseline denotes the dumb baseline model which always predicts the more-represented author of the pair. Both SVMs are from F15, and we report her results. SVM (LexSyn) takes character and word bi/trigrams and POS tags. SVM (LexSyn-PV) additionally includes probability vectors, similar to our CNN2PV. In this part of the experiment, while the CNNs clear a large margin over SVMs, adding discourse in CNN2-PV brings only a small performance gain.\nTable 6 reports the results from the multi-class classification task, the more difficult task. Here, probability vector features (i.e., PV) again fail to contribute much. The discourse embedding features, on the other hand, manage to increase the F1 score by a noticeable amount, with the maximal improvement seen in the CNN2-DE (global) model with RST features (by 2.6 points). In contrast, the discourse-enhanced SVM2-PVs increase F1 by about 1 point, with overall much lower scores in comparison to the CNNs. In general, RST features work better than GR features.\nThe results of the varying-sizes experiments are plotted in Figure 4. Again, we observe the overall pattern that discourse features improve the F1 score, and RST features procure superior performance. Crucially, however, we note there is no\nperformance boost below the chunk size of 1000 for GR features, and below 600 for RST features. Where discourse features do help, the GR-based models achieve, on average, 1 extra point on F1, and the RST-based models around 2.\nGeneralization-dataset experiments. Table 7 summarizes the results of the generalizationdataset experiments. On novel-50, most discourse-enhanced models improve the performance of the baseline non-discourse CNN2 to varying degrees. The clear pattern again emerges that RST features work better, with the best F1 score evidenced in the CNN2-DE (global) model (3.5 improvement in F1). On IMDB62, as expected with short text inputs (mean=349 words/review), the discourse features in general do not add further contribution. Even the best model CNN2-DE brings only marginal improvement, confirming our findings from varying the chunk size on novel-9, where discourse features did not help at this input size. Equipped with discourse features, SVM2-PV performs slightly better than SVM2 on novel-50 (by 0.4 with GR, 0.9 with RST features). On IMDB62, the same pattern persists for the SVMs: discourse features do not make noticeable improvements (by 0.0 and 0.5 with GR and RST respectively)."
    }, {
      "heading" : "5 Analysis",
      "text" : "General analysis. Overall, we have shown that discourse information can improve authorship attribution, but only when properly encoded. This result is critical in demonstrating the particular value of discourse information, because typical\nstylometric features such as word n-grams and POS tags do not add additional performance improvements (Ruder et al., 2016; Sari et al., 2017).\nIn addition, the type of discourse information and the way in which it is featurized are tantamount to this performance improvement: RST features provide overall stronger improvement, and the global reading scheme for discourse embedding works better than the local one. The discourse embedding proves to be a superior featurization technique, as evidenced by the generally higher performance of CNN2-DE models over CNN2-PV models. With an SVM, where the option is not available, we are only able to use relation probability vectors to obtain a very modest performance improvement.\nFurther, we found an input-length threshold for the discourse features to help (Section 4.4). Not surprisingly, discourse does not contribute on shorter texts. Many of the feature grids are empty for these shorter texts– either there are no coreference chains or they are not correctly resolved. Currently we only have empirical results on short novel chunks and movie reviews, but believe the finding would generalize to Twitter or blog posts.\nDiscourse embeddings. It does not come as a surprise that discourse embedding-based models perform better than their relation probability-based peers. The former (i) leverages the weight learning of the entire computational graph of the CNN rather than only the softmax layer, as the PV models do, and (ii) provides a more fine-grained featurization of the discourse information. Rather than merely taking a probability over grammatical\nrelation transitions (in GR) or discourse relation types (in RST), in DE-based models we learn the dependency between grammatical relation transitions/discourse relations through the w-sized filter sweeps.\nTo further study the information encoded in the discourse embeddings, we perform t-SNE clustering (van der Maaten and Hinton, 2008) on them, using the best performing model CNN2-DE (global). We examine the closest neighbors of each embedding, and observe that similar discourse relations tend to go together (e.g., explanation and interpretation; consequence and result). Some examples are given in Table 8. However, it is unclear how this pattern helps improve classification performance. We intend to investigate this question in future work.\nGlobal vs. Local featurization. As described in Section 4.2, the global reading processes all the discourse features for one entity at a time, while the local approach reads one sentence (or one sentence pair) at a time. In all the relevant experiments, global featurization showed a clear performance advantage (on average 1 point gain in F1). Recall that the creation of the grids (both GR and RST) depend on coreference chains of entities (Section 2), and only the global reading scheme takes advantage of the coreference pattern whereas the local reading breaks the chains. To find out whether coreference pattern is the key to the performance difference, we further ran a probe experiment where we read RST discourse relations in the order in which EDUs are arranged in the RST tree (i.e., left-to-right), and evaluated this model on novel-50 and IMDB62 with the same hyperparameter setting. The F1 scores turned out to be very close to the CNN2-DE (local) model, at 97.5 and 90.9. Based on this finding, we tentatively confirm the importance of the coreference pattern, and intend to further investigate how ex-\nactly it matters for the classification performance.\nGR vs. RST. RST features in general effect higher performance gains than GR features (Table 7). The RST parser produces a tree of discourse relations for the input text, thus introducing a “global view.” The GR features, on the other hand, are more restricted to a “local view” on entities between consecutive sentences. While a deeper empirical investigation is needed, one can intuitively imagine that identifying authorship by focusing on the local transitions between grammatical relations (as in GR) is more difficult than observing how the entire text is organized (as in RST).10"
    }, {
      "heading" : "6 Conclusion",
      "text" : "We have conducted an in-depth investigation of techniques that (i) featurize discourse information, and (ii) effectively integrate discourse features into the state-of-the-art character-bigram CNN classifier for AA. Beyond confirming the overall superiority of RST features over GR features in larger and more difficult datasets, we present a discourse embedding technique that is unavailable for previously proposed discourse-enhanced models. The new technique enabled us to push the envelope of the current performance ceiling by a large margin.\nAdmittedly, in using the RST features with entity-grids, we lose the valuable RST tree structure. In future work, we intend to adopt more sophisticated methods such as RecNN, as per Ji and Smith (2017), to retain more information from the RST trees while reducing the parameter size. Further, we aim to understand how discourse embeddings contribute to AA tasks, and find alternatives to coreference chains for shorter texts.\n10Note that, however, it is simpler to extract GR features, as we rely solely on a high-performance dependency parser, which is widely available, whereas for RST features, we need gold RST-labeled training data, which incurs higher cost and potentially relatively limited generalizability."
    } ],
    "references" : [ {
      "title" : "Modeling local coherence: an entity-based approach",
      "author" : [ "Regina Barzilay", "Mirella Lapata." ],
      "venue" : "Computational Linguistics 34(1):1–34.",
      "citeRegEx" : "Barzilay and Lapata.,? 2008",
      "shortCiteRegEx" : "Barzilay and Lapata.",
      "year" : 2008
    }, {
      "title" : "Deep reinforcement learning for mention-ranking coreference models",
      "author" : [ "Kevin Clark", "Christopher D. Manning." ],
      "venue" : "Empirical Methods on Natural Language Processing.",
      "citeRegEx" : "Clark and Manning.,? 2016",
      "shortCiteRegEx" : "Clark and Manning.",
      "year" : 2016
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa." ],
      "venue" : "Journal of Machine Learning Research 12:2493–2537.",
      "citeRegEx" : "Collobert et al\\.,? 2011",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "RST-style discourse parsing and its applications in discourse analysis",
      "author" : [ "Vanessa Wei Feng." ],
      "venue" : "Ph.D. thesis, University of Toronto.",
      "citeRegEx" : "Feng.,? 2015",
      "shortCiteRegEx" : "Feng.",
      "year" : 2015
    }, {
      "title" : "Patterns of local discourse coherence as a feature for authorship attribution",
      "author" : [ "Vanessa Wei Feng", "Graeme Hirst." ],
      "venue" : "Literary and Linguistic Computing 29(2):191–198.",
      "citeRegEx" : "Feng and Hirst.,? 2014",
      "shortCiteRegEx" : "Feng and Hirst.",
      "year" : 2014
    }, {
      "title" : "Centering: A framework for modeling the local coherence of discourse",
      "author" : [ "Barbara J Grosz", "Scott Weinstein", "Aravind K Joshi." ],
      "venue" : "Computational linguistics 21(2):203–225.",
      "citeRegEx" : "Grosz et al\\.,? 1995",
      "shortCiteRegEx" : "Grosz et al\\.",
      "year" : 1995
    }, {
      "title" : "Representation learning for text-level discourse parsing",
      "author" : [ "Yangfeng Ji", "Jacob Eisenstein." ],
      "venue" : "ACL (1). pages 13–24.",
      "citeRegEx" : "Ji and Eisenstein.,? 2014",
      "shortCiteRegEx" : "Ji and Eisenstein.",
      "year" : 2014
    }, {
      "title" : "Neural discourse structure for text categorization",
      "author" : [ "Yangfeng Ji", "Noah Smith." ],
      "venue" : "ACL 2017 to appear.",
      "citeRegEx" : "Ji and Smith.,? 2017",
      "shortCiteRegEx" : "Ji and Smith.",
      "year" : 2017
    }, {
      "title" : "A convolutional neural network for modeling sentences",
      "author" : [ "Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom." ],
      "venue" : "ACL 2014 1:655–665.",
      "citeRegEx" : "Kalchbrenner et al\\.,? 2014",
      "shortCiteRegEx" : "Kalchbrenner et al\\.",
      "year" : 2014
    }, {
      "title" : "Adam: a method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba." ],
      "venue" : "CoRR: arXiv1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Rhetorical structure theory: Toward a functional theory of text organization",
      "author" : [ "William Mann", "Sandra Thompson." ],
      "venue" : "Text 8(3):243–281.",
      "citeRegEx" : "Mann and Thompson.,? 1988",
      "shortCiteRegEx" : "Mann and Thompson.",
      "year" : 1988
    }, {
      "title" : "A neural local coherence model",
      "author" : [ "Dat Tien Nguyen", "Shafiq Joty." ],
      "venue" : "ACL 2017, to appear.",
      "citeRegEx" : "Nguyen and Joty.,? 2017",
      "shortCiteRegEx" : "Nguyen and Joty.",
      "year" : 2017
    }, {
      "title" : "Character-level and multi-channel convolutional neural networks for large-scale authorship attribution",
      "author" : [ "Sebastian Ruder", "Parsa Ghaffari", "John Breslin." ],
      "venue" : "CoRR, arXiv:1609.06686 .",
      "citeRegEx" : "Ruder et al\\.,? 2016",
      "shortCiteRegEx" : "Ruder et al\\.",
      "year" : 2016
    }, {
      "title" : "Continuous n-gram representations for authorship attribution",
      "author" : [ "Yunita Sari", "Andreas Vlachos", "Mark Stevenson." ],
      "venue" : "Proceedings of EACL.",
      "citeRegEx" : "Sari et al\\.,? 2017",
      "shortCiteRegEx" : "Sari et al\\.",
      "year" : 2017
    }, {
      "title" : "Authorship attribution of micromessages",
      "author" : [ "Roy Schwartz", "Oren Tsur", "Ari Rappoport", "Moshe Koppel." ],
      "venue" : "EMNLP 2013 pages 1880–1891.",
      "citeRegEx" : "Schwartz et al\\.,? 2013",
      "shortCiteRegEx" : "Schwartz et al\\.",
      "year" : 2013
    }, {
      "title" : "Authorship attribution with Latent Dirichlet Allocation",
      "author" : [ "Yanir Seroussi", "Ingrid Zukerman", "Fabian Bohnert." ],
      "venue" : "Proceedings of CNLL.",
      "citeRegEx" : "Seroussi et al\\.,? 2011",
      "shortCiteRegEx" : "Seroussi et al\\.",
      "year" : 2011
    }, {
      "title" : "Convolutional neural networks for authorship attribution of short texts",
      "author" : [ "Prasha Shrestha", "Sebastian Sierra", "Fabio A González", "Paolo Rosso", "Manuel Montes-y Gómez", "Thamar Solorio." ],
      "venue" : "EACL 2017 page 669.",
      "citeRegEx" : "Shrestha et al\\.,? 2017",
      "shortCiteRegEx" : "Shrestha et al\\.",
      "year" : 2017
    }, {
      "title" : "Dropout: a simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov." ],
      "venue" : "Journal of Machine Learning Research 15.",
      "citeRegEx" : "Srivastava et al\\.,? 2014",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2014
    }, {
      "title" : "A survey of modern authorship attribution methods",
      "author" : [ "Efstathios Stamatatos." ],
      "venue" : "Journal of the Americal Society for Information Science and Technology pages 155–159.",
      "citeRegEx" : "Stamatatos.,? 2009",
      "shortCiteRegEx" : "Stamatatos.",
      "year" : 2009
    }, {
      "title" : "Overview of the Author Identification Task at PAN 2015",
      "author" : [ "Efstathios Stamatatos", "Walter Daelemans", "Ben Verhoeven", "Patrick Juola", "Aurelio López-López", "Martin Potthast", "Benno Stein." ],
      "venue" : "Linda Cappellato and Nicola Ferro and Gareth Jones and Eric",
      "citeRegEx" : "Stamatatos et al\\.,? 2015",
      "shortCiteRegEx" : "Stamatatos et al\\.",
      "year" : 2015
    }, {
      "title" : "Functional centering: Grounding referential coherence in information structure",
      "author" : [ "Michael Strube", "Udo Hahn." ],
      "venue" : "Computational linguistics 25(3):309– 344.",
      "citeRegEx" : "Strube and Hahn.,? 1999",
      "shortCiteRegEx" : "Strube and Hahn.",
      "year" : 1999
    }, {
      "title" : "Improved semantic representations from tree-structured long short-term memory networks",
      "author" : [ "Kai Sheng Tai", "Richard Socher", "Christopher Manning." ],
      "venue" : "ACL 2015 1:1556–1566.",
      "citeRegEx" : "Tai et al\\.,? 2015",
      "shortCiteRegEx" : "Tai et al\\.",
      "year" : 2015
    }, {
      "title" : "Visualizing high-dimensional data using t-SNE",
      "author" : [ "L.J.P. van der Maaten", "G.E. Hinton." ],
      "venue" : "Journal of Machine Learning Research 9.",
      "citeRegEx" : "Maaten and Hinton.,? 2008",
      "shortCiteRegEx" : "Maaten and Hinton.",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : "This task typically makes use of stylometric cues at the surface lexical and syntactic level (Stamatatos et al., 2015), although Feng and Hirst (2014) and Feng (2015) go beyond the sentence level, showing that discourse information can help.",
      "startOffset" : 93,
      "endOffset" : 118
    }, {
      "referenceID" : 12,
      "context" : "More recently, convolutional neural networks (CNNs) have demonstrated considerable success on AA relying only on character-level n-grams (Ruder et al., 2016; Shrestha et al., 2017).",
      "startOffset" : 137,
      "endOffset" : 180
    }, {
      "referenceID" : 16,
      "context" : "More recently, convolutional neural networks (CNNs) have demonstrated considerable success on AA relying only on character-level n-grams (Ruder et al., 2016; Shrestha et al., 2017).",
      "startOffset" : 137,
      "endOffset" : 180
    }, {
      "referenceID" : 12,
      "context" : "The strength of these models is evidenced by findings that traditional stylometric features such as word n-grams and POS-tags do not improve, and can sometimes even hurt performance (Ruder et al., 2016; Sari et al., 2017).",
      "startOffset" : 182,
      "endOffset" : 221
    }, {
      "referenceID" : 13,
      "context" : "The strength of these models is evidenced by findings that traditional stylometric features such as word n-grams and POS-tags do not improve, and can sometimes even hurt performance (Ruder et al., 2016; Sari et al., 2017).",
      "startOffset" : 182,
      "endOffset" : 221
    }, {
      "referenceID" : 3,
      "context" : ", 2015), although Feng and Hirst (2014) and Feng (2015) go beyond the sentence level, showing that discourse information can help.",
      "startOffset" : 18,
      "endOffset" : 40
    }, {
      "referenceID" : 3,
      "context" : ", 2015), although Feng and Hirst (2014) and Feng (2015) go beyond the sentence level, showing that discourse information can help.",
      "startOffset" : 18,
      "endOffset" : 56
    }, {
      "referenceID" : 0,
      "context" : "They employ an entity-grid model, an approach introduced by Barzilay and Lapata (2008) for the task of ordering sentences.",
      "startOffset" : 60,
      "endOffset" : 87
    }, {
      "referenceID" : 0,
      "context" : "They employ an entity-grid model, an approach introduced by Barzilay and Lapata (2008) for the task of ordering sentences. This model tracks how the grammatical relations of salient entities (e.g., subj, obj, etc.) change between pairs of sentences in a document, thus capturing a form of discourse coherence. The grid is summarized into a vector of transition probabilities. However, because the model only records the transition between two consecutive sentences at a time, the coherence is local. Feng (2015) (henceforth F15) further extends the entity-grid model by replacing grammatical relations with discourse relations from Rhetorical Structure Theory (Mann and Thompson, 1988, RST).",
      "startOffset" : 60,
      "endOffset" : 512
    }, {
      "referenceID" : 0,
      "context" : "They employ an entity-grid model, an approach introduced by Barzilay and Lapata (2008) for the task of ordering sentences. This model tracks how the grammatical relations of salient entities (e.g., subj, obj, etc.) change between pairs of sentences in a document, thus capturing a form of discourse coherence. The grid is summarized into a vector of transition probabilities. However, because the model only records the transition between two consecutive sentences at a time, the coherence is local. Feng (2015) (henceforth F15) further extends the entity-grid model by replacing grammatical relations with discourse relations from Rhetorical Structure Theory (Mann and Thompson, 1988, RST). Their study uses a linear-kernel SVM to perform pairwise author classifications, where a non-discourse model captures lexical and syntactic features. They find that adding the entitygrid with grammatical relations enhances the nondiscourse model by almost 1% in accuracy, and using RST relations provides an improvement of 3%. The study, however, works with only one small dataset and their models produce overall unremarkable performance (∼85%). Ji and Smith (2017) propose an advanced Recursive Neural Network (RecNN) architecture to work with RST in the more general area of text categorization and present impressive results.",
      "startOffset" : 60,
      "endOffset" : 1159
    }, {
      "referenceID" : 0,
      "context" : "Their work is based on the entity-grid model of Barzilay and Lapata (2008) (henceforth B&L).",
      "startOffset" : 48,
      "endOffset" : 75
    }, {
      "referenceID" : 5,
      "context" : "Extensive literature has shown that subject and object relations are a strong signal for salience and it follows from the Centering Theory that you want to avoid rough shifts in the center (Grosz et al., 1995; Strube and Hahn, 1999).",
      "startOffset" : 189,
      "endOffset" : 232
    }, {
      "referenceID" : 20,
      "context" : "Extensive literature has shown that subject and object relations are a strong signal for salience and it follows from the Centering Theory that you want to avoid rough shifts in the center (Grosz et al., 1995; Strube and Hahn, 1999).",
      "startOffset" : 189,
      "endOffset" : 232
    }, {
      "referenceID" : 14,
      "context" : "They report stateof-the-art performance on a corpus of Twitter data (Schwartz et al., 2013), and compare their models with alternative architectures proposed in the literature: (i) SCH: an SVM that also uses character n-grams, among other stylometric features (Schwartz et al.",
      "startOffset" : 68,
      "endOffset" : 91
    }, {
      "referenceID" : 14,
      "context" : ", 2013), and compare their models with alternative architectures proposed in the literature: (i) SCH: an SVM that also uses character n-grams, among other stylometric features (Schwartz et al., 2013); (ii) LSTM-2: an LSTM trained on bigrams (Tai et al.",
      "startOffset" : 176,
      "endOffset" : 199
    }, {
      "referenceID" : 21,
      "context" : ", 2013); (ii) LSTM-2: an LSTM trained on bigrams (Tai et al., 2015); (iii) CHAR: a Logistic Regression model that takes character n-grams (Stamatatos, 2009); (iv) CNN-W: a CNN trained on word embeddings (Kalchbrenner et al.",
      "startOffset" : 49,
      "endOffset" : 67
    }, {
      "referenceID" : 18,
      "context" : ", 2015); (iii) CHAR: a Logistic Regression model that takes character n-grams (Stamatatos, 2009); (iv) CNN-W: a CNN trained on word embeddings (Kalchbrenner et al.",
      "startOffset" : 78,
      "endOffset" : 96
    }, {
      "referenceID" : 8,
      "context" : ", 2015); (iii) CHAR: a Logistic Regression model that takes character n-grams (Stamatatos, 2009); (iv) CNN-W: a CNN trained on word embeddings (Kalchbrenner et al., 2014).",
      "startOffset" : 143,
      "endOffset" : 170
    }, {
      "referenceID" : 12,
      "context" : "Shrestha et al. (2017) propose a convolutional neural network formulation for AA tasks (detailed in Section 3).",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 8,
      "context" : ", 2015); (iii) CHAR: a Logistic Regression model that takes character n-grams (Stamatatos, 2009); (iv) CNN-W: a CNN trained on word embeddings (Kalchbrenner et al., 2014). The authors show that the model CNN23 produces the best performance overall. Ruder et al. (2016) apply character n-gram CNNs to a wide range of datasets, providing strong empirical evidence that the architecture generalizes well.",
      "startOffset" : 144,
      "endOffset" : 269
    }, {
      "referenceID" : 8,
      "context" : ", 2015); (iii) CHAR: a Logistic Regression model that takes character n-grams (Stamatatos, 2009); (iv) CNN-W: a CNN trained on word embeddings (Kalchbrenner et al., 2014). The authors show that the model CNN23 produces the best performance overall. Ruder et al. (2016) apply character n-gram CNNs to a wide range of datasets, providing strong empirical evidence that the architecture generalizes well. Further, they find that including word n-grams in addition to character n-grams reduces performance, which is in agreement with Sari et al. (2017)’s findings.",
      "startOffset" : 144,
      "endOffset" : 549
    }, {
      "referenceID" : 16,
      "context" : "Building on Shrestha et al. (2017)’s work, we employ their character-bigram CNN (CNN2)4, and",
      "startOffset" : 12,
      "endOffset" : 35
    }, {
      "referenceID" : 2,
      "context" : "We then apply the max-over-time pooling (Collobert et al., 2011) to the feature maps from each filter, and concatenate the resulting vectors to obtain a single vector y, which then goes through the softmax layer to produce predictions.",
      "startOffset" : 40,
      "endOffset" : 64
    }, {
      "referenceID" : 1,
      "context" : "Stanford Core NLP (Clark and Manning, 2016).",
      "startOffset" : 18,
      "endOffset" : 43
    }, {
      "referenceID" : 1,
      "context" : "Stanford Core NLP (Clark and Manning, 2016). Using RST Parser from Ji and Eisenstein (2014). The sequence comes in two variants, depending on the featurization technique, see Section 4.",
      "startOffset" : 19,
      "endOffset" : 92
    }, {
      "referenceID" : 15,
      "context" : "IMDB62 consists of 62K movie reviews from 62 users (1,000 each) from the Internet Movie dataset, compiled by Seroussi et al. (2011). Unlike the novel datasets, the reviews are considerably shorter, with a mean of 349 words per text.",
      "startOffset" : 109,
      "endOffset" : 132
    }, {
      "referenceID" : 9,
      "context" : "001 using the Adam Optimizer (Kingma and Ba, 2014).",
      "startOffset" : 29,
      "endOffset" : 50
    }, {
      "referenceID" : 17,
      "context" : "75 (Srivastava et al., 2014), and run 50 epochs (batch size 32).",
      "startOffset" : 3,
      "endOffset" : 28
    }, {
      "referenceID" : 12,
      "context" : "stylometric features such as word n-grams and POS tags do not add additional performance improvements (Ruder et al., 2016; Sari et al., 2017).",
      "startOffset" : 102,
      "endOffset" : 141
    }, {
      "referenceID" : 13,
      "context" : "stylometric features such as word n-grams and POS tags do not add additional performance improvements (Ruder et al., 2016; Sari et al., 2017).",
      "startOffset" : 102,
      "endOffset" : 141
    }, {
      "referenceID" : 7,
      "context" : "In future work, we intend to adopt more sophisticated methods such as RecNN, as per Ji and Smith (2017), to retain more information from the RST trees while reducing the parameter size.",
      "startOffset" : 84,
      "endOffset" : 104
    } ],
    "year" : 2017,
    "abstractText" : "We explore techniques to maximize the effectiveness of discourse information in the task of authorship attribution. We present a novel method to embed discourse features in a Convolutional Neural Network text classifier, which achieves a state-ofthe-art result by a substantial margin. We empirically investigate several featurization methods to understand the conditions under which discourse features contribute non-trivial performance gains, and analyze discourse embeddings.",
    "creator" : "LaTeX with hyperref package"
  }
}