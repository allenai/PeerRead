{
  "name" : "1301.7382.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Inferring Informational Goals from Free-Text Queries: A Bayesian Approach",
    "authors" : [ "David Heckerman", "Eric Horvitz" ],
    "emails" : [ "}@microsoft.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "People using consumer software applications typically do not use technical jargon when querying an online database of help topics. Rather, they attempt to communicate their goals with common words and phrases that describe software functionality in terms of structure and objects they understand. We describe a Bayesian approach to modeling the relationship between words in a user's query for assistance and the informational goals of the user. After reviewing the general method, we describe several extensions that center on integrating additional distinctions and struc ture about language usage and user goals into the Bayesian models.\n1 Introduction\nThe management of uncertainty plays an important role in understanding the goals or intentions behind the words a person uses to communicate ideas. Prob lems with communication and understanding are ex acerbated when people attempt to describe unfamil iar ideas and concepts. We present a Bayesian ap proach to interpreting queries composed by users of software applications searching for information about the means for achieving goals with the software. In the realm of assistance with consumer software appli cations, people typically are unfamiliar with the terms that expert users or software designers may use to re fer to structures displayed in a user interface, states of data structures1 and classes of software functional ity. The problem of interpreting a user's informational goals given a query is typically one of reasoning under uncertainty.\nWe take a Bayesian perspective on information re trieval (IR) for inferring the goals and needs of soft ware users. The approach centers on the construction\nof probabilistic knowledge bases for interpreting user queries. Within the knowledge bases, variables repre senting a broad set of informational goals influence the likelihood of a user generating different words in their queries for assistance. Our work comes in the context of the broader history of investigation of probabilistic and utility-theoretic methods in IR (Maron & Kuhns, 1960; Robertson, 1977; Cooper & Maron, 1978; Tur tle & Croft, 1996; Keirn, Lewis, & Madigan, 1997). Probabilistic analyses have been employed in a vari ety of ways in information retrieval, including work on ascribing a probabilistic semantics to notions of rele vance associated with ranking strategies and learning from data. There has been growing interest in employ ing graphical probabilistic models to retrieval prob lems (Turtle & Croft, 1990; Fung & Del Favero, 1995). Discussion has continued in the IR community on the challenges and pitfalls of probabilistic representations and analysis (Cooper, 1994).\nIn contrast to previous data-centric, statistical ap proaches to information retrieval, we present our inves tigation of a more resource-intensive approach that re lies on the framing and assessment of knowledge bases by human experts. Such handcrafting of knowledge bases is infeasible for grappling with massive retrieval problems like accessing information from large, hetero geneous databases like the World Wide Web. However, we found that the approach is appropriate for building a powerful retrieval tool for use in the focused context of providing help for users of the Microsoft Office suite of applications. The Microsoft Office suite of word processing, spreadsheet analysis, database, messaging and scheduling, and presentation applications is used by millions of users. In the context of Microsoft Office, the potential value associated with gains in quality of retrieval by building and deploying a custom-tailored Bayesian IR system could justify the significant ex pense incurred by extensive user modeling.\nWe shall describe a general approach to modeling the probabilistic relationships between a user's words and\ntheir informational goals. First, we describe a basic model and updating procedure. We then discuss our work to introduce default probabilities to ease the as sessment burden as well as to leverage additional struc ture in language to enhance the performance of the system following. Finally, we discuss how the assess ment and inference strategies were scaled up to handle user assistance for applications in the Microsoft Office suite of desktop applications.\n2 User goals, queries, terms\nAt the time we initiated our project in Bayesian infor mation retrieval, managers in the Office division were finding that users were having difficulty finding assis tance efficiently. Problems with users accessing so lutions to problems and discovering functionalies were clearly rooted in problems with terminology. As an ex ample, users working with the Excel spreadsheet might have required assistance with formatting \"a graph.\" Unfortunately, Excel had no knowledge about the com mon term, \"graph,\" and only considered in its keyword indexing the term \"chart.\" One approach to this prob lem would be to construct sets of synonyms and to remain within the status quo paradigms of keyword search and full-text search. However, this approach would not have addressed the uncertain relationship between a user's query and informational goals. Users tend to use very few words in their queries and to use the same words to refer to different goals. Coherent reasoning under uncertainty appeared critical. Thus, we turned to a probabilistic approach for diagnosing a user's problems given a query.\nIn a general user-modeling approach to information re trieval, we seek to infer a probability distribution over concepts given a user's query or utterance. For ex ample, if a user states \"help with graphing,\" we would like to infer a probability distribution over a set of user goals. An appropriate probability distribution would likely be different for the query, \"changing the way the graph looks.\" We pursued a Bayesian analysis of terms\nInferring Goals from Free-Text Queries 231\nFigure 2: Bayesian network for diagnosing goals from a user query. This representation is infeasible for knowl edge acquisition given the large number of potential queries.\nto generate a probability distribution over goals.\nTraditional approaches to Bayesian IR have employed word counting to gather probabilistic information on the relationships of words to concepts. With a user modeling approach, we have the ability to leverage knowledge that is typically not available in text writ ten for online assistance with software. We are inter ested in capturing the way people actually describe their problems.\nTo build the most general model, we would have to consider all possible queries and goals, as captured by the Bayesian network in Figure 2. We structure this IR problem with a set of problem variables, representing a user's problems, influencing the probability distribu tion over plausible queries. As indicated in the figure, it may be important to represent dependencies among goals at different times in the recent past. In a high fidelity model, we might seek to consider dependencies among goals over time and represent explicit tempo ral probabilistic relationships among problems. At run time, we observe the user's query and infer probabili ties of concepts.\nIt is infeasible to represent the large number of dis tinct queries that might be composed by users. In stead, we developed an approximation. To simply the assessment and inference, we made the following as sumptions:\n• Single user problem: Only a single problem is ac tive at any time.\n• Order irrelevance: The order of terms in a query is disregarded.\n• Irrelevance of unrecognized terms: Only words in the lexicon are included in the analysis.\nTaken together, these assumptions define a Bayesian term-spotting methodology.\n232 Heckerman and Horvitz\nFigure 3: A reformulation of the problem into a Bayesian term-spotting analysis. We consider the presence and absence of terms, discarding key infor mation about the sequence of terms and dependencies among terms.\nTo ease the assessment burden we made a fourth as sumption. We additionally assumed term indepen dence: terms are independent of other terms, condi tioned on the problem. A Bayesian network for the re formulated term-spotting problem is displayed in Fig ure 3. Clearly, assumptions of term independence and order irrelevance may lead to significant information loss, given the great importance of structure and de pendencies among words in human communication. Nevertheless, we were interested in the quality of in ference for Bayesian term spotting.\nTo construct the knowledge bases, we enlisted usability specialists to assist with the identification of goals of users and their associated help topics, as well as terms that might be seen typically in queries conditioned on the goals of users. For a prototyping effort, we speci fied the set of problems associated with help topics for the Microsoft Excel application. For each problem, we identified sets of terms whose likelihood of appearing in a query would be influenced by the presence of a problem.\nTo simplify assessment, we employed a stemmer to re duce the number of terms by collapsing a multitude of derived forms into more basic roots or lemmas. For example, derived forms of the reference to printing a document, including \"print,\" \"printed\" and \"printing\" were reduced to the root, \"print.\"\nBeyond consideration of root forms of words, special phrases and distinctions were added to consideration when deemed important. Such special distinctions in cluded term with distinguished patterns of capitaliza tion (e.g., \"Word,\" as in Microsoft Word, and \"wore!' are treated as distinct terms) . Heuristics were devel oped for detecting and using phrases in lieu of single words.\nAfter constructing the problem and term distinctions, we pursued knowledge from experts on the probabili ties of terms being used in a query conditioned on the existence of each problem. Specifically, we sought, for\nFigure 4: Building a knowledge base. Usability experts worked to identify key terms used by consumers and assessed the likelihood of terms used in a query for assistance conditioned on different problems.\neach term t, the conditional probability that the term would appear in a query given a user assistance goal g linked to the terms, p(tlg,�), where � refers to the background state of knowledge of the person assessing the probability. To ease the assessment, a log scale was employed which discretized the conditional prob abilities into a set of buckets which were separated by equivalent likelihood ratios.\nGiven an assessed Bayesian model for term spotting, we can identify the presence of terms in queries and employ Bayesian updating to infer the likelihood of al ternate user goals-and direct users to appropriate help topics. At run time, we collect recognized terms from a query, reduce them to root form and input the find ings for inference. We then compute the probability of all user problems (as represented by appropriate help topics) , given all of the terms present (t+) and absent (r) in a query, denoted p(glt+r, �). Given the in dependence assumptions we have made, the posterior probability of each topic is given by\np(g; lt+r; �) = a p(g; I�) Tij p(tj lg;, 0 Tik 1 - p(t; lg;, �) (1)\nwhere p(g; I�) is the prior probability of each user in formational goal g; and a is a renormalizing constant equal to the probability of seeing the set of terms in the phrase. We do not need to compute a if we wish only to generate a ranking.\n3 Leak probabilities and assessment\nMethods for limiting knowledge acquisition effort are paramount in attempts to handcraft a knowledge base for Bayesian IR. To ease the assessment burden, we de veloped a means for focusing probability assessment on relevant terms-user goal relationships. In fleshing out the term and goal distinctions in the knowledge base,\nexperts were asked to identify terms associated with each goal that were \"relevant\" or made more likely by the presence of that problem. As indicated in Figure 4, these positive influences were indicated by establishing a link between each user goal and the term positively influenced by the presence of the user goal.\nDuring probability assessment, we explicitly assessed only the conditional probabilities of terms t for user goals g linked to the terms. Rather than assess con ditional probabilities for terms not explicitly linked to topics, we assume a small default leak probability, p(tjg,e) = c, for the likelihood of seeing a term given an unlinked topic. The leak probability captures the notion that user goals may cause terms to appear in a query even if they are not believed to be relevant at the time the probabilistic IR model is constructed.\nAs captured by F igure 5, at run time, we consider whether or not terms are present in the query, and, for each user goal, whether the goal is explicitly linked to a term or not. We consider four possible outcomes for each help topic and term in the knowledge base:\n1. A user goal has links to a term that is not in the query\n2. A user goal has links to a term that is in the query\n3. A user goal does not have links to a term that is in a query\n4. A user goal does not have links to a term that is not in the query\nTo compute the posterior probability for a user goal, for each term appearing in the query that is not linked to that goal, we fold in the small leak probability. Thus, when a query is analyzed, we consider each of the four conditions, and compute the probability of each user goal as\np(g; it+r; e) = (2)\na p(g; le) nj p(tj jg;, e) Tik[l- p(tk jg;, e)]c1(1- c)m\nInferring Goals from Free-Text Queries 233\nwhere tk are terms linked to user goal g; but not seen in the query, tj are terms seen in the query and linked to user informational goal g;, l is the number of terms seen in the query but not linked to the goal, and m is the number of terms that are not linked to the goal and not seen in the query.\n4 Prototyping and refinement\nWe worked with the Microsoft Office division to build an initial prototype of the Bayesian term-spotting methodology. The prototype was based on a knowl edge base composed of approximately 600 terms to reason about the likelihood of about 40 user problems, abstracted to cover a large class of problems in the Ex cel domain. Informal validation of the performance of the system demonstrated that the system performed significantly better than keyword-based help systems, and a commitment was made to begin to scale the system to realistic databases covering thousands of help topics. Work on the scaling the system to such large databases and continued testing led to further improvements.\nSeveral refinements to the basic Bayesian approach were introduced during the period of testing. Specif ically, we developed additional abstractions of sets of terms that have related meaning and added new struc ture based in language usage in the user assistance realm.\n4.1 Additional abstraction of terms\nAs part of scaling up the size of the Bayesian IR knowl edge bases, we found opportunity for providing addi tional abstraction of terms to minimize the number of assessments and links. We developed the notion of a metanym to refer to sets of words that are probabilis tically influenced by the existence of user problems in a similar manner. Metanyms include phrases that point at the same basic observation or concept, more closely related synonyms, hypernyms, and hyponyms. As an example, a user may employ the terms \"delete,\" \"erase,\" \"remove,\" \"kill,\" \"lose,\" and \"get rid of\" in an equivalent pattern of usage. To reduce the number of links and assessments, we can define a metanym re ferred to as \"deletion\" and define it with terms that refer to the concept of deleting. Seeing any term con tained in the metanym activates the metanym. The the probabilities of metanym conditioned on user prob lem is stored and used in the Bayesian information retrieval.\n234 Heckerman and Horvitz\n4.2 Modeling language about existing and desired states\nIn the coarse of building and testing early prototypes of the Bayesian term-spotting methodology, we iden tified several opportunities for refining the approach centering on a consideration of patterns in natural queries. In particular, specific cases where inappro priate topics percolated to the top of a ranked list led to valuable insights about how we might integrate ad ditional knowledge about the relationship of user goals and language used in queries.\nWe found that typical queries for assistance with soft ware frequently contain noun phrases that refer to ei ther ( 1) the current state of affairs and objects that exist now, or to (2) desired states of affairs achievable via the transformation of the current state of affairs with some software functionality or through through the creation of new objects. As an example, the prob abilistic relationships between \"chart\" and sets of user problems depend on whether \"chart\" is used in the phrase \"this chart\" versus when the word is used in the phrase \"a chart.\" Thus, the conditional proba bilities of terms given user problems may depend on whether the terms are being used to refer to existing or desired objects. Articles (\"a chart,\" \"this chart\" ) , prepositions (e.g., \"under this chart\" ) , and adjectives adjacent to noun phrases appeared to be rich sources of evidence about the form of word.\nWe found that our intuitions about the importance of existing versus desired objects in understanding the goal of queries coincided with the notion of definite ness studied by linguists. Indo-European languages as well as many non-Indo-European languages (e. g., Japanese) make use of special words and structure to communicate existing versus desired objects and states of affairs. For example in English, definite articles, in cluding \"the\" and \"this,\" adjacent to noun phrases typicaly signals that an object exists, while indefinite articles, such as \"a\" and \"some,\" implies nonexistence of the object described in the adjacent noun phrase.\nGiven the potential importance of discriminating ex isting versus desired objects or states in interpreting queries for assistance with software applications, we developed a Bayesian approach to modeling terms used in the definite versus the indefinite sense. The ap proach is based on the observation that the type and number of functional words such as articles, conjunc tions, prepositions, and possessives provide evidence about the probability that objects are being referred to in the definite form. For example, the use of pos sessives is strong evidence that the noun referred to by a function word exists. Consider the phrase, \"I 'd like to change the colors of text under my chart.\" The\nDesired result\nFigure 6: A Bayesian approach to considering indefi niteness in queries for assistance. We identify clauses and compute the probability of indefinite usage of terms based on adjacent function words.\npreposition, \"under,\" and the possessive, \"my,\" tells us that the chart is likely to exist. On the other hand, consider the phrase, \"How can I create a chart?\" The article \"a\" is an indefinite article, as it suggests that \"chart\" is a desired, but as of yet, nonexisting object.\nWe integrated an analysis of the use of the definite versus the indefinite form of a term with the basic Bayesian information retrieval methods we have de scribed. In the approach, an expert constructing the knowledge base has the option of noting that the use of a term or metanym can be split into the indefinite and definite uses, and to indicate distinct links between user goals and the different uses of the terms. Then, conditional probabilities are assessed for the likelihood of a term being used in a query given the indefinite usage, p(t+ii,gi,e), and for the definite usage of the term, p(t+ ID, 9i' e).\nFigure 6 captures the run-time analysis of the definite versus indefinite usage. When a query is analyzed, function words such as articles, possessives, and prepo sitions are noted and used to identify noun clauses. Then the function words adjacent to and modifying noun clauses are used to compute the probability that the noun in the adjacent clause is being used in the indefinite versus the definite sense.\nTo model the likelihood of indefinite usage, we consider the probability, p(IIF1 ... Fn, g, e), that the terms in a clause are being used in the indefinite form, given adjacent function words F1 ... Fn. We can simplify the additional assessment task by assuming that I is conditionally independent of the goals and construct functions that estimate the probability of indefinite use given the set of function words adjacent to the clause. If we make an additional assumption that func tion words are independent of other function words given I, we can compute p(IIF1 ... Fn,e) from an as sessment of the likelihood of a function word given the presence of a noun of indefinite form, p(FII, e), and a\nprior probability of the indefinite form, p(IIe). Given a knowledge base extended with assessments of the likelihoods for indefinite and definite forms of terms conditioned on goals, we use p(IIF1 .. Fn, e) to compute p(t+lg;,e) as follows:\np(t+lg;,e) = p(t+II,g;,e) p(IIF1 ... Fn,e) + p(t+I..,I,g;,e) p(..,IIFl···Fn,e) (3)\nIf there are no function words yielding information about the usage in the adjacent noun clause, we simply use the prior probability of the indefinite form.\nAssessing and implementing the existence versus de sired usages in the prototype led to improvements in the performance of the system. Most noticeable was the appropriate lowering of probabilities assigned to topics related to the creation of new objects when queries were issues about modifications of existing ob jects.\n4.3 Disambiguating noun and verb usages\nWe also sought to enhance the system by adding knowledge of additional structure in language. In En glish, many words can be used as nouns or verbs de pending on the structure of the phrase. As an exam ple, consider the word \"print\" appearing in the phrase \"How do I print this?\" (verb form) or the phrase \"How can I make this print darker?\" (noun). The property describing the dual use of these words is called zero derivation.\nWe can enhance the accuracy of the Bayesian term spotting analysis by assessing separate probabilities and links for the noun form of words and the verb form of words. That is, for a subset of specially marked zero-derivation terms, we link and assess conditional probabilities for the noun form, p(t+IN,g;,e) and the verb form, p(t+ I..,N, g;, e)' of usage. A set of function words that appear after and before the zero-derivation terms give us deterministic knowledge on whether the term is being used as a noun or verb. At run-time, we employ templates that detect whether zero-derivation terms are being used in the noun form or verb form and return information about the use of the word. The appropriate conditional probabilities is then passed to the base probabilistic updating described above.\nOn rare occasions where we are uncertain of the usage, we can gather evidence about the probability that the word is being used in noun or verb form and compute the conditional probabilities of the term given goals, similar to the computation of probabilities in the con text of uncertainty about the existing versus nonexist ing usage. We expand the independence assumptions described in Section 4.2 to assume that g is indepen-\nInferring Goals from Free-Text Queries 235\ndent of N and I given F to compute p(t+ lg;, e).\n5 Scaling the approach to the real world\nGiven the solid performance of the prototype, a deci sion was made by the Microsoft Office product divi sion to collaborate with our team on scaling the ap proach up to handle thousands of topics and for the creation of distinct knowledge bases for foreign lan guages where Microsoft Office has a large user base. Knowledge bases were created for the Microsoft Of fice Suite, including Access, Excel, Powerpoint, Word, Outlook, as well as for Microsoft Project application, employing a derivative updating scheme.\n5.1 Framing and Assessment\nAs part of the effort of scaling up the approach, an assessment and testing environment was created for building knowledge bases and team of usability experts was assembled for enumerating distinctions and for as sessing conditional probabilities. This group of people includes psychologists and other specialists in human computer interaction.\nDuring the early phases of assembling a team and con structing knowledge bases, numerous questions arose on the process of assessing conditional probabilities. These questions led to the development of guidelines and prototypical examples to assist with transmitting a unified vision of the nature of the probability of terms conditioned on user goals.\nThe implementation team found that it was help ful to allow experts to assess conditional probabili ties on a scale of 1 to 13, and to later remap the assessed numbers to probabilities. The mapping be tween the numbers and probabilities, and associated guidelines for experts, was defined by assuming equal likelihood ratios among adjacent buckets. Definitions were provided for each point on the scale with exam ples and natural-language definitions that character ized the likelihood of terms, given the presence of a user goal. Guidelines were created in terms of typical sentences and parts of speech used to refer to specific goals. Such definitions assisted with explaining the task to assessors as well as for normalizing the proba bilities assessed by different groups of people.\nThe assessment task posed a significant challenge, in cluding formulating a distinct set of topics represent ing distinct user goals, identifying terms and phrases employed by users to describe their goals, to explic itly linking terms to multiple topics, and to assessing the conditional probabilities. The Bayesian IR model for the Microsoft Word application is representative\n236 Heckerman and Horvitz\nFigure 7: A derivative of the Bayesian term-spotting approach was scaled up to handle thousands of topics and served as the primary user assistance system in Microsoft Office '95, named Answer Wizard.\nfor the Office applications. In Office '95, the Bayesian model for Microsoft Word included over 1,000 topics, 5,000 terms, and 145,000 dependencies.\nEach full-time usability expert completed on average about 40 topics per week. After creating an English version, the models were translated into twelve lan guages, including German, French, Spanish, Italian, Swedish, Japanese, Brazilian Portuguese, Dutch, Dan ish, Norwegian, Korean, and Traditional Chinese. Lo calization of the English knowledge base to the other languages was found to take approximately one month for a full-time person.\n5.2 Testing the Knowledge Bases\nTo test the knowledge bases as well as to monitor for problems as real-world implementations were con structed, refined, and integrated into products, a large, covering set of \"smoke test\" queries was created for each software application. These queries were gleaned by usability experts from online forums, email, and studies with human subjects at Microsoft's usability labs. The performance requirement was to have a good answer for queries appear in the top five of the re turned list for no less than 99 per cent of the queries.\nStudies were also performed with users. In one of the studies, users of intermediate skill were given a set of tasks for the Excel, Word, and Powerpoint applica tions. To minimize the influence of task descriptions on the words used by users, participants were shown pictures of the tasks. Users were given up to four at tempts to rephrase their questions for each informa tional goal. The study considered the performance of\nthe term-spotting analysis as well as a broader, end-to end evaluation of the overall success of assisting users with their goals. In addition to evaluating return ac curacy, the study explored the ability of users to rec ognize appropriate topics when they appeared on the return list and to understand that the text within a topic applies to their goal.\nThe correct topic was found to appear within the top five recommendations on the return list approximately 75 percent of the time. For these cases, users were found to recognize the appropriate topic about 75 per cent of the time. Furthermore, users recognizing and selecting the correct topic would understand the ap plicability of the topic content to their goal about 95 percent of the time. Considering these steps, users were found to have an overall success with accessing and understanding assistance in about 55 percent of the cases. Nearly 75 percent of the complete successes occurred on the first query. Similar results were ob tained in tests run on the knowledge bases localized to other languages. A large fraction of failures was at tributed to users inputting single, vague words instead of describing their goal more naturally, in a manner they might request assistance from a colleague.\n5.3 Product Realization\nThe Bayesian term-spotting methodology served as the basis for the Answer Wizard help system in the Of fice '95 suite of applications. Figure 7 displays the user interface for entering queries and reviewing recommen dations. In the Office '97 product line, the Bayesian term-spotting approach was extended with a proba-\nbilistic analysis of contextual information, including user actions (e.g., cutting and pasting, dwelling on documents or displayed objects, etc.) and the status of data structures, leading to a help system named, the Office Assistant. Details on Bayesian user modeling, considering context information in IR, and the Office Assistant are found in Horvitz, Breese, Heckerman et al., 1998.\n6 Conclusion\nWe have described a Bayesian term-spotting method ology that allows users of computer software to request assistance by composing natural free-text queries. We presented a basic set of assumptions and a Bayesian updating method, and then described how we ex tended the initial approach by considering additional structure in queries.\nThe Bayesian framework we provided allowed for the construction and assessment of a large probabilistic information retrieval system by a team of usability ex perts. The costs of the intensive assessment effort re quired to build a large handcrafted knowledge base typically may overwhelm the value of enhanced infor mation retrieval. However, for the case of enhancing a product used by many millions of people, the detailed assessment can be worth the cost of manual construc tion of a database. We are now exploring means for au tomating the construction of Bayesian term-spotting knowledge bases.\nAcknowledgments\nWe are indebted to Erich Finkelstein and Sam Hob son for alerting us to the need for enhancing access to assistance in software applications. Erich Finkelstein carried out the initial prototyping work in collabora tion with researchers in our group. Sam Hobson led Microsoft Office decision making on making a com mitment to implement the Bayesian term-spotting ap proach in Office. Adrian Klein managed the overall details of the real-world scaling up of this technology into the Answer Wizard in Microsoft Office. Kristin Dukay and Eric Hawley led the team of usability ex perts who built the Answer Wizard knowledge bases. Leah Kaufman and Marcella Rader designed and im plemented the user studies. We thank Jack Breese, Karen Jensen, and Greg Shaw for their feedback and comments on the methods.\nReferences\nCooper, W. (1994). The formalism of probability the ory in IR: A foundation or an encumbrance. In\nInferring Goals from Free-Text Queries 237\nCroft, W. and van Rijsbergen, C., editors, Pro ceedings of the Seventeenth Annual SIGIR, pages 242-247. ACM SIGIR, Springer-Verlag.\nCooper, W. and Maron, M. (1978). Foundations of probabilistic and utility-theoretic indexing. Journal of the ACM, 25:67-80.\nFung, R. and Favero, B. D. (1995). Applying bayesian networks to information retrieval. Communica tions of the ACM, 38(3):42-48.\nHorvitz, E., Breese, J., Heckerman, D., Hovel, D., and Rommelse, K. (1998). The Lumiere Project: Bayesian user modeling for inferring the goals and needs of software users. In Proceedings of the Fourteenth Conference on Uncertainty in Artifi cial Intelligence. AUAI, Morgan Kaufmann.\nKeirn, M., Lewis, D., and Madigan, D. (1997). Bayesian information retrieval: Preliminary evaluation. In In Preliminary Papers of the Sixth International Workshopon Artificial Intelligence and Statistics, pages 303-310. AI and Statistics.\nMaron, M. and Kuhns, J. (1960). On relevance, proba bilistic indexing and information retrieval. Jour nal of the ACM, 7:216-244.\nRobertson, S. (1977). The probability ranking prin ciple in IR. Journal of Documentation, 33:294- 304.\nTurtle, H. and Croft, W. B. (1990). Inference net works for document retrieval. In Proceedings of the 1990 SIGIR, pages 1-24. ACM SIGIR, ACM.\n· Turtle, H. and Croft, W. B. (1996). Uncertainty in in formation retrieval systems. Uncertainty Man agement in Information Systems 1996, pages 189-224."
    } ],
    "references" : [ {
      "title" : "The formalism of probability the­",
      "author" : [ "W. Cooper" ],
      "venue" : null,
      "citeRegEx" : "Cooper,? \\Q1994\\E",
      "shortCiteRegEx" : "Cooper",
      "year" : 1994
    }, {
      "title" : "Foundations of probabilistic and utility-theoretic indexing",
      "author" : [ "W. Cooper", "M. Maron" ],
      "venue" : "Journal of the ACM,",
      "citeRegEx" : "Cooper and Maron,? \\Q1978\\E",
      "shortCiteRegEx" : "Cooper and Maron",
      "year" : 1978
    }, {
      "title" : "Applying bayesian networks to information retrieval",
      "author" : [ "R. Fung", "B.D. Favero" ],
      "venue" : "Communica­ tions of the ACM,",
      "citeRegEx" : "Fung and Favero,? \\Q1995\\E",
      "shortCiteRegEx" : "Fung and Favero",
      "year" : 1995
    }, {
      "title" : "The Lumiere Project: Bayesian user modeling for inferring the goals and needs of software users",
      "author" : [ "E. Horvitz", "J. Breese", "D. Heckerman", "D. Hovel", "K. Rommelse" ],
      "venue" : "In Proceedings of the Fourteenth Conference on Uncertainty in Artifi­",
      "citeRegEx" : "Horvitz et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Horvitz et al\\.",
      "year" : 1998
    }, {
      "title" : "Bayesian information retrieval: Preliminary evaluation",
      "author" : [ "M. Keirn", "D. Lewis", "D. Madigan" ],
      "venue" : "Preliminary Papers of the Sixth International Workshopon Artificial Intelligence and Statistics,",
      "citeRegEx" : "Keirn et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Keirn et al\\.",
      "year" : 1997
    }, {
      "title" : "On relevance, proba­ bilistic indexing and information retrieval",
      "author" : [ "M. Maron", "J. Kuhns" ],
      "venue" : "Jour­ nal of the ACM,",
      "citeRegEx" : "Maron and Kuhns,? \\Q1960\\E",
      "shortCiteRegEx" : "Maron and Kuhns",
      "year" : 1960
    }, {
      "title" : "The probability ranking prin­ ciple in IR",
      "author" : [ "S. Robertson" ],
      "venue" : "Journal of Documentation,",
      "citeRegEx" : "Robertson,? \\Q1977\\E",
      "shortCiteRegEx" : "Robertson",
      "year" : 1977
    }, {
      "title" : "Inference net­ works for document retrieval",
      "author" : [ "H. Turtle", "W.B. Croft" ],
      "venue" : "In Proceedings of the 1990 SIGIR,",
      "citeRegEx" : "Turtle and Croft,? \\Q1990\\E",
      "shortCiteRegEx" : "Turtle and Croft",
      "year" : 1990
    }, {
      "title" : "Uncertainty in in­ formation retrieval systems",
      "author" : [ "H. Turtle", "W.B. Croft" ],
      "venue" : "Uncertainty Man­ agement in Information Systems",
      "citeRegEx" : "Turtle and Croft,? \\Q1996\\E",
      "shortCiteRegEx" : "Turtle and Croft",
      "year" : 1996
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "Our work comes in the context of the broader history of investigation of probabilistic and utility-theoretic methods in IR (Maron & Kuhns, 1960; Robertson, 1977; Cooper & Maron, 1978; Tur­ tle & Croft, 1996; Keirn, Lewis, & Madigan, 1997).",
      "startOffset" : 123,
      "endOffset" : 238
    }, {
      "referenceID" : 0,
      "context" : "Discussion has continued in the IR community on the challenges and pitfalls of probabilistic representations and analysis (Cooper, 1994).",
      "startOffset" : 122,
      "endOffset" : 136
    } ],
    "year" : 2011,
    "abstractText" : "People using consumer software applications typically do not use technical jargon when querying an online database of help topics. Rather, they attempt to communicate their goals with common words and phrases that describe software functionality in terms of structure and objects they understand. We describe a Bayesian approach to modeling the relationship between words in a user's query for assistance and the informational goals of the user. After reviewing the general method, we describe several extensions that center on integrating additional distinctions and struc­ ture about language usage and user goals into the Bayesian models.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}