{
  "name" : "396.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Jianwei Yang", "Anitha Kannan" ],
    "emails" : [ "jw2yang@vt.edu", "akannan@fb.com", "parikh}@gatech.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We present LR-GAN: an adversarial image generation model which takes scene structure and context into account. Unlike previous generative adversarial networks (GANs), the proposed GAN learns to generate image background and foregrounds separately and recursively, and stitch the foregrounds on the background in a contextually relevant manner to produce a complete natural image. For each foreground, the model learns to generate its appearance, shape and pose. The whole model is unsupervised, and is trained in an end-to-end manner with gradient descent methods. The experiments demonstrate that LR-GAN can generate more natural images with objects that are more human recognizable than DCGAN."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Generative adversarial networks (GANs) (Goodfellow et al., 2014) have shown significant promise as generative models for natural images. A flurry of recent work has proposed improvements over the original GAN work for image generation (Radford et al., 2015; Denton et al., 2015; Salimans et al., 2016; Chen et al., 2016; Zhu et al., 2016; Zhao et al., 2016), multi-stage image generation including part-based models (Im et al., 2016; Kwak & Zhang, 2016), image generation conditioned on input text or attributes (Mansimov et al., 2015; Reed et al., 2016b;a), image generation based on 3D structure (Wang & Gupta, 2016), and even video generation (Vondrick et al., 2016).\nWhile the holistic ‘gist’ of images generated by these approaches is beginning to look natural, there is clearly a long way to go. For instance, the foreground objects in these images tend to be deformed, blended into the background, and not look realistic or recognizable.\nOne fundamental limitation of these methods is that they attempt to generate images without taking into account that images are 2D projections of a 3D visual world, which has a lot of structures in it. This manifests as structure in the 2D images that capture this world. One example of this structure is that images tend to have a background, and foreground objects are placed in this background in contextually relevant ways.\nWe develop a GAN model that explicitly encodes this structure. Our proposed model generates images in a recursive fashion: it first generates a background, and then conditioned on the background generates a foreground along with a shape (mask) and a pose (affine transformation) that together define how the background and foreground should be composed to obtain a complete image. Conditioned on this composite image, a second foreground and an associated shape and pose are generated, and so on. As a byproduct in the course of recursive image generation, our approach generates some object-shape foreground-background masks in a completely unsupervised way, without access to any object masks for training. Note that decomposing a scene into foreground-background layers is a classical ill-posed problem in computer vision. By explicitly factorizing appearance and transformation, LR-GAN encodes natural priors about the images that the same foreground can be ‘pasted’ to the different backgrounds, under different affine transformations. According to the experiments, the absence of these priors result in degenerate foreground-background decompositions, and thus also degenerate final composite images.\n∗Work was done while visiting Facebook AI Research.\nWe mainly evaluate our approach on four datasets: MNIST-ONE (one digit) and MNIST-TWO (two digits) synthesized from MNIST (LeCun et al., 1998), CIFAR-10 (Krizhevsky & Hinton, 2009) and CUB-200 (Welinder et al., 2010). We show qualitatively (via samples) and quantitatively (via evaluation metrics and human studies on Amazon Mechanical Turk) that LR-GAN generates images that globally look natural and contain clear background and object structures in them that are realistic and recognizable by humans as semantic entities. An experimental snapshot on CUB-200 is shown in Fig. 1. We also find that LR-GAN generates foreground objects that are contextually relevant to the backgrounds (e.g., horses on grass, airplanes in skies, ships in water, cars on streets, etc.). For quantitative comparison, besides existing metrics in the literature, we propose two new quantitative metrics to evaluate the quality of generated images. The proposed metrics are derived from the sufficient conditions for the closeness between generated image distribution and real image distribution, and thus supplement existing metrics."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "Early work in parametric texture synthesis was based on a set of hand-crafted features (Portilla & Simoncelli, 2000). Recent improvements in image generation using deep neural networks mainly fall into one of the two stochastic models: variational autoencoders (VAEs) (Kingma et al., 2016) and generative adversarial networks (GANs) (Goodfellow et al., 2014). VAEs pair a top-down probabilistic generative network with a bottom up recognition network for amortized probabilistic inference. Two networks are jointly trained to maximize a variational lower bound on the data likelihood. GANs consist of a generator and a discriminator in a minmax game with the generator aiming to fool the discriminator with its samples with the latter aiming to not get fooled.\nSequential models have been pivotal for improved image generation using variational autoencoders: DRAW (Gregor et al., 2015) uses attention based recurrence conditioning on the canvas drawn so far. In Eslami et al. (2016), a recurrent generative model that draws one object at a time to the canvas was used as the decoder in VAE. These methods are yet to show scalability to natural images. Early compelling results using GANs used sequential coarse-to-fine multiscale generation and classconditioning (Denton et al., 2015). Since then, improved training schemes (Salimans et al., 2016) and better convolutional structure (Radford et al., 2015) have improved the generation results using\nGANs. PixelRNN (van den Oord et al., 2016) is also recently proposed to sequentially generates a pixel at a time, along the two spatial dimensions.\nIn this paper, we combine the merits of sequential generation with the flexibility of GANs. Our model for sequential generation imbibes a recursive structure that more naturally mimics image composition by inferring three components: appearance, shape, and pose. One closely related work combining recursive structure with GAN is that of Im et al. (2016) but it does not explicitly model object composition and follows a similar paradigm as by Gregor et al. (2015). Another closely related work is that of Kwak & Zhang (2016). It combines recursive structure and alpha blending. However, our work differs in three main ways: (1) We explicitly use a generator for modeling the foreground poses. That provides significant advantage for natural images, as shown by our ablation studies; (2) Our shape generator is separate from the appearance generator. This factored representation allows more flexibility in the generated scenes; (3) Our recursive framework generates subsequent objects conditioned on the current and previous hidden vectors, and previously generated object. This allows for explicit contextual modeling among generated elements in the scene. See Fig. 17 for contextually relevant foregrounds generated for the same background, or Fig. 6 for meaningful placement of two MNIST digits relative to each.\nModels that provide supervision to image generation using conditioning variables have also been proposed: Style/Structure GANs (Wang & Gupta, 2016) learns separate generative models for style and structure that are then composed to obtain final images. In Reed et al. (2016a), GAN based image generation is conditioned on text and the region in the image where the text manifests, specified during training via keypoints or bounding boxes. While not the focus of our work, the model proposed in this paper can be easily extended to take into account these forms of supervision."
    }, {
      "heading" : "3 PRELIMINARIES",
      "text" : ""
    }, {
      "heading" : "3.1 GENERATIVE ADVERSARIAL NETWORKS",
      "text" : "Generative Adversarial Networks (GANs) consist of a generator G and a discriminator D that are simultaneously trained with competing goals: The generator G is trained to generate samples that can ‘fool’ a discriminator D, while the discriminator is trained to classify its inputs as either real (coming from the training dataset) or fake (coming from the samples of G). This competition leads to a minmax formulation with a value function:\nmin θG max θD\n( Ex∼pdata(x)[log(D(x; θD))] + Ez∼pz(z)[log(1−D(G(z; θG); θD))] ) , (1)\nwhere z is a random vector from a standard multivariate Gaussian or a uniform distribution pz(z), G(z; θG) maps z to the data space, D(x) is the probability that x is real estimated by D. The advantage of the GANs formulation is that it lacks an explicit loss function and instead uses the discriminator to optimize the generative model. The discriminator, in turn, only cares whether the sample it receives is on the data manifold, and not whether it exactly matches a particular training example (as opposed to losses such as MSE). Hence, the discriminator provides a gradient signal only when the generated samples do not lie on the data manifold so that the generator can readjust its parameters accordingly. This form of training enables learning the data manifold of the training set and not just optimizing to reconstruct the dataset, as in autoencoder and its variants.\nWhile the GANs framework is largely agnostic to the choice of G and D, it is clear that generative models with the ‘right’ inductive biases will be more effective in learning from the gradient information (Denton et al., 2015; Im et al., 2016; Gregor et al., 2015; Reed et al., 2016a; Yan et al., 2015). With this motivation, we propose a generator that models image generation via a recurrent process – in each time step of the recurrence, an object with its own appearance and shape is generated and warped according to a generated pose to compose an image in layers."
    }, {
      "heading" : "3.2 LAYERED STRUCTURE OF IMAGE",
      "text" : "An image taken of our 3D world typically contains a layered structure. One way of representing an image layer is by its appearance and shape. As an example, an image x with two layers, foreground f and background b may be factorized as:\nx = f m+ b (1−m), (2)\nwhere m is the mask depicting the shapes of image layers, and the element wise multiplication operator. Some existing methods assume the access to the shape of the object either during training (Isola & Liu, 2013) or both at train and test time (Reed et al., 2016a; Yan et al., 2015). Representing images in layered structure is even straightforward for video with moving objects (Darrell & Pentland, 1991; Wang & Adelson, 1994; Kannan et al., 2005). Vondrick et al. (2016) generates videos by separately generating a fixed background and moving foregrounds. A similar way of generating single image can be found in Kwak & Zhang (2016).\nAnother way is modeling the layered structure with object appearance and pose as:\nx = ST (f ,a) + b, (3)\nwhere f and b are foreground and background, respectively; a is the affine transformation; ST is the spatial transformation operator. Several works fall into this group (Roux et al., 2011; Huang & Murphy, 2015; Eslami et al., 2016). In Huang & Murphy (2015), images are decomposed into layers of objects with specific poses in a variational autoencoder framework, while the number of objects (i.e., layers) is adaptively estimated in Eslami et al. (2016).\nTo contrast with these works, LR-GAN uses a layered composition, and the foreground layers simultaneously model all three dominant factors of variation: appearance f , shape m and pose a. We will elaborate it in the following section."
    }, {
      "heading" : "4 LAYERED RECURSIVE GAN (LR-GAN)",
      "text" : "The basic structure of LR-GAN is similar to GAN: it consists of a discriminator and a generator that are simultaneously trained using the minmax formulation of GAN, as described in §.3.1. The key innovation of our work is the layered recursive generator, which is what we describe in this section.\nThe generator in LR-GAN is recursive in that the image is constructed recursively using a recurrent network. Layered in that each recursive step composes an object layer that is ‘pasted’ on the image generated so far. Object layer at timestep t is parameterized by the following three constituents – ‘canonical’ appearance ft, shape (or mask) mt, and pose (or affine transformation) at for warping the object before pasting in the image composition.\nFig. 2 shows the architecture of the LR-GAN with the generator architecture unrolled for generating background x0 ( . = xb) and foreground x1 and x2. At each time step t, the generator composes the next image xt via the following recursive computation:\nxt = ST (mt,at)︸ ︷︷ ︸ affine transformed mask ST (ft,at)︸ ︷︷ ︸ affine transformed appearance +(1− ST (mt,at)) xt−1︸ ︷︷ ︸ pasting on image composed so far , ∀t ∈ [1, T ] (4)\nwhere ST ( ,at) is a spatial transformation operator that outputs the affine transformed version of with at indicating parameters of the affine transformation. Since our proposed model has an explicit transformation variable at that is used to warp the object, it can learn a canonical object representation that can be re-used to generate scenes where the object occurs as mere transformations of it, such as different scales or rotations. By factorizing the appearance, shape and pose, the object generator can focus on separately capturing regularities in these three factors that constitute an object. We will demonstrate in our experiments that removing these factorizations from the model leads to its spending capacity in variability that may not solely be about the object in Section 5.5 and 5.6."
    }, {
      "heading" : "4.1 DETAILS OF GENERATOR ARCHITECTURE",
      "text" : "Fig. 2 shows our LR-GAN architecture in detail – we use different shapes to indicate different kinds of layers (convolutional, fractional convolutional, (non)linear, etc), as indicated by the legend. Our model consists of two main pieces – a background generator Gb and a foreground generator Gf . Gb and Gf do not share parameters with each other. Gb computation happens only once, while Gf is recurrent over time, i.e., all object generators share the same parameters. In the following, we will introduce each module and connections between them.\nTemporal Connections. LR-GAN has two kinds of temporal connections – informally speaking, one on ‘top’ and one on ‘bottom’. The ‘top’ connections perform the act of sequentially ‘pasting’\nobject layers (Eqn. 4). The ‘bottom’ connections are constructed by a LSTM on the noise vectors z0, z1, z2. Intuitively, this noise-vector-LSTM provides information to the foreground generator about what else has been generated in past. Besides, when generating multiple objects, we use a pooling layer P cf and a fully-connected layer E c f to extract the information from previous generated object response map. By this way, the model is able to ‘see’ previously generated objects.\nBackground Generator. The background generatorGb is purposely kept simple. It takes the hidden state of noise-vector-LSTM h0l as the input and passes it to a number of fractional convolutional layers (also called ‘deconvolution’ layer in some papers) to generate images at its end. The output of background generator xb will be used as the canvas for the following generated foregrounds.\nForeground Generator. The foreground generatorGf is used to generate an object with appearance and shape. Correspondingly, Gf consists of three sub-modules, Gcf , which is a common ‘trunk’ whose outputs are shared by Gif and G m f . G i f is used to generate the foreground appearance ft, while Gmf generates the mask mt for the foreground. All three sub-modules consists of one or more fractional convolutional layers combined with batch-normalization and nonlinear layers. The generated foreground appearance and mask have the same spatial size as the background. The top of Gmf is a sigmoid layer in order to generate one channel mask whose values range in (0, 1).\nSpatial Transformer. To spatially transform foreground objects, we need to estimate the transformation matrix. As in Jaderberg et al. (2015), we predict the affine transformation matrix with a linear layer Tf that has six-dimensional outputs. Then based on the predicted transformation matrix, we use a grid generator Gg to generate the corresponding sampling coordinates in the input for each location at the output. The generated foreground appearance and mask share the same transformation matrix, and thus the same sampling grid. Given the grid, the sampler S will simultaneously sample the ft and mt to obtain f̂t and m̂t, respectively. Different from Jaderberg et al. (2015), our sampler here normally performs downsampling, since the the foreground typically has smaller size than the background. Pixels in f̂t and m̂t that are from outside the extent of ft and mt are set to zero. Finally, f̂t and m̂t are sent to the compositor C which combines the canvas xt−1 and f̂t through layered composition with blending weights given by m̂t (Eqn. 4).\nPseudo-code for our approach and detailed model configuration are provided in the Appendix."
    }, {
      "heading" : "4.2 NEW EVALUATION METRICS",
      "text" : "Several metrics have been proposed to evaluate GANs, such as Gaussian parzen window (Goodfellow et al., 2014), Generative Adversarial Metric (GAM) (Im et al., 2016) and Inception Score (Salimans et al., 2016). The common goal is to measure the similarity between the generated data distribution Pg(x) = G(z; θz) and the real data distribution P (x). Most recently, Inception Score has been used in several works (Salimans et al., 2016; Zhao et al., 2016). However, it is an assymetric metric and could be easily fooled by generating centers of data modes.\nIn addition to these metrics, we present two new metrics based on the following intuition – a sufficient (but not necessary) condition for closeness of Pg(x) and P (x) is closeness of Pg(x|y) and P (x|y), i.e., distributions of generated data and real data conditioned on all possible variables of interest y, e.g., category label. One way to obtain this variable of interest y is via human annotation. Specifically, given the data sampled from Pg(x) and P (x), we ask people to label the category of the samples according to some rules. Note that such human annotation is often easier than comparing samples from the two distributions (e.g., because there is no 1:1 correspondence between samples to conduct forced-choice tests).\nAfter the annotations, we need to verify whether the two distributions are similar in each category. Clearly, directly comparing the distributions Pg(x|y) and P (x|y) may be as difficult as comparing Pg(x) and P (x). Fortunately, we can use Bayes rule and alternatively compare Pg(y|x) and P (y|x), which is a much easier task. In this case, we can simply train a discriminative model on the samples from Pg(x) and P (x) together with the human annotations about categories of these samples. With a slight abuse of notation, we use Pg(y|x) and P (y|x) to denote probability outputs from these two classifiers (trained on generated samples vs trained on real samples). We can then use these two classifiers to compute the following two evaluation metrics:\nAdversarial Accuracy: Computes the classification accuracies achieved by these two classifiers on a validation set, which can be the training set or another set of real images sampled from P (x). If Pg(x) is close to P (x), we expect to see similar accuracies.\nAdversarial Divergence: Computes the KL divergence between Pg(y|x) and P (y|x). The lower the adversarial divergence, the closer two distributions are. The low bound for this metric is exactly zero, which means Pg(y|x) = P (y|x) for all samples in the validation set. As discussed above, we need human efforts to label the real and generated samples. Fortunately, we can further simplify this. Based on the labels given on training data, we split the training data into categories, and train one generator for each category. With all these generators, we generate samples of all categories. This strategy will be used in our experiments on the datasets with labels given."
    }, {
      "heading" : "5 EXPERIMENT",
      "text" : "We conduct qualitative and quantitative evaluations on three datasets: 1) MNIST (LeCun et al., 1998); 2) CIFAR-10 (Krizhevsky & Hinton, 2009); 3) CUB-200 (Welinder et al., 2010). To add variability to the MNIST images, we randomly scale (factor of 0.8 to 1.2) and rotate (−π4 to π 4 ) the digits and then stitch them to 48 × 48 uniform backgrounds with random grayscale value between [0, 200]. Images are then rescaled back to 32 × 32. Each image thus has a different background grayscale value and a different transformed digit as foreground. We rename this sythensized dataset as MNIST-ONE (single digit on a gray background). We also synthesize a dataset MNIST-TWO containing two digits on a grayscale background. We randomly select two images of digits and perform similar transformations as described above, and put one on the left and the other on the right side of a 78× 78 gray background. We resize the whole image to 64× 64. We develop LR-GAN based on open source code1. We assume the number of objects is known. Therefore, for MNIST-ONE, MNIST-TWO, CIFAR-10, and CUB-200, our model has two, three, two, and two timesteps, respectively. Since the size of foreground object should be smaller than that of canvas, we set the minimal allowed scale 2 in affine transforamtion to be 1.2 for all datasets except for MNIST-TWO, which is set to 2 (objects are smaller in MNIST-TWO). In LR-GAN, the\n1https://github.com/soumith/dcgan.torch 2Scale corresponds to the size of the target canvas with respect to the object – the larger the scale, the larger\nthe canvas, and the smaller the relative size of the object in the canvas. 1 means the same size as the canvas.\nbackground generator and foreground generator have similar architectures. One difference is that the number of channels in the background generator is half of the one in the foreground generator. We compare our results to that of DCGAN (Radford et al., 2015). Note that LR-GAN without LSTM at the first timestep corresponds exactly to the DCGAN. This allows us to run controlled experiments. In both generator and discriminator, all the (fractional) convolutional layers have 4 × 4 filter size with stride 2. As a result, the number of layers in the generator and discriminator automatically adapt to the size of training images. Please see the Appendix (Section 6.2) for details about the configurations. We use three metrics for quantitative evaluation, including Inception Score (Salimans et al., 2016) and the proposed Adversarial Accuracy, Adversarial Divergence. Note that we report two versions of Inception Score. One is based on the pre-trained Inception net, and the other one is based on the pre-trained classifier on the target datasets."
    }, {
      "heading" : "5.1 QUALITATIVE RESULTS",
      "text" : "In Fig. 3 and 4, we show the generated samples for CIFAR-10 and CUB-200, respectively. MNIST results are shown in the next subsection. As we can see from the images, the compositional nature of our model results in the images being free of blending artifacts between backgrounds and foregrounds. For CIFAR-10, we can see the horses and cars with clear shapes. For CUB-200, the bird shapes tend to be even sharper."
    }, {
      "heading" : "5.2 MNIST-ONE AND MNIST-TWO",
      "text" : "We now report the results on MNIST-ONE and MNIST-TWO. Fig. 5 shows the generation results of our model on MNIST-ONE. As we can see, our model generates the background and the foreground in separate timestep, and can disentagle the foreground digits from background nearly perfectly. Though initial values of the mask randomly distribute in the range of (0, 1), after training, the masks are nearly binary and accurately carve out the digits from the generated foreground. More results on MNIST-ONE (including human studies) can be found in the Appendix (Section 6.3).\nFig. 6 shows the generation results for MNIST-TWO. Similarly, the model is also able to generate background and the two foreground objects separately. The foreground generator tends to generate a single digit at each timestep. Meanwhile, it captures the context information from the previous time steps. When the first digit is placed to the left side, the second one tends to be placed on the right side, and vice versa."
    }, {
      "heading" : "5.3 CUB-200",
      "text" : "We study the effectiveness of our model trained on the CUB-200 bird dataset. In Fig. 1, we have shown a random set of generated images, along with the intermediate generation results of the model. While being completely unsupervised, the model, for a large fraction of the samples, is able to\nsuccessfully disentangle the foreground and the background. This is evident from the generated bird-like masks.\nWe do a comparative study based on Amazon Mechanical Turk (AMT) between DCGAN and LRGAN to quantify relative visual quality of the generated images. We first generated 1000 samples from both the models. Then, we performed perfect matching between the two image sets using the Hungarian algorithm on L2 norm distance in the pixel space. This resulted in 1000 image pairs. Some examplar pairs are shown in Fig. 7. For each image pair, 9 judges are asked to choose the one that is more realistic. Based on majority voting, we find that our generated images are selected 68.4% times, compared with 31.6% times for DCGAN. This demonstrates that our model has generated more realistic images than DCGAN. We can attribute this difference to our model’s ability to generate foreground separately from the background, enabling stronger edge cues."
    }, {
      "heading" : "5.4 CIFAR-10",
      "text" : "We now qualitatively and quantitatively evaluate our model on CIFAR-10, which contains multiple object categories and also various backgrounds.\nComparison of image generation quality: We conduct AMT studies to compare the fidelity of image generation. Towards this goal, we generate 1000 images from DCGAN and LR-GAN, respectively. We ask 5 judges to label each image to either belong to one of the 10 categories or as ‘non recognizable’ or ‘recognizable but not belonging to the listed categories’. We then assign each image a quality level between [0,5] that captures the number of judges that agree with the majority choice. Fig. 8 shows the images generated by both approaches, ordered by increasing quality level. We merge images at quality level 0 (all judges said non-recognizable) and 1 together, and similarly images at level 4 and 5. Visually, the generated samples by our model have clearer boundaries and object structures. We also computed the fraction of non-recognizable images: Our model had a 10% absolute drop in non-recognizability rate (67.3% for ours vs. 77.7% for DCGAN). For reference, 11.4% of real CIFAR images were categorized as non-recognizable. Fig. 9 shows more generated (intermediate) results of our model.\nQuantitative evaluation on generators: We evaluate the generators based on three metrics: 1) Inception Score; 2) Adversarial Accuracy; 3) Adversarial Divergence. To obtain a classifier model for evaluation, we remove the top layer in the discriminator used in our model, and then append two fully connected layers on the top of it. We train this classifier using the training samples of CIFAR-10 based on the annotations. Following Salimans et al. (2016), we generated 50,000 images\nbased on DCGAN and LR-GAN, repsectively. We compute two types of Inception Scores. The standard Inception Score is based on the Inception net as in Salimans et al. (2016), and the contextual Inception Score is based on our trained classifier model. To distinguish, we denote the standard one as ‘Inception Score†’, and the contextual one as ‘Inception Score††’. To obtain the Adversarial Accuracy and Adversarial Divergence scores, we train one generator on each of 10 categories for DCGAN and LR-GAN, respectively. Then, we use these generators to generate samples of different categories. Given these generated samples, we train the classifiers for DCGAN and LR-GAN separately. Along with the classifier trained on the real samples, we compute the Adversarial Accuracy\nand Adversarial Divergence on the real training samples. In Table 1, we report the Inception Scores, Adversarial Accuracy and Adversarial Divergence for comparison. We can see that our model outperforms DCGAN across the board. To point out, we obtan different Inception Scores based on different classifier models, which indicates that the Inception Score varies with different models.\nQuantitative evaluation on discriminators: We evaluate the discriminator as an extractor for deep representations. Specifically, we use the output of the last convolutional layer in the discriminator as features. We perform a 1-NN classification on the test set given the full training set. Cosine similarity is used as the metric. On the test set, our model achieves 62.09%±0.01% compared to DCGAN’s 56.05%±0.02%. Contextual generation: We also show the efficacy of our approach to generate diverse foregrounds conditioned on fixed background. The results in Fig. 17 in Appendix showcase that the foreground generator generates objects that are compatible with the background. This indicates that the model has captured contextual dependencies between the image layers.\nCategory specific models: The objects in CIFAR-10 exhibit huge variability in shapes. That can partly explain why some of the generated shapes are not as compelling in Fig. 9. To test this hypothesis, we reuse the generators trained for each of 10 categories used in our metrics to obtain the generation results. Fig. 10 shows results for categories ‘horse’, ‘frog’ and ‘cat’. We can see that the model is now able to generate object-specific appearances and shapes, similar in vein to our results on the CUB-200 dataset."
    }, {
      "heading" : "5.5 IMPORTANCE OF TRANSFORMATIONS",
      "text" : "Fig. 11 shows results from an ablated model without affine transformations in the foreground layers, and compares the results with the full model that does include these transformations. We note that one significant problem emerges that the decompositions are degenerate, in the sense that the model is unable to break the symmetry between foreground and background layers, often generating object appearances in the model’s background layer and vice versa. For CUB-200, the final generated images have some blendings between foregrounds and backgrounds. This is particularly the case for\nthose images without bird-shape masks. For CIFAR-10, a number of generated masks are inverted. In this case, the background images are carved out as the foreground objects. The foreground generator takes almost all the duty to generate the final images, which make it harder to generate images as clear as the model with transformation. From these comparisons, we qualitatively demonstrate the importance of modeling transformations in the foreground generation process. Another merit of using transformation is that the intermediate outputs of the model are more interpretable and faciliate to the downstreaming tasks, such as scene paring, which is demonstrated in Section 6.8."
    }, {
      "heading" : "5.6 IMPORTANCE OF SHAPES",
      "text" : "We perform another ablation study by removing the mask generator to understand the importance of modeling object shapes. In this case, the generated foreground is simply pasted on top of the generated background after being transformed. There is no alpha blending between the foregrounds and backgrounds. The generation results for three datasets, MNIST-ONE, CUB-200, CIFAR-10 are shown in Fig. 12. As we can see, though the model works well for the generation of MNIST-ONE, it fails to generate reasonable images across the other datasets. Particularly, the training does not even converge for CUB-200. Based on these results, we qualitatively demonstrate that mask generator in our model is fairly important to obtain plausible results, especially for realistic images."
    }, {
      "heading" : "6 APPENDIX",
      "text" : ""
    }, {
      "heading" : "6.1 ALGORITHM",
      "text" : "Algo. 1 illustrates the generative process in our model. g(?) evaluates the function g at ?. ◦ is a composition operator that composes its operands so that f ◦ g(?) = f(g(?)).\nAlgorithm 1 Stochastic Layered Recursive Image Generation 1: z0 ∼ N (0, I) 2: x0 = Gb(z0) . background generator 3: h0l ← 0 4: c0l ← 0 5: for t ∈ [1 · · ·T ] do 6: zt ∼ N (0, I) 7: htl , c t l ← LSTM([zt, h t−1 l , c t−1 l ]) . pass through LSTM\n8: if t = 1 then 9: yt← htl\n10: else 11: yt← Elf ([htl h t−1 f ]) . pass through non-linear embedding layers E l f 12: end if 13: st← Gcf (yt) . predict shared cube for Gif and Gmf 14: at ← Tf (yt) . object transformation 15: ft← Gif (st) . generate object appearance 16: mt← Gmf (st) . generate object shape 17: htf ← Ecf ◦ P cf (st) . predict shared represenation embedding 18: xt← ST (mt,at) ST (ft,at) + (1− ST (mt,at)) xt−1 19: end for"
    }, {
      "heading" : "6.2 MODEL CONFIGURATIONS",
      "text" : "Table 2 lists the information and model configuration for different datasets. The dimensions of random vectors and hidden vectors are all set to 100. We also compare the number of parameters in DCGAN and LR-GAN. The numbers before ‘/’ are our model, after ‘/’ are DCGAN. Based on the same notation used in (Zhao et al., 2016), the architectures for the different datasets are:"
    }, {
      "heading" : "6.3 RESULTS ON MNIST-ONE",
      "text" : "We conduct human studies on generation results on MNIST-ONE. Specifically, we generate 1,000 images using both LR-GAN and DCGAN. As references, we also include 1000 real images. Then we ask the users on AMT to label each image to be one of the digits (0-9). We also provide them an option ‘non recognizable’ in case the generated image does not seem to contain a digit. Each image was judged by 5 unique workers. Similar to CIFAR-10, if an image is recognized to be the same digit by all 5 users, it is assigned to quality level 5. If it is not recognizable according to all users, it is assigned to quality level 0. Fig. 13 (left) shows the number of images assigned to all six quality levels. Compared to DCGAN, our model generated more samples with high quality levels. As expected, the real images have many samples with high quality levels. In Fig. 13 (right), we show the number of images that are recognized to each digit category (0-9). For qualitative comparison, we show examplar images at each quality level in Fig. 14. From left to right, the quality level increases from 0 to 5. As expected, the images with higher quality level are more clear.\nFor quantitative evaluation, we use the same way as for CIFAR-10. The classifier model used for contextual Inception Score is trained based on the training set. We generate 60,000 samples based on DCGAN and LR-GAN for evaluation, respectively. To obtain the Adversarial Accuracy and Adversarial Divergence, we first train 10 generators for 10 digit categories separately, and then use the generated samples to train the classifier. As shown in Table 3, our model has higher scores than DCGAN on both standard and contextual Inception Score. Also, our model has a slightly higher\nadversarial accuracy, and lower adversarial divergence than DCGAN. We find that the all three image sets have low standard Inception Scores. This is mainly because the Inception net is trained on ImageNet, which has a very different data distribution from the MNIST dataset. Based on this, we argue that the standard Inception Score is not suitable for some image datasets."
    }, {
      "heading" : "6.4 MORE RESULTS ON CUB-200",
      "text" : "In this experiment, we reduce the minimal allowed object scale to 1.1, which allows the model to generate larger foreground objects. The results are shown in Fig. 15. Similar to the results when the constraint is 1.2, the crisp bird-like masks are generated automatically by our model."
    }, {
      "heading" : "6.5 MORE RESULTS ON CIFAR-10",
      "text" : ""
    }, {
      "heading" : "6.5.1 QUALITATIVE RESULTS",
      "text" : "In Fig. 16, we show more results on CIFAR-10 when setting minimal allowed object scale to 1.1. The rightmost column block also shows the training images that are closest to the generated images (cosine similarity in pixel space). We can see our model does not memorize the training data."
    }, {
      "heading" : "6.5.2 WALKING IN THE LATENT SPACE",
      "text" : "Similar to DCGAN, we also show results by walking in the latent space. Note that our model has two or more inputs. So we can walk along any of them or their combination. In Fig. 17, we generate multiple foregrounds for the same fixed generated background. We find that our model consistently generates contextually compatible foregrounds. For example, for the grass-like backgrounds, the foreground generator generates horses and deer, and airplane-like objects for the blue sky."
    }, {
      "heading" : "6.5.3 WORD CLOUD BASED ON HUMAN STUDY",
      "text" : "As we mentioned above, we conducted human studies on CIFAR-10. Besides asking people to select a name from a list for an image, we also conducted another human study where we ask people to use one word (free-form) to describe the main object in the image. Each image was ‘named’ by 5 unique people. We generate word clouds for real images, images generated by DCGAN and LR-GAN, as shown in Fig. 18."
    }, {
      "heading" : "6.6 RESULTS ON LFW FACE DATASET",
      "text" : "We conduct experiment on face images in LFW dataset (Huang et al., 2007). Different from previous works which work on cropped and aligned faces, we directly generate the original images which contains a large portion of backgrounds. This configuration helps to verify the efficiency of LR-GAN to model the object appearance, shape and pose. In Fig. 19, we show the (intermediate) generation results of LR-GAN. Surprisingly, without any supervisions, the model generated background and faces in separate steps, and the generated masks accurately depict face shapes. Moreover, the model\nlearns where to place the generated faces so that the whole image looks natural. For comparison, please refer to (Kwak & Zhang, 2016) which does not model the transformation. We can find the generation results degrade much."
    }, {
      "heading" : "6.7 STATISTICS ON TRANSFORMATION MATRICES",
      "text" : "In this part, we analyze the statistics on the transformation matrices generated by our model for different datasets, including MNIST-ONE, CUB-200, CIFAR-10 and LFW. We used affine transformation in our model. So there are 6 parameters, scaling in the x coordinate (sx), scaling in the y coordinate (sy), translation in the x coordinate (tx), translation in the y coordinate (ty), rotation in the x coordinate (rx) and rotation in the y coordinate (ry). In Fig. 20, we show the histograms on different parameters for different datasets.These histograms show that the model produces non-trivial varied scaling, translation and rotation on all datasets. For different datasets, the learned transformation have different patterns. We hypothesize that this is mainly determined by the configurations of objects in the images. For example, on MNIST-ONE, all six parameters have some fluctuations since the synthetic dataset contains digits randomly placed at different locations. For the other three datasets, the scalings converge to single value since the object sizes do not vary much, and the variations on rotation and translation suffice to generate realistic images. Specifically, we can find the generator largely relies on the translation on x coordinate for generating CUB-200. This makes sense since birds in the images have similar scales, orientations but various horizontal locations. For CIFAR-10, since there are 10 different object categories, the configurations are more diverse, hence the generator uses all parameters for generation except for the scaling. For LFW, since faces have similar configurations, the learned transformations have less fluctuation as well. As a result, we can see that LR-GAN indeed models the transformations on the foreground to generate images."
    }, {
      "heading" : "6.8 CONDITIONAL IMAGE GENERATION",
      "text" : "Considering our model can generate object-like masks (shapes) for images, we conducted an experiment to evaluate whether our model can be potentially used for image segmentation and object detection. We make some changes to the model. For the background generator, the input is a real image instead of a random vector. Then the image is passed through an encoder to extract the hidden features, which replaces the random vector z0 and are fed to the background generator. For the foreground generator, we subtract the image generated by the background generator from the input image to obtain a residual image. Then this residual image is fed to the same encoder to get the hidden features, which are used as the input for foreground generator. In our conditional model, we want to reconstruct the image, so we add a reconstruction loss along with the adversarial loss. We train this conditional model on CIFAR-10. The (intermediate) outputs of the model is shown in Fig. 21. Interestingly, the model successfully learned to decompose the input images into background and foreground. The background generator tends to do an image inpainting by generating a complete background without object, while the foreground generator works as a segmentation model to get object mask from the input image.\nSimilarly, we also run the conditional LR-GAN on LFW dataset. As we can see in Fig. 22, the foreground generator automatically and consistently learned to generate the face regions, even though there are large portion of background in the input images. In other words, the conditional LR-GAN successfully learned to detection faces in images. We suspect this success is due to that it has low cost for the generator to generate similar images, and thus converge to the case that the first generator generate background, and the second generator generate face images.\nBased on these experiments, we argue that our model can be possibly used for image segmentation and object detection in a generative and unsupervised manner. One future work would be verifying this by applying it to high-resolution and more complicate datasets."
    } ],
    "references" : [ {
      "title" : "Infogan: Interpretable representation learning by information maximizing generative adversarial nets",
      "author" : [ "Xi Chen", "Yan Duan", "Rein Houthooft", "John Schulman", "Ilya Sutskever", "Pieter Abbeel" ],
      "venue" : "arXiv preprint arXiv:1606.03657,",
      "citeRegEx" : "Chen et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Robust estimation of a multi-layered motion representation",
      "author" : [ "Trevor Darrell", "Alex Pentland" ],
      "venue" : "IEEE Workshop on Visual Motion,",
      "citeRegEx" : "Darrell and Pentland.,? \\Q1991\\E",
      "shortCiteRegEx" : "Darrell and Pentland.",
      "year" : 1991
    }, {
      "title" : "Deep generative image models using a laplacian pyramid of adversarial networks",
      "author" : [ "Emily L Denton", "Soumith Chintala", "Rob Fergus" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Denton et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Denton et al\\.",
      "year" : 2015
    }, {
      "title" : "Attend, infer, repeat: Fast scene understanding with generative models",
      "author" : [ "S.M. Ali Eslami", "Nicolas Heess", "Theophane Weber", "Yuval Tassa", "Koray Kavukcuoglu", "Geoffrey E. Hinton" ],
      "venue" : null,
      "citeRegEx" : "Eslami et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Eslami et al\\.",
      "year" : 2016
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Draw: A recurrent neural network for image generation",
      "author" : [ "Karol Gregor", "Ivo Danihelka", "Alex Graves", "Danilo Jimenez Rezende", "Daan Wierstra" ],
      "venue" : "arXiv preprint arXiv:1502.046239,",
      "citeRegEx" : "Gregor et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gregor et al\\.",
      "year" : 2015
    }, {
      "title" : "Labeled faces in the wild: A database for studying face recognition in unconstrained environments",
      "author" : [ "Gary B. Huang", "Manu Ramesh", "Tamara Berg", "Erik Learned-Miller" ],
      "venue" : "Technical Report 07-49,",
      "citeRegEx" : "Huang et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2007
    }, {
      "title" : "Efficient inference in occlusion-aware generative models of images",
      "author" : [ "Jonathan Huang", "Kevin Murphy" ],
      "venue" : "CoRR, abs/1511.06362,",
      "citeRegEx" : "Huang and Murphy.,? \\Q2015\\E",
      "shortCiteRegEx" : "Huang and Murphy.",
      "year" : 2015
    }, {
      "title" : "Generating images with recurrent adversarial networks",
      "author" : [ "Daniel Jiwoong Im", "Chris Dongjoo Kim", "Hui Jiang", "Roland Memisevic" ],
      "venue" : "arXiv preprint arXiv:1602.05110,",
      "citeRegEx" : "Im et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Im et al\\.",
      "year" : 2016
    }, {
      "title" : "Scene collaging: Analysis and synthesis of natural images with semantic layers",
      "author" : [ "Phillip Isola", "Ce Liu" ],
      "venue" : "In IEEE International Conference on Computer Vision, pp",
      "citeRegEx" : "Isola and Liu.,? \\Q2013\\E",
      "shortCiteRegEx" : "Isola and Liu.",
      "year" : 2013
    }, {
      "title" : "Spatial transformer networks",
      "author" : [ "Max Jaderberg", "Karen Simonyan", "Andrew Zisserman", "koray kavukcuoglu" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Jaderberg et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Jaderberg et al\\.",
      "year" : 2015
    }, {
      "title" : "Generative model for layers of appearance and deformation",
      "author" : [ "Anitha Kannan", "Nebojsa Jojic", "Brendan Frey" ],
      "venue" : null,
      "citeRegEx" : "Kannan et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Kannan et al\\.",
      "year" : 2005
    }, {
      "title" : "Improving variational inference with inverse autoregressive flow",
      "author" : [ "Diederik P Kingma", "Tim Salimans", "Max Welling" ],
      "venue" : "arXiv preprint arXiv:1606.04934,",
      "citeRegEx" : "Kingma et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning multiple layers of features from tiny images",
      "author" : [ "Alex Krizhevsky", "Geoffrey Hinton" ],
      "venue" : null,
      "citeRegEx" : "Krizhevsky and Hinton.,? \\Q2009\\E",
      "shortCiteRegEx" : "Krizhevsky and Hinton.",
      "year" : 2009
    }, {
      "title" : "Generating images part by part with composite generative adversarial networks",
      "author" : [ "Hanock Kwak", "Byoung-Tak Zhang" ],
      "venue" : "arXiv preprint arXiv:1607.05387,",
      "citeRegEx" : "Kwak and Zhang.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kwak and Zhang.",
      "year" : 2016
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Yann LeCun", "Léon Bottou", "Yoshua Bengio", "Patrick Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "Generating images from captions with attention",
      "author" : [ "Elman Mansimov", "Emilio Parisotto", "Jimmy Lei Ba", "Ruslan Salakhutdinov" ],
      "venue" : "arXiv preprint arXiv:1511.02793,",
      "citeRegEx" : "Mansimov et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mansimov et al\\.",
      "year" : 2015
    }, {
      "title" : "A parametric texture model based on joint statistics of complex wavelet coefficients",
      "author" : [ "Javier Portilla", "Eero P Simoncelli" ],
      "venue" : "International journal of computer vision,",
      "citeRegEx" : "Portilla and Simoncelli.,? \\Q2000\\E",
      "shortCiteRegEx" : "Portilla and Simoncelli.",
      "year" : 2000
    }, {
      "title" : "Unsupervised representation learning with deep convolutional generative adversarial networks",
      "author" : [ "Alec Radford", "Luke Metz", "Soumith Chintala" ],
      "venue" : "arXiv preprint arXiv:1511.06434,",
      "citeRegEx" : "Radford et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning what and where to draw",
      "author" : [ "Scott Reed", "Zeynep Akata", "Santosh Mohan", "Samuel Tenka", "Bernt Schiele", "Honglak Lee" ],
      "venue" : "arXiv preprint arXiv:1610.02454,",
      "citeRegEx" : "Reed et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Reed et al\\.",
      "year" : 2016
    }, {
      "title" : "Generative adversarial text to image synthesis",
      "author" : [ "Scott Reed", "Zeynep Akata", "Xinchen Yan", "Lajanugen Logeswaran", "Bernt Schiele", "Honglak Lee" ],
      "venue" : "arXiv preprint arXiv:1605.05396,",
      "citeRegEx" : "Reed et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Reed et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning a generative model of images by factoring appearance and shape",
      "author" : [ "Nicolas Le Roux", "Nicolas Heess", "Jamie Shotton", "John Winn" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Roux et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Roux et al\\.",
      "year" : 2011
    }, {
      "title" : "Improved techniques for training gans",
      "author" : [ "Tim Salimans", "Ian Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen" ],
      "venue" : "arXiv preprint arXiv:1606.03498,",
      "citeRegEx" : "Salimans et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Salimans et al\\.",
      "year" : 2016
    }, {
      "title" : "Generating videos with scene dynamics",
      "author" : [ "Carl Vondrick", "Hamed Pirsiavash", "Antonio Torralba" ],
      "venue" : "arXiv preprint arXiv:1609.02612,",
      "citeRegEx" : "Vondrick et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Vondrick et al\\.",
      "year" : 2016
    }, {
      "title" : "Representing moving images with layers",
      "author" : [ "John Wang", "Edward Adelson" ],
      "venue" : "IEEE Transactions on Image Processing,",
      "citeRegEx" : "Wang and Adelson.,? \\Q1994\\E",
      "shortCiteRegEx" : "Wang and Adelson.",
      "year" : 1994
    }, {
      "title" : "Generative image modeling using style and structure adversarial networks",
      "author" : [ "Xiaolong Wang", "Abhinav Gupta" ],
      "venue" : "arXiv preprint arXiv:1603.05631,",
      "citeRegEx" : "Wang and Gupta.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wang and Gupta.",
      "year" : 2016
    }, {
      "title" : "Caltech-UCSD Birds 200",
      "author" : [ "P. Welinder", "S. Branson", "T. Mita", "C. Wah", "F. Schroff", "S. Belongie", "P. Perona" ],
      "venue" : "Technical Report CNS-TR-2010-001, California Institute of Technology,",
      "citeRegEx" : "Welinder et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Welinder et al\\.",
      "year" : 2010
    }, {
      "title" : "Attribute2image: Conditional image generation from visual attributes",
      "author" : [ "Xinchen Yan", "Jimei Yang", "Kihyuk Sohn", "Honglak Lee" ],
      "venue" : "CoRR, abs/1512.00570,",
      "citeRegEx" : "Yan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Yan et al\\.",
      "year" : 2015
    }, {
      "title" : "Energy-based generative adversarial network",
      "author" : [ "Junbo Zhao", "Michael Mathieu", "Yann LeCun" ],
      "venue" : "arXiv preprint arXiv:1609.03126,",
      "citeRegEx" : "Zhao et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2016
    }, {
      "title" : "Generative visual manipulation on the natural image manifold",
      "author" : [ "Jun-Yan Zhu", "Philipp Krähenbühl", "Eli Shechtman", "Alexei A Efros" ],
      "venue" : "In European Conference on Computer Vision,",
      "citeRegEx" : "Zhu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Generative adversarial networks (GANs) (Goodfellow et al., 2014) have shown significant promise as generative models for natural images.",
      "startOffset" : 39,
      "endOffset" : 64
    }, {
      "referenceID" : 18,
      "context" : "A flurry of recent work has proposed improvements over the original GAN work for image generation (Radford et al., 2015; Denton et al., 2015; Salimans et al., 2016; Chen et al., 2016; Zhu et al., 2016; Zhao et al., 2016), multi-stage image generation including part-based models (Im et al.",
      "startOffset" : 98,
      "endOffset" : 220
    }, {
      "referenceID" : 2,
      "context" : "A flurry of recent work has proposed improvements over the original GAN work for image generation (Radford et al., 2015; Denton et al., 2015; Salimans et al., 2016; Chen et al., 2016; Zhu et al., 2016; Zhao et al., 2016), multi-stage image generation including part-based models (Im et al.",
      "startOffset" : 98,
      "endOffset" : 220
    }, {
      "referenceID" : 22,
      "context" : "A flurry of recent work has proposed improvements over the original GAN work for image generation (Radford et al., 2015; Denton et al., 2015; Salimans et al., 2016; Chen et al., 2016; Zhu et al., 2016; Zhao et al., 2016), multi-stage image generation including part-based models (Im et al.",
      "startOffset" : 98,
      "endOffset" : 220
    }, {
      "referenceID" : 0,
      "context" : "A flurry of recent work has proposed improvements over the original GAN work for image generation (Radford et al., 2015; Denton et al., 2015; Salimans et al., 2016; Chen et al., 2016; Zhu et al., 2016; Zhao et al., 2016), multi-stage image generation including part-based models (Im et al.",
      "startOffset" : 98,
      "endOffset" : 220
    }, {
      "referenceID" : 29,
      "context" : "A flurry of recent work has proposed improvements over the original GAN work for image generation (Radford et al., 2015; Denton et al., 2015; Salimans et al., 2016; Chen et al., 2016; Zhu et al., 2016; Zhao et al., 2016), multi-stage image generation including part-based models (Im et al.",
      "startOffset" : 98,
      "endOffset" : 220
    }, {
      "referenceID" : 28,
      "context" : "A flurry of recent work has proposed improvements over the original GAN work for image generation (Radford et al., 2015; Denton et al., 2015; Salimans et al., 2016; Chen et al., 2016; Zhu et al., 2016; Zhao et al., 2016), multi-stage image generation including part-based models (Im et al.",
      "startOffset" : 98,
      "endOffset" : 220
    }, {
      "referenceID" : 8,
      "context" : ", 2016), multi-stage image generation including part-based models (Im et al., 2016; Kwak & Zhang, 2016), image generation conditioned on input text or attributes (Mansimov et al.",
      "startOffset" : 66,
      "endOffset" : 103
    }, {
      "referenceID" : 23,
      "context" : ", 2016b;a), image generation based on 3D structure (Wang & Gupta, 2016), and even video generation (Vondrick et al., 2016).",
      "startOffset" : 99,
      "endOffset" : 122
    }, {
      "referenceID" : 26,
      "context" : "Figure 1: Generation results of our model on CUB-200 (Welinder et al., 2010).",
      "startOffset" : 53,
      "endOffset" : 76
    }, {
      "referenceID" : 15,
      "context" : "We mainly evaluate our approach on four datasets: MNIST-ONE (one digit) and MNIST-TWO (two digits) synthesized from MNIST (LeCun et al., 1998), CIFAR-10 (Krizhevsky & Hinton, 2009) and CUB-200 (Welinder et al.",
      "startOffset" : 122,
      "endOffset" : 142
    }, {
      "referenceID" : 26,
      "context" : ", 1998), CIFAR-10 (Krizhevsky & Hinton, 2009) and CUB-200 (Welinder et al., 2010).",
      "startOffset" : 58,
      "endOffset" : 81
    }, {
      "referenceID" : 12,
      "context" : "Recent improvements in image generation using deep neural networks mainly fall into one of the two stochastic models: variational autoencoders (VAEs) (Kingma et al., 2016) and generative adversarial networks (GANs) (Goodfellow et al.",
      "startOffset" : 150,
      "endOffset" : 171
    }, {
      "referenceID" : 4,
      "context" : ", 2016) and generative adversarial networks (GANs) (Goodfellow et al., 2014).",
      "startOffset" : 51,
      "endOffset" : 76
    }, {
      "referenceID" : 5,
      "context" : "Sequential models have been pivotal for improved image generation using variational autoencoders: DRAW (Gregor et al., 2015) uses attention based recurrence conditioning on the canvas drawn so far.",
      "startOffset" : 103,
      "endOffset" : 124
    }, {
      "referenceID" : 2,
      "context" : "Early compelling results using GANs used sequential coarse-to-fine multiscale generation and classconditioning (Denton et al., 2015).",
      "startOffset" : 111,
      "endOffset" : 132
    }, {
      "referenceID" : 22,
      "context" : "Since then, improved training schemes (Salimans et al., 2016) and better convolutional structure (Radford et al.",
      "startOffset" : 38,
      "endOffset" : 61
    }, {
      "referenceID" : 18,
      "context" : ", 2016) and better convolutional structure (Radford et al., 2015) have improved the generation results using",
      "startOffset" : 43,
      "endOffset" : 65
    }, {
      "referenceID" : 2,
      "context" : "In Eslami et al. (2016), a recurrent generative model that draws one object at a time to the canvas was used as the decoder in VAE.",
      "startOffset" : 3,
      "endOffset" : 24
    }, {
      "referenceID" : 7,
      "context" : "One closely related work combining recursive structure with GAN is that of Im et al. (2016) but it does not explicitly model object composition and follows a similar paradigm as by Gregor et al.",
      "startOffset" : 75,
      "endOffset" : 92
    }, {
      "referenceID" : 5,
      "context" : "(2016) but it does not explicitly model object composition and follows a similar paradigm as by Gregor et al. (2015). Another closely related work is that of Kwak & Zhang (2016).",
      "startOffset" : 96,
      "endOffset" : 117
    }, {
      "referenceID" : 5,
      "context" : "(2016) but it does not explicitly model object composition and follows a similar paradigm as by Gregor et al. (2015). Another closely related work is that of Kwak & Zhang (2016). It combines recursive structure and alpha blending.",
      "startOffset" : 96,
      "endOffset" : 178
    }, {
      "referenceID" : 5,
      "context" : "(2016) but it does not explicitly model object composition and follows a similar paradigm as by Gregor et al. (2015). Another closely related work is that of Kwak & Zhang (2016). It combines recursive structure and alpha blending. However, our work differs in three main ways: (1) We explicitly use a generator for modeling the foreground poses. That provides significant advantage for natural images, as shown by our ablation studies; (2) Our shape generator is separate from the appearance generator. This factored representation allows more flexibility in the generated scenes; (3) Our recursive framework generates subsequent objects conditioned on the current and previous hidden vectors, and previously generated object. This allows for explicit contextual modeling among generated elements in the scene. See Fig. 17 for contextually relevant foregrounds generated for the same background, or Fig. 6 for meaningful placement of two MNIST digits relative to each. Models that provide supervision to image generation using conditioning variables have also been proposed: Style/Structure GANs (Wang & Gupta, 2016) learns separate generative models for style and structure that are then composed to obtain final images. In Reed et al. (2016a), GAN based image generation is conditioned on text and the region in the image where the text manifests, specified during training via keypoints or bounding boxes.",
      "startOffset" : 96,
      "endOffset" : 1245
    }, {
      "referenceID" : 2,
      "context" : "While the GANs framework is largely agnostic to the choice of G and D, it is clear that generative models with the ‘right’ inductive biases will be more effective in learning from the gradient information (Denton et al., 2015; Im et al., 2016; Gregor et al., 2015; Reed et al., 2016a; Yan et al., 2015).",
      "startOffset" : 205,
      "endOffset" : 302
    }, {
      "referenceID" : 8,
      "context" : "While the GANs framework is largely agnostic to the choice of G and D, it is clear that generative models with the ‘right’ inductive biases will be more effective in learning from the gradient information (Denton et al., 2015; Im et al., 2016; Gregor et al., 2015; Reed et al., 2016a; Yan et al., 2015).",
      "startOffset" : 205,
      "endOffset" : 302
    }, {
      "referenceID" : 5,
      "context" : "While the GANs framework is largely agnostic to the choice of G and D, it is clear that generative models with the ‘right’ inductive biases will be more effective in learning from the gradient information (Denton et al., 2015; Im et al., 2016; Gregor et al., 2015; Reed et al., 2016a; Yan et al., 2015).",
      "startOffset" : 205,
      "endOffset" : 302
    }, {
      "referenceID" : 27,
      "context" : "While the GANs framework is largely agnostic to the choice of G and D, it is clear that generative models with the ‘right’ inductive biases will be more effective in learning from the gradient information (Denton et al., 2015; Im et al., 2016; Gregor et al., 2015; Reed et al., 2016a; Yan et al., 2015).",
      "startOffset" : 205,
      "endOffset" : 302
    }, {
      "referenceID" : 27,
      "context" : "Some existing methods assume the access to the shape of the object either during training (Isola & Liu, 2013) or both at train and test time (Reed et al., 2016a; Yan et al., 2015).",
      "startOffset" : 141,
      "endOffset" : 179
    }, {
      "referenceID" : 11,
      "context" : "Representing images in layered structure is even straightforward for video with moving objects (Darrell & Pentland, 1991; Wang & Adelson, 1994; Kannan et al., 2005).",
      "startOffset" : 95,
      "endOffset" : 164
    }, {
      "referenceID" : 11,
      "context" : "Representing images in layered structure is even straightforward for video with moving objects (Darrell & Pentland, 1991; Wang & Adelson, 1994; Kannan et al., 2005). Vondrick et al. (2016) generates videos by separately generating a fixed background and moving foregrounds.",
      "startOffset" : 144,
      "endOffset" : 189
    }, {
      "referenceID" : 11,
      "context" : "Representing images in layered structure is even straightforward for video with moving objects (Darrell & Pentland, 1991; Wang & Adelson, 1994; Kannan et al., 2005). Vondrick et al. (2016) generates videos by separately generating a fixed background and moving foregrounds. A similar way of generating single image can be found in Kwak & Zhang (2016). Another way is modeling the layered structure with object appearance and pose as:",
      "startOffset" : 144,
      "endOffset" : 351
    }, {
      "referenceID" : 21,
      "context" : "Several works fall into this group (Roux et al., 2011; Huang & Murphy, 2015; Eslami et al., 2016).",
      "startOffset" : 35,
      "endOffset" : 97
    }, {
      "referenceID" : 3,
      "context" : "Several works fall into this group (Roux et al., 2011; Huang & Murphy, 2015; Eslami et al., 2016).",
      "startOffset" : 35,
      "endOffset" : 97
    }, {
      "referenceID" : 3,
      "context" : ", 2011; Huang & Murphy, 2015; Eslami et al., 2016). In Huang & Murphy (2015), images are decomposed into layers of objects with specific poses in a variational autoencoder framework, while the number of objects (i.",
      "startOffset" : 30,
      "endOffset" : 77
    }, {
      "referenceID" : 3,
      "context" : ", 2011; Huang & Murphy, 2015; Eslami et al., 2016). In Huang & Murphy (2015), images are decomposed into layers of objects with specific poses in a variational autoencoder framework, while the number of objects (i.e., layers) is adaptively estimated in Eslami et al. (2016). To contrast with these works, LR-GAN uses a layered composition, and the foreground layers simultaneously model all three dominant factors of variation: appearance f , shape m and pose a.",
      "startOffset" : 30,
      "endOffset" : 274
    }, {
      "referenceID" : 10,
      "context" : "As in Jaderberg et al. (2015), we predict the affine transformation matrix with a linear layer Tf that has six-dimensional outputs.",
      "startOffset" : 6,
      "endOffset" : 30
    }, {
      "referenceID" : 10,
      "context" : "As in Jaderberg et al. (2015), we predict the affine transformation matrix with a linear layer Tf that has six-dimensional outputs. Then based on the predicted transformation matrix, we use a grid generator Gg to generate the corresponding sampling coordinates in the input for each location at the output. The generated foreground appearance and mask share the same transformation matrix, and thus the same sampling grid. Given the grid, the sampler S will simultaneously sample the ft and mt to obtain f̂t and m̂t, respectively. Different from Jaderberg et al. (2015), our sampler here normally performs downsampling, since the the foreground typically has smaller size than the background.",
      "startOffset" : 6,
      "endOffset" : 570
    }, {
      "referenceID" : 4,
      "context" : "2 NEW EVALUATION METRICS Several metrics have been proposed to evaluate GANs, such as Gaussian parzen window (Goodfellow et al., 2014), Generative Adversarial Metric (GAM) (Im et al.",
      "startOffset" : 109,
      "endOffset" : 134
    }, {
      "referenceID" : 8,
      "context" : ", 2014), Generative Adversarial Metric (GAM) (Im et al., 2016) and Inception Score (Salimans et al.",
      "startOffset" : 45,
      "endOffset" : 62
    }, {
      "referenceID" : 22,
      "context" : ", 2016) and Inception Score (Salimans et al., 2016).",
      "startOffset" : 28,
      "endOffset" : 51
    }, {
      "referenceID" : 22,
      "context" : "Most recently, Inception Score has been used in several works (Salimans et al., 2016; Zhao et al., 2016).",
      "startOffset" : 62,
      "endOffset" : 104
    }, {
      "referenceID" : 28,
      "context" : "Most recently, Inception Score has been used in several works (Salimans et al., 2016; Zhao et al., 2016).",
      "startOffset" : 62,
      "endOffset" : 104
    }, {
      "referenceID" : 15,
      "context" : "We conduct qualitative and quantitative evaluations on three datasets: 1) MNIST (LeCun et al., 1998); 2) CIFAR-10 (Krizhevsky & Hinton, 2009); 3) CUB-200 (Welinder et al.",
      "startOffset" : 80,
      "endOffset" : 100
    }, {
      "referenceID" : 26,
      "context" : ", 1998); 2) CIFAR-10 (Krizhevsky & Hinton, 2009); 3) CUB-200 (Welinder et al., 2010).",
      "startOffset" : 61,
      "endOffset" : 84
    }, {
      "referenceID" : 18,
      "context" : "We compare our results to that of DCGAN (Radford et al., 2015).",
      "startOffset" : 40,
      "endOffset" : 62
    }, {
      "referenceID" : 22,
      "context" : "We use three metrics for quantitative evaluation, including Inception Score (Salimans et al., 2016) and the proposed Adversarial Accuracy, Adversarial Divergence.",
      "startOffset" : 76,
      "endOffset" : 99
    }, {
      "referenceID" : 22,
      "context" : "Following Salimans et al. (2016), we generated 50,000 images",
      "startOffset" : 10,
      "endOffset" : 33
    }, {
      "referenceID" : 22,
      "context" : "06 †Evaluate using the pre-trained Inception net as Salimans et al. (2016) ††Evaluate using the supervisedly trained classifier based on the discriminator in LR-GAN.",
      "startOffset" : 52,
      "endOffset" : 75
    }, {
      "referenceID" : 22,
      "context" : "The standard Inception Score is based on the Inception net as in Salimans et al. (2016), and the contextual Inception Score is based on our trained classifier model.",
      "startOffset" : 65,
      "endOffset" : 88
    } ],
    "year" : 2017,
    "abstractText" : "We present LR-GAN: an adversarial image generation model which takes scene structure and context into account. Unlike previous generative adversarial networks (GANs), the proposed GAN learns to generate image background and foregrounds separately and recursively, and stitch the foregrounds on the background in a contextually relevant manner to produce a complete natural image. For each foreground, the model learns to generate its appearance, shape and pose. The whole model is unsupervised, and is trained in an end-to-end manner with gradient descent methods. The experiments demonstrate that LR-GAN can generate more natural images with objects that are more human recognizable than DCGAN.",
    "creator" : "LaTeX with hyperref package"
  }
}