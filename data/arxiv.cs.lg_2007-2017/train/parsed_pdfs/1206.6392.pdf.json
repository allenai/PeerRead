{
  "name" : "1206.6392.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Modeling Temporal Dependencies in High-Dimensional Sequences: Application to Polyphonic Music Generation and Transcription",
    "authors" : [ "Nicolas Boulanger-Lewandowski", "Yoshua Bengio" ],
    "emails" : [ "boulanni@iro.umontreal.ca", "bengioy@iro.umontreal.ca", "vincentp@iro.umontreal.ca" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Modeling sequences is an important area of machine learning since many naturally occurring phenomena such as music, speech, or human motion are inherently sequential. Complex sequences are non-local in that the impact of a factor localized in time can be delayed by an arbitrarily long time-lag. For example, musical patterns or themes appearing at the beginning of a piece are often repeated towards the end. Recurrent neural networks (RNN) (Rumelhart et al., 1986) incorporate an internal memory that can, in principle, summarize the entire sequence history. This property makes them well suited to represent long-term dependencies, but it is nevertheless a challenge to train them efficiently by gradient-based optimization (Bengio et al., 1994). It was recently shown that training RNNs via Hessian-free (HF) optimization could help reduce these difficulties (Martens & Sutskever, 2011).\nMany sequences of interest are over high-dimensional\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nobjects, such as images in video, short-term spectra in audio music, tuples of notes in musical scores, or words in text. In these cases, simply predicting the expected value at the next time step given the observed values of the previous time steps is not satisfying. With such high-dimensional objects at each time step, the conditional distribution is very often multi-modal, and we would strongly prefer our models of such sequences to predict the conditional distribution of the next time step given previous time steps. For the case of polyphonic music, it is obvious that the occurrence of a particular note at a particular time modifies considerably the probability with which other notes may occur at the same time. In other words, notes appear together in correlated patterns, or simultaneities, that cannot be conveniently described by a typical RNN architecture designed for the multiclass classification task, for example, because enumerating all configurations of the variable to predict would be very expensive. This difficulty motivates energy-based models which allow us to express the negative log-likelihood of a given configuration by an arbitrary energy function, among which the restricted Boltzmann machine (RBM) (Smolensky, 1986) has become notorious.\nIn this context, we wish to exploit the ability of RBMs to represent a complicated distribution for each time step, with parameters that depend on the previous ones, an idea first put forward with the so-called temporal RBM (Taylor et al., 2007; Sutskever & Hinton, 2007) which is trained via a heuristic procedure. Combining the desirable characteristics of RNNs and RBMs has proven to be non-trivial. The recurrent temporal RBM (RTRBM) (Sutskever et al., 2008) is a similar model that allows for exact inference and efficient training by contrastive divergence (CD). Despite its simplicity, this model successfully accounts for several interesting sequences. A similar architecture based on the echo state network was also recently developed (Schrauwen & Buesing, 2009). In this work,\nwe demonstrate that the RTRBM outperforms many traditional models of polyphonic music, and we introduce a generalization of the RTRBM, called the RNNRBM, that allows more freedom to describe the temporal dependencies involved.\nMore precisely, we will consider sequences of symbolic music, i.e. represented by the explicit timing, pitch, velocity and instrumental information typically contained in a score or a MIDI file rather than more complex, acoustically rich audio signals. Musical models mostly focus on the basic components of western music, harmony and rhythm, and are trained to predict the pattern of notes (simultaneities) to be played together in the next time interval, given the previous ones. Two elements characterize the qualitative performance of a model: temporal dependencies and chord conditional distributions. While most existing models output only monophonic notes along with predefined chords or other reduced-dimensionality representation (e.g. Mozer, 1994; Eck & Schmidhuber, 2002; Paiement et al., 2009), we aim to model unconstrained polyphonic music in the piano-roll representation, i.e. as a binary matrix specifying precisely which notes occur at each time step. Despite ignoring dynamics and other score annotations, this task represents a welldefined framework to improve machine learning algorithms and is directly applicable to polyphonic transcription.\nThe objective of polyphonic transcription is to determine the underlying notes of a polyphonic audio signal without access to its score. Human experts approach this difficult problem by giving importance to what they expect to hear rather than exclusively to what is present in the actual signal. Most existing transcription algorithms are frame-based and rely exclusively on the audio signal, even though some approaches employ rudimentary musicological constraints (e.g. Li & Wang, 2007). It has long been known that, in the same way that natural language models tremendously improve the performance of speech recognition systems, musical language models can improve purely auditive approaches to music information retrieval (Cemgil, 2004). However, combining these two sources of information is not trivial, with the result that temporal smoothing with an HMM is often the only post-processing involved in state-of-the-art transcription (Nam et al., 2011). We will show how to enrich an arbitrary transcription algorithm (under basic assumptions) to include the advice of an expert trained on symbolic sequences. Using our hybrid approach, we can improve transcription accuracy (Bay et al., 2009) much more than the popular HMM approach.\nThe remainder of the paper is organized as follows. In Sections 2, 3 and 4 we introduce the RBM, the RTRBM and the RNN-RBM architectures. In Section 5 we validate our model on benchmark datasets. In Section 6 we present our results on musical sequences, and we detail our hybrid transcription approach in Section 7."
    }, {
      "heading" : "2 Restricted Boltzmann machines",
      "text" : "An RBM is an energy-based model where the joint probability of a given configuration of the visible vector v (inputs) and the hidden vector h is:\nP (v, h) = exp(−bTv v − bThh− hTWv)/Z (1)\nwhere bv, bh and W are the model parameters and Z is the usually intractable partition function. When the vector v is given, the hidden units hi are conditionally independent of one another, and vice versa:\nP (hi = 1|v) = σ(bh +Wv)i (2) P (vj = 1|h) = σ(bv +WTh)j (3)\nwhere σ(x) ≡ (1 + e−x)−1 is the element-wise logistic sigmoid function. The marginalized probability of v is related to the free-energy F (v) by P (v) ≡ e−F (v)/Z:\nF (v) = −bTv v − ∑ i log(1 + ebh+Wv)i (4)\nInference in RBMs consists of sampling the hi given v (or the vj given h) according to their conditional Bernoulli distribution (eq. 2). Sampling v from the RBM can be performed efficiently by block Gibbs sampling, i.e. by performing k alternating steps of sampling h|v and v|h. The gradient of the negative loglikelihood of an input vector v(l) involves two opposing terms, called the positive and negative phase:\n∂(− logP (v(l))) ∂Θ = ∂F (v(l)) ∂Θ − ∂(− logZ) ∂Θ (5)\nwhere Θ ≡ {bv, bh,W}. The second term can be estimated by a single sample v(l)∗ obtained from a k-step Gibbs chain starting at v(l):\n∂(− logP (v(l))) ∂Θ ' ∂F (v (l)) ∂Θ − ∂F (v (l)∗) ∂Θ . (6)\nresulting in the well-known contrastive divergence (CDk) algorithm (Hinton, 2002).\nThe neural autoregressive distribution estimator (NADE) (Larochelle & Murray, 2011) is a tractable model inspired by the RBM and specializing (with tying constraints) an earlier model for the joint distribution of high-dimensional variables (Bengio & Bengio,\n2000). NADE is similar to a fully visible sigmoid belief network in that the conditional probability distribution of a visible unit vj is expressed as a nonlinear function of vk,∀k < j. In the following discussion, one can substitute RBMs with NADEs by replacing equation (6) with the exact gradient defined in (Larochelle & Murray, 2011) where the biases are set to b = v (t) b , c = v (t) h . The advantages of a tractable distribution estimator will become obvious when used as part of sequential models.\nFigure 1 presents mean-field samples P (vj = 1|h∗), where h∗ ∼ P (h), drawn from RBMs trained on a diverse collection of classical piano music (top) and on the four-part chorales by J. S. Bach (bottom), along with chord labels where the analysis is unambiguous. It is obvious that for the diverse collection, each sample has some room for additional melody notes with probabilities depending on the harmonic context (grey), whereas for JSB chorales, the simultaneities are taken from a more restricted pool and the samples are more clear-cut. This mechanism makes sense musically and the fact that RBMs can adapt to various styles will be useful for the following."
    }, {
      "heading" : "3 The RTRBM",
      "text" : "The RTRBM (Sutskever et al., 2008) is a sequence of conditional RBMs (one at each time step) whose\nparameters b (t) v , b (t) h ,W (t) are time-dependent and depend on the sequence history at time t, denoted A(t) ≡ {v(τ), ĥ(τ)|τ < t} where ĥ(t) is the mean-field value of h(t). Its graphical structure is depicted in Figure 2(a). The RTRBM is formally defined by its joint probability distribution:\nP ({v(t), h(t)}) = T∏ t=1 P (v(t), h(t)|A(t)) (7)\nwhere P (v(t), h(t)|A(t)) is the joint probability (eq. 1) of the tth RBM whose parameters are defined below (eq. 8 and 9).\nWhile all the parameters of the RBMs can depend on the previous time steps, we will consider the case where only the biases depend on ĥ(t−1):\nb (t) h = bh +W ′ĥ(t−1) (8)\nb(t)v = bv +W ′′ĥ(t−1) (9)\nwhich gives the RTRBM six parameters: W, bv, bh,W\n′,W ′′, ĥ(0). The general case is derived in a similar manner.\nWhile the hidden units h(t) are binary during inference and sampling, it is the mean-field value ĥ(t) that is transmitted to its successors (see eq. 10). This important distinction makes exact inference of the ĥ(t) very easy and improves the efficiency of training (Sutskever et al., 2008):\nĥ(t) = σ(Wv(t) + b (t) h ) = σ(Wv (t) +W ′ĥ(t−1) + bh) (10) is obtained directly from equations (2) and (8). Note that equation (10) is exactly the defining equation of a single-layer RNN with hidden units ĥ(t)."
    }, {
      "heading" : "4 The RNN-RBM",
      "text" : "The RTRBM can be understood as a sequence of conditional RBMs whose parameters are the output of a deterministic RNN, with the constraint that the hidden units must describe the conditional distributions and convey temporal information. This constraint can be lifted by combining a full RNN with distinct hidden units ĥ(t) with the RTRBM graphical model as shown in Figure 2(b). We call this model the RNN-RBM. The joint probability distribution of the RNN-RBM is also given by equation (7), but with ĥ(t) defined arbitrarily, here as per equation (11).\nFor simplicity, we consider the RBM parameters to be W, b (t) v , b (t) h (i.e. only the biases are variable) and a single-layer RNN (bottom portion of Fig. 2(b)) whose\nhidden units ĥ(t) are only connected to their direct predecessor ĥ(t−1) and to v(t) by the relation:\nĥ(t) = σ(W2v (t) +W3ĥ (t−1) + bĥ). (11)\nThe RBM portion of the RNN-RBM (upper portion of Fig. 2(b)) is otherwise exactly the same as its RTRBM counterpart. This gives the single-layer RNN-RBM nine parameters: W, bv, bh,W ′,W ′′, ĥ(0),W2,W3, bĥ.\nThe training algorithm is slightly different than for the RTRBM since the mean-field values of the h(t) are now distinct from ĥ(t). An iteration of training is based on the following general scheme:\n1. Propagate the current values of the hidden units ĥ(t) in the RNN portion of the graph using (11), 2. Calculate the RBM parameters that depend on the ĥ(t) (eq. 8 and 9) and generate the negative particles v(t)∗ using k-step block Gibbs sampling, 3. Use CDk to estimate the log-likelihood gradient\n(eq. 6) with respect to W , b (t) v and b (t) h ,\n4. Propagate the estimated gradient with respect to\nb (t) v , b (t) h backward through time (BPTT) (Rumelhart et al., 1986) to obtain the estimated gradient with respect to the RNN parameters.\nThis procedure can be adapted to any RNN architecture and conditional distribution estimator assuming the RNN provides the estimator’s parameters (step 2) and can be trained based on a stochastic gradi-\nent signal on those parameters (obtained in step 3). The RNN-NADE, obtained by substituting NADEs for RBMs, allows for exact gradient computation.\nNote that the single-layer RNN-RBM is a generalization of the RTRBM and reduces to this simpler model by setting W2 = W , W3 = W\n′ and bĥ = bh in equations (10) and (11). The RTRBM was not gaining computationally from sharing these connections, hence untying them does not make it slower. In practice, the ability to distinguish between the number of hidden units h and ĥ allows to scale RBMs to several hundred hidden units while keeping the RNNs to their (typically smaller) optimal size, improving performance."
    }, {
      "heading" : "4.1 Initialization strategies",
      "text" : "Initialization strategies based on unsupervised pretraining of each layer have been shown to be important both for supervised and unsupervised training of deep architectures (Bengio, 2009). A recurrent network corresponds to a very deep architecture when unfolded in time, and indeed we find that pretraining can clearly affect the overall performance of both the RTRBM and the RNN-RBM. To ensure the quality of the learned weight matrices, we found that initializing the W , bv and bh parameters from a trained RBM yields less noisy filters. The hidden-to-bias weights W ′,W ′′ can then be initialized to small random values, such that the sequential model will initially behave like independent RBMs, eventually departing from that state.\nIn order to capture better temporal dependencies, we initialize the W2,W3, bĥ,W ′′, bv, ĥ (0) parameters of the RNN-RBM from an RNN trained with the crossentropy cost:\nL({v(t)}) = 1 T T∑ t=1 nv∑ j=1 −v(t)j log y (t) j −(1−v (t) j ) log(1−y (t) j )\n(12)\nwhere y(t) = σ(b (t) v ) and equations (9) and (11) hold. This deterministic objective allows the use of a secondorder optimization method for pretraining of the RNN. Note that the RTRBM could use this strategy to initialize W,W ′, bv, bh,W\n′′, ĥ(0), but in practice we have found the initialization from an RBM more important."
    }, {
      "heading" : "4.2 Details of the BPTT algorithm",
      "text" : "Suppose we want to minimize the negative loglikelihood cost C ≡ − logP ({v(t)}). The gradient of C with respect to the parameters of the conditional RBMs can be estimated by CD using equations (4) and (6):\n∂C\n∂b (t) v\n' v(t)∗ − v(t) (13)\n∂C ∂W ' T∑ t=1 σ(Wv(t)∗−b(t)h )v (t)∗T−σ(Wv(t)−b(t)h )v (t)T\n(14) ∂C\n∂b (t) h\n' σ(Wv(t)∗ − b(t)h )− σ(Wv (t) − b(t)h ). (15)\nThe gradient then back-propagates through the hidden-to-bias parameters (eq. 8 and 9):\n∂C\n∂W ′ = T∑ t=1 ∂C ∂b (t) h ĥ(t−1)T (16)\n∂C\n∂W ′′ = T∑ t=1 ∂C ∂b (t) v ĥ(t−1)T (17)\n∂C ∂bh = T∑ t=1 ∂C ∂b (t) h and ∂C ∂bv = T∑ t=1 ∂C ∂b (t) v . (18)\nFor the single-layer RNN-RBM, the BPTT recurrence relation follows from (11):\n∂C\n∂ĥ(t) = W3\n∂C\n∂ĥ(t+1) ĥ(t+1)(1− ĥ(t+1))\n+W ′ ∂C\n∂b (t+1) h\n+W ′′ ∂C\n∂b (t+1) v\n(19)\nfor 0 ≤ t < T (ĥ(0) being a parameter of the model) and ∂C/∂ĥ(T ) = 0. Formulas for the remaining RNNRBM parameters are:\n∂C ∂bĥ = T∑ t=1 ∂C ∂ĥ(t) ĥ(t)(1− ĥ(t)) (20)\n∂C\n∂W3 = T∑ t=1 ∂C ∂ĥ(t) ĥ(t)(1− ĥ(t))ĥ(t−1)T (21)\n∂C\n∂W2 = T∑ t=1 ∂C ∂ĥ(t) ĥ(t)(1− ĥ(t))v(t)T. (22)"
    }, {
      "heading" : "5 Baseline experiments",
      "text" : "In this section, we compare the performance of the RTRBM with the RNN-RBM on two baseline datasets: bouncing balls videos and motion capture data (Sutskever et al., 2008). We use the mean framelevel squared prediction error as a basis of comparison. The prediction of the tth conditional RBM is performed by 50 steps of block Gibbs sampling starting at v(t−1) and hoping to reconstruct v(t) optimally.\nThe bouncing ball videos dataset1 is based on a simulation of balls bouncing in a box (Sutskever & Hinton,\n1www.cs.utoronto.ca/~ilya/code/2008/RTRBM.tar\n2007). The generated videos are of length T = 128 and of resolution 15 × 15 pixels in the [0, 1] interval, which makes binary RBMs (eq. 1) well suited for this task. With up to 300 hidden units and an initial learning rate of 0.01, we obtain a squared prediction error of 2.11 for the RTRBM and 0.96 for the RNN-RBM, i.e. less than half the error. The receptive fields (weights) of the first 48 hidden units h(t) (RNN-RBM) are plotted in Figure 3. Localized edge detectors are apparent in nearly all the learned filters.\nThe human motion capture dataset2 is represented by a sequence of joint angles, translations and rotations of the base of the spine in an exponential-map parameterization (Hsu et al., 2005; Taylor et al., 2007). Since the data consists of 49 real values per time step, we use the Gaussian RBM variant (Welling et al., 2005) for this task. We use up to 450 hidden units and an initial learning rate of 0.001. The mean squared prediction test error is 20.1 for the RTRBM and reduced substantially to 16.2 for the RNN-RBM."
    }, {
      "heading" : "6 Modeling sequences of polyphonic music",
      "text" : "In this section, we show results with main application of interest for this paper: probabilistic modeling of sequences of polyphonic music. We report our experiments on four datasets of varying complexity converted to our input format.\nPiano-midi.de is a classical piano MIDI archive that was split according to Poliner & Ellis (2007). Nottingham is a collection of 1200 folk tunes3 with chords instantiated from the ABC format. MuseData is an electronic library of orchestral and piano classical music from CCARH4. JSB chorales refers to the entire corpus of 382 fourpart harmonized chorales by J. S. Bach with the split of Allan & Williams (2005).\n2people.csail.mit.edu/ehsu/work/sig05stf 3ifdo.ca/~seymour/nottingham/nottingham.html 4www.musedata.org\nEach dataset contains at least 7 hours of polyphonic music and the total duration is approximately 67 hours. The polyphony (number of simultaneous notes) varies from 0 to 15 and the average polyphony is 3.9. We use an input of 88 binary visible units that span the whole range of piano from A0 to C8 and temporally aligned on an integer fraction of the beat (quarter note). Consequently, pieces with different time signatures will not have their measures start at the same interval. Although it is not strictly necessary, learning is facilitated if the sequences are transposed in a common tonality (e.g. C major/minor) as preprocessing.\nIn addition to the models previously described, we evaluate the following commonly used methods:\n• The simplest baseline model consists in outputting a Gaussian density centered on the previous frame µ = v(t−1) and learned covariance Σ. • N-grams simulate the evolution of note simultaneities as an (N − 1)th-order Markov chain. We use add-p or Gaussian smoothing and back-off. • Note N-grams model each note independently by a binary N-gram, possibly with shared parameters (IID). • An interesting model for chorales harmonisation (Allan & Williams, 2005) has been adapted to serve as a generative model. It can only be evaluated on the JSB chorales dataset. • The ‘random fields’ approach of Lavrenko & Pickens (2003) is a type of fully visible sigmoid belief network with learned connectivity. • Other common methods include Gaussian mixture models (GMM), hidden Markov models (HMM) using GMM indices as their state, and multilayer perceptrons (MLP) with the last n time steps as input.\nThe log-likelihood (LL) and expected frame-level accuracy (ACC) (Bay et al., 2009) of the symbolic models are presented in Table 1. We estimate the partition function of each conditional RBM by 100 runs of annealed importance sampling (Salakhutdinov & Murray, 2008). We make a few key observations:\n• The complexity of the dataset, such as the simplistic chord accompaniment of Nottingham and the redundant style of four-part chorales by a single composer, in comparison with diverse piano and orchestral music, is clearly reflected in the obtained log-likelihoods and accuracies. • N-gram models (optimal N∗ = 2) perform reasonably well for simple datasets but fail in more realistic settings due to the increased data sparsity. In this case, note N-grams (N∗ ∈ [8, 14]) are a better alternative albeit ignoring harmonic dependencies. This inherent trade-off in traditional polyphonic music\nmodels can be addressed robustly by the RNN-based models, that perform better on a range of datasets. • The harmonisation model of Allan & Williams (2005), tailored to the specific style of four-part chorales, requires annotated harmonic symbols and yet performs relatively poorly compared to our best performer. Similarly to the GMM + HMM, this model is penalized by the limited history of the HMM and by the difficulty to generalize to new chord voicings in a principled manner. • In accordance with earlier results (Martens & Sutskever, 2011), the use of HF significantly helps the density estimation and prediction performance of RNNs (eq. 12) which would otherwise perform worse than simpler MLPs. This motivates our strategy of pretraining the RNN layer of an RNN-RBM via HF. • In addition to the distinct recurrent hidden units ĥ(t) that convey temporal information more freely, and the fact that suitable learning rates can be specified differently for the RNN and the RBM parts, pretraining the W2, W3 and bĥ parameters can have the most impact on the RNN-RBM prediction performance. Figure 4 clearly demonstrates the importance of pretraining and finetuning the RNN and the additional advantage of using HF. • Although frame-level NADEs are slightly less powerful than RBMs, their desirable properties make the combined RNN-NADE model the most robust distribution estimator. We believe this is due to their tractable distribution, for two reasons. First, CD may not be ideally suited for conditional RBMs with slowly-mixing Gibbs chains (Mnih et al., 2011), a non-issue for exact-gradient models. Secondly, the joint sequential model, and not only the RNN portion, can benefit from second-order optimization as can be seen from the last two rows of Table 1.\nWe evaluate our models qualitatively by generating sample sequences, provided on the authors’ website5, and discussed here. While note correlations are obviously neglected in the simpler models (sequence 2), RBM-based models learned basic harmony rules (sequence 3), melody lines (sequences 4, 8) and local temporal coherence (sequence 5). However, long-term structure and musical meter remain elusive."
    }, {
      "heading" : "7 Polyphonic transcription",
      "text" : "Multiple fundamental frequency (f0) estimation, or polyphonic transcription, consists in estimating the audible note pitches in the signal at 10 ms intervals without tracking note contours. We combine our polyphonic sequence models with the acoustic model of Nam et al. (2011) in order to demonstrate a practical application of the sequence models. Their model was adapted for multiple instruments, and it can be generalized to any method that can score hypothetical combinations of f0 for a given time frame.\nAt each time frame, the Nam et al. (2011) algorithm outputs independent probabilities that each note is present and reports every note with probability p ≥ 0.5. To incorporate our symbolic model prediction Ps(v\n(t)|A(t)), we consider the k most promising f0 candidates (k = 7) from the acoustic model Pa(v (t))\n5www-etud.iro.umontreal.ca/~boulanni/icml2012\nand jointly evaluate all combinations of M candidates ∀M ≤ k by the following cost function:\nC = − logPa(v(t))− α logPs(v(t)|Ã(t)) (23)\nwhere Ã(t) is the approximate sequence history constructed from the f0 estimated so far in at least half the audio frames corresponding to each past symbolic time step6. This corresponds to a product of experts where the hyperparameter α is the confidence coefficient of our symbolic predictor. If our algorithm is run on audio signals without preprocessing, tempo tracking must be performed first. Since the symbolic models describe only fixed tonality pieces, a first audio-only pass is needed to transpose the estimated f0 in the correct tonality. Once the optimal f0 estimates have been determined, HMM smoothing can still filter out spurious results and enhance onset accuracies.\nDigital audio has been generated for the four datasets and we report in Figure 5 the frame-level transcription accuracy of the Nam et al. (2011) algorithm, either alone, after HMM smoothing, or using our best performing model as a symbolic prior. We observe an improvement in absolute accuracy between 1.3% and 10% over the HMM approach. It can be seen easily\n6This can create a ‘snowball’ effect where accurate baseline transcriptions form accurate Ã(t) estimates, resulting in more relevant symbolic predictions Ps(v\n(t)|Ã(t)), which in turn improve the final transcription.\nthat an HMM with emission probabilities Pa(v (t)) is equivalent to equation (23) with a note 2-gram symbolic model, one time step per audio frame and α = 1. It is therefore unsurprising that the advantage of our search algorithm decreases when the note N-gram already performs well, e.g. for Piano-midi.de (Table 1). However, the HMM allows for a global search of the most likely f0 (the Viterbi path), whereas our algorithm requires a greedy chronological search, a limitation we are currently working to address."
    }, {
      "heading" : "8 Conclusions",
      "text" : "We presented an RNN-based model that can learn harmonic and rhythmic probabilistic rules from polyphonic music scores of varying complexity, substantially better than popular methods in music information retrieval. We showed that different strategies related to the description of temporal dependencies can improve prediction accuracy of such models. While longer-term musical structure remains elusive in our unconstrained representation, our model can immediately serve as a symbolic prior for polyphonic transcription, clearly improving the state of the art in this area."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors would like to thank NSERC, CIFAR and the Canada Research Chairs for funding, and Compute Canada/Calcul Québec for computing resources."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2012,
    "abstractText" : "We investigate the problem of modeling symbolic sequences of polyphonic music in a completely general piano-roll representation. We introduce a probabilistic model based on distribution estimators conditioned on a recurrent neural network that is able to discover temporal dependencies in high-dimensional sequences. Our approach outperforms many traditional models of polyphonic music on a variety of realistic datasets. We show how our musical language model can serve as a symbolic prior to improve the accuracy of polyphonic transcription.",
    "creator" : "TeX"
  }
}