{
  "name" : "1602.01064.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Minimum Regret Search for Single- and Multi-Task Optimization",
    "authors" : [ "Jan Hendrik Metzen" ],
    "emails" : [ "JANMETZEN@MAILBOX.ORG" ],
    "sections" : [ {
      "heading" : "1. INTRODUCTION",
      "text" : "Bayesian optimization (BO, Shahriari et al., 2016) denotes a sequential, model-based, global approach for optimizing black-box functions. It is particularly well-suited for problems which are non-convex, do not necessarily provide derivatives, are expensive to evaluate (either computationally, economically, or morally), and can potentially be noisy. Under these conditions, there is typically no guarantee for finding the true optimum of the function with a finite number of function evaluations. Instead, one often aims at finding a solution which has small simple regret (Bubeck et al., 2009) with regard to the true optimum, where simple regret denotes the difference of the true optimal function value and the function value of the “solution” selected by the algorithm after a finite number of function evaluations. BO aims at finding such a solution of small simple regret while minimizing at the same time the number of evaluations of the expensive target function. For this, BO maintains a probabilistic surrogate model of the objective function, and a myopic utility or acquisition function,\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\nwhich defines the “usefulness” of performing an additional function evaluation at a certain input for learning about the optimum.\nBO has been applied to a diverse set of problems, ranging from hyperparameter optimization of machine learning models (Bergstra et al., 2011; Snoek et al., 2012) over robotics (Calandra et al., 2015; Lizotte et al., 2007; Kroemer et al., 2010; Marco et al., 2015) to sensor networks (Srinivas et al., 2010) and environmental monitoring (Marchant & Ramos, 2012). For a more comprehensive overview of application areas, we refer to Shahriari et al. (2016).\nA critical component for the performance of BO is the acquisition function, which controls the exploratory behavior of the sequential search procedure. Different kinds of acquisition functions have been proposed, ranging from improvement-based acquisition functions over optimistic acquisition functions to information- theoretic acquisition functions (see Section 2). In the latter class, the group of entropy search-based approaches (Villemonteix et al., 2008; Hennig & Schuler, 2012; Hernández-Lobato et al., 2014), which aims at maximizing the information gain regarding the true optimum, has achieved state-of-the-art performance on a number of synthetic and real-world problems. However, performance is often reported as the median over many runs, which bears the risk that the median masks “outlier” runs that perform considerably worse than the rest. In fact, our results indicate that the performance of sampling-based entropy search is not necessarily better than traditional and cheaper acquisition functions according to the mean simple regret.\nIn this work, we propose minimum regret search (MRS), a novel acquisition function that explicitly aims at minimizing the expected simple regret (Section 3). MRS performs well according to both the mean and median performance on a synthetic problem (Section 5.1). Moreover, we discuss how MRS can be extended to multi-task optimization problems (Section 4) and present empirical results on a simulated robotic control problem (Section 5.2).\nar X\niv :1\n60 2.\n01 06\n4v 3\n[ st\nat .M\nL ]\n2 4\nM ay\n2 01\n6"
    }, {
      "heading" : "2. BACKGROUND",
      "text" : "In this section, we provide a brief overview of Bayesian Optimization (BO). We refer to Shahriari et al. (2016) for a recent more extensive review of BO. BO can be applied to black-box optimization problems, which can be framed as optimizing an objective function f : X → R on some bounded set X ⊂ RD. In contrast to most other blackbox optimization methods, BO is a global method which makes use of all previous evaluations of f(x) rather than using only a subset of the history for approximating a local gradient or Hessian. For this, BO maintains a probabilistic model for f(x), typically a Gaussian process (GP, Rasmussen & Williams, 2006), and uses this model for deciding at which xn+1 the function f will be evaluated next.\nAssume we have already queried n datapoints and observed their (noisy) function values Dn = {(xi, yi)}ni=1. The choice of the next query point xn+1 is based on a utility function over the GP posterior, the so-called acquisition function a : X → R, via xn+1 = arg maxx a(x). Since the maximum of the acquisition function cannot be computed directly, a global optimizer is typically used to determine xn+1. A common strategy is to use DIRECT (Jones et al., 1993) to find the approximate global maximum, followed by L-BFGS (Byrd et al., 1995) to refine it.\nThe first class of acquisition functions are optimistic policies such as the upper confidence bound (UCB) acquisition function. aUCB aims at minimizing the regret during the course of BO and has the form\naUCB(x;Dn) = µn(x) + κnσn(x),\nwhere κn is a tunable parameter which balances exploitation (κn = 0) and exploration (κn 0), and µn and σn denote mean and standard deviation of the GP at x, respectively. Srinivas et al. (2010) proposed GP-UCB, which entails a specific schedule for κn that yields provable cumulative regret bounds.\nThe second class of acquisition functions are improvementbased policies, such as the probability of improvement (PI, Kushner, 1964) over the current best value, which can be calculated in closed-form for a GP model:\naPI(x;Dn) := P[f(x) > τ ] = Φ(γ(x)),\nwhere γ(x) = (µn(x)−τ)/σn(x), Φ(·) denotes the cumulative distribution function of the standard Gaussian, and τ denotes the incumbent, typically the best function value observed so far: τ = maxi yi. Since PI exploits quite aggressively (Jones, 2001), a more popular alternative is the expected improvement (EI, Mockus et al., 1978) over the current best value aEI(x;D) := E[(f(x) − τ)I(f(x) > τ)], which can again be computed in closed form for a GP model as aEI(x;D) = σn(x) (γ(x)Φ(γ(x)) + φ(γ(x))),\nwhere φ(·) denotes the standard Gaussian density function. A generalization of EI is the knowledge gradient factor (Frazier et al., 2009), which can better handle noisy observations, which impede the estimation of the incumbent. The knowledge gradient requires defining a set An from which one would choose the final solution.\nThe third class of acquisition functions are informationbased policies, which entail Thompson sampling and entropy search (ES, Villemonteix et al., 2008; Hennig & Schuler, 2012; Hernández-Lobato et al., 2014). Let p?(x|Dn) denote the posterior distribution of the unknown optimizer x? = arg maxx∈X f(x) after observing Dn. The objective of ES is to select the query point that results in the maximal reduction in the differential entropy of p?. More formally, the entropy search acquisition function is defined as\naES(x,Dn) = H(x?|Dn)−Ey|x,Dn [H(x ?|Dn∪{(x, y))],\nwhere H(x?|Dn) denotes the differential entropy of p?(x|Dn) and the expectation is with respect to the predictive distribution of the GP at x, which is a normal distribution y ∼ N (µn(x), σ2n(x) + σ2). Computing aES directly is intractable for continuous spaces X ; prior work has discretized X and used either Monte Carlo sampling (Villemonteix et al., 2008) or expectation propagation (Hennig & Schuler, 2012). While the former may require many Monte Carlo samples to reduce variance, the latter incurs a run time that is quartic in the number of representer points used in the discretization of p?. An alternative formulation is obtained by exploiting the symmetric property of the mutual information, which allows rewriting the ES acquisition function as\naPES(x,Dn) = H(y|Dn,x)− Ex?|Dn [H(y|Dn,x,x ?)].\nThis acquisition function is known as predictive entropy search (PES, Hernández-Lobato et al., 2014). PES does not require discretization and allows a formal treatment of GP hyperparameters.\nContextual Policy Search (CPS) denotes a model-free approach to reinforcement learning, in which the (low-level) policy πθ is parametrized by a vector θ. The choice of θ is governed by an upper-level policy πu. For generalizing learned policies to multiple tasks, the task is characterized by a context vector s and the upper-level policy πu(θ|s) is conditioned on the respective context. The objective of CPS is to learn the upper-level policy πu such that the expected return J over all contexts is maximized, where J = ∫ s p(s) ∫ θ πu(θ|s)R(θ, s)dθds. Here, p(s) is the distribution over contexts and R(θ, s) is the expected return when executing the low level policy with parameter θ in context s. We refer to Deisenroth et al. (2013) for a recent overview of (contextual) policy search approaches in robotics."
    }, {
      "heading" : "3. MINIMUM REGRET SEARCH",
      "text" : "Information-based policies for Bayesian optimization such as ES and PES have performed well empirically. However, as we discuss in Section 3.3, their internal objective of minimizing the uncertainty in the location of the optimizer x?, i.e., minimizing the differential entropy of p?, is actually different (albeit related) to the common external objective of minimizing the simple regret of x̃N , the recommendation of BO for x? after N trials. We define the simple regret of x̃N as Rf (x̃N ) = f(x?) − f(x̃N ) = maxx f(x)− f(x̃N ).\nClearly, x? has zero and thus minimum simple regret, but a query that results in the maximal decrease in H(x?) is not necessarily the one that also results in the maximal decrease in the expected simple regret. In this section, we propose minimum regret search (MRS), which explicitly aims at minimizing the expected simple regret."
    }, {
      "heading" : "3.1. Formulation",
      "text" : "Let X ⊂ RD be some bounded domain and f : X 7→ R be a function. We are interested in finding the maximum x? of f on X . Let there be a probability measure p(f) over the space of functions f : X 7→ R, such as a GP. Based on this p(f), we would ultimately like to select an x̃ which has minimum simple regretRf (x̃). We define the expected simple regret ER of selecting parameter x under p(f) as:\nER(p)(x) = Ep(f)[Rf (x)] = Ep(f)[max x f(x)− f(x)]\nIn N -step Bayesian optimization, we are given a budget of N function evaluations and are to choose a sequence of N query points XqN = {x q 1, . . . ,x q N} for which we evaluate yi = f(x q i ) + to obtain DN = {x q i , yi}Ni=1. Based on this, pN (f) = p(f |DN ) is estimated and a point x̃N is recommended as the estimate of x? such that the expected simple regret under pN is minimized, i.e., x̃N = arg minx ER(pN )(x). The minimizer x̃N of the expected simple regret under fixed p(f) can be approximated efficiently in the case of a GP since it is identical to the maximizer of the GP’s mean µN (x).\nHowever, in general, p(f) depends on data and it is desirable to select the data such that ER is also minimized with regard to the resulting p(f). We are thus interested in choosing a sequence XqN such that we minimize the expected simple regret of x̃N with respect to the pN , where pN depends on X q N and the (potentially noisy) observations yi. As choosing the optimal sequence X q N at once is intractable, we follow the common approach in BO and select xqn sequentially in a myopic way. However, as x̃N itself depends on DN and is thus unknown for n < N , we have to use proxies for it based on the currently available subset Dn ⊂ DN . One simple choice for a proxy is\nto use the point which has minimal expected simple regret under pn(f) = p(f |Dn), i.e., x̃ = arg minx ER(pn)(x). Let us denote the updated probability measure on f after performing a query at xq and observing the function value y = f(xq) + by p[x q,y] n = p(f |Dn ∪ {xq, y}). We define the acquisition function MRSpoint as the expected reduction of the minimum expected regret for a query at xq , i.e.,\naMRSpoint(x q) = min x̃ ER(pn)(x̃)\n− Ey|pn(f),xq [min x̃\nER(p[x q,y]\nn )(x̃)],\nwhere the expectation is with respect to pn(f)’s predictive distribution at xq and we drop the implicit dependence on pn(f) in the notation of aMRSpoint . The next query point xqn+1 would thus be selected as the maximizer of aMRSpoint .\nOne potential drawback of MRSpoint, however, is that it does not account for the inherent uncertainty about x̃N . To address this shortcoming, we propose using the measure p?Dn = p\n?(x|Dn) as defined in entropy search (see Section 2) as proxy for x̃N . We denote the resulting acquisition function by MRS and define it analogously to aMRSpoint :\naMRS(x q) = Ex̃∼p?n [ER(pn)(x̃)]\n− Ey|pn(f),xq [Ex̃∼p?Dn∪{(xq,y)} [ER(p [xq,y] n )(x̃)]].\nMRS can thus be seen as a more Bayesian treatment, where we marginalize our uncertainty about x̃N , while MRSpoint is more akin to a point-estimate since we use a single point (the minimizer of the expected simple regret) as proxy for x̃N ."
    }, {
      "heading" : "3.2. Approximation",
      "text" : "Since several quantities in MRS cannot be computed in closed form, we resort to similar discretizations and approximations as proposed for entropy search by Hennig & Schuler (2012). We focus here on sampling based approximations; for an alternative way of approximating Ep(f) based on expectation propagation, we refer to Hennig & Schuler (2012).\nFirstly, we approximate Ep(f) by taking nf Monte Carlo samples from p(f), which is straightforward in the case of GPs. Secondly, we approximate Ey|p(f),xq by taking ny Monte Carlo samples from p(f)’s predictive distribution at xq . And thirdly, we discretize p? to a finite set of nr representer points chosen from a non-uniform measure, which turns Ex̃∼p?n in the definition of MRS(x\nq) into a weighted sum. The discretization of p? is discussed by Hennig & Schuler (2012) in detail; we select the representer points as follows: for each representer point, we sample 250 candidate points uniform randomly from X and select the representer point by Thompson sampling from p(f) on the candidate points. Moreover, estimating p?Dn on the representer\npoints can be done relatively cheap by reusing the samples used for approximating Ep(f), which incurs a small bias which had, however, a negligible effect in preliminary experiments.\nThe resulting estimate of aMRS would have high variance and would require nf to be chosen relatively large; however, we can reduce the variance considerably by using common random numbers (Kahn & Marshall, 1953) in the estimation of ER(p)(x) for different p(f)."
    }, {
      "heading" : "3.3. Illustration",
      "text" : "Figure 1 presents an illustration of different acquisition functions on a simple one-dimensional target function. The left graphic shows a hypothetical GP posterior (illustrated by its mean and standard deviation) for length scale l = 0.75, and the resulting probability of x being the optimum of f denoted by p?. Moreover, the expected simple regret of selecting x denoted by ER(x) is shown. The minimum of ER(x) and the maximum of p? are both at x̃ = 1.5. The expected regret of x̃ is approximately ER(x̃) = 0.07. We plot E[max(f(x)−f(x̃), 0)] additionally to shed some light onto situations in which x̃ = 1.5 would incur a significant regret: this quantity shows that most of the expected regret of x̃ stems from situations where the “true” optimum is located at x ≥ 3.5. This can be explained by the observation that this area has high uncertainty and is at the same time largely uncorrelated with x̃ = 1.5 because of the small length-scale of the GP.\nThe right graphic compares different acquisition functions for nf = 1000, nr = 41, ny = 11, and fixed representer points. Since the assumed GP is noise-free, the acquisitionvalue of any parameter that has already been evaluated is approximately 0. The acquisition functions differ considerably in their global shape: EI becomes large for areas\nwith close-to-maximal predicted mean or with high uncertainty. ES becomes large for parameters which are informative with regard to most of the probability mass of p?, i.e., for xq ∈ [0.5, 3.0]. In contrast MRSpoint becomes maximal for xq ≈ 4.0. This can be explained as follows: according to the current GP posterior, MRSpoint selects x̃ = 1.5. As shown in the Figure 1 (left), most of the expected regret for this value of x̃ stems from scenarios where the true optimum would be at x ≥ 3.5. Thus, sampling in this parameter range can reduce the expected regret considerably— either by confirming that the true value of f(x) on x ≥ 3.5 is actually as small as expected or by switching x̃n+1 to this area if f(x) turns out to be large. The maximum of MRS is similar to MRSpoint. However, since it also takes the whole measure p? into account, its acquisition surface is smoother in general; in particular, it assigns a larger value to regions such as xq ≈ 3.0, which do not cause regret for x̃ = 1.5 but for alternative choices such as x̃ = 4.0.\nWhy does ES not assign a large acquisition value to query points xq ≥ 3.0? This is because ES does not take into account the correlation of different (representer) points under p(f). This, however, would be desirable as, e.g., reducing uncertainty regarding optimality among two highly correlated points with large p? (for instance x1 = 1.5 and x2 = 1.55 in the example) will not change the expected regret considerably since both points will have nearly identical value under all f ∼ p(f). On the other hand, the value of two points which are nearly uncorrelated under p(f) and have non-zero p? such as x1 and x3 = 4.0 might differ considerably under different f ∼ p(f) and choosing the wrong one as x̃ might cause considerable regret. Thus, identifying which of the two is actually better would reduce the regret considerably. This is exactly why MRS assigns large value to xq ≈ 4.0."
    }, {
      "heading" : "4. MULTI-TASK MINIMUM REGRET SEARCH",
      "text" : "Several extensions of Bayesian optimization for multi-task learning have been proposed, both for discrete set of tasks (Krause & Ong, 2011) and for continuous set of tasks (Metzen, 2015). Multi-task BO has been demonstrated to learn efficiently about a set of discrete tasks concurrently (Krause & Ong, 2011), to allow transferring knowledge learned on cheaper tasks to more expensive tasks (Swersky et al., 2013), and to yield state-of-the-art performance on low-dimensional contextual policy search problems (Metzen et al., 2015), in particular when combined with active learning (Metzen, 2015). In this section, we focus on multitask BO for a continuous set of tasks; a similar extension for discrete multi-task learning would be straightforward.\nA continuous set of tasks is encountered for instance when applying BO to contextual policy search (see Section 2). We follow the formulation of BO-CPS (Metzen et al., 2015) and adapt it for MRS were required. In BO-CPS, the set of tasks is encoded in a context vector s ∈ S and BOCPS learns a (non-parametric) upper-level policy πu which selects for a given context s the parameters θ ∈ X of the low-level policy πθ. The unknown function f corresponds to the expected return R(θ, s) of executing a low-level policy with parameters θ in context s. Thus, the probability measure p(f) (typically a GP) is defined over functions f : X × S 7→ R on the joint parameter-context space. The probability measure is conditioned on the trials performed so far, i.e., pn(f) = p(f |Dn) withDn = {((θi, si), ri)}ni=1 for ri = R(θi, si). Since pn is defined over the joint parameter and context space, experience collected in one context is naturally generalized to similar contexts.\nIn passive BO-CPS on which we focus here (please refer to Metzen (2015) for an active learning approach), the context (task) sn of a trial is determined externally according to p(s), for which we assume a uniform distribution in this work. BO-CPS selects the parameter θn by conditioning p(f) on s = sn and finding the maximum of the acquisition function a for fixed sn, i.e., θn = arg maxθ a(θ, sn). Acquisition functions such as PI and EI are not easily generalized to multi-task problems as they are defined relative to an incumbent τ , which is typically the best function value observed so far in a task. Since there are infinitely many tasks and no task is visited twice with high probability, the notion of an incumbent is not directly applicable. In contrast, the acquisition functions GP-UCB (Srinivas et al., 2010) and ES (Metzen et al., 2015) have been extended straightforwardly and the same approach applies also to MRS."
    }, {
      "heading" : "5. EXPERMENTS",
      "text" : ""
    }, {
      "heading" : "5.1. Synthetic Single-Task Benchmark",
      "text" : "In the first experiment1, we conduct a similar analysis as Hennig & Schuler (2012, Section 3.1): we compare different algorithms on a number of single-task functions sampled from a generative model, namely from the same GPbased model that is used by the optimization internally as surrogate model. This precludes model-mismatch issues and unwanted bias which could be introduced by resorting to common hand-crafted test functions2. More specifically, we choose the parameter space to be the 2-dimensional unit domain X = [0, 1]2 and generate test functions by sampling 250 function values jointly from a GP with an isotropic RBF kernel of length scale l = 0.1 and unit signal variance. A GP is fitted to these function values and the resulting posterior mean is used as the test function. Moreover, Gaussian noise with standard deviation σ = 10−3 is added to each observation. The GP used as surrogate model in the optimizer employed the same isotropic RBF kernel with fixed, identical hyperparameters. In order to isolate effects of the different acquisition functions from effects of different recommendation mechanisms, we used the point which maximizes the GP posterior mean as recommendation x̃N regardless of the employed acquisition function. All algorithms were tested on the same set of 250 test functions, and we used nf = 1000, nr = 25, and ny = 51. We do not provide error-bars on the estimates as the data sets have no parametric distribution. However, we provide additional histograms on the regret distribution.\nFigure 2 summarizes the results for a pure exploration setting, where we are only interested in the quality of the algorithm’s recommendation for the optimum after N queries but not in the quality of the queries themselves: according to the median of the simple regret (top left), ES, MRS, and MRSpoint perform nearly identical, while EI is about an order of magnitude worse. GP-UCB performs even worse initially but surpasses EI eventually3. PI performs the worst as it exploits too aggressively. These results are roughly in correspondence with prior results (Hennig & Schuler, 2012; Hernández-Lobato et al., 2014) on the same task; note, however, that Hernández-Lobato et al. (2014) used a lower noise level and thus, absolute values are not comparable. However, according to the mean simple regret, the picture changes considerably (top right): here, MRS,\n1Source code for replicating the reported experiment is available under https://github.com/jmetzen/bayesian_ optimization.\n2Please refer to the Appendix A for an analogous experiment with model-mismatch.\n3GP-UCB would reach the same level of mean and median simple regret as MRS eventually after N = 200 steps (in mean and median) with no significant difference according to a Wilcoxon signed-rank test.\nMRSpoint, and EI perform roughly on par while ES is about an order of magnitude worse. This can be explained by the distribution of the simple regrets (bottom): while the distributions are fairly non-normal for all acquisition functions, there are considerably more runs with very high simple regret (Rf (x̃N ) > 10−2) for ES (10) than for MRS (4) or EI (4).\nWe illustrate one such case where ES incurs high simple regret in Figure 3. The same set of representer points has been used for ES and MRS. While both ES and MRS assign a non-zero density p? (representer points) to the area of the true optimum of the function (bottom center), ES assigns high acquisition value only to areas with a high density of p? in order to further concentrate density in these areas. Note that this is not due to discretization of p? but because of ES’ objective, which is to learn about the precise location of the optimum, irrespective of how much correlation their is between the representer points according to p(f). Thus, predictive entropy search (Hernández-Lobato et al., 2014) would likely be affected by the same deficiency. In contrast, MRS focuses first on areas which have not been explored and have a non-zero p?, since those are areas with high expected simple regret (see Section 3.3). Accordingly, MRS is less likely to incur a high simple regret. In summary, the MRS-based acquisition functions are the only acquisition functions that perform well both according to the median and the mean simple regret; moreover, MRS performs slightly better than MRSpoint and we will focus on MRS subsequently."
    }, {
      "heading" : "5.2. Multi-Task Robotic Behavior Learning",
      "text" : "We present results in the simulated robotic control task used by Metzen (2015), in which the robot arm COMPI (Bargsten & de Gea, 2015) is used to throw a ball at a target on the ground encoded in a two-dimensional context vector. The target area is S = [1, 2.5]m× [−1, 1]m and the robot arm is mounted at the origin (0, 0) of this coordinate system. Contexts are sampled uniform randomly from S. The low-level policy is a joint-space dynamical movement primitives (DMP, Ijspeert et al., 2013) with preselected start and goal angle for each joint and all DMP weights set to 0. This DMP results in throwing a ball such that it hits the ground close to the center of the target area. Adaptation to different target positions is achieved by modifying the parameter θ: the first component of θ corresponds to the execution time τ of the DMP, which determines how far the ball is thrown, and the further components encode the final angle gi of the i-th joint. We compare the learning performance for different number of controllable joints; the notcontrolled joints keep the preselect goal angles of the initial throw. The limits on the parameter space are gi ∈ [ −π2 , π 2\n] and τ ∈ [0.4, 2].\nAll approaches use a GP with anisotropic Matérn kernel for representing p(f) and the kernel’s length scales and signal variance are selected in each BO iteration as point estimates using maximum marginal likelihood. Based on preliminary experiments, UCB’s exploration parameter κ is set to a constant value of 5.0. For MRS and ES, we use the same parameter values as in the Section 5.1, namely nf = 1000, nr = 25, and ny = 51. Moreover, we add a “greedy” acquisition function, which always selects θ that maximizes the mean of the GP for the given context (UCB with κ = 0), and a “random” acquisition function that selects θ randomly. The return is defined as R(θ, s) = −||s − bs||2 − 0.01 ∑ t v 2 t , where bs denotes\nthe position hit by the ball, and ∑ t v 2 t denotes a penalty term on the sum of squared joint velocities during DMP execution; both bs and ∑ t v 2 t depend indirectly on θ.\nFigure 4 summarizes the main results of the empirical evaluation for different acquisition functions. Shown is the mean offline performance of the upper-level policy at 16 test contexts on a grid over the context space. Selecting parameters randomly (“Random”) or greedily (“Greedy”) during learning is shown as a baseline and indicates that generalizing experience using a GP model alone does not suffice for quick learning in this task. In general, performance when learning only the execution time and the first joint is better than learning several joins at once. This is because the execution time and first joint allow already adapting the throw to different contexts (Metzen et al., 2015); controlling more joints mostly adds additional search dimensions. MRS and ES perform on par for controlling one or two joints and outperform UCB. For higher-dimensional search spaces (three or four controllable joints), MRS performs slightly better than ES (p < 0.01 after 200 episodes for a Wilcoxon signed-rank test). A potential reason for this might be the increasing number of areas with potentially high regret in higher dimensional spaces that may remain unexplored by ES; however, this hypothesis requires further investigation in the future."
    }, {
      "heading" : "6. DISCUSSION AND CONCLUSION",
      "text" : "We have proposed MRS, a new class of acquisition functions for single- and multi-task Bayesian optimization that is based on the principle of minimizing the expected simple regret. We have compared MRS empirically to other acquisition functions on a synthetic single-task optimization problem and a simulated multi-task robotic control problem. The results indicate that MRS performs favorably compared to the other approaches and incurs less often a high simple regret than ES since its objective is explicitly focused on minimizing the regret. An empirical comparison with PES remains future work; since PES uses the same objective as ES (minimizing H(x?)), it will likely show\nthe same deficit of ignoring areas that have small probability p? but could nevertheless cause a large potential regret. On the other hand, in contrast to ES and MRS, PES allows a formal treatment of GP hyperparameters, which can make it more sample-efficient. Potential future research on approaches for addressing GP hyperparameters and more efficient approximation techniques for MRS would thus be desirable. Additionally, combining MRS with active learning as done for entropy search by Metzen (2015) would be interesting. Moreover, we consider MRS to be a valuable addition to the set of base strategies in a portfolio-based BO approach (Shahriari et al., 2014). On a more theoretical level, it would be interesting if formal regret bounds can be proven for MRS.\nAcknowledgments This work was supported through two grants of the German Federal Ministry of Economics and Technology (BMWi, FKZ 50 RA 1216 and FKZ 50 RA 1217)."
    }, {
      "heading" : "A. Synthetic Single-Task Benchmark with Model Mismatch",
      "text" : "We present results for an identical setup as reported in Section 5.1, with the only difference being that the test functions have been sampled from a GP with rational quadratic kernel with length scale l = 0.1 and scale mixture α = 1.0. The kernel used in the GP surrogate model is not modified, i.e., an RBF kernel with length scale l = 0.1 is used. Thus, since different kind of kernel govern test functions and surrogate model, we have model mismatch as would be the common case on real-world problems. Figure 5 summarizes the results of the experiment. Interestingly, in contrast to the experiment without model mismatch, for this setup there are also considerable differences in the mean simple regret between MRS and ES: while ES performs slightly better initially, it is outperformed by MRS for N > 60. We suspect that this is because ES tends to explore more locally than MRS once p? has mostly settled onto one region of the search space. More local exploration, however, can be detrimental in the case of model-mismatch since the surrogate model is more likely to underestimate the function value in regions which have not been sampled. Thus a more homogeneous sampling of the search space as done by the more global exploration of MRS is beneficial. As a second observation, in contrast to a no-model-mismatch scenario, MRSpoint performs considerably worse than MRS when there is model-mismatch. This emphasizes the importance of accounting for uncertainty, particularly when there is model mis-specification.\nAccording to the median simple regret, the difference between MRS, MRSpoint, ES, and EI is less pronounced in Figure 5. Moreover, the histograms of the regret distribution exhibit less outliers (regardless of the method). We suspect that this stems from properties of the test functions that are sampled from a GP with rational quadratic rather than from the model-mismatch. However, a conclusive answer on this would require further experiments."
    } ],
    "references" : [ {
      "title" : "COMPI: Development of a 6-DOF compliant robot arm for human-robot cooperation",
      "author" : [ "Bargsten", "Vinzenz", "de Gea", "Jose" ],
      "venue" : "In Proceedings of the 8th International Workshop on Human-Friendly Robotics (HFR-2015)",
      "citeRegEx" : "Bargsten et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bargsten et al\\.",
      "year" : 2015
    }, {
      "title" : "Algorithms for Hyper-Parameter Optimization",
      "author" : [ "Bergstra", "James", "Bardenet", "Remi", "Bengio", "Yoshua", "Kegl", "Balazs" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Bergstra et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Bergstra et al\\.",
      "year" : 2011
    }, {
      "title" : "Pure Exploration in Multi-armed Bandits Problems",
      "author" : [ "Bubeck", "Sébastien", "Munos", "Rémi", "Stoltz", "Gilles" ],
      "venue" : null,
      "citeRegEx" : "Bubeck et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bubeck et al\\.",
      "year" : 2009
    }, {
      "title" : "A Limited-Memory Algorithm for Bound Constrained Optimization",
      "author" : [ "Byrd", "Richard H", "Lu", "Peihuang", "Nocedal", "Jorge", "Zhu", "Ciyou" ],
      "venue" : "SIAM Journal on Scientific Computing,",
      "citeRegEx" : "Byrd et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Byrd et al\\.",
      "year" : 1995
    }, {
      "title" : "Bayesian Optimization for Learning Gaits under Uncertainty",
      "author" : [ "Calandra", "Roberto", "Seyfarth", "Andr", "Peters", "Jan", "Deisenroth", "Marc P" ],
      "venue" : "Annals of Mathematics and Artificial Intelligence (AMAI),",
      "citeRegEx" : "Calandra et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Calandra et al\\.",
      "year" : 2015
    }, {
      "title" : "A Survey on Policy Search for Robotics",
      "author" : [ "Deisenroth", "Marc Peter", "Neumann", "Gerhard", "Peters", "Jan" ],
      "venue" : "Foundations and Trends in Robotics,",
      "citeRegEx" : "Deisenroth et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Deisenroth et al\\.",
      "year" : 2013
    }, {
      "title" : "The Knowledge-Gradient Policy for Correlated Normal Beliefs",
      "author" : [ "Frazier", "Peter", "Powell", "Warren", "Dayanik", "Savas" ],
      "venue" : "INFORMS Journal on Computing,",
      "citeRegEx" : "Frazier et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Frazier et al\\.",
      "year" : 2009
    }, {
      "title" : "Entropy Search for Information-Efficient",
      "author" : [ "Hennig", "Philipp", "Schuler", "Christian J" ],
      "venue" : "Global Optimization. JMLR,",
      "citeRegEx" : "Hennig et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hennig et al\\.",
      "year" : 2012
    }, {
      "title" : "Predictive Entropy Search for Efficient Global Optimization of Black-box Functions",
      "author" : [ "Hernández-Lobato", "José Miguel", "Hoffman", "Matthew W", "Ghahramani", "Zoubin" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Hernández.Lobato et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hernández.Lobato et al\\.",
      "year" : 2014
    }, {
      "title" : "Dynamical Movement Primitives: Learning Attractor Models for Motor Behaviors",
      "author" : [ "Ijspeert", "Auke Jan", "Nakanishi", "Jun", "Hoffmann", "Heiko", "Pastor", "Peter", "Schaal", "Stefan" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Ijspeert et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Ijspeert et al\\.",
      "year" : 2013
    }, {
      "title" : "Lipschitzian optimization without the Lipschitz constant",
      "author" : [ "D.R. Jones", "C.D. Perttunen", "B.E. Stuckman" ],
      "venue" : "Journal of Optimization Theory and Applications,",
      "citeRegEx" : "Jones et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Jones et al\\.",
      "year" : 1993
    }, {
      "title" : "A Taxonomy of Global Optimization Methods Based on Response Surfaces",
      "author" : [ "Jones", "Donald R" ],
      "venue" : "Journal of Global Optimization,",
      "citeRegEx" : "Jones and R.,? \\Q2001\\E",
      "shortCiteRegEx" : "Jones and R.",
      "year" : 2001
    }, {
      "title" : "Methods of Reducing Sample Size in Monte Carlo Computations",
      "author" : [ "H. Kahn", "A.W. Marshall" ],
      "venue" : "Journal of the Operations Research Society of America,",
      "citeRegEx" : "Kahn and Marshall,? \\Q1953\\E",
      "shortCiteRegEx" : "Kahn and Marshall",
      "year" : 1953
    }, {
      "title" : "Contextual Gaussian Process Bandit Optimization",
      "author" : [ "Krause", "Andreas", "Ong", "Cheng S" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Krause et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Krause et al\\.",
      "year" : 2011
    }, {
      "title" : "Combining active learning and reactive control for robot grasping",
      "author" : [ "Kroemer", "O. B", "R. Detry", "J. Piater", "J. Peters" ],
      "venue" : "Robot. Auton. Syst.,",
      "citeRegEx" : "Kroemer et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Kroemer et al\\.",
      "year" : 2010
    }, {
      "title" : "A New Method of Locating the Maximum Point of an Arbitrary Multipeak Curve in the Presence of Noise",
      "author" : [ "H.J. Kushner" ],
      "venue" : "Journal of Fluids Engineering,",
      "citeRegEx" : "Kushner,? \\Q1964\\E",
      "shortCiteRegEx" : "Kushner",
      "year" : 1964
    }, {
      "title" : "Automatic gait optimization with gaussian process regression",
      "author" : [ "Lizotte", "Daniel", "Wang", "Tao", "Bowling", "Michael", "Schuurmans", "Dale" ],
      "venue" : null,
      "citeRegEx" : "Lizotte et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Lizotte et al\\.",
      "year" : 2007
    }, {
      "title" : "Bayesian optimisation for intelligent environmental monitoring",
      "author" : [ "R. Marchant", "F. Ramos" ],
      "venue" : "In Intelligent Robots and Systems (IROS),",
      "citeRegEx" : "Marchant and Ramos,? \\Q2012\\E",
      "shortCiteRegEx" : "Marchant and Ramos",
      "year" : 2012
    }, {
      "title" : "Active Contextual Entropy Search",
      "author" : [ "Metzen", "Jan Hendrik" ],
      "venue" : "In Proceedings of NIPS Workshop on Bayesian Optimization,",
      "citeRegEx" : "Metzen and Hendrik.,? \\Q2015\\E",
      "shortCiteRegEx" : "Metzen and Hendrik.",
      "year" : 2015
    }, {
      "title" : "Bayesian Optimization for Contextual Policy Search",
      "author" : [ "Metzen", "Jan Hendrik", "Fabisch", "Alexander", "Hansen", "Jonas" ],
      "venue" : "In Proceedings of the Second Machine Learning in Planning and Control of Robot Motion Workshop.,",
      "citeRegEx" : "Metzen et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Metzen et al\\.",
      "year" : 2015
    }, {
      "title" : "The application of Bayesian methods for seeking the extremum",
      "author" : [ "J. Mockus", "V. Tiesis", "A. Zilinskas" ],
      "venue" : "Toward Global Optimization,",
      "citeRegEx" : "Mockus et al\\.,? \\Q1978\\E",
      "shortCiteRegEx" : "Mockus et al\\.",
      "year" : 1978
    }, {
      "title" : "Gaussian Processes for Machine Learning",
      "author" : [ "Rasmussen", "Carl", "Williams", "Christopher" ],
      "venue" : null,
      "citeRegEx" : "Rasmussen et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Rasmussen et al\\.",
      "year" : 2006
    }, {
      "title" : "Taking the Human Out of the Loop: A Review of Bayesian Optimization",
      "author" : [ "B. Shahriari", "K. Swersky", "Z. Wang", "R.P. Adams", "N. de Freitas" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "Shahriari et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Shahriari et al\\.",
      "year" : 2016
    }, {
      "title" : "An Entropy Search Portfolio for Bayesian Optimization",
      "author" : [ "Shahriari", "Bobak", "Wang", "Ziyu", "Hoffman", "Matthew W", "Bouchard-Côté", "Alexandre", "de Freitas", "Nando" ],
      "venue" : "In NIPS workshop on Bayesian optimization,",
      "citeRegEx" : "Shahriari et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Shahriari et al\\.",
      "year" : 2014
    }, {
      "title" : "Practical Bayesian Optimization of Machine Learning Algorithms",
      "author" : [ "Snoek", "Jasper", "Larochelle", "Hugo", "Adams", "Ryan P" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Snoek et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Snoek et al\\.",
      "year" : 2012
    }, {
      "title" : "Gaussian process optimization in the bandit setting: No regret and experimental design",
      "author" : [ "Srinivas", "Niranjan", "Krause", "Andreas", "Seeger", "Matthias" ],
      "venue" : "In Proceedings of the 27th International Conference on Machine Learning,",
      "citeRegEx" : "Srinivas et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Srinivas et al\\.",
      "year" : 2010
    }, {
      "title" : "MultiTask Bayesian Optimization",
      "author" : [ "Swersky", "Kevin", "Snoek", "Jasper", "Adams", "Ryan P" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Swersky et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Swersky et al\\.",
      "year" : 2013
    }, {
      "title" : "An informational approach to the global optimization of expensive-to-evaluate functions",
      "author" : [ "Villemonteix", "Julien", "Vazquez", "Emmanuel", "Walter", "Eric" ],
      "venue" : "Journal of Global Optimization,",
      "citeRegEx" : "Villemonteix et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Villemonteix et al\\.",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Instead, one often aims at finding a solution which has small simple regret (Bubeck et al., 2009) with regard to the true optimum, where simple regret denotes the difference of the true optimal function value and the function value of the “solution” selected",
      "startOffset" : 76,
      "endOffset" : 97
    }, {
      "referenceID" : 1,
      "context" : "BO has been applied to a diverse set of problems, ranging from hyperparameter optimization of machine learning models (Bergstra et al., 2011; Snoek et al., 2012) over robotics (Calandra et al.",
      "startOffset" : 118,
      "endOffset" : 161
    }, {
      "referenceID" : 24,
      "context" : "BO has been applied to a diverse set of problems, ranging from hyperparameter optimization of machine learning models (Bergstra et al., 2011; Snoek et al., 2012) over robotics (Calandra et al.",
      "startOffset" : 118,
      "endOffset" : 161
    }, {
      "referenceID" : 4,
      "context" : ", 2012) over robotics (Calandra et al., 2015; Lizotte et al., 2007; Kroemer et al., 2010; Marco et al., 2015) to sensor networks (Srinivas et al.",
      "startOffset" : 22,
      "endOffset" : 109
    }, {
      "referenceID" : 16,
      "context" : ", 2012) over robotics (Calandra et al., 2015; Lizotte et al., 2007; Kroemer et al., 2010; Marco et al., 2015) to sensor networks (Srinivas et al.",
      "startOffset" : 22,
      "endOffset" : 109
    }, {
      "referenceID" : 14,
      "context" : ", 2012) over robotics (Calandra et al., 2015; Lizotte et al., 2007; Kroemer et al., 2010; Marco et al., 2015) to sensor networks (Srinivas et al.",
      "startOffset" : 22,
      "endOffset" : 109
    }, {
      "referenceID" : 25,
      "context" : ", 2015) to sensor networks (Srinivas et al., 2010) and environmental monitoring (Marchant & Ramos, 2012).",
      "startOffset" : 27,
      "endOffset" : 50
    }, {
      "referenceID" : 1,
      "context" : "BO has been applied to a diverse set of problems, ranging from hyperparameter optimization of machine learning models (Bergstra et al., 2011; Snoek et al., 2012) over robotics (Calandra et al., 2015; Lizotte et al., 2007; Kroemer et al., 2010; Marco et al., 2015) to sensor networks (Srinivas et al., 2010) and environmental monitoring (Marchant & Ramos, 2012). For a more comprehensive overview of application areas, we refer to Shahriari et al. (2016).",
      "startOffset" : 119,
      "endOffset" : 454
    }, {
      "referenceID" : 27,
      "context" : "In the latter class, the group of entropy search-based approaches (Villemonteix et al., 2008; Hennig & Schuler, 2012; Hernández-Lobato et al., 2014), which aims at maximizing the information gain regarding the true optimum, has achieved state-of-the-art performance on a number of synthetic and real-world problems.",
      "startOffset" : 66,
      "endOffset" : 148
    }, {
      "referenceID" : 8,
      "context" : "In the latter class, the group of entropy search-based approaches (Villemonteix et al., 2008; Hennig & Schuler, 2012; Hernández-Lobato et al., 2014), which aims at maximizing the information gain regarding the true optimum, has achieved state-of-the-art performance on a number of synthetic and real-world problems.",
      "startOffset" : 66,
      "endOffset" : 148
    }, {
      "referenceID" : 22,
      "context" : "We refer to Shahriari et al. (2016) for a recent more extensive review of BO.",
      "startOffset" : 12,
      "endOffset" : 36
    }, {
      "referenceID" : 10,
      "context" : "A common strategy is to use DIRECT (Jones et al., 1993) to find the approximate global maximum, followed by L-BFGS (Byrd et al.",
      "startOffset" : 35,
      "endOffset" : 55
    }, {
      "referenceID" : 3,
      "context" : ", 1993) to find the approximate global maximum, followed by L-BFGS (Byrd et al., 1995) to refine it.",
      "startOffset" : 67,
      "endOffset" : 86
    }, {
      "referenceID" : 25,
      "context" : "Srinivas et al. (2010) proposed GP-UCB, which entails a specific schedule for κn that yields provable cumulative regret bounds.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 6,
      "context" : "A generalization of EI is the knowledge gradient factor (Frazier et al., 2009), which can better handle noisy observations, which impede the estimation of the incumbent.",
      "startOffset" : 56,
      "endOffset" : 78
    }, {
      "referenceID" : 8,
      "context" : "The third class of acquisition functions are informationbased policies, which entail Thompson sampling and entropy search (ES, Villemonteix et al., 2008; Hennig & Schuler, 2012; Hernández-Lobato et al., 2014).",
      "startOffset" : 122,
      "endOffset" : 208
    }, {
      "referenceID" : 27,
      "context" : "Computing aES directly is intractable for continuous spaces X ; prior work has discretized X and used either Monte Carlo sampling (Villemonteix et al., 2008) or expectation propagation (Hennig & Schuler, 2012).",
      "startOffset" : 130,
      "endOffset" : 157
    }, {
      "referenceID" : 5,
      "context" : "We refer to Deisenroth et al. (2013) for a recent overview of (contextual) policy search approaches in robotics.",
      "startOffset" : 12,
      "endOffset" : 37
    }, {
      "referenceID" : 26,
      "context" : "Multi-task BO has been demonstrated to learn efficiently about a set of discrete tasks concurrently (Krause & Ong, 2011), to allow transferring knowledge learned on cheaper tasks to more expensive tasks (Swersky et al., 2013), and to yield state-of-the-art performance on low-dimensional contextual policy search problems (Metzen et al.",
      "startOffset" : 203,
      "endOffset" : 225
    }, {
      "referenceID" : 19,
      "context" : ", 2013), and to yield state-of-the-art performance on low-dimensional contextual policy search problems (Metzen et al., 2015), in particular when combined with active learning (Metzen, 2015).",
      "startOffset" : 104,
      "endOffset" : 125
    }, {
      "referenceID" : 19,
      "context" : "We follow the formulation of BO-CPS (Metzen et al., 2015) and adapt it for MRS were required.",
      "startOffset" : 36,
      "endOffset" : 57
    }, {
      "referenceID" : 25,
      "context" : "In contrast, the acquisition functions GP-UCB (Srinivas et al., 2010) and ES (Metzen et al.",
      "startOffset" : 46,
      "endOffset" : 69
    }, {
      "referenceID" : 19,
      "context" : ", 2010) and ES (Metzen et al., 2015) have been extended straightforwardly and the same approach applies also to MRS.",
      "startOffset" : 15,
      "endOffset" : 36
    }, {
      "referenceID" : 8,
      "context" : "These results are roughly in correspondence with prior results (Hennig & Schuler, 2012; Hernández-Lobato et al., 2014) on the same task; note, however, that Hernández-Lobato et al.",
      "startOffset" : 63,
      "endOffset" : 118
    }, {
      "referenceID" : 8,
      "context" : "These results are roughly in correspondence with prior results (Hennig & Schuler, 2012; Hernández-Lobato et al., 2014) on the same task; note, however, that Hernández-Lobato et al. (2014) used a lower noise level and thus, absolute values are not comparable.",
      "startOffset" : 88,
      "endOffset" : 188
    }, {
      "referenceID" : 8,
      "context" : "Thus, predictive entropy search (Hernández-Lobato et al., 2014) would likely be affected by the same deficiency.",
      "startOffset" : 32,
      "endOffset" : 63
    }, {
      "referenceID" : 19,
      "context" : "This is because the execution time and first joint allow already adapting the throw to different contexts (Metzen et al., 2015); controlling more joints mostly adds additional search dimensions.",
      "startOffset" : 106,
      "endOffset" : 127
    }, {
      "referenceID" : 23,
      "context" : "Moreover, we consider MRS to be a valuable addition to the set of base strategies in a portfolio-based BO approach (Shahriari et al., 2014).",
      "startOffset" : 115,
      "endOffset" : 139
    } ],
    "year" : 2016,
    "abstractText" : "We propose minimum regret search (MRS), a novel acquisition function for Bayesian optimization. MRS bears similarities with information-theoretic approaches such as entropy search (ES). However, while ES aims in each query at maximizing the information gain with respect to the global maximum, MRS aims at minimizing the expected simple regret of its ultimate recommendation for the optimum. While empirically ES and MRS perform similar in most of the cases, MRS produces fewer outliers with high simple regret than ES. We provide empirical results both for a synthetic single-task optimization problem as well as for a simulated multi-task robotic control problem.",
    "creator" : "LaTeX with hyperref package"
  }
}