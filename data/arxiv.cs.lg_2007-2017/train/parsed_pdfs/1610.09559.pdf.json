{
  "name" : "1610.09559.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Rawlsian Fairness for Machine Learning",
    "authors" : [ "Matthew Joseph", "Michael Kearns", "Jamie Morgenstern", "Aaron Roth" ],
    "emails" : [ "aaroth@cis.upenn.edu.", "sethneel@wharton.upenn.edu." ],
    "sections" : [ {
      "heading" : null,
      "text" : "∗majos, mkearns, jamiemor, aaroth@cis.upenn.edu. Department of Computer and Information Sciences, University of Pennsylvania. †sethneel@wharton.upenn.edu. Department of Statistics, The Wharton School, University of Pennsylvania.\nar X\niv :1\n61 0.\n09 55\n9v 1\n[ cs\n.L G\n] 2\n9 O\nct 2\n01 6\nContents"
    }, {
      "heading" : "1 Introduction 3",
      "text" : ""
    }, {
      "heading" : "2 Definitions: Learning and Fairness 5",
      "text" : ""
    }, {
      "heading" : "3 Provably Fair and No-Regret Algorithms 6",
      "text" : "3.1 IntervalChaining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 3.2 RidgeFair . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9"
    }, {
      "heading" : "4 Experimental Results 12",
      "text" : "4.1 Empirical Cost of Fairness: Regret . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 4.2 Empirical Unfairness of Standard Algorithms . . . . . . . . . . . . . . . . . . . . . . 14"
    }, {
      "heading" : "5 Supplement 21",
      "text" : "5.1 Complete Description of Interval Chaining . . . . . . . . . . . . . . . . . . . . . . . . 21 5.2 Proof of the Fairness of Interval Chaining . . . . . . . . . . . . . . . . . . . . . . . . 21"
    }, {
      "heading" : "1 Introduction",
      "text" : "Automated techniques from statistics and machine learning are increasingly being used to make important decisions that directly affect the lives of individuals, including hiring [25], lending [8], policing [30], and criminal sentencing [6]. These high-stakes uses of machine learning have led to increasing concern in law and policy circles about the potential for (often opaque) machine learning techniques to be discriminatory or unfair [11, 5, 26]. These concerns are more than hypothetical. For example, a 2016 ProPublica study [4] of the COMPAS Recidivism Algorithm (used to inform criminal sentencing decisions by attempting to predict recidivism) found that the algorithm was significantly more likely to incorrectly label black defendants as recidivism risks compared to white defendants, despite similar overall rates of prediction accuracy between groups.1 Despite the recognized importance of this problem, very little is known about technical solutions to the problem of algorithmic unfairness,2 or the extent to which “fairness” is in conflict with the goals of learning, which typically emphasize predictive accuracy and computational efficiency.\nIn this paper, we propose a mathematically precise definition of fairness in machine learning for sequential decision making, and analyze its implications on the quality of learning, both theoretically and experimentally. As a running expository example, we shall consider a scenario in which a machine learning algorithm is making decisions about whom to grant loans to, based on the data provided about applicants. There are several extant definitions of fairness in machine learning, most of which formalize some notion of “group fairness”, a constraint that binds at the group level (see e.g. [9, 24, 18, 13, 14] for a sample of papers studying such definitions). One such notion is that of statistical parity, which requires that the fraction of individuals granted loans by the algorithm should be equal across “protected” groups (e.g. race, ethnicity, or gender).\nHowever, as discussed in Dwork et al. [12], these group-level definitions often fail at both fairness and accurate learning. If two groups actually have different proportions of individuals who are able to pay back their loans, then the accuracy of any learning algorithm will obviously suffer when constrained to predict an equal proportion of paybacks for the two groups. Furthermore, such definitions do not guarantee that a truly creditworthy individual from one group has an equal chance of being given a loan as a similarly creditworthy individual from another group. With these problems in mind, Dwork et al. [12] advocate that technical definitions of fairness should focus on individual fairness, rather than fairness at the group level.\nOur work explores the consequences of one such definition of individual fairness, and can be viewed as a mathematical formalization of Rawls’ notion of “fair equality of opportunity”3:\n“. . . assuming there is a distribution of natural assets, those who are at the same level of talent and ability, and have the same willingness to use them, should have the same prospects of success regardless of their initial place in the social system.” [27]\nThis Rawlsian view of fairness is consistent with much of the legal doctrine on non-discrimination. For instance, Title VII of the 1964 Civil Rights Act prohibits “disparate impact discrimination” —\n1The company that sells COMPAS has raised methodological concerns about the ProPublica study, but it is apparent that not only are some of the objections raised valid in the case of COMPAS, they are more generally unavoidable [19].\n2In a recent speech FTC Commissioner Julie Brill [17] observed, “. . . a lot remains unknown about how big datadriven decisions may or may not use factors that are proxies for race, sex, or other traits that U.S. laws generally prohibit from being used in a wide range of commercial decisions . . . What can be done to make sure these products and services–and the companies that use them treat consumers fairly and ethically?”\n3An interesting recent paper [15] gives a different formalization of the idea of “equality of opportunity” in a classification setting. The definition suggested by [15] is a “group fairness” constraint on the average behaviour of the algorithm over all individuals of the same quality. In contrast, in our work, we impose a stronger constraint that binds at the individual level.\nand therefore forbids not only explicit discrimination, in which choices are directly based on race or other protected attributes, but also discrimination resulting from policies that put members of a protected group at a disadvantage. Interpreting Title VII, Supreme Court Chief Justice Warren Burger writes in Griggs v. Duke Power Co. (1971):\n“Nothing in the Act precludes the use of testing or measuring procedures; obviously they are useful. What Congress has forbidden is giving these devices and mechanisms controlling force unless they are demonstrably a reasonable measure of job performance. Congress has not commanded that the less qualified be preferred over the better qualified simply because of minority origins. Far from disparaging job qualifications as such, Congress has made such qualifications the controlling factor, so that race, religion, nationality, and sex become irrelevant. What Congress has commanded is that any tests used must measure the person for the job and not the person in the abstract.” [1]\nWe formalize our Rawlsian definition of fairness in a sequential decision-making framework that is known as the contextual bandits setting in machine learning. Continuing with our loan example, on each day t we are given k loan applications, one each from a known set of k distinct groups.4 The application for the individual from group i comes in the form of a vector xt,i ∈ Rd, which summarizes their salient properties (e.g. xt,i might include attributes such income, credit history, employment history, educational background, etc.). We assume that for each group i there is some (unknown) function fi : Rd → R which maps these attribute vectors to the “quality” of an applicant. In our example, this could represent the probability of repaying a loan. Note that this framework explicitly allows that the functions fi mapping attributes to quality might be different for different groups— which in fact might be necessary to achieve fairness. For example, while having a college degree might be strongly predictive of creditworthiness in the overall group, it might be less so for individuals from a protected subgroup that generally had few financial resources growing up. For this group, attributes related to employment history might be more predictive. Insisting on one model for the entire group could lead to discriminating against highly creditworthy members of the protected group simply because they did not attend college.\nOur notion of fairness asks that at every step, a learning algorithm must never “favor” (that is, choose with higher probability) an applicant whose true quality (as determined by their attributes and their group’s mapping) is lower than that of another applicant. Note that if the mappings fi are known perfectly, it would always be fair to simply choose the candidate of highest quality at each step. Thus fairness and optimal decision-making are perfectly aligned (see Burger’s comments above on choosing the most qualified candidates). The challenge arises from the fact that the fi are initially unknown — they must be learned from data — but we ask that a learning algorithm be fair at every step, not just asymptotically. This requirement is especially important in an era in which large-scale machine learning systems are perpetually learning and refining their models, and thus should not be “forgiven” for unfair decisions made during their training process.\nOur primary interest is in designing learning algorithms that can (provably) converge to optimal decision-making, while being (provably) fair at every step. We seek to quantify the frictions or tradeoffs between fairness and fast convergence to optimality, and thus compare (both theoretically and experimentally) our fair learning algorithms with standard algorithms that are unconstrained by fairness.\nWe focus on the case in which the underlying quality of each individual is governed by the classic ordinary least squares model: each group i has an underlying weight βi on its observable attributes,\n4Note that both the assumption that only one loan is given and exactly one applicant from each group arrives on each day are for simplicity of exposition. Both can be relaxed in our results with mild degradation in performance guarantees.\nso that each individual x in the group has expected true quality βi · x. The decision maker (bank) then learns (approximate) βi by choosing individuals (granting loans) from the various groups and receiving noisy feedback about their true quality (observing repayment). Crucially, the decision maker does not observe the quality of the individuals not served; this models the fact that a bank does not observe whether an individual denied a loan would have paid it back. This setting is a generalization of the classical “multi-armed bandit” problem [28, 29, 21] called the contextual bandit problem [23, 10].\nOur main theoretical results are positive: we describe simple learning algorithms that are provably fair, and for which the cost of fairness is small in the sense that their rate of convergence to optimal decisions is only a factor k √ d worse than the best non-fair algorithms, where d is the context dimensionality and k number of distinct groups. We complement these theoretical results with an empirical evaluation and comparison of our fair algorithms with standard non-fair approaches. Our empirical results support the theory’s prediction that after a period of “fair exploration” based on the notion of chained confidence intervals, our algorithms become competitive with the non-fair algorithms in terms of predictive decision-making performance. In our technical companion paper [16], we generalize the framework presented here beyond linear functions, and relax several other simplifying assumptions made here (at the cost of less tight quantitative bounds)."
    }, {
      "heading" : "2 Definitions: Learning and Fairness",
      "text" : "In this section, we describe the formal model that we study. A problem instance is defined by a domain X from which the salient features of each individual are drawn, which we take to be X = Rd, and a set of k different groups, indexed by i ∈ {1, . . . , k}. Each group j is endowed with an unknown function fi : X → R mapping the features of an individual from group i to their true “quality”. In this paper, these functions have a linear form: that is, there is some unknown vector of coefficients βi ∈ R such that fi(x) = βi · x.\nIn rounds t = 1, . . . , T , an individual from each of the k groups arrives, and the salient features xt,i of each are observed by the learning algorithm A. In this paper, we assume that the features of the individual from group i that arrives at time t are drawn independently from a distribution Di, which may be different for each group: xt,i ∼ Di. The algorithm must then choose one of the individuals it (e.g. to grant a loan to), and observes a reward rt,it from the individual it chose (e.g. the payoff on the granted loan). The reward is stochastically generated and equal to the quality of an individual plus noise generated from a standard Gaussian distribution: rt,i = fi(xt,i) + et,i, where et,i ∼ N(0, 1). In other words, observed individual qualities follow a standard ordinary least squares model, which may have different parameters for each group. Crucially, the algorithm does not observe the reward for those individuals not chosen. This leads to the classical tension between exploration (choosing potentially suboptimal individuals in order to learn more about their groups) and exploitation (serving those individuals who seem to be best qualified given current knowledge about the groups).\nWe measure the performance of learning algorithms via regret, the difference between the expected reward of the optimal policy and the expected reward of the algorithm. The (omniscient) optimal policy chooses the individual with highest expected reward every day, and so if pt denotes the distribution over choices at round t for an algorithm A, we define the regret of A by:\nRegret(x1, . . . , xT ) = ∑ t max i (fi(xt,i))− ∑ t Eît∼pt [ fît(xt,̂it) ] .\nWe say that A satisfies regret bound R(T ) if\nmax D1,...,Dk Ext,1∼D1,...,xt,k∼Dk [Regret(x1, . . . , xT )] ≤ R(T ).\nLet the history ht ∈ ( X k × [k]× R )t−1 be a record of the t − 1 rounds experienced by A up until round t. Thus ht encodes, for each of the t − 1 rounds, 3 things: the attribute vectors of all k individuals observed, the index of the individual chosen, and the chosen reward observed. The history is a sufficient statistic to determine the distribution of the algorithm’s choices at the next round. We write pt,j|ht to denote the probability that A chooses individual j after observing features xt,i, given ht. For notational simplicity, we will often drop the superscript t on the history when referring to the distribution over individuals: pt,i|h = pt,i|ht .\nWe now define what it means for an algorithm to be fair. Informally, this will mean that with high probability (with respect to the randomness inherent in noisy observations, and any probabilistic decisions made by the algorithm), at every round, and for any set of individuals the algorithm is presented at that round, the algorithm will choose a higher quality individual with probability at least that with which it chooses a lower quality individual. More formally:\nDefinition 1 (Fairness). Algorithm A is fair if, for any input δ > 0, with probability at least 1− δ over the realization of the history h, for all rounds t ∈ [T ] and all pairs of choices i, j ∈ [k], if fi(xt,i) > fj(xt,j) then pt,i|h > pt,j|h.\nThis definition requires that, with high probability, the algorithm does not make a discriminatory decision at any round t, regardless of the specific individuals being compared at each round. A weaker condition would require that the fraction of discriminatory rounds is small. Under this alternative definition, it is permissible for an algorithm to always be discriminatory under certain infrequent but specific conditions — such as always preferring less qualified white male lawyers to more qualified female scientists when such comparisons arise — so long as it is not discriminatory on a large fraction of rounds. In contrast, our (stronger) definition requires fair treatment of the specific individuals who arrive at every round, thus precluding this type of rare-subgroup discrimination."
    }, {
      "heading" : "3 Provably Fair and No-Regret Algorithms",
      "text" : "One immediate consequence of our fairness definition is that the optimal policy which deterministically plays the highest quality individual at each round satisfies our fairness constraint. This suggests that the goal of fairness is not intrinsically at odds with the goal of accuracy (i.e. regret minimization). When designing fair algorithms, then, a natural starting place is the class of well-studied bandit algorithms that do not obey a fairness condition, but instead purely aim for accuracy [29]. A defining characteristic of this class of algorithms is that they balance exploration (choosing individuals from groups about which the algorithm has high uncertainty to learn more about that group) and exploitation (choosing individuals that have the highest estimated quality based on the algorithm’s observations so far). As the algorithm gradually learns the true parameters, the exploration steps become increasingly infrequent, until the algorithm is essentially playing optimally. Optimal learning algorithms thus delicately balance exploration and exploitation.\nHowever, both of these steps can result in unfair play: exploration steps can be unfair because they implicitly favor uncertainty rather than quality, and exploitation steps can be unfair if the algorithm misidentifies the best individual. In contrast, a fair algorithm must carry out this exploration and exploitation without violating fairness. A key observation here is that uniformly random\nselection is fair, as all individuals are selected with the same probability. Since, as mentioned above, optimal selection is also fair, this suggests a general outline for a fair algorithm: begin by exploring uniformly at random and gradually transition to optimal exploitation, while preserving fairness throughout. This raises a central challenge in the design of fair no-regret algorithms: in order to both satisfy fairness and guarantee good performance, an algorithm must quickly transition from uniformly random to near-optimal play. Much of the technical difficulty in designing fair algorithms stems from managing this transition such that the algorithm does not violate the fairness constraint at any step along the way.\nWe now present two closely related algorithms, IntervalChaining and RidgeFair, both with accompanying regret bounds. We present IntervalChaining first for its simplicity, and will use IntervalChaining in our experiments. We then present RidgeFair, which obtains a superior theoretical regret guarantee even in the more general setting in which contexts are adversarially chosen. IntervalChaining achieves regret\nR(T ) = Õ (√ k3d·T 2/3 + √ d3k3 ) in many settings but depends on a distribution dependent constant that can in principle be arbitrarily large. We postpone the proof to the supplement, since RidgeFair achieves a stronger regret bound with fewer assumptions. For RidgeFair, a sharper technical analysis yields a regret bound of\nR(T ) = Õ(d √ k3T )\nIn our companion paper [16] we prove a lower bound of R(T ) = Ω(k3) for fair algorithms in the multi-armed bandit setting (i.e. in the special case in which contexts are unchanging between rounds). A lower bound of R(T ) = Ω( √ T ) is also known for this setting, even absent a fairness constraint [7]. Since that simpler setting is a special case of the contextual bandit framework, it follows that our dependence on k and T is optimal up to logarithmic factors. In [3] an R(T ) = Ω( √ Td) lower bound is proven for linear contextual bandit algorithms absent a fairness constraint, along with an algorithm enjoying a matching Õ( √ Td) upper bound. Taken together, these results show that the regret bound achieved by RidgeFair has optimal dependence (for fair algorithms) on T and k, with the possibility of at most a √ d improvement in d. The cost of fairness, or the degradation of the regret guarantee due to the fairness constraint, is thus between Õ(k3) and Õ(k3 √ d). Finally, we note that this is a significant improvement over the R(T ) = Õ(T 4/5k6/5d3/5) bound for the linear contextual bandit setting given in our companion paper [16], which is derived as a corollary of a general black box reduction rather than with a specialized analysis."
    }, {
      "heading" : "3.1 IntervalChaining",
      "text" : "We now present IntervalChaining, a provably fair algorithm with strong performance guarantees. In IntervalChaining, round t is chosen independently with probability ηt to be an exploration round, in which the algorithm chooses uniformly at random among all individuals to learn better estimates of the k linear models. All other rounds are exploitation rounds, in which the algorithm uses those estimates to choose high-quality individuals, to the extent possible subject to the fairness constraint (the algorithm also improves its model estimates from the data gathered in these rounds). In exploitation rounds, IntervalChaining uses OLS estimators β̂t,i for each group i in each round t to compute estimated qualities ŷt,i = β̂t,i ·xt,i for each individual xt,i. The algorithm also computes confidence interval widths wt,i around these estimates, such that the true qualities lie within these confidence intervals: yt,i ∈ [ŷt,i−wt,i, ŷt,i +wt,i], with probability 1− δ, for all i and t. Using these confidence intervals, the algorithm finds the individual with highest upper confidence bound it∗.\nStandard no-regret algorithms would simply choose it∗, but this will not in general be fair, since the individual with highest upper confidence bound is not necessarily the highest quality individual. Instead, our algorithm chooses uniformly at random amongst St, the set of individuals chained to it∗, defined as follows. For any individual xt,i whose confidence interval overlaps with it∗’s ([` t i, u t i]∩ [`tit∗ , u t it∗\n] 6= ∅), the algorithm does not have enough data to know with confidence which of xt,i or xt,it∗ has higher quality. The algorithm therefore behaves conservatively with respect to these estimates to guarantee fairness with respect to any qualities consistent with these confidence intervals: it will treat any pair of individuals whose confidence intervals overlap equally (namely, will choose them with equal probability). This motivates the following definition of chaining, which is the transitive closure of the “overlapping” relation. If the confidence intervals around i and j overlap, then we will say that i and j are chained. Further, if i and j are chained, and j and j′ are chained, then i and j′ are chained (even if the confidence intervals around i and j′ do not overlap). See Figure 1 for a visual representation of chaining, and Figure 2 for the pseudo-code for IntervalChaining.\nIntervalChaining, is fair and satisfies the following performance guarantee, when imple-\nmented with ηt, wt,i as defined in the formal statement and proof in the supplement.\nTheorem 1. IntervalChaining is fair, and has regret\nR(T ) = O √k3d · ln 2kTδ ` T 2/3 + ( dkL ` ( ln2 2kT δ + ln d ))3/2 where ` = mini Ext,i∼Di [ λmin (∑ t∈[d] x T t,ixt,i )] (the minimum over i of the expectation of the mini-\nmum eigenvalue of the random design matrix of d observations from group i) and L ≥ maxt λmax ( xTt,ixt,i ) , assuming ||xt,i||, ||βi|| = O (1).\nWe note briefly that, in many interesting settings, the regret bound given in Theorem 1 reduces to Õ (√ k3d·T 2/3 + √ d3k3 ) .5 If, for example, the distribution over feature vectors has ||xt,i|| = 1, E [xt,i,j ] = 0, and xt,i,j and xt,i,j are independent for all i, t and j, j′, the regret bound has this form. We now present a high-level outline of the techniques used to bound the regret of IntervalChaining (and RidgeFair).\nProof Sketch: We define the confidence interval widths wt,i such that, with probability 1 − δ, for all rounds t and all groups i, yt,i ∈ [ŷt,i − wt,i, ŷt,i + wt,i]. So, for any pair of groups i, j, if yt,i > yt,j , then ŷt,i + wt,i ≥ ŷt,j − wt,j : if j is chained to it∗, then so will be i. Since the algorithm chooses uniformly from the set of individuals chained to the individual with highest upper confidence interval, for every pair of individuals, either they are selected with identical probabilities, or else one of them is selected with probability 0 (but has lower quality unless the confidence intervals have failed).\nThe regret guarantee follows by calculating, for each group, a lower bound on the number of rounds for which the algorithm has selected a member of that group (and hence has obtained a data point with which to update its OLS estimator). Given that lower bound, we upper bound the width of the confidence interval around individuals from that group. Since chains can have length at most k, any individual chained to the individual with highest upper confidence bound has quality that differs from the quality of the best individual by a term that is at most k times this upper bound on the confidence width. Summing up over all rounds yields the desired result."
    }, {
      "heading" : "3.2 RidgeFair",
      "text" : "In this section we define RidgeFair, observe that it is fair by the same argument we saw for IntervalChaining, and prove its regret bound. RidgeFair estimates β using confidence regions centered at the `2-regularized least squares estimator, and then translates these regions into confidence intervals for each group payoff at each time step. The algorithm then follows a similar chaining procedure, and plays a group uniformly at random from the set of arms chained to the top arm at every time t. The regularized least squares estimator given a design matrix X, response vector y, and regularization parameter λ ≥ 1, is of the form β̂ = (XTX + λI)−1XT y. Using this ridge estimator decreases the variance in each groups’s estimated quality, but as a result the estimated payoff is no longer an unbiased estimate for the true payoff mean. Consequently valid confidence intervals are harder to derive than for the simple OLS estimator, and rely on martingale techniques borrowed from [2], which derives similar bounds absent a fairness constraint.\n5Õ hides constants and logarithmic factors.\nRidgeFair thus possesses a number of theoretical advantages over IntervalChaining. First, its narrower confidence intervals allow us to derive our tightest regret bound for a fair algorithm, Õ(d √ k3T ), under far less restrictive assumptions. We no longer need to assume normally distributed noise, instead requiring only that noise is mean zero and R sub-Gaussian. Moreover, we can assume contexts are selected adversarially from a bounded set, rather than drawn i.i.d. from a distribution, as we no longer require the Matrix Chernoff bound that powers the regret bounds for IntervalChaining. For the analysis below we assume for simplicity that the noise is R subGaussian with R = 1, and that all contexts lie in the unit ball. This is without loss of generality up to scaling — any scaling parameters will appear in the final bound.\nWe now formally state that RidgeFair is both δ-fair and has a sublinear regret guarantee.\nTheorem 2. Assume that all for all contexts ||xt,i|| ≤ 1, that ||βi|| ≤ 1 for all i, λ = 1, and the noise is R sub-Gaussian with R = 1. Then RidgeFair is fair, and has regret Õ(d √ k3T ln 1δ ) for all T .\nRemark. Note that the Õ hides logarithmic factors in 1/δ, and T . The assumptions of at most unit norm for contexts and parameters are only for convenience, and the argument works equally well for both lying in a bounded set.\nBefore proceeding with the proof, we state a theorem we will use in its proof.\nTheorem 3 (From [2]). For any δ > 0, with probability at least 1− δ,\n∀t ≥ 0, ||XTi ηi||V̄ −1it ≤ R √ d log( 1 + t/λ δ )\nWe now prove our tightest regret bound for any fair algorithm.\nProof of Theorem 2. The fact that RidgeFair is fair follows from an identical argument as the one for IntervalChaining, and the fact that the confidence intervals derived hold over all t with\nprobability 1 − δ. Toward proving the regret bound, we prove a slightly stronger result. In our definition of regret in the preliminaries, we measure the expected regret: the definition takes an expectation over the randomization of the algorithm. In fact, we can show that our regret bound holds with high probability (1− δ); not only in expectation. We now define\nR(T ) = Regret(x1, . . . , xT ) = ∑ t max i (fi(xt,i))− ∑ t fît(xt,̂it).\nTaking δ = O( 1√ T\n), and noting that R(T ) ≤ T , we see that an O( √ T ) bound with probability\n1−δ on R(T ), also implies an O( √ T ) bound on the expected regret. We now proceed with bounding R(T ) with high probability. We adopt the notation in Abbasi-yadkori et al. [2]: let V̄it = X T i Xi + λI, where Xi is the design matrix at time t corresponding to group i, λ ≥ 1. Let β̂it = V̄ −1it XTi Yi be the regularized least squares estimator for group i at time t. Consider the feature vector xt,i at time t. For a d-dimensional vector z and a d× d p.d. matrix A, let 〈z, z〉A denote ztAz. Letting ηi be the noise sequence corresponding to group i, we have β̂it = V̄ −1 it X T i (Xβi + ηi). Then some matrix algebra from [2] shows: xt,i · (β̂it − βi) = xTt,iV̄ −1it X T i ηi − λxTt,iV̄ −1it βi,\nwhich using the above notation gives\nxt,i · (β̂it − βi) = 〈xt,i, XTi ηi〉V̄ −1it − λ〈xt,i, βi〉V̄ −1it\nApplying Cauchy-Schwarz,\n|xt,i · (β̂i − βi)| ≤ ||xt,i||V̄ −1it (||X T i ηi||V̄ −1it +\n√ λ)\nwhich follows from the fact that ||βi||V̄ −1it ≤ 1√ λ (a basic corollary of the Raleigh quotient, and the fact that by assumption ||βi|| ≤ 1). By Theorem 3, and combining the inequalities we get that over all rounds t ≥ 0 with probability 1− δ:\nxt,i · (β̂i − βi)| ≤ ||xt,i||V̄ −1it (R √ d log( 1 + t/λ δ ) + √ λ) (1)\nFinally, in the proof of Lemma 11 in [2] it is noted that:\nT∑ t=1 ||xt,i||2V̄ −1it ≤ 2d log(1 + T dλ ). (2)\nWe now have all of the tools in hand to analyze the chaining algorithm. Let rji be the number of times group i is active (and thus picked uniformly at random among) between pulls j, j + 1. Conditioning on the event that all payoff means lie in their respective confidence intervals (an event with probability mass 1 − δ by construction) the total regret incurred by RidgeFair up to time T , is upper bounded by the sum of the widths of all the confidence intervals around the means for each group i, over all time steps that group i is active. Let wji be the width of the confidence interval around group i after j pulls. Let Ri(T ) denote the sum of the widths of the confidence interval around the payoff for group i, summed over all times t ≤ T where group i is active, and let\nni(T ) be the number of times group i is pulled up to time T . Note that the regret R(T ) ≤ ∑\niRi(T ) with probability 1− δ. Then\nRi(T ) ≤ nTi∑ j=1 wji · r j i\nand hence,\nR(T ) ≤ k∑ i=1 nTi∑ j=1 wji · r j i .\nNow when group i is active, it has at least a 1/k chance of being pulled uniformly at random. Thus with high probability, rji = Õ(k). More formally, P(r j i ≥ h) = (1 − 1/k)h ≤ e−h/k. Letting h = k log(j2k/δ) gives P(rji ≥ h) ≤ δ/kj2, thus we can assume over all time steps t ≤ T that rji ≤ k log(T 2k/δ). Under the constraint ∑ i n T i = T , R(T ) is maximized at n T i = T/k, which gives\nR(T ) ≤ k2 log(T 2k/δ) T/k∑ t=1 wti\n≤ 2k2 log(T 2k/δ) T/k∑ t=1 ||xt||V̄ −1it (R √ d log( 1 + t/λ δ ) + √ λ)\n≤ 2k2 log(T 2k/δ) √√√√T/k∑ t=1 ||xt||2V̄ −1it\n· √√√√T/k∑ t=1 R2d log( 1 + t/λ δ ) + Tλ/k\nwhere the last equality follows from Cauchy-Schwarz. Combining this bound with that from Equation 2 gives that with probability 1− δ,\nR(T ) ≤ 2k2 log(T 2k/δ) √ 2d log(1 + T dλ ) · √ T k (R2d log( 1 + T/kλ δ ) + λ)\nor R(T ) = Õ(d √ Tk3) for λ = 1, as desired."
    }, {
      "heading" : "4 Experimental Results",
      "text" : "In this section we provide an empirical evaluation of IntervalChaining. Recalling that the confidence intervals for RidgeFair hold in greater generality, when we specialize to the case of normal noise they are wider than the confidence intervals of IntervalChaining, which are derived using normality explicitly. Since our experiments feature normal noise, it follows that IntervalChaining reliably outperforms RidgeFair empirically in spite of RidgeFair’s superior theoretical regret bound. We therefore focus exclusively on IntervalChaining in simulations.\nBefore turning to a more systematic set of experiments, we first give an illustration of the effects and variability of chaining over the course of a single run of IntervalChaining. In Figure 4, we\nvisualize the chaining behavior of IntervalChaining over a run of T = 106 rounds on a log scale of the x axis, which measures t. There are k = 5 groups, each represented by a different color, and the upper and lower confidence bounds for each of the groups at each t are plotted using colored dots connected by lines. The number above each round’s confidence intervals indicates the number of groups chained to the top group, while the gray bars show the union of the intervals chained to the top interval. In early rounds we see that we often have full chaining, with the gray bars encompassing all the colored dots; in later rounds the gray bars generally include only an upper subset of the colored intervals. Both the chaining number and the width of the grey bars shrink on average, but not montonically due to the variability of contexts in each round."
    }, {
      "heading" : "4.1 Empirical Cost of Fairness: Regret",
      "text" : "We now present experimental evaluations of the regret of IntervalChaining compared to TopInterval6 for various settings of d, k, and T . TopInterval uses the same OLS estimated qualities and confidence intervals as IntervalChaining, but chooses only the interval with highest upper confidence bound (rather than needing to play amongst all individuals chained to the top individual). We performed three kinds of experiments:\n(a) Varying T : fixing d = k = 2, we measured the average regret of IntervalChaining as a function of increasing T .\n(b) Varying k: fixing d = 2 and T = 1000, we measured the average regret of IntervalChaining as a function of increasing k.\n(c) Varying d: fixing k = 2 and T = 1000, we measured the average regret of IntervalChaining as a function of increasing d.\nThe resulting plots are collected in Figure 5. Each plot contains four lines, one for each value of a parameter c (whose effects are discussed below) controlling the distribution of β ∼ U [0, c]d for each group. In all cases the values presented were averaged over 1000 trials, with contexts drawn uniformly at random from [0, 1]d and standard Gaussian noise. In each case we plot the difference in regret between TopInterval and IntervalChaining. We note that our implementation of IntervalChaining does not include the random sampling component given in its formal presentation in the previous section, as we have found that this improves empirical performance over the\n6TopInterval is a variant of the standard linear bandits algorithm “LinUCB” [22], simplified to take advantages of the assumptions in our model.\ntheoretical regret guarantees presented above. However, we emphasize that this modification does not alter the fairness guarantee of IntervalChaining.\n(a) Varying T : As T increases, the performance of IntervalChaining relative to TopInterval improves with time. This is consistent with our theory showing IntervalChaining has sublinear regret. As c varies, another trend also emerges: larger c increases both the range of possible values for each β and the regret incurred by playing randomly. This is reflected, for example, by the early spike in regret when c = 10. Since IntervalChaining explores randomly for more rounds than TopInterval, the prefix for which IntervalChaining continues to explore extends beyond the point where TopInterval begins to exploit. For c = 10, this results in a short period of high regret (the spike) before IntervalChaining begins exploiting. Conversely, once IntervalChaining begins exploiting, a larger c value corresponds to a higher signal-to-noise ratio for uncovering the βs (as standard Gaussian noise is relatively smaller compared to larger normed β). The βs also become more separated as c increases. For these reasons, c = 10 produces the lowest asymptotic regret and c = 1 produces the highest: when c = 10, distinguishing groups is easy, and chaining is not an issue for IntervalChaining; the opposite is true for c = 1.\n(b) Varying k: As k increases but T = 1000, d = 2 are held constant, we observe a general linear increase in regret with k, improving upon the O(k3/2) dependence expected from Theorem 1. We also see a general pattern in k’s effect on regret fixing c. c = 5 is a particularly instructive case. For small k, slightly increasing k does not increase regret substantially: random additional βs are still well-separated and don’t cause significant chaining (or regret) for IntervalChaining. For similar reasons, for small k, regret is low and nearly flat. However, as the number of groups increases, the βs populate the [0, c]d hypercube more densely, and IntervalChaining chains become more frequent and longer, leading to the apparent linear increase of regret in k for the middle range of k. Finally, for large values of k the cube is densely populated with groups and chaining forces IntervalChaining to play essentially at random, leading to the high regret plateau on the plot’s right side, and a general S-shape for the curve as a whole. Varying c then changes the size and scale of the resulting S curve, but not its underlying shape: a smaller cube fills (and therefore chains) more quickly with increasing k, but the smaller cube also prevents the best group from being much better than a random group (constraining regret from growing too large). The plateaus of regret therefore begin at values of k which increase with c.\n(c) Varying d: As d increases, we observe a general linear increase in regret, in accordance with the O(d) dependence from Theorem 1. Once again, varying c interacts with increases in d in different ways. For small c such as c = 1, the lower signal-to-noise ratio makes estimation difficult; this is compounded by the increase in d, resulting in the high regret shown in the plot. In contrast, for large c such as c = 10 the effect of noise is relatively small, and the small number of groups makes distinguishing between groups relatively easy, even for large dimension."
    }, {
      "heading" : "4.2 Empirical Unfairness of Standard Algorithms",
      "text" : "Standard algorithms like TopInterval lack IntervalChaining’s formal fairness guarantee, but it is not obvious that these algorithms actually exhibit discriminatory behaviour. Indeed, one might suspect that the harm that is caused by those rounds in which TopInterval selects a sub-optimal\nindividual would be spread uniformly across all groups and all members of each group, since there is no intentional discrimination built into the algorithm. In this section, we demonstrate empirically that this is not the case: in particular, the costs of the incorrect decisions made by TopInterval can accrue disproportionately to certain groups and to structured subgroups within a given group, which we term structural unfairness.\nConcepts of structural unfairness. To talk about structural unfairness, we introduce some notation. We will be interested in studying how the cost of those rounds at which the algorithm does not select the most qualified individual accrues to different subsets of individuals. Fix a group i, and consider two disjoint subgroups P1, P2 ⊂ X within that group (if the groups represent e.g. income brackets, the subgroups might represent racial background). For any round at which a sub-optimal decision is made, a given individual may have either benefited (if they were chosen despite not being the best individual), or have been victimized (if they were not chosen, despite being the best individual). Let T vi be the set of rounds in which an individual from Pi was victimized, and let T b i be the set rounds in which an individual from Pi benefited. We can now define the discrimination\nindex with respect to Pi as di = E [ |T vi |\n|T vi |+|T bi |\n] – the fraction of times that individuals from P1\nwere victimized, as a proportion of the times they were involved in sub-optimal decisions by the algorithm.\nAn algorithm A then exhibits structural discrimination with respect to P2 relative to P1 if d1 d2. Intuitively, the discrimination index quantifies how the burden of suboptimal decisions is distributed: a subgroup with high discrimination index is victimized in a disproportionately high number of suboptimal decisions, while a subgroup with low discrimination index is benefited in a disproportionately high number of suboptimal decisions.\nEmpirical verification of structural unfairness. We now discuss an illustrative 2-dimensional instance on which TopInterval exhibits structural discrimination. Informally, there will be two groups. Group 1 contains two structured subgroups, while group 2 is homogeneous. In group 1’s majority subgroup, the two features describing an individual are perfectly correlated. For individuals in group 1’s minority subgroup, however, the two features are uncorrelated. Moreover, all individuals in group 1 have quality entirely determined by feature 1 – a fact only apparent from observations of individuals in the minority subgroup. The algorithm will therefore have increased uncertainty about the quality of individuals from the minority subgroup. As we will show, because TopInterval implicitly favors uncertainty, on this instance TopInterval exhibits structural discrimination in favor of the minority subgroup.\nFormally, we consider an instance in which d = k = 2, and X = [−1, 1]2. The qualities of\ngroups 1 and 2 are determined by the unknown coefficient vectors β1 = (1, 0), and β2 = (.5, .5) respectively. In group 1, we encode the majority subgroup by drawing 90% of individuals from the diagonal (x = y) and draw the remaining minority 10% of individuals uniformly off the diagonal (x 6= y). In group 2 contexts are drawn uniformly in each coordinate. Note that the quality of an individual from both the majority and minority subgroup of group 1 are identically distributed uniformly random in x, so there is no “reason” for discrimination.\nWe ran 106 simulations of TopInterval and IntervalChaining, for T = 25 rounds. We found that TopInterval exhibited strong structural discrimination in favor of the minority subgroup among group 1. Put simply, when a sub-optimal decision is made, individuals from group 1’s majority subgroup are nearly 7 times more likely to be victimized than individuals from group 1’s minority subgroup. See Figure 6 which plots the average discrimination index — the sharp ridge along y = x represents a spike in discrimination index and corresponds to structural discrimination against group 1’s majority subgroup. Finally, 59.6% of the sub-optimal decisions victimized group 1, and 40.4% of the sub-optimal decisions victimized group 2. Thus TopInterval is both structurally unfair in its unequal treatment of subgroups in group 1 and unfair in its unequal treatment of groups 1 and 2.\nIn Figure 6 we also show the discrimination index of IntervalChaining. A potential critique of our definition of fairness is that it binds between all individuals in a single round, but does not bind between individuals from the same group across time. As a result, there is no mathematical guarantee that algorithms that satisfy our fairness definition do not exhibit structural discrimination. For example, our fairness definition would in principle allow for the following kind of discrimination against a structured subgroup: when an individual arrives from the majority subgroup and is the most qualified, the algorithm gives them a loan deterministically, but when an individual arrives from the minority sub-group and is the most qualified, the algorithm chooses uniformly at random. Note that this discriminatory strategy would not have a diminishing regret guarantee as our algorithms do – but nothing in our definition itself rules out this kind of behavior early in the learning process. However, figure 6 shows empirically that IntervalChaining does not exhibit structural discrimination on the same instance on which TopInterval does. This is\nan interesting empirical finding that is not explained by our theory, and one which we feel merits further investigation."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We thank Glen Weyl for pointing out the connection between our fairness definition and the ideas of John Rawls, and Adel Boyarsky for help researching the legal interpretations of disparate impact discrimination."
    }, {
      "heading" : "5 Supplement",
      "text" : ""
    }, {
      "heading" : "5.1 Complete Description of Interval Chaining",
      "text" : ""
    }, {
      "heading" : "5.2 Proof of the Fairness of Interval Chaining",
      "text" : "Proof of fairness in Theorem 1. Fix some t ∈ {1, . . . , T}], i ∈ {1, . . . , k}. Let pt,i represent the probability the algorithm places on action i in round t in this particular run of the algorithm. If t is a round in which uniformly random play occurs, then pt,i = pt,i′ for all i, i\n′ ∈ [k], and so the algorithm satisfies the fairness condition in any such round.\nNow, suppose t is a round in which the OLS estimates are used. By standard properties of OLS estimators (see, e.g. Kuan [20, Theorem 3.7]), β̂t,i ∼ N (βt,i, σ2(XTt,iXt,i)−1). Then, for any fixed xt,i, we have that\nβ̂t,i · xt,i ∼ N (βt,i · xt,i, xTt,iσ2(XTt,iXt,i)−1xt,i) (3)\nby the properties of normally distributed random variables. By the definition of the quantile function and the fact that the normal distribution is symmetric, with probability at least 1− δTk ,\n|β̂t,i · xt,i − βt,i · xt,i| ≤ wt,i. (4)\nThat is, βt,i · xt,i /∈ [`ti, uti] with probability at most δkT . Thus, the probability that this fails to hold for any i at any time t is at most kT · δkT = δ. Thus, we condition on this event for the\nremainder of the argument and show that for all arms i, j and all rounds t, pt,i ≥ pt,j whenever βt,i · xt,i ≥ βt,j · xt,j with probability 1 when it holds.\nFor all fixed pair of actions i, j and a fixed round t, if βt,i · xt,i ≥ βt,j · xt,j , then as uti ≥ βt,i · xt,i and `tj ≤ βt,j · xt,j , we have that\nuti ≥ βt,i · xt,i ≥ βt,j · xt,j ≥ `tj\nand so either (a) `ti ≥ utj , in which case the two intervals don’t overlap, or (b) `ti ≤ utj . In either case, if j is chained to it∗, then so will be i. So, either both will be played with probability 1 |St| , or only i will be played with that probability and j with probability 0, or both will be played with probability 0. In all such cases, pt,i ≥ pt,j .\nProof of Regret in Theorem 1. First, recall L ≥ maxt λmax ( xTt,ixt,i ) . Since maxt λmax ( xTt,ixt,i ) ≤ maxt ||xt,i||2, and we assume xt,i bounded, we can always choose finite L. Also for each i let\nλmind,i = Ext,i∼Di λmin ∑ t∈[d] xTt,ixt,i  be the expected minimum eigenvector of a random design matrix made up of d observations from group i.\nThe entire regret of IntervalChaining can be broken into three components: the regret in exploration rounds, the regret of exploitation rounds before the estimators have enough samples to have concentrated, and the regret of exploitation rounds once the estimators are sufficiently accurate. We will bound the regret of IntervalChaining for each of these three phases, e.g., for any T1 ≤ T :\nRegret(IntervalChaining, T ) = ∑\nt:t is an exploit round\nRegret(t)\n+ ∑\nt:t is an exploit round and t < T1\nRegret(t)\n+ ∑\nt:t is an exploit round and t ≥ T1\nRegret(t)\n≤ ∑ t≤T 1 t1/3 + T1 + ∑ t:t is an exploit round and t ≥ T1 Regret(t)\nLet T1 be defined as follows:\nT1 = Θ ( min i ( dkL\nλmind,i\n( ln2 2\nδ′ + ln d\n))3/2) . (5)\nInformally, T1 is the number of rounds after which our estimators will be computed on sufficiently many samples such that the estimates are well-concentrated.\nLet pt = 1 t1/3\ndenote the probability that round t is an exploration round. Then, for any t, we have that ∑\nt′<t\npt′ = Θ(t 2/3) (6)\nand for any t ≥ T1, we have ∑ t′<t pt′ = Ω(min i ( dk2L λmind,i (ln2 2 δ′ + ln d))) (7)\nLet δ denote the probability that some estimator we use falls outside its 1−δ′ probability bound; we will choose δ = O ( 1\nT 1/3\n) . Then, we have\nR(T ) ≤ T∑ t=1 pt · 1 + T1 + T∑\nt=T1\nregret(t, A) + δT\n≤ O T 2/3 + T1 + T∑ t>T1,\nt is an exploit round\nregret(t, A)  . (8)\nWe now upper-bound the regret in the exploitation rounds after round T1, namely the terms above corresponding to regret(t, A). We will upper-bound wt,i, the width of the confidence intervals, which will satisfy βi · xt,i ∈ [ŷt,i − wt,i, ŷt,i + wt,i] for all groups i and rounds t, with probability 1− δ. We will then condition on the event that over all rounds and all arms this holds. Recall that wt,i = QN (0,xt,i(XTt,iXt,i)−1xTt,i) ( δ2kT ). Trivially, this confidence bound fails to hold for a fixed i and t with probability at most δTk : thus, for any times and any groups, some confidence intervals will fail with probability at most δ.\nNow, we condition on the confidence intervals holding for all t, i. Let ît be the optimal arm in round t, and recall that it∗ is the arm round with highest upper confidence interval in round t and ît the arm chosen by IntervalChaining. Since the confidence intervals are valid, it must be that [`tit∗ , utit∗ ] ∩ [`t ît , ut ît ] 6= ∅, so the instantaneous regret for any chained action is at most\nregret(t) ≤ 4 ∑ i∈St wt,i ≤ 4kmax i∈St wt,i (9)\nTo bound this term, we will focus on bounding maxi∈St wt,i = QN (0,xt,i(XTt,iXt,i)−1xt,i) ( 2kT δ ) . We first bound\nxt,i(X T t,iXt,i) −1xt,i ≤ ||xt,i||λmax ( (XTt,iXt,i) −1) = ||xt,i|| 1\nλmin ( XTt,iXt,i ) ≤ 1\nλmin ( XTt,iXt,i\n) (10)\nwhere the last inequality holds as ||xt,i|| ≤ 1 for all t, i. We now bound λmin ( XTt,iXt,i ) . Let us use the notation E [λmin] = E [ λmin ( Xtt,iXt,i )]\nLet Gt,i be the number of observations of action i with contexts drawn uniformly from the distribution for action i prior to round t, and let L ≥ maxt λmax ( xTt,ixt,i ) . Then for any α ∈ [0, 1], by the superadditivity of minimum eigenvalues for PSD matrices and linearity of expectations, we get\nE [λmin] ≥ Gt,i d λmind,i ≥ b Gt,i d cλmind,i .\nThis implies that\nPXt,i [ λmin ( XTt,iXt,i ) ≤ αbGt,i\nd cλmind,i ] ≤ PXt,i [ λmin ( XTt,iXt,i ) ≤ αE [λmin]\n] ≤ PXt,i [ λmin ( Xtt,iXt,i ) ≤ αλmin ( E [ Xtt,iXt,i\n])] ≤ de−(1−α) 2λmin(E[Xtt,iXt,i])/2L\n≤ de−(1−α)2E[λmin]/2L\n≤ de−(1−α) 2b Gt,i d cλmind,i/2L\n(11)\nwhere the second and fourth inequalities follow from Jensen’s inequality (which implies that E [λmin] ≤ λmin ( E [ Xtt,iXt,i ]) ) and the third inequlity follows from a matrix Chernoff bound (see e.g. Tropp et al. [31]). Then, taking logs and rearranging, with probability 1− δ,\nλmin ( XTt,iXt,i ) ≥ αbGt,i\nd cλmind,i (12)\nwhenever\nGt,i ≥ d (\nL\n(1− α)2λmind,i\n)( ln 1\nδ + ln d\n) . (13)\nA standard multiplicative Chernoff bound implies, for any fixed t, with probability 1 − δ′, the number of exploration rounds Gt prior to round t will satisfy\n|Gt − ∑ t′<t pt′ | ≤ √ ln 2 δ′ ∑ t′<t pt′ . (14)\nSimilarly, after Gt rounds of exploration, Gt,i, the number of exploration rounds in which a fixed action i was explored, with probability 1− δ′, a Chernoff bound implies\n|Gt,i − Gt k | ≤\n√ ln 2\nδ′ Gt k . (15)\nCombining Equation 14 and 15, with probability at least 1− 2δ′, for a fixed i and t, if ∑\nt′<t pt′ ≥ 36k ln2 2δ′ we have that\n|Gt,i − ∑ t′<t pt′\nk | ≤\n∑ t′<t pt′\n2k (16)\nThus, with probability 1− δ′, Equation 13 and therefore Equation 12 hold for any t such that∑ t′<t pt′ ≥ 36k ln\n2 2 δ′ and ∑\nt′<t pt 2k ≥ d\n( L\n(1− α)2λmind,i\n)( ln 1\nδ + ln d\n) . (17)\nBoth Equation 16 and 17 hold by our assumption that t > T1 and Equation 7. In total, we now upper-bound the sum of the instantaneous regrets from the exploitation rounds in the remaining T − T1 rounds. ∑\nt>T1, t is an exploit round\nregret(t, A)\n≤ 4k ∑ t>T1 max i QN (0,λmax((XTt,iXt,i))−1) ( δ 2kT ) ≤ 4k\n∑ t>T1 QN (0, 1 mini λmin((XTt,iXt,i)) ) ( δ 2kT ) ≤ 4k\n∑ t>T1 QN (0, 1 mini αb Gt,i d cλmind,i ) ( δ 2kT ) + δT\n≤ 4k ∑ t>T1 √√√√ ln 2kTδ mini αbGt,id cλmind,i + 2δT\n= O k∑ t>T1 √ d · ln 2kTδ miniGt,iλmind,i + 2δT  = O k√d · ln 2kTδ mini λmind,i ∑ t>T1 √ 1 miniGt,i + 2δT\n = O k√d · ln 2kTδ mini λmind,i ∑ t>T1 √ k∑ t′<t pt′ + 2δT\n = O k√d · ln 2kTδ mini λmind,i ∑ t>T1 √ k t2/3 + 2δT\n = O k3/2√d · ln 2kTδ mini λmind,i ∑ t∈[T1,T ] 1 t1/3 + 2δT\n = O k3/2√d · ln 2kTδ mini λmind,i T 2/3 + δT \n(18)\nwhere the third inequality follows from Equation 12, the fourth from a Chernoff bound for subgaussian random variables, the fifth and sixth from basic algebra and choosing α = 12 , the seventh from Equation 16, the eighth from Equation 6, and the ninth and tenth from basic algebra.\nThe final regret bound follows from Equation 8, substituting in the value for T1 in Equation 5 and the upper bound on the exploitation rounds in times after T1 steps given by Equation 18, using δ′ = min (\n1 3kT , 1 T 1/3\n) :\nR(T ) = T 2/3 +O k3/2√d · ln 2kTδ mini λmind,i T 2/3 + T 2/3 + Θ ( min i ( dkL\nλmind,i\n( ln2 2TK\nδ + ln d\n))3/2)\nas desired."
    } ],
    "references" : [ {
      "title" : "Improved algorithms for linear stochastic bandits",
      "author" : [ "Yasin Abbasi-yadkori", "Dávid Pál", "Csaba Szepesvári" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2011
    }, {
      "title" : "Taming the monster: A fast and simple algorithm for contextual bandits",
      "author" : [ "Alekh Agarwal", "Daniel J. Hsu", "Satyen Kale", "John Langford", "Lihong Li", "Robert E. Schapire" ],
      "venue" : "In Proceedings of the 31th International Conference on Machine Learning,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "URL https://www.propublica.org/article/ machine-bias-risk-assessments-in-criminal-sentencing",
      "author" : [ "Julia Angwin", "Jeff Larson", "Surya Mattu", "Lauren Kirchner" ],
      "venue" : "Machine bias. Propublica,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2016
    }, {
      "title" : "Big data’s disparate impact",
      "author" : [ "Solon Barocas", "Andrew D. Selbst" ],
      "venue" : "California Law Review,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2016
    }, {
      "title" : "The new science of sentencing",
      "author" : [ "Anna Maria Barry-Jester", "Ben Casselman", "Dana Goldstein" ],
      "venue" : "The Marshall Project, August",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2015
    }, {
      "title" : "Regret analysis of stochastic and nonstochastic multi-armed bandit problems",
      "author" : [ "Sébastien Bubeck", "Nicolo Cesa-Bianchi" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2012
    }, {
      "title" : "Artificial intolerance. MIT Technology Review, March 28 2016",
      "author" : [ "Nanette Byrnes" ],
      "venue" : "URL https: //www.technologyreview.com/s/600996/artificial-intolerance/",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2016
    }, {
      "title" : "Three naive bayes approaches for discrimination-free classification",
      "author" : [ "Toon Calders", "Sicco Verwer" ],
      "venue" : "Data Mining and Knowledge Discovery,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2010
    }, {
      "title" : "Contextual bandits with linear payoff functions",
      "author" : [ "Wei Chu", "Lihong Li", "Lev Reyzin", "Robert E. Schapire" ],
      "venue" : "In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2011
    }, {
      "title" : "Regulating by robot: Administrative decision-making in the machine-learning era",
      "author" : [ "Cary Coglianese", "David Lehr" ],
      "venue" : "Georgetown Law Journal,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2016
    }, {
      "title" : "Fairness through awareness",
      "author" : [ "Cynthia Dwork", "Moritz Hardt", "Toniann Pitassi", "Omer Reingold", "Richard Zemel" ],
      "venue" : "In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2012
    }, {
      "title" : "Certifying and removing disparate impact",
      "author" : [ "Michael Feldman", "Sorelle A. Friedler", "John Moeller", "Carlos Scheidegger", "Suresh Venkatasubramanian" ],
      "venue" : "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2015
    }, {
      "title" : "A confidence-based approach for balancing fairness and accuracy",
      "author" : [ "Benjamin Fish", "Jeremy Kun", "Ádám D Lelkes" ],
      "venue" : "SIAM International Symposium on Data Mining,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2016
    }, {
      "title" : "Equality of opportunity in supervised learning",
      "author" : [ "Moritz Hardt", "Eric Price", "Nathan Srebro" ],
      "venue" : "arXiv preprint arXiv:1610.02413,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2016
    }, {
      "title" : "Fairness in learning: Classic and contextual bandits",
      "author" : [ "Matthew Joseph", "Michael Kearns", "Jamie Morgenstern", "Aaron Roth" ],
      "venue" : "arXiv preprint arXiv:1605.07139,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2016
    }, {
      "title" : "Navigating the “trackless ocean”: Fairness in big data research and decision making",
      "author" : [ "FTC Commisioner Julie Brill" ],
      "venue" : "Keynote Address at the Columbia",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2015
    }, {
      "title" : "Fairness-aware learning through regularization approach",
      "author" : [ "Toshihiro Kamishima", "Shotaro Akaho", "Jun Sakuma" ],
      "venue" : "In Data Mining Workshops (ICDMW),",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2011
    }, {
      "title" : "Inherent trade-offs in the fair determination of risk scores",
      "author" : [ "Jon Kleinberg", "Sendhil Mullainathan", "Manish Raghavan" ],
      "venue" : "arXiv preprint arXiv:1609.05807,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2016
    }, {
      "title" : "Classical least squares theory. Available on: http://homepage",
      "author" : [ "Chung-Ming Kuan" ],
      "venue" : "ntu. edu. tw/ ̃ ckuan/pdf/et01/et Ch3",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2004
    }, {
      "title" : "Asymptotically efficient adaptive allocation rules",
      "author" : [ "Tze Leung Lai", "Herbert Robbins" ],
      "venue" : "Advances in applied mathematics,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1985
    }, {
      "title" : "A contextual-bandit approach to personalized news article recommendation",
      "author" : [ "Lihong Li", "Wei Chu", "John Langford", "Robert E Schapire" ],
      "venue" : "In Proceedings of the 19th international conference on World wide web,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2010
    }, {
      "title" : "Contextual multi-armed bandits",
      "author" : [ "Tyler Lu", "Dávid Pál", "Martin Pál" ],
      "venue" : "In AISTATS, pages 485–492,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2010
    }, {
      "title" : "k-nn as an implementation of situation testing for discrimination discovery and prevention",
      "author" : [ "Binh Thanh Luong", "Salvatore Ruggieri", "Franco Turini" ],
      "venue" : "In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2011
    }, {
      "title" : "Can an algorithm hire better than a human",
      "author" : [ "Clair C Miller" ],
      "venue" : "The New York Times, June",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2015
    }, {
      "title" : "Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy",
      "author" : [ "Cathy O’Neil" ],
      "venue" : null,
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2016
    }, {
      "title" : "A Theory of Justice",
      "author" : [ "John Rawls" ],
      "venue" : "Harvard university press,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2009
    }, {
      "title" : "Some aspects of the sequential design of experiments",
      "author" : [ "Herbert Robbins" ],
      "venue" : "Bulletin of the American Mathematical Society,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 1952
    }, {
      "title" : "A sequential decision problem with a finite memory",
      "author" : [ "Herbert Robbins" ],
      "venue" : "Proceedings of the National Academy of Sciences,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 1956
    }, {
      "title" : "Predictive policing using machine learning to detect patterns of crime",
      "author" : [ "Cynthia Rudin" ],
      "venue" : "URL http://www.wired.com/insights/2013/08/ predictive-policing-using-machine-learning-to-detect-\\patterns-of-crime/. Retrieved 4/28/2016",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "We prove a regret bound for fair algorithms in the linear contextual bandit framework that is a significant improvement over our technical companion paper [16], which gives black-box reductions in a more general setting.",
      "startOffset" : 155,
      "endOffset" : 159
    }, {
      "referenceID" : 23,
      "context" : "Automated techniques from statistics and machine learning are increasingly being used to make important decisions that directly affect the lives of individuals, including hiring [25], lending [8], policing [30], and criminal sentencing [6].",
      "startOffset" : 178,
      "endOffset" : 182
    }, {
      "referenceID" : 6,
      "context" : "Automated techniques from statistics and machine learning are increasingly being used to make important decisions that directly affect the lives of individuals, including hiring [25], lending [8], policing [30], and criminal sentencing [6].",
      "startOffset" : 192,
      "endOffset" : 195
    }, {
      "referenceID" : 28,
      "context" : "Automated techniques from statistics and machine learning are increasingly being used to make important decisions that directly affect the lives of individuals, including hiring [25], lending [8], policing [30], and criminal sentencing [6].",
      "startOffset" : 206,
      "endOffset" : 210
    }, {
      "referenceID" : 4,
      "context" : "Automated techniques from statistics and machine learning are increasingly being used to make important decisions that directly affect the lives of individuals, including hiring [25], lending [8], policing [30], and criminal sentencing [6].",
      "startOffset" : 236,
      "endOffset" : 239
    }, {
      "referenceID" : 9,
      "context" : "These high-stakes uses of machine learning have led to increasing concern in law and policy circles about the potential for (often opaque) machine learning techniques to be discriminatory or unfair [11, 5, 26].",
      "startOffset" : 198,
      "endOffset" : 209
    }, {
      "referenceID" : 3,
      "context" : "These high-stakes uses of machine learning have led to increasing concern in law and policy circles about the potential for (often opaque) machine learning techniques to be discriminatory or unfair [11, 5, 26].",
      "startOffset" : 198,
      "endOffset" : 209
    }, {
      "referenceID" : 24,
      "context" : "These high-stakes uses of machine learning have led to increasing concern in law and policy circles about the potential for (often opaque) machine learning techniques to be discriminatory or unfair [11, 5, 26].",
      "startOffset" : 198,
      "endOffset" : 209
    }, {
      "referenceID" : 2,
      "context" : "For example, a 2016 ProPublica study [4] of the COMPAS Recidivism Algorithm (used to inform criminal sentencing decisions by attempting to predict recidivism) found that the algorithm was significantly more likely to incorrectly label black defendants as recidivism risks compared to white defendants, despite similar overall rates of prediction accuracy between groups.",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 7,
      "context" : "[9, 24, 18, 13, 14] for a sample of papers studying such definitions).",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 22,
      "context" : "[9, 24, 18, 13, 14] for a sample of papers studying such definitions).",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 16,
      "context" : "[9, 24, 18, 13, 14] for a sample of papers studying such definitions).",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 11,
      "context" : "[9, 24, 18, 13, 14] for a sample of papers studying such definitions).",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 12,
      "context" : "[9, 24, 18, 13, 14] for a sample of papers studying such definitions).",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 10,
      "context" : "[12], these group-level definitions often fail at both fairness and accurate learning.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[12] advocate that technical definitions of fairness should focus on individual fairness, rather than fairness at the group level.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 25,
      "context" : "” [27]",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 17,
      "context" : "For instance, Title VII of the 1964 Civil Rights Act prohibits “disparate impact discrimination” — The company that sells COMPAS has raised methodological concerns about the ProPublica study, but it is apparent that not only are some of the objections raised valid in the case of COMPAS, they are more generally unavoidable [19].",
      "startOffset" : 324,
      "endOffset" : 328
    }, {
      "referenceID" : 15,
      "context" : "In a recent speech FTC Commissioner Julie Brill [17] observed, “.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 13,
      "context" : "What can be done to make sure these products and services–and the companies that use them treat consumers fairly and ethically?” An interesting recent paper [15] gives a different formalization of the idea of “equality of opportunity” in a classification setting.",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 13,
      "context" : "The definition suggested by [15] is a “group fairness” constraint on the average behaviour of the algorithm over all individuals of the same quality.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 26,
      "context" : "This setting is a generalization of the classical “multi-armed bandit” problem [28, 29, 21] called the contextual bandit problem [23, 10].",
      "startOffset" : 79,
      "endOffset" : 91
    }, {
      "referenceID" : 27,
      "context" : "This setting is a generalization of the classical “multi-armed bandit” problem [28, 29, 21] called the contextual bandit problem [23, 10].",
      "startOffset" : 79,
      "endOffset" : 91
    }, {
      "referenceID" : 19,
      "context" : "This setting is a generalization of the classical “multi-armed bandit” problem [28, 29, 21] called the contextual bandit problem [23, 10].",
      "startOffset" : 79,
      "endOffset" : 91
    }, {
      "referenceID" : 21,
      "context" : "This setting is a generalization of the classical “multi-armed bandit” problem [28, 29, 21] called the contextual bandit problem [23, 10].",
      "startOffset" : 129,
      "endOffset" : 137
    }, {
      "referenceID" : 8,
      "context" : "This setting is a generalization of the classical “multi-armed bandit” problem [28, 29, 21] called the contextual bandit problem [23, 10].",
      "startOffset" : 129,
      "endOffset" : 137
    }, {
      "referenceID" : 14,
      "context" : "In our technical companion paper [16], we generalize the framework presented here beyond linear functions, and relax several other simplifying assumptions made here (at the cost of less tight quantitative bounds).",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 27,
      "context" : "When designing fair algorithms, then, a natural starting place is the class of well-studied bandit algorithms that do not obey a fairness condition, but instead purely aim for accuracy [29].",
      "startOffset" : 185,
      "endOffset" : 189
    }, {
      "referenceID" : 14,
      "context" : "For RidgeFair, a sharper technical analysis yields a regret bound of R(T ) = Õ(d √ k3T ) In our companion paper [16] we prove a lower bound of R(T ) = Ω(k3) for fair algorithms in the multi-armed bandit setting (i.",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 5,
      "context" : "A lower bound of R(T ) = Ω( √ T ) is also known for this setting, even absent a fairness constraint [7].",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 1,
      "context" : "In [3] an R(T ) = Ω( √ Td) lower bound is proven for linear contextual bandit algorithms absent a fairness constraint, along with an algorithm enjoying a matching Õ( √ Td) upper bound.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 14,
      "context" : "Finally, we note that this is a significant improvement over the R(T ) = Õ(T 4/5k6/5d3/5) bound for the linear contextual bandit setting given in our companion paper [16], which is derived as a corollary of a general black box reduction rather than with a specialized analysis.",
      "startOffset" : 166,
      "endOffset" : 170
    }, {
      "referenceID" : 0,
      "context" : "Consequently valid confidence intervals are harder to derive than for the simple OLS estimator, and rely on martingale techniques borrowed from [2], which derives similar bounds absent a fairness constraint.",
      "startOffset" : 144,
      "endOffset" : 147
    }, {
      "referenceID" : 0,
      "context" : "Theorem 3 (From [2]).",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 0,
      "context" : "[2]: let V̄it = X T i Xi + λI, where Xi is the design matrix at time t corresponding to group i, λ ≥ 1.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "Then some matrix algebra from [2] shows: xt,i · (β̂it − βi) = xt,iV̄ −1 it X T i ηi − λxt,iV̄ −1 it βi, which using the above notation gives",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 0,
      "context" : "Finally, in the proof of Lemma 11 in [2] it is noted that:",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 20,
      "context" : "We note that our implementation of IntervalChaining does not include the random sampling component given in its formal presentation in the previous section, as we have found that this improves empirical performance over the TopInterval is a variant of the standard linear bandits algorithm “LinUCB” [22], simplified to take advantages of the assumptions in our model.",
      "startOffset" : 299,
      "endOffset" : 303
    } ],
    "year" : 2017,
    "abstractText" : "Motivated by concerns that automated decision-making procedures can unintentionally lead to discriminatory behavior, we study a technical definition of fairness modeled after John Rawls’ notion of “fair equality of opportunity”. In the context of a simple model of online decision making, we give an algorithm that satisfies this fairness constraint, while still being able to learn at a rate that is comparable to (but necessarily worse than) that of the best algorithms absent a fairness constraint. We prove a regret bound for fair algorithms in the linear contextual bandit framework that is a significant improvement over our technical companion paper [16], which gives black-box reductions in a more general setting. We analyze our algorithms both theoretically and experimentally. Finally, we introduce the notion of a “discrimination index”, and show that standard algorithms for our problem exhibit structured discriminatory behavior, whereas the “fair” algorithms we develop do not. ∗majos, mkearns, jamiemor, aaroth@cis.upenn.edu. Department of Computer and Information Sciences, University of Pennsylvania. †sethneel@wharton.upenn.edu. Department of Statistics, The Wharton School, University of Pennsylvania. 1 ar X iv :1 61 0. 09 55 9v 1 [ cs .L G ] 2 9 O ct 2 01 6",
    "creator" : "LaTeX with hyperref package"
  }
}