{
  "name" : "1511.06279.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "NEURAL PROGRAMMER-INTERPRETERS", "Scott Reed" ],
    "emails" : [ "scott.ellison.reed@gmail.com", "nandodefreitas@google.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We propose the neural programmer-interpreter (NPI): a recurrent and compositional neural network that learns to represent and execute programs. NPI has three learnable components: a task-agnostic recurrent core, a persistent key-value program memory, and domain-specific encoders that enable a single NPI to operate in multiple perceptually diverse environments with distinct affordances. By learning to compose lower-level programs to express higher-level programs, NPI reduces sample complexity and increases generalization ability compared to sequence-tosequence LSTMs. The program memory allows efficient learning of additional tasks by building on existing programs. NPI can also harness the environment (e.g. a scratch pad with read-write pointers) to cache intermediate results of computation, lessening the long-term memory burden on recurrent hidden units. In this work we train the NPI with fully-supervised execution traces; each program has example sequences of calls to the immediate subprograms conditioned on the input. Rather than training on a huge number of relatively weak labels, NPI learns from a small number of rich examples. We demonstrate the capability of our model to learn several types of compositional programs: addition, sorting, and canonicalizing 3D models. Furthermore, a single NPI learns to execute these programs and all 21 associated subprograms."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Teaching machines to learn new programs, to rapidly compose new programs from existing programs, and to conditionally execute these programs automatically so as to solve a wide variety of tasks is one of the central challenges of AI. Programs appear in many guises in various AI problems; including motor behaviours, image transformations, reinforcement learning policies, classical algorithms, and symbolic relations.\nIn this paper, we develop a compositional architecture that learns to represent and interpret programs. We refer to this architecture as the Neural Programmer-Interpreter (NPI). The core module is an LSTM-based sequence model that takes as input a learnable program embedding, program arguments passed on by the calling program, and a feature representation of the environment. The output of the core module is a key indicating what program to call next, arguments for the following program and a flag indicating whether the program should terminate. In addition to the recurrent core, the NPI architecture includes a learnable key-value memory of program embeddings. This program-memory is essential for learning and re-using programs in a continual manner. Figures 1 and 2 illustrate the NPI on two different tasks.\nWe show in our experiments that the NPI architecture can learn 21 programs, including addition, sorting, and trajectory planning from image pixels. Crucially, this can be achieved using a single core model with the same parameters shared across all tasks. Different environments (for example images, text, and scratch-pads) may require specific perception modules or encoders to produce the features used by the shared core, as well as environment-specific actuators. Both perception modules and actuators can be learned from data when training the NPI architecture.\nTo train the NPI we use curriculum learning and supervision via example execution traces. Each program has example sequences of calls to the immediate subprograms conditioned on the input.\nar X\niv :1\n51 1.\n06 27\n9v 1\n[ cs\n.L G\n] 1\n9 N\nov 2\n01 5\nBy using neural networks to represent the subprograms and learning these from data, the approach can generalize on tasks involving rich perceptual inputs and uncertainty.\nWe may envision two approaches to provide supervision. In one, we provide a very large number of labeled examples, as in object recognition, speech and machine translation. In the other, the approached followed in this paper, the aim is to provide far fewer labeled examples, but where the labels contain richer information allowing the model to learn compositional structure. While unsupervised and reinforcement learning play important roles in perception and motor control, other cognitive abilities are possible thanks to rich supervision and curriculum learning. This is indeed the reason for sending our children to school.\nAn advantage of our approach to model building and training is that the learned programs exhibit strong generalization. Specifically, when trained to sort sequences of up to twenty numbers in length, they can sort much longer sequences at test time. In contrast, the experiments will show that more standard sequence to sequence LSTMs only exhibit weak generalization, see Figure 6.\nA trained NPI with fixed parameters and a learned library of programs, can act both as an interpreter and as a programmer. As an interpreter, it takes input in the form of a program embedding and input data and subsequently executes the program. As a programmer, it uses samples drawn from a new task to generate a new program embedding that can be added to its library of programs."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "Several ideas related to our approach have a long history. For example, the idea of using dynamically programmable networks in which the activations of one network become the weights (the\nprogram) of a second network was mentioned in the Sigma-Pi units section of the influential PDP paper (Rumelhart et al., 1986). This idea appeared in (Sutskever & Hinton, 2009) in the context of learning higher order symbolic relations and in (Donnarumma et al., 2015) as the key ingredient of an architecture for prefrontal cognitive control. Schmidhuber (1992) proposed a related meta-learning idea, whereby one learns the parameters of a slowly changing network, which in turn generates context dependent weight changes for a second rapidly changing network. These approaches have only been demonstrated in very limited settings.\nIn cognitive science, several theories of brain areas controlling other brain parts so as to carry out multiple tasks have been proposed; see for example Schneider & Chein (2003); Anderson (2010) and Donnarumma et al. (2012).\nRecently, Graves et al. (2014) advanced a neural Turing machine that is capable of learning and executing simple programs such as repeat copying, simple priority sorting and associative recall. Instead of using input and output pairs, our model is trained on program execution traces at varying levels of abstraction. In exchange for this richer supervision, we get the benefit of learning compositionality of programs, and also data efficient training of complex programs.\nRelated problems have been studied in the literature on hierarchical reinforcement learning (e.g., Dietterich (2000); Andre & Russell (2001); Sutton et al. (1999) and Schaul et al. (2015)), imitation and apprenticeship learning (e.g., Kolter et al. (2008) and Rothkopf & Ballard (2013)) and elicitation of options through human interaction (Subramanian et al., 2011). These ideas have held great promise, but have not enjoyed significant impact. We believe the recurrent compositional neural representations proposed in this paper could help these approaches in the future, and in particular in overcoming feature engineering.\nThis work is also closely related to program induction. Most previous work on program induction, i.e. inducing a program given example input and output pairs, has used genetic programming (Banzhaf et al., 1998) to evolve useful programs from candidate populations.\nThere have been several recent works extending recurrent networks to solve problems not traditionally cast as simple sequence prediction. Zaremba & Sutskever (2014) trained LSTM models to read in the text of simple programs character-by-character and correctly predict the program output. Though not practical as a real interpreter, it is somewhat astonishing that it works. Mou et al. (2014) process program symbols to learn max-margin program embeddings with the help of parse trees. Vinyals et al. (2015) developed Pointer Networks that generalize the notion of encoder attention in order to provide the decoder a variable-sized output space depending on the input sequence length. This model was shown to be effective for combinatorial optimization problems such as the traveling salesman and Delaunay triangulation. Our work is inspired by these but tackles a different problem."
    }, {
      "heading" : "3 MODEL",
      "text" : "The NPI core is a long short-term memory (LSTM) network (Hochreiter & Schmidhuber, 1997) that acts as a router between programs conditioned on the current state observation and previous hidden unit states. At each time step, the core module can select another program to invoke using content-based addressing. It emits the probability of ending the current program with a single binary unit. If this probability is over threshold (we used 0.5), control is returned to the caller by popping the caller’s LSTM hidden units and program embedding off of a program call stack and resuming execution in this context.\nThe NPI may also optionally write arguments (ARG) that are passed by reference or value to the invoked sub-programs. For example, an argument could indicate a specific location in the input sequence (by reference), or it could specify a number to write down at a particular location in the sequence (by value). The subsequent state consists of these arguments and observations of the environment. The approach is illustrated in Figures 1 and 2.\nIt must be emphasized that there is a single inference core. That is, all the LSTM instantiations executing arbitrary programs share the same parameters. Different programs correspond to program embeddings, which are stored in a learnable persistent memory. The programs therefore have a more succinct representation than neural programs encoded as the full set of weights in a neural network (Rumelhart et al., 1986; Graves et al., 2014).\nThe output of an NPI, conditioned on an input state and a program to run, is a sequence of actions in a given environment. In this work, we consider several environments: a 1-D array with read-only pointers and a swap action, a 2-D scratch pad with read-write pointers, and a CAD renderer with controllable elevation and azimuth movements. Note that the sequence of actions for a program is not fixed, but dependent also on the input state."
    }, {
      "heading" : "3.1 INFERENCE",
      "text" : "Denote the environment observation at time t as et ∈ E , and the current program arguments as at ∈ A. The form of et can vary dramatically by environment; for example it could be a color image or an array of numbers. The program arguments at can also vary by environment, but in the experiments for this paper we always used a 3-tuple of integers (at(1), at(2), at(3)). Given the environment and arguments at time t, a fixed-length state encoding st ∈ RD is extracted by a domain-specific encoder fenc : E×A → RD. In section 4 we provide examples of several encoders. Note that a single NPI network can have multiple encoders for multiple environments, and encoders can potentially also be shared across tasks.\nWe denote the current program embedding as pt ∈ RP . The previous hidden unit and cell states are h(l)t−1 ∈ RM and c (l) t−1 ∈ RM , l = 1, ..., L where L is the number of layers in the LSTM. The program and state vectors are then propagated forward through an LSTM mapping flstm as in (Sutskever et al., 2014). How to fuse pt and st within flstm is an implementation detail, but in this work we concatenate and feed through a 2-layer MLP with rectified linear (ReLU) hidden activation and linear decoder.\nFrom the top LSTM hidden state hLt , several decoders generate the outputs. The probability of finishing the program and returning to the caller 1 is computed by fend : RM → [0, 1]. The lookup key embedding used for retrieving the next program from memory is computed by fprog : RM → RK . Note that RK can be much smaller than RP because the key only need act as the identifier of a program, while the program embedding must have enough capacity to conditionally generate a sequence of actions. The contents of the arguments to the next program to be called are generated by farg : RM → A. The feed-forward steps of program inference are summarized below:\nst = fenc(et, at) (1) ht = flstm(st, pt, ht−1) (2) rt = fend(ht), kt = fprog(ht), at+1 = farg(ht) (3)\nwhere rt, kt and at+1 correspond to the end-of-program probability, program key embedding, and output arguments at time t, respectively. These yield input arguments at time t+ 1. To simplify the notation, we have abstracted properties such as layers and cell memory in the sequence-to-sequence LSTM of equation (2); see (Sutskever et al., 2014) for details.\nThe NPI representation is equipped with key-value memory structures Mkey ∈ RN×K and Mprog ∈ RN×P storing program keys and program embeddings, respectively, where N is the current number of programs in memory. We can add more programs by adding rows to memory.\nDuring training, the next program identifier is provided to the model as ground-truth, so that its embedding can be retrieved from the corresponding row of Mprog. At test time, we compute the “program ID” by comparing the key embedding kt to each row of Mkey storing all program keys. Then the program embedding is retrieved from Mprog as follows:\ni∗ = arg max i=1..N (Mkeyi,: ) T kt , pt+1 =M prog i∗,: (4)\nThe next environmental state et+1 will be determined by the dynamics of the environment and can be affected by both the choice of program pt and the contents of the output arguments at, i.e.\net+1 ∼ fenv(et, pt, at) (5)\nThe transition mapping fenv is domain-specific and will be discussed in Section 4. A description of the inference procedure is given in Algorithm 1.\n1In our implementation, a program may first call a subprogram before itself finishing. The only exception is the ACT program that signals a low-level action to the environment, e.g. moving a pointer one step left or writing a value. By convention ACT does not call any further sub-programs.\nAlgorithm 1: Neural programming inference Data: Program stack S, observation e, program id i∗, args a, stop threshold α h = 0, p =Mprogi∗,: , PUSH(S, (i ∗, p, h)) while |S| > 0 do s = fenc(e, a), h = flstm(s, p, h) r = fend(h), k = fprog(h), a = farg(h) if i∗ 6= ACT then\nPUSH(S, (i∗, p, h)), h = 0 i∗ = arg max\ni=1..N\n(Mkeyi,: ) T k, p =Mprogi∗,:\nend if r ≥ α then\n(i∗, p, h) = POP(S) end e ∼ fenv(e, p, a)\nend\nEach task will have a set of actions that affect the environment. For example, in the case of addition there are LEFT and RIGHT actions that move a specified pointer, and a WRITE action which writes a value at a specified location. These actions are encapsulated into a generalpurpose ACT program shared across tasks, and the concrete action to be taken is indicated by the NPI-generated arguments at.\nNote that the core LSTM module of our NPI representation is completely agnostic to the data modality used to produce the state encoding. As long as the same fixed-length embedding is extracted, the same module can in practice route between programs related to sorting arrays just as easily as between programs related to rotating 3D objects. In the experimental sections, we provide details of the modality-specific deep neural networks that we use to produce these fixed-length state vectors."
    }, {
      "heading" : "3.2 TRAINING",
      "text" : "To train we use execution traces ξinpt : {et, it, at} and ξoutt : {it+1, at+1, rt}, t = 1, ...T , where T is the sequence length. Program IDs it and it+1 are row-indices in Mkey and Mprog of the programs to run at time t and t+1, respectively. We propose to directly maximize the probability of the correct execution trace output ξout conditioned on ξinp:\nθ∗ = arg max θ ∑ (ξinp,ξout) logP (ξout|ξinp; θ) (6)\nwhere θ are the parameters of our model. Since the traces are variable in length depending on the input, we apply the chain rule to model the joint probability over ξout1 , ..., ξ out T as follows:\nlogP (ξout|ξinp; θ) = T∑ t=1 logP (ξoutt |ξ inp 1 , ..., ξ inp t ; θ) (7)\nNote that for many problems the input history ξinp1 , ..., ξ inp t is critical to deciding future actions because the environment observation at the current time-step et alone does not contain enough information. The hidden unit activations of the LSTM in NPI are capable of capturing these temporal dependencies. The single-step conditional probability in equation (7) can be factorized into three further conditional distributions, corresponding to predicting the next program, next arguments, and whether to halt execution:\nlogP (ξoutt |ξ inp 1 , ..., ξ inp t ) = logP (it+1|ht) + logP (at+1|ht) + logP (rt|ht) (8)\nwhere ht is the output of flstm at time t, carrying information from previous time steps. We train by gradient ascent on the likelihood in equation (7).\nIn practice we found it effective to employ a curriculum learning strategy. The intuition is that we first learn the most basic subprograms such as shifting pointers left or right, followed by more challenging programs such as comparing and conditionally swapping two numbers. The knowledge of subprogram difficulty is itself a form of supervision, but there could be many reasonable heuristics such as counting the average number of low-level actions generated by the program.\nWe also note that our program has a distinct memory advantage over basic LSTMs because all subprograms can be trained in parallel. For programs whose execution length grows e.g. quadratically with the input sequence length, an LSTM will by highly constrained by device memory to train on short sequences. By exploiting compositionality, an effective curriculum can often be developed with sublinear-length subprograms, enabling our NPI model to train on order of magnitude larger sequences than the LSTM."
    }, {
      "heading" : "4 EXPERIMENTS",
      "text" : "This section describes the environment and state encoder function for each task, and shows example outputs and prediction accuracy results. For all tasks, the core LSTM had two layers of size 256. We trained the NPI model and all program embeddings jointly using RMSprop with base learning rate 0.0001, batch size 1, and decayed the learning rate by a factor of 0.95 every 10,000 steps."
    }, {
      "heading" : "4.1 TASK AND ENVIRONMENT DESCRIPTIONS",
      "text" : "In this section we provide an overview of the tasks used to evaluate our model. Table 2 in the appendix provides a full listing of all the programs and subprograms learned by our model.\nADDITION\nThe task in this environment is to read in the digits of two base-10 numbers and produce the digits of the answer. Our goal is to teach the model the standard (at least in the US) grade school algorithm of adding, in which one works from right to left applying single-digit add and carry operations.\nIn this environment, the network is endowed with a “scratch pad” with which to store intermediate computations; e.g. to record carries. There are four pointers; one for each of the two input numbers, one for the carry, and another to write the output. At each time step, a pointer can be moved left or right, or it can record a value to the pad. Figure 3a illustrates the environment of this model, and Figure 3b provides a real execution trace generated by our model.\nFor the state encoder fenc, the model is allowed a view of the scratch pad from the perspective of each of the four pointers. That is, the model sees the current values at pointer locations of the two inputs, the carry row and the output row, as 1-of-K encodings, where K is 10 because we are working in base 10. We also append the values of the input argument tuple at:\nfenc(Q, i1, i2, i3, i4, at) =MLP ([Q(1, i1), Q(2, i2), Q(3, i3), Q(4, i4), at(1), at(2), at(3)]) (9)\nwhere Q ∈ R4×N×K , and i1, ..., i4 are pointers, one per scratch pad row. The first dimension of Q corresponds to scratch pad rows, N is the number of columns (digits) andK is the one-hot encoding dimension. To begin the ADD program, we set the initial arguments to a default value and initialize all pointers to be at the rightmost column. The only subprogram with non-default arguments is ACT, in which case the arguments indicate an action to be taken by a specified pointer.\nSORTING\nIn this section we apply our model to a setting with potentially much longer execution traces: sorting an array of numbers using bubblesort. As in the case of addition we can use a scratch pad to store intermediate states of the array. We define the encoder as follows:\nfenc(Q, i1, i2, at) =MLP ([Q(1, i1), Q(1, i2), at(1), at(2), at(3)]) (10)\nwhereQ ∈ R1×N×K is the pad,N is the array length and K is the array entry embedding dimension. Figure 4 shows an example series of array states and an excerpt of an execution trace.\nCANONICALIZING 3D MODELS\nWe also apply our model to a vision task with a very different perceptual environment - pixels. Given a rendering of a 3D car, we would like to learn a visual program that “canonicalizes” the model with respect to its pose. Whatever the starting position, the program should generate a trajectory of actions that delivers the camera to the target view, e.g. frontal pose at a 15◦ elevation. For training data, we used renderings of the 3D car CAD models from (Fidler et al., 2012).\nThis is a nontrivial problem because different starting positions will require quite different trajectories to reach the target. Further complicating the problem is the fact that the model will need to generalize to different car models than it saw during training.\nWe again use a scratch pad, but here it is a very simple read-only pad that only contains a target camera elevation and azimuth – i.e., the “canonical pose”. Since observations come in the form of image pixels, we use a convolutional neural network fCNN as the image encoder:\nfenc(Q, x, i1, i2, at) =MLP ([Q(1, i1), Q(2, i2), fCNN (x), at(1), at(2), at(3)]) (11)\nwhere x ∈ RH×W×3 is a car rendering at the current pose, Q ∈ R2×1×K is the pad containing canonical azimuth and elevation, i1, i2 are the (fixed at 1) pointer locations, and K is the one-hot encoding dimension of pose coordinates. We set K = 24 corresponding to 15◦ pose increments.\nNote, critically, that our NPI model only has access to pixels of the rendering and the target pose, and is not provided the pose of query frames. We are also aware that one solution to this problem would be to train a pose classifier network and then find the shortest path to canonical pose via classical methods. That is also a sensible approach. However, our purpose here is to show that our method generalizes beyond the scratch pad domain to detailed images of 3D objects, and also to other environments with a single multi-task model."
    }, {
      "heading" : "4.2 SAMPLE COMPLEXITY AND GENERALIZATION",
      "text" : "Previous works have established that recurrent networks and particularly LSTMs and Neural Turing Machines can in fact sort arrays of numbers. However, we are interested not only in whether sorting can be accomplished, but whether a particular sorting algorithm (e.g. bubblesort) can be learned by the model, and how effectively in terms of sample complexity and generalization.\nWe compare the generalization ability of our model to a flat sequence-to-sequence LSTM (Sutskever et al., 2014), using the same number of layers (2) and hidden units (256). Note that a flat 2 version of NPI could also learn sorting of short arrays, but because bubblesort runs in O(N2) for arrays of length N , the execution traces quickly become far too long to store the required number of LSTM states in memory. Our NPI architecture can train on much larger arrays by exploiting compositional structure; the memory requirements of any given subprogram can be restricted to O(N).\nA strong indicator of whether a neural network has learned a program well is whether it can run the program on inputs of previously-unseen sizes. To evaluate this property, we train both the sequenceto-sequence LSTM and NPI to perform bubblesort on arrays of single-digit numbers from length 2\n2By flat in this case, we mean non-compositional, not making use of subprograms, and only making calls to ACT in order to swap values and move pointers.\nto length 20. Compared to fixed-length inputs this raises the challenge level during training, but in exchange we can get a more flexible and generalizable sorting program.\nTo handle variable-sized inputs, the state representation must have some information about input sequence length and the number of steps taken so far. For example, the main BUBBLESORT program naturally needs to call its helper function BUBBLE a number of times dependent on the sequence length. We enable this in our model by adding a third pointer that acts as a counter; each time BUBBLE is called the pointer is advanced by one step. The scratch pad environment also provides a bit indicating whether a pointer is at the start or end of a sequence, equivalent in purpose to end tokens used in a sequence-to-sequence model.\nFor each length, we provided 64 example bubblesort traces, for a total of 1,216 examples. Then, we evaluated whether the network can learn to sort arrays beyond length 20. We found that the trained model generalizes well, and is capable of sorting arrays up to size 60; see Figure 6. At 60 and beyond, we observed a failure mode in which sweeps of pointers across the array would take the wrong number of steps, suggesting that the limiting performance factor is related to counting. In stark contrast, when provided with the 1,216 examples, the sequence-to-sequence LSTMs fail to generalize beyond arrays of length 25 as shown in Figure 6.\nTo study sample complexity further, we fix the length of the arrays to 20 and vary the number of training examples. We see in Figure 5 that NPI starts learning with 2 examples and is able to sort almost perfectly with only 8 examples. The sequence-to-sequence model on the other hand requires 64 examples to start learning and only manages to sort well with over 250 examples.\nFigure 7 shows several example canonicalization trajectories generated by our model, starting from the leftmost car. The image encoder was a convolutional network with three passes of stride-2 convolution and pooling, trained on renderings of size 128× 128. The canonical target pose in this case is frontal with 15◦ elevation. At test time, from an initial rendering, NPI is able to canonicalize cars of varying appearance from multiple starting positions. Importantly, it can generalize to car appearances not encountered in the training set as shown in Figure 7."
    }, {
      "heading" : "4.3 LEARNING NEW PROGRAMS WITH A FIXED CORE",
      "text" : "One challenge for continual learning of neural-network-based agents is that training on new tasks and experiences can lead to degraded performance in old tasks. The learning of new tasks may require that the network weights change substantially, so care must be taken to avoid catastrophic forgetting(Mccloskey & Cohen, 1989; OReilly et al., 2014). Using NPI, one solution is to fix the weights of the core routing module, and only make sparse updates to the program memory.\nWhen adding a new program the core module’s routing computation will be completely unaffected; all the learning for a new task occurs in program embedding space. Of course, the addition of new programs to the memory adds a new choice of program at each time step, and an old program could mistakenly call a newly added program. To overcome this, when learning a new set of program vectors with a fixed core, in practice we train not only on example traces of the new program, but also traces of existing programs. Alternatively, a simpler approach is to prevent existing programs from calling subsequently added programs, allowing addition of new programs without ever looking back at training data for known programs. In either case, note that only the memory slots of the new programs are updated, and all other weights, including other program embeddings, are fixed.\nTable 1 shows the result of adding a maximum-finding program MAX to a multitask NPI trained on addition, sorting and canonicalization. The MAX program works by first calling BUBBLESORT and then a new program RJMP, which moves pointers all the way to the right of the sorted array, where the max element can be read. We performed training as in the from-scratch case, but froze all weights except for the two newly-added program embeddings. Table 1 shows that the NPI learns MAX at high accuracy (95.8) without forgetting the other tasks. In particular, after training a single multi-task model as outlined in the following section, learning the MAX program with this fixedcore multi-task NPI results in no performance deterioration for all three tasks."
    }, {
      "heading" : "4.4 SOLVING MULTIPLE TASKS WITH A SINGLE NETWORK",
      "text" : "In this section we perform a controlled experiment to compare the performance of a multi-task NPI with several single-task NPI models. Table 1 shows the results for addition, sorting and canonicalizing 3D car models. We trained and evaluated on 10-digit numbers for addition, length-5 arrays for sorting, and up to four-step trajectories for canonicalization. As shown in Table 1, a single multitask NPI can learn all three programs (and necessarily the 21 subprograms) with modest performance degradation compared to when it is trained on single-tasks."
    }, {
      "heading" : "5 CONCLUSION",
      "text" : "We have shown that the NPI can learn programs in very dissimilar environments with different affordances. In the context of sorting we showed that NPI exhibits very strong generalization in comparison to sequence-to-sequence LSTMs. We also showed how a trained NPI with a fixed core can continue to learn new programs without forgetting already learned programs."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "We sincerely thank Arun Nair and Ed Grefenstette for helpful suggestions."
    }, {
      "heading" : "6 APPENDIX",
      "text" : ""
    }, {
      "heading" : "6.1 LISTING OF LEARNED PROGRAMS",
      "text" : "Below we list the programs learned by our model:"
    }, {
      "heading" : "6.2 GENERATED EXECUTION TRACE OF BUBBLESORT",
      "text" : "Figure 8 shows the sequence of program calls for BUBBLESORT. Pointers 1 and 2 are used to im-\nplement the “bubble” operation involving the comparison and swapping of adjacent array elements. The third pointer (referred to in the trace as “PTR 3”) is used to count the number of calls to BUBBLE. After every call to RESET the swapping pointers are moved to the beginning of the array and the counting pointer is advanced by 1. When it has reached the end of the scratch pad, the model learns to halt execution of BUBBLESORT."
    } ],
    "references" : [ {
      "title" : "Neural reuse: A fundamental organizational principle of the brain",
      "author" : [ "Anderson", "Michael L" ],
      "venue" : "Behavioral and Brain Sciences, 33:245–266,",
      "citeRegEx" : "Anderson and L.,? \\Q2010\\E",
      "shortCiteRegEx" : "Anderson and L.",
      "year" : 2010
    }, {
      "title" : "Programmable reinforcement learning agents",
      "author" : [ "Andre", "David", "Russell", "Stuart J" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Andre et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Andre et al\\.",
      "year" : 2001
    }, {
      "title" : "Hierarchical reinforcement learning with the MAXQ value function decomposition",
      "author" : [ "Dietterich", "Thomas G" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Dietterich and G.,? \\Q2000\\E",
      "shortCiteRegEx" : "Dietterich and G.",
      "year" : 2000
    }, {
      "title" : "Programming in the brain: A neural network theoretical framework",
      "author" : [ "Donnarumma", "Francesco", "Prevete", "Roberto", "Trautteur", "Giuseppe" ],
      "venue" : "Connection Science,",
      "citeRegEx" : "Donnarumma et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Donnarumma et al\\.",
      "year" : 2012
    }, {
      "title" : "A programmerinterpreter neural network architecture for prefrontal cognitive control",
      "author" : [ "Donnarumma", "Francesco", "Prevete", "Roberto", "Chersi", "Fabian", "Pezzulo", "Giovanni" ],
      "venue" : "International Journal of Neural Systems,",
      "citeRegEx" : "Donnarumma et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Donnarumma et al\\.",
      "year" : 2015
    }, {
      "title" : "3D object detection and viewpoint estimation with a deformable 3D cuboid model",
      "author" : [ "Fidler", "Sanja", "Dickinson", "Sven", "Urtasun", "Raquel" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Fidler et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Fidler et al\\.",
      "year" : 2012
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Hochreiter", "Sepp", "Schmidhuber", "Jürgen" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Hochreiter et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter et al\\.",
      "year" : 1997
    }, {
      "title" : "Hierarchical apprenticeship learning with application to quadruped locomotion",
      "author" : [ "Kolter", "Zico", "Abbeel", "Pieter", "Ng", "Andrew Y" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Kolter et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Kolter et al\\.",
      "year" : 2008
    }, {
      "title" : "Catastrophic interference in connectionist networks: The sequential learning problem",
      "author" : [ "Mccloskey", "Michael", "Cohen", "Neal J" ],
      "venue" : "In The psychology of learning and motivation,",
      "citeRegEx" : "Mccloskey et al\\.,? \\Q1989\\E",
      "shortCiteRegEx" : "Mccloskey et al\\.",
      "year" : 1989
    }, {
      "title" : "Building program vector representations for deep learning",
      "author" : [ "Mou", "Lili", "Li", "Ge", "Liu", "Yuxuan", "Peng", "Hao", "Jin", "Zhi", "Xu", "Yan", "Zhang", "Lu" ],
      "venue" : "arXiv preprint arXiv:1409.3358,",
      "citeRegEx" : "Mou et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mou et al\\.",
      "year" : 2014
    }, {
      "title" : "Complementary learning systems",
      "author" : [ "OReilly", "Randall C", "Bhattacharyya", "Rajan", "Howard", "Michael D", "Ketz", "Nicholas" ],
      "venue" : "Cognitive Science,",
      "citeRegEx" : "OReilly et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "OReilly et al\\.",
      "year" : 2014
    }, {
      "title" : "Modular inverse reinforcement learning for visuomotor behavior",
      "author" : [ "Rothkopf", "ConstantinA", "Ballard", "DanaH" ],
      "venue" : "Biological Cybernetics,",
      "citeRegEx" : "Rothkopf et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Rothkopf et al\\.",
      "year" : 2013
    }, {
      "title" : "Parallel distributed processing: Explorations in the microstructure of cognition, vol. 1. chapter A General Framework for Parallel",
      "author" : [ "D.E. Rumelhart", "G.E. Hinton", "J.L. McClelland" ],
      "venue" : "Distributed Processing,",
      "citeRegEx" : "Rumelhart et al\\.,? \\Q1986\\E",
      "shortCiteRegEx" : "Rumelhart et al\\.",
      "year" : 1986
    }, {
      "title" : "Universal value function approximators",
      "author" : [ "Schaul", "Tom", "Horgan", "Daniel", "Gregor", "Karol", "Silver", "David" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Schaul et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Schaul et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning to control fast-weight memories: An alternative to dynamic recurrent networks",
      "author" : [ "Schmidhuber", "Jürgen" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Schmidhuber and Jürgen.,? \\Q1992\\E",
      "shortCiteRegEx" : "Schmidhuber and Jürgen.",
      "year" : 1992
    }, {
      "title" : "Controlled and automatic processing: behavior, theory, and biological mechanisms",
      "author" : [ "Schneider", "Walter", "Chein", "Jason M" ],
      "venue" : "Cognitive Science,",
      "citeRegEx" : "Schneider et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Schneider et al\\.",
      "year" : 2003
    }, {
      "title" : "Learning options through human interaction",
      "author" : [ "Subramanian", "Kaushik", "Isbell", "Charles", "Thomaz", "Andrea" ],
      "venue" : "In IJCAI Workshop on Agents Learning Interactively from Human Teachers,",
      "citeRegEx" : "Subramanian et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Subramanian et al\\.",
      "year" : 2011
    }, {
      "title" : "Using matrices to model symbolic relationship",
      "author" : [ "Sutskever", "Ilya", "Hinton", "Geoffrey E" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Sutskever et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2009
    }, {
      "title" : "Sequence to sequence learning with neural networks. In Advances in neural information processing",
      "author" : [ "Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc VV" ],
      "venue" : null,
      "citeRegEx" : "Sutskever et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning",
      "author" : [ "Sutton", "Richard S", "Precup", "Doina", "Singh", "Satinder" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Sutton et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 1999
    }, {
      "title" : "Learning to execute",
      "author" : [ "Zaremba", "Wojciech", "Sutskever", "Ilya" ],
      "venue" : "arXiv preprint arXiv:1410.4615,",
      "citeRegEx" : "Zaremba et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zaremba et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "program) of a second network was mentioned in the Sigma-Pi units section of the influential PDP paper (Rumelhart et al., 1986).",
      "startOffset" : 102,
      "endOffset" : 126
    }, {
      "referenceID" : 4,
      "context" : "This idea appeared in (Sutskever & Hinton, 2009) in the context of learning higher order symbolic relations and in (Donnarumma et al., 2015) as the key ingredient of an architecture for prefrontal cognitive control.",
      "startOffset" : 115,
      "endOffset" : 140
    }, {
      "referenceID" : 16,
      "context" : "(2008) and Rothkopf & Ballard (2013)) and elicitation of options through human interaction (Subramanian et al., 2011).",
      "startOffset" : 91,
      "endOffset" : 117
    }, {
      "referenceID" : 3,
      "context" : "This idea appeared in (Sutskever & Hinton, 2009) in the context of learning higher order symbolic relations and in (Donnarumma et al., 2015) as the key ingredient of an architecture for prefrontal cognitive control. Schmidhuber (1992) proposed a related meta-learning idea, whereby one learns the parameters of a slowly changing network, which in turn generates context dependent weight changes for a second rapidly changing network.",
      "startOffset" : 116,
      "endOffset" : 235
    }, {
      "referenceID" : 3,
      "context" : "This idea appeared in (Sutskever & Hinton, 2009) in the context of learning higher order symbolic relations and in (Donnarumma et al., 2015) as the key ingredient of an architecture for prefrontal cognitive control. Schmidhuber (1992) proposed a related meta-learning idea, whereby one learns the parameters of a slowly changing network, which in turn generates context dependent weight changes for a second rapidly changing network. These approaches have only been demonstrated in very limited settings. In cognitive science, several theories of brain areas controlling other brain parts so as to carry out multiple tasks have been proposed; see for example Schneider & Chein (2003); Anderson (2010) and Donnarumma et al.",
      "startOffset" : 116,
      "endOffset" : 684
    }, {
      "referenceID" : 3,
      "context" : "This idea appeared in (Sutskever & Hinton, 2009) in the context of learning higher order symbolic relations and in (Donnarumma et al., 2015) as the key ingredient of an architecture for prefrontal cognitive control. Schmidhuber (1992) proposed a related meta-learning idea, whereby one learns the parameters of a slowly changing network, which in turn generates context dependent weight changes for a second rapidly changing network. These approaches have only been demonstrated in very limited settings. In cognitive science, several theories of brain areas controlling other brain parts so as to carry out multiple tasks have been proposed; see for example Schneider & Chein (2003); Anderson (2010) and Donnarumma et al.",
      "startOffset" : 116,
      "endOffset" : 701
    }, {
      "referenceID" : 3,
      "context" : "This idea appeared in (Sutskever & Hinton, 2009) in the context of learning higher order symbolic relations and in (Donnarumma et al., 2015) as the key ingredient of an architecture for prefrontal cognitive control. Schmidhuber (1992) proposed a related meta-learning idea, whereby one learns the parameters of a slowly changing network, which in turn generates context dependent weight changes for a second rapidly changing network. These approaches have only been demonstrated in very limited settings. In cognitive science, several theories of brain areas controlling other brain parts so as to carry out multiple tasks have been proposed; see for example Schneider & Chein (2003); Anderson (2010) and Donnarumma et al. (2012). Recently, Graves et al.",
      "startOffset" : 116,
      "endOffset" : 730
    }, {
      "referenceID" : 3,
      "context" : "This idea appeared in (Sutskever & Hinton, 2009) in the context of learning higher order symbolic relations and in (Donnarumma et al., 2015) as the key ingredient of an architecture for prefrontal cognitive control. Schmidhuber (1992) proposed a related meta-learning idea, whereby one learns the parameters of a slowly changing network, which in turn generates context dependent weight changes for a second rapidly changing network. These approaches have only been demonstrated in very limited settings. In cognitive science, several theories of brain areas controlling other brain parts so as to carry out multiple tasks have been proposed; see for example Schneider & Chein (2003); Anderson (2010) and Donnarumma et al. (2012). Recently, Graves et al. (2014) advanced a neural Turing machine that is capable of learning and executing simple programs such as repeat copying, simple priority sorting and associative recall.",
      "startOffset" : 116,
      "endOffset" : 762
    }, {
      "referenceID" : 3,
      "context" : "This idea appeared in (Sutskever & Hinton, 2009) in the context of learning higher order symbolic relations and in (Donnarumma et al., 2015) as the key ingredient of an architecture for prefrontal cognitive control. Schmidhuber (1992) proposed a related meta-learning idea, whereby one learns the parameters of a slowly changing network, which in turn generates context dependent weight changes for a second rapidly changing network. These approaches have only been demonstrated in very limited settings. In cognitive science, several theories of brain areas controlling other brain parts so as to carry out multiple tasks have been proposed; see for example Schneider & Chein (2003); Anderson (2010) and Donnarumma et al. (2012). Recently, Graves et al. (2014) advanced a neural Turing machine that is capable of learning and executing simple programs such as repeat copying, simple priority sorting and associative recall. Instead of using input and output pairs, our model is trained on program execution traces at varying levels of abstraction. In exchange for this richer supervision, we get the benefit of learning compositionality of programs, and also data efficient training of complex programs. Related problems have been studied in the literature on hierarchical reinforcement learning (e.g., Dietterich (2000); Andre & Russell (2001); Sutton et al.",
      "startOffset" : 116,
      "endOffset" : 1322
    }, {
      "referenceID" : 3,
      "context" : "This idea appeared in (Sutskever & Hinton, 2009) in the context of learning higher order symbolic relations and in (Donnarumma et al., 2015) as the key ingredient of an architecture for prefrontal cognitive control. Schmidhuber (1992) proposed a related meta-learning idea, whereby one learns the parameters of a slowly changing network, which in turn generates context dependent weight changes for a second rapidly changing network. These approaches have only been demonstrated in very limited settings. In cognitive science, several theories of brain areas controlling other brain parts so as to carry out multiple tasks have been proposed; see for example Schneider & Chein (2003); Anderson (2010) and Donnarumma et al. (2012). Recently, Graves et al. (2014) advanced a neural Turing machine that is capable of learning and executing simple programs such as repeat copying, simple priority sorting and associative recall. Instead of using input and output pairs, our model is trained on program execution traces at varying levels of abstraction. In exchange for this richer supervision, we get the benefit of learning compositionality of programs, and also data efficient training of complex programs. Related problems have been studied in the literature on hierarchical reinforcement learning (e.g., Dietterich (2000); Andre & Russell (2001); Sutton et al.",
      "startOffset" : 116,
      "endOffset" : 1346
    }, {
      "referenceID" : 3,
      "context" : "This idea appeared in (Sutskever & Hinton, 2009) in the context of learning higher order symbolic relations and in (Donnarumma et al., 2015) as the key ingredient of an architecture for prefrontal cognitive control. Schmidhuber (1992) proposed a related meta-learning idea, whereby one learns the parameters of a slowly changing network, which in turn generates context dependent weight changes for a second rapidly changing network. These approaches have only been demonstrated in very limited settings. In cognitive science, several theories of brain areas controlling other brain parts so as to carry out multiple tasks have been proposed; see for example Schneider & Chein (2003); Anderson (2010) and Donnarumma et al. (2012). Recently, Graves et al. (2014) advanced a neural Turing machine that is capable of learning and executing simple programs such as repeat copying, simple priority sorting and associative recall. Instead of using input and output pairs, our model is trained on program execution traces at varying levels of abstraction. In exchange for this richer supervision, we get the benefit of learning compositionality of programs, and also data efficient training of complex programs. Related problems have been studied in the literature on hierarchical reinforcement learning (e.g., Dietterich (2000); Andre & Russell (2001); Sutton et al. (1999) and Schaul et al.",
      "startOffset" : 116,
      "endOffset" : 1368
    }, {
      "referenceID" : 3,
      "context" : "This idea appeared in (Sutskever & Hinton, 2009) in the context of learning higher order symbolic relations and in (Donnarumma et al., 2015) as the key ingredient of an architecture for prefrontal cognitive control. Schmidhuber (1992) proposed a related meta-learning idea, whereby one learns the parameters of a slowly changing network, which in turn generates context dependent weight changes for a second rapidly changing network. These approaches have only been demonstrated in very limited settings. In cognitive science, several theories of brain areas controlling other brain parts so as to carry out multiple tasks have been proposed; see for example Schneider & Chein (2003); Anderson (2010) and Donnarumma et al. (2012). Recently, Graves et al. (2014) advanced a neural Turing machine that is capable of learning and executing simple programs such as repeat copying, simple priority sorting and associative recall. Instead of using input and output pairs, our model is trained on program execution traces at varying levels of abstraction. In exchange for this richer supervision, we get the benefit of learning compositionality of programs, and also data efficient training of complex programs. Related problems have been studied in the literature on hierarchical reinforcement learning (e.g., Dietterich (2000); Andre & Russell (2001); Sutton et al. (1999) and Schaul et al. (2015)), imitation and apprenticeship learning (e.",
      "startOffset" : 116,
      "endOffset" : 1393
    }, {
      "referenceID" : 3,
      "context" : "This idea appeared in (Sutskever & Hinton, 2009) in the context of learning higher order symbolic relations and in (Donnarumma et al., 2015) as the key ingredient of an architecture for prefrontal cognitive control. Schmidhuber (1992) proposed a related meta-learning idea, whereby one learns the parameters of a slowly changing network, which in turn generates context dependent weight changes for a second rapidly changing network. These approaches have only been demonstrated in very limited settings. In cognitive science, several theories of brain areas controlling other brain parts so as to carry out multiple tasks have been proposed; see for example Schneider & Chein (2003); Anderson (2010) and Donnarumma et al. (2012). Recently, Graves et al. (2014) advanced a neural Turing machine that is capable of learning and executing simple programs such as repeat copying, simple priority sorting and associative recall. Instead of using input and output pairs, our model is trained on program execution traces at varying levels of abstraction. In exchange for this richer supervision, we get the benefit of learning compositionality of programs, and also data efficient training of complex programs. Related problems have been studied in the literature on hierarchical reinforcement learning (e.g., Dietterich (2000); Andre & Russell (2001); Sutton et al. (1999) and Schaul et al. (2015)), imitation and apprenticeship learning (e.g., Kolter et al. (2008) and Rothkopf & Ballard (2013)) and elicitation of options through human interaction (Subramanian et al.",
      "startOffset" : 116,
      "endOffset" : 1461
    }, {
      "referenceID" : 3,
      "context" : "This idea appeared in (Sutskever & Hinton, 2009) in the context of learning higher order symbolic relations and in (Donnarumma et al., 2015) as the key ingredient of an architecture for prefrontal cognitive control. Schmidhuber (1992) proposed a related meta-learning idea, whereby one learns the parameters of a slowly changing network, which in turn generates context dependent weight changes for a second rapidly changing network. These approaches have only been demonstrated in very limited settings. In cognitive science, several theories of brain areas controlling other brain parts so as to carry out multiple tasks have been proposed; see for example Schneider & Chein (2003); Anderson (2010) and Donnarumma et al. (2012). Recently, Graves et al. (2014) advanced a neural Turing machine that is capable of learning and executing simple programs such as repeat copying, simple priority sorting and associative recall. Instead of using input and output pairs, our model is trained on program execution traces at varying levels of abstraction. In exchange for this richer supervision, we get the benefit of learning compositionality of programs, and also data efficient training of complex programs. Related problems have been studied in the literature on hierarchical reinforcement learning (e.g., Dietterich (2000); Andre & Russell (2001); Sutton et al. (1999) and Schaul et al. (2015)), imitation and apprenticeship learning (e.g., Kolter et al. (2008) and Rothkopf & Ballard (2013)) and elicitation of options through human interaction (Subramanian et al.",
      "startOffset" : 116,
      "endOffset" : 1491
    }, {
      "referenceID" : 3,
      "context" : "This idea appeared in (Sutskever & Hinton, 2009) in the context of learning higher order symbolic relations and in (Donnarumma et al., 2015) as the key ingredient of an architecture for prefrontal cognitive control. Schmidhuber (1992) proposed a related meta-learning idea, whereby one learns the parameters of a slowly changing network, which in turn generates context dependent weight changes for a second rapidly changing network. These approaches have only been demonstrated in very limited settings. In cognitive science, several theories of brain areas controlling other brain parts so as to carry out multiple tasks have been proposed; see for example Schneider & Chein (2003); Anderson (2010) and Donnarumma et al. (2012). Recently, Graves et al. (2014) advanced a neural Turing machine that is capable of learning and executing simple programs such as repeat copying, simple priority sorting and associative recall. Instead of using input and output pairs, our model is trained on program execution traces at varying levels of abstraction. In exchange for this richer supervision, we get the benefit of learning compositionality of programs, and also data efficient training of complex programs. Related problems have been studied in the literature on hierarchical reinforcement learning (e.g., Dietterich (2000); Andre & Russell (2001); Sutton et al. (1999) and Schaul et al. (2015)), imitation and apprenticeship learning (e.g., Kolter et al. (2008) and Rothkopf & Ballard (2013)) and elicitation of options through human interaction (Subramanian et al., 2011). These ideas have held great promise, but have not enjoyed significant impact. We believe the recurrent compositional neural representations proposed in this paper could help these approaches in the future, and in particular in overcoming feature engineering. This work is also closely related to program induction. Most previous work on program induction, i.e. inducing a program given example input and output pairs, has used genetic programming (Banzhaf et al., 1998) to evolve useful programs from candidate populations. There have been several recent works extending recurrent networks to solve problems not traditionally cast as simple sequence prediction. Zaremba & Sutskever (2014) trained LSTM models to read in the text of simple programs character-by-character and correctly predict the program output.",
      "startOffset" : 116,
      "endOffset" : 2262
    }, {
      "referenceID" : 3,
      "context" : "This idea appeared in (Sutskever & Hinton, 2009) in the context of learning higher order symbolic relations and in (Donnarumma et al., 2015) as the key ingredient of an architecture for prefrontal cognitive control. Schmidhuber (1992) proposed a related meta-learning idea, whereby one learns the parameters of a slowly changing network, which in turn generates context dependent weight changes for a second rapidly changing network. These approaches have only been demonstrated in very limited settings. In cognitive science, several theories of brain areas controlling other brain parts so as to carry out multiple tasks have been proposed; see for example Schneider & Chein (2003); Anderson (2010) and Donnarumma et al. (2012). Recently, Graves et al. (2014) advanced a neural Turing machine that is capable of learning and executing simple programs such as repeat copying, simple priority sorting and associative recall. Instead of using input and output pairs, our model is trained on program execution traces at varying levels of abstraction. In exchange for this richer supervision, we get the benefit of learning compositionality of programs, and also data efficient training of complex programs. Related problems have been studied in the literature on hierarchical reinforcement learning (e.g., Dietterich (2000); Andre & Russell (2001); Sutton et al. (1999) and Schaul et al. (2015)), imitation and apprenticeship learning (e.g., Kolter et al. (2008) and Rothkopf & Ballard (2013)) and elicitation of options through human interaction (Subramanian et al., 2011). These ideas have held great promise, but have not enjoyed significant impact. We believe the recurrent compositional neural representations proposed in this paper could help these approaches in the future, and in particular in overcoming feature engineering. This work is also closely related to program induction. Most previous work on program induction, i.e. inducing a program given example input and output pairs, has used genetic programming (Banzhaf et al., 1998) to evolve useful programs from candidate populations. There have been several recent works extending recurrent networks to solve problems not traditionally cast as simple sequence prediction. Zaremba & Sutskever (2014) trained LSTM models to read in the text of simple programs character-by-character and correctly predict the program output. Though not practical as a real interpreter, it is somewhat astonishing that it works. Mou et al. (2014) process program symbols to learn max-margin program embeddings with the help of parse trees.",
      "startOffset" : 116,
      "endOffset" : 2490
    }, {
      "referenceID" : 3,
      "context" : "This idea appeared in (Sutskever & Hinton, 2009) in the context of learning higher order symbolic relations and in (Donnarumma et al., 2015) as the key ingredient of an architecture for prefrontal cognitive control. Schmidhuber (1992) proposed a related meta-learning idea, whereby one learns the parameters of a slowly changing network, which in turn generates context dependent weight changes for a second rapidly changing network. These approaches have only been demonstrated in very limited settings. In cognitive science, several theories of brain areas controlling other brain parts so as to carry out multiple tasks have been proposed; see for example Schneider & Chein (2003); Anderson (2010) and Donnarumma et al. (2012). Recently, Graves et al. (2014) advanced a neural Turing machine that is capable of learning and executing simple programs such as repeat copying, simple priority sorting and associative recall. Instead of using input and output pairs, our model is trained on program execution traces at varying levels of abstraction. In exchange for this richer supervision, we get the benefit of learning compositionality of programs, and also data efficient training of complex programs. Related problems have been studied in the literature on hierarchical reinforcement learning (e.g., Dietterich (2000); Andre & Russell (2001); Sutton et al. (1999) and Schaul et al. (2015)), imitation and apprenticeship learning (e.g., Kolter et al. (2008) and Rothkopf & Ballard (2013)) and elicitation of options through human interaction (Subramanian et al., 2011). These ideas have held great promise, but have not enjoyed significant impact. We believe the recurrent compositional neural representations proposed in this paper could help these approaches in the future, and in particular in overcoming feature engineering. This work is also closely related to program induction. Most previous work on program induction, i.e. inducing a program given example input and output pairs, has used genetic programming (Banzhaf et al., 1998) to evolve useful programs from candidate populations. There have been several recent works extending recurrent networks to solve problems not traditionally cast as simple sequence prediction. Zaremba & Sutskever (2014) trained LSTM models to read in the text of simple programs character-by-character and correctly predict the program output. Though not practical as a real interpreter, it is somewhat astonishing that it works. Mou et al. (2014) process program symbols to learn max-margin program embeddings with the help of parse trees. Vinyals et al. (2015) developed Pointer Networks that generalize the notion of encoder attention in order to provide the decoder a variable-sized output space depending on the input sequence length.",
      "startOffset" : 116,
      "endOffset" : 2605
    }, {
      "referenceID" : 12,
      "context" : "The programs therefore have a more succinct representation than neural programs encoded as the full set of weights in a neural network (Rumelhart et al., 1986; Graves et al., 2014).",
      "startOffset" : 135,
      "endOffset" : 180
    }, {
      "referenceID" : 18,
      "context" : "The program and state vectors are then propagated forward through an LSTM mapping flstm as in (Sutskever et al., 2014).",
      "startOffset" : 94,
      "endOffset" : 118
    }, {
      "referenceID" : 18,
      "context" : "To simplify the notation, we have abstracted properties such as layers and cell memory in the sequence-to-sequence LSTM of equation (2); see (Sutskever et al., 2014) for details.",
      "startOffset" : 141,
      "endOffset" : 165
    }, {
      "referenceID" : 5,
      "context" : "For training data, we used renderings of the 3D car CAD models from (Fidler et al., 2012).",
      "startOffset" : 68,
      "endOffset" : 89
    }, {
      "referenceID" : 18,
      "context" : "We compare the generalization ability of our model to a flat sequence-to-sequence LSTM (Sutskever et al., 2014), using the same number of layers (2) and hidden units (256).",
      "startOffset" : 87,
      "endOffset" : 111
    }, {
      "referenceID" : 10,
      "context" : "The learning of new tasks may require that the network weights change substantially, so care must be taken to avoid catastrophic forgetting(Mccloskey & Cohen, 1989; OReilly et al., 2014).",
      "startOffset" : 139,
      "endOffset" : 186
    } ],
    "year" : 2017,
    "abstractText" : "We propose the neural programmer-interpreter (NPI): a recurrent and compositional neural network that learns to represent and execute programs. NPI has three learnable components: a task-agnostic recurrent core, a persistent key-value program memory, and domain-specific encoders that enable a single NPI to operate in multiple perceptually diverse environments with distinct affordances. By learning to compose lower-level programs to express higher-level programs, NPI reduces sample complexity and increases generalization ability compared to sequence-tosequence LSTMs. The program memory allows efficient learning of additional tasks by building on existing programs. NPI can also harness the environment (e.g. a scratch pad with read-write pointers) to cache intermediate results of computation, lessening the long-term memory burden on recurrent hidden units. In this work we train the NPI with fully-supervised execution traces; each program has example sequences of calls to the immediate subprograms conditioned on the input. Rather than training on a huge number of relatively weak labels, NPI learns from a small number of rich examples. We demonstrate the capability of our model to learn several types of compositional programs: addition, sorting, and canonicalizing 3D models. Furthermore, a single NPI learns to execute these programs and all 21 associated subprograms.",
    "creator" : "LaTeX with hyperref package"
  }
}