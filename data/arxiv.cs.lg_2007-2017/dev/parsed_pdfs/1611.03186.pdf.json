{
  "name" : "1611.03186.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "SoK: Applying Machine Learning in Security - A Survey",
    "authors" : [ "Heju Jiang", "Jasvir Nagra", "Parvez Ahammad" ],
    "emails" : [ "}@instartlogic.com" ],
    "sections" : [ {
      "heading" : "1. INTRODUCTION AND MOTIVATION",
      "text" : "Since Dorothy Denning’s seminal 1987 paper on intrusion detection [1], ML and data mining(DM) have steadily gained attention in security applications. DARPA’s 1998 network intrusion detection evaluation [2], and KDD(Conference on Knowledge Discovery and Data Mining) Cup’s 1999 challenge [3, 4] have raised profile of ML in security contexts. Yet, constrained by hardware and system resources[4], largescale ML applications did not receive much attention for many years.\n∗Corresponding authors\nIn 2008, ACM Conference on Computer and Communications Security(CCS) hosted the 1st Artificial Intelligence in Security(AISec) Workshop, which has since been a dedicated venue at a top-level security conference for the intersection of ML and security. From 2008, the pace of research and publicity of ML in security started to accelerate in academic communities (section 2.3), and industry venues (e.g. Black Hat, RSA) also shifted interests. For instance, ML in security was still a topic of minority interest at Black Hat USA 2014 in August [5], but at RSA 2016 in February, the majority of vendors claimed to deploy ML in their products [6]. A part of this shift may be motivated by the sudden increase in blackswan events like the discovery of CRIME, BEAST and Heartbleed vulnerabilities. The discovery of these vulnerabilities suggest that organizations may be attacked via previously unknown classes of attacks. To defend against these types of attacks requires monitoring not just for known vectors attacks, but also for behavior suggestive of a compromised machine. The latter requires the gathering and analysis of much larger sets of data.\nAdvances in hardware and data processing capacities enabled large-scale systems. With increasing amount of data from growing numbers of information channels and devices, the analytic tools and intelligent behaviors provided by ML becomes increasingly important in security. With DARPA’s Cyber Grand Challenge final contest looming [7], research interest in ML and security is becoming even more conspicuous. Now is the crucial time to examine research works done in ML applications and security. To do so, we studied the state-of-art of ML research in security between 2008 and early 2016, and systematize this research area in 3 ways:\n1. We survey cutting-edge research on applied ML in security, and provide a high-level overview taxonomy of ML paradigms and security domains. 2. We point to research challenges that will improve, enhance, and expand our understanding, designs, and efficacy of applying ML in security. 3. We emphasize a position which treats security as a game theory problem."
    }, {
      "heading" : "2. OVERVIEW: STRUCTURE & SCOPE",
      "text" : "While we realize there are different ways to classify existing security problems based on purpose, mechanism, targeted assets, and point of flow of the attack, our SoK’s section structure is based on the “Security and Privacy” category of 2012 ACM Computing Classification System[8], which is a\nar X\niv :1\n61 1.\n03 18\n6v 1\n[ cs\n.C R\n] 1\n0 N\nov 2\n01 6\ncombination of specific use cases(e.g. malware, phishing), technique (e.g. information flow), and targeted assets(e.g. web application, proxies). We present the state-of-art ML applications in security as the following: Section 3 and Table 2 & 3 discusses Network Security1, Section 4 and Table 4 surveys Security Services, Section 5 and Table 5 specifies advances in Software & Applications Security, Section 6 and Table 6 & 7 lays out taxonomy for System Security, and Section 7 and Table 8, 9 & 10 summarizes progress since 2008 in Malware Detection, IDS, and Social Engineering. Throughout the survey, we share our frameworks for ML system designs, assumptions, and algorithm deployments in security.\nWe focus our survey on security applications and securityrelated ML and AI problems on the defense side, hence our scope excludes theories related to security such as differential privacy and privacy-preservation in ML algorithms[9, 10, 11], and excludes ML applications in side channel attacks such as [12, 13, 14]. Partly because there is already a 2013 SoK on evolution of Sybil defense[15] in online social networks(OSN), and partly because we would like to leave it as a small exercise to our readers, we excluded Sybil defense schemes in OSN as well[16, 17, 18, 19, 20]. Still with a broad base, we propose an alternative position to frame security issues, and we also recommend a taxonomy for ML applications in security use cases. Yet, we do not conclude with a terminal list of “right” or “correct” approaches or methods. We believe that the range of the applications is too wide to fit into one singular use case or analysis framework. Instead, we intend this paper as a systematic design and method overview of thinking about researching and developing ML algorithms and applications, that will guide researchers in their problem domains on an individual basis. We target our work to security researchers and practitioners, so we assume that our readers have general knowledge for key security domains and awareness of common ML algorithms, and we also define terms when needed.\nWe agree with assessment of top conferences in [21]2. We\n1All papers are listed in chronological order with the first author’s last name followed by venue acronym and year. 2We use the same conference-ranking websites to first decide our list of top-level conferences:(1)Microsoft Academic Search - Top Conferences in Security & Privacyhttp://academic.research.microsoft.com/ RankList?entitytype=3&topDomainID=2&subDomainID=2, (2)Guofei Gu - Computer Security Conference Ranking and Statistichttp://faculty.cs.tamu.edu/guofei/sec_\nsystematically went through all proceedings between 2008 and early 2016 of the top 6 network- and computer-security conferences to collect relevant papers. Because of KDD’s early and consistent publication record on ML applications in Security, and its status as a top-level venue for ML and DM applications, we also include KDD’s 2008-2015 proceedings. To demonstrate the wide-ranging research attention drawn to ML applications in security, we also added chosen selections from the workshop AISec, International Conference on Machine Learning(ICML), Neural Information Processing Systems(NIPS), and Internet Measurement Conference(IMC) papers between 2008-2015, mostly in the “Future Development” section."
    }, {
      "heading" : "2.1 ML System Designs in Security",
      "text" : "Figure 1 shows the generalization of ML system designs when applied in security, that emerged from our survey of the papers(the legend is on the figure’s bottom left). In different use cases, the system components may embody different names, but their functionalities and positions are captured in the figure. For example:\n1. Knowledge base is baseline of known normality and/or abnormality, depending on use cases, they include but are not limited to blacklist(BL), whitelist(WL), watchlist; known malware signatures, system traces, and their families; initial set of malicious web pages; existing security policies or rules, etc.. 2. Data sources are where relevant data is collected. They can be either off-line or live online data feed, e.g. malware traces collected after execution(off-line), URL stream(online). 3. Training data are labeled data which are fed to classifiers in training. They can be standard research datasets, new data(mostly from industry) labeled by human, synthetic datasets, or a mix. 4. Pre-processor and feature extractor construct features from data sources, for example: URL aggregators, graph representations, SMTP header extractions, n-gram model builders.\nconf_stat.htm, and (3) Jianying Zhou - Top Crypto and Security Conferences Ranking http://icsd.i2r.a-star. edu.sg/staff/jianying/conference-ranking.html. All 3 rankings have the same top 6, and because Crypto and Eurocrypt do not have papers within our focus, we decided on these 4: ACM CCS, IEEE S&P(hereafter “SP”), NDSS, and USENIX Security(hereafter “Sec” or “USENIX”)\nDynamic analyzer and static analyzer are used most often in malware-related ML tasks, and human feedback loop is added when the system’s design intends to be semi-supervised or human-in-the-loop(HITL)."
    }, {
      "heading" : "2.2 ML Paradigms in Security Problems",
      "text" : "Table 1 shows a matrix with rows indicating different ways of classifying the security problems, and the columns showing well-understood ML paradigms. Based on the threat models and modeling purposes presented in the papers, we qualitatively group the attacker into three groups. If there are multiple attacker types in one section, the section’s numbering appears multiple times accordingly.\n1. Passive attackers make no attempt to evade detections; their behaviors fit into descriptions of the threat models. 2. Semi-aggressive attackers have knowledge of the detectors, and only attempt to evade detections. 3. Active attackers do not only have knowledge of the detectors and attempt to evade detections, but also actively try to poison, mislead, or thwart detection. 4. Knowledge of attackers, is the information in at least one of the five aspects: the learning algorithms themselves, the algorithms’ feature spaces, the algorithm’s parameters, training and evaluation data - regardless of being labeled or not - used by the algorithms, and decision feedback given by the algorithms [23, 24, 25].\nInfluenced by [22, 23], we extend their definitions, and qualitatively categorize attackers’ primary purpose as to compromise confidentiality, availability or integrity of legitimate systems, services, and users.\n1. Attacks on confidentiality compromise the confidential or secret information of systems, services, or users (e.g. password crackers). 2. Attacks on availability make systems and services unusable with unwanted information, requests, or many errors in defense schemes (e.g. DDoS, spam). 3. Attacks on integrity masquerade maliciously intentions as benign intentions in systems, services, and users (e.g. malware).\nWe also define ML paradigms shown in the matrix:\n1. Supervised learning uses labeled data for training. 2. Semi-supervised learning uses both labeled and un-\nlabeled data for training. 3. Unsupervised learning has no labeled data available\nfor training. 4. Human-in-the-loop(HITL) learning incorporates ac-\ntive human feedback to algorithm’s decisions into the knowledge base and/or algorithms.\n5. Game Theory(GT)-based learning considers learning as a series of strategic interactions between the model learner and actors with conflicting goals. The actors can be data generators, feature generators, chaotic human actors, or a combination[23, 24, 26, 27, 28].\nFor ”Means of Attacks” in Table 1, server, network, and user are straightforward and intuitive, so here we only describe “client app” and “client machine”. Client app is any browser-based means of attack on any client device, and client machine is any non-browser-based means of attack on any client device.\nAs shown in Table 1, the majority of surveyed papers in different security domains use supervised learning to deal with passive or semi-aggressive attackers. However, the core requirement of supervised learning - labeled data - is not always viable or easy to obtain, and authors have repeatedly written about the difficulty of obtaining labeled data for training. Based on this observation, we conclude that exploring semi-supervised and unsupervised learning approaches would expand the research foundation of ML applications in security domains, because semi-supervised and unsupervised learning can utilize unlabeled datasets which had not been used by supervised learning approaches before.\nMoreover, during our survey, we realized that many ML applications in security assume that training and testing data come from the same distribution (in statistical terms, this is the assumption of stationarity). However, in the real world, it is highly unlikely that data are stationary, let alone that the data could very well be generated by an adversarial data generator producing training and/or testing data sets, as the case in [26], or simply be generated responding to specific\nmodels as in [29]. Our observation from the comprehensive survey confirmed [23]’s statement, and we propose that GT-based learning approaches and HITL learning system designs should be explored more, in order to design more efficient security defense mechanisms to deal with active and unpredictable adversaries. At the same time, human knowledge and judgment in HTIL should go beyond feature engineering, to providing feedback to decisions made by ML models. Some theory-leaning papers have modeled spam filtering as Bayesian games or Stackelberg games[26, 29]. Use cases in data sampling, model training with poisoned or low-confidence data have also been briefly explored in literature[27, 30, 31]."
    }, {
      "heading" : "2.3 Timeline of ML in Security",
      "text" : "Based on seminal works and establishments in notable venues, the gradually increasing levels of interest in ML research applied to Security is fairly visible. Here we gathered some milestone events:\n1. 1987: Denning published “An Intrusion Detection System” [1], first framing security as a learning problem 2. 1998: DARPA IDS design challenge[2] 3. 1999: KDD Cup IDS design challenge[3, 4] 4. 2008: CCS hosted the 1st AISec workshop. Continues\nto operate each year[9] 5. 2007, 2008: Twice, KDD hosted the International Work-\nshop on Privacy, Security, and Trust(PinKDD)[32] 6. 2010, 2012: Twice, KDD hosted Intelligence and Se-\ncurity Informatics Workshop(ISI)[33, 34] 7. 2011: “Adversarial Machine Learning”published in 4th\nAISec[23] 8. 2012: “Privacy and Cybersecurity: The Next 100 Years”\nby Landwehr et al published[35] 9. 2013: Manifesto from Dagstuhl Perspectives Workshop\npublished as “Machine Learning Methods for Computer Security” by Joseph et al [36]. 10. 2014: KDD hosted its 1st “Security & Privacy” session in the main conference program[37] 11. 2014: ICML hosted its 1st, and so far the only workshop on Learning, Security, and Privacy(LSP)[38] 12. 2016: AAAI hosted its 1st Artificial Intelligence for Cyber Security workshop(AISC)[39]"
    }, {
      "heading" : "2.4 Related Work",
      "text" : "Despite the surge of research interests and industry applications in the intersection of ML and security, few surveys or overviews were published after 2008, the watershed year of increasing interest in this particular domain. In 2013 [40]surveyed server-side web application security, [41] surveyed data mining applied to security in the cloud focusing on intrusion detection, [42] discussed an ML perspective in network anomaly detection. While they are helpful and informative, the former two are limited by their scope and perspective, and the latter serves as a textbook, hence absent the quintessential of survey - mapping the progresses and charting the state-of-art. A collection of papers in 2002 and 2012 [43] discussed applications of DM in computer security, but lacks a systematic survey on ML applications in resolving security issues. [44] briefly compared two network anomaly detection techniques, but limited in scope. [45] of 2009 conducted a comprehensive survey in anomaly detection techniques, some involving discussions of security domains. The Dagstuhl Manifesto in 2013 [36] articulated the status quo and looked to the future of ML in security, but the majority of the literature listed were published before 2008. [46] of 2010 highlighted use cases and challenges for ML in network intrusion detection, but did not incorporate a high-level review of ML in security in recent years."
    }, {
      "heading" : "3. NETWORK SECURITY",
      "text" : ""
    }, {
      "heading" : "3.1 Botnets and Honeypots",
      "text" : "Research works on botnets among our surveyed literature focuses mainly on designing systems to detect commandand-control(C&C) botnets, where many bot-infected machines are controlled and coordinated by few entities to carry out malicious activities[47, 48, 49]. Those systems need to learn decision boundaries between human and bot activities, therefore ML-based classifiers are at the core of those systems, and are often trained by labeled data in supervised learning environments. The most popular classifier is support vector machines(SVMs) with different kernels, while spatial-temporal time series analysis and probabilistic inferences are also notable techniques employed in ML-based classifiers. Topic clustering, mostly seen in natural language processing(NLP), is used to build a large-scale system to\nidentify bot queries [50]. In botnet detection literature, 3 core assumptions are widely shared:\n1. Botnet protocols are mostly C&C [47, 48, 51] 2. Individual bots within same botnets behave similarly\nand can be correlated to each other [51, 52] 3. Botnet behaviors are different and distinguishable from\nlegitimate human user, e.g. human behaviors are more complex[53, 54, 55]\nOther stronger assumptions include that bots and humans interact with different server groups [56], and content features from messages generated by bots and human are independent [53]. While classification techniques differ, WLs, BLs, hypothesis testing, and a classifier [47, 49, 53] are usual system components. Attempts have been made to abstract state machine models of network to simulate realworld network traffic and create honeypots [57]. Ground truths are often heuristic [52], labeled by human experts, or a combination - even at large scale, human labeled ground truths are used, for example in [55], game masters’ visual inspections serve as ground truth to detect bots in online games. In retrospect, the evolution of botnet detection is clear: from earlier and more straightforward uses of classification techniques such as clustering and NB, the research focus has expanded from the last step of classification, to the important preceding step of constructing suitable metrics, that measures and distinguishes bot-based and humanbased activities[50, 55]."
    }, {
      "heading" : "3.2 Proxies and DNS",
      "text" : "Classifying DNS domains that distribute or host malware, scams, and malicious content has drawn research interest especially in passive DNS analysis. There are two main approaches: reputation system[58, 59, 60] and classifier[61, 62]. Reputation system scores benign and malicious domains and DNS hosts, and a ML-based classifier learns boundaries between the two. Nonetheless, both reputation system and classifier use various decision trees, random forest(RF), näıve Bayes(NB), SVM, and clustering techniques for mostly supervised learning-based scoring and classification. Many features used are from protocols and network infrastructures, e.g. border gateway protocol(BGP) and updates, automated systems(AS), registration, zone, hosts, and public BLs. Similar to botnet detectors, variations of BL, WL, and honeypots[63] are used in similar functions as knowledge bases, while ground truths are often taken from public BLs, limited WLs, and anti-virus(AV) vendors such as McAfee and Norton [58, 59, 61]. But before any ML attempts take place, most studies would assume the following:\n1. Malicious uses of DNS are distinct and distinguishable from legitimate DNS services. 2. The data collection process - regardless of different names such as data flow, traffic recorder, or packet assembler - follows a centralized model. In other words, all the traffic/data/packets flow through certain central node or nodes to be collected.\nStronger assumptions include that AS hijackers cannot manipulate AS path before it reaches them[60], and maliciousness will trigger an accurate IP address classifier to fail[64]. Besides analyzing the status quo, [64, 65, 66] showed efforts\nto preemptively protect network measurement integrity and predict potentially malicious activities from web domains and IP address spaces."
    }, {
      "heading" : "4. SECURITY SERVICES",
      "text" : "Both offense and defense for access control, authentication, and authorization reside within the domain of Security Services. Defeating audio and visual CAPTCHAs(Completely Automated Public Turing test to tell Computers and Humans Apart)[67, 68, 68, 69, 70], cracking passwords[71, 72, 73], measuring password strengths[72, 74, 75], and uncovering anonymity[76, 77, 78, 79] are 4 major use cases. On the offense, specialized ML domains such as computer vision, signal processing, and NLP automate attacks on user authentication services i.e. textual or visual passwords and CAPTCHAs, and uncover hidden identities and services. On the defense side, entropy-based and ML-based systems calculate password strengths. Other than traditional user authentication schemes, behavioral metrics of users are also introduced. Following the generalized ML pipeline shown in Figure 1, the “classifier” is replaced by “recognition engine” in the password cracking process, and “user differentiation engine” in authentic metric engineering [80, 81]. Hence the process becomes: “Data source→ Pre-process & feature extraction→ Recognition or user differentiation engine→ Decision” for ML-based security services. A noteworthy trend to observe, is that attacks on CAPTCHAs are getting more generalized - from utilizing SVM in 2008 to attack a specific type of text CAPTCHA[67], in 2015 a generic attach approach to attack text-based CAPTCHA [70] was proposed.\nML-based attacks on textual and visual CAPTCHA typically follow the 4-step process:\n1. Segmentation: e.g. signal to noise ratio(SNR) for audio; hue, color, value(HSV) for visual [67, 69, 70, 82] 2. Signal or image representation: e.g. discrete Fourier transformation(audio)[82], letter binarization(visual) [70] 3. Feature extraction: e.g. spectro-temporal features, character strokes [67, 82] 4. Recognition: K-nearest neighbor(KNN), SVM(RBF kernel), convolutional neural networks(CNN) [68, 71]\nOn the side of password-related topics in security services, there are 2 password models: whole-string Markov models, and template-based models [73]. Concepts in statistical language modeling, such as natural language encoder and ngrams associated with Markov models(presented as directed graphs with nodes labeled by n-grams), and context-free grammars are common probabilistic foundations to build password strength meters and password crackers [71, 72, 82]."
    }, {
      "heading" : "5. SOFTWARE & APPLICATION SECURITY",
      "text" : "ML research in software and applications security mostly concentrate on web application security in our survey, and have used supervised learning to train popular classifiers such as NB and SVM to detect web-based malware and JavaScript(JS) code[83, 84], filter unwanted resources and requests such as malicious advertisements[85, 86, 87, 88], predict unwanted resources and requests(e.g. future blacklisted websites)[89, 90, 91], and quantify web application vulnerabilities[92]. While [91] explored building web application anomaly detector with scarce training data, most use\ncases follow the supervised paradigm assuming plentiful labeled data: Data source(web applications, static/dynamic analyzers)→ feature extraction(often with specific pre-filter, metrics, and de-obfuscator if needed) → classifiers trained with labeled data. Apart from this supervised setting, if a human expert’s feedback is added after classifiers’ decisions[85], it forms a semi-supervised system. Regardless of system designs, the usual assumption holds: malicious activities or actors are different from normal and benign ones likely do not change much. The knowledge bases of normality and abnormality can vary, from historical regular expression lists[86] to other publicly available detectors[84]. Graphbased algorithms[87] and image recognition[90] are both used in resource filtering, but in detecting JS malware and evasions and quantifying leaks, having suitable measurements of similarities is a significant focal point. Indeed, from [83, 84, 92], ML-based classifiers do well in finding similarities between mutated malicious code snippets, while the same code pieces could evade static or dynamic analyzer detections."
    }, {
      "heading" : "6. SYSTEM SECURITY",
      "text" : ""
    }, {
      "heading" : "6.1 Vulnerability and Policy Management",
      "text" : "As Landwehr noticed[93], ML can be applied in SPM. However, in automatic fingerprinting of operating systems(OS), C4.5 decision tree, SVM, RF, KNN - some most commonly used ML-based classifiers in security - failed to distinguish remote machine instances with coarse- and fine-grained differences, as the algorithms cannot exploit semantic knowledge of protocols or send multi-packet probes [94]. Yet by taking advantage of semantic and syntactic features, plus semi-supervised system design, [95, 96, 97] showed that SVM(optimized by sequential minimal optimization[SMO] algorithm), KNN, and NLP techniques do well in Android SPM. On the other hand, in vulnerability management, [98, 99, 100, 101, 102], clustering techniques have done well in predicting future incidents and infer vulnerability patterns in code, as well as NB, SVM, and RF in ranking risks and identifying proper permission levels. Both vulnerability management and SPM also focus on devising proper metrics for ML applications: from heuristics based on training set [100], Jaro distance [101], to outside reputation system oracles [99], metrics are needed to compare dependency graphs, string similarities, and inferred vulnerability patterns. In most use cases, because of the need for labeled data to train supervised learning systems, many systems follow the generalized training process in Figure 1: “Knowledge base → offline trainer → online or offline classifier”. When policy management decisions need feedback, a HITL design is in place where end human users’ feedback is directed to knowledge base. One distinguishing tradition in ML applications research in this domain, is a strong emphasis on measurement - selecting or engineering proper similarity or scoring metrics are often important points of discussion in research literature. From earlier uses of heuristics in clustering algorithms, to more recent semantic connectivity measurement applied in semisupervised systems, both the metrics and the system designs for vulnerability and security policy management have evolved to not only identify, but also to infer and predict future vulnerable instances.\n6.2 Information Flow and DDoS\nCompared to other security domains, ML research in information flow and DDoS focus more on evasion tactics and limits of ML systems in adversarial environments. Hence we grouped together the two sub-domains, and marked studies in Table 7 with “(IF)” and “(DDoS)” accordingly. For DDoS[27, 103, 104], the usual assumption is that patterns of attack and abuse traffic are different from normal traffic [103], but [104] challenged it by proposing an adversary who can generate attributes that look as plausible as actual attributes in benign patterns, and caused failure in ML-based automated signature generation to distinguish benign and malicious byte sequences. Then, [27] introduced GT to evaluate DDoS attack and defense in real-world. For information flow[24, 105, 106, 107], assumptions can take various forms. In PDF classifiers based on document structural features, it is malicious PDF has different document structures than good PDFs [105]; in Android privacy leak detector, it is the majority of an Android application’s semantically similar peers has similar privacy disclosure scenarios[106]. But [24] poses semi-aggressive and active attackers with some information about the data, feature sets, and/or algorithms, and then attackers successfully evade ML-based PDF classifiers. Another example is, PDF malware could be classified [105], and then a generic and automated evasion technique based on genetic programming is successfully experimented[107]. Overall, while using SVM, RF, and decision trees trained with labeled data to detect and predict DDoS and malicious information and data flows, ML applications in information flow and DDoS challenge the usual assumption of stationary adversary behaviors. From collecting local information only, to proposing a general game theory-based framework to evaluate DDoS attacks and defense, and from using static method to detect malicious PDF file to generic automated evasion, the scope of ML applications in both DDoS and IF have expanded and generalized over the years."
    }, {
      "heading" : "7. MALWARE, SOCIAL ENGINEERING & IDS",
      "text" : ""
    }, {
      "heading" : "7.1 Malware Detection and Mitigations",
      "text" : "Program-centric or system-centric, there are 3 areas that draw most ML application research attention in malware: malware detection[108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119], classifying unknown malware into families[31, 120, 121, 122, 123, 124], and auto-extract program or protocol specifications [125, 126, 127]. Realizing the signature and heuristic-based malware detectors can be evaded by obfuscation and polymorphism [118], more behavior-based matching and clustering systems and algorithms have been researched. Figure 1 already shows a generalized ML system design for malware detection and classification, and a more detailed description is below:\n1. Collect malware artifacts and samples, analyze them, execute them in a controlled virtual environment to collect traces, system calls, API calls, etc. [116, 119, 122] 3. Or, directly use information from already completed static and/or dynamic analyses.\n3We direct our readers for more details in [21], which evaluated rigor and prudence of academic research up to 2011 that rely on malware execution, and provided a guide rubric for safety for handling malware datasets\n2. Decide or devise similarity measurements between generalized binaries, system call graphs(SCG), function call graphs(FCG), etc., then extract features [120, 121, 124, 126] 3. Classify malware artifacts into families in-sample, or cluster them with known malware families. The classifiers and clustering engines are usually trained with labeled data[58, 118, 123]. Popular ones are SVM and RF for classification, and hidden Markov model(HMM) and KNN alongside different clustering techniques.\nEven in the use case of auto-extract specifications, supervised learning with labeled data is needed when behavior profiles, state machine inferences, fuzzing, and message clustering are present. Evasion techniques of detectors and poisoning of ML algorithms are also discussed, and typical evasion techniques include obfuscation, polymorphism, mimicry, and reflecting set generation[24, 113]. Malware detection and matching based on structural information and behavior profiles[118, 120, 124] show a tendency to use graph-based clustering and detection algorithms, and similarity measurement used in these algorithms have ranged from Jaccard distance to new graph-based matching metrics. While clustering techniques have been mostly used in malware detection, a nearest neighbor technique is explored to evade malware detection."
    }, {
      "heading" : "7.2 Social Engineering: Phishing, Malicious Content and Behaviors",
      "text" : "Spams, malicious webpages and URLs that redirect or mislead un-suspecting users to malware, scams, or adult content [66] is perhaps as old as civilian use of the Internet. Research literature mostly focus on 3 major areas: detecting phishing malicious URLs[128, 129, 130, 131, 132, 133], filtering spam or fraudulent content [26, 29, 134, 135, 136, 137, 138, 139, 140, 141], and detecting malicious user account behaviors[142, 143, 144, 145, 146]. Moreover, because phishing 4 is a classic social engineering tactic, it is often the gateway of many studies to detect malicious URLs, spam, and fraudulent content. To identify malicious URLs, MLbased classifiers draw features from webpage content(lexical, visual, etc.), URL lexical features, redirect paths, host-based features, or some combinations of them. Such classifiers usually act in conjunction with knowledge bases which are usually in-browser URL BLs or from web service providers. If the classifier is fed with URL-based features, it is common to set an URL aggregator as a pre-processor before extracting features. Mostly using supervised learning paradigm, NB, SVM with different kernels, and LR are popular ML classifiers for filtering spam and phishing. Meanwhile, GTbased learning to deal with active attackers is also evaluated in spam filtering. [26] evaluates a Bayesian game model where the defense is not fully informed of the attacker’s objectives and the active adversary can exercise control over data generation, [29] proposes a Stackelberg game where spammer reacts to the learner’s moves. Stronger assumptions also exist: for example, [139] assumes spammers’\n4We agree with [131]’s definition of phishing: “without permission, alleges to act on behalf of a third party with the intention of confusing viewers into performing an action with which the viewer would only trust a true agent of the third party”\nphone blocks follow a beta distribution as conjugate prior for Bernoulli and binomial distribution. Another social engineering tactic is spoofing identities with fake or compromised user accounts, and detection of such malicious behaviors utilize features from user profiles, spatial-, temporal-, and spatial-temporal patterns, and user profiles are used in particular to construct normality. Graph representation and trust propagation models are also deployed to distinguish genuine and malicious accounts with different behavior and representations[144, 145, 146]. Tracing the chronology of applying ML to defend against social engineering, one trend is clear: while content-, lexical-, and syntactic-based features are still being widely used, constructing graph representations and exploring temporal patterns of redirect paths, events, accounts, and behaviors have been on the rise as feature spaces for ML applications in defend against social engineering efforts. Accordingly, the ML techniques have also changed from different classification schemes to graphic models. It is also noteworthy that in [29, 146], addressing adversarial environments’ challenges to ML systems is elaborated as primary research areas, instead of a short discussion point."
    }, {
      "heading" : "7.3 IDS",
      "text" : "From feature sets to algorithms and systems, IDS has been extensively studied. However, as [147] cautioned, ML can be easily conflated with anomaly detection. While both are applied to build IDS, important difference is that ML aims to generalize expert-defined distinctions, but anomaly detection focuses on finding unusual patterns, while attacks are not necessarily anomalous. For example, [148] distinguished n-gram model’s different use cases: anomaly detection uses it to construct normality(hence more appropriate when no attack is available for learning), and ML classifiers learn to discriminate between benign and malicious n-grams(hence more appropriate when more labeled data is present). Since 2008, works at top venues have added to the rigor for ML applications in IDS. For example, a common assumption of IDS is: Anomalous or malicious behaviors or traffic flows are fundamentally different from normal ones, but [147] challenges the assumption by studying low-cardinality intrusions where attackers don’t send a large number of probes. To address adversarial learning environment and minimal labels in training data, semisupervised paradigms, especially active learning, are also used[147, 149]. Heterogeneous designs of IDS in different use cases give rise to many ad-hoc evaluations in research works, and a reproducibility and comparison framework was proposed to address the issue[150]. Meanwhile, techniques such as graph-based community detection[151], time series-based methods[152, 153], and generalized support vector data description in cyber-physical system and adversarial environment for auto-feature selection[154], have also emerged. Although they carry different assumptions of normality and feature representations, the supervised ML system design remains largely the same. Besides the fact the more techniques and use cases have been proposed, the focus of research in IDS had evolved from discovering new techniques and use cases, to rigorously evaluating fundamental assumptions and workflows of IDS. For example, while feature selection has stayed as a major component, there are re-examination of assumptions and measurements on what constitutes normality and abnormality[151], alternative to more easily acquire\ndata and use low-confidence data for ML systems[149], and proposal on validating reproducibility of results from different settings[150]."
    }, {
      "heading" : "8. FUTURE DEVELOPMENT",
      "text" : "One key goal of our SoK survey is to help researchers look into the future. ML applications in security domains are attracting academic research attention as well as industrial interest, and this presents a valuable opportunity for researchers to navigate the landscapes between ML theories and security applications. There are also opportunities to explore if there are some types of ML paradigms that are especially well suited to particular security problems. Apart from highlighting that 1) semi-supervised and unsupervised ML paradigms are more effective in utilizing unlabeled data, hence ease the difficulty of obtaining labeled data, and 2) GT-based ML paradigms and HITL ML system designs will become more influential in dealing with semi-aggressive and aggressive attackers, we also share the following seven speculations of future trends, based on our current SoK.\n1. Metric Learning: Measurement has become more and more conspicuous for ML research in security, mostly in similarity measurement for clustering algorithms[53, 65, 76, 120]. Proper measurements and metrics are also used to construct ground truths to evaluate ML-based classifiers, and also have important roles in feature engineering[55, 140, 155, 156]. Given the ubiquitous presence of metrics and the complex nature of constructing them, ML applications in security will benefit much from metric learning. 2. NLP: Malicious content, spam, and malware analysis and detections have used tools from statistical language modeling(e.g. n-gram-based representation for strings in code and HTTP request)[62, 74, 116, 138, 141, 157], As textual information explodes, NLP will become more widely used beyond source filtering and clustering e.g. [57] use n-gram models to infer state machines of protocols. 3. Upstream movement of ML in security defense designs. In malware detection and classifications, behaviorand signature-based malware classifiers have used inputs from static and dynamic binary analysis as features[110, 111, 123, 126], and [112] already shows RNN can be applied to automatically recognize functions in binary analysis. We also see ML algorithms applied in vulnerability, device, and security policy management, DDoS mitigation, information flow quantifications, and network infrastructure[103, 106, 116, 141]. Hence, it is reasonable to expect that more ML systems and algorithms will move upstream in more security domains. 4. Scalability: With increasing amount of data from growing numbers of information channels and devices, scale of ML-based security defenses will become a more important aspect in researching ML applications in security [12, 50, 109, 131]. As a result, large-scale systems will enable distributed graph algorithms in malware analysis, AS path hijacker tracing, cyberphysical system fault correlation, etc..[49, 56, 72, 96, 106, 118] 5. Specialized probabilistic models will be applied beyond the context of classifiers, e.g. access control[81]. 6. High FP rates have always been a concern for system\narchitects and algorithm researchers [86, 150]. Reducing FP rates will grow from an ad-hoc component in various system designs, to independent formal frameworks, algorithms, and system designs. 7. Privacy enforcement was framed as a learning problem recently in [158], in the light of many publications on privacy-preservation in ML algorithms, and privacy enhancement by probabilistic models[11, 159, 160, 161, 162, 163]. This new trend will become more prominent."
    }, {
      "heading" : "9. CONCLUSION",
      "text" : "In this paper, we analyzed ML applications in security domains by surveying literature from top venues of our field between 2008 and early 2016. We attempted to bring clarity to a complex field with intersecting expertises by identifying common use cases, generalized system designs, common assumptions, metrics or features, and ML algorithms applied in different security domains. We constructed a matrix showing the intersections of ML paradigms and three different taxonomy structures to classify security domains, and show that while much research has been done, explorations in GT-based ML paradigms and HITL ML system designs are still much desired (and under-utilized) in the context of active attackers. We point out 7 promising areas of research based on our observations, and argue that while ML applications can be powerful in security domains, it is critical to match the ML system designs with the underlying constraints of the security applications appropriately."
    }, {
      "heading" : "10. ACKNOWLEDGMENT",
      "text" : "We would like to thank Megan Yahya, Krishnaprasad Vikram, and Scott Algatt for their time and valuable feedback.\nAPPENDIX References\n[1] D. E. Denning, “An intrusion-detection model,” IEEE Trans. Softw. Eng., 1987.\n[2] (1998) Darpa intrusion detection evaluation. [Online]. Available: http://www.ll.mit.edu/ideval/data/\n[3] (1999) Kdd cup 1999 data. [Online]. Available: http:// kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\n[4] C. Elkan. (1999) Results of the kdd’99 classifier learning contest. [Online]. Available: http://cseweb. ucsd.edu/˜elkan/clresults.html\n[5] Black hat usa 2014. [Online]. Available: https: //www.blackhat.com/us-14/training\n[6] Rsa 2016. [Online]. Available: https://www. rsaconference.com/events/us16/expo-sponsors/ exhibitor-list\n[7] (2016) Darpa cyber grand challenge. [Online]. Available: http://www.cybergrandchallenge.com\n[8] ACM. (2012) Acm 2012 computing classification system. [Online]. Available: https://www.acm.org/ publications/class-2012\n[9] (2016) Aisec 2016 topic classifications. [Online]. Available: http://teamcore.usc.edu/people/arunesh/ AISec2016/call-for-papers.html\n[10] R. Chow, P. Golle, and J. Staddon, “Detecting privacy leaks using corpus-based association rules,” in KDD 2008.\n[34] ——. (2012) 2012 kdd intelligence and security informatics workshop. [Online]. Available: http: //cci.drexel.edu/isi/isi-kdd2012/\n[35] C. Landwehr, D. Boneh, J. C. Mitchell, S. M. Bellovin, S. Landau, and M. E. Lesk, “Privacy and cybersecurity: The next 100 years,” Proceedings of the IEEE, vol. 100, no. Special Centennial Issue, 2012.\n[36] A. D. Joseph, P. Laskov, F. Roli, J. Tygar, and B. Nelson, “Dagstuhl Manifestos, Volume 3, Issue 1, January to December 2013, Complete Issue,” Dagstuhl Manifestos, 2014.\n[37] KDD. (2014) 2014 kdd security and privacy session. [Online]. Available: http://dl.acm.org/citation.cfm? id=2623330\n[38] ICML. (2014) Icml 2014 workshop on learning, security and privacy. [Online]. Available: https: //sites.google.com/site/learnsecprivacy2014/\n[39] AAAI. Aaai 2016 workshop aisc. [Online]. Available: http://www-bcf.usc.edu/˜aruneshs/AICS2016/ index.html\n[40] X. Li and Y. Xue, “A survey on server-side approaches to securing web applications,” ACM Comput. Surv., vol. 46, no. 4, April 2014.\n[41] P. Aggarwal and M. Chaturvedi, “Application of data mining techniques for information security in a cloud: A survey,” International Journal of Computer Applications, vol. 80, no. 13, 2013.\n[42] D. Bhattacharyya and J. Kalita, Network Anomaly Detection: A Machine Learning Perspective. CRC Press, 2013.\n[43] D. Barbará and S. Jajodia, Applications of Data Mining in Computer Security, ser. Advances in Information Security, 2012, 2012.\n[44] T. Ahmed, B. Oreshkin, and M. Coates, “Machine learning approaches to network anomaly detection,” in Proceedings of the 2nd USENIX workshop on Tackling computer systems problems with machine learning techniques, 2007.\n[45] V. Chandola, A. Banerjee, and V. Kumar, “Anomaly detection: A survey,” ACM Comput. Surv., vol. 41, no. 3, 2009.\n[46] R. Sommer and V. Paxson, “Outside the closed world: On using machine learning for network intrusion detection,” in SP 2010.\n[47] G. Gu, J. Zhang, and W. Lee, “Botsniffer: Detecting botnet command and control channels in network traffic,” in NDSS 2008.\n[48] G. Jacob, R. Hund, C. Kruegel, and T. Holz, “Jackstraws: Picking command and control connections from bot traffic,” in USENIX Security’11.\n[49] S. Nagaraja, P. Mittal, C.-Y. Hong, M. Caesar, and N. Borisov, “Botgrep: Finding p2p bots with structured graph analysis,” in USENIX Security’10.\n[50] J. Zhang, Y. Xie, F. Yu, D. Soukal, and W. Lee, “Intention and origination: An inside look at large-scale bot queries,” in NDSS 2013.\n[51] G. Jacob, E. Kirda, C. Kruegel, and G. Vigna, “Pubcrawl: Protecting users and businesses from crawlers,” in USENIX Security’12.\n[52] G. Gu, R. Perdisci, J. Zhang, W. Lee et al., “Botminer: Clustering analysis of network traffic for protocol-and structure-independent botnet detection,” in USENIX Security 2009.\n[53] S. Gianvecchio, M. Xie, Z. Wu, and H. Wang, “Measurement and classification of humans and bots in internet chat,” in USENIX security 2008.\n[54] X. Hu, M. Knysz, and K. G. Shin, “Rb-seeker: Autodetection of redirection botnets.” in NDSS 2009.\n[55] E. Lee, J. Woo, H. Kim, A. Mohaisen, and H. K. Kim, “You are a game bot!: uncovering game bots in mmorpgs via self-similarity in the wild,” in NDSS 2016.\n[56] F. Chen, S. Ranjan, and P.-N. Tan, “Detecting bots via incremental ls-svm learning with dynamic feature adaptation,” in KDD 2011.\n[57] T. Krueger, H. Gascon, N. Krämer, and K. Rieck, “Learning stateful models for network honeypots,” in AISec 2012.\n[58] M. Antonakakis, R. Perdisci, D. Dagon, W. Lee, and N. Feamster, “Building a dynamic reputation system for dns.” in USENIX Security’10.\n[59] M. Antonakakis, R. Perdisci, W. Lee, N. Vasiloglou II, and D. Dagon, “Detecting malware domains at the upper dns hierarchy.” in USENIX Security’11.\n[60] T. Qiu, L. Ji, D. Pei, J. Wang, J. J. Xu, and H. Ballani, “Locating prefix hijackers using lock,” in USENIX Security 2009.\n[61] L. Bilge, E. Kirda, C. Kruegel, and M. Balduzzi, “Exposure: Finding malicious domains using passive dns analysis,” in NDSS 2011.\n[62] Y. Song, A. D. Keromytis, and S. J. Stolfo, “Spectrogram: A mixture-of-markov-chains model for anomaly detection in web traffic.” in NDSS 2009.\n[63] S. Small, J. Mason, F. Monrose, N. Provos, and A. Stubblefield, “To catch a predator: A natural language approach for eliciting malicious payloads,” in USENIX Security 2008.\n[64] S. Venkataraman, D. Brumley, S. Sen, and O. Spatscheck, “Automatically inferring the evolution of malicious activity on the internet.”\n[65] O. Fatemieh, A. Farhadi, R. Chandra, and C. A. Gunter, “Using classification to protect the integrity of spectrum measurements in white space networks.” in NDSS 2011.\n[66] T. Vissers, W. Joosen, and N. Nikiforakis, “Parking sensors: Analyzing and detecting parked domains.” in NDSS 2015.\n[67] P. Golle, “Machine learning attacks against the asirra captcha,” in CCS 2008.\n[68] E. Bursztein, M. Martin, and J. Mitchell, “Text-based captcha strengths and weaknesses,” in CCS 2011.\n[69] H. Gao, W. Wang, J. Qi, X. Wang, X. Liu, and J. Yan, “The robustness of hollow captchas,” in CCS 2013.\n[70] H. Gao, J. Yan, F. Cao, Z. Zhang, L. Lei, M. Tang, P. Zhang, X. Zhou, X. Wang, and J. Li, “A simple generic attack on text captchas,” in NDSS 2016.\n[71] M. Weir, S. Aggarwal, B. De Medeiros, and B. Glodek, “Password cracking using probabilistic context-free grammars,” in SP 2009.\n[72] R. Chatterjee, J. Bonneau, A. Juels, and T. Ristenpart, “Cracking-resistant password vaults using natural language encoders,” in SP 2015.\n[73] J. Ma, W. Yang, M. Luo, and N. Li, “A study of probabilistic password models,” in SP 2014.\n[74] C. Castelluccia, M. Dürmuth, and D. Perito, “Adaptive password-strength meters from markov models.” in NDSS 2012.\n[75] P. G. Kelley, S. Komanduri, M. L. Mazurek, R. Shay, T. Vidas, L. Bauer, N. Christin, L. F. Cranor, and J. Lopez, “Guess again (and again and again): Measuring password strength by simulating passwordcracking algorithms,” in SP 2012.\n[76] S. Zander and S. J. Murdoch,“An improved clock-skew measurement technique for revealing hidden services,” in USENIX Security 2008.\n[77] S. Afroz, A. C. Islam, A. Stolerman, R. Greenstadt, and D. McCoy, “Doppelgänger finder: Taking stylometry to the underground,” in SP 2014.\n[78] A. Narayanan, H. Paskov, N. Z. Gong, J. Bethencourt, E. Stefanov, E. C. R. Shin, and D. Song, “On the feasibility of internet-scale author identification,” in SP 2012.\n[79] K. P. Dyer, S. E. Coull, T. Ristenpart, and T. Shrimpton, “Peek-a-boo, i still see you: Why efficient traffic analysis countermeasures fail,” in SP 2012.\n[80] N. Zheng, A. Paloski, and H. Wang, “An efficient user verification system via mouse movements,” in CCS 2011.\n[81] M. Frank, D. Basin, and J. M. Buhmann, “A class of probabilistic models for role engineering,” in CCS 2008.\n[82] E. Bursztein, R. Beauxis, H. Paskov, D. Perito, C. Fabry, and J. Mitchell, “The failure of noise-based non-continuous audio captchas,” in SP 2011.\n[83] C. Curtsinger, B. Livshits, B. Zorn, and C. Seifert, “Zozzle: Fast and precise in-browser javascript malware detection,” in USENIX Security’11.\n[84] A. Kapravelos, Y. Shoshitaishvili, M. Cova, C. Kruegel, and G. Vigna, “Revolver: An automated approach to the detection of evasive web-based malware,” in USENIX Security 2013.\n[85] D. Sculley, M. E. Otey, M. Pohl, B. Spitznagel, J. Hainsworth, and Y. Zhou, “Detecting adversarial advertisements in the wild,” in KDD 2011.\n[86] S. Bhagavatula, C. Dunn, C. Kanich, M. Gupta, and B. Ziebart, “Leveraging machine learning to improve unwanted resource filtering,” in AISec 2014.\n[87] J. Zhang, P. A. Porras, and J. Ullrich, “Highly predictive blacklisting,” in USENIX Security 2008.\n[88] L. Lu, R. Perdisci, and W. Lee, “Surf: detecting and measuring search poisoning,” in CCS 2011.\n[89] K. Soska and N. Christin, “Automatically detecting vulnerable websites before they turn malicious,” in USENIX Security 2014.\n[90] K. Borgolte, C. Kruegel, and G. Vigna, “Meerkat: Detecting website defacements through image-based object recognition,” in USENIX Security 2015.\n[91] W. K. Robertson, F. Maggi, C. Kruegel, and G. Vigna, “Effective anomaly detection with scarce training data.” in NDSS 2010.\n[92] P. Chapman and D. Evans, “Automated black-box detection of side-channel vulnerabilities in web applications,” in CCS 2011.\n[93] C. Landwehr, “Cyber security and artificial intelligence: From fixing the plumbing to smart water,” in AISec 2008.\n[94] D. W. Richardson, S. D. Gribble, and T. Kohno, “The limits of automatic os fingerprint generation,” in AISec 2010.\n[95] S. Rasthofer, S. Arzt, and E. Bodden, “A machinelearning approach for classifying and categorizing android sources and sinks.” in NDSS 2014.\n[96] R. Pandita, X. Xiao, W. Yang, W. Enck, and T. Xie, “Whyper: Towards automating risk assessment of mobile applications,” in USENIX Security 2013.\n[97] R. Wang, W. Enck, D. Reeves, X. Zhang, P. Ning, D. Xu, W. Zhou, and A. M. Azab, “Easeandroid: Automatic policy analysis and refinement for security enhanced android via large-scale semi-supervised learning,” in USENIX Security 2015.\n[98] A. A. Makanju, A. N. Zincir-Heywood, and E. E. Milios, “Clustering event logs using iterative partitioning,” in KDD 2009.\n[99] M. Bozorgi, L. K. Saul, S. Savage, and G. M. Voelker, “Beyond heuristics: learning to classify vulnerabilities and predict exploits,” in KDD 2010.\n[100] H. Peng, C. Gates, B. Sarma, N. Li, Y. Qi, R. Potharaju, C. Nita-Rotaru, and I. Molloy, “Using probabilistic generative models for ranking risks of android apps,” in CCS 2012.\n[101] F. Yamaguchi, A. Maier, H. Gascon, and K. Rieck, “Automatic inference of search patterns for taint-style vulnerabilities,” in SP 2015.\n[102] Y. Liu, A. Sarabi, J. Zhang, P. Naghizadeh, M. Karir, M. Bailey, and M. Liu, “Cloudy with a chance of breach: Forecasting cyber security incidents,” in USENIX Security 2015.\n[103] J. L. Berral, N. Poggi, J. Alonso, R. Gavalda, J. Torres, and M. Parashar, “Adaptive distributed mechanism against flooding network attacks based on machine learning,” in AISec 2008.\n[104] S. Venkataraman, A. Blum, and D. Song, “Limits of learning-based signature generation with adversaries,” in NDSS 2008.\n[105] N. Šrndic and P. Laskov, “Detection of malicious pdf files based on hierarchical document structure,” in NDSS 2013.\n[106] K. Lu, Z. Li, V. P. Kemerlis, Z. Wu, L. Lu, C. Zheng, Z. Qian, W. Lee, and G. Jiang, “Checking more and alerting less: Detecting privacy leakages via enhanced data-flow analysis and peer voting.” in NDSS 2015.\n[107] W. Xu, Y. Qi, and D. Evans, “Automatically evading classifiers,” in NDSS 2016.\n[108] A. Tamersoy, K. Roundy, and D. H. Chau, “Guilt by association: large scale malware detection by mining file-relation graphs,” in KDD 2014.\n[109] L. Invernizzi, S. Miskovic, R. Torres, C. Kruegel, S. Saha, G. Vigna, S.-J. Lee, and M. Mellia, “Nazca: Detecting malware distribution in large-scale networks.” in NDSS 2014.\n[110] D. Arp, M. Spreitzenbarth, M. Hubner, H. Gascon, and K. Rieck, “Drebin: Effective and explainable detection of android malware in your pocket.” in NDSS 2014.\n[111] M. Graziano, D. Canali, L. Bilge, A. Lanzi, and D. Balzarotti, “Needles in a haystack: Mining information from public dynamic analysis sandboxes for malware intelligence,” in USENIX Security 2015.\n[112] E. C. R. Shin, D. Song, and R. Moazzezi, “Recognizing functions in binaries with neural networks,” in USENIX Security 2015.\n[113] C. Smutz and A. Stavrou, “When a tree falls: Using diversity in ensemble classifiers to identify evasion in malware detectors.”\n[114] M. Antonakakis, R. Perdisci, Y. Nadji, N. Vasiloglou, S. Abu-Nimeh, W. Lee, and D. Dagon, “From throwaway traffic to bots: Detecting the rise of dga-based malware,” in USENIX Security’12.\n[115] M. S. Rahman, T.-K. Huang, H. V. Madhyastha, and M. Faloutsos, “Efficient and scalable socware detection in online social networks,” in USENIX Security’13.\n[116] A. Lanzi, D. Balzarotti, C. Kruegel, M. Christodorescu, and E. Kirda, “Accessminer: Using systemcentric models for malware protection,” in CCS 2010.\n[117] Y. Ye, T. Li, S. Zhu, W. Zhuang, E. Tas, U. Gupta, and M. Abdulhayoglu, “Combining file content and file relations for cloud based malware detection,” in KDD 2011.\n[118] C. Kolbitsch, P. M. Comparetti, C. Kruegel, E. Kirda, X. Zhou, and X. Wang, “Effective and efficient malware detection at the end host,” in USENIX Security 2009.\n[119] F. Ahmed, H. Hameed, M. Z. Shafiq, and M. Farooq, “Using spatio-temporal information in api calls with machine learning algorithms for malware detection,” in AISec 2009.\n[120] D. Kong and G. Yan, “Discriminant malware distance learning on structural information for automated malware classification,” in KDD 2013.\n[121] K. Borgolte, C. Kruegel, and G. Vigna, “Delta: automatic identification of unknown web-based infection campaigns,” in CCS 2013.\n[122] J. Jang, D. Brumley, and S. Venkataraman, “Bitshred: Feature hashing malware for scalable triage and semantic analysis,” in CCS 2011.\n[123] Y. Ye, T. Li, Y. Chen, and Q. Jiang, “Automatic malware categorization using cluster ensemble,” in KDD 2010.\n[124] U. Bayer, P. M. Comparetti, C. Hlauschek, C. Kruegel, and E. Kirda, “Scalable, behavior-based malware clustering.” in NDSS 2009.\n[125] M. Fredrikson, S. Jha, M. Christodorescu, R. Sailer, and X. Yan, in SP’10.\n[126] P. M. Comparetti, G. Wondracek, C. Kruegel, and E. Kirda, “Prospex: Protocol specification extraction,” in SP 2009.\n[127] D. Kirat and G. Vigna, “Malgene: Automatic extraction of malware analysis evasion signature,” in CCS 2015.\n[128] J. Ma, L. K. Saul, S. Savage, and G. M. Voelker, “Identifying suspicious urls: an application of large-scale online learning,” in ICML 2009.\n[129] ——, “Beyond blacklists: learning to detect malicious web sites from suspicious urls,” in KDD 2009.\n[130] A. Blum, B. Wardman, T. Solorio, and G. Warner, “Lexical feature based phishing url detection using online learning,” in AISec 2010.\n[131] C. Whittaker, B. Ryner, and M. Nazif, “Large-scale automatic classification of phishing pages.” in NDSS 2016.\n[132] S. Lee and J. Kim,“Warningbird: Detecting suspicious urls in twitter stream.” in NDSS 2012.\n[133] P. Zhao and S. C. Hoi, “Cost-sensitive online active learning with application to malicious url detection,” in KDD 2013.\n[134] K. Chatterjee, L. de Alfaro, and I. Pye, “Robust content-driven reputation,” in AISec 2008.\n[135] S. Hao, N. A. Syed, N. Feamster, A. G. Gray, and S. Krasser, “Detecting spammers with snare: Spatiotemporal network-level automatic reputation engine.” in USENIX Security 2009.\n[136] K. Thomas, C. Grier, J. Ma, V. Paxson, and D. Song, “Design and evaluation of a real-time url spam filtering service,” in SP 2011.\n[137] S. Afroz, M. Brennan, and R. Greenstadt, “Detecting hoaxes, frauds, and deception in writing style online,” in SP 2012.\n[138] L. Invernizzi, P. M. Comparetti, S. Benvenuti, C. Kruegel, M. Cova, and G. Vigna, “Evilseed: A guided approach to finding malicious web pages,” in SP 2012.\n[139] N. Jiang, Y. Jin, A. Skudlark, and Z.-L. Zhang, “Greystar: Fast and accurate detection of sms spam numbers in large cellular networks using gray phone space,” in USENIX Security’13.\n[140] Q. Zhang, D. Y. Wang, and G. M. Voelker, “Dspin: Detecting automatically spun content on the web,” in NDSS 2014.\n[141] S. Whalen, N. Boggs, and S. J. Stolfo, “Model aggregation for distributed content anomaly detection,” in AISec 2014.\n[142] M. Egele, G. Stringhini, C. Kruegel, and G. Vigna, “Compa: Detecting compromised accounts on social networks,” in NDSS 2013.\n[143] B. Viswanath, M. A. Bashir, M. Crovella, S. Guha, K. P. Gummadi, B. Krishnamurthy, and A. Mislove, “Towards detecting anomalous user behavior in online social networks,” in USENIX Security 2014.\n[144] Y. Boshmaf, D. Logothetis, G. Siganos, J. Leŕıa, J. Lorenzo, M. Ripeanu, and K. Beznosov, “Integro: Leveraging victim prediction for robust fake account detection in osns,” in NDSS 2015.\n[145] G. Stringhini, P. Mourlanne, G. Jacob, M. Egele, C. Kruegel, and G. Vigna, “Evilcohort: Detecting communit ies of malicious accounts on online services,” in USENIX Security 2015.\n[146] G. Wang, T. Wang, H. Zheng, and B. Y. Zhao, “Man vs. machine: Practical adversarial detection of malicious crowdsourcing workers,” in USENIX Security 2014.\n[147] C. T. Symons and J. M. Beaver, “Nonparametric semisupervised learning for network intrusion detection: combining performance improvements with realistic in-situ training,” in AISec 2012.\n[148] C. Wressnegger, G. Schwenk, D. Arp, and K. Rieck, “A close look on n-grams in intrusion detection: anomaly detection vs. classification,” in AISec 2013.\n[149] N. Görnitz, M. Kloft, K. Rieck, and U. Brefeld, “Active learning for network intrusion detection,” in AISec 2009.\n[150] B. Juba, C. Musco, F. Long, S. Sidiroglou-Douskos, and M. C. Rinard, “Principled sampling for anomaly detection.” in NDSS 2015.\n[151] Q. Ding, N. Katenka, P. Barford, E. Kolaczyk, and M. Crovella, “Intrusion as (anti) social communication: characterization and detection,” in KDD 2012.\n[152] S. Xie, G. Wang, S. Lin, and P. S. Yu, “Review spam detection via temporal pattern discovery,” in KDD 2012.\n[153] M. Momtazpour, J. Zhang, S. Rahman, R. Sharma, and N. Ramakrishnan, “Analyzing invariants in cyberphysical systems using latent factor regression,” in KDD 2015.\n[154] M. Kloft, U. Brefeld, P. Düessel, C. Gehl, and P. Laskov, “Automatic feature selection for anomaly detection,” in AISec 2008.\n[155] T. Holz, C. Gorecki, K. Rieck, and F. C. Freiling, “Measuring and detecting fast-flux service networks.” in NDSS 2008.\n[156] B. I. Rubinstein, B. Nelson, L. Huang, A. D. Joseph, S.-h. Lau, S. Rao, N. Taft, and J. Tygar, “Antidote: understanding and defending against poisoning of anomaly detectors,” in IMC 2009.\n[157] R. Moskovitch, N. Nissim, and Y. Elovici, “Acquisition of malicious code using active learning,” in PinKDD’08.\n[158] O. Tripp and J. Rubin, “A bayesian approach to privacy enforcement in smartphones,” in USENIX Security 2014.\n[159] Q. Wang, Z. Lin, N. Borisov, and N. Hopper, “rbridge: User reputation based tor bridge distribution with privacy preservation,” in NDSS 2013.\n[160] Y. Cao and J. Yang, “Towards making systems forget with machine unlearning,” in SP 2015.\n[161] Z. Huang, E. Ayday, J. Fellay, J.-P. Hubaux, and A. Juels, “Genoguard: Protecting genomic data against brute-force attacks,” in SP 2015.\n[162] R. Kumar, “Mining web logs: applications and challenges,” in KDD 2009.\n[163] R. Shokri, G. Theodorakopoulos, J.-Y. Le Boudec, and J.-P. Hubaux, “Quantifying location privacy,” in SP 2011.\n[164] B. A. Prakash, N. Valler, D. Andersen, M. Faloutsos, and C. Faloutsos, “Bgp-lens: Patterns and anomalies in internet routing updates,” in KDD 2009."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "The idea of applying machine learning(ML) to solve prob-<lb>lems in security domains is almost 3 decades old. As infor-<lb>mation and communications grow more ubiquitous and more<lb>data become available, many security risks arise as well as<lb>appetite to manage and mitigate such risks. Consequently,<lb>research on applying and designing ML algorithms and sys-<lb>tems for security has grown fast, ranging from intrusion de-<lb>tection systems(IDS) and malware classification to security<lb>policy management(SPM) and information leak checking. In this paper, we systematically study the methods, algorithms, and system designs in academic publications from 2008-2015 that applied ML in security domains. 98% of the surveyed papers appeared in the 6 highest-ranked academic<lb>security conferences and 1 conference known for pioneering<lb>ML applications in security. We examine the generalized<lb>system designs, underlying assumptions, measurements, and<lb>use cases in active research. Our examinations lead to 1) a<lb>taxonomy on ML paradigms and security domains for future exploration and exploitation, and 2) an agenda detailing open and upcoming challenges. Based on our survey, we also suggest a point of view that treats security as a game theory problem instead of a batch-trained ML prob-<lb>lem.",
    "creator" : "LaTeX with hyperref package"
  }
}