{
  "name" : "1311.7679.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Combination of Diverse Ranking Models for Personalized Expedia Hotel Searches",
    "authors" : [ "Xudong Liu", "Bing Xu", "Yuyu Zhang", "Qiang Yan", "Liang Pang", "Qiang Li", "Hanxiao Sun", "Bin Wang" ],
    "emails" : [ "pl8787}@gmail.com", "4yanqiang.yq@taobao.com", "NDCG@38" ],
    "sections" : [ {
      "heading" : null,
      "text" : "I. INTRODUCTION\nICDM Challenge 2013 requires learning to rank hotels to maximize purchases for given hotel queries by Expedia.com. The dataset which is provided by Expedia.com, contains hotel characteristics, location attractiveness of hotels, users aggregate purchase history and competitive OTA information for each search id-hotel pair. Hotels for each user query are assigned relevance grades as following: 5 for user purchased a room; 1 for user clicked the information of the hotel and 0 for user neither click nor book. The data is split by organizers by randomly split.\nThe training data contains 399,344 unique search lists and 9,917,530 points. The test data contains 266,230 search lists and 6,622,629 points. 25% of the test data is used for evaluating in public leaderboard and the remaining 75% is used as final private test data.1\nThe evaluation metric for this competition is Normalized Discounted Cumulative Gain (NDCG), which is commonly used in ranking[1]. According to the announced result, our approach achieved 5th on the private leaderboard with 0.53102 NDCG@38 score.\n∗ These authors equally contributed to this work. † Team advisor 1Complete leaderboard can be found at http://www.kaggle.com/c/expedia-\npersonalized-sort/leaderboard\nThe paper is organized as follows. Section 2 outlines the framework of our approaches. Section 3 discusses the preprocessing and feature engineering. Section 4 introduces the single effective models we use. Section 5 describes the ensemble experiments we use to boost our performance. Finally we conclude this paper and further discuss lessons we learned in Section 6."
    }, {
      "heading" : "II. FRAMEWORK",
      "text" : "This section introduces the architecture and softwares we used in our system. Then it discusses the self-split internal validation set from the training set, which is important in model evaluating and combining different models."
    }, {
      "heading" : "A. System Overview",
      "text" : "Our system can be divided into three parts: data infrastructure, training individual models and ensemble as shown in Figure 1. The data infrastructure is based on pandas[4]. In this step we use pandas to store data. And we do some feature engineering in this stage. The output of the data infrastructure can be in different format like numpy binary array, SVMRank[5] text format and LIBSVM[6] text format. In the second stage, we explore diverse approaches to generate various models, including logistic regression, support vector machine, random forest, gradient boosting machine, factorization machine, LambdaMART and deep neural network. In the last step, we combine all different results on the internal validation set and test set by using listwise approach, linear approach and deep neural network approach.\nWe use LIBLINEAR[7] and SVMRank[5]for pairwise logistic regression; Random Forest[8] from scikit-learn[2]; Ranking algorithms like AdaRank[9], LambdaMART[10] from RankLib2. And we also use Gradient Boosting Machine3[12], Extremely Randomized Trees4[13] from R, deep neural net-\n2http://sourceforge.net/p/lemur/wiki/RankLib/ 3http://cran.r-project.org/web/packages/gbm/index.html 4http://cran.r-project.org/web/packages/extraTrees/index.html\nar X\niv :1\n31 1.\n76 79\nv1 [\ncs .L\nG ]\n2 9\nwork implementation from PyLearn2[3] and Factorization Machine libFM5[15].\nB. Internal Validation Set\nA validation set can be used to evaluate single model without submitting the test result to the leaderboard. And validation set is very important for combining different models. Usually, the training set can be divided in the ratio 6:2:2. But in this dataset the validation result is quite robust so we just keep 10% data as the validation. We use the rule srch id%10 == 1 to generate validation set and others to be sub training set."
    }, {
      "heading" : "III. PREPROCESSING",
      "text" : ""
    }, {
      "heading" : "A. Listwise Feature for Point/Pairwise Model",
      "text" : "Some of the feature’s rank in the list is used as a single feature for each hotel choice. Here are the most important listwise ranking features.\n• price usd\n• prop starrating\n• prop location score2\nOther listwise ranking features we proposed but had no time to evaluate including: rank of exp(prop log historical price)price usd, rank of click/booking bias and so on.\nListwise features is a bridge to bring listwise information to point/pair wise models."
    }, {
      "heading" : "B. Composite Features",
      "text" : "Composite features is a method that combine two different features. For example, now we combine srch room count and srch booking window, the count window feature equals srch room count ∗ max(srch booking window) + srch booking window.\n5http://www.libfm.org/"
    }, {
      "heading" : "C. Dealing Missing Feature Values",
      "text" : "There are many missing feature values in the data such as prop location score2. We use the first quartile calculated by the country which the data point located in to represent the missing data."
    }, {
      "heading" : "D. Use 10% data",
      "text" : "We randomly sample 10% of the data by srch id to generate new training data. Using the new training data we can train a model with very small difference from the model trained by total training data."
    }, {
      "heading" : "E. Use Balanced Data",
      "text" : "Balanced data is used in training random forest and deep neural network. For there are only 4.4% positive data points among the 9.9 million data points, we choose one positive example and randomly choose one negative example. In this way we can train tree-based models with a large amount of trees in a reasonable time."
    }, {
      "heading" : "F. Split Data by prop country id",
      "text" : "Based on prop country id we split the data into 172 pieces and train independent models on each piece. This method greatly reduces time on training tree based models."
    }, {
      "heading" : "G. Use Bucket to Binarize Float Feature",
      "text" : "Bucket is a strong rounding method to binarize the float feature. The bucket algorithm can be described as Algorithm 1. By using bucket, the float features are in smaller variance.\nAlgorithm 1 Bucket(feature, bucket number) Require: An integer BUCKET > 0\ndescription = {} binary feature = zeros((feature.size, bucket number))\nfor i = 1 to bucket number do description[i] = feature.quantile(i/bucket number) end for for i = 0 to feature.size do j = 1 while feature.at(i) < description[j] do j = j + 1\nend while binary feature[i][j] = 1\nend for return binary feature"
    }, {
      "heading" : "IV. MODELS",
      "text" : ""
    }, {
      "heading" : "A. Logistic Regression",
      "text" : "As a classical model for binary classification, logistic regression is used as our initial attempt in this competition. We tried both the binary logistic regression and the multinomial logistic regression, while the former one performs obviously better. So in this part, we will only introduce our approach on binary logistic regression.\nWe firstly pre-process the data by merging the clicked and booked items within each query as positive instances, while all the left items are regarded as negative instances. With some feature engineering work, which will be discussed in details later, all the instances can be represented as a series of feature vectors. Now it becomes a standard binary classification problem, though the data here is very unbalanced since the negative instances are overwhelming. Therefore, we then adjust the weight in the cross-entropy error function (also known as negative log-likelihood function) to tackle the issue of data unbalance. The revised error function is shown as follows:\nCEE = − N∑ n=1 log[µ αI(yi=1) i · (1− µi) I(yi=0) ]\n= − N∑ n=1 [αyi logµi + (1− yi) log(1− µi)] (1)\nwhere α is the parameter of class weight decided by input data, µ is the output of sigmoid function representing the probability to be positive instance, µi is for the ith instance.\nSince the cross-entropy error above is convex, it has a unique global minimum, we use gradient descent approach to find the optimal weight vectors or model parameters. Surprisingly, although the model of logistic regression is simple and the model itself is designed for classification rather than object ranking, the performance is fairly good, which can achieve over 0.52 in terms of NDCG on public leaderboard."
    }, {
      "heading" : "B. Pairwise Logistic Regression",
      "text" : "In order to use full train set as pairwise in logistic regression model, we use FTRL-Proximal algorithm and liblinear[7] and SVMRank[5] build in function. The FTRLProximal algorithm [18], can be seen as a hybrid of FOBOS and RDA algorithms, and significantly outperforms both on a large, realworld dataset.\nWe use the whole training set to train this model with simply seven features. The featurelist is srch id, prop id, srch destination id, prop starrating, prop location score1, prop location score2, price usd. And this single FTRL model archives 0.51273 NDCG@38 on validation set."
    }, {
      "heading" : "C. Random Forest",
      "text" : "After forming a split dataset in 172 pieces by using prop country id, we balanced each piece. For each unique balanced prop country id data piece we train an independent random forest model [8] with 3200 trees. With listwise ranking features, the 172 random forest models achieve nearly 0.51 NDCG@38 score in internal validation set. Some failed cases happened in predicting for some countries, so to make the example count of test and internal validation set equal we simply combine the score with a pairwise logistic regression trained by liblinear[7] model with 0.47 NDCG@38 score on validation, then the mixture balanced random forest model achieves nearly 0.52 NDCG@38 on validation set."
    }, {
      "heading" : "D. Gradient Boosting Machine",
      "text" : "Gradient Boosting Machine(GBM) [12] is a machine learning technique for regression problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function. The gradient boosting method can also be used for classification problems by reducing them to regression with a suitable loss function.\nWe use GBM in R language with the balanced data. The most 20 relevant feature show in Table I. Fm score and Lr Score is the rank score that simple Factorization Machine and Linear Regression predicted using only visitor and query features. Date time is transform to unixstamp as a continuous feature. Feature name with the suffix cnt means one-way count of the feature in the combine of train and test set. The feature ump, price diff, starrating diff, per fee, score2ma, total fee and score1d2 is generated by the formula below.\nump = exp(prop log historical price) − price usd\nprice diff = visitor hist adr usd− price usd\nstarrating diff = visitor hist starrating − prop starrating\nper fee = price usd ∗ srch room count\nsrch adults count+ srch children count\nscore2ma = prop location score2∗srch query affinity score\ntotal fee = price usd ∗ srch room count\nscore1d2 = prop location score2 + 0.0001\nprop location score1 + 0.0001\nThis single GBM model archives 0.52477 NDCG@38 on validation set.\nWithout one-way count features the GBM model archives 0.50099 NDCG@38 on validation set which is useful in ensemble process."
    }, {
      "heading" : "E. Extreme Randomized Trees",
      "text" : "Extremely Randomized Trees (ERT) is proposed by [13].This method is similar to the Random Forests algorithm in the sense that it is based on selecting at each node a random subset of K features to decide on the split. Unlike in the Random Forests method, each tree is built from the complete learning sample (no bootstrap copying) and, most importantly, for each of the features (randomly selected at each interior node) a discretization threshold (cut-point) is selected at random to define a split, instead of choosing the best cut-point based on the local sample (as in Tree Bagging or in the Random Forests method).\nERT model using in learning to rank task is proposed by [14]. We also use this model with the same feature set as GBM model and archives 0.51699 NDCG@38 on validation set.\nTABLE II. FEATURES FOR FACTORIZATION MACHINE.\nBin ID Feature Normalized Feature Ranking Feature prop id price usd price rank\nsrch destination id prop location score1 price diff rank srch room count prop location score2 star rank\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0 50 100 150 200 250 300 350 400 450 discrete price_usd\nCTR in price CVR in price\nFig. 2. The CTR and CVR in discrete price usd scale."
    }, {
      "heading" : "F. Factorization Machine",
      "text" : "Factorization Machine [15] is widely used in Recommender System. It is also a kind of Regression model. So it can be used as a pointwise model. We did a lot of work in feature engineering and the single model archives 0.5171 NDCG@38 on validation set. As the feature engineering and the model itself bring much diverse, it works well when ensemble. Some of the features are listed in Tab. II.\nFor the model of Factorization Machine, our features are built in different ways. Some of the features are normalized (e.g. pro location score1), others are divided into bins (e.g. srch booking window). The ranking feature (e.g. price rank) means the rank value of the identical query, which works fairly well in this model. The key point of this model is that features should be used in the right way and ranking feature brings listwise information to this pointwise model."
    }, {
      "heading" : "G. LambdaMART",
      "text" : "LambdaMART model [10] [11] with CTR(clickthrough rates Eq 2) and CVR(Booking rates Eq 3) features and original features in full train set. CTR and CVR are calculate in two scale, prop id and discrete price usd (Fig. 2). This model archives 0.51149 NDCG@38 on validation set.\nCTRi = #(Clicki)\n#(Presentationi) (2)\nCV Ri = #(Bookingi)\n#(Clicki) (3)\nTwo of the team members use LambdaMART independently. The other LambdaMART model is based on normalized features, ranking features and result features. Some features are listed in Tab. III and this single LambdaMART model archives 0.5243 NDCG@38 on validation set All these features except fm score and lr score are introduced in the Factorization Machine part. Fm score and lr score are learnt by visitor and query features. These features won’t work in pairwise or listwise models as they have the same value in one query. But in this way, these features contributes its bias in pairwise or listwise models.\nTABLE III. FEATURES FOR LAMBDAMART.\nNormalized Feature Ranking Feature Result Feature prop starrating price rank fm score prop location score1 price diff rank lr score prop location score2 star rank\nDenoising Autoencoder [Hidden: 300]\nMaxout Network [Unit: 240]\nMaxout Network[Unit: 220]\nSoftmax[Output:2]\nData\nDenoising Autoencoder [Hidden: 300]\nDenoising Autoencoder [Hidden: 300]\nFig. 3. The architecture of deep neural network."
    }, {
      "heading" : "H. Deep Learning Approach",
      "text" : "Deep Learning has been successfully used in many fields such as image processing and acoustic processing. It is an open question to know whether the complex network and non-convex model can improve ranking and recommendation system. In practice, we first transform selected features into binary by using Algorithm 1. To make it simpler, we choose to train independent models for each country by using same network framework in the figure. 3. We choose Maxout network[16] because it provides much more regularization than other network layer. We discover the following situations while pretraining the denoising autoencoders:\n1) Reconstruction error fixed around a large number (in most of cases) 2) Reconstruction error gradually become small (especially for prop country id=219)\nAs the result shown before, composite feature is important. We want to use 3 layer denoising autoencoder to find at least 3-level and robust composite features. But for most of cases, the training data is not enough to make training process jump out of local minimum. And for the largest No.219 model, although pretraining looks fine, but while the training, the error rate on validation sucks to 4.3%, which is quite near the positive ratio of the data. The mean of parameters of the softmax layer stay in less than 10−4. It means this deep neural network can not accurate predict the unbalanced data. But it achieves 0.48 NDCG@38 on local validation set, which means it learns something out. After switching to balanced data, the training process is a little smooth. The error rate reduces from 50% to near 10%. But the local NDCG@38 score only improve to near 0.49. Vertical ensemble[19] helps improve a little but still can not make deep network as a strong single model. Linear embedding with other models can improve 0.004 NDCG@38 score on the local validation set of No. 219. We will provide further discussion in later chapter."
    }, {
      "heading" : "V. ENSEMBLE MODELS",
      "text" : ""
    }, {
      "heading" : "A. z-score",
      "text" : "As mentioned, we split the test set into two parts, by query. 10% of the data served for a validation set, which helps us to choose\nand combine rankers. We experimented with several linear methods to combine and the linear combining is nothing more than selecting several effective rankers and assign a set of parameters to them that the combination can achieve a better result than ever single model.\nThe simplest, also work best, method we try was z-score ensemble Eq 4. We added corresponding score in each ranker into the ensemble ranker in the beginning, but find it was not working well. For the rankers were derived from different models, the scope of their scores vary greatly. It is unreasonable to simply add them without normalization, so we employed z-score to normalize the rank scores.\nZ(x) = x− x̄ σ(x)\n= x− x̄√\n(x− x̄)2/n (4)\nWe tried two ways of z-score, global z-score and query z-score. For the reason that we just compared the scores in each query, we calculated the z-score of every each query.\nIn addition, instead of using greedy search, we tuned parameter manually. Lacking of a local test set to do cross validation and for fear of overfitting on local test set, we have to do it manually. Manually tuning could avoid excessive parameters, so, to some extent, it is a way of simple normalization, even not beautiful enough. Global zscore was the way we finally choose to ensemble and also the way achieve the highest NDCG score on private board."
    }, {
      "heading" : "B. GBM Ensemble",
      "text" : "Treat each model’s output on local test set as a feature of GBM model [12]. The target to learn is the click ground truth in local test set. In order to get a better result, we also include some significant features, such as prop location score1, prop location score2 and price usd.\nWe use 30 models and 120 trees, and it archives 0.53573 NDCG@38 on the local test set, but only 0.53053 NDCG@38 on online test set."
    }, {
      "heading" : "C. Deep Learning",
      "text" : "We want to check whether the deep composite of the models output can make any progress. By using models with local NDCG score of 0.505, 0.508, 0.510, 0.511, 0.512, 0.513,0.519, 0.521. With ReLU[17] network or Maxout network, our model achieves around 0.526 on private leaderboard. And dropout logistic regression achieves best result of 0.52729 NDCG@38 on private leaderboard. They are still weaker than other ensemble methods."
    }, {
      "heading" : "D. Listwise Ensemble",
      "text" : "Previous ensemble methods don’t involve the listwise information. So we are trying to use LambdaMART to ensemble all the models. By using z-score normalization, it achieves our final score: 0.53249 NDCG@38 on public leaderboard, and 0.53102 on the final private leaderboard."
    }, {
      "heading" : "VI. CONCLUSION AND FURTHER DISCUSSION",
      "text" : ""
    }, {
      "heading" : "A. Conclusion",
      "text" : "In this paper, we present our approaches in ICDM Contest 2013. Due to the large size of data provided by Expedia, we use both random sampling and balanced sampling methods to construct reliable validation set. Diverse ranking models are described in this paper, including modified logistic regression, random forest, gradient boosting machine, extreme randomized trees, factorization machine, and lambdaMART. We also attempt to adopt deep learning approach,\nwhich will be further discussed in next sub-section. With these individual ranking models, we introduce our ensemble methods to combine individual models, including z-score, GBM, deep learning and listwise ensemble. The combination of models significantly improves the ranking accuracy in terms of NDCG on both public and private leaderboards."
    }, {
      "heading" : "B. Lessons Learned in Deep Learning",
      "text" : "Based on the split data, there is a serious problem of lacking training data. And reconstruction error may not reflect the best optimization direction. We suggest in later practice it is better to check whether pretraining learns out distributed representations other than only rely on the reconstruction error. Bucket may not be a optimized solution to normalize feature input A better normalization may lead to better generalization capacity for the deep networks."
    }, {
      "heading" : "ACKNOWLEDGMENT",
      "text" : "We thank to Prof. Russ Greiner from University of Alberta for his advice while writing this report. And we also thank to Prof. Hong Hu from Institute of Computing Technology for his advice on data analysis."
    } ],
    "references" : [ {
      "title" : "Learning to rank for information retrieval. Foundations and Trends in Information Retrieval",
      "author" : [ "Liu", "Tie-Yan" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2009
    }, {
      "title" : "Scikit-learn: Machine learning in Python",
      "author" : [ "Pedregosa", "Fabian", "Gal Varoquaux", "Alexandre Gramfort", "Vincent Michel", "Bertrand Thirion", "Olivier Grisel", "Mathieu Blondel" ],
      "venue" : "The Journal of Machine Learning Research",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2011
    }, {
      "title" : "Pylearn2: a machine learning research",
      "author" : [ "Goodfellow", "Ian J", "David Warde-Farley", "Pascal Lamblin", "Vincent Dumoulin", "Mehdi Mirza", "Razvan Pascanu", "James Bergstra", "Frdric Bastien", "Yoshua Bengio" ],
      "venue" : "library. arXiv preprint,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2013
    }, {
      "title" : "Training Linear SVMs in Linear Time",
      "author" : [ "T. Joachims" ],
      "venue" : "Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD),",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2006
    }, {
      "title" : "LIBSVM: a library for support vector machines",
      "author" : [ "Chang", "Chih-Chung", "Chih-Jen Lin" ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology (TIST)",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2011
    }, {
      "title" : "LIBLINEAR: A library for large linear classification",
      "author" : [ "Fan", "Rong-En" ],
      "venue" : "The Journal of Machine Learning Research",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2008
    }, {
      "title" : "Random Forests",
      "author" : [ "L. Breiman" ],
      "venue" : "Machine Learning 45 (1): 532",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Adarank: a boosting algorithm for information retrieval",
      "author" : [ "Xu", "Jun", "Hang Li" ],
      "venue" : "Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval. ACM,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2007
    }, {
      "title" : "From ranknet to lambdarank to lambdamart: An overview",
      "author" : [ "Burges", "Chris" ],
      "venue" : "Learning",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2010
    }, {
      "title" : "Adapting boosting for information retrieval measures",
      "author" : [ "Q. Wu", "C.J. Burges", "K.M. Svore", "J. Gao" ],
      "venue" : "Information Retrieval 13.3 ",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Greedy function approximation: a gradient boosting machine",
      "author" : [ "Friedman", "Jerome H" ],
      "venue" : "Annals of Statistics",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2001
    }, {
      "title" : "Extremely randomized trees",
      "author" : [ "Geurts", "Pierre", "Damien Ernst", "Louis Wehenkel" ],
      "venue" : "Machine learning",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2006
    }, {
      "title" : "Learning to rank with extremely randomized trees",
      "author" : [ "Geurts", "Pierre", "Gilles Louppe" ],
      "venue" : "JMLR: Workshop and Conference Proceedings",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2011
    }, {
      "title" : "Factorization machines with libFM",
      "author" : [ "Rendle", "Steffen" ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology (TIST)",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2012
    }, {
      "title" : "Rectified linear units improve restricted boltzmann machines",
      "author" : [ "Nair", "Vinod", "Geoffrey E. Hinton" ],
      "venue" : "Proceedings of the 27th International Conference on Machine Learning",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2010
    }, {
      "title" : "Follow-the-regularized-leader and mirror descent: Equivalence theorems and l1 regularization",
      "author" : [ "McMahan", "H. Brendan" ],
      "venue" : "International Conference on Artificial Intelligence and Statistics",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2011
    }, {
      "title" : "Horizontal and Vertical Ensemble with Deep Representation for Classification",
      "author" : [ "Xie", "Jingjing", "Bing Xu", "Zhang Chuang" ],
      "venue" : "arXiv preprint arXiv:1306.2759",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "The evaluation metric for this competition is Normalized Discounted Cumulative Gain (NDCG), which is commonly used in ranking[1].",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 3,
      "context" : "The output of the data infrastructure can be in different format like numpy binary array, SVMRank[5] text format and LIBSVM[6] text format.",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 4,
      "context" : "The output of the data infrastructure can be in different format like numpy binary array, SVMRank[5] text format and LIBSVM[6] text format.",
      "startOffset" : 123,
      "endOffset" : 126
    }, {
      "referenceID" : 5,
      "context" : "We use LIBLINEAR[7] and SVMRank[5]for pairwise logistic regression; Random Forest[8] from scikit-learn[2]; Ranking algorithms like AdaRank[9], LambdaMART[10] from RankLib2.",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 3,
      "context" : "We use LIBLINEAR[7] and SVMRank[5]for pairwise logistic regression; Random Forest[8] from scikit-learn[2]; Ranking algorithms like AdaRank[9], LambdaMART[10] from RankLib2.",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 6,
      "context" : "We use LIBLINEAR[7] and SVMRank[5]for pairwise logistic regression; Random Forest[8] from scikit-learn[2]; Ranking algorithms like AdaRank[9], LambdaMART[10] from RankLib2.",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 1,
      "context" : "We use LIBLINEAR[7] and SVMRank[5]for pairwise logistic regression; Random Forest[8] from scikit-learn[2]; Ranking algorithms like AdaRank[9], LambdaMART[10] from RankLib2.",
      "startOffset" : 102,
      "endOffset" : 105
    }, {
      "referenceID" : 7,
      "context" : "We use LIBLINEAR[7] and SVMRank[5]for pairwise logistic regression; Random Forest[8] from scikit-learn[2]; Ranking algorithms like AdaRank[9], LambdaMART[10] from RankLib2.",
      "startOffset" : 138,
      "endOffset" : 141
    }, {
      "referenceID" : 8,
      "context" : "We use LIBLINEAR[7] and SVMRank[5]for pairwise logistic regression; Random Forest[8] from scikit-learn[2]; Ranking algorithms like AdaRank[9], LambdaMART[10] from RankLib2.",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 10,
      "context" : "And we also use Gradient Boosting Machine3[12], Extremely Randomized Trees4[13] from R, deep neural net-",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 11,
      "context" : "And we also use Gradient Boosting Machine3[12], Extremely Randomized Trees4[13] from R, deep neural net-",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 2,
      "context" : "work implementation from PyLearn2[3] and Factorization Machine libFM5[15].",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 13,
      "context" : "work implementation from PyLearn2[3] and Factorization Machine libFM5[15].",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 5,
      "context" : "In order to use full train set as pairwise in logistic regression model, we use FTRL-Proximal algorithm and liblinear[7] and SVMRank[5] build in function.",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 3,
      "context" : "In order to use full train set as pairwise in logistic regression model, we use FTRL-Proximal algorithm and liblinear[7] and SVMRank[5] build in function.",
      "startOffset" : 132,
      "endOffset" : 135
    }, {
      "referenceID" : 15,
      "context" : "The FTRLProximal algorithm [18], can be seen as a hybrid of FOBOS and RDA algorithms, and significantly outperforms both on a large, realworld dataset.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 6,
      "context" : "For each unique balanced prop country id data piece we train an independent random forest model [8] with 3200 trees.",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 5,
      "context" : "Some failed cases happened in predicting for some countries, so to make the example count of test and internal validation set equal we simply combine the score with a pairwise logistic regression trained by liblinear[7] model with 0.",
      "startOffset" : 216,
      "endOffset" : 219
    }, {
      "referenceID" : 10,
      "context" : "Gradient Boosting Machine Gradient Boosting Machine(GBM) [12] is a machine learning technique for regression problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 11,
      "context" : "Extremely Randomized Trees (ERT) is proposed by [13].",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 12,
      "context" : "ERT model using in learning to rank task is proposed by [14].",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 13,
      "context" : "Factorization Machine [15] is widely used in Recommender System.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 8,
      "context" : "LambdaMART model [10] [11] with CTR(clickthrough rates Eq 2) and CVR(Booking rates Eq 3) features and original features in full train set.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 9,
      "context" : "LambdaMART model [10] [11] with CTR(clickthrough rates Eq 2) and CVR(Booking rates Eq 3) features and original features in full train set.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 16,
      "context" : "Vertical ensemble[19] helps improve a little but still can not make deep network as a strong single model.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 10,
      "context" : "Treat each model’s output on local test set as a feature of GBM model [12].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 14,
      "context" : "With ReLU[17] network or Maxout network, our model achieves around 0.",
      "startOffset" : 9,
      "endOffset" : 13
    } ],
    "year" : 2013,
    "abstractText" : "The ICDM Challenge 2013 is to apply machine learning to the problem of hotel ranking, aiming to maximize purchases according to given hotel characteristics, location attractiveness of hotels, users aggregated purchase history and competitive online travel agency (OTA) information for each potential hotel choice. This paper describes the solution of team ”binghsu & MLRush & BrickMover”. We conduct simple feature engineering work and train different models by each individual team member. Afterwards, we use listwise ensemble method to combine each model’s output. Besides describing effective model and features, we will discuss about the lessons we learned while using deep learning in this competition.",
    "creator" : "LaTeX with hyperref package"
  }
}