{
  "name" : "1206.6479.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "The Landmark Selection Method for Multiple Output Prediction",
    "authors" : [ "Krishnakumar Balasubramanian", "Guy Lebanon" ],
    "emails" : [ "krishnakumar3@gatech.edu", "lebanon@cc.gatech.edu" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Conditional modeling x 7→ y is a central problem in machine learning. Specific cases include classification, where y is a discrete random variable, and regression, where y is a continuous random variable. Much of the attention in recent years has focused on the case where x is a high dimensional vector. In this case, traditional statistical methods are inefficient due to overfitting. Proposed alternatives for high dimensional x include feature selection and regularized models.\nWe consider, instead, the case of a high dimensional y, where x is either low dimensional or high dimensional. The baseline approach in this case is to independently construct models x 7→ yi ∈ R for i = 1, . . . , k (assuming y is a k-dimensional real vector). This approach has the advantage of drawing from a wide variety of available single output models, including linear and non-linear regression, logistic regression, and support vector machines. The main disadvantage is that the\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nindependent models do not take advantage of a likely correlation between the dimensions of y. Incorporating this correlation becomes especially important when the dimensionality of y is higher or of similar order to the dimensionality of x.\nOur approach is based on selecting a small subset L ⊂ {1, . . . , k} of the dimensions of y, and constructing two models:\nx 7→ yL (1)\nyL 7→ y, (2)\nwhere we use the standard notation yL = {yi : i ∈ L}. We thus have three problems: selecting the subset L, estimating (1), and estimating (2).\nSpecifically, we estimate model (2) in conjunction with selecting L via least-squares regression with group Lasso based hierarchical regularization. The precise model (1) varies, based on whether y is discrete or continuous. It may be any low-dimensional multiple output model, such as multilabel logistic regression and SVM, or multiple linear regression. If the dimensionality of x is high, regularization for model (1) is also necessary.\nThe underlying assumption of our model is that there exists a subset L of the dimensions of y, called landmark variables, such that the remaining dimensions of y may be expressed as a noisy linear combination y = AyL + ǫ, with sparse coefficients. Several practical data sets exhibit such a kind of relationship. One example is the prediction of future stock prices y from current stock prices x. The relationship y = AyL + ǫ is motivated by the identical trends of stock prices of multiple companies with a similar business model, or of multiple investment banks with similar holding portfolio. This phenomenon has been well documented in finance under the term cointegration. Another example is the classification of images (x) depending on what objects appear or do not appear in them (y). Obviously, some objects tend to appear or to not appear simultaneously, such as sky and tree, or car and road.\nThe cardinality s of the subset L is typically orders of magnitude lesser than the actual dimension of the\noutput space making the method scale well to ultrahigh dimensional outputs. For example, the naive one vs. all method requires O(k) independent models that need to be learnt from the data, whereas the number of subproblems selected in the proposed approach scales at the rate of O(s). Assuming s ≪ d we see that there is a huge advantage in terms of number of subproblems selected.\nWe report in this paper experimental results for classification and regression on multiple datasets. Based on our experimental study, we conclude that our model outperforms the one vs. all approach as well as several sophisticated multiple output prediction methods."
    }, {
      "heading" : "2. Related Work",
      "text" : "Several methods have been proposed for multi-output prediction both in regression and classification setting. In the regression setting, most approaches have focused on penalization of the regression matrix or input space sharing. For example (Izenman, 1975) introduced low-rank penalization of the regression matrix, which was analyzed in (Reinsel & Velu, 1998) in the low-dimensional setting. Recent work focused on analyzing penalized regression in high dimensions (Rohde & Tsybakov, 2011). An alternative approach that is directly applicable to multi-output prediction is group lasso (Yuan & Lin, 2006). Though these methods are popular and widely applicable, they do not directly model correlations in the output dimensions, which can be used to reduce the complexity of the problem. A notable exception is the curds and whey method (Breiman & Friedman, 1997) which uses shrinkage techniques in output space to reduce prediction error.\nIn the classification setting, the popular approach of one-vs-all was proposed by several researchers (see (Rifkin & Klautau, 2004) for a discussion). This method ignores the dependencies between the different dimensions of y, and is inefficient when y is high dimensional. A summary of improvements over the one-versus-all method is available in (Tsoumakas et al., 2010). Alternative approaches assume a class hierarchy on the output space (Cesa-Bianchi et al., 2006), graph structure on the output space (Vens et al., 2008) and joint feature extraction from output and input spaces in large margin setting (Tsochantaridis et al., 2006).\nA paper related to our proposed method is (Hsu et al., 2009), which consider multi-label prediction in a sparse high-dimensional output space. Their proposed method for multi-label classification is to randomly project y and construct a regression model on the reduced subspace. There are two significant dif-\nferences between this paper and our approach: (i) our approach uses data-dependent transformation, rather than a random projection, and (ii) our approach selects a subset of the dimensions of y that contributes to computational efficiency, statistical analysis, and is in line with some practical scenarios (see previous section). Furthermore, the approach by (Hsu et al., 2009) might not be applicable in the regression setting, as output sparsity assumption does not hold for regression in practice.\nRecently proposed variations on (Hsu et al., 2009) include (Tai & Lin, 2012) that propose to reduce the dimensionality of the output space by PCA, and (Bi & Kwok, 2011) that propose to reduce the label space by preserving a graph structure hierarchy on y. While these methods are sub-linear, they still project on to a low-dimensional real subspace, and hence they do not guarantee that the problem in the reduced subspace is easier than the original problem.\nOur approach also has a close connection to sparse PCA (Zou et al., 2006). Two significant differences are: (i) sparse PCA is generally applied to the covariates x rather than y, and (ii) our focus is on identifying the landmarks L and the relationship yL 7→ y rather than estimating the principal components themselves."
    }, {
      "heading" : "3. The landmark selection method:",
      "text" : "We consider a common scenario where x ∈ Rd and y ∈ Y, where Y is either Rk (regression) or {0, 1}k (classification), and k, d are high dimensional. We denote the data matrices, containing n labeled samples, by X ∈ Rn×d and Y ∈ Rn×k.\nStep 1: Selecting the landmark set L and modeling (2)\nA convenient way to select the set of landmark dimensions L, and to model (2) simultaneously is the following regularized least squares regression model\nÂ = argmin A∈Rk×k\n‖Y − Y A‖2F + λ1‖A‖1,2 + λ2‖A‖1 (3)\nwhere\n‖A‖F def = √ tr(A⊤A)\n‖A‖1,2 def = k ∑\ni=1\n√ √ √ √ k ∑\nj=1\nA2i,j\n‖A‖1 def =\nk ∑\ni=1\nk ∑\nj=1\n|Aij |.\nThe first term in (3) is the least squares empirical risk that is standard in linear regression models. Obvi-\nously, the identity A = I minimizing that term constitutes a trivial solution that is ineffective when y is high dimensional. The second and third terms in (3) promote a “small” A and thus prevent the estimated model to be the trivial minimizer I of the first term.\nMuch like group lasso, the second term in (3) enforces joint group sparsity across the rows of A. To see this note that ‖A‖1,2 is the L1 norm of the L2 norms of the individual rows. Due to the sparsity promoting nature of the L1 minimizer, Â will have only a few rows that are not identically zero. The resulting effect is the selection of landmark dimensions yL where L corresponds to the non-zero rows. We thus have that the first two terms in (3) simultaneously select the landmark dimensions L, and model yL 7→ y. The third term ‖A‖1 promotes sparsity within the coefficients of the model yL 7→ y. This additional sparsity assumption reduces the prediction risk when y is high dimensional.\nThe regularization parameter λ1 controls the number of landmark output dimensions. The regularization parameter λ2 controls the sparsity of the model yL 7→ y. Both λ1 and λ2 should increase with k. When the landmark assumption holds and there exists a landmark set L∗ such that y is a noisy sparse linear combination of yL∗ , the row sparsity pattern of Â should coincide with L∗ (assuming an appropriate selection of λ1, λ2). As λ1/λ2 increases, the group sparsity constraints become dominant implying that each dimension of y depends on all of the dimensions of yL. As λ1/λ2 decreases, Â tends to be more sparse within groups, implying that the dimensions of y are sparse linear combinations of the yL.\nFrom a practical point of view, with a proper selection of the regularization parameters λ1, λ2 (for example using cross-validation), the model (3) is quite flexible. It allows handling situations involving large landmark sets L and small landmark sets L, and high or low sparsity for the model yL 7→ y. Empirically, the dependence on the precise value of λ1, λ2 is robust, as small variations in λ1, λ2 do not substantially change the predicted values.\nHandling non-linear output relationship: In order to select and learn non-linear relationships between the outputs and the landmarks, one could use functional joint sparsity models with L1/L∞ constraints as proposed by (Liu et al., 2008) or with L1+ L1/L2 constraints (appropriately defined on a function space). With this change in step 1, the proposed approach could be used to handle non-linear relationships between the outputs, making the proposed method more flexible. Developing concrete algorithms and analysis for this setting is left as future work.\nStep 2: Estimating (1)\nOnce the landmark outputs L are identified, we can proceed with fitting model (1). In the case of continuous y (regression), model (1) can be estimated using a using multivariate regression model. In the case of a discrete y (classification), a one vs. all classifier may be used for x 7→ yi, i = 1, . . . , s, or alternatively a multiple output classifier may be used for x 7→ yL. Examples include support vector machines and loglinear models. From a statistical perspective, when y is high dimensional the reduction in the number of estimated parameters from kd to sd (in the regression setting) where s ≪ k, contributes to lower prediction risk. If the dimensionality of x is also high, the models x 7→ yL or x 7→ yi should use careful feature selection or regularization to avoid overfitting.\nStep 3: Prediction\nIn many cases, a statistical model for (1) provides not only point estimates, but also a full probabilistic model p(yL|x). Similarly, a statistical model for (2) provides a full probabilistic model for p(y|yL). The implied model\np(y|x) = p(y|yL)p(yL|x)\nsuggests the following procedure for predicting y from x\ny∗L = argmax yL p(yL|x) (4)\ny∗Lc = argmax yLc\n∫\np(y|yL)p(yL|x) dyL. (5)\nAn alternative to (5) is to use the following approximation\nargmax y p(y|x) ≈ argmax y p\n(\ny| argmax yL p(yL|x)\n)\n.\nIn other words, given a new test sample x, we predict yL using the model from step 2, and then estimate yLc using the model from step 1, operating on the predicted yL. In the case of classification, we follow standard practice and set the components of y to 1 if the corresponding prediction of model (2) is greater than 0.5 and to 0 if it lesser than 0.5. Finally the outputs are concatenated and they represent the prediction for the given sample x. Algorithm 1 summarizes this procedure."
    }, {
      "heading" : "4. Theory",
      "text" : "In this section, we give a brief theoretical analysis of the proposed approach in the regression setting highlighting the advantage of the proposed approach. We assume that there exist a true landmark subset L∗ and provide conditions under which it could be recovered consistently. Specifically, following the analysis\nAlgorithm 1 Landmark selection method\nInput: data {(x1, y1), . . . , (xn, yn)} in the form of X ∈ Rn×d and Y ∈ Rn×k Step 1: Simultaneously find the landmark set L and solve the optimization problem in step 1 to obtain the model yL → y and estimate Â. Step 2: Estimate the model x → yL using independent models for each component of yL or using multiple-output classification or regression algorithms. Step 3: Given a new test point x, estimate y by (4)-(5).\ndeveloped in (Obozinski et al., 2011) for random design linear regression with group Lasso regularization, we can get a lower bound on the number of samples needed for recovering the support of the subset L∗ of the landmark labels. For simplicity, we consider the regression setting with the assumption that λ2 = 0.\nWe assume that Y consists of i.i.d. rows sampled from N(0,Σ). This distribution could in fact be any subGaussian distribution (which includes any bounded random variable for example the Bernoulli random variable) for which a similar analysis could be carried out. We make the following assumption on the the covariance matrix Σ: (1) there exists ρmin > 0 and ρmax < ∞ such that all the eigenvalues of the s × s covariance matrix Σs of the the landmark output yL ∈ R\ns are contained in the closed interval [ρmin, ρmax], (2) mutual incoherence: there exist a incoherence parameter γ ∈ (0, 1] such that ‖ΣScSc(ΣSS)\n−1‖∞ ≤ 1−γ and (3) self-incoherence: there exists Dmax < ∞ such that ‖(ΣSS)\n−1‖∞ ≤ Dmax. Note that these are standard conditions assumed for support recovery results in the modern sparse recovery analysis. Condition (1) is needed to prevent over-dependency between the landmark outputs. Conditions (2) and (3) are necessary conditions for model selection consistency of sparse recovery problems. For example, several classes of matrices, for example Toeplitz matrices, tree-structured matrices and bounded off-diagonal matrices are shown to satisfy the above conditions (Zhao & Yu, 2006). In the absence of these conditions, landmark recovery might fail even with arbitrarily large training set.\nWe also make the following assumption on the regression matrix. Let amin def\n= mini∈L ‖Ai‖2 where Ai denote the ith non-zero row of the matrix A. We denote As ∈ Rs×k to be the subset of the matrix A with nonzero rows, ζ(As) ∈ R\ns×k to be the row normalized matrix, and\nφ(A) def = λmax ( ζ(As) ⊤(ΣSS) −1ζ(As) ) .\nThis quantity characterizes the amount of overlap that\ncould be captured given the output samples. Note that the support overlap function φ(A) satisfies\ns\nρmaxK ≤ φ(A) ≤\ns\nρmin\nfor any Y that satisfies assumption (1).\nProposition 1. Consider the label matrix Y with rows i.i.d. drawn from N(0,Σ) satisfying assumptions (1)- (3), suppose that a2min decays no more slowly than f(k)min{ 1\ns , 1log(k−s)} for some function f(k) such that\nf(k)/s → 0 and f(k) → ∞. Then, as long as n > C ′ρmaxφ(A\n∗) log(k − s), we have with probability greater than 1 − c1 exp (c2 log s): (1) the optimization problem in 3 (with λ2 = 0) has a unique solution when λ1 = √ f(k) log k n and (2) the row support specified by the unique solution of the optimization problem 3 is equal to the row support of the true model.\nProof. The proof follows from the corresponding proof in (Obozinski et al., 2011).\nThe main consequence of the above proposition is that if there exist a set of landmark variable L∗ in the output space, the sample complexity is of logarithmic order in the original dimension of the output space k. Using sub-Gaussian assumptions on the label matrix, analogous conditions for classification are possible.\nFollowing (Reinsel & Velu, 1998) we note that for a matrix regression problem y = Θx + ǫ with Θ ∈ R\nm1×m2 , the Frobenious norm error rate (with n samples, unit noise variance and no assumption on the regression matrix)\n‖Θ̂−Θ‖2F = O (m1m2\nn\n)\n.\nSince in our case the estimated matrix (1) (assuming linear regression model) is of the dimension s× d, the error is of the order of O( sd\nn ) samples (Reinsel & Velu,\n1998), much smaller than the classical setting without the landmark selection method of O(kd\nn ). In particu-\nlar, when s ≪ k, there is a significant gain in efficiency.\nWe conclude that the landmark method makes a structural assumption on the output space in order to facilitate regression in high dimensional setting (n ≪ kd). Other methods, making a different set of structural assumptions (e.g., low-rank regression) try to achieve the same goal, but work under a different set of assumptions. Empirically, the landmark method works better than low-rank regression and group Lasso based multivariate regression on a variety of datasets (see Section 6)."
    }, {
      "heading" : "5. Optimization procedure",
      "text" : "Here, we provide the optimization procedure required to solve the optimization problem described in step 1. The spaRSA method, proposed recently in (Wright et al., 2009), is a solver for optimization problems of the form\nmin a∈Rp f(a) + λφ(a)\nwhere f is a convex loss function and φ is a convex regularizer. The main advantage of spaRSA is that when the regularizer is group separable, the problem decomposes over the group.\nUsing vectorization and block-diagonalization, it can be shown that (3) falls under this framework. Upon initial investigation, it appears that the blockdiagonalization operation complicates the solver as it increases the size of the data matrix. However, we describe below a variation on spaRSA that works directly with the Y and A. A similar approach was used in (Sprechmann et al., 2011) for the problem of collaborative dictionary learning with hierarchical penalty. The main advantage of the spaRSA procedure (that the problem decouples across groups) is still preserved and further in our case, each subproblem could be solved via thresholding.\nIn order to solve the optimization problem, the spaRSA procedure generates a sequence of updates that converges to the solution. We refer the reader to (Wright et al., 2009) for a complete description of the general procedure. In our case, we let f(Ai) denote the reconstruction error (the squared loss in our case) for Ai (here and below we denote the i-column of a matrix A as Ai) and define the matrix U (t) ∈ Rk×k whose i-column is given by\nU (t) i = A (t) i − (1/α t)∇f(A (t) i ).\nThe sequence of spaRSA updates that converge to the true solution is\nA(t+1) = argmin Z∈Rk×k ‖Z − U (t)‖F + λ1 α(t) ‖Z‖F + λ2 α(t) ‖Z‖1,\nwhich is group separable into k independent problems as below:\nA (t+1) i = argmin\nZi∈Rk ‖Zi − U\n(t) i ‖2 + λ1 α(t) ‖Zi‖2 + λ2 α(t) ‖Z‖1.\nThe solutions for each of these sub-problems are available in closed form (similar to (Sprechmann et al., 2011)) as follows:\nA (t+1) i =\n{\nmax{0, ‖h‖2 − λ1}h/‖h‖2 if ‖h‖2 > 0 0 if ‖h‖2 = 0\nwhere hj = sign(U (t) i,j )max{0, |U (t) i,j |−λ2}. The thresholding require operations that are linear in the dimensionality of the matrix Y . The above procedure is repeated until convergence to obtain the final solution that features row sparsity, and potential sparsity within rows as well."
    }, {
      "heading" : "6. Experiments",
      "text" : "In this section, we compare our landmark selection method, which we refer to below as moplms, to alternative baselines on classification and regression problems. In our experiments we used code from (Wright et al., 2009) for performing the mixed norm penalty (group lasso and lasso) landmark selection. The regularization parameters were set by cross-validation."
    }, {
      "heading" : "6.1. Synthetic experiments",
      "text" : "We conducted an experiment on synthetic regression data with k = 500 (dimensionality of y), d = 500 (dimensionality of x). The number of landmark outputs s was varied in the set {50, 100, 200}. The data was simulated from the above model, including the specified landmark outputs. Figure 1 (left) shows the plot of the test MSE prediction error as a function of the sample size for various values of the parameter s/k.\nFrom section 4, we have that if the landmark output selection method is not used, with a linear regression model for x → y,the Frobenious norm error between the true and estimated matrix scales as O (\nkd n\n)\n. Where as with the landmark output assumption the error for model 1 scales as O (\nsd n\n)\n. This benefit in the estimation error of the regression matrix is reflected in the MSE prediction error. Specifically, as s decreases, the sample complexity decreases. This phenomenon is especially important in high-dimensional cases, when there are fewer samples than the number of parameters to be estimated. We also compared the proposed approach to group-Lasso and low-rank multivariate regression.\nFigure 1 (middle) shows the mse prediction error rate of the moplms method decays faster compared to the other methods.\nWe also experimented with synthetic classification data where y is a 500 dimensional binary vector and the input x ∈ R500. Similar to the regression setting, the landmark outputs were first generated with s ∈ {50, 100, 200} and the dependent outputs where generated as sparse linear combination of the landmark outputs. Figure 1 (right) shows the Hamming loss as a function of the sample size. The x 7→ yL model was collection of multiple one-vs-all SVMs. Similar to the regression case, the prediction error decays with the number of landmark outputs s/k. We further compare the proposed approach on synthetic data set against the following methods:\n1. One vs. all: This is a standard baseline approach for multi-label classification, for e.g., (Rifkin & Klautau, 2004).\n2. Multilabel compressive sensing (mlcs): This approach was proposed in (Hsu et al., 2009) where the label vector is projected to a random m dimensional sub-space followed by regression on the compressed subspace.\n3. Multi-label classification via canonical correlation analysis (ml-cca): After performing canonical correlation analysis (CCA) on the input and output variables, a model is learned in the resulting subspace, followed by projection to the original label space.\nFrom Figure 2, we note that the proposed approach has a better rate of decay of hamming loss compared to the other approaches. This phenomenon is further observed in the real world data sets as described in the next section.\nWe conducted an additional experiment to study the number of sub-problems selected. Specifically, we var-\nied the number of sub-problems and the tuning parameters of mlcs, and noted the values achieving the lowest prediction error. We then trained moplms, gradually reducing the regularization parameter until the prediction error matched that of mlcs. The two methods achieved identical prediction error with the following (mean) values of s/k: 0.45 (mlcs) and 0.30 (moplms), indicating moplms selected fewer sub-problems while achieving identical performance. Note, however, that mlcs always uses base regressors and moplms uses base classifiers."
    }, {
      "heading" : "6.2. Real-world data sets",
      "text" : ""
    }, {
      "heading" : "6.2.1. Classification",
      "text" : "We experimented with the following two multiple output classification datasets.\n1. del.icio.us This dataset consists of data from del.icio.us, a social bookmarking site where webpages are labeled with multiple contextual tags. The data set contains about 16000 labeled web page and 983 unique labels. We follow the experimental setup followed in (Hsu et al., 2009) and represent web page as a boolean bag-of-words vector, with the vocabulary chosen using a combination of frequency thresholding and χ2 feature ranking, resulting in 500 features.\n2. Image data set. This dataset contains 68000 images, with about 22000 unique word tags for each image. Following (Hsu et al., 2009) we retained the 1000 most frequent labels. We represented each image via codes computed with a learned dictionary (of size 1024) via sparse coding (Yang et al., 2009). Specifically, we densely sampled 10×10 patches from the image and computed sparse codes. Finally max-pooling was used to pool the codes obtained for the patches.\nNote that we use thresholding to convert the real output to the binary form of the data. The regulariza-\nDelicious Image Ham. loss F-score Ham. loss F-score\nmlcs 0.0187 0.3732 0.0047 0.3012 ml-cca 0.0164 0.3822 0.0041 0.3183 one.vs.all 0.0144 0.4512 0.0034 0.3923 moplms 0.0142 0.4522 0.0032 0.4031\nTable 1. Test set Hamming loss and F1 measure evaluation of the four classification approaches: mlcs, ml-cca, one vs. all, and moplms. The base classifiers in the reduced space were SVM.\ntion parameters λ1 and λ2 were estimated using crossvalidation. The number of selected landmarks s was 231 for the del.ic.ious data and 278 for the image data set. This was less than the number of sub-problems in both the mlcs and ml-cca approaches, which were also tuned for optimal prediction error.\nTable 1 displays the F1-score and hamming loss that are two standard evaluation metrics for multi-label classification.\nHamming loss = y⊤1+ ŷ⊤1− 2y⊤ŷ\nk\nF1 score = 2y⊤ŷ\n∑k i=1 yi + ∑ i=1 ŷi .\nThe landmark selection method performed better in terms of both evaluation metrics. The one-versus-all method was the second best in terms of prediction accuracy, but takes a significantly greater amount of train and test time, compared to the alternative methods.\nFigure 3 (left and middle) shows the decay of the Hamming loss as a function of the sample size for mlcs, moplms and ml-cca method. We omitted the one-vs-all method as it took significantly more amount of time compared to the other approaches, and thus is not computationally attractive. The proposed landmark selection approach has lower prediction error than mlcs and ml-cca."
    }, {
      "heading" : "6.2.2. Regression",
      "text" : "In the regression setting, we consider predicting the stock prices of several companies based on previous values via the landmark selection approach on the SP 500 data set. More specifically, the data consists of closing stock prices of the 500 companies in the S&P index in the period from August 21, 2009 to August 20, 2010 (a total of 245 entries). We assume the following autoregressive 1 or AR(1) model\nytL = Byt−1L + E (6)\nwhere yt = log St\nSt−1 represents the log returns (St is\nthe stock price at time t) for day t and E is the noise matrix. The problem is motivated by the observation in finance that multiple companies have stock prices that share identical stochastic trends (cointegration).\nWe compare our landmark selection approach to lowrank multivariate regression (using trace norm regularization) and group lasso based multivariate regression. These two baselines are popular multivariate regression methods. In our case (moplms), we used a multivariate ridge regression for estimating model (1), which is Equation 6 in the current setting. As in the classification setting, the regularization parameter was tuned by cross validation, and resulted in s = 98 landmark outputs.\nTable 2 shows that moplms outperformed the two baselines (group lasso and low-rank multivariate regression). Figure 3 displays the prediction error rate as a function of the sample size. It confirms this conclusion as the prediction error of moplms decays faster than the baselines."
    }, {
      "heading" : "7. Discussion",
      "text" : "In this paper we propose a framework for multi-output prediction based on parsimonious modeling on the output space. By selecting a subset of the output dimensions (landmarks) and focusing on modeling the dependency of that subset of y on x, we reduce the sample complexity considerably. This is most noticeable when the output dimensionality is high and the different component feature high correlation. Our experiments indicate that the proposed method outperforms standard multi-output methods in both the classification and regression scenarios.\nThe results in this paper raise several interesting questions and follow up directions. First, a detailed analysis is required to characterize the improvements of the proposed methods over competing methods. Second it is interesting to consider cases in which the label vector has a pattern of missingness.\nAcknowledgments: The authors would like to thank Kai Yu and Parikshit Ram for discussions."
    } ],
    "references" : [ {
      "title" : "Multi-label classification on tree and dag structured hierarchies",
      "author" : [ "W. Bi", "J. Kwok" ],
      "venue" : null,
      "citeRegEx" : "Bi and Kwok,? \\Q2011\\E",
      "shortCiteRegEx" : "Bi and Kwok",
      "year" : 2011
    }, {
      "title" : "Predicting multivariate responses in multiple linear regression",
      "author" : [ "L. Breiman", "J.H. Friedman" ],
      "venue" : "Journal of the Royal Statistical Society:B,",
      "citeRegEx" : "Breiman and Friedman,? \\Q1997\\E",
      "shortCiteRegEx" : "Breiman and Friedman",
      "year" : 1997
    }, {
      "title" : "Incremental algorithms for hierarchical classification",
      "author" : [ "N. Cesa-Bianchi", "C. Gentile", "L. Zaniboni" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Cesa.Bianchi et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Cesa.Bianchi et al\\.",
      "year" : 2006
    }, {
      "title" : "Multi-label prediction via compressed sensing",
      "author" : [ "D. Hsu", "S.M. Kakade", "J. Langford", "T. Zhang" ],
      "venue" : null,
      "citeRegEx" : "Hsu et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Hsu et al\\.",
      "year" : 2009
    }, {
      "title" : "Reduced-rank regression for the multivariate linear model",
      "author" : [ "A.J. Izenman" ],
      "venue" : "Journal of multivariate analysis,",
      "citeRegEx" : "Izenman,? \\Q1975\\E",
      "shortCiteRegEx" : "Izenman",
      "year" : 1975
    }, {
      "title" : "Nonparametric regression and classification with joint sparsity constraints",
      "author" : [ "H. Liu", "J. Lafferty", "L. Wasserman" ],
      "venue" : null,
      "citeRegEx" : "Liu et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2008
    }, {
      "title" : "I. support union recovery in high-dimensional multivariate regression",
      "author" : [ "G. Obozinski", "M.J. Wainwright", "M. Jordan" ],
      "venue" : "Annals of statistics,",
      "citeRegEx" : "Obozinski et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Obozinski et al\\.",
      "year" : 2011
    }, {
      "title" : "Multivariate reducedrank regression: theory and applications",
      "author" : [ "G.C. Reinsel", "R.P. Velu" ],
      "venue" : null,
      "citeRegEx" : "Reinsel and Velu,? \\Q1998\\E",
      "shortCiteRegEx" : "Reinsel and Velu",
      "year" : 1998
    }, {
      "title" : "In defense of one-vs-all classification",
      "author" : [ "R. Rifkin", "A. Klautau" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Rifkin and Klautau,? \\Q2004\\E",
      "shortCiteRegEx" : "Rifkin and Klautau",
      "year" : 2004
    }, {
      "title" : "Estimation of highdimensional low-rank matrices",
      "author" : [ "A. Rohde", "A.B. Tsybakov" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Rohde and Tsybakov,? \\Q2011\\E",
      "shortCiteRegEx" : "Rohde and Tsybakov",
      "year" : 2011
    }, {
      "title" : "C-hilasso: A collaborative hierarchical sparse modeling framework",
      "author" : [ "P. Sprechmann", "I. Ramı́rez", "G. Sapiro", "Y.C. Eldar" ],
      "venue" : "Signal Processing, IEEE Transactions on,",
      "citeRegEx" : "Sprechmann et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Sprechmann et al\\.",
      "year" : 2011
    }, {
      "title" : "Multi-label classification with principle label space transformation",
      "author" : [ "F. Tai", "H.T. Lin" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Tai and Lin,? \\Q2012\\E",
      "shortCiteRegEx" : "Tai and Lin",
      "year" : 2012
    }, {
      "title" : "Large margin methods for structured and interdependent output variables",
      "author" : [ "I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Tsochantaridis et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Tsochantaridis et al\\.",
      "year" : 2006
    }, {
      "title" : "Mining multi-label data. Data mining and knowledge discovery handbook",
      "author" : [ "G. Tsoumakas", "I. Katakis", "Vlahavas" ],
      "venue" : null,
      "citeRegEx" : "Tsoumakas et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Tsoumakas et al\\.",
      "year" : 2010
    }, {
      "title" : "Decision trees for hierarchical multilabel classification",
      "author" : [ "C. Vens", "J. Struyf", "L. Schietgat", "S. Džeroski", "H. Blockeel" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Vens et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Vens et al\\.",
      "year" : 2008
    }, {
      "title" : "Sparse reconstruction by separable approximation",
      "author" : [ "S.J. Wright", "R.D. Nowak", "M.A.T. Figueiredo" ],
      "venue" : "Signal Processing, IEEE Transactions on,",
      "citeRegEx" : "Wright et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Wright et al\\.",
      "year" : 2009
    }, {
      "title" : "Linear spatial pyramid matching using sparse coding for image classification",
      "author" : [ "J. Yang", "K. Yu", "Y. Gong", "T. Huang" ],
      "venue" : "Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Yang et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2009
    }, {
      "title" : "Model selection and estimation in regression with grouped variables",
      "author" : [ "M. Yuan", "Y. Lin" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
      "citeRegEx" : "Yuan and Lin,? \\Q2006\\E",
      "shortCiteRegEx" : "Yuan and Lin",
      "year" : 2006
    }, {
      "title" : "On model selection consistency of lasso",
      "author" : [ "P. Zhao", "B. Yu" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Zhao and Yu,? \\Q2006\\E",
      "shortCiteRegEx" : "Zhao and Yu",
      "year" : 2006
    }, {
      "title" : "Sparse principal component analysis",
      "author" : [ "H. Zou", "T. Hastie", "R. Tibshirani" ],
      "venue" : "Journal of computational and graphical statistics,",
      "citeRegEx" : "Zou et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Zou et al\\.",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "For example (Izenman, 1975) introduced low-rank penalization of the regression matrix, which was analyzed in (Reinsel & Velu, 1998) in the low-dimensional setting.",
      "startOffset" : 12,
      "endOffset" : 27
    }, {
      "referenceID" : 13,
      "context" : "A summary of improvements over the one-versus-all method is available in (Tsoumakas et al., 2010).",
      "startOffset" : 73,
      "endOffset" : 97
    }, {
      "referenceID" : 2,
      "context" : "Alternative approaches assume a class hierarchy on the output space (Cesa-Bianchi et al., 2006), graph structure on the output space (Vens et al.",
      "startOffset" : 68,
      "endOffset" : 95
    }, {
      "referenceID" : 14,
      "context" : ", 2006), graph structure on the output space (Vens et al., 2008) and joint feature extraction from output and input spaces in large margin setting (Tsochantaridis et al.",
      "startOffset" : 45,
      "endOffset" : 64
    }, {
      "referenceID" : 12,
      "context" : ", 2008) and joint feature extraction from output and input spaces in large margin setting (Tsochantaridis et al., 2006).",
      "startOffset" : 90,
      "endOffset" : 119
    }, {
      "referenceID" : 3,
      "context" : "A paper related to our proposed method is (Hsu et al., 2009), which consider multi-label prediction in a sparse high-dimensional output space.",
      "startOffset" : 42,
      "endOffset" : 60
    }, {
      "referenceID" : 3,
      "context" : "Furthermore, the approach by (Hsu et al., 2009) might not be applicable in the regression setting, as output sparsity assumption does not hold for regression in practice.",
      "startOffset" : 29,
      "endOffset" : 47
    }, {
      "referenceID" : 3,
      "context" : "Recently proposed variations on (Hsu et al., 2009) include (Tai & Lin, 2012) that propose to reduce the dimensionality of the output space by PCA, and (Bi & Kwok, 2011) that propose to reduce the label space by preserving a graph structure hierarchy on y.",
      "startOffset" : 32,
      "endOffset" : 50
    }, {
      "referenceID" : 19,
      "context" : "Our approach also has a close connection to sparse PCA (Zou et al., 2006).",
      "startOffset" : 55,
      "endOffset" : 73
    }, {
      "referenceID" : 5,
      "context" : "Handling non-linear output relationship: In order to select and learn non-linear relationships between the outputs and the landmarks, one could use functional joint sparsity models with L1/L∞ constraints as proposed by (Liu et al., 2008) or with L1+ L1/L2 constraints (appropriately defined on a function space).",
      "startOffset" : 219,
      "endOffset" : 237
    }, {
      "referenceID" : 6,
      "context" : "developed in (Obozinski et al., 2011) for random design linear regression with group Lasso regularization, we can get a lower bound on the number of samples needed for recovering the support of the subset L of the landmark labels.",
      "startOffset" : 13,
      "endOffset" : 37
    }, {
      "referenceID" : 6,
      "context" : "The proof follows from the corresponding proof in (Obozinski et al., 2011).",
      "startOffset" : 50,
      "endOffset" : 74
    }, {
      "referenceID" : 15,
      "context" : "The spaRSA method, proposed recently in (Wright et al., 2009), is a solver for optimization problems of the form",
      "startOffset" : 40,
      "endOffset" : 61
    }, {
      "referenceID" : 10,
      "context" : "A similar approach was used in (Sprechmann et al., 2011) for the problem of collaborative dictionary learning with hierarchical penalty.",
      "startOffset" : 31,
      "endOffset" : 56
    }, {
      "referenceID" : 15,
      "context" : "We refer the reader to (Wright et al., 2009) for a complete description of the general procedure.",
      "startOffset" : 23,
      "endOffset" : 44
    }, {
      "referenceID" : 10,
      "context" : "The solutions for each of these sub-problems are available in closed form (similar to (Sprechmann et al., 2011)) as follows:",
      "startOffset" : 86,
      "endOffset" : 111
    }, {
      "referenceID" : 15,
      "context" : "In our experiments we used code from (Wright et al., 2009) for performing the mixed norm penalty (group lasso and lasso) landmark selection.",
      "startOffset" : 37,
      "endOffset" : 58
    }, {
      "referenceID" : 3,
      "context" : "Multilabel compressive sensing (mlcs): This approach was proposed in (Hsu et al., 2009) where the label vector is projected to a random m dimensional sub-space followed by regression on the compressed subspace.",
      "startOffset" : 69,
      "endOffset" : 87
    }, {
      "referenceID" : 3,
      "context" : "We follow the experimental setup followed in (Hsu et al., 2009) and represent web page as a boolean bag-of-words vector, with the vocabulary chosen using a combination of frequency thresholding and χ feature ranking, resulting in 500 features.",
      "startOffset" : 45,
      "endOffset" : 63
    }, {
      "referenceID" : 3,
      "context" : "Following (Hsu et al., 2009) we retained the 1000 most frequent labels.",
      "startOffset" : 10,
      "endOffset" : 28
    }, {
      "referenceID" : 16,
      "context" : "We represented each image via codes computed with a learned dictionary (of size 1024) via sparse coding (Yang et al., 2009).",
      "startOffset" : 104,
      "endOffset" : 123
    } ],
    "year" : 2012,
    "abstractText" : "Conditional modeling x 7→ y is a central problem in machine learning. A substantial research effort is devoted to such modeling when x is high dimensional. We consider, instead, the case of a high dimensional y, where x is either low dimensional or high dimensional. Our approach is based on selecting a small subset yL of the dimensions of y, and proceed by modeling (i) x 7→ yL and (ii) yL 7→ y. Composing these two models, we obtain a conditional model x 7→ y that possesses convenient statistical properties. Multi-label classification and multivariate regression experiments on several datasets show that this method outperforms the one vs. all approach as well as several sophisticated multiple output prediction methods.",
    "creator" : "dvips(k) 5.98 Copyright 2009 Radical Eye Software"
  }
}