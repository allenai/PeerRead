{
  "name" : "1602.05629.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Federated Learning of Deep Networks using Model Averaging",
    "authors" : [ "H. Brendan McMahan", "Eider Moore", "Daniel Ramage" ],
    "emails" : [ "MCMAHAN@GOOGLE.COM", "EIDERM@GOOGLE.COM", "DRAMAGE@GOOGLE.COM", "BLAISEA@GOOGLE.COM" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We present a practical method for the federated learning of deep networks that proves robust to the unbalanced and non-IID data distributions that naturally arise. This method allows high-quality models to be trained in relatively few rounds of communication, the principal constraint for federated learning. The key insight is that despite the non-convex loss functions we optimize, parameter averaging over updates from multiple clients produces surprisingly good results, for example decreasing the communication needed to train an LSTM language model by two orders of magnitude."
    }, {
      "heading" : "1. Introduction",
      "text" : "As datasets grow larger and models more complex, machine learning increasingly requires distributing the optimization of model parameters over multiple machines, e.g., (Dean et al., 2012). Many algorithms exist for distributed optimization, but these algorithms typically have communication requirements that realistically are only satisfied by a data-center-grade network fabric. Further, the theoretical\njustification and practical performance for these algorithms rests heavily on the assumption the data is IID (independently and identically distributed) over the compute nodes. Taken together, these requirements amount to an assumption that the full training dataset is controlled by the modeler and stored in a centralized location.\nA parallel trend is the rise of phones and tablets as primary computing devices for many people. The powerful sensors present on these devices (including cameras, microphones, and GPS), combined with the fact these devices are frequently carried, means they have access to data of an unprecedentedly private nature. Models learned on such data hold the promise of greatly improving usability by powering more intelligent applications, but the sensitive nature of the data means there are risks and responsibilities to storing it in a centralized location.\nWe investigate a learning technique that allows users to collectively reap the benefits of shared models trained from this rich data, without the need to centrally store it. This approach also allows us to scale up learning by utilizing the cheap computation available at the edges of the network. We term our approach Federated Learning, since the learning task is solved by a loose federation of participating devices (which we refer to as clients) which are coordinated by a central server. Each client has a local training dataset which is never uploaded to the server. Instead, each client computes an update to the current global model maintained by the central server, and only this update is communicated. This is a direct application of the principle of focused collection or data minimization as outlined in the Consumer Privacy Bill of Rights (White House Report, 2013). Since these updates are specific to improving the current model, there is no reason to store them once they have been applied.\nWe introduce the FederatedAveraging algorithm, which combines local SGD training on each client with communication rounds where the central server performs model averaging. We perform extensive experiments on this algorithm, demonstrating it is robust to unbalanced and\nar X\niv :1\n60 2.\n05 62\n9v 1\n[ cs\n.L G\n] 1\n7 Fe\nb 20\n16\nnon-IID data distributions, and can reduce the rounds of communication needed to train a deep network by one to two orders of magnitude."
    }, {
      "heading" : "1.1. Federated Learning",
      "text" : "What tasks are best suited to federated learning? The ideal problems have the following properties:\n• Training on real-world data from mobile devices provides a distinct advantage over training on proxy data that is generally available in the data-center. • This data is privacy sensitive or large in size (compared to the size of the model), so it is preferable not to log it to the data-center purely for the purpose of model training (in service of the focused collection principle). • For supervised tasks, labels on the data can be inferred naturally from a user’s interaction with their device.\nMany models that power intelligent behavior on mobile devices fit the above criteria. As two examples, we consider:\n• Image classification, for example predicting which photos are most likely to be viewed multiple times in the future, or shared. • Language models, which can be used to improve voice recognition and text entry on touch-screen keyboards by improving decoding, next-word-prediction, and even predicting whole replies (Corrado, 2015).\nThe potential training data for both these tasks (all the photos a user takes and everything they type on their mobile keyboard, including passwords, URLs, messages, etc) can be privacy sensitive. The distributions from which these examples are drawn are also likely to differ substantially from easily available proxy datasets: the use of language in chat and text messages is generally much different than standard language corpora, e.g., Wikipedia and other web documents, or public-domain books; the photos people take on their phone are likely quite different than typical Flickr photos. And finally, the labels for these problems are directly available: entered text is self-labeled for learning a language model, and photo labels can be defined by natural user interaction with their photo app (which photos are deleted, shared, or viewed).\nBoth of these tasks are well-suited to learning a neural network. For image classification feed-forward deep networks, and in particular convolutional networks, are wellknown to provide state-of-the-art results (LeCun et al., 1998; Krizhevsky et al., 2012). For language modeling tasks recurrent neural networks, and in particular LSTMs, have achieved state-of-the-art results (Hochreiter & Schmidhuber, 1997; Bengio et al., 2003; Kim et al., 2015).\nIn the remainder of this section, we consider the privacy advantages of federated optimization, and the potential to decrease communication costs for large datasets.\nPrivacy for federated learning There are two main aspects to data privacy for federated learning. First, we must consider what an attacker might learn by inspecting the model parameters, which are shared with all clients participating in the optimization. Given this wide availability, we cannot rely on security to mitigate such attacks. However, because the model is the aggregate of updates from a large number of individual users, for many model classes such attacks are much more difficult.\nFor truly privacy sensitive learning tasks, techniques from differential privacy can provide rigorous worst-case privacy guarantees even when the adversary has arbitrary sideinformation; however, this comes at some cost in utility, as these techniques rely on adding some random noise to the model training process (Dwork & Roth, 2014). Additional steps may also be needed to address model inversion attacks (Wang et al., 2015; Fredrikson et al., 2015). We note that these same issues arise for a model trained on private data held in the data center, and then released for on-device inference; hence it is not specific to federated learning.\nThe next question is what can an adversary learn by gaining access to the update messages of an individual client. If one trusts the central server, then encryption and other standard security protocols are a primary line of defense for this type of attack. A stronger guarantee can be achieved by enforcing local differential privacy (Kasiviswanathan et al., 2008; Duchi et al., 2014), where rather than adding noise to the final model, we noise the individual updates, which precludes the central server from making any definitive inference about a client. It is also possible to use secure multiparty computation to perform aggregation over multiple client updates, allowing local differential privacy to be achieved using much less random noise (Goryczka et al., 2013).\nEven unpaired with a differential privacy guarantee, federated learning has distinct privacy advantages compared to data-center training on persisted data. Holding even an “anonymized” dataset can still put user privacy at risk via joins with other data (Sweeney, 2000). In contrast, the information transmitted for federated learning is the minimal update necessary to improve a particular model.1 The up-\n1Naturally, the strength of the privacy benefit depends on the content of the updates. For example, if the update is the total gradient of the loss on all of the local data, and the features are a sparse bag-of-words, then the non-zero gradients reveal exactly which words the user has entered on the device. In contrast, the sum of many gradients for a dense model such as a CNN offers a harder target for attackers seeking information about individual training instances (though attacks are still possible).\ndates themselves can (and should) be ephemeral. And the source of the updates is not needed by the aggregation algorithm, so updates can be transmitted without identifying meta-data over a mix network such as Tor (Chaum, 1981) or via a trusted third party. Thus, federated learning is strictly preferable to directly logging the raw data to a central server, and can be further enhanced using known techniques to provide even stronger privacy guarantees.\nAdvantages for large datasets Federated learning can also provide a distinct advantage when training on large volumes of data. The network traffic per-client necessary to train in the data-center is simply the size of a client’s local dataset, which must be transmitted once; for federated learning, the per-client traffic is (#-communication-rounds) × (update-size). This latter quantity can be substantially smaller if the updatesize (generally O(#-model-parameters)) is relatively small compared to the volume of training data needed, as when training on high-resolution photos or videos."
    }, {
      "heading" : "1.2. Federated Optimization",
      "text" : "We refer to the optimization problem implicit in federated learning as federated optimization, drawing a connection (and contrast) to distributed optimization. As hinted above, federated optimization has several key properties that differentiate it from the typical distributed optimization problem:\n• Non-IID The training data on a given client is typically based on the usage of the mobile device by a particular user, and hence any particular user’s local dataset will not be representative of the population distribution. • Unbalanced Similarly, some users will make much heavier use of the service or app that produces training data, leading to some clients having large local training data sets, while others have little or no data. • Massively distributed In realistic scenarios, we expect the number of clients participating in an optimization to be much larger than the average number of examples per client.\nIn this work, our emphasis will be on the Non-IID and Unbalanced properties, as dealing with these aspects potentially requires the most substantial algorithmic advances.\nA deployed federated optimization system must address a myriad of practical issues: client datasets that change as data is added and deleted; client availability that correlates with the local data distribution in complex ways (e.g., phones from speakers of American English will likely be plugged in at different times than speakers of British English); and clients that never respond or send corrupted updates.\nThese issues are beyond the scope of the current work; instead, we use a controlled environment that is suitable for experiments, but still address the key issues of client availability and unbalanced and non-IID data. We assume a synchronous update scheme that proceeds in rounds of communication. There is a fixed set of K clients, each with a fixed local dataset. At the beginning of each round, a random fraction C of clients is selected, and the server sends the current global algorithm state to each of these clients (e.g., the current model parameters). Each client then performs local computation based on the global state and its local dataset, and sends an update to the server. The server then applies these updates to its global state, and the process repeats.\nWhile we focus on non-convex neural network objectives, the algorithm we consider is applicable to any finite-sum objective of the form\nmin w∈Rd\nf(w) where f(w) def= 1\nn n∑ i=1 fi(w). (1)\nFor a machine learning problem, we typically take fi(w) = `(xi, yi;w), that is, the loss of the prediction on example (xi, yi) made with model parameters w.\nWe assume there are K clients over which the data is partitioned, with Pk the set of indexes of data points on client k, with nk = |Pk|. Thus, we can re-write (1) via\nf(w) = K∑ k=1 nk n Fk(w) where Fk(w) = 1 nk ∑ i∈Pk fi(w).\nIf the partition Pk was formed by distributing the training examples over the clients uniformly at random, then we would have EPk [Fk(w)] = f(w), where the expectation is over the set of examples assigned to a fixed client k. This is the IID assumption typically made by distributed optimization algorithms; we refer to the case where this does not hold (that is, Fk could be an arbitrarily bad approximation to f ) as the Non-IID setting.\nIn data-center optimization, communication costs are relatively small, and computational costs dominate, with much of the recent emphasis being on using GPUs to lower these costs. In contrast, in federated optimization communication costs dominate: since the communication costs are symmetric, we will typically be limited by an upload bandwidth of 1 MB/s or less. Further, clients will typically only volunteer to participate in the optimization when they are charged, plugged-in, and on an unmetered wi-fi connection. Further, we expect each client will only participate in a small number of update rounds per day. On the other hand, since any single on-device dataset is small compared to the total dataset size, and modern smartphones have rel-\natively fast processors (including GPUs), computation becomes essentially free compared to communication costs for many model types. Thus, our goal is to use additional computation in order to decrease the number of rounds of communication needed to train a model. There are two primary ways we can add computation:\n• Increased parallelism Use more clients working independently between each communication round. • Increased computation on each client Rather than performing a simple computation like a gradient calculation, each client performs a more complex calculation between each communication round.\nWe investigate both of these approaches, but the speedups we achieve are due primarily to adding more computation on each client, once a minimum level of parallelism over clients is used."
    }, {
      "heading" : "1.3. Related Work",
      "text" : "In the convex setting, the problem of distributed optimization and estimation has received significant attention (Balcan et al., 2012; Fercoq et al., 2014; Shamir & Srebro, 2014), and some algorithms do focus specifically on communication efficiency (Zhang et al., 2013; Shamir et al., 2013; Yang, 2013; Ma et al., 2015; Zhang & Xiao, 2015). In addition to assuming convexity, this existing work generally requires that the number of clients is much smaller than the number of examples per client, that the data is distributed across the clients in IID fashion, and that each node has an identical number of data points — all of these assumptions are violated in the federated optimization setting. Asynchronous distributed forms of SGD have also been applied to training neural networks, e.g., Dean et al. (2012), but these approaches require a prohibitive number of updates in the federated setting.\nOne endpoint of the (parameterized) algorithm family we consider is simple one-shot averaging, where each client solves for the model that minimizes (possibly regularized) loss on their local data, and these models are averaged to produce the final global model. This approach has been studied extensively in the convex case with IID data, and it is known that in the worst-case, the global model produced is no better than training a model on a single client (Zhang et al., 2012; Arjevani & Shamir, 2015). Zinkevich et al. (2011) studies an averaging algorithm very similar to ours in the convex, balanced, IID setting.\nPerhaps the most relevant prior work is that of Shokri & Shmatikov (2015). They focus on training deep networks, emphasize the role of (global) differential privacy, and address communication costs by only sharing a subset of the parameters during each round of communication. However, they do not consider datasets that are unbalanced and\nNon-IID, properties that we believe are essential to the federated learning setting.\n2. The FederatedAveraging Algorithm The recent multitude of successful applications of deep learning have almost exclusively relied on variants of stochastic gradient descent (SGD) as the optimization algorithm; in fact, many advances can be understood as adapting the structure of the model (and hence the loss function) to be more amenable to optimization by simple gradientbased methods (Goodfellow et al., 2016). Thus, it is natural that we build algorithms for federated optimization by starting from SGD.\nSGD can be applied naively to the federated optimization problem, where a single minibatch gradient calculation (say on a randomly selected client) is done per round of communication. This approach is computationally efficient, but requires very large numbers of rounds of training to produce good models (e.g., even using an advanced approach like batch normalization, Ioffe & Szegedy (2015) trained MNIST for 50000 steps on minibatches of size 60).\nThe algorithm family we study, which we term FederatedAveraging (or FedAvg), allows us to add computation along both axes outlined above, with the goal of decreasing communication. The amount of computation is controlled by three key parameters: C, the fraction of clients that perform computation on each round; E, then number of training passes each client performs over its local dataset on each round; and B, the minibatch size used for the client updates. We write B = ∞ to indicate that the full local dataset is treated as a single minibatch.\nAt one endpoint of this algorithm family, we can take B = ∞ and E = 1 to produce a form of SGD with a varying minibatch size. This algorithm selects a C-fraction of clients on each round, and computes the gradient of the loss over all the data held by these clients. Thus, in this algorithm C controls the global batch size, with C = 1 corresponding to full-batch (non-stochastic) gradient descent. Since we still select batches by using all the data on the chosen clients, we refer to this simple baseline algorithm as FederatedSGD. While the batch selection mechanism is different than selecting a batch by choosing individual examples uniformly at random, the batch gradients g computed by FederatedSGD still satisfy E[g] = Of(w).\nA typical implementation of distributed gradient descent with a fixed learning rate η has each client k compute gk = OFk(wt), the average gradient on its local data at the current model wt, and the central server aggregates these\ngradients and applies the update\nwt+1 ← wt − η K∑\nk=1\nnk n gk,\nsince ∑K\nk=1 nk n gk = Of(wt). However, it is easy to check\nthat an equivalent update is given by ∀k, wkt+1 ← wt− ηgk and wt+1 ← K∑\nk=1\nnk n wkt+1.\nThat is, each client locally takes one step of gradient descent on the current model using its local data, and the server then takes a weighted average of the resulting models. This is in fact how FederatedSGD is implemented as a special case of FedAvg in Algorithm 1. Once the algorithm is written this way, it is natural to ask what happens when the client iterates the local update wk ← wk − ηOFk(wk) multiple times before the averaging step. For a client with nk local examples, the number of local updates per round is given by uk = E nkB ; complete pseudocode is given in Algorithm 1.\nOf course, for general non-convex objectives, averaging models in parameter space could produce an arbitrarily bad model. Following the approach of Goodfellow et al. (2015), we see exactly this bad behavior when we average two MNIST models2 trained from different initial conditions (Figure 1, left). For this figure, the parent models w and w′ were each trained on non-overlapping IID samples of 600 examples from the MNIST training set. Training\n2We use the “2NN” model architecture described in Section 3.\nAlgorithm 1 FederatedAveraging Server executes:\ninitialize w0 for each round t = 1, 2, . . . do St = (random set of max(C ·K, 1) clients) for each client k ∈ St in parallel do wkt+1 ← ClientUpdate(k,wt)\nwt+1 ← ∑K t=1 nk n w k t+1\nClientUpdate(k,w): // Executed on client k for each local epoch i from 1 to E do\nbatches← (data Pk split into batches of size B) for batch b in batches do w ← w − ηO`(w; b)\nreturn w to server\nwas via SGD with a fixed learning rate of 0.1 for 240 updates on minibatches of size 50 (or E = 20 passes over the mini-datasets of size 600). This is approximately the amount of training where the models begin to overfit their local datasets.\nHowever, recent work indicates that in practice, the loss surfaces of sufficiently over-parameterized NNs are surprisingly well-behaved and in particular less prone to bad local minima than previously thought (Dauphin et al., 2014; Goodfellow et al., 2015; Choromanska et al., 2015). And indeed, when we start two models from the same random initialization and then again train each independently on a different subset of the data (as described above), we find that naive parameter averaging works surprisingly well (Figure 1, right): the average of these two models, 1 2w + 1 2w\n′, achieves significantly lower loss on the full MNIST training set than the best model achieved by training on either of the small datasets independently.\nThe success of dropout training also provides some intuition for the success of our model averaging scheme; dropout training can be interpreted as averaging models of different architectures which share parameters, and the inference-time scaling of the model parameters is analogous to the model averaging used in FedAvg (Srivastava et al., 2014)."
    }, {
      "heading" : "3. Experimental Results",
      "text" : "We are motivated by both image classification and language modeling tasks where good models can greatly enhance the usability of mobile devices. For each of these tasks we pick a proxy dataset of modest enough size that we can thoroughly investigate the hyper-parameters of the FedAvg algorithm. Thus, while each individual training run is relatively small, we trained over 2000 individual models for these experiments.\nWe study three model families on two datasets. The first two are for the MNIST digit recognition task (LeCun et al., 1998):\n• A simple 2-hidden-layer model with 200 units per layer using ReLu activations (199,210 total parameters), which we refer to as the MNIST 2NN. • A CNN for MNIST with two 5x5 convolution layers (the first with 32 channels, the second with 64, each followed with 2x2 max pooling), a fully connected layer with 512 units and ReLu activation, and a final softmax output layer (1,663,370 total parameters).\nTo study federated optimization, we also need to specify how the data is distributed over the clients. We study two ways of partitioning the MNIST data: IID, where the data is shuffled, and then partitioned into 100 clients each receiving 600 examples, and Non-IID, where we first sort the data by digit label, divide it into 200 shards of size 300, and assign each of 100 clients 2 shards. This is a pathological non-IID partition of the data, as most clients will only have examples from two digits. Thus, this lets us explore the degree to which our algorithms will break on highly non-IID data. Both of these partitions are balanced, however.\nTo study federated optimization for language models, we built a dataset from The Complete Works of William Shakespeare.3 We construct a client dataset for each speaking role in each play with at least two lines. This produced a dataset with 1146 clients. For each client, we split the data into a set of training lines (the first 80% of lines for the role), and test lines (the last 20%, rounded up to at least one line). The resulting dataset has 3,564,579 characters in the training set, and 870,014 characters4 in the test set. This data is substantially unbalanced, with many roles having only a few lines, and a few with a large number of lines. Further, observe the test set is not a random sample of lines, but is temporally separated by the chronology of each play. Using an identical train/test split, we also form a balanced and IID version of the dataset, also with 1146 clients.\nOn this data we train a stacked character-level LSTM language model, which after reading each character in a line, predicts the next character (Kim et al., 2015). The model takes a series of characters as input and embeds each of these into a learned 8 dimensional space. The embedded characters are then processed through 2 LSTM layers, each with 256 nodes. Finally the output of the second LSTM layer is sent to a softmax output layer with one node per character. The full model has 866,578 parameters, and we trained using an unroll length of 80 characters.\n3Available as a single UTF-8 text file from https://www. gutenberg.org/ebooks/100\n4We always use character to refer to a one byte string, and use role to refer to a part in the play.\nSGD is sensitive to the tuning of the learning-rate parameter η. Thus, all of the results reported here are based on training over a sufficiently wide grid of learning rates (typically 11-13 values for η on a multiplicative grid of resolution 10 1 3 or 10 1 6 ). We checked to ensure the best learning rates were in the middle of our grids, and that there was not a significant difference between the best learning rates. Unless otherwise noted, we plot metrics for the best performing rate selected individually for each x-axis value. We find that the optimal learning rates do not vary too much as a function of the other parameters.\nIncreasing parallelism We first experiment with the effect of C, which controls the amount of multi-client parallelism. Table 1 shows the impact of varying C for both MNIST models. We report the number of communication rounds necessary to achieve a target test-set accuracy. To compute this, we construct a learning curve for each combination of parameter setting (optimizing η as described above), force the curve to be monotonically improving, and then compute the number of rounds where the curve reaches the target, using linear interpolation between the discrete points forming the curve. This is perhaps best understood by reference to Figure 2, where the light gray lines show the targets.\nWithB =∞ (e.g., for MNIST processing all 600 client examples as a single batch per round), there is only a small advantage in increasing the client fraction. Using the smaller batch size B = 10 shows a significant improvement in using C ≥ 0.1, especially in the Non-IID case. Based on these results, for most of the remainder of our experiments\nwe fix C = 0.1, which strikes a good balance between computational efficiency and convergence rate. While increasing C for a fixed B has a modest effect, comparing the number of rounds for B = ∞ and B = 10 shows a dramatic speedup. We investigate this in more detail in the next section.\nIncreasing computation per client In this section, we fix C = 0.1, and add more computation per client on each round, either decreasing B, increasing E, or both. The expected number of updates per client per round is given by u = (E[nk]/B)E = En/(kB), where the expectation is over the draw of a random client k. We see that increasing u by varying both E and B is effective. As long as B is large enough to take full advantage of available parallelism on the client hardware, there is essentially no cost in computation time for lowering it, and so in practice this should be the first parameter tuned.\nFigures 2 and 3 demonstrate that adding more local SGD updates per round and then averaging the resulting models can produce a dramatic speedup, and Tables 2 and 3 quantify these speedups. For the IID sharding of the MNIST data, using more computation per client decreases the number of rounds to reach the target accuracy by 35× for the\nCNN and 46× for the 2NN. The speedups for the pathologically sharded Non-IID data are smaller, but still substantial (2.8 – 3.7×). It is impressive that averaging provides any advantage (vs. actually diverging) when we naively average the parameters of models trained on entirely different pairs of digits. Thus, we view this as strong evidence for the robustness of this approach.\nThe unbalanced and non-IID distribution of the Shakespeare data (by role in the play) is much more representative of the kind of data distribution we expect for real-world applications. Encouragingly, for this problem learning on the non-IID and unbalanced data is actually much easier (a 95× speedup vs 13× for the balanced IID data); we conjecture this is largely due to the fact some roles have relatively large local datasets, which makes increased local training particularly valuable.\nInterestingly, for all three model classes, training runs\nbased on more local updates converge to a higher level of test-set accuracy than the baseline models. This trend continues even if the lines are extended beyond the plotted ranges. For example, for the CNN the B = ∞, E = 1 FedSGD model eventually reaches 99.22% accuracy after 1200 rounds (and had not improved further after 6000 rounds), while the B = 10, E = 20 FedAvg model reaches an accuracy of 99.44% after 300 rounds. We conjecture that in addition to lowering communication costs, model averaging produces a regularization benefit similar to that achieved by dropout (Srivastava et al., 2014).\nWe are primarily concerned with generalization performance, but FedAvg is effective at optimizing the training loss as well, even beyond the point where test-set accuracy plateaus. We observed similar behavior for all three model classes, and present plots for the MNIST CNN in Figure 4.\nCan we over-optimize on the client datasets? The current model parameters only influence the optimization performed in each ClientUpdate via initialization. Thus, as E → ∞, at least for a convex problem eventually the initial conditions should be irrelevant, and the global minimum would be reached regardless of initialization. Even for a non-convex problem, one might conjecture the algorithm would converge to the same local minimum as long as the initialization was in the same basin. That is, we would\nexpect that while one round of averaging might produce a reasonable model, additional rounds of communication (and averaging) would not produce further improvements.\nFigure 5 (top row) shows the impact of large E during initial training on the Shakespeare LSTM problem. Indeed, for very large numbers of local epochs, FedAvg can plateau or diverge.5 This result suggests that for some models, especially in the later stages of convergence, it may be useful to decay the amount of local computation per round (moving to smaller E or larger B) in the same way decaying learning rates can be useful. Figure 5 (bottom row) gives the analogous experiment for the MNIST CNN. Interestingly, for this model we see no significant degradation in the convergence rate for large values of E.\n5 Note that due to this behavior and because for largeE not all experiments for all learning rates were run for the full number of rounds, we report results for a fixed learning rate (which perhaps surprisingly was near-optimal across the range of E parameters) and without forcing the lines to be monotonic."
    }, {
      "heading" : "4. Conclusions and Future Work",
      "text" : "Our experiments show that federated learning has significant promise, as high-quality models can be trained using relatively few rounds of communication. Further empirical evaluation of the proposed approach on larger datasets that truly capture the massively distributed nature of realworld problems are an important next step. In order to keep the scope of algorithms explored tractable, we limited ourselves to building on vanilla SGD. Investigating the compatibility of our approach with other optimization algorithms such as AdaGrad (McMahan & Streeter, 2010; Duchi et al., 2011) and ADAM (Kingma & Ba, 2015), as well as with changes in model structure that can aid optimization, such as dropout (Srivastava et al., 2014) and batch-normalization (Ioffe & Szegedy, 2015), are another natural direction for future work."
    } ],
    "references" : [ {
      "title" : "Communication complexity of distributed convex learning and optimization",
      "author" : [ "Arjevani", "Yossi", "Shamir", "Ohad" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Arjevani et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Arjevani et al\\.",
      "year" : 2015
    }, {
      "title" : "Distributed learning, communication complexity and privacy",
      "author" : [ "Balcan", "Maria-Florina", "Blum", "Avrim", "Fine", "Shai", "Mansour", "Yishay" ],
      "venue" : "arXiv preprint arXiv:1204.3514,",
      "citeRegEx" : "Balcan et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Balcan et al\\.",
      "year" : 2012
    }, {
      "title" : "A neural probabilistic language model",
      "author" : [ "Bengio", "Yoshua", "Ducharme", "Réjean", "Vincent", "Pascal", "Janvin", "Christian" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2003
    }, {
      "title" : "Untraceable electronic mail, return addresses, and digital pseudonyms",
      "author" : [ "Chaum", "David L" ],
      "venue" : "Commun. ACM,",
      "citeRegEx" : "Chaum and L.,? \\Q1981\\E",
      "shortCiteRegEx" : "Chaum and L.",
      "year" : 1981
    }, {
      "title" : "The loss surfaces of multilayer networks",
      "author" : [ "Choromanska", "Anna", "Henaff", "Mikael", "Mathieu", "Michaël", "Arous", "Gérard Ben", "LeCun", "Yann" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Choromanska et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Choromanska et al\\.",
      "year" : 2015
    }, {
      "title" : "Computer, respond to this email",
      "author" : [ "Corrado", "Greg" ],
      "venue" : "http://googleresearch.blogspot.com/2015/ 11/computer-respond-to-this-email.html,",
      "citeRegEx" : "Corrado and Greg.,? \\Q2015\\E",
      "shortCiteRegEx" : "Corrado and Greg.",
      "year" : 2015
    }, {
      "title" : "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization",
      "author" : [ "Dauphin", "Yann N", "Pascanu", "Razvan", "Gülçehre", "Çaglar", "Cho", "KyungHyun", "Ganguli", "Surya", "Bengio", "Yoshua" ],
      "venue" : "uNIPS,",
      "citeRegEx" : "Dauphin et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Dauphin et al\\.",
      "year" : 2014
    }, {
      "title" : "Large scale distributed deep networks",
      "author" : [ "Dean", "Jeffrey", "Corrado", "Greg S", "Monga", "Rajat", "Chen", "Kai", "Devin", "Matthieu", "Le", "Quoc V", "Mao", "Mark Z", "Ranzato", "Marc’Aurelio", "Senior", "Andrew", "Tucker", "Paul", "Yang", "Ke", "Ng", "Andrew Y" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Dean et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Dean et al\\.",
      "year" : 2012
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "Duchi", "John", "Hazan", "Elad", "Singer", "Yoram" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Duchi et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2011
    }, {
      "title" : "Privacy aware learning",
      "author" : [ "Duchi", "John", "Jordan", "Michael I", "Wainwright", "Martin J" ],
      "venue" : "Journal of the Association for Computing Machinery,",
      "citeRegEx" : "Duchi et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2014
    }, {
      "title" : "The Algorithmic Foundations of Differential Privacy",
      "author" : [ "Dwork", "Cynthia", "Roth", "Aaron" ],
      "venue" : "Foundations and Trends in Theoretical Computer Science. Now Publishers,",
      "citeRegEx" : "Dwork et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Dwork et al\\.",
      "year" : 2014
    }, {
      "title" : "Fast distributed coordinate descent for non-strongly convex losses",
      "author" : [ "Fercoq", "Olivier", "Qu", "Zheng", "Richtárik", "Peter", "Takác", "Martin" ],
      "venue" : "In Machine Learning for Signal Processing (MLSP),",
      "citeRegEx" : "Fercoq et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Fercoq et al\\.",
      "year" : 2014
    }, {
      "title" : "Model inversion attacks that exploit confidence information and basic countermeasures",
      "author" : [ "Fredrikson", "Matt", "Jha", "Somesh", "Ristenpart", "Thomas" ],
      "venue" : "In ACM Conference on Computer and Communications Security,",
      "citeRegEx" : "Fredrikson et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Fredrikson et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep learning. Book in preparation for",
      "author" : [ "Goodfellow", "Ian", "Bengio", "Yoshua", "Courville", "Aaron" ],
      "venue" : null,
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2016
    }, {
      "title" : "Qualitatively characterizing neural network optimization problems",
      "author" : [ "Goodfellow", "Ian J", "Vinyals", "Oriol", "Saxe", "Andrew M" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2015
    }, {
      "title" : "Secure multiparty aggregation with differential privacy: A comparative study",
      "author" : [ "Goryczka", "Slawomir", "Xiong", "Li", "Sunderam", "Vaidy" ],
      "venue" : "In Proceedings of the Joint EDBT/ICDT 2013 Workshops,",
      "citeRegEx" : "Goryczka et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Goryczka et al\\.",
      "year" : 2013
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Hochreiter", "Sepp", "Schmidhuber", "Jürgen" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Hochreiter et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter et al\\.",
      "year" : 1997
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "Ioffe", "Sergey", "Szegedy", "Christian" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Ioffe et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ioffe et al\\.",
      "year" : 2015
    }, {
      "title" : "What can we learn privately",
      "author" : [ "Kasiviswanathan", "Shiva Prasad", "Lee", "Homin K", "Nissim", "Kobbi", "Raskhodnikova", "Sofya", "Smith", "Adam" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "Kasiviswanathan et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Kasiviswanathan et al\\.",
      "year" : 2008
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Kingma", "Diederik", "Ba", "Jimmy" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Kingma et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2015
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "Adding vs. averaging in distributed primal-dual optimization",
      "author" : [ "Ma", "Chenxin", "Smith", "Virginia", "Jaggi", "Martin", "Jordan", "Michael I", "Richtárik", "Peter", "Takáč" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Ma et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2015
    }, {
      "title" : "Adaptive bound optimization for online convex optimization",
      "author" : [ "McMahan", "H. Brendan", "Streeter", "Matthew" ],
      "venue" : "In COLT,",
      "citeRegEx" : "McMahan et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "McMahan et al\\.",
      "year" : 2010
    }, {
      "title" : "Distributed stochastic optimization and learning",
      "author" : [ "Shamir", "Ohad", "Srebro", "Nathan" ],
      "venue" : "In Communication, Control, and Computing (Allerton),",
      "citeRegEx" : "Shamir et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Shamir et al\\.",
      "year" : 2014
    }, {
      "title" : "Communication efficient distributed optimization using an approximate newton-type method",
      "author" : [ "Shamir", "Ohad", "Srebro", "Nathan", "Zhang", "Tong" ],
      "venue" : "arXiv preprint arXiv:1312.7853,",
      "citeRegEx" : "Shamir et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Shamir et al\\.",
      "year" : 2013
    }, {
      "title" : "Privacy-preserving deep learning",
      "author" : [ "Shokri", "Reza", "Shmatikov", "Vitaly" ],
      "venue" : "In Proceedings of the 22Nd ACM SIGSAC Conference on Computer and Communications Security, CCS",
      "citeRegEx" : "Shokri et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Shokri et al\\.",
      "year" : 2015
    }, {
      "title" : "Dropout: A simple way to prevent neural networks from overfitting",
      "author" : [ "Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Srivastava et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2014
    }, {
      "title" : "Simple demographics often identify people uniquely",
      "author" : [ "Sweeney", "Latanya" ],
      "venue" : null,
      "citeRegEx" : "Sweeney and Latanya.,? \\Q2000\\E",
      "shortCiteRegEx" : "Sweeney and Latanya.",
      "year" : 2000
    }, {
      "title" : "Regression model fitting under differential privacy and model inversion attack",
      "author" : [ "Wang", "Yue", "Si", "Cheng", "Wu", "Xintao" ],
      "venue" : "In Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "Wang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2015
    }, {
      "title" : "Consumer data privacy in a networked world: A framework for protecting privacy and promoting innovation in the global digital economy",
      "author" : [ "White House Report" ],
      "venue" : "Journal of Privacy and Confidentiality,",
      "citeRegEx" : "Report.,? \\Q2013\\E",
      "shortCiteRegEx" : "Report.",
      "year" : 2013
    }, {
      "title" : "Trading computation for communication: Distributed stochastic dual coordinate ascent",
      "author" : [ "Yang", "Tianbao" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Yang and Tianbao.,? \\Q2013\\E",
      "shortCiteRegEx" : "Yang and Tianbao.",
      "year" : 2013
    }, {
      "title" : "Communication-efficient distributed optimization of self-concordant empirical loss",
      "author" : [ "Zhang", "Yuchen", "Xiao", "Lin" ],
      "venue" : "arXiv preprint arXiv:1501.00263,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    }, {
      "title" : "Communication-efficient algorithms for statistical optimization",
      "author" : [ "Zhang", "Yuchen", "Wainwright", "Martin J", "Duchi", "John C" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2012
    }, {
      "title" : "Information-theoretic lower bounds for distributed statistical estimation with communication constraints",
      "author" : [ "Zhang", "Yuchen", "Duchi", "John", "Jordan", "Michael I", "Wainwright", "Martin J" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2013
    }, {
      "title" : "Parallelized stochastic gradient descent",
      "author" : [ "Zinkevich", "Martin", "Weimer", "Markus", "Smola", "Alexander J", "Li", "Lihong" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Zinkevich et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Zinkevich et al\\.",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : ", (Dean et al., 2012).",
      "startOffset" : 2,
      "endOffset" : 21
    }, {
      "referenceID" : 21,
      "context" : "For image classification feed-forward deep networks, and in particular convolutional networks, are wellknown to provide state-of-the-art results (LeCun et al., 1998; Krizhevsky et al., 2012).",
      "startOffset" : 145,
      "endOffset" : 190
    }, {
      "referenceID" : 20,
      "context" : "For image classification feed-forward deep networks, and in particular convolutional networks, are wellknown to provide state-of-the-art results (LeCun et al., 1998; Krizhevsky et al., 2012).",
      "startOffset" : 145,
      "endOffset" : 190
    }, {
      "referenceID" : 2,
      "context" : "For language modeling tasks recurrent neural networks, and in particular LSTMs, have achieved state-of-the-art results (Hochreiter & Schmidhuber, 1997; Bengio et al., 2003; Kim et al., 2015).",
      "startOffset" : 119,
      "endOffset" : 190
    }, {
      "referenceID" : 29,
      "context" : "Additional steps may also be needed to address model inversion attacks (Wang et al., 2015; Fredrikson et al., 2015).",
      "startOffset" : 71,
      "endOffset" : 115
    }, {
      "referenceID" : 12,
      "context" : "Additional steps may also be needed to address model inversion attacks (Wang et al., 2015; Fredrikson et al., 2015).",
      "startOffset" : 71,
      "endOffset" : 115
    }, {
      "referenceID" : 18,
      "context" : "A stronger guarantee can be achieved by enforcing local differential privacy (Kasiviswanathan et al., 2008; Duchi et al., 2014), where rather than adding noise to the final model, we noise the individual updates, which precludes the central server from making any definitive inference about a client.",
      "startOffset" : 77,
      "endOffset" : 127
    }, {
      "referenceID" : 9,
      "context" : "A stronger guarantee can be achieved by enforcing local differential privacy (Kasiviswanathan et al., 2008; Duchi et al., 2014), where rather than adding noise to the final model, we noise the individual updates, which precludes the central server from making any definitive inference about a client.",
      "startOffset" : 77,
      "endOffset" : 127
    }, {
      "referenceID" : 15,
      "context" : "It is also possible to use secure multiparty computation to perform aggregation over multiple client updates, allowing local differential privacy to be achieved using much less random noise (Goryczka et al., 2013).",
      "startOffset" : 190,
      "endOffset" : 213
    }, {
      "referenceID" : 1,
      "context" : "In the convex setting, the problem of distributed optimization and estimation has received significant attention (Balcan et al., 2012; Fercoq et al., 2014; Shamir & Srebro, 2014), and some algorithms do focus specifically on communication efficiency (Zhang et al.",
      "startOffset" : 113,
      "endOffset" : 178
    }, {
      "referenceID" : 11,
      "context" : "In the convex setting, the problem of distributed optimization and estimation has received significant attention (Balcan et al., 2012; Fercoq et al., 2014; Shamir & Srebro, 2014), and some algorithms do focus specifically on communication efficiency (Zhang et al.",
      "startOffset" : 113,
      "endOffset" : 178
    }, {
      "referenceID" : 34,
      "context" : ", 2014; Shamir & Srebro, 2014), and some algorithms do focus specifically on communication efficiency (Zhang et al., 2013; Shamir et al., 2013; Yang, 2013; Ma et al., 2015; Zhang & Xiao, 2015).",
      "startOffset" : 102,
      "endOffset" : 192
    }, {
      "referenceID" : 25,
      "context" : ", 2014; Shamir & Srebro, 2014), and some algorithms do focus specifically on communication efficiency (Zhang et al., 2013; Shamir et al., 2013; Yang, 2013; Ma et al., 2015; Zhang & Xiao, 2015).",
      "startOffset" : 102,
      "endOffset" : 192
    }, {
      "referenceID" : 22,
      "context" : ", 2014; Shamir & Srebro, 2014), and some algorithms do focus specifically on communication efficiency (Zhang et al., 2013; Shamir et al., 2013; Yang, 2013; Ma et al., 2015; Zhang & Xiao, 2015).",
      "startOffset" : 102,
      "endOffset" : 192
    }, {
      "referenceID" : 1,
      "context" : "In the convex setting, the problem of distributed optimization and estimation has received significant attention (Balcan et al., 2012; Fercoq et al., 2014; Shamir & Srebro, 2014), and some algorithms do focus specifically on communication efficiency (Zhang et al., 2013; Shamir et al., 2013; Yang, 2013; Ma et al., 2015; Zhang & Xiao, 2015). In addition to assuming convexity, this existing work generally requires that the number of clients is much smaller than the number of examples per client, that the data is distributed across the clients in IID fashion, and that each node has an identical number of data points — all of these assumptions are violated in the federated optimization setting. Asynchronous distributed forms of SGD have also been applied to training neural networks, e.g., Dean et al. (2012), but these approaches require a prohibitive number of updates in the federated setting.",
      "startOffset" : 114,
      "endOffset" : 814
    }, {
      "referenceID" : 33,
      "context" : "This approach has been studied extensively in the convex case with IID data, and it is known that in the worst-case, the global model produced is no better than training a model on a single client (Zhang et al., 2012; Arjevani & Shamir, 2015).",
      "startOffset" : 197,
      "endOffset" : 242
    }, {
      "referenceID" : 32,
      "context" : "This approach has been studied extensively in the convex case with IID data, and it is known that in the worst-case, the global model produced is no better than training a model on a single client (Zhang et al., 2012; Arjevani & Shamir, 2015). Zinkevich et al. (2011) studies an averaging algorithm very similar to ours in the convex, balanced, IID setting.",
      "startOffset" : 198,
      "endOffset" : 268
    }, {
      "referenceID" : 13,
      "context" : "The recent multitude of successful applications of deep learning have almost exclusively relied on variants of stochastic gradient descent (SGD) as the optimization algorithm; in fact, many advances can be understood as adapting the structure of the model (and hence the loss function) to be more amenable to optimization by simple gradientbased methods (Goodfellow et al., 2016).",
      "startOffset" : 354,
      "endOffset" : 379
    }, {
      "referenceID" : 13,
      "context" : "Following the approach of Goodfellow et al. (2015), we see exactly this bad behavior when we average two MNIST models2 trained from different initial conditions (Figure 1, left).",
      "startOffset" : 26,
      "endOffset" : 51
    }, {
      "referenceID" : 6,
      "context" : "However, recent work indicates that in practice, the loss surfaces of sufficiently over-parameterized NNs are surprisingly well-behaved and in particular less prone to bad local minima than previously thought (Dauphin et al., 2014; Goodfellow et al., 2015; Choromanska et al., 2015).",
      "startOffset" : 209,
      "endOffset" : 282
    }, {
      "referenceID" : 14,
      "context" : "However, recent work indicates that in practice, the loss surfaces of sufficiently over-parameterized NNs are surprisingly well-behaved and in particular less prone to bad local minima than previously thought (Dauphin et al., 2014; Goodfellow et al., 2015; Choromanska et al., 2015).",
      "startOffset" : 209,
      "endOffset" : 282
    }, {
      "referenceID" : 4,
      "context" : "However, recent work indicates that in practice, the loss surfaces of sufficiently over-parameterized NNs are surprisingly well-behaved and in particular less prone to bad local minima than previously thought (Dauphin et al., 2014; Goodfellow et al., 2015; Choromanska et al., 2015).",
      "startOffset" : 209,
      "endOffset" : 282
    }, {
      "referenceID" : 27,
      "context" : "The success of dropout training also provides some intuition for the success of our model averaging scheme; dropout training can be interpreted as averaging models of different architectures which share parameters, and the inference-time scaling of the model parameters is analogous to the model averaging used in FedAvg (Srivastava et al., 2014).",
      "startOffset" : 321,
      "endOffset" : 346
    }, {
      "referenceID" : 21,
      "context" : "The first two are for the MNIST digit recognition task (LeCun et al., 1998):",
      "startOffset" : 55,
      "endOffset" : 75
    }, {
      "referenceID" : 27,
      "context" : "We conjecture that in addition to lowering communication costs, model averaging produces a regularization benefit similar to that achieved by dropout (Srivastava et al., 2014).",
      "startOffset" : 150,
      "endOffset" : 175
    }, {
      "referenceID" : 8,
      "context" : "Investigating the compatibility of our approach with other optimization algorithms such as AdaGrad (McMahan & Streeter, 2010; Duchi et al., 2011) and ADAM (Kingma & Ba, 2015), as well as with changes in model structure that can aid optimization, such as dropout (Srivastava et al.",
      "startOffset" : 99,
      "endOffset" : 145
    }, {
      "referenceID" : 27,
      "context" : ", 2011) and ADAM (Kingma & Ba, 2015), as well as with changes in model structure that can aid optimization, such as dropout (Srivastava et al., 2014) and batch-normalization (Ioffe & Szegedy, 2015), are another natural direction for future work.",
      "startOffset" : 124,
      "endOffset" : 149
    } ],
    "year" : 2016,
    "abstractText" : "Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data-center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networks that proves robust to the unbalanced and non-IID data distributions that naturally arise. This method allows high-quality models to be trained in relatively few rounds of communication, the principal constraint for federated learning. The key insight is that despite the non-convex loss functions we optimize, parameter averaging over updates from multiple clients produces surprisingly good results, for example decreasing the communication needed to train an LSTM language model by two orders of magnitude.",
    "creator" : "LaTeX with hyperref package"
  }
}