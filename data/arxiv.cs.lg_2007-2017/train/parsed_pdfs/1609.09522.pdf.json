{
  "name" : "1609.09522.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "CHARGED POINT NORMALIZATION AN EFFICIENT SOLUTION TO THE SADDLE POINT PROBLEM",
    "authors" : [ "Armen Aghajanyan" ],
    "emails" : [ "armen.ag@live.com" ],
    "sections" : [ {
      "heading" : "1 SADDLE POINT PROBLEM",
      "text" : ""
    }, {
      "heading" : "1.1 INTRODUCTION",
      "text" : "Recently more and more attention has focused on the problem of saddle points in very high dimensional non-convex optimization. Saddle points represent points in the optimization problem where the first order gradients are all zero, but the stationary point is neither a maxima or a minima. The saddle point of a function can be confirmed by using the eigenvalues of Hessian matrix. If the set of eigenvalues contains at least one negative eigenvalue and at least one positive eigenvalue the point is said to be a saddle point. One way to analyze the prevalence of saddle point is to assign a joint probability density to the eigenvalues of the Hessian matrix at a critical point.\n• If the eigenvalues are all negative, then the critical point is a local maximum • If the eigenvalues are all positive, then the critical point is a local minimum. • If the eigenvalues contain at least one positive and at least one negative eigenvalue then the\npoint is a saddle point.\nIf p(λ1, λ2, ..., λn) is the joint probability density function then the probability that the Hessian matrix resembles a saddle point is, given that the Hessian is not singular is:\n1− ∫ ∞ 0 ∫ ∞ 0 ... ∫ ∞ 0 p(λ1, λ2, ..., λn)dλ1dλ2...dλn\n− ∫ 0 −∞ ∫ 0 −∞ ... ∫ 0 −∞ p(λ1, λ2, ..., λn)dλ1dλ2...dλn\n(1)\nAnother way to interpret the expression above is to realize that each of the two n-integrals represents the joint density summation of the two hyper-cubes, one in the direction of all the positive axis, and the other in all the negative axis. Each respectively representing minimas and maximas.\nTheorem 1 The space of eigenvalues of a non-singular Hessian matrix that represent minimas and maximas in comparison to the total space, decreases by 2n asymptotically.\nThe amount of unique hypercubes starting from the origin and spanning along the axis is 2n. The amount of hypercubes representing minimas and maximas is two. Therefore the fraction of the space that contains the eigenvalues that would indicate either a minima or a maxima is 21−n, where n represents the dimensionality of the Hessian matrix.\nWhat this shows is that as we increase the dimensionality of our optimization problem, the fraction of the total space that represents either a local minima or maxima decreases exponentially by a factor of two.\nAlthough this interpretation gives some intuition behind the saddle point problem, we cannot conclusively say that the probability of a critical point being a saddle point increases exponentially because we do not know the behavior of the joint probability function."
    }, {
      "heading" : "1.2 GRADIENT DESCENT BEHAVIOR AROUND SADDLE POINTS",
      "text" : "To understand the shortcomings of first order gradient descent algorithms around saddle points we will analyze the neighborhood a saddle point. Given a function f , the Taylor expansion around the saddle point x is given by:\nf(x+ δ) = f(x) + 1\n2 δTHδ (2)\nThe first order term disappears because we are at a critical point. Denoting e1, e2, ..., en as the eigenvectors of the non-degenerate Hessian H , and λ1, λ2, ..., λn as the respective eigenvalues, we can use the change of coordinates methods to rewrite the Taylor expansion in terms of the eigenvectors:\nv = 1\n2 eT1... eTn  δ f(x+ δ) = f(x) + 1\n2 n∑ i=1 λi(e T i δ) 2 = f(x) + n∑ i=1 λiv 2 i\n(3)\nFrom the last equation we can analyze the behavior of first order gradient descent algorithms. Specifically by looking at the behavior with respect to the signs of the eigenvalues. If eigenvalue λi is positive then the optimization point will move toward the critical point x. If eigenvalue λi is negative the optimization point will move away from the critical point.\nThis shows that the direction of the gradient descent algorithm is not the problem with gradient descent algorithms around saddle points, but rather the step of the algorithm. This problem is sometimes amplified because of the plateaus surrounding the critical point, as shown in (Saad & Solla, 1996). Another complication visible from equation 2, is that if the step size is greater than maxλ−1, the gradient descent algorithm will begin to diverge. Therefore one large eigenvalues of the surface of the error function, will force the gradient descent algorithms to take very small steps in all the other directions.\nA very similiar derivation and explanation was shows in (Dauphin et al., 2014)"
    }, {
      "heading" : "2 CHARGED POINT NORMALIZATION",
      "text" : ""
    }, {
      "heading" : "2.1 METAPHOR",
      "text" : "The metaphor for our method goes as follows. The current point in our optimization is a small positively charged point that is moving on the neutral surface of error. Our normalization works by dynamically placing other positively charged points around the surface of error to ‘push’ our optimization point away from undesirable positions. Optimally we would run the gradient descent algorithm until convergence, check if the converged point is a saddle point, place a positively charged point near the saddle point and continue the optimization. This metaphor was what gave inspiration to the derivation of our normalization."
    }, {
      "heading" : "2.2 INTRODUCTION",
      "text" : "The general optimization problem is defined as:\nL(f ;X,Y ) = n∑ i=1 V (f(Xi),Yi) + λR(f) (4)\nThe formulation is static, given the same function and the same X and Y the loss will always be equal. Our formulation introduces a dynamic normalization function R. Therefore the loss function becomes defined as:\nLt(f ;X,Y ) = n∑ i=1 V (f(Xi),Yi) + λRt(f) (5)\nThe function f contains dynamic parameters W t1 ,W t 2 , ...,W t n, while the function R contains parameters: β, p, φ and Ŵ t1 , Ŵ t 2 , ..., Ŵ t n, symbolizing the decay factor, norm, merge function and merge values respectfully. The t term inW tn represents the value ofWn at time t of the optimization algorithm. Charged Point Normalization is now defined as:\nRt(f) = e−βt∑n\ni=1 ||W ti − Ŵ ti ||p (6)\nThe update for the merge values is defined as:\nŴ t+1i = φ(Ŵ t i ,W t i )\nŴ 1i =W 1 i +\n(7)\nwhere is a source of random error to ensure we do not have a division by zero. In our experiments was a matrix of the same size as W 1i with random entries sampled from a normal distribution with a zero mean and a very small standard deviation.\nWhat this type of normalization attempts to do is to reward the optimization algorithm for taking steps that maximize the distance between the new point and the trailing point"
    }, {
      "heading" : "2.3 CHOICE OF HYPERPARAMETER",
      "text" : "The φ function can be any function that merges the two parameters into one parameter of the same dimension. Throughout this whole paper we used the exponential moving average for our φ function.\nφ(Ŵ ti ,W t i ) = αŴ t + (1− α)W ti α ∈ (0, 1)\n(8)\nAlthough to keep up with the metaphor Coulomb’s inverse squared law did not work as well as projected, through trial and error, the p value that worked the best was 1. The 1-norm simply is the sum of absolute values."
    }, {
      "heading" : "3 EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "3.1 INTRODUCTION",
      "text" : "Charged Point Normalization was implemented in Theano (Bastien et al., 2012) and integrated with the Keras (Chollet, 2015) library. We utilized the convolutional neural networks and recurrent networks in the keras library as well. All training and testing was run on a Nvidia GTX 980 GPU. We do not show results on a validation set, because we care about the efficiency and performance of the\noptimization algorithm, not whether or not it overfits. The over-fitting of a model is not the fault of the optimization routine but rather the fault of the field it is optimizing over. All comparisons between the standard and charged model, started with identical set’s of weights. Throughout all of our experiments, we utilize a softmax layer as the final layer, and consequently all the losses measured throughout this paper will be in terms of categorical cross-entropy. We used the train split of each data-set."
    }, {
      "heading" : "3.2 SIMPLE DEEP NEURAL NETWORKS",
      "text" : ""
    }, {
      "heading" : "3.2.1 MNIST: MULTILAYER PERCEPTRON",
      "text" : "The first test conducted was using a multilayer perceptron on the MNIST dataset. The architecture of the neural net contained layers with sizes 784→ 512→ 512→10. All intermediate layers contained rectified linear activations (He et al., 2015), while the final layer is a softmax layer. Between layers, dropout (Hinton et al., 2012) with a probability of 0.2 was added. We compare the standard batch gradient descent algorithm with a step size of 0.001 and batchsize of 128, on the net described above and the same net with Charged Point Normalization (CPN). The CPN hyper-parameters were: β = 0.001, λ = 0.1 with the moving average parameter α = 0.95. The loss we were optimizing over was categorical cross entropy."
    }, {
      "heading" : "3.2.2 MNIST:DEEP AUTOENCODER",
      "text" : "The second test conducted on simple neural networks, was in the form of an autoencoder. The architecture of the autoencoder contained layers with sizes 784→ 512→ 512→10→ 512→ 512→10. All layers contained rectified linear activations. Between layers, dropout with a probability of 0.2 was added. The set up of the experiment is almost identical to the previous experiment. The only difference being that in this case, we optimized for binary cross-entropy.\n0 20 40 60 80 100 0\n0.2\n0.4\n0.6\n0.8\n1\nEpoch\nLoss on MNIST Dataset (MLP)\nCharged Standard\n0 20 40 60 80 100 0.2\n0.4\n0.6\nEpoch\nLoss on MNIST Dataset (AutoEncoder)\nCharged Standard"
    }, {
      "heading" : "3.2.3 NOTES",
      "text" : "It is interesting to note that when the optimization problem is relatively simple, more specifically if the optimization algorithm takes smooth steps, CPN allows the optimization algorithm to take bigger steps in the correct direction. CPN does not display any periodic or chaotic behavior in this scenario. This is not the case for more complicated optimization problems that will be presented below."
    }, {
      "heading" : "3.3 CONVOLUTIONAL NEURAL NETWORKS",
      "text" : ""
    }, {
      "heading" : "3.3.1 CIFAR10",
      "text" : "The next experiment conducted was using a convolutional neural network on the CIFAR10 (Krizhevsky et al., a). The architecture was as such:\nConvolution2D (32,3,3) → ReLU → Convolution2D (32,3,3) → ReLU → MaxPooling (2,2) → Dropout (0.25)→ Convolution2D (64,3,3)→ ReLU→ Convolution2D (64,3,3)→ ReLU→MaxPooling (2,2)→Dropout (0.25)→Dense (512)→ReLU→Dropout (0.5)→Dense (10)→ Softmax Convolution2D takes the parameters, number of filters, width and height respectfully. Dense take one parameter describing the size of the layer. MaxPooling takes two parameters that signify the pool size. ReLU is the rectified linear function, while Softmax is the softmax activation function.\nThe optimization algorithm used was stochastic gradient descent with a learning rate of 0.01, decay of 1e − 6, momentum of 0.9, with nesterov acceleration. The batch size used was 32. The hyperparameters for CPN were: β = 0.01, λ = 0.1 with the moving average parameter α = 0.95. 10,000 random images were used from the CIFAR10 data-set instead of the full dataset to speed up learning.\n0 20 40 60 80 100 0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n1.4\n1.6\n1.8\nEpoch\nLoss on CIFAR10 Dataset\nCharged Standard\n0 20 40 60 80 100 0\n0.2\n0.4\n0.6\n0.8\n1\nEpoch\nAccuracy on CIFAR10 Dataset\nCharged Standard\nIt is interesting to note that CPN performs worse until the optimization algorithm reaches the ‘elbow’ of the curve, where then CPN continues along its path, while the standard algorithm begins to converge. CPN also takes steps that are much less ‘optimal’ in the greedy sense, which is why both the loss and accuracy curve behave partially chaotic."
    }, {
      "heading" : "3.3.2 CIFAR100",
      "text" : "The CIFAR100 (Krizhevsky et al., b) setup was nearly identical to the CIFAR10 setup. The same architecture of the neural network was used. The only difference was in the λ parameter in the normalization term, which in this case was equal to 0.01. 20, 000 random images were used.\n0 20 40 60 80 100 0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\n5\nEpoch\nLoss on CIFAR100 Dataset\nCharged Standard\n0 20 40 60 80 100 0\n0.2\n0.4\n0.6\n0.8\n1\nEpoch\nAccuracy on CIFAR100 Dataset\nCharged Standard\nThe same behavior as in the CIFAR10 experiment was exhibited. The elbow of the loss curve was the point where CPN began to outperform standard optimization."
    }, {
      "heading" : "3.4 RECURRENT NEURAL NETWORKS",
      "text" : ""
    }, {
      "heading" : "3.4.1 INTRODUCTION",
      "text" : "Recurrent neural networks are notorious for being hard to train, and having a tendency to generally underfit (Pascanu et al., 2012). In this section we show that CPN successfully escapes saddle points presented in recurrent neural networks.\nSentence Embedding Question Embedding\nRNN RNN\nConcat\nDense"
    }, {
      "heading" : "3.4.2 BABI",
      "text" : "We selected the path-finding problem of the BABI dataset due to it being the most difficult task. The architecture consisted of two recurrent networks and one standard neural network. Each of the recurrent neural networks had a structure: Embedding→ RNN. The embedding, sentence and query hidden layer size was set to 3. The final network concatenated the two recurrent network outputs and fed the result into a dense layer with an output size of vocabsize. Refer to figure 1 for a diagram. We ran our experiment with two different recurrent neural network structures: Gated Recurrent Units (GRU) (Chung et al., 2014) and Long Short Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) . The ADAM (Kingma & Ba, 2014) optimization algorithm was used for both recurrent structures with the parameters: α = 0.001, β1 = 0.9, β2 = 0.999, = 1e − 08. For the LSTM architecture, CPN hyper-parameters were: β = 0.0025, λ = .03, α = 0.95. For the GRU architecture, CPN hyper-parameters were: β = 0.1, λ = .1, α = 0.95.\nFrom the accuracy graphs we can see the CPN causes the recurrent network to escape the saddle point earlier than a recurrent network with no CPN."
    }, {
      "heading" : "4 NORMALIZATION BEHAVIOR",
      "text" : ""
    }, {
      "heading" : "4.1 EXPLORATION VS EXPLOITATION",
      "text" : "In a standard gradient descent with no normalization, the updates taken by the algorithm are always greedy, in terms of always minimizing the loss of the model. Their is no exploration done;\n-0 .0 05 0. 0 0. 00 5 0. 01 0. 01 5 0. 02 0. 02 5 0. 03 0. 03 5 0. 04\n0\n10\n20\n30\n40\n50\n60 I->H\n-0 .0 2 0. 0 0. 02 0. 04 0. 06 0. 08 0. 1 0. 12 0. 14 0. 16\n0\n10\n20\n30\n40\n50\n60 H->O\nCharged: Eigenvalue Distribution\n-0 .0 05 0. 0 0. 00 5 0. 01 0. 01 5 0. 02 0. 02 5 0. 03 0. 03 5\n0\n10\n20\n30\n40\n50\n60\n70\n80 I->H\n-0 .0 5 0. 0 0. 05 0. 1 0. 15 0. 2 0. 25\n0\n1\n2\n3\n4\n5\n6\n7\n8 H->O\nVanilla: Eigenvalue Distribution\ngradient descent is by nature a greedy algorithm, optimizing only locally. What CPN allows the gradient descent to do, is to move in non-optimal directions early on in the optimization routine, while still allowing for precise finetuning at the end of the model. This trade-off is controlled by the β parameter."
    }, {
      "heading" : "4.2 BEHAVIOR AROUND SADDLE POINTS",
      "text" : "A vanilla neural network with one single hidden layer was trained on a down sampled 8× 8 version of the MNIST dataset (Lecun et al., 1998). Full gradient descent was ran on the 10, 000 random images until convergence. We compare the differences between the eigenvalue distributions between the neural network with CPN and the neural network without it. Recall the tighter the range of the eigenvalues is, the larger steps the gradient descent algorithm can take without worrying about divergence as explained in section 1.2.\nThe graph above shows a kernel density estimation done on the input to hidden and hidden to output Hessian’s at the near critical point. There are both negative and positive eigenvalues, especially in the hidden to output weights, therefore it is safe enough to say that we are at a saddle point (Turlach, 1993). The first graph represents the CPN neural network while next graph represents a non-normalized neural network. The CPN network shows a tighter distribution as well as more of the eigenvalues being focused near 0."
    }, {
      "heading" : "4.3 TOY EXAMPLE",
      "text" : "To ensure that the normalization is actually repelling the optimization point from saddle points, and that the results achieved in the experimental section are not due to some confounding factors we utilize a low-dimensional experiment to show the repelling effects of CPN.\nWe utilize the monkey saddle as the optimization surface. The monkey saddle has a saddle point surrounded by plateaus in all directions. It is defined as x3 − 3xy2. Referring to section 1.2, we discussed that the direction of gradient descent algorithms was not the shortcoming of gradient descent algorithms around saddle points,but rather the problem was with the step size. What CPN should in theory do is allow the optimization routine to take larger steps.\nBelow are two figures. The first one shows the behavior of a five common gradient descent algorithms starting at a point near the saddle point (point: (x = 0.0001, y = −0.0001)) (Zeiler, 2012), (Duchi et al., 2010). The next figure shows the same algorithms starting at the same point but utilizing CPN. All visualization were done using the matplotlib library (Hunter, 2007).\nThe hyper-parameters used, were all the default hyper-parameters in the keras library apart from Adam (to make it visible on the graphs). All hyper-parameters are available in Table 1. SGD Accelerated refers to the standard SGD algorithm using momentum and nesterov acceleration (Nesterov, 1983).\nEach algorithm performed better when coupled with CPN than without, the loss was computed using the monkey saddle equation above. All the losses for both CPN and Non-CPN are available in Table 2. CPN allowed the optimization algorithms to escape the saddle point quickly even though the gradient near the starting point of the optimization was near zero.\n• Without CPN only the Adam algorithm escaped the plateau in less than a 1000 iterations.\n• With CPN every algorithm apart from AdaGrad successfully escaped the plateau in less than 120 iterations, most notable being SGD Accelerated, which escaped in just 8 iterations.\nFrom this toy example we can conclude that CPN does in fact repel the optimization algorithm away from saddle points, and therefore the results from the experiments are due to this phenomena and most likely no other confounding factors."
    }, {
      "heading" : "4.4 PERIODICITY AND TERMINAL BEHAVIOR",
      "text" : "As shown in the experiments done on the CIFAR datasets, CPN has a tendency to force the optimization algorithm to act more chaotically. The exponential term in the normalization term is there to ensure that the optimization algorithm does not get stuck in an periodic path. It is trivial to see that as the time of the optimization goes toward infinity the impact of the normalization will tend toward 0. Therefore if the optimization algorithm does not reach a local minimum, but is rather in an elliptical path, assuming that the λ term is not great enough to push the point out of the local minimum, the optimization algorithm will eventually reach the local minimum."
    }, {
      "heading" : "5 NOTES ON HYPER-PARAMETERS",
      "text" : "Due to restrictions on our hardware resources, we did not have enough time to run a comprehensive study on the behavior of CPN with respect to its hyper-parameters. Throughout this paper the selection of hyper-parameters was kept rather simple. We selecting the hyper-parameters in a feasible range, and then adjusted them by hand around 4-8 times. So in no way are the hyper-parameters for CPN chosen in this paper optimal for the various setups explained, but yet the results we found were somewhat substantial, which we find quite optimistic."
    }, {
      "heading" : "6 WEAKNESSES",
      "text" : "• CPN with a exponential moving average for the φ function, introduces 2 extra hyperparameters, not including the normalization scalar λ.\n• In terms of implementation; CPN doubles the amount of memory needed for the optimization problem, as a trailing copy of the parameters must be kept.\n• The fraction term in CPN will generally contain small floating points in both numerator and denominator and this can sometimes lead to numerical instability.\n• If saddle points are reached at a really late time in the optimization algorithm, the exponential decay will nullify the effects of CPN. A possible solution would be to substitute the exponential decay term with some type of periodic decay."
    }, {
      "heading" : "7 CONCLUSION",
      "text" : "In this paper we introduced a new type of dynamic normalization that allows gradient based optimization algorithms to escape saddle points. We showed empirical results on standard data-sets, that show CPN successful escapes saddle points on various neural network architectures. We discussed the theoretical properties of first order gradient descent algorithms around saddle points as well as discussed the influence of the largest eigenvalue on the step taken. Empirical results were shown that confirmed the hunch that the hessian of charged point normalized neural networks contains eigenvalues which are less in magnitude than their non-normalized counterpart."
    } ],
    "references" : [ {
      "title" : "Theano: new features and speed improvements",
      "author" : [ "Frédéric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio" ],
      "venue" : "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,",
      "citeRegEx" : "Bastien et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bastien et al\\.",
      "year" : 2012
    }, {
      "title" : "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "author" : [ "Junyoung Chung", "Çaglar Gülçehre", "KyungHyun Cho", "Yoshua Bengio" ],
      "venue" : "CoRR, abs/1412.3555,",
      "citeRegEx" : "Chung et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2014
    }, {
      "title" : "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization",
      "author" : [ "Yann Dauphin", "Razvan Pascanu", "Çaglar Gülçehre", "Kyunghyun Cho", "Surya Ganguli", "Yoshua Bengio" ],
      "venue" : "CoRR, abs/1406.2572,",
      "citeRegEx" : "Dauphin et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Dauphin et al\\.",
      "year" : 2014
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "John Duchi", "Elad Hazan", "Yoram Singer" ],
      "venue" : "Technical Report UCB/EECS-2010-24, EECS Department,",
      "citeRegEx" : "Duchi et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2010
    }, {
      "title" : "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "CoRR, abs/1502.01852,",
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Improving neural networks by preventing co-adaptation of feature detectors",
      "author" : [ "Geoffrey E. Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov" ],
      "venue" : "CoRR, abs/1207.0580,",
      "citeRegEx" : "Hinton et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2012
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural Comput.,",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Matplotlib: A 2d graphics environment",
      "author" : [ "J.D. Hunter" ],
      "venue" : "Computing In Science & Engineering,",
      "citeRegEx" : "Hunter.,? \\Q2007\\E",
      "shortCiteRegEx" : "Hunter.",
      "year" : 2007
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba" ],
      "venue" : "CoRR, abs/1412.6980,",
      "citeRegEx" : "Kingma and Ba.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "A method of solving a convex programming problem with convergence rate O(1/sqr(k))",
      "author" : [ "Yurii Nesterov" ],
      "venue" : "Soviet Mathematics Doklady,",
      "citeRegEx" : "Nesterov.,? \\Q1983\\E",
      "shortCiteRegEx" : "Nesterov.",
      "year" : 1983
    }, {
      "title" : "Dynamics of on-line gradient descent learning for multilayer neural networks",
      "author" : [ "David Saad", "Sara A. Solla" ],
      "venue" : "Advances in neural information processing systems,",
      "citeRegEx" : "Saad and Solla.,? \\Q1996\\E",
      "shortCiteRegEx" : "Saad and Solla.",
      "year" : 1996
    }, {
      "title" : "Bandwidth Selection in Kernel Density Estimation: A Review",
      "author" : [ "Berwin A. Turlach" ],
      "venue" : "In CORE and Institut de Statistique, pp",
      "citeRegEx" : "Turlach.,? \\Q1993\\E",
      "shortCiteRegEx" : "Turlach.",
      "year" : 1993
    }, {
      "title" : "ADADELTA: an adaptive learning rate method",
      "author" : [ "Matthew D. Zeiler" ],
      "venue" : "CoRR, abs/1212.5701,",
      "citeRegEx" : "Zeiler.,? \\Q2012\\E",
      "shortCiteRegEx" : "Zeiler.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "A very similiar derivation and explanation was shows in (Dauphin et al., 2014)",
      "startOffset" : 56,
      "endOffset" : 78
    }, {
      "referenceID" : 0,
      "context" : "1 INTRODUCTION Charged Point Normalization was implemented in Theano (Bastien et al., 2012) and integrated with the Keras (Chollet, 2015) library.",
      "startOffset" : 69,
      "endOffset" : 91
    }, {
      "referenceID" : 4,
      "context" : "All intermediate layers contained rectified linear activations (He et al., 2015), while the final layer is a softmax layer.",
      "startOffset" : 63,
      "endOffset" : 80
    }, {
      "referenceID" : 5,
      "context" : "Between layers, dropout (Hinton et al., 2012) with a probability of 0.",
      "startOffset" : 24,
      "endOffset" : 45
    }, {
      "referenceID" : 1,
      "context" : "We ran our experiment with two different recurrent neural network structures: Gated Recurrent Units (GRU) (Chung et al., 2014) and Long Short Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) .",
      "startOffset" : 106,
      "endOffset" : 126
    }, {
      "referenceID" : 11,
      "context" : "There are both negative and positive eigenvalues, especially in the hidden to output weights, therefore it is safe enough to say that we are at a saddle point (Turlach, 1993).",
      "startOffset" : 159,
      "endOffset" : 174
    }, {
      "referenceID" : 12,
      "context" : "0001)) (Zeiler, 2012), (Duchi et al.",
      "startOffset" : 7,
      "endOffset" : 21
    }, {
      "referenceID" : 3,
      "context" : "0001)) (Zeiler, 2012), (Duchi et al., 2010).",
      "startOffset" : 23,
      "endOffset" : 43
    }, {
      "referenceID" : 7,
      "context" : "All visualization were done using the matplotlib library (Hunter, 2007).",
      "startOffset" : 57,
      "endOffset" : 71
    }, {
      "referenceID" : 9,
      "context" : "SGD Accelerated refers to the standard SGD algorithm using momentum and nesterov acceleration (Nesterov, 1983).",
      "startOffset" : 94,
      "endOffset" : 110
    } ],
    "year" : 2016,
    "abstractText" : "Recently, the problem of local minima in very high dimensional non-convex optimization has been challenged and the problem of saddle points has been introduced. This paper introduces a dynamic type of normalization that forces the system to escape saddle points. Unlike other saddle point escaping algorithms, second order information is not utilized, and the system can be trained with an arbitrary gradient descent learner. The system drastically improves learning in a range of deep neural networks on various data-sets in comparison to non-CPN neural networks. 1 SADDLE POINT PROBLEM 1.",
    "creator" : "LaTeX with hyperref package"
  }
}