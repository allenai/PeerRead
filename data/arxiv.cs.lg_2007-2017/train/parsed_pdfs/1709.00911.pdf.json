{
  "name" : "1709.00911.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Neural Networks for Safety-Critical Applications - Challenges, Experiments and Perspectives",
    "authors" : [ "Chih-Hong Cheng", "Frederik Diehl", "Yassine Hamza", "Gereon Hinz", "Georg Nührenberg", "Markus Rickert", "Harald Ruess", "Michael Troung-Le" ],
    "emails" : [ "lastname@fortiss.org" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms—autonomous driving, neural network, dependability, certification, formal verification, research challenges\nI. INTRODUCTION\nThe recent burst of applying artificial neural network (ANN) technologies has created an impact on applications such as autonomous driving. Although using ANN-based techniques had shown great promise (e.g., substantially superior image recognition [6]) compared to classical approaches, there have been huge barriers in using neural networks in safety critical domains (e.g., report from NASA [2]).\nIn this paper, we propose a methodology for enabling the usage of ANN by considering reasonable extensions for existing safety standards (Sec. II). We examine the technology readiness of our proposed methodology by applying a case study regarding highway motion prediction for autonomous driving (Sec. III), and address further research needs (Sec. IV)."
    }, {
      "heading" : "II. CERTIFICATION CONSIDERATIONS FOR DEPENDABLE NEURAL NETWORKS",
      "text" : "For certification of safety critical systems, safety is established by rigorous engineering processes (i.e., these processes are defined in a way such that engineering complying to these processes can eliminate or prevent errors). Although it is more process-oriented than function-oriented, the basic principle of (1) ensuring that the specification is correct and (2) ensuring that an implementation satisfies the specification is well perceived. Table I summarizes three critical aspects over the underlying intention of certifying safety-critical systems, namely specification validity, implementation understandability, and implementation correctness.\n• The validity of specification is important to ensure that one “builds the right system”. Several methods can be used in this regard, such as prototyping, design-time analysis and reviews, or product acceptance tests. • The well-behaving of an implementation is captured by two aspects: (1) understandability via requirement-tocode traceability, and (2) correctness via extensive testing,\nwith coverage criterion such as Modified Condition / Decision Coverage (MC/DC).\nAlthough these approaches are valid for classical engineering using V-models, applying them on neural networks has created the following issues:\n• (Black-box structure) For ANN-based systems, implementations consist of layers of neurons operating on and transforming high-dimensional vectors. This makes understandability arguments such as fine-grained requirement-to-code traceability difficult. • (Testing for correctness claims) Depending on the activation function, applying traditional coverage-based approaches makes the system testing either trivially satisfiable or almost intractable. (i) When one uses tan−1\nas the activation function, one only needs one test case to satisfy MC/DC as there is no if-then-else branch in every neuron. (ii) When one uses ReLU as the activation function, every neuron contains an if-then-else statement. MC/DC is then intractable, as branching possibilities are exponential to the number of neurons. • (Implicit specification) For implementing systems using ANNs, the specification refers to a combination of data (which specifies input-output behaviors) as well as classical specifications for domain knowledge such as traffic or safety rules. The “specification knowledge” inside the data is implicit, compared to cases such as traffic rules.\nBased on the above issues, Table I further summarizes our considered additions towards safety certification of ANNs.\n(A) (Neuron-to-feature understandability) One should provide confidence regarding the meaning of a neural network by associating individual neurons with conditions (features) when it can be activated. (B) (From testing to formal analysis) The result of certification should provide (best effort) correctness claims over the (partially incomplete) classical specification, such as obeying traffic rules or ensuring road safety. As testing approaches its limitation, we suggest to apply formal methods such as static analysis or symbolic reasoning. (C) (Validating the “new specification”) One needs to check the validity of the data, to ensure that only sanitized data will be used in training. For examples such as autonomous driving, one needs to enhance raw data with sure guarantees such as no data containing risky driving has been introduced for training the maneuver of vehicles. ar X\niv :1\n70 9.\n00 91\n1v 1\n[ cs\n.S E\n] 4\nS ep\n2 01\n7"
    }, {
      "heading" : "III. CASE STUDY: HIGHWAY MOTION PREDICTION FOR",
      "text" : "AUTONOMOUS VEHICLES\nWe outline how we applied the strategy above in verifying a highway overtaking ANN-based motion predictor used in autonomous driving (developed by Lenz et al. [7]). Figure 1 provides a snapshot on the simulation of the vehicle.\nIn Figure 1, the ANN-based predictor takes three categories of inputs: (i) its own speed profile, (ii) parameters of its nearest surrounding vehicles for each orientation, and (iii) the road condition. The total number of input variables to the network is 84. Given the current state of the perceived environment, it produces in real-time the probability distribution over all possible actions for a vehicle, characterized as a Gaussian mixture model. The action of the ego vehicle is decomposed into two parts: (i) indicator over possible lateral velocity (i.e., if it is feasible to switch lanes), and (ii) indicator over longitudinal acceleration (i.e., if it is feasible to accelerate). In Figure 1, the motion predictor on the right suggests to slightly decelerate and to switch to left lanes, as the generated Gaussian mixture is within the lower left part.\nOne of the most critical safety requirements is to ensure that if there is a vehicle in the left of the ego vehicle, the predictor never suggests a large left velocity to the ego vehicle; when such a scenario occurs, it may lead to crashes. In this example, it is regulated that the mean value of the probability distribution should be limited to certain threshold.\nOnce we validated that the training data never contains such inputs (as in Sec. II (A)), we perform formal verification (as in Sec. II (B)) following the methodology developed by Cheng et al. [3], which encodes the structure of a neural\nnetwork into a set of mixed integer linear constraints. With the technology we are able to successfully verify safety properties. Surprisingly, we have trained a couple of neural networks under the same data, but not all of them can guarantee the safety property (see Fig. II for a summary of verification results, being experimented on a Google VM with 12 Cores)."
    }, {
      "heading" : "IV. CONCLUDING REMARKS",
      "text" : "The proposed certification methodology, during the case study, has also indicated further research needs.\n(i) During the study, we found that implementation understandability can only be partially achieved by technologies such as deconvolution [8].\n(ii) Scalability of automated verification requires improvement (cf. Table II for required verification time). Recent results on quantized neural networks [5] might make verification more scalable via an encoding to bitvector theories in SMT [4].\n(iii) Apart from verification, another important direction is to consider training under known properties on the target function (known as hints [1]), such as safety rules."
    } ],
    "references" : [ {
      "title" : "Hints",
      "author" : [ "Y.S. Abu-Mostafa" ],
      "venue" : "Neural Computation 7(4), pages 639–671",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Certification considerations for adaptive systems",
      "author" : [ "S. Bhattacharyya", "D. Cofer", "D Musliner", "J. Mueller", "E. Engstrom" ],
      "venue" : "In: ICUAS, pages 270– 279. IEEE",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Maximum resilience of artificial neural networks",
      "author" : [ "C.-H. Cheng", "G. Nührenberg", "H. Rueß" ],
      "venue" : "ATVA",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Quantized neural networks: Training neural networks with low precision weights and activations",
      "author" : [ "I. Hubara", "M. Courbariaux", "D. Soudry", "R. El-Yaniv", "Y. Bengio" ],
      "venue" : "In: arXiv:1609.07061",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "NIPS, pages 1097–1105",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Deep neural networks for markovian interactive scene prediction in highway scenarios",
      "author" : [ "D. Lenz", "F. Diehl", "M. Troung Le", "A. Knoll" ],
      "venue" : "In: IV. IEEE",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Adaptive deconvolutional networks for mid and high level feature learning",
      "author" : [ "M. Zeiler", "G. W Taylor", "R. Fergus" ],
      "venue" : "ICCV, pages 2018– 2025. IEEE",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : ", substantially superior image recognition [6]) compared to classical approaches, there have been huge barriers in using neural networks in safety critical domains (e.",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 1,
      "context" : ", report from NASA [2]).",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 5,
      "context" : "[7]).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[3], which encodes the structure of a neural TABLE II RESULTS OF VERIFYING ANN-BASED MOTION PREDICTORS.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "(i) During the study, we found that implementation understandability can only be partially achieved by technologies such as deconvolution [8].",
      "startOffset" : 138,
      "endOffset" : 141
    }, {
      "referenceID" : 3,
      "context" : "Recent results on quantized neural networks [5] might make verification more scalable via an encoding to bitvector theories in SMT [4].",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 0,
      "context" : "(iii) Apart from verification, another important direction is to consider training under known properties on the target function (known as hints [1]), such as safety rules.",
      "startOffset" : 145,
      "endOffset" : 148
    } ],
    "year" : 2017,
    "abstractText" : "We propose a methodology for designing dependable Artificial Neural Networks (ANN) by extending the concepts of understandability, correctness, and validity that are crucial ingredients in existing certification standards. We apply the concept in a concrete case study in designing a high-way ANNbased motion predictor to guarantee safety properties such as impossibility for the ego vehicle to suggest moving to the right lane if there exists another vehicle on its right.",
    "creator" : "LaTeX with hyperref package"
  }
}