{
  "name" : "1305.4076.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Contractive De-noising Auto-encoder",
    "authors" : [ "Fu-qiang Chen", "Yan Wu", "Guo-dong Zhao", "Jun-ming Zhang", "Ming Zhu", "Jing Bai" ],
    "emails" : [ "yanwu@tongji.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "De-noising auto-encoder (DAE) is an improved auto-encoder which is robust to the input by corrupting the original data first and then reconstructing the original input by minimizing the reconstruction error function. And contractive auto-encoder (CAE) is another kind of improved auto-encoder to learn robust feature by introducing the Frobenius norm of the Jacobean matrix of the learned feature with respect to the original input. In this paper, we combine de-noising auto-encoder and contractive auto-encoder, and propose another improved auto-encoder, contractive de-noising auto-encoder (CDAE), which is robust to both the original input and the learned feature. We stack CDAE to extract more abstract features and apply SVM for classification. The experiment result on benchmark dataset MNIST shows that our proposed CDAE performed better than both DAE and CAE, proving the effective of our method.\nKeywords: De-noising auto-encoder, Contractive auto-encoder, deep learning, SVM"
    }, {
      "heading" : "1 Introduction",
      "text" : "Generic neural network is composed of three layers, one input layer, one hidden layer and one output layer. Generally, for the input layer, we need only input the original data to it. While for the hidden layer, we need to compute a kind of transformation of the input to get a kind of feature from the input by a special kind of feature mapping function. And for the output layer, we can get the values of the neurons in the layer as the label of the corresponding input also by a special kind of feature mapping function. In 1988, Bourlard and Kamp [1] first proposed auto-association, a kind of neural network which replaces the output layer with the data same as the input layer, while in\n Corresponding a ：uthor. Email yanwu@tongji.edu.cn\nthe generic neural network the output layer is set with the label of the corresponding input sample. And auto-association is the original auto-encoder. It can be learned by error back propagation (BP) [2]. In 2006, Hinton and Salakhutdinov [3] stacked several layers of auto-encoders to get deep auto-encoders for dimension reduction, and they pre-trained the deep auto-encoder with restricted Boltzmann machine (RBM) [4], which is a special kind of neural network based on Boltzmann machine. Here the restricted is referred to that there are no intra-layer connections both in the visible layer and the hidden layer, and there are only inter-layer connections between the neurons in the visible layer and the hidden layer. In 2008, Vicent P. et al. [5] proposed de-noising auto-encoder (DAE), a kind of improved auto-encoder which first corrupts the original data and then transforms or encodes the corrupted input to the hidden layer and then decodes to the 'corrupted' 1 output layer, where the 'real' output layer is identical to the original input layer. Then by error back propagation, de-noising auto-encoder is robust to the original input. In 2010, Vicent P. et al. stacked de-noising auto-encoder to form deep neural network to learn useful representations [6], and they showed that SDAE performed better than deep belief nets (DBN) in several cases. Besides, they found DAE could learn Gaborlike edge detectors from natural image patches and larger stroke detectors from digit images [6]. Then in 2011, Rifai et al. [7] proposed another kind of improved auto-encoder, contractive auto-encoder (CAE). They improved auto-encoder by introducing the Frobenius norm of the Jacobean matrix of all the learned features in the hidden layer with respect to the original input for the reconstruction loss function in AE. CAE can be regarded as an approach for nonlinear dimension reduction related to manifold learning. And they found that CAE outperformed DAE on some datasets. For DAE, it only takes the corruption of the original data into consideration, which makes the auto-encoder robust to the input. While for CAE, it adds a penalty term to the reconstruction function on the basis of the original auto-encoder. And it makes the learned feature robust. In this paper, we combined DAE and CAE, and we proposed contractive de-noising auto-encoder (CDAE).\n1 Here, we call the output layer 'corrupted' because the output layer is got based on the 'corrupted'\ninput.\nCDAE is both robust to the input and also makes the learned feature robust by adding a penalty term to the reconstruction loss function. After extracting feature with CDAE, we apply SVM for classification. And our experiment on benchmark dataset MNIST shows that our method surpasses both CAE and DAE, which proves the effective of our method."
    }, {
      "heading" : "2 CDAE",
      "text" : "In this section, we will present our proposed algorithm, CDAE. Firstly, we will introduce the formulation of generic auto-encoder and its two variants, i.e., DAE and CAE."
    }, {
      "heading" : "2.1 AE",
      "text" : "Let ( in this paper, is a column vector unless otherwise stated ) represents a specific input or sample for AE, the input layer size or the dimension for the input,\nthe size of the hidden layer or the number of neurons in the hidden layer.\nTo encode into the hidden layer, we can use the following activation function:\n= ( + ), (1)\nwhere ( ) =\n.\nAnother commonly used activation function is\nℎ( ) =\n.\nIn formula (1), is a matrix of × X (usually < X), while is a × 1 bias vector for the hidden layer. Thence, + is a column vector, and the sigmoid function maps + element-wisely, then we can get a column vector of size × 1. Then we need to decode the hidden layer to the output layer according to the following formula:\n= ( + ),\nwhere the subscript 'rec' means reconstruction considering that the output of an auto-encoder is the input itself. And is a × 1 bias vector for the output layer.\nLast, we need to compute and minimize the loss function or the reconstruction cost\nfunction with respect to , and by error back propagation as following:\nMinimize = ∑ ( − ) ∈ (2)\nwhere is the set composed of all the input samples, while is the cardinality2 of set . And here the loss function is the squared error function, which is a widely used cost function in neural network. Another form of cost function (cross-entropy loss function) is as follows:\n ( , ) = = − ∑ + (1 − ) (1 − ) .\nFor clarity, we show the architecture of the auto-encoder in Fig. 1."
    }, {
      "heading" : "2.2 DAE",
      "text" : "In DAE, we need to corrupt the original data firstly. There are two common ways to corrupt the original data, additive isotropic Gaussian noise and binary masking noise.\n2 For finite set, the cardinality of a set is the number of the elements in the set. For detail, the\nreaders are referred to http://en.wikipedia.org/wiki/Cardinality\nHere, we denote the corrupted version of the original input sample as x . Then, for additive isotropic 3 Gaussian noise,\n= + ( , ),\nwhere σ is a small constant maybe usually smaller than 0.5 [6]. And is the identity matrix, i.e., with all the elements in the diagonal equal 1 while other elements all equal 0. And for binary masking noise, a small fraction of the input sample is set to be zero, and the fraction can take values on 10%, 25% etc. For a specific input , first we need to get the corrupted version of , x . Then we can get\n= ( + ),\nand satisfies the same condition as in AE.\nSubsequently, we can get the reconstructed version of input by\n= + .\nFinally, we can get the object function of DAE as following:\nMinimize = ∑ ( − ) ∈ (3)\nFor clarity, we show the architecture of de-noising auto-encoder in Fig. 2."
    }, {
      "heading" : "2.3 CAE",
      "text" : "We can get CAE by adding one kind of penalty term to the original cost function (i.e. (2)) of AE. The added term is the Frobenius norm4 of the Jacobean matrix of the\n3 Here, the 'isotropic' means that the variances of all the input dimensions are equal. 4 For detail of Frobenius norm, the readers are referred to\nlearned feature in the hidden layer with respect to the original input. The corresponding formula can be shown as following:\nMinimize = ∑ (( − ) + ‖ ( )‖ ) ∈ . (4)\nIn formula (4), the λ is a parameter and it takes values of 0.1 in this paper [7]. For activation function , the second term in formula (4) can be calculated as following:\n‖ ( )‖ = ∑ (1 − ) ∑\n,\nfor activation function ℎ, the second term in formula (4) can be calculated as following:\n‖ ( )‖ = (1 + )(1 − )\n."
    }, {
      "heading" : "2.4 CDAE",
      "text" : "From the above formula (2-4), we can see that DAE improves the traditional AE by corruption-reconstruction (first corrupt the original data and then reconstruct the original data by minimize the reconstruction loss function), which makes auto- encoder robust to the input. While CAE improves the traditional AE by introducing a penalty term in the object function for every sample, the Frobenius norm of the Jacobin matrix of the learned hidden feature with respect to the original input. CAE makes AE robust by changing the object function, which indeed makes some neurons in the hidden layer less active to learn more helpful and robust feature. To combine the advantages of both CAE and DAE, we propose another improved AE. Here we call it contractive de-noising auto-encoder (CDAE). The object function of CDAE can be written as following:\n= ∑ (( − ) + ‖ ( )‖ ) ∈ . (5)\nhttp://mathworld.wolfram.com/FrobeniusNorm.html\nWe first corrupt the original data x to get x . Then we can map x to the hidden layer and we can get h . Subsequently, we can get the reconstruction of the original sample x by decoding h . From the object function, we can see that CDAE is a directly improved version of CAE, by adding the corruption and de-noising. And similarly, CDAE is also a directly improved version of DAE, by adding a penalty term, the Forbenius norm of the learned feature with respect to the corrupted version of the original input, to the object function."
    }, {
      "heading" : "3 Experiment",
      "text" : "Considering that we'll apply support vector machine (SVM) [8] in our experiment, we'll introduce SVM briefly in this section. SVM is a commonly adopted classifier in various applications. It is based on kernel function, dual problem and convex quadratic programming. For any two specific inputs, and y, there are three common kernel functions in support vector machine as following:\n( , ) = ( ∙ + 1) , (6)\n( , ) =\n‖ ‖\n, (7)\n( , ) = ℎ( ∙ − ). (8)\nThe formula (6-8) is polynomial kernel function; Gaussian radial basis kernel function and ℎ kernel function respectively. Among them, the Gaussian radial basis kernel function is the most popular one. And we adopt Gaussian radial basis function in our experiment. Transforming the original problem to the dual form of the original problem is another feature of SVM. When the original problem is hard to solve, sometimes we can resort to its dual problem to make it easier to solve. The convex quadratic programming guarantees that we can find a global optimal solution for the problem. For neural network, it is incline to get a local optimal solution. Our experiment data set is from MNIST [9] and to get the .mat form, the readers are referred to [10]. The experiment is implemented on matlab2011b, win32, Intel(R) Core (TM) i3-2310, CPU @ 2.10GHz 2.00GB (RAM). Confined to the memory, we\nchose 18,000 of all the samples 5 for our experiment, 1,800 for each of the number between 0 and 9. For the 18,000 samples, we chose one half for training and another half for testing. For each number between 0 and 9, we all chose half for training and half for testing. And we applied one of the most common classifier, support vector machine (SVM) [11] for classification. In our experiment, the architecture of the deep auto-encoder is 784-200- 100-200-784 and 784-200-50-200-784 [12]. And we chose the third layer (with 100 or 50 neurons) to classify with SVM. The activation function we applied is ℎ. And we corrupt the original input with masking noise, and we mask the original data as follows: we implement the masking by making the elements of the input with the index as 1:80:784. Besides, we initialize the weight matrix with each element in the following range [12]:\n− √6\n+ , +\n√6\n+ .\nAnd we initialize the bias vector and with . For clarity, we show the architecture we use in our experiment (Fig. 3.).\nThe experiment result is shown in the following table (Table 1 and Table 2). And we can see that our method performs best, beating AE, DAE and CAE.\n5 The MNIST database of handwritten digits has a training set of 60,000 examples, and a test set\nof 10,000 examples."
    }, {
      "heading" : "4 Conclusion",
      "text" : "In this paper, we proposed CDAE, a novel improved auto-encoder by combining CAE and DAE. CAE can make less neuron in the hidden layer active to learn more meaningful, abstract and robust features for classification, while DAE is robust to the input by first corrupting the original data and then minimizing the reconstruction loss function. Our proposed method CDAE combines the advantages of both CAE and DAE. The experiment (classification with SVM) shows that CDAE outperforms both CAE and DAE, which also shows the effective of CDAE. Since CDAE performed better on MNIST, we'll try CDAE on other problems in future."
    }, {
      "heading" : "Acknowledgement",
      "text" : "The authors thank Yuan-fang Ren for helpful discussion."
    } ],
    "references" : [ {
      "title" : "Auto-association by multilayer perceptrons and singular value decomposition",
      "author" : [ "H. Bourlard", "Y. Kamp" ],
      "venue" : "Biological cybernetics, vol.59",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1988
    }, {
      "title" : "Learning representations by back-propagating errors",
      "author" : [ "D.E. Rumelhart", "G.E. Hinton", "R.J. Williams" ],
      "venue" : "Nature, vol. 323,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1986
    }, {
      "title" : "Reducing the dimensionality of data with neural networks",
      "author" : [ "G.E. Hinton", "R.R. Salakhutdinov" ],
      "venue" : "Science, vo. 313,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2006
    }, {
      "title" : "A fast learning algorithm for deep belief nets",
      "author" : [ "G.E. Hinton", "S. Osindero", "Y.W. Teh" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2006
    }, {
      "title" : "Extracting and composing robust features with denoising autoencoders",
      "author" : [ "P. Vincent", "H. Larochelle", "Y. Bengio", "P.A. Manzagol" ],
      "venue" : "Proceedings of the 25th international conference on Machine learning,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2008
    }, {
      "title" : "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion",
      "author" : [ "P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.A. Manzagol" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2010
    }, {
      "title" : "Contractive auto- encoders: Explicit invariance during feature Extraction",
      "author" : [ "S. Rifai", "P. Vincent", "X. Muller", "X. Glorot", "Y. Bengio" ],
      "venue" : "Proceedings of the 28th International Conference on Machine Learning,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2011
    }, {
      "title" : "A tutorial on support vector machines for pattern recognition”, Data mining and knowledge discovery",
      "author" : [ "C.J. Burges" ],
      "venue" : "vo.l.2, no",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1998
    }, {
      "title" : "LIBSVM: a library for support vector machines",
      "author" : [ "Chih-Chung Chang", "Chih-Jen Lin" ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2011
    }, {
      "title" : "On optimization methods for deep learning",
      "author" : [ "J. Ngiam", "A. Coates", "A. Lahiri", "B. Prochnow", "A. Ng", "Q.V. Le" ],
      "venue" : "Proceedings of the 28th International Conference on Machine Learning,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "In 1988, Bourlard and Kamp [1] first proposed auto-association, a kind of neural network which replaces the output layer with the data same as the input layer, while in",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 1,
      "context" : "It can be learned by error back propagation (BP) [2].",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 2,
      "context" : "In 2006, Hinton and Salakhutdinov [3] stacked several layers of auto-encoders to get deep auto-encoders for dimension reduction, and they pre-trained the deep auto-encoder with restricted Boltzmann machine (RBM) [4], which is a special kind of neural network based on Boltzmann machine.",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 3,
      "context" : "In 2006, Hinton and Salakhutdinov [3] stacked several layers of auto-encoders to get deep auto-encoders for dimension reduction, and they pre-trained the deep auto-encoder with restricted Boltzmann machine (RBM) [4], which is a special kind of neural network based on Boltzmann machine.",
      "startOffset" : 212,
      "endOffset" : 215
    }, {
      "referenceID" : 4,
      "context" : "[5] proposed de-noising auto-encoder (DAE), a kind of improved auto-encoder which first corrupts the original data and then transforms or encodes the corrupted input to the hidden layer and then decodes to the 'corrupted' 1",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "stacked de-noising auto-encoder to form deep neural network to learn useful representations [6], and they showed that SDAE performed better than deep belief nets (DBN) in several cases.",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 5,
      "context" : "Besides, they found DAE could learn Gaborlike edge detectors from natural image patches and larger stroke detectors from digit images [6].",
      "startOffset" : 134,
      "endOffset" : 137
    }, {
      "referenceID" : 6,
      "context" : "[7] proposed another kind of improved auto-encoder, contractive auto-encoder (CAE).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "5 [6].",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 6,
      "context" : "1 in this paper [7].",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 7,
      "context" : "Considering that we'll apply support vector machine (SVM) [8] in our experiment, we'll introduce SVM briefly in this section.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 8,
      "context" : "And we applied one of the most common classifier, support vector machine (SVM) [11] for classification.",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 9,
      "context" : "In our experiment, the architecture of the deep auto-encoder is 784-200- 100-200-784 and 784-200-50-200-784 [12].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 9,
      "context" : "Besides, we initialize the weight matrix with each element in the following range [12]:",
      "startOffset" : 82,
      "endOffset" : 86
    } ],
    "year" : 2014,
    "abstractText" : "Auto-encoder is a special kind of neural network based on reconstruction. De-noising auto-encoder (DAE) is an improved auto-encoder which is robust to the input by corrupting the original data first and then reconstructing the original input by minimizing the reconstruction error function. And contractive auto-encoder (CAE) is another kind of improved auto-encoder to learn robust feature by introducing the Frobenius norm of the Jacobean matrix of the learned feature with respect to the original input. In this paper, we combine de-noising auto-encoder and contractive auto-encoder, and propose another improved auto-encoder, contractive de-noising auto-encoder (CDAE), which is robust to both the original input and the learned feature. We stack CDAE to extract more abstract features and apply SVM for classification. The experiment result on benchmark dataset MNIST shows that our proposed CDAE performed better than both DAE and CAE, proving the effective of our method.",
    "creator" : "þÿ"
  }
}