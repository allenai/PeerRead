{
  "name" : "1203.1483.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning Random Kernel Approximations for Object Recognition",
    "authors" : [ "Eduard Gabriel Băzăvan", "Fuxin Li", "Cristian Sminchisescu" ],
    "emails" : [ "eduard.bazavan@imar.ro", "fli@cc.gatech.edu", "cristian.sminchisescu@ins.uni-bonn.de" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "The proper choice of kernel function and its hyperparameters are crucial to the success of applying kernel methods to practical applications. These selections span a number of different problems: from choosing a single width parameter in radial basis kernels, scaling different feature dimensions with different weights [6], to learning a linear or a nonlinear combination of multiple kernels (MKL) [17]. In complicated practical problems such as computer vision, sometimes the need of multiple kernels arises naturally [34, 14]. Images can be represented using descriptors based on shape, color and texture, and these descriptors have different roles\nin classifying different categories. In such situations it is in principle easy to design a kernel classifier where each kernel represents one of the descriptors and the classifier would be based on a weighted combination of kernels with category dependent learnt weights.\nA natural difficulty in kernel learning is scalability. Kernel methods scale with an already mediocre time complexity of at least O(N2.3) with respect to the size N of the training set, but combining multiple kernels or learning the hyperparameters of a single kernel significantly slows down training. Some speed-ups apply for specific kernels, but only in limited scenarios [22]. In consequence, most of the kernel learning approaches so far are only capable to handle a few thousand training examples at most. This is insufficient for the current age of massive datasets, such as a 11 million images ImageNet, or the 19 million articles within Wikipedia.\nAn emerging technique that can in principle speed up the costly kernel method while at the same time preserving its non-linear predictive power is the random Fourier feature methodology (RFF) [24, 35, 19]. By sampling components from the frequency space of the kernel using Monte Carlo methods, RFF obtains a bounded, approximate representation of a kernel embedding that may initially span an infinite-dimensional space. Many operations are simplified once such representation is available, the most notable being that any kernel learning algorithm would now scale as O(N), where N is the number of examples. This opens the path for applying kernel methods to the realm of massive datasets.\nIn the seminal work on random Fourier features [24], the methodology was developped primarily for radial basis kernels. Recent work [35, 19] focused on extending this technique to a number of other useful kernels defined on histogram features (empirical estimates of multinomial distributions), such as the χ2 and histogram intersection measures. However, the potential of the linear random\n1\nar X\niv :1\n20 3.\n14 83\nv1 [\ncs .C\nV ]\n7 M\nar 2\nFourier methodology for kernel learning remains largely unexplored. In this paper we develop the methodology for learning both single kernel and for multiple kernel combinations in the Fourier domain and show that these produce accurate and efficient models. We conduct experiments in visual object recognition, using the difficult PASCAL Visual Object Challenges 2011 dataset, in order to demonstrate the performance of the proposed Fourier kernel learning methodology and compare against non-linear learning algorithms, designed to operate in the original kernel space."
    }, {
      "heading" : "2. Related work",
      "text" : "Approaches to kernel learning can be broadly classified into methods that estimate the hyper-parameters of a single kernel[25, 6, 15] and methods that learn the weights of a linear combinations of kernels and possibly their hyperparameters – the so-called multiple kernel learning framework or MKL[31, 2, 16, 36].\nA popular approach for single kernel learning is the gradient-based method pursued by Chapelle et al. [6, 15]. Keerthi et al. [15] give an efficient algorithm that alternates between learning an SVM and optimizing the feature weights. Cortes et al. [8] propose a two-stage method based on a modification of a classical kernel alignment [9] metric and prove a number of learning bounds. Approaches to single kernel learning under a kernel prior have been pursued both as semi-definite programs[17] and within unconstrained optimization formulations[11], although in both cases the optimization is involved and complexity is an issue. More recent methods attempt to track the entire regularization path in the non-linear case [27, 20].\nMultiple kernel learning provides a powerful conceptual framework for both model combination and model selection and has attracted significant research recently. Initial approaches originating with work by Lanckriet et al. [17] estimated a linear combination of kernels using semi-definite programming. This was reformulated by Bach et al. [3] as block-norm regularization, reducing the optimization problem to a second order cone program applicable to medium scale problems. More recent methods [7] learn a polynomial combination of kernels. A number of approaches have pursued different lp-norms for kernel selection [31, 16, 36]. Hierarchical kernel learning approaches like (HKL) [2] perform feature selection combinatorially, by choosing kernel combinations obtained by mapping to a directed acyclic graph. The search for such combinations can then be performed in polynomial time.\nA difficulty with many single or multiple kernel learning formulations is their relatively unfavorable scaling properties. When kernels are used, such methods usually scale at least quadratically with the number of examples. Sometimes scaling with the number of kernel parameters and the number of kernels can also become an issue. A formulation\nof kernel learning within a linear Fourier framework, as pursued in this paper, carries the promise of better scalability and wider applicability to large datasets, while at the same time preserving the non-linear predictive power that makes kernel methods attractive."
    }, {
      "heading" : "3. Learning Kernels Parameters",
      "text" : ""
    }, {
      "heading" : "3.1. Random Fourier Approximations",
      "text" : "Kernels offer substantial power and flexibility to predictive models. Central to their use is the ‘kernel trick’ which allows the design of algorithms that depend only on the inner product of their arguments. This is based on the property of positive definite kernel functions k(·, ·) to define a dot product and a lifting φ so that the dot product between lifted data points can be efficiently computed as φ(x)>φ(y) = k(x,y). Algorithms become linear in a complex feature space induced by φ, but access the data only through evaluations of the kernel k in input space. This makes possible to handle complex or infinite dimensional feature space mappings φ, and indeed these usually give the best results in visual recognition datasets [13, 18]. However the very advantage that made kernel methods popular– the kernel trick, requires the manipulation of matrices of pairwise examples. For large datasets, the scaling of kernel methods is at least quadratic in the number of examples. This makes their direct usage impractical beyond datasets of 105 elements. The methodology we pursue is to approximate the kernel function linearly using random feature maps.\nKey to the random Fourier methodology is Bochner’s theorem, that connects positive definite kernels and their Fourier transforms [28, 24, 19]. For positive definite translation-invariant kernels k(x,y) = k(x − y) on Rm, Bochner’s theorem guarantees that every kernel is the inverse Fourier transform of a proper probability distribution µ. Defining ζ(x) = ejx\n>γ (with j the imaginary unit), the following equality holds:\nkσ(x,y) = ∫ Rm ej(x−y) >γdµk(γ)\n= Eµ[ζγ(x)ζγ(y) ∗] ≈ φΓ(x)>φΓ(y)\nwhere ∗ is the (complex) conjugate, and\nφΓ(x) =\n√ 2\nd\n[ cos ( x>γi + 2πbi )] i=1,d\n(1)\nis the random feature map at frequencies Γ = {γ1, . . . ,γd}, with b ∼ U[0, 2π], the uniform distribution in the interval [0, 2π]. We approximate the expectation Eµ[ζγ(x)ζγ(y)∗] by means of a Monte-Carlo sample\ndrawn from the distribution µk. This is an operation on linear functions with explicit features. The algorithm for the change of representation has two steps: i) Generate d random samples Γ from the distribution µk; ii) Compute the random projection φΓ using (1), for all training examples. The approximation has the convergence rate of Monte Carlo methods, O(d−1/2), dependent on the random sample size, but independent of the input dimension. One usually needs up to a few thousand dimensions to approximate the original kernel accurately. Datasets containing hundreds of thousands of examples can be trained in a few hours, for simpler models. This motivates our interest in advancing the fundamental theory and practice of kernel learning for an efficient class of approximations with randomized feature dimension, with linear scaling in the number of examples, and with predictable convergence rates."
    }, {
      "heading" : "3.2. Single Kernel Learning",
      "text" : "First we consider learning strategies for the hyperparameters σ of the kernel k based on the Fourier feature methodology. To achieve this goal we need a learning criterion and an algorithm for optimizing the hyper-parameters. Since model training is much faster due to the linear dependence on the training set size, we can perform more complex parameter learning than classic approaches that usually rely on a grid search procedure, like cross-validation. This efficiency of the linear formulation also allows us to scale parameter learning towards numbers of parameters which are unattainable with classical methods. Our approach to kernel learning will optimize the hyper-parameters with respect to the error on a held-out validation set, for models obtained on the training set. This has been shown to be a viable procedure to prevent overfitting[13].\nIn the sequel we denote X and y the matrix of inputs or covariates (row-wise organized) and y the matrix of targets on the training set, respectively, whereas U and v are the validation inputs and targets, respectively. We use φΓ, introduced earlier, as our random Fourier feature map but we will drop Γ to simplify notation. We define φ(X) to be the feature map applied to all the rows of the matrix X.\nWe will learn the hyper-parameters of a kernel ridge regression model β ≡ β(σ) (other margin-based training costs, such as hinge loss, logistic loss, etc., can be similarly adopted into the framework)\nmin β\n1 2 ‖φ(X)β − y‖22 + λ‖β‖22. (2)\nFor this problem the optimum can be obtained in closed form as\nβ = ( φ(X)>φ(X) + λId )−1 φ(X)>y. (3)\nTo learn the hyper-parametersσ we optimize the squared l2 validation error. Given f = φ(U)Tβ − v and r(σ) a\nregularizer for the hyper-parameters, e.g. r(σ) = ‖σ‖22 we optimize\nmin σ ‖f‖22 + r(σ) (4)\nNote that we can easily obtain a gradient with regard to the kernel parameters for this optimization. The random feature mapφ establishes a connection between the original input representation X and the approximation of its lifting φ(X). This can be used to derive analytical expressions not only for kernel expansions in the input space but also for the gradient with regard to the kernel parameters. We can minimize the loss using a local-descent based optimization.\nGiven σi the i’th dimension of the kernel parameter vector σ, then\n∂ ‖f‖22 ∂σi = Tr (∂ ‖f‖22 ∂f )> ∂f ∂σi  (5) = f> ∂f\n∂σi = f>\n( ∂φ(U)\n∂σi β + φ(U)\n∂β\n∂σi ) For translation invariant kernels, we can easily compute φΓ. Manipulating the sampling distribution µk associated with kernel k in an optimization process identifies the different frequency bands that are useful for prediction on training data. This effectively leads to a posterior frequency distribution, given µk as a prior. Generating samples from this posterior and optimizing with respect to their expectation on the training set gives a direct way of learning the parameters of the kernel. However as the kernel hyper-parameters σ change, Γ needs to be re-sampled. Although this is feasible, changing the sampling distribution introduces conceptual difficulties during optimization as the objective function change, and sampling becomes a source of noise and non-smoothness for the optimization. Importance sampling can be used to avoid resampling at each step, but if the parametrized frequency distribution drifts far away from the starting distribution, then the original samples will have little importance weights, and resampling would potentially be needed nevertheless for convergence. Such difficulties can be overcome for several interesting kernels that belong to a class of functions where the samples from µk can be written as γ = σ · h(ω) (6) where h is the quantile function, ω are uniformly sampled and fixed, and · is the Hadamard product of two vectors. In this case, throughout the entire optimization process, sampling needs to be done only once from the uniform distribution for ω. When σ is changed, samples from the new distribution are generated automatically from ω. Examples of such kernels are the Gaussian, the generalized skewedχ2 and the generalized skewed intersection[19] (see table 1).\nBased on this property, by differentiating the feature map (1) we obtain\n∂φ(uk)\n∂σi =\n√ 2\nd\n[ ∂ cos(u>k (σ · h(ωj)) + 2πbj)\n∂σi\n] j=1,d (7)\n=\n√ 2\nd\n[ −u>k,ih(ωj,i) sin(u>k (σ · h(ωj)) + 2πbj) ] j=1,d\nTo compute the second term ∂β∂σi , we first define the matrix Q = φ(X)>φ(X) + λId and using the standard result ∂Q−1\n∂σi = −Q−1 ∂Q∂σiQ −1, we obtain\n∂β ∂σi =\n∂Q−1\n∂σi φ(X)>y + Q−1\n( ∂φ(X)\n∂σi\n)> y (8)\n= −Q−1 ∂Q ∂σi\nQ−1φ(X)>y + Q−1 ( ∂φ(X)\n∂σi\n)> y\nIt is easy to see that ∂φ(X)∂σi can be computed in the same way as in equation (7), as the gradient of Q with respect to σi can be obtained as\n∂Q ∂σi =\n( ∂φ(X)\n∂σi\n)> φ(X) + φ(X)> ∂φ(X)\n∂σi . (9)\nComputing the gradient of r(σ) will depend on the type of regularization chosen. In general this is a smooth function of σ (e.g. l2 norm) so it will be straightforward to compute ∂r(σ) ∂σi\n. Now that we have a closed form formula for the gradient with respect to all kernel parameters in the Fourier domain we can plug it into a non-linear optimizer and estimate the parameters.\nThis overall philosophy bears a certain similarity to the objective introduced in [6] where the authors use a gradient descent learning technique for kernel ridge regression based on the exact kernel matrix. It is also similiar to the multiple kernel learning technique for products of kernels introduced in [33]. However, besides the technical differences that are introduced by the usage of a Fourier embedding map, an important advantage for our methodology is that we do not have to store the kernel matrices in memory\nwhich has O(N2) memory cost. Our memory requirement is just O(Nd + d2) where d is the size of the Fourier embedding. The computational complexity of our method is dominated byO(iskl(Nd2 +d3 +rNd)) where d is the size of the random Fourier features, N is the number of training samples, r is the number of parameters and iskl is the number of iterations to convergence. O(Nd2 + d3) is the cost of computing matrix Q and inverting it."
    }, {
      "heading" : "3.3. Multiple Kernel Learning",
      "text" : "Previous work has proven an underlying connection between multiple kernel learning and group Lasso[1]. Although this is an interesting property, it has found relatively limited practical applications so far. In this section we show that by using the random Fourier methodology, we can do just the opposite and lift group Lasso to the kernel domain. We propose a multiple kernel learning formulation where the features are initially transformed using the random Fourier framework, concatenated, and group Lasso is applied to them (RFF-GL). We prove that this new formulation is equivalent with the multiple kernel learning formulation introduced in [33] and then compare the two methodologies. Experiments show that both approaches have similar performance. In contrast, group Lasso based on random Fourier features runs faster and scales better since we do not need to compute or store the Gramm matrices associated with the features.\nLet X be the input matrix for the training set organized row-wise, y the associated targets and {k1, . . . , kr} a set of kernels. For example X could be a concatenation of multiple image features such as SIFT or HOG and therefore should be represented as {X1, . . . ,Xr}. But to simplify the notation we would not refer to the individial components and will just use X when we refer to the input. We denote Fi the matrix of random Fourier features obtained from approximating ki on inputs X. We concatenate Fi columnwise to obtain a matrix F on which we can apply group Lasso with r non-overlapping groups, each corresponding to the Fourier embedding of a different kernel. We use the l1-l2 formulation [37] which can be written as:\nmin w λ r∑ i=1 ‖wFi‖2 + l(y,Fw), (10)\nwhere wFi are the weights applied to the features Fi and l(y, f(x)) is a loss function, which can be a quadratic loss or an approximation for the -insensitive regression loss. In our experiments the optimization was performed with a group Lasso solver [21] which was adapted for our specific loss functions.\nWe compare the above formulation with the standard multiple kernel learning method (GMKL) presented in [33] where we consider the regression problem under l1 regularization. For GMKL we have to build the Gram matrices {Ki}i=1..r for each kernel ki on X. We focus on the regression problem and define Kd =\n∑ i=1..r diKi. The al-\ngorithm will output the optimized values for d and a set of support vectors and both these elements will be leveraged in the model used for testing."
    }, {
      "heading" : "3.3.1 Proof of equivalence",
      "text" : "We now show that GMKL is equivalent as an optimization procedure with RFF-GL. For GMKL we have the following primal problem\nmin w,d\n1 2 w>w + C n∑ i=1 l(yi, ψ(xi) >w) + r∑ t=1 dt\nsubject to d ≥ 0 (11)\nwhere\nψ(xi) = [ √ d1ψ1(xi) >, . . . , √ drψr(xi) >]> (12)\nand we defined ψt(x) as the feature embedding associated to kernel kt.\nFrom the Representer Theorem [29] we now show that the solution for w will be a linear combination of the basis functions\nw = n∑ i=1 αiψ(xi) (13)\n= [√\nd1w > 1 , . . . , √ dkw > k ]> where we have dropped the bias term for simplicity. We can rewrite the above optimization problem\nmin w,d k∑ t=1 dt 2 ‖wt‖22 + C n∑ i=1 l(yi, k∑ t=1 dtψt(xi) >wt) + k∑ t=1 dt\nsubject to d ≥ 0 (14)\nFollowing a standard trick from the multiple kernel learning literature[1] we make the substitutions wt → dtwt to obtain\nmin w,d k∑ t=1 ‖wt‖22 2dl + C n∑ i=1 l(yi, k∑ t=1 ψt(xi) >wt) + k∑ t=1 dt subject to d ≥ 0 (15)\nIt can easily be shown that\n1\n2 ‖wt‖22 dt + dt ≥ √ 2‖wt‖2 (16)\nand we observe that the loss function no longer depends on d. Therefore the primal multiple kernel learning formulation is equivalent to the following group Lasso formulation\nmin w λ k∑ t=1 ‖wt‖2 + n∑ i=1 l(yi, k∑ t=1 ψt(xi) >wl) (17)\nwhere we set λ = √\n2 C . The equality happens when dt =\n1√ 2 ‖wt‖2. This approach is suitable for other types of regularization for the parameters d as well, not only the l1 norm which we have used. The only restriction is that we should be able to find a closed form solution for (16).\nNow we consider the loss function which has to be differentiable in order to be suitable for a group Lasso formulation. The -insensitive loss ( -IL) used in standard support vector regression is not differentiable but we can use a smooth approximation instead. In our case we have selected an -insensitive γ logistic loss ( -IGLL) function defined as [10, 26]\nlγ, (yi, f(x)) = 1 γ log ( 1 + eγ(f(x)−yi− ) )\n(18)\n+ 1 γ log ( 1 + eγ(−f(x)+yi− )) )\n− 2 γ log(1 + e−γ )\nWe could also consider quadratic or logistic losses for solving the group Lasso."
    }, {
      "heading" : "3.3.2 Computational complexity",
      "text" : "We assume an average time complexity for a support vector machine algorithm [5] to be O(N3sv) where Nsv is the number of support vectors. Since GMKL is based on several evaluations of the standard SVM solver we can show the time complexity of the multiple kernel learning framework is O ( rN2m2 + iGMKL(N 2r +N3sv) + rNsvNm 2 ) where r is the number of kernels, N is the number of training samples, m is the size of the training input (we assume it is the same for all data for simplicity) and iGMKL is the\nmaximum number of calls to and SVM solver. The complexity is dominated by the term O(rN2m2) which is the cost of computing the kernel matrices.\nOn the other hand for the group Lasso formulation we have complexity O(rNm2d + iglNdr) where d is the size of the random Fourier features and igl is the number of iterations required for group Lasso. If we use an approximated kernel as in [30] another (2p + 1)2 cost is added to the preprocessing step. The complexity becomes O(rNm2(2p + 1)2d) where 2p + 1 is the dimension of the approximated kernel as discussed in [35]. The memory complexity is O(Nrd) which is definitely smaller than O(rN2) for GMKL.\nWe see that our algorithm scales linearly, whilst the nonlinear kernel method is quadratic. The constants matter because for a small number of samples our method could run slower given that d is in the range of thousands, iGMKL is usually 102 and igl is in the order of d."
    }, {
      "heading" : "4. Experiments",
      "text" : "We present experiments for single kernel learning and multiple kernel learning, comparing the random linear Fourier methodology with its non-linear kernel counterparts both in terms of running times and in terms of accuracy.\nWe have experimented with the PASCAL VOC2011 segmentation challenge. This consists of 2223 images for training with ground truth split into halves for training and validation. Another 1111 images are given for testing without ground truth. Following the standard procedure we have trained the methods on the training set (further split, internally into training and validation for kernel hyper-parameter learning) and tested on the PASCAL VOC2011 validation set. We have used the methodology presented in Li et al. [18] and relied on a segmentation algorithm from Carreira and Sminchisescu [4] to filter the initial pool of segments to around 100 segments per image. For each of these segments\nwe extracted 8 types of features among which two bag of visual words for color SIFT [32] and two dense gray scale SIFT descriptors one on each segment and one on the background of each segment, three types of phog descriptors two on the pb edges given by [12] computed at different scales, one on the contour and one on the foreground. The third phog descriptor uses no pb. The last descriptor is a pyramid of locally binary pattern features [23] for texture classification. Because the number of segments (around 105) was still too large for any kernel support vector based algorithm to cope with, we have chosen up to 104 segments for each class. The segments were chosen based on their individual scores and we balanced the examples to have a fair split between positive and negative segments. More details on how the scores are defined and computed could be found in [18]."
    }, {
      "heading" : "4.1. Single kernel learning",
      "text" : "We ran a set of experiments for our random Fourier features single kernel learning technique (RFF-SKL) and compared in terms of accuracy with the single kernel learning technique introduced by Chapelle et al. [6] (KRR-GD). We want to predict the class for more than 105 segments. We expect from RFF-SKL to give comparable results in terms of accuracy to KRR-GD. Results are shown in Table 2.\nWe vary the size of the training set from 103 to 105 and measured both the running times of RFF-SKL and the accuracy. In Figure 2 we present how the accuracy depends on the size of the training data and the average time required for RFF-SKL and KRR-GD to run on a class. We observe that RFF-SKL running time scales linearly in the number of examples. The number of random Fourier samples we have chosen was d = 3000. This is consistent with our computational complexity discussed in Section 3.2. We are able to tune the hyper-parameters of a model trained with more\nthan 105 samples, each with 3000 attributes in less than 15 minutes."
    }, {
      "heading" : "4.2. Multiple Kernel Learning",
      "text" : "We also report experiments for multiple kernel learning. For GMKL we have evaluated an exponentiated χ2 kernel for each image feature. We set the scaling parameter to be the mean of the chi-square distance matrix following the procedure from [13]. The Gramm matrices were created for each class since for different classes we had to select different representative samples. For our random Fourier features within the group Lasso framework (RFF-GL) we have approximated the kernel as in [30] using the recommended settings. In Figure 3 we compare the accuracy of predicting the right class for the segments. We see that for a small number of training samples GMKL is slightly superior, but RFF-GL catches up due to its scalability. Working with kernel matrices larger than 104 was not feasible for GMKL.\nFor group Lasso we have adapted the implementation presented in [21] whereas for comparisons with standard multiple kernel learning, we have used the GMKL implementation presented by Varma and Babu [33].\nIn Figure 4 we show the average running times for a class. We see that RFF-GL scales linearly and GMKL scales quadratically.\nFollowing the relation given by eq. (16) in Table 3 we compare the weights given by the two methods on one of the classes on which we have performed regression – in this\ncase the aeroplane class. We see that both RFF-GL and GMKL favour the SIFT over pHOG."
    }, {
      "heading" : "5. Conclusions",
      "text" : "Fourier methodology is a powerful and formally consistent class of linear approximation techniques for nonlinear kernel machines that carries the promise of combining good model scalability and non-linear prediction power. This has motivated research in extending the class of useful kernels that can be approximated, e.g. Chi-square[35, 19], but leaves ample space for reformulating standard problems like single or multiple kernel learning in the linear Fourier domain. In this paper we have developed gradientbased methods for single and multiple-kernel learning in the Fourier domain and showed that these are efficient and produce accurate results on a complex computer vision dataset like VOC2011. In future work we plan to explore alternative kernel basis expansions, feature selection and nonconvex optimization techniques for learning."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2012,
    "abstractText" : "Approximations based on random Fourier features have<lb>recently emerged as an efficient and formally consistent<lb>methodology to design large-scale kernel machines [24].<lb>By expressing the kernel as a Fourier expansion, features<lb>are generated based on a finite set of random basis pro-<lb>jections, sampled from the Fourier transform of the kernel,<lb>with inner products that are Monte Carlo approximations of<lb>the original kernel. Based on the observation that different<lb>kernel-induced Fourier sampling distributions correspond<lb>to different kernel parameters, we show that an optimiza-<lb>tion process in the Fourier domain can be used to identify<lb>the different frequency bands that are useful for prediction<lb>on training data. Moreover, the application of group Lasso<lb>[37] to random feature vectors corresponding to a linear<lb>combination of multiple kernels, leads to efficient and scal-<lb>able reformulations of the standard multiple kernel learning<lb>model [33]. In this paper we develop the linear Fourier<lb>approximation methodology for both single and multiple<lb>gradient-based kernel learning and show that it produces<lb>fast and accurate predictors on a complex dataset such as<lb>the Visual Object Challenge 2011 (VOC2011).",
    "creator" : "LaTeX with hyperref package"
  }
}