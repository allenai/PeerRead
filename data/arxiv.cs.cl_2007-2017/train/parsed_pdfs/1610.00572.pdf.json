{
  "name" : "1610.00572.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "An Arabic-Hebrew parallel corpus of TED talks",
    "authors" : [ "Mauro Cettolo" ],
    "emails" : [ "cettolo@fbk.eu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "The benchmark consists of about 2,000 talks, whose subtitles in Arabic and Hebrew have been accurately aligned and rearranged in sentences, for a total of about 3.5M tokens per language. Talks have been partitioned in train, development and test sets similarly in all respects to the MT tasks of the IWSLT 2016 evaluation campaign.\nIn addition to describing the benchmark, we list the problems encountered in preparing it and the novel methods designed to solve them. Baseline MT results and some measures on sentence length are provided as an extrinsic evaluation of the quality of the benchmark."
    }, {
      "heading" : "1 Introduction",
      "text" : "TED is a nonprofit organization that “invites the world’s most fascinating thinkers and doers [...] to give the talk of their lives”. Its website1 makes the video recordings of the best TED talks available under a Creative Commons license. All talks have English captions, which have also been translated into many languages by volunteers worldwide.\nWIT3 (Cettolo et al., 2012)2 is a Web inventory that offers access to a collection of TED talks, redistributing the original TED website contents through\n1www.ted.com 2wit3.fbk.eu\nyearly releases. Each release is specifically prepared for supplying train, development and test data to participants at MT and SLT tracks of the evaluation campaign organized by the International Workshop on Spoken Language Translation (IWSLT).\nDespite almost all English subtitles of TED talks have been translated into both Arabic and Hebrew, no IWSLT evaluation campaign proposed ArabicHebrew as an MT task. Actually, early releases of WIT3 distributed train data for hundreds of pairs, including Arabic-Hebrew. Nevertheless, those linguistic resources were prepared by means of a totally automatic procedure, with only rough sanity checks, and include talks available at that time.\nGiven the increasing interest in the ArabicHebrew task and the many more TED talks translated into the two languages available to date, we decided to prepare a benchmark for Arabic-Hebrew. We exploited WIT3 for collecting raw data; moreover, for making the dissemination of results easier to users, we borrowed the partition of TED talks into train, development and test sets adopted in the IWSLT 2016 evaluation campaign.\nThe Arabic-Hebrew benchmark is available for download at:\nwit3.fbk.eu/mt.php?release=2016-01-more\nIn this paper we present the benchmark, list the problems encountered while developing it and describe the methods applied to solve them. Baseline MT results and specific measures on the train sets are given as an extrinsic evaluation of the quality of the generated bitext.\nar X\niv :1\n61 0.\n00 57\n2v 1\n[ cs\n.C L\n] 3\nO ct\n2 01"
    }, {
      "heading" : "2 Related Work",
      "text" : "To the best of our knowledge, to date the richest collection of publicly available Arabic-Hebrew parallel corpora is part of the OPUS project;3 in total, it provides more than 110M tokens per language subdivided into 5 corpora, OpenSubtitles2016 being by far the largest. The OpenSubtitles2016 collection (Lison and Tiedemann, 2016)4 provides parallel subtitles of movies and TV programs made available by the Open multilanguage subtitle database.5 The size of this corpus makes it outstandingly valuable; nevertheless, the translation of such kind of subtitles is often less literal than in other domains (even TED), likely affecting the accuracy of the fully automatic processing implemented for parallelizing the Arabic and Hebrew subtitles.\nAnother Arabic-Hebrew corpus we are aware of is that manually prepared by Shilon et al. (2012) for development and evaluation purposes; no statistics on its size is provided in the paper, nor it is publicly available; according to El Kholy and Habash (2015), it consists of some hundred of sentences, definitely less than those included in our benchmark."
    }, {
      "heading" : "3 Parallel Corpus Creation",
      "text" : "English subtitles of TED talks are segmented on the basis of the recorded speech, for example in correspondence of pauses, and to fit the caption space, which is limited; hence, in general, the single caption does not correspond to a sentence.\nThe natural translation unit considered by human translators is the caption, as defined by the original transcript. While translators can look at the context of the single captions, arranging this way any NLP task – in particular MT – would make it particularly difficult, especially when word re-ordering across consecutive captions occurs. For this reason, we aim to re-build the original sentences, thus making the NLP/MT tasks more realistic."
    }, {
      "heading" : "3.1 Collection of talks",
      "text" : "For each language, WIT3 distributes a single XML file which includes all talks subtitled in that language; the XML format is defined in a\n3opus.lingfil.uu.se 4opus.lingfil.uu.se/OpenSubtitles2016.php 5www.opensubtitles.org\nspecific DTD.6 Thus, we did not need to crawl any data, as we could download the three XML files of Arabic, Hebrew and English, available at wit3.fbk.eu/mono.php?release=XML releases."
    }, {
      "heading" : "3.2 Alignment issues",
      "text" : "Even if translators volunteering for TED translated the English captions as pointed out above, sometimes they did not adhere to the source segmentation. For example, in talk n. 2357,7 the English subtitle:\nFrench sign language was brought to America during the early 1800s,\nis put between timestamps 53851 and 59091, while the corresponding Arabic translation is split into two subtitles:\nA¾K QÓ@ ú ¯ HYÒ J«@ éJ Q ®Ë @ èPA B @ é ªË\nQå « © A JË @ àQ ®Ë@ É K@ð @ ú ¯\nwhich span the audio recording from 53851 to 56091 and from 56091 to 59091, and literally mean “French sign language was brought to America” and “in the early nineteenth century”, respectively.\nEven though the differences produced by translators involve a small amount of captions (0.5% in the Arabic-Hebrew case), these differences affect a relevant number of talks (9%) and in them all subtitles following those differently segmented are desynchronized, making the re-alignment indispensable."
    }, {
      "heading" : "3.3 Sentence rebuilding issues",
      "text" : "For rebuilding sentences, WIT3 automatic tools leverage strong punctuation. Unfortunately, Arabic spelling is often inconsistent in terms of punctuation, as both Arabic UTF8 symbols and ASCII English punctuation symbols are used. Even worse, both in Arabic and Hebrew translations the original English punctuation is often ignored. An extreme case is talk n. 14438 where 97% of full stops at the end of English subtitles does not appear in the Hebrew translations. The initial subtitles of that talk are shown in Figure 1.\n6wit3.fbk.eu/archive/XML releases/wit3.dtd 7www.ted.com/talks/christine sun kim the enchanting mu sic of sign language 8www.ted.com/talks/joshua foer feats of memory anyone can do\nI’d like to invite you to close your eyes. Imagine yourself [...] front door of your home. I’d like you to notice the color of the door, the material that it’s made out of.\nNote that the strong punctuation of the English side never appears in the Hebrew side. This example also shows the misalignment between subtitles discussed in Section 3.2, being the second English subtitle split into two Hebrew subtitles (the second and the third).\nThe two issues discussed above led us to believe that trying to directly align Arabic and Hebrew subtitles and rebuild sentences can fail in so many cases that the overall quality of the final bitext can be seriously affected. We thus designed a two-stage process in which English plays the role of the pivot. The two stages are described in the two following sections."
    }, {
      "heading" : "3.4 Pivot-based alignment",
      "text" : "The alignment of Arabic and Hebrew subtitles is obtained by means of the algorithm sketched in Figure 2.\nThe starting point are the XML files of subtitles\nin the three languages. English is aligned to Arabic and to Hebrew (step 1 in the figure) by means of two independent runs of Gargantua, a sentence aligner described in (Braune and Fraser, 2010). As discussed in Section 3.2, the two resulting English sides can be desynchronized, as indeed it is: in one third of the talks, the number of subtitles differs in the two alignments. Then, Gargantua is run again to align the two desynchronized English sides (step 2); now, the two maps from English to English are used to rearrange the Arabic and Hebrew sides (step 3), that at this point are aligned.\nThe automatic procedure drafted above is not error-proof; while measuring failures in steps 1 and 3 of the algorithm is unfeasible without gold references, it is simple for step 2, which should output two perfectly equal English sides; on the contrary, about 2,000 aligned English subtitles (out of 530,000, 0.4%) are different, involving less than 0.5% of all words. Even if not immune from mistakes, the error rate is so small that can be accepted in our context."
    }, {
      "heading" : "3.5 Pivot-based sentence rebuilding",
      "text" : "The last stage in the preparation of the ArabicHebrew parallel corpus is the rebuilding of sentences from the aligned subtitles. As discussed in Section 3.3, we cannot rely on strong punctuation occurring in the texts of these two languages. Once again, the English side comes in handy. In fact, the procedure presented in Section 3.4 outputs the lists of Arabic, Hebrew and English subtitles perfectly synchronized. Since punctuation marks on the English side are reliable, sentences in the three languages are regenerated by concatenating consecutive captions until a proper punctuation mark is detected on the English side."
    }, {
      "heading" : "4 Data Partitioning and Statistics",
      "text" : "As of April 2016, WIT3 distributes the English transcriptions of 2085 TED talks; for 2029 of them the Arabic translation is available, while 2065 have been translated into Hebrew.\nThe talks common to the three languages (2023) have been processed by means of the alignment/sentence-rebuilding procedure described in the previous section. They have been arranged in\ntrain/development/test sets following the same partitioning adopted in MT tasks of the IWSLT 2016 evaluation campaign. Following the IWSLT practice, the talks that are included in evaluation sets of any past evaluation campaign based on TED talks have been removed from the train sets, even if they do not appear in dev/test sets of this Arabic-Hebrew release. For this reason, the release has a total number of aligned talks (1908) smaller than 2023.\nTables 1 and 2 provide statistics on monolingual and bilingual corpora of the Arabic-Hebrew release. Monolingual resources slightly extend the bilingual train sets by including those talks that were not aligned for some reason, e.g. the lack of translation in the other language.\nFigures refer to tokenized texts. The standard tokenization via the tokenizer script released with the Europarl corpus (Koehn, 2005) was applied to English and Hebrew languages, while Arabic was normalized and tokenized by means of the QCRI Arabic Normalizer 3.0.9"
    }, {
      "heading" : "5 Extrinsic Quality Assessment",
      "text" : "The most reliable intrinsic evaluation of the quality of the benchmark would consist in asking human experts in the two languages to judge the level of parallelism of a statistically significant amount of randomly selected bitext. Since we could not afford it,\n9alt.qcri.org/tools/arabic-normalizer/\nwe performed a series of extrinsic checks based on both MT runs and measures on the train sets."
    }, {
      "heading" : "5.1 MT baseline performance",
      "text" : "Performance of baseline MT systems on two test sets have been measured. The assumption behind this indirect check is that the better the MT performance, the higher the quality of the train data (and by extension of the whole benchmark).\nSMT systems were developed with the MMT toolkit,10 which builds engines on the Moses decoder (Koehn et al., 2007), IRSTLM (Federico et al., 2008) and fast align (Dyer et al., 2013).\nThe baseline MT engine (named pivot) was estimated on the train data of the benchmark; for comparison purposes, two additional MT systems were trained on two Arabic-Hebrew bitexts built on the same train TED talks of our benchmark but differently processed; in both, subtitles were aligned directly, without pivoting through English; then, in one case the original captions were kept as they are, i.e. without any sentence reconstruction (none); in the other case, sentences were rebuilt by looking at the strong punctuation of the Hebrew side, without using English as the pivot (strngP). Note that the strngP method is the one typically used in WIT3 releases. Table 3 collects the BLEU scores of our MT systems and of Google Translate on tst2012 and tst2013 sets. The first three rows refer to the test sets with sentences rebuilt on the Hebrew strong punctuation; the last row regards the actual benchmark in all respects. The score gaps are small but it has to be considered that they are only due to the possible differences of just a portion of subtitles (those desynchronized by the translators, as dis-\n10www.modernmt.eu\ncussed in Section 3.2) in a small fraction of talks (9%, again Section 3.2) used for training. Other differences, like those shown in Section 5.3, cannot impact too much on the overall quality of the models. Given such a limited field of action, the gain yielded by the proposed approach is even unexpected.\nIt is worth to note that the quality of our baseline systems is on a par with Google Translate and with the state of the art phrase-based and neural MT systems trained on our benchmark and described in (Belinkov and Glass, 2016)."
    }, {
      "heading" : "5.2 Measurements on the train sets",
      "text" : "A set of measurements regarding the length of paired sentences has been performed on the train set. Table 4 summarizes the values of original subtitles (none) and of sentences generated by the strngP and pivot methods. We see that the variability of sentence length in the pivot version equals that of the original subtitles, which can be taken as the reference, while the length of strngP sentences vary much more. Moreover, the amount of sentences longer than 100 tokens, which typically are unmanageable/useless in standard processing, is four/five times lower in pivot case than in strngP.\nFinally, Table 5 provides the mean and the standard deviation of the difference of the number of tokens between Arabic and Hebrew subtitles. Also here the statistics on original subtitles (none) can be assumed to be the gold reference, and again the pivot version is preferable to the strngP version."
    }, {
      "heading" : "5.3 Example",
      "text" : "Here we show how the three methods none, strngP and pivot process the example of Figure 1. For the sake of readability, only the English translation is given.\nAll methods properly align original captions. Differences come from the sentence rebuilding.\nBy definition, none keeps the five original Ar/He subtitles:\nI’d like to invite you to close your eyes. Imagine yourself standing outside the front door of your home. I’d like you to notice the color of the door, the material that it’s made out of.\nstrngP, misled by the absence of strong punctuation on the Hebrew side, appends together the five subtitles (and many more) into one long “sentence”:\nI’d like to invite [...] that it’s made out of. [...]\npivot is instead able to properly reconstruct sentences from the original captions:\nI’d like to invite you to close your eyes. Imagine yourself standing [...] door of your home. I’d like you to notice [...] that it’s made out of.\nso providing the best segmentation from a linguistic point of view."
    }, {
      "heading" : "6 Summary",
      "text" : "In this paper we have described an Arabic-Hebrew benchmark built on data made available by WIT3. The Arabic and Hebrew subtitles of around 2,000 TED talks have been accurately rearranged in sentences and aligned by means of a novel and effective procedure which relies on English as the pivot. The talks count a total of 225k sentences and 3.5M tokens per language and have been partitioned in train, development and test sets following the split of the MT tasks of the IWSLT 2016 evaluation campaign."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was partially supported by the CRACKER project, which received funding from the European Union’s Horizon 2020 research and innovation programme under grant no. 645357.\nThe author wants to thank Yonatan Belinkov for providing invaluable suggestions in the preparation of the benchmark."
    } ],
    "references" : [ {
      "title" : "Large-scale Machine Translation between Arabic and Hebrew: Available Corpora and Initial Results",
      "author" : [ "Belinkov", "Glass2016] Yonatan Belinkov", "James Glass" ],
      "venue" : "In Proc. of SeMaT,",
      "citeRegEx" : "Belinkov et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Belinkov et al\\.",
      "year" : 2016
    }, {
      "title" : "Improved Unsupervised Sentence Alignment for Symmetrical and Asymmetrical Parallel Corpora",
      "author" : [ "Braune", "Fraser2010] Fabienne Braune", "Alexander Fraser" ],
      "venue" : "In Proc. of Coling 2010: Posters,",
      "citeRegEx" : "Braune et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Braune et al\\.",
      "year" : 2010
    }, {
      "title" : "WIT: Web Inventory of Transcribed and Translated Talks",
      "author" : [ "Christian Girardi", "Marcello Federico" ],
      "venue" : "In Proc. of EAMT,",
      "citeRegEx" : "Cettolo et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Cettolo et al\\.",
      "year" : 2012
    }, {
      "title" : "A Simple, Fast, and Effective Reparameterization of IBM Model 2",
      "author" : [ "Dyer et al.2013] Chris Dyer", "Victor Chahuneau", "Noah A. Smith" ],
      "venue" : "In Proc. of NAACL,",
      "citeRegEx" : "Dyer et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Dyer et al\\.",
      "year" : 2013
    }, {
      "title" : "Morphological Constraints for Phrase Pivot Statistical Machine Translation",
      "author" : [ "El Kholy", "Habash2015] Ahmed El Kholy", "Nizar Habash" ],
      "venue" : "In Proc. of MT Summit XV,",
      "citeRegEx" : "Kholy et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kholy et al\\.",
      "year" : 2015
    }, {
      "title" : "IRSTLM: an Open Source Toolkit for Handling Large Scale Language Models",
      "author" : [ "Nicola Bertoldi", "Mauro Cettolo" ],
      "venue" : "In Proc. of Interspeech,",
      "citeRegEx" : "Federico et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Federico et al\\.",
      "year" : 2008
    }, {
      "title" : "Europarl: A Parallel Corpus for Statistical Machine Translation",
      "author" : [ "Philipp Koehn" ],
      "venue" : "In Proc. of MT Summit X,",
      "citeRegEx" : "Koehn.,? \\Q2005\\E",
      "shortCiteRegEx" : "Koehn.",
      "year" : 2005
    }, {
      "title" : "Opensubtitles2016: Extracting Large Parallel Corpora from Movie and TV Subtitles",
      "author" : [ "Lison", "Tiedemann2016] Pierre Lison", "Jörg Tiedemann" ],
      "venue" : "In Proc. of LREC,",
      "citeRegEx" : "Lison et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lison et al\\.",
      "year" : 2016
    }, {
      "title" : "Machine Translation between Hebrew and Arabic",
      "author" : [ "Shilon et al.2012] Reshef Shilon", "Nizar Habash", "Alon Lavie", "Shuly Wintner" ],
      "venue" : "Machine Translation,",
      "citeRegEx" : "Shilon et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Shilon et al\\.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "WIT3 (Cettolo et al., 2012)2 is a Web inventory that offers access to a collection of TED talks, redistributing the original TED website contents through",
      "startOffset" : 5,
      "endOffset" : 27
    }, {
      "referenceID" : 8,
      "context" : "Another Arabic-Hebrew corpus we are aware of is that manually prepared by Shilon et al. (2012) for development and evaluation purposes; no statistics on its size is provided in the paper, nor it is publicly available; according to El Kholy and Habash (2015), it consists of some hundred of sentences, definitely less than those included in our benchmark.",
      "startOffset" : 74,
      "endOffset" : 95
    }, {
      "referenceID" : 8,
      "context" : "Another Arabic-Hebrew corpus we are aware of is that manually prepared by Shilon et al. (2012) for development and evaluation purposes; no statistics on its size is provided in the paper, nor it is publicly available; according to El Kholy and Habash (2015), it consists of some hundred of sentences, definitely less than those included in our benchmark.",
      "startOffset" : 74,
      "endOffset" : 258
    }, {
      "referenceID" : 6,
      "context" : "The standard tokenization via the tokenizer script released with the Europarl corpus (Koehn, 2005) was applied to English and Hebrew languages, while Arabic was normalized and tokenized by means of the QCRI Arabic Normalizer 3.",
      "startOffset" : 85,
      "endOffset" : 98
    }, {
      "referenceID" : 5,
      "context" : ", 2007), IRSTLM (Federico et al., 2008) and fast align (Dyer et al.",
      "startOffset" : 16,
      "endOffset" : 39
    }, {
      "referenceID" : 3,
      "context" : ", 2008) and fast align (Dyer et al., 2013).",
      "startOffset" : 23,
      "endOffset" : 42
    } ],
    "year" : 2016,
    "abstractText" : "We describe an Arabic-Hebrew parallel corpus of TED talks built upon WIT, the Web inventory that repurposes the original content of the TED website in a way which is more convenient for MT researchers. The benchmark consists of about 2,000 talks, whose subtitles in Arabic and Hebrew have been accurately aligned and rearranged in sentences, for a total of about 3.5M tokens per language. Talks have been partitioned in train, development and test sets similarly in all respects to the MT tasks of the IWSLT 2016 evaluation campaign. In addition to describing the benchmark, we list the problems encountered in preparing it and the novel methods designed to solve them. Baseline MT results and some measures on sentence length are provided as an extrinsic evaluation of the quality of the benchmark.",
    "creator" : "LaTeX with hyperref package"
  }
}