{
  "name" : "1206.6835.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Dimension Reduction in Singularly Perturbed Continuous-Time Bayesian Networks",
    "authors" : [ "Nir Friedman" ],
    "emails" : [ "nir@cs.huji.ac.il", "raz@math.huji.ac.il" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Continuous-time Bayesian networks (CTBNs) are graphical representations of multi-component continuous-time Markov processes as directed graphs. The edges in the network represent direct influences among components. The joint rate matrix of the multi-component process is specified by means of conditional rate matrices for each component separately. This paper addresses the situation where some of the components evolve on a time scale that is much shorter compared to the time scale of the other components. We prove that in the limit where the separation of scales is infinite, the Markov process converges (in distribution, or weakly) to a reduced, or effective Markov process that only involves the slow components. We also demonstrate that for a reasonable separation of scales (an order of magnitude) the reduced process is a good approximation of the marginal process over the slow components. We provide a simple procedure for building a reduced CTBN for this effective process, with conditional rate matrices that can be directly calculated from the original CTBN, and discuss the implications for approximate reasoning in large systems."
    }, {
      "heading" : "1 Introduction",
      "text" : "Continuous-time Markov processes are a framework used in the modeling of a huge range of stochastic dynamical systems, e.g., chemical kinetics, population dynamics, stock markets, and many more (Gardiner [2004]). We consider Markov processes that are homogeneous in time and have a finite state space. Such systems are fully determined by the state space S, the distribution of the process at the initial time, and a description of the dynamics of the process. These dynamics are specified by a rate matrix Q, whose off-diagonal entries qa,b are exponential rate inten-\nsities for transitioning between states. Intuitively, we can think of the entry qa,b as the rate parameter of an exponential distribution whose value is the duration of time spent in state a before transitioning to b. When more than one transition is possible, the shortest duration determines the next state. Thus, q−1a,b is the expected duration in state a before transitioning to state b (assuming it were the only possible transition), and ( ∑ b,a qa,b)−1 is the expected duration before transitioning out of state a.\nIn many applications, the state space is of the form of a product space S = S1 × S1 × · · · × SM, where M is the number of components (such processes are called multicomponent). Even if each of the Si is of low dimension, the dimension of the state space is exponential in the number of components, which often poses computational difficulties, e.g., in learning applications. Continuous-time Bayesian networks (Nodelman et al. [2002, 2003]) are a graphical representation for Markov processes that have extra structure, therefore allowing for more a compact representation with fewer parameters. The first assumption is that transitions only occur in one component at a time. Second, the transition rates associated with each component are assumed to only depend on the state of a collection of “parent components”. The CTBN is a directed, possibly cyclic graph whose nodes are the components of the process, and whose edges represent parent-child relations.\nIt is also often the case that certain components are considerably faster than the others. Mathematically, it is assumed that the (conditional) rates associated with the fast components are larger by a factor of 1/ than the (conditional) rates associated with the other, slow components, with 1. Systems having such property are said to have a separation of scales, or to be singularly perturbed. Such situations are ubiquitous, for example, in chemical kinetics, where some reactions may occur much faster than other. In such situations, the fast components tend to reach “local equilibrium”, relative to the slow components, and under certain conditions, reduced Markovian dynamics can be derived for a lower dimensional system that only involves the slow components (see van Kampen [1985] for a classical\nreview on dimension reduction in scale separated systems; see Givon et al. [2004] for a recent review).\nIn this paper we derive the limiting Markov process and show how to reduce a CTBN with fast components into a smaller CTBN that involves only the slow components. We discuss the implications of this result for inference in CTBNs with different time scales."
    }, {
      "heading" : "2 Continuous-time Bayesian networks",
      "text" : "In this section we briefly review the CTBN model (Nodelman et al. [2002]). Consider an M-component Markov process\nX(t) = (X1(t),X2(t), . . .XM(t))\nwith state space\nS = S1 × S2 × · · · × SM. A notational convention: vectors are denoted by boldface symbols, e.g., X,a, and matrices are denoted by blackboard style characters, e.g., Q. The states in S are denoted by vectors of indexes, a = (a1, . . . , aM). The indexes 1 ≤ i, j ≤ M are used to enumerate the components. The dynamics of a time-homogeneous continuous-time Markov process are fully determined by the Markov transition function,\npa,b(t) = Pr(Xt+s = b|Xs = a), where time-homogeneity implies that the right hand side does not depend on s. Provided that the transition function satisfies certain analytical properties (continuity, and regularity; see Chung [1960]) the dynamics are fully captured by a constant matrix Q—the rate, or intensity matrix—whose entries qa,b are defined by\nqa,b = lim h↓0 pa,b(h) − δa,b h ,\nwhere δa,b is a multivariate Kronecker delta (an alternative notation using an indicator function is 1(a = b)). The Markov process Xt can also be given a pathwise characterization. Suppose the process starts in a state a. After spending a finite amount of time in state a it transitions, at a random time, to a random state b , a. The transition times to the potential new states are exponentially distributed, with qa,b, a , b, being the exponential rate for transitioning from state a to state b. The diagonal elements of Q satisfy the condition that each row sums up to zero. Suppose, for simplicity, that each component X j takes values in a d-dimensional space. Then, the state space is dM-dimensional, and the Q-matrix involves dM(dM − 1) parameters.\nThe time-dependent probability distribution of the process, p(t), whose entries are defined by\npa(t) = Pr(X(t) = a), a ∈ S,\nsatisfies the so-called master equation,\ndp dt = QTp. (1)\nIt is important to note that the master equation (1) encompasses all the statistical properties of the Markov process. There is a one-to-one correspondence between the description of a Markov process by means of a master equation, and by means of a “pathwise” characterization (up to stochastic equivalence of the latter; see Gikhman and Skorokhod [1975]).\nWe are concerned here with processes for which every transition involves a single component. In such case, the most general rate matrix takes the form\nqa,b = M∑\ni=1 qia,bi ∏ j,i δa j,b j , (2)\nwhere the qia,bi are the entries of a conditional rate matrix Qi for Xi transitioning from ai to bi given that the state of the system is a. The structure (2) represents the fact that each component undergoes transitions independently from the other components, but at a rate that depends on the current state of the entire system. A Q-matrix of the form (2) requires MdM(d − 1) independent parameters, which may still be a large number.\nFurther reduction in the number of parameters is obtained if additional structure is incorporated. CTBNs are applicable to situations where each of the conditional rate matrices Qi is only influenced by a subset of component. Specifically, a parent-child relation is introduced between ordered pairs of components. To every 1 ≤ i ≤ M we define the (possibly empty) set of indexes\nPar(i) = { 1 ≤ j ≤ M : X j is a parent of Xi } ,\nand the state space associated with the parents of Xi,\nSPar(i) = j∈Par(i) S j.\nWe then introduce a restriction operator Pi : S → SPar(i), which extracts from the state of the system the state of the subsystem that consists of the parents of Xi,\nPi(a) = (am1 , am2 , . . . , ami ),\nwhere Par(i) = {m1,m2, . . . ,mi}. The conditional rate matrix associated with the i-th component only depends on the state of the parent components. To make this dependence explicit, we denote the conditional rate matrices byQi|Par(i) with entries qi|Par(i)ai,bi |Pi(a). Thus,\nthe joint rate matrix of the whole process assumes the reduced form\nqa,b = M∑\ni=1 qi|Par(i)ai,bi |Pi(a) ∏ j,i δa j,b j . (3)\nEquation (3) is, using the terminology of Nodelman et al. [2002], the “amalgamation” of the M conditional rate matrices. Note the compact representation which is valid for both diagonal and off-diagonal entries. It is also noteworthy that amalgamation is a summation, rather than a product; indeed, independent exponential rates are additive. If, for example, every component has k parents, the rate matrix requires now only Mdk+1(d − 1) parameters. The dependency relations between components can be represented graphically as a directed graph, G, in which each node corresponds to a component, and each directed edge defines a parent-child relation. A CTBN consists of such a graph, supplemented with a set of M conditional rate matricesQi|Par(i), and an initial distribution. Formally, we define a CTBN as a tuple\nC = 〈 G, {Qi|Par(i)}Mi=1,P0 〉 ,\nwhere P0 is the initial distribution over X(0).\nAs stated in Nodelman et al. [2002], the graph structure has two main roles: (i) it provides a data structure to which parameters are associated; (ii) it provides a qualitative description of dependencies among the various components of the system. The graph structure also reveals statistical (possibly conditional) independencies between sets of components. An example of a four-component CTBN is shown in Figure 1.\nFor later use, we note that if we substitute the structure (3) of the rate matrix into the master equation (1), the latter takes the particular form,\ndpb dt = M∑ i=1 ∑ ai∈Si qi|Par(i)ai,bi |Pi(b)p(b1,...,bi−1,ai,bi+1,...,bM). (4)"
    }, {
      "heading" : "3 Singularly perturbed CTBNs",
      "text" : "In many situations, it is possible to partition the M components into two sets: “fast” components and “slow” components. A standard measure for the “speed” of a Markov process is the rate at which it equilibrates, which is commonly taken to be the absolute value of the second largest eigenvalue of the Q matrix. In the context of CTBNs every component is assigned a conditional rate matrix. Scale separation holds if all the equilibration rates associated with the conditional rate matrices of a subset of components are much faster than all the equilibration rates associated with\nthe conditional rate matrices of the complementary components. Intuitively, this means that every fast component has undergone many transitions during a time interval characteristic of a single transition in the slow components. For the sake of mathematical analysis, we will consider CTBNs that are parametrized by a small parameter that represents the ratio of a characteristic time of residence in a state of a fast component, and a characteristic time of residence in a state of a slow component.\nWe now introduce definitions pertinent to the classification of components into fast ones and slow ones: let\nIfast = {1 ≤ i ≤ M : Xi is a fast component} Islow = {1 ≤ i ≤ M : Xi is a slow component}\nbe the sets of indexes of fast and slow components, respectively, and let\nSfast = i∈Ifast Si and Sslow = i∈Islow Si\nbe the state spaces associated with the two sets of components. For a ∈ S we define the restriction operator Fast : S → Sfast,\nFast(a) = (ai1 , ai2 , . . . , aim ),\nwith {i1, i2, . . . , im} = I f . The restriction operator Slow : S → Sslow is defined similarly. Below we will derive expressions that involves states in S, Sslow and Sfast. For the sake of readability, we will use the symbols a, b for states in S, α,β for states in Sslow, and ζ for states in Sfast.\nWe make the following assumptions:\nAssumption 3.1 The conditional rate matrices associated with fast components can be expressed as 1/ times an - independent rate matrix, while the conditional rate matrices associated with the slow components do not depend on\n. Furthermore, for every fixed state of the slow components, the Markov process defined by the conditional rate matrices of the fast components is ergodic over Sfast.\nNote that even if the entire system is ergodic, the subsystem that consists only of the fast components, with the slow components fixed, is not necessarily ergodic. Thus, we need to explicitly require this additional property. This condition is automatically satisfied, for example, if the conditional rate matrices have strictly positive off-diagonal entries. We will denote the (conditional) equilibrium distribution of the fast components, given the state of the slow components, by πIfast |Islow with entries πIfast |Islowζ|α (where, as stated above, ζ ∈ Sfast and α ∈ Sslow). Graphically, we will mark fast components by nodes with shaded fillings. For example, Figure 1 represents a fourcomponent CTBN in which only X3 is a fast component. Consequently,\nIfast = {3} Islow = {1, 2, 4} Sfast = S3 Sslow = S1 × S2 × S4\nFast(a) = a3 Slow(a) = (a1, a2, a4),\nand the conditional rate matrices can be written as Q1, Q2|1, 1 Q\n3|1 and Q4|2,3, where each of the Qi|Par(i) is - independent.\nThe goal is to study the limiting behavior of the system as → 0. Our main theorem, whose proof is sketched in Appendix A, is:\nTheorem 3.1 Let X(t) be an M-component Markov process satisfying Assumption 3.1. Then as → 0, the distribution p(t) converges to a product distribution of the form,\npa(t) = π Ifast |Islow Fast(a)| Slow(a)p̃Slow(a)(t),\nwhere p̃ is the marginal distribution of the slow components. The latter, satisfies the reduced, or effective master equation,\nd dt p̃ = Q̃Tp̃, (5)\nwhere Q̃ is a rate matrix over Sslow. Its entries are given by q̃α,β = ∑\ni∈Islow q̃iα,β ∏ Islow3 j,i δα j,β j , (6)\nwhere Q̃i with entries\nq̃iα,β = ∑ ζ∈Sfast π Ifast |Islow ζ|α q i|Par(i) αi,βi |Pi((α,ζ)) (7)\nis the effective conditional rate matrix associated with the slow component Xi.\nThe fact that the marginal distribution over Sslow satisfies a master equation means that the limiting behavior of the slow components is Markovian.\nRemark. When interpreting (7) note that α and β are both elements in Sslow, whereas the summation variables ζ are elements in Sfast. The concatenation (α,ζ) is identified as an element of the full space S, and Pi((α,ζ)) restricts the state (α,ζ) to only those components that are parents of Xi.\nThe expression (6) for the effective rates can be written in an alternative form. Writing\nQ = 1 Qfast +Qslow,\nwhere Qfast and Qslow are -independent and have entries\nqfasta,b = ∑ i∈Ifast qi|Par(i)ai,bi |Pi(a) ∏ j,i δa j,b j\nqslowa,b = ∑\ni∈Islow qi|Par(i)ai,bi |Pi(a) ∏ j,i δa j,b j ,\nthe entries of Q̃ can be written as\nq̃α,β = ∑ ζ∈Sfast πIfast |Islowζ|α q slow (α,ζ),(β,ζ). (8)\nIn simple words, the reduced rate matrix associated with the limiting dynamics of the slow components is the full rate matrix, averaged over the conditional equilibrium distribution of the fast components. If we denote by E f |s expectation with respect to the conditional distribution πIfast |Islow , then (8) takes the more suggestive form,\nQ̃ = E f |s[Qslow].\nExample 3.1 As the simplest illustration of Theorem 3.1, consider a two-component system X(t) = (X1(t),X2(t)) with rate matrix of the form\nQ = 1 Q1 +Q2|1.\nThat is, X1 is a fast component and it influences the slow component X2. The equilibrium distribution of X1 is denoted by π1. Theorem 3.1 asserts that as → 0, X2(t) converges in a weak sense (i.e., in distribution) to a Markov process with effective rate Q̃, whose entries are given by\nq̃α2,β2 = q̃ 2 α2,β2 = ∑ ζ1∈S1 π1ζ1 q 2|1 α2,β2 |ζ1 .\nThe next section addresses the systematic derivation of reduced CTBNs."
    }, {
      "heading" : "4 Dimension reduction of CTBNs",
      "text" : ""
    }, {
      "heading" : "4.1 Segregated fast components",
      "text" : "We start by considering CTBNs in which fast components are segregated: there are no parent-child relations between two fast components. In such case the conditional equilibrium distribution of the fast components factors into a product distribution on the form\nπIfast |Islowζ|α = ∏ i∈Ifast πi|Par(i) ζi |Pi(α), (9)\nwhere, with a slight abuse of notations, Pi(α) extracts from the vector α ∈ Sslow those components that belong to Par(i). That is, the marginal equilibrium distribution of each fast components depends only on the state of its parent components, which by assumption are all slow components.\nSubstituting the factorization (9) into the effective rate matrix (7), we obtain\nq̃iα,β = ∑ ζ ∏ k πk|Par(k) ζk |Pk(α)  qi|Par(i)αi,βi |(Pi(α),ζ)), where the product is over k ∈ Ifast ∩ Par(i) (i.e., fast parents of Xi) and the sum is over ζ in the corresponding state space Sfast ∩ SPar(i). Note that Q̃i is only conditional on those components that are either slow parents of Xi, or (non-exclusively) slow parents of fast parents of Xi.\nRemark. In such cases where the fast dynamics can be factored into independent components, there is no necessity for all fast components to evolve on the same fast scale. The results remain unchanged if each fast component has its own time scale i, as long as i → 0 for all i ∈ Ifast.\nExample 4.1 Consider the three-component system depicted in Figure 2 (left), which consists of a chain of three components, the one in the middle being fast. The dynamics are defined by the conditional rate matricesQ1,Q2|1 and Q3|2. Let π2|1 be the equilibrium distribution of X2 given a fixed state of X1. Theorem 3.1 implies that as → 0, the joint distribution p̃ of the slow components X1,X3 tends to the solution of a master equation which corresponds to the reduced two-state CTBN shown in Figure 2 (right). The effective rate matrix Q̃ is determined by the conditional rate matrices Q̃1 and Q̃3|1 given by\nq̃1α1,β1 = q 1 α1,β1\nq̃3|1 α3,β3 |α1 = ∑ ζ2∈S2 π2|1 ζ2 |α1 q 3|2 α3,β3 |ζ2 .\nExample 4.2 Consider the CTBN shown in Figure 3 (left). The dynamics are defined by the conditional rate matrices\nQ1, Q2, Q3|1, Q4|1,2, Q5|1,3,4 and Q6|3. The components X3,X4 are assumed to be fast, and have conditional equilibrium distributions π3|1 and π4|1,2, respectively. As → 0, the slow components weakly converge to a four-component Markov process on Sslow defined by the conditional rate matrices Q̃1, Q̃2, Q̃5|1,2, and Q̃6|1 whose entries are given by\nq̃1α1,β1 = q 1 α1,β1 q̃2α2,β2 = q 2 α2,β2\nq̃5|1,2 α5,β5 |(α1,α2) = ∑ ζ3∈S3 π3|1 ζ3 |α1 ∑ ζ4∈S4 π4|1,2 ζ4 |(α1,α2)q 5|1,3,4 α5,β5 |(α1,ζ3,ζ4)\nq̃6|1 α6,β6 |α1 = ∑ ζ3∈S3 π3|1 ζ3 |α1 q 6|3 α6,β6 |ζ3 .\nNote that although X6 and X5 are statistically dependent, both being descendants of X3, they do not directly influence each other in the reduced CTBN; in general, the elimination of a fast component does not introduce graphical connections between its descendants, in contrast to node elimination in Bayesian networks. This point is highly non-trivial: for finite the knowledge of X5 influences the posterior distribution of X3, which in turn affects the evolution of X6. In the limit of extreme scale separation, X3 equilibrates between successive transitions of the slow components, therefore the effective rate matrix of X6 is only affected by the\nstationary distribution of X3, which, in turn, is only affected by X1."
    }, {
      "heading" : "4.2 Connected fast components",
      "text" : "In situations where parent-child relations exist between fast components, the equilibrium distribution on Sfast conditional of Sslow does not factor into a product over fast components. A simple minded solution is to group together fast components that are connected together into a compound component, as illustrated in the following example:\nExample 4.3 Consider the CTBN shown in Figure 4 (left), which is defined by the conditional rate matrices Q1, Q2, Q3|1, Q4|3,2 and Q5|4. The component X3,X4 are assumed to be fast. This situation can be adapted to the framework considered in the previous subsection by grouping together X3 and X4 into a single component with state space S3×S4. This intermediate CTBN is depicted in Figure 4 (center). Let then π3,4|1,2 denote the equilibrium distribution of the pair X3,X4 given the pair X1,X2. As → 0, the slow components weakly converge to the Markov processes depicted in the right, with conditional rate matrices Q̃1, Q̃2, and Q̃5|1,2, whose entries are given by\nq̃1α1,β1 = q 1 α1,β1 q̃2α2,β2 = q 2 α2,β2\nq̃5|1,2 α5,β5 |(α1,α2) = ∑ ζ3∈S3 ∑ ζ4∈S4 π3,4|1,2(ζ3,ζ4)|(α1,α2)q 5|4 α5,β5 |ζ4 .\nThis solution, however, misses some of the structure in the reduced process.\nExample 4.4 Consider the CTBN shown in Figure 5 (left), which is defined by the conditional rate matrices Q1, Q2, Q3|1, Q4|3,2, Q5|3, and Q6|4. The components X3,X4 are\nassumed to be fast. If we group X3 and X4, we again have that π3,4|1,2 describes the equilibrium distribution of X3 and X4 given X1 and X2. This would imply that X5 depends on both X1 and X2 in the reduced CTBN. However, if we examine the pattern of influence in the original CTBN, the intuition is that there is no path of influence from X2 to X5.\nThe situation becomes clearer once we realize that we can represent the equilibrium distribution of X3 and X4 as\nπ3,4|1,2(ζ3,ζ4)|(α1,α2) = π 3|1 ζ3 |α1π 4|1,2,3 ζ4 |(α1,α2,ζ3).\nTo see this, note that X4 does not influence X3. Thus, if we fix the values of X1, the process X3 is Markovian and reaches the same equilibrium distribution regardless of the state of X2. Repeating the same argument as in the previous example, we conclude that as → 0, the slow components weakly converge to the Markov processes depicted on the right, with conditional rate matrices Q̃1, Q̃2, Q̃5|1, and Q̃6|1,2, whose entries are given by\nq̃1α1,β1 = q 1 α1,β1 q̃2α2,β2 = q 2 α2,β2\nq̃5|1 α5,β5 |α1 = ∑ ζ3∈S3 π3|1 ζ3 |α1 q 5|3 α5,β5 |ζ3\nq̃6|1,2 α6,β6 |(α1,α2) = ∑ ζ3∈S3 ∑ ζ4∈S4 π3,4|1,2(ζ3,ζ4)|(α1,α2)q 6|4 α6,β6 |ζ4 .\nNote that in the last equation we sum over ζ3, for the purpose of finding the marginal probability of X4 in π3,4|1,2.\nThe point of the last example is that although we cannot factor the equilibrium distribution of the two fast components X3 and X4, the marginal distribution of one variable, X3 in this example, does not depend on the conditional rate matrix of the other variable.\nTo generalize this line of reasoning, we need to characterize which marginal distributions of the equilibrium distribution can be computed independently of rates of the other components. To analyze such situations, we consider a more general result about marginal distributions in CTBNs.\nDefinition 4.1 Let C be an M-component CTBN, and let J be a subset of the components 1, . . . ,M; we denote by\nX J(t) = {Xi(t) : i ∈ J}\nthe corresponding sub-process. We say that J is upward closed if for every i ∈ J, we have that Par(i) ⊆ J. We define the upward closure Up(J) to be the minimal upward closed set that contains J.\nExample 4.5 In Example 4.4 (Figure 5) J = {1, 3, 5} is an upward closed subset of components, and\nUp({4}) = {1, 2, 3, 4} .\nSuppose we are given a CTBN C. Given an upward closed subset of components, J, we can define the sub-CTBN spanned by J. Formally, we define\nCJ = 〈 G|J, { Qi|Par(i) } i∈J ,P0|J 〉 ,\nwhere G|J is the sub-graph of G restricted to the components in J, and P0|J is the marginal distribution of P0 over X J(0). The sub-CTBN spanned by J, contains the conditional rate matrices from the original CTBN for all the components in J. Since J is upward closed, this results in a well defined CTBN, as the parents of every component in J appear in the sub-CTBN.\nTheorem 4.1 Let C be an M-component CTBN, and let J be an upward closed subset of components. Then, the marginal distribution over X J(t) in C is identical to the distribution over X J(t) in CJ.\nThe proof is straightforward, as we can show that the probability over any trajectory of X J is the same in both distributions. This follows, for example, if we sum the master equation (4) over all indexes, bi, i < J.\nCorollary 4.1 Let C be an M-component CTBN, and let J be an upward closed subset of components. Then, the marginal equilibrium distribution over X J does not depend on the rates associated with the remaining variables.\nUsing this result we can return to the question of elimination of fast components. Suppose we have a connected set of fast components. Since they are much faster than the slow components, we can view their behavior as though the slow components are fixed. This implies that the equilibrium distribution over the connected fast components has\nthe behavior of the conditional-CTBN defined by their conditional rate matrices, with the slow components fixed. In this conditional CTBN, we can apply Corollary 4.1 and find the set of conditional rate matrices that determine the equilibrium distribution of any particular subset of fast components.\nFor example, this result implies that in Example 4.4 the equilibrium distribution over X3 in the “fast” conditional CTBN depends on the rate matrix Q3|1 but not on Q4|2,3. As a consequence the child, X5, of X3 depends on X1 in the reduced CTBN, but not on X2.\nWe now use this intuition to define the reduced CTBN in a precise manner. Consider a CTBN with fast and slow components. Given a set, J ⊆ Ifast of fast components, we define Up f (J) to be the upward closure of J in the subgraph that only consists of fast component; Up f (J) is the smallest subset of fast components that contains J, such that if i ∈ J, then j ∈ J for all j ∈ Par(i) ∩ Ifast; to shorten the terminology, we will call Up f (J) the fast-upward closure of J. We then define for J ⊆ Ifast the set\nsPar(J) = { i ∈ Islow : ∃ j ∈ Up f (J), i ∈ Par( j) } .\nThat is, sPar(J) are the slow components that are parents of components in J, or components in the fast upward closure of J. We will call sPar(J) the set of last slow ancestors of J.\nExample 4.6 Consider once again Example 4.4 (Figure 5). There,\nUp f ({4}) = {3, 4}\nis the upward closure of the subset of components {4}, in the subgraph that only contains the fast components. Moreover,\nsPar({4}) = {1, 2} ,\nsince X2 is a parent of X4 and X1 is a parent of X3, which belongs to fast-upward closure of {4}.\nWe now can formally define the procedure of building a reduced CTBN. Assume we are given a CTBN C with scale separation. We define the reduced CTBN C̃ as follows:\n1. G̃ is the graph over Islow such that for each i ∈ Islow\nP̃ar(i) = (Par(i) ∩ Islow) ∪ sPar(Par(i) ∩ Ifast)\nIn other words, the parents of each slow component Xi in the reduced CTBN C̃ are its slow parents in C supplemented by the last slow ancestors of its fast parents in C. Consistently with out notations we define P̃i(α) to be the restriction of α ∈ Sslow to those components that belong to P̃ar(i).\n2. For each i ∈ Islow we define the conditional rate matrix Q̃i|P̃ar(i) with entries\nq̃i|P̃ar(i) αi,βi |P̃i(α) = ∑ ζ π Up f (Par(i)∩Ifast)|P̃ar(i) ζ|P̃i(α) qi|Par(i) αi,βi |Pi((α,ζ)),\nwhere the summation variable ζ takes values in the state space spanned by the fast-upper closure of the fast parents of Xi. While the last equation may be difficult to parse, it bears a simple interpretation. The conditional rate matrix of the i-th component in the reduced CTBN is obtained by averaging over Qi|Par(i) with respect to the marginal equilibrium distribution of the fast-upward closure of the fast parents of Xi, conditioned by the last slow ancestors of this fastclosure. The effective conditional rate matrix depends, as a result, only on the slow parents of Xi and on the last slow ancestors of its fast parents.\n3. P̃0 = P0|Islow .\nOur main Theorem 3.1, reformulated in the language of CTBNs, implies:\nTheorem 4.2 Let C be an M-component CTBN with conditional rate matrices satisfying Assumption 3.1. Then, as → 0, the marginal distribution of the sub-process spanned by the slow components Islow converges to the distribution induced by the reduced CTBN C̃."
    }, {
      "heading" : "5 Numerical examples",
      "text" : "Example 5.1 Consider Example 4.1 with state space S j = {0, 1}, j = 1, 2, 3, and conditional rate matrices\nQ1 = ( −1 1 2 −2 ) Q2|1·|0 = ( −2 2 3 −3 ) Q2|1·|1 = ( −3 3 2 −2\n) Q3|2·|0 = ( −3 3 4 −4 ) Q3|2·|1 = ( −5 5 6 −6\n) The conditional equilibrium distribution π2|1 is\nπ2|1·|0 = 1 5 ( 3 2 ) π2|1·|1 = 1 5 ( 2 3 ) .\nAs → 0 the slow components X1,X3 weakly converge to a Markov process with effective rate matrices Q̃1 = Q1, and Q̃3|1 given by\nQ̃3|1·|0 = π 2|1 0|0Q 3|2 ·|0 + π 2|1 1|0Q 3|2 ·|1 = 1 5 ( −19 19 24 −24 ) Q̃3|1·|1 = π 2|1 0|1Q 3|2 ·|0 + π 2|1 1|1Q 3|2 ·|1 = 1 5 ( −21 21 26 −26 ) .\nThe total effective matrix is\nQ̃ =  −4.75 1 3.75 0 2 −6.25 0 4.25 4.75 0 −5.75 1\n0 5.25 2 −7.25  , with a lexicographic ordering of the states.\nTo test the accuracy of the procedure we have generated paths of length T = 50000 and used standard maximum likelihood to estimate the rate matrix, assuming that the process (X1(t),X3(t)) is a Markov process. For = 0.05 we obtained\nQ̃ =0.05est. =  −4.7907 0.9942 3.7965 0 1.9958 −6.1869 0 4.1911 4.8004 0 −5.8066 1.0062\n0 5.1638 1.9886 −7.1524  , i.e., deviations of about one percent. For = 0.2 we obtained\nQ̃ =0.2est. =  −4.8259 0.9955 3.8304 0 2.0033 −6.2026 0 4.1993 4.8555 0 −5.8644 1.0089\n0 5.2026 1.9936 −7.1962  , i.e., deviations of about two percent. In either case the main source of error seems to be statistical. The fact that parameter estimation converges as T → ∞ is by itself not surprising, since the whole process is ergodic. The question is to what extent is the reduced process (X1(t),X3(t)) Markovian? To test this we re-evaluated the entries of the rate matrix conditional on the preceding state. This yielded changes of about one percent, which again, could be attributed to a lack of statistics.\nExample 5.2 Consider next Example 4.4 with S j = {0, 1}, j = 1, . . . , 6, and conditional rate matrices,\nQ1 = ( −1 1 2 −2 ) Q2 = ( −2 2 1 −1 ) Q3|1·|0 = ( −2 2 3 −3 ) Q3|1·|1 = ( −3 3 2 −2\n) Q4|2,3·|(0,0) = ( −3 3 4 −4 ) Q4|2,3·|(1,0) = ( −4 4 3 −3\n) Q4|2,3·|(0,1) = ( −1 1 2 −2 ) Q4|2,3·|(1,1) = ( −2 2 1 −1\n) Q5|3·|0 = ( −1 1 3 −3 ) Q5|3·|1 = ( −3 3 1 −1\n) Q6|4·|0 = ( −1 1 4 −4 ) Q6|4·|1 = ( −4 4 1 −1 ) .\nThe slow component X5 is only influenced by the fast component X3, whose conditional equilibrium distribution only\ndepends on its slow ancestor X1:\nπ3|1·|0 = 1 5 ( 3 2 ) π3|1·|1 = 1 5 ( 2 3 ) .\nAs → 0 the conditional rate matrix associated with X5 converges to\nQ̃5|1·|0 = π 3|1 0|0Q 5|3 ·|0 + π 3|1 1|0Q 5|3 ·|1 = 1 5 ( −9 9 11 −11 ) Q̃5|1·|1 = π 3|1 0|1Q 5|3 ·|0 + π 3|1 1|1Q 5|3 ·|1 = 1 5 ( −11 11 9 −9 ) .\nTo test the reduction procedure, we generated a trajectory of the full process with 107 transitions and = 0.05. We used maximum likelihood to estimate the rates associated with the reduced process that only consists of the slow components X1,X2,X5,X6. For example, we estimated the rates associated with X5 transitioning from 0 to 1; one can estimate these rates assuming that they depend on the state of all other (slow) components. The estimation procedure gives\nq5|1,2,60,1|(0,0,0) = 1.78 q 5|1,2,6 0,1|(1,0,0) = 2.19 q5|1,2,60,1|(0,1,0) = 1.77 q 5|1,2,6 0,1|(1,1,0) = 2.20 q5|1,2,60,1|(0,0,1) = 1.77 q 5|1,2,6 0,1|(1,0,1) = 2.18 q5|1,2,60,1|(0,1,1) = 1.80 q 5|1,2,6 0,1|(1,1,1) = 2.18.\nThese results are in very good agreement with our prediction that the rates in the left column be equal to 9/5 = 1.8 and the rates in the right column to 11/5 = 2.2, irrespectively of the values of X2 and X6. Finally, we show in Table 1 the estimated values of q̃5|10,1|0 and q̃5|10,1|1 for various values of . The case = 1 means that all components evolve on comparable time scales, in which case the reduction procedure does not hold. This table confirms the intuitive feeling that the approximation gets reasonably accurate for of the order of 0.1. Note that the estimate of q̃5|10,1|1 for = 0.025 is less accurate than for = 0.05; this is attributed to the fact that the smaller , the less (relatively) probable it is to observe a transition in the slow variables, resulting in poor statistics."
    }, {
      "heading" : "6 Discussion",
      "text" : "In this paper we have proved a theorem about dimension reduction of CTBNs in the limit of an infinite separation of scales between fast and slow components. We showed the implications of this theorem for constructing a reduced CTBN that captures the dynamics of the slow components without explicitly dealing with the fast ones.\nOur results show that the elimination of fast components has a counter intuitive property. The typical intuition is that\nintegrating out a variable introduces dependencies among its children. However, when eliminating fast components this intuition does not apply directly, and in some cases we end with a simpler CTBN than we started with. Thus, our reduction leads to further simplifications than one might expect from basic intuitions about Bayesian networks.\nIn practice, one seldom encounters systems in which a small parameter is explicitly given. In many applications there exists a range of characteristic rates, and one has to verify to what extent the dimension reduction is a good approximation. Since equilibration is exponentially fast, dimensional reduction is expected to be a good approximation when the equilibration rates associated with a subset of components are larger, by at least an order of magnitude, than the equilibration rates associated with the other components.\nTo put the results we introduce here to use we need to develop them into concrete approximation algorithms. The appeal of such results is that they give us a strategy to use separation of scales to reason about the system at different levels of time granularity. For reasoning about coarse time scales, our results allow to reduce the system to examine only the slow components. To reason about fine time scales we can then assume that most of the slow components are fixed, and then reason about the dynamics of the fast components. Clearly this intuition can be extended to a hierarchy of time scales.\nGiven a CTBN, we can assess the characteristic equilibration rate of each conditional Q-matrix by computing the absolute value of its second largest eigenvalue. There are, however, multiple ways of using these values to separate the system into an approximate hierarchy of scales. Another issue deals with evidence. Clearly, once we find a reduced CTBN we can incorporate evidence and reason about the posterior probability of slow components and consequently fast components. However, it is also fairly clear that the frequency of observations and the time scale of the observed variables can make important impact on the approximation.\nThe results we presented here provide solid foundations for introducing scale-based approximation in real applications. Clearly, these initial results are only the first step in the development of promising approximate inference procedures."
    }, {
      "heading" : "Acknowledgments",
      "text" : "Nir Friedman was supported in part by grants from the Israel Science Foundation (ISF) and from the Binational USIsrael Science Foundation (BSF). Raz Kupferman was supported in part by a grant from the Israel Science Foundation (ISF)."
    }, {
      "heading" : "A Proof sketch of Theorem 3.1",
      "text" : "Using the partition of the Q-matrix as the sum of fast and slow components, the master equation (1) takes the form\nd dt p − 1 (Qfast)Tp = (Qslow)Tp.\nIf we treat the right hand side as an inhomogeneous term, this equation can be integrated, resulting in an integral equation,\np(t) = e t (Q fast)T p(0) + ∫ t\n0 e\n(t−s) (Q\nfast)T (Qslow)Tp(s) ds.\nThe one-parameter semigroup of operators exp(tQfast) is the solution operator of the master equation derived from the fast dynamics, with the slow components held fixed.\nAssumption 3.1 implies that exp(tQfast) converges exponentially fast, as t → ∞, to an operator G, which is the orthogonal projection onto the subspace of distributions that are invariant under Qfast. The projection G has entries\nga,b = πIfast |IslowFast(b)| Slow(b) δSlow(a),Slow(b).\nAs → 0, the distribution p(t) tends to the solution of the limiting equation,\np(t) = GTp(0) + ∫ t\n0 GT(Qslow)Tp(s) ds,\nwhich is equivalent to the differential equation,\nd dt p = GT(Qslow)Tp. (10)\nNote that according to the limiting equation p(0) is in the range of the projection GT; if this assumption does not hold, the actual limit of p(t) deviates from the solution of the limiting equation only in a short time interval after the initial time (a “boundary layer”).\nThe range ofGT, which consists of distributions of the form\npa = πIfast |IslowFast(a)| Slow(a)p̃Slow(a),\nwhere p̃ is the marginal distribution over Sslow, is invariant under Equation (10). Substituting this product into Equation (10) and summing over all Fast(a), we get an equation for the marginal distribution,\nd dt p̃α = ∑ β∈Sslow  ∑ ζ∈Sfast qslow(β,ζ),(α,ζ)π Ifast |Islow ζ|β  p̃β. Comparing with (8), the expression in the brackets is identified as q̃α,β, i.e., p̃ satisfies a master equation with rate matrix Q̃."
    } ],
    "references" : [ {
      "title" : "Markov chains with stationary transition probabilities",
      "author" : [ "K.L. Chung" ],
      "venue" : null,
      "citeRegEx" : "Chung.,? \\Q1960\\E",
      "shortCiteRegEx" : "Chung.",
      "year" : 1960
    }, {
      "title" : "Handbook of stochastic methods",
      "author" : [ "C.W. Gardiner" ],
      "venue" : "SpringerVerlag, New-York, third edition,",
      "citeRegEx" : "Gardiner.,? \\Q2004\\E",
      "shortCiteRegEx" : "Gardiner.",
      "year" : 2004
    }, {
      "title" : "Skorokhod. The theory of Stochastic processes II",
      "author" : [ "A.V.I.I Gikhman" ],
      "venue" : null,
      "citeRegEx" : "Gikhman,? \\Q1975\\E",
      "shortCiteRegEx" : "Gikhman",
      "year" : 1975
    }, {
      "title" : "Extracting macroscopic dynamics: model problems and algorithms",
      "author" : [ "D. Givon", "R. Kupferman", "A.M. Stuart" ],
      "venue" : "Nonlinearity, 17:R55–R127,",
      "citeRegEx" : "Givon et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Givon et al\\.",
      "year" : 2004
    }, {
      "title" : "Continuous time Bayesian networks",
      "author" : [ "U. Nodelman", "C.R. Shelton", "D. Koller" ],
      "venue" : "In Eighteenth Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Nodelman et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Nodelman et al\\.",
      "year" : 2002
    }, {
      "title" : "Learning continuous time Bayesian networks",
      "author" : [ "U. Nodelman", "C.R. Shelton", "D. Koller" ],
      "venue" : "In Nineteenth Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Nodelman et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Nodelman et al\\.",
      "year" : 2003
    }, {
      "title" : "Elimination of fast variables",
      "author" : [ "N.G. van Kampen" ],
      "venue" : "Phys. Rep.,",
      "citeRegEx" : "Kampen.,? \\Q1985\\E",
      "shortCiteRegEx" : "Kampen.",
      "year" : 1985
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : ", chemical kinetics, population dynamics, stock markets, and many more (Gardiner [2004]).",
      "startOffset" : 72,
      "endOffset" : 88
    }, {
      "referenceID" : 1,
      "context" : ", chemical kinetics, population dynamics, stock markets, and many more (Gardiner [2004]). We consider Markov processes that are homogeneous in time and have a finite state space. Such systems are fully determined by the state space S, the distribution of the process at the initial time, and a description of the dynamics of the process. These dynamics are specified by a rate matrix Q, whose off-diagonal entries qa,b are exponential rate intensities for transitioning between states. Intuitively, we can think of the entry qa,b as the rate parameter of an exponential distribution whose value is the duration of time spent in state a before transitioning to b. When more than one transition is possible, the shortest duration determines the next state. Thus, q−1 a,b is the expected duration in state a before transitioning to state b (assuming it were the only possible transition), and ( ∑ b,a qa,b)−1 is the expected duration before transitioning out of state a. In many applications, the state space is of the form of a product space S = S1 × S1 × · · · × SM, where M is the number of components (such processes are called multicomponent). Even if each of the Si is of low dimension, the dimension of the state space is exponential in the number of components, which often poses computational difficulties, e.g., in learning applications. Continuous-time Bayesian networks (Nodelman et al. [2002, 2003]) are a graphical representation for Markov processes that have extra structure, therefore allowing for more a compact representation with fewer parameters. The first assumption is that transitions only occur in one component at a time. Second, the transition rates associated with each component are assumed to only depend on the state of a collection of “parent components”. The CTBN is a directed, possibly cyclic graph whose nodes are the components of the process, and whose edges represent parent-child relations. It is also often the case that certain components are considerably faster than the others. Mathematically, it is assumed that the (conditional) rates associated with the fast components are larger by a factor of 1/ than the (conditional) rates associated with the other, slow components, with 1. Systems having such property are said to have a separation of scales, or to be singularly perturbed. Such situations are ubiquitous, for example, in chemical kinetics, where some reactions may occur much faster than other. In such situations, the fast components tend to reach “local equilibrium”, relative to the slow components, and under certain conditions, reduced Markovian dynamics can be derived for a lower dimensional system that only involves the slow components (see van Kampen [1985] for a classical",
      "startOffset" : 72,
      "endOffset" : 2720
    }, {
      "referenceID" : 3,
      "context" : "review on dimension reduction in scale separated systems; see Givon et al. [2004] for a recent review).",
      "startOffset" : 62,
      "endOffset" : 82
    }, {
      "referenceID" : 3,
      "context" : "In this section we briefly review the CTBN model (Nodelman et al. [2002]).",
      "startOffset" : 50,
      "endOffset" : 73
    }, {
      "referenceID" : 0,
      "context" : "Provided that the transition function satisfies certain analytical properties (continuity, and regularity; see Chung [1960]) the dynamics are fully captured by a constant matrix Q—the rate, or intensity matrix—whose entries qa,b are defined by",
      "startOffset" : 111,
      "endOffset" : 124
    }, {
      "referenceID" : 2,
      "context" : "There is a one-to-one correspondence between the description of a Markov process by means of a master equation, and by means of a “pathwise” characterization (up to stochastic equivalence of the latter; see Gikhman and Skorokhod [1975]).",
      "startOffset" : 207,
      "endOffset" : 236
    }, {
      "referenceID" : 4,
      "context" : "Equation (3) is, using the terminology of Nodelman et al. [2002], the “amalgamation” of the M conditional rate matrices.",
      "startOffset" : 42,
      "endOffset" : 65
    }, {
      "referenceID" : 4,
      "context" : "As stated in Nodelman et al. [2002], the graph structure has two main roles: (i) it provides a data structure to which parameters are associated; (ii) it provides a qualitative description of dependencies among the various components of the system.",
      "startOffset" : 13,
      "endOffset" : 36
    } ],
    "year" : 2006,
    "abstractText" : "Continuous-time Bayesian networks (CTBNs) are graphical representations of multi-component continuous-time Markov processes as directed graphs. The edges in the network represent direct influences among components. The joint rate matrix of the multi-component process is specified by means of conditional rate matrices for each component separately. This paper addresses the situation where some of the components evolve on a time scale that is much shorter compared to the time scale of the other components. We prove that in the limit where the separation of scales is infinite, the Markov process converges (in distribution, or weakly) to a reduced, or effective Markov process that only involves the slow components. We also demonstrate that for a reasonable separation of scales (an order of magnitude) the reduced process is a good approximation of the marginal process over the slow components. We provide a simple procedure for building a reduced CTBN for this effective process, with conditional rate matrices that can be directly calculated from the original CTBN, and discuss the implications for approximate reasoning in large systems.",
    "creator" : "TeX"
  }
}