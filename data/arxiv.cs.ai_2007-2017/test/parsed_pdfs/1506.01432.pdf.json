{
  "name" : "1506.01432.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 6.\n01 43\n2v 2\n[ cs\n.A I]\n8 J\nun 2\nMarkov logic uses weighted formulas to compactly encode a probability distribution over possible worlds. Despite the use of logical formulas, Markov logic networks (MLNs) can be difficult to interpret, due to the often counter-intuitive meaning of their weights. To address this issue, we propose a method to construct a possibilistic logic theory that exactly captures what can be derived from a given MLN using maximum a posteriori (MAP) inference. Unfortunately, the size of this theory is exponential in general. We therefore also propose two methods which can derive compact theories that still capture MAP inference, but only for specific types of evidence. These theories can be used, among others, to make explicit the hidden assumptions underlying an MLN or to explain the predictions it makes."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Markov logic [22] and possibilistic logic [9] are two popular logics for modelling uncertain beliefs. Both logics share a number of important characteristics. At the syntactic level, formulas correspond to pairs (α, λ), consisting of a classical formula α and a certainty weight λ, while at the semantic level, sets of these formulas induce a mapping from possible worlds to [0, 1], encoding the relative plausibility of each possible world.\nDespite their close similarities, however, Markov logic and possibilistic logic have been developed in different communities and for different purposes: Markov logic has mainly been studied in a machine learning context whereas possibilistic logic has been studied as a knowledge representation language. This reflects the complementary strengths and weaknesses of these logics. On the one hand, the qualitative nature of possibilistic logic makes it challenging to use for learning; although a few interesting approaches for\nlearning possibilistic logic theories from data have been explored (e.g. [24]), their impact on applications to date has been limited. On the other hand, the intuitive meaning of Markov logic theories is often difficult to grasp, which limits the potential of Markov logic for knowledge representation. The main culprit is that the meaning of a theory can often not be understood by looking at the individual formulas in isolation. This issue, among others, has been highlighted in [26], where coherence measures are proposed that evaluate to what extent the formulation of a Markov logic theory is misleading.\nExample 1. Consider the following Markov logic formulas:\n+∞ : antarctic-bird(X) → bird(X)\n10 : bird(X) → flies(X)\n5 : antarctic-bird(X) → ¬flies(X)\nWhile the last formula might appear to suggest that antarctic birds cannot fly, in combination with the other two formulas, it merely states that antarctic birds are less likely to fly than birds in general.\nPossibilistic logic is based on a purely qualitative, comparative model of uncertainty: while a Markov logic theory compactly encodes a probability distribution over the set of possible worlds, a possibilistic logic theory merely encodes a ranking of these possible worlds. Even though a probability distribution offers a much richer uncertainty model, many applications of Markov logic are based on MAP inference, which only relies on the ranking induced by the probability distribution.\nIn this paper, we first show how to construct a possibilistic logic theory Θ, given a Markov logic theory M, such that the conclusions that we can infer from Θ are exactly those conclusions that we can obtain from M using MAP inference. Our construction can be seen as the syntactic counterpart of the probability-possibility transformation from [10]. In principle, it allows us to combine the best of both worlds, using M for making predictions while using Θ for elucidating the knowledge that is captured by M (e.g. to verify\nthat the theory M is sensible). However, the size of Θ can be exponential in the size of M, which is unsurprising given that the computational complexity of MAP inference is higher than the complexity of inference in possibilistic logic. To overcome this problem, we begin by studying ground (i.e. propositional) theories and propose two novel approaches for transforming a ground MLN into a compact ground possibilistic logic theory that still correctly captures MAP inference, but only for specific types of evidence (e.g. sets of at most k literals). Then we lift one of these approaches such that it can transform a first-order MLN into a first-order possibilistic logic theory. Finally, we present several examples that illustrate how the transformation process can be used to help identify unintended consequences of a given MLN, and more generally, to better understand its behaviour.\nThe remainder of the paper is structured as follows. In the next section, we provide some background on Markov logic and possibilistic logic. In Section 3, we analyse the relation between MAP inference in ground Markov logic networks and possibilistic logic inference, introducing in particular two methods for deriving compact theories. Section 4 then discusses how we can exploit the symmetries in the case of an ungrounded Markov logic network, while Section 5 provides some illustrative examples. Finally, we provide an overview of related work in Section 6.\nDue to space limitations, some of the proofs have been omitted from this paper. These proofs can be found in an online appendix.1"
    }, {
      "heading" : "2 BACKGROUND",
      "text" : ""
    }, {
      "heading" : "2.1 MARKOV LOGIC",
      "text" : "A Markov logic network (MLN) [22] is a set of pairs (F,wF ), whereF is a formula in first-order logic and wF is a real number, intuitively reflecting a penalty that is applied to possible worlds (i.e. logical interpretations) that violate F . In examples, we will also use the notation wF : F to denote the formula (F,wF ). An MLN serves as a template for constructing a propositional Markov network. In particular, given a set of constants C, an MLN M induces the following probability distribution on possible worlds ω:\npM(ω) = 1\nZ exp\n  ∑\n(F,wF )∈M\nwFnF (ω)\n  , (1)\nwhere nF (x) is the number of true groundings of F in the possible world ω, and Z is a normalization constant to ensure that pM can be interpreted as a probability distribution. Sometimes, formulas (F,wF ) are considered where wF = +∞, to represent hard constraints. In such cases,\n1http://arxiv.org/abs/1506.01432\nwe define pM(ω) = 0 for all possible worlds that do not satisfy all of the hard constraints, and only formulas with a real-valued weight are considered in (1) for the possible worlds that do. Note that a Markov logic network can be seen as a weighted set of propositional formulas, which are obtained by grounding the formulas in M w.r.t. the set of constants C in the usual way. In the particular case that all formulas in M are already grounded, M corresponds to a theory in penalty logic [13].\nOne common inference task in MLNs is full MAP inference. In this setting, given a set of ground literals (the evidence) the goal is to compute the most probable configuration of all unobserved variables (the queries). Two standard approaches for performing MAP inference in MLNs are to employ a strategy based on MaxWalkSAT [22] or to use a cutting plane based strategy [23, 18]. Given a set of ground formulas E, we write max(M, E) for the set of most probable worlds of the MLN that satisfy E. For each ω ∈ max(M, E),\n∑ (F,wF )∈M\nwFnF (ω) evaluates to the same value, which we will refer to as sat(M, E). We define the penalty pen(M, E) of E as follows:\npen(M, E) = sat(M, ∅)− sat(M, E)\nWe will sometimes identify possible worlds with the set of literals they make true, writing pen(M, ω). We will also write pen(M, α), with α a ground formula, as a shorthand for pen(M, {α}). We will consider the following inference relation, which has been considered among others in [13]:\n(M, E) ⊢MAP α iff ∀ω ∈ max(M, E) : ω |= α (2)\nwith M an MLN, α a ground formula andE a set of ground formulas. It can be shown that checking (M, E) ⊢MAP α for a ground network M is ∆P2 -complete 2 [4]."
    }, {
      "heading" : "2.2 POSSIBILISTIC LOGIC",
      "text" : "A possibility distribution in a universe Ω is a mapping π from Ω to [0, 1], encoding our knowledge about the possible values that a given variable X can take; throughout this paper, we will assume that all universes are finite. For each x ∈ Ω, π(x) is called the possibility degree of x. By convention, in a state of complete ignorance, we have π(x) = 1 for all x ∈ Ω; conversely, if X = x0 is known, we have π(x0) = 1 and π(x) = 0 for x 6= x0. Possibility theory [27, 12] is based on the possibility measure Π and dual necessity measure N , induced by a possibility distribution π as follows (A ⊆ Ω):\nΠ(A) = max a∈A π(a)\nN(A) = 1−Π(Ω \\A)\n2The complexity class ∆P2 contains those decision problems that can be solved in polynomial time on a deterministic Turing machine with access to an NP oracle.\nIntuitively, Π(A) is the degree to which available evidence is compatible with the view that X belongs to A, whereas N(A) is the degree to which available evidence implies that X belongs to A, i.e. the degree to which it is certain that X belongs to A.\nA theory in possibilistic logic [9] is a set of formulas of the form (α, λ), where α is a propositional formula and λ ∈ [0, 1] is a certainty weight. A possibility distribution π satisfies (α, λ) iff N(JαK) ≥ λ, with N the necessity measure induced by π and JαK the set of propositional models of α. We say that a possibilistic logic theory Θ entails (α, λ), written Θ |= (α, λ), if every possibility distribution which satisfies all the formulas in Θ also satisfies (α, λ). A possibility distribution π1 is called less specific than a possibility distribution π2 if π1(ω) ≥ π2(ω) for every ω. It can be shown that the set of models of Θ always has a least element w.r.t. the minimal specificity ordering, which is called the least specific model π∗ of Θ. It is easy to see that Θ |= (α, λ) iff π∗ satisfies (α, λ).\nEven though the semantics of possibilistic logic is defined at the propositional level, we will also use first-order formulas such as (p(X) → q(X,Y ), λ) throughout the paper. As in Markov logic, we will interpret these formulas as abbreviations for a set of propositional formulas, obtained using grounding in the usual way. In particular, we will always assume that first-order formulas are defined w.r.t. a finite set of constants.\nThe λ-cut Θλ of a possibilistic logic theory Θ is defined as follows:\nΘλ = {α | (α, µ) ∈ Θ, µ ≥ λ}\nIt can be shown that Θ |= (α, λ) iff Θλ |= α, which means that inference in possibilistic logic can straightforwardly be implemented using a SAT solver.\nAn inconsistency-tolerant inference relation ⊢poss for possibilistic logic can be defined as follows:\nΘ ⊢poss α iff Θcon(Θ) |= α\nwhere the consistency level con(Θ) of Θ is the lowest certainty level λ for which Θλ is satisfiable (among the certainty levels that occur in Θ). Note that all formulas with a certainty level below con(Θ) are ignored, even if they are unrelated to any inconsistency in Θ. This observation is known as the drowning effect.\nWe will write (Θ, E) ⊢poss α, with E a set of propositional formulas, as an abbreviation for Θ∪ {(e, 1) | e ∈ E} ⊢poss α. Despite its conceptual simplicity, ⊢poss has many desirable properties. Among others, it is closely related to AGM belief revision [8] and default reasoning [2]. It can be shown that checking Θ ⊢poss (α, λ) is a ΘP2 complete problem3 [17]. In this paper, ⊢poss will allow us to capture the non-monotonicity of MAP inference.\n3The complexity class ΘP2 contains those decision problems\nExample 2. Let Θ consist of the following formulas:\n(penguin(X) → bird(X), 1)\n(penguin(X) → ¬flies(X), 1)\n(bird(X) → flies(X), 0.5)\nThen we find:\n(Θ, {bird(tweety)}) ⊢poss flies(tweety)\n(Θ, {bird(tweety), penguin(tweety)}) ⊢poss ¬flies(tweety)\nIn general, ⊢poss allows us to model rules with exceptions, by ensuring that rules about specific contexts have a higher certainty weight than rules about general contexts."
    }, {
      "heading" : "3 ENCODING GROUND NETWORKS",
      "text" : "Throughout this section, we will assume thatM is a ground MLN in which all the weights are strictly positive. This can always be guaranteed for ground MLNs by replacing formulas (α, λ) with λ < 0 by (¬α,−λ), and by discarding any formula whose weight is 0. For a subset X ⊆ M, we write X∗ for the set of corresponding classical formulas, e.g. for X = {(F1, w1), ..., (Fn, wn)} we have X∗ = {F1, ..., Fn}. In particular, M∗ are the classical formulas appearing in the MLN M.\nThe following transformation constructs a possibilistic logic theory that is in some sense equivalent to a given MLN. It is inspired by the probability-possiblity transformation from [10].\nTransformation 1. We define the possibilistic logic theory ΘM corresponding to an MLN M as follows:\n{( ∨ X∗, φ(¬ ∨ X∗)) |X ⊆ M, φ(¬ ∨ X∗) > 0} (3)\nwhere for a propositional formula α:\nφ(α) =\n{ K+pen(M,α)\nL if α satisfies the hard constraints\n1 otherwise\nand the constants K and L are chosen such that 0 = φ(⊤) ≤ φ(α) < 1 for every α that satisfies the hard constraints (i.e. the formulas with weight +∞).\nIn the following we will use the notations φ(ω) for a possible world ω and φ(E) for a set of formulas E, defined entirely analogously. Throughout the paper we will also write (−K < x < L−K):\nλx = K + x\nL\nThe correctness of Transformation 1 follows from the next proposition, which is easy to show.\nthat can be solved in polynomial time on a deterministic Turing machine, by making at most a logaritmic number of calls to an NP oracle.\nProposition 1. Let M be a ground MLN and ΘM the corresponding possibilistic logic theory. Let π be the least specific model of ΘM. It holds that:\nπ(ω) = 1− φ(ω)\nCorollary 1. Let M be a ground MLN and ΘM the corresponding possibilistic logic theory. It holds that for λ < 1:\nΘM |= (α, λ) iff pen(M,¬α) ≥ λL−K\nand\nΘM |= (α, 1) iff pen(M,¬α) = +∞\nCorollary 2. Let M be a ground MLN and ΘM the corresponding possibilistic logic theory. For pM the probability distribution induced by M and π the least specific model of ΘM, it holds that\npM(ω1) > pM(ω2) iff π(ω1) > π(ω2)\nfor all possible worlds ω1 and ω2. In particular, it follows that for every propositional formula α and every set of propositional formulas E:\n(M, E) ⊢MAP α iff (Θ, E) ⊢poss α (4)\nExample 3. Consider the MLN M containing the following formulas:\n5 : a → x 5 : a → y 10 : a ∧ b → ¬y\nThen ΘM contains the following formulas:\nλ5 : a → x λ5 : a → y\nλ10 : a ∧ b → ¬y λ10 : a → x ∨ y\nλ15 : a ∧ b → x ∨ ¬y\nIt can be verified that:\n(ΘM, {a}) ⊢poss x ∧ y (ΘM, {a, b}) ⊢poss x ∧ ¬y\nAn important drawback of the transformation to possibilistic logic is that the number of formulas in ΘM is exponential in |M|. This makes the transformation inefficient, and moreover limits the interpretability of the possibilistic logic theory. In general, the exponential size of ΘM cannot be avoided if we want (4) to hold for any E and α. However, more compact theories can be found if we focus on specific types of evidence. Sections 3.1 and 3.2 introduce two practical methods to accomplish this."
    }, {
      "heading" : "3.1 SELECTIVELY AVOIDING DROWNING",
      "text" : "In many applications, we are only interested in particular types of evidence sets E. For example, we may only\nbe interested in evidence sets that contain at most k literals, or in evidence sets that only contain positive literals. In such cases, we can often derive a more compact possibilistic logic theory ΘE as follows. Let E be the set of evidence sets that we wish to consider, where each E ∈ E is a set of ground formulas. Given E ∈ E we write SE for the set of all minimal subsets {F1, ..., Fl} of M∗E = {F |F ∈ M ∗, pen(M,¬F ) < pen(M, E)} s.t.\npen(M, ∧\nE ∧ ¬F1 ∧ ... ∧ ¬Fl) > pen(M, E) (5)\nThe following transformation constructs a possibilistic logic theory that correctly captures MAP inference for evidence sets in E . The basic intuition is that we want to weaken the formulas in M∗ just enough to ensure that the resulting certainty level prevents them from drowning when the evidence E becomes available.\nTransformation 2. Given a ground MLN M and a set of evidence sets E , we define the possibilistic logic theory ΘEM as follows:\n{(F1, φ(¬F1)) |F1 ∈ M ∗} (6)\n∪ {(¬ ∧ E ∨ ∨ Z, φ( ∧ E ∧ ¬ ∧\nZ)) |Z ∈ SE , (7)\nE ∈ E} ∪ {(¬ ∧ E, φ( ∧ E)) |E ∈ E} (8)\nIf M is clear from the context, we will omit the subscript in ΘEM. The formulas in (6) are the direct counterpart of the MLN. Intuitively, there are two reasons why these formulas are not sufficient. First, due to the drowning effect, formulas F such that pen(M,¬F ) < pen(M, E) will be ignored under the evidenceE. In such cases we should look at minimal ways to weaken these formulas such that the certainty level of the resulting formula is sufficient to avoid drowning under the evidence E. This is accomplished by adding the formulas in (7). Second, as ΘE contains less information than ΘM, we need to ensure that the consistency level for ΘE is never lower than the consistency level for ΘM, given an evidence set E ∈ E . To this end, ΘE includes the formulas in (8). The following example illustrates why these formulas are needed.\nExample 4. Consider the following MLN M:\n3 : u 2 : a 10 : (a ∨ b) ∧ (u ∨ v) → ¬x\n2 : b 1 : v\nand let E = {{x}}, i.e. the only evidence set in which we are interested is {x}. It holds that\nSE = {{a, u}, {b, u}, {a, v}, {b, v}} (9)\nand ΘE = Θ ∪Ψ ∪ Γ, where:\nΘ = {(u, λ3), (a, λ2), ((a ∨ b) ∧ (u ∨ v) → ¬x, λ10),\n(b, λ2), (v, λ1)}\nΨ = {(a ∨ u ∨ ¬x, λ6), (b ∨ u ∨ ¬x, λ6),\n(a ∨ v ∨ ¬x, λ5), (b ∨ v ∨ ¬x, λ5)\nΓ = {(¬x, λ4)}\nIt is easy to verify that (Θ ∪ Ψ, {x}) ⊢poss u whereas (M, {x}) 6⊢MAP u and (Θ ∪Ψ ∪ Γ, {x}) 6⊢poss u.\nWe now prove the correctness of Transformation 2.\nProposition 2. For any formula α and any evidence set E ∈ E , it holds that (ΘM, E) ⊢poss α iff (ΘE , E) ⊢poss α.\nProof. Let us introduce the following notation:\nλE = con(ΘM ∪ {(e, 1) | e ∈ E})\nλEE = con(Θ E ∪ {(e, 1) | e ∈ E})\nA = (ΘM ∪ {(e, 1) | e ∈ E})λE\nAE = (ΘE ∪ {(e, 1) | e ∈ E})λE E\nWe need to show that A is equivalent to AE , for anyE ∈ E .\nBy Corollary 1, we know that every formula (α, λ) in ΘEM is entailed by ΘM, hence λ E E ≤ λE . Since λE is the smallest certainty level from ΘM which is strictly higher than φ(E), it follows that AE contains every formula which appears in ΘE with a weight that is strictly higher than φ(E). Moreover, since ΘE by construction contains (¬ ∧ E, φ( ∧ E)), we find that AE can only contain such formulas:\nAE = E ∪ {α | (α, λ) ∈ ΘE , λ > φ(E)} (10)\nIt follows that A |= AE .\nLet G1 ∨ ... ∨ Gs be a formula from A. From Corollary 1 we know that:\npen(M,¬G1 ∧ ... ∧ ¬Gs) > pen(M, E)\nand a fortiori\npen(M, E ∧ ¬G1 ∧ ... ∧ ¬Gs) > pen(M, E)\nThis means that for any formula G1 ∨ ... ∨Gs in A, either pen(M,¬Gi) > pen(M, E) for some i or SE contains a subset {H1, ..., Hr} of {G1, ..., Gs}. Then ΘE contains either Gi or the formula ¬E ∨ H1... ∨ Hr with a weight which is strictly higher than φ(E) and thus either Gi or ¬E ∨ H1... ∨ Hr belongs to AE . In both cases we find AE |= G1 ∨ ... ∨Gs. We conclude AE |= A.\nAn alternative, which would make the approach in this section closer to the standard encoding in (1), is to define S ′E as the set of minimal subsets {F1, ..., Fl} of M∗E such that\npen(M,¬F1 ∧ ... ∧ ¬Fl) > pen(M, E) (11)\nand then replace the formulas in (7) by\n{( ∨ Z, φ(¬ ∧\nZ)) |Z ∈ S ′E} (12)\nThe advantage of (7), however, is that we can expect many of the sets in SE to be singletons. To see why this is\nthe case, first note that for each world ω in max(M, E), the set of formulas Y ⊆ M∗ satisfied by ω is such that pen(M,¬ ∨ (M∗ \\ Y)) is minimal among all sets Y ′ ⊆ M∗ for which E ∧ ∧ Y ′ is consistent. Let us write ConsE(M) for the set of all these maximally consistent subsets of M∗. Note that max(M, E) = J ∨ { ∧ Y |Y ∈ ConsE(M)}K.\nLemma 1. For a set of formulas {F1, ..., Fl} ⊆ M∗ it holds that pen(M, E ∧ ¬F1 ∧ ... ∧ ¬Fl) > pen(M, E) iff {F1, ..., Fl} ∩ Y 6= ∅ for every Y in ConsE(M).\nCorollary 3. Let ConsE(M) = {Y1, ...,Ys}. It holds that SE consists of the subset-minimal elements of {{y1, ..., ys} | y1 ∈ Y1 ∩M∗E , ..., ys ∈ Ys ∩M ∗ E}.\nExample 5. Consider again the MLN M from Example 4 and let E = {x}. It holds that ConsE(M) = {Y1,Y2}, where\nY1 = {(a ∨ b) ∧ (u ∨ v) → ¬x, a, b}\nY2 = {(a ∨ b) ∧ (u ∨ v) → ¬x, u, v}\nM∗E = {a, b, u, v}\nFrom Corollary 3,it follows that SE is given by (9).\nIn practice, ConsE(M) will often contain a single element, in which case all the elements of SE will be singletons."
    }, {
      "heading" : "3.2 MAP INFERENCE AS DEFAULT REASONING",
      "text" : "A large number of approaches has been proposed for reasoning with a set of default rules of the form “if α then typically β” [15, 19, 14]. At the core, each of the proposed semantics corresponds to the intuition that a set of default rules imposes a preference order on possible worlds, where “if α then β” means that β is true in the most preferred models of α. The approaches from [15] and [19] can be elegantly captured in possibilistic logic [2], by interpreting the default rule as the constraint Π(α∧β) > Π(α∧¬β). In Markov logic, the same constraint on the ordering of possible worlds can be expressed by imposing the constraint (M, α) ⊢MAP β. In other words, we can view the MAP consequences of an MLN as a set of default rules, and encode these default rules in possibilistic logic. The following transformation is based on this idea.\nTransformation 3. Given a ground MLN M and a positive integer k, we construct a possibilistic logic theory ΘkM as follows:\n• For each hard rule F from M, add (F, 1) to ΘkM.\n• For each set of literals E such that 0 ≤ |E| ≤ k, let X = {x | (M, E) ⊢MAP x} be the set of literals that are true in all the most plausible models of E. Unless there is a literal y ∈ E such that ∧ (E \\ {y}) ⊢MAP y,\nadd (∧ E → ∧ X,λE )\nto ΘkM, where λE = φ( ∧\nE). If pen(M, E) > pen(M, ∅), add also\n(¬( ∧ E ∧ ∧\nX), λ′E) (13)\nwhere λ′E is the certainty level just below λE in Θ k M, i.e. λE′ = max{λF |λF < λE , |F | ≤ k}.\nIf M is clear from the context, we will omit the subscript in ΘkM. The possibilistic encoding of default rules used in Transformation 3 is similar in spirit to the method from [2], which is based on the Z-ranking from [19]. However, because pM already provides us with a model of the default rules, we can directly encode default rules in possibilistic logic, without having to rely on the Z-ranking. Also note that although the method is described in terms of an MLN, it can be used for encoding any ranking on possible worlds (assuming a finite set of atoms).\nAs illustrated in the following example, (13) is needed to avoid deriving too much, serving a similar purpose to (8) in the approach from Section 3.1.\nExample 6. Consider the following MLN M:\n2 : ¬a ∨ b 2 : a ∨ b 1 : a ∨ ¬b\nThen Θ1 = Θ ∪Ψ, where\nΘ = {(⊤ → a ∧ b, λ0), (¬a → b, λ1), (¬b → ⊤, λ2)}\nΨ = {(b, λ1), (a ∨ ¬b, λ0)}\nWe find (Θ, {¬b}) ⊢poss a while (M, {¬b}) 6⊢MAP a. Accordingly, we have (Θ ∪Ψ, {¬b}) 6⊢poss a.\nTransformations 2 and 3 have complementary strengths. For example, Transformation 2 may lead to more compact theories for relatively simple MLNs, e.g. if for most of the considered evidence sets, there is a unique set of formulas from the MLN that characterizes the most probable models of the evidence (cf. Lemma 1). On the other hand, Transformation 3 may lead to substantially more compact theories in cases where the number of formulas is large relative to the number of atoms.\nWe now show the correctness of Transformation 3.\nProposition 3. Let M be an MLN, k a positive integer and Θk the proposed possibilistic logic encoding of M. Furthermore, let E and C be sets of literals such that |E|+ |C| ≤ k+ 1. It holds that (M, E) ⊢MAP ∨ C if and only if\n(Θk, E) ⊢poss ∨ C.\nBefore we prove Proposition 3, we present a number of lemmas. In the lemmas and proofs below, M will always be an MLN, Θk will be the corresponding possibilistic logic theory and k will be the maximum size of the evidence sets considered in the translation.\nLemma 2. If E is a set of literals, |E| ≤ k, λ = φ(E) and (M, E) ⊢MAP x then\n{(∧ E′ → ∧ X ) ∈ Θkλ s.t. |E ′| ≤ |E| } ⊢ ∧ E → x\nLemma 3. If φ(ω) ≤ λ then ω is a model of Θkλ.\nProof. If there were a formula F = ( ∧ E) → ( ∧ X) in Θkλ that was not satisfied by ω, then its body would have to be true in ω but then necessarily\nλ ≤ φ(E) ≤ φ(ω) ≤ λ.\nThe first inequality follows from the fact that, by the construction of Θk, if the certainty weight of F is at least λ then it must be the case that φ(E) ≥ λ. The second inequality follows from the fact that ω was assumed to be a model of ∧ E. It follows that:\npen(M, ω) = pen(M, E).\nHowever, this would mean that ω is also a most probable world of (M, E), but then ω |= F by construction of Θk. If there were an unsatisfied formula F = ¬ ( ∧ E ∧ ∧ X) in Θkλ then by construction we would have φ(E ∪X) > λ. However, from ω |= ∧ E ∧ ∧ X we find φ(E ∪ X) ≤ φ(ω) ≤ λ, a contradiction.\nSince all formulas in Θk are of the two considered types, it follows that all formulas from Θk whose certainty weight is at least λ must be satisfied in ω.\nLemma 4. If (M, E) ⊢MAP (y1 ∨ · · · ∨ ym) then\n(i) for any i, either (M, E ∪ {¬yi}) ⊢MAP (y1 ∨ · · · ∨ yi−1 ∨ yi+1 ∨ · · · ∨ ym) or (M, E) ⊢MAP yi,\n(ii) there exist a j and a set {y′1, . . . , y ′ m′} ⊆\n{y1, . . . , ym} \\ {yj} such that (M, E ∪ {¬y′1, . . . ,¬y ′ m′}) ⊢MAP yj .\nLemma 5. If |C| + |E| ≤ k + 1, and λ = φ(E) then Θkλ ∪E ⊢ ∨ C if and only if (M, E) ⊢MAP ∨ C.\nWe now turn to the proof of Proposition 3.\nProof of Proposition 3. Let E be an evidence set such that |E| ≤ k and let λ = φ(E). Given Lemma 5, it is sufficient to show that con(Θk, E) = λ. It follows from Lemma 3 that con(Θk, E) ≤ λ. Let X = {x | (M, E) ⊢MAP x} be the set of literals which can be derived from (M, E) using MAP inference. By construction, Θk contains a formula ¬( ∧ E ∧ ∧ X) with a certainty weight which is just below λ. Specifically, for λ′ < λ we either have Θkλ = Θ k λ′ or\nΘkλ′ |= ¬ ∧ E, from which we find con(Θk, E) = λ.\nIt is of interest to remove any formulas in Θk that are redundant, among others because this is likely to make the theory easier to interpret. Although we can use possibilistic logic inference to identify redundant formulas, in some cases we can avoid adding the redundant formulas altogether. For example, in the transformation procedure, we do not add any rules for E if it holds that E \\ {y} ⊢MAP y for some y ∈ E. This pruning rule is the counterpart of the cautious monotonicity property, which is well-known in the context of default reasoning [15]. Any ranking on possible worlds also satisfies the stronger rational monotonicity property, which translated to our setting states that when (M, E \\ {y}) ⊢MAP x and (M, E \\ {y}) 6⊢MAP ¬y it holds that (M, E) ⊢MAP x. Accordingly, when processing the evidence set E in the transformation procedure, instead of ( ∧ E → ∧ X,λE) it is sufficient to add the following rule:\n( ∧ E → ∧\n(X \\X0), λE)\nwhere\nX0 = {x |E \\ {y} ⊢MAP x and E \\ {y} 6⊢MAP ¬y}\nThe correctness of this pruning step follows from the following proposition.\nProposition 4. Let x and y be literals. If |E| < k, (M, E) ⊢MAP x and (M, E) 6⊢MAP ¬y then:\nΘk \\ {F} |= (∧ E ∧ y → x, λE∪{y} )\nwhere F is the formula in Θk corresponding to the evidence set E ∪ {y}, i.e.:\nF = ∧ (E ∪ {y}) → ∧\n{x | (M, E ∪ {y}) ⊢MAP x}\nProof. If (M, E) ⊢MAP x and (M, E) 6⊢MAP ¬y then (M, E ∪ {y}) ⊢MAP x and pen(M, E) = pen(M, E ∪ {y}) = pen(M, E ∪ {x, y}). Therefore using Lemma 2, we find that ΘkλE∪{y} ⊢ ∧ E → x. From Lemma 2, it\nfurthermore follows that ∧ E → x can be derived from rules with antecedents of length at most |E|. In particular, we find that ∧ E → x can be derived without using the formula ∧ E ∧ y → ∧ X .\nFinally, note that formulas of the form (13) can be omitted when λ′E = λ(E\\{y}) for some y ∈ E. Indeed, in such a case we find from pen(M, E \\ {y}) < pen(M, E) that (M, E \\ {y}) ⊢MAP ¬y, hence (13) will be entailed by a formula of the form (∧ (E \\ {y}) → ∧ X,λE\\{y} ) in Θk."
    }, {
      "heading" : "4 ENCODING NON-GROUND NETWORKS",
      "text" : "We now provide the counterpart to the construction from Section 3.2 for non-ground MLNs. The first-order nature\nof MLNs often leads to distributions with many symmetries which can be exploited by lifted inference methods [20]. We can similarly exploit these symmetries for constructing more compact possibilistic logic theories from MLNs.\nFor convenience, in the possibilistic logic theories, we will use typed formulas. For instance, when we have the formula α = owns(person : X, thing : Y ) and the set of constants of the type person is {alice, bob} and the set of constants of the type thing is {car} then α corresponds to the ground formulas owns(alice, car) and owns(bob, car). In cases where there is only one type, we will not write it explicitly.\nTwo typed formulas F1 and F2 are said to be isomorphic when there is a type-respecting substitution θ of the variables of F1 such that F1θ ≡ F2 (where ≡ denotes equivalence of logical formulas). Two MLNs M1 and M2 are said to be isomorphic, denoted by M1 ≈ M2, if there is a bijection i from formulas of M1 to formulas of M2 such that for i(F,w) = (F ′, w′) it holds that w = w′ and the formulas F and F ′ are isomorphic. When j is a permutation of a subset of constants from M then j(M) denotes the MLN obtained by replacing any constant c from the subset by its image j(c).\nGiven a non-ground MLN M, we can first identify sets of constants which are interchangeable, where a set of constants Ct is said to be interchangeable if j(M) ≈ M for any permutation j of the constants in Ct. Note that to check whether a set of constants Ct is interchangeable, it is sufficient to check that j(M) ≈ M for those permutations which swap just two constants from Ct. For every maximal set Ct of interchangeable constants, we introduce a new type t. For a constant c, we write τ(c) to denote its type. When F is a ground formula, variabilize(F ) denotes the following formula:\n∧ {Vc 6= Vd | c, d ∈ const(F ), τ(c) = τ(d)} → F ′\nwhere const(F ) is the set of constants appearing in F and F ′ is obtained from F by replacing all constants c by a new variable Vc of type τ(c).\nTransformation 4. Given an MLN M and a positive integer k, we construct a possibilistic logic theory ΘkM as follows:\n• For each hard rule F from M, add (F, 1) to ΘkM.\n• For each set of literals E such that 0 ≤ |E| ≤ k, let X = {x | (M, E) ⊢MAP x}. For all x ∈ X , unless there is a literal y ∈ E such that (M, E\\{y}) ⊢MAP y and unless ΘkM already contains a formula isomorphic to variabilize ( ∧ E → x), add\n( variabilize (∧ E → x ) , λE )\nto ΘkM. If pen(M, E) > pen(M, ∅) and Θ k M does not already contain a formula isomorphic to variabilize (¬ ( ∧ E ∧ ∧ X)), add also\n( variabilize ( ¬ (∧ E ∧ ∧ X ))\n, λ′E\n) (14)\nwhere λ′E is the certainty level just below λE in Θ k M.\nAs before, we will usually omit the subscript in ΘkM. We can show that after grounding, Θk is equivalent to the theory that would be obtained by first grounding the MLN and then applying the method from Section 3.2. The correctness proof is provided in the online appendix.\nOur implementation4 of Transformation 4 relies on an efficient implementation of inference in possibilistic logic and Markov logic, efficient generation of non-redundant candidate evidence sets and efficient filtering of isomorphic formulas. For MAP inference in MLNs, we used a cuttingplane inference algorithm based on a SAT-based optimization. For inference in possibilistic logic, we also used cutting-plane inference in order to avoid having to ground the whole theory. To find the ground rules that need to be added by the cutting-plane method, we used a modified querying system from [16]. For solving and optimizing the resulting ground programs, we used the SAT4J library [3].\nNote that to check whether Θkλ ⊢ F , where F is a (not necessarily ground) clause, it is sufficient to find one (typerespecting) grounding θ of F , and check whether Θkλ ∪ {¬(Fθ)} is inconsistent. In this way, we can check whether a rule is implied by Θk without grounding the whole theory because, as for MLNs, inference in non-ground possibilistic logic theories can be carried out by cutting-plane inference methods.\nWe implemented the transformation as a modification of the standard best-first search (BFS) algorithm which constructs incrementally larger candidate evidence sets, checks their MAP consequences and adds the respective rules to the possibilistic logic theory being constructed. Like the standard BFS algorithm it uses a hash-table based data structure closed, in which already processed evidence sets are stored. In order to avoid having to check isomorphism with every evidence set in closed, each time a new evidence set is considered, the stored evidence sets are enriched by fingerprints which contain some invariants, guaranteeing that no two variabilized evidence sets with different fingerprints are isomorphic. In this way, we can efficiently check for a given evidence set E whether there is a previously generated evidence set E′ such that variabilize(E) and variabilize(E′) are isomorphic.\nAs a final remark, we note that for the non-ground transformation, it may be preferable to replace any\n4The implementation can be downloaded from: https://github.com/supertweety/mln2poss.\nrule (variabilize (¬ ( ∧ E ∧ ∧ X)) , λ′E) by the rule\n(variabilize (¬ ∧ E) , λ′E). The reason is that the former rules may often become too long in the non-ground case. On the other hand, for the ground transformation, the advantage of the longer rules is that they will often be the same for different sets E, which, in effect, means a smaller number of rules in the possibilistic logic theory. The correctness of this alternative to Transformation 4 is also shown in the online appendix."
    }, {
      "heading" : "5 ILLUSTRATIVE EXAMPLES",
      "text" : "The first example is a variation on a classical problem from non-monotonic reasoning. Here, we want to express that birds generally fly, but heavy antarctic birds do not fly, unless they have a jet pack. The MLN which we will convert into possibilistic logic contains the following rules: 10 : bird(X) → flies(X), 1 : antarctic(X) → ¬flies(X), 10 : heavy(X) → ¬flies(X), 100 : hasJetPack(X) → flies(X). When presented with this MLN, Transformation 4 produces the following possibilistic logic theory.\n(¬antarctic(X) ∨ ¬flies(X), λ0)\n(¬bird(X) ∨ flies(X), λ0)\n(¬heavy(X) ∨ ¬flies(X), λ0)\n(flies(X) ∨ ¬hasJetPack(X), λ0)\n(¬bird(X) ∨ flies(X) ∨ hasJetPack(X), λ1)\n(¬heavy(X) ∨ antarctic(X) ∨ ¬flies(X), λ1)\n(¬bird(X) ∨ ¬heavy(X), λ1)\n(¬antarctic(X) ∨ ¬heavy(X) ∨ ¬flies(X), λ10)\n(flies(X) ∨ ¬hasJetPack(X) ∨ bird(X), λ11)\n(¬bird(X) ∨ flies(X) ∨ ¬hasJetPack(X), λ100)\nLet us consider the evidence set E = {bird(tweety), heavy(tweety)}. Then the levels λ0 and λ1 drown because of the inconsistency with the rule (¬bird(X) ∨ ¬heavy(X), λ1) which was produced as one of the rules (14). We can see from the rest of the possibilistic logic theory that unless we add either antarctic(tweety) or hasJetPack(tweety), we cannot say anything about whether tweety flies or not. It can be verified that the same is true also for the respective MLN.\nThe second example consists of formulas from a classical MLN about smokers. There are three predicates in this MLN: a binary predicate f(A,B) denoting that A and B are friends, and two unary predicates s(A) and c(A) denoting that A smokes and that A has cancer, respectively. The MLN contains the following hard rules: ¬f(A,B) ∨ f(B,A) and ¬f(A,A). In addition, we have two soft rules. The first soft rule 10: ¬s(A)∨¬f(A,B)∨ s(B) states that if A and B are friends and A smokes then B is more likely to smoke too. The second rule 10: ¬s(A) ∨ c(A) states that smoking increases the likelihood of cancer. The following possi-\nbilistic logic theory was obtained using Transformation 4 with k = 4.\n(s(B) ∨ ¬f(A,B) ∨ ¬s(A) ∨ ¬alldiff(A,B), λ0)\n(¬s(A) ∨ c(A), λ0)\n(¬f(C,B) ∨ ¬f(A,B) ∨ s(A) ∨ s(C)\n∨¬alldiff(A,B,C) ∨ ¬s(B), λ10)\n(¬f(C,B) ∨ ¬s(A) ∨ ¬f(A,C) ∨ s(C)\n∨¬alldiff(A,B,C) ∨ ¬s(B), λ10)\n(¬s(A) ∨ ¬f(C,A) ∨ s(C) ∨ c(B)\n∨¬alldiff(A,B,C) ∨ ¬s(B), λ10)\n(¬s(A) ∨ c(A) ∨ c(B) ∨ ¬s(B) ∨ ¬alldiff(A,B), λ10)\n(s(B) ∨ ¬f(A,B) ∨ ¬s(A) ∨ c(A) ∨ ¬alldiff(A,B), λ10)\n(¬f(A,B) ∨ f(B,A), 1)\n(¬f(A,A), 1)\nAt the lowest level λ0 we find the counterparts of the soft rules from the MLN, whereas at level 1 we find the hard rules. At the intermediate level we intuitively find weakened rules from the MLN. For instance, the rule (¬s(A) ∨ c(A)∨c(B)∨¬s(B)∨¬alldiff(A,B), λ1) can be interpreted as: if A and B smoke then at least one of them has cancer. It is quite natural that this rule has higher certainty weight than the rule: if A smokes then A has cancer.\nA final, more elaborate example is provided in the online appendix."
    }, {
      "heading" : "6 RELATED WORK",
      "text" : "One line of related work focuses on extracting a comprehensible model from another learned model that is difficult or impossible to interpret. A seminal work in this area is the TREPAN [5] algorithm. Given a trained neural network and a data set, TREPAN learns a decision tree to mimic the predictions of the neural network. In addition to producing interpretable output, this algorithm was shown to learn accurate models that faithfully mimicked the neural network’s predictions. More recent research has focused on approximating complex ensemble classifiers with a single model. For example, Popovic et al. [21] proposed a method for learning a single decision tree that mimics the predictions of a random forest.\nWhile, to the best of our knowledge, this is the first paper that studies the relation between Markov logic and possibilistic logic, the links between possibility theory and probability theory have been widely studied. For example, [10] has proposed a probability-possibility transformation based on the view that a possibility measure corresponds to a particular family of probability measures. Dempster-Shafer evidence theory [25] has also been used to provide a probabilistic interpretation to possibility degrees. In particular, a possibility distribution can be interpreted as the contour\nfunction of a mass assignment; see [11] for details. In [13] it is shown how the probability distribution induced by a penalty logic theory corresponds to the contour function of a mass assignment, which suggests that it is indeed natural to interpret this probability distribution as a possibility distribution. Several other links between possibility theory and probability theory have been discussed in [6].\nIn this paper, we have mainly focused on MAP inference. An interesting question is whether it would be possible to construct a (possibilistic) logic base that captures the set of accepted beliefs encoded by a probability distribution, where A is accepted if P (A) > P (¬A). Unfortunately, the results in [7] show that this is only possible for the limited class of so-called big-stepped probability distributions. In practice, this means that we would have to define a partition of the set of possible worlds, such that the probability distribution over the partition classes is big-stepped, and only capture the beliefs that are encoded by the latter, less informative, probability distribution. A similar approach was taken in [1] to learn default rules from data."
    }, {
      "heading" : "7 CONCLUSIONS",
      "text" : "This paper has focused on how a Markov logic network M can be encoded in possibilistic logic. We started from the observation that it is always possible to construct a possibilistic logic theory ΘM that is equivalent to M, in the sense that the probability distribution induced by M is isomorphic to the possibility distribution induced by ΘM. As a result, applying possibilistic logic inference to ΘM yields the same conclusions as applying MAP inference to M. Although the size of ΘM is exponential in the number of formulas in M, we have shown how more compact theories can be obtained in cases where we can put restrictions on the types of evidence that need to be considered (e.g. small sets of literals).\nOur main motivation has been to use possibilistic logic as a way to make explicit the assumptions encoded in a given MLN. Among others, the possibilistic logic theory could be used to generate explanations for predictions made by the MLN, to gain insight into the data from which the MLN was learned, or to identify errors in the structure or weights of the MLN. Taking this last idea one step further, our aim for future work is to study methods for repairing a given MLN, based on the mistakes that have thus been identified."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We would like to thank the anonymous reviewers for their helpful comments. This work has been supported by a grant from the Leverhulme Trust (RPG-2014-164). JD is partially supported by the Research Fund KU Leuven (OT/11/051), EU FP7 Marie Curie Career Integration Grant (#294068) and FWO-Vlaanderen(G.0356.12)."
    }, {
      "heading" : "A PROOFS",
      "text" : "Proof of Proposition 1. Let X = {(F,w) ∈ M : ω 6|= F} be the set of rules from the MLN not satisfied in ω. By construction, the possibilistic logic theory Θ contains the rule ( ∨ X∗, λ) where λ = φ(¬ ∨ X∗) = K+pen(M,¬ ∨ X∗)\nL =\nK+pen(M,ω) L\n. As a result, ω 6|= Θλ. On the other hand, ω |= Θλ′ for any λ′ > λ. Indeed, for ( ∨ Y ∗, λ′) ∈ Θ and λ′ > λ, we have by construction that Y ∗ 6⊆ X∗, i.e. Y ∗ contains a formula α which is not in X∗. By definition of X , this means ω |= α, and thus in particular ω |= ∨ Y ∗. Since π was assumed to be the least specific model of Θ, it holds that π(ω) = 1−max(αi,λi){λi|ω 6|= αi}. Hence we can conclude π(ω) = 1− φ(ω).\nProof of Lemma 2. The lemma can be proved by induction on |E|. The base case |E| = 0 is obvious. Let us assume that the lemma holds for |E| = n < k. If E does not contain any literal y such that (M, E \\ {y}) ⊢MAP y then Θkλ contains a formula ∧ E → ∧ X with x ∈ X and the lemma clearly holds. Otherwise, if there is a literal y ∈ E such that (M, E \\ {y}) ⊢MAP y then we must also have (M, E \\ {y}) ⊢MAP x. Moreover φ(E) = φ(E \\ {y}) = λ in this case. By induction we find that the formula\n∧ (E \\ {y}) → x can be derived from{\n( ∧ E′ → ∧ X) ∈ Θkλ s.t. |E ′| ≤ |E| } .\nProof of Lemma 1. We have that {F1, ..., Fl} ∩ Y 6= ∅ for every Y ∈ ConsE(M) iff ∨ { ∧ Y |Y ∈ ConsE(M)} ∧ ¬F1 ∧ ... ∧ ¬Fl is inconsistent. This in turn means that the most plausible world of E∧¬F1∧...∧¬Fl cannot be among the most plausible worlds ofE, and thus pen(M, E∧¬F1∧ ... ∧ ¬Fl) > pen(M, E).\nProof of Lemma 4. (i) Let Ω be the set of most probable worlds of (M, E), let Ω′i be the set of most probable worlds of (M, E ∪ {¬yi}) and let Ω′′i be the set of most probable worlds of (M, E) in which yi is false. In general Ω′i 6= Ω ′′ i . Either (M, E) ⊢MAP (y1 ∧ · · · ∧ yk) or at least one of Ω′′i must be nonempty. Let Ω′′i∗ be such a nonempty set. It must hold (M, E∪{¬yi∗}) ⊢MAP (y1∨· · ·∨yi∗−1∨yi∗+1∨· · ·∨ yk) because, in this case, Ω′i∗ = Ω ′′ i∗ ⊆ Ω. (ii) The second part of the lemma follows from repeated application of the first part.\nProof of Lemma 5. (⇒) It follows from Lemma 3 that any most probable model of E is a model of Θkλ ∪ E, hence if Θkλ ∪E ⊢ ∨ C then (M, E) ⊢MAP ∨ C.\n(⇐) If (M, E) ⊢MAP (c1 ∨ · · · ∨ cm), then by Lemma 4 we have (M, E ∪ C′) ⊢MAP cj for some j ∈ {1, ...,m} and C′ ⊆ {¬c1, . . . ,¬cj−1,¬cj+1, . . . ,¬cm}. Since |E| + |C′| ≤ k, by Lemma 2 we haveΘkλ ⊢ ¬ ( ∧ E)∨¬ ( ∧ C′)∨\ncj and therefore Θkλ ∪ E ⊢ ∨ C.\nCorrectness of Transformation 4\nIn the following lemmas, M will be an MLN, k will be an integer, ΥkM will be the ground possibilistic logic encoding given by Transformation 3 in which, for convenience, we replace any rule ( ∧ E → ∧ X,λE) by a set of rules\n( ∧ E → x, λE) where x ∈ X . ΘkM will be the non-ground possibilistic logic encoding given by Transformation 4. Recall that ΘkM is given together with a set of constants, typed according to their interchangeability. The ground possibilistic logic theory obtained by grounding ΘkM using this set of typed constants will be denoted by Θ̂kM. Lemma 6. Let α = ( ∧ E → x, λE) be a rule. Then α ∈ ΥkM if and only if α ∈ Θ̂ k M.\nProof. (⇒) If α ∈ ΥkM then a rule isomorphic to (variabilize ( ∧ E → x) , λE) must be contained in ΘkM but then α must be among the groundings of that rule and therefore also be in Θ̂kM.\n(⇐) If α ∈ Θ̂kM then Θ k M must contain a\nrule (variabilize ( ∧ E′ → x′) , λE) isomorphic to\n(variabilize ( ∧ E → x) , λE) such that (M, E′) ⊢MAP x′ and such that there is no literal y′ ∈ E′ satisfying (M, E′ \\ {y′}) ⊢MAP y′. It follows from the fact that constants of the same type are interchangeable that then also (M, E) ⊢MAP x and that there is no literal y ∈ E satisfying (M, E \\ {y}) ⊢MAP y. From this it immediately follows that α ∈ ΥkM. Lemma 7. Let α = (¬ ( ∧ E ∧ ∧ X) , λ′E) be a rule. Then α ∈ ΥkM if and only if α ∈ Θ̂ k M.\nProof. The proof of this lemma is very similar to the proof of Lemma 6.\n(⇒) If α ∈ ΥkM then a rule isomorphic to (variabilize ( ∧ E ∧ ∧ X) , λ′E) must be contained in Θ k M but then α must be among the groundings of that rule and therefore also be in Θ̂kM.\n(⇐) If α ∈ Θ̂kM then Θ k M must contain a rule\n(variabilize ( ∧ E′ ∧ ∧ X ′) , λ′E) isomorphic to\n(variabilize ( ∧ E ∧ ∧ x) , λ′E) such that (M, E ′) ⊢MAP x ′ for all x′ ∈ X ′. It follows from the fact that constants of the same type are interchangeable that then also (M, E) ⊢MAP x for all x ∈ X , from which it immediately follows that α ∈ ΥkM.\nProposition 5. Given an MLN M and an integer k, the possibilistic logic theories obtained by Transformation 3 and by grounding the possibilistic logic theory obtained by Transformation 4 are equivalent.\nProof. The proof follows from Lemma 6 and Lemma 7.\nCorrectness of the alternative to Transformation 4\nWe show the correctness of the alternative transformation, in which we replace rules of the form\n( variabilize ( ¬ (∧ E ∧ ∧ X ))\n, λ′E\n)\nby the rule (\nvariabilize ( ¬ ∧ E ) , λ′E )\nThe correctness of this alternative transformation can be shown as follows. Let Θk be the possibilistic logic theory obtained by Transformation 4 and let Φk be the possibilistic logic theory obtained by the alternative transformation. It holds that ¬ ∧ E |= ¬( ∧ E ∧ ∧ X) and consequently also variabilize(¬ ∧ E) |= variabilize(¬( ∧ E∧ ∧\nX)). Therefore if Θkλ ∪ F is inconsistent then Φ k λ ∪ F must be inconsistent as well. Moreover, since λ′E < φ(E), we can show (using arguments analogical to those used in the proof of Proposition 3) that this alternative transformation also satisfies the properties stated for Transformation 4 in Proposition 5."
    }, {
      "heading" : "B ADDITIONAL EXAMPLE",
      "text" : "In this section we illustrate Transformation 4 on a larger MLN trained for predicting categories of computer science papers5, which is shown in Table 1. This MLN contains rules for predicting categories of papers from categories of other papers which refer to them (rules 2-3) or from categories of papers written by the same author (rule 3). In addition it contains rules giving prior probabilities of the individual probabilities and a hard rule specifying that every paper has at most one category (for simplicity).\nWe applied Transformation 4 on this MLN which resulted in a possibilistic logic theory with 200 rules6. Table 2 displays a subset of the rules in the resulting theory, which are interesting for illustrating some properties of the MLN and which are not immediately obvious from the MLN itself. Notice that we represent, apart from the formulas of the form (14), we present formuals in the implication form to make the interpretation easier.\nRule α is one of the rules enforcing drowning. Rule β states that in absence of other evidence, the category of any paper\n5We obtained the structure of the MLN from the Tuffy web http://i.stanford.edu/hazy/tuffy/download/.\n6There are two modes for filtering redundant rules in the possibilistic logic theory in our implementation. The more aggressive filtering iteratively removes rules which are entailed by other rules in the theory (with the same or greater certainty weights) whereas the less aggressive filtering only iteratively removes the rules which are entailed by subset of the rules from the theory which are all shorter or equally long. The latter filtering results in slightly more interpretable results. In the experiments reported here, we have used the less aggressive filtering.\nis assumed to be AI. Rule γ cannot intuitively be justified and is probably an unintended consequence of the MLN. It states that if V 1 wrote a paper which is not from the category AI, then V 1 did not write any other paper. This rule is not harmful when the MLN is used for the purpose of predicting categories but it indicates that the MLN would not be suitable for predicting authorship (which is not really surprising given that the MLN was not trained for this task). The rules δ and η are representatives of rules which capture the prior distribution of the categories (note that there are more rules of this kind which we do not show here). Rule ζ is similar to γ. The rules ι, κ, λ, µ, ν and ξ state that if one paper refers to another paper then they typically have the same category. Such rules actually give evidence of meaningfulness of the MLN for prediction of categories."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "Markov logic uses weighted formulas to com-<lb>pactly encode a probability distribution over pos-<lb>sible worlds. Despite the use of logical formu-<lb>las, Markov logic networks (MLNs) can be diffi-<lb>cult to interpret, due to the often counter-intuitive<lb>meaning of their weights. To address this issue,<lb>we propose a method to construct a possibilis-<lb>tic logic theory that exactly captures what can<lb>be derived from a given MLN using maximum<lb>a posteriori (MAP) inference. Unfortunately, the<lb>size of this theory is exponential in general. We<lb>therefore also propose two methods which can<lb>derive compact theories that still capture MAP<lb>inference, but only for specific types of evidence.<lb>These theories can be used, among others, to<lb>make explicit the hidden assumptions underlying<lb>an MLN or to explain the predictions it makes.",
    "creator" : "LaTeX with hyperref package"
  }
}