{
  "name" : "1705.08499.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "The Prediction Advantage: A Universally Meaningful Performance Measure for Classification and Regression",
    "authors" : [ "Ran El-Yaniv", "Yonatan Geifman" ],
    "emails" : [ "rani@cs.technion.ac.il", "yonatang@cs.technion.ac.il", "yair@jether-energy.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Consider the task of training a classifier on a binary problem with a very small minority class whose proportion is 1%. A machine learning intern generates a classifier whose test accuracy was 97%. Proud of this excellent result, the intern reports to his boss and is promptly fired. Many of us have encountered this embarrassing situation where, in retrospect, the classifier we have worked hard to train, turns out to be no better than the trivial classifier that labels everything as the majority class (achieving 99% accuracy in our example).\nWhile this example was intentionally contrived, it occurs widely, even with less extreme imbalance. For example, the proportion of the minority class in the well-known UCI Haberman dataset [16] is 26.47% and yet it fails even experienced ML researchers [23, 14, 24, 5, 12, 18, 15, 4, 1]. In all these publications, the authors have reported the (positive) results of a classifier for the Haberman set whose\nar X\niv :1\n70 5.\n08 49\n9v 2\nerror is near or worse than the trivial 26.47% error (see more details in Appendix A). Prediction under other loss functions (not the 0/1 loss) is also susceptible to a qualitatively similar problem.\nKnown partial remedies for handling imbalanced datasets include theF -measure, true positive/negative rates, and Cohen’s kappa. These measures can sometimes detect trivial classifiers, but not always. They are also not general in the sense that they are not defined for any desired loss function. For example, Cohen’s kappa is defined only for the 0/1 loss (i.e., classification). Moreover, each of these known measures can still fail a machine learning practitioner in some problems. In the context of squared loss regression, the well-known R2 measure provides an effective solution (and, as will be shown below, R2 is a special case of our measure).\nIn this paper we propose a single performance measure, potentially applicable to any loss function. Consider a learning problem defined in terms of a feature space X , a label space Y , a distribution P (X,Y ), where X ∈ X , Y ∈ Y , and a loss function ` : Y × Y → R+. To measure the performance of a prediction function f , the proposed measure considers the advantage of the risk R`(f) =\n∆ EX,Y {`(f(X), Y )} over the risk of the best prediction given knowledge of the marginal P (Y ), which we term the Bayesian marginal prediction (BMP) risk. The new measure is termed the prediction advantage (PA). We derive the PA for the most popular loss functions, namely, 0/1 loss, cross-entropy loss, squared loss, absolute loss, and general cost-sensitive loss. We then argue that the PA measure always prevents triviality whereas each of the other known alternative measures can fail.\nA striking benefit of the PA is that it also enables performance comparisons across learning problems, e.g., in classification problems that differ in the number of classes (and imbalance rate). To understand this effect, consider the following story. The intern from our first vignette becomes a university professor and composes a multiple choice exam with 100 questions for his machine learning class. To prevent cheating, he writes two versions of the exam, both containing precisely the same questions but version A has 3 optional answers for each question and version B, 4 optional answers. Bob and Alice write the exam and receive versions A and B, respectively. Their exam sheets are graded and both get a grade of 30. It is easy to see that Bob’s grade is worse than the trivial 33.33% mark that can be achieved for a guess (for version A), while Alice’s is better than the corresponding trivial 25% mark for version B. Using PA, however, not only does the PA detect Bob’s trivial achievement, it also allows us to quantify his performance compared Alice’s. For example, it enables us to determine how much better Alice’s performance is than Bob’s if they both got a mark of 60% (see analysis in Section 5)."
    }, {
      "heading" : "2 Prediction Advantage",
      "text" : "In this section we introduce the Bayesian marginal prediction (BMP), defined to be the optimal prediction in hindsight given the marginal distribution of the labels, P (Y ). For any given loss function, the risk of the BMP is taken as a reference point for meaningless prediction, which we would like outperform. To this end, we define the prediction advantage (PA) as the (additive) reciprocal of the performance ratio between our prediction function and the BMP. We then instantiate the PA to several important loss functions."
    }, {
      "heading" : "2.1 Bayesian Marginal Prediction",
      "text" : "We define the Bayesian Marginal Prediction function (BMP) to be the optimal prediction function with respect to the marginal distribution of Y , P (Y ), and denote it as f0.1 The BMP predicts a constant\n1In the case that the BMP is not unique, we will choose it arbitrarily among the optimal, since we are interested in its risk.\nvalue/class while being oblivious to X and P (Y |X). In this way we expect the BMP to obtain only the complexity of the problem latent in P (Y ). We denote by B the set of all probability distributions over Y . Clearly, any BMP must reside in B.\nFor any prediction function g ∈ B, and any loss function `, we have R`(g) = EY `(Y, g). This can be easily established by noting that g is independent of X:\nEX,Y `(Y, g) = ∫ X,Y `(Y, g)dP (X,Y ) = ∫ Y `(Y, g) ∫ X dP (X,Y ) = ∫ Y `(Y, g)dP (Y ) = EY `(Y, g).\nWe will use this simple relation throughout the paper. By Yao’s principle [25, 2] (which follows from von Neumann’s minimax theorem), we can restrict attention only to deterministic BMPs (i.e., constants). For self-containment, we provide a direct proof of this statement for the case where the loss function is convex in its second argument.\nLemma 2.1 Let `(r, s) be a loss function convex with respect to s. There exists a constant prediction function f0 ∈ B such that\nR`(f0) ≤ min g∈B R`(g).\nProof: Let g be any function in B defined with respect to the distribution Q over Y . Clearly, R`(g) = EY EQ{`(Y, g(X))}. Using the Jensen inequality we obtain the desired bound as follows:\nR`(g) = EY EQ`(Y, g)\n≥ EY `(Y,EQ{g}) = R`(EQ{g}).\nThus, we have shown that for any g = gQ ∈ B, the risk of the constant prediction EQ{g} is no worse than R`(g). This also holds for the best prediction function in B. 2\nFor optimality, the BMP will be defined explicitly according to a specific loss function. We will define it and prove its optimality for several loss functions."
    }, {
      "heading" : "2.2 The Prediction Advantage",
      "text" : "After defining the BMP, we will use its risk as the baseline for our measure. The Prediction Advantage (PA) of a prediction function f is defined to be the advantage of the expected performance of f over the BMP:\nPA`(f) = ∆ 1− R`(f) R`(f0) = 1− EX,Y (`(f(X), Y )) EX,Y (`(f0(X), Y ) .\nThe following are basic properties of the PA measure:\n1. Order preservation: The PA forms a weak ordering of the functions f ∈ F (given any function class F for our problem). Since the prior probabilities over the classes of a problem are constant, the PA preserves an order that is inverse to the order formed by the risk.\nPA`(f1) > PA`(f2) iff R`(f1) < R`(f2).\n2. Boundedness: supf∈F (PA`(f)) = 1; the PA is bounded by 1. Notice that PA`(f) = 1 is obtained only when the R`(f) = 0.\n3. Meaningfulness: PA`(f) = 0 when f has no advantage over the BMP. PA`(f) < 0 when f is worse than the BMP. A function with a negative PA is meaningless in the sense that one can use the (empirical) BMP, potentially reaching better results.\nWe emphasize that throughout the paper we consider the “true prediction advantage”, which corresponds to the underlying unknown distribution. The empirical version of the PA is straightforwardly defined. To calculate the empirical PA, all we need to do is to estimate the BMP (using appropriate estimators) based on a labeled sample. Confidence levels for these estimates can be obtained using standard concentrations of measure bounds. For simplicity, we deliberately ignore in this paper the empirical PA, and these estimation problems."
    }, {
      "heading" : "2.3 Prediction Advantage for the Cross-Entropy Loss",
      "text" : "We now look at a multi-class classification problem using the cross-entropy loss defined as, `(f(X), Y ) = − ∑\ni∈C Pr{Y = i} log (Pr{f(X) = i}). This loss function is extensively used in deep learning and other multi-class learning frameworks. Having defined the PA in Section 2.2, in this section we identify the BMP and present an explicit expression for the PA in this setting. Let C =∆ {1, 2, . . . , k} be the set of the classes and consider a multi-class classifier, f(x) : X → Rk, whose ith coordinate, f(x)i, attempts to predict the probability that an instance x belongs to class i ∈ C; that is, f(x)i =∆ Pr{Y = i|x}. For simplicity, we assume from now on that the label Y is represented in unary; for example, the label Y = i is represented by the unit vector ei (also known as “one-hot” encoding). In this setting the cross-entropy loss is defined as `(f(x), y) =∆ − ∑ i∈C yi · log(f(x)i)).\nLemma 2.2 (BMP for cross-entropy loss) Let f0 = f0(X) be the vector whose ith coordinate is f0(X)i =∆ Pr{Y = ei}. Then f0 is the BMP for the multi-class classification under the cross-entropy loss. Moreover, the BMP risk is the Shannon entropy of P (Y ), the marginal distribution of Y.\nProof: Consider an arbitrary marginal classifier fQ(X) defined with respect to the probability mass function Q whose support set is the set of classes C; that is, fQ(X) ∼ Q. We prove that R`(f0) ≤ R`(fQ). Applying the cross-entropy loss on f0, we have\nR`(f0) = E`(f0(X), Y ) = ∑ i∈C Pr{Y = ei}`(f0(X), ei)\n= ∑ i∈C −Pr{Y = ei} log (Pr{Y = ei})\n= H(Y ).\nThe risk of fQ is\nR`(fQ) = E`(fQ(X), Y ) = ∑ i∈C Pr{Y = ei}`(fQ(X), ei)\n= ∑ i∈C −Pr{Y = ei} log (fQi(X)).\nUsing the non-negativity of the Kullback-Leibler divergence, we show that the difference R`(fQ) − R`(f0) is not negative (meaning that f0 is optimal):\nR`(fQ)−R`(f0) = ∑ i∈C −Pr{Y = ei} log (fQi) + ∑ i∈C Pr{Y = ei} log (Pr{Y = ei})\n= ∑ i∈C Pr{Y = ei} log (Pr{Y = ei}/fQi(X)) = Dkl(f0(X)||fQ(X)) ≥ 0.\n2\nBy Lemma 2.2, the risk of the BMP in the cross-entropy loss setting is R`(f0) = H(P (Y )), and therefore, the Prediction Advantage for the cross-entropy loss is\nPA`(f) = ∆ 1− R`(f)\nH(P (Y )) ."
    }, {
      "heading" : "2.4 Prediction Advantage for 0-1 Loss",
      "text" : "Consider a 0-1 multi-class classification problem in which the set of classes is C =∆ {1, 2, . . . , k}. In this case (and in contrast to the cross-entropy case discussed above), the classifier f predicts a nominal value, f(x) : X → C. The 0-1 loss is defined as `(ŷ, y) = I(y 6= ŷ), where I is the indicator function. Clearly, when the 0-1 loss function is used, the resulting risk equals the probability of misclassification. To derive the PA for this setting, we now show that the BMP strategy is f0 =\n∆ argmax i (Pr{Y = i}); that is, the\nBMP outputs the most probable class in P (Y ) for every X .\nLemma 2.3 (BMP for 0-1 loss) For (multi-class) 0-1 loss classification, the Bayesian marginal prediction is f0 =\n∆ argmax i ((Pr{Y = i})).\nProof: Consider an arbitrary marginal classifier fQ(X) defined with respect to the probability mass function Q whose support set is the set of classes C; that is, fQ(X) ∼ Q. We prove that R`(f0) ≤ R`(fQ). We denote the most probable class in P(Y) as j. Applying the 0-1 loss on f0, we have\nR`0−1(f0) = 1−max i∈C (Pr{Y = i}) = 1−Pr{Y = j}.\nThe risk of fQ is\nR`0−1(fQ) = EY `(Y, YQ) = 1− ∑ i∈C Pr{Y = i}PrQ{YQ = i}.\nTherefore, R`0−1(fQ) ≥ 1−Pr{Y = j} ∑ i∈C PrQ{YQ = i} = 1−Pr{Y = j} = R`0−1(f0).\n2\nWe conclude that the Prediction Advantage for multi-class classification under the 0-1 loss function (which includes binary classification) is\nPA`(f) = ∆ 1− R`(f) R`(f0) = 1− R`(f) 1−maxi∈C(Pr{Y = i}) ."
    }, {
      "heading" : "2.5 Prediction Advantage for Squared Loss in Regression",
      "text" : "We now discuss a (multiple) regression setting under the squared loss. For simplicity, we consider the univariate outcome model where Y = R, but the results can be extended to multivariate outcome models Y = Rm in a straightforward manner. Thus, the predicted variable is a real number, Y ∈ R, and the squared loss function is defined as `(r, s) = (r−s)2. We now show that the BMP strategy for this setting is f0 = ∆ E(Y ).\nLemma 2.4 (BMP for squared loss in regression) f0 =∆ E[Y ] is the BMP for regression under the squared loss.\nThe proof for this lemma is the known result of minimum mean squared error in the field of signal processing, for further reading see [20] (chapter 8).\nHaving identified the BMP and observing that\nR`(f0) = EY [(Y − f0)2] = EY [(Y − E[Y ])2] = var(Y ),\nwe obtain the following expression for the PA:\nPA`(f) = ∆ 1− R`(f) R`(f0) = 1− R`(f) var(Y ) .\nApparently, the PA in regression is precisely the well-known R-squared measure in regression analysis (also known as the coefficient of determination) [13]."
    }, {
      "heading" : "2.6 Prediction Advantage for Absolute Loss in Regression",
      "text" : "Consider a univariate (multiple) regression setting under the absolute loss. The predicted variable is a real number, Y ∈ R, and the absolute loss function is defined as `(r, s) = |r − s|. We now show that the BMP strategy for this setting is f0 = ∆ median(Y ).\nLemma 2.5 (BMP for absolute loss in regression) f0 =∆ median(Y ) is the BMP for regression under the squared loss.\nProof: According to Lemma 2.1 the BMP in regression is a constant function, if we consider an arbitrary constant function fa = a ∈ R, the risk of fa is\nR`(fa) = EY |Y − a|\n= ∫ Y |Y − a|P (Y )dP (Y )\n= ∫ a −∞ −(Y − a)P (Y )dP (Y ) + ∫ ∞ a (Y − a)P (Y )dP (Y ).\nTaking the derivative with respect to a to find the risk minimizer, we have\n∂R`(fa)\n∂a = ∫ a −∞ P (Y )dP (Y )− ∫ ∞ a P (Y )dP (Y ) = 0.\nThus, the BMP is f0 = median(Y ). 2\nHaving identified the BMP, we find that the risk is the mean absolute deviation (MAD) around the median,\nR`(f0) = EY [|Y −median(Y )|] = Dmed.\nWe obtain the following expression for the PA:\nPA`(f) = ∆ 1− R`(f) R`(f0) = 1− R`(f) Dmed ."
    }, {
      "heading" : "2.7 Prediction Advantage for Cost-Sensitive Loss",
      "text" : "Consider a multi-class classification problem where the classes are C =∆ {1, 2, . . . , k}), and the loss function is defined with a specific cost for each misclassification type (see Elkan, (2001), for further details). For 1 ≤ i, j ≤ k, bi,j is the cost for predicting label i while the true label is j. Clearly, the BMP in this setting is\nf0 = ∆ argmin\ni∈C ∑ j∈C bi,j ·Pr(Y = j)  . We skip the proof of optimality of this proposed BMP, which is similar to the proof of Lemma 2.3. The risk of the BMP is\nR`c(f0) = min i∈C ∑ j∈C bi,j ·Pr(Y = j)  , and therefore, the prediction advantage for the cost-sensitive loss is\nPA`c(f) = ∆ 1−\nR`c(f) R`c(f0) = 1−\nR`c(f)\nmini∈C (∑ j∈C bi,j ·Pr(Y = j) ) ."
    }, {
      "heading" : "3 Related Measures",
      "text" : "The false positive and false negative rates (called also type 1 and type 2 errors, respectively) are typically used in statistics in the context of hypothesis testing. One can meaningfully compare two classifiers by considering (separately) their false positive and false negative rates. Moreover, Brodersen et al. [3] proposed to compare the mean of the true positive and true negative rates and defined it as the balanced accuracy.\nEmerging from information retrieval, precision and recall are two popular measures that can be used to meaningfully measure performance in imbalanced problems [21]. In the context of a binary classification problem with a target minority class, precision is the fraction of instances classified to be in the target that truly are in the target. Recall is the fraction of true target instances that are classified correctly. As\nstated in Manning et al. [17], the use of these measures prevents meaningless assessments in imbalance problems. Precision and recall are scaled similarly and thus can be combined to a single quantity. It is common in text categorization to take their harmonic mean as a single performance measure to be optimized. This quantity is called the F -measure and it approximates the so-called break-even point, defined to be the equilibrium of the trade-off between precision and recall,\nAnother performance measure, which emerged from experimental psychology, is Cohen’s kappa [7]. Originally, Cohen’s kappa was introduced as a statistical measure to quantify the agreement between two raters, each of whom classifies N items into C mutually exclusive categories, while taking into account the probability that the raters agree by chance alone. Formally, it was defined as κ =∆ 1 − 1−p01−pe , where p0 is the fraction of items the raters agreed upon, and pe is the probability of chance agreement on a new item given that each rater only knows her own class distribution. Thus, κ is a normalized measure, where κ = 1 represents complete agreement, and κ = 0, complete disagreement. Cohen’s kappa has been advocated by Fatourechi et al. [10] to tackle imbalanced classification problems. To apply it in classification, we take one of the raters to be nature, which assigns true labels, and the other rater is the classifier. Then, p0 is the 0/1 accuracy of the classifier (over a sample of N instances) and pe is the (label) marginal of the classifier multiplied by the (label) marginal of nature. While the use of κ to quantify agreement between raters is well motivated, its use in classification, where the classifier and nature do not play symmetric roles, is problematic. Given a labeled training sample, the learner has some knowledge of nature’s marginal, which can be utilized to obtain higher chances to hit the true label. In contrast, in the case of two symmetric raters, no one knows anything about the other’s marginal. The prediction advantage measure make up this deficiency, and unlike Cohen’s kappa, it also applies to any loss function.\nR-squared is probably the most popular measure of fit in statistical modeling and particularly in the context of regression analysis [13]. There have been numerous attempts to extend the R-squared measure to logistic regression. For example, Efron [8] extended the R-squared measure by calculating the squared loss over the predicted probabilities divided by the variance measured in the probability space and McFadden [19] extended the R-squared measure by replacing the loss with the log-likelihood of the model, and the variance with the log-likelihood of the intercept. In fact, the large number of proposed performance measures in the context of logistic regression is somewhat confusing and indicates a lack of wide consensus on how to extend the R-squared measure to this setting. We emphasize that all these proposals only relate to (logistic) regression and generally are not defined for other loss functions."
    }, {
      "heading" : "4 Analysis",
      "text" : "When the PA of a prediction function is not positive, the BMP outperforms our function. In this case our function is possibly no better than trivial. Thus, using the PA allows us to detect such trivial cases. Most of the alternative methods mentioned in Section 3 are defined in terms of the 0-1 loss function in a binary classification setting. For this setting we can show that the PA lower bounds all other measures. Consequently, when the PA is zero, all the alternative measures will be positive and wrongly qualify meaningless functions.\nThe formal introduction of all the measures we discuss above can be made simple using the Venn diagram in Figure 2. Let f be a classifier whose performance we would like to quantify. The areas in this diagram are defined as follows.\n(a) (b) (c)\nRecalling that the balanced accuracy (see Section 3) is defined as the arithmetic mean of TP and TN , we can state the following bounds.\nLemma 4.1 For any classifier f ,\nPA`(f(X), Y ) ≤ TP (f(X), Y ) (1) PA`(f(X), Y ) ≤ TN(f(X), Y ) (2)\nPA`(f(X), Y ) ≤ TN(f(X), Y ) + TP (f(X), Y )\n2 =∆ BA(f(X), Y ), (3)\nwith strict inequality in (3) when TP (f(X), Y ) < 1 or TN(f(X), Y ) < 1 (i.e., the 0/1 error of f is not zero).\nProof: Clearly,\nPA`(f(X), Y ) = b− a b+ c ≤ b b+ c = TP (f(X), Y ),\nwhich proves (1), and clearly, when a > 0, (1) holds with strict inequality. To prove (2), we first note that since the label ’1’ represents the minority class, b + c ≤ a + d. Therefore,\nPA`(f(X), Y ) = b− a b+ c = 1− a+ c b+ c ≤ 1− a b+ c ≤ 1− a a+ d = d a+ d = TN(f(X), Y ),\nand whenever c > 0, (2) holds with strict inequality. Obviously, (1) and (2) together imply (3), and whenever (1) or (2) hold with strict inequality, (3) also holds with strict inequality. 2\nTurning now to precision and recall and the related F-measure (and break-even point), the precision PRE(f(X), Y ) and recall REC(f(X), Y ) can be expressed as\nPRE(f(X), Y ) =∆ b\na+ b REC(f(X), Y ) =∆\nb\nb+ c .\nThe F-measure is\nF (f(X), Y ) =∆ 2\n1/PRE(f(X), Y ) + 1/REC(f(X), Y ) =\n2PRE(f(X), Y )REC(f(X), Y )\nPRE(f(X), Y ) +REC(f(X), Y ) .\nClearly, a trivial classifier f that classifies everything as ‘0’ will have a zero F-measure. The following lemma shows the strict domination of the PA relative to precision and recall and the derived F-measure and BEP.\nLemma 4.2 For any classifier f ,\nPA`(f(X), Y ) ≤ PRE(f(X), Y ) (4) PA`(f(X), Y ) ≤ REC(f(X), Y ) (5) PA`(f(X), Y ) ≤ F (f(X), Y ), (6)\nand when PRE(f(X), Y ) < 1, (6) holds with strict inequality.\nProof: Note that it is always true that −a2 ≤ 0 ≤ bc. Adding b2 to both sides, we get\n(b+ a)(b− a) = b2 − a2 ≤ bc+ b2 = b(b+ c).\nDividing both sides by (b+ c)(a+ b), we get\nPA`(f(X), Y ) = b− a b+ c ≤ b a+ b = PRE(f(X), Y ).\nProving that PA`(f(X), Y ) ≤ REC(f(X), Y ) is immediate:\nPA`(f(X), Y ) = b− a b+ c ≤ b b+ c = REC(f(X), Y ).\nWhenever a > 0, (4) and (5) hold with strict inequality. Obviously, if both (4) and (5) hold, (6) also holds, and whenever PRE(f(X), Y ) < 1, both (4) and (5) hold with strict inequality, in which case (6) also holds with strict inequality. 2\nTurning now to the Cohen’s kappa (see Section 3), we first express it in terms of the areas. Let p0 = b+ d and pe = (a+ b)(b+ c) + (a+ d)(c+ d). We get that the kappa is,\nκ(f(X), Y ) =∆ 1− 1− p0 1− pe = 1− a+ c 1− (a+ b)(b+ c)− (a+ d)(c+ d) .\nThe following lemma shows that the PA lower bounds Cohen’s kappa.\nLemma 4.3 For any classifier f ,\nPA`(f(X), Y ) ≤ κ(f(X), Y ),\nwith strict inequality whenever the problem is imbalanced.\nProof: Recalling that b+ c is the mass of the minority class, and thus b+ c ≤ a+ d,\npe = (a+ b)(b+ c) + (a+ d)(c+ d) ≤ (a+ b)(a+ d) + (a+ d)(c+ d) = a+ d,\nand when the problem is imbalanced, namely b+ c < a+ d, pe < a+ d. Therefore,\nκ(f(X), Y ) = 1− a+ c 1− (a+ b)(b+ c)− (a+ d)(c+ d)\n≥ 1− a+ c 1− (a+ d)\n= 1− a+ c b+ c = PA`(f(X), Y ),\nand clearly, strict inequality will be obtained if the problem is imbalanced. 2"
    }, {
      "heading" : "5 Numerical Examples",
      "text" : "The main benefit in using the PA is its ability to detect meaningless performance of prediction functions relative to the Bayesian marginal prediction. In this section we first show a number of numerical examples that highlight the advantage of using the PA in certain cases of imbalance and noise.\nConsider the following situation: You have purchased a classifier f whose 0-1 accuracy is claimed by the factory to be 70%. Is this classifier meaningful for your problem? Unfortunately, it may very well be the case that you would be better off using the Bayesian marginal classifier (which you can easily train on your own based on label proportions). However, if you are using the 0-1 loss, you would have no way of knowing this. Using the PA, however, would allow you to detect this situation. Some alternative measures, such as Cohen’s kappa, can also help somewhat. In this section we empirically rank the various performance measures discussed above and show several values of imbalance and noise where only the PA will detect meaningless classifiers.\nTo this end, we consider the UCI breast cancer dataset [16], which is nearly balanced and nearly realizable.2 We use this set to synthetically generate a sequence of imbalanced and noisy independent test cases on which we compare all performance measures mentioned in Section 3. We control imbalance by synthetically inflating class proportions to desired levels using bootstrap sampling. Label noise is also controlled to desired levels by flipping the labels of randomly selected equal size subsets of both the minority and majority classes.\nConsider Figure 1, where we show performance levels of the various measures on a performanceimbalance plane for several noise levels. For example, in 1(a) we consider 20% label noise. It is evident that the PA (green curve) lower bounds all other measures. Moreover, all performance measures are always montonically ordered as follows: the PA (green) is the lowest, and above it are Cohen’s kappa (purple) , the F-measure (turqoise) , the balanced accuracy (red), and accuracy (blue). It is possible to prove that this order always holds. The interesting region in this graph is the top left quadrant defined by the intersection of the PA with zero. In this region, all the other measures indicate that the classifier at hand is useful even though it in fact achieves lower performance than the BMP. While Cohen’s kappa can detect some of this triviality, it also falls into this trap in many cases. We can also see that when the problem is balanced, the PA and Cohen’s kappa are equal and, when the problem has any level of noise and imbalance, the PA is strictly lower than all the other measures and will be the first to detect trivial solutions. Taken together with the proofs given in Section 4 (showing relationships of the PA to all the other measures), the above support the claim that the PA is a favorable measure for imbalance problems.\nRecall the numerical example from Section 1. We use the prediction advantage to measure the performance of Bob and Alice in the exam. Bob received a grade of 60 on exam A where there were three optional answers for each question. His loss is `(Bob(A)) = 0.4. The loss of the BMP for this test is `(f0(A)) = 23 , leading to a PA for Bob of PABob = 1 − 0.4/ 2 3 = 0.4. Calculating the same for Alice on the second test yields `(Alice(B)) = 0.4, `(f0(B)) = 0.75, and the PA is PAAlice = 1 − 0.4/0.75 = 715 = 0.46. We see that when using the PA, the grades that were indistinguishable are now distinguishable: the PA takes into account the complexity of the problem represented by the marginal distribution of Y , and by doing so gives more information.\n2For example, we trained a random forest classifier for this set whose test 0-1 error is R̂` = 0.038."
    }, {
      "heading" : "6 Concluding Remarks",
      "text" : "We proposed a general performance measure to quantify prediction quality. The measure quantifies prediction quality regardless of problem-dependent distortions such as class imbalance, noise, variance and number of classes. Unlike previous methods, the proposed measure is defined for any loss function, and we derived simple formulas for all popular loss functions. In the case of the squared loss function, the well-known R-squared measure emerged.\nOne reason for the popularity of theR-squared measure in regression analysis is its ability to measure performance in a “unified” manner across problems. This attractive property is obtained by normalizing the risk of the regression function by the variance of the marginal outcome distribution. PA generalizes this attractive property of the R-squared measure to any supervised learning problem under any loss function and enjoys the same uniformity property across problems. In general, the normalizing factor of any problem is captured by the performance of the Baysian marginal prediction (BMP). For nontrivial solutions, the PA uniformly quantifies the quality in percentage units on a scale where 0 represents triviality (BMP) and 1 represents perfect prediction. PA has no disadvantages whatsoever even if the problem is binary and perfectly balanced. To prevent trivial solutions and allow for comparisons across problems, we therefore recommend using the PA in all cases.\nWhile in this paper we advocate the use of the PA mainly as a means to prevent triviality in imbalanced problems, a very important application of the PA would be selective prediction (otherwise known as classification with a reject option) [6, 9, 22, 11], where the goal is to abstain over a subset of the domain so as to minimize risk in the covered region. In this setting, the PA is essential. For example, consider an imbalanced problem with a minority class consisting of 20% of the domain. Two selective classifiers were trained to cover 70% of the domain and both achieved 5% risk at this coverage. It is not hard to see that it is meaningless to compare these classifiers based on risk alone because their class distribution is different; e.g., only one of these classifiers rejected the minority class in its entirety, thus giving a trivial solution. The interesting open question in this regard would be how to use the PA as an optimization criterion for learning optimal selective prediction functions."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This research was supported by The Israel Science Foundation (grant No. 1890/14)"
    }, {
      "heading" : "A Prediction Advantage on Haberman Dataset",
      "text" : "The widely used Haberman dataset contains cases from a study that was conducted between 1958 and 1970 at the University of Chicago’s Billings Hospital on the survival of patients who had undergone surgery for breast cancer [16]. The dataset represents a small binary classification problem where the proportion of the minority class is 26.47% . Quite a few papers reported on classifiers they have trained for this dataset whose 0/1 test errors are close or no better than the trivial classifier that always classifies according to the majority label. To convincingly demonstrate that this is indeed a widely occurring problem, in this appendix we briefly provide the details of a few such results.\n[23] used SVM with a reject option and reported on 0.27 0/1 error without any rejection; this result is equivalent to a PA of -0.02. [14] trained a classifier whose error of 0.3 with a corresponding PA of -0.13. [24] reported on a classifier whose error is 0.273 (PA = −0.0313), and [5] reported on 0.2742 error whose PA is negative as well. [12] compared several methods to learn imbalanced problems. A few of the proposed methods suffered 0/1 errors greater than 0.3; the use of theF -measure in this study couldn’t help detecting these problematic results. [18] reached 0/1 accuracy of 73.4 (0/1 error 0.266) corresponding again to a negative PA. [15] reached accuracy below 70% in almost all of their experiments, and the only method achieving a non-trivial solution was a naive Bayes classifier (in all the other cases the PA was negative). [4] reported on a maximal 0/1 test accuracy of 71.7% , and [1] reported on 26.6% error, both corresponding to a negative prediction advantage."
    } ],
    "references" : [ {
      "title" : "Support vector machines with indefinite kernels",
      "author" : [ "Ibrahim Alabdulmohsin", "Xin Gao", "Xiangliang Zhang Zhang" ],
      "venue" : "In Proceedings of the Sixth Asian Conference on Machine Learning,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2014
    }, {
      "title" : "On randomization in on-line computation",
      "author" : [ "Allan Borodin", "Ran El-Yaniv" ],
      "venue" : "Information and Computation,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1999
    }, {
      "title" : "The balanced accuracy and its posterior distribution",
      "author" : [ "Kay Henning Brodersen", "Cheng Soon Ong", "Klaas Enno Stephan", "Joachim M Buhmann" ],
      "venue" : "In 20th international conference on Pattern recognition (ICPR),",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2010
    }, {
      "title" : "A robust algorithm for classification using decision trees",
      "author" : [ "B Chandra", "V Pallath Paul" ],
      "venue" : "In IEEE Conference on Cybernetics and Intelligent Systems,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2006
    }, {
      "title" : "Fuzzifying gini index based decision trees",
      "author" : [ "B Chandra", "P Paul Varghese" ],
      "venue" : "Expert Systems with Applications,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2009
    }, {
      "title" : "On optimum recognition error and reject tradeoff",
      "author" : [ "C Chow" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1970
    }, {
      "title" : "A coefficient of agreement for nominal scales",
      "author" : [ "Jacob Cohen" ],
      "venue" : "Educational and Psychological Measurement,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1960
    }, {
      "title" : "Regression and anova with zero-one data: Measures of residual variation",
      "author" : [ "Bradley Efron" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1978
    }, {
      "title" : "On the foundations of noise-free selective classification",
      "author" : [ "Ran El-Yaniv", "Yair Wiener" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2010
    }, {
      "title" : "Comparison of evaluation metrics in classification applications with imbalanced datasets",
      "author" : [ "Mehrdad Fatourechi", "Rabab K Ward", "Steven G Mason", "Jane Huggins", "Alois Schlögl", "Gary E Birch" ],
      "venue" : "In International Conference on Machine Learning and Applications (ICMLA),",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2008
    }, {
      "title" : "The Relationship Between Agnostic Selective Classification and Active",
      "author" : [ "R. Gelbhart", "R. El-Yaniv" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2017
    }, {
      "title" : "An effective integrated method for learning big imbalanced data",
      "author" : [ "Mojgan Ghanavati", "Raymond K Wong", "Fang Chen", "Yang Wang", "Chang-Shing Perng" ],
      "venue" : "In IEEE International Congress on Big Data (BigData Congress),",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2014
    }, {
      "title" : "On the interpretation and use of r2 in regression analysis",
      "author" : [ "Inge S Helland" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1987
    }, {
      "title" : "Correcting sample selection bias by unlabeled data",
      "author" : [ "Jiayuan Huang", "Arthur Gretton", "Karsten M Borgwardt", "Bernhard Schölkopf", "Alex J Smola" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2007
    }, {
      "title" : "Missing values: how many can they be to preserve classification reliability",
      "author" : [ "Martti Juhola", "Jorma Laurikkala" ],
      "venue" : "Artificial Intelligence Review,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "Introduction to Information Retrieval, volume 1",
      "author" : [ "Christopher D Manning", "Prabhakar Raghavan", "Hinrich Schütze" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2008
    }, {
      "title" : "Building predictors from vertically distributed data",
      "author" : [ "Sabine McConnell", "David B Skillicorn" ],
      "venue" : "In Proceedings of the 2004 conference of the Centre for Advanced Studies on Collaborative Research,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2004
    }, {
      "title" : "Conditional logit analysis of qualitative choice behavior",
      "author" : [ "D McFadden" ],
      "venue" : "Frontiers in Econometrics,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1974
    }, {
      "title" : "Signals, systems and inference",
      "author" : [ "Alan V Oppenheim", "George C Verghese" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2015
    }, {
      "title" : "A critical investigation of recall and precision as measures of retrieval system performance",
      "author" : [ "Vijay Raghavan", "Peter Bollmann", "Gwang S Jung" ],
      "venue" : "ACM Transactions on Information Systems (TOIS),",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1989
    }, {
      "title" : "Agnostic pointwise-competitive selective classification",
      "author" : [ "Y. Wiener", "R. El-Yaniv" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2015
    }, {
      "title" : "Agnostic selective classification",
      "author" : [ "Yair Wiener", "Ran El-Yaniv" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2011
    }, {
      "title" : "Random forests for metric learning with implicit pairwise position dependence",
      "author" : [ "Caiming Xiong", "David Johnson", "Ran Xu", "Jason J Corso" ],
      "venue" : "In Proceedings of the 18th ACM SIGKDD international conference on Knowledge Discovery and Data Mining,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2012
    }, {
      "title" : "Probabilistic computations: Toward a unified measure of complexity",
      "author" : [ "Andrew Chi-Chin Yao" ],
      "venue" : "In Proceedings of the 18th Annual Symposium on Foundations of Computer Science,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 1977
    } ],
    "referenceMentions" : [ {
      "referenceID" : 21,
      "context" : "47% and yet it fails even experienced ML researchers [23, 14, 24, 5, 12, 18, 15, 4, 1].",
      "startOffset" : 53,
      "endOffset" : 86
    }, {
      "referenceID" : 13,
      "context" : "47% and yet it fails even experienced ML researchers [23, 14, 24, 5, 12, 18, 15, 4, 1].",
      "startOffset" : 53,
      "endOffset" : 86
    }, {
      "referenceID" : 22,
      "context" : "47% and yet it fails even experienced ML researchers [23, 14, 24, 5, 12, 18, 15, 4, 1].",
      "startOffset" : 53,
      "endOffset" : 86
    }, {
      "referenceID" : 4,
      "context" : "47% and yet it fails even experienced ML researchers [23, 14, 24, 5, 12, 18, 15, 4, 1].",
      "startOffset" : 53,
      "endOffset" : 86
    }, {
      "referenceID" : 11,
      "context" : "47% and yet it fails even experienced ML researchers [23, 14, 24, 5, 12, 18, 15, 4, 1].",
      "startOffset" : 53,
      "endOffset" : 86
    }, {
      "referenceID" : 16,
      "context" : "47% and yet it fails even experienced ML researchers [23, 14, 24, 5, 12, 18, 15, 4, 1].",
      "startOffset" : 53,
      "endOffset" : 86
    }, {
      "referenceID" : 14,
      "context" : "47% and yet it fails even experienced ML researchers [23, 14, 24, 5, 12, 18, 15, 4, 1].",
      "startOffset" : 53,
      "endOffset" : 86
    }, {
      "referenceID" : 3,
      "context" : "47% and yet it fails even experienced ML researchers [23, 14, 24, 5, 12, 18, 15, 4, 1].",
      "startOffset" : 53,
      "endOffset" : 86
    }, {
      "referenceID" : 0,
      "context" : "47% and yet it fails even experienced ML researchers [23, 14, 24, 5, 12, 18, 15, 4, 1].",
      "startOffset" : 53,
      "endOffset" : 86
    }, {
      "referenceID" : 23,
      "context" : "By Yao’s principle [25, 2] (which follows from von Neumann’s minimax theorem), we can restrict attention only to deterministic BMPs (i.",
      "startOffset" : 19,
      "endOffset" : 26
    }, {
      "referenceID" : 1,
      "context" : "By Yao’s principle [25, 2] (which follows from von Neumann’s minimax theorem), we can restrict attention only to deterministic BMPs (i.",
      "startOffset" : 19,
      "endOffset" : 26
    }, {
      "referenceID" : 18,
      "context" : "The proof for this lemma is the known result of minimum mean squared error in the field of signal processing, for further reading see [20] (chapter 8).",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 12,
      "context" : "Apparently, the PA in regression is precisely the well-known R-squared measure in regression analysis (also known as the coefficient of determination) [13].",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 2,
      "context" : "[3] proposed to compare the mean of the true positive and true negative rates and defined it as the balanced accuracy.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 19,
      "context" : "Emerging from information retrieval, precision and recall are two popular measures that can be used to meaningfully measure performance in imbalanced problems [21].",
      "startOffset" : 159,
      "endOffset" : 163
    }, {
      "referenceID" : 15,
      "context" : "[17], the use of these measures prevents meaningless assessments in imbalance problems.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 6,
      "context" : "This quantity is called the F -measure and it approximates the so-called break-even point, defined to be the equilibrium of the trade-off between precision and recall, Another performance measure, which emerged from experimental psychology, is Cohen’s kappa [7].",
      "startOffset" : 258,
      "endOffset" : 261
    }, {
      "referenceID" : 9,
      "context" : "[10] to tackle imbalanced classification problems.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "R-squared is probably the most popular measure of fit in statistical modeling and particularly in the context of regression analysis [13].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 7,
      "context" : "For example, Efron [8] extended the R-squared measure by calculating the squared loss over the predicted probabilities divided by the variance measured in the probability space and McFadden [19] extended the R-squared measure by replacing the loss with the log-likelihood of the model, and the variance with the log-likelihood of the intercept.",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 17,
      "context" : "For example, Efron [8] extended the R-squared measure by calculating the squared loss over the predicted probabilities divided by the variance measured in the probability space and McFadden [19] extended the R-squared measure by replacing the loss with the log-likelihood of the model, and the variance with the log-likelihood of the intercept.",
      "startOffset" : 190,
      "endOffset" : 194
    }, {
      "referenceID" : 5,
      "context" : "While in this paper we advocate the use of the PA mainly as a means to prevent triviality in imbalanced problems, a very important application of the PA would be selective prediction (otherwise known as classification with a reject option) [6, 9, 22, 11], where the goal is to abstain over a subset of the domain so as to minimize risk in the covered region.",
      "startOffset" : 240,
      "endOffset" : 254
    }, {
      "referenceID" : 8,
      "context" : "While in this paper we advocate the use of the PA mainly as a means to prevent triviality in imbalanced problems, a very important application of the PA would be selective prediction (otherwise known as classification with a reject option) [6, 9, 22, 11], where the goal is to abstain over a subset of the domain so as to minimize risk in the covered region.",
      "startOffset" : 240,
      "endOffset" : 254
    }, {
      "referenceID" : 20,
      "context" : "While in this paper we advocate the use of the PA mainly as a means to prevent triviality in imbalanced problems, a very important application of the PA would be selective prediction (otherwise known as classification with a reject option) [6, 9, 22, 11], where the goal is to abstain over a subset of the domain so as to minimize risk in the covered region.",
      "startOffset" : 240,
      "endOffset" : 254
    }, {
      "referenceID" : 10,
      "context" : "While in this paper we advocate the use of the PA mainly as a means to prevent triviality in imbalanced problems, a very important application of the PA would be selective prediction (otherwise known as classification with a reject option) [6, 9, 22, 11], where the goal is to abstain over a subset of the domain so as to minimize risk in the covered region.",
      "startOffset" : 240,
      "endOffset" : 254
    } ],
    "year" : 2017,
    "abstractText" : "We introduce the Prediction Advantage (PA), a novel performance measure for prediction functions under any loss function (e.g., classification or regression). The PA is defined as the performance advantage relative to the Bayesian risk restricted to knowing only the distribution of the labels. We derive the PA for well-known loss functions, including 0/1 loss, cross-entropy loss, absolute loss, and squared loss. In the latter case, the PA is identical to the well-known R-squared measure, widely used in statistics. The use of the PA ensures meaningful quantification of prediction performance, which is not guaranteed, for example, when dealing with noisy imbalanced classification problems. We argue that among several known alternative performance measures, PA is the best (and only) quantity ensuring meaningfulness for all noise and imbalance levels.",
    "creator" : "LaTeX with hyperref package"
  }
}