{
  "name" : "1703.05364.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Study of Complex Deep Learning Networks on High Performance, Neuromorphic, and Quantum Computers",
    "authors" : [ "Thomas E. Potok", "Catherine Schuman", "Steven R. Young", "Robert M. Patton", "Federico Spedalieri", "Jeremy Liu", "Ke-Thia Yao", "Garrett Rose", "Gangotree Chakma" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Notice: This manuscript has been authored by UT-Battelle, LLC under Contract No. DE-AC05-00OR22725 with the U.S. Department of Energy. The United States Government retains and the publisher, by accepting the article for publication, acknowledges that the United States Government retains a non-exclusive, paid-up, irrevocable, world-wide license to publish or reproduce the published form of this manuscript, or allow others to do so, for United States Government purposes. The Department of Energy will provide public access to these results of federally sponsored research in accordance with the DOE Public Access Plan (http://energy.gov/downloads/doe-public-access-plan).\nar X\niv :1\n70 3.\n05 36\n4v 1\n[ cs\n.N E\n] 1"
    }, {
      "heading" : "1 Introduction",
      "text" : "Deep Learning is inspired by the networks of neurons in the visual cortex of the brain. Early versions of these neural networks have been simulated on a computer to analyze imagery. While promising, a significant limitation of this approach has been the computational time required to train or optimally set the weights within a network. Graphical processing units (GPUs) have provided a significant speedup in training, due to their ability to perform multiple simple network weight calculations in parallel, which allows for larger and more complex networks to be studied. These more complex networks containing multiple hidden layers are known as deep learning networks.\nIn this paper we explore the current limitations of deep learning, and test potential solutions on the emerging computational architectures of high performance computing, quantum computing, and neuromorphic computing. For this study, we will focus on convolutional neural networks (CNNs) and Boltzmann Machines (BMs).\nThere are multiple designs for deep learning networks, with, CNNs being the most widely used deep learning models [23] and are most commonly used for image classification with remarkably good results. CNNs have many similarities to traditional multi-layer perceptron (MLP) neural networks. They utilize stochastic gradient descent and backpropagation for training a set of learnable weights in a supervised manner. The definings feature of convolutional networks are their convolutional layers and pooling layers. Convolutional layers consist of multiple sets of weights, or kernels, that are convolved with their input to form a set of feature maps. Pooling layers are used to subsample the outputs of a convolutional layer to produce a smaller feature map, usually by using an max or average operation. These layers combine to utilize shared weights and pooling to take advantage of the locality that exists within images. The concept of shared weights builds upon the assumption that if a feature is useful in one location of an image it is also useful in other locations. Thus, the network need not calculate a unique set of weights for every position in the image. Since these weights are shared, activations from one position in an image are calculated using the same weights as other locations, and these weights can be pooled in a meaningful way. This pooling operation allows the network to be resilient to shifts of features within the image [31]. Subsequent improvements to CNNS, such as dropout [15], rectified linear units [17, 27], and efficient GPU DL codes [1, 3, 18] have allowed CNNs to achieve impressive results on a variety of benchmarks. These include the ImageNet [30] dataset where CNNs have surpassed human level performance at object recognition [11] and the Labeled Faces in the Wild (LFW) [16] dataset where CNNs have surpassed human level performance at face recognition [32].\nA Boltzmann Machine (BM) is a recurrent neural network consisting of neurons that make binary stochastic decisions based on the states of their symmetrically connected neuron neighbors [2]. Boltzmann Machines are well-suited for solving constraint satisfaction tasks with many weak constraints. These tasks include digit recognition, object recognition, compression/coding and natural language processing. A BM training algorithm was proposed in [2]. This training algorithm relies on iterations of updating the states until a thermal equilibrium is reached, and then updating weights based on a simple learning rule. Though this process can be improved by utilizing simulated annealing\nto reach thermal equilibrium, it is a very slow process for large networks. As there are 2N possible states the network could take for a network comprised of N neurons along with 2N weights to be learned. Identifying the state of thermal equilibrium and calculating equilibrium statistics becomes a challenge for large networks even for the best computational resources available as the computation grows exponentially with N . As such, the training of a BM is impractical for complex network topologies. This has given rise to the development of a Restricted Boltzmann Machine (RBM). The RBM network topology is restricted to a bipartite graph [14]. Deep belief networks (DBNs) can be created by composing many RBM layers [13].\nThere are currently three main challenges in Deep Learning. The first is how to train models with complex topologies that are closer representations of nature. Current deep learning networks limit the scale and complexity of the neuron models they use. While very successful in solving challenging classification problems, these neuron models are not comparable to those produced by nature. Early deep learning models proposed networks which contained intra-layer connections, but proved to be intractable to train on conventional computer systems. We believe that quantum computing may offer a potential solution with the ability to sample from complex probability distributions like those generated by neural networks that contain intra-layer connections.\nThe second challenge is how to automatically configure a network to an optimal or near optimal topology. Current deep learning models are created, trained and tested on reference datasets, with high performing network configurations reported in the literature. A significant challenge is how to construct a high performing network from previously unexamined data. GPU-based high performance computing provides an opportunity to train, test, and evolve thousands of deep learning networks to find well performing network hyperparamaters.\nThe last challenge is how to natively implement a complex deep learning model using a simulated neuron and synapse hardware architecture, as opposed to a CPU/Memory based model. While conventional computer can and are being used to deploy a deep learning network, the power requirements are high, especially when compared with nature. Neuromorphic devices provide neuron and synapse hardware architecture that incorporates a time component (spike), and when implemented with memristive technology has the potential to run deep learning networks with very low power consumption.\nOur hypothesis is that these three deep learning challenges can be addressed through a combinations of quantum, high performance, and neuromorphic computing. To test this hypothesis we use a simple deep learning problem, MNIST, using a native deep learning network representation for each of the the three computing platforms, i.e., a BM for quantum, a convolutional neural network for high performance computers, and a spiking neural network for neuromorphic.\nThis paper will provide a brief background on the challenges of deep learning related to quantum, high performance, and neuromorphic computing, followed by our experimental approach, results and future research."
    }, {
      "heading" : "2 Related Work",
      "text" : "First we look at the current state of art of quantum, high performance, and neuromorphic computing as related to the challenges in deep learning as stated above."
    }, {
      "heading" : "2.1 Quantum Computing",
      "text" : "Computing using quantum computers was first discussed by Feynman [10] who was motivated by the fact that simulating a quantum system using a classical computer seems to be intractable. Interest in quantum computing increased dramatically with the discovery of the Shor’s polynomial quantum algorithm for factoring numbers [35], because all known classical probabilistic factoring algorithms require exponential time. Several approaches to quantum computing were since developed, and they include the well-known quantum circuit model (used by Shor’s algorithm), the measurement-based quantum computing model, and the adiabatic quantum computing model [9]. All three have been shown to have the same computational power. In this paper, we focus on a restricted form of the adiabatic quantum computing model which performs adiabatic quantum optimization (AQO) to find the minimum energy state of an Ising Hamiltonian system. Actual implementations of adiabatic quantum machines, such as the D-Wave, operate at a finite temperature [20].\nThe output of these machines is a sequence of samples from a probability distribution defined by the Ising Hamiltonian system. The ability to draw samples from complex probability distributions is at the core of probabilistic deep learning approaches, like the BM. As stated above, the training of a BM is impractical on traditional computer systems, thus the development of a RBM restricts the network topology to that of a bipartite graphs [14]. Bipartite graphs allow for techniques like contrastive divergence to efficiently draw samples in linear time from the probability distribution defined by the BM. This potentially enables the evaluation of a network of unrestricted BM. Sampling is a fundamental building block and part of the inner loop of the Boltzmann learning algorithm. Without the bipartite restriction, sampling a BM with a general topology is a NP-hard problem. However, adiabatic quantum machines, like the D-Wave, have the potential to efficiently sample a richer set of graph topologies that are supersets of bipartite graphs."
    }, {
      "heading" : "2.2 High Performance Computing",
      "text" : "Deep learning, being an early adopter of GPU technology, has benefited greatly from the speedup offered by these accelerated computing devices and has received great support from device manufacturers in the form of deep learningspecific GPU libraries. General purpose GPUs are the basic building blocks of today’s HPC platforms and next generation machines will rely on them to an even greater degree. Thus, deep learning provides a great opportunity to fully utilize these machines, as they will have multiple GPUs per compute node. This leaves the question of how to best utilize thousands of GPUs for deep learning, as previous work has only utilized a maximum of 64 GPUs before encountering scaling problems when trying to exploit model parallelism to spread the weights of the network across multiple GPUs [6]. HPC provides the unique opportunity\nto address the problem of network specification. This refers to the problem of deciding upon the set of hyper-parameters needed to specify the network and training procedure in order to apply deep learning to a new dataset.\nFor convolutional neural networks, this could involve specifying parameters such as the number of layers, the number of hidden units, or the kernel size. For more general networks, such as RBMs, this could involve defining much more complicated connectivity between neurons.\nPreviously, it has been shown that HPC can be utilized to optimize the hyper-parameters of a deep learning network [37]. This work utilized an evolutionary algorithm distributed across the nodes of Oak Ridge National Laboratory’s (ORNL’s) Titan supercomputer in order to optimize the performance of deep learning algorithms. Hyper-parameters in deep learning refer to the model parameters, i.e., the activation function used, the number of hidden units in a layer, the kernel size of a convolutional layer, and the learning rate of the solver. As the size of the network grows, the hyper-parameter space grows increasingly larger. The size of deep learning networks used today have resulted in a hyperparameter space that cannot be searched on a single machine or a small cluster. This is a result of the computational complexity of training and evaluating these networks. Without utilizing the computational capabilities provided by supercomputers, evaluating a sufficient number of hyper-parameter sets to search the enormous hyper-parameter space of these methods would be impossible."
    }, {
      "heading" : "2.3 Neuromorphic Computing",
      "text" : "Neuromorphic computing architectures have historically been developed with one of two goals in mind: either developing custom hardware devices to accurately simulate biological neural systems with the goal of studying biological brains or building computationally useful architectures that are inspired by the operation of biological brains and have some of their characteristics. In developing neuromorphic computing devices for computational purposes, there have been two main approaches: building devices based on spiking neural networks (SNNs), such as IBM’s TrueNorth [5] or Darwin [34], and building devices based on convolutional neural networks, such as Google’s Tensor Processing Unit [21] or Nervana’s Nervana Engine [28], to serve as deep learning accelerators. The neuromorphic devices that have been built based on SNNs or built to simulate biologically-accurate systems have vastly different characteristics than those that have been built based on deep learning networks, such as CNNs or RBMs. The neurons in SNN-based systems are typically not organized in layers, and there are fewer restrictions on connectivity between neurons. The neuron and synapse models also differ from those in convolutional neural networks. In SNN-based neuromorphic systems, the neuron is typically some form of spiking neuron, such as a leaky-integrate-and-fire neuron, and the synapses have a delay value in addition to a weight value, thus introducing a temporal component to the processing of the network.\nThe primary computational issue associated with SNN-based systems is that very few algorithms that train native networks for those systems have been developed. Two of the key reasons why algorithms have not been developed are the computational difficulty introduced by broader connectivity and the computational difficulty introduced by the inclusion of the temporal component in both the neurons and synapses. In fact, one approach for training networks\nfor neuromorphic computers has been to train a CNN offline and then create a mapping process from the CNN to the associated SNN-based neuromorphic hardware [8].\nOne of the key properties of neuromorphic systems is their potential for more energy-efficient computation. To achieve energy-efficiency, we (and many others) have explored an implementation of a spiking neural network system utilizing memristors. Memristors are one of the four fundamental circuit elements. They are “memory resistors” in that their resistance can be altered depending on the magnitude of the voltage applied. Likewise, when no voltage is applied across a memristor, the most recent resistance value is retained [36]. Memristors have similar behavior to biological synapses, and as such, have been frequently utilized to implement neuromorphic systems [19, 22, 29]."
    }, {
      "heading" : "3 Approach",
      "text" : "The three platforms we are studying, quantum, high performance, and neuromorphic computing, are quite different in the way that they process data. Selecting a deep learning problem that can be used on all three is constrained by the amount of data that each can support. Currently D-Wave supports 1000 qubits, which limits the size of a problem to the inputs and deep learning network. MNIST is a collection of hand-written digits that has been very widely studied in the deep learning community [24]. The images of the digits are very small (28 X 28 pixels totaling 784 pixels) that can be analyzed using 1000 qubit of quantum computer as well as the other architectures.\nThe next challenge is selecting the type of deep learning network that can be natively supported on the three platforms. While many deep learning methods have been proposed over the years, CNN have consistently provided the highest accuracy on standard datasets, and typically would be the network of choice for such a comparison. However, the quantum architecture provides a native BM representation that does not restrict intra-layer connections, which is computationally impractical for conventional computers. Likewise, for a neuromorphic computer, a spiking neural network provides a native time-based analysis model. Both a strongly connected BM, and time dependent spiking neural network operate quite differently than a CNN, but in conjugation with their respective platforms, provide a distinctive capabilities that we believe can augment or strengthen a CNN model.\nThe last challenge is on how to compare the experimental results of the three platforms. A nominal performance comparison of the three approach provides little insight to the deep learning challenges we are addressing. Likewise an accuracy comparison on the all but solved MNIST problem is not helpful either. Instead, we look to compare the capabilities the three architectures provide in addressing the deep learning challenges we stated above. We believe that a combination of the architectures may provide a significant benefit over that of a single platform.\nFigure 1 shows a notional diagram of the deep learning networks that will be applied to each of the different architectures.\nSpecifically, a quantum computer will used to address the first deep learning challenge we list, namely, complex topologies that are closer representations of nature. Quantum computers have the ability to sample a probability distri-\nbution of a network of BM, and may provide a feasible approach to training a highly connected BM network. We will use high performance computers to address the second challenge of how to automatically configure a network to an optimal or near optimal topology using evolutionary algorithm to evolve a high performing network. And lastly, we will address the how to natively implement such a model using a neuron and synapse hardware architecture simulation of a memristive based spiking neural network."
    }, {
      "heading" : "3.1 Quantum",
      "text" : "The quantum computer we are using is a D-Wave adiabatic quantum computer located at the University of Southern California Lockheed Martin Quantum Computing Center. We propose a network of BMs to represent the MNIST problem. As discussed in Section 2.1, a deep learning network of BM has been previously proposed (Deep Boltzmann Machine); however, learning is intractable for a BM with a fully connected topology, since computing expected values over the model requires computing sums over an exponentially large state space. RBMs were introduced to circumvent this issue by discarding couplings between nodes within the same layer. Removing intra-layer couplings introduces conditional independence between nodes within the same layer. This computational advantage comes at the cost of lower representational power. The D-Wave device provides an opportunity to test this approach. First, we will create and train a RBM on D-Wave, applying it to the MNIST handwritten digit classification problem [24] to establish a reference result.\nNext, we will consider a more complex topology that allows for intra-layer connections between nodes. We call this semi-restricted BM a ”Limited Boltzmann Machine” (LBM). The LBM has two layers: one visible and one hidden. With a 1000 qubit, the visible layer requires 784 qubits, leaving a little over 200 qubits for the hidden layer. The visible layer is the same as in the RBM, with no intra-layer couplings. All the nodes of the visible layer are connected to all the nodes of the hidden layer. However, in contrast to the RBM, the LBM’s hidden layer is allowed to have intra-layer couplings.\nThe hidden layer topology of the LBM is based on the Chimera graph, which represents the underlying connection topology of the D-Wave processor. Chimera graphs are composed of 8-qubit cells. Within each cell, the nodes have a four-by-four fully bipartite connectivity. The bipartite cells are arranged in a grid pattern. The four nodes from one side of the bipartite graph are linked horizontally to two other nodes of adjacent cells in horizontal direction. The other four nodes are linked vertically to two other nodes of adjacent cells in the vertical direction (Figure 2).\nAs a consequence, the probability distribution of the hidden layer nodes no longer factorizes when the values of the visible layer nodes are fixed. To estimate the expected values required for the learning process, we used the D-Wave processor to generate samples of the hidden layer configuration and estimate probabilities.\nWe also included 10 “classification” digits used to calculate gradients and determine the LBM’s output label, i.e., what digit has been read.\nWe chose 6,000 images from the MNIST database for training the LBM. We use a subset of randomly selected images instead of using the full MNIST dataset due to time considerations. Our D-Wave system is time-shared between different organizations, and within each time-share there are many users. Limiting ourselves to 10% of the data allowed us to run experiments in a timely manner.\nEach 28x28 image is represented by a vector of 784 length, each unit with holding a value representing pixel intensity in the range [0, 1]. In addition to these 784 units representing pixel intensities, we include the previously mentioned 10 additional units to represent the image’s digit label (0 through 9) using 1-hot encoding. For example, an image of the digit “3” will have label unit 3 marked “on” while all other classification nodes will be off). Each image is thus represented with a vector of length 794. When we show each image to the RBM, we hide the image labels and have the RBM attempt to reconstruct the label units. We choose the unit label of highest ”on” probability as the RBM’s digit classification choice for each image.\nTraining weights for the LBM are randomly initialized, just as they are in the RBM. Also as in the RBM, their values are sampled from a standard normal distribution and are updated using the same gradient descent procedure."
    }, {
      "heading" : "3.2 HPC",
      "text" : "The HPC computer we are using is the ORNL’s Titan computer with roughly 300,000 cores, and 18,000 GPUs. This is currently the fastest open science computer in the world.\nClearly a supercomputer is not needed to solve the MNIST problem; however, a supercomputer is needed to automatically find an optimal deep learning topology for such a problem. Rather than using a trial-and-error method for finding a well performing network topology, we propose to use evolutionary optimization on Titan to evaluate tens of thousands of topologies; therefore, systematically finding the best performing networks on this problem. If achievable, this would solve one of the major challenges in building deep learning networks.\nFor this experiment we use a CNN as our deep learning network since CNNs are currently producing the top results. We approach the network topology problem of selecting optimal hyper-parameters as a massive search problem, where Titan can be used to quickly search the space.\nWe represent each individual within the population of the evolutionary algorithm (EA) as a single deep neural network or CNN. An individual consists of a genome where the genes represent the various hyper-parameters that define the network topology, i.e., the number of layers, type of layers (convolution, pooling, etc.), and order of the layers. We then apply parameters defined in the genes of the individual to construct and train a deep learning network on the MNIST dataset. The results of the network’s performance in testing are then used as the ”fitness” of the individual in the EA population, i.e., individual networks that have high accuracy are considered to be the most fit. Typically, generating the results for a single network on a small dataset like MNST will require a modest amount of GPU/CPU time, and memory. However, creating, training, and evaluating tens of thousands networks requires a significant number of GPUs, like those in the Titan high performance computer.\nAfter all the individuals in the population are evaluated, the top performing individuals are selected to generate a new population of individuals that represent the next generation of the EA. These new generations contain a mix of the well performing hyperparameters from the best performing networks in the population. Successive generations of individuals gradually leads to an improved set of hyperparameters over time. This method is called Multi-node Evolutionary Neural Networks for Deep Learning (MENNDL).\nFor this experiment we are looking to automatically discover hyperparameters of a well performing deep learning network on the MNIST dataset. We used a simple EA that limits the search to the number of neurons per layer and the kernel size of convolutional layers.\nThe network architecture utilized was LeNet [23]. This network utilizes 2 convolutional layers, 2 pooling layers, and one hidden fully-connected layer. This is the network that is most often used with the MNIST dataset in the literature.\nWe have shown that even with this widely studied MNIST dataset, better hyper-parameters can be found than those widely reported in the literature. An EA that can evolve the topology provides the opportunity for improved results, and the ability to process more challenging datasets. Such an EA will also provide the opportunity to meaningfully utilize the entirety of Titan’s capacity. It will provide challenging data management problems on a machine designed primarily for modeling and simulation, as opposed to these deep learning algorithms which require heavy amounts of data input in addition to heavy computation."
    }, {
      "heading" : "3.3 Neuromorphic",
      "text" : "A neuromorphic approach to the MNIST problem is not the ideal solution since there is not a temporal component in the task of recognizing a handwritten digit. To add a temporal component, we use a streaming scan of the digits as input to the SNN. The SNN learns to recognize digits based on this scan pattern. The goal is to understand the deployment benefits of using an SSN in memristive hardware, as opposed to classification accuracy on this problem.\nWe then propose to evaluate the performance of this network on memristor hardware having the potential to enable a low power hardware implementation of a deep learning network, further, with the ability to represent spatial and temporal data.\nAs noted in Section 2.3, there are not very many SNN training methods or training methods that can be applied to neuromorphic networks. To train both SNN models and neuromorphic networks, we utilize an EA approach to determine the structure (e.g., number of neurons and synapses and how they are connected) and parameters (e.g., weight values of synapses and threshold values of neurons).\nThe neuromorphic system we will use to explore the MNIST problem is a memristive implementation of the neuroscience-inspired dynamic architectures (NIDA) system [33]. NIDA is a simple SNN model composed of integrate-andfire neurons and synapses with delays and weights that are affected by processes similar to long-term potentiation and long-term depression in biological brains. A digital hardware implementation based on NIDA, called Dynamic Adaptive Neural Network Array (DANNA), has also been created and is currently implemented on FPGA with a digital VLSI implementation in progress [7]. NIDA synapses have analog weight values, while DANNA is restricted to a finite set of digital weight values. DANNA also has restricted connectivity between neurons, whereas the NIDA model allows for up to fully connected networks. The NIDA model allows for us to study neuromorphic models in software and determine how restrictions different restrictions in hardware (such as weight resolution or connectivity) affect performance. The EA approach for training networks for the MNIST problem was previously applied to the NIDA SNN [33] and to DANNA [7]. For both NIDA and DANNA, an ensemble approach is utilized, where each network in the ensemble is responsible for recognizing a particular digit type. For example, a network may be trained to recognize zeros, in which case the network will take the handwritten digit image as input and its output corresponds to either “yes, it is a zero” or “no, it is not a zero.” Using this approach, ensembles that achieve around 90 percent accuracy for NIDA and around 80 percent accuracy for DANNA were created.\nFor this work, we extend the prior work by simulating a SNN (specifically, a NIDA network) implemented in memristive hardware in order to demonstrate the potential of significant power reductions for simulating the behavior of neural networks."
    }, {
      "heading" : "4 Results",
      "text" : ""
    }, {
      "heading" : "4.1 Quantum",
      "text" : "We utilize common parameters to control the learning progress, the same ones found in training RBMs [12]. We chose to conduct training over 25 epochs (one epoch is a complete run over all the training data) instead of 10 epochs to get a better idea of what performance we can potentially achieve. Another parameter is the learning rate, or how much the LBM learns from each example. Setting the parameter too high can cause unstable behavior. This can be thought of as the LBM compensating too much for an error. Setting the parameter too low has the obvious downside of the LBM not learning anything of value from an\nexample. We chose a relatively standard learning rate of 0.1 for visible-to-hidden edges and 0.0001 for hidden-to-hidden edges.\nWe wanted to determine if any performance advantage would be gained from using this LBM topology instead of the traditional RBM topology. First we ran a small experiment to confirm that the training of the LBM would behave correctly. Figure 3 shows the input reconstruction error and the classification rate for a LBM, confirming that it learns the MNIST data. In 4 we also include a table comparing RBM performance against LBM performance on this MNIST digit classification task.\nThe RBM and LBM were both implemented on D-Wave and on MNIST images using the same number of hidden and visible units over ten training epochs. The RBM configuration, as discussed, has no intra-layer connections, whereas the LBM configuration has limited connections between the hidden nodes.\nWe initially found that the LBM configuration performed worse than the RBM configuration when we included couplings between nodes in the hidden layer. This was not what we expected, so we introduced a hybrid learning scheme where these intra-layer couplings were redrawn from a random normal distribution for the first three training epochs. From epoch 4 on, the weights were allowed to follow the typical learning rule used in BMs. The results can be seen in the blue series in Figure 5. The choice of using a three epoch duration for randomization was rather arbitrary and the full effects of choosing different durations can be explored in future work. We were primarily interested in providing some randomization while retaining a modest amount of learning time (seven epochs). The final classification rate for one of our trained LBMs was 88.53 percent. Reconstruction error dropped in a regular, expected manner as seen in Figure 5. In practical terms, we want to compare the cost of training LBMs on quantum devices versus training LBMs on traditional architectures."
    }, {
      "heading" : "4.2 HPC",
      "text" : "We used the Titan computer and the MENNDL system to discover a near optimal topology of a deep network trained on the MNIST handwritten digit dataset [24] by utilizing the method presented in [37]. The hyper-parameters optimized were the kernel size, the number of hidden units for each of the convolutional layers, and the number of hidden units in the fully connected layer. The structure of the network is shown in Figure 7. Utilizing 500 nodes of Titan, the evolutionary algorithm was trained for 32 generations with 500 individuals in the population allowing us to evaluate 16, 000 networks. Each hyper-parameter is encoded as an integer gene, and the range of this integer is limited in order to avoid evaluating hyper-parameter values that are not of interest. A single node of Titan evaluates the core of the evolutionary algorithm and distributes the fitness function to the rest of the nodes to evaluate the network using Titan’s GPUs.\nThe optimal network, shown in Figure 7, demonstrates some significant differences from the starting network. The optimal network achieved 98.5 percent classification accuracy, representing a slight increase over the baseline network. This demonstrates that accuracy can be improved by optimizing hyperparameters even on networks and datasets that have been widely used. Figure 6 highlights the best performing networks and shows their corresponding hyperparameters. It is interesting to note that the best performing networks had a wide variety of values for the number of hidden units in the fully connected layer. However, there was little variation in the kernel size of the convolutional layers which indicates that the performance of the network is much more sensitive to this parameter, and the kernel size of the second layer converged to a much smaller value than the value that is typically used. This indicates that even for very well studied problems, i.e., MNIST, the networks typically used are not optimal since it is difficult to find the correct hyper-parameters without an automated search process and significant computing resources."
    }, {
      "heading" : "4.3 Neuromorphic",
      "text" : "This experiment was done in two parts, the first was to implement a SNN using NIDA to demonstrate a neuromorphic solution to the MNIST problem is feasible. The second part is to simulation the characteristics of this SNN implemented on memristive hardware.\nWe started by simulating a memristive implementation of a NIDA network trained to classify MNIST images (Figure 8). The NIDA network itself was generated using evolutionary optimization and was part of an ensemble of networks that classifies MNIST images with an accuracy of approximately 90 percent. Energy consumption was also estimated for a NIDA representation of this network where synaptic elements are implemented using metal-oxide memristors.\nThe memristive device technology assumed for this simulation is characterized by a low resistance state (LRS) of 60kΩ, about an order of magnitude larger than the resistance of a typical deep-submicron CMOS transistor. This relatively high LRS for the memristor is desirable such that the CMOS channel resistance can effectively be neglected. The on-off ratio is assumed to be 10, providing a high resistance state (HRS) of 600kΩ. Such characteristics for LRS, HRS and the associated on-off ratio have been observed for a range of memristive devices, including hafnium-oxide (HfO2)[4], tantalum-oxide (TaO2), and titanium-oxide (TiO2), to name a few. All of these memristive material stacks consist of an oxide layer sandwiched between two metallic layers. Depending on the polarity and magnitude of an applied voltage bias, the oxide layer transitions between being less or more conductive, providing the switching characteristics desirable for representing synaptic weights.\nOur memristive NIDA simulation setup also includes analog integrate-andfire neurons, implemented using a 65nm CMOS process technology. Neuromorphic elements (neurons and synapses) were simulated using Cadence Spectre\nand system-level energy and power estimates were calculated using a high-level simulator written in C++.\nThe memristive NIDA simulation is based on two significant steps. Initially the Evolutionary Optimization based simulator is used to generate optimized networks for the low level simulation. Then the transistor level simulation is done using Cadence Spectre simulator. Some power estimates are collected for the design components in different conditions. Those are used to calculate the total energy consumed during the simulation of the application.\nTo collect energy data, we considered three different phases for neuron and those are fire, accumulation, and idle phase. Using Cadence simulator energy of each phase has been determined as per spike energy. Similar method is followed for synapses as well. But here we considered the active and passive phases of a synapse. And for the programmable delay chain, we collected energy data for each spike existing in the delay chain. The EO based memristive NIDA simulator is used to generate network with some activity factors for specific applications. For instance, MNIST dataset was used to verify in recognizing the handwritten characters and the total energy of the system was calculated. To calculate the average energy for a single image run, the total energy consumed by the network is calculated for the total number of runs (which, for example, is 10000 for MNIST dataset application). The network has 128 neurons and 357 synapses associated with 357 programmable delays. The neurons and synapses are mostly analog circuit components but the programmable logic delay consists of digital components. That is why the energy consumed is mostly because of the digital parts. The average power observed for the total network with 16.67 MHz clock speed is 304.3mW and the respective energy is 18.26nJ (including the digital programmable delays). But if we consider the core analog neuromorphic logic the energy per spike is 5.24nJ and the average power is 87.43mW which is consistent with similar memristor-based neuromorphic systems [25]. Research has also shown that memristive neuromorphic systems are typically 20× more energy-efficient than their CMOS counterparts [26], and our results are consistent with this estimation. Further improvements in energy-efficiency are possible through the use of memristors with a higher on-off ratio and/or higher low resistance state. Ultra low-power CMOS circuit design techniques (i.e., sub-threshold operation) can also be used to further reduce the power consumption of CMOS neurons. Thus, a CMOS-memristive neuromorphic implementation is particularly well suited for energy-constrained, resource limited application domains."
    }, {
      "heading" : "5 Discussion",
      "text" : "The goal of this study is to explore ways of addressing some of the current limitations of deep learning, namely, complex topologies that are closer representations of nature, automatically configuring a the hyperparameters of a network, and natively implement deep learning model using neuron and synapse hardware.\nWe use three architectures, quantum computing, high performance computer, and neuromorphic computing and three different deep learning models, LBM, CNN, and SNN to address these issues.\nThe quantum approach allows the deep learning network topologies to be much more complex than is feasible with conventional computers. The results\nshow training convergence with a high number of intra-layer connections, thus opening the possibility of using much more complex topologies that can be trained on a quantum computer. There is not a time-based performance penalty for increased intra-layer connections, although, there may be the need to do more sampling in order to reduce potential errors.\nHPC’s contribution to the problem focuses on automatically developing an optimal network topology to create a high performing network. Many of the topologies used today are developed through trial and error methods. This approach works well with standard research datasets since the research community can learn and publish the topologies that produce the highest accuracy networks for these data. It is a different matter when working with datasets that have not been widely studied. The HPC approach provides a way to optimize the hyper-parameters of a CNN, saving significant amounts of time when working on new datasets.\nThe neuromorphic contribution to this problem is to provide a native implementation and a low-power memristor-based hardware to implement of a SNN. The network has the potential to have broader connections than a CNN and the ability to dynamically reconfigure itself over time. There are many benefits to neuromorphic computers (including robustness, low energy usage, and small device footprint) that could be useful in a real-world environment today if we had a mechanism for finding good network solutions to deploy on those devices.\nReviewing the results of the three experiments opens the possibility of using these three architectures in tandem to create powerful deep learning systems that are beyond our current capabilities. Practically, the current quantum computer is quite limited in the size and scope of the problem it can address, but the ability to train a very complex deep learning network gives it a very interesting potential. It could be used to generate weights for very complex networks that are untrainable using current systems opening the possibilities of potentially solving more complex and challenging problems. However, the scalability of a quantum machine is a real concern. As we observed, limiting the size of the\ninput layer to 1000 qubit severely limits the size of a problem that can be analyzed using this approach. We believe that the best use for complex networks may be as higher layers in a CNN. These layers usually are combining fairly rich features, and may benefit from increased inter-layer connections. These layers usually have a smaller input size than the original input, which eases the scalability concerns of this approach, and may improve over all accuracy.\nThe HPC approach of automatically finding optimal deep learning topologies is a fairly robust and scalable capability, though quite expensive in development and computer costs. Having the ability to use deep learning methods on unstudied datasets (experimental scientific data) can provide a huge time savings and analytical benefit to the scientific community.\nThe neuromorphic approach is limited by the lack of robust neuromorphic hardware and algorithms, yet it holds the potential of analyzing complex data using temporal analysis and very low power hardware. One of the most compelling aspects of this approach is the combination of a SNN and neuromorphic hardware that can analyze the temporal aspects of data. The MNIST problem does not have a temporal component, but one can imagine a dataset that has both image and temporal aspects, such as a video. A CNN approach has been shown to perform well on the image side, perhaps a SNN can provide increased accuracy by analyzing the temporal aspects as well.\nThese experiments provide valuable insights into deep learning by exploring the combination of three novel approaches to challenging deep learning problems. We believe that these three architectures can be combined to gain greater accuracy, flexibility, and insight into a deep learning approach. Figure 9 shows a possible configuration of the three approaches that addresses the three deep learning challenges we discussed above. The high performance computer is used to create a well performing CNN on image type data. The final layer or two is then processed by the quantum computer using a LBM network that contains greater complexity than and CNN. The temporal aspects of the data are modeled using a SNN, and the ensemble models are then merged and an output produced. Our belief is that this approach has the potential to yield greater accuracy than existing CNN models."
    }, {
      "heading" : "5.1 Future Work",
      "text" : "Our next step is to test the hypothesis that this proposed architecture does indeed provide greater accuracy, flexibility, and insight into a dataset than can be derived from a traditional CNN approach. We will apply this proposed architecture to a large scientific dataset, and compare the results of a traditional approach to this proposed architecture."
    }, {
      "heading" : "6 Conclusion",
      "text" : "Current Deep Learning networks are loosely based on this neural model of human perception and have been highly optimized using CNNs trained on large clusters of GPUs. This technology has been instrumental in solving problems that have challenged researchers for years, such as object and facial recognition within photographs. The topology of these CNN networks consist of convolutional layers with shared weights and fully connected layers, without inter-layer\nconnections, which while powerful, are quite simplistic. This paper addresses three main limitations in deep learning: 1) training models with complex topologies that contain intra-layer connections; 2) automatically determining an optimal configuration for a network topology; and 3) implementing a complex topology in native hardware. To address these problems we explored a simple deep learning problem on three different architectures: quantum, high performance, and neuromorphic computers. These architectures address the three problems: complex topologies with quantum computing; network topology optimization with high performance computing; and low-power implementation with neuromorphic computing.\nGiven input size limitations of 1,000 qubits, we use the MNIST dataset for this evaluation, and use neuron models and topologies that are best suited to the architectures: CNN for HPC; SNN for neuromorphic; and BMs for quantum.\nOur results from these three experiments demonstrate the possibility of using these three architectures to solve complex deep learning networks that are currently untrainable using a von Neumann architecture.\nThe quantum computer experiment demonstrated that a complex neural network, i.e., one with intra-layer connections, can be successfully trained on the MNIST problem. This is a key advantage for a quantum approach and opens the possibility of training very complex networks. A high performance computer can be used to take the complex networks as building blocks and compare thousands of models to find the best performing networks for a given problem. And finally, the best performing neural network and weights can be implemented into a complex network of memristors producing a low-power hardware device. This is a capability that is not feasible with a von Neumann architecture. This holds the potential to solve much more complicated problems than can currently be solved with deep learning.\nWe propose a new deep learning architecture based on the unique capabilities of the quantum, high performance, and neuromorphic approaches presented in this paper. This new architecture addresses the three main limitations we see in current deep learning methods, and holds the promise of higher classification accuracy, faster network creation times, and low power, native implementation in hardware."
    }, {
      "heading" : "7 Acknowledgments",
      "text" : "This material is based upon work supported by the U.S. Department of Energy, Office of Science, Office of Advanced Scientific Computing Research, Robinson Pino, program manager, under contract number DE-AC05-00OR22725. This research used resources of the Oak Ridge Leadership Computing Facility, which is a DOE Office of Science User Facility supported under Contract DE-AC0500OR22725."
    } ],
    "references" : [ {
      "title" : "and X",
      "author" : [ "M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "I. Goodfellow", "A. Harp", "G. Irving", "M. Isard", "Y. Jia", "R. Jozefowicz", "L. Kaiser", "M. Kudlur", "J. Levenberg", "D. Mané", "R. Monga", "S. Moore", "D. Murray", "C. Olah", "M. Schuster", "J. Shlens", "B. Steiner", "I. Sutskever", "K. Talwar", "P. Tucker", "V. Vanhoucke", "V. Vasudevan", "F. Viégas", "O. Vinyals", "P. Warden", "M. Wattenberg", "M. Wicke", "Y. Yu" ],
      "venue" : "Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A learning algorithm for boltzmann machines",
      "author" : [ "D.H. Ackley", "G.E. Hinton", "T.J. Sejnowski" ],
      "venue" : "Cognitive science, 9(1):147–169",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1985
    }, {
      "title" : "Towards memristive dynamic adaptive neural network arrays",
      "author" : [ "N. Cady", "K. Beckmann", "H. Manem", "M. Dean", "G. Rose", "J.V. Nostrand" ],
      "venue" : "In Proceedings of the Government Microcircuit Applications and Critical Technology Conference (GOMACTech),",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2016
    }, {
      "title" : "et al",
      "author" : [ "A.S. Cassidy", "P. Merolla", "J.V. Arthur", "S.K. Esser", "B. Jackson", "R. Alvarez- Icaza", "P. Datta", "J. Sawada", "T.M. Wong", "V. Feldman" ],
      "venue" : "Cognitive computing building block: A versatile and efficient digital neuron model for neurosynaptic cores. In Neural Networks (IJCNN), The 2013 International Joint Conference on, pages 1–10. IEEE",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Deep learning with COTS HPC systems",
      "author" : [ "A. Coates", "B. Huval", "T. Wang", "D.J. Wu", "B. Catanzaro", "A.Y. Ng" ],
      "venue" : "Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, pages 1337–1345",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "DANNA: A neuromorphic software ecosystem",
      "author" : [ "A. Disney", "J. Reynolds", "C.D. Schuman", "A. Klibisz", "A. Young", "J.S. Plank" ],
      "venue" : "Biologically-Insipred Cognitive Architectures 2016, page In press",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Backpropagation for energy-efficient neuromorphic computing",
      "author" : [ "S.K. Esser", "R. Appuswamy", "P. Merolla", "J.V. Arthur", "D.S. Modha" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 1117–1125",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Quantum computation by adiabatic evolution",
      "author" : [ "E. Farhi", "J. Goldstone", "S. Gutmann", "M. Sipser" ],
      "venue" : "Report MIT-CTP-2936, Massachusetts Institute of Technology",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Simulating physics with computers",
      "author" : [ "R.P. Feynman" ],
      "venue" : "International journal of theoretical physics, 21(6):467–488",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1982
    }, {
      "title" : "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
      "author" : [ "K. He", "X. Zhang", "S. Ren", "J. Sun" ],
      "venue" : "Proceedings of the IEEE International Conference on Computer Vision, pages 1026–1034",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A practical guide to training restricted boltzmann machines",
      "author" : [ "G. Hinton" ],
      "venue" : "Momentum, 9(1):926",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A fast learning algorithm for deep belief nets",
      "author" : [ "G.E. Hinton", "S. Osindero", "Y.-W. Teh" ],
      "venue" : "Neural Comput.,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2006
    }, {
      "title" : "Reducing the dimensionality of data with neural networks",
      "author" : [ "G.E. Hinton", "R.R. Salakhutdinov" ],
      "venue" : "Science, 313(5786):504–507",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Improving neural networks by preventing co-adaptation of feature detectors",
      "author" : [ "G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov" ],
      "venue" : "arXiv preprint arXiv:1207.0580",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Labeled faces in the wild: A database forstudying face recognition in unconstrained environments",
      "author" : [ "G.B. Huang", "M. Mattar", "T. Berg", "E. Learned-Miller" ],
      "venue" : "Workshop on faces in’Real-Life’Images: detection, alignment, and recognition",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "et al",
      "author" : [ "K. Jarrett", "K. Kavukcuoglu", "Y. LeCun" ],
      "venue" : "What is the best multi-stage architecture for object recognition? In Computer Vision, 2009 IEEE 12th International Conference on, pages 2146–2153. IEEE",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Caffe: Convolutional architecture for fast feature embedding",
      "author" : [ "Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell" ],
      "venue" : "arXiv preprint arXiv:1408.5093",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Nanoscale memristor device as synapse in neuromorphic systems",
      "author" : [ "S.H. Jo", "T. Chang", "I. Ebong", "B.B. Bhadviya", "P. Mazumder", "W. Lu" ],
      "venue" : "Nano letters, 10(4):1297–1301",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Google supercharges machine learning tasks with tpu custom chip, May 2016",
      "author" : [ "N. Jouppi" ],
      "venue" : null,
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2016
    }, {
      "title" : "A functional hybrid memristor crossbar-array/cmos system for data storage and neuromorphic applications",
      "author" : [ "K.-H. Kim", "S. Gaba", "D. Wheeler", "J.M. Cruz-Albrecht", "T. Hussain", "N. Srinivasa", "W. Lu" ],
      "venue" : "Nano letters, 12(1):389–395",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "Proceedings of the IEEE, 86(11):2278– 2324",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "and C",
      "author" : [ "Y. LeCun", "C. Cortes" ],
      "venue" : "J. Burges. The MNIST database of handwritten digits",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "A memristor crossbar based computing engine optimized for high speed and accuracy",
      "author" : [ "C. Liu", "Q. Yang", "B. Yan", "J. Yang", "X. Du", "W. Zhu", "H. Jiang", "Q. Wu", "M. Barnell", "H. Li" ],
      "venue" : "VLSI (ISVLSI), 2016 IEEE Computer Society Annual Symposium on, pages 110–115. IEEE",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Harmonica: A framework of heterogeneous computing systems with memristor-based neuromorphic computing accelerators",
      "author" : [ "X. Liu", "M. Mao", "B. Liu", "B. Li", "Y. Wang", "H. Jiang", "M. Barnell", "Q. Wu", "J. Yang", "H. Li", "Y. Chen" ],
      "venue" : "IEEE Transactions on Circuits and Systems I: Regular Papers,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2016
    }, {
      "title" : "Rectified linear units improve restricted boltzmann machines",
      "author" : [ "V. Nair", "G.E. Hinton" ],
      "venue" : "Proceedings of the 27th international conference on machine learning (ICML-10), pages 807–814",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Training and operation of an integrated neuromorphic network based on metal-oxide memristors",
      "author" : [ "M. Prezioso", "F. Merrikh-Bayat", "B. Hoskins", "G. Adam", "K.K. Likharev", "D.B. Strukov" ],
      "venue" : "Nature, 521(7550):61–64",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "et al",
      "author" : [ "O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein" ],
      "venue" : "Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211–252",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Evaluation of pooling operations in convolutional architectures for object recognition",
      "author" : [ "D. Scherer", "A. Müller", "S. Behnke" ],
      "venue" : "International Conference on Artificial Neural Networks, pages 92–101. Springer",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Facenet: A unified embedding for face recognition and clustering",
      "author" : [ "F. Schroff", "D. Kalenichenko", "J. Philbin" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 815–823",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Spatiotemporal classification using neuroscience-inspired dynamic architectures",
      "author" : [ "C.D. Schuman", "J.D. Birdwell", "M.E. Dean" ],
      "venue" : "Procedia Computer Science, 41:89 – 97",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Darwin: a neuromorphic hardware co-processor based on spiking neural networks",
      "author" : [ "J. Shen", "D. Ma", "Z. Gu", "M. Zhang", "X. Zhu", "X. Xu", "Q. Xu", "Y. Shen", "G. Pan" ],
      "venue" : "Science China Information Sciences, 59(2):1–5",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Polynomial-time algorithms for prime factorization and discrete logarithms on a quantum computer",
      "author" : [ "P.W. Shor" ],
      "venue" : "SIAM J. Comput.,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 1997
    }, {
      "title" : "How we found the missing memristor",
      "author" : [ "R.S. Williams" ],
      "venue" : "IEEE Spectrum,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2008
    }, {
      "title" : "Optimizing deep learning hyper-parameters through an evolutionary algorithm",
      "author" : [ "S.R. Young", "D.C. Rose", "T.P. Karnowski", "S.-H. Lim", "R.M. Patton" ],
      "venue" : "Proceedings of the Workshop on Machine Learning in High- Performance Computing Environments, MLHPC ’15, pages 4:1–4:5, New York, NY, USA",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 20,
      "context" : "There are multiple designs for deep learning networks, with, CNNs being the most widely used deep learning models [23] and are most commonly used for image classification with remarkably good results.",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 27,
      "context" : "This pooling operation allows the network to be resilient to shifts of features within the image [31].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 13,
      "context" : "Subsequent improvements to CNNS, such as dropout [15], rectified linear units [17, 27], and efficient GPU DL codes [1, 3, 18] have allowed CNNs to achieve impressive results on a variety of benchmarks.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 15,
      "context" : "Subsequent improvements to CNNS, such as dropout [15], rectified linear units [17, 27], and efficient GPU DL codes [1, 3, 18] have allowed CNNs to achieve impressive results on a variety of benchmarks.",
      "startOffset" : 78,
      "endOffset" : 86
    }, {
      "referenceID" : 24,
      "context" : "Subsequent improvements to CNNS, such as dropout [15], rectified linear units [17, 27], and efficient GPU DL codes [1, 3, 18] have allowed CNNs to achieve impressive results on a variety of benchmarks.",
      "startOffset" : 78,
      "endOffset" : 86
    }, {
      "referenceID" : 0,
      "context" : "Subsequent improvements to CNNS, such as dropout [15], rectified linear units [17, 27], and efficient GPU DL codes [1, 3, 18] have allowed CNNs to achieve impressive results on a variety of benchmarks.",
      "startOffset" : 115,
      "endOffset" : 125
    }, {
      "referenceID" : 16,
      "context" : "Subsequent improvements to CNNS, such as dropout [15], rectified linear units [17, 27], and efficient GPU DL codes [1, 3, 18] have allowed CNNs to achieve impressive results on a variety of benchmarks.",
      "startOffset" : 115,
      "endOffset" : 125
    }, {
      "referenceID" : 26,
      "context" : "These include the ImageNet [30] dataset where CNNs have surpassed human level performance at object recognition [11] and the Labeled Faces in the Wild (LFW) [16] dataset where CNNs have surpassed human level performance at face recognition [32].",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 9,
      "context" : "These include the ImageNet [30] dataset where CNNs have surpassed human level performance at object recognition [11] and the Labeled Faces in the Wild (LFW) [16] dataset where CNNs have surpassed human level performance at face recognition [32].",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 14,
      "context" : "These include the ImageNet [30] dataset where CNNs have surpassed human level performance at object recognition [11] and the Labeled Faces in the Wild (LFW) [16] dataset where CNNs have surpassed human level performance at face recognition [32].",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 28,
      "context" : "These include the ImageNet [30] dataset where CNNs have surpassed human level performance at object recognition [11] and the Labeled Faces in the Wild (LFW) [16] dataset where CNNs have surpassed human level performance at face recognition [32].",
      "startOffset" : 240,
      "endOffset" : 244
    }, {
      "referenceID" : 1,
      "context" : "A Boltzmann Machine (BM) is a recurrent neural network consisting of neurons that make binary stochastic decisions based on the states of their symmetrically connected neuron neighbors [2].",
      "startOffset" : 185,
      "endOffset" : 188
    }, {
      "referenceID" : 1,
      "context" : "A BM training algorithm was proposed in [2].",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 12,
      "context" : "The RBM network topology is restricted to a bipartite graph [14].",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 11,
      "context" : "Deep belief networks (DBNs) can be created by composing many RBM layers [13].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 8,
      "context" : "Computing using quantum computers was first discussed by Feynman [10] who was motivated by the fact that simulating a quantum system using a classical computer seems to be intractable.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 31,
      "context" : "Interest in quantum computing increased dramatically with the discovery of the Shor’s polynomial quantum algorithm for factoring numbers [35], because all known classical probabilistic factoring algorithms require exponential time.",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 7,
      "context" : "Several approaches to quantum computing were since developed, and they include the well-known quantum circuit model (used by Shor’s algorithm), the measurement-based quantum computing model, and the adiabatic quantum computing model [9].",
      "startOffset" : 233,
      "endOffset" : 236
    }, {
      "referenceID" : 12,
      "context" : "As stated above, the training of a BM is impractical on traditional computer systems, thus the development of a RBM restricts the network topology to that of a bipartite graphs [14].",
      "startOffset" : 177,
      "endOffset" : 181
    }, {
      "referenceID" : 4,
      "context" : "This leaves the question of how to best utilize thousands of GPUs for deep learning, as previous work has only utilized a maximum of 64 GPUs before encountering scaling problems when trying to exploit model parallelism to spread the weights of the network across multiple GPUs [6].",
      "startOffset" : 277,
      "endOffset" : 280
    }, {
      "referenceID" : 33,
      "context" : "Previously, it has been shown that HPC can be utilized to optimize the hyper-parameters of a deep learning network [37].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 3,
      "context" : "In developing neuromorphic computing devices for computational purposes, there have been two main approaches: building devices based on spiking neural networks (SNNs), such as IBM’s TrueNorth [5] or Darwin [34], and building devices based on convolutional neural networks, such as Google’s Tensor Processing Unit [21] or Nervana’s Nervana Engine [28], to serve as deep learning accelerators.",
      "startOffset" : 192,
      "endOffset" : 195
    }, {
      "referenceID" : 30,
      "context" : "In developing neuromorphic computing devices for computational purposes, there have been two main approaches: building devices based on spiking neural networks (SNNs), such as IBM’s TrueNorth [5] or Darwin [34], and building devices based on convolutional neural networks, such as Google’s Tensor Processing Unit [21] or Nervana’s Nervana Engine [28], to serve as deep learning accelerators.",
      "startOffset" : 206,
      "endOffset" : 210
    }, {
      "referenceID" : 18,
      "context" : "In developing neuromorphic computing devices for computational purposes, there have been two main approaches: building devices based on spiking neural networks (SNNs), such as IBM’s TrueNorth [5] or Darwin [34], and building devices based on convolutional neural networks, such as Google’s Tensor Processing Unit [21] or Nervana’s Nervana Engine [28], to serve as deep learning accelerators.",
      "startOffset" : 313,
      "endOffset" : 317
    }, {
      "referenceID" : 6,
      "context" : "for neuromorphic computers has been to train a CNN offline and then create a mapping process from the CNN to the associated SNN-based neuromorphic hardware [8].",
      "startOffset" : 156,
      "endOffset" : 159
    }, {
      "referenceID" : 32,
      "context" : "Likewise, when no voltage is applied across a memristor, the most recent resistance value is retained [36].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 17,
      "context" : "Memristors have similar behavior to biological synapses, and as such, have been frequently utilized to implement neuromorphic systems [19, 22, 29].",
      "startOffset" : 134,
      "endOffset" : 146
    }, {
      "referenceID" : 19,
      "context" : "Memristors have similar behavior to biological synapses, and as such, have been frequently utilized to implement neuromorphic systems [19, 22, 29].",
      "startOffset" : 134,
      "endOffset" : 146
    }, {
      "referenceID" : 25,
      "context" : "Memristors have similar behavior to biological synapses, and as such, have been frequently utilized to implement neuromorphic systems [19, 22, 29].",
      "startOffset" : 134,
      "endOffset" : 146
    }, {
      "referenceID" : 21,
      "context" : "MNIST is a collection of hand-written digits that has been very widely studied in the deep learning community [24].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 21,
      "context" : "First, we will create and train a RBM on D-Wave, applying it to the MNIST handwritten digit classification problem [24] to establish a reference result.",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 0,
      "context" : "Each 28x28 image is represented by a vector of 784 length, each unit with holding a value representing pixel intensity in the range [0, 1].",
      "startOffset" : 132,
      "endOffset" : 138
    }, {
      "referenceID" : 20,
      "context" : "The network architecture utilized was LeNet [23].",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 29,
      "context" : "The neuromorphic system we will use to explore the MNIST problem is a memristive implementation of the neuroscience-inspired dynamic architectures (NIDA) system [33].",
      "startOffset" : 161,
      "endOffset" : 165
    }, {
      "referenceID" : 5,
      "context" : "A digital hardware implementation based on NIDA, called Dynamic Adaptive Neural Network Array (DANNA), has also been created and is currently implemented on FPGA with a digital VLSI implementation in progress [7].",
      "startOffset" : 209,
      "endOffset" : 212
    }, {
      "referenceID" : 29,
      "context" : "The EA approach for training networks for the MNIST problem was previously applied to the NIDA SNN [33] and to DANNA [7].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 5,
      "context" : "The EA approach for training networks for the MNIST problem was previously applied to the NIDA SNN [33] and to DANNA [7].",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 10,
      "context" : "We utilize common parameters to control the learning progress, the same ones found in training RBMs [12].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 21,
      "context" : "We used the Titan computer and the MENNDL system to discover a near optimal topology of a deep network trained on the MNIST handwritten digit dataset [24] by utilizing the method presented in [37].",
      "startOffset" : 150,
      "endOffset" : 154
    }, {
      "referenceID" : 33,
      "context" : "We used the Titan computer and the MENNDL system to discover a near optimal topology of a deep network trained on the MNIST handwritten digit dataset [24] by utilizing the method presented in [37].",
      "startOffset" : 192,
      "endOffset" : 196
    }, {
      "referenceID" : 2,
      "context" : "Such characteristics for LRS, HRS and the associated on-off ratio have been observed for a range of memristive devices, including hafnium-oxide (HfO2)[4], tantalum-oxide (TaO2), and titanium-oxide (TiO2), to name a few.",
      "startOffset" : 150,
      "endOffset" : 153
    }, {
      "referenceID" : 22,
      "context" : "43mW which is consistent with similar memristor-based neuromorphic systems [25].",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 23,
      "context" : "Research has also shown that memristive neuromorphic systems are typically 20× more energy-efficient than their CMOS counterparts [26], and our results are consistent with this estimation.",
      "startOffset" : 130,
      "endOffset" : 134
    } ],
    "year" : 2017,
    "abstractText" : "Current Deep Learning approaches have been very successful using convolutional neural networks (CNN) trained on large graphical processing units (GPU)-based computers. Three limitations of this approach are: 1) they are based on a simple layered network topology, i.e., highly connected layers, without intra-layer connections; 2) the networks are manually configured to achieve optimal results, and 3) the implementation of neuron model is expensive in both cost and power. In this paper, we evaluate deep learning models using three different computing architectures to address these problems: quantum computing to train complex topologies, high performance computing (HPC) to automatically determine network topology, and neuromorphic computing for a low-power hardware implementation. We use the MNIST dataset for our experiment, due to input size limitations of current quantum computers. Our results show the feasibility of using the three architectures in tandem to address the above deep learning limitations. We show a quantum computer can find high quality values of intra-layer connections weights, in a tractable time as the complexity of the network increases; a high performance computer can find optimal layer-based topologies; and a neuromorphic computer can represent the complex topology and weights derived from the other architectures in low power memristive hardware. Notice: This manuscript has been authored by UT-Battelle, LLC under Contract No. DE-AC05-00OR22725 with the U.S. Department of Energy. The United States Government retains and the publisher, by accepting the article for publication, acknowledges that the United States Government retains a non-exclusive, paid-up, irrevocable, world-wide license to publish or reproduce the published form of this manuscript, or allow others to do so, for United States Government purposes. The Department of Energy will provide public access to these results of federally sponsored research in accordance with the DOE Public Access Plan (http://energy.gov/downloads/doe-public-access-plan). 1 ar X iv :1 70 3. 05 36 4v 1 [ cs .N E ] 1 5 M ar 2 01 7",
    "creator" : "LaTeX with hyperref package"
  }
}