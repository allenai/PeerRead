{
  "name" : "1603.03714.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Distribution Free Learning with Local Queries∗",
    "authors" : [ "Galit Bary-Weisberg", "Amit Daniely", "Shai Shalev-Shwartz" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "We continue this line of work, proving both positive and negative results in the distribution free setting. We restrict to the boolean cube {−1, 1}n, and say that a query is q-local if it is of a hamming distance ≤ q from some training example. On the positive side, we show that 1-local queries already give an additional strength, and allow to learn a certain type of DNF formulas. On the negative side, we show that even ( n0.99 ) -local queries cannot help to learn various classes including Automata, DNFs and more. Likewise, q-local queries for any constant q cannot help to learn Juntas, Decision Trees, Sparse Polynomials and more. Moreover, for these classes, an algorithm that uses ( log0.99(n) ) -local queries would lead to a breakthrough in the best known running times.\n∗This paper is based on the M.Sc. thesis [6] of the first author. The thesis offers a more elaborated discussion, as well as experiments. †Matific inc. Most work was done while the author was an M.Sc. student at the Hebrew University, Jerusalem, Israel ‡Google inc. Most work was done while the author was a Ph.D. student at the Hebrew University, Jerusalem, Israel §School of Computer Science and Engineering, The Hebrew University, Jerusalem, Israel\nar X\niv :1\n60 3.\n03 71\n4v 1\n[ cs\n.L G\n] 1\n1 M\nar 2"
    }, {
      "heading" : "1 Introduction",
      "text" : "A child typically learns to recognize a cat based on two types of input. The first is given by her parents, pointing at a cat and saying “Look, a cat!”. The second is given as a response to the child’s frequent question “What is that?”. These two types of input were the basis for the learning model originally suggested by Valiant [21]. Indeed, in Valiant’s model, the learner can randomly sample labelled examples from “nature”, but it can also make a membership query (MQ) for the label of any unseen example. Today, the acronym PAC stands for the restricted model in which MQ are forbidden, while the full model is called PAC+MQ. Much work has been done investigating the limits and the strengths of MQ. In particular, membership queries were proven stronger than the vanilla PAC model [1, 10, 12, 16]. Yet, MQ are rarely used in practice. This is commonly attributed to the fact that MQ algorithms query very artificial examples, that are uninterpretable by humans (e.g., [8]).\nAwasthi et al. [5] suggested a solution to the problem of unnatural examples. They considered a mid-way model that allows algorithms to make only local queries, i.e., query examples that are close to examples from the sample set. Hopefully, examples which are similar to natural examples will appear natural to humans. Awasti et al. considered the case where the instance space is {−1, 1}n and the distance between examples is the Hamming distance. They proved positive results on learning sparse polynomials with O(log(n))-local queries under what they defined as locally smooth distributions1. They also proposed an algorithm that learns DNF formulas under the uniform distribution in quasi-polynomial time using O(log(n))-local queries.\nOur work follows Awasthi et al. and investigates local queries in the distribution free setting, in which no explicit assumptions are made on the underlying distribution, but only on the learned hypothesis. We prove both positive and negative results in this context:\n• One of the strongest and most beautiful results in the MQ model shows that automata are learnable with membership queries [1]. We show that unfortunately, this is probably not the case with local queries. Concretely, we show that even (n0.99)-local queries cannot help to learn automata. Namely, such an algorithm will imply a standard PAC algorithm for automata. As learning automata is hard under several assumptions [17, 13], our result suggests that it is hard to learn automata even with (n0.99)-local queries.\n• We prove a similar result for several additional classes. Namely, we show that (n0.99)local queries cannot help to learn DNFs, intersection of halfspaces, decision lists, depthd circuits for any d ≥ 2, and depth-d threshold circuits for any d ≥ 2. Likewise, for any constant q, q-local queries cannot help to learn Juntas, Decision Trees, Sparse polynomials, and Sparse polynomial threshold functions. In fact, we show that even( log0.99(n) ) -local queries are unlikely to lead to polynomial time algorithms. Namely,\nany algorithm that uses ( log0.99(n) ) -local queries will result with a PAC algorithm whose running time significantly improves the state of the art in these well studied problems.\n• On the positive side we show that already 1-local queries are probably stronger than the vanilla PAC model. Concretely, we show that a certain simplistic but natural\n1A distribution is locally α-smooth for α ≥ 1 if its density function is log(α)-Lipschitz.\nlearning problem, which we term learning DNFs with evident examples, is learnable with 1-local queries. Furthermore, we show that without queries, this problem is at least as hard as learning decision trees, that are conjectured to be hard to learn."
    }, {
      "heading" : "2 Previous Work",
      "text" : "Membership Queries Several concept classes are known to be learnable only if membership queries are allowed: Deterministic Finite Automatons [1], k-term DNF for k =\nlog(n) log(log(n)) [10], decision trees and k-almost monotone-DNF formulas [12], intersections of khalfspaces [7] and DNF formulas under the uniform distribution [16]. The last result builds on Freund’s boosting algorithm [15] and the Fourier-based technique for learning using membership queries due to [18]. We note that there are cases in which MQ do not help. E.g., in the case of learning DNF and CNF formulas [2], assuming that one way functions exist, and in the case of distribution free agnostic learning [14].\nLocal Membership Queries Awasthi et al. focused on learning with O(log(n))-local queries. They showed that t-sparse polynomials are learnable under locally smooth distributions using O (log(n) + log(t))-local queries, and that DNF formulas are learnable under the uniform distribution in quasi-polynomial time (nO(log logn)) using O(log(n))-local queries. They also presented some results regarding the strength of local MQ. They proved that under standard cryptographic assumptions, (r+1)-local queries are more powerful than r-local queries (for every 1 ≤ r ≤ n − 1). They also showed that local queries do not always help. They showed that if a concept class is agnostically learnable under the uniform distribution using k-local queries (for constant k) then it is also agnostically learnable (under the uniform distribution) in the PAC model.\nWe remark that besides local queries, there were additional suggestions to solve the problem of unnatural examples. For example, to allow “I don’t know” answers, or to be tolerant to some incorrect answers [3, 4, 11, 20, 9]."
    }, {
      "heading" : "3 Setting",
      "text" : "We consider binary classification where the instance space is X = Xn = {−1, 1}n and the label space is Y = {0, 1}. A learning problem is defined by a hypothesis class H ⊂ {0, 1}X . The learner receives a training set\nS = {(x1, h?(x1)), (x2, h?(x2)), . . . , (xm, h?(xm))} ∈ (X × Y)m\nwhere the xi’s are sampled i.i.d. from some unknown distribution D on X and h? : X → Y is some unknown hypothesis. The learner is also allowed to make membership queries. Namely, to call an oracle, which receives as input some x ∈ X and returns h?(x). We say that a membership query for x ∈ X is q-local if there exists a training example xi whose Hamming distance from x is at most q.\nThe learner returns (a description of) a hypothesis ĥ : X → Y . The goal is to approximate h?, namely to find ĥ : X → Y with loss as small as possible, where the loss is defined as\nLD,h?(ĥ) = Prx∼D\n( ĥ(x) 6= h?(x) ) . We will focus on the so-called realizable case where h? is\nassumed to be in H, and will require algorithms to return a hypothesis with loss < in time that is polynomial in n and 1 . Concretely,\nDefinition 3.1 (Membership-Query Learning Algorithm) We say that a learning algorithm A learns H with q-local membership queries if\n• There exists a function mA (n, ) ≤ poly ( n, 1 ) , such that for every distribution D over\nX , every h? ∈ H and every > 0, if A is given access to q-local membership queries, and a training sequence\nS = {(x1, h?(x1)), (x2, h?(x2)), . . . , (xm, h?(xm))}\nwhere the xi’s are sampled i.i.d. from D and m ≥ mA(n, ), then with probability of at least2 3\n4 (over the choice of S), the output ĥ of A satisfies LD,h?(ĥ) < .\n• Given a training set of size m\n– A asks at most poly(m,n) membership queries. – A runs in time poly(m,n). – The hypothesis returned by A can be evaluated in time poly(m,n).\nWe remark that learning with 0-local queries is equivalent to PAC learning, while learning with n-local queries is equivalent to PAC+MQ learning."
    }, {
      "heading" : "4 Learning DNFs with Evident Examples",
      "text" : "Intuitively, when evaluating a DNF formula on a given example, we check a few conditions corresponding to the formula’s terms, and deem the example positive if one of them holds. We will consider the case that for each of these conditions, there is a chance to see a “prototype example”, that satisfies it in a strong or evident way. In the sequel, we denote by hF : {−1, 1}n → {0, 1} the function induced by a DNF formula F over n variables.\nDefinition 4.1 Let F = T1 ∨ T2 ∨ . . . ∨ Td be a DNF formula. We say that an example x ∈ {−1, 1}n satisfies a term Ti (with respect to the formula F ) evidently and denote Ti(x) ≡ 1 if :\n• It satisfies Ti. (In particular, hF (x) = 1)\n• It does not satisfy any other term Tk (for k 6= i) from F.\n• No coordinate change will turn Ti False and another term Tk True. Concretely, if for j ∈ [n] we denote x⊕j = (x1, . . . , xj−1,−xj, xj+1, . . . , xn), then for every coordinate j ∈ [n], if x⊕j satisfies F (i.e. if hF (x⊕j) = 1) then x⊕j satisfies Ti and only Ti.\n2The success probability can be amplified to 1− δ by repetition.\nDefinition 4.2 Let F = T1 ∨ T2 ∨ . . .∨ Td be a DNF formula. We say that h? : {−1, 1}n → {0, 1} is realized by F with evident examples w.r.t. a distribution D if h? = hF and for every term3 Ti, Prx∼D (Ti(x) ≡ 1|Ti(x) = 1) ≥ 1n .\nFor example, the definition holds whenever h? is realized by a DNF in which any pair of different terms contains two opposite literals. Also, functions that are realized by a decision tree can be also realized by a DNF in which every positive example satisfies a single term, corresponding to root-to-leaf path that ends with a leaf labelled by 1. Hence, the assumption holds for decision trees provided that for every such path, there is a chance to see an example satisfying it evidently, in the sense that flipping each variable in the path will turn the example negative."
    }, {
      "heading" : "4.1 An algorithm",
      "text" : "Algorithm 1 Input: S1, S2 ∈ ({−1, 1}n × {0, 1})m Output: A DNF formula H\nStart with an empty DNF formula H for all positive examples (x, y) ∈ S1 do\nDefine T = x1 ∧ x1 ∧ x2 ∧ x2 ∧ . . . ∧ xn ∧ xn for 1 ≤ j ≤ n do\nQuery x⊕j to get h?(x⊕j) if h?(x⊕j) = 1 then\nRemove xj and xj from T if h?(x⊕j) = 0 then\nif xj = 1 then Remove xj from T if xj = 0 then Remove xj from T\nH = H ∨ T for all T in H do\nif T (x) = 1 but y = 0 for some (x, y) ∈ S2 then Remove T from H\nReturn H\nTheorem 4.3 Algorithm 1 learns with 1-local-queries poly-sized DNFs with evident examples.\nIdea The algorithm is based on the following claim that follows easily from definition 4.1.\n3We note that the quantity 1n in the following equation is arbitrary, and can be replaced by 1 nc for any\nc > 0.\nClaim 1 Let F = T1∨T2∨ . . .∨Td be a DNF formula over {−1, 1}n. For every x ∈ {−1, 1}n that satisfies a term Ti evidently (with respect to F ), and for every j ∈ [n] it holds that:\nhF (x ⊕j) = 1⇐⇒ the term Ti does not contain the variable xj\nBy this claim, if x is an evident example for a certain term T , one can easily reconstruct T . Indeed, by flipping the value of each variable and checking if the label changes, one can infer which variables appear in T . Furthermore, the sign of these variables can be inferred from their sign in x. Hence, after seeing an evident example for all terms, one can have a list of terms containing all terms in the DNF. This list might have terms that are not part of the DNF. Yet, such terms can be thrown away later by testing if they make wrong predictions.\nProof (of Theorem 4.3) We will show that algorithm 1 learns with 1-local-queries any function that is realized by a DNF of size ≤ n2 with evident examples. Adapting the proof to hold with nc instead of n2, for any c > 0, is straight-forward. First, it is easy to see that this algorithm is efficient. Now, fix a distribution D and let h? : {−1, 1}n → {0, 1} be a hypothesis that is realized, w.r.t. D, by a DNF formula F = T1 ∨ T2 ∨ . . . ∨ Td, d ≤ n2 with evident examples. Let > 0, and suppose we run the algorithm on two indepent samples from D, denoted S1 = {(xi, h?(xi)}m1i=1 and S2 = {(x′i, h?(x′i)} m2 i=1. We will show that if m1 ≥ 32n 3 log 32n 2 ≥ 32nd log 32d and m2 ≥ 32m1 log 32m1\nthen with probability of at least 3 4 , the algorithm will return a function ĥ with LD,h?(ĥ) < . Let H be the DNF formula returned by the algorithm, and let ĥ be the function induced by H. We have that\nLD,h?(ĥ) = Pr x∼D\n( h?(x) 6= ĥ(x) ) = Pr\nx∼D\n( h?(x) = 1 and ĥ(x) = 0 ) + Pr\nx∼D\n( h?(x) = 0 and ĥ(x) = 1 ) The proof will now follow from claims 2 and 3.\nClaim 2 With probability at least 7 8 over the choice of S1, S2 we have\nPr x∼D\n( h?(x) = 1 and ĥ(x) = 0 ) ≤\n2 (1)\nProof We first note that if there is an evident example x for a term Ti in S1, then Ti will be in the output formula. Indeed, in the for-loop that go over the examples in S1, when processing the example (x, h?(x)), it is not hard to see that Ti will be added. We furthermore claim that the term Ti won’t be removed at the for loop that tests the terms collected in the first loop. Indeed, if for some (x, h?(x)) ∈ S2 we have Ti(x) = 1, it must be the case that h?(x) = 1. Now, we say that the term Ti is revealed if we see an evident example for this term in S1. We also denote pi = Prx∼D (Ti(x) = 1). We have\nPr x∼D\n( h?(x) = 1 and ĥ(x) = 0 ) ≤ d∑ i=1 Pr x∼D ( Ti(x) = 1 and ĥ(x) = 0 ) ≤\n∑ i:Ti is not revealed pi\nNow, by the assumption that h? is realized with evident queries, the probability (over the choice of S1, S2) that Ti is not revealed is at most ( 1− pi\nn )m1 . Hence, if we denote by Ai the event that Ti is not revealed, we have\nE S1∼Dm1 [ Pr x∼D ( h?(x) = 1 and ĥ(x) = 0 )] ≤ ES1∼Dm1 [ d∑ i=1 pi · 1Ai ]\n= d∑ i=1 piES1∼Dm1 [1Ai ]\n= d∑ i=1 piPrS1∼Dm1 [Ai]\n= d∑ i=1 pi ( 1− pi n )m1 =\n∑ i|pi< 32d pi ( 1− pi n )m1 + ∑ i|pi≥ 32d pi ( 1− pi n )m1 ≤\n∑ i|pi< 32d 32d + ∑ i|pi≥ 32d ( 1− pi n )m1 ≤ d ·\n32d + ∑ i|pi≥ 32d e− m1pi n\n≤ 32\n+ d · e− m1 32dn\nSince m1 ≥ 32dn log 32d\nthe last expression is bounded by 16 . By Markov’s inequality we conclude that the probabilty over the coice of S1, S2 that (1) does not lold is less than 1 8 . 2\nClaim 3 With probability at least 7 8 over the choice of S1, S2 we have\nPr x∼D\n( h?(x) = 0 and ĥ(x) = 1 ) ≤\n2 (2)\nProof Let T̂1, . . . , T̂r be the terms that were added to H at the first for-loop. Denote\nqi = Prx∼D ( T̂i(x) = 1 and h ?(x) = 0 ) . We have\nPr x∼D\n( h?(x) = 0 and ĥ(x) = 1 ) ≤ ∑ i : T̂i is not removed qi\nNow, the probability that T̂i is not removed is (1− qi)m2 . Hence, using an argument similar to the one used in the proof of claim 2, and since m2 ≥ 32m1 log 32m1 ≥ 32r log 32r , the claim follows. 2 2"
    }, {
      "heading" : "4.2 A matching Lower Bound",
      "text" : "In this section we provide evidence that the use of queries in our algorithm is crucial. We will show that the problem of learning poly-sized decision trees can be reduced to the problem of learning DNFs with evident examples. As learning decision trees is conjectured to be intractable, this reduction serves as an indication that learning DNFs with strongly evident examples is hard without membership queries. In fact, we will show that learning decision trees can be even reduced to the case that all positive examples are evident.\nTheorem 4.4 Learning poly-sized DNFs with evident examples is as hard as learning polysized decision trees.\nWe denote by hT the function induced by a decision tree T . The proof will use the following claim:\nClaim 4 There exists a mapping (a reduction) ϕ : {−1, 1}n → {−1, 1}2n, that can be evaluated in poly(n) time so that for every decision tree T over {−1, 1}n there exists a DNF formula F over {−1, 1}2n such that the following holds:\n1. The number of terms in F is upper bounded by the number of leaves in T\n2. hT = hF ◦ ϕ\n3. ∀x such that hT (x) = 1 , ϕ(x) satisfies some term in F evidently.\nProof Define ϕ as follows:\n∀x = (x1, x2, . . . , xn) ∈ Xn ϕ(x1, x2, . . . , xn) = (x1, x1, x2, x2, . . . , xn, xn)\nNow, for every tree T , we will build the desired DNF formula F as follows: First we build a DNF formula F ′ over {−1, 1}n . Every leaf labeled ’1’ in T will define the following termtake the path from the root to that leaf and form the logical AND of the literals describing the path. F ′ will be a disjunction of these terms. Now, for every term T in F ′ we will define a term φ(T ) over X2n in the following way: Let PT = {i ∈ [n] : xi appear in T} and NT = {i ∈ [n] : xi appear in T}. So\nT = ∧ j∈PT xj ∧ j∈NT xj\nDefine\nφ(T ) = ∧ j∈PT x2j−1 ∧ j∈PT x2j ∧ j∈NT x2j−1 ∧ j∈NT x2j\nFinally, define F to be the DNF formula over X2n given by F = ∨ T∈F ′ φ(T )\nWe will now prove that ϕ and F satisfy the required conditions. First, ϕ can be evaluated in linear time in n. Second, it is easy to see that hT = hF ◦ϕ, and as every term in F matches one of T ’s leaves, the number of terms in F cannot exceed the number of leaves in T . It is left to show that the third requirement holds. Let there be an x such that hT (x) = 1, then x is matched to one and only one path from T ’s root to a leaf labeled ’1’. From the construction of F , x satisfies one and only one term in F ′. Regarding the last requirement, that no coordinate change will make one term from F False and another one True, we made sure this will not happen by “doubling” each variable. By this construction, in order to change a term from False to True at least two coordinate must change their value. 2\nWe are now ready to prove theorem 4.4.\nProof [of theorem 4.4] Suppose that A learns size-n DNFs with evident examples. Using the reduction from claim 4 we will build an efficient algorithm B that learns size-n decision trees. For every training set\nS = {(x1, h?(x1)), (x2, h?(x2)), . . . , (xm, h?(xm))} ∈ (Xn × {0, 1})m\nwe define\nϕ(S) := {(ϕ(x1), h?(x1)), (ϕ(x2)), h?(x2)), . . . , (ϕ(xm), h?(xm))} ∈ (X2n × {0, 1})m\nThe algorithm B will work as follows: Given a training set S, B will run A on ϕ(S), and will return ĥ ◦ ϕ, where ĥ is the hypothesis returned by A. Since ϕ can be evaluated in poly(n) time and A is efficient, B is also efficient. We will prove that B learns size-n trees. Since A learns size-n DNFs with evident examples, there exists a function mA (n, ) ≤ poly ( n, 1 ) , such that if A is given a training sequence\nS = {(x1, h?(x1)), (x2, h?(x2)), . . . , (xm, h?(xm))} ∈ (Xn × {0, 1})m\nwhere the xi’s are sampled i.i.d. from a distribution D, h? is realized by a poly-sized DNF with evident examples, and m ≥ mA(n, ), then with probability of at least 34 (over the choice of S), the output ĥ of A satisfies LD,h?(ĥ) ≤ . Let D be a distribution on Xn and let hT be a hypothesis that can be realized by a tree with ≤ n leafs. Define a distribution D̃ on X2n by,\n˜(D)(z) = { D(x) if ∃x ∈ Xn such that z = ϕ(x) 0 otherwise\nNow, since hT is realized by T , from the conditions that ϕ satisfies, we get that hT = h ◦ϕ, where h is realized by a DNF of size ≤ n with evident examples w.r.t. D̃. Now if S ∈ (Xn × {0, 1})m is an i.i.d. sample with m ≥ mA(2n, ) we have that with probability of at least 3\n4 it holds that\nLD,hT (B(S)) = LD,hT (ĥ ◦ ϕ) = Pr\nx∼D [hT (x) 6= ĥ ◦ ϕ(x)]\n= Pr x∼D\n[h ◦ ϕ(x) 6= ĥ ◦ ϕ(x)]\n= Pr z∼D̃\n[h(z) 6= ĥ(z)]\n= LD̃,h(ĥ) = LD̃,h(A(ϕ(S))) <\n2"
    }, {
      "heading" : "5 Lower Bounds",
      "text" : "We first present a general technique to prove lower bounds on learning with local queries. For A ⊂ Xn and q > 0 denote\nB(A, q) = {x ∈ Xn | ∃a ∈ A, d(x, a) ≤ q}\nWe say that a mapping ϕ : {−1, 1}n → {−1, 1}n′ is a q-reduction of type A from a class H to a class H′ if the following holds:\n1. ϕ is efficiently computable.\n2. For every h ∈ H there is h′ ∈ H′ such that\n(a) h = h′ ◦ ϕ (b) The restriction of h′ to B(ϕ(Xn), q) \\ ϕ(Xn) is the constant function 1.\nWe say that a mapping ϕ : {−1, 1}n → {−1, 1}n′ is a q-reduction of type B if the same requirements hold, except that (2b) is replaced by the following condition: For every z ∈ B(ϕ(Xn), q) there is a unique x ∈ Xn satisfying d(z, ϕ(x)) ≤ q, and furthermore, h′(z) = h(x).\nLemma 5.1 Suppose that there is a q-reduction ϕ from H to H′. Then, learning H′ with q-local queries is as hard as PAC learning H.\nProof (sketch) Suppose that A′ learns H′ with q-local queries. We need to show that there is an algorithm that learns H. Indeed, by an argument similar to the one in theorem 4.4, it is not hard to verify that the following algorithm learns H. Given a sample {(x1, y1), . . . , (xm, ym)} ⊂ {−1, 1}n × {0, 1}, run A′ on the sample {(ϕ(x1), y1), . . . , (ϕ(xm), ym)} ⊂ {−1, 1}n\n′ × {0, 1}. Whenever A′ makes a q-local query for z ∈ {−1, 1}n′ , respond 1 if ϕ is of type A. If ϕ is of type B, respond yi, where xi is the unique training sample satisfying d(z, ϕ(xi)) ≤ q. Finally, if A′ returned the hypothesis h′, return h′ ◦ ϕ. 2\nWe next use Lemma 5.1 to prove that for several classes, local queries are not useful. Namely, if the class is learnable with local queries then it is also learnable without queries. We will use the following terminology. We say that q-local queries cannot help to learn a class H if H is learnable if and only if it is learnable with q-local queries.\nCorollary 5.2 For every 0 > 0, (n 1− 0)-local queries cannot help to learn poly-sized DNFs, intersection of halfspaces, decision lists, depth-d circuits for any d = d(n) ≥ 2, and depth-d threshold circuits for any d = d(n) ≥ 2.\nProof We will only prove the corollary for DNFs. The proofs for the remaining classes are similar. Also, for simplicity, we will assume that 0 = 1 3 . Consider the mapping ϕ : {−1, 1}n → {−1, 1}n3 that replicates each coordinate n2 times. To establish the corollary, we show that ϕ is an ( (n′) 2 3 − 1 ) -reduction of type A from poly-sized DNF to poly-sized\nDNF. Indeed, let F = T1 ∨ . . . ∨ Td be a DNF formula on n variables, consider the following formula on the n3 variables {xi,j}1≤i≤n,1≤j≤n2 . Let,\nT ′t({xi,j}i,j) = Tt(x1,1, . . . , xn,1) G′({xi,j}i,j) = ∨ni=1 ∨n 2−1 j=1 (xi,j ∧ ¬xi,j+1) ∨ (xi,j+1 ∧ ¬xi,j)\nF ′ = (T ′1 ∨ . . . ∨ T ′d) ∨G′\nIt is not hard to verify that hF = hF ′◦ϕ. Moreover, if x = {xi,j}i,j ∈ B(ϕ(Xn), n2−1)\\ϕ(Xn) then xi,j 6= xi,j+1 for some i, j, meaning that hG(x) = 1 and therefore also hF (x) = 1. 2\nCorollary 5.3 For every 0 > 0, (n 1− 0)-local queries cannot help to learn poly-sized automata.\nProof Again, for simplicity, we assume that 0 = 1 3 , and consider the mapping ϕ : {−1, 1}n → {−1, 1}n3 that replicates each coordinate n2 times. To establish the corollary, we show that ϕ is an ( (n′) 2 3 − 1 ) -reduction of type A from poly-sized automata to\npoly-sized automata. Indeed, let A be an automaton on n variables. It is enough to show that there is an automaton A′ on the n3 variables {xi,j}1≤i≤n,1≤j≤n2 such that (i) the size of A′ is polynomial, (ii) A′ accepts any string in B(ϕ(Xn), n2− 1) \\ϕ(Xn), and (iii) A′ accepts ϕ(x) if and only if A accepts x. Now, by the product construction of automatons [19], if A′1, A ′ 2 are automata that induce the functions hA′1 , hA′2 : {−1, 1} n3 → {0, 1}, then the function hA′1 ∨ hA′2 can be induced by an automaton of size |A′1| · |A′2|. Hence, it is enough to show that there are poly-sized automata A′1, A ′ 2 that satisfies (ii) and (iii) respectively.\nA construction of a size O(n2) automaton that satisfies (ii) is a simple exercise. We next explain how to construct a poly-sized automaton A′2 satisfying (iii). The states of A ′ 2 will be the S(A) × [n2] (here, S(A) denotes the set of states of A). The start state of A′2 will be (α0, 1), where α0 is the start state of A, and the accept states of A ′ 2 will be the cartesian product of the accept states of A with [n2]. Finally if δ : S(A) × {−1, 1} → S(A) is the transition function of A, then the transition function of A′2 is defined by\nδ′((α, i), b) =\n{ (α, i+ 1) 1 ≤ i < n2\n(δ(α, b), 1) i = n2\nIt is not hard to verify that A′2 satisfies (iii). 2 In the next corollary, a Junta of size t is a function h : {−1, 1}n → {0, 1} that depends on ≤ log(t) variables. The rational behind this definition of size, is the fact that in order to describe a general function that depends on K variables, at least 2K bits are required. Likewise, the sample complexity of learning Juntas is proportional to t rather than log(t)\nCorollary 5.4 For every constant q0 > 0, q0-local queries cannot help to learn poly-sized Juntas.\nProof Consider the mapping ϕ : {−1, 1}n → {−1, 1}(2q0+1)n that replicates each coordinate 2q0 + 1 times. To establish the corollary, we show that ϕ is a q0-reduction of type B from poly-sized Juntas to poly-sized Juntas. Indeed, let h : {−1, 1}n → {0, 1} be a function that depends on K variables. It is enough to show that there is a function h′ on the variables {zi,j}i∈[n],j∈[2q0+1] that satisfies (i) h = h′ ◦ ϕ, (ii) for every x ∈ X , if z ∈ {−1, 1}(2q0+1)n is obtained from ϕ(x) by modifying ≤ q0 coordinates, then h′(z) = h′(ϕ(x)) and (iii) h′ depends on (2q0 + 1)K variables. It is not hard to check that the following function satisfies these requirements:\nh′(z) = h (MAJ(z1,1, . . . , z1,2q0+1), . . . ,MAJ(zn,1, . . . , zn,2q0+1))\n2\nWe remark that by taking q0 = log 1− 0(n) for some 0 > 0, and using a similar argument, it can be shown that an efficient algorithm for learning poly-sized Juntas with ( log1− 0(n) ) -local queries would imply a PAC algorithm for poly-sized Juntas that runs in time nO(log 1− 0 (n)).\nCorollary 5.5 For every constant q0 > 0, q0-local queries cannot help to learn poly-sized decision tress.\nProof As with Juntas, consider the mapping ϕ : {−1, 1}n → {−1, 1}(2q0+1)n that replicates each coordinate 2q0 + 1 times. To establish the corollary, we show that ϕ is a q0-reduction of type B from poly-sized decision trees to poly-sized decision trees. Indeed, let h : {−1, 1}n → {0, 1} be a function that is realized by a decision tree T . It is enough to show that there is a function h′ on the variables {zi,j}i∈[n],j∈[2q0+1] that satisfies (i) h = h′ ◦ ϕ, (ii) for every x ∈ X , if z ∈ {−1, 1}(2q0+1)n is obtained from ϕ(x) by modifying ≤ q0 coordinates, then h′(z) = h′(ϕ(x)) and (iii) h′ can be realized by a tree of size |T |2q0+1. As we explain, this will hold for the following function:\nh′(z) = MAJ (h(z1,1, . . . , zn,1), . . . , h(z1,2q0+1, . . . , zn,2q0+1))\nIt is not hard to check that h′ satisfies (i) and (ii). As for (iii), consider the following tree T ′. First replicate T on the variables z1,1, . . . , zn,1. Then, on the obtained tree, replace each leaf by a replica of T on the variables z1,2, . . . , zn,2. Then, again, replace each leaf by a replica of T on the variables z1,3, . . . , zn,3. Continues doing so, until the leafs are replaced by a replica on the variables z1,2q0+1, . . . , zn,2q0+1. Now, each leaf in the resulted tree corresponds to (2q0 + 1) root-to-leaf paths in the original tree T . The label of such leaf will be the majority of the labels of these paths. 2\nAs with Juntas, we remark that by taking q0 = log 1− 0(n) for some 0 > 0, and using a similar argument, it can be shown that an efficient algorithm for learning poly-sized trees with ( log1− 0(n) ) -local queries would imply a PAC algorithm for poly-sized trees that runs in time nO(log 1− 0 (n)).\nIn the sequel, we say that a function h : {−1, 1}n → {0, 1} is realized by a poly-sized polynomial, if it is the function induced by a polynomial with polynomially many nonzero coefficient and degree O (log(n)). A similar convention applies to the term poly-sized polynomial threshold function. We remark that the following corollary and its proof remain correct also if in our definition, we replace the number of coefficients with the `1 norm of the coefficients vector.\nCorollary 5.6 For every constant q0 > 0, q0-local queries cannot help to learn poly-sized polynomials, as well as poly-sized polynomial threshold functions.\nProof We will prove the corollary for polynomials. The proof for polynomial threshold functions is similar. Consider the mapping ϕ : {−1, 1}n → {−1, 1}(2q0+1)n that replicates each coordinate 2q0 + 1 times. To establish the corollary, we show that ϕ is a q0-reduction of type B from poly-sized polynomials to poly-sized polynomials. Indeed, let h : {−1, 1}n → {0, 1} be a poly-sized polynomial. It is enough to show that there is a poly-sized polynomial h′ on the variables {zi,j}i∈[n],j∈[2q0+1] that satisfies (i) h = h′ ◦ ϕ, and (ii) for every x ∈ X , if z ∈ {−1, 1}(2q0+1)n is obtained from ϕ(x) by modifying ≤ q0 coordinates, then h′(z) = h′(ϕ(x)). As we explain next, this will hold for the following polynomial:\nh′(z) = h (MAJ(z1,1, . . . , z1,2q0+1), . . . ,MAJ(zn,1, . . . , zn,2q0+1))\nIt is not hard to check that h′ satisfies (i) and (ii). It remains to show that h′ is poly-sized. Indeed, the majority function on 2q0 +1 coordinates is a polynomial of degree ≤ 2q0 +1 with at most 22q0+1 non zero coefficients (this is true for any function on 2q0 + 1 coordinates). Hence, if we replace each variable in h by a polynomial of the form MAJ(zi,1, . . . , zi,2q0+1), the degree is multiplied by at most (2q0 + 1), while the number of non zero coefficients is multiplied by at most 22q0+1. 2\nAs with Juntas and decision trees, by taking q0 = log 1− 0(n) for some 0 > 0, and using a similar argument, it can be shown that an efficient algorithm for learning poly-sized polynomials or polynomial threshold functions with ( log1− 0(n) ) -local queries would imply a PAC algorithm for the same problem that runs in time nO(log 1− 0 (n))."
    }, {
      "heading" : "6 Conclusion and Future Work",
      "text" : "We have shown that in the distribution free setting, for many hypothesis classes, local queries are not useful. As our proofs show, this stems from the fact that learning these classes without queries can be reduced to a case where local queries are pointless, in the sense that the answer to them is either always 1, or the label of the closest training example. On the other hand, the learning problem of DNFs with evident examples circumvents this property. Indeed, the underlying assumption enforces that local changes can change the label in a\nnon-trivial manner. While this assumption might be intuitive in some cases, it is certainly very restrictive. Therefore, a natural future research direction is to seek less restrictive assumptions, that still posses this property.\nMore concrete direction arising from our work concern classes for which we have shown that ( log0.99(n) ) -local queries are unlikely to lead to efficient algorithms. We conjecture that for some a > 0 even (na)-local queries won’t lead to efficient distribution free algorithms for these classes."
    } ],
    "references" : [ {
      "title" : "Learning regular sets from queries and counterexamples",
      "author" : [ "Dana Angluin" ],
      "venue" : "Information and computation,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1987
    }, {
      "title" : "When won t membership queries help",
      "author" : [ "Dana Angluin", "Michael Kharitonov" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1995
    }, {
      "title" : "Randomly fallible teachers: Learning monotone dnf with an incomplete membership oracle",
      "author" : [ "Dana Angluin", "Donna K Slonim" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1994
    }, {
      "title" : "Malicious omissions and errors in answers to membership queries",
      "author" : [ "Dana Angluin", "Mārtiņš Kriķis", "Robert H Sloan", "György Turán" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1997
    }, {
      "title" : "Learning using local membership queries",
      "author" : [ "Pranjal Awasthi", "Vitaly Feldman", "Varun Kanade" ],
      "venue" : "arXiv preprint arXiv:1211.0996,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2012
    }, {
      "title" : "Learning using 1-local membership queries",
      "author" : [ "Galit Bary" ],
      "venue" : "arXiv preprint arXiv:1512.00165,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2015
    }, {
      "title" : "Neural net algorithms that learn in polynomial time from examples and queries",
      "author" : [ "Eric B Baum" ],
      "venue" : "Neural Networks, IEEE Transactions on,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1991
    }, {
      "title" : "Query learning can work poorly when a human oracle is used",
      "author" : [ "Eric B Baum", "Kenneth Lang" ],
      "venue" : "In International Joint Conference on Neural Networks,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1992
    }, {
      "title" : "Learning with errors in answers to membership queries",
      "author" : [ "Laurence Bisht", "Nader H Bshouty", "Lawrance Khoury" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2008
    }, {
      "title" : "Fast learning of k-term dnf formulas with queries",
      "author" : [ "Avrim Blum", "Steven Rudich" ],
      "venue" : "In Proceedings of the twenty-fourth annual ACM symposium on Theory of computing,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1992
    }, {
      "title" : "Learning with unreliable boundary queries",
      "author" : [ "Avrim Blum", "Prasad Chalasani", "Sally A Goldman", "Donna K Slonim" ],
      "venue" : "In Proceedings of the eighth annual conference on Computational learning theory,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1995
    }, {
      "title" : "Exact learning boolean functions via the monotone theory",
      "author" : [ "Nader H Bshouty" ],
      "venue" : "Information and Computation,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1995
    }, {
      "title" : "Complexity theoretic limitations on learning dnf’s",
      "author" : [ "Amit Daniely", "Shai Shalev-Shwatz" ],
      "venue" : "arXiv preprint arXiv:1404.3378,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "On the power of membership queries in agnostic learning",
      "author" : [ "Vitaly Feldman" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2009
    }, {
      "title" : "Boosting a weak learning algorithm by majority",
      "author" : [ "Yoav Freund" ],
      "venue" : "Information and computation,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1995
    }, {
      "title" : "An efficient membership-query algorithm for learning dnf with respect to the uniform distribution",
      "author" : [ "Jeffrey Jackson" ],
      "venue" : "In Foundations of Computer Science,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1994
    }, {
      "title" : "Cryptographic limitations on learning boolean formulae and finite automata",
      "author" : [ "Michael Kearns", "Leslie Valiant" ],
      "venue" : "Journal of the ACM (JACM),",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1994
    }, {
      "title" : "Learning decision trees using the fourier spectrum",
      "author" : [ "Eyal Kushilevitz", "Yishay Mansour" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1993
    }, {
      "title" : "Learning with queries but incomplete information",
      "author" : [ "Robert H Sloan", "György Turán" ],
      "venue" : "In Proceedings of the seventh annual conference on Computational learning theory,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1994
    }, {
      "title" : "A theory of the learnable",
      "author" : [ "Leslie G Valiant" ],
      "venue" : "Communications of the ACM,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1984
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "[5], aims to facilitate practical use of membership queries.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "thesis [6] of the first author.",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 19,
      "context" : "These two types of input were the basis for the learning model originally suggested by Valiant [21].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 0,
      "context" : "In particular, membership queries were proven stronger than the vanilla PAC model [1, 10, 12, 16].",
      "startOffset" : 82,
      "endOffset" : 97
    }, {
      "referenceID" : 9,
      "context" : "In particular, membership queries were proven stronger than the vanilla PAC model [1, 10, 12, 16].",
      "startOffset" : 82,
      "endOffset" : 97
    }, {
      "referenceID" : 11,
      "context" : "In particular, membership queries were proven stronger than the vanilla PAC model [1, 10, 12, 16].",
      "startOffset" : 82,
      "endOffset" : 97
    }, {
      "referenceID" : 15,
      "context" : "In particular, membership queries were proven stronger than the vanilla PAC model [1, 10, 12, 16].",
      "startOffset" : 82,
      "endOffset" : 97
    }, {
      "referenceID" : 7,
      "context" : ", [8]).",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 4,
      "context" : "[5] suggested a solution to the problem of unnatural examples.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "We prove both positive and negative results in this context: • One of the strongest and most beautiful results in the MQ model shows that automata are learnable with membership queries [1].",
      "startOffset" : 185,
      "endOffset" : 188
    }, {
      "referenceID" : 16,
      "context" : "As learning automata is hard under several assumptions [17, 13], our result suggests that it is hard to learn automata even with (n)-local queries.",
      "startOffset" : 55,
      "endOffset" : 63
    }, {
      "referenceID" : 12,
      "context" : "As learning automata is hard under several assumptions [17, 13], our result suggests that it is hard to learn automata even with (n)-local queries.",
      "startOffset" : 55,
      "endOffset" : 63
    }, {
      "referenceID" : 0,
      "context" : "Membership Queries Several concept classes are known to be learnable only if membership queries are allowed: Deterministic Finite Automatons [1], k-term DNF for k = log(n) log(log(n)) [10], decision trees and k-almost monotone-DNF formulas [12], intersections of khalfspaces [7] and DNF formulas under the uniform distribution [16].",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 9,
      "context" : "Membership Queries Several concept classes are known to be learnable only if membership queries are allowed: Deterministic Finite Automatons [1], k-term DNF for k = log(n) log(log(n)) [10], decision trees and k-almost monotone-DNF formulas [12], intersections of khalfspaces [7] and DNF formulas under the uniform distribution [16].",
      "startOffset" : 184,
      "endOffset" : 188
    }, {
      "referenceID" : 11,
      "context" : "Membership Queries Several concept classes are known to be learnable only if membership queries are allowed: Deterministic Finite Automatons [1], k-term DNF for k = log(n) log(log(n)) [10], decision trees and k-almost monotone-DNF formulas [12], intersections of khalfspaces [7] and DNF formulas under the uniform distribution [16].",
      "startOffset" : 240,
      "endOffset" : 244
    }, {
      "referenceID" : 6,
      "context" : "Membership Queries Several concept classes are known to be learnable only if membership queries are allowed: Deterministic Finite Automatons [1], k-term DNF for k = log(n) log(log(n)) [10], decision trees and k-almost monotone-DNF formulas [12], intersections of khalfspaces [7] and DNF formulas under the uniform distribution [16].",
      "startOffset" : 275,
      "endOffset" : 278
    }, {
      "referenceID" : 15,
      "context" : "Membership Queries Several concept classes are known to be learnable only if membership queries are allowed: Deterministic Finite Automatons [1], k-term DNF for k = log(n) log(log(n)) [10], decision trees and k-almost monotone-DNF formulas [12], intersections of khalfspaces [7] and DNF formulas under the uniform distribution [16].",
      "startOffset" : 327,
      "endOffset" : 331
    }, {
      "referenceID" : 14,
      "context" : "The last result builds on Freund’s boosting algorithm [15] and the Fourier-based technique for learning using membership queries due to [18].",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 17,
      "context" : "The last result builds on Freund’s boosting algorithm [15] and the Fourier-based technique for learning using membership queries due to [18].",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 1,
      "context" : ", in the case of learning DNF and CNF formulas [2], assuming that one way functions exist, and in the case of distribution free agnostic learning [14].",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 13,
      "context" : ", in the case of learning DNF and CNF formulas [2], assuming that one way functions exist, and in the case of distribution free agnostic learning [14].",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 2,
      "context" : "For example, to allow “I don’t know” answers, or to be tolerant to some incorrect answers [3, 4, 11, 20, 9].",
      "startOffset" : 90,
      "endOffset" : 107
    }, {
      "referenceID" : 3,
      "context" : "For example, to allow “I don’t know” answers, or to be tolerant to some incorrect answers [3, 4, 11, 20, 9].",
      "startOffset" : 90,
      "endOffset" : 107
    }, {
      "referenceID" : 10,
      "context" : "For example, to allow “I don’t know” answers, or to be tolerant to some incorrect answers [3, 4, 11, 20, 9].",
      "startOffset" : 90,
      "endOffset" : 107
    }, {
      "referenceID" : 18,
      "context" : "For example, to allow “I don’t know” answers, or to be tolerant to some incorrect answers [3, 4, 11, 20, 9].",
      "startOffset" : 90,
      "endOffset" : 107
    }, {
      "referenceID" : 8,
      "context" : "For example, to allow “I don’t know” answers, or to be tolerant to some incorrect answers [3, 4, 11, 20, 9].",
      "startOffset" : 90,
      "endOffset" : 107
    } ],
    "year" : 2016,
    "abstractText" : "The model of learning with local membership queries interpolates between the PAC model and the membership queries model by allowing the learner to query the label of any example that is similar to an example in the training set. This model, recently proposed and studied by Awasthi et al. [5], aims to facilitate practical use of membership queries. We continue this line of work, proving both positive and negative results in the distribution free setting. We restrict to the boolean cube {−1, 1}n, and say that a query is q-local if it is of a hamming distance ≤ q from some training example. On the positive side, we show that 1-local queries already give an additional strength, and allow to learn a certain type of DNF formulas. On the negative side, we show that even ( n0.99 ) -local queries cannot help to learn various classes including Automata, DNFs and more. Likewise, q-local queries for any constant q cannot help to learn Juntas, Decision Trees, Sparse Polynomials and more. Moreover, for these classes, an algorithm that uses ( log(n) ) -local queries would lead to a breakthrough in the best known running times. ∗This paper is based on the M.Sc. thesis [6] of the first author. The thesis offers a more elaborated discussion, as well as experiments. †Matific inc. Most work was done while the author was an M.Sc. student at the Hebrew University, Jerusalem, Israel ‡Google inc. Most work was done while the author was a Ph.D. student at the Hebrew University, Jerusalem, Israel §School of Computer Science and Engineering, The Hebrew University, Jerusalem, Israel ar X iv :1 60 3. 03 71 4v 1 [ cs .L G ] 1 1 M ar 2 01 6",
    "creator" : "LaTeX with hyperref package"
  }
}