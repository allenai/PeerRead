{
  "name" : "1301.7364.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Query Expansion in Information Retrieval Systems using a Bayesian Network-Based Thesaurus",
    "authors" : [ "Luis M. de Campos", "Juan M. Fernandez", "Juan F. Huete" ],
    "emails" : [ "@decsai.", "hg@decsai." ],
    "sections" : [ {
      "heading" : null,
      "text" : "Information Retrieval (IR) is concerned with the identification of documents in a collec tion that are relevant to a given information need, usually represented as a query contain ing terms or keywords, which are supposed to be a good description of what the user is look ing for. IR systems may improve their effect iveness (i.e., increasing the number of relev ant documents retrieved) by using a process of query expansion, which automatically adds new terms to the original query posed by an user. In this paper we develop a method of query expansion based on Bayesian networks. Using a learning algorithm, we construct a Bayesian network that represents some of the relationships among the terms appearing in a given document collection; this network is then used as a thesaurus (specific for that col lection). We also report the results obtained by our method on three standard test collec tions.\n1 INTRODUCTION\nA good definition that establishes the whole process of Information Retrieval (IR) is given by G. Salton [25]: 'IR is concerned with representation, storage, organ isation, and accessing of information items'. When all these tasks are carried out by means of a computer, we will refer to 'automatic information retrieval' and we will define an Information Retrieval System (IRS) as the software that implements these tasks in a com puter. With respect to the meaning of an information item, we will only deal with documents, or in a broader sense, textual representations of any type of object, i.e. a research article, a book, a message in an electronic mail file, etc.\nIn this paper, we mainly focus our attention in the part\nof an IRS devoted to accessing to information items, i.e., the identification of documents in a collection that are relevant to a particular information need: an user interacts with the IRS by formulating a query, which is a description of his/her information need, getting, as a result, a set of documents which are intended to be the most suitable for the request. All IRSs draw con clusions about the content of a document (and there fore about the appropriateness of this document with respect to a given query) by examining some repres entation of that document, which consists of several document features, usually in the form of particular words and phrases which are intended as a good con cise description of the content of the document.\nTo solve the IR problem, a great number of retrieval models, i.e., specifications about how to represent doc uments and queries and how they are compared, have been developed, since it was set up in the 1940s. In [1, 8], the reader can find a very complete review of most of the techniques used in the IR field. It should be noted that the IR problem is pervaded with uncer tainty, most of the tasks involved in this area could be described as uncertain processes. In fact, the user's query is a vague description of his/her information need because, due to several reasons, he/she can not express in a precise way what he/she really wants. The query and document representation construction is an other example, because it gives as a result incomplete characterisations, in the form of keywords or terms, of the content of both queries and documents.\nFor any given retrieval model, there is a great vari ety of techniques designed to improve the effectiveness of the retrieval. Among them, we stand out the use of thesauri and query expansion. A typical thesaurus contains a set of words and the relationships among them. The user of the IRS can use this structure to build queries or improve them (in the sense of get ting new words to formulate the query), restricting the query if it retrieves too many documents, or broaden ing it if it gets too few ones. On the other hand, query\n54 de Campos, Fernandez, and Huete\nexpansion is the addition of new words to the query by the IRS, based on previously retrieved documents or on thesauri, having as an objective increasing the number of relevant documents retrieved.\nThe purpose of this research is to explore the possibil ities of using Bayesian networks to automatically con struct, for any given document collection, a thesaurus which may be used to improve the performance of an IRS by means of the query expansion technique (the thesaurus itself being a Bayesian network). More pre cisely, from the set of terms appearing in a document collection, we build a Bayesian network that represents some of the relationships among these terms using a learning algorithm. So, given a particular query, we instantiate the terms that compose it and propagate this information through the network. Then; by select ing the new terms whose posterior probability is high and adding them to the original query, we obtain the expanded query. In our experiments, we have used this method (on several standard test collections) as a pre processing step for a classic system in the IR context, and one of the most used experimental IRSs: Smart. It was proposed by Salton [25] and developed at the Cornell University.\nThe paper is divided into 5 sections. Section 2 contains a short review of several basic concepts and methods used in Information Retrieval, which can help non specialized readers to better understand the rest of the paper. In Section 3 we explain the learning algorithm used to construct the Bayesian network representing the thesaurus. Section 4 describes the experiments we have carried out with three standard test collections ( Adi, Cranfield and M edlars), as well as the results obtained. Finally, Section 5 contains the concluding remarks and some proposals for future research.\n2 PRELIMINARIES: INFORMATION RETRIEVAL BASICS\nFirst, we are going to describe the operation of an IRS. Given a set of documents in their original format, the first step is to translate each document to a suitable representation for a computer to use. That translation is called indexing, and the output of this process is a list of words, known as terms or keywords, extracted from the text and considered significant. Sometimes, the terms have associated a number which may rep resents the frequency of occurrence of that. term, or a more complex weight expressing the importance of the term. The IRS stores these representations instead of the complete documents, so for each document, a list of terms is stored. An inverted file· is a structure in which each term has a list of documents where it occurs.\nThe user expresses his/her information needs formulat ing a query, using a formal query language or natural language. That query is also indexed to get a query representation and the retrieval continues with the part of the process in which the query representation is matched with the stored document representations us ing a search strategy. Finally, a set of document identi fiers is presented to the user. In an operational system, the process stops here, but in an experimental one, the next step is to measure the effectiveness of the retrieval, i.e. IRS evaluation. That evaluation can be carried out with different methods, but the main one is that based on recall and precision estimations [21, 25]. The first one measures the ability of the IRS to present all the relevant documents (recall = number of relevant docu ments retrieved / number of relevant documents). The second one, precision, measures its ability to present only the relevant documents (precision = number of relevant documents retrieved / number of documents retrieved). To compare IRS performance in terms of these measures, the curve recall-precision is plotted for each query.\nThere are four classic retrieval models: Boolean, vec tor space, cluster and probabilistic models [21, 25]. As Boolean and cluster models are not directly related to our work, we shall briefly describe only vector space and probabilistic models. In the vector-space model, documents and queries are both represented as vectors of length n, being n the number of terms in the collec tion, each position containing a weight. Documents and queries are compared using a similarity function, the most common one being the cosine of the angle between the two vectors. Smart, the IRS we are go ing to use as the basic method, belongs to this kind of models. The probabilistic retrieval also represents the documents and the queries as vectors containing a probabilistic weight for each term, which expresses the degree of importance of that term. These models compute the probabj}ity of relevance given a document and a query (the probability that a document satisfies a query) and are based on the 'Probability ranking principle'. This principle states that the best overall retrieval effectiveness will be achieved when documents are ranked in decreasing order of their probability of relevance [23]. Many probabilistic IR models have been developed, which mainly differ in the way in which they estimate the probability of relevance [12, 17, 21, 22].\nAs a brief bibliographical review, we are going to mention the main research lines related to applying Bayesian networks to IR. We will start at Croft and Turtle's studies [7, 28}, where these authors have car ried out one of the most important works in this field, developing a complete IR model based on two networks, the document and query networks, linked\nbetween them. Closely related to this work, Ghazfan et al. [13] give a different meaning to Croft and Turtle's network. Fung et al. [10] build a knowledge base, also composed of two types of Bayesian networks, that rep resents the relationships among concepts in the docu ments. Other interesting works on Bayesian networks and IR can be found in [2, 9, 18, 24].\nThere are several techniques which can be used in com bination with any IR model to improve the effective ness of the retrieval. We shall mention only thesauri and query expansion-based methods.\nThesauri are useful tools for the indexing and retrieval processes. A thesaurus may contain a set of basic terms and synonyms, antonyms, more general terms, more specific terms, etc, but usually in specific subject areas. Schutze and Pederson [26] make a good review of the different approaches to build these structures. We are going to center on automatic thesaurus con struction. Mainly, these methods are based on statist ical techniques, which try to find patterns of cooccur rence between terms among all the documents in the collection. This task is carried out by computing any kind of similarity function between all pairs of terms, and grouping in common classes all terms whose sim ilarity coefficients are sufficiently large (25].\nClassified as a query modification technique, query ex pansion is the addition of new terms to the query, based on previously retrieved documents or on thesauri, with the objective of recall improvement. Using the re trieved documents, those terms that appear in relev ant documents are added to the query, and using a thesaurus, synonyms or broader terms, related terms in general, are added to query terms [15]. The basis for the cooccurrence-based thesaurus use in query ex pansion is the Association Hypothesis [21], that states that terms which occur in common documents tend to be about the same subject. Therefore, if a query term is good at distinguishing relevant and not relevant doc uments, any other term closely related to it, i.e., which cooccurs frequently with it, is also likely to be good discriminating documents and should be added to the query. Han et al. (14] present a good classification of query expansion methods.\n3 THE THESAURUS CONSTRUCTION ALGORITHM\nGiven a document collection, we have built a thesaurus based on a Bayesian network. From an inverted file used as a learning file, our method learns a polytree of terms, i.e. a directed acyclic graph ( dag) where there is no more than one undirected path connecting each pair of nodes. The polytree nodes represent terms of the\nQuery Expansion using Bayesian Networks 55\ncollection in the form of binary variables. Each vari able, a, takes its values from the set {a0, a! } , where a0 stands for 'the term a is not relevant', and a1 rep resents 'the term a is relevant'.\nThere are two important reasons for learning a poly tree instead of a more general Bayesian network: any document collection contains a very big num ber (typically thousands) of terms. As each term is associated to a node, our network shall be very large. So, the processes needed to estimate a net work from empirical data (learning) can be extremely time-consuming. This task can be obviously allevi ated by applying efficient learning algorithms [3, 20] which build more straightforward graphs as singly con nected networks (forest, trees and polytrees), inten ded as approximations of more complex models due to the loss of expressiveness (since the kind of depend ency /independency relationships that singly connected networks may represent is more restrictive than for general dags). The second reason is based on the methods of inference available for Bayesian networks (i.e., the propagation algorithms). Again, we deal with a potentially time-consuming process, but for singly connected networks there are also efficient and exact propagation methods which run in a time proportional to the number of nodes [1 9]. Among the singly con nected networks, our choice was polytrees because we gain a little more accuracy. Therefore, our aim is to approximate the real dependency model of the terms in a document collection by means of a polytree, in which the most important (in)dependencies between the terms of the collection are expressed.\nThe algorithm we have implemented to learn the poly tree is described in Figure 1. Our algorithm is quite similar to two algorithms for learning polytrees, the PA algorithm [3] and Rebane and Pearl's (RP) [20] (the two algorithms being based on Chow and Lin's method for constructing dependence trees [5]). Ac tually, we could say that it is a combination of both algorithms, with some additional features.\nAs it can be noticed, this algorithm has three main different parts: in step 1, we compute the degrees of dependency between all pairs of nodes. The second part, step 2, represents the tree skeleton construction, and the last one, from step 3 until the end, performs the orientation of the edges in the tree, finally making up a polytree.\nSeveral remarks have to be made about these three parts. First, the measure used to establish the depend ency between nodes (which is, in some sense, analog ous to the functions usually employed in IR systems for measuring the similarity between the terms in the\n56 de Campos, Fernandez, and Huete\n1. For every pair of nodes a, (3 E U, being U the set of nodes, do 1.1. Compute Dep(a, (3/0). 2. Build a maximum weight spanning tree G, where the weight of each edge a-{3 is\nD ( � ) _ { Dep(a, (3/0)\nep a,�-' - 0 if •I(a, (3/0) if I( a, (3/0)\n(1)\n3. For every triplet of nodes a, (3, 1 E U such that a-1, 1-(3 E G do 3.1. If Dep(a,(J/0) < Dep(a,,B/J) and --.J(a,,B/J) then direct the subgraph a-1-(3 as a 4 1 +- (3. 4. Direct the remaining edges without introducing new head to head connections. 5. Return G.\nThat is, the Kullback-Leibler cross entropy (also called Mutual information measure) , which measures the de pendency degree between two variables a and (3 (which is equal to zero if a and (3 are marginally independ ent, and such that the more dependent a and (3 are, the greater Dep( a, (3/0) is) . The probabilities p( ai, (Jj) are estimated from the inverted file by counting fre quencies. So, for example, p( a1, (30) is the probabil ity that the term a appears in a document and not (3, and p( at) is the probability of occurrence of term a (we are assuming that a term a is relevant for a given document if it appears in that document) . Here, we use, as the RP algorithm does, the marginal cross entropy as dependency measure. The PA algorithm uses a combination of Dep(a, (3/0) and the conditional dependency degrees (conditional mutual information measures) Dep( a, (3/J), for every other node T\nThe reason why we have not combined the marginal dependency of two terms with the conditional depend encies of these two terms conditioned to the rest of terms, has been that, due to the great amount of terms in a collection, the computation of the conditional de pendencies, although it has to be carried out only once, has been proved extremely time-consuming but also a big storage is needed. These two obstacles have leaded us to use only the marginal dependency as the depend ency measure; methods to include some conditional de pendencies in certain cases are being studied.\nThe next step is the tree skeleton construction. If we assume that the computed dependency values are link weights in a graph, this algorithm gets a maximum weight spanning tree (MWST), i.e. a tree where the sum of the weigths of its links is maximum. We con sidered two different methods to obtain the MWST: Kruskal's and Prim's algorithms [6], although, finally, we opted for the latter. The fundamental reason for this choice was that Kruskal's algorithm is suggested for sparse graphs and not for complete ones, as our case is, because it requires sorting all the computed dependencies. Prim's method begins with a one-node tree, and at each step adds to the tree the link between a node inside the tree and a node outside it having the maximum dependency degree, until n - 1 links have been adjoined, where n is the number of nodes in the graph.\nDue to the great number of terms that there are gener ally in a collection, the values of the dependencies are very low in general, and sometimes the algorithm does not have any good choice and selects as the highest value among all the dependencies being considered a very low value, adding the corresponding link to the tree. The problem lies in the fact that the two linked nodes are almost more independent than dependent, and therefore the model we are building loses accuracy with respect to the original one.\nTo solve this problem, the algorithm, once it has selec ted a. new link a-{3 to be added to the tree, performs an independency test between a and (3 (namely a Chi Square test with one degree of freedom based on the own value of Dep(a, (3/0) [16]); then it really adds this link to the tree only if the independency test fails (in the algorithm in Figure 1, this is denoted as --.J(a, ,B/0)). This is equivalent to redefine the weights of the links, using eq. (1) instead of eq. (2). In this way, we can obtain a non-connected tree, i.e., a forest, as the result of this step.\nOnce the skeleton is built, the last part of the learning algorithm deals with the orientation of the tree, getting as a result a polytree. This process is based on the PA algorithm: in a head to head pattern a ---* 1 +- ;3, the instantiation of the head to head node 1 should normally increase the degree of dependency between a and {3, whereas in a non-head to head pattern such as a +- 1 4 (3, the instantiation of the middle node 1 should produce the opposite effect, decreasing the degree of dependency between a and ;3. So, we com pare the degree of dependency between a and (3 after the instantiation of 1, Dep( a, (3/J), with the degree of dependency between a and (3 before the instanti ation of/, Dep(a, (3/0), and direct the edges toward 1 if the former is greater than the latter. Finally, the algorithm directs the remaining edges without intra-\nclueing new head to head connections. This strategy produced, in our preliminary experiments, structures where several nodes had a great number of parents; this fact leads to have very big probability tables and, as a consequence, it causes problems of storage and reliab ility (in the estimation of these tables). For that reason we have restricted a bit the rule that produces head to head connections, by including another condition in the antecedent: we want to be sure that if we decide to in clude a head to head connection ct -+ 1 +-- {3, then the nodes ct and f3 are not conditionally independent given I· So, we also test this condition, once again using a Chi Square test of independency based of the value Dep(ct,/311) (in this case with two degrees of freedom).\nAt last, and once the polytree structure has been built, the algorithm has to compute (from the inverted file) the prior probabilities for the root nodes, and the con ditional probabilities of the rest of the nodes, given all their parents, and the Bayesian network is completely specified.\n4 EXPERIMENTATION\nIn this section we are going to briefly describe the material used in our experiments (databases and soft ware), as well as the experimental design and the ob tained results.\n4.1 STANDARD TEST COLLECTIONS AND SOFTWARE USED\nOur experiments in query expansion using a Bayesian network-based thesaurus have been carried out with a set of three standard test collections. It is very com mon in IR that the experiments are made over standard test collections in order to compare several retrieval models. These collections are composed of a set of documents, a set of queries submitted by users and, finally, a set of relevance judgements, i.e. the set of documents which are relevant to each query. This last set is used to measure the retrieval effectiveness.\nThree well-known standard collections have been the basis on which our experiments have been run: Adi, Cranfield and Medlars. Table 1 shows their basic char acteristics. These collections were obtained from the Computer Science Department ftp site at Cornell Uni versity, as well as the Smart IR Software [27]. We have chosen these three collections because their character istics are quite different. In particular, with respect to the number of terms, they cover a wide range of situ ations, going from a relatively small to a large number of terms.\nSmart, as we said in section 2, is an IRS which im plements the vector space model. An example of a\nQuery Expansion using Bayesian Networks 57\ntypical representation of the ith document or query is ((t;l,Wil);(t;2,Wi2); ... ;(t;n,Win)), where each pair is a term and its weight.\nSmart has different weighting schemes. The weighting process is composed of three phases:\n1.- From indexing, the number of times that a term occurs in the document or in the query is associated to each term. This value is called term frequency (tf). Also, the weight can be 0 if the term does not appear or 1 if it does. These weights can be normalised.\n2.- Modification of the tf, normalised or not, using information obtained from the whole collection. The objective of this modification is to increase the weight of those terms rarest in the collection, and decrease the weight of the most common terms.\n3.- Finally, a normalization process can be carried out over the entire vector.\nSmart represents the three steps of the weighting pro cess by three characters, each one representing a dif ferent weighting strategy in the whole weighting con struction. In our experiments, we have used the tf weight without normalisation, no modification of the tf and no normalisation of the entire vector. In the Smart notation, our weighting scheme is 'nnn'. We have chosen it because is the 'default' option of Smart (although we know that it is not the best weighting scheme).\n4.2 THE QUERY EXPANSION PROCESS. EXPERIMENT DESIGN\nGiven a query submitted to our system, the query expansion process starts placing the evidences in the learnt polytree. This action means looking for the terms that appear in the query in the polytree nodes and setting their states to 'the term is relevant'. After that, a propagation process is carried out. As our net work is a polytree, we can use an exact and efficient inference method to propagate the probabilities [ 19]. As a result of the propagation, the probability that a term is relevant, given that all the terms in the query are relevant, is obtained for each node. The next step is to get those terms with highest probability. To select the terms to be added to the query we use a threshold\n58 de Campos, Fernandez, and Huete\nvalue that establishes a lower limit, i.e., those terms whose posterior probability is higher than the given threshold are chosen to be incorporated in the query, setting up the expanded query. The tf weights of the added terms are precisely their posterior probabilit ies. Once the new query vector is completely created, Smart takes that new query and runs a retrieval, get ting a set of retrieved documents.\nSumming up, the whole expansion process is divided into two parts: the learning and the propagation phases. The former has as arguments the document collection (the learning file) and the confidence level for the two independency tests carried out. The lat, ter has two arguments: the Bayesian network and the threshold to select the additional terms. With this set of arguments we have designed a battery of experi ments for each collection, by using different confidence levels (90%, 95%, 97.5%, 99% and 99.5%) and different thresholds (0.5, 0.6, 0.7, 0.8 and 0.9)\nSo, for each collection a set of five thesauri has been created, each one with a different confidence level. The next step is to expand the set of standard queries of each collection using each one of the five thesauri of that collection. Each expansion is run five times, once per threshold, getting five new query files per thesaurus. Once twenty-five new sets of queries per collection are ready, Smart runs a retrieval and evalu ates the retrieval effectiveness.\n4.3 RESULTS\nTables 2, 3 and 4 display some of the results obtained for the three collections from all our experiments. In each table, precision/recall data are presented show ing precision at ten standard recall levels and the av erage of precision at all ten levels, using Smart with and without query expansion (Q.E.); we also show the difference as the percent change from the baseline (Smart). As suggested in [28], a difference of 5% in av erage precision is generally considered significant and a 10% difference is considered very significant. The precision value displayed at each fixed recall level is the average for the given number of queries. The last two rows in each table represent recall and precision obtained from the retrieval of a fixed number of doc uments (we used the default value of Smart, which is 15).\nFor each collection, we show the results of using query expansion only for one experiment, as well as the aver age results over the twenty-five experiments. Neverthe less, for the Adi and Medlars collections we obtained better results than the baseline in all the twenty-five experiments, in terms of both the average precision val ues and number of documents retrieved. So, our query\nexpansion method seems quite robust for these two col lections. The best average precision values are more of ten achieved in low-medium thresholds (0.5-0.7). The precision values in our experiments for the ten recall points are always better than those ones in the ori ginal set of queries, establishing greater distances in medium-high recall values. In these two collections, there are not significant differences between the five networks, corresponding to the five confidence levels used in the independency tests.\nHowever, for the Cranfield collection, it can be ob served how the threshold used to select the terms to expand the queries is an important parameter in the retrieval performance: the greater the threshold is the better the results are. The expansions are not good\nat all for lower thresholds but they improve when this limit is higher: we only obtain better results than the baseline using high thresholds (0.8 and 0.9) (this ex plains the rather poor averages displayed in the last column in table 3). On the other hand, there are not remarkable differences between the results from the different networks, having a very similar behaviour. These results suggest that in this case query expan sion is useful when the number of terms added to each query is rather small (this fact is probably due to the characteristics of this collection).\nSo far we do not know exactly why our method per formed for Adi and Medlars much better than for Cran field. To solve this question will probably require an in-depth study of the characteristics of these collections (specificity or generality of queries and indexing terms, number of terms per query, etc).\nIn general, we may conclude that the improvement in retrieval effectiveness induced by our method is signi ficant but moderate. This fact encourages us to con tinue studying this topic, in order to get more sensible improvements. Other conclusions are, on the one hand, that the confidence level for the independency test is not very relevant to the retrieval performance of the expanded queries, but, on the other hand, the threshold imposed to select those terms which are going to be ad ded to the original query may be quite relevant. This suggests the necessity of developing some heuristics for automatically selecting the threshold according to the data in a given collection (for example taking into ac count the inverse document frequencies of terms in the queries). Anyway, it seems us that the improvement in retrieval performance is collection-dependent.\nQuery Expansion using Bayesian Networks 59\n5 CONCLUDING REMARKS\nWe have developed a new query expansion method us ing a Bayesian network-based thesaurus (constructed automatically using a learning algorithm). Although we have applied it in combination with the well-known Smart system, our method could also be used as a preprocessing step for any other Information Retrieval System. The results obtained from the experiments show a moderate improvement of the retrieval effective ness when using our query expansion technique. Any way, we consider this work as a first step in a more general research centered on applying Bayesian net works to Information Retrieval.\nOur short-term research lines are to develop new learn ing algorithms, completely adapted to the IR context, or modify existing ones and test them on the good test ing ground of query expansion, helped by an IRS like Smart to run the retrievals. We will focus our long term efforts on the development of an IRS completely based on Bayesian networks.\nFor our future developments, the main requirements that we are going to ask for are:\n• We will look for more accuracy in the thesaurus learning algorithms, in the sense of creating more complicated structures than the polytree intro duced in this paper, with the aim of being nearer to the real model defined by data, and at the same time taking into account the complexity of the problem. To carry out this idea, one of the first attempts will be the incorporation of conditional dependencies in the learning process.\n• Incorporating documents to our models, creating structures which would represent the relationships between them, and learning the document network as precisely as is possible.\n• Improving the performance of the propagation process, choosing the most adequate techniques and modifying them to reduce the time consumed in the propagation. For example, by making a pre processing in which the probability tables were in volved, so that at the moment of propagation the calculations would be minimum.\nAcknowledgments\nThis work has been supported by the Spanish Comisi6n Interministerial de Ciencia y Tecnologia (CICYT) under Project n. TIC96-0781.\n60 de Campos, Fernandez, and Huete\nReferences\n(1] N. Belkin, W. B. Croft, Retrieval Techniques, An nual review of information sciences and techno logy, Chapter 4 (Elsevier Science Publisher, 1987).\n(2] P. Bruza, L. C. Van der Gaag, Index expression Belief networks for information retrieval disclos ure, International Journal of Expert Systems, 7(2) 107-138 (1994).\n(3] L.M. de Campos, Independency relationships and learning algorithms for singly connected networks, To appear in Journal of Experimental and The oretical Artificial Intelligence. Also available as Technical Report D ECSAI-96-02-04 ( 1996).\n(4] E. Castillo, J.M. Gutierrez, A. Hadi, Experts Sys tems and Probabilistic Network Models (Springer Verlag, 1997).\n(5] C.K. Chow, C.N. Lin, Approximating discrete probability distributions with dependence trees, IEEE Transactions on Information Theory, 14 462-467 (1968).\n(6] N. Christofides, Graph Theory, An Algorithmic Approach (Academic Press, 1975).\n(7] W. B. Croft, H. Turtle, Text Retrieval and Infer ence, Text-based Intelligent Systems, Paul Jac obs, ed., 127-156 (Lawrence Erlbaum, 1992).\n(8] D. Ellis, Progress and Problem in Information Re trieval, 2nd edition (Library Association Publish ing, 1996).\n(9] M.E. Frisse, S.B. Cousins, Information retrieval from hypertext: update of the dynamic med ical handbook project, Proceedings of the Hyper text'89 Conference (1989).\n[10} R. Fung, S. Crawford, L. Appelbaum, R. Tong, An architecture for probabilistic concept-based information retrieval, Proceedings of the ACM SIGIR'90 Conference, J.L. Vidick, ed., 455-467 ( 1990).\n(11] R. Fung, B. Del Favero, Applying Bayesian net works to information retrieval, Communications of the ACM, 38(3) 42-57 (1995).\n(12] N. Fuhr, Models for retrieval with probabilistic in dexing, Information Processing and Management, 25(1) 55-72 (1989).\n(13] D. Ghazfan, M. Indrawan, B. Srinivasan, Towards meaningful Bayesian networks for information re trieval systems, Proceedings of the IPMU'96 Con ference, 841-846 (1996).\n(14] C. Han, H. Fujii, W.B. Croft, Automatic query ex pansion for Japanese text retrieval, UMass Tech nical Report ( 1994).\n[15] D. Harman. Relevance feedback and other query modification techniques in Information Retrieval. Data Structures & Algorithms, W.B. Frakes, R. Baeza-Yates Eds., 241-263 (Prentice Hall, 1992).\n(16] S. Kullback, Information Theory and Statistics (Dover Publications, 1968).\n[17} M. E. Maron, J. Kuhns, On relevance, probabil istic indexing and information retrieval, Journal of the ACM, 7 216-244 (1960).\n[18] Y. Park, Key-Sun Choi, Automatic thesaurus construction using Bayesian networks, Informa tion Processing and Management, 32(5) 543-553 (1996).\n[19] .J. Pearl, Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference (Mor gan and Kaufmann, 1988).\n[20] G. Rebane, J. Pearl, The recovery of causal poly trees from statistical data, Uncertainty in Artifi cial Intelligence, L.N. Kanal, T.S. Levitt and J.F. Lemmer, Eds., 175-182 (North-Holland, 1989).\n(21} C.J. van Rijsbergen, Information Retrieval, 2nd. edition (Butterworths, 1979).\n(22] S.E. Robertson, K. Spark Jones, Relevance weighting of search terms, Journal of the Amer ican Society of Information Science, 27 294-305 (1976).\n[23] S. E. Robertson, The probability ranking principle in IR, Journal of Documentation, 33(4) 294-304 (1977).\n(24] J. Savoy, D. Des bois, Information retrieval in hy pertext systems: an approach using Bayesian net works, Electronic Publishing, 4(2) 87-108 (1991).\n[25) G. Salton, J. McGill, Introduction to modern in formation retrieval (McGraw Hill, 1 993).\n[26) H. Schiitze, J. Pederson, A cooccurrence-based thesaurus and two applications to information re trieval, Information Processing and Management, :33(3) 307-318 (1997).\n[27) Smart, Computer Science Department, Cornell University, ftp site: ftp.cs.cornell.edu.\n(28] H. Turtle, Inference networks for document re trieval, Ph.D. Dissertation, Computer and In formation Science Department, University of Mas sachusetts (1991)."
    } ],
    "references" : [ {
      "title" : "Retrieval Techniques, An­ nual review of information sciences and techno­ logy, Chapter 4 (Elsevier",
      "author" : [ "N. Belkin", "W.B. Croft" ],
      "venue" : "Science Publisher,",
      "citeRegEx" : "Belkin and Croft,? \\Q1987\\E",
      "shortCiteRegEx" : "Belkin and Croft",
      "year" : 1987
    }, {
      "title" : "Index expression Belief networks for information retrieval disclos­",
      "author" : [ "P. Bruza", "L.C. Van der Gaag" ],
      "venue" : "ure, International Journal of Expert Systems,",
      "citeRegEx" : "Bruza and Gaag,? \\Q1994\\E",
      "shortCiteRegEx" : "Bruza and Gaag",
      "year" : 1994
    }, {
      "title" : "Approximating discrete probability distributions with dependence trees",
      "author" : [ "C.K. Chow", "C.N. Lin" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Chow and Lin,? \\Q1968\\E",
      "shortCiteRegEx" : "Chow and Lin",
      "year" : 1968
    }, {
      "title" : "Graph Theory, An Algorithmic Approach",
      "author" : [ "N. Christofides" ],
      "venue" : null,
      "citeRegEx" : "Christofides,? \\Q1975\\E",
      "shortCiteRegEx" : "Christofides",
      "year" : 1975
    }, {
      "title" : "Progress and Problem in Information Re­ trieval, 2nd edition (Library Association Publish­",
      "author" : [ "D. Ellis" ],
      "venue" : null,
      "citeRegEx" : "Ellis,? \\Q1996\\E",
      "shortCiteRegEx" : "Ellis",
      "year" : 1996
    }, {
      "title" : "Information retrieval from hypertext: update of the dynamic med­ ical handbook project, Proceedings of the Hyper­",
      "author" : [ "M.E. Frisse", "S.B. Cousins" ],
      "venue" : null,
      "citeRegEx" : "Frisse and Cousins,? \\Q1989\\E",
      "shortCiteRegEx" : "Frisse and Cousins",
      "year" : 1989
    }, {
      "title" : "Applying Bayesian net­ works to information retrieval",
      "author" : [ "R. Fung", "B. Del Favero" ],
      "venue" : "Communications of the ACM,",
      "citeRegEx" : "Fung and Favero,? \\Q1995\\E",
      "shortCiteRegEx" : "Fung and Favero",
      "year" : 1995
    }, {
      "title" : "Models for retrieval with probabilistic in­ dexing",
      "author" : [ "N. Fuhr" ],
      "venue" : "Information Processing and Management,",
      "citeRegEx" : "Fuhr,? \\Q1989\\E",
      "shortCiteRegEx" : "Fuhr",
      "year" : 1989
    }, {
      "title" : "Towards meaningful Bayesian networks for information re­ trieval systems",
      "author" : [ "D. Ghazfan", "M. Indrawan", "B. Srinivasan" ],
      "venue" : "Proceedings of the IPMU'96 Con­ ference,",
      "citeRegEx" : "Ghazfan et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Ghazfan et al\\.",
      "year" : 1996
    }, {
      "title" : "Automatic query ex­ pansion for Japanese text retrieval, UMass Tech­ nical Report",
      "author" : [ "C. Han", "H. Fujii", "W.B. Croft" ],
      "venue" : null,
      "citeRegEx" : "Han et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 1994
    }, {
      "title" : "Relevance feedback and other query modification techniques in Information Retrieval",
      "author" : [ "D. Harman" ],
      "venue" : "Data Structures & Algorithms,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1992
    }, {
      "title" : "On relevance, probabil­ istic indexing and information retrieval",
      "author" : [ "M.E. Maron", "J. Kuhns" ],
      "venue" : "Journal of the ACM,",
      "citeRegEx" : "Maron and Kuhns,? \\Q1960\\E",
      "shortCiteRegEx" : "Maron and Kuhns",
      "year" : 1960
    }, {
      "title" : "Automatic thesaurus construction using Bayesian networks, Informa­",
      "author" : [ "Y. Park", "Key-Sun Choi" ],
      "venue" : "Processing and Management,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1996
    }, {
      "title" : "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference (Mor­",
      "author" : [ "J. Pearl" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1988
    }, {
      "title" : "The recovery of causal poly­ trees from statistical data, Uncertainty",
      "author" : [ "G. Rebane", "J. Pearl" ],
      "venue" : "Artifi­ cial Intelligence,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1989
    }, {
      "title" : "Relevance weighting of search terms",
      "author" : [ "S.E. Robertson", "K. Spark Jones" ],
      "venue" : "Journal of the Amer­ ican Society of Information Science,",
      "citeRegEx" : "Robertson and Jones,? \\Q1976\\E",
      "shortCiteRegEx" : "Robertson and Jones",
      "year" : 1976
    }, {
      "title" : "The probability ranking principle in IR",
      "author" : [ "S.E. Robertson" ],
      "venue" : "Journal of Documentation,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1977
    }, {
      "title" : "Des bois, Information retrieval in hy­ pertext systems: an approach using Bayesian net­ works, Electronic Publishing",
      "author" : [ "D.J. Savoy" ],
      "venue" : null,
      "citeRegEx" : "Savoy,? \\Q1991\\E",
      "shortCiteRegEx" : "Savoy",
      "year" : 1991
    }, {
      "title" : "A cooccurrence-based thesaurus and two applications to information re­ trieval",
      "author" : [ "H. Schiitze", "J. Pederson" ],
      "venue" : "Information Processing and Management,",
      "citeRegEx" : "Schiitze and Pederson,? \\Q1997\\E",
      "shortCiteRegEx" : "Schiitze and Pederson",
      "year" : 1997
    }, {
      "title" : "Inference networks for document re­ trieval, Ph.D. Dissertation, Computer and In­ formation Science Department, University of Mas­",
      "author" : [ "H. Turtle" ],
      "venue" : null,
      "citeRegEx" : "Turtle,? \\Q1991\\E",
      "shortCiteRegEx" : "Turtle",
      "year" : 1991
    } ],
    "referenceMentions" : [ ],
    "year" : 2011,
    "abstractText" : "Information Retrieval (IR) is concerned with the identification of documents in a collec­ tion that are relevant to a given information need, usually represented as a query contain­ ing terms or keywords, which are supposed to be a good description of what the user is look­ ing for. IR systems may improve their effect­ iveness (i.e., increasing the number of relev­ ant documents retrieved) by using a process of query expansion, which automatically adds new terms to the original query posed by an user. In this paper we develop a method of query expansion based on Bayesian networks. Using a learning algorithm, we construct a Bayesian network that represents some of the relationships among the terms appearing in a given document collection; this network is then used as a thesaurus (specific for that col­ lection). We also report the results obtained by our method on three standard test collec­ tions.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}