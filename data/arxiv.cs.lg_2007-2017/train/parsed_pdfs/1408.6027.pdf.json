{
  "name" : "1408.6027.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Quan Zhao" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 8.\n60 27\nv1 [\ncs .L\nG ]\n2 6\nA ug\n2 01\n4 1\nIndex Terms—Multi-label learning, label distribution learning, learning with ambiguity\n✦"
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "L Earning with ambiguity is a hot topic in recent machinelearning and data mining research. A learning process is essentially building a mapping from the instances to the labels. Ambiguity at the label side of the mapping is embodied by the way the labels are assigned to the instances. There are mainly two ways to label an instance in existing learning paradigms: (1) a single label is assigned to this instance, and (2) multiple labels are assigned to this instance. Single-label learning assumes that all the instances in the training set are labeled in the first way. Multi-label learning (MLL) [31] allows the training instances to be labeled in the second way, and thus can deal with the ambiguity at the label side. Generally speaking, current MLL algorithms have been developed with two strategies [30]. The first strategy is problem transformation, where the basic idea is to transform the MLL task into one or more single-label learning tasks. For example, the MLL problem could be transformed into binary classification problems [14], [25], a label ranking problem [15], [9], or an ensemble learning problem [26], [17]. The second strategy is algorithm adaptation, where the basic idea is to extend specific learning algorithms to handle multi-label data. For example, it can be extended from k-NN [37], [5], decision tree [6], [24], or neural networks [36], [27].\nMLL has been successfully applied to many real-world problems because it can well model the ambiguity of “what describes the instance”, i.e., multiple labels can be used to describe the same instance. However, MLL can hardly deal with the more general ambiguity of “how to describe the instance”, i.e., the exact role of each label is also involved in the description of the instance. Surprisingly, the real-world\n• Xin Geng and Quan Zhao are with the School of Computer Science and Engineering, and the Key Lab of Computer Network and Information Integration (Ministry of Education), Southeast University, Nanjing 211189, China. * Corresponding author. E-mail: {xgeng, zhaoquan}@seu.edu.cn\ndata with the information about such exact role of each label might be more common than many people think. For example, in many scientific experiments [8], the result is not a single output, but a series of numerical outputs (e.g., gene expression levels over a period of time). Each of these outputs alone might be of little importance. What really matters is the overall distribution of the whole series of outputs. If the learning task is to predict such distributions, then it can hardly fit into the MLL framework because the exact role of each output in the distribution is crucial, and there is no partition of relevant and irrelevant labels at all. Another example is from the research on the relationship between protein and cancer. It was discovered [34] that one particular kind of protein could be related to several cancers, and the expression levels of the protein in these related cancer cells are generally different. Higher expression level indicates closer relationship, i.e., the corresponding cancer plays a more important role when describing the protein. By regarding the cancers with the protein expression level higher than a threshold as the positive labels of the protein, the problem can fit into the MLL framework. Unfortunately, this will lose the important information of the different protein expression levels in the related cancers. Yet another example is from the overlapping community analysis of complex networks [20]. One node in a complex network might belong to multiple communities. But the importance of each community to this node is usually different. Such importance can be represented by certain criterion, such as the z-score [13], which indicates the role of the corresponding community when describing the node. Similarly, a threshold of the z-score can be used to determine the positive labels of a node, and then MLL can be applied to this problem. But this will lose the valuable information about the different importance of the overlapping communities to the node.\nFor the applications mentioned above, a more natural way to label an instance x is to assign a real number dy\nx to each\npossible label y, representing the degree to which y describes x. For example, if x represents a protein, y represents a cancer, then dy\nx should be the expression level of the protein x in the\ncancer y. Without loss of generality, assume that dy x ∈ [0, 1]. Further suppose that the label set is complete, i.e., using all the labels in the set can always fully describe the instance. Then, ∑\ny d y x = 1. Such d y x is called the description degree of y to x. For a particular instance, the description degrees of all the labels constitute a data form similar to probability distribution. So, it is called label distribution. The learning process on the instances labeled by label distributions is therefore called label distribution learning (LDL). The aforementioned applications reveal that the nature of label ambiguity might exceed the current framework of MLL. In such case, it is necessary to extend MLL to LDL. In fact, the real applications suitable for LDL might be more common than those suitable for MLL because when there are multiple labels associated with one instance, their importance or relevance to the instance can hardly be exactly the same.\nIt is not a rare case in existing single-label or multi-label machine learning literatures that a numerical indicator (e.g., probability, confidence, grade, score, votes, etc.) is associated with each label [33], [37], [15], [4]. As shown in Fig. 1, label distribution learning is different from these learning methods in mainly three aspects:\n1) Each training instance of LDL is explicitly associated with a label distribution, rather than a single label or a relevant (positive) label set. The label distribution comes from the application itself as a natural part of the original data, while most numerical label indicators used\nin previous learning algorithms are artificially generated from the original data for later decision making. 2) The purpose of most kinds of numerical label indicators used in previous learning algorithms is to rank the labels, and then decide the positive label(s) through, say, thresholding over the ranking. The actual value of the indicator does not matter much so long as the ranking does not change. In most cases, it only cares about the first or first several labels in the ranking. On the other hand, what LDL cares about is the overall label distribution. The exact value of each label’s description degree is thus important. 3) The performance evaluation measures of previous learning algorithms with numerical label indicators are still those commonly used for single-label learning (e.g., classification accuracy, error rate, etc.) or multi-label learning (e.g., Hamming loss, one-error, coverage, ranking loss, etc.). On the other hand, the performance of LDL should be evaluated by the similarity or distance between the predicted label distribution and the real label distribution, which will be further discussed in Section 3.4.\nThis paper extends our preliminary work [10]. The main contribution of this paper includes: 1) A novel machine learning paradigm named label distribution learning is formulated; 2) Six working label distribution learning algorithms are proposed and compared in the experiments; 3) Six measures\nfor evaluation of label distribution learning algorithms are suggested; 4) The first batch of real-world label distribution datasets are prepared and made publicly available to the research community. The rest of the paper is organized as follows. Firstly, the problem of LDL is formulated in Section 1. Then, six LDL algorithms are presented in Section 3. After that, the experiments on artificial as well as real-world datasets are reported in Section 4. Finally, the paper is summarized and some discussions on future work are given in Section 5."
    }, {
      "heading" : "2 FORMULATION OF LDL",
      "text" : "By the definition of label distribution given in Section 1, both single-label and multi-label annotations can be viewed as special cases of label distribution. Fig. 2 gives one label distribution example for the single-label, multi-label, and general cases, respectively. For the single-label case (a), the label y2 fully describes the instance, so dy2x = 1. For the multi-label case (b), each of the two positive labels y2 and y4 by default describes 50% of the instance, so dy2\nx = dy4 x = 0.5. Finally, (c)\nrepresents a general case of label distribution, which satisfies the constraints dy\nx ∈ [0, 1] and\n∑\ny d y x = 1. Such examples\nillustrate that label distribution is more general than both the single-label and multi-label cases, and thus can provide more flexibility in the learning process.\nNevertheless, more flexibility usually means larger output space. From single label to multi-label, and then to label distribution, the size of the output space of the learning process becomes increasingly larger. In detail, for a problem with c different labels, there are c possible outputs for a single-label learner, and 2c − 1 possible outputs for a multi-label learner. As for a label distribution learner, there are infinite possible outputs as long as they satisfy that dy\nx ∈ [0, 1] and\n∑\ny d y x = 1.\nFig. 3 gives three cases of the decision regions for a typical two-label problem, where red represents the decision region of the label y1, and yellow represents that of the label y2. In the single-label case (a), the feature space is explicitly divided into two decision regions for y1 and y2, respectively. In the multilabel case (b), the feature space is explicitly divided into three decision regions for y1, y2 and {y1, y2} (the orange region in the middle), respectively. Finally in the label distribution case (c), every point in the feature space corresponds to a description degree of y1 and a description degree of y2. As a result, there is no rigid decision boundary in this case.\nIt is worthwhile to distinguish description degree from the concept membership used in fuzzy classification [38]. Mem-\n(a) Single-label (b) Multi-label (c) Label distribution\nFig. 3. Three cases of the decision regions for a typical two-label problem.\nbership is a truth value that may range between completely true and completely false. It is designed to handle the status of partial truth which often appears in the non-numeric linguistic variables. On the other hand, description degree reflects the ambiguity of the label description of the instance, i.e., one label may only partially describe the instance, but it is completely true that the label describes the instance. Based on fuzzy set theory, a recent extension of multi-label classification, namely graded multilabel classification [4], allows for graded membership of an instance belonging to a class. Apart from the different meanings of continuous description degree and graded membership, LDL is also different from this work in methodology. The strategy in [4] is to reduce graded multilabel problems to conventional multi-label problems, while LDL aims to directly model the mapping from instances to label distributions.\nNote also that dyx is not the probability that y correctly labels x, but the proportion that y accounts for in a full description of x. Thus, all the labels with a non-zero description degree are actually the ‘correct’ labels to describe the instance, but just with different importance measured by dy\nx . Recognizing\nthis, one can distinguish label distribution from the previous studies on probabilistic labels [28], [7], [22], where the basic assumption is that there is only one ‘correct’ label for each instance. Fortunately, although not a probability by definition, dy x still shares the same constraints with probability, i.e., dy x ∈ [0, 1] and ∑\ny d y x = 1. Thus, many theories and methods in\nstatistics can be applied to label distributions. First of all, dyx can be represented by the form of conditional probability, i.e., dy x = P (y|x). Then, the problem of LDL can be formulated as follows.\nLet X = Rq denote the input space and Y =\n4 {y1, y2, · · · , yc} denote the complete set of labels. Given a training set S = {(x1, D1), (x2, D2), · · · , (xn, Dn)}, where xi ∈ X is an instance and Di = {dy1xi , d y2 xi , · · · , dyc xi } is the label distribution associated with xi, the goal of label distribution learning is to learn a conditional probability mass function p(y|x) from S, where x ∈ X and y ∈ Y .\nSuppose p(y|x) is a parametric model p(y|x; θ), where θ is the parameter vector. Given the training set S, the goal of LDL is to find the θ that can generate a distribution similar to Di given the instance xi. As will be discussed in Section 3.4, there are different criteria that can be used to measure the distance or similarity between two distributions. For example, if the Kullback-Leibler divergence is used as the distance measure, then the best parameter vector θ∗ is determined by\nθ∗ = argmin θ\n∑\ni\n∑\nj\n(\nd yj xi ln\nd yj xi\np(yj|xi; θ)\n)\n= argmax θ\n∑\ni\n∑\nj\nd yj xi ln p(yj |xi; θ). (1)\nIt is interesting to examine the traditional learning paradigms under the optimization criterion shown in Eq. (1). For single-label learning (see Fig. 2(a) for example), dyjxi = Kr(yj , y(xi)), where Kr(·, ·) is the Kronecker delta function and y(xi) is the single label of xi. Consequently, Eq. (1) can be simplified to\nθ∗ = argmax θ\n∑\ni\nln p(y(xi)|xi; θ). (2)\nThis is actually the maximum likelihood (ML) estimation of θ. The later use of p(y|x; θ) for classification is equivalent to the maximum a posteriori (MAP) decision.\nFor multi-label learning, each instance is associated with a label set (see Fig. 2(b) for example). Consequently, Eq. (1) can be changed into\nθ∗ = argmax θ\n∑\ni\n1\n|Yi|\n∑\ny∈Yi\nln p(y|xi; θ), (3)\nwhere Yi is the label set associated with xi. Eq. (3) can be viewed as a ML criterion weighted by the reciprocal cardinality of the label set associated with each instance. In fact, this is equivalent to first applying Entropy-based Label Assignment (ELA) [31] to transform the multi-label instances into the weighted single-label instances, and then optimizing the ML criterion based on the weighted single-label instances.\nIt can be seen from the above analysis that with proper constraints, a LDL model can be transformed into the commonly used methods for single-label or multi-label learning. Thus, LDL may be viewed as a more general learning framework which includes both single-label and multi-label learning as its special cases."
    }, {
      "heading" : "3 LDL ALGORITHMS",
      "text" : "We follow three strategies to design algorithms for LDL. The first strategy is problem transformation, i.e., transform the\nLDL problem into existing learning paradigms. The second strategy is algorithm adaptation, i.e., extend existing learning algorithms to deal with label distributions. Finally, the third strategy is to design specialized algorithms according to the characteristics of LDL. Following each of the three strategies, two typical algorithms are proposed in this section."
    }, {
      "heading" : "3.1 Problem Transformation",
      "text" : "One straightforward way to transform a LDL problem into a single-label learning problem is to change the training examples to weighted single-label examples. In detail, each training example (xi, Di) is transformed into c single-label examples (xi, yj) with the weight d yj xi , where i = 1, . . . , n and j = 1, . . . , c. The training set is then resampled according to the weight of each example. The resampled training set becomes a standard single-label training set including c × n examples, and then any single-label learning algorithms can be applied to the training set.\nIn order to predict the label distribution of a previously unseen instance x, the learner must be able to output the confidence/probability for each label yj , which can be regarded as the description degree of the corresponding label, i.e., d yj x = P (yj|x). Two representative algorithms are adopted here for this purpose. One is the Bayes classifier, the other is SVM. In detail, the Bayes classifier assumes Gaussian distribution for each class, and the posterior probability computed by the Bayes rule is regarded as the description degree of the corresponding label. As to SVM, the probability estimates are obtained by a pairwise coupling method proposed in [33] for multi-class classification, which has also been implemented in the software LIBSVM [3]. Again, the probability for each label is regarded as the description degree of the corresponding label. When Bayes and SVM are applied to the resampled training set, the resulted methods are denoted by PT-Bayes and PT-SVM, respectively, where ‘PT’ is the abbreviation of ‘Problem Transformation’."
    }, {
      "heading" : "3.2 Algorithm Adaptation",
      "text" : "Certain existing learning algorithms can be naturally extended to deal with label distributions, among which two adapted algorithms are proposed here. The first one is k-NN. Given a new instance x, its k nearest neighbors are first found in the training set. Then, the mean of the label distributions of all the k nearest neighbors is calculated as the label distribution of x, i.e.,\np(yj |x) = 1\nk\n∑\ni∈Nk(x)\nd yj xi , (j = 1, 2, . . . , c), (4)\nwhere Nk(x) is the index set of the k nearest neighbors of x in the training set. This adapted algorithm is denoted by AA-kNN, where ‘AA’ is the abbreviation of ‘Algorithm Adaptation’.\nThe second algorithm is the backpropagation (BP) neural network. The three-layer neural network has q (the dimensionality of x) input units which receive x, and c (the number of different labels) output units each of which outputs the\n5 description degree of a label yj . For single-label or multilabel learning, the desired output t is usually a vector with 1s at the positions corresponding to the positive labels of the input instance, and 0s otherwise. For LDL, t becomes the real label distribution of the input training instance. Thus, the target of the BP algorithm is to minimize the sum-squared error of the output of the neural network compared with the real label distributions. To make sure the output of the neural network z = {z1, z2, . . . , zc} satisfies that zj ∈ [0, 1] for j = 1, 2, . . . , c and ∑\nj zj = 1, the softmax activation function is used in each output unit. Let the net input to the j-th output unit be ηj , then the softmax output zj is\nzj = exp(ηj) c ∑\nk=1\nexp(ηk) , (j = 1, 2, . . . , c), (5)\nThis adapted algorithm is denoted by AA-BP."
    }, {
      "heading" : "3.3 Specialized Algorithms",
      "text" : "Different from the indirect strategy of problem transformation and algorithm adaptation, the specialized algorithms directly match the LDL problem, e.g., by directly solving the optimization problem in Eq. (1). One good start toward this end might be our previous work on facial age estimation [11], [12], where in order to solve the insufficient training data problem, each face image is labeled by not only its chronological age, but also the neighboring ages. This actually forms a special label distribution with the highest description degree at the chronological age, and gradually decreasing description degrees on both neighboring sides of the chronological age. We proposed two algorithms IIS-LLD and CPNN for such special data, both of which were designed to solve an optimization problem similar to Eq. (1). Unfortunately, CPNN is only suitable for totally ordered labels (such as the age), and thus cannot be applied to the general LDL problem. As for IIS-LLD, although it was designed for a particular form of label distribution, it can be generalized to deal with any LDL problems. The optimization process of IIS-LLD, however, has been evidenced to be not very efficient [18]. So, an improved version is further proposed in this section.\nIIS-LLD assumes the parametric model p(y|x; θ) to be the maximum entropy model [1], i.e.,\np(y|x; θ) = 1\nZ exp\n(\n∑\nk\nθy,kgk(x)\n)\n, (6)\nwhere Z = ∑ y exp ( ∑\nk θy,kgk(x)) is the normalization factor, θy,k is an element in θ, and gk(x) is the k-th feature of x. Substituting Eq. (6) into Eq. (1) yields the target function of θ\nT (θ) = ∑\ni,j\nd yj xi\n∑\nk\nθyj,kgk(xi) (7)\n− ∑\ni\nln ∑\nj\nexp\n(\n∑\nk\nθyj,kgk(xi)\n)\n.\nIn each step of the optimization, IIS-LLD updates the current estimate of the parameters θ to θ+∆, where ∆ maximizes a\nAlgorithm 1: IIS-LLD Input: The training set S = {(xi, Di)}ni=1 and the\nconvergence criterion ε Output: p(y|x; θ)\n1 Initialize the model parameter vector θ(0); 2 l ← 0; 3 repeat 4 l ← l + 1; 5 Solve Eq. (8) for δy,k; 6 θ(l) ← θ(l−1) +∆; 7 until T (θ(l))− T (θ(l−1)) < ε;\n8 p(y|x; θ) ← 1 Z exp\n(\n∑ k θ (l) y,kgk(x)\n)\n;\nlower bound to the change of the target function output. The element of ∆, δyj,k, can be obtained by solving the equation\n∑\ni\np(yj |xi; θ)gk(xi) exp(δyj,ks(gk(xi))g #(xi)) (8)\n− ∑\ni\nd yj xigk(xi) = 0,\nwhere g#(xi) = ∑\nk |gk(xi)| and s(gk(xi)) is the sign of gk(xi). Eq. (8) can be solved by nonlinear equation solvers, such as the Gauss-Newton method. The pseudocode of IISLLD is given in Algorithm 1.\nThe optimization in IIS-LLD uses a strategy similar to Improved Iterative Scaling (IIS) [21], which is a well-known algorithm for maximizing the likelihood of the maximum entropy model. However, it has been reported in the literature [18] that IIS often performs worse than several other optimization algorithms such as conjugate gradient and quasi-Newton methods. Here we follow the idea of an effective quasi-Newton method BFGS [19] to further improve IIS-LLD.\nConsider the second-order Taylor series of T ′(θ) = −T (θ) at the current estimate of the parameter vector θ(l):\nT ′(θ(l+1)) ≈ T ′(θ(l))+∇T ′(θ(l))T∆+ 1\n2 ∆\nTH(θ(l))∆, (9)\nwhere ∆ = θ(l+1) − θ(l) is the update step, ∇T ′(θ(l)) and H(θ(l)) are the gradient and Hessian matrix of T ′(θ) at θ(l), respectively. The minimizer of Eq. (9) is\n∆ (l) = −H−1(θ(l))∇T ′(θ(l)). (10)\nThe line search Newton method uses ∆(l) as the search direction p(l) = ∆(l) and updates the parameter vector by\nθ(l+1) = θ(l) + α(l)p(l), (11)\nwhere the step length α(l) is obtained from a line search procedure to satisfy the strong Wolfe conditions [19]:\nT ′(θ(l) + α(l)p(l)) ≤ T ′(θ(l)) + c1α (l)∇T ′(θ(l))Tp(l), (12)\n|∇T ′(θ(l) + α(l)p(l))Tp(l)| ≤ c2|∇T ′(θ(l))Tp(l)|, (13)\nwhere 0 < c1 < c2 < 1.\n6\nAlgorithm 2: BFGS-LLD Input: The training set S = {(xi, Di)}ni=1 and the\nconvergence criterion ε Output: p(y|x; θ)\n1 Initialize the model parameter vector θ(0); 2 Initialize the inverse Hessian approximation B(0); 3 Compute ∇T ′(θ(0)) by Eq. (18); 4 l ← 0; 5 repeat 6 Compute search direction p(l) ← −B(l)∇T ′(θ(l)); 7 Compute the step length α(l) by a line search procedure to satisfy Eq. (12) and (13); 8 θ(l+1) ← θ(l) + α(l)p(l); 9 Compute ∇T ′(θ(l+1)) by Eq. (18);\n10 s(l) ← θ(l+1) − θ(l); 11 u(l) ← ∇T ′(θ(l+1))−∇T ′(θ(l)); 12 ρ(l) ← 1\ns(l)u(l) ;\n13 B(l+1) ← (I − ρ(l)s(l)(u(l))T)B(l)(I − ρ(l)u(l)(s(l))T) + ρ(l)s(l)(s(l))T; 14 l ← l + 1; 15 until ‖∇T ′(θ(l))‖ < ε;\n16 p(y|x; θ) ← 1 Z exp\n(\n∑ k θ (l) y,kgk(x)\n)\n;\nOne problem of the above method is the calculation of the inverse Hessian matrix in each iteration, which is computationally expensive. The idea of BFGS is to avoid explicit calculation of H−1(θ(l)) by approximating it with an iteratively updated matrix B:\nB(l+1) = (I − ρ(l)s(l)(u(l))T)B(l)(I − ρ(l)u(l)(s(l))T) (14)\n+ρ(l)s(l)(s(l))T,\nwhere\ns(l) = θ(l+1) − θ(l), (15)\nu(l) = ∇T ′(θ(l+1))−∇T ′(θ(l)), (16)\nρ(l) = 1\ns(l)u(l) . (17)\nAs to the optimization of the target function T ′(θ), the computation of BFGS is mainly related to the first-order gradient, which can be obtained through\n∂T ′(θ) ∂θyj ,k = ∑\ni\nexp\n(\n∑\nk\nθyj ,kgk(xi)\n)\ngk(xi)\n∑\nj\nexp\n(\n∑\nk\nθyj,kgk(xi)\n) − ∑\ni\nd yj xigk(xi).\n(18) Thus it performs much more efficiently than the standard line search Newton method, and based on previous studies [18], it stands a good chance of outperforming the IIS-based algorithm IIS-LLD. This improved algorithm is denoted by BFGS-LLD, and its pseudocode is shown in Algorithm 2."
    }, {
      "heading" : "3.4 Evaluation Measures",
      "text" : "The output of a LDL algorithm is a label distribution, which is different from both the single label output of single-label\nlearning and the label set output of multi-label learning. Accordingly, the evaluation measures for LDL algorithms should be different from those used for single-label and multi-label learning algorithms. A natural choice of such measure is the average distance or similarity between the predicted label distribution and the real label distribution. There are many measures for the distance/similarity between probability distributions [2], which can be well borrowed to measure the distance/similarity between label distributions. In this paper, six representative measures are selected as the evaluation measures for the LDL algorithms. Four of them are distance measures (the smaller the better), i.e., Euclidean, Sørensen, Squared χ2, and Kullback-Leibler (K-L), and two of them are similarity measures (the larger the better), i.e., Intersection and Fidelity. Suppose the real label distribution is D = {d1, d2, . . . , dc}, the predicted label distribution is D̂ = {d̂1, d̂2, . . . , d̂c}, then the formulae of the six measures are summarized in Table 1.\nOn a particular dataset, each of the measures may reflect a certain aspect of an algorithm. It is hard to say which measure is the best. Thus we suggest to use all of the six measures when comparing different LDL algorithms, which is analogous to the common practice of using multiple evaluation measures for MLL algorithms."
    }, {
      "heading" : "4 EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "4.1 Methodology",
      "text" : "There are three datasets used in the experiments: an artificial toy dataset, a moderate-sized real-world dataset (Yeast Gene) with ten subsets, and a large-scale real-world dataset (Human Gene)1. Some basic statistics about these three datasets are given in Table 2.\nThe artificial toy dataset is generated to show in a direct and visual way whether the LDL algorithms can learn the mapping from the instance to the label distribution. In this dataset, the instance x is of three-dimensional and there are three labels, i.e., q = 3 and c = 3. The label distribution\n1. The datasets and the Matlab code used in the experiments can be downloaded from http://cse.seu.edu.cn/PersonalPage/xgeng/LDL/index.htm\n7\nD = {dy1x , d y2 x , d y3 x } of an instance x = [x1, x2, x3] T is created in the following way.\nt1 = ax1 + bx 2 1 + cx 3 1 + d, (19) t2 = ax2 + bx 2 2 + cx 3 2 + d, (20) t3 = ax3 + bx 2 3 + cx 3 3 + d, (21)\nψ1 = (w T 1t) 2, (22) ψ2 = (w T 2t+ λ1ψ1) 2, (23) ψ3 = (w T 3t+ λ2ψ2) 2, (24)\ndy1x = ψ1\nψ1 + ψ2 + ψ3 , (25)\ndy2 x\n= ψ2\nψ1 + ψ2 + ψ3 , (26)\ndy3 x\n= ψ3\nψ1 + ψ2 + ψ3 , (27)\nwhere t = [t1, t2, t3]T. Note that Eq. (23) and (24) deliberately make the description degree of one label depend on those of other labels. The parameters in Eq. (19)-(24) are set as a = 1, b = 0.5, c = 0.2, d = 1, w1 = [4, 2, 1]T, w2 = [1, 2, 4]T, w3 = [1, 4, 2]\nT, and λ1 = λ2 = 0.01. To generate the training set, each component of x is uniformly sampled within the range [−1, 1]. In total, there are 500 instances sampled in this way. Then, the label distribution corresponding to each instance is calculated via Eq. (19)-(27). Such 500 examples are used as the training set for the LDL algorithms proposed in Section 3.\nIn order to show the result of the LDL algorithms in a direct and visual way, the test examples of the toy dataset are selected from a certain manifold in the instance space. The first two components of the test instance x, x1 and x2, are located at a grid of the interval 0.01 within the range [−1, 1] on both dimensions, i.e., there are in total 201 × 201 = 40, 401 test instances. The third component x3 is calculate by\nx3 = sin((x1 + x2)× π). (28)\nThen, the label distribution of each test instance, either the ground-truth calculated via Eq. (19)-(27) or the prediction given by the LDL algorithms, is transformed into a color. Thus the ground-truth and predicted label distributions of the test instances can be compared visually through the color pattern on the manifold.\nThe second dataset is a real-world dataset collected from ten biological experiments on the budding yeast Saccharomyces cerevisiae [8]. The result of each of the ten experiments constitutes a subset of this dataset. There are in total 2, 465\nyeast genes included, each of which is represented by an associated phylogenetic profile vector of length 24. The labels correspond to the discrete time points in different biological experiments. The gene expression level (after normalization) at each time point provides a natural measure of the description degree of the corresponding label. The number of labels in the ten subsets of the Yeast Gene dataset is summarized in Table 2. The description degrees (normalized gene expression level) of all the labels (time points) constitute a label distribution for a particular yeast gene.\nThe third dataset is a large-scale real-world dataset collected from the biological research on the relationship between human genes and diseases. There are in total 30, 542 human genes included in this dataset, each of which is represented by the 36 numerical descriptors for a gene sequence proposed in [35]. The labels corresponds to 68 different diseases. The gene expression level (after normalization) for each disease is regarded as the description degree of the corresponding label. The description degrees (normalized gene expression level) of all the 68 labels (diseases) constitute a label distribution for a particular human gene.\nAll the six algorithms described in Section 3, i.e., PT-Bayes, PT-SVM, AA-kNN, AA-BP, IIS-LLD, and BFGS-LLD, are applied to the three datasets shown in Table 2 and compared by the six measures listed in Table 1. On the Yeast Gene and Human Gene datasets, ten-fold cross validation is conducted for each algorithm and the mean value and standard deviation of each evaluation measure are recorded.\nFor each algorithm, several parameter configurations are tried. On the artificial dataset, one fifth of the training set is randomly selected as the validation set. The model is trained on those examples left in the training set and tested on the validation set to select the best parameters. Then, the model is trained on the whole training set with the best parameters and tested on the test set. On the Yeast Gene and Human Gene datasets, the parameter selection process is nested into the ten-fold cross validation. In detail, the whole data is first randomly split into 10 chunks. Each time, one chunk is used as test set, another is used as validation set, and the rest 8 chunks are used as training set. Then, the model is trained with different parameter settings on the training set and tested on the validation set. This procedure is repeated 10 times (each time with different training and validation sets), and the parameter setting with the best average performance is selected. After that, the original validation set is merged into the training set and the test set remains unchanged. The model is trained with the selected parameter setting on the\n9\nupdated training set and tested on the test set. This procedure is repeated 10 times and the average performance is recorded."
    }, {
      "heading" : "4.2 Results",
      "text" : ""
    }, {
      "heading" : "4.2.1 Artificial Dataset",
      "text" : "In order to visually show the label distributions of the test examples of the artificial dataset, the description degrees of the three labels are regarded as the three color channels of the RGB color space. Then, each label distribution is converted into a color, and the predictions made by the LDL algorithms can be compared with the ground-truth label distributions through observing the color patterns on the manifold where the test examples lie on. For easier comparison, the images are visually enhanced by applying a decorrelation stretch process. The results are shown in Fig. 4. It can be seen that the two specialized LDL algorithms, IIS-LLD and BFGS-LLD, predict almost identical color patterns with the ground-truth. PT-Bayes and AA-kNN can also discover similar color patterns with the ground-truth. However, PT-SVM and AA-BP fail to obtain a reasonable result.\nFor more detailed comparison, the six numerical evaluation measures listed in Table 1 are given in Table 3. For the four distance measures (Euclidean, Sørensen, Squared χ2, and K-L), “↓” indicates “the smaller the better”. For the two similarity measures (Intersection and Fidelity), “↑” indicates “the larger the better”. The best performance on each measure is highlighted by boldface. On each measure, the algorithms are ranked in decreasing order of their performance. The ranks are given in the brackets right after the performance values. The result of the numerical measure comparison in Table 3 is consistent with that of the visual comparison in Fig. 4. The performance of the six LDL algorithms on the artificial dataset can be ranked as BFGS-LLD ≻ IIS-LLD ≻ PT-Bayes ≻ AAkNN ≻ AA-BP ≻ PT-SVM."
    }, {
      "heading" : "4.2.2 Yeast Gene Dataset",
      "text" : "To give a direct idea of the predictions made by the LDL algorithms, some typical examples of the predicted label distributions on the Yeast Gene dataset given by the six LDL algorithms are first shown in Table 4. The first row shows the real label distributions of four typical test instances with 18, 7, 4 and 2 labels, respectively. Each of the following rows shows the corresponding predictions of one LDL algorithm. The evaluation measures are given under each predicted label distribution in two sets. The first set includes the four distance measures {Euclidean, Sørensen, Squared χ2, K-L}, and the\nsecond set includes the two similarity measures {Intersection, Fidelity}. The best performance in each case is highlighted by boldface. As can be seen that the specialized algorithms (IIS-LLD and BFGS-LLD) can generally give better label distribution predictions compared with other algorithms, in the sense of either visual similarity or numerical evaluation measures.\nThe experimental results of the six LDL algorithms on the ten subsets of the Yeast Gene dataset are tabulated in Table 5. On each subset and each measure, the algorithms are ranked in decreasing order of their performance indicated by the mean value of the corresponding measure. The ranks are given in the brackets right after the performance values. The average rank of each algorithm over all subsets is calculated for each measure and given in the rows named “Avg. Rank”. There are in total 60 pairs of subset and measure in Table 5, i.e., 10 subsets × 6 measures. In each case specified by the (subset, measure) pair, the best (rank 1) performance among the six compared algorithms is highlighted by boldface. Based on the average rank, the specialized LDL algorithms (IIS-LLD and BFGS-LLD) perform remarkably better than those algorithms obtained from problem transformation (PT-Bayes and PTSVM) and algorithm adaptation (AA-kNN and AA-BP). In all the 60 cases, BFGS-LLD and IIS-LLD consistently achieve the best and the second best performance, respectively. The performance comparison result between the ‘PT’ algorithms and the ‘AA’ algorithms, however, is not so constant: while the average rank of PT-SVM is before that of AA-kNN, the performance of PT-Bayes ranks consistently after that of AABP."
    }, {
      "heading" : "4.2.3 Human Gene Dataset",
      "text" : "The experimental results of the six LDL algorithms on the large-scale Human Gene dataset are tabulated in Table 6. Again, the algorithms are ranked in decreasing order of their performance on each criterion. As can be seen from Table 6, the performances of all the algorithms are generally worse than those on the Yeast Gene dataset. This is because that the Human Gene dataset is much more complex than the Yeast Gene dataset in the sense of the number of examples, features, and labels. However, the relative performances, i.e., the rankings, of the algorithms are similar to those on the Yeast Gene dataset. The specialized LDL algorithms (IIS-LLD and BFGS-LLD) again perform remarkably better than both the ‘PT’ algorithms (PT-Bayes and PT-SVM) and the ‘AA’ algorithms (AA-kNN and AA-BP).\n10\n11\n12\nIn summary, the experimental results on all of the artificial, Yeast Gene and Human Gene datasets show that the specialized algorithms directly designed for the LDL problem (IISLLD and BFGS-LLD) generally perform better than those transformed from traditional learning algorithms (PT-Bayes, PT-SVM, AA-kNN, and AA-BP). This is due to the advantages of the specialized algorithms in discovering the overall label distributions. By using more effective optimization process, BFGS-LLD remarkably improves the performance of IIS-LLD. This also indicates the necessity of further research on the algorithms specially designed for the LDL problem."
    }, {
      "heading" : "5 SUMMARY AND DISCUSSIONS",
      "text" : "This paper proposes label distribution learning, which is a more general learning framework than single-label and multilabel learning. It can deal with not only multiple labels of one instance, but also the different importance of these labels. This paper proposes six working LDL algorithms in three ways: problem transformation, algorithm adaptation, and specialized algorithm design. In order to compare these algorithms, six evaluation measures are used and the first batch of label distribution datasets are prepared and made publicly available. Experimental results on one artificial and two real-world datasets show clear advantage of the specialized algorithms. This illustrates that the characteristics of LDL require special design to achieve good performance.\nThis paper is motivated by the real-world data with natural measures of description degrees (e.g., gene expression level). However, as a general learning framework, label distribution learning might also be used to solve other kinds of problems. Generally speaking, there are at least three scenarios where label distribution learning could be helpful:\n1) There is a natural measure of description degree that associates the labels with the instances. This is the most direct application of label distribution learning, as described in this paper. 2) When there are multiple labeling sources (e.g., multiple experts) for one instance, the annotations from different sources might be significantly inconsistent. In such case, it is usually better for the learning algorithm to integrate the labels from all the sources rather than to decide one or more ‘winning label(s)’ via majority voting [23]. One good way to incorporate all the labeling sources is to generate a label distribution for the instance: the label favored by more sources is given a higher description degree, while that chosen by fewer sources\nis assigned with a lower description degree. In this way, the multi-labeling-source problem is transformed into a label distribution learning problem. 3) Some labels are highly correlated with other labels. Utilizing such correlation is one of the most important approaches to improve the learning process [16], [32], [29]. Label distribution learning provides a new way toward this purpose. The key step is to transform a single-label or multi-label learning problem into a label distribution learning problem. This can be achieved by generating a label distribution for each instance according to the correlation among the labels.\nEach of the three scenarios actually covers a vast area of applications. A lot of interesting work, both at the theoretical level and at the application level, may be conducted in the future."
    }, {
      "heading" : "ACKNOWLEDGMENT",
      "text" : "This research was supported by the National Science Foundation of China (61273300, 61232007), and Jiangsu Natural Science Funds for Distinguished Young Scholar (BK20140022)."
    } ],
    "references" : [ {
      "title" : "A maximum entropy approach to natural language processing",
      "author" : [ "A.L. Berger", "S.D. Pietra", "V.J.D. Pietra" ],
      "venue" : "Computational Linguistics, vol. 22, no. 1, pp. 39–71, 1996.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Comprehensive survey on distance/similarity measures between probability density functions",
      "author" : [ "S.-H. Cha" ],
      "venue" : "International Journal of Mathematical Models and Methods in Applied Sciences, vol. 1, pp. 300–307, 2007.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "LIBSVM: A library for support vector machines",
      "author" : [ "C.-C. Chang", "C.-J. Lin" ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology, vol. 2, pp. 27:1–27:27, 2011.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Graded multilabel classification: The ordinal case",
      "author" : [ "W. Cheng", "K. Dembczynski", "E. Hüllermeier" ],
      "venue" : "Proc. 27th Int’l Conf. Machine Learning, Haifa, Israel, 2010, pp. 223–230.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Combining instance-based learning and logistic regression for multilabel classification",
      "author" : [ "W. Cheng", "E. Hüllermeier" ],
      "venue" : "Machine Learning, vol. 76, no. 2-3, pp. 211–225, 2009.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Knowledge discovery in multi-label phenotype data",
      "author" : [ "A. Clare", "R.D. King" ],
      "venue" : "Proc. European Conf. Principles of Data Mining and Knowledge Discovery, Freiburg, Germany, 2001, pp. 42–53.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Handling possibilistic labels in pattern classification using evidential reasoning",
      "author" : [ "T. Denoeux", "L.M. Zouhal" ],
      "venue" : "Fuzzy Sets and Systems, vol. 122, no. 3, pp. 409–424, 2001.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Cluster analysis and display of genome-wide expression patterns",
      "author" : [ "M.B. Eisen", "P.T. Spellman", "P.O. Brown", "D. Botstein" ],
      "venue" : "Proceedings of the National Academy of Science, vol. 95, no. 25, pp. 14 863–14 868, 1998.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Multilabel classification via calibrated label ranking",
      "author" : [ "J. Fürnkranz", "E. Hüllermeier", "E.L. Mencı́a", "K. Brinker" ],
      "venue" : "Machine Learning, vol. 73, no. 2, pp. 133–153, 2008.  13",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Label distribution learning",
      "author" : [ "X. Geng", "R. Ji" ],
      "venue" : "Proc. 2013 Int’l Conf. Data Mining Workshops, Dallas, TA, 2013, pp. 377–383.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Facial age estimation by learning from label distributions",
      "author" : [ "X. Geng", "K. Smith-Miles", "Z.-H. Zhou" ],
      "venue" : "Proc. 24th AAAI Conf. Artificial Intelligence, Atlanta, GA, 2010, pp. 451–456.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Facial age estimation by learning from label distributions",
      "author" : [ "X. Geng", "C. Yin", "Z.-H. Zhou" ],
      "venue" : "IEEE Trans. Pattern Anal. Machine Intell., vol. 35, no. 10, pp. 2401–2412, 2013.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Functional cartography of complex metabolic networks",
      "author" : [ "R. Guimerà", "L.A.N. Amaral" ],
      "venue" : "Nature, vol. 433, pp. 895–900, 2005.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Multi-label classification using conditional dependency networks",
      "author" : [ "Y. Guo", "S. Gu" ],
      "venue" : "Proc. 22nd Int’l Joint Conf. Artificial Intelligence, Barcelona, Spain, 2011, pp. 1300–1305.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Label ranking by learning pairwise preferences",
      "author" : [ "E. Hüllermeier", "J. Fürnkranz", "W. Cheng", "K. Brinker" ],
      "venue" : "Artif. Intell., vol. 172, no. 16-17, pp. 1897–1916, 2008.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1897
    }, {
      "title" : "Correlated label propagation with application to multi-label learning",
      "author" : [ "F. Kang", "R. Jin", "R. Sukthankar" ],
      "venue" : "Proc. IEEE Conf. Computer Vision and Pattern Recognition, New York, NY, 2006, pp. 1719–1726.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Multi-label ensemble based on variable pairwise constraint projection",
      "author" : [ "P. Li", "H. Li", "M. Wu" ],
      "venue" : "Information Sciences, vol. 222, pp. 269– 281, 2013.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A comparison of algorithms for maximum entropy parameter estimation",
      "author" : [ "R. Malouf" ],
      "venue" : "Proc. 6th Conf. Computational Natural Language Learning, Taipei, Taiwan, 2002, pp. 49–55.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Numerical Optimization, 2nd ed",
      "author" : [ "J. Nocedal", "S. Wright" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2006
    }, {
      "title" : "Uncovering the overlapping community structure of complex networks in nature and society",
      "author" : [ "G. Palla", "I. Derenyi", "I. Farkas", "T. Vicsek" ],
      "venue" : "Nature, vol. 435, pp. 814–818, 2005.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Inducing features of random fields",
      "author" : [ "S.D. Pietra", "V.J.D. Pietra", "J.D. Lafferty" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell., vol. 19, no. 4, pp. 380–393, 1997.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Learning from data with uncertain labels by boosting credal classifiers",
      "author" : [ "B. Quost", "T. Denoeux" ],
      "venue" : "Proc. 1st ACM SIGKDD Workshop on Knowledge Discovery from Uncertain Data, Paris, France, 2009, pp. 38–47.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Learning from crowds",
      "author" : [ "V.C. Raykar", "S. Yu", "L.H. Zhao", "G.H. Valadez", "C. Florin", "L. Bogoni", "L. Moy" ],
      "venue" : "Journal of Machine Learning Research, vol. 11, no. Apr, pp. 1297–1322, 2010.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Scalable and efficient multi-label classification for evolving data streams",
      "author" : [ "J. Read", "A. Bifet", "G. Holmes", "B. Pfahringer" ],
      "venue" : "Machine Learning, vol. 88, no. 1-2, pp. 243–272, 2012.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Classifier chains for multi-label classification",
      "author" : [ "J. Read", "B. Pfahringer", "G. Holmes", "E. Frank" ],
      "venue" : "Machine Learning, vol. 85, no. 3, pp. 333– 359, 2011.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Multi-label classification using ensembles of pruned sets",
      "author" : [ "J. Read", "B. Pfahringer", "G. Holmes" ],
      "venue" : "Proc. IEEE Int’l Conf. Data Mining, Pisa, Italy, 2008, pp. 995–1000.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Art-based neural networks for multi-label classification",
      "author" : [ "E.P. Sapozhnikova" ],
      "venue" : "Int’l Symposium Intelligent Data Analysis, Lyon, France, 2009, pp. 167–177.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Learning with probabilistic supervision",
      "author" : [ "P. Smyth" ],
      "venue" : "Computational Learning Theory and Natural Learning System, T. Petsche, Ed. MA: MIT Press, 1995, vol. III, pp. 163–182.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Multi-label learning with weak label",
      "author" : [ "Y.-Y. Sun", "Y. Zhang", "Z.-H. Zhou" ],
      "venue" : "Proc. 24th AAAI Conf. Artificial Intelligence, Atlanta, GA, 2010, pp. 593–598.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Tutorial on learning from multi-label data",
      "author" : [ "G. Tsoumakas", "M.-L. Zhang", "Z.-H. Zhou" ],
      "venue" : "European Conf. Machine Learning and Principles and Practice of Knowledge Discovery in Databases, Bled, Slovenia, 2009.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Multi-label classification: An overview",
      "author" : [ "G. Tsoumakas", "I. Katakis" ],
      "venue" : "Int’l Journal of Data Warehousing and Mining, vol. 3, no. 3, pp. 1–13, 2007.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "A generative probabilistic model for multi-label classification",
      "author" : [ "H. Wang", "M. Huang", "X. Zhu" ],
      "venue" : "Proc. 8th IEEE Int’l Conf. Data Mining, Pisa, Italy, 2008, pp. 628–637.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Probability estimates for multiclass classification by pairwise coupling",
      "author" : [ "T.-F. Wu", "C.-J. Lin", "R.C. Weng" ],
      "venue" : "Journal of Machine Learning Research, vol. 5, pp. 975–1005, 2004.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Proteomic applications for the early detection of cancer",
      "author" : [ "J.D. Wulfkuhle", "L.A. Liotta", "E.F. Petricoin" ],
      "venue" : "Nature Reviews Cancer, vol. 3, no. 4, pp. 267–275, 2003.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Discriminate the falsely predicted proteinccoding genes in aeropyrum pernix k1 genome based on graphical representation",
      "author" : [ "J.-F. Yu", "D.-K. Jiang", "K. Xiao", "Y. Jin", "J.-H. Wang", "X. Sun" ],
      "venue" : "MATCH Commun. Math. Comput. Chem., vol. 67, no. 3, pp. 845–866, 2012.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Multilabel neural networks with applications to functional genomics and text categorization",
      "author" : [ "M.-L. Zhang", "Z.-H. Zhou" ],
      "venue" : "IEEE Trans. Knowl. Data Eng., vol. 18, no. 10, pp. 1338–1351, 2006.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Multi-label learning by instance differentiation",
      "author" : [ "——" ],
      "venue" : "Proc. 22nd AAAI Conf. Artificial Intelligence, Vancouver, Canada, 2007, pp. 669– 674.",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Practical Applications of Fuzzy Technologies",
      "author" : [ "H.-J. Zimmermann", "Ed" ],
      "venue" : "Quan Zhao received the B.Sc. degree in computer science and technology from Southeast University, China,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 1999
    } ],
    "referenceMentions" : [ {
      "referenceID" : 30,
      "context" : "Multi-label learning (MLL) [31] allows the training instances to be labeled in the second way, and thus can deal with the ambiguity at the label side.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 29,
      "context" : "Generally speaking, current MLL algorithms have been developed with two strategies [30].",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 13,
      "context" : "For example, the MLL problem could be transformed into binary classification problems [14], [25], a label ranking problem [15], [9], or an ensemble learning problem [26], [17].",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 24,
      "context" : "For example, the MLL problem could be transformed into binary classification problems [14], [25], a label ranking problem [15], [9], or an ensemble learning problem [26], [17].",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 14,
      "context" : "For example, the MLL problem could be transformed into binary classification problems [14], [25], a label ranking problem [15], [9], or an ensemble learning problem [26], [17].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 8,
      "context" : "For example, the MLL problem could be transformed into binary classification problems [14], [25], a label ranking problem [15], [9], or an ensemble learning problem [26], [17].",
      "startOffset" : 128,
      "endOffset" : 131
    }, {
      "referenceID" : 25,
      "context" : "For example, the MLL problem could be transformed into binary classification problems [14], [25], a label ranking problem [15], [9], or an ensemble learning problem [26], [17].",
      "startOffset" : 165,
      "endOffset" : 169
    }, {
      "referenceID" : 16,
      "context" : "For example, the MLL problem could be transformed into binary classification problems [14], [25], a label ranking problem [15], [9], or an ensemble learning problem [26], [17].",
      "startOffset" : 171,
      "endOffset" : 175
    }, {
      "referenceID" : 36,
      "context" : "For example, it can be extended from k-NN [37], [5], decision tree [6], [24], or neural networks [36], [27].",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 4,
      "context" : "For example, it can be extended from k-NN [37], [5], decision tree [6], [24], or neural networks [36], [27].",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 5,
      "context" : "For example, it can be extended from k-NN [37], [5], decision tree [6], [24], or neural networks [36], [27].",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 23,
      "context" : "For example, it can be extended from k-NN [37], [5], decision tree [6], [24], or neural networks [36], [27].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 35,
      "context" : "For example, it can be extended from k-NN [37], [5], decision tree [6], [24], or neural networks [36], [27].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 26,
      "context" : "For example, it can be extended from k-NN [37], [5], decision tree [6], [24], or neural networks [36], [27].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 7,
      "context" : "For example, in many scientific experiments [8], the result is not a single output, but a series of numerical outputs (e.",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 33,
      "context" : "It was discovered [34] that one particular kind of protein could be related to several cancers, and the expression levels of the protein in these related cancer cells are generally different.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 19,
      "context" : "Yet another example is from the overlapping community analysis of complex networks [20].",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 12,
      "context" : "Such importance can be represented by certain criterion, such as the z-score [13], which indicates the role of the corresponding community when describing the node.",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 0,
      "context" : "Without loss of generality, assume that d x ∈ [0, 1].",
      "startOffset" : 46,
      "endOffset" : 52
    }, {
      "referenceID" : 32,
      "context" : ") is associated with each label [33], [37], [15], [4].",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 36,
      "context" : ") is associated with each label [33], [37], [15], [4].",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 14,
      "context" : ") is associated with each label [33], [37], [15], [4].",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 3,
      "context" : ") is associated with each label [33], [37], [15], [4].",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 9,
      "context" : "This paper extends our preliminary work [10].",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 0,
      "context" : "Finally, (c) represents a general case of label distribution, which satisfies the constraints d x ∈ [0, 1] and ∑",
      "startOffset" : 100,
      "endOffset" : 106
    }, {
      "referenceID" : 0,
      "context" : "As for a label distribution learner, there are infinite possible outputs as long as they satisfy that d x ∈ [0, 1] and ∑",
      "startOffset" : 108,
      "endOffset" : 114
    }, {
      "referenceID" : 37,
      "context" : "It is worthwhile to distinguish description degree from the concept membership used in fuzzy classification [38].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 3,
      "context" : "Based on fuzzy set theory, a recent extension of multi-label classification, namely graded multilabel classification [4], allows for graded membership of an instance belonging to a class.",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 3,
      "context" : "The strategy in [4] is to reduce graded multilabel problems to conventional multi-label problems, while LDL aims to directly model the mapping from instances to label distributions.",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 27,
      "context" : "Recognizing this, one can distinguish label distribution from the previous studies on probabilistic labels [28], [7], [22], where the basic assumption is that there is only one ‘correct’ label for each instance.",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 6,
      "context" : "Recognizing this, one can distinguish label distribution from the previous studies on probabilistic labels [28], [7], [22], where the basic assumption is that there is only one ‘correct’ label for each instance.",
      "startOffset" : 113,
      "endOffset" : 116
    }, {
      "referenceID" : 21,
      "context" : "Recognizing this, one can distinguish label distribution from the previous studies on probabilistic labels [28], [7], [22], where the basic assumption is that there is only one ‘correct’ label for each instance.",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 0,
      "context" : ", d x ∈ [0, 1] and ∑",
      "startOffset" : 8,
      "endOffset" : 14
    }, {
      "referenceID" : 30,
      "context" : "In fact, this is equivalent to first applying Entropy-based Label Assignment (ELA) [31] to transform the multi-label instances into the weighted single-label instances, and then optimizing the ML criterion based on the weighted single-label instances.",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 32,
      "context" : "As to SVM, the probability estimates are obtained by a pairwise coupling method proposed in [33] for multi-class classification, which has also been implemented in the software LIBSVM [3].",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 2,
      "context" : "As to SVM, the probability estimates are obtained by a pairwise coupling method proposed in [33] for multi-class classification, which has also been implemented in the software LIBSVM [3].",
      "startOffset" : 184,
      "endOffset" : 187
    }, {
      "referenceID" : 0,
      "context" : ", zc} satisfies that zj ∈ [0, 1] for j = 1, 2, .",
      "startOffset" : 26,
      "endOffset" : 32
    }, {
      "referenceID" : 10,
      "context" : "One good start toward this end might be our previous work on facial age estimation [11], [12], where in order to solve the insufficient training data problem, each face image is labeled by not only its chronological age, but also the neighboring ages.",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 11,
      "context" : "One good start toward this end might be our previous work on facial age estimation [11], [12], where in order to solve the insufficient training data problem, each face image is labeled by not only its chronological age, but also the neighboring ages.",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 17,
      "context" : "The optimization process of IIS-LLD, however, has been evidenced to be not very efficient [18].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 0,
      "context" : "IIS-LLD assumes the parametric model p(y|x; θ) to be the maximum entropy model [1], i.",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 20,
      "context" : "The optimization in IIS-LLD uses a strategy similar to Improved Iterative Scaling (IIS) [21], which is a well-known algorithm for maximizing the likelihood of the maximum entropy model.",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 17,
      "context" : "However, it has been reported in the literature [18] that IIS often performs worse than several other optimization algorithms such as conjugate gradient and quasi-Newton methods.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 18,
      "context" : "Here we follow the idea of an effective quasi-Newton method BFGS [19] to further improve IIS-LLD.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 18,
      "context" : "where the step length α is obtained from a line search procedure to satisfy the strong Wolfe conditions [19]:",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 17,
      "context" : "(18) Thus it performs much more efficiently than the standard line search Newton method, and based on previous studies [18], it stands a good chance of outperforming the IIS-based algorithm IIS-LLD.",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 1,
      "context" : "There are many measures for the distance/similarity between probability distributions [2], which can be well borrowed to measure the distance/similarity between label distributions.",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 3,
      "context" : "2, d = 1, w1 = [4, 2, 1]T, w2 = [1, 2, 4]T, w3 = [1, 4, 2] T, and λ1 = λ2 = 0.",
      "startOffset" : 15,
      "endOffset" : 24
    }, {
      "referenceID" : 1,
      "context" : "2, d = 1, w1 = [4, 2, 1]T, w2 = [1, 2, 4]T, w3 = [1, 4, 2] T, and λ1 = λ2 = 0.",
      "startOffset" : 15,
      "endOffset" : 24
    }, {
      "referenceID" : 0,
      "context" : "2, d = 1, w1 = [4, 2, 1]T, w2 = [1, 2, 4]T, w3 = [1, 4, 2] T, and λ1 = λ2 = 0.",
      "startOffset" : 15,
      "endOffset" : 24
    }, {
      "referenceID" : 0,
      "context" : "2, d = 1, w1 = [4, 2, 1]T, w2 = [1, 2, 4]T, w3 = [1, 4, 2] T, and λ1 = λ2 = 0.",
      "startOffset" : 32,
      "endOffset" : 41
    }, {
      "referenceID" : 1,
      "context" : "2, d = 1, w1 = [4, 2, 1]T, w2 = [1, 2, 4]T, w3 = [1, 4, 2] T, and λ1 = λ2 = 0.",
      "startOffset" : 32,
      "endOffset" : 41
    }, {
      "referenceID" : 3,
      "context" : "2, d = 1, w1 = [4, 2, 1]T, w2 = [1, 2, 4]T, w3 = [1, 4, 2] T, and λ1 = λ2 = 0.",
      "startOffset" : 32,
      "endOffset" : 41
    }, {
      "referenceID" : 0,
      "context" : "2, d = 1, w1 = [4, 2, 1]T, w2 = [1, 2, 4]T, w3 = [1, 4, 2] T, and λ1 = λ2 = 0.",
      "startOffset" : 49,
      "endOffset" : 58
    }, {
      "referenceID" : 3,
      "context" : "2, d = 1, w1 = [4, 2, 1]T, w2 = [1, 2, 4]T, w3 = [1, 4, 2] T, and λ1 = λ2 = 0.",
      "startOffset" : 49,
      "endOffset" : 58
    }, {
      "referenceID" : 1,
      "context" : "2, d = 1, w1 = [4, 2, 1]T, w2 = [1, 2, 4]T, w3 = [1, 4, 2] T, and λ1 = λ2 = 0.",
      "startOffset" : 49,
      "endOffset" : 58
    }, {
      "referenceID" : 7,
      "context" : "The second dataset is a real-world dataset collected from ten biological experiments on the budding yeast Saccharomyces cerevisiae [8].",
      "startOffset" : 131,
      "endOffset" : 134
    }, {
      "referenceID" : 34,
      "context" : "There are in total 30, 542 human genes included in this dataset, each of which is represented by the 36 numerical descriptors for a gene sequence proposed in [35].",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 22,
      "context" : "In such case, it is usually better for the learning algorithm to integrate the labels from all the sources rather than to decide one or more ‘winning label(s)’ via majority voting [23].",
      "startOffset" : 180,
      "endOffset" : 184
    }, {
      "referenceID" : 15,
      "context" : "Utilizing such correlation is one of the most important approaches to improve the learning process [16], [32], [29].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 31,
      "context" : "Utilizing such correlation is one of the most important approaches to improve the learning process [16], [32], [29].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 28,
      "context" : "Utilizing such correlation is one of the most important approaches to improve the learning process [16], [32], [29].",
      "startOffset" : 111,
      "endOffset" : 115
    } ],
    "year" : 2017,
    "abstractText" : "Although multi-label learning can deal with many problems with label ambiguity, it does not fit some real applications well where the overall distribution of the importance of the labels matters. This paper proposes a novel learning paradigm named label distribution learning (LDL) for such kind of applications. The label distribution covers a certain number of labels, representing the degree to which each label describes the instance. LDL is a more general learning framework which includes both single-label and multi-label learning as its special cases. This paper proposes six working LDL algorithms in three ways: problem transformation, algorithm adaptation, and specialized algorithm design. In order to compare their performance, six evaluation measures are suggested for LDL algorithms, and the first batch of label distribution datasets are collected and made publicly available. Experimental results on one artificial and two real-world datasets show clear advantage of the specialized algorithms, which indicates the importance of special design for the characteristics of the LDL problem.",
    "creator" : "LaTeX with hyperref package"
  }
}