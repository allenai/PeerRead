{
  "name" : "1302.4381.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Reasoning about Independence in Probabilistic Models of Relational Data",
    "authors" : [ "Marc Maier", "Katerina Marazopoulou", "David Jensen" ],
    "emails" : [ "maier@cs.umass.edu", "kmarazo@cs.umass.edu", "jensen@cs.umass.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: relational models, d -separation, conditional independence, lifted representations, directed graphical models"
    }, {
      "heading" : "1. Introduction",
      "text" : "The rules of d -separation can algorithmically derive all conditional independence facts that hold in distributions represented by a Bayesian network. In this paper, we show that d -separation may not correctly infer conditional independence when applied directly to the graphical structure of a relational model. We introduce the notion of relational dseparation—a graphical criterion for deriving conditional independence facts from relational models—and define its semantics to be consistent with traditional d -separation (i.e., it claims independence only when it is guaranteed to hold for all model instantiations). We present an alternative, lifted representation—the abstract ground graph—that enables an algorithm for deriving conditional independence facts from relational models. We show that this approach is sound, complete, and computationally efficient, and we provide an empirical demonstration of effectiveness across synthetic causal structures of relational domains.\nThe main contributions of this work are:\n• A precise formalization of fundamental concepts of relational data and relational models necessary to reason about conditional independence (Section 4)\nc©2013 Marc Maier and Katerina Marazopoulou and David Jensen.\nar X\niv :1\n30 2.\n43 81\nv3 [\ncs .A\nI] 6\n• A formal definition of d -separation for relational models analogous to d -separation for Bayesian networks (Section 5)\n• The abstract ground graph—a lifted representation that abstracts all possible ground graphs of a given relational model structure, as well as proofs of the soundness and completeness of this abstraction (Section 5.1)\n• Proofs of soundness and completeness for a method that answers relational d -separation queries (Section 5.2)\nWe also provide an empirical comparison of relational d -separation to traditional d - separation applied directly to relational model structure, showing that, not only would most queries be undefined, but those that can be represented yield an incorrect judgment of conditional independence up to 50% of the time (Section 6). Finally, we offer additional empirical results on synthetic data that demonstrate the effectiveness of relational d -separation with respect to complexity and consistency (Section 7). The remainder of this introductory section first gives a brief overview of Bayesian networks and their generalization to relational models and then describes why d -separation is a useful theory."
    }, {
      "heading" : "1.1 From Bayesian Networks to Relational Models",
      "text" : "Bayesian networks are a widely used class of graphical models that are capable of compactly representing a joint probability distribution over a set of variables. The joint distribution can be factored into a product of conditional distributions by assuming that variables are independent of their non-descendants given their parents (the Markov condition). The Markov condition ties the structure of the model to the set of conditional independencies that hold over all probability distributions the model can represent. Accurate reasoning about such conditional independence facts is the basis for constraint-based algorithms, such as PC and FCI (Spirtes et al., 2000), and hybrid approaches, such as MMHC (Tsamardinos et al., 2006), that are commonly used to learn the structure of Bayesian networks. Under a small number of assumptions and with knowledge of the conditional independencies, these algorithms can identify causal structure (Pearl, 2000; Spirtes et al., 2000).\nDeriving the full set of conditional independencies implied by the Markov condition is complex, requiring manipulation of the joint distribution and various probability axioms. Fortunately, the exact same set of conditional independencies entailed by the Markov condition are also entailed by d -separation, a set of graphical rules that algorithmically derive conditional independence facts directly from the graphical structure of the model. That is, the Markov condition and d -separation are equivalent approaches for producing conditional independence from Bayesian networks (Verma and Pearl, 1988; Geiger and Pearl, 1988; Neapolitan, 2004). When interpreting a Bayesian network causally, the causal Markov condition (variables are independent of their non-effects given their direct causes) and d - separation have been shown to provide the correct connection between causal structure and conditional independence (Scheines, 1997).\nBayesian networks assume that data instances are independent and identically distributed, but many real-world systems are characterized by interacting heterogeneous entities. For example, social network data consist of individuals, groups, and their relationships; citation data involve researchers collaborating and authoring scholarly papers that cite prior\nwork; and sports data include players, coaches, teams, referees, and their competitive interactions. Over the past 15 years, researchers in statistics and computer science have devised more expressive classes of directed graphical models, such as probabilistic relational models (PRMs), which remove the assumptions of independent and identically distributed instances to more accurately describe these types of domains (Getoor and Taskar, 2007). Relational models generalize other classes of models that incorporate interference, spillover effects, or violations of the stable unit treatment value assumption (SUTVA) (Hudgens and Halloran, 2008; Tchetgen Tchetgen and VanderWeele, 2012) and multilevel or hierarchical models (Gelman and Hill, 2007).\nMany practical applications have also benefited from learning and reasoning with relational models. Examples include analysis of gene regulatory interactions (Segal et al., 2001), scholarly citations (Taskar et al., 2001), ecosystems (D’Ambrosio et al., 2003), biological cellular networks (Friedman, 2004), epidemiology (Getoor et al., 2004), and security in information systems (Sommestad et al., 2010). The structure and parameters of these models can be learned from a relational data set. The model is typically used either to predict values of certain attributes (e.g., topics of papers) or the structure is examined directly (e.g., to determine predictors of disease spread). A major goal in many of these applications is to promote understanding of a domain or to determine causes of various outcomes. However, as with Bayesian networks, to effectively interpret and reason about relational models causally, it is necessary to understand their conditional independence implications.\n1.2 Why d-Separation Is Useful\nA Bayesian network, as a model of a joint probability distribution, enables a wide array of useful tasks by supporting inference over an entire set of variables. Bayesian networks have been successfully applied to model many domains, ranging from bioinformatics and medicine to computer vision and information retrieval. Näıvely specifying a joint distribution by hand requires an exponential number of states; however, Bayesian networks leverage the Markov condition to factor a joint probability distribution into a compact product of conditional probability distributions.\nThe theory of d -separation is an alternative to the Markov condition that provides equivalent implications. It provides an algorithmic framework for deriving the conditional independencies encoded by the model. These conditional independence facts are guaranteed to hold in every joint distribution the model represents and, consequently, in any data instance sampled from those distributions. The semantics of holding across all distributions is the main reason why d -separation is useful, enabling two large classes of applications:\n(1) Identification of causal effects: The theory of d -separation connects the causal structure encoded by a Bayesian network to the set of probability distributions it can represent. On this basis, many researchers have developed accompanying theory that describes the conditions under which certain causal effects are identifiable (uniquely known) and algorithms for deriving those quantities from the joint distribution. This work enables sound and complete identification of causal effects, not only with respect to conditioning, but also under counterfactuals and interventions—via the do-calculus introduced by Pearl (2000)— and in the presence of latent variables (Tian and Pearl, 2002; Huang and Valtorta, 2006; Shpitser and Pearl, 2008).\n(2) Constraint-based causal discovery algorithms: Causal discovery, the task of learning generative models of observational data, superficially appears to be a futile endeavor. Yet learning and reasoning about the causal structure that underlies real domains is a primary goal for many researchers. Fortunately, d -separation offers a connection between causal structure and conditional independence. The theory of d -separation can be leveraged to constrain the hypothesis space by eliminating models that are inconsistent with observed conditional independence facts. While many distributions do not lead to uniquely identifiable models, this approach (under simple assumptions) frequently discovers useful causal knowledge for domains that can be represented as a Bayesian network. This approach to learning causal structure is referred to as the constraint-based paradigm, and many algorithms that follow this approach have been developed over the past 20 years, including Inductive Causation (IC) (Pearl and Verma, 1991), PC (Spirtes et al., 2000) and its variants, Three Phase Dependency Analysis (TPDA) (Cheng et al., 1997), Grow-Shrink (Margaritis and Thrun, 1999), Total Conditioning (TC) (Pellet and Elisseeff, 2008), Recursive Autonomy Identification (RAI) (Yehezkel and Lerner, 2009), and hybrid methods that partially employ this approach, including Max-Min Hill Climbing (MMHC) (Tsamardinos et al., 2006) and Hybrid HPC (H2PC) (Gasse et al., 2012).\nAs described above, relational models more closely represent the real-world domains that many social scientists and other researchers investigate. To successfully learn causal models from observational data of relational domains, we need a theory for deriving conditional independence from relational models. In this paper, we formalize the theory of relational d-separation and provide a method for deriving conditional independence facts from the structure of a relational model. In another paper, we have used these results to provide a theoretical framework for a sound and complete constraint-based algorithm—the Relational Causal Discovery (RCD) algorithm (Maier et al., 2013)—that learns causal models of relational domains."
    }, {
      "heading" : "2. Example",
      "text" : "Consider a corporate analyst who was hired to identify which employees are effective and productive for some organization. If the company is structured as a pure project-based organization (for which company personnel are structured around projects, not departments), the analyst may collect data as described by the relational schema in Figure 2.1(a). The schema denotes that employees can collaborate and work on multiple products, each of which is funded by a specific business unit. The analyst has also obtained attributes on each entity—salary and competence of employees, the success of each product, and the budget and revenue of business units. In this example, the organization consists of five employees, five products, and two business units, which are shown in the relational skeleton in Figure 2.1(b).\nAssume that the organization operates under the model depicted in Figure 2.2(a). For example, the success of a product depends on the competence of employees that develop it, and the revenue of a business unit is influenced by the success of products that it funds. If this were known by the analyst (who happens to have experience in graphical models), then it would be conceivable to spot-check the model and test whether some of the conditional independencies encoded by the model are reflected in the data. The analyst then näıvely\nDEVELOPS\nPRODUCT\nSuccess\nEMPLOYEE\nSalary\nCompetence\nBUSINESS-UNIT\nRevenue\nBudget\nFUNDS\n(a) Example relational schema for an organization consisting of employees working on products, which are funded by specific business units within a corporation.\nPaul Competence\nSalary\nQuinn\nRoger\nSally\nThomas\nCase Success\nAdapter Success Laptop\nSuccess Tablet Success\nSmartphone Success\nDEVELOPS\nDE VE LO PS\nAccessories\nRevenue\nBudget\nDevices\nRevenue\nBudget FUN DS\nDE VE LO PS\nD EVELO\nPS\nDEVELOPS\nD EV EL O PS DE VEL\nOP S\nFU ND S\nFUNDS FU ND SFUNDS D EV EL O PS\nDEVELOPS\nCompetence\nSalary Competence\nSalary\nCompetence\nSalary\nCompetence\nSalary\n(b) Example fragment of a relational skeleton. Roger and Sally are employees, both of whom develop the Laptop product, but, of the two, only Sally works on product Tablet. Both products Laptop and Tablet are funded by business unit Devices. For convenience, we depict attribute placeholders on each entity instance.\nFigure 2.1: An example relational schema and skeleton for the organization domain.\napplies d -separation to the model structure in an attempt to derive conditional independencies to test. However, applying d -separation directly to the structure of relational models may not correctly derive conditional independencies, violating the Markov condition. If the analyst were to discover significant and substantive effects, he may believe the model structure is incorrect and needlessly search for alternative dependencies.\nNäıvely applying d -separation to the model in Figure 2.2(a) suggests that employee competence is conditionally independent of the revenue of business units given the success of products: Employee.Competence ⊥⊥ Business-Unit.Revenue | Product.Success To see why this approach is flawed, we must consider the ground graph. A necessary precondition for inference is to apply a model to a data instantiation, which yields a ground graph to which d -separation can be applied. For a Bayesian network, a ground graph consists of replicates of the model structure for each data instance. In contrast, a relational model defines a template for how dependencies apply to a data instantiation, resulting in a ground graph with varying structure. See Section 4 for more details on ground graphs.\nFigure 2.2(b) shows the ground graph for the relational model in Figure 2.2(a) applied to the relational skeleton in Figure 2.1(b). This ground graph illustrates that, for a single employee, simply conditioning on the success of developed products can activate a path through the competence of other employees who develop the same products—we call\nDEVELOPS\nPRODUCT\nSuccess FUNDS\nEMPLOYEE\nSalary\nCompetence\nBUSINESS-UNIT\nRevenue\nBudget\n[PRODUCT, DEVELOPS, EMPLOYEE].Competence [PRODUCT].Success\n[BUSINESS-UNIT].Revenue [BUSINESS-UNIT].Budget\n[EMPLOYEE].Competence [EMPLOYEE].Salary [EMPLOYEE, DEVELOPS, PRODUCT, FUNDS, BUSINESS-UNIT].Budget [EMPLOYEE].Salary\n[BUSINESS-UNIT, FUNDS, PRODUCT].Success [BUSINESS-UNIT].Revenue\n(a) Example relational model. Competence of employees cause the success of products they develop, which in turn influences the revenue received by the business unit funding the product. Additional dependencies involve the budget of business units and employee salaries. The dependencies are specified by relational paths, listed below the graphical model.\nPaul.Competence\nPaul.Salary\nQuinn.Competence\nQuinn.Salary\nRoger.Competence\nRoger.Salary\nSally.Competence\nSally.Salary\nThomas.Competence\nThomas.Salary\nCase.Success Adapter.Success Laptop.Success\nTablet.Success\nSmartphone.Success\nAccessories.Revenue\nAccessories.Budget\nDevices.Revenue\nDevices.Budget\n(b) Example fragment of a ground graph. The success of product Laptop is influenced by the competence of both Roger and Sally. The revenue of business unit Devices is caused by the success of all its funded products—Laptop, Tablet, and Smartphone.\nFigure 2.2: An example relational model and ground graph for the organization domain.\nthis a relationally d-connecting path.1 Checking d -separation on the ground graph indicates that to d -separate an employee’s competence from the revenue of funding business units, we should not only condition on the success of developed products, but also on the competence of other employees who work on those products (e.g., Roger.Competence ⊥⊥ Devices.Revenue | {Laptop.Success, Sally.Competence}).\nThis example also demonstrates that the Markov condition can be violated when directly applied to the structure of a relational model. In this case, the Markov condition according to the model structure in Figure 2.2(a) implies that P (Competence,Revenue | Success) = P (Competence | Success)P (Revenue | Success), that revenue is independent of its nondescendants (competence) given its parents (success). However, the ground graph shows the opposite, for example, P (Roger.Competence, Devices.Revenue | Laptop.Success) 6= P (Roger.Competence |Laptop.Success) P (Devices.Revenue | Laptop.Success). In fact, for this model, d -separation produces many other incorrect judgments of conditional independence. Through simulation, we found that only 25% of the pairs of variables can even be\n1. The indirect effect attributed to a relationally d-connecting path is often referred to as interference, a spillover effect, or a violation of the stable unit treatment value assumption (SUTVA) because the treatment of one instance (employee competence) affects the outcome of another (the revenue of another employee’s business unit).\ndescribed by direct inspection of this model structure, and of those (such as the above example), 75% yield an incorrect conclusion. This is a single data point of a larger empirical evaluation presented in Section 6. Those results provide quantitative details of how often to expect traditional d -separation to fail when applied to the structure of relational models."
    }, {
      "heading" : "3. Semantics and Alternatives",
      "text" : "The example in Section 2 provides a useful basis to describe the semantics imposed by relational d -separation and the characteristics of our approach. There are two primary concepts:\n(1) All-ground-graphs semantics: It might appear that, since the standard rules of d - separation apply to Bayesian networks and the ground graphs of relational models are also Bayesian networks, that applying d -separation to relational models is a non-issue. However, applying d -separation to a single ground graph may result in potentially unbounded runtime if the instantiation is large (i.e., since relational databases can be arbitrarily large). Further, and more importantly, the semantics of d -separation require that conditional independencies hold across all possible model instantiations. Although d -separation can apply directly to a ground graph, these semantics prohibit reasoning about a single ground graph.\nThe conditional independence facts derived from d -separation hold for all distributions represented by a Bayesian network. Analogously, the implications of relational d -separation should hold for all distributions represented by a relational model. It is simple to show that these implications hold for all ground graphs of a Bayesian network—every ground graph consists of a set of disconnected subgraphs, each of which has a structure that is identical to that of the model. However, the set of distributions represented by a relational model depends on both the relational skeleton (constrained by the schema) and the model parameters. That is, the ground graphs of relational models vary with the structure of the underlying relational skeleton (e.g., different products are developed by varying numbers of employees). As a result, answering relational d -separation queries requires reasoning without respect to ground graphs.\n(2) Perspective-based analysis: Relational models make explicit one implicit choice underlying nearly any form of data analysis. This choice—what we refer to here as a perspective—concerns the selection of a particular unit or subject of analysis. For example, in the social sciences, a commonly used acronym is UTOS, for framing an analysis by choosing a unit, treatment, outcome, and setting. Any method, such as Bayesian network modeling, that assumes IID data makes the implicit assumption that the attributes on data instances correspond to attributes of a single unit or perspective. In the example, we targeted a specific conditional independence regarding employee instances (as opposed to products or business units).\nThe concept of perspectives is not new, but it is central to statistical relational learning because relational data sets may be heterogeneous, involving instances that refer to multiple, distinct perspectives. The inductive logic programming (ILP) community has discussed individual-centered representations (Flach, 1999), and many approaches to propositionalizing relational data have been developed to enforce a single perspective in order to rely on existing propositional learning algorithms (Kramer et al., 2001). An alternative strategy is to explicitly acknowledge the presence of multiple perspectives and learn jointly\namong them. This approach underlies many algorithms that learn the types of probabilistic models of relational data applicable in this work, e.g., learning the structure of probabilistic relational models, relational dependency networks, or parametrized Bayesian networks (Friedman et al., 1999; Neville and Jensen, 2007; Schulte et al., 2012).\nOften, data sets are derivative, leading to little or no choice about which perspectives to analyze. However, for relational domains, from which these data sets are derived, it is assumed that there are multiple perspectives, and we can dynamically analyze different perspectives. In the example, we chose the employee perspective, and the analysis focused on the dependence between an employee’s competence and the revenue of business units that fund developed products. However, if the question were posed from the perspective of business units, then we could conceivably condition on the success of products for each business unit. In this scenario, reasoning about d -separation at the model level would lead to a correct conditional independence statement. Some (though fairly infrequent) d -separation queries produce accurate conditional independence facts when applied to relational model structure (see Section 6). However, the model is often unknown, a perspective may be chosen a priori, and a theory that is occasionally correct is clearly undesirable. Additionally, to support constraint-based learning algorithms, it is important to reason about conditional independence implications from different perspectives.\nOne plausible alternative approach would be to answer d -separation queries by ignoring perspectives and considering just the attribute classes (i.e., reason about Competence and Revenue given Success). However, it remains to define explicit semantics for grounding and evaluating the query based on the relational skeleton. There are at least three options:\n• Construct three sets of variables, including all instances of competence, revenue, and success variables: Although the ground graph has the semantics of a Bayesian network, there is only a single ground graph—one data sample (Xiang and Neville, 2011). Consequently, this analysis would be statistically meaningless and is the primary reason why relational learning algorithms dynamically generate propositional data for each instance of a given perspective.\n• Test the Cartesian product of competence and revenue variables, conditioned on all success variables: Testing all pairs invariably leads to independence. Moreover, these semantics are incoherent; only reachable pairs of variables should be compared. For propositional data, variable pairs are constructed by choosing attribute values, e.g., height and weight, within an individual. The same is true for relational data: Only choose the success of products for employees that actually develop them, following the underlying relational connections.\n• Test relationally connected pairs of competence and revenue variables, conditioned on all success variables: Again, this appears plausible based on traditional d -separation; every instance in the table conditions on the same set of success values. Therefore, this is akin to not conditioning because the conditioning variable is a constant.\nWe argue that the desired semantics are essentially the explicit semantics of perspectivebased queries. Therefore, we advocate perspective-based analysis as the only statistically and semantically meaningful approach for relational data and models.\nOur approach to answering relational d -separation queries incorporates the two aforementioned semantics. In Section 5, we describe a new, lifted representation—the abstract\nground graph—that is provably sound and complete in its abstraction of all ground graphs for a given relational model. As their name suggests, abstract ground graphs abstract all ground graphs of a relational model, representing any potential relationally d -connecting path (recall the example d -connecting path that only manifests in the ground graph). A relational model has a corresponding set of abstract ground graphs, one for each perspective (i.e., entity or relationship class in its underlying schema), and can be used to reason about relational d -separation with respect to any given perspective. Figure 3.1 shows a fragment of an abstract ground graph from the employee perspective for the model in Figure 2.2a. The nodes are depicted with their intuitive meaning rather than their actual syntax for this example. Representational details and accompanying theory are presented in Section 5."
    }, {
      "heading" : "4. Concepts of Relational Data and Models",
      "text" : "Propositional representations describe domains with a single entity type, but many realworld systems involve multiple types of interacting entities with probabilistic dependencies among their variables. For example, in the model in Figure 2.2(a) the competence of employees affects the success of products they develop. Many researchers have focused on modeling such domains, which are generally characterized as relational. These relational representations can be divided into two main categories: probabilistic graphical models— such as probablistic relational models (PRMs) (Koller and Pfeffer, 1998), directed acyclic probabilistic entity-relationship (DAPER) models (Heckerman et al., 2004), and relational Markov networks (RMNs) (Taskar et al., 2002)—and probabilistic logic models—such as Bayesian logic programs (BLPs) (Kersting and De Raedt, 2002), Markov logic networks (MLNs) (Richardson and Domingos, 2006), parametrized Bayesian networks (PBNs) (Poole, 2003), Bayesian logic (Blog) (Milch et al., 2005), multi-entity Bayesian networks (MEBNs) (Laskey, 2008), and relational probability models (RPMs) (Russell and Norvig, 2010).\nTo facilitate an extension to the graphical criterion of d -separation, we currently focus on directed, acyclic, graphical models of conditional independence. As most of the above models have similar expressive power, the results in this paper could generalize across representations—even for undirected relational models, such as RMNs and MLNs, after moralization. However, we found it simpler to define and prove relevant theoretical properties for relational d -separation in a representation most similar to Bayesian networks. In\nthis section, we formally define the concepts of relational data and models using a similar representation to PRMs and DAPER models.\nA relational schema is a top-level description of what data exist in a particular domain. Specifically (adapted from Heckerman et al., 2007):\nDefinition 4.1 (Relational schema) A relational schema S = (E ,R,A, card) consists of a set of entity classes E = {E1, . . . , Em}; a set of relationship classes R = {R1, . . . , Rn}, where each Ri = 〈Ei1, . . . , Eiai〉, with Eij ∈ E and ai is the arity for Ri; a set of attribute classes A(I) for each item class I ∈ E ∪ R; and a cardinality function card : R × E → {one, many}.\nA relational schema can be represented graphically with an entity-relationship (ER) diagram. We adopt a slightly modified ER diagram using Barker’s notation (1990), where entity classes are rectangular boxes, relationship classes are diamonds with dashed lines connecting their associated entity classes, attribute classes are ovals residing on entity and relationship classes, and cardinalities are represented with crow’s foot notation.\nExample 4.1 The relational schema S for the organization domain example depicted in Figure 2.1(a) consists of entities E = {Employee, Product, Business-Unit}; relationships R = {Develops, Funds}, where Develops = 〈Employee, Product〉, Funds = 〈Business-Unit, Product〉 and having cardinalities card(Develops, Employee) = many, card(Develops, Product) = many, card(Funds, Business-Unit) = many, and card(Funds, Product) = one; and attributes A(Employee) = {Competence, Salary}, A(Product) = {Success}, and A(Business-Unit) = {Budget, Revenue}.\nA relational schema is a template for a relational skeleton (also referred to as a data graph by Neville and Jensen, 2007), an instantiation of entity and relationship classes. Specifically (adapted from Heckerman et al., 2007):\nDefinition 4.2 (Relational skeleton) A relational skeleton σ for relational schema S = (E ,R,A, card) specifies a set of entity instances σ(E) for each E ∈ E and relationship instances σ(R) for each R ∈ R. Relationship instances adhere to the cardinality constraints of S: If card(R,E) = one, then for each e ∈ σ(E) there is at most one r ∈ σ(R) such that e participates in r.\nFor convenience, we use the notation E ∈ R if entity class E is a component of relationship class R, and, similarly, e ∈ r if entity instance e is a component of the relationship instance r. We also denote the set of all skeletons for schema S as ΣS .\nExample 4.2 The relational skeleton σ for the organization example is depicted in Figure 2.1(b). The sets of entity instances are σ(Employee) = {Paul, Quinn, Roger, Sally, Thomas}, σ(Product) = {Case, Adapter, Laptop, Tablet, Smartphone}, and σ(BusinessUnit) = {Accessories, Devices}. The sets of relationship instances are σ(Develops) = {〈Paul, Case〉, 〈Quinn, Case〉, . . . , 〈Thomas, Smartphone〉} and σ(Funds) = {〈Accessories, Case〉, 〈Accessories, Adapter〉, . . . , 〈Devices, Smartphone〉}. The relationship instances adhere to their cardinality constraints (e.g., Funds is a one-to-many relationship—within σ(Funds), every product has a single business unit, and every business unit may have multiple products).\nIn order to specify a model over a relational domain, we must define a space of possible variables and dependencies. Consider the example dependency [Product, Develops, Employee].Competence → [Product].Success from the model in Figure 2.2(a), expressing that the competence of employees developing a product affects the success of that product. For relational data, the variable space includes not only intrinsic entity and relationship attributes (e.g., success of a product), but also the attributes on other entity and relationship classes that are reachable by paths along the relational schema (e.g., the competence of employees that develop a product). We define relational paths to formalize the notion of which item classes are reachable on the schema from a given item class.2\nDefinition 4.3 (Relational path) A relational path [Ij , . . . , Ik] for relational schema S is an alternating sequence of entity and relationship classes Ij , . . . , Ik ∈ E ∪ R such that:\n(1) For every pair of consecutive item classes [E,R] or [R,E] in the path, E ∈ R. (2) For every triple of consecutive item classes [E,R,E′], E 6= E′.3 (3) For every triple of consecutive item classes [R,E,R′], if R = R′, then card(R,E) =\nmany. Ij is called the base item, or perspective, of the relational path.\nCondition (1) enforces that entity classes participate in adjacent relationship classes in the path. Conditions (2) and (3) remove any paths that would invariably reach an empty terminal set (see Definition 4.4 and Appendix C). This definition of relational paths is similar to “meta-paths” and “relevance paths” in similarity search and information retrieval in heterogeneous networks (Sun et al., 2011; Shi et al., 2012). Relational paths also extend the notion of “slot chains” from the PRM framework (Getoor et al., 2007) by including cardinality constraints and formally describing the semantics under which repeated item classes may appear on a path. Relational paths are also a specialization of the first-order constraints on arc classes imposed on DAPER models (Heckerman et al., 2007).\nExample 4.3 Consider the example relational schema in Figure 2.1(a). Some example relational paths from the Employee perspective (with an intuitive meaning of what the paths describe) include the following: [Employee] (an employee), [Employee, Develops, Product] (products developed by an employee), [Employee, Develops, Product, Funds, Business-Unit] (business units of the products developed by an employee), and [Employee, Develops, Product, Develops, Employee] (co-workers developing the same products). Invalid relational paths include [Employee, Develops, Employee] (because Employee=Employee and Develops ∈ R) and [Business-Unit, Funds, Product, Funds, Business-Unit] (because Product ∈ E and card(Funds, Product) = one).\nRelational paths are defined at the level of relational schemas, and as such are templates for paths in a relational skeleton. An instantiated relational path produces a set of traversals\n2. Because the term “path” is also commonly used to describe chains of dependencies in graphical models, we will explicitly qualify each reference to avoid ambiguity. 3. This condition suggests at first glance that self-relationships (e.g., employees manage other employees, individuals in social networks maintain friendships, scholarly articles cite other articles) are prohibited. We discuss this and other model assumptions in Section 8.\non a relational skeleton. However, the quantity of interest is not the traversals, but the set of reachable item instances (i.e., entity or relationship instances). These reachable instances are the fundamental elements that support model instantiations (i.e., ground graphs).\nDefinition 4.4 (Terminal set) For skeleton σ ∈ ΣS and ij ∈ σ(Ij), the terminal set P |ij for relational path P = [Ij , . . . , Ik] of length n is defined inductively as\nP 1|ij = [Ij ]|ij = {ij} ...\nPn|ij = [Ij , . . . , Ik]|ij = ⋃\nim∈Pn−1|ij\n{ ik | ( (im ∈ ik if Ik ∈ R) ∨ (ik ∈ im if Ik ∈ E) )\n∧ ik /∈ n−1⋃\nl=1\nP l|ij }\nA terminal set of a relational path P = [Ij , . . . , Ik] consists of instances of class Ik, the terminal item on the path. Conceptually, a terminal set is produced by traversing a skeleton beginning at a single instance of the base item class, ij ∈ σ(Ij), following instances of the item classes in the relational path, and reaching a set of instances of class Ik. The term ik /∈ ⋃n−1 l=1 P\nl|ij in the definition implies a “bridge burning” semantics under which no item instances are revisited (ik does not appear in the terminal set of any prefix of P ).4 The notion of terminal sets is a necessary concept for grounding any relational model and has been described in previous work—e.g., for PRMs (Getoor et al., 2007) and MLNs (Richardson and Domingos, 2006)—but has not been explicitly named. We emphasize their importance because terminal sets are also critical for defining relational d -separation, and we formalize the semantics for bridge burning.\nExample 4.4 We can generate terminal sets by pairing the set of relational paths for the schema in Figure 2.1(a) with the relational skeleton in Figure 2.1(b). Let Quinn be our base item instance. Then [Employee]|Quinn = {Quinn}, [Employee, Develops, Product]|Quinn = {Case, Adapter, Laptop}, [Employee, Develops, Product, Funds, Business-Unit]|Quinn = {Accessories, Devices}, and [Employee, Develops, Product, Develops, Employee]|Quinn = {Paul, Roger, Sally}. The bridge burning semantics enforce that Quinn is not also included in this last terminal set.\nFor a given base item class, it is common (depending on the schema) for distinct relational paths to reach the same terminal item class. The following lemma states that if two relational paths with the same base item and the same terminal item differ at some point in the path, then for some relational skeleton and some base item instance, their terminal sets will have a non-empty intersection. This property is important to consider for relational d -separation.\nLemma 4.1 For two relational paths of arbitrary length from Ij to Ik that differ in at least one item class, P1 = [Ij , . . . , Im, . . . , Ik] and P2 = [Ij , . . . , In, . . . , Ik] with Im 6= In, there exists a skeleton σ ∈ ΣS such that P1|ij ∩ P2|ij 6= ∅ for some ij ∈ σ(Ij). 4. The bridge burning semantics yield terminal sets that are necessarily subsets of terminal sets that would\notherwise be produced without bridge burning. Although this appears to be limiting, it actually enables a strictly more expressive class of relational models. See Appendix B for more details and an example.\nProof. See Appendix A.\nExample 4.5 Let P1 = [Employee, Develops, Product, Develops, Employee, Develops, Product], the terminal sets for which yield other products developed by collaborating employees. Let P2 = [Employee, Develops, Product, Funds, Business-Unit, Funds, Product], the terminal sets for which consist of other products funded by the business units funding products developed by a given employee. Intersection among terminal sets for these paths occurs even in the small example skeleton. In fact, the intersection of the terminal sets for P1 and P2 is non-empty for all employees. For example, Paul: P1|Paul = {Adapter, Laptop} and P2|Paul = {Adapter}; Quinn: P1|Quinn = {Tablet} and P2|Quinn = {Tablet, Smartphone}.\nGiven the definition for relational paths, it is simple to define relational variables and their instances.\nDefinition 4.5 (Relational variable) A relational variable [Ij , . . . , Ik].X consists of a relational path [Ij , . . . , Ik] and an attribute class X ∈ A(Ik).\nAs with relational paths, we refer to Ij as the perspective of the relational variable. Relational variables are templates for sets of random variables (see Definition 4.6). Sets of relational variables are the basis of relational d -separation queries, and consequently they are also the nodes of the abstract representation that answers those queries. There is an equivalent formulation in the PRM framework, although not explicitly named (they are simply denoted as attribute classes of K-related item classes via slot chain K). As they are critical to relational d -separation, we provide this concept with an explicit designation.\nExample 4.6 Relational variables for the relational paths in Example 4.3 include intrinsic attributes such as [Employee].Competence and [Employee].Salary, and also attributes on related entity classes such as [Employee, Develops, Product].Success, [Employee, Develops, Product, Funds, Business-Unit].Revenue, and [Employee, Develops, Product, Develops, Employee].Salary.\nDefinition 4.6 (Relational variable instance) For skeleton σ ∈ ΣS and ij ∈ σ(Ij), a relational variable instance [Ij , . . . , Ik].X|ij for relational variable [Ij , . . . , Ik].X is the set of random variables {ik.X | X∈A(Ik) ∧ ik∈ [Ij , . . . , Ik]|ij ∧ ik∈σ(Ik)}.\nTo instantiate a relational variable [Ij , . . . , Ik].X for a specific base item instance ij , we first find the terminal set of the underlying relational path [Ij , . . . , Ik]|ij and then take the X attributes of the Ik item instances in that terminal set. This produces a set of random variables ik.X, which also correspond to nodes in the ground graph. As a notational convenience, if X is a set of relational variables, all from a common perspective Ij , then we say that X|ij for some item ij ∈ σ(Ij) is the union of all instantiations, {x | x∈X|ij ∧ X∈ X}.\nExample 4.7 Instantiating the relational variables from Example 4.6 with base item instance Sally yields [Employee].Competence|Sally = {Sally.Competence}, [Employee,\nDevelops, Product].Success|Sally = {Laptop.Success, Tablet.Success}, [Employee, Develops, Product, Funds, Business-Unit].Revenue|Sally = {Devices.Revenue}, and [Employee, Develops, Product, Develops, Employee].Salary |Sally = {Quinn.Salary, Thomas.Salary}.\nGiven the definitions for relational variables, we can now define relational dependencies.\nDefinition 4.7 (Relational dependency) A relational dependency [Ij , . . . , Ik].Y → [Ij ].X is a directed probabilistic dependence from attribute class Y to X through the relational path [Ij , . . . , Ik].\nDepending on the context, [Ij , . . . , Ik].Y and [Ij ].X can be referred to as treatment and outcome, cause and effect, or parent and child. A relational dependency consists of two relational variables having a common perspective. The relational path of the child is restricted to a single item class, ensuring that the terminal sets consist of a single value. This is consistent with PRMs, except that we explicitly delineate dependencies rather than define parent sets of relational variables. Note that relational variables are not nodes in a relational model, but they form the space of parent variables for relational dependencies. The relational path specification (before the attribute class of the parent) is equivalent to a slot chain, as in PRMs, or the logical constraint on a dependency, as in DAPER models.\nExample 4.8 The dependencies in the relational model displayed in Figure 2.2(a) can be specified as: [Product, Develops, Employee].Competence → [Product].Success (product success is influenced by the competence of the employees developing the product), [Employee].Competence→ [Employee].Salary (an employee’s competence affects his or her salary), [Business-Unit, Funds, Product].Success → [Business-Unit].Revenue (the success of the products funded by a business unit influences that unit’s revenue), [Employee, Develops, Product, Funds, Business-Unit].Budget→[Employee].Salary (employee salary is governed by the budget of the business units for which they develop products), and [Business-Unit].Revenue → [Business-Unit].Budget (the revenue of a business unit influences its budget).\nWe now have sufficient information to define relational models.\nDefinition 4.8 (Relational model) A relational model MΘ consists of two parts: 1. The structure M = (S,D): a schema S paired with a set of relational dependencies D defined over S.\n2. The parameters Θ: a conditional probability distribution P ( [Ij ].X | parents([Ij ].X) )\nfor each relational variable of the form [Ij ].X, where Ij ∈ E ∪ R, X ∈ A(Ij) and parents ( [Ij ].X ) = { [Ij , . . . , Ik].Y | [Ij , . . . , Ik].Y → [Ij ].X ∈ D }\nis the set of parent relational variables.\nThe structure of a relational model can be represented graphically by superimposing dependencies on the ER diagram of a relational schema (see Figure 2.2(a) for an example). A relational dependency of the form [Ij , . . . , Ik].Y → [Ij ].X is depicted as a directed arrow from attribute class Y to X with the specification listed separately. Note that the subset\nof relational variables with singleton paths [I].X in the definition correspond to the set of attribute classes in the schema.\nA common technique in relational learning is to use aggregation functions to transform parent multi-sets to single values within the conditional probability distributions. Typically, aggregation functions are simple, such as mean or mode, but they can be complex, such as those based on vector distance or object identifiers, as in the ACORA system (Perlich and Provost, 2006). However, aggregates are a convenience for increasing power and accuracy during learning, but they are not necessary for model specification.\nThis definition of relational models is consistent with and yields structures expressible as DAPER models (Heckerman et al., 2007). These relational models are also equivalent to PRMs, but we extend slot chains as relational paths and provide a formal semantics for their instantiation. These models are also more general than plate models because dependencies can be specified with arbitrary relational paths as opposed to simple intersections among plates (Buntine, 1994; Gilks et al., 1994).\nJust as the relational schema is a template for skeletons, the structure of a relational model can be viewed as a template for ground graphs: dependencies applied to skeletons.\nDefinition 4.9 (Ground graph) The ground graph GGMσ = (V,E) for relational model structure M = (S,D) and skeleton σ ∈ ΣS is a directed graph with nodes V = { i.X | I ∈\nE ∪ R ∧ X ∈A(I) ∧ i∈ σ(I) } and edges E = { ik.Y → ij .X | ik.Y, ij .X ∈ V ∧ ik.Y ∈\n[Ij , . . . , Ik].Y |ij ∧ [Ij , . . . , Ik].Y → [Ij ].X∈D } .\nA ground graph is a directed graph with (1) a node (random variable) for each attribute of every entity and relationship instance in a skeleton and (2) an edge from ik.Y to ij .X if they belong to the parent and child relational variable instances, respectively, of some dependency in the model. The concept of a ground graph appears for any type of relational model, graphical or logic-based. For example, PRMs produce “ground Bayesian networks” that are structurally equivalent to ground graphs, and Markov logic networks yield ground Markov networks by applying all formulas to a set of constants (Richardson and Domingos, 2006). The example ground graph shown in Figure 2.2(b) is the result of applying the dependencies in the relational model shown in Figure 2.2(a) to the skeleton in Figure 2.1(b).\nSimilar to Bayesian networks, given the parameters of a relational model, a parameterized ground graph can express a joint distribution that factors as a product of the conditional distributions:\nP (GGMΘσ) = ∏\nI∈E∪R\n∏\nX∈A(I)\n∏\ni∈σ(I)\nP ( i.X | parents(i.X) )\nwhere each i.X is assigned the conditional distribution defined for [I].X (a process referred to as parameter-tying).\nRelational models only define coherent joint probability distributions if they produce acyclic ground graphs. A useful construct for checking model acyclicity is the class dependency graph (Getoor et al., 2007), defined as:\nDefinition 4.10 (Class dependency graph) The class dependency graph GM = (V,E) for relational model structureM = (S,D) is a directed graph with a node for each attribute\nof every item class V = { I.X | I∈E ∪ R ∧ X∈A(I) } and edges between pairs of attributes supported by relational dependencies in the model E = { Ik.Y → Ij .X | [Ij , . . . , Ik].Y →\n[Ij ].X∈D } .\nIf the relational dependencies form an acyclic class dependency graph, then every possible ground graph of that model is acyclic as well (Getoor et al., 2007). Given an acyclic relational model, the ground graph has the same semantics as a Bayesian network (Getoor, 2001; Heckerman et al., 2007). All future references to acyclic relational models refer to relational models whose structure forms acyclic class dependency graphs.\nBy Lemma 4.1 and Definition 4.9, one relational dependency may imply dependence between the instances of many relational variables. If there is an edge from ik.Y to ij .X in the ground graph, then there is an implied dependency between all relational variables for which ik.Y and ij .X are elements of their instances.\nExample 4.9 The relational dependency [Employee].Competence→[Employee].Salary yields the edge Roger.Competence → Roger.Salary in the ground graph of Figure 2.2(b) because Roger.Competence ∈ [Employee].Competence|Roger. However, Roger.Competence ∈ [Employee, Develops, Product, Develops, Employee].Competence|Sally (as is Roger.Salary, replacing Competence with Salary). Consequently, the relational dependency implies dependence among the random variables in the instances of [Employee, Develops, Product, Develops, Employee].Competence and [Employee, Develops, Product, Develops, Employee].Salary.\nThese implied dependencies form the crux of the challenge of identifying independence in relational models. Additionally, the intersection between the terminal sets of two relational paths is crucial for reasoning about independence because a random variable can belong to the instances of more than one relational variable. Since d -separation only guarantees independence when there are no d -connecting paths, we must consider all possible paths between pairs of random variables, either of which may be a member of multiple relational variable instances. In Section 5, we define relational d -separation and provide an appropriate representation, the abstract ground graph, that enables straightforward reasoning about d -separation.\n5. Relational d-Separation\nConditional independence facts are correctly entailed by the rules of d -separation, but only when applied to the graphical structure of Bayesian networks. Every ground graph of a Bayesian network consists of a set of identical copies of the model structure (see Appendix D). Thus, the implications of d -separation on Bayesian networks hold for all instances in every ground graph. In contrast, the structure of a relational model is a template for ground graphs, and the structure of a ground graph varies with the underlying skeleton (which is typically more complex than a set of disconnected instances). Conditional independence facts are only useful when they hold across all ground graphs that are consistent with the model, which leads to the following definition:\nDefinition 5.1 (Relational d-separation) Let X, Y, and Z be three distinct sets of relational variables with the same perspective B ∈ E ∪R defined over relational schema S. Then, for relational model structure M, X and Y are d -separated by Z if and only if, for all skeletons σ ∈ ΣS , X|b and Y|b are d -separated by Z|b in ground graph GGMσ for all b ∈ σ(B).\nFor any relational d -separation query, it is necessary that all relational variables in X, Y, and Z have the same perspective (otherwise, the query would be incoherent).5 For X and Y to be d -separated by Z in relational model structure M, d -separation must hold for all instantiations of those relational variables for all possible skeletons. This is a conservative definition, but it is consistent with the semantics of d -separation on Bayesian networks—it guarantees independence, but it does not guarantee dependence. If there exists even one skeleton and faithful distribution represented by the relational model for which X ⊥⊥/ Y | Z, then X and Y are not d -separated by Z.\nGiven the semantics specified in Definition 5.1, answering relational d -separation queries is challenging for several reasons:\nD-separation must hold over all ground graphs: The implications of d -separation on Bayesian networks hold for all possible ground graphs. However, the ground graphs of a Bayesian network consist of identical copies of the structure of the model, and it is sufficient to reason about d -separation on a single subgraph. Although it is possible to verify d -separation on a single ground graph of a relational model, the conclusion may not generalize, and ground graphs can be arbitrarily large.\nRelational models are templates: The structure of a relational model is a directed acyclic graph, but the dependencies are actually templates for constructing ground graphs. The rules of d -separation do not directly apply to relational models, only to their ground graphs. Applying the rules of d -separation to a relational model frequently leads to incorrect conclusions because of unrepresented d -connecting paths that are only manifest in ground graphs.\nInstances of relational variables may intersect : The instances of two different relational variables may have non-empty intersections, as described by Lemma 4.1. These intersections may be involved in relationally d -connecting paths, such as the example in Section 2. As a result, a sound and complete approach to answering relational d -separation queries must account for these paths.\nRelational models may be specified from multiple perspectives: Relational models are defined by relational dependencies, each specified from a single perspective. However, variables in a ground graph may contribute to multiple relational variable instances, each defined from a different perspective. Thus, reasoning about implied dependencies between arbitrary relational variables, such as the one described in Example 4.9, requires a method to translate dependencies across perspectives."
    }, {
      "heading" : "5.1 Abstracting over All Ground Graphs",
      "text" : "The definition of relational d -separation and its challenges suggest a solution that abstracts over all possible ground graphs and explicitly represents the potential intersection between\n5. This trivially holds for d-separation in Bayesian networks as all “propositional” variables have the same implicit perspective.\npairs of relational variable instances. We introduce a new lifted representation, called the abstract ground graph, that captures all dependencies among arbitrary relational variables for all ground graphs, using knowledge of only the schema and the model. To represent all dependencies, the construction of an abstract ground graph uses the extend method, which maps a relational dependency to a set of implied dependencies for different perspectives. Each abstract ground graph of a relational model is defined with respect to a given perspective and can be used to reason about relational d -separation queries for that perspective.\nDefinition 5.2 (Abstract ground graph) An abstract ground graph AGGMB = (V,E) for relational model structure M = (S,D) and perspective B ∈ E ∪ R is a directed graph that abstracts the dependencies D for all ground graphs GGMσ, where σ ∈ ΣS .\nThe set of nodes in AGGMB is V = RV ∪ IV , where • RV is the set of all relational variables of the form [B, . . . , Ij ].X • IV is the set of all pairs of relational variables that could have non-empty intersections\n(referred to as intersection variables):\n{ RV1 ∩ RV2 | RV1,RV2∈RV ∧ RV1 = [B, . . . , Ik, . . . , Ij ].X\n∧ RV2 = [B, . . . , Il, . . . , Ij ].X ∧ Ik 6= Il }\nThe set of edges in AGGMB is E = RVE ∪ IVE , where • RVE ⊂ RV × RV is the set of edges between pairs of relational variables:\nRVE = {\n[B, . . . , Ik].Y → [B, . . . , Ij ].X | [Ij , . . . , Ik].Y → [Ij ].X ∈ D ∧ [B, . . . , Ik] ∈ extend([B, . . . , Ij ], [Ij , . . . , Ik]) }\n• IVE ⊂ IV ×RV ∪ RV ×IV is the set of edges inherited from both relational variables involved in every intersection variable in IV :\nIVE = { Ŷ → [B, . . . , Ij ].X | Ŷ = P1.Y ∩ P2.Y ∈ IV ∧\n(P1.Y → [B, . . . , Ij ].X ∈ RVE ∨ P2.Y → [B, . . . , Ij ].X ∈ RVE ) }\n⋃\n{ [B, . . . , Ik].Y → X̂ | X̂ = P1.X ∩ P2.X ∈ IV ∧\n([B, . . . , Ik].Y → P1.X ∈ RVE ∨ [B, . . . , Ik].Y → P2.X ∈ RVE ) }\nThe extend method is described in Definition 5.3 below. Essentially, the construction of an abstract ground graph for relational model structureM and perspectiveB ∈ E∪R follows three simple steps: (1) Add a node for all relational variables from perspective B.6 (2) Insert edges for the direct causes of every relational variable by translating the dependencies in D using extend. (3) For each pair of potentially intersecting relational variables, create a new node that inherits the direct causes and effects from both participating relational variables in the intersection. Then, to answer queries of the form “Are X and Y d -separated by\nZ?” simply (1) augment X, Y, and Z with their corresponding intersection variables that they subsume and (2) apply the rules of d -separation on the abstract ground graph for the common perspective of X, Y, and Z. Since abstract ground graphs are defined from a specific perspective, every relational model produces a set of abstract ground graphs, one for each perspective in its underlying schema.\nExample 5.1 Figure 5.1 shows the abstract ground graph AGGM,Employee for the organization example from the Employee perspective with hop threshold h = 6.7 As in Section 2, we derive an appropriate conditioning set Z in order to d -separate individual employee competence (X = {[Employee].Competence}) from the revenue of the employee’s funding business units (Y = {[Employee, Develops, Product, Funds, BusinessUnit].Revenue}). Applying the rules of d -separation to the abstract ground graph, we see that it is necessary to condition on both product success ([Employee, Develops, Product].Success) and the competence of other employees developing the same products ([Employee, Develops, Product, Develops, Employee].Competence). For h = 6, augmenting X, Y, and Z with their corresponding intersection variables does not result in any changes. For h = 8, the abstract ground graph includes a node for relational variable [Employee, Develops, Product, Develops, Employee, Develops, Product, Funds, Business-Unit].Revenue (the revenue of the business units funding the other products of collaborating employees) which, by Lemma 4.1, could have a non-empty intersection with [Employee, Develops, Product, Funds, Business-Unit].Revenue. Therefore, Y would also include the intersection with this other relational variable. However, for this query, the conditioning set Z for h = 6 happens to also d -separate at h = 8 (and any h ∈ N0).\nUsing the algorithm devised by Geiger et al. (1990), relational d -separation queries can be answered in O(|E|) time with respect to the number of edges in the abstract ground graph. In practice, the size of an abstract ground graph depends on the relational schema and model (e.g., the number of entity classes, the types of cardinalities, the number of dependencies—see the experiment in Section 7.1), as well as the hop threshold limiting the length of relational paths. For the example in Figure 5.1, the abstract ground graph has 7 nodes and 7 edges (including 1 intersection node with 2 edges); for h = 8, it would have 13 nodes and 21 edges (including 4 intersection nodes with 13 edges). Abstract ground graphs are invariant to the size of ground graphs, even though ground graphs can be arbitrarily large—that is, relational databases have no maximum size.\nNext, we formally define the extend method, which is used internally for the construction of abstract ground graphs. This method translates dependencies specified in the model into dependencies in the abstract ground graph.\n6. In theory, abstract ground graphs can have an infinite number of nodes as relational paths may have no bound. In practice, a hop threshold h ∈ N0 is enforced to limit the length of these paths. Hops are defined as the number of times the path “hops” between item classes in the schema, or one less than the length of the path. 7. The variables Salary and Budget are removed for simplicity. They are irrelevant for this d-separation example as they are solely effects of other variables.\nDefinition 5.3 (Extending relational paths) Let Porig and Pext be two relational paths for schema S. The following three functions extend Porig with Pext : extend(Porig , Pext) = { P =P 1,no−i+1orig +P i+1,ne ext | i∈pivots(reverse(Porig), Pext) ∧ isValid(P ) }\npivots(P1, P2) = {i | P 1,i1 = P 1,i2 }\nisValid(P ) = { True if P does not violate Definition 4.3 False otherwise\nwhere no is the length of Porig , ne is the length of Pext , P i,j corresponds to 1-based iinclusive, j-inclusive subpath indexing, + is concatenation of paths, and reverse is a method that reverses the order of the path.\nThe extend method constructs a set of valid relational paths from two input relational paths. It first finds the indices (called pivots) of the item classes for which the input paths (reverse(Porig) and Pext) have a common starting subpath. Then, it concatenates the two input paths at each pivot, removing one of the duplicated subpaths (see Example 5.2). Since d -separation requires blocking all paths of dependence between two sets of variables, the extend method is critical to ensure the soundness and completeness of our approach. The abstract ground graph must capture all paths of dependence among the random variables in the relational variable instances for all represented ground graphs. However, relational model structures are specified by relational dependencies, each from a given perspective and with outcomes that have singleton relational paths. The extend method is called repeatedly during the creation of an abstract ground graph, with Porig set to some relational path and Pext drawn from the relational path of the treatment in some relational dependency.\nExample 5.2 During the construction of the abstract ground graph AGGM,Employee depicted in Figure 5.1, the extend method is called several times. First, all relational variables\nfrom the Employee perspective are added as nodes in the graph. Next, extend is used to insert edges corresponding to direct causes. Consider the node for [Employee, Develops, Product].Success. The construction of AGGM,Employee calls extend(Porig , Pext) with Porig = [Employee, Develops, Product] and Pext = [Product, Develops, Employee] because [Product, Develops, Employee].Competence→ [Product].Success∈ D. Here, extend(Porig , Pext) = {[Employee], [Employee, Develops, Product, Develops, Employee]}, which leads to the insertion of two edges in the abstract ground graph. Note that pivots(reverse(Porig), Pext) = {1, 2, 3}, and the pivot at i = 2 yields the invalid relational path [Employee, Develops, Employee].\nWe also describe two important properties of the extend method with the following two lemmas. The first lemma states that every relational path produced by extend yields a terminal set for some skeleton such that there is an item instance also reachable by the two original paths. This lemma is useful for proving the soundness of our abstraction: All edges inserted in an abstract ground graph correspond to edges in some ground graph.\nLemma 5.1 Let Porig = [I1, . . . , Ij ] and Pext = [Ij , . . . , Ik] be two relational paths with P = extend(Porig , Pext). Then, ∀P ∈ P there exists a relational skeleton σ ∈ ΣS such that ∃i1 ∈ σ(I1) such that ∃ik ∈ P |i1 and ∃ij ∈ Porig |i1 such that ik ∈ Pext |ij .\nProof. See Appendix A.\nExample 5.3 Let σ be the skeleton shown in Figure 2.1(b), let Porig = [Employee, Develops, Product], let Pext = [Product, Develops, Employee], and let i1 = Sally ∈ σ(Employee). From Example 5.2, we know that P = extend(Porig , Pext) = {[Employee], [Employee, Develops, Product, Develops, Employee]}. We also have [Employee]|Sally = {Sally} and [Employee, Develops, Product, Develops, Employee]|Sally = {Quinn, Roger, Thomas}. By Lemma 5.1, there should exist an ij ∈ Porig |i1 such that Sally and at least one of Quinn, Roger, and Thomas are in the terminal set Pext |ij . We have Porig |Sally = {Laptop, Tablet}, and Pext |Laptop = {Quinn, Roger, Sally} and Pext |Tablet = {Sally, Thomas}. So, the lemma clearly holds for this example.\nLemma 5.1 guarantees that, for some relational skeleton, there exists an item instance in the terminal sets produced by extend that also appears in the terminal set of Pext via some instance in the terminal set of Porig . It is also possible (although infrequent) that there exist items reachable by Porig and Pext that are not in the terminal set of any path produced with extend(Porig , Pext). The following lemma describes this unreachable set of items, stating that there must exist an alternative relational path P ′orig that intersects with Porig and, when using extend, catches those remaining items. This lemma is important for proving the completeness of our abstraction: All edges in all ground graphs are represented in the abstract ground graph.\nLemma 5.2 Let σ ∈ ΣS be a relational skeleton, and let Porig = [I1, . . . , Ij ] and Pext = [Ij , . . . , Ik] be two relational paths with P = extend(Porig , Pext). Then, ∀i1 ∈ σ(I1) ∀ij ∈ Porig |i1 ∀ik ∈Pext |ij if ∀P ∈ P ik /∈ P |i1, then ∃P ′orig such that Porig |i1 ∩ P ′orig |i1 6= ∅ and ik ∈ P ′|i1 for some P ′ ∈ extend(P ′orig , Pext).\nProof. See Appendix A.\nExample 5.4 Although Lemma 5.2 does not apply to the organization domain as currently represented, it could apply if either (1) there were cycles in the relational schema or (2) the path specifications on the relational dependencies included a cycle. Consider additional relationships between employees and products. If employees could be involved with products at various stages (e.g., research, development, testing, marketing), then there would be alternative relational paths for which the lemma might apply. The proof of the lemma in Appendix A provides abstract conditions describing when the lemma applies."
    }, {
      "heading" : "5.2 Proof of Correctness",
      "text" : "The correctness of our approach to relational d -separation relies on several facts: (1) d - separation is valid for directed acyclic graphs; (2) ground graphs are directed acyclic graphs; and (3) abstract ground graphs are directed acyclic graphs that represent exactly the edges that could appear in all possible ground graphs. It follows that d -separation on abstract ground graphs, augmented by intersection variables, is sound and complete for all ground graphs.8 Additionally, we show that since relational d -separation is sound and complete, it is also equivalent to the Markov condition for relational models. Using the previous definitions and lemmas, the following sequence of results proves the correctness of our approach to identifying independence in relational models.\nTheorem 5.1 The rules of d-separation are sound and complete for directed acyclic graphs.\nProof. Due to Verma and Pearl (1988) for soundness and Geiger and Pearl (1988) for completeness.\nTheorem 5.1 implies that (1) all conditional independence facts derived by d -separation on a Bayesian network structure hold in any distribution represented by that model (soundness) and (2) all conditional independence facts that hold in a faithful distribution can be inferred from d -separation applied to the Bayesian network that encodes the distribution (completeness).\nLemma 5.3 For every acyclic relational model structure M and skeleton σ ∈ ΣS , the ground graph GGMσ is a directed acyclic graph.\nProof. Due to both Heckerman et al. (2007) for DAPER models and Getoor (2001) for PRMs.\nBy Theorem 5.1 and Lemma 5.3, d -separation is sound and complete when applied to a ground graph. However, Definition 5.1 states that relational d -separation must hold across all possible ground graphs, which is the reason for constructing the abstract ground graph representation.\n8. In Appendix E, we provide proofs of soundness and completeness for abstract ground graphs and relational d-separation that are limited by practical hop threshold bounds.\nTheorem 5.2 For every acyclic relational model structure M and perspective B ∈ E ∪ R, the abstract ground graph AGGMB is sound and complete for all ground graphs GGMσ with skeleton σ ∈ ΣS .\nProof. See Appendix A.\nTheorem 5.2 guarantees that, for a given perspective, an abstract ground graph captures all possible paths of dependence between any pair of variables in any ground graph. The details of the proof provide the reasons why explicitly representing intersection variables is necessary for ensuring a sound and complete abstraction.\nTheorem 5.3 For every acyclic relational model structure M and perspective B ∈ E ∪ R, the abstract ground graph AGGMB is directed and acyclic.\nProof. See Appendix A.\nTheorem 5.3 ensures that the standard rules of d -separation can apply directly to abstract ground graphs because they are acyclic given an acyclic model. We now have sufficient supporting theory to prove that d -separation on abstract ground graphs is sound and complete. In the following theorem, we define W̄ as the set of nodes augmented with their corresponding intersection nodes for the set of relational variables W: W̄ = W ∪ ⋃W∈W{W ∩W ′ | W ∩W ′ is an intersection node in AGGMB}.\nTheorem 5.4 Relational d-separation is sound and complete for abstract ground graphs. Let M be an acyclic relational model structure, and let X, Y, and Z be three distinct sets of relational variables for perspective B ∈ E ∪ R defined over relational schema S. Then, X̄ and Ȳ are d-separated by Z̄ on the abstract ground graph AGGMB if and only if for all skeletons σ ∈ ΣS and for all b ∈ σ(B), X|b and Y|b are d-separated by Z|b in ground graph GGMσ.\nProof. We must show that d -separation on an abstract ground graph implies d -separation on all ground graphs it represents (soundness) and that d -separation facts that hold across all ground graphs are also entailed by d -separation on the abstract ground graph (completeness).\nSoundness: Assume that X̄ and Ȳ are d -separated by Z̄ on AGGMB. Assume for contradiction that there exists an item instance b ∈ σ(B) such that X|b and Y|b are not d -separated by Z|b in the ground graph GGMσ for some arbitrary skeleton σ. Then, there must exist a d -connecting path p from some x ∈ X|b to some y ∈ Y|b given all z ∈ Z|b. By Theorem 5.2, AGGMB is complete, so all edges in GGMσ are captured by edges in AGGMB. So, path p must be represented from some node in {Nx | x ∈ Nx|b} to some node in {Ny | y ∈ Ny|b}, where Nx, Ny are nodes in AGGMB. If p is d -connecting in GGMσ, then it is d -connecting in AGGMB, implying that X̄ and Ȳ are not d -separated by Z̄. So, X|b and Y|b must be d -separated by Z|b.\nCompleteness: Assume that X|b and Y|b are d -separated by Z|b in the ground graph GGMσ for all skeletons σ for all b ∈ σ(B). Assume for contradiction that X̄ and Ȳ are not d -separated by Z̄ on AGGMB. Then, there must exist a d -connecting path p for some\nrelational variable X ∈ X̄ to some Y ∈ Ȳ given all Z ∈ Z̄. By Theorem 5.2, AGGMB is sound, so every edge in AGGMB must correspond to some pair of variables in some ground graph. So, if p is d -connecting in AGGMB, then there must exist some skeleton σ such that p is d -connecting in GGMσ for some b ∈ σ(B), implying that d -separation does not hold for that ground graph. So, X̄ and Ȳ must be d -separated by Z̄ on AGGMB.\nTheorem 5.4 proves that d -separation on abstract ground graphs is a sound and complete solution to identifying independence in relational models. Theorem 5.1 also implies that the set of conditional independence facts derived from abstract ground graphs is exactly the same as the set of conditional independencies that all distributions represented by all possible ground graphs have in common.\nCorollary 5.1 X̄ and Ȳ are d-connected given Z̄ on the abstract ground graph AGGMB if and only if there exists a skeleton σ ∈ ΣS and an item instance b ∈ σ(B) such that X|b and Y|b are d-connected given Z|b in ground graph GGMσ.\nCorollary 5.1 is logically equivalent to Theorem 5.4. While a simple restatement of the previous theorem, it is important to emphasize that relational d -separation claims d - connection if and only if there exists a ground graph for which X|b and Y |b are d -connected given Z|b. This implies that there may be some ground graphs for which X|b and Y |b are d -separated by Z|b, but the abstract ground graph still claims d -connection. This may happen if the relational skeleton does not enable certain underlying relational connections. For example, if the relational skeleton in Figure 2.1(b) included only products that were developed by a single employee, then there would be no relationally d -connecting path in the example in Section 2. If this is a fundamental property of the domain (e.g., there are products developed by a single employee and products developed by multiple employees), then revising the underlying schema to include two different classes of products would yield a more accurate model implying a larger set of conditional independencies.\nAdditionally, we can show that relational d -separation is equivalent to the Markov condition on relational models.\nDefinition 5.4 (Relational Markov condition) Let X be a relational variable for perspective B ∈ E ∪ R defined over relational schema S. Let nd(X) be the non-descendant variables of X, and let pa(X) be the set of parent variables of X. Then, for relational model MΘ, P ( X | nd(X), pa(X) ) = P ( X | pa(X) ) if and only if ∀x∈X|b P ( x | nd(x), pa(x) ) =\nP ( x | pa(x) ) in parameterized ground graph GGMΘσ for all skeletons σ ∈ ΣS and for all b ∈ σ(B).\nIn other words, a relational variable X is independent of its non-descendants given its parents if and only if, for all possible parameterized ground graphs, the Markov condition holds for all instances of X. For Bayesian networks, the Markov condition is equivalent to d -separation (Neapolitan, 2004). Because parameterized ground graphs are Bayesian networks (implied by Lemma 5.3) and relational d -separation on abstract ground graphs is sound and complete (by Theorem 5.4), we can conclude that relational d -separation is equivalent to the relational Markov condition.\n6. Näıve Relational d-Separation Is Frequently Incorrect\nIf the rules of d -separation for Bayesian networks were applied directly to the structure of relational models, how frequently would the conditional independence conclusions be correct? In this section, we evaluate the necessity of our approach—relational d -separation executed on abstract ground graphs. We empirically compare the consistency of a näıve approach against our sound and complete solution over a large space of synthetic causal models. To promote a fair comparison, we restrict the space of relational models to those with underlying dependencies that could feasibly be represented and recovered by a näıve approach. We describe this space of models, present a reasonable approach for applying traditional d -separation to the structure of relational models, and quantify its decrease in expressive power and accuracy.\nConsider the following limited definition of relational paths, which itself limits the space of models and conditional independence queries. A simple relational path P = [Ij , . . . , Ik] for relational schema S is a relational path such that Ij 6= · · · 6= Ik. The sole difference between relational paths (Definition 4.3) and simple relational paths is that no item class may appear more than once along the latter. This yields paths drawn directly from a schema diagram. For the example in Figure 2.1(a), [Employee, Develops, Product] is simple whereas [Employee, Develops, Product, Develops, Employee] is not.\nAdditionally, we define simple relational schemas such that, for any two item classes Ij , Ik ∈ E ∪ R, there exists at most one simple relational path between them (i.e., no cycles occur in the schema diagram). The example in Figure 2.1(a) is a simple relational schema. The restriction to simple relational paths and schemas yields similar definitions for simple relational variables, simple relational dependencies, and simple relational models. The relational model in Figure 2.2(a) is simple because it includes only simple relational dependencies.\nA first approximation to relational d -separation would be to apply the rules of traditional d -separation directly to the graphical representation of relational models. This is equivalent to applying d -separation to the class dependency graph GM (see Definition 4.10) of relational modelM. The class dependency graph for the model in Figure 2.2(a) is shown in Figure 6.1(a). Note that the class dependency graph ignores path designators on dependencies, does not include all implications of dependencies among arbitrary relational variables, and does not represent intersection variables.\nAlthough the class dependency graph is independent of perspectives, testing any conditional independence fact requires choosing a perspective. All relational variables must have a common base item class; otherwise, no method can produce a single consistent, propositional table from a relational database. For example, consider the construction of a table describing employees with columns for their salary, the success of products they develop, and the revenue of the business units they operate under. This procedure requires joining the instances of three relational variables ([Employee].Salary, [Employee, Develops, Product].Success, and [Employee, Develops, Product, Funds, Business-Unit].Revenue) for every common base item instance, from Paul to Thomas. See, for example, the resulting propositional table for these relational variables and an example query in Table D.1 and Figure D.2, respectively. An individual relational variable requires joining the item classes within its relational path, but combining a collection of relational variables requires joining\non their common base item class. Fortunately, given a perspective and the space of simple relational schemas and models, a class dependency graph is equivalent to a simple abstract ground graph.\nDefinition 6.1 (Simple abstract ground graph) For simple relational model M = (S,D) and perspective B ∈ E∪R, the simple abstract ground graph AGGsMB is the directed acyclic graph (V,E) that abstracts the dependencies D among simple relational variables. The nodes consist of simple relational variables { [B, . . . , Ij ].X | B 6= · · · 6= Ij } , and the\nedges connect those nodes {\n[B, . . . , Ik].Y → [B, . . . , Ij ].X | [Ij , . . . , Ik].Y → [Ij ].X ∈ D ∧ [B, . . . , Ik] ∈ extend([B, . . . , Ij ], [Ij , . . . , Ik]) ∧ [B, . . . , Ik].Y, [B, . . . , Ij ].X ∈ V } .\nSimple abstract ground graphs only include nodes for simple relational variables and necessarily exclude intersection variables. Lemma 4.1—which characterizes the intersection between a pair of relational paths—only applies to pairs of simple relational paths if the schema contains cycles, which is not the case for simple relational schemas by definition. As a result, the simple abstract ground graph for a given schema and model contains the same number of nodes and edges, regardless of perspective; the nodes simply have path designators redefined from the given perspective. Figure 6.1(b) shows three simple abstract ground graphs from distinct perspectives for the model in Figure 2.2(a). As noted above, simple abstract ground graphs are qualitatively the same as the class dependency graph, but they enable answering relational d -separation queries, which requires a common perspective in order to be semantically meaningful.\nThe näıve approach to relational d -separation derives conditional independence facts from simple abstract ground graphs (Definition 6.1). The sound and complete approach\ndescribed in this paper applies d -separation—with input variable sets augmented by their intersection variables—to “regular” abstract ground graphs, as described by Definition 5.2. Clearly, if d -separation on a simple abstract ground graph claims that X is d -separated from Y given Z, then d -separation on the regular abstract ground graph yields the same conclusion if and only if there are no d -connecting paths in the regular abstract ground graph. Either X and Y can be d -separated by a set of simple relational variables Z, or they require non-simple relational variables—those involving relational paths with repeated item classes.9\nTo evaluate the necessity of regular abstract ground graphs (i.e., the additional paths involving non-simple relational variables and intersection variables), we compared the frequency of equivalence between the conclusions of d -separation on simple and regular abstract ground graphs. The two approaches are only equivalent if a minimal separating set consists entirely of simple relational variables.10\nThus, for an arbitrary pair of relational variables X and Y with a common perspective, we test the following on regular abstract ground graphs:\n1. Is either X or Y a non-simple relational variable?\n2. Are X and Y marginally independent?\n3. Does a minimal separating set Z d -separate X and Y, where Z consists solely of simple relational variables?\n4. Is there any separating set Z that d -separates X and Y ?\nIf the answer to (1) is yes, then the näıve approach cannot apply since either X or Y is undefined for the simple abstract ground graph. If the answer to (2) is yes, then there is equivalence; this is a trivial case because there are no d -connecting paths for Z = ∅. If the answer to (3) is yes, then there is a minimal separating set Z consisting of only simple relational variables. In this case, the simple abstract ground graph is sufficient, and we also have equivalence. If the answer to (4) is no, then no separating set Z, simple or otherwise, renders X and Y conditionally independent.\nWe randomly generated simple relational schemas and models for 100 trials for each setting using the following parameters:\n• Number of entity classes, ranging from 1 to 4. • Number of relationship classes, fixed at one less than the number of entities, ensuring\nsimple, connected relational schemas. Relationship cardinalities are chosen uniformly at random.\n• Number of attributes for each entity and relationship class, randomly drawn from a shifted Poisson distribution with λ = 1.0 (∼ Pois(1.0) + 1). • Number of dependencies in the model, ranging from 1 to 10.\n9. The theoretical conditions under which equivalence occurs are sufficiently complex that they provide little utility as they essentially require reconstructing the regular abstract ground graph and checking a potentially exponential number of dependency paths. 10. If X and Y are d-separated given Z, then Z is a separating set for X and Y. A separating set Z is minimal if there is no proper subset of Z that is also a separating set.\nThen, for all pairs of relational variables with a common perspective limited by a hop threshold of h = 4, we ran the aforementioned tests against the regular abstract ground graph, limiting its relational variables by a hop threshold of h = 8 (a sufficient hop threshold for soundness and completeness—see Appendix E).\nThis procedure generated a total of almost 3.6 million pairs of relational variables to test. Approximately 56% included a non-simple relational variable; the näıve approach cannot be used to derive a conditional independence statement in these cases, requiring the full abstract ground graph in order to represent these variables. Of the remaining 44% (roughly 1.6 million), 82% were marginally independent, and 9% were not conditionally independent given any conditioning set Z. Then, of the remaining 9% (roughly 145,000), we can test the frequency of equivalence for conditional independence facts with non-empty separating\nsets—the proportion of cases for which only simple relational variables are required in a minimal separating set Z.\nFigure 6.2 shows this frequency for schemas of increasing numbers of entity classes (1–4) for varying numbers of dependencies in the causal model (1–10). Since relational schemas with a single entity class and no relationships describe propositional data, the simple abstract ground graph is equivalent to the full abstract ground graph, which is also equivalent to the model itself. In this case, the näıve approach is always equivalent because it is exactly d -separation on Bayesian networks. For truly relational schemas (with more than one entity class and at least one relationship class), the frequency of equivalence decreases as the number of dependencies in the model increases. Additionally, the frequency of equivalence decreases more as the number of entities in the schema increases. For example, the frequency of equivalence for nine dependencies is 60.3% for two entities, 51.2% for three entities, and 43.2% for four entities.\nWe also learned statistical models that predict the number of equivalent and nonequivalent statements in order to identify key factors that affect the frequency of equivalence. We found that the number of dependencies and size of the relational model (regulated by the number of entities and many cardinalities) dictate the equivalence. As a relational model deviates from a Bayesian network, we should expect more d -connecting paths in the regular but not simple abstract ground graph. This property also depends on the specific combination of dependencies in the model. Appendix F presents details of this analysis.\nThis experiment suggests that applying traditional d -separation directly to a relational model structure will frequently derive incorrect conditional independence facts. Additionally, there is a large class of conditional independence queries involving non-simple variables for which such an approach is undefined. These results indicate that fully specifying abstract ground graphs and applying d -separation augmented with intersection variables (as described in Section 5) is critical for accurately deriving most conditional independence facts from relational models."
    }, {
      "heading" : "7. Experiments",
      "text" : "To complement the theoretical results, we present three experiments on synthetic data. The primary goal of these empirical results is to demonstrate the feasibility of applying relational d -separation in practice. The experiment in Section 7.1 describes the factors that influence the size of abstract ground graphs and thus the computational complexity of relational d -separation. The experiment in Section 7.2 evaluates the growth rate of separating sets produced by relational d -separation as abstract ground graphs become large. The results indicate that minimal separating sets grow much more slowly than abstract ground graphs. The experiment in Section 7.3 tests how the expectations of the relational d -separation theory match statistical conclusions on simulated data. As expected from the proofs of correctness in Section 5.2, the results indicate a close match, aside from Type I errors and certain biases of conventional statistical tests on relational data."
    }, {
      "heading" : "7.1 Abstract Ground Graph Size",
      "text" : "Relational d -separation is executed on abstract ground graphs. Consequently, it is important to quantify the size of abstract ground graphs and identify which factors influence their\nsize. We randomly generated relational schemas and models for 1,000 trials of each setting using the following parameters:\n• Number of entity classes, ranging from 1 to 4. • Number of relationship classes, ranging from 0 to 4. The schema is guaranteed to be\nfully connected and includes at most a single relationship between a pair of entities. Relationship cardinalities are selected uniformly at random.\n• Number of attributes for each entity and relationship class, randomly drawn from a shifted Poisson distribution with λ = 1.0 (∼ Pois(1.0) + 1). • Number of dependencies in the model, ranging from 1 to 15.\nThis procedure generated a total of 450,000 abstract ground graphs, which included every perspective (all entity and relationship classes) for each experimental combination. We measure size as the number of nodes and edges in a given abstract ground graph. Figure 7.1(a) depicts how the size of abstract ground graphs varies with respect to the number of many cardinalities in the schema (fixed for models with 10 dependencies), and Figure 7.1(b) shows how it varies with respect to the number of dependencies in the model. Recall that for a single entity, abstract ground graphs are equivalent to Bayesian networks.\nTo determine the most influential factors of abstract ground graph size, we ran log-linear regression using independent variables that describe only the schema and model. Detailed results are provided in Appendix G. This analysis indicates that (1) as the number of entities, relationships, attributes, and many cardinalities increases, the number of nodes and edges grows at an exponential rate. (2) As the number of dependencies in the model increases,\nthe number of edges increases linearly, but the number of nodes remains invariant. And (3) abstract ground graphs for relationship perspectives are larger than entity perspectives because more relational variables can be defined."
    }, {
      "heading" : "7.2 Minimal Separating Set Size",
      "text" : "Because abstract ground graphs can become large, one might expect that separating sets could also grow to impractical sizes. Fortunately, relational d -separation produces minimal separating sets that are empirically observed to be small. We ran 1,000 trials of each setting using the following parameters:\n• Number of entity classes, ranging from 1 to 4. • Number of relationship classes, fixed at one less than the number of entities. Rela-\ntionship cardinalities are selected uniformly at random.\n• Total number of attributes across entity and relationship classes, fixed at 10. • Number of dependencies in the model, ranging from 1 to 10.\nFor each relational model, we identified a single minimal separating set for up to 100 randomly chosen pairs of conditionally independent relational variables. This procedure generated almost 2.5 million pairs of variables.\nTo identify a minimal separating set between relational variables X and Y, we modified Algorithm 4 devised by Tian et al. (1998) by starting with all parents of X̄ and Ȳ, the variables augmented with the intersection variables they subsume in the abstract ground graph. While the discovered separating sets are minimal, they are not necessarily of minimum size because of the greedy process for removing conditioning variables from the separating set. Figure 7.2 shows the frequency of separating set size as both the number of entities and dependencies vary. In summation, roughly 83% of the pairs are marginally independent (having empty separating sets), 13% have separating sets of size one, and less than 0.1% have separating sets with more than five variables. The experimental results indicate that separating set size is strongly influenced by model density, primarily because the number of potential d -connecting paths increases as the number of dependencies increases."
    }, {
      "heading" : "7.3 Empirical Validity",
      "text" : "As a practical demonstration, we examined how the expectations of the relational d - separation theory match the results of statistical tests on actual data. We use a standard procedure for empirically measuring internal validity of algorithms. In this case, we (1) randomly generate a relational schema, (2) randomly generate a relational model structure for that schema, (3) parameterize the model structure, (4) generate synthetic data according to the model structure and parameters, (5) randomly choose relational d -separation queries according to the known ground-truth model, and (6) compare the model theory (i.e., the d - separation conclusions) against corresponding statistical tests of conditional independence.\nFor steps (1) and (2), we randomly generated a relational schema S and relational model structure M for S for 100 trials using the following settings: • Number of entity classes, ranging from 1 to 4. • Number of relationship classes, fixed at one less than the number of entities. Rela-\ntionship cardinalities are selected uniformly at random.\n• Number of attributes for each entity and relationship class, randomly drawn from a shifted Poisson distribution with λ = 1.0 (∼ Pois(1.0) + 1). • Number of dependencies in the model, fixed at 10.\nDependencies were selected greedily, choosing each one uniformly at random, subject to a maximum of 3 parent relational variables for each attribute [Ij ].X and enforcing acyclicity of the model structure.\nFor step (3), we parameterized relational models using simple additive linear equations with independent, normally distributed error and the average aggregate for relational variable instances. For each attribute [Ij ].X, we assign a conditional probability distribution\n∑\n[Ij ,...,Ik].Y ∈parents([Ij ].X)\n( β · avg([Ij , . . . , Ik].Y ) ) + 0.1\nif [Ij ].X has parents, where\nβ = 0.9\n|parents([Ij ].X)|\nto provide equal contribution for each direct cause and ∼ N(0, 1) (error drawn from a standard normal distribution). If [Ij ].X has no parents, its value is just drawn from .\nFor step (4), we first generated a relational skeleton σ (because the current model space assumes that attributes do not cause entity or relationship existence) and then populated each attribute value by drawing from its corresponding conditional distribution. Each entity class is initialized to 1,000 instances. Relationship instances were constructed via a latent homophily process, similar to the method used by Shalizi and Thomas (2011). Each entity instance received a single latent variable, marginally independent from all other variables. The probability of any relationship instance was drawn from\ne−αd\n1 + e−αd ,\nthe inverse logistic function, where d = |LE1 − LE2 |, the difference between the latent variables on the two entities, and α = 10, set as the decay parameter. We also scaled\nIndependence in Models of Relational Data\n1 entity 2 entities 3 entities 4 entities\nD-SEPARATED QUERIES\n0 0. 00 5\n0. 05 0. 5\nA vg\n. s tre\nng th\no f e ffe ct\n1 entity 2 entities 3 entities 4 entities\nD-CONNECTED QUERIES\n0 0. 00 5\n0. 05 0. 5\nA vg\n. s tre\nng th\no f e ffe ct\nthe probabilities in order to produce an expected degree of five for each entity instance when the cardinality of the relationship is many. Since the latent variables are marginally independent of all others, they are safely omitted from abstract ground graphs; their sole purpose is to generate relational skeletons that provide a greater probability of non-empty intersection variables as opposed to a random underlying link structure. We generated 100 independent relational skeletons and attribute values (i.e., 100 instantiated relational databases) for each schema and model.\nStep (5) randomly chooses up to 100 true and false relational d -separation queries for a given model.11 Since we have the ground-truth model, we can evaluate with our approach (abstract ground graphs and relational d -separation) whether these queries are true (d - separated) or false (d -connected). Each query is of the form X ⊥⊥ Y | Z such that X and Y are single relational variables, Z is a set of relational variables, Y has a singleton relational path (e.g., [Ik].Y ), and all variables are from a common perspective. These queries correspond to testing potential direct causal dependencies in the relational model, similar to the tests used by constraint-based methods for learning relational models, such as RPC (Maier et al., 2010) and RCD (Maier et al., 2013).\nFinally, step (6) tests for conditional independence for all such 〈X,Y,Z〉 d -separation queries using linear regression (because the models were parameterized linearly) for each of the 100 data instantiations. Specifically, we tested the t-statistic for the coefficient of avg(X) in the equation Y = β0 + β1 · avg(X) + ∑ Zi∈Z βi · avg(Zi). For each query, we recorded two measurements:\n• The average strength of effect, measured as squared partial correlation—the proportion of remaining variance of Y explained by X after conditioning on Z\n11. Depending on the properties of the schema and model, it may not always be feasible to identify 100 true or false d-separation statements.\n1 entity 2 entities 3 entities 4 entities\nD-SEPARATED QUERIES\n0 0. 00 5\n0. 05\n0. 5\nA vg\n. s tre\nng th\no f e\nffe ct\n1 entity 2 entities 3 entities 4 entities\nD-CONNECTED QUERIES\n0 0. 00 5\n0. 05\n0. 5\nA vg\n. s tre\nng th\no f e\nffe ct\n1 entity 2 entities 3 entities 4 entities\nD-SEPARATED QUERIES\n0 0. 01\n0. 05\n0. 5\n1\nP ro\npo rti\non s\nig ni\nfic an\nt\n1 entity 2 entities 3 entities 4 entities\nD-CONNECTED QUERIES\n0 0. 01\n0. 05\n0. 5\n1\nP ro\npo rti\non s\nig ni\nfic an\nt\nFigure 7.4: The average strength of effect of each query (measured as squared partial correlation) on actual data. (Left) Evaluating queries that the model claims to be d - separated or conditionally independent produces low average effect sizes. (Right) Queries that the model claims are d -connected or dependent produce high average effect sizes.\n• The proportion of trials for which each query was deemed significant at α = 0.01 adjusted using Bonferroni correction with the number of queries per trial\nFigure 7.3 shows the distribution of the proportion of significant trials for both true (left) and false queries (right) for varying numbers of entities. Figure 7.4 shows the corresponding average strength of effects for true (left) and false (right) queries. The graph uses a standard box-and-whisker plot with values greater or less than 1.5 times the inner quartile range—the difference between the upper and lower quartiles—marked as outliers.\nIn the vast majority of cases, relational d -separation is consistent with tests on actual data (i.e., most d -separated queries have low effect sizes and are rarely deemed significant, whereas most d -connected queries have high effect sizes and are mostly deemed significant). For approximately 23,000 true queries, 14.9% are significant in more than one trial, but most are insubstantive, with only 2.2% having an average effect size greater than 0.01. There are three potential reasons why a d -separation in theory may appear to be d -connected in practice: (1) Type I error; (2) high power given a large sample size; or (3) bias. We have discovered that a small number of cases exhibit an interaction between aggregation and relational structure (i.e., degree or the cardinality of relational variable instances). This interaction violates the identically distributed assumption of data instances, which produces a biased estimate of effect size for simple linear regression. Linear regression does not account for these interaction effects, suggesting the need for more accurate statistical tests of conditional independence for relational data."
    }, {
      "heading" : "8. Model Assumptions and Related Work",
      "text" : "The class of relational models considered in Section 4, while strictly more expressive than Bayesian networks, has limitations in its current formalization. In this section, we highlight these assumptions and discuss how related and future work could address them.\nSelf-relationships: Self-relationships are relationship classes that involve the same entity class more than once. Relational schemas, as defined in Definition 4.1, can express these types of relationships. Only the definition of relational paths—which govern the space of variables and dependencies—requires unique entity class names within [E,R,E] triples (see condition (2) of Definition 4.3). However, a common procedure in entity-relationship modeling is to map entity names to unique role indicators within the context of a self-relationship, such as manager/subordinate, friend1/friend2, or citing-paper/cited-paper (Ramakrishnan and Gehrke, 2002). This approach does not duplicate entity instances in the skeleton or ground graph; it only modifies their reference names within the relational path, requiring extended semantics for terminal sets. Incorporating self-relationships is a straightforward extension, but for simplicity, we omit this additional layer of complexity.\nRelational autocorrelation: In contrast to self-relationships, relational autocorrelation is a statistical dependency among the values of the same attribute class frequently found in relational data sets (Jensen and Neville, 2002). Various models and learning algorithms have been developed to capture these types of dependencies, such as RDNs (Neville and Jensen, 2007), PBNs with an extended normal form (Schulte et al., 2012), and PRMs with dependencies that follow guaranteed acyclic relationships (Getoor et al., 2007). Our formalism, and equivalently PRMs (without guaranteed acyclic relationships), can represent a class of models for apparent autocorrelation. Any relational dependency that yields a common cause for grounded variables of the same attribute class—essentially any dependency that crosses a many cardinality—produces relational autocorrelation. The only autocorrelations not accounted for involve latent causes or those produced by temporal processes (e.g., feedback).\nContext-specific independence: Context-specific independence (CSI) introduces independence of some variable and its parents, depending on the values of other variables. This can be achieved within the specification of conditional probability distributions as if-then-else statements of logical conditions, such as in DAPER models (Heckerman et al., 2007) or RPMs (Russell and Norvig, 2010), encoded as regularities in conditional probability tables (Boutilier et al., 1996), or with the recent graphical convention of gates (Minka and Winn, 2009). However, this introduces a notion of independence that cannot be inferred from model structure via traditional d -separation. In fact, Boutilier et al. (1996) define an analogous approach based on d -separation of a manipulated Bayesian network through deletion of vacuous dependencies given some context. Winn (2012) extends the rules of d -separation to reason over the additional paths and their collective state introduced by gates. An alternative and more general approach to encoding CSIs is to develop an ontology for which (in)dependencies hold depending on the type of entity or relationship. PRMs with class hierarchies allow a hierarchy of entity types where the dependency structure can vary depending on the type (Getoor et al., 2000). Rules of inheritance derived from object-oriented programming are used to define a coherent joint probability distribution. This aligns with our formalism, as relational schemas can be viewed as an ontology defined at a particular level. However, the semantics of d -separation under inheritance has not been developed and is a profitable direction of future research.\nCauses of entity and relationship existence: Without a generative model of relational skeletons, the relational models are not truly generative as the skeleton must be generated prior to the attributes. However, the same issue occurs for Bayesian networks: Relational\nskeletons consist of disconnected entity instances, but the model does not specify how many instances to create. There are relational models that attempt to learn and represent models with unknown numbers of entity instances, such as Blog (Milch et al., 2005), or uncertain relationship instances, such as PRMs with existence uncertainty (Getoor et al., 2002). However, reasoning about the connection between conditional independence and existence is an open problem. For relationship existence, selection bias (conditioning) occurs when testing marginal dependence between variables across a particular relationship (Maier et al., 2010). For entity existence, some researchers argue that existence cannot be represented as a variable or predicate (Poole, 2007), while others represent them as predicates (Laskey, 2008). Therefore, we currently choose simple processes for generating skeletons, allowing us to focus on relational models of attributes and leaving structural causes and effects as future work.\nCausal sufficiency : The relational models we consider assume that all common causes of observed variables are also observed and included in the model—an assumption commonly referred to as causal sufficiency. Many researchers have developed methods for learning and inference by explicitly modeling unobserved variables—typically termed latent variable models (Bishop, 1999)—or inferring the presence of latent entity classes—for example, latent group models (Neville and Jensen, 2005). However, only ancestral graphs and acyclic directed mixed graphs (ADMGs) do so in order to preserve an underlying conditional independence structure (Richardson and Spirtes, 2002; Richardson, 2009). These models are paired with the theory of m-separation, which is a generalization of d -separation for Bayesian networks. The generalization of ancestral graphs or ADMGs to relational models requires extensive theoretical exploration; therefore, we leave this as an important direction for future work. Given that a primary motivation for d -separation is to support constraint-based causal discovery, any relational extension to algorithms that learn causal models without assuming causal sufficiency, such as FCI (Spirtes et al., 1995; Zhang, 2008), its variants (Claassen and Heskes, 2011; Colombo et al., 2012), and BCCD (Claassen and Heskes, 2012), would require such an extension to m-separation.\nTemporal and cyclic models: Currently, the relational model is assumed to be acyclic (with respect to the class dependency graph), and consequently, atemporal. Model-level cycles typically result from temporal processes for which grounding across time would yield an acyclic ground graph, such as in dynamic Bayesian networks (Dean and Kanazawa, 1989; Murphy, 2002). However, cycles can also be due to temporal processes where the interaction occurs at a faster rate than measurement. As a result, there has been considerable attention devoted to models that explicitly encode cyclic dependencies, such as the work by Spirtes (1995), Pearl and Dechter (1996), Richardson (1996), Dash (2005), Schmidt and Murphy (2009), and Hyttinen et al. (2012). Our formalism currently prohibits any relational dependency that has a common attribute class for the cause and effect, regardless of the relational path constraint. Relaxing this assumption would require either explicitly modeling temporal dynamics or enabling feedback loops. We reserve temporal dynamics and feedback as another important avenue for future research.\nDespite these assumptions, our current work extends the notion of d -separation to a much more expressive class of models than Bayesian networks. This work is a first step toward deriving conditional independencies from expressive classes of models. Incorporating existence, ontologies, temporal dynamics and feedback, and latent variables into our model\nis important future work, especially in the context of representing and learning causal models of realistic domains."
    }, {
      "heading" : "9. Discussion",
      "text" : "In this paper, we extend the theory of d -separation to graphical models of relational data. We present the abstract ground graph, a new representation that is sound and complete in its abstraction of dependencies across all possible ground graphs of a given relational model. We formally define relational d -separation and offer a sound, complete, and computationally efficient approach to deriving conditional independence facts from relational models by exploiting their abstract ground graphs. We also show that relational d -separation is equivalent to the Markov condition for relational models. We provide an empirical analysis of relational d -separation on synthetic data, demonstrating a close correspondence between the theory and statistical results in practice. Finally, we evaluate how frequently the additional complexity of abstract ground graphs proves necessary for accurately deriving conditional independence facts.\nThe results of this paper imply potential flaws in the design and analysis of some realworld studies. If researchers of social or economic systems choose inappropriate data and model representations, then their analyses may omit important classes of dependencies. Specifically, our theory implies that choosing a propositional representation from an inherently relational domain may lead to serious errors. An abstract ground graph from a given perspective defines the exact set of variables that must be included in any propositionalization. The absence of any relational variable (including intersection variables) may unnecessarily violate causal sufficiency, which could result in the inference of a causal dependency where conditional independence was not detected. Our work indicates that researchers should carefully consider how to represent their domains in order to accurately reason about conditional independence.\nThe abstract ground graph representation also presents an opportunity to derive new edge orientation rules for algorithms that learn the structure of relational models, such as RPC (Maier et al., 2010) and RCD (Maier et al., 2013). There are unique orientations of edges that are consistent with a given pattern of association that can only be recognized in an abstract ground graph. For example, in contrast to bivariate IID data, it is simple to establish the direction of causality for bivariate relational data. Consider the two bivariate, two-entity relational models depicted in Figure 9.1(a). The first model implies that values of X on A entities are caused by the values of Y on related B entities. The second model implies the opposite, that values of Y on B entities are caused by the values of X on related A entities. For simplicity, we show the relationship class only as a dashed line between entity classes and omit it from relational paths.\nFigure 9.1(b) illustrates a fragment of the abstract ground graph (for hop threshold h=4) that each of the two relational models implies. As expected, the directions of the edges in the two abstract ground graphs are counterposed. Both models produce observable statistical dependencies for relational variable pairs 〈[B].Y, [B,A].X〉 and 〈[B,A].X, [B,A,B].Y 〉. However, the relational variables [B].Y and [B,A,B].Y have different observable statistical dependencies: In the first model, they are marginally independent and conditionally dependent given [B,A].X, and in the second model, they are marginally dependent and\nconditionally independent given [B,A].X. As a result, we can uniquely determine the direction of causality of the single dependence by exploiting relational structure. (There is symmetric reasoning for relational variables from A’s perspective, and this result is also applicable to one-to-many data.)\nTo illustrate this fact more concretely, consider the small relational skeleton shown in Figure 9.1(c) and the ground graphs applied to this skeleton in Figure 9.1(d). In the first ground graph, we have y1 ⊥⊥ y2 and y1 ⊥⊥/ y2 |x1, but in the second ground graph, we have y1 ⊥⊥/ y2 and y1 ⊥⊥ y2 |x1. These opposing conditional independence relations uniquely determine the correct causal model. In prior work, we formalized this idea as a new rule, called relational bivariate orientation (RBO) (Maier et al., 2013), to orient dependencies in a constraint-based causal discovery algorithm.\nDeriving and formalizing the implications of relational d -separation is a main direction of future research. Additionally, our experiments suggest that more accurate tests of conditional independence for relational data need to be developed, specifically tests that can address the interaction between relational structure and aggregation across terminal sets of relational variables. This work has also focused solely on relational models of attributes; future work should consider models of relationship and entity existence to fully characterize generative models of relational structure. The theory could also be extended to incorporate functional or deterministic dependencies, as D-separation extends d -separation for Bayesian networks. Finally, the work on identifying causal effects in Bayesian networks could be extended to relational models. This may similarly require an extension of do-calculus to consider the space of relational interventions, which may include adding or removing entity or relationship instances, as well as fixing attribute values."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors wish to thank Cindy Loiselle for her editing expertise. The authors also thank the anonymous reviewers for their helpful comments, prompting us to create a much more readable, precise, correct, and useful paper. This effort is supported by the Intelligence Advanced Research Project Agency (IARPA) via Department of Interior National Business Center Contract number D11PC20152, Air Force Research Lab under agreement number FA8750-09-2-0187, the National Science Foundation under grant number 0964094, and Science Applications International Corporation (SAIC) and DARPA under contract number P010089628. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright notation hereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/NBC, AFRL, NSF, SAIC, DARPA, or the U.S. Government. Katerina Marazopoulou received scholarship support from the Greek State Scholarships Foundation."
    }, {
      "heading" : "Appendix A. Proofs",
      "text" : "In this appendix, we provide detailed proofs for all previous lemmas, theorems, and corollaries.\nLemma 4.1 For two relational paths of arbitrary length from Ij to Ik that differ in at least one item class, P1 = [Ij , . . . , Im, . . . , Ik] and P2 = [Ij , . . . , In, . . . , Ik] with Im 6= In, there exists a skeleton σ ∈ ΣS such that P1|ij ∩ P2|ij 6= ∅ for some ij ∈ σ(Ij).\nProof. Proof by construction. Let S be an arbitrary schema with two arbitrary relational paths P1 = [Ij , . . . , Im, . . . , Ik] and P2 = [Ij , . . . , In, . . . , Ik] where Im 6= In. We will construct a skeleton σ ∈ ΣS such that the terminal sets for item ij ∈ σ(Ij) along P1 and P2 have a non-empty intersection, that is, an item ik ∈ P1|ij ∩ P2|ij 6= ∅ (roughly depicted in Figure A.1). We use the following procedure to build σ:\n1. Simultaneously traverse P1 and P2 from Ij until the paths diverge. For each entity class E ∈ E reached, add a unique entity instance e to σ(E).\n2. Simultaneously traverse P1 and P2 backwards from Ik until the paths diverge. For each entity class E ∈ E reached, add a unique entity instance e to σ(E).\n3. For the divergent subpaths of both P1 and P2, add unique entity instances for each entity class E ∈ E .\n4. Repeat 1–3 for relationship classes. For each R ∈ R reached, add a unique relationship instance r connecting the entity instances from classes on P1 and P2, and add unique entity instances for classes E ∈ R not appearing on P1 and P2.\nThis process constructs an admissible skeleton—all instances are unique and this process assumes no cardinality constraints aside from those required by Definition 4.3. By construction, there exists an item ij ∈ σ(Ij) such that P1|ij ∩ P2|ij = {ik} 6= ∅.\nLemma 5.1 Let Porig = [I1, . . . , Ij ] and Pext = [Ij , . . . , Ik] be two relational paths with P = extend(Porig , Pext). Then, ∀P ∈ P there exists a relational skeleton σ ∈ ΣS such that ∃i1 ∈ σ(I1) such that ∃ik ∈ P |i1 and ∃ij ∈ Porig |i1 such that ik ∈ Pext |ij .\nProof. Let P ∈ P be an arbitrary valid relational path, where P = P 1,no−c+1orig + P c+1,ne ext for pivot c. There are two subcases: (a) c = 1 and P = [I1, . . . , Ij , . . . , Ik]. This subcase holds generally for any skeleton. Proof by contradiction. Let σ be an arbitrary skeleton, choose i1 ∈ σ(I1) arbitrarily, and choose ik ∈ P |i1 arbitrarily. Assume for contradiction that there is no ij in the terminal set Porig |i1 such that ik would be in the terminal set Pext |ij , that is, ∀ij ∈ Porig |i1 ik /∈ Pext |ij . Since P = [I1, . . . , Ij , . . . , Ik], we know that ik is reached by traversing σ from i1 via some ij to ik. But the traversal from i1 to ij implies that ij ∈ [I1, . . . , Ij ]|i1 = Porig |i1 , and the traversal from ij to ik implies that ik ∈ [Ij , . . . , Ik]|ij = Pext |ij . Therefore, there must exist an ij ∈ Porig |i1 such that ik ∈ Pext |ij .\n(b) c > 1 and P = [I1, . . . , Im, . . . , Ik]. Proof by construction. We build a relational skeleton σ following the same procedure as outlined in the proof of Lemma 4.1. Add instances to σ for every item class that appears on Porig and Pext . Since P = [I1, . . . , Im, . . . , Ik], we know that ik is reached by traversing σ from i1 via some im to ik. By case (a), ∃im ∈ [I1, . . . , Im]|i1 such that ik ∈ [Im, . . . , Ik]|im . We then must show that\nthere exists an ij ∈ [Im, . . . , Ij ]|im with im ∈ [Ij , . . . , Im]|ij . But constructing the skeleton with unique item instances for every appearance of an item class on the relational paths provides this and does not violate any cardinality constraints. If any item class appears more than once, then the bridge burning semantics are induced. However, adding an additional item instance for every reappearance of an item class enables the traversal from ij to im and vice versa. An example of this construction is displayed in Figure A.2. This is also a valid relational skeleton because Porig and Pext are valid relational paths, and by definition, the cardinality constraints of the schema permit multiple instances in the skeleton of any repeated item class. By this procedure, we show that there exists a skeleton σ such that there exists an ij ∈ Porig|i1 such that ik ∈ Pext|ij .\nLemma 5.2 Let σ ∈ ΣS be a relational skeleton, and let Porig = [I1, . . . , Ij ] and Pext = [Ij , . . . , Ik] be two relational paths with P = extend(Porig , Pext). Then, ∀i1 ∈ σ(I1) ∀ij ∈ Porig |i1 ∀ik ∈Pext |ij if ∀P ∈ P ik /∈ P |i1, then ∃P ′orig such that Porig |i1 ∩ P ′orig |i1 6= ∅ and ik ∈ P ′|i1 for some P ′ ∈ extend(P ′orig , Pext).\nProof. Proof by construction. Let i1 ∈ σ(I1), ij ∈ Porig |i1 , and ik ∈ Pext |ij be arbitrary instances such that ik /∈ P |i1 for all P ∈ P.\nSince ij ∈ Porig |i1 and ik ∈ Pext |ij , but ik /∈ P |i1 , there exists no pivot that yields a common subsequence in Porig and Pext that produces a path in extend that can reach ik. Let the first divergent item class along the reverse of Porig be Il and along Pext be In. The two paths must not only diverge, but they also necessarily reconverge at least once. If Porig and Pext do not reconverge, then there are no reoccurrences of an item class along any P ∈ P that would restrict the inclusion of ik in some terminal set P |i1 . The sole reason that ik /∈ P |i1 for all P ∈ P is due to the bridge burning semantics specified in Definition 4.4.\nWithout loss of generality, assume Porig and Pext reconverge once, at item class Im. So, Porig = [I1, . . . , Im, . . . , Il, . . . , Ij ] and Pext = [Ij , . . . , In, . . . , Im, . . . , Ik] with Il 6= In, as depicted in Figure A.3. Let P ′orig = [I1, . . . , Im, . . . , In, . . . , Ij ], which is a valid relational path because [I1, . . . , Im] is a subpath of Porig and [Im, . . . , In, . . . , Ij ] is a subpath of Pext .\nBy construction, ij ∈ Porig |i1 ∩ P ′orig |i1 . If P ′ = [I1, . . . , Im, . . . , Ik] ∈ extend(P ′orig , Pext) with pivot at Im, then ik ∈ P ′|i1 .\nTheorem 5.2 For every acyclic relational model structure M and perspective B ∈ E ∪ R, the abstract ground graph AGGMB is sound and complete for all ground graphs GGMσ with skeleton σ ∈ ΣS .\nProof. LetM = (S,D) be an arbitrary acyclic relational model structure and let B ∈ E∪R be an arbitrary perspective.\nSoundness: To prove that AGGMB is sound, we must show that for every edge Pk.X → Pj .Y in AGGMB, there exists a corresponding edge ik.X → ij .Y in the ground graph GGMσ for some skeleton σ ∈ ΣS , where ik ∈ Pk|b and ij ∈ Pj |b for some b ∈ σ(B). There are three subcases, one for each type of edge in an abstract ground graph:\n(a) Let [B, . . . , Ik].X → [B, . . . , Ij ].Y ∈ RVE be an arbitrary edge in AGGMB between a pair of relational variables. Assume for contradiction that there exists no edge ik.X → ij .Y in any ground graph:\n∀σ∈ΣS ∀b∈σ(B) ∀ik∈ [B, . . . , Ik]|b ∀ij∈ [B, . . . , Ij ]|b ( ik.X → ij .Y /∈GGMσ )\nBy Definition 5.2 for abstract ground graphs, if [B, . . . , Ik].X → [B, . . . , Ij ].Y ∈ RVE , then the model must have dependency [Ij , . . . , Ik].X → [Ij ].Y ∈ D such that [B, . . . , Ik] ∈ extend([B, . . . , Ij ], [Ij , . . . , Ik]). So, by Definition 4.9 for ground graphs, there is an edge from every ik.X to every ij .Y , where ik is in the terminal set for ij along [Ij , . . . , Ik]:\n∀σ ∈ ΣS ∀ij ∈ σ(Ij) ∀ik ∈ [Ij , . . . , Ik]|ij ( ik.X → ij .Y ∈ GGMσ )\nSince [B, . . . , Ik] ∈ extend([B, . . . , Ij ], [Ij , . . . , Ik]), by Lemma 5.1 we know that ∃σ ∈ ΣS ∃b ∈ σ(B) ∃ik ∈ [B, . . . , Ik]|b ∃ij ∈ [B, . . . , Ij ]|b ( ik ∈ [Ij , . . . , Ik]|ij ) Therefore, there exists a ground graph GGMσ such that ik.X → ij .Y ∈ GGMσ, which contradicts the assumption.\n(b) Let P1.X ∩ P2.X → [B, . . . , Ij ].Y ∈ IVE be an arbitrary edge in AGGMB between an intersection variable and a relational variable, where P1= [B, . . . , Im, . . . , Ik] and P2 = [B, . . . , In, . . . , Ik] with Im 6= In. By Lemma 4.1, there exists a skeleton σ ∈ ΣS and b ∈ σ(B) such that P1|b ∩ P2|b 6= ∅. Let ik ∈ P1|b ∩ P2|b and assume for contradiction that for all ij ∈ [B, . . . , Ij ]|b there is no edge ik.X → ij .Y in the ground graph GGMσ. By Definition 5.2, if the abstract ground graph has edge P1.X ∩ P2.X → [B, . . . , Ij ].Y ∈ IVE , then either P1.X → [B, . . . , Ij ].Y ∈ RVE or P2.X → [B, . . . , Ij ].Y ∈ RVE . Then, as shown in case (a), there exists an ij ∈ [B, . . . , Ij ]|b such that ik.X → ij .Y ∈ GGMσ, which contradicts the assumption.\n(c) Let [B, . . . , Ik].X → P1.Y ∩ P2.Y ∈ IVE be an arbitrary edge in AGGMB between a relational variable and an intersection variable, where P1 = [B, . . . , Im, . . . , Ij ] and P2 = [B, . . . , In, . . . , Ij ] with Im 6= In. The proof follows case (b) to show that there exists a skeleton σ ∈ ΣS and b ∈ σ(B) such that for all ik ∈ [B, . . . , Ik]|b there exists an ij ∈ P1∩P2|b such that ik.X → ij .Y ∈ GGMσ.\nCompleteness: To prove that the abstract ground graph AGGMB is complete, we show that for every edge ik.X → ij .Y in every ground graph GGMσ where σ ∈ ΣS , there is a set of corresponding edges in AGGMB. Specifically, the edge ik.X → ij .Y yields two sets of relational variables for some b ∈ σ(B), namely Pk.X = {Pk.X | ik ∈ Pk|b} and\nPj.Y = {Pj .Y | ij ∈ Pj |b}. Note that all relational variables in both Pk.X and Pj.Y are nodes in AGGMB, as are all pairwise intersection variables: ∀Pk.X, P ′k.X ∈Pk.X ( Pk.X ∩\nP ′k.X ∈ AGGMB ) and ∀Pj .Y, P ′j .Y ∈ Pj.Y ( Pj .Y ∩ P ′j .Y ∈ AGGMB ) . We show that for all Pk.X ∈ Pk.X and for all Pj .Y ∈ Pj.Y either (a) Pk.X → Pj .Y ∈ AGGMB, (b) Pk.X ∩ P ′k.X → Pj .Y ∈ AGGMB, where P ′k.X ∈ Pk.X, or (c) Pk.X → Pj .Y ∩ P ′j .Y ∈ AGGMB, where P ′ j .Y ∈ Pj.Y.\nLet σ ∈ ΣS be an arbitrary skeleton, let ik.X → ij .Y ∈ GGMσ be an arbitrary edge drawn from [Ij , . . . , Ik].X → [Ij ].Y ∈ D, and let Pk.X ∈ Pk.X, Pj .Y ∈ Pj.Y be an arbitrary pair of relational variables.\n(a) If Pk ∈ extend(Pj , [Ij , . . . , Ik]), then Pk.X → Pj .Y ∈ AGGMB by Definition 5.2. (b) If Pk /∈ extend(Pj , [Ij , . . . , Ik]), but ∃P ′k ∈ extend(Pj , [Ij , . . . , Ik]) such that P ′k.X ∈ Pk.X, then P ′ k.X → Pj .Y ∈ AGGMB, and Pk.X ∩ P ′k.X → Pj .Y ∈ AGGMB by Definition 5.2. (c) If ∀P ∈ extend(Pj , [Ij , . . . , Ik]) ( P.X /∈ Pk.X ) , then by Lemma 5.2, ∃P ′j such that ij ∈ P ′j |b and Pk ∈ extend(P ′j , [Ij , . . . , Ik]). Therefore, P ′j .Y ∈ Pj.Y, Pk.X → P ′j .Y ∈ AGGMB, and Pk.X → P ′j .Y ∩ Pj .Y ∈ AGGMB by Definition 5.2.\nTheorem 5.3 For every acyclic relational model structure M and perspective B ∈ E ∪ R, the abstract ground graph AGGMB is directed and acyclic.\nProof. LetM be an arbitrary acyclic relational model structure, and let B ∈ E ∪R be an arbitrary perspective. It is clear by Definition 5.2 that every edge in the abstract ground graph AGGMB is directed by construction. All edges inserted in any abstract ground graph are drawn from the directed dependencies in a relational model. Since M is acyclic, the class dependency graph GM is also acyclic by Definition 4.10. Assume for contradiction that there exists a cycle of length n in AGGMB that contains both relational variables and intersection variables. By Definition 5.2, all edges inserted in AGGMB are drawn from some dependency in M, even for nodes corresponding to intersection variables. Retaining only the final item class in each relational path for every node in the cycle will yield a cycle in GM by Definition 4.10. Therefore,M could not have been acyclic, which contradicts the assumption."
    }, {
      "heading" : "Appendix B. The Semantics of Bridge Burning",
      "text" : "In this appendix, we provide an example to show that the bridge burning semantics for terminal sets of relational paths yields a strictly more expressive class of relational models than semantics without bridge burning. The bridge burning semantics produces terminal sets that are necessarily subsets of terminal sets which would otherwise be produced without bridge burning. Paradoxically, this enables a superset of relational models.\nRecall the definition of a terminal set for a relational path:\nDefinition 4.4 (Terminal set) For skeleton σ ∈ ΣS and ij ∈ σ(Ij), the terminal set P |ij for relational path P = [Ij , . . . , Ik] of length n is defined inductively as\nP 1|ij = [Ij ]|ij = {ij} ...\nPn|ij = [Ij , . . . , Ik]|ij = ⋃\nim∈Pn−1|ij\n{ ik | ( (im ∈ ik if Ik ∈ R) ∨ (ik ∈ im if Ik ∈ E) )\n∧ ik /∈ n−1⋃\nl=1\nP l|ij }\nThe final condition in the inductive definition (ik /∈ [I1, . . . , Ij ]|i1 for j = 1 to k − 1) encodes bridge burning. The item ik is only added to the terminal set if it is not a member of the terminal set of any previous subpath. For example, let P be the relational path [Employee, Develops, Product, Develops, Employee]. This relational path produces terminal sets that include the employees that work on the same products (that is, coworkers). Instantiating this path with the employee Quinn, P |Quinn, produces the terminal\nset {Paul, Roger, Sally}. Since Quinn ∈ [Employee]|Quinn, the bridge burning semantics excludes Quinn from this set. This makes intuitive sense as well—Quinn should not be considered her own colleague.\nA relational model is simply a collection of relational dependencies. Each relational dependency is primarily described by the relational path of the parent relational variable (because, for canonically specified dependencies, the relational path of the child consists of a single item class). The relational path specification is used in the construction of ground graphs, connecting variable instances that appear in the terminal sets of the parent and child relational variables.\nTo characterize the expressiveness of relational models, we can inspect the space of representable ground graphs by choosing an arbitrary relational skeleton and a small set of relational dependencies. We show with a simple example that the bridge burning semantics for a model over a two-entity, bivariate schema yields more possible ground graphs than without bridge burning. (We omit the relationship class for simplicity.) In Figure B.1(a), we present such a model with two possible relational dependencies labeled (1) and (2). Figure B.1(b) provides a simple relational skeleton involving three A and three B instances (relationship instances are represented as dashed lines for simplicity). As shown in Figure B.1(c), the bridge burning semantics leads to three possible ground graphs, one for each combination of the dependencies (1), (2), and both (1) and (2) together. Without bridge burning, only two ground graphs are possible because dependency (2) completely subsumes dependency (1) with those semantics.\nThis example generalizes to arbitrary dependencies. The terminal sets of relational paths that repeat item classes subsume subpaths under the semantics without bridge burning. This leads to fewer possible relational models, which justifies our choice of semantics for terminal sets of relational paths."
    }, {
      "heading" : "Appendix C. Soundness and Completeness of Relational Paths",
      "text" : "In this appendix, we prove that the definition of relational paths (repeated below) is sound and complete with respect to producing non-empty terminal sets for at least one relational skeleton.\nDefinition 4.3 (Relational path) A relational path [Ij , . . . , Ik] for relational schema S is an alternating sequence of entity and relationship classes Ij , . . . , Ik ∈ E ∪ R such that:\n(1) For every pair of consecutive item classes [E,R] or [R,E] in the path, E ∈ R. (2) For every triple of consecutive item classes [E,R,E′], E 6= E′. (3) For every triple of consecutive item classes [R,E,R′], if R = R′, then card(R,E) =\nmany.\nLemma C.1 Let S be a relational schema and [Ij , . . . , Ik] be a sequence of alternating entity and relationship classes of S that satisfy participation constraints (condition (1) of Definition 4.3). The relational path [Ij , . . . , Ik] satisfies conditions (2) and (3) of Definition 4.3 if and only if there exists a relational skeleton σ ∈ ΣS and an item instance ij ∈ σ(Ij) such that [Ij , . . . , Ik]|ij 6= ∅. More formally,\n∃σ ∈ ΣS ∃ij∈σ(Ij) ( [Ij , . . . , Ik]|ij 6= ∅ ) ⇔ ( [ERE] 6∈ [Ij , . . . , Ik] )\n∧( [RER] ∈ [Ij , . . . , Ik]→ card(R,E) = many )\nProof. Left-to-right ⇒: Assume that there exists a skeleton σ ∈ ΣS and item instance ij ∈ σ(Ij) such that [Ij , . . . , Ik]|ij 6= ∅. We must show that [Ij , . . . , Ik] obeys conditions (2) and (3), i.e., [Ij , . . . , Ik] does not contain any [ERE] patterns, and if it contains an [RER] pattern, then card(R,E) = many.\n• Assume for contradiction that [Ij , . . . , Ik] contains a pattern of the form [ERE]. From Definition 4.4 for terminal sets, it follows that if the terminal set of a path is not empty, then the terminal set of every prefix of that path is not empty:\n[Ij , . . . , Ik]|ij 6= ∅ ⇒ [Ij , . . . , Im]|ij 6= ∅ for all [Ij , . . . , Im] ≤ [Ij , . . . , Ik]\nBy assumption, [Ij , . . . , Ik]|ij 6= ∅; therefore, the prefix [Ij , . . . , Im] that ends in the ERE pattern also has a non-empty terminal set:\n[Ij , . . . , Ik]|ij 6= ∅ ⇒ [Ij , . . . , E,R,E]|ij 6= ∅ [Ij , . . . , Ik]|ij 6= ∅ ⇒ [Ij , . . . , E,R]|ij 6= ∅ [Ij , . . . , Ik]|ij 6= ∅ ⇒ [Ij , . . . , E]|ij 6= ∅\nLet e ∈ σ(E) be an entity instance in the terminal set [Ij , . . . , E]|ij . Since the terminal set [Ij , . . . , E,R]|ij is not empty, it follows that there exists a relationship instance r = 〈. . . , e, . . .〉 such that r ∈ [Ij , . . . , E,R]|ij . However, [Ij , . . . , E,R,E]|ij is also not empty; thus, there exists some e′ ∈ σ(E) such that e′ ∈ [Ij , . . . , E,R,E]|ij , where e′ 6= e, and e′ ∈ r. It follows that both e and e′ participate in the relationship instance r, which is a contradiction.\n• Assume for contradiction that [Ij , . . . , Ik] contains a pattern of the form [R,E,R] and card(R,E) = one.\n[Ij , . . . , R]|ij 6= ∅ ⇒ ∃r = 〈e, . . .〉 ∈ [Ij , . . . , R]|ij (1) [Ij , . . . , R,E]|ij 6= ∅ ⇒ ∃e ∈ [Ij , . . . , R,E]|ij and e ∈ r [Ij , . . . , R,E,R]|ij 6= ∅ ⇒ ∃r′ = 〈e, . . .〉 such that r′ ∈ [Ij , . . . , R,E,R]|ij (2) and r′ 6= r (bridge burning semantics)\nFrom (1) and (2) it follows that e participates in two instances of R; therefore, card(R,E) must be many, which is a contradiction.\nRight-to-left⇐: Assume that [Ij , . . . , Ik] adheres to Definition 4.3 for relational paths. We must show that ∃σ ∈ ΣS ∃ij ∈ σ(Ij) ( [Ij , . . . , Ik]|ij 6= ∅ ) . We can construct such a skeleton σ according to the following procedure: For each entity class E on the path, add a unique entity instance e to σ(E). Then, for each relationship class R on the path, add a unique relationship instance r connecting the previously created unique entity instances that participate in R, and add unique entity instances for classes E ∈ R not appearing on the path. This process constructs an admissible skeleton—all instances are unique and this process assumes no cardinality constraints aside from those required by Definition 4.3. By construction, there exists an item instance ij ∈ σ(Ij) such that [Ij , . . . , Ik]|ij 6= ∅."
    }, {
      "heading" : "Appendix D. Background on Propositional Data and Models",
      "text" : "In this appendix, we provide a brief review of Bayesian networks, traditional d -separation, and their connection to causality. We also describe why the class of Bayesian networks is a special case of relational models. Finally, we give an example of how to propositionalize a data set drawn from a relational domain.\nA common assumption in classical statistics, machine learning, and causal discovery is that data instances are independent and identically distributed (IID). The first condition assumes that the variables on any given data instance are marginally independent of the variables of any other data instance. The second condition assumes that every data instance is drawn from the same underlying joint probability distribution. IID data (also referred to as propositional data12) are effectively represented as a single table, where rows correspond to the independent instances and columns are attributes of those instances.\nA Bayesian network is a widely used probabilistic graphical model of propositional data (Pearl, 1988). A Bayesian network is represented as a directed acyclic graph G = (V,E), where V is a set of vertices corresponding to random variables in the data and E ⊂ V×V is a set of edges encoding the probabilistic dependencies among the variables. Each random variable V ∈ V is associated with a conditional probability distribution P ( V | parents(V ) ) , where parents(V ) ⊆ V \\ {V } is the set of parent variables for V. If the joint probability distribution P (V) satisfies the Markov condition for G, then P (V) can be factored as ∏ V ∈V P ( V | parents(V ) ) using the conditional distributions. The Markov condition states that every variable V ∈ V is conditionally independent of its non-descendants given its parents, where the descendants of V are all variables reachable by a directed path from V. Deriving the set of conditional independencies from G based on the Markov condition is cumbersome, requiring complex combinations of probability axioms. Fortunately, d -separation, a set of graphical criteria, provides the foundation for algorithmic derivation of all conditional independencies in G and entails the exact same set of conditional independencies as the Markov condition (Verma and Pearl, 1988; Geiger and Pearl, 1988; Neapolitan, 2004).\nIn the following definition, a path is a sequence of vertices following edges in either direction. We say that a variable V is a collider on a path p if the two arrowheads point at each other (collide) at V; otherwise, V is a non-collider on p.\nDefinition D.1 (d-separation) Let X, Y, and Z be disjoint sets of variables in directed acyclic graph G. A path from some X ∈ X to some Y ∈ Y is d-connected given Z if and only if every collider W on the path, or a descendant of W, is a member of Z, and there are no non-colliders in Z. Then, say that X and Y are d-separated by Z if and only if there are no d -connecting paths between X and Y given Z.\nFigure D.1(a) depicts the graphical patterns found along paths that lead to d -separation or d -connection based on Definition D.1, and Figure D.1(b) provides example d -separated and d -connected paths. At first glance, identifying conditional independence facts using the rules of d -separation appears computationally intensive, testing a potentially exponential\n12. IID data are typically referred to as propositional because the data can be equivalently expressed under propositional logic.\nd-separating path elements (exists one on path)\nd-connecting path elements (exists all on path)\nnumber of paths. However, Geiger et al. (1990) provide a linear-time algorithm based on breadth-first search and reachability on G.\nUnder a few assumptions, Bayesian networks can be interpreted causally, with edges corresponding to direct causal dependencies. If X → Y is an edge in the causal model G, then manipulating or changing the value of X will alter the conditional distribution of Y— denoted as P ( Y | do(X) ) using Pearl’s do-calculus notation for interventions (Pearl, 2000). The causal interpretation of G assumes the causal Markov condition, which is identical to the Markov condition, replacing parents with direct causes and non-descendants with non-effects. In order for the causal Markov condition to hold, the variables V must also be causally sufficient : There are no latent common causes for any pair of variables in V. The causal Markov condition is also equivalent to d -separation; therefore, both provide the connection between causal structures and probability distributions.\nThe conditional independencies entailed by both the causal Markov condition and d - separation hold in all distributions that G represents. A distribution P is faithful to G if all conditional independencies in P are entailed by the causal Markov condition on G. If P is assumed to be faithful to G, then there are algorithms that can learn the Markov,\nor likelihood, equivalent set of causal models. These algorithms assume causal sufficiency, faithfulness, and model acyclicity to identify the edges inG that are consistent with observed conditional independencies and to determine the direction of causality (Spirtes et al., 2000).\nThe relational representation presented in Section 4 is strictly more expressive than the propositional representation used in Bayesian network modeling. Propositional representations describe domains with a single entity class; thus, they produce schemas with |E| = 1 (one entity class) and |R| = 0 (no relationship classes). For the organization domain example, consider data about only employees (E = {Employee}). Variables would include intrinsic attributes, such as salary, but could also include variables describing other related entities, all from the employee perspective. This technique of translating a relational database down to a single, propositional representation is often referred to as propositionalization (Kramer et al., 2001). That is, we could construct a single table for employees that includes columns for the success of developed products, the revenue of all business units they work under, etc. In Figure D.2, we show an example SQL-like query that would produce such data, and the resulting data set applied to the example in Figure 2.1(b) is shown in Table D.1.13\nThe relational skeleton of a Bayesian network consists of a set of disconnected entity instances, all drawn from the same entity class. Consequently, the skeleton has a simple one-to-one mapping with the representation as a table: Each entity instance corresponds\n13. Note that modeling propositionalized data with Bayesian networks still requires the IID assumption, which is often violated since variables of one instance can influence variables of another. For example, according to the model in Figure 2.2(a) the competence of collaborating employees influences the success of products, which affects the revenue of business units, which affects its budget, thereby influencing an employee’s salary. As a result, modeling relational data with a propositional representation may unnecessarily lose valuable information, especially in the context of causal reasoning and accurate estimation of causal effects.\nto a single row, and each variable is a column. In this example, each employee would be an entity instance, and no instances of other entity types or relationships would appear in the skeleton. Because all variables in a Bayesian network are defined for a single entity class and no relationships, the relational path specification becomes trivial and, hence, implicit. All relational paths, relational variables, and relational dependencies are defined from a single perspective with singleton paths (e.g., [Employee]). The ground graph of a Bayesian network, similar to the skeleton, has a very regular structure. The ground graph consists of a set of identical copies of the model structure, one for each instance in the skeleton. For a Bayesian network, d -separation can be applied directly to the model structure because there is no variability in its ground graphs."
    }, {
      "heading" : "Appendix E. Hop Thresholds",
      "text" : "For practical implementations, the size of the abstract ground graphs should be limited by a domain-specific threshold. In this work, we choose to apply a singular hop threshold to the relational paths that are represented in an abstract ground graph. In this appendix, we examine the effect of choosing a particular hop threshold.\nFirst, we introduce the notion of (B, h)-reachability, which describes the conditions under which an edge in a ground graph is represented in an abstract ground graph.\nDefinition E.1 ((B, h)-reachability) Let GGMσ be the ground graph for some relational model structure M and skeleton σ ∈ ΣS . Then, ik.X → ij .Y ∈ GGMσ is (B, h)-reachable for perspective B and hop threshold h if there exist relational variables Pk.X = [B, . . . , Ik].X and Pj .Y = [B, . . . , Ij ].Y such that length(Pk) ≤ h+ 1, length(Pj) ≤ h+ 1, and there exists an instance b ∈ σ(B) with ik ∈ Pk|b and ij ∈ Pj |b.\nIn other words, the edge ik.X → ij .Y in the ground graph is (B, h)-reachable if an instance of the base item b ∈ σ(B) can reach ik and ij in at most h hops.\nSince Definition E.1 pertains to edges reachable via a particular perspective B and hop threshold h, it relates to the reachability of edges in abstract ground graphs. We denote abstract ground graphs for perspective B, limited by a hop threshold h as AGGMBh. Definition E.1 implies that (1) for every edge in ground graph GGMσ, we can derive a set of abstract ground graphs for which that edge is (B, h)-reachable, and (2) for every abstract ground graph AGGMBh, we can derive the set of (B, h)-reachable edges for a given ground graph. Given (B, h)-reachability, we can now express the soundness and completeness of abstract ground graphs.\nTheorem E.1 For every acyclic relational model structure M, perspective B ∈ E ∪R, and hop threshold ha ∈ N0, the abstract ground graph AGGMBha is sound up to hop threshold ha for all ground graphs GGMσ with skeleton σ ∈ ΣS .\nProof. Soundness means that for every edge [B, . . . , Ij ].X → [B, . . . , Ik].Y in the abstract ground graph AGGMBha , there exists a skeleton σ ∈ ΣS , a base item instance b ∈ σ(B), an instance ij ∈ [B, . . . , Ij ]|b, and an instance ik ∈ [B, . . . , Ik]|b such that ij .X → ik.Y is a (B, ha)-reachable edge in GGMσ. The proof is identical to the proof of soundness for Theorem 5.2 (see Appendix A).\nTheorem E.2 For every acyclic relational model structure M, perspective B ∈ E ∪R, and hop threshold hr ∈ N0, the abstract ground graph AGGMBha is complete up to hop threshold hr for all ground graphs GGMσ with skeleton σ ∈ ΣS , where ha = max(hr+hm, hr+2hm−2) and hm is the maximum number of hops for a dependency in M.\nProof. Let M = (S,D) be an arbitrary acyclic relational model structure, let B ∈ E ∪ R be an arbitrary perspective, and let hr ∈ N0 be an arbitrary hop threshold.\nTo prove that the abstract ground graph AGGMBha is complete up to hop threshold hr, we show that for every (B, hr)-reachable edge ik.X → ij .Y in every ground graph GGMσ with σ ∈ ΣS , there is a set of corresponding edges in AGGMBha . Specifically, the (B, hr)reachable edge ik.X → ij .Y yields two sets of relational variables for some b ∈ σ(B), namely Pk.X = {Pk.X | ik ∈ Pk|b ∧ length(Pk) ≤ hr + 1} and Pj.Y = {Pj .Y | ij ∈ Pj |b ∧ length(Pj) ≤ hr + 1} by Definition E.1. Note that all relational variables in both Pk.X and Pj.Y are nodes in AGGMBha . We show that for all Pk.X ∈Pk.X and for all Pj .Y ∈Pj.Y either (a) Pk.X → Pj .Y ∈ AGGMBha , (b) Pk.X ∩P ′k.X → Pj .Y ∈ AGGMBha or Pk.X ∩ P ′k.X → P ′j .Y ∈ AGGMBha , where ik ∈ P ′k|b and ij ∈ P ′j |b, or (c) Pk.X → Pj .Y ∩ P ′j .Y ∈ AGGMBha or P ′k.X → Pj .Y ∩ P ′j .Y ∈ AGGMBha , where ik ∈ P ′k|b and ij ∈ P ′j |b.\nLet σ ∈ ΣS be an arbitrary skeleton, let ik.X → ij .Y ∈ GGMσ be an arbitrary (B, hr)reachable edge drawn from [Ij , . . . , Ik].X → [Ij ].Y ∈ D where length([Ij , . . . , Ik]) ≤ hm + 1, and let Pk.X ∈ Pk.X, Pj .Y ∈ Pj.Y be an arbitrary pair of relational variables. There are three cases:\n(a) Pk ∈ extend(Pj , [Ij , . . . , Ik]). Then, length(Pk) ≤ (hr+1)+(hm+1)−1 = hr+hm+ 1 ≤ ha + 1. Therefore, Pk.X is a node in the abstract ground graph, and Pk.X → Pj .Y ∈ AGGMBha by Definition 5.2.\n(b) Pk /∈ extend(Pj , [Ij , . . . , Ik]), but ∃P ′k ∈ extend(Pj , [Ij , . . . , Ik]) such that ik ∈ P ′k|b. Then, length(P ′k) ≤ (hr+1)+(hm+1)−1 = hr+hm+1 ≤ ha+1. Therefore, P ′k is a node in the\nabstract ground graph, P ′k.X → Pj .Y ∈ AGGMBha , and Pk.X∩P ′k.X → Pj .Y ∈ AGGMBha by Definition 5.2.\n(c) For all Pk ∈ extend(Pj , [Ij , . . . , Ik]), it is the case that ik /∈ Pk.X|b. Then by Lemma 5.2, there exists a P ′j such that ij ∈ P ′j |b and there exists a P ′′k ∈ extend(P ′j , [Ij , . . . , Ik]). Given the way P ′j is constructed, its length is bounded by:\nlength(P ′j) ≤ length(Pj) + length([Ij , . . . , Ik])− 3 ≤ (hr + 1) + (hm + 1)− 3 = hr + hm − 1\nP ′′k intersects with Pk since they both reach ik, and the length of P ′′ k is bounded by:\nlength(P ′′k ) ≤ length(P ′j)+length([Ij , . . . , Ik])−1 ≤ (hr+hm−1)+(hm+1)−1 = hr+2hm−1\nAlso by Lemma 5.2, we know that Pj and P ′ j intersect. Since length(P ′′ k ) ≤ hr + 2hm − 1 ≤ ha + 1, P ′′ k is a node in the abstract ground graph, P ′′ k .X → P ′j .Y ∈ AGGMBha P ′′k .X → P ′j .Y ∩ Pj .Y ∈ AGGMBha , and Pk.X ∩ P ′′k .X → P ′j .Y ∈ AGGMBha by Definition 5.2. From the above three cases, it follows that to guarantee completeness up to hr, the abstract ground graph must contain nodes up to the hop threshold ha = max(hr +hm, hr + 2hm − 2).\nTheorems E.1 and E.2 guarantee that if an abstract ground graph is constructed with a hop threshold of ha from perspective B, it captures all paths of dependence in all ground graphs, where (1) the variables along those paths are reachable in hr hops from instances of B and (2) the underlying dependencies are bounded by a threshold of hm.\nIn the following, we say that d -separation holds up to a specified hop threshold h if there are no d -connecting paths involving relational variables of length greater than h+ 1.\nTheorem E.3 Relational d-separation is sound and complete for abstract ground graphs up to a specified hop threshold. Let M be an acyclic relational model structure, and let hm be the maximum number of hops for a dependency in M. Let X, Y, and Z be three distinct sets of relational variables for perspective B ∈ E ∪R defined over relational schema S, and let hr be the maximum number of hops of relational variables in X,Y, and Z. Then, X̄ and Ȳ are d-separated by Z̄ on the abstract ground graph AGGMBha if and only if for all skeletons σ ∈ ΣS and for all b ∈ σ(B), X|b and Y|b are d-separated by Z|b up to hop threshold hr in ground graph GGMσ, where ha = max(hr + hm, hr + 2hm − 2).\nProof. We must show that d -separation on an abstract ground graph implies d -separation on all ground graphs it represents (soundness) and that d -separation facts that hold across all ground graphs are also entailed by d -separation on the abstract ground graph (completeness).\nSoundness: Assume that X̄ and Ȳ are d -separated by Z̄ on AGGMBha . Assume for contradiction that there exists a skeleton σ ∈ ΣS and an item instance b ∈ σ(B) such that X|b and Y|b are not d -separated by Z|b in the ground graph GGMσ. Then, there must exist a d -connecting path p from some x ∈ X|b to some y ∈ Y|b given all z ∈ Z|b such that every edge of p is (B, hr)-reachable. By Theorem E.2, AGGMBha is (B, hr)-reachably complete, so all (B, hr)-reachable edges in GGMσ are captured by edges in AGGMBha . Thus, path p must be represented from some node in {Nx | x ∈ Nx|b} to some node in {Ny | y ∈ Ny|b}, where Nx, Ny are nodes in AGGMBha . If p is d -connecting in GGMσ,\nPredictor Coefficient Partial Semipartial\nlog(# dependencies) × # entities 1.38 0.232 0.085 log(# dependencies) 1.14 0.135 0.044\nlog(# dependencies) × # many cardinalities -0.71 0.092 0.028 # entities × # relational variables -0.32 0.044 0.013\nTable F.1: Number of equivalent conditional independence judgments: estimated standardized coefficient, squared partial correlation coefficient, and squared semipartial correlation coefficient for each predictor.\nthen it is d -connecting in AGGMBha , which implies that X̄ and Ȳ are not d -separated by Z̄. Therefore, X|b and Y|b must be d -separated by Z|b.\nCompleteness: Assume that X|b and Y|b are d -separated by Z|b in the ground graph GGMσ for all skeletons σ ∈ ΣS and for all b ∈ σ(B). Assume for contradiction that X̄ and Ȳ are not d -separated by Z̄ on AGGMBha . Then, there must exist a d -connecting path p for some relational variable X ∈ X̄ to some Y ∈ Ȳ given all Z ∈ Z̄. By Theorem E.1, AGGMBha is (B, ha)-reachably sound, so every edge in AGGMBh must correspond to some pair of variables in some ground graph. Thus, if p is d -connecting in AGGMBha , then there must exist some skeleton σ such that p is d -connecting in GGMσ for some b ∈ σ(B), which implies that d -separation does not hold for that ground graph. Therefore, X̄ and Ȳ must be d -separated by Z̄ on AGGMBha ."
    }, {
      "heading" : "Appendix F. Experimental Details—Equivalence of a Näıve Approach",
      "text" : "In this appendix, we provide additional details for the experiment in Section 6. The main goal of this experiment is to quantify how often traditional d -separation applied directly to relational model structures produces incorrect conditional independence facts. This provides a rough measurement for the additional representational power of relational d -separation on abstract ground graphs. Here, we present an analysis of which factors influence the number of equivalent and non-equivalent conditional independence judgments between both approaches (näıvely applying traditional d -separation versus relational d -separation).\nSpecifically, we show here the results of running log-linear regression to predict the number of equivalent and non-equivalent judgments for varying schemas and models. We first applied lasso for feature selection (Tibshirani, 1996) to minimize the number of predictors while maximizing model fit. We also standardized the input variables by dividing by two standard deviations, as recommended by Gelman (2008). Since the predictor for the number of dependencies is log-transformed, the standardization for that variable occurs after taking the logarithm.\nIn predicting the (log of the) number of equivalent conditional independencies, the following variables were significantly and substantively predictive (in order of decreasing predictive power):\n• Interaction between the log of the number of dependencies and the number of entities (positive)\n• Log of the number of dependencies (positive)\nPredictor Coefficient Partial Semipartial\n# many cardinalities × # entities -2.22 0.207 0.064 log(# dependencies) × # entities 0.90 0.165 0.048\n# many cardinalities 3.24 0.128 0.036 log(# dependencies) × # many cardinalities 1.47 0.127 0.036\nTable F.2: Number of non-equivalent conditional independence judgments: estimated standardized coefficient, squared partial correlation coefficient, and squared semipartial correlation coefficient for each predictor.\n• Interaction between the log of the number of dependencies and the number of many cardinalities (negative)\n• Number of entities (negative) • Interaction between the number of entities and the number of relational variables in\nthe AGG (negative)\nThe fit for the equivalent model has an R2 = 0.721 for n = 4, 000, and Table F.1 contains the standardized coefficients as well as the squared partial and semipartial correlation coefficients for each predictor. For lasso, λ = 0.0076 offered the fewest predictors while increasing the model fit by at least 0.01.\nIn predicting the (log of the) number of non-equivalent conditional independencies, the following variables were significantly and substantively predictive (in order of decreasing predictive power):\n• Interaction between the number of many cardinalities and the number of entities (negative)\n• Interaction between the log of the number of dependencies and the number of entities (positive)\n• Number of many cardinalities (positive) • Interaction between the log of the number of dependencies and the number of many\ncardinalities (positive)\nThe fit for the non-equivalent model has an R2 = 0.755 for n = 4, 000, and Table F.2 contains the standardized coefficients and the squared partial and semipartial correlation coefficients for each predictor. For lasso, λ = 0.0155 offered the fewest predictors while increasing the model fit by at least 0.01."
    }, {
      "heading" : "Appendix G. Experimental Details—Abstract Ground Graph Size",
      "text" : "In this appendix, we provide additional details for the experiment in Section 7.1. The goal of this experiment is to determine which factors influence the size of abstract ground graphs because the computational complexity of relational d -separation depends on their size. Specifically, we show here the results of running log-linear regression to predict the size of abstract ground graphs for varying schemas and models. We first applied lasso for feature selection (Tibshirani, 1996) to minimize the number of predictors while maximizing\nPredictor Coefficient Partial Semipartial\n# relationships 3.24 0.452 0.150 # many cardinalities × isEntity=F 3.09 0.349 0.109 # entities -2.11 0.359 0.102 # many cardinalities × isEntity=T 2.51 0.216 0.053 # many cardinalities × # relationships -0.88 0.100 0.020 # attributes 0.23 0.024 0.004\nTable G.1: Number of nodes in an abstract ground graph: estimated standardized coefficient, squared partial correlation coefficient, and squared semipartial correlation coefficient for each predictor.\nPredictor Coefficient Partial Semipartial\nlog(# dependencies) 1.44 0.440 0.165 # relationships 3.86 0.395 0.138\n# many cardinalities × isEntity=F 4.27 0.356 0.123 # entities -2.78 0.353 0.115\n# many cardinalities × isEntity=T 3.52 0.231 0.067 # many cardinalities × # relationships -1.35 0.127 0.031\nTable G.2: Number of edges in an abstract ground graph: estimated standardized coefficient, squared partial correlation coefficient, and squared semipartial correlation coefficient for each predictor.\nmodel fit. We also standardized the input variables by dividing by two standard deviations, as recommended by Gelman (2008). Since the predictor for the number of dependencies is log-transformed, the standardization for that variable occurs after taking the logarithm.\nIn predicting the (log of the) number of nodes, the following variables were significantly and substantively predictive (in order of decreasing predictive power):\n• Number of relationships (positive) • Interaction between many cardinalities and an indicator variable for whether the\nabstract ground graph is from an entity or relationship perspective (positive)\n• Number of entities (negative) • Interaction between the number of many cardinalities and relationships (negative) • Total number of attributes (positive)\nThe fit for the nodes model has an R2 = 0.818 for n = 450, 000, and Table G.1 contains the standardized coefficients as well as the squared partial and semipartial correlation coefficients for each predictor. For lasso, λ = 0.0095 offered the fewest predictors while increasing the model fit by at least 0.01.\nIn predicting the (log of the) number of edges, the following variables were significantly and substantively predictive (in order of decreasing predictive power):\n• Log of the number of dependencies (positive)\n• Number of relationships (positive) • Interaction between many cardinalities and an indicator variable for whether the\nabstract ground graph is from an entity or relationship perspective (positive)\n• Number of entities (negative) • Interaction between the number of many cardinalities and relationships (negative) The fit for the edges model has an R2 = 0.789 for n = 450, 000, and Table G.2 contains the standardized coefficients and the squared partial and semipartial correlation coefficients for each predictor. For lasso, λ = 0.0164 offered the fewest predictors while increasing the model fit by at least 0.01."
    } ],
    "references" : [ {
      "title" : "CASE Method: Entity Relationship Modeling",
      "author" : [ "Richard Barker" ],
      "venue" : null,
      "citeRegEx" : "Barker.,? \\Q1990\\E",
      "shortCiteRegEx" : "Barker.",
      "year" : 1990
    }, {
      "title" : "Latent variable models",
      "author" : [ "Christopher M. Bishop" ],
      "venue" : null,
      "citeRegEx" : "Bishop.,? \\Q1999\\E",
      "shortCiteRegEx" : "Bishop.",
      "year" : 1999
    }, {
      "title" : "Context-specific independence in Bayesian networks",
      "author" : [ "Craig Boutilier", "Nir Friedman", "Moises Goldszmidt", "Daphne Koller" ],
      "venue" : "In Proceedings of the Twelfth Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Boutilier et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Boutilier et al\\.",
      "year" : 1996
    }, {
      "title" : "Operations for learning with graphical models",
      "author" : [ "Wray L. Buntine" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Buntine.,? \\Q1994\\E",
      "shortCiteRegEx" : "Buntine.",
      "year" : 1994
    }, {
      "title" : "Learning belief networks from data: An information theory based approach",
      "author" : [ "Jie Cheng", "David A. Bell", "Weiru Liu" ],
      "venue" : "In Proceedings of the Sixth International Conference on Information and Knowledge Management,",
      "citeRegEx" : "Cheng et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 1997
    }, {
      "title" : "A logical characterization of constraint-based causal discovery",
      "author" : [ "Tom Claassen", "Tom Heskes" ],
      "venue" : "In Proceedings of Twenty-Seventh Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Claassen and Heskes.,? \\Q2011\\E",
      "shortCiteRegEx" : "Claassen and Heskes.",
      "year" : 2011
    }, {
      "title" : "A Bayesian approach to constraint based causal inference",
      "author" : [ "Tom Claassen", "Tom Heskes" ],
      "venue" : "In Proceedings of Twenty-Eighth Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Claassen and Heskes.,? \\Q2012\\E",
      "shortCiteRegEx" : "Claassen and Heskes.",
      "year" : 2012
    }, {
      "title" : "Learning high-dimensional directed acyclic graphs with latent and selection variables",
      "author" : [ "Diego Colombo", "Marloes H. Maathuis", "Markus Kalisch", "Thomas S. Richardson" ],
      "venue" : "In The Annals of Statistics,",
      "citeRegEx" : "Colombo et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Colombo et al\\.",
      "year" : 2012
    }, {
      "title" : "Ecosystem analysis using probabilistic relational modeling",
      "author" : [ "Bruce D’Ambrosio", "Eric Altendorf", "Jane Jorgensen" ],
      "venue" : "In Proceedings of the International Joint Conference on Artificial Intelligence Workshop on Learning Statistical Models from Relational Data,",
      "citeRegEx" : "D.Ambrosio et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "D.Ambrosio et al\\.",
      "year" : 2003
    }, {
      "title" : "Restructuring dynamic causal systems in equilibrium",
      "author" : [ "Denver Dash" ],
      "venue" : "In Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Dash.,? \\Q2005\\E",
      "shortCiteRegEx" : "Dash.",
      "year" : 2005
    }, {
      "title" : "A model for reasoning about persistence and causation",
      "author" : [ "Thomas Dean", "Keiji Kanazawa" ],
      "venue" : "Computational Intelligence,",
      "citeRegEx" : "Dean and Kanazawa.,? \\Q1989\\E",
      "shortCiteRegEx" : "Dean and Kanazawa.",
      "year" : 1989
    }, {
      "title" : "Knowledge representation for inductive learning",
      "author" : [ "Peter A. Flach" ],
      "venue" : "In Proceedings of the Fifth European Conference on Symbolic and Quantitative Approaches to Reasoning with Uncertainty,",
      "citeRegEx" : "Flach.,? \\Q1999\\E",
      "shortCiteRegEx" : "Flach.",
      "year" : 1999
    }, {
      "title" : "Inferring cellular networks using probabilistic graphical models",
      "author" : [ "Nir Friedman" ],
      "venue" : "Science,",
      "citeRegEx" : "Friedman.,? \\Q2004\\E",
      "shortCiteRegEx" : "Friedman.",
      "year" : 2004
    }, {
      "title" : "Learning probabilistic relational models",
      "author" : [ "Nir Friedman", "Lise Getoor", "Daphne Koller", "Avi Pfeffer" ],
      "venue" : "In Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "Friedman et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Friedman et al\\.",
      "year" : 1999
    }, {
      "title" : "An experimental comparison of hybrid algorithms for Bayesian network structure learning",
      "author" : [ "Maxime Gasse", "Alex Aussem", "Haytham Elghazel" ],
      "venue" : "In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases,",
      "citeRegEx" : "Gasse et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Gasse et al\\.",
      "year" : 2012
    }, {
      "title" : "On the logic of causal models",
      "author" : [ "Dan Geiger", "Judea Pearl" ],
      "venue" : "In Proceedings of the Fourth Annual Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Geiger and Pearl.,? \\Q1988\\E",
      "shortCiteRegEx" : "Geiger and Pearl.",
      "year" : 1988
    }, {
      "title" : "Identifying independence in",
      "author" : [ "Dan Geiger", "Thomas Verma", "Judea Pearl" ],
      "venue" : "Bayesian networks. Networks,",
      "citeRegEx" : "Geiger et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Geiger et al\\.",
      "year" : 1990
    }, {
      "title" : "Scaling regression inputs by dividing by two standard deviations",
      "author" : [ "Andrew Gelman" ],
      "venue" : "Statistics in Medicine,",
      "citeRegEx" : "Gelman.,? \\Q2008\\E",
      "shortCiteRegEx" : "Gelman.",
      "year" : 2008
    }, {
      "title" : "Data Analysis Using Regression and Multilevel/Hierarchical Models",
      "author" : [ "Andrew Gelman", "Jennifer Hill" ],
      "venue" : null,
      "citeRegEx" : "Gelman and Hill.,? \\Q2007\\E",
      "shortCiteRegEx" : "Gelman and Hill.",
      "year" : 2007
    }, {
      "title" : "Learning Statistical Models from Relational Data",
      "author" : [ "Lise Getoor" ],
      "venue" : "Ph.D. thesis, Stanford University,",
      "citeRegEx" : "Getoor.,? \\Q2001\\E",
      "shortCiteRegEx" : "Getoor.",
      "year" : 2001
    }, {
      "title" : "Introduction to Statistical Relational Learning",
      "author" : [ "Lise Getoor", "Ben Taskar", "editors" ],
      "venue" : null,
      "citeRegEx" : "Getoor et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Getoor et al\\.",
      "year" : 2007
    }, {
      "title" : "From instances to classes in probabilistic relational models",
      "author" : [ "Lise Getoor", "Daphne Koller", "Nir Friedman" ],
      "venue" : "In Proceedings of the International Conference on Machine Learning Workshop on Attribute-Value and Relational Learning,",
      "citeRegEx" : "Getoor et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Getoor et al\\.",
      "year" : 2000
    }, {
      "title" : "Learning probabilistic models of link structure",
      "author" : [ "Lise Getoor", "Nir Friedman", "Daphne Koller", "Ben Taskar" ],
      "venue" : null,
      "citeRegEx" : "Getoor et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Getoor et al\\.",
      "year" : 2002
    }, {
      "title" : "Understanding tuberculosis epidemiology using structured statistical models",
      "author" : [ "Lise Getoor", "Jeanne T. Rhee", "Daphne Koller", "Peter Small" ],
      "venue" : "Artificial Intelligence in Medicine,",
      "citeRegEx" : "Getoor et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Getoor et al\\.",
      "year" : 2004
    }, {
      "title" : "Probabilistic relational models",
      "author" : [ "Lise Getoor", "Nir Friedman", "Daphne Koller", "Avi Pfeffer", "Ben Taskar" ],
      "venue" : "Introduction to Statistical Relational Learning,",
      "citeRegEx" : "Getoor et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Getoor et al\\.",
      "year" : 2007
    }, {
      "title" : "A language and program for complex Bayesian modeling",
      "author" : [ "Walter R. Gilks", "Andrew Thomas", "David J. Spiegelhalter" ],
      "venue" : "The Statistician,",
      "citeRegEx" : "Gilks et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Gilks et al\\.",
      "year" : 1994
    }, {
      "title" : "Probabilistic Models for Relational Data",
      "author" : [ "David Heckerman", "Christopher Meek", "Daphne Koller" ],
      "venue" : "Technical Report MSR-TR-2004-30, Microsoft Research,",
      "citeRegEx" : "Heckerman et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Heckerman et al\\.",
      "year" : 2004
    }, {
      "title" : "Probabilistic entity-relationship models, PRMs, and plate models",
      "author" : [ "David Heckerman", "Chris Meek", "Daphne Koller" ],
      "venue" : "Introduction to Statistical Relational Learning,",
      "citeRegEx" : "Heckerman et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Heckerman et al\\.",
      "year" : 2007
    }, {
      "title" : "Identifiability in causal Bayesian networks: A sound and complete algorithm",
      "author" : [ "Yimin Huang", "Marco Valtorta" ],
      "venue" : "In Proceedings of the Twenty-First National Conference on Artificial Intelligence,",
      "citeRegEx" : "Huang and Valtorta.,? \\Q2006\\E",
      "shortCiteRegEx" : "Huang and Valtorta.",
      "year" : 2006
    }, {
      "title" : "Toward causal inference with interference",
      "author" : [ "Michael G. Hudgens", "M. Elizabeth Halloran" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Hudgens and Halloran.,? \\Q2008\\E",
      "shortCiteRegEx" : "Hudgens and Halloran.",
      "year" : 2008
    }, {
      "title" : "Learning linear cyclic causal models with latent variables",
      "author" : [ "Antti Hyttinen", "Frederick Eberhardt", "Patrik O. Hoyer" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Hyttinen et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hyttinen et al\\.",
      "year" : 2012
    }, {
      "title" : "Linkage and autocorrelation cause feature selection bias in relational learning",
      "author" : [ "David Jensen", "Jennifer Neville" ],
      "venue" : "In Proceedings of the Nineteenth International Conference on Machine Learning,",
      "citeRegEx" : "Jensen and Neville.,? \\Q2002\\E",
      "shortCiteRegEx" : "Jensen and Neville.",
      "year" : 2002
    }, {
      "title" : "Basic Principles of Learning Bayesian Logic Programs",
      "author" : [ "Kristian Kersting", "Luc De Raedt" ],
      "venue" : "Technical Report 174, Institute for Computer Science,",
      "citeRegEx" : "Kersting and Raedt.,? \\Q2002\\E",
      "shortCiteRegEx" : "Kersting and Raedt.",
      "year" : 2002
    }, {
      "title" : "Probabilistic frame-based systems",
      "author" : [ "Daphne Koller", "Avi Pfeffer" ],
      "venue" : "In Proceedings of the Fifteenth National Conference on Artificial Intelligence,",
      "citeRegEx" : "Koller and Pfeffer.,? \\Q1998\\E",
      "shortCiteRegEx" : "Koller and Pfeffer.",
      "year" : 1998
    }, {
      "title" : "Propositionalization approaches to relational data mining",
      "author" : [ "Stefan Kramer", "Nada Lavrač", "Peter Flach" ],
      "venue" : "In Sašo Džeroski and Nada Lavrač, editors, Relational Data Mining,",
      "citeRegEx" : "Kramer et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Kramer et al\\.",
      "year" : 2001
    }, {
      "title" : "MEBN: A language for first-order Bayesian knowledge bases",
      "author" : [ "Kathryn B. Laskey" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Laskey.,? \\Q2008\\E",
      "shortCiteRegEx" : "Laskey.",
      "year" : 2008
    }, {
      "title" : "Learning causal models of relational domains",
      "author" : [ "Marc Maier", "Brian Taylor", "Hüseyin Oktay", "David Jensen" ],
      "venue" : "In Proceedings of the Twenty-Fourth National Conference on Artificial Intelligence,",
      "citeRegEx" : "Maier et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Maier et al\\.",
      "year" : 2010
    }, {
      "title" : "A sound and complete algorithm for learning causal models from relational data",
      "author" : [ "Marc Maier", "Katerina Marazopoulou", "David Arbour", "David Jensen" ],
      "venue" : "In Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Maier et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Maier et al\\.",
      "year" : 2013
    }, {
      "title" : "Bayesian network induction via local neighborhoods",
      "author" : [ "Dimitris Margaritis", "Sebastian Thrun" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Margaritis and Thrun.,? \\Q1999\\E",
      "shortCiteRegEx" : "Margaritis and Thrun.",
      "year" : 1999
    }, {
      "title" : "BLOG: Probabilistic models with unknown objects",
      "author" : [ "Brian Milch", "Bhaskara Marthi", "Stuart J. Russell", "David Sontag", "Daniel L. Ong", "Andrey Kolobov" ],
      "venue" : "In Proceedings of the Nineteenth International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "Milch et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Milch et al\\.",
      "year" : 2005
    }, {
      "title" : "Dynamic Bayesian Networks: Representation, Inference and Learning",
      "author" : [ "Kevin P. Murphy" ],
      "venue" : "Ph.D. thesis,",
      "citeRegEx" : "Murphy.,? \\Q2002\\E",
      "shortCiteRegEx" : "Murphy.",
      "year" : 2002
    }, {
      "title" : "Learning Bayesian Networks",
      "author" : [ "Richard E. Neapolitan" ],
      "venue" : null,
      "citeRegEx" : "Neapolitan.,? \\Q2004\\E",
      "shortCiteRegEx" : "Neapolitan.",
      "year" : 2004
    }, {
      "title" : "Leveraging relational autocorrelation with latent group models",
      "author" : [ "Jennifer Neville", "David D. Jensen" ],
      "venue" : "In Proceedings of the Fifth IEEE International Conference on Data Mining,",
      "citeRegEx" : "Neville and Jensen.,? \\Q2005\\E",
      "shortCiteRegEx" : "Neville and Jensen.",
      "year" : 2005
    }, {
      "title" : "Relational dependency networks",
      "author" : [ "Jennifer Neville", "David D. Jensen" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Neville and Jensen.,? \\Q2007\\E",
      "shortCiteRegEx" : "Neville and Jensen.",
      "year" : 2007
    }, {
      "title" : "Causality: Models, Reasoning, and Inference",
      "author" : [ "Judea Pearl" ],
      "venue" : null,
      "citeRegEx" : "Pearl.,? \\Q2000\\E",
      "shortCiteRegEx" : "Pearl.",
      "year" : 2000
    }, {
      "title" : "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference",
      "author" : [ "Judea Pearl" ],
      "venue" : null,
      "citeRegEx" : "Pearl.,? \\Q1988\\E",
      "shortCiteRegEx" : "Pearl.",
      "year" : 1988
    }, {
      "title" : "Identifying independencies in causal graphs with feedback",
      "author" : [ "Judea Pearl", "Rina Dechter" ],
      "venue" : "In Proceedings of the Twelfth International Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Pearl and Dechter.,? \\Q1996\\E",
      "shortCiteRegEx" : "Pearl and Dechter.",
      "year" : 1996
    }, {
      "title" : "A theory of inferred causation",
      "author" : [ "Judea Pearl", "Thomas S. Verma" ],
      "venue" : "In Principles of Knowledge Representation and Reasoning: Proceedings of the Second International Conference,",
      "citeRegEx" : "Pearl and Verma.,? \\Q1991\\E",
      "shortCiteRegEx" : "Pearl and Verma.",
      "year" : 1991
    }, {
      "title" : "Using Markov blankets for causal structure learning",
      "author" : [ "Jean-Philippe Pellet", "André Elisseeff" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Pellet and Elisseeff.,? \\Q2008\\E",
      "shortCiteRegEx" : "Pellet and Elisseeff.",
      "year" : 2008
    }, {
      "title" : "Distribution-based aggregation for relational learning with identifier attributes",
      "author" : [ "Claudia Perlich", "Foster Provost" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Perlich and Provost.,? \\Q2006\\E",
      "shortCiteRegEx" : "Perlich and Provost.",
      "year" : 2006
    }, {
      "title" : "First-order probabilistic inference",
      "author" : [ "David Poole" ],
      "venue" : "In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "Poole.,? \\Q2003\\E",
      "shortCiteRegEx" : "Poole.",
      "year" : 2003
    }, {
      "title" : "Logical generative models for probabilistic reasoning about existence, roles and identity",
      "author" : [ "David Poole" ],
      "venue" : "In Proceedings of the Twenty-Second National Conference on Artificial Intelligence,",
      "citeRegEx" : "Poole.,? \\Q2007\\E",
      "shortCiteRegEx" : "Poole.",
      "year" : 2007
    }, {
      "title" : "Database Management Systems",
      "author" : [ "Raghu Ramakrishnan", "Johannes Gehrke" ],
      "venue" : null,
      "citeRegEx" : "Ramakrishnan and Gehrke.,? \\Q2002\\E",
      "shortCiteRegEx" : "Ramakrishnan and Gehrke.",
      "year" : 2002
    }, {
      "title" : "Ancestral graph Markov models",
      "author" : [ "Thomas Richardson", "Peter Spirtes" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Richardson and Spirtes.,? \\Q2002\\E",
      "shortCiteRegEx" : "Richardson and Spirtes.",
      "year" : 2002
    }, {
      "title" : "Feedback Models: Interpretation and Discovery",
      "author" : [ "Thomas S. Richardson" ],
      "venue" : "Ph.D. thesis,",
      "citeRegEx" : "Richardson.,? \\Q1996\\E",
      "shortCiteRegEx" : "Richardson.",
      "year" : 1996
    }, {
      "title" : "A factorization criterion for acyclic directed mixed graphs",
      "author" : [ "Thomas S. Richardson" ],
      "venue" : "Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Richardson.,? \\Q2009\\E",
      "shortCiteRegEx" : "Richardson.",
      "year" : 2009
    }, {
      "title" : "Artificial Intelligence: A Modern Approach",
      "author" : [ "Stuart Russell", "Peter Norvig" ],
      "venue" : null,
      "citeRegEx" : "Russell and Norvig.,? \\Q2010\\E",
      "shortCiteRegEx" : "Russell and Norvig.",
      "year" : 2010
    }, {
      "title" : "An introduction to causal inference",
      "author" : [ "Richard Scheines" ],
      "venue" : null,
      "citeRegEx" : "Scheines.,? \\Q1997\\E",
      "shortCiteRegEx" : "Scheines.",
      "year" : 1997
    }, {
      "title" : "Modeling discrete interventional data using directed cyclic graphical models",
      "author" : [ "Mark Schmidt", "Kevin Murphy" ],
      "venue" : "In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Schmidt and Murphy.,? \\Q2009\\E",
      "shortCiteRegEx" : "Schmidt and Murphy.",
      "year" : 2009
    }, {
      "title" : "Learning directed relational models with recursive dependencies",
      "author" : [ "Oliver Schulte", "Hassan Khosravi", "Tong Man" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Schulte et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Schulte et al\\.",
      "year" : 2012
    }, {
      "title" : "Rich probabilistic models for gene expression",
      "author" : [ "Eran Segal", "Ben Taskar", "Audrey Gasch", "Nir Friedman", "Daphne Koller" ],
      "venue" : "Bioinformatics, 17(suppl 1):S243–S252,",
      "citeRegEx" : "Segal et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Segal et al\\.",
      "year" : 2001
    }, {
      "title" : "Homophily and contagion are generically confounded in observational social network studies",
      "author" : [ "Cosma R. Shalizi", "Andrew C. Thomas" ],
      "venue" : "Sociological Methods & Research,",
      "citeRegEx" : "Shalizi and Thomas.,? \\Q2011\\E",
      "shortCiteRegEx" : "Shalizi and Thomas.",
      "year" : 2011
    }, {
      "title" : "Relevance search in heterogeneous networks",
      "author" : [ "Chuan Shi", "Xiangnan Kong", "Philip S. Yu", "Sihong Xie", "Bin Wu" ],
      "venue" : "In Proceedings of the Fifteenth International Conference on Extending Database Technology,",
      "citeRegEx" : "Shi et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2012
    }, {
      "title" : "Complete identification methods for the causal hierarchy",
      "author" : [ "Ilya Shpitser", "Judea Pearl" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Shpitser and Pearl.,? \\Q2008\\E",
      "shortCiteRegEx" : "Shpitser and Pearl.",
      "year" : 2008
    }, {
      "title" : "A probabilistic relational model for security risk analysis",
      "author" : [ "Teodor Sommestad", "Mathias Ekstedt", "Pontus Johnson" ],
      "venue" : "Computers & Security,",
      "citeRegEx" : "Sommestad et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Sommestad et al\\.",
      "year" : 2010
    }, {
      "title" : "Directed cyclic graphical representations of feedback models",
      "author" : [ "Peter Spirtes" ],
      "venue" : "In Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Spirtes.,? \\Q1995\\E",
      "shortCiteRegEx" : "Spirtes.",
      "year" : 1995
    }, {
      "title" : "Causal inference in the presence of latent variables and selection bias",
      "author" : [ "Peter Spirtes", "Christopher Meek", "Thomas Richardson" ],
      "venue" : "In Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Spirtes et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Spirtes et al\\.",
      "year" : 1995
    }, {
      "title" : "Causation, Prediction and Search",
      "author" : [ "Peter Spirtes", "Clark Glymour", "Richard Scheines" ],
      "venue" : null,
      "citeRegEx" : "Spirtes et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Spirtes et al\\.",
      "year" : 2000
    }, {
      "title" : "PathSim: Meta pathbased top-k similarity search in heterogeneous information networks",
      "author" : [ "Yizhou Sun", "Jiawei Han", "Xifeng Yan", "Philip S. Yu", "Tianyi Wu" ],
      "venue" : "In Proceedings of the VLDB Endowment,",
      "citeRegEx" : "Sun et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2011
    }, {
      "title" : "Probabilistic classification and clustering in relational data",
      "author" : [ "Ben Taskar", "Eran Segal", "Daphne Koller" ],
      "venue" : "In Proceedings of the Seventeenth International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "Taskar et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Taskar et al\\.",
      "year" : 2001
    }, {
      "title" : "Discriminative probabilistic models for relational data",
      "author" : [ "Ben Taskar", "Pieter Abbeel", "Daphne Koller" ],
      "venue" : "In Proceedings of the Eighteenth Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Taskar et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Taskar et al\\.",
      "year" : 2002
    }, {
      "title" : "On causal inference in the presence of interference",
      "author" : [ "Eric J. Tchetgen Tchetgen", "Tyler J. VanderWeele" ],
      "venue" : "Statistical Methods in Medical Research,",
      "citeRegEx" : "Tchetgen and VanderWeele.,? \\Q2012\\E",
      "shortCiteRegEx" : "Tchetgen and VanderWeele.",
      "year" : 2012
    }, {
      "title" : "A general identification condition for causal effects",
      "author" : [ "Jin Tian", "Judea Pearl" ],
      "venue" : "In Proceedings of the Eighteenth National Conference on Artificial Intelligence,",
      "citeRegEx" : "Tian and Pearl.,? \\Q2002\\E",
      "shortCiteRegEx" : "Tian and Pearl.",
      "year" : 2002
    }, {
      "title" : "Finding Minimal D-separators",
      "author" : [ "Jin Tian", "Azaria Paz", "Judea Pearl" ],
      "venue" : "Technical Report R-254,",
      "citeRegEx" : "Tian et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Tian et al\\.",
      "year" : 1998
    }, {
      "title" : "Regression shrinkage and selection via the lasso",
      "author" : [ "Robert Tibshirani" ],
      "venue" : "Journal of the Royal Statistical Society. Series B (Methodological),",
      "citeRegEx" : "Tibshirani.,? \\Q1996\\E",
      "shortCiteRegEx" : "Tibshirani.",
      "year" : 1996
    }, {
      "title" : "The max-min hillclimbing Bayesian network structure learning algorithm",
      "author" : [ "Ioannis Tsamardinos", "Laura E. Brown", "Constantin F. Aliferis" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Tsamardinos et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Tsamardinos et al\\.",
      "year" : 2006
    }, {
      "title" : "Causal networks: Semantics and expressiveness",
      "author" : [ "Thomas Verma", "Judea Pearl" ],
      "venue" : "In Proceedings of the Fourth Annual Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Verma and Pearl.,? \\Q1988\\E",
      "shortCiteRegEx" : "Verma and Pearl.",
      "year" : 1988
    }, {
      "title" : "Causality with gates",
      "author" : [ "John Winn" ],
      "venue" : "In Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Winn.,? \\Q2012\\E",
      "shortCiteRegEx" : "Winn.",
      "year" : 2012
    }, {
      "title" : "Relational learning with one network: An asymptotic analysis",
      "author" : [ "Rongjing Xiang", "Jennifer Neville" ],
      "venue" : "In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Xiang and Neville.,? \\Q2011\\E",
      "shortCiteRegEx" : "Xiang and Neville.",
      "year" : 2011
    }, {
      "title" : "Bayesian network structure learning by recursive autonomy identification",
      "author" : [ "Raanan Yehezkel", "Boaz Lerner" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Yehezkel and Lerner.,? \\Q2009\\E",
      "shortCiteRegEx" : "Yehezkel and Lerner.",
      "year" : 2009
    }, {
      "title" : "On the completeness of orientation rules for causal discovery in the presence of latent confounders and selection bias",
      "author" : [ "Jiji Zhang" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Zhang.,? \\Q2008\\E",
      "shortCiteRegEx" : "Zhang.",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 67,
      "context" : "Accurate reasoning about such conditional independence facts is the basis for constraint-based algorithms, such as PC and FCI (Spirtes et al., 2000), and hybrid approaches, such as MMHC (Tsamardinos et al.",
      "startOffset" : 126,
      "endOffset" : 148
    }, {
      "referenceID" : 75,
      "context" : ", 2000), and hybrid approaches, such as MMHC (Tsamardinos et al., 2006), that are commonly used to learn the structure of Bayesian networks.",
      "startOffset" : 45,
      "endOffset" : 71
    }, {
      "referenceID" : 44,
      "context" : "Under a small number of assumptions and with knowledge of the conditional independencies, these algorithms can identify causal structure (Pearl, 2000; Spirtes et al., 2000).",
      "startOffset" : 137,
      "endOffset" : 172
    }, {
      "referenceID" : 67,
      "context" : "Under a small number of assumptions and with knowledge of the conditional independencies, these algorithms can identify causal structure (Pearl, 2000; Spirtes et al., 2000).",
      "startOffset" : 137,
      "endOffset" : 172
    }, {
      "referenceID" : 76,
      "context" : "That is, the Markov condition and d -separation are equivalent approaches for producing conditional independence from Bayesian networks (Verma and Pearl, 1988; Geiger and Pearl, 1988; Neapolitan, 2004).",
      "startOffset" : 136,
      "endOffset" : 201
    }, {
      "referenceID" : 15,
      "context" : "That is, the Markov condition and d -separation are equivalent approaches for producing conditional independence from Bayesian networks (Verma and Pearl, 1988; Geiger and Pearl, 1988; Neapolitan, 2004).",
      "startOffset" : 136,
      "endOffset" : 201
    }, {
      "referenceID" : 41,
      "context" : "That is, the Markov condition and d -separation are equivalent approaches for producing conditional independence from Bayesian networks (Verma and Pearl, 1988; Geiger and Pearl, 1988; Neapolitan, 2004).",
      "startOffset" : 136,
      "endOffset" : 201
    }, {
      "referenceID" : 57,
      "context" : "When interpreting a Bayesian network causally, the causal Markov condition (variables are independent of their non-effects given their direct causes) and d separation have been shown to provide the correct connection between causal structure and conditional independence (Scheines, 1997).",
      "startOffset" : 271,
      "endOffset" : 287
    }, {
      "referenceID" : 29,
      "context" : "Relational models generalize other classes of models that incorporate interference, spillover effects, or violations of the stable unit treatment value assumption (SUTVA) (Hudgens and Halloran, 2008; Tchetgen Tchetgen and VanderWeele, 2012) and multilevel or hierarchical models (Gelman and Hill, 2007).",
      "startOffset" : 171,
      "endOffset" : 240
    }, {
      "referenceID" : 18,
      "context" : "Relational models generalize other classes of models that incorporate interference, spillover effects, or violations of the stable unit treatment value assumption (SUTVA) (Hudgens and Halloran, 2008; Tchetgen Tchetgen and VanderWeele, 2012) and multilevel or hierarchical models (Gelman and Hill, 2007).",
      "startOffset" : 279,
      "endOffset" : 302
    }, {
      "referenceID" : 60,
      "context" : "Examples include analysis of gene regulatory interactions (Segal et al., 2001), scholarly citations (Taskar et al.",
      "startOffset" : 58,
      "endOffset" : 78
    }, {
      "referenceID" : 69,
      "context" : ", 2001), scholarly citations (Taskar et al., 2001), ecosystems (D’Ambrosio et al.",
      "startOffset" : 29,
      "endOffset" : 50
    }, {
      "referenceID" : 8,
      "context" : ", 2001), ecosystems (D’Ambrosio et al., 2003), biological cellular networks (Friedman, 2004), epidemiology (Getoor et al.",
      "startOffset" : 20,
      "endOffset" : 45
    }, {
      "referenceID" : 12,
      "context" : ", 2003), biological cellular networks (Friedman, 2004), epidemiology (Getoor et al.",
      "startOffset" : 38,
      "endOffset" : 54
    }, {
      "referenceID" : 23,
      "context" : ", 2003), biological cellular networks (Friedman, 2004), epidemiology (Getoor et al., 2004), and security in information systems (Sommestad et al.",
      "startOffset" : 69,
      "endOffset" : 90
    }, {
      "referenceID" : 64,
      "context" : ", 2004), and security in information systems (Sommestad et al., 2010).",
      "startOffset" : 45,
      "endOffset" : 69
    }, {
      "referenceID" : 72,
      "context" : "This work enables sound and complete identification of causal effects, not only with respect to conditioning, but also under counterfactuals and interventions—via the do-calculus introduced by Pearl (2000)— and in the presence of latent variables (Tian and Pearl, 2002; Huang and Valtorta, 2006; Shpitser and Pearl, 2008).",
      "startOffset" : 247,
      "endOffset" : 321
    }, {
      "referenceID" : 28,
      "context" : "This work enables sound and complete identification of causal effects, not only with respect to conditioning, but also under counterfactuals and interventions—via the do-calculus introduced by Pearl (2000)— and in the presence of latent variables (Tian and Pearl, 2002; Huang and Valtorta, 2006; Shpitser and Pearl, 2008).",
      "startOffset" : 247,
      "endOffset" : 321
    }, {
      "referenceID" : 63,
      "context" : "This work enables sound and complete identification of causal effects, not only with respect to conditioning, but also under counterfactuals and interventions—via the do-calculus introduced by Pearl (2000)— and in the presence of latent variables (Tian and Pearl, 2002; Huang and Valtorta, 2006; Shpitser and Pearl, 2008).",
      "startOffset" : 247,
      "endOffset" : 321
    }, {
      "referenceID" : 43,
      "context" : "This work enables sound and complete identification of causal effects, not only with respect to conditioning, but also under counterfactuals and interventions—via the do-calculus introduced by Pearl (2000)— and in the presence of latent variables (Tian and Pearl, 2002; Huang and Valtorta, 2006; Shpitser and Pearl, 2008).",
      "startOffset" : 193,
      "endOffset" : 206
    }, {
      "referenceID" : 47,
      "context" : "This approach to learning causal structure is referred to as the constraint-based paradigm, and many algorithms that follow this approach have been developed over the past 20 years, including Inductive Causation (IC) (Pearl and Verma, 1991), PC (Spirtes et al.",
      "startOffset" : 217,
      "endOffset" : 240
    }, {
      "referenceID" : 67,
      "context" : "This approach to learning causal structure is referred to as the constraint-based paradigm, and many algorithms that follow this approach have been developed over the past 20 years, including Inductive Causation (IC) (Pearl and Verma, 1991), PC (Spirtes et al., 2000) and its variants, Three Phase Dependency Analysis (TPDA) (Cheng et al.",
      "startOffset" : 245,
      "endOffset" : 267
    }, {
      "referenceID" : 4,
      "context" : ", 2000) and its variants, Three Phase Dependency Analysis (TPDA) (Cheng et al., 1997), Grow-Shrink (Margaritis and Thrun, 1999), Total Conditioning (TC) (Pellet and Elisseeff, 2008), Recursive Autonomy Identification (RAI) (Yehezkel and Lerner, 2009), and hybrid methods that partially employ this approach, including Max-Min Hill Climbing (MMHC) (Tsamardinos et al.",
      "startOffset" : 65,
      "endOffset" : 85
    }, {
      "referenceID" : 38,
      "context" : ", 1997), Grow-Shrink (Margaritis and Thrun, 1999), Total Conditioning (TC) (Pellet and Elisseeff, 2008), Recursive Autonomy Identification (RAI) (Yehezkel and Lerner, 2009), and hybrid methods that partially employ this approach, including Max-Min Hill Climbing (MMHC) (Tsamardinos et al.",
      "startOffset" : 21,
      "endOffset" : 49
    }, {
      "referenceID" : 48,
      "context" : ", 1997), Grow-Shrink (Margaritis and Thrun, 1999), Total Conditioning (TC) (Pellet and Elisseeff, 2008), Recursive Autonomy Identification (RAI) (Yehezkel and Lerner, 2009), and hybrid methods that partially employ this approach, including Max-Min Hill Climbing (MMHC) (Tsamardinos et al.",
      "startOffset" : 75,
      "endOffset" : 103
    }, {
      "referenceID" : 79,
      "context" : ", 1997), Grow-Shrink (Margaritis and Thrun, 1999), Total Conditioning (TC) (Pellet and Elisseeff, 2008), Recursive Autonomy Identification (RAI) (Yehezkel and Lerner, 2009), and hybrid methods that partially employ this approach, including Max-Min Hill Climbing (MMHC) (Tsamardinos et al.",
      "startOffset" : 145,
      "endOffset" : 172
    }, {
      "referenceID" : 75,
      "context" : ", 1997), Grow-Shrink (Margaritis and Thrun, 1999), Total Conditioning (TC) (Pellet and Elisseeff, 2008), Recursive Autonomy Identification (RAI) (Yehezkel and Lerner, 2009), and hybrid methods that partially employ this approach, including Max-Min Hill Climbing (MMHC) (Tsamardinos et al., 2006) and Hybrid HPC (H2PC) (Gasse et al.",
      "startOffset" : 269,
      "endOffset" : 295
    }, {
      "referenceID" : 14,
      "context" : ", 2006) and Hybrid HPC (H2PC) (Gasse et al., 2012).",
      "startOffset" : 30,
      "endOffset" : 50
    }, {
      "referenceID" : 37,
      "context" : "In another paper, we have used these results to provide a theoretical framework for a sound and complete constraint-based algorithm—the Relational Causal Discovery (RCD) algorithm (Maier et al., 2013)—that learns causal models of relational domains.",
      "startOffset" : 180,
      "endOffset" : 200
    }, {
      "referenceID" : 11,
      "context" : "The inductive logic programming (ILP) community has discussed individual-centered representations (Flach, 1999), and many approaches to propositionalizing relational data have been developed to enforce a single perspective in order to rely on existing propositional learning algorithms (Kramer et al.",
      "startOffset" : 98,
      "endOffset" : 111
    }, {
      "referenceID" : 34,
      "context" : "The inductive logic programming (ILP) community has discussed individual-centered representations (Flach, 1999), and many approaches to propositionalizing relational data have been developed to enforce a single perspective in order to rely on existing propositional learning algorithms (Kramer et al., 2001).",
      "startOffset" : 286,
      "endOffset" : 307
    }, {
      "referenceID" : 13,
      "context" : ", learning the structure of probabilistic relational models, relational dependency networks, or parametrized Bayesian networks (Friedman et al., 1999; Neville and Jensen, 2007; Schulte et al., 2012).",
      "startOffset" : 127,
      "endOffset" : 198
    }, {
      "referenceID" : 43,
      "context" : ", learning the structure of probabilistic relational models, relational dependency networks, or parametrized Bayesian networks (Friedman et al., 1999; Neville and Jensen, 2007; Schulte et al., 2012).",
      "startOffset" : 127,
      "endOffset" : 198
    }, {
      "referenceID" : 59,
      "context" : ", learning the structure of probabilistic relational models, relational dependency networks, or parametrized Bayesian networks (Friedman et al., 1999; Neville and Jensen, 2007; Schulte et al., 2012).",
      "startOffset" : 127,
      "endOffset" : 198
    }, {
      "referenceID" : 78,
      "context" : "There are at least three options: • Construct three sets of variables, including all instances of competence, revenue, and success variables: Although the ground graph has the semantics of a Bayesian network, there is only a single ground graph—one data sample (Xiang and Neville, 2011).",
      "startOffset" : 261,
      "endOffset" : 286
    }, {
      "referenceID" : 33,
      "context" : "These relational representations can be divided into two main categories: probabilistic graphical models— such as probablistic relational models (PRMs) (Koller and Pfeffer, 1998), directed acyclic probabilistic entity-relationship (DAPER) models (Heckerman et al.",
      "startOffset" : 152,
      "endOffset" : 178
    }, {
      "referenceID" : 26,
      "context" : "These relational representations can be divided into two main categories: probabilistic graphical models— such as probablistic relational models (PRMs) (Koller and Pfeffer, 1998), directed acyclic probabilistic entity-relationship (DAPER) models (Heckerman et al., 2004), and relational Markov networks (RMNs) (Taskar et al.",
      "startOffset" : 246,
      "endOffset" : 270
    }, {
      "referenceID" : 70,
      "context" : ", 2004), and relational Markov networks (RMNs) (Taskar et al., 2002)—and probabilistic logic models—such as Bayesian logic programs (BLPs) (Kersting and De Raedt, 2002), Markov logic networks (MLNs) (Richardson and Domingos, 2006), parametrized Bayesian networks (PBNs) (Poole, 2003), Bayesian logic (Blog) (Milch et al.",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 50,
      "context" : ", 2002)—and probabilistic logic models—such as Bayesian logic programs (BLPs) (Kersting and De Raedt, 2002), Markov logic networks (MLNs) (Richardson and Domingos, 2006), parametrized Bayesian networks (PBNs) (Poole, 2003), Bayesian logic (Blog) (Milch et al.",
      "startOffset" : 209,
      "endOffset" : 222
    }, {
      "referenceID" : 39,
      "context" : ", 2002)—and probabilistic logic models—such as Bayesian logic programs (BLPs) (Kersting and De Raedt, 2002), Markov logic networks (MLNs) (Richardson and Domingos, 2006), parametrized Bayesian networks (PBNs) (Poole, 2003), Bayesian logic (Blog) (Milch et al., 2005), multi-entity Bayesian networks (MEBNs) (Laskey, 2008), and relational probability models (RPMs) (Russell and Norvig, 2010).",
      "startOffset" : 246,
      "endOffset" : 266
    }, {
      "referenceID" : 35,
      "context" : ", 2005), multi-entity Bayesian networks (MEBNs) (Laskey, 2008), and relational probability models (RPMs) (Russell and Norvig, 2010).",
      "startOffset" : 48,
      "endOffset" : 62
    }, {
      "referenceID" : 56,
      "context" : ", 2005), multi-entity Bayesian networks (MEBNs) (Laskey, 2008), and relational probability models (RPMs) (Russell and Norvig, 2010).",
      "startOffset" : 105,
      "endOffset" : 131
    }, {
      "referenceID" : 0,
      "context" : "We adopt a slightly modified ER diagram using Barker’s notation (1990), where entity classes are rectangular boxes, relationship classes are diamonds with dashed lines connecting their associated entity classes, attribute classes are ovals residing on entity and relationship classes, and cardinalities are represented with crow’s foot notation.",
      "startOffset" : 46,
      "endOffset" : 71
    }, {
      "referenceID" : 68,
      "context" : "This definition of relational paths is similar to “meta-paths” and “relevance paths” in similarity search and information retrieval in heterogeneous networks (Sun et al., 2011; Shi et al., 2012).",
      "startOffset" : 158,
      "endOffset" : 194
    }, {
      "referenceID" : 62,
      "context" : "This definition of relational paths is similar to “meta-paths” and “relevance paths” in similarity search and information retrieval in heterogeneous networks (Sun et al., 2011; Shi et al., 2012).",
      "startOffset" : 158,
      "endOffset" : 194
    }, {
      "referenceID" : 20,
      "context" : "Relational paths also extend the notion of “slot chains” from the PRM framework (Getoor et al., 2007) by including cardinality constraints and formally describing the semantics under which repeated item classes may appear on a path.",
      "startOffset" : 80,
      "endOffset" : 101
    }, {
      "referenceID" : 27,
      "context" : "Relational paths are also a specialization of the first-order constraints on arc classes imposed on DAPER models (Heckerman et al., 2007).",
      "startOffset" : 113,
      "endOffset" : 137
    }, {
      "referenceID" : 20,
      "context" : ", for PRMs (Getoor et al., 2007) and MLNs (Richardson and Domingos, 2006)—but has not been explicitly named.",
      "startOffset" : 11,
      "endOffset" : 32
    }, {
      "referenceID" : 49,
      "context" : "Typically, aggregation functions are simple, such as mean or mode, but they can be complex, such as those based on vector distance or object identifiers, as in the ACORA system (Perlich and Provost, 2006).",
      "startOffset" : 177,
      "endOffset" : 204
    }, {
      "referenceID" : 27,
      "context" : "This definition of relational models is consistent with and yields structures expressible as DAPER models (Heckerman et al., 2007).",
      "startOffset" : 106,
      "endOffset" : 130
    }, {
      "referenceID" : 3,
      "context" : "These models are also more general than plate models because dependencies can be specified with arbitrary relational paths as opposed to simple intersections among plates (Buntine, 1994; Gilks et al., 1994).",
      "startOffset" : 171,
      "endOffset" : 206
    }, {
      "referenceID" : 25,
      "context" : "These models are also more general than plate models because dependencies can be specified with arbitrary relational paths as opposed to simple intersections among plates (Buntine, 1994; Gilks et al., 1994).",
      "startOffset" : 171,
      "endOffset" : 206
    }, {
      "referenceID" : 20,
      "context" : "A useful construct for checking model acyclicity is the class dependency graph (Getoor et al., 2007), defined as:",
      "startOffset" : 79,
      "endOffset" : 100
    }, {
      "referenceID" : 20,
      "context" : "If the relational dependencies form an acyclic class dependency graph, then every possible ground graph of that model is acyclic as well (Getoor et al., 2007).",
      "startOffset" : 137,
      "endOffset" : 158
    }, {
      "referenceID" : 19,
      "context" : "Given an acyclic relational model, the ground graph has the same semantics as a Bayesian network (Getoor, 2001; Heckerman et al., 2007).",
      "startOffset" : 97,
      "endOffset" : 135
    }, {
      "referenceID" : 27,
      "context" : "Given an acyclic relational model, the ground graph has the same semantics as a Bayesian network (Getoor, 2001; Heckerman et al., 2007).",
      "startOffset" : 97,
      "endOffset" : 135
    }, {
      "referenceID" : 16,
      "context" : "Using the algorithm devised by Geiger et al. (1990), relational d -separation queries can be answered in O(|E|) time with respect to the number of edges in the abstract ground graph.",
      "startOffset" : 31,
      "endOffset" : 52
    }, {
      "referenceID" : 43,
      "context" : "Due to Verma and Pearl (1988) for soundness and Geiger and Pearl (1988) for completeness.",
      "startOffset" : 17,
      "endOffset" : 30
    }, {
      "referenceID" : 15,
      "context" : "Due to Verma and Pearl (1988) for soundness and Geiger and Pearl (1988) for completeness.",
      "startOffset" : 48,
      "endOffset" : 72
    }, {
      "referenceID" : 25,
      "context" : "Due to both Heckerman et al. (2007) for DAPER models and Getoor (2001) for PRMs.",
      "startOffset" : 12,
      "endOffset" : 36
    }, {
      "referenceID" : 19,
      "context" : "(2007) for DAPER models and Getoor (2001) for PRMs.",
      "startOffset" : 28,
      "endOffset" : 42
    }, {
      "referenceID" : 41,
      "context" : "For Bayesian networks, the Markov condition is equivalent to d -separation (Neapolitan, 2004).",
      "startOffset" : 75,
      "endOffset" : 93
    }, {
      "referenceID" : 73,
      "context" : "To identify a minimal separating set between relational variables X and Y, we modified Algorithm 4 devised by Tian et al. (1998) by starting with all parents of X̄ and Ȳ, the variables augmented with the intersection variables they subsume in the abstract ground graph.",
      "startOffset" : 110,
      "endOffset" : 129
    }, {
      "referenceID" : 61,
      "context" : "Relationship instances were constructed via a latent homophily process, similar to the method used by Shalizi and Thomas (2011). Each entity instance received a single latent variable, marginally independent from all other variables.",
      "startOffset" : 102,
      "endOffset" : 128
    }, {
      "referenceID" : 36,
      "context" : "These queries correspond to testing potential direct causal dependencies in the relational model, similar to the tests used by constraint-based methods for learning relational models, such as RPC (Maier et al., 2010) and RCD (Maier et al.",
      "startOffset" : 196,
      "endOffset" : 216
    }, {
      "referenceID" : 37,
      "context" : ", 2010) and RCD (Maier et al., 2013).",
      "startOffset" : 16,
      "endOffset" : 36
    }, {
      "referenceID" : 52,
      "context" : "However, a common procedure in entity-relationship modeling is to map entity names to unique role indicators within the context of a self-relationship, such as manager/subordinate, friend1/friend2, or citing-paper/cited-paper (Ramakrishnan and Gehrke, 2002).",
      "startOffset" : 226,
      "endOffset" : 257
    }, {
      "referenceID" : 31,
      "context" : "Relational autocorrelation: In contrast to self-relationships, relational autocorrelation is a statistical dependency among the values of the same attribute class frequently found in relational data sets (Jensen and Neville, 2002).",
      "startOffset" : 204,
      "endOffset" : 230
    }, {
      "referenceID" : 43,
      "context" : "Various models and learning algorithms have been developed to capture these types of dependencies, such as RDNs (Neville and Jensen, 2007), PBNs with an extended normal form (Schulte et al.",
      "startOffset" : 112,
      "endOffset" : 138
    }, {
      "referenceID" : 59,
      "context" : "Various models and learning algorithms have been developed to capture these types of dependencies, such as RDNs (Neville and Jensen, 2007), PBNs with an extended normal form (Schulte et al., 2012), and PRMs with dependencies that follow guaranteed acyclic relationships (Getoor et al.",
      "startOffset" : 174,
      "endOffset" : 196
    }, {
      "referenceID" : 20,
      "context" : ", 2012), and PRMs with dependencies that follow guaranteed acyclic relationships (Getoor et al., 2007).",
      "startOffset" : 81,
      "endOffset" : 102
    }, {
      "referenceID" : 27,
      "context" : "This can be achieved within the specification of conditional probability distributions as if-then-else statements of logical conditions, such as in DAPER models (Heckerman et al., 2007) or RPMs (Russell and Norvig, 2010), encoded as regularities in conditional probability tables (Boutilier et al.",
      "startOffset" : 161,
      "endOffset" : 185
    }, {
      "referenceID" : 56,
      "context" : ", 2007) or RPMs (Russell and Norvig, 2010), encoded as regularities in conditional probability tables (Boutilier et al.",
      "startOffset" : 16,
      "endOffset" : 42
    }, {
      "referenceID" : 2,
      "context" : ", 2007) or RPMs (Russell and Norvig, 2010), encoded as regularities in conditional probability tables (Boutilier et al., 1996), or with the recent graphical convention of gates (Minka and Winn, 2009).",
      "startOffset" : 102,
      "endOffset" : 126
    }, {
      "referenceID" : 21,
      "context" : "PRMs with class hierarchies allow a hierarchy of entity types where the dependency structure can vary depending on the type (Getoor et al., 2000).",
      "startOffset" : 124,
      "endOffset" : 145
    }, {
      "referenceID" : 2,
      "context" : ", 2007) or RPMs (Russell and Norvig, 2010), encoded as regularities in conditional probability tables (Boutilier et al., 1996), or with the recent graphical convention of gates (Minka and Winn, 2009). However, this introduces a notion of independence that cannot be inferred from model structure via traditional d -separation. In fact, Boutilier et al. (1996) define an analogous approach based on d -separation of a manipulated Bayesian network through deletion of vacuous dependencies given some context.",
      "startOffset" : 103,
      "endOffset" : 360
    }, {
      "referenceID" : 2,
      "context" : ", 2007) or RPMs (Russell and Norvig, 2010), encoded as regularities in conditional probability tables (Boutilier et al., 1996), or with the recent graphical convention of gates (Minka and Winn, 2009). However, this introduces a notion of independence that cannot be inferred from model structure via traditional d -separation. In fact, Boutilier et al. (1996) define an analogous approach based on d -separation of a manipulated Bayesian network through deletion of vacuous dependencies given some context. Winn (2012) extends the rules of d -separation to reason over the additional paths and their collective state introduced by gates.",
      "startOffset" : 103,
      "endOffset" : 519
    }, {
      "referenceID" : 39,
      "context" : "There are relational models that attempt to learn and represent models with unknown numbers of entity instances, such as Blog (Milch et al., 2005), or uncertain relationship instances, such as PRMs with existence uncertainty (Getoor et al.",
      "startOffset" : 126,
      "endOffset" : 146
    }, {
      "referenceID" : 22,
      "context" : ", 2005), or uncertain relationship instances, such as PRMs with existence uncertainty (Getoor et al., 2002).",
      "startOffset" : 86,
      "endOffset" : 107
    }, {
      "referenceID" : 36,
      "context" : "For relationship existence, selection bias (conditioning) occurs when testing marginal dependence between variables across a particular relationship (Maier et al., 2010).",
      "startOffset" : 149,
      "endOffset" : 169
    }, {
      "referenceID" : 51,
      "context" : "For entity existence, some researchers argue that existence cannot be represented as a variable or predicate (Poole, 2007), while others represent them as predicates (Laskey, 2008).",
      "startOffset" : 109,
      "endOffset" : 122
    }, {
      "referenceID" : 35,
      "context" : "For entity existence, some researchers argue that existence cannot be represented as a variable or predicate (Poole, 2007), while others represent them as predicates (Laskey, 2008).",
      "startOffset" : 166,
      "endOffset" : 180
    }, {
      "referenceID" : 1,
      "context" : "Many researchers have developed methods for learning and inference by explicitly modeling unobserved variables—typically termed latent variable models (Bishop, 1999)—or inferring the presence of latent entity classes—for example, latent group models (Neville and Jensen, 2005).",
      "startOffset" : 151,
      "endOffset" : 165
    }, {
      "referenceID" : 42,
      "context" : "Many researchers have developed methods for learning and inference by explicitly modeling unobserved variables—typically termed latent variable models (Bishop, 1999)—or inferring the presence of latent entity classes—for example, latent group models (Neville and Jensen, 2005).",
      "startOffset" : 250,
      "endOffset" : 276
    }, {
      "referenceID" : 53,
      "context" : "However, only ancestral graphs and acyclic directed mixed graphs (ADMGs) do so in order to preserve an underlying conditional independence structure (Richardson and Spirtes, 2002; Richardson, 2009).",
      "startOffset" : 149,
      "endOffset" : 197
    }, {
      "referenceID" : 55,
      "context" : "However, only ancestral graphs and acyclic directed mixed graphs (ADMGs) do so in order to preserve an underlying conditional independence structure (Richardson and Spirtes, 2002; Richardson, 2009).",
      "startOffset" : 149,
      "endOffset" : 197
    }, {
      "referenceID" : 66,
      "context" : "Given that a primary motivation for d -separation is to support constraint-based causal discovery, any relational extension to algorithms that learn causal models without assuming causal sufficiency, such as FCI (Spirtes et al., 1995; Zhang, 2008), its variants (Claassen and Heskes, 2011; Colombo et al.",
      "startOffset" : 212,
      "endOffset" : 247
    }, {
      "referenceID" : 80,
      "context" : "Given that a primary motivation for d -separation is to support constraint-based causal discovery, any relational extension to algorithms that learn causal models without assuming causal sufficiency, such as FCI (Spirtes et al., 1995; Zhang, 2008), its variants (Claassen and Heskes, 2011; Colombo et al.",
      "startOffset" : 212,
      "endOffset" : 247
    }, {
      "referenceID" : 5,
      "context" : ", 1995; Zhang, 2008), its variants (Claassen and Heskes, 2011; Colombo et al., 2012), and BCCD (Claassen and Heskes, 2012), would require such an extension to m-separation.",
      "startOffset" : 35,
      "endOffset" : 84
    }, {
      "referenceID" : 7,
      "context" : ", 1995; Zhang, 2008), its variants (Claassen and Heskes, 2011; Colombo et al., 2012), and BCCD (Claassen and Heskes, 2012), would require such an extension to m-separation.",
      "startOffset" : 35,
      "endOffset" : 84
    }, {
      "referenceID" : 6,
      "context" : ", 2012), and BCCD (Claassen and Heskes, 2012), would require such an extension to m-separation.",
      "startOffset" : 18,
      "endOffset" : 45
    }, {
      "referenceID" : 10,
      "context" : "Model-level cycles typically result from temporal processes for which grounding across time would yield an acyclic ground graph, such as in dynamic Bayesian networks (Dean and Kanazawa, 1989; Murphy, 2002).",
      "startOffset" : 166,
      "endOffset" : 205
    }, {
      "referenceID" : 40,
      "context" : "Model-level cycles typically result from temporal processes for which grounding across time would yield an acyclic ground graph, such as in dynamic Bayesian networks (Dean and Kanazawa, 1989; Murphy, 2002).",
      "startOffset" : 166,
      "endOffset" : 205
    }, {
      "referenceID" : 1,
      "context" : "Many researchers have developed methods for learning and inference by explicitly modeling unobserved variables—typically termed latent variable models (Bishop, 1999)—or inferring the presence of latent entity classes—for example, latent group models (Neville and Jensen, 2005). However, only ancestral graphs and acyclic directed mixed graphs (ADMGs) do so in order to preserve an underlying conditional independence structure (Richardson and Spirtes, 2002; Richardson, 2009). These models are paired with the theory of m-separation, which is a generalization of d -separation for Bayesian networks. The generalization of ancestral graphs or ADMGs to relational models requires extensive theoretical exploration; therefore, we leave this as an important direction for future work. Given that a primary motivation for d -separation is to support constraint-based causal discovery, any relational extension to algorithms that learn causal models without assuming causal sufficiency, such as FCI (Spirtes et al., 1995; Zhang, 2008), its variants (Claassen and Heskes, 2011; Colombo et al., 2012), and BCCD (Claassen and Heskes, 2012), would require such an extension to m-separation. Temporal and cyclic models: Currently, the relational model is assumed to be acyclic (with respect to the class dependency graph), and consequently, atemporal. Model-level cycles typically result from temporal processes for which grounding across time would yield an acyclic ground graph, such as in dynamic Bayesian networks (Dean and Kanazawa, 1989; Murphy, 2002). However, cycles can also be due to temporal processes where the interaction occurs at a faster rate than measurement. As a result, there has been considerable attention devoted to models that explicitly encode cyclic dependencies, such as the work by Spirtes (1995), Pearl and Dechter (1996), Richardson (1996), Dash (2005), Schmidt and Murphy (2009), and Hyttinen et al.",
      "startOffset" : 152,
      "endOffset" : 1814
    }, {
      "referenceID" : 1,
      "context" : "Many researchers have developed methods for learning and inference by explicitly modeling unobserved variables—typically termed latent variable models (Bishop, 1999)—or inferring the presence of latent entity classes—for example, latent group models (Neville and Jensen, 2005). However, only ancestral graphs and acyclic directed mixed graphs (ADMGs) do so in order to preserve an underlying conditional independence structure (Richardson and Spirtes, 2002; Richardson, 2009). These models are paired with the theory of m-separation, which is a generalization of d -separation for Bayesian networks. The generalization of ancestral graphs or ADMGs to relational models requires extensive theoretical exploration; therefore, we leave this as an important direction for future work. Given that a primary motivation for d -separation is to support constraint-based causal discovery, any relational extension to algorithms that learn causal models without assuming causal sufficiency, such as FCI (Spirtes et al., 1995; Zhang, 2008), its variants (Claassen and Heskes, 2011; Colombo et al., 2012), and BCCD (Claassen and Heskes, 2012), would require such an extension to m-separation. Temporal and cyclic models: Currently, the relational model is assumed to be acyclic (with respect to the class dependency graph), and consequently, atemporal. Model-level cycles typically result from temporal processes for which grounding across time would yield an acyclic ground graph, such as in dynamic Bayesian networks (Dean and Kanazawa, 1989; Murphy, 2002). However, cycles can also be due to temporal processes where the interaction occurs at a faster rate than measurement. As a result, there has been considerable attention devoted to models that explicitly encode cyclic dependencies, such as the work by Spirtes (1995), Pearl and Dechter (1996), Richardson (1996), Dash (2005), Schmidt and Murphy (2009), and Hyttinen et al.",
      "startOffset" : 152,
      "endOffset" : 1840
    }, {
      "referenceID" : 1,
      "context" : "Many researchers have developed methods for learning and inference by explicitly modeling unobserved variables—typically termed latent variable models (Bishop, 1999)—or inferring the presence of latent entity classes—for example, latent group models (Neville and Jensen, 2005). However, only ancestral graphs and acyclic directed mixed graphs (ADMGs) do so in order to preserve an underlying conditional independence structure (Richardson and Spirtes, 2002; Richardson, 2009). These models are paired with the theory of m-separation, which is a generalization of d -separation for Bayesian networks. The generalization of ancestral graphs or ADMGs to relational models requires extensive theoretical exploration; therefore, we leave this as an important direction for future work. Given that a primary motivation for d -separation is to support constraint-based causal discovery, any relational extension to algorithms that learn causal models without assuming causal sufficiency, such as FCI (Spirtes et al., 1995; Zhang, 2008), its variants (Claassen and Heskes, 2011; Colombo et al., 2012), and BCCD (Claassen and Heskes, 2012), would require such an extension to m-separation. Temporal and cyclic models: Currently, the relational model is assumed to be acyclic (with respect to the class dependency graph), and consequently, atemporal. Model-level cycles typically result from temporal processes for which grounding across time would yield an acyclic ground graph, such as in dynamic Bayesian networks (Dean and Kanazawa, 1989; Murphy, 2002). However, cycles can also be due to temporal processes where the interaction occurs at a faster rate than measurement. As a result, there has been considerable attention devoted to models that explicitly encode cyclic dependencies, such as the work by Spirtes (1995), Pearl and Dechter (1996), Richardson (1996), Dash (2005), Schmidt and Murphy (2009), and Hyttinen et al.",
      "startOffset" : 152,
      "endOffset" : 1859
    }, {
      "referenceID" : 1,
      "context" : "Many researchers have developed methods for learning and inference by explicitly modeling unobserved variables—typically termed latent variable models (Bishop, 1999)—or inferring the presence of latent entity classes—for example, latent group models (Neville and Jensen, 2005). However, only ancestral graphs and acyclic directed mixed graphs (ADMGs) do so in order to preserve an underlying conditional independence structure (Richardson and Spirtes, 2002; Richardson, 2009). These models are paired with the theory of m-separation, which is a generalization of d -separation for Bayesian networks. The generalization of ancestral graphs or ADMGs to relational models requires extensive theoretical exploration; therefore, we leave this as an important direction for future work. Given that a primary motivation for d -separation is to support constraint-based causal discovery, any relational extension to algorithms that learn causal models without assuming causal sufficiency, such as FCI (Spirtes et al., 1995; Zhang, 2008), its variants (Claassen and Heskes, 2011; Colombo et al., 2012), and BCCD (Claassen and Heskes, 2012), would require such an extension to m-separation. Temporal and cyclic models: Currently, the relational model is assumed to be acyclic (with respect to the class dependency graph), and consequently, atemporal. Model-level cycles typically result from temporal processes for which grounding across time would yield an acyclic ground graph, such as in dynamic Bayesian networks (Dean and Kanazawa, 1989; Murphy, 2002). However, cycles can also be due to temporal processes where the interaction occurs at a faster rate than measurement. As a result, there has been considerable attention devoted to models that explicitly encode cyclic dependencies, such as the work by Spirtes (1995), Pearl and Dechter (1996), Richardson (1996), Dash (2005), Schmidt and Murphy (2009), and Hyttinen et al.",
      "startOffset" : 152,
      "endOffset" : 1872
    }, {
      "referenceID" : 1,
      "context" : "Many researchers have developed methods for learning and inference by explicitly modeling unobserved variables—typically termed latent variable models (Bishop, 1999)—or inferring the presence of latent entity classes—for example, latent group models (Neville and Jensen, 2005). However, only ancestral graphs and acyclic directed mixed graphs (ADMGs) do so in order to preserve an underlying conditional independence structure (Richardson and Spirtes, 2002; Richardson, 2009). These models are paired with the theory of m-separation, which is a generalization of d -separation for Bayesian networks. The generalization of ancestral graphs or ADMGs to relational models requires extensive theoretical exploration; therefore, we leave this as an important direction for future work. Given that a primary motivation for d -separation is to support constraint-based causal discovery, any relational extension to algorithms that learn causal models without assuming causal sufficiency, such as FCI (Spirtes et al., 1995; Zhang, 2008), its variants (Claassen and Heskes, 2011; Colombo et al., 2012), and BCCD (Claassen and Heskes, 2012), would require such an extension to m-separation. Temporal and cyclic models: Currently, the relational model is assumed to be acyclic (with respect to the class dependency graph), and consequently, atemporal. Model-level cycles typically result from temporal processes for which grounding across time would yield an acyclic ground graph, such as in dynamic Bayesian networks (Dean and Kanazawa, 1989; Murphy, 2002). However, cycles can also be due to temporal processes where the interaction occurs at a faster rate than measurement. As a result, there has been considerable attention devoted to models that explicitly encode cyclic dependencies, such as the work by Spirtes (1995), Pearl and Dechter (1996), Richardson (1996), Dash (2005), Schmidt and Murphy (2009), and Hyttinen et al.",
      "startOffset" : 152,
      "endOffset" : 1899
    }, {
      "referenceID" : 1,
      "context" : "Many researchers have developed methods for learning and inference by explicitly modeling unobserved variables—typically termed latent variable models (Bishop, 1999)—or inferring the presence of latent entity classes—for example, latent group models (Neville and Jensen, 2005). However, only ancestral graphs and acyclic directed mixed graphs (ADMGs) do so in order to preserve an underlying conditional independence structure (Richardson and Spirtes, 2002; Richardson, 2009). These models are paired with the theory of m-separation, which is a generalization of d -separation for Bayesian networks. The generalization of ancestral graphs or ADMGs to relational models requires extensive theoretical exploration; therefore, we leave this as an important direction for future work. Given that a primary motivation for d -separation is to support constraint-based causal discovery, any relational extension to algorithms that learn causal models without assuming causal sufficiency, such as FCI (Spirtes et al., 1995; Zhang, 2008), its variants (Claassen and Heskes, 2011; Colombo et al., 2012), and BCCD (Claassen and Heskes, 2012), would require such an extension to m-separation. Temporal and cyclic models: Currently, the relational model is assumed to be acyclic (with respect to the class dependency graph), and consequently, atemporal. Model-level cycles typically result from temporal processes for which grounding across time would yield an acyclic ground graph, such as in dynamic Bayesian networks (Dean and Kanazawa, 1989; Murphy, 2002). However, cycles can also be due to temporal processes where the interaction occurs at a faster rate than measurement. As a result, there has been considerable attention devoted to models that explicitly encode cyclic dependencies, such as the work by Spirtes (1995), Pearl and Dechter (1996), Richardson (1996), Dash (2005), Schmidt and Murphy (2009), and Hyttinen et al. (2012). Our formalism currently prohibits any relational dependency that has a common attribute class for the cause and effect, regardless of the relational path constraint.",
      "startOffset" : 152,
      "endOffset" : 1927
    }, {
      "referenceID" : 36,
      "context" : "The abstract ground graph representation also presents an opportunity to derive new edge orientation rules for algorithms that learn the structure of relational models, such as RPC (Maier et al., 2010) and RCD (Maier et al.",
      "startOffset" : 181,
      "endOffset" : 201
    }, {
      "referenceID" : 37,
      "context" : ", 2010) and RCD (Maier et al., 2013).",
      "startOffset" : 16,
      "endOffset" : 36
    }, {
      "referenceID" : 37,
      "context" : "In prior work, we formalized this idea as a new rule, called relational bivariate orientation (RBO) (Maier et al., 2013), to orient dependencies in a constraint-based causal discovery algorithm.",
      "startOffset" : 100,
      "endOffset" : 120
    }, {
      "referenceID" : 45,
      "context" : "A Bayesian network is a widely used probabilistic graphical model of propositional data (Pearl, 1988).",
      "startOffset" : 88,
      "endOffset" : 101
    }, {
      "referenceID" : 76,
      "context" : "Fortunately, d -separation, a set of graphical criteria, provides the foundation for algorithmic derivation of all conditional independencies in G and entails the exact same set of conditional independencies as the Markov condition (Verma and Pearl, 1988; Geiger and Pearl, 1988; Neapolitan, 2004).",
      "startOffset" : 232,
      "endOffset" : 297
    }, {
      "referenceID" : 15,
      "context" : "Fortunately, d -separation, a set of graphical criteria, provides the foundation for algorithmic derivation of all conditional independencies in G and entails the exact same set of conditional independencies as the Markov condition (Verma and Pearl, 1988; Geiger and Pearl, 1988; Neapolitan, 2004).",
      "startOffset" : 232,
      "endOffset" : 297
    }, {
      "referenceID" : 41,
      "context" : "Fortunately, d -separation, a set of graphical criteria, provides the foundation for algorithmic derivation of all conditional independencies in G and entails the exact same set of conditional independencies as the Markov condition (Verma and Pearl, 1988; Geiger and Pearl, 1988; Neapolitan, 2004).",
      "startOffset" : 232,
      "endOffset" : 297
    }, {
      "referenceID" : 44,
      "context" : "If X → Y is an edge in the causal model G, then manipulating or changing the value of X will alter the conditional distribution of Y— denoted as P ( Y | do(X) ) using Pearl’s do-calculus notation for interventions (Pearl, 2000).",
      "startOffset" : 214,
      "endOffset" : 227
    }, {
      "referenceID" : 16,
      "context" : "However, Geiger et al. (1990) provide a linear-time algorithm based on breadth-first search and reachability on G.",
      "startOffset" : 9,
      "endOffset" : 30
    }, {
      "referenceID" : 67,
      "context" : "These algorithms assume causal sufficiency, faithfulness, and model acyclicity to identify the edges inG that are consistent with observed conditional independencies and to determine the direction of causality (Spirtes et al., 2000).",
      "startOffset" : 210,
      "endOffset" : 232
    }, {
      "referenceID" : 34,
      "context" : "This technique of translating a relational database down to a single, propositional representation is often referred to as propositionalization (Kramer et al., 2001).",
      "startOffset" : 144,
      "endOffset" : 165
    }, {
      "referenceID" : 74,
      "context" : "We first applied lasso for feature selection (Tibshirani, 1996) to minimize the number of predictors while maximizing model fit.",
      "startOffset" : 45,
      "endOffset" : 63
    }, {
      "referenceID" : 17,
      "context" : "We also standardized the input variables by dividing by two standard deviations, as recommended by Gelman (2008). Since the predictor for the number of dependencies is log-transformed, the standardization for that variable occurs after taking the logarithm.",
      "startOffset" : 99,
      "endOffset" : 113
    }, {
      "referenceID" : 74,
      "context" : "We first applied lasso for feature selection (Tibshirani, 1996) to minimize the number of predictors while maximizing",
      "startOffset" : 45,
      "endOffset" : 63
    }, {
      "referenceID" : 17,
      "context" : "We also standardized the input variables by dividing by two standard deviations, as recommended by Gelman (2008). Since the predictor for the number of dependencies is log-transformed, the standardization for that variable occurs after taking the logarithm.",
      "startOffset" : 99,
      "endOffset" : 113
    } ],
    "year" : 2014,
    "abstractText" : "We extend the theory of d -separation to cases in which data instances are not independent and identically distributed. We show that applying the rules of d -separation directly to the structure of probabilistic models of relational data inaccurately infers conditional independence. We introduce relational d-separation, a theory for deriving conditional independence facts from relational models. We provide a new representation, the abstract ground graph, that enables a sound, complete, and computationally efficient method for answering d -separation queries about relational models, and we present empirical results that demonstrate effectiveness.",
    "creator" : "LaTeX with hyperref package"
  }
}