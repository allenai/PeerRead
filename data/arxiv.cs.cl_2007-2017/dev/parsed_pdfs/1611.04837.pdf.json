{
  "name" : "1611.04837.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "LOST IN SPACE: GEOLOCATION IN EVENT DATA",
    "authors" : [ "SOPHIE J. LEE" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keyword: Natural Language Processing, Information Extraction, Text Analysis, Supervised Machine Learning, Geolocation, Event Data\n1. INTRODUCTION Many quantitative studies of conflict rely on event data. Recently, these studies have also re-\ntreated from the country-year framework and have focused on disaggregating the event flows both in terms of space and time. Disaggregating temporality—even to the daily level—is a straightforward task. But figuring out precisely where an event actually occurred is a difficult and uncertain task that has been perplexing formost contemporary event data efforts (Boschee et al. 2015), PHOENIX (OEDA 2016), SCAD (Salehyan 2015), ACLED (Raleigh et al. 2010). At the same time there is widespread interest in disentangling investigations from the countryyear framework. The country-year—as an observational framework—has a longstanding tradition in political science. Indeed two of the three most cited articles in the American Political Science Review focus on the country-year (Beck & Katz 1995; Fearon & Laitin 2003). As recently as 2009, collections have focused on disaggregation of the country-year in conflict studies (Cederman & Gleditsch 2009). More broadly, many focus on hierarchical approaches that simultaneously include subnational, national, and even international aspects. Efforts at the World Bank\nDate: November 16, 2016. We appreciate the comments of Kyle Beardsley, Andrew Hall, Jan Kleinnijenhuis, Sayan Mukherjee, Jonathan Nagler, Molly Roberts, Colin Rundel, David Siegel, Brandon Stewart, Joshua Tucker, members of Wardlab at Duke University, and the SMaPP lab at New York University. This research was partially supported by the National Science Foundation Award 1259266. Corresponding Authors: ophie J. Lee and Howard Liu.\nar X\niv :1\n61 1.\n04 83\n7v 1\n[ cs\n.C L\n] 1\n4 N\nov 2\n01 6\nand other international organizations (Frank & Martinez-Vazquez 2014; Easton et al. 2011) have emphasized this deeper dive into the political and economic landscape. Much of this deeper dive is coming from organizations and governments in terms of their reporting on demographic, economic, financial, and health data that are subnational. Most data in the conflict realm comes from non-offical sources. For many that means some form of data collected from historical and journalistic sources. This need is often filled by event data, which are typically collected on a daily basis, and can be aggregated temporally to the level required by the analysis. Event data can also be aggregated to the geographical region that is appropriate. Given the increasing demands for event data, the scientific community has recently devoted significant efforts to automate the data collection process. Having humans read and code a large set of archive documents sometimes limits reproducibility, and hence hinders scientific research. It is also expensive and limits the currency of the data. Further, ensuring inter-coder reliability is challenging, especially over global events that span decades. Several efforts utilize machine-coding to collect event information and determine event features automatically. Projects such as the Integrated Crisis EarlyWarning System (ICEWS) (Boschee et al. 2015; Lautenschlager et al. 2015; O’Brien 2010) and the Open Source Event Data Alliance 2016 are two prominent examples. A good overview on event data in political science is found in Schrodt & Yonamine (2013). These automated event data allow researchers to observe and extract information on politically relevant events around the world in near real-time. Despite the apparent advantages of automated data collection, the machine-coding ontologies for event data require further research (Grimmer & Stewart 2013; Lucas et al. 2015). NSF currently sponsors a multidisciplinary project to look into formulating the generation of event data. 1 At the same time, IARPA is reportedly looking to fund an event data challenge that could lead to new ways of collecting and analyzing event data. This attention by funding agencies illustrates that not all issues in this research domain are yet resolved. Outstanding issues include machine translation of texts in foreign languages (wherein great progress is being made in both Chinese and Arabic), duplicate reports from multiple sources, and the relatively low accuracy in determining the event location (D’Orazio et al. 2014; Schrodt 2015). While all of these are important, in this article, we focus on the sole issue of geolocation in event data. For human coders, locating events by reading a news article may be time-intensive, but straightforward. This is not the case for machine-coding: many news articles contain multiple location names, such as the location of the journalist writing the story, the birthplace of a person being interviewed, or the place of a similar event that occurred several decades ago; at times, human names are identical to geographic names; and location names are transliterated into English in a variety of potentially confusing ways. All these sources of noise in the data increase the difficulty in automatically locating events. A good algorithm should read texts like human coders and code only the correct event locations. We treat the geolocation task as a classification problem where each location word is predicted to be correct or incorrect. With the goal of developing an algorithm to discern correct event locations automatically, we extract the contextual information of location words (the Ngram patterns for location words, the frequency of mention, and the context of the sentences\n1Modernizing Political Event Data for Big Data Social Science Research, Patrick T. Brandt (PI, EPPS), Vito D’Orazio (Senior Personnel, EPPS), Jennifer S. Holmes (Co-PI, EPPS), Latifur R. Khan (Co-PI, ECS), Vincent Ng (Co-PI, ECS), National Science Foundation, RIDIR, $1,497,358, September 2015—August 2018.\ncontaining location words) from the training set. We check the accuracy against a hand-coded set of ground truth data on locations. To do so, estimated parameters from the training set were used to predict the event locations in the test set (for which we also know the ground truth), using three classification methods: artificial neural networks with back propagation, support vector machine (SVM), and random forests. Our supervised machine learning language model codes locations correctly at an accuracy rate close to 90%when the texts contain a single correct location per article.2 Our approach is fully automated, but does require some hand coding of a small number of stories to contextualize the coders for specific countries. While the process described in this article is generalizable, we selected data from ICEWS (Boschee et al. 2015) and OEDA.3 We began with an investigation of 250 protests in China (CAMEO code 14: protest), but to anneal the generalizability of our approach, we added (and coded) 250 violent events in the Democratic Republic of the DRC (CAMEO code 19: fight) drawn from the ICEWS data, and 250 violent events drawn from the PHEONIX data on Syria (OEDA, CAMEO code 19: fight).4 Each of these cases presents difficult problems for automated geolocation.\n2. AUTOMATED GEOLOCATION The task of determining event locations involves three steps, each non-trivial. In the first step, known as named entity recognition in Computer Science and Computational Linguistics (D’Orazio et al. 2014; Cardie & Wilkerson 2008; Guerini et al. 2008; Arguello et al. 2008; Nadeau & Sekine 2007), all location names are identified and extracted from an appropriately preprocessed text. This step is a prerequisite for the other steps because to determine the location of an event in a news article, capturing the exhaustive list of location names is required. Next comes entity disambiguation/resolution, which involves identifying the actual location of the recognized name string(Cucerzan 2007; D’Orazio et al. 2014; Bunescu & Pasca 2006). Once this is accomplished, it is possible to extract the ontologically defined meaning from the text in terms of who does what to whom, and when and where via CAMEO,5 PETRARCH,6 or some other coding framework (Schrodt 2006). Lastly, the disambiguated location names are evaluated to determine whether they represent the event-occurring location. While this step requires the completion of the two preceding steps, the enormous, extant body of work that has built up over the past decades of machine-coding of events has made the first two steps more manageable. Named entity recognition in the context of geolocation involves determining which words in the given sentences are location names. In principle, the task of capturing location names from texts can be done easily by using a dictionary. In practice, however, developing a dictionary that is sufficiently comprehensive for such a task may be challenging. To begin, the geographic boundary of the texts being analyzedmay be unclear, given that the domain of many event data 2Upon publication a repository of our project will be available at: texttthttps:github.com/(author ID hidden)/LostInSpace. 3For the OEDA project, which is still in early development, see: http:phoenixdata.org. 4While there are twenty action verbs in the CAMEO ontology, verbs such as “appeal,” “consult,” or “yield” do not yield as many events in the data as “protest,” “assault,” or “fight” do. Also, the data of such events are not in the settings of interstate or intrastate conflicts. Hence we selected “protest” and “fight” data as test cases. Our method, however, is applicable to all the other CAMEO event types. 5Formore information on CAMEOontology, please see: http:eventdata.parusanalytics.com/data.dir/cameo.html. 6See https:github.com/openeventdata/petrarch.\nis the entire world. Further, because conflict events often spread to new and rural places, texts may include location names not defined in the gazetteer. Still further, a location name may be written in multiple forms, requiring the dictionary to comprise every variant for each location. This commonly occurs when a foreign location name is transliterated into another language such as English. For instance, the transliterations dei ez-zor, Deir-al-Zour, Dayr al-Zawr, and dei ezZour all refer to the same province in Syria. Without specific dictionaries and correspondences, this is difficult to determine automatically. Further complicating matters, news articles often use nearby landmarks to indicate the location, in lieu of using the official names. The 2014 Ukrainian revolution, for example, was often described as having taken place at Mariinsky Park, rather than in Kiev. The same is true for the so-called Martyr Square (Tahrir Square) and its role as a site of protests during the Egyptian revolution of 2011. We encountered this problem in our data set as well. For example, “U.N. attack helicopters whirred overhead as armoured personnel carriers ploughed through forests in Virunga National Park [a park in North Kivu province] in Democratic Republic of Congo . . . ”.7 The dictionary approach can be complemented by the part-of-speech (POS) tagging method that grammatically parses sentences in the text and classifies each word into various categories such as persons, organizations, and location names. The technique can identify landmarks (for example, Mriinsky Park, Martir Square, or Virunda National Park) as locations even if they were not pre-defined in the dictionary. Currently, there are a number of open source systems available for named entity recognition. Typical software programs for this task include the Stanford Named Entity Recognizer (part of Stanford NLP), Apache Open NLP algorithm, and MIT Information Extraction (MITIE), developed by MIT’s Lincoln Laboratory. The parsing stream for the OEDA pipeline, for example, combines POS tagging and the dictionary approach. As shown in Figure 1, in OEDA the parser calls the Stanford CoreNLP, which returns to the Mongo database parsed sentences with parts of speech tagged. These parsed stories then are coded via the PETRARCH ontology. Only then is geolocation undertaken, using calls to the CLIFF geolocation software. While this method extracts an extensive list of location words, coding all country-relevant location words as the correct event location introduces a different problem. Entity disambiguation is a nascent field of research aiming at determining the true location of the referent location word. (Han et al. 2011; Rao et al. 2013; Bunescu & Pasca 2006; Cucerzan 2007). For instance, “Durham” in the sentence,“the group moved to the intersection of Duke and Chapel Hill streets near the Durham Police Department headquarters” ( Jul. 21st, 2016. CBS News Carolina), would most likely to refer to a city in North Carolina, U.S., while the same word in the sentence “Paul Collingwood commits for another year to Durham” ( Jul. 26th, 2016. AFP) would most likely to refer to a city in England. Different approaches exist for assigning location name strings to the referent location words. The co-occurences of location names, i.e. which location words frequently appear together in the corpus, could be modeled and used to link the name strings and the true locations (Han et al. 2011). Similarly, theMordecai algorithm links the extracted location names to the locations defined in a gazetteer, by adopting Word2vec model (Mikolov et al. 2013) that calculates the co-occurrences of the words and quantifies the contexts in which specific words appear.8 A well-trained corpus archive should be able to show words such as “Duke” and “Chapel Hill”\n7ICEWS story ID: 4590482, DRC data 8Mordecai is described at https:github.com/openeventdata/mordecai.\nappear commonly with “Durham” when “Durham” is the city name in North Carolina. While the disambiguation task is not simple, well-defined dictionaries may suffice these techniques when processing news articles that have clearly defined country bounds. But for projects that contain new and unknown sub-national location names, building an extensive dictionary is a daunting task, especially if the locales are not named in English. Finally, geolocating events (identifying the location of the event described in a document) is an objective for many scholars, particularly those who intend to collect and build original databases from text corpora, be they news articles, congressional records, campaign speeches, party constitutions, or twitter feeds. While automating this task will aid many, the research avenue in this topic is still under development. One of the most commonly used methods is building location dictionaries and capturing location names. For instance, the principal investigators of the Project Civil Strife (PCS) data used three location dictionaries—“cities,” “regions/provinces,” and “others”—and coded all captured location names as the place of event (Shellman 2008). This approach, however, also includes irrelevant places as event locations. In our China data, for example, a total of 614 location words were captured from 250 news articles but only half of them (314 correct location words) are actual event locations. Given that a substantial number of location words are incorrect event locations, the automated event data community needs a better coding scheme that can reduce the error rates. Many have noted that this problem is yet to be solved. We discuss the remaining challenges in detail in the ensuing section.\n3. CHALLENGES IN GEOLOCATING EVENTS Selecting the correct location word among all captured locations is a difficult problem. In the data we examined, nine out of ten news articles contain multiple locations. Some of these are specious locations, indicating for example the location of news agencies, the location of a similar, often previous event, or the current location of a reporter. The occurrence of multiple location names not only introduces noise into the data, but also escalates the difficulty in automating the task of geolocation. Noting this difficulty, Schrodt & Yonamine (2012) states that “the main challenge is to empirically determine which place name should be assigned to the specific event, especially when multiple events and location names occur in a single article” (page 19). Multiple approaches have been devised. One approach, adopted by ICEWS, is to code the location name that is the nearest to the action verb (identified by the TABARI coder9) in the text. Under such a scheme, a location word that is distant from the action verb is automatically discarded. Although the rationale for the algorithm sounds intuitive, errors frequently occur because action verbs are not always adjacent to the names of the event location. The current OEDA data deployed uses a java-based web service named CLIFF.10 This approach selects the most likely place as the “focus” location of the article, based on the frequencies of mentions and the order of appearance (D’Ignazio et al. 2014). Notably, both ICEWS and OEDA assume that one correct event location exists per article. Yet, that assumption does not always hold. Over 30% of Syrian stories we examined contain multiple true locations, and in China and the DRC data, these numbers are about one-third to one-half this amount. For the other 175 countries in the world, these ratios, to our knowledge, are not yet known, but we can assume that the ratio is greater than zero. As the article in Table 4 demonstrates, single stories frequently contain multiple true location names. By assuming that a single location word exists per article as “the most appropriate location” (Lautenschlager et al. 2016), such a coding rule misses many true event locations, thereby hindering the accuracy of the coded location names. Moreover, a researcher who analyzes an event data under these approaches maymisinterpret the data, for example, concluding that protests in China are concentrated in the capital, Beijing. However, even when there is a single location, the OEDA and ICEWS geolocation still will make many mistakes. On the other hand, coding all location names in a text as the correct locations, as the PCS project has done, reduces false negatives but increases false positives. Hence, the optimal approach would determine which set of location words in the article is more likely to be the correct ones, in addition to relaxing the assumption that only a single location word represents each news article.\n4. CATEGORIZING MULTIPLE LOCATIONS In examining articles with multiple locations, we observed four mutually exclusive types of location words: 1) event-relevant and event-occurring, 2) event-irrelevant and event-occurring, 3) event-relevant and non-event-occurring, and 4) event-irrelevant and non-event-occurring. We define event-relevant locations as those locations that are part of the main description of the 9For the TABARI coder see: http:eventdata.parusanalytics.com/software.dir/tabari.html. 10CLIFF documentation: https:github.com/openeventdata/phoenix_pipeline and http:cliff.mediameter.org\nEg. 2 1.. . . The government denounces a named refugee camp near Goma that was attacked by M23 soldiers. . . 6. Prime Minister Ponyo addressing an opening session of a seminar on agricultural sector in Kinshasa today.. . .\nevent of interest, i.e. all locations that are key to the narrative of the event of interest. Eventoccuring locations refer to all locations where events occurred regardless of whether the event is the event of interest. Thus, the first category, event-relevant and event-occurring, refers to the locations where events occurred while the occurred events are within the scope of interest. We aim to detect this type of location words as the correct ones. Regarding the second category, a small portion of articles contain event-irrelevant and eventoccurring location words in our data. Such could occur when the raw texts contain news summaries of events that are not of interest. Table 1 shows examples of articles that contain both event-irrelevant and event-occurring (Idlib and Kinshasa) as well as the event-relevant and event-occurring (Aleppo and Goma) location words. The first example is from an article that describes a rebel attack event involving 15 civilian causalities and then describes a ceasefire agreement, an event not of interest. Sometimes, news articles contain summaries of completely unrelated events as in the second article, which consists of six reports that summarize events that occurred in Syria on that day. The third type is event-irrelevant and non-event-occurring locations. Some of the most commonly observed event-irrelevant and non-event-occurring location names refer to the location of the news agencies and spokespersons. For example, Beijing in the first article in Table 2 indicates where the story was being written, but coded as the actual event location in ICEWS. We suspect that this is because the location word that refers to the reporting location is the closest location name from the action verb (strike), and hence was mistakenly coded as the event location. The true event location in this article is Guangdong. Reporting locations often appear in the first line of the article. Discarding the reporting locations can alleviate the problem to a certain extent, but the problem persists because reporting locations are frequently embedded in the middle of texts, as demonstrated in the second and the third articles in Table 2. Also, the event-irrelevant and non-event-occurring location words are embedded for other reasons, such as referring to the birthplace of someone being interviewed, as in: “ ‘we can mix in any society,’ said Amar Aldoura from Damascus.”11\n11OEDA story ID: 1424875 v0.2.0\nThe event-relevant and non-event-occurring locations complicate matters even more. Journalists often provide the background of the event being described. They may recite locations of the stronghold of a rebel group, the province name to which victims fled, or the place where the perpetrators of incidents are being trialed. The articles in Table 3 are examples of stories containing both event-relevant but non-event-occurring (Orientale and Damascus) and eventrelevant and event-occurring locations (North Kivu and Daraa). In the first article, Orientale is a place to which the rebel leader was heading, so the location word is mentioned as part of the description of the rebel attack. But the actual attack was in North Kivu. The writer of the second article mentioned Damascus to describe a goal that the rebel group wishes to achieve. The actual attack was in Daraa. To tackle the issue of creating the exhaustive list of location words that should be considered, we combine existing named-entity recognition, POS tagging, and entity resolution (matching location strings referenced in a gazetteer) techniques. Furthermore, based on the assessment of the types of multiple locations, we have come to the conclusion that each location word should be evaluated and determined whether it is an event-relevant and event-occuring location. For determining the boolean status (true event location or not) of each captured location word in the exhaustive list, we adopt a classification approach, which we discuss more in the next section. A sophisticated algorithm would distinguish the correct locations from the incorrect ones by filtering out the event-irrelevant locations, as well as non-event-occurring ones.\nEg. 2 The clashes, which started at Luozi and to Seke Banza [towns in Bas-Congo Province]. . . Speaking to reporters in Kinshasa, the parliamentarian said that the clashes had left at least 100 people dead andmanymore nursing serious injuries since last Friday.\nEg. 3 The protesters gathered outside the office of Southern Weekly in Guangzhou, capital of southern Guangdong province, on Monday calling for media freedom, a taboo subject in the country, holding banners and chanting slogans.. . . A foreign ministry spokesperson in Beijing is reported to have said: “There is no so-called news censorship in China.”\nEg. 2 Elsewhere in Syria, 51 rebel factions operating in the southern province of Daraa announced a campaign to wrest control of areas of Daraa city [capital of Daraa governorate] from the government.. . . SANA reported that an attack by terrorists had been thwarted, with fighter jets pounding rebel targets in surrounding villages. If successful, it would grant the rebels a rear supply base to mount operations on Damascus. . .\n5. CLASSIFICATION ALGORITHMS For classifying location words either as correct or incorrect, various machine learning techniques could be used, such as the following: Neural Networks (Müller & Reinhardt 2012; Mehrotra et al. 1997; Cheng & Titterington 1994),12 SVM (Cristianini & Shawe-Taylor 2000; Vapnik 1995; Cortes & Vapnik 1995),13 random forests (Liaw & Wiener 2002; Breiman 2001),14 AdaBoost (Freund & Schapire 1997), K-nearest neighbors (K-NN) (Dasarthy 1990), and naive Bayes (Zhang 2004; Murphy 2006). Of these, we employ three classifiers—artificial neural networks, SVM, random forests—and compare the performance of each. The artificial neural network models the relationship between a set of input signals, the desired feature from the texts, and an output signal—whether a location word refers to the event location—using concepts borrowed from our understanding of how a human brain processes information from sensory dendrites through neurons while allowing the impulse to be weighted according to its relative importance (model parameters). Although the algorithm is notoriously slow, the artificial neural networks have more flexibility in terms of structures and parameters compared to other classifiers (Lantz 2013; Zurada 1992). But a potential downside of neural network model is that its prediction performances usually relies on a considerable amount of training data. SVM refers to support vector machines. These were initially introduced for solving two-group classification problems where the data are mapped into a higher dimensional input space and construct an optimal separating hyperplane (Vapnik 1998; 1995). This approach is often viewed as superior to other machine learning algorithms, including neural networks, because the quadratic programming guarantees reaching the global optimum, which often leads to the larger\n12See Zhang & Zhou (2006); Ng et al. (1997) for examples of neural networks applications in text analysis. 13Examples of SVM applications in text analysis can be found inMinhas et al. (2015); Tong & Koller (2001); T. Joachims (1998). 14See Fette et al. (2007) for examples.\noverall classification accuracy (Li 2003; Vapnik 1998; T. Joachims 1998; Maroco et al. 2011). Furthermore, SVMmodels are typically less prone to over-fitting (Mukherjee et al. 1997). SVMs also provide a computationally efficient way to achieve a reasonably accurate model (Amami et al. 2012; Li 2003). Finally, the random forests (or decision tree forest) model, championed by Leo Breiman (2001) and Adele Cutler (Breiman & Cutler 2007), combines the principle of bagging with random feature selection to add complexity to the decision tree models. After the ensemble of classification regression trees (hence the name forest) is generated, the model combines these trees’ predictions. Because the ensemble uses only a small, random portion of the full feature set, the model can handle large data sets wherein the high dimensionality may cause other models to fail. Despite the difficulty in interpreting the results, it is an all-purpose approach that performs well on most problems (Lantz 2013). These classifiers boast two primary strengths. The first is that they do not require prespecifying the type of relationship between the covariates and the response variable. They are powerful information extraction tools that can capture underlying relationships not explained by known structures (Jones & Linder 2015; Lantz 2013; Günther & Fritsch 1998; Beck et al. 2000). Second, these models achieve accurate prediction rates given large enough input sizes (Maroco et al. 2011; Hsieh et al. 2011; Beck et al. 2000). Lantz (2013) suggest that these algorithms are the most accurate state-of- the-art approaches, and make few assumptions about the data. Some scholars oppose the use of the machine learning in fields that require substantive interpretations of the parameters (de Marchi et al. 2004) because they are “difficult to interpret” (Lantz 2013). Despite such pitfalls, the prediction performance of these models make them attractive for geolocation. Whether they produce interpretable results can not be determined a priori.\n6. BUILDING DICTIONARIES Before classification can begin, correctly formatted text data with desirable features is required. We developed four types of dictionaries in order to preprocess the text data. First and foremost, a location dictionary for each country was compiled. The initial location lists were imported from Geonames, Wikipedia, and Google map. These dictionaries contained province names (standardized province and governorate names) and sub-province names (city, village and town names, spelling variations of both province and sub-province names, and frequently used famous location names) as two separate columns. To ensure that our location dictionary was as comprehensive as possible, we used an iterative process to build it. After the initial location dictionary was built, we went back to the text data and parsed sentences using MITIE. The parts of speech elements classified as location words were then sent to the Genomes API and the returned entity pairs that were not already in, but should have been in the dictionary, were added. We also developed dictionaries for actors. While we imported the actor lists from ICEWS and OEDA data and manually modified them depending on the salient actors in each country, the entire process can be donemanually. Without the prior knowledge about the events of the data at hand, one may resort to sentence parsers and build dictionaries iteratively as we did for the location dictionary.\nFor the relevant words dictionaries, we first imported action verb lists from the CAMEO ontology, on top of which our data sets were built. The verbs for the protest data included words such as “rally,” “demonstrate,” and “march.” For the fight data, the verb list included “air-strike,” “bomb,” and “shoot.” For both dictionaries, we then added key nouns that capture the context of the location sentence such as “bloodshed” and “casualty.” Likewise, a dictionary including irrelevant words, such as “report” and “interview”, was compiled. As in the process of building the other dictionaries, the relevant words dictionary does not have to depend on any existing ontology but we chose to adopt the pre-existing framework of CAMEO because those action verbs were used to collect the news articles in our data in the first place. Finally, dictionaries containing generic words that are not data specific were compiled. The lists included names of news agencies (for example, AFP, AP, and CNN), directional words (southern, southeastern, . . . ), the names ofmonths (january, jan, february, feb, . . . ), and days (monday, mon, . . . ). All of these dictionaries were used as part of preprocessing.\n7. PREPROCESSING THE TEXTS The literature on text analysis describes a few common preprocessing steps. Following D’Orazio\net al. (2014), we first removed punctuation and special characters from the text data that contain sentences with location words. We next converted all sentences to lower letters to avoid confusion in recognizing word patterns. We then removed stop words in English(Shellman 2008; Monroe et al. 2008). The stop words list was imported from the Stanford NLP Group, but we modified it to exclude prepositions related to locations, such as “in”, “at”, and “from”. Next, we performed stemming (Grimmer & Stewart 2013), using Porter Stemmer (Porter 1980).15 In addition to the tasks performed prior tomost text analysis projects, we also performed two additional text treatment tasks that are critical in our algorithm: 1) homogenization of location words and 2) generalization of texts using the dictionaries described above. As with many other text analysis projects, the accuracy of our algorithm depends highly on the quality of the pretreatment process. The homogenization step is important because the use of location names in news articles is not always consistent with respect to the spelling of location names, particularly of those in nonEnglish speaking countries. In addition to the transliteration issue, the different conventions of stating locations also complicate the process. For example, news articles by local agencies cite only city names while those by national or international agencies often indicate only province names. Accordingly, we used the location dictionaries to standardize these variations across data. For the sentences that contain only the lower level location words, we converted the administrative division names (city) to higher level ones (province/governorate) with the prefix of “sub-”. For instance, “Fataki, Orientale\" would be converted to “sub-orientale, orientale.” In building the dictionaries, we used the administrative division at the time of the news reports. For instance, city A in year 2005, the year of the event, may be in province B, but in province C in year 2016. In such a case, we used province B as the correct province.\n15We were careful to preserve important information. For instance, Porter Stemmer removes ‘ing’ at the end of each word, so we converted some province names such as “liaon” back to “liaoning”.\nAs the last step in the preprocessing stage, we generalized the news texts using the aforementioned dictionaries of actors, relevant and irrelevant words (action verbs, key nouns, irrelevant verbs and nouns), numbers, dates and news agencies.16 The purpose of this step is to ensure that the algorithm would recognize the following two N-grams as identical17: “33 people in Beijing” and “2000 people in New York”.18 More generalized sentence patterns are desirable because the approach aims to match patterns of phrases and sentences from different news articles. An example of a preprocessed text looks like the right side of Table 4.\n8. IMPLEMENTATION OF AUTOMATED CLASSIFIERS To mimic the way human coders would parse sentences and retrieve the relevant information, we trained the machine to learn the collocation patterns of the correct and incorrect location words and then to predict the correctness of a new set of location words based on the collocation patterns of those new words. This approach of storing patterns and solving problems known as case-based reasoning is a common paradigm in automated reasoning and machine learning in which a reasoner solves a new problem by using a similar problem that has already been solved (Kolodner 1992; de Mántaras & Plaza 1997). The implementation of our algorithm involves two stages: feature selection and model estimation. These stages require pre-treatment of the text data as illustrated in Figure 2. To describe the feature selection stage in detail, we take examples from the China data, which consist of 250 news articles on protest towards the government from 2001 to 2014.19 On average, each\n16The entries in these dictionaries were stemmed. 17Location names are generalized in the algorithm after a specific location name is collected. Hence, they should not be generalized during the preprocessing stage. 18If these two phrases are generalized in terms of numerals and location names, they become “numeral people in location”. 19We removed duplicate reports.\narticle contains about 398 words before the preprocessing treatment and 284 after the treatment. For each validation score (of the nine results that are averaged and presented in Table 7), we randomly divided the treated articles into training and test sets. What information then do we feed the machine to develop classifiers? Our goal is to differentiate event-relevant words from irrelevant ones and event-occurring words from non-eventoccurring ones. Therefore, we select variables that can provide information about “event-relevance” and “event-occurrence.” Specifically, 1) N-gram collocation patterns, 2) frequency of location words, and 3) context of the sentences that contain the location word are extracted from the news articles. An N-gram is a sequence of N words. Collections of N-grams are known to provide valuable information about each word in a phrase, taking into account the complexity and long distance dependencies of languages.20 In a sentence “Factory workers protested\", an N-gram of order 2 (or bigram) is a two-word-sequence of words (for example “factory workers\", and “workers protested\") while an N-gram of order 3 (or trigram) is a three-word-sequence of words (such as “factory workers protested\"). Given that the collocation patterns in which the event-occurring location words appear differ from those of the non-event-occurring collocations, the N-gram patterns are able to provide the contextual information of event-occurence to our classifiers. We thus compare the frequencies of each N-gram in correct and incorrect corpus and compute the relative frequencies, respectively. Some examples of N-grams collected from one of the training sets are shown in Table 5. From the raw text on the left in 4, “LOCATION MONTH”—the bigram for Beijing—will be stored in the\n20For more information on N-gram, see Ch. 4 in Jurafsky & Martin (2009).\nincorrect bi-gram corpus while “DIRECTIONAL LOCATION”, “LOCATION ADMIN”, —the bigrams of Shandong—and “DIRECTIONAL LOCATION’, “LOCATION ADMIN”—the bigrams of Fujian—will be stored in the correct bigram corpus. Some examples in the bi-grams (N-grams of N=2) of correct location words from the training set in the China data include “LOCATION ADMIN\" (frequency: 17), “LOCATION ACTION-VERB\" (frequency: 15), and “outsid LOCATION\" (frequency: 15). These bi-grams were extracted from sentences such as the following: “The Guizhou provincial government deployed thousands of police\",21 “Workers at IBM Systems Technology Company (ISTC) in Shenzhen are protesting since March\",22 and “500 villagers had been protesting outside the Qingdao naval base\".23 Two of the most frequent bi-grams in the incorrect corpus are “LOCATION MONTH\", and “LOCATION ACTOR\". They are from phrases such as “BEIJING, Dec 3, 2007 (AFP)\",24 “Shandong farmers protested. . . \".25 Table 5 shows the top 10 most frequent bi-grams for both correct and incorrect location words in one of the training sets.\nThen we compare the captured collocation patterns, consisted of location words and their neighboring words, to the correct and incorrect N-gram lists. In the texts in Table 6, for instance, “heilongjiang” and “beijing” would be captured. While creating covariates for “heilongjiang”, the N-gram collocation patterns, such as “of heilingjiang”, “at sub-heilongjiang”, and “in subheilongjiang”, are converted to “of LOCATION”, “at sub-LOCATION”, and “in sub-LOCATION”. For “beijing”, the N-gram collocation patterns such as “to beijing”, “beijing therefor”, and “in beijing” would be converted to “to LOCATION”, “LOCATION therefor”, and “in LOCATION”. These generalized N-gram patterns are compared to the correct and incorrect pattern lists (compiled from the training set) that looks like the list in Table 5. Then the N-gram pattern feature is converted to numeric values in two ways. The first N-gram variables compute the ratio the collocation patterns comparing both the stored correct and incorrect pattern lists. The other N-gram variables\n21Story ID: 35682875, ICEWS China data 22Story ID: 32977476, ICEWS China data 23Story ID: 32852391, ICEWS China data 24Story ID: 22997344, ICEWS China data 25Story ID: 21984369, ICEWS China data\nreflect how many of these collected patterns can be matched to the most frequent patterns in each list.26 The second type of variables, the frequencies of location words, provide the information about the relevance of a particular location word. Assuming that the news articles in the data are well-sorted and contain articles mostly pertinent to the research interest, the set of location words that arementioned several times should have higher chances of being correct, compared to the ones with low frequencies (D’Ignazio et al. 2014).\nTesting the context, sometimes called materiality, of the sentence that contains location words is another way of capturing relevant location words. The idea is that, if the sentence contains more action verbs and key nouns, the location word in that sentence is highly likely to be relevant. Likewise, a location word in a sentence with “report” or news agency names may be less likely to be relevant. Finally, we designed the data so that it can account for the variations at the article level as well as the data level, assuming that 1) some location words are more correct than others in each article and 2) some articles contain location words that are collectively more likely to be correct or incorrect altogether. In other words, these variables are calculated in relative terms within the article and data levels. This means that for the within article level variables, the location word with the largest value in that article has the value of 1. At the data level, only one location word with the largest value within the data has the value of 1. For example, the relative within article ratio of frequency for Heilongjiang in the example in Table 6 would be 1 while that for Beijing would be 0.67. Table 6 also shows the first and the second sentences containing “heilongjiang” include a irrelevant word “said”, converted as “NONTOPIC”, and therefore, the within-article materiality ratio for Heilongjiang and Beijing would be 0 and 0 while the immateriality ratio for the two would be 1 and 0 respectively. All the positive values would be much smaller in the within data ratios. To extract the above mentioned features, we start with the training set, which consists of two thirds of our text data. We first capture all location words that match the list in our location dictionary, and determine whether the location word falls into the correct category or the incorrect category, based on human coding. Once the recognized location word is determined as either correct or incorrect, they are stored separately for correct and incorrect corpora. Once the corpora of correct and incorrect N-grams are created, we compute the N-gram pattern information (N being the range specified27), the frequencies of mention, and themateriality of the location sentences, in terms of both within article and data level ratios. The final data generated would contain the dependent variable indicating whether the particular location word is correct (Y=1) or not (Y=0), and the covariates of the frequencies of each N-gram and the three covariates mentioned above. Figure 3 shows the first thirteen rows and parts of covariates of the data generated using the Chinese news articles. The number of rows of the data equals the number of total province names appearing in all news articles. The first column represents the unique story IDs from ICEWS and the next column contains all of the location words in the article. The Y variable shows whether the location word is correct or not, based on the human coders’ judgment. In the data shown, the first row represents the story 1517019 from the ICEWS data, the example in Table 6. The article contains two location words, ‘beijing’ and ‘heiliongjian’, of which the second is the correct event location. The next four location words in rows three through six are from a single article, ICEWS story 16963437. Of these, only ‘sichuan’ is the correct event-relevant and event-occurring location. The covariates with the suffix “article” are the relative ratios within articles. Location words within articles that do not contain any other locations are therefore assigned the value of one. The covariates with the suffix “data” represent the relative ratios within the data. Correct and\n27We computed this frequency rate for each location word for N-grams of two to seven.\nincorrect N-grams of two to seven, frequencies, and materiality variables are constructed in this manner. In Stage 2, with the data (of the training set) generated, we fit the artificial neural networks, the support vector machine, and the random forests models. Using the random forests Recursive Feature Elimination (RFE) algorithm, we selected the variables of which the combination yields the highest accuracy rates in the training data.28 The parameters of each classifier were adjusted to get the optimal result. In random forests, the number of trees was set to 1000 and the kernel radial was set for SVM. The artificial neural networks model was tuned in each iteration, selecting automatically the best decay rate and the number of dendrites in the hidden layer. The estimated parameters were then used to predict the boolean status of each location word in the test set. For the example in Table 6, the average predicted probabilities for Beijing as the correct location is 6% (neural net), 9% (SVM), and 27% (random forests) and those for Heilongjiang as the correct location is 97% (neural net), 75%(SVM), and 98% (random forests), making the predictions for both location words correct."
    }, {
      "heading" : "9. RESULTS",
      "text" : "Table 7 summarizes the performance statistics of various methods including our own classification approach. The columns represent each method and the rows indicate the data sets used. All numbers are rounded up. For the classification algorithms, we performed three 3-fold cross validations, thus the scores in the columns of neural net, SVM, and random forests are averages of nine iterations in total. The dictionary column represents the accuracy rate of the dictionary method that codes all captured location words as correct event locations.\nAs Figure 4 shows, the accuracy rates across models do not vary much with about 3% maximum difference. While the performance of our algorithm is consistently high across classifiers, the highest accuracy rates in each data set were produced by SVM for the China and DRC data, and by random forests for the Syria data. However, the results vary across data sets, with the 28The N-gram patterns were the most powerful variables consistently.\nhighest rates in the DRC data. This difference comes from the number of true locations in the article. The accuracy rates for location words in the subset consisting of only articles with one correct location range from 86% in China to 90% in the DRC.\nWe have compared the true positives of our results to the currentmachine-coded data. These are the location words that each algorithm classifies as the actual event locations. Figures 5 to 7 compare the results to the ground truth which is plotted on the left and the current ICEWS or OEDA locations on the right. The sizes of bubbles represent the relative shares of events and the colors represent frequencies with the legends on the far right. In the China data, our algorithmmisses eleven protests in Sichuan, but in all other provinces the differences are single digits. On the other hand, ICEWS codes Beijing as the event location more than 30 cases than the ground truth and misses more than 30 protest cases in Guangdong province alone. Results in the DRC are accurate regardless of the choice of classifiers and the best performance is around 85%. This is true in part because the events in the DRC do not typically include a large number of locations. The civil conflicts which show up in the fight category in the DRC are concentrated in a small number of areas. By comparision, protests in China are not only in a much larger country, but are in a wide variety of locations. Accordingly, stories about China have many more location words per story, and are harder to correctly identify than is the case in the DRC. Figure 6 shows that the bubbles of the human-coded map on the left and the machinecoded map in the middle are nearly identical. Compared to the human coded locations, the locations coded by ICEWS model are correct at around 67% with over 30 under-reporting cases of fight in North Kivu and over 20 over-reporting cases in Kinshasa. In the Syria case, our overall predicted event locations also look very similar to the ground truth while OEDA not only misses over one-half of the true event locations (129 NAs in 250 news articles), but also includes event locations that are not in Syria such as Beirut (three events), Illinois (one event), Moscow (one event), New Jersey (one event) and Pennsylvania (four events). Compared to the human coded locations, event locations in OEDA data are correct 31% of the time. Overall, the performance of our classifiers is strong, improving the accuracy rate by as much as 25% from the dictionary approach. Furthermore, even if the accuracy rate is not 100%, because our algorithm evaluates each location word, it does not symmetrically miss or favor certain locations as the current ICEWS and OEDA algorithms do. Hence, the visualized results seem very close to the ground truth.\n10. CONCLUSION We examined some problems associated with current geolocation methods employed in existing machine-coded event data. Locations of events contain valuable information that is of interest to many scholars and policy makers. To address discrepancies in geolocation between automated and human coders, we developed a supervised machine learning algorithm that filters out event-irrelevant locations as well as non-event-occurring ones. Departing from the assumption that one correct location exists in a news article, we evaluate each location word. By\ndoing so, we diverge from algorithms that are systematically biased towards certain locations such as the capital of a country and locations that appear frequently in the corpus. Using human coded ground-truth, we demonstrate that this approach is superior to extant approaches in the cases we have studied. Interested scholars can extend the current work to a wider range of event ontologies and locations. While we have studied only a few countries, the protocol we developed may aid others who are interested in different countries to geolocate extant event datamore accurately. Others who wish to extract location information from structured text data written in formal language, such as the United Nations reports on Children and Armed Conflict (https:childrenandarmedconflict.un.org) or Amnesty International country reports (https:www.amnesty.orgen/latest/research/2016/02/annual-report-201516/), can utilize our (open source) protocol—available upon publication—to create new event data streams in which the events are geolocated."
    } ],
    "references" : [ {
      "title" : "An Empirical Comparison",
      "author" : [ "Amami", "Rimah", "Ayed", "Dorra Ben", "Ellouze", "Noureddine" ],
      "venue" : null,
      "citeRegEx" : "Amami et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Amami et al\\.",
      "year" : 2012
    }, {
      "title" : "Recognizing Citations in Public Comments",
      "author" : [ "Arguello", "Jaime", "Callan", "Jamie", "Shulman", "Stuart" ],
      "venue" : "Journal of Information Technology and Politics,",
      "citeRegEx" : "Arguello et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Arguello et al\\.",
      "year" : 2008
    }, {
      "title" : "What to Do (and Not to Do) With Pooled Time-Series Cross-Section Data",
      "author" : [ "Beck", "Nathaniel", "Katz", "Jonathan N" ],
      "venue" : "American Political Science Review,",
      "citeRegEx" : "Beck et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Beck et al\\.",
      "year" : 1995
    }, {
      "title" : "Improving Quantitative Studies of International Conflict: A Conjecture",
      "author" : [ "Beck", "Nathaniel", "King", "Gary", "Zeng", "Langche" ],
      "venue" : "American Political Science Review,",
      "citeRegEx" : "Beck et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Beck et al\\.",
      "year" : 2000
    }, {
      "title" : "Random Forests-classification Description",
      "author" : [ "Breiman", "Leo", "Cutler", "Adele" ],
      "venue" : null,
      "citeRegEx" : "Breiman et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Breiman et al\\.",
      "year" : 2007
    }, {
      "title" : "Using Encyclopedic Knowledge for Named Entity",
      "author" : [ "Bunescu", "Razvan", "Pasca", "Marius" ],
      "venue" : null,
      "citeRegEx" : "Bunescu et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Bunescu et al\\.",
      "year" : 2006
    }, {
      "title" : "Text Annotation for Political Science Research",
      "author" : [ "Cardie", "Claire", "Wilkerson", "John" ],
      "venue" : "Journal of Information Technology & Politics,",
      "citeRegEx" : "Cardie et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Cardie et al\\.",
      "year" : 2008
    }, {
      "title" : "Introduction to Special Issue on “Disaggregating Civil War",
      "author" : [ "Cederman", "Lars-Erik", "Gleditsch", "Kristian Skrede" ],
      "venue" : "Journal of Conflict Resolution,",
      "citeRegEx" : "Cederman et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Cederman et al\\.",
      "year" : 2009
    }, {
      "title" : "Neural Networks: A Review from a Statistical",
      "author" : [ "Cheng", "Bing", "Titterington", "D. Michael" ],
      "venue" : null,
      "citeRegEx" : "Cheng et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 1994
    }, {
      "title" : "An Introduction to Support Vector Machines",
      "author" : [ "Cristianini", "Nello", "Shawe-Taylor", "John" ],
      "venue" : null,
      "citeRegEx" : "Cristianini et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Cristianini et al\\.",
      "year" : 2000
    }, {
      "title" : "Large-Scale Named Entity Disambiguation Based on Wikipedia Data",
      "author" : [ "Cucerzan", "Silviu" ],
      "venue" : null,
      "citeRegEx" : "Cucerzan and Silviu.,? \\Q2007\\E",
      "shortCiteRegEx" : "Cucerzan and Silviu.",
      "year" : 2007
    }, {
      "title" : "Nearest Neighbor Pattern Classification Techniques",
      "author" : [ "Dasarthy", "Belur V." ],
      "venue" : "Hoboken, NJ: IEEE Computer Society Press.",
      "citeRegEx" : "Dasarthy and V.,? 1990",
      "shortCiteRegEx" : "Dasarthy and V.",
      "year" : 1990
    }, {
      "title" : "Case-Based Reasoning: An Overview",
      "author" : [ "de Mántaras", "Ramon López", "Plaza", "Enric" ],
      "venue" : "AI Communications,",
      "citeRegEx" : "Mántaras et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Mántaras et al\\.",
      "year" : 1997
    }, {
      "title" : "Untangling Neural Nets",
      "author" : [ "de Marchi", "Scott", "Gelpi", "Christopher", "Grynaviski", "Jeffrey D" ],
      "venue" : "American Political Science Review,",
      "citeRegEx" : "Marchi et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Marchi et al\\.",
      "year" : 2004
    }, {
      "title" : "Cliff-Clavin: Determining geographic focus for news articles. In: title = KDD ’14",
      "author" : [ "D’Ignazio", "Catherine", "Bhargava", "Rahul", "Zuckerman", "Ethan", "Beck", "Luisa" ],
      "venue" : "Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 20th ACM SIGKDD Conference on Knowledge Discovery and Data Mining",
      "citeRegEx" : "D.Ignazio et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "D.Ignazio et al\\.",
      "year" : 2014
    }, {
      "title" : "Separating the Wheat from the Chaff: Applications of Automated Document Classification Using Support Vector Machines",
      "author" : [ "D’Orazio", "Vito", "Landis", "Steven T", "Palmer", "Glenn", "Schrodt", "Philip" ],
      "venue" : "Political Analysis,",
      "citeRegEx" : "D.Orazio et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "D.Orazio et al\\.",
      "year" : 2014
    }, {
      "title" : "The Political Economy of Decentralization: Implications for Aid Effectiveness",
      "author" : [ "Easton", "Kent", "Kaiser", "Kai", "Smoke", "Paul" ],
      "venue" : null,
      "citeRegEx" : "Easton et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Easton et al\\.",
      "year" : 2011
    }, {
      "title" : "Ethnicity, insurgency, and civil war",
      "author" : [ "Fearon", "James D", "Laitin", "David D" ],
      "venue" : "American Political Science Review,",
      "citeRegEx" : "Fearon et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Fearon et al\\.",
      "year" : 2003
    }, {
      "title" : "Learning to Detect Phishing Emails",
      "author" : [ "Fette", "Ian", "Sadeh", "Norman", "Tomasic", "Anthony" ],
      "venue" : "Proceedings of the 16th international conference on World Wide Web. ACM",
      "citeRegEx" : "Fette et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Fette et al\\.",
      "year" : 2007
    }, {
      "title" : "Decentralization and Infrastructure: From Gaps to Solutions. Working Paper 14-05",
      "author" : [ "Frank", "Jonas", "Martinez-Vazquez", "Jorge" ],
      "venue" : "Andrew Young School of Policy Studies,",
      "citeRegEx" : "Frank et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Frank et al\\.",
      "year" : 2014
    }, {
      "title" : "A Decision-Theoretic Generalization of On-Line Learningand an Application to Boosting",
      "author" : [ "Freund", "Yoav", "Schapire", "Robert" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Freund et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Freund et al\\.",
      "year" : 1997
    }, {
      "title" : "Text as data: The promise and pitfalls of automatic content analysis methods for political texts",
      "author" : [ "Grimmer", "Justin", "Stewart", "BrandonM" ],
      "venue" : "Political Analysis,",
      "citeRegEx" : "Grimmer et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Grimmer et al\\.",
      "year" : 2013
    }, {
      "title" : "CORPS: A Corpus of Tagged Political Speeches for Persuasive Communication Processing",
      "author" : [ "Guerini", "Marco", "Strapparava", "Carlo", "Stock", "Oliviero" ],
      "venue" : "Journal of Information Technology & Politics,",
      "citeRegEx" : "Guerini et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Guerini et al\\.",
      "year" : 2008
    }, {
      "title" : "neuralnet: Training of Neural Networks",
      "author" : [ "Günther", "Frauke", "Fritsch", "Stefan" ],
      "venue" : "The R journal,",
      "citeRegEx" : "Günther et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Günther et al\\.",
      "year" : 1998
    }, {
      "title" : "Collective Entity Linking In Web Text: A Graph-based Method",
      "author" : [ "Han", "Xianpei", "Sun", "Le", "Zhao", "Jun" ],
      "venue" : "Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval. ACM",
      "citeRegEx" : "Han et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2011
    }, {
      "title" : "Novel Solutions for an Old Disease: Diagnosis of Acute Appendicitis with Random Forest",
      "author" : [ "Hsieh", "Chung-Ho", "Lu", "Ruey-Hwa", "Lee", "Nai-Hsin", "Chiu", "Wen-Ta", "Hsu", "Min-Huei", "Li", "Yu-Chuan Jack" ],
      "venue" : "Support Vector Machines, and Artificial Neural Networks. Surgery,",
      "citeRegEx" : "Hsieh et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Hsieh et al\\.",
      "year" : 2011
    }, {
      "title" : "Exploratory Data Analysis Using Random Forests. In: Prepared for the 73rd annual MPSA conference",
      "author" : [ "Jones", "Zachary", "Linder", "Fridolin" ],
      "venue" : null,
      "citeRegEx" : "Jones et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Jones et al\\.",
      "year" : 2015
    }, {
      "title" : "Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition",
      "author" : [ "Jurafsky", "Dan", "Martin", "James H" ],
      "venue" : "Upper Saddle River,",
      "citeRegEx" : "Jurafsky et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Jurafsky et al\\.",
      "year" : 2009
    }, {
      "title" : "An Introduction to Case-Based Reasoning",
      "author" : [ "Geolocation in Event Data Kolodner", "Janet L." ],
      "venue" : "Artificial Intelligence Review,",
      "citeRegEx" : "Kolodner and L.,? 1992",
      "shortCiteRegEx" : "Kolodner and L.",
      "year" : 1992
    }, {
      "title" : "Machine Learning with R. Birmingham, UK: Packt Publishing",
      "author" : [ "3–24. Lantz", "Brett" ],
      "venue" : "ICEWS Coded Event",
      "citeRegEx" : "Lantz and Brett.,? \\Q2013\\E",
      "shortCiteRegEx" : "Lantz and Brett.",
      "year" : 2013
    }, {
      "title" : "A Statistical Approach to the Sub",
      "author" : [ "Version]. Lautenschlager", "Jennifer", "Starz", "James", "Warfield", "Ian" ],
      "venue" : null,
      "citeRegEx" : "Lautenschlager et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lautenschlager et al\\.",
      "year" : 2016
    }, {
      "title" : "Classification and Regression by randomForest",
      "author" : [ "Liaw", "Andy", "Wiener", "Matthew" ],
      "venue" : "The First Instructional Conference on Machine Learning,",
      "citeRegEx" : "Liaw et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Liaw et al\\.",
      "year" : 2002
    }, {
      "title" : "Computer-Assisted Text Analysis for Comparative Politics",
      "author" : [ "Tingley", "Dustin" ],
      "venue" : null,
      "citeRegEx" : "Tingley and Dustin.,? \\Q2015\\E",
      "shortCiteRegEx" : "Tingley and Dustin.",
      "year" : 2015
    }, {
      "title" : "Dataminingmethods in the prediction of Dementia: A Real-data Comparison",
      "author" : [ "Alexandre" ],
      "venue" : null,
      "citeRegEx" : "2011.,? \\Q2011\\E",
      "shortCiteRegEx" : "2011.",
      "year" : 2011
    }, {
      "title" : "Mining texts to efficiently generate global",
      "author" : [ "Systems. Minhas", "Shahryar", "Ulfelder", "Jay", "Ward", "Michael" ],
      "venue" : null,
      "citeRegEx" : "Minhas et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Minhas et al\\.",
      "year" : 2015
    }, {
      "title" : "Nonlinear Prediction of Chaotic Time",
      "author" : [ "372–403. Mukherjee", "Sayan", "Osuna", "Edgar", "Girosi", "Federico" ],
      "venue" : null,
      "citeRegEx" : "Mukherjee et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Mukherjee et al\\.",
      "year" : 1997
    }, {
      "title" : "Neural Networks: An Introduction",
      "author" : [ "Müller", "Berndt", "Reinhardt", "Joachim" ],
      "venue" : null,
      "citeRegEx" : "Müller et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Müller et al\\.",
      "year" : 1997
    }, {
      "title" : "Naive Bayes Classifiers",
      "author" : [ "Media. Murphy", "Kevin P" ],
      "venue" : null,
      "citeRegEx" : "Murphy and P.,? \\Q2006\\E",
      "shortCiteRegEx" : "Murphy and P.",
      "year" : 2006
    }, {
      "title" : "Crisis Early Warning and Decision Support: Contemporary Approaches and Thoughts on Future Research",
      "author" : [ "O’Brien", "Sean P" ],
      "venue" : "International Studies Review,",
      "citeRegEx" : "O.Brien and P.,? \\Q2010\\E",
      "shortCiteRegEx" : "O.Brien and P.",
      "year" : 2010
    }, {
      "title" : "Entity Linking: Finding Extracted Entities",
      "author" : [ "Rao", "Delip", "McNamee", "Paul", "Dredze", "Mark" ],
      "venue" : null,
      "citeRegEx" : "Rao et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Rao et al\\.",
      "year" : 2013
    }, {
      "title" : "Best Practices in the Collection of Conflict Data",
      "author" : [ "Salehyan", "Idean." ],
      "venue" : "Journal of Peace Research, 52(1), 105–109.",
      "citeRegEx" : "Salehyan and Idean.,? 2015",
      "shortCiteRegEx" : "Salehyan and Idean.",
      "year" : 2015
    }, {
      "title" : "Twenty years of the Kansas event data system project",
      "author" : [ "Schrodt", "Philip A." ],
      "venue" : "The political methodologist, 14(1), 2–8.",
      "citeRegEx" : "Schrodt and A.,? 2006",
      "shortCiteRegEx" : "Schrodt and A.",
      "year" : 2006
    }, {
      "title" : "April). Event data in forecasting models: where does it come",
      "author" : [ "Schrodt", "Philip A" ],
      "venue" : null,
      "citeRegEx" : "Schrodt and A.,? \\Q2015\\E",
      "shortCiteRegEx" : "Schrodt and A.",
      "year" : 2015
    }, {
      "title" : "A Guide to Event Data: Past, Present, and Future",
      "author" : [ "Schrodt", "Philip A", "Yonamine", "James E" ],
      "venue" : "All Azimuth,",
      "citeRegEx" : "Schrodt et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Schrodt et al\\.",
      "year" : 2013
    }, {
      "title" : "Coding Disaggregated Intrastate Conflict: Machine Processing the Behavior of Substate Actors over Time and Space",
      "author" : [ "Shellman", "Stephen M." ],
      "venue" : "Political Analysis, 16(4), 464–477.",
      "citeRegEx" : "Shellman and M.,? 2008",
      "shortCiteRegEx" : "Shellman and M.",
      "year" : 2008
    }, {
      "title" : "Text Categorization with Support Vector Machines: Learning",
      "author" : [ "T. Joachims", "Thorsten" ],
      "venue" : null,
      "citeRegEx" : "Joachims and Thorsten.,? \\Q1998\\E",
      "shortCiteRegEx" : "Joachims and Thorsten.",
      "year" : 1998
    }, {
      "title" : "Support Vector Machine Active Learning with Applications to Text Classification",
      "author" : [ "Tong", "Simon", "Koller", "Daphne" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Tong et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Tong et al\\.",
      "year" : 2001
    }, {
      "title" : "The Nature of Statistical Learning Theory",
      "author" : [ "Vapnik", "Vladimir N." ],
      "venue" : "New York: Springer-Verlag,",
      "citeRegEx" : "Vapnik and N.,? 1995",
      "shortCiteRegEx" : "Vapnik and N.",
      "year" : 1995
    }, {
      "title" : "Statistical Learning Theory",
      "author" : [ "Vapnik", "Vladimir N." ],
      "venue" : "Vol. 1. New York: Wiley.",
      "citeRegEx" : "Vapnik and N.,? 1998",
      "shortCiteRegEx" : "Vapnik and N.",
      "year" : 1998
    }, {
      "title" : "The Optimality of Naive Bayes",
      "author" : [ "Zhang", "Harry." ],
      "venue" : "In: Proceedings of the 17th International FLAIRS",
      "citeRegEx" : "Zhang and Harry.,? 2004",
      "shortCiteRegEx" : "Zhang and Harry.",
      "year" : 2004
    }, {
      "title" : "Multilabel Neural Networks with Applications to Func",
      "author" : [ "Zhang", "Min-Ling", "Zhou", "Zhi-Hua" ],
      "venue" : null,
      "citeRegEx" : "Zhang et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2006
    }, {
      "title" : "Introduction to Artificial Neural Systems",
      "author" : [ "Zurada", "JacekM." ],
      "venue" : "Vol. 8. Boston, MA: PWS Publishing",
      "citeRegEx" : "Zurada and JacekM.,? 1992",
      "shortCiteRegEx" : "Zurada and JacekM.",
      "year" : 1992
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "and other international organizations (Frank & Martinez-Vazquez 2014; Easton et al. 2011) have emphasized this deeper dive into the political and economic landscape.",
      "startOffset" : 38,
      "endOffset" : 89
    }, {
      "referenceID" : 15,
      "context" : "Outstanding issues include machine translation of texts in foreign languages (wherein great progress is being made in both Chinese and Arabic), duplicate reports from multiple sources, and the relatively low accuracy in determining the event location (D’Orazio et al. 2014; Schrodt 2015).",
      "startOffset" : 251,
      "endOffset" : 287
    }, {
      "referenceID" : 15,
      "context" : "and other international organizations (Frank & Martinez-Vazquez 2014; Easton et al. 2011) have emphasized this deeper dive into the political and economic landscape. Much of this deeper dive is coming from organizations and governments in terms of their reporting on demographic, economic, financial, and health data that are subnational. Most data in the conflict realm comes from non-offical sources. For many that means some form of data collected from historical and journalistic sources. This need is often filled by event data, which are typically collected on a daily basis, and can be aggregated temporally to the level required by the analysis. Event data can also be aggregated to the geographical region that is appropriate. Given the increasing demands for event data, the scientific community has recently devoted significant efforts to automate the data collection process. Having humans read and code a large set of archive documents sometimes limits reproducibility, and hence hinders scientific research. It is also expensive and limits the currency of the data. Further, ensuring inter-coder reliability is challenging, especially over global events that span decades. Several efforts utilize machine-coding to collect event information and determine event features automatically. Projects such as the Integrated Crisis EarlyWarning System (ICEWS) (Boschee et al. 2015; Lautenschlager et al. 2015; O’Brien 2010) and the Open Source Event Data Alliance 2016 are two prominent examples. A good overview on event data in political science is found in Schrodt & Yonamine (2013). These automated event data allow researchers to observe and extract information on politically relevant events around the world in near real-time.",
      "startOffset" : 70,
      "endOffset" : 1592
    }, {
      "referenceID" : 15,
      "context" : "In the first step, known as named entity recognition in Computer Science and Computational Linguistics (D’Orazio et al. 2014; Cardie & Wilkerson 2008; Guerini et al. 2008; Arguello et al. 2008; Nadeau & Sekine 2007), all location names are identified and extracted from an appropriately preprocessed text.",
      "startOffset" : 103,
      "endOffset" : 215
    }, {
      "referenceID" : 22,
      "context" : "In the first step, known as named entity recognition in Computer Science and Computational Linguistics (D’Orazio et al. 2014; Cardie & Wilkerson 2008; Guerini et al. 2008; Arguello et al. 2008; Nadeau & Sekine 2007), all location names are identified and extracted from an appropriately preprocessed text.",
      "startOffset" : 103,
      "endOffset" : 215
    }, {
      "referenceID" : 1,
      "context" : "In the first step, known as named entity recognition in Computer Science and Computational Linguistics (D’Orazio et al. 2014; Cardie & Wilkerson 2008; Guerini et al. 2008; Arguello et al. 2008; Nadeau & Sekine 2007), all location names are identified and extracted from an appropriately preprocessed text.",
      "startOffset" : 103,
      "endOffset" : 215
    }, {
      "referenceID" : 15,
      "context" : "Next comes entity disambiguation/resolution, which involves identifying the actual location of the recognized name string(Cucerzan 2007; D’Orazio et al. 2014; Bunescu & Pasca 2006).",
      "startOffset" : 121,
      "endOffset" : 180
    }, {
      "referenceID" : 24,
      "context" : "(Han et al. 2011; Rao et al. 2013; Bunescu & Pasca 2006; Cucerzan 2007).",
      "startOffset" : 0,
      "endOffset" : 71
    }, {
      "referenceID" : 39,
      "context" : "(Han et al. 2011; Rao et al. 2013; Bunescu & Pasca 2006; Cucerzan 2007).",
      "startOffset" : 0,
      "endOffset" : 71
    }, {
      "referenceID" : 24,
      "context" : "which location words frequently appear together in the corpus, could be modeled and used to link the name strings and the true locations (Han et al. 2011).",
      "startOffset" : 137,
      "endOffset" : 154
    }, {
      "referenceID" : 14,
      "context" : "This approach selects the most likely place as the “focus” location of the article, based on the frequencies of mentions and the order of appearance (D’Ignazio et al. 2014).",
      "startOffset" : 149,
      "endOffset" : 172
    }, {
      "referenceID" : 30,
      "context" : "By assuming that a single location word exists per article as “the most appropriate location” (Lautenschlager et al. 2016), such a coding rule misses many true event locations, thereby hindering the accuracy of the coded location names.",
      "startOffset" : 94,
      "endOffset" : 122
    }, {
      "referenceID" : 34,
      "context" : "13Examples of SVM applications in text analysis can be found inMinhas et al. (2015); Tong & Koller (2001); T.",
      "startOffset" : 63,
      "endOffset" : 84
    }, {
      "referenceID" : 34,
      "context" : "13Examples of SVM applications in text analysis can be found inMinhas et al. (2015); Tong & Koller (2001); T.",
      "startOffset" : 63,
      "endOffset" : 106
    }, {
      "referenceID" : 18,
      "context" : "14See Fette et al. (2007) for examples.",
      "startOffset" : 6,
      "endOffset" : 26
    }, {
      "referenceID" : 35,
      "context" : "Furthermore, SVMmodels are typically less prone to over-fitting (Mukherjee et al. 1997).",
      "startOffset" : 64,
      "endOffset" : 87
    }, {
      "referenceID" : 0,
      "context" : "SVMs also provide a computationally efficient way to achieve a reasonably accurate model (Amami et al. 2012; Li 2003).",
      "startOffset" : 89,
      "endOffset" : 117
    }, {
      "referenceID" : 3,
      "context" : "They are powerful information extraction tools that can capture underlying relationships not explained by known structures (Jones & Linder 2015; Lantz 2013; Günther & Fritsch 1998; Beck et al. 2000).",
      "startOffset" : 123,
      "endOffset" : 198
    }, {
      "referenceID" : 25,
      "context" : "Second, these models achieve accurate prediction rates given large enough input sizes (Maroco et al. 2011; Hsieh et al. 2011; Beck et al. 2000).",
      "startOffset" : 86,
      "endOffset" : 143
    }, {
      "referenceID" : 3,
      "context" : "Second, these models achieve accurate prediction rates given large enough input sizes (Maroco et al. 2011; Hsieh et al. 2011; Beck et al. 2000).",
      "startOffset" : 86,
      "endOffset" : 143
    }, {
      "referenceID" : 0,
      "context" : "SVMs also provide a computationally efficient way to achieve a reasonably accurate model (Amami et al. 2012; Li 2003). Finally, the random forests (or decision tree forest) model, championed by Leo Breiman (2001) and Adele Cutler (Breiman & Cutler 2007), combines the principle of bagging with random feature selection to add complexity to the decision tree models.",
      "startOffset" : 90,
      "endOffset" : 213
    }, {
      "referenceID" : 0,
      "context" : "SVMs also provide a computationally efficient way to achieve a reasonably accurate model (Amami et al. 2012; Li 2003). Finally, the random forests (or decision tree forest) model, championed by Leo Breiman (2001) and Adele Cutler (Breiman & Cutler 2007), combines the principle of bagging with random feature selection to add complexity to the decision tree models. After the ensemble of classification regression trees (hence the name forest) is generated, the model combines these trees’ predictions. Because the ensemble uses only a small, random portion of the full feature set, the model can handle large data sets wherein the high dimensionality may cause other models to fail. Despite the difficulty in interpreting the results, it is an all-purpose approach that performs well on most problems (Lantz 2013). These classifiers boast two primary strengths. The first is that they do not require prespecifying the type of relationship between the covariates and the response variable. They are powerful information extraction tools that can capture underlying relationships not explained by known structures (Jones & Linder 2015; Lantz 2013; Günther & Fritsch 1998; Beck et al. 2000). Second, these models achieve accurate prediction rates given large enough input sizes (Maroco et al. 2011; Hsieh et al. 2011; Beck et al. 2000). Lantz (2013) suggest that these algorithms are the most accurate state-of- the-art approaches, and make few assumptions about the data.",
      "startOffset" : 90,
      "endOffset" : 1348
    }, {
      "referenceID" : 15,
      "context" : "Following D’Orazio et al. (2014), we first removed punctuation and special characters from the text data that contain sentences with location words.",
      "startOffset" : 10,
      "endOffset" : 33
    }, {
      "referenceID" : 14,
      "context" : "Assuming that the news articles in the data are well-sorted and contain articles mostly pertinent to the research interest, the set of location words that arementioned several times should have higher chances of being correct, compared to the ones with low frequencies (D’Ignazio et al. 2014).",
      "startOffset" : 269,
      "endOffset" : 292
    } ],
    "year" : 2016,
    "abstractText" : "Extracting the “correct” location information from text data, i.e., determining the place of event, has long been a goal for automated text processing. To approximate human-like coding schema, we introduce a supervised machine learning algorithm that classifies each location word to be either correct or incorrect. We use news articles collected from around the world (Integrated Crisis Early Warning System [ICEWS] data and Open Event Data Alliance [OEDA] data) to test our algorithm that consists of two stages. In the feature selection stage, we extract contextual information from texts, namely, the N-gram patterns for location words, the frequency of mention, and the context of the sentences containing location words. In the classification stage, we use three classifiers to estimate the model parameters in the training set and then to predict whether a location word in the test set news articles is the place of the event. The validation results show that our algorithm improves the accuracy rate of the current geolocation methods of dictionary approach by as much as 25%. Keyword: Natural Language Processing, Information Extraction, Text Analysis, Supervised Machine Learning, Geolocation, Event Data",
    "creator" : "LaTeX with hyperref package"
  }
}