{
  "name" : "1506.00552.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Coordinate Descent Converges Faster with the Gauss-Southwell Rule Than Random Selection",
    "authors" : [ "Julie Nutini", "Mark Schmidt", "Issam H. Laradji", "Michael Friedlander", "Hoyt Koepke" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Coordinate Descent Methods",
      "text" : "There has been substantial recent interest in applying coordinate descent methods to solve large-scale optimization problems, starting with the seminal work of Nesterov [2012], who gave the first global rate-ofconvergence analysis for coordinate-descent methods for minimizing convex functions. This analysis suggests that choosing a random coordinate to update gives the same performance as choosing the “best” coordinate to update via the more expensive Gauss-Southwell (GS) rule. (Nesterov also proposed a more clever randomized scheme, which we consider later in this paper.) This result gives a compelling argument to use randomized coordinate descent in contexts where the GS rule is too expensive. It also suggests that there is no benefit to using the GS rule in contexts where it is relatively cheap. But in these contexts, the GS rule often substantially outperforms randomized coordinate selection in practice. This suggests that either the analysis of GS is not tight, or that there exists a class of functions for which the GS rule is as slow as randomized coordinate descent.\nAfter discussing contexts in which it makes sense to use coordinate descent and the GS rule, we answer this theoretical question by giving a tighter analysis of the GS rule (under strong-convexity and standard smoothness assumptions) that yields the same rate as the randomized method for a restricted class of functions, but is otherwise faster (and in some cases substantially faster). We further show that, compared to the usual constant step-size update of the coordinate, the GS method with exact coordinate optimization has a provably faster rate for problems satisfying a certain sparsity constraint (Section 5). We believe that this is the first result showing a theoretical benefit of exact coordinate optimization; all previous analyses show that these strategies obtain the same rate as constant step-size updates, even though exact optimization tends to be faster in practice. Furthermore, in Section 6, we propose a variant of the GS rule that, similar to Nesterov’s more clever randomized sampling scheme, uses knowledge of the Lipschitz constants of the coordinate-wise gradients to obtain a faster rate. We also analyze approximate GS rules (Section 7), which\nar X\niv :1\n50 6.\n00 55\n2v 1\n[ m\nat h.\nO C\n] 1\nJ un\n2 01\nprovide an intermediate strategy between randomized methods and the exact GS rule. Finally, we analyze proximal-gradient variants of the GS rule (Section 8) for optimizing problems that include a separable nonsmooth term."
    }, {
      "heading" : "2 Problems of Interest",
      "text" : "The rates of Nesterov show that coordinate descent can be faster than gradient descent in cases where, if we are optimizing n variables, the cost of performing n coordinate updates is similar to the cost of performing one full gradient iteration. This essentially means that coordinate descent methods are useful for minimizing convex functions that can be expressed in one of the following two forms:\nh1(x) := n∑ i=1 gi(xi) + f(Ax), h2(x) := ∑ i∈V gi(xi) + ∑ (i,j)∈E fij(xi, xj),\nwhere xi is element i of x, f is smooth and cheap, the fij are smooth, G = {V,E} is a graph, and A is a matrix. (It is assumed that all functions are convex.)1 The family of functions h1 includes core machinelearning problems such as least squares, logistic regression, lasso, and SVMs (when solved in dual form) [Hsieh et al., 2008]. Family h2 includes quadratic functions, graph-based label propagation algorithms for semisupervised learning [Bengio et al., 2006], and finding the most likely assignments in continuous pairwise graphical models [Rue and Held, 2005].\nIn general, the GS rule for problem h2 is as expensive as a full gradient evaluation. However, the structure of G often allows efficient implementation of the GS rule. For example, if each node has at most d neighbours, we can track the gradients of all the variables and use a max-heap structure to implement the GS rule in O(d log n) time [Meshi et al., 2012]. This is similar to the cost of the randomized algorithm if d ≈ |E|/n (since the average cost of the randomized method depends on the average degree). This condition is true in a variety of applications. For example, in spatial statistics we often use two-dimensional grid-structured graphs, where the maximum degree is four and the average degree is slightly less than 4. As another example, for applying graph-based label propagation on the Facebook graph (to detect the spread of diseases, for example), the average number of friends is around 200 but no user has more than seven thousand friends.2 The maximum number of friends would be even smaller if we removed edges based on proximity. A non-sparse example where GS is efficient is complete graphs, since here the average degree and maximum degree are both (n−1). Thus, the GS rule is efficient for optimizing dense quadratic functions. On the other hand, GS could be very inefficient for star graphs.\nIf each column of A has at most c non-zeroes and each row has at most r non-zeroes, then for many notable instances of problem h1 we can implement the GS rule in O(cr log n) time by maintaining Ax as well as the gradient and again using a max-heap (see Appendix A). Thus, GS will be efficient if cr is similar to the number of non-zeroes in A divided by n. Otherwise, Dhillon et al. [2011] show that we can approximate the GS rule for problem h1 with no gi functions by solving a nearest-neighbour problem. Their analysis of the GS rule in the convex case, however, gives the same convergence rate that is obtained by random selection (although the constant factor can be smaller by a factor of up to n). More recently, Shrivastava and Li [2014] give a general method for approximating the GS rule for problem h1 with no gi functions by writing it as a maximum inner-product search problem."
    }, {
      "heading" : "3 Existing Analysis",
      "text" : "We are interested in solving the convex optimization problem\nmin x∈Rn f(x), (1)\n1We could also consider slightly more general cases like functions that are defined on hyper-edges [Richtárik and Takáč, 2015], provided that we can still perform n coordinate updates for a similar cost to one gradient evaluation.\n2https://recordsetter.com/world-record/facebook-friends\nwhere ∇f is coordinate-wise L-Lipschitz continuous, i.e., for each i = 1, . . . , n,\n|∇if(x+ αei)−∇if(x)| ≤ L|α|, ∀x ∈ Rn and α ∈ R,\nwhere ei is a vector with a one in position i and zero in all other positions. For twice-differentiable functions, this is equivalent to the assumption that the diagonal elements of the Hessian are bounded in magnitude by L. In contrast, the typical assumption used for gradient methods is that ∇f is Lf -Lipschitz continuous (note that L ≤ Lf ≤ Ln). The coordinate-descent method with constant step-size is based on the iteration\nxk+1 = xk − 1 L ∇ikf(xk)eik .\nThe randomized coordinate-selection rule chooses ik uniformly from the set {1, 2, . . . , n}. Alternatively, the GS rule\nik = argmax i\n|∇if(xk)|,\nchooses the coordinate with the largest directional derivative. Under either rule, because f is coordinate-wise Lipschitz continuous, we obtain the following bound on the progress made by each iteration:\nf(xk+1) ≤ f(xk) +∇ikf(xk)(xk+1 − xk)ik + L\n2 (xk+1 − xk)2ik\n= f(xk)− 1 L (∇ikf(xk))2 + L 2\n[ 1\nL ∇ikf(xk) ]2 = f(xk)− 1\n2L [∇ikf(xk)]2.\n(2)\nWe focus on the case where f is µ-strongly convex, meaning that, for some positive µ,\nf(y) ≥ f(x) + 〈∇f(x), y − x〉+ µ 2 ‖y − x‖2, ∀x, y ∈ Rn, (3)\nwhich implies that\nf(x∗) ≥ f(xk)− 1 2µ ‖∇f(xk)‖2, (4)\nwhere x∗ is the optimal solution of (1). This bound is obtained by minimizing both sides of (3) with respect to y."
    }, {
      "heading" : "3.1 Randomized Coordinate Descent",
      "text" : "Conditioning on the σ-field Fk−1 generated by the sequence {x0, x1, . . . , xk−1}, and taking expectations of both sides of (2), when ik is chosen with uniform sampling we obtain\nE[f(xk+1)] ≤ E [ f(xk)− 1\n2L\n( ∇ikf(xk) )2] = f(xk)− 1\n2L n∑ i=1 1 n ( ∇if(xk) )2 = f(xk)− 1\n2Ln ‖∇f(xk)‖2.\nUsing (4) and subtracting f(x∗) from both sides, we get\nE[f(xk+1)]− f(x∗) ≤ (\n1− µ Ln\n) [f(xk)− f(x∗)]. (5)\nThis is a special of case of Nesterov [2012, Theorem 2] with α = 0 in his notation."
    }, {
      "heading" : "3.2 Gauss-Southwell",
      "text" : "We now consider the progress implied by the GS rule. By the definition of ik,\n(∇ikf(xk))2 = ‖∇f(xk)‖2∞ ≥ (1/n)‖∇f(xk)‖2. (6)\nApplying this inequality to (2), we obtain\nf(xk+1) ≤ f(xk)− 1 2Ln ‖∇f(xk)‖2,\nwhich together with (4), implies that f(xk+1)− f(x∗) ≤ (\n1− µ Ln\n) [f(xk)− f(x∗)]. (7)\nThis is a special case of Boyd and Vandenberghe [2004, §9.4.3], viewing the GS rule as performing steepest descent in the 1-norm. While this is faster than known rates for cyclic coordinate selection [Beck and Tetruashvili, 2013] and holds deterministically rather than in expectation, this rate is the same as the randomized rate given in (5)."
    }, {
      "heading" : "4 Refined Gauss-Southwell Analysis",
      "text" : "The deficiency of the existing GS analysis is that too much is lost when we use the inequality in (6). To avoid the need to use this inequality, we instead measure strong-convexity in the 1-norm, i.e.,\nf(y) ≥ f(x) + 〈∇f(x), y − x〉+ µ1 2 ‖y − x‖21,\nwhich is the analogue of (3). Minimizing both sides with respect to y, we obtain\nf(x∗) ≥ f(x)− sup y {〈−∇f(x), y − x〉 − µ1 2 ‖y − x‖21}\n= f(x)− (µ1\n2 ‖ · ‖21\n)∗ (−∇f(x))\n= f(x)− 1 2µ1 ‖∇f(x)‖2∞,\n(8)\nwhich makes use of the convex conjugate (µ12 ‖ · ‖ 2 1) ∗ = 12µ1 ‖ · ‖ 2 ∞ [Boyd and Vandenberghe, 2004, §3.3]. Using (8) in (2), and the fact that (∇ikf(xk))2 = ‖∇f(xk)‖2∞ for the GS rule, we obtain\nf(xk+1)− f(x∗) ≤ (\n1− µ1 L\n) [f(xk)− f(x∗)]. (9)\nIt is evident that if µ1 = µ/n, then the rates implied by (5) and (9) are identical, but (9) is faster if µ1 > µ/n. In Appendix B, we show that the relationship between µ and µ1 can be obtained through the relationship between the squared norms || · ||2 and || · ||21. In particular, we have\nµ n ≤ µ1 ≤ µ.\nThus, at one extreme the GS rule obtains the same rate as uniform selection (µ1 ≈ µ/n). However, at the other extreme, it could be faster than uniform selection by a factor of n (µ1 ≈ µ). This analysis, that the GS rule only obtains the same bound as random selection in an extreme case, supports the better practical behaviour of GS."
    }, {
      "heading" : "4.1 Comparison for Separable Quadratic",
      "text" : "We illustrate these two extremes with the simple example of a quadratic function with a diagonal Hessian ∇2f(x) = diag(()λ1, . . . , λn). In this case,\nµ = min i λi, and µ1 = ( n∑ i=1 1 λi )−1 .\nWe prove the correctness of this formula for µ1 in Appendix C. The parameter µ1 achieves its lower bound when all λi are equal, λ1 = · · · = λn = α > 0, in which case\nµ = α and µ1 = α/n.\nThus, uniform selection does as well as the GS rule if all elements of the gradient change at exactly the same rate. This is reasonable: under this condition, there is no apparent advantage in selecting the coordinate to update in a clever way. Intuitively, one might expect that the favourable case for the Gauss-Southwell rule would be where one λi is much larger than the others. However, in this case, µ1 is again similar to µ/n. To achieve the other extreme, suppose that λ1 = β and λ2 = λ3 = · · · = λn = α with α ≥ β. In this case, we have µ = β and\nµ1 = βαn−1\nαn−1 + (n− 1)βαn−2 =\nβα\nα+ (n− 1)β .\nIf we take α → ∞, then we have µ1 → β, so µ1 → µ. This case is much less intuitive; GS is n times faster than random coordinate selection if one element of the gradient changes much more slowly than the others."
    }, {
      "heading" : "4.2 ‘Working Together’ Interpretation",
      "text" : "In the separable quadratic case above, µ1 is given by the harmonic mean of the eigenvalues of the Hessian divided by n. The harmonic mean is dominated by its smallest values, and this is why having one small value is a notable case. Furthermore, the harmonic mean divided by n has an interpretation in terms of processes ‘working together’ [Ferger, 1931]. If each λi represents the time taken by each process to finish a task (e.g., large values of λi correspond to slow workers), then µ is the time needed by the fastest worker to complete the task, and µ1 is the time needed to complete the task if all processes work together (and have independent effects). Using this interpretation, the GS rule provides the most benefit over random selection when working together is not efficient, meaning that if the n processes work together, then the task is not solved much faster than if the fastest worker performed the task alone. This gives an interpretation of the non-intuitive scenario where GS provides the most benefit: if all workers have the same efficiency, then working together solves the problem n times faster. Similarly, if there is one slow worker (large λi), then the problem is solved roughly n times faster by working together. On the other hand, if most workers are slow (many large λi), then working together has little benefit."
    }, {
      "heading" : "4.3 Fast Convergence with Bias Term",
      "text" : "Consider the standard linear-prediction framework,\nargmin x,β m∑ i=1 f(aTi x+ β) + λ 2 ‖x‖2 + σ 2 β2,\nwhere we have included a bias variable β (an example of problem h1). Typically, the regularization parameter σ of the bias variable is set to be much smaller than the regularization parameter λ of the other covariates, to avoid biasing against a global shift in the predictor. Assuming that there is no hidden strong-convexity in the sum, this problem has the structure described in the previous section (µ1 ≈ µ) where GS has the most benefit over random selection."
    }, {
      "heading" : "5 Rates with Different Lipschitz Constants",
      "text" : "Consider the more general scenario where we have a Lipschitz constant Li for the partial derivative of f with respect to each coordinate i,\n|∇if(x+ αei)−∇if(x)| ≤ Li|α|, ∀x ∈ Rn and α ∈ R,\nand we use a coordinate-dependent step-size at each iteration:\nxk+1 = xk − 1 Lik ∇ikf(xk)eik . (10)\nBy the logic of (2), in this setting we have\nf(xk+1) ≤ f(xk)− 1 2Lik [∇ikf(xk)]2, (11)\nand thus a convergence rate of\nf(xk)− f(x∗) ≤  k∏ j=1 ( 1− µ1 Lij ) [f(x0)− f(x∗)]. (12) Noting that L = maxi{Li}, we have\nk∏ j=1 ( 1− µ1 Lij ) ≤ ( 1− µ1 L )k . (13)\nThus, the convergence rate based on the Li will be faster, provided that at least one iteration chooses an ik with Lik < L. In the worst case, however, (13) holds with equality even if the Li are distinct, as we might need to update a coordinate with Li = L on every iteration. (For example, consider a separable function where all but one coordinate is initialized at its optimal value, and the remaining coordinate has Li = L.) In Section 6, we discuss selection rules that incorporate the Li to achieve faster rates whenever the Li are distinct, but first we consider the effect of exact coordinate optimization on the choice of the Lik ."
    }, {
      "heading" : "5.1 Gauss-Southwell with Exact Optimization",
      "text" : "For problems involving functions of the form h1 and h2, we are often able to perform exact (or numerically very precise) coordinate optimization, even if the objective function is not quadratic (e.g., by using a linesearch or a closed-form update). Note that (12) still holds when using exact coordinate optimization rather than using a step-size of 1/Lik , as in this case we have\nf(xk+1) = min α {f(xk + αeik)} ≤ f ( xk − 1\nLik ∇iif(xk)eik ) ≤ f(xk)− 1\n2Lik [∇ikf(xk)]2,\n(14)\nwhich is equivalent to (11). However, in practice using exact coordinate optimization leads to better performance. In this section, we show that using the GS rule results in a convergence rate that is indeed faster than (9) for problems with distinct Li when the function is quadratic, or when the function is not quadratic but we perform exact coordinate optimization.\nThe key property we use is that, after we have performed exact coordinate optimization, we are guaranteed to have∇ikf(xk+1) = 0. Because the GS rule chooses ik+1 = argmaxi |∇if(xk+1)|, we cannot have ik+1 = ik,\nunless xk+1 is the optimal solution. Hence, we never choose the same coordinate twice in a row, which guarantees that the inequality (13) is strict (with distinct Li) and exact coordinate optimization is faster. We note that the improvement may be marginal, as we may simply alternate between the two largest Li values. However, consider minimizing h2 when the graph is sparse; after updating ik, we are guaranteed to have ∇ikf(xk+m) = 0 for all future iterations (k+m) until we choose a variable ik+m−1 that is a neighbour of node ik in the graph. Thus, if the two largest Li are not connected in the graph, GS cannot simply alternate between the two largest Li.\nBy using this property, in Appendix D we show that the GS rule with exact coordinate optimization for problem h2 under a chain-structured graph has a convergence rate of the form\nf(xk)− f(x∗) ≤ O ( max{ρG2 , ρG3 }k ) [f(x0)− f(x∗)],\nwhere ρG2 is the maximizer of √\n(1− µ1/Li)(1− µ1/Lj) among all consecutive nodes i and j in the chain, and ρG3 is the maximizer of\n3 √\n(1− µ1/Li)(1− µ1/Lj)(1− µ1/Lk) among consecutive nodes i, j, and k. The implication of this result is that, if the large Li values are more than two edges from each other in the graph, then we obtain a much better convergence rate. We conjecture that for general graphs, we can obtain a bound that depends on the largest value of ρG2 among all nodes i and j connected by a path of length 1 or 2. Note that we can obtain similar results for problem h1, by forming a graph that has an edge between nodes i and j whenever the corresponding variables are both jointly non-zero in at least one row of A."
    }, {
      "heading" : "6 Rules Depending on Lipschitz Constants",
      "text" : "If the Li are known, Nesterov [2012] showed that we can obtain a faster convergence rate by sampling proportional to the Li. We review this result below and compare it to the GS rule, and then propose an improved GS rule for this scenario. Although in this section we will assume that the Li are known, this assumption can be relaxed using a backtracking procedure [Nesterov, 2012, §6.1]."
    }, {
      "heading" : "6.1 Lipschitz Sampling",
      "text" : "Taking the expectation of (11) under the distribution pi = Li/ ∑n j=1 Lj and proceeding as before, we obtain\nE[f(xk+1)]− f(x∗) ≤ (\n1− µ nL̄\n) [f(xk)− f(x∗)],\nwhere L̄ = 1n ∑n j=1 Lj is the average of the Lipschitz constants. This was shown by Leventhal and Lewis [2010] and is a special case of Nesterov [2012, Theorem 2] with α = 1 in his notation. This rate is faster than (5) for uniform sampling if any Li differ.\nUnder our analysis, this rate may or may not be faster than (9) for the GS rule. On the one extreme, if µ1 = µ/n and any Li differ, then this Lipschitz sampling scheme is faster than our rate for GS. Indeed, in the context of the problem from Section 4.1, we can make Lipschitz sampling faster than GS by a factor of nearly n by making one λi much larger than all the others (recall that our analysis shows no benefit to the GS rule over randomized selection when only one λi is much larger than the others). At the other extreme, in our example from Section 4.1 with many large α and one small β, the GS and Lipschitz sampling rates are the same when n = 2, with a rate of (1 − β/(α + β)). However, the GS rate will be faster than the Lipschitz sampling rate for any α > β when n > 2, as the Lipschitz sampling rate is (1− β/((n− 1)α+ β)), which is slower than the GS rate of (1− β/(α+ (n− 1)β))."
    }, {
      "heading" : "6.2 Gauss-Southwell-Lipschitz Rule",
      "text" : "Since neither Lipschitz sampling nor GS dominates the other in general, we are motivated to consider if faster rules are possible by combining the two approaches. Indeed, we obtain a faster rate by choosing the\nik that minimizes (11), leading to the rule\nik = argmax i |∇if(xk)|√ Li ,\nwhich we call the Gauss-Southwell-Lipschitz (GSL) rule. Following a similar argument to Section 4, but using (11) in place of (2), the GSL rule obtains a convergence rate of\nf(xk+1)− f(x∗) ≤ (1− µL)[f(xk)− f(x∗)],\nwhere µL is the strong-convexity constant with respect to the norm ‖x‖L = ∑n i=1 √ Li|xi|. This is shown in Appendix E, and in Appendix F we show that\nmax { µ nL̄ , µ1 L } ≤ µL ≤ µ1 mini{Li} .\nThus, the GSL rule is always at least as fast as the fastest of the GS rule and Lipschitz sampling. Indeed, it can be more than a factor of n faster than using Lipschitz sampling, while it can obtain a rate closer to the minimum Li, instead of the maximum Li that the classic GS rule depends on.\nAn interesting property of the GSL rule for quadratic functions is that it is the optimal myopic coordinate update. That is, if we have an oracle that can choose the coordinate and the step-size that decreases f by the largest amount, i.e.,\nf(xk+1) = argmin i,α {f(xk + αei)}, (15)\nthis is equivalent to using the GSL rule and the update in (10). This follows because (11) holds with equality in the quadratic case, and the choice αk = 1/Lik yields the optimal step-size. Thus, although faster schemes could be possible with non-myopic strategies that cleverly choose the sequence of coordinates or step-sizes, if we can only perform one iteration, then the GSL rule cannot be improved.\nFor general f , (15) is known as the maximum improvement (MI) rule. This rule has been used in the context of boosting [Rätsch et al., 2001], graphical models [Della Pietra et al., 1997, Lee et al., 2006, Scheinberg and Rish, 2009], Gaussian processes [Bo and Sminchisescu, 2008], and low-rank tensor approximations [Li et al., 2015]. Using an argument similar to (14), our GSL rate also applies to the MI rule, improving existing bounds on this strategy. However, the GSL rule is much cheaper and does not require any special structure (recall that we can estimate Li as we go)."
    }, {
      "heading" : "6.3 Connection between GSL Rule and Normalized Nearest Neighbour Search",
      "text" : "Dhillon et al. [2011] discuss an interesting connection between the GS rule and the nearest-neighbour-search (NNS) problem for objectives of the form\nmin x∈IRn F (x) = f(Ax), (16)\nThis is a special case of h1 with no gi functions, and its gradient has the special form\n∇F (x) = AT r(x),\nwhere r(x) = ∇f(Ax). We use the symbol r because it is the residual vector (Ax− b) in the special case of least squares. For this problem structure the GS rule has the form\nik = argmax i |∇if(xk)|\n= argmax i |r(xk)Tai|,\nwhere ai denotes column i of A for i = 1, . . . , n. Dhillon et al. [2011] propose to approximate the above argmax by solving the following NNS problem\nik = argmin i∈[2n]\n‖r(xk)− ai‖,\nwhere i in the range (n+ 1) through 2n refers to the negation −(ai−n) of column (i− n) and if the selected ik is greater than n we return (i− n). We can justify this approximation using the logic\nik = argmin i∈[2n]\n‖r(xk)− ai‖\n= argmin i∈[2n]\n1 2 ‖r(xk)− ai‖2\n= argmin i∈[2n]\n1 2 ‖r(xk)‖2︸ ︷︷ ︸ constant −r(xk)Tai + 1 2 ‖ai‖2\n= argmax i∈[2n]\nr(xk)Tai − 1\n2 ‖ai‖2\n= argmax i∈[n]\n|r(xk)Tai| − 1\n2 ‖ai‖2\n= argmax i∈[n]\n|∇if(xk)| − 1\n2 ‖ai‖2.\nThus, the NNS computes an approximation to the GS rule that is biased towards coordinates where ‖ai‖ is small. Note that this formulation is equivalent to the GS rule in the special case that ‖ai‖ = 1 (or any other constant) for all i. Shrivastava and Li [2014] have more recently considered the case where ‖ai‖ ≤ 1 and incorporate powers of ‖ai‖ in the NNS to yield a better approximation.\nA further interesting property of the GSL rule is that we can often formulate the exact GSL rule as a normalized NNS problem. In particular, for problem (16) the Lipschitz constants will often have the form Li = γ‖ai‖2 for a some positive scalar γ. For example, least squares has γ = 1 and logistic regression has γ = 0.25. When the Lipschitz constants have this form, we can compute the exact GSL rule by solving a normalized NNS problem,\nik = argmin i∈[2n] ∣∣∣∣∣∣∣∣r(xk)− ai‖ai‖ ∣∣∣∣∣∣∣∣ . (17)\nThe exactness of this formula follows because\nik = argmin i∈[2n] ∣∣∣∣∣∣∣∣r(xk)− ai‖ai‖ ∣∣∣∣∣∣∣∣\n= argmin i∈[2n]\n1 2 ‖r(xk)− ai/‖ai‖‖2\n= argmin i∈[2n]\n1 2 ‖r(xk)‖2︸ ︷︷ ︸ constant −r(x k)Tai ‖ai‖ + 1 2 ‖ai‖2 ‖ai‖2︸ ︷︷ ︸ constant\n= argmax i∈[n] |r(xk)Tai| ‖ai‖\n= argmax i∈[n] |r(xk)Tai|√ γ‖ai‖\n= argmax i∈[n] |∇if(xk)|√ Li .\nThus, the form of the Lipschitz constant conveniently removes the bias towards smaller values of ‖ai‖ that gets introduced when we try to formulate the classic GS rule as a NNS problem. Interestingly, in this setting we do not need to know γ to implement the GSL rule as a NNS problem."
    }, {
      "heading" : "7 Approximate Gauss-Southwell",
      "text" : "In many applications, computing the exact GS rule is too inefficient to be of any practical use. However, a computationally cheaper approximate GS rule might be available. Approximate GS rules under multiplicative and additive errors were considered by Dhillon et al. [2011] in the convex case, but in this setting the convergence rate is similar to the rate achieved by random selection. In this section, we give rates depending on µ1 for approximate GS rules."
    }, {
      "heading" : "7.1 Multiplicative Errors",
      "text" : "In the multiplicative error regime, the approximate GS rule chooses an ik satisfying\n|∇ikf(xk)| ≥ ‖∇f(xk)‖∞(1− k),\nfor some k ∈ [0, 1). In this regime, our basic bound on the progress (2) still holds, as it was defined for any ik. We can incorporate this type of error into our lower bound (8) to obtain\nf(x∗) ≥ f(xk)− 1 2µ1 ‖∇f(xk)‖2∞\n≥ f(xk)− 1 2µ1(1− k)2 |∇ikf(xk)|2.\nThis implies a convergence rate of f(xk+1)− f(x∗) ≤ ( 1− µ1(1− k) 2\nL\n) [f(xk)− f(x∗)].\nThus, the convergence rate of the method is nearly identical to using the exact GS rule for small k (and it degrades gracefully with k). This is in contrast to having an error in the gradient [Friedlander and Schmidt, 2012], where the error must decrease to zero over time."
    }, {
      "heading" : "7.2 Additive Errors",
      "text" : "In the additive error regime, the approximate GS rule chooses an ik satisfying\n|∇ikf(xk)| ≥ ‖∇f(xk)‖∞ − k,\nfor some k ≥ 0. In Appendix G, we show that under this rule, we have\nf(xk)− f(x∗) ≤ (\n1− µ1 L\n)k [ f(x0)− f(x∗) +Ak ] ,\nwhere\nAk ≤ min { k∑ i=1 ( 1− µ1 L )−i i √ 2L1 L √ f(x0)− f(x∗), k∑ i=1 ( 1− µ1 L )−i( i √ 2 L √ f(x0)− f(x∗) + 2 i 2L )} ,\nwhere L1 is the Lipschitz constant of ∇f with respect to the 1-norm. Note that L1 could be substantially larger than L, so the second part of the maximum in Ak is likely to be the smaller part unless the i are large. This regime is closer to the case of having an error in the gradient, as to obtain convergence the k\nmust decrease to zero. This result implies that a sufficient condition for the algorithm to obtain a linear convergence rate is that the errors k converge to zero at a linear rate. Further, if the errors satisfy k = O(ρ\nk) for some ρ < (1 − µ1/L), then the convergence rate of the method is the same as if we used an exact GS rule. On the other hand, if k does not decrease to zero, we may end up repeatedly updating the same wrong coordinate and the algorithm will not converge (though we could switch to the randomized method if this is detected)."
    }, {
      "heading" : "8 Proximal-Gradient Gauss-Southwell",
      "text" : "One of the key motivations for the resurgence of interest in coordinate descent methods is their performance on problems of the form\nmin x∈Rn F (x) ≡ f(x) + n∑ i=1 gi(xi),\nwhere f is smooth and convex and the gi are convex, but possibly non-smooth. This includes problems with `1-regularization, and optimization with lower and/or upper bounds on the variables. Similar to proximalgradient methods, we can apply the proximal operator to the coordinate update,\nxk+1 = prox 1 L gik\n[ xk − 1\nL ∇ikf(xk)eik\n] ,\nwhere\nproxαgi [y] = argmin x∈Rn\n1 2 ‖x− y‖2 + αgi(x).\nWith random coordinate selection, Richtárik and Takáč [2014] show that this method has a convergence rate of E[F (xk+1)− F (x∗)] ≤ (\n1− µ nL\n) [F (xk)− F (x∗)],\nsimilar to the unconstrained/smooth case. There are several generalizations of the GS rule to this scenario. Here we consider three possibilities, all of which are equivalent to the GS rule if the gi are not present. First, the GS-s rule chooses the coordinate with the most negative directional derivative. This strategy is popular for `1-regularization [Shevade and Keerthi, 2003, Wu and Lange, 2008, Li and Osher, 2009] and in general is given by [see Bertsekas, 1999, §8.4]\nik = argmax i { min s∈∂gi |∇if(xk) + s| } .\nHowever, the length of the step (‖xk+1 − xk‖) could be arbitrarily small under this choice. In contrast, the GS-r rule chooses the coordinate that maximizes the length of the step [Tseng and Yun, 2009, Dhillon et al., 2011],\nik = argmax i\n{∣∣∣∣xki − prox 1L gi [ xki − 1 L ∇if(xk) ]∣∣∣∣} . This rule is effective for bound-constrained problems, but it ignores the change in the non-smooth term (gi(x k+1 i )−gi(xkk)). Finally, the GS-q rule maximizes progress assuming a quadratic upper bound on f [Tseng and Yun, 2009],\nik = argmin i { min d { f(xk) +∇if(xk)d+ L 2 d2 + gi(x k i + d)− gi(xki ) }} .\nWhile the least intuitive rule, the GS-q rule seems to have the best theoretical properties. Further, if we use Li in place of L in the GS-q rule (which we call the GSL-q strategy), then we obtain the GSL rule if the gi\nare not present. In contrast, using Li in place of L in the GS-r rule (which we call the GSL-r strategy) does not yield the GSL rule as a special case.\nIn Appendix H, we show that using the GS-q rule yields a convergence rate of F (xk+1)− F (x∗) ≤ min {(\n1− µ Ln\n) [f(xk)− f(x∗)], ( 1− µ1\nL\n) [f(xk)− f(x∗)] + k } ,\nwhere k is bounded above by a measure of the non-linearity of the gi along the possible coordinate updates times the inverse condition number µ1/L. Note that k goes to zero as k increases and we conjecture that the above bound actually holds with k = 0. In contrast, in Appendix H we also give counter-examples showing that the above rate does not hold with k = 0 for the GS-s or GS-r rule, even if you replace the minimum by a maximum. Thus, any bound for the GS-s or GS-r rule would be slower than the expected rate under random selection, while the GS-q rule leads to a better bound."
    }, {
      "heading" : "9 Experiments",
      "text" : "We first compare the efficacy of different coordinate selection rules on the following simple instances of h1. `2-regularized sparse least squares: Here we consider the problem\nmin x\n1 2n ‖Ax− b‖2 + λ 2 ‖x‖2,\nan instance of problem h1. We set A to be an m by n matrix with entries sampled from a N (0, 1) distribution (with m = 1000 and n = 1000). We then added 1 to each entry (to induce a dependency between columns), multiplied each column by a sample from N (0, 1) multiplied by ten (to induce different Lipschitz constants across the coordinates), and only kept each entry of A non-zero with probability 10 log(n)/n (a sparsity level that allows the Gauss-Southwell rule to be applied with cost O(log3(n)). We set λ = 1 and b = Ax + e, where the entries of x and e were drawn from a N (0, 1) distribution. In this setting, we used a step-size of 1/Li for each coordinate i, which corresponds to exact coordinate optimization.\n`2-regularized sparse logistic regression: Here we consider the problem\nmin x\n1\nn n∑ i=1 log(1 + exp(−biaTi x)) + λ 2 ‖x‖2.\nWe set the aTi to be the rows of A from the previous problem, and set b = sign(Ax), but randomly flipping each bi with probability 0.1. In this setting, we compared using a step-size of 1/Li to using exact coordinate optimization.\nOver-determined dense least squares: Here we consider the problem\nmin x\n1\n2n ‖Ax− b‖2,\nbut, unlike the previous case, we do not set elements of A to zero and we make A have dimension 1000 by 100. Because the system is over-determined, it does not need an explicit strongly-convex regularizer to induce global strong-convexity. In this case, the density level means that the exact GS rule is not efficient. Hence, we use a balltree structure [Omohundro, 1989] to implement an efficient approximate GS rule based on the connection to the NNS problem discovered by Dhillon et al. [2011]. On the other hand, we can compute the exact GSL rule for this problem as a NNS problem as discussed in Section 6.3.\n`1-regularized underdetermined sparse least squares: Here we consider the non-smooth problem\nmin x\n1\n2n ‖Ax− b‖2 + λ‖x‖1.\nWe generate A as we did for the `2-regularized sparse least squares problem, except with the dimension 1000 by 10000. This problem is not globally strongly-convex, but will be strongly-convex along the dimensions that are non-zero in the optimal solution.\nWe plot the objective function (divided by its initial value) of coordinate descent under different selection rules in Figure 1. Even on these simple datasets, we see dramatic differences in performance between the different strategies. In particular, the GS rule outperforms random coordinate selection (as well as cyclic selection) by a substantial margin in all cases. The Lipschitz sampling strategy can narrow this gap, but it remains large (even when an approximate GS rule is used). The difference between GS and randomized selection seems to be most dramatic for the `1-regularized problem; the GS rules tend to focus on the nonzero variables while most randomized/cyclic updates focus on the zero variables, which tend not to move away from zero.3 Exact coordinate optimization and using the GSL rule seem to give modest but consistent improvements. The three non-smooth GS-∗ rules had nearly identical performance despite their different theoretical properties. The GSL-q rule gave better performance than the GS-∗ rules, while the the GSL-r variant performed worse than even cyclic and random strategies. We found it was also possible to make the GS-s rule perform poorly by perturbing the initialization away from zero. While these experiments plot the performance in terms of the number of iterations, in Appendix I we show that the GS-∗ rules can also be advantageous in terms of runtime.\nWe next consider an instance of problem h2, performing label propagation for semi-supervised learning in the ‘two moons’ dataset [Zhou et al., 2004]. We generate 500 samples from this dataset, randomly label five points in the data, and connect each node to its five nearest neighbours. This high level of sparsity is typical of graph-based methods for semi-supervised learning, and allows the exact Gauss-Southwell rule to be implemented efficiently. We use the quadratic labeling criterion of Bengio et al. [2006], which allows exact coordinate optimization and is normally optimized with cyclic coordinate descent. We plot the performance under different selection rules in Figure 2. Here, we see that even cyclic coordinate descent outperforms randomized coordinate descent, but that the GS and GSL rules give even better performance. We note that the GS and GSL rules perform similarly on this problem since the Lipschitz constants do not vary much."
    }, {
      "heading" : "10 Discussion",
      "text" : "It is clear that the GS rule is not practical for every problem where randomized methods are applicable. Nevertheless, we have shown that even approximate GS rules can obtain better convergence rate bounds than fully-randomized methods. We have given a similar justification for the use of exact coordinate optimization,\n3To reduce the cost of the GS-s method in this context, Shevade and Keerthi [2003] consider a variant where we first compute the GS-s rule for the non-zero variables and if an element is sufficiently large then they do not consider the zero variables.\nand we note that our argument could also be used to justify the use of exact coordinate optimization within randomized coordinate descent methods (as used in our experiments). We have also proposed the improved GSL rule, and considered approximate/proximal variants. We expect our analysis also applies to block updates by using mixed norms ‖ · ‖p,q, and could be used for accelerated/parallel methods [Fercoq and Richtárik, 2013], for primal-dual rates of dual coordinate ascent [Shalev-Shwartz and Zhang, 2013], for successive projection methods [Leventhal and Lewis, 2010], for boosting algorithms [Rätsch et al., 2001], and for scenarios without strong-convexity under general error bounds [Luo and Tseng, 1993]."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We would like to thank the anonymous referees for their useful comments that significantly improved the paper. Julie Nutini is funded by an NSERC Canada Graduate Scholarship."
    }, {
      "heading" : "Appendix A Efficient calculation of GS rules for sparse problems",
      "text" : "We first give additional details on how to calculate the GS rule efficiently for sparse instances of problems h1 and h2. We will consider the case where each gi is smooth, but the ideas can be extended to allow a non-smooth gi. Further, note that the efficient calculation does not rely on convexity, so these strategies can also be used for non-convex problems.\nA.1 Problem h2\nProblem h2 has the form\nh2(x) := ∑ i∈V gi(xi) + ∑ (i,j)∈E fij(xi, xj),\nwhere each gi and fij are differentiable and G = {V,E} is a graph where the number of vertices |V | is the same as the number of variables n. If all nodes in the graph have a degree (number of neighbours) bounded above by some constant d, we can implement the GS rule in O(d log n) after an O(n+ |E|) time initialization by maintaining the following information about xk:\n1. A vector containing the values ∇igi(xki ).\n2. A matrix containing the values∇ifij(xki , xkj ) in the first column and∇jfij(xki , xkj ) in the second column.\n3. The elements of the gradient vector ∇h2(xk) stored in a binary max heap data structure [see Cormen et al., 2001, Chapter 6].\nGiven the heap structure, we can compute the GS rule in O(1) by simply reading the index value of the root node in the max heap. The costs for initializing these structures are:\n1. O(n) to compute gi(x 0 i ) for all n nodes.\n2. O(|E|) to compute ∇ijfij(x0i , x0j ) for all |E| edges.\n3. O(n + |E|) to sum the values in the above structures to compute ∇h(x0), and O(n) to construct the initial max heap.\nThus, the one-time initialization cost is O(n + |E|). The costs of updating the data structures after we update xkik to x k+1 ik for the selected coordinate ik are:\n1. O(1) to compute gik(x k+1 ik ).\n2. O(d) to compute ∇ijfij(xk+1i , x k+1 j ) for (i, j) ∈ E and i = ik or j = ik (only d such values exist by\nassumption, and all other ∇ijfij(xi, xj) are unchanged).\n3. O(d) to update up to d elements of ∇h(xk+1) that differ from ∇h(xk) by using differences in changed values of gi and fij , followed by O(d log n) to perform d updates of the heap at a cost of O(log n) for each update.\nThe most expensive part of the update is modifying the heap, and thus the total cost is O(d log n).4\nA.2 Problem h1\nProblem h1 has the form\nh1(x) := n∑ i=1 gi(xi) + f(Ax),\nwhere gi and f are differentiable, and A is an m by n matrix where we denote column i by ai and row j by aTj . Note that f is a function from IR\nm to IR, and we assume ∇jf only depends on aTj x. While this is a strong assumption (e.g., it rules out f being the product function), this class includes a variety of notable problems like the least squares and logistic regression models from our experiments. If A has z non-zero elements, with a maximum of c non-zero elements in each column and r non-zero elements in each row, then with a pre-processing cost of O(z) we can implement the GS rule in this setting in O(cr log n) by maintaining the following information about xk:\n1. A vector containing the values ∇igi(xki ).\n2. A vector containing the product Axk.\n3. A vector containing the values ∇f(Axk).\n4. A vector containing the product AT∇f(Axk).\n5. The elements of the gradient vector ∇h1(xk) stored in a binary max heap data structure.\nThe heap structure again allows us to compute the GS rule in O(1), and the costs of initializing these structures are:\n1. O(n) to compute gi(x 0 i ) for all n variables.\n2. O(z) to compute the product Ax0.\n3. O(m) to compute ∇f(Ax0) (using that ∇jf only depends on aTj x0).\n4. O(z) to compute AT∇f(Ax0).\n5. O(n) to add the ∇igi(x0i ) to the above product to obtain ∇h1(x0) and construct the initial max heap.\nAs it is reasonable to assume that z ≥ m and z ≥ n (e.g., we have at least one non-zero in each row and column), the cost of the initialization is thus O(z). The costs of updating the data structures after we update xkik to x k+1 ik for the selected coordinate ik are:\n1. O(1) to compute gik(x k+1 ik ).\n2. O(c) to update the product using Axk+1 = Axk + (xk+1ik − x k ik\n)ai, since ai has at most c non-zero values.\n3. O(c) to update up to c elements of ∇f(Axk+1) that have changed (again using that ∇jf only depends on aTj x k+1).\n4For less-sparse problems where n < d logn, using a heap is actually inefficient and we should simply store ∇h(xk) as a vector. The initialization cost is the same, but we can then perform the GS rule in O(n) by simply searching through the vector for the maximum element.\n4. O(cr) to perform up to c updates of the form AT∇f(Axk+1) = AT∇f(Axk) + (∇jf(Axk+1) − ∇jf(Axk))(ai)T , where each update costs O(r) since each ai has at most r non-zero values.\n5. O(cr log n) to update the gradients in the heap.\nThe most expensive part is again the heap update, and thus the total cost is O(cr log n)."
    }, {
      "heading" : "Appendix B Relationship between µ1 and µ",
      "text" : "We can establish the relationship between µ and µ1 by using the known relationship between the 2-norm and the 1-norm,\n‖x‖1 ≥ ‖x‖ ≥ 1√ n ‖x‖1.\nIn particular, if we assume that f is µ-strongly convex in the 2-norm, then for all x and y we have\nf(y) ≥ f(x) + 〈∇f(x), y − x〉+ µ 2 ‖y − x‖2\n≥ f(x) + 〈∇f(x), y − x〉+ µ 2n ‖y − x‖21,\nimplying that f is at least µn -strongly convex in the 1-norm. Similarly, if we assume that a given f is µ1-strongly convex in the 1-norm then for all x and y we have\nf(y) ≥ f(x) + 〈∇f(x), y − x〉+ µ1 2 ‖y − x‖21\n≥ f(x) + 〈∇f(x), y − x〉+ µ1 2 ‖y − x‖2,\nimplying that f is at least µ1-strongly convex in the 2-norm. Summarizing these two relationships, we have\nµ n ≤ µ1 ≤ µ."
    }, {
      "heading" : "Appendix C Analysis for separable quadratic case",
      "text" : "We first establish an equivalent definition of strong-convexity in the 1-norm, along the lines of Nesterov [2004, Theorem 2.1.9]. Subsequently, we use this equivalent definition to derive µ1 for a separable quadratic function.\nC.1 Equivalent definition of strong-convexity\nAssume that f is µ1-strongly convex in the 1-norm, so that for any x, y ∈ IRn we have\nf(y) ≥ f(x) + 〈∇f(x), y − x〉+ µ1 2 ‖y − x‖21.\nReversing x and y in the above gives\nf(x) ≥ f(y) + 〈∇f(y), x− y〉+ µ1 2 ‖x− y‖21,\nand adding these two together yields\n〈∇f(y)−∇f(x), y − x〉 ≥ µ1‖y − x‖21. (18)\nConversely, assume that for all x and y we have\n〈∇f(y)−∇f(x), y − x〉 ≥ µ1‖y − x‖21,\nand consider the function g(τ) = f(x+ τ(y − x)) for τ ∈ IR. Then\nf(y)− f(x)− 〈∇f(x), y − x〉 = g(1)− g(0)− 〈∇f(x), y − x〉\n= ∫ 1 0 dg dτ (τ)− 〈∇f(x), y − x〉 dτ\n= ∫ 1 0 〈∇f(x+ τ(y − x)), y − x〉 − 〈∇f(x), y − x〉 dτ\n= ∫ 1 0 〈∇f(x+ τ(y − x))−∇f(x), y − x〉 dτ\n≥ ∫ 1 0 µ1 τ ‖τ(y − x)‖21 dτ\n= ∫ 1 0 µ1τ‖y − x‖21 dτ\n= µ1 2 τ2‖y − x‖21 ∣∣∣∣1 0 = µ1 2 ‖y − x‖21.\nThus, µ1-strong convexity in the 1-norm is equivalent to having\n〈∇f(y)−∇f(x), y − x〉 ≥ µ1‖y − x‖21 ∀ x, y. (19)\nC.2 Strong-convexity constant µ1 for separable quadratic functions\nConsider a strongly convex quadratic function f with a diagonal Hessian H = ∇2f(x) = diag(λ1, . . . , λn), where λi > 0 for all i = 1, . . . , n. We show that in this case\nµ1 = ( n∑ i=1 1 λi )−1 .\nFrom the previous section, µ1 is the minimum value such that (19) holds,\nµ1 = inf x 6=y 〈∇f(y)−∇f(x), y − x〉 ‖y − x‖21 .\nUsing ∇f(x) = Hx+ b for some b and letting z = y − x, we get\nµ1 = inf x6=y 〈(Hy − b)− (Hx− b), y − x〉 ‖y − x‖21\n= inf x6=y 〈H(y − x), y − x〉 ‖y − x‖21\n= inf z 6=0\nzTHz\n‖z‖21 = min ‖z‖1=1 zTHz\n= min eT z=1 n∑ i=1 λiz 2 i ,\nwhere the last two lines use that the objective is invariant to scaling of z and to the sign of z (respectively), and where e is a vector containing a one in every position. This is an equality-constrained strictly-convex\nquadratic program, so its solution is given as a stationary point (z∗, η∗) of the Lagrangian,\nΛ(z, η) = n∑ i=1 λiz 2 i + η(1− eT z).\nDifferentiating with respect to each zi for i = 1, . . . , n and equating to zero, we have for all i that 2λiz ∗ i −η∗ = 0, or\nz∗i = η∗\n2λi . (20)\nDifferentiating the Lagrangian with respect to η and equating to zero we obtain 1−eT z∗ = 0, or equivalently\n1 = eT z∗ = η∗\n2 ∑ j 1 λj ,\nwhich yields\nη∗ = 2 ∑ j 1 λj −1 . Combining this result for η∗ with equation (20), we have\nz∗i = 1\nλi ∑ j 1 λj −1 . This gives the minimizer, so we evaluate the objective at this point to obtain µ1,\nµ1 = n∑ i=1 λi(z ∗ i ) 2\n= n∑ i=1 λi  1 λi  n∑ j=1 1 λj −1  2\n= n∑ i=1 1 λi  n∑ j=1 1 λj −2\n=  n∑ j=1 1 λj −2( n∑ i=1 1 λi )\n=  n∑ j=1 1 λj −1 ."
    }, {
      "heading" : "Appendix D Gauss-Southwell with exact optimization",
      "text" : "We can obtain a faster convergence for GS using exact coordinate optimization for sparse variants of problems h1 and h2, by observing that the convergence rate can be expressed in terms of the sequence of (1−µ1/Lik) values,\nf(xk)− f(x∗) ≤  k∏ j=1 ( 1− µ1 Lij ) [f(x0)− f(x∗)].\nThe worst case occurs when the product of the (1 − µ1/Lik) values is as large as possible. However, using exact coordinate optimization guarantees that, after we have updated coordinate i, the GS rule will never select it again until one of its neighbours has been selected. Thus, we can obtain a tighter bound on the worst-case convergence rate using GS with exact coordinate optimization on iteration k, by solving the following combinatorial optimization problem defined on a weighted graph:\nProblem 1. We are given a graph G = (V,E) with n nodes, a number Mi associated with each node i, and an iteration number k. Choose a sequence {it}kt=1 that maximizes the sum of the Mit , subject to the following constraint: after each time node i has been chosen, it cannot be chosen again until after a neighbour of node i has been chosen.\nWe can use the Mi chosen by this problem to obtain an upper-bound on the sequence of log(1−µ1/Li) values, and if the largest Mi values are not close to each other in the graph, then this rate can be much faster than the rate obtained by alternating between the largest Mi values. In the particular case of chain-structured graphs, a worst-case sequence can be constructed that spends all but O(n) iterations in one of two solution modes: (i) alternate between two nodes i and j that are connected by an edge with the highest value of Mi+Mj\n2 , or (ii) alternate between three nodes {i, j, k} with the highest value of Mi+Mj+Mk\n3 , where there is an edge from i to j and from j to k, but not from i to k. To show that these are the two solution modes, observe that the solution must eventually cycle because there are a finite number of nodes. If you have more than three nodes in the cycle, then you can always remove one node from the cycle to obtain a better average weight for the cycle without violating the constraint. We will fall into mode (i) if the average of Mi and Mj in this mode is larger than the average of Mi, Mj and Mk in the second mode. We can construct a solution to this problem that consists of a ‘burn-in’ period, where we choose the largest Mi, followed by repeatedly going through the better of the two solution modes up until the final three steps, where a ‘burn-out’ phase arranges to finish with several large Mi. By setting Mi = log(1− µ1/Li), this leads to a convergence rate of the form\nf(xk)− f(x∗) ≤ O ( max{ρG2 , ρG3 }k ) [f(x0)− f(x∗)],\nwhere ρG2 is the maximizer of √\n(1− µ1/Li)(1− µ1/Lj) among all consecutive nodes i and j in the chain, and ρG3 is the maximizer of\n3 √\n(1− µ1/Li)(1− µ1/Lj)(1− µ1/Lk) among consecutive nodes i, j, and k. The O() notation gives the constant due to choosing higher (1− µ1/Li) values during the burn-in and burn-out periods. The implication of this result is that, if the large Li values are more than two edges away from each other in the graph, then the convergence rate can be much faster."
    }, {
      "heading" : "Appendix E Gauss-Southwell-Lipschitz rule: convergence rate",
      "text" : "The coordinate-descent method with a constant step-size of Lik uses the iteration\nxk+1 = xk − 1 Lik ∇ikf(xk)eik .\nBecause f is coordinate-wise Lik -Lipschitz continuous, we obtain the following bound on the progress made by each iteration:\nf(xk+1) ≤ f(xk) +∇ikf(xk)(xk+1 − xk)ik + Lik 2 (xk+1 − xk)2ik\n= f(xk)− 1 Lik (∇ikf(xk))2 + Lik 2\n[ 1\nLik ∇ikf(xk) ]2 = f(xk)− 1\n2Lik [∇ikf(xk)]2\n= f(xk)− 1 2\n[ ∇ikf(xk)√\nLik\n]2 .\n(21)\nBy choosing the coordinate to update according to the Gauss-Southwell-Lipchitz (GSL) rule,\nik = argmax i |∇if(xk)|√ Li ,\nwe obtain the tightest possible bound on (21). We define the following norm,\n‖x‖L = n∑ i=1 √ Li|xi|, (22)\nwhich has a dual norm of\n‖x‖∗L = max i 1√ Li |xi|.\nUnder this notation, and using the GSL rule, (21) becomes\nf(xk+1) ≤ f(xk)− 1 2\n( ‖∇f(xk)‖∗L )2 ,\nMeasuring strong-convexity in the norm ‖ · ‖L we get\nf(y) ≥ f(x) + 〈∇f(x), y − x〉+ µL 2 ‖y − x‖2L.\nMinimizing both sides with respect to y we get\nf(x∗) ≥ f(x)− sup y {〈−∇f(x), y − x〉 − µL 2 ‖y − x‖2L}\n= f(x)− (µL\n2 ‖ · ‖2L\n)∗ (−∇f(x))\n= f(x)− 1 2µL\n( ‖∇f(x)‖∗L )2 .\nPutting these together yields\nf(xk+1)− f(x∗) ≤ (1− µL)[f(xk)− f(x∗)]. (23)"
    }, {
      "heading" : "Appendix F Comparing µL to µ1 and µ",
      "text" : "By the logic Appendix B, to establish a relationship between different strong-convexity constants under different norms, it is sufficient to establish the relationships between the squared norms. In this section, we use this to establish the relationship between µL defined in (22) and both µ1 and µ.\nF.1 Relationship between µL and µ1\nWe have c‖x‖1 − ‖x‖L = c ∑ i |xi| − ∑ i √ Li|xi| = ∑ i (c− √ Li)|xi|, Assuming c ≥ √ L, where L = maxi{Li}, the expression is non-negative and we get\n‖x‖L ≤ √ L‖x‖1.\nBy using\nc‖x‖L − ‖x‖1 = ∑ i (c √ Li − 1)|xi|,\nand assuming c ≥ 1√ Lmin , where Lmin = mini{Li}, this expression is nonnegative and we get\n‖x‖1 ≤ 1√ Lmin ‖x‖L.\nThe relationship between µL and µ1 is based on the squared norm, so in summary we have\nµ1 L ≤ µL ≤ µ1 Lmin .\nF.2 Relationship between µL and µ Let ~L denote a vector with elements √ Li, and we note that\n‖~L‖ = (∑\ni\n( √ Li) 2 )1/2 = (∑ i Li )1/2 = √ nL̄, where L̄ = 1 n ∑ i Li.\nUsing this, we have\n‖x‖L = xT (sign(x) ◦ ~L) ≤ ‖x‖‖ sign(x) ◦ ~L‖ = √ nL̄‖x‖.\nThis implies that µ\nnL̄ ≤ µL.\nNote that we can also show that µL ≤ µLmin , but this is less tight than the upper bound from the previous section because µ1 ≤ µ."
    }, {
      "heading" : "Appendix G Approximate Gauss-Southwell with additive error",
      "text" : "In the additive error regime, the approximate Gauss-Southwell rule chooses an ik satisfying\n|∇ikf(xk)| ≥ ‖∇f(xk)‖∞ − k, where k ≥ 0 ∀k,\nand we note that we can assume k ≤ ‖∇f(xk)‖∞ without loss of generality because we must always choose an i with |∇ikf(xk)| ≥ 0. Applying this to our bound on the iteration progress, we get\nf(xk+1) ≤ f(xk)− 1 2L\n[ ∇ikf(xk) ]2 ≤ f(xk)− 1\n2L\n( ‖∇f(xk)‖∞ − k )2 = f(xk)− 1\n2L\n( ‖∇f(xk)‖2∞ − 2 k‖∇f(xk)‖∞ + 2k ) = f(xk)− 1\n2L ‖∇f(xk)‖2∞ + k L ‖∇f(xk)‖∞ − 2k 2L\n(24)\nWe first give a result that assumes f is L1-Lipschitz continuous in the 1-norm. This implies an inequality that we prove next, followed by a convergence rate that depends on L1. However, note that L ≤ L1 ≤ Ln, so this potentially introduces a dependency on n. We subsequently give a slightly less concise result that has a worse dependency on but does not rely on L1.\nG.1 Gradient bound in terms of L1\nWe say that ∇f is L1-Lipschitz continuous in the 1-norm if we have for all x and y that\n‖∇f(x)−∇f(y)‖∞ ≤ L1‖x− y‖1.\nSimilar to Nesterov [2004, Theorem 2.1.5], we now show that this implies\nf(y) ≥ f(x) + 〈∇f(x), y − x〉+ 1 2L1 ‖∇f(y)−∇f(x)‖2∞, (25)\nand subsequently that ‖∇f(xk)‖∞ = ‖∇f(xk)−∇f(x∗)‖∞ ≤ √ 2L1(f(xk)− f(x∗)) ≤ √ 2L1(f(x0)− f(x∗)), (26)\nwhere we have used that f(xk) ≤ f(xk−1) for all k and any choice of ik−1 (this follows from the basic bound on the progress of coordinate descent methods).\nWe first show that ∇f being L1-Lipschitz continuous in the 1-norm implies that\nf(y) ≤ f(x) + 〈∇f(x), y − x〉+ L1 2 ‖y − x‖21,\nfor all x and y. Consider the function g(τ) = f(x+ τ(y − x)) with τ ∈ IR. Then\nf(y)− f(x)− 〈∇f(x), y − x〉 = g(1)− g(0)− 〈∇f(x), y − x〉\n= ∫ 1 0 dg dτ (τ)− 〈∇f(x), y − x〉 dτ\n= ∫ 1 0 〈∇f(x+ τ(y − x)), y − x〉 − 〈∇f(x), y − x〉 dτ\n= ∫ 1 0 〈∇f(x+ τ(y − x))−∇f(x), y − x〉 dτ\n≤ ∫ 1 0 ‖∇f(x+ τ(y − x))−∇f(x)‖∞‖y − x‖1 dτ\n≤ ∫ 1 0 L1τ‖y − x‖21 dτ\n= L1 2 τ2‖y − x‖21 ∣∣∣∣1 0 = L1 2 ‖y − x‖21.\nTo subsequently show (25), fix x ∈ IRn and consider the function\nφ(y) = f(y)− 〈∇f(x), y〉,\nwhich is convex on IRn and also has an L1-Lipschitz continuous gradient in the 1-norm, as\n‖φ′(y)− φ′(x)‖∞ = ‖(∇f(y)−∇f(x))− (∇f(x)−∇f(x))‖∞ = ‖∇f(y)−∇f(x)‖∞ ≤ L1‖y − x‖1.\nAs the minimizer of φ is x (i.e., φ′(x) = 0), for any y ∈ IRn we have\nφ(x) = min v φ(v) ≤ min v φ(y) + 〈φ′(y), v − y〉+ L1 2 ‖v − y‖21\n= φ(y)− sup v 〈−φ′(y), v − y〉 − L1 2 ‖v − y‖21 = φ(y)− 1 2L1 ‖φ′(y)‖2∞.\nSubstituting in the definition of φ, we have\nf(x)− 〈∇f(x), x〉 ≤ f(y)− 〈∇f(x), y〉 − 1 2L1 ‖∇f(y)−∇f(x)‖2∞\n⇐⇒ f(x) ≤ f(y) + 〈∇f(x), x− y〉 − 1 2L1 ‖∇f(y)−∇f(x)‖2∞ ⇐⇒ f(y) ≥ f(x) + 〈∇f(x), y − x〉+ 1 2L1 ‖∇f(y)−∇f(x)‖2∞.\nG.2 Additive error bound in terms of L1\nUsing (26) in (24) and noting that k ≥ 0, we obtain\nf(xk+1) ≤ f(xk)− 1 2L ‖∇f(xk)‖2∞ + k L ‖∇f(xk)‖∞ − 2k 2L\n≤ f(xk)− 1 2L ‖∇f(xk)‖2∞ + k L\n√ 2L1(f(x0)− f(x∗))−\n2k 2L\n≤ f(xk)− 1 2L ‖∇f(xk)‖2∞ + k √ 2L1 L\n√ f(x0)− f(x∗).\nApplying strong convexity (taken with respect to the 1-norm), we get\nf(xk+1)− f(x∗) ≤ (\n1− µ1 L\n)[ f(xk)− f(x∗) ] + k √ 2L1 L √ f(x0)− f(x∗),\nwhich implies\nf(xk+1)− f(x∗) ≤ (\n1− µ1 L\n)k[ f(x0)− f(x∗) ] + k∑ i=1 ( 1− µ1 L )k−i i √ 2L1 L √ f(x0)− f(x∗)\n= ( 1− µ1\nL\n)k[ f(x0)− f(x∗) + √ f(x0)− f(x∗)Ak ] ,\nwhere\nAk = √ 2L1 L k∑ i=1 ( 1− µ1 L )−i i.\nG.3 Additive error bound in terms of L\nBy our additive error inequality, we have\n|∇ikf(xk)|+ k ≥ ‖∇f(xk)‖∞.\nUsing this again in (24) we get\nf(xk+1) ≤ f(xk)− 1 2L ‖∇f(xk)‖2∞ + k L ‖∇f(xk)‖∞ − 2k 2L\n≤ f(xk)− 1 2L ‖∇f(xk)‖2∞ + k L\n( |∇ikf(xk)|+ k ) − 2 k\n2L\n= f(xk)− 1 2L ‖∇f(xk)‖2∞ + k L |∇ikf(xk)|+ 2k 2L .\nFurther, from our basic progress bound that holds for any ik we have\nf(x∗) ≤ f(xk+1) ≤ f(xk)− 1 2L\n[ ∇ikf(xk) ]2 ≤ f(x0)− 1\n2L\n[ ∇ikf(xk) ]2 ,\nwhich implies\n|∇ikf(xk)| ≤ √ 2L(f(x0)− f(x∗)).\nand thus that\nf(xk+1) ≤ f(xk)− 1 2L ‖∇f(xk)‖2∞ + k L\n√ 2L(f(x0)− f(x∗)) + 2 k\n2L\n= f(xk)− 1 2L ‖∇f(xk)‖2∞ + k\n√ 2\nL\n√ f(x0)− f(x∗) + 2 k\n2L .\nApplying strong convexity and applying the inequality recursively we obtain\nf(xk+1)− f(x∗) ≤ (\n1− µ1 L\n)k[ f(x0)− f(x∗) ] + k∑ i=1 ( 1− µ1 L )k−i( i √ 2 L √ f(x0)− f(x∗) + 2 i 2L )\n= ( 1− µ1\nL\n)k[ f(x0)− f(x∗) +Ak ] ,\nwhere\nAk = k∑ i=1 ( 1− µ1 L )−i(√ 2 L i √ f(x0)− f(x∗) + 2 i 2L ) .\nAlthough uglier than the expression depending on L1, this expression will tend to be smaller unless k is not small.\nAppendix H Convergence Analysis of GS-s, GS-r, and GS-q Rules\nIn this section, we consider problems of the form\nmin x∈IRn F (x) = f(x) + g(x) = f(x) + n∑ i=1 gi(xi),\nwhere f satisfies our usual assumptions, but the gi can be non-smooth. We first introduce some notation that will be needed to state our result for the GS-q rule, followed by stating the result and then showing that it holds in two parts. We then turn to showing that the rule cannot hold in general for the GS-s and GS-r rules.\nH.1 Notation and basic inequality\nTo analyze this case, an important inequality we will use is that the L-Lipschitz-continuity of ∇if implies that for all x, i, and d that\nF (x+ dei) = f(x+ dei) + g(x+ dei) ≤ f(x) + 〈∇f(x), dei〉+ L\n2 d2 + g(x+ dei)\n= f(x) + g(x) + 〈∇f(x), dei〉+ L\n2 d2 + gi(xi + d)− gi(xi)\n= F (x) + Vi(x, d),\n(27)\nwhere\nVi(x, d) ≡ 〈∇f(x), dei〉+ L\n2 d2 + gi(xi + d)− gi(xi).\nNotice that the GS-q rule is defined by\nik = argmin i {min d Vi(x, d)}.\nWe use the notation dki = argmind Vi(x k, d) and we will use dk to denote the vector containing these values for all i. When using the GS-q rule, the iteration is defined by\nxk+1 = xk + dikeik\n= xk + argmin d {Vik(x, d)}eik .\n(28)\nIn this notation the GS-r rule is given by\njk = argmax i |dki |.\nWe will use the notation xk+ to be the step that would be taken at xk if we update coordinate jk according the GS-r rule\nxk+ = x k + djkejk .\nFrom the optimality of dki , we have for any i that\n− L[(xki − 1\nL ∇if(xk))− (xki + dki )] ∈ ∂gi(xki + dki ), (29)\nand we will use the notation skj for the unique element of ∂gj(x k j + d k j ) satisfying this relationship. We use sk to denote the vector containing these values.\nH.2 Convergence bound for GS-q rule\nUnder this notation, we can show that coordinate descent with the GS-q rule satisfies the bound F (xk+1)− F (x∗) ≤ min {(\n1− µ Ln\n) [f(xk)− f(x∗)], ( 1− µ1\nL\n) [f(xk)− f(x∗)] + k } , (30)\nwhere k ≤\nµ1 L\n( g(xk+)− g(xk + dk) + 〈sk, (xk + dk)− xk+〉 ) .\nWe note that if g is linear then k = 0 and this convergence rate reduces to F (xk+1)− F (x∗) ≤ (\n1− µ1 L\n)[ F (xk)− F (x∗) ] .\nOtherwise, k depends on how far g(x k +) lies above a particular linear underestimate extending from (x k+dk), as well as the conditioning of f . We show this result by first showing that the GS-q rule makes at least as much progress as randomized selection (first part of the min), and then showing that the GS-q rule also makes at least as much progress as the GS-r rule (second part of the min).\nH.3 GS-q is at least as fast as random\nOur argument in this section follows a similar approach to Richtárik and Takáč [2014]. In particular, combining (27) and (28) we have the following upper bound on the iteration progress\nF (xk+1) ≤ F (xk) + min i∈{1,2,...,n} { min d∈IR Vi(x k, d) } ,\n= F (xk) + min i∈{1,2,...,n} { min y∈IRn Vi(x k, yi − xki ) } ,\n= F (xk) + min y∈IRn\n{ min\ni∈{1,2,...,n} Vi(x\nk, yi − xki ) } ,\n≤ F (xk) + min y∈IRn\n{ 1\nn n∑ i=1 Vi(x k, yi − xk)\n}\n= F (xk) + 1\nn min y∈IRn\n{ 〈∇f(xk), y − xk〉+ L\n2 ‖y − xk‖2 + g(y)− g(xk) } = ( 1− 1\nn\n) F (xk) + 1\nn min y∈IRn\n{ f(xk) + 〈∇f(xk), y − xk〉+ L\n2 ‖y − xk‖2 + g(y)\n} .\nFrom strong convexity of f , we have that F is also µ-strongly convex and that\nf(xk) ≤ f(y)− 〈∇f(xk), y − xk)〉 − µ 2 ‖y − xk‖2,\nF (αx∗ + (1− α)xk) ≤ αF (x∗) + (1− α)F (xk)− α(1− α)µ 2 ‖xk − x∗‖2,\nfor any y ∈ IRn and any α ∈ [0, 1] [see Nesterov, 2004, Theorem 2.1.9]. Using these gives us\nF (xk+1) ≤ (\n1− 1 n\n) F (xk) + 1\nn min y∈IRn\n{ f(y)− µ\n2 ‖y − x‖2 + L 2 ‖y − xk‖2 + g(y) } = ( 1− 1\nn\n) F (xk) + 1\nn min y∈IRn\n{ F (y) +\nL− µ 2 ‖y − xk‖2 } ≤ (\n1− 1 n\n) F (xk) + 1\nn min α∈[0,1]\n{ F (αx∗ + (1− α)xk) + α\n2(L− µ) 2\n‖xk − x∗‖2 }\n≤ (\n1− 1 n\n) F (xk) + 1\nn min α∈[0,1]\n{ αF (x∗) + (1− α)F (xk) + α\n2(L− µ)−α(1− α)µ 2\n‖xk − x∗‖2 }\n≤ (\n1− 1 n\n) F (xk) + 1\nn\n[ α∗F (x∗) + (1− α∗)F (xk) ] ( choosing α∗ = µ\nL ∈ (0, 1] ) = ( 1− 1\nn\n) F (xk) + α∗\nn F (x∗) + (1− α∗) n F (xk)\n= F (xk)− α ∗\nn [F (xk)− F (x∗)].\nSubtracting F (x∗) from both sides of this inequality gives us\nF (xk+1)− F (x∗) ≤ (\n1− µ nL\n) [F (xk)− F (x∗)].\nH.4 GS-q is at least as fast as GS-r\nIn this section we derive the right side of the bound (30) for the GS-r rule, but note it also applies to the GS-q rule because from (27) and (28) we have\nF (xk+1) ≤ F (xk) + min i Vi(x, d k i ) (GS-q rule)\n≤ F (xk) + Vjk(x, dkjk) (jk selected by the GS-r rule).\nNote that we lose progress by considering a bound based on the GS-r rule, but its connection to the∞-norm will make it easier to derive an upper bound.\nBy the convexity of gjk we have\ngjk(x k jk ) ≥ gjk(xkjk + d k jk ) + skjk(x k jk − (xkjk + d k jk ))\n= gjk(x k jk + dkjk)− (−Ld k jk −∇jkf(xk))(dkjk) = gjk(x k jk + dkjk) +∇jkf(x k)dkjk + L(d k jk )2,\nwhere ski is defined by (29). Using this we have that\nF (xk+1) ≤ F (xk) + Vj(x, dkjk)\n= F (xk) +∇jf(xk)(dkjk) + L\n2 (dkjk) 2 + gi(x k jk + dkjk)− gi(x k jk )\n≤ F (xk) +∇jf(xk)(dkjk) + L\n2 (dkjk) 2 −∇jkf(xk)dkjk − L(d k jk )2\n= F (xk)− L 2 (dkjk) 2.\nAdding and subtracting F (x∗) and noting that jk is selected using the GS-r rule, we obtain the upper bound\nF (xk+1)− F (x∗) ≤ F (xk)− F (x∗)− L 2 ||dk||2∞. (31)\nRecall that we use xk+ to denote the iteration that would result if we chose jk and actually performed the GS-r update. Using the Lipschitz continuity of the gradient and definition of the GS-q rule again, we have\nF (xk+1) ≤ F (xk) +∇f(xk)T (xk+1 − xk) + L 2 ||xk+1 − xk||2 + g(xk+1)− g(xk)\n≤ F (xk) +∇f(xk)T (xk+ − xk) + L\n2 ||xk+ − xk||2 + g(x+k )− g(x k)\n= f(xk) +∇f(xk)T (xk+ − xk) + L\n2 ‖dk‖2∞ + g(xk+)\nBy the strong-convexity of f , for any y ∈ IRN we have\nf(xk) ≤ f(y)−∇f(xk)T (y − xk)− µ1 2 ‖y − xk‖21,\nand using this we obtain\nF (xk+1) ≤ f(y) +∇f(xk)T (xk+ − y)− µ1 2 ‖y − xk‖21 + L 2 ‖dk‖2∞ + g(xk+). (32)\nBy the convexity of g and sk ∈ ∂g(xk + dk), we have\ng(y) ≥ g(xk + dk) + 〈sk, y − (xk + dk)〉.\nCombining (32) with the above inequality, we have\nF (xk+1)− F (y) ≤ 〈∇f(xk), xk+ − y〉 − µ1 2 ‖y − xk‖21 + L 2 ‖dk‖2∞\n+ g(xk+)− g(xk + dk) + 〈sk, (xk + dk)− y〉.\nWe add and subtract 〈sk, xk+〉 on the right-hand side to get\nF (xk+1)− F (y) ≤ 〈∇f(xk) + sk, xk+ − y〉 − µ1 2 ‖y − xk‖21 + L 2 ‖dk‖2∞\n+ g(xk+)− g(xk + dk) + 〈sk, (xk + dk)− xk+〉.\nLet ck = g(xk+)− g(xk + dk) + 〈sk, (xk + dk)− xk+〉, which is non-negative by the convexity g. Making this substitution, we have\nF (y) ≥ F (xk+1) + 〈−Ldk, y − xk+〉+ µ1 2 ‖y − xk‖21 − L 2 ‖dk‖2∞ − ck.\nNow add and subtract 〈−Ldk, xk〉 to the right-hand side and use (29) to get\nF (y) ≥ F (xk+1) + 〈−Ldk, y − xk〉+ µ1 2 ‖y − xk‖21 − L 2 ‖dk‖2∞ − L〈dk, xk − xk+〉 − ck.\nMinimizing both sides with respect to y results in\nF (x∗) ≥ F (xk+1)− L 2\n2µ1 ‖dk‖2∞ −\nL 2 ‖dk‖2∞ − L〈dk, xk − xk+〉 − ck\n≥ F (xk+1)− L 2\n2µ1 ‖dk‖2∞ −\nL 2 ‖dk‖2∞ + L‖dk‖2∞ − ck\n= F (xk+1)− L(L− µ1) 2µ1 ‖dk‖2∞ − ck,\nwhere we have used that xk+ = x k + dkjkejk and |d k jk | = ‖dk‖∞. Combining this with equation (31), we get\nF (xk+1)− F (x∗) ≤ F (xk)− F (x∗)− L 2 ‖dk‖2∞ F (xk+1)− F (x∗) ≤ F (xk)− F (x∗)− µ1 (L− µ1) [ F (xk+1)− F (x∗)− ck ] (\n1 + µ1\n(L− µ1)\n)[ F (xk+1)− F (x∗) ] ≤ F (xk)− F (x∗) + k µ1\n(L− µ1)\nF (xk+1)− F (x∗) ≤ (L− µ1) L\n[ F (xk)− F (x∗) ] + ck\nµ1 L\nF (xk+1)− F (x∗) ≤ (\n1− µ1 L\n)[ F (xk)− F (x∗) ] + ck\nµ1 L .\nH.5 Lack of progress of the GS-s rule\nWe now show that the rate (1 − µ1/L), and even the slower rate (1 − µ/Ln), cannot hold for the GS-s rule. We do this by constructing a problem where an iteration of the GS-s method does not make sufficient progress. In particular, consider the bound-constrained problem\nmin x∈C\nf(x) = 1\n2 ‖Ax− b‖22,\nwhere C = {x : x ≥ 0}, and\nA = ( 1 0 0 0.7 ) , b = ( −1 −3 ) , x0 = ( 1 0.1 ) , x∗ = ( 0 0 ) .\nWe thus have that\nf(x0) = 1\n2 ((1 + 1)2 + (.07 + 3)2) ≈ 6.7\nf(x∗) = 1\n2 ((−1)2 + (−3)2) = 5 ∇f(x0) = AT (Ax0 − b) ≈ (\n2.0 2.1 ) ∇2f(x) = ATA = ( 1 0 0 0.49 ) .\nThe parameter values for this problem are\nn = 2\nµ = λmin = 0.49\nL = λmax = 1\nµ1 =\n( 1\nλ1 +\n1\nλ2\n)−1 = 1 + 1\n0.49 ≈ 0.33,\nwhere the λi are the eigenvalues of A TA, and µ and µ1 are the corresponding strong-convexity constants for the 2-norm and 1-norm, respectively. The proximal operator of the indicator function is the projection onto the set C, which involves setting negative elements to zero. Thus, our iteration update is given by\nxk+1 = prox δC [xk − 1 L ∇ikf(xk)eik ] = max(xk − 1 L ∇ikf(xk)eik , 0),\nFor this problem, the GS-s rule is given by\ni = argmax i |ηki |,\nwhere\nηki = { ∇if(xk), if xki 6= 0 or ∇if(xk) < 0 0, otherwise .\nBased on the value of ∇f(x0), the GS-s rule thus chooses to update coordinate 2, setting it to zero and obtaining\nf(x1) = 1\n2 ((1 + 1)2 + (−3)2) = 6.5.\nThus we have f(x1)− f(x∗) f(x0)− f(x∗) ≈ 6.5− 5 6.7− 5 ≈ 0.88,\neven though the bounds obtain the faster rates of( 1− µ\nLn\n) = ( 1− 0.49\n2 ) ≈ 0.76,(\n1− µ1 L\n) ≈ (1− 0.33) = 0.67.\nThus, the GS-s rule does not satisfy either bound. On the other hand, the GS-r and GS-q rules are given in this context by\nik = argmax i ∣∣∣∣max(xk − 1L∇if(xk)ei, 0 ) − xk ∣∣∣∣ , and thus both these rules choose to update coordinate 1, setting it to zero to obtain f(x1) ≈ 5.2 and a progress ratio of\nf(x1)− f(x∗) f(x0)− f(x∗) ≈ 5.2− 5 6.7− 5 ≈ 0.12,\nwhich clearly satisfies both bounds.\nH.6 Lack of progress of the GS-r rule\nWe now turn to showing that the GS-r rule does not satisfy these bounds in general. It will not be possible to show this for a simple bound-constrained problem since the GS-r and GS-q rules are equivalent for these problems. Thus, we consider the following `1-regularized problem\nmin x∈IR2\n1 2 ‖Ax− b‖22 + λ‖x‖1 ≡ F (x).\nWe use the same A as the previous section, so that n, µ, L, and µ1 are the same. However, we now take\nb = ( 2 −1 ) , x0 = ( 0.4 0.5 ) , x∗ = ( 1 0 ) , λ = 1,\nso we have\nf(x0) ≈ 3.1, f(x∗) = 2\nThe proximal operator of the absolute value function is given by the soft-threshold function, and our coordinate update of variable ik is given by\nxk+1ik = prox λ|·|\n[x k+ 12 ik ] = sgn(x k+ 12 ik ) ·max(xk+ 1 2 ik − λ/L, 0),\nwhere we have used the notation\nx k+ 12 i = x k i − 1\nL ∇if(xk)ei.\nThe GS-r rule is defined by ik = argmax\ni |dki |,\nwhere dki = proxλ|·|[x k+ 12 i ]− xki and in this case\nd0 = ( 0.6 −0.5 ) .\nThus, the GS-r rule chooses to update coordinate 1. After this update the function value is\nF (x1) ≈ 2.9,\nso the progress ratio is F (x1)− F (x∗) F (x0)− F (x∗) ≈ 2.9− 2 3.1− 2 ≈ 0.84.\nHowever, the bounds suggest faster progress ratios of( 1− µ\nLn\n) ≈ 0.76,\n( 1− µ1\nL\n) ≈ 0.67,\nso the GS-r rule does not satisfy either bound. In contrast, in this setting the GS-q rule chooses to update coordinate 2 and obtains F (x1) ≈ 2.2, obtaining a progress ratio of\nF (x1)− F (x∗) F (x0)− F (x∗) ≈ 2.2− 2 3.1− 2 ≈ 0.16,\nwhich satisfies both bounds by a substantial margin. Indeed, we used a genetic algorithm to search for a setting of the parameters of this problem (values of x0, λ, b, and the diagonals of A) that would make the GS-q not satisfy the bound depending on µ1, and it easily found counter-examples for the GS-s and GS-r rules but was not able to produce a counter example for the GS-q rule.\nAppendix I Runtime Experiments\nIn Figure 3 we plot the objective against the runtime for the `2-regularized sparse least squares problem from our experiments. Although runtimes are very sensitive to exact implementation details and we believe that more clever implementations than our naive Python script are possible, this figure does show that the GS and GSL rules offer benefits in terms of runtime with our implementation and test hardware."
    } ],
    "references" : [ {
      "title" : "On the convergence of block coordinate descent type methods",
      "author" : [ "A. Beck", "L. Tetruashvili" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "Beck and Tetruashvili.,? \\Q2013\\E",
      "shortCiteRegEx" : "Beck and Tetruashvili.",
      "year" : 2013
    }, {
      "title" : "Label propagation and quadratic criterion",
      "author" : [ "Y. Bengio", "O. Delalleau", "N. Le Roux" ],
      "venue" : "Semi-Supervised Learning,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2006
    }, {
      "title" : "Nonlinear Programming",
      "author" : [ "D.P. Bertsekas" ],
      "venue" : "Athena Scientific, second edition,",
      "citeRegEx" : "Bertsekas.,? \\Q1999\\E",
      "shortCiteRegEx" : "Bertsekas.",
      "year" : 1999
    }, {
      "title" : "Greedy block coordinate descent for large scale gaussian process regression",
      "author" : [ "L. Bo", "C. Sminchisescu" ],
      "venue" : "Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Bo and Sminchisescu.,? \\Q2008\\E",
      "shortCiteRegEx" : "Bo and Sminchisescu.",
      "year" : 2008
    }, {
      "title" : "Convex Optimization",
      "author" : [ "S.P. Boyd", "L. Vandenberghe" ],
      "venue" : null,
      "citeRegEx" : "Boyd and Vandenberghe.,? \\Q2004\\E",
      "shortCiteRegEx" : "Boyd and Vandenberghe.",
      "year" : 2004
    }, {
      "title" : "Introduction to Algorithms",
      "author" : [ "T.H. Cormen", "C.E. Leiserson", "R.L. Rivest", "C. Stein" ],
      "venue" : null,
      "citeRegEx" : "Cormen et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Cormen et al\\.",
      "year" : 2001
    }, {
      "title" : "Inducing features of random fields",
      "author" : [ "S. Della Pietra", "V. Della Pietra", "J. Lafferty" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Pietra et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Pietra et al\\.",
      "year" : 1997
    }, {
      "title" : "Nearest neighbor based greedy coordinate descent",
      "author" : [ "I.S. Dhillon", "P.K. Ravikumar", "A. Tewari" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Dhillon et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Dhillon et al\\.",
      "year" : 2011
    }, {
      "title" : "Accelerated, parallel and proximal coordinate descent",
      "author" : [ "O. Fercoq", "P. Richtárik" ],
      "venue" : null,
      "citeRegEx" : "Fercoq and Richtárik.,? \\Q2013\\E",
      "shortCiteRegEx" : "Fercoq and Richtárik.",
      "year" : 2013
    }, {
      "title" : "The nature and use of the harmonic mean",
      "author" : [ "W.F. Ferger" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Ferger.,? \\Q1931\\E",
      "shortCiteRegEx" : "Ferger.",
      "year" : 1931
    }, {
      "title" : "Hybrid deterministic-stochastic methods for data fitting",
      "author" : [ "M.P. Friedlander", "M. Schmidt" ],
      "venue" : "SIAM Journal on Scientific Computing,",
      "citeRegEx" : "Friedlander and Schmidt.,? \\Q2012\\E",
      "shortCiteRegEx" : "Friedlander and Schmidt.",
      "year" : 2012
    }, {
      "title" : "A dual coordinate descent method for large-scale linear SVM",
      "author" : [ "C.-J. Hsieh", "K.-W. Chang", "C.-J. Lin", "S.S. Keerthi", "S. Sundararajan" ],
      "venue" : "International Conference on Machine Learning,",
      "citeRegEx" : "Hsieh et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Hsieh et al\\.",
      "year" : 2008
    }, {
      "title" : "Efficient structure learning of Markov networks using `1regularization",
      "author" : [ "S.-I. Lee", "V. Ganapathi", "D. Koller" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Lee et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2006
    }, {
      "title" : "Randomized methods for linear constraints: convergence rates and conditioning",
      "author" : [ "D. Leventhal", "A.S. Lewis" ],
      "venue" : "Mathematics of Operations Research,",
      "citeRegEx" : "Leventhal and Lewis.,? \\Q2010\\E",
      "shortCiteRegEx" : "Leventhal and Lewis.",
      "year" : 2010
    }, {
      "title" : "Coordinate descent optimization for `1 minimization with application to compressed sensing; a greedy algorithm",
      "author" : [ "Y. Li", "S. Osher" ],
      "venue" : "Inverse Problems and Imaging,",
      "citeRegEx" : "Li and Osher.,? \\Q2009\\E",
      "shortCiteRegEx" : "Li and Osher.",
      "year" : 2009
    }, {
      "title" : "On convergence of the maximum block improvement method",
      "author" : [ "Z. Li", "A. Uschmajew", "S. Zhang" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "Li et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2015
    }, {
      "title" : "Error bounds and convergence analysis of feasible descent methods: a general approach",
      "author" : [ "Z.-Q. Luo", "P. Tseng" ],
      "venue" : "Annals of Operations Research,",
      "citeRegEx" : "Luo and Tseng.,? \\Q1993\\E",
      "shortCiteRegEx" : "Luo and Tseng.",
      "year" : 1993
    }, {
      "title" : "Convergence rate analysis of MAP coordinate minimization algorithms",
      "author" : [ "O. Meshi", "T. Jaakkola", "A. Globerson" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Meshi et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Meshi et al\\.",
      "year" : 2012
    }, {
      "title" : "Introductory Lectures on Convex Optimization: A Basic Course",
      "author" : [ "Y. Nesterov" ],
      "venue" : null,
      "citeRegEx" : "Nesterov.,? \\Q2004\\E",
      "shortCiteRegEx" : "Nesterov.",
      "year" : 2004
    }, {
      "title" : "Efficiency of coordinate descent methods on huge-scale optimization problems",
      "author" : [ "Y. Nesterov" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "Nesterov.,? \\Q2012\\E",
      "shortCiteRegEx" : "Nesterov.",
      "year" : 2012
    }, {
      "title" : "Five balltree construction algorithms",
      "author" : [ "S.M. Omohundro" ],
      "venue" : "Technical report, International Computer Science Institute,",
      "citeRegEx" : "Omohundro.,? \\Q1989\\E",
      "shortCiteRegEx" : "Omohundro.",
      "year" : 1989
    }, {
      "title" : "On the convergence of leveraging",
      "author" : [ "G. Rätsch", "S. Mika", "M.K. Warmuth" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Rätsch et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Rätsch et al\\.",
      "year" : 2001
    }, {
      "title" : "Gaussian Markov Random Fields: Theory and Applications",
      "author" : [ "H. Rue", "L. Held" ],
      "venue" : null,
      "citeRegEx" : "Rue and Held.,? \\Q2005\\E",
      "shortCiteRegEx" : "Rue and Held.",
      "year" : 2005
    }, {
      "title" : "Asymmetric LSH (ALSH) for sublinear time maximum inner product search",
      "author" : [ "A. Shrivastava", "P. Li" ],
      "venue" : null,
      "citeRegEx" : "Shrivastava and Li.,? \\Q2003\\E",
      "shortCiteRegEx" : "Shrivastava and Li.",
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "There has been substantial recent interest in applying coordinate descent methods to solve large-scale optimization problems, starting with the seminal work of Nesterov [2012], who gave the first global rate-ofconvergence analysis for coordinate-descent methods for minimizing convex functions.",
      "startOffset" : 160,
      "endOffset" : 176
    }, {
      "referenceID" : 11,
      "context" : ") The family of functions h1 includes core machinelearning problems such as least squares, logistic regression, lasso, and SVMs (when solved in dual form) [Hsieh et al., 2008].",
      "startOffset" : 155,
      "endOffset" : 175
    }, {
      "referenceID" : 1,
      "context" : "Family h2 includes quadratic functions, graph-based label propagation algorithms for semisupervised learning [Bengio et al., 2006], and finding the most likely assignments in continuous pairwise graphical models [Rue and Held, 2005].",
      "startOffset" : 109,
      "endOffset" : 130
    }, {
      "referenceID" : 22,
      "context" : ", 2006], and finding the most likely assignments in continuous pairwise graphical models [Rue and Held, 2005].",
      "startOffset" : 89,
      "endOffset" : 109
    }, {
      "referenceID" : 17,
      "context" : "For example, if each node has at most d neighbours, we can track the gradients of all the variables and use a max-heap structure to implement the GS rule in O(d log n) time [Meshi et al., 2012].",
      "startOffset" : 173,
      "endOffset" : 193
    }, {
      "referenceID" : 1,
      "context" : "Family h2 includes quadratic functions, graph-based label propagation algorithms for semisupervised learning [Bengio et al., 2006], and finding the most likely assignments in continuous pairwise graphical models [Rue and Held, 2005]. In general, the GS rule for problem h2 is as expensive as a full gradient evaluation. However, the structure of G often allows efficient implementation of the GS rule. For example, if each node has at most d neighbours, we can track the gradients of all the variables and use a max-heap structure to implement the GS rule in O(d log n) time [Meshi et al., 2012]. This is similar to the cost of the randomized algorithm if d ≈ |E|/n (since the average cost of the randomized method depends on the average degree). This condition is true in a variety of applications. For example, in spatial statistics we often use two-dimensional grid-structured graphs, where the maximum degree is four and the average degree is slightly less than 4. As another example, for applying graph-based label propagation on the Facebook graph (to detect the spread of diseases, for example), the average number of friends is around 200 but no user has more than seven thousand friends. The maximum number of friends would be even smaller if we removed edges based on proximity. A non-sparse example where GS is efficient is complete graphs, since here the average degree and maximum degree are both (n−1). Thus, the GS rule is efficient for optimizing dense quadratic functions. On the other hand, GS could be very inefficient for star graphs. If each column of A has at most c non-zeroes and each row has at most r non-zeroes, then for many notable instances of problem h1 we can implement the GS rule in O(cr log n) time by maintaining Ax as well as the gradient and again using a max-heap (see Appendix A). Thus, GS will be efficient if cr is similar to the number of non-zeroes in A divided by n. Otherwise, Dhillon et al. [2011] show that we can approximate the GS rule for problem h1 with no gi functions by solving a nearest-neighbour problem.",
      "startOffset" : 110,
      "endOffset" : 1945
    }, {
      "referenceID" : 1,
      "context" : "Family h2 includes quadratic functions, graph-based label propagation algorithms for semisupervised learning [Bengio et al., 2006], and finding the most likely assignments in continuous pairwise graphical models [Rue and Held, 2005]. In general, the GS rule for problem h2 is as expensive as a full gradient evaluation. However, the structure of G often allows efficient implementation of the GS rule. For example, if each node has at most d neighbours, we can track the gradients of all the variables and use a max-heap structure to implement the GS rule in O(d log n) time [Meshi et al., 2012]. This is similar to the cost of the randomized algorithm if d ≈ |E|/n (since the average cost of the randomized method depends on the average degree). This condition is true in a variety of applications. For example, in spatial statistics we often use two-dimensional grid-structured graphs, where the maximum degree is four and the average degree is slightly less than 4. As another example, for applying graph-based label propagation on the Facebook graph (to detect the spread of diseases, for example), the average number of friends is around 200 but no user has more than seven thousand friends. The maximum number of friends would be even smaller if we removed edges based on proximity. A non-sparse example where GS is efficient is complete graphs, since here the average degree and maximum degree are both (n−1). Thus, the GS rule is efficient for optimizing dense quadratic functions. On the other hand, GS could be very inefficient for star graphs. If each column of A has at most c non-zeroes and each row has at most r non-zeroes, then for many notable instances of problem h1 we can implement the GS rule in O(cr log n) time by maintaining Ax as well as the gradient and again using a max-heap (see Appendix A). Thus, GS will be efficient if cr is similar to the number of non-zeroes in A divided by n. Otherwise, Dhillon et al. [2011] show that we can approximate the GS rule for problem h1 with no gi functions by solving a nearest-neighbour problem. Their analysis of the GS rule in the convex case, however, gives the same convergence rate that is obtained by random selection (although the constant factor can be smaller by a factor of up to n). More recently, Shrivastava and Li [2014] give a general method for approximating the GS rule for problem h1 with no gi functions by writing it as a maximum inner-product search problem.",
      "startOffset" : 110,
      "endOffset" : 2301
    }, {
      "referenceID" : 0,
      "context" : "While this is faster than known rates for cyclic coordinate selection [Beck and Tetruashvili, 2013] and holds deterministically rather than in expectation, this rate is the same as the randomized rate given in (5).",
      "startOffset" : 70,
      "endOffset" : 99
    }, {
      "referenceID" : 9,
      "context" : "Furthermore, the harmonic mean divided by n has an interpretation in terms of processes ‘working together’ [Ferger, 1931].",
      "startOffset" : 107,
      "endOffset" : 121
    }, {
      "referenceID" : 18,
      "context" : "6 Rules Depending on Lipschitz Constants If the Li are known, Nesterov [2012] showed that we can obtain a faster convergence rate by sampling proportional to the Li.",
      "startOffset" : 62,
      "endOffset" : 78
    }, {
      "referenceID" : 13,
      "context" : "This was shown by Leventhal and Lewis [2010] and is a special case of Nesterov [2012, Theorem 2] with α = 1 in his notation.",
      "startOffset" : 18,
      "endOffset" : 45
    }, {
      "referenceID" : 21,
      "context" : "This rule has been used in the context of boosting [Rätsch et al., 2001], graphical models [Della Pietra et al.",
      "startOffset" : 51,
      "endOffset" : 72
    }, {
      "referenceID" : 3,
      "context" : ", 2006, Scheinberg and Rish, 2009], Gaussian processes [Bo and Sminchisescu, 2008], and low-rank tensor approximations [Li et al.",
      "startOffset" : 55,
      "endOffset" : 82
    }, {
      "referenceID" : 15,
      "context" : ", 2006, Scheinberg and Rish, 2009], Gaussian processes [Bo and Sminchisescu, 2008], and low-rank tensor approximations [Li et al., 2015].",
      "startOffset" : 119,
      "endOffset" : 136
    }, {
      "referenceID" : 7,
      "context" : "3 Connection between GSL Rule and Normalized Nearest Neighbour Search Dhillon et al. [2011] discuss an interesting connection between the GS rule and the nearest-neighbour-search (NNS) problem for objectives of the form",
      "startOffset" : 70,
      "endOffset" : 92
    }, {
      "referenceID" : 7,
      "context" : "Dhillon et al. [2011] propose to approximate the above argmax by solving the following NNS problem ik = argmin i∈[2n] ‖r(x)− ai‖,",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 23,
      "context" : "Shrivastava and Li [2014] have more recently considered the case where ‖ai‖ ≤ 1 and incorporate powers of ‖ai‖ in the NNS to yield a better approximation.",
      "startOffset" : 0,
      "endOffset" : 26
    }, {
      "referenceID" : 7,
      "context" : "Approximate GS rules under multiplicative and additive errors were considered by Dhillon et al. [2011] in the convex case, but in this setting the convergence rate is similar to the rate achieved by random selection.",
      "startOffset" : 81,
      "endOffset" : 103
    }, {
      "referenceID" : 10,
      "context" : "This is in contrast to having an error in the gradient [Friedlander and Schmidt, 2012], where the error must decrease to zero over time.",
      "startOffset" : 55,
      "endOffset" : 86
    }, {
      "referenceID" : 20,
      "context" : "Hence, we use a balltree structure [Omohundro, 1989] to implement an efficient approximate GS rule based on the connection to the NNS problem discovered by Dhillon et al.",
      "startOffset" : 35,
      "endOffset" : 52
    }, {
      "referenceID" : 7,
      "context" : "Hence, we use a balltree structure [Omohundro, 1989] to implement an efficient approximate GS rule based on the connection to the NNS problem discovered by Dhillon et al. [2011]. On the other hand, we can compute the exact GSL rule for this problem as a NNS problem as discussed in Section 6.",
      "startOffset" : 156,
      "endOffset" : 178
    }, {
      "referenceID" : 1,
      "context" : "We use the quadratic labeling criterion of Bengio et al. [2006], which allows exact coordinate optimization and is normally optimized with cyclic coordinate descent.",
      "startOffset" : 43,
      "endOffset" : 64
    }, {
      "referenceID" : 8,
      "context" : "We expect our analysis also applies to block updates by using mixed norms ‖ · ‖p,q, and could be used for accelerated/parallel methods [Fercoq and Richtárik, 2013], for primal-dual rates of dual coordinate ascent [Shalev-Shwartz and Zhang, 2013], for successive projection methods [Leventhal and Lewis, 2010], for boosting algorithms [Rätsch et al.",
      "startOffset" : 135,
      "endOffset" : 163
    }, {
      "referenceID" : 13,
      "context" : "We expect our analysis also applies to block updates by using mixed norms ‖ · ‖p,q, and could be used for accelerated/parallel methods [Fercoq and Richtárik, 2013], for primal-dual rates of dual coordinate ascent [Shalev-Shwartz and Zhang, 2013], for successive projection methods [Leventhal and Lewis, 2010], for boosting algorithms [Rätsch et al.",
      "startOffset" : 281,
      "endOffset" : 308
    }, {
      "referenceID" : 21,
      "context" : "We expect our analysis also applies to block updates by using mixed norms ‖ · ‖p,q, and could be used for accelerated/parallel methods [Fercoq and Richtárik, 2013], for primal-dual rates of dual coordinate ascent [Shalev-Shwartz and Zhang, 2013], for successive projection methods [Leventhal and Lewis, 2010], for boosting algorithms [Rätsch et al., 2001], and for scenarios without strong-convexity under general error bounds [Luo and Tseng, 1993].",
      "startOffset" : 334,
      "endOffset" : 355
    }, {
      "referenceID" : 16,
      "context" : ", 2001], and for scenarios without strong-convexity under general error bounds [Luo and Tseng, 1993].",
      "startOffset" : 79,
      "endOffset" : 100
    } ],
    "year" : 2015,
    "abstractText" : "There has been significant recent work on the theory and application of randomized coordinate descent algorithms, beginning with the work of Nesterov [SIAM J. Optim., 22(2), 2012 ], who showed that a random-coordinate selection rule achieves the same convergence rate as the Gauss-Southwell selection rule. This result suggests that we should never use the Gauss-Southwell rule, because it is typically much more expensive than random selection. However, the empirical behaviours of these algorithms contradict this theoretical result: in applications where the computational costs of the selection rules are comparable, the Gauss-Southwell selection rule tends to perform substantially better than random coordinate selection. We give a simple analysis of the Gauss-Southwell rule showing that—except in extreme cases—its convergence rate is faster than choosing random coordinates. We also (i) show that exact coordinate optimization improves the convergence rate for certain sparse problems, (ii) propose a Gauss-Southwell-Lipschitz rule that gives an even faster convergence rate given knowledge of the Lipschitz constants of the partial derivatives, (iii) analyze the effect of approximate Gauss-Southwell rules, and (iv) analyze proximal-gradient variants of the Gauss-Southwell rule. 1 Coordinate Descent Methods There has been substantial recent interest in applying coordinate descent methods to solve large-scale optimization problems, starting with the seminal work of Nesterov [2012], who gave the first global rate-ofconvergence analysis for coordinate-descent methods for minimizing convex functions. This analysis suggests that choosing a random coordinate to update gives the same performance as choosing the “best” coordinate to update via the more expensive Gauss-Southwell (GS) rule. (Nesterov also proposed a more clever randomized scheme, which we consider later in this paper.) This result gives a compelling argument to use randomized coordinate descent in contexts where the GS rule is too expensive. It also suggests that there is no benefit to using the GS rule in contexts where it is relatively cheap. But in these contexts, the GS rule often substantially outperforms randomized coordinate selection in practice. This suggests that either the analysis of GS is not tight, or that there exists a class of functions for which the GS rule is as slow as randomized coordinate descent. After discussing contexts in which it makes sense to use coordinate descent and the GS rule, we answer this theoretical question by giving a tighter analysis of the GS rule (under strong-convexity and standard smoothness assumptions) that yields the same rate as the randomized method for a restricted class of functions, but is otherwise faster (and in some cases substantially faster). We further show that, compared to the usual constant step-size update of the coordinate, the GS method with exact coordinate optimization has a provably faster rate for problems satisfying a certain sparsity constraint (Section 5). We believe that this is the first result showing a theoretical benefit of exact coordinate optimization; all previous analyses show that these strategies obtain the same rate as constant step-size updates, even though exact optimization tends to be faster in practice. Furthermore, in Section 6, we propose a variant of the GS rule that, similar to Nesterov’s more clever randomized sampling scheme, uses knowledge of the Lipschitz constants of the coordinate-wise gradients to obtain a faster rate. We also analyze approximate GS rules (Section 7), which 1 ar X iv :1 50 6. 00 55 2v 1 [ m at h. O C ] 1 J un 2 01 5 provide an intermediate strategy between randomized methods and the exact GS rule. Finally, we analyze proximal-gradient variants of the GS rule (Section 8) for optimizing problems that include a separable nonsmooth term. 2 Problems of Interest The rates of Nesterov show that coordinate descent can be faster than gradient descent in cases where, if we are optimizing n variables, the cost of performing n coordinate updates is similar to the cost of performing one full gradient iteration. This essentially means that coordinate descent methods are useful for minimizing convex functions that can be expressed in one of the following two forms:",
    "creator" : "LaTeX with hyperref package"
  }
}