{
  "name" : "1705.07381.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Generalizing the Role of Determinization in Probabilistic Planning",
    "authors" : [ "Luis Pineda" ],
    "emails" : [ "shlomo}@cs.umass.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "One of the most popular models for probabilistic planning is the Stochastic Shortest Path SSP) [Bertsekas and Tsitsiklis, 1991]. A solution to an SSP is a sequence of actions that, starting from a given initial state, minimizes the expected cost of reaching a state from a given set of goal states. The uncertainty associated with the outcome of each action induces probabilistic transitions between states. This model has been used for a wide variety of applications, such as route planning in the presence of traffic delays [Lim et al., 2013], quantifying the value of battery energy storage systems [Tan et al., 2015], modeling wildfire propagation [Hajian et al., 2016], and semi-autonomous driving [Wray et al., 2016].\nDespite its popularity, a major obstacle to the wide applicability of SSPs is the prohibitive computational cost of finding an optimal solution. This is due to the fact that optimal policies can, in the worst case, cover a number of states linear in the size of the state space, which in turn can be exponential in the number of variables describing the problem.\nGiven the difficulty of solving SSPs optimally, there has been much interest in developing methods that sacrifice optimality for the sake of computational efficiency. Among these\nmethods, one of the most successful approaches has been the use of determinization complemented by replanning. These methods became popular thanks to the unexpected success of FF-Replan [Yoon et al., 2007] in the International Probabilistic Planning Competition (IPPC) [Bryce and Buffet, 2008]. FF-Replan works by transforming the original problem into a deterministic one (e.g., by only considering most probable outcomes), and using the FF classical planner [Hoffmann and Nebel, 2001] to solve the resulting problems. This process is then repeated during execution whenever a state not covered by the current deterministic plan is observed.\nAlthough FF-Replan is significantly faster than any optimal probabilistic planner, it performs poorly in many problem domains due to the fact that it ignores the uncertainty. This has led to the development of more robust determinizationbased algorithms, that incorporate some form of probabilistic reasoning.\nRFF [Teichteil-Königsbuch et al., 2010]—the winner of IPPC’08—works by incrementally aggregating partial plans until the result covers a high probability envelop of states; each partial plan is computed using determinization and the FF planner. FF-Hindsight [Yoon et al., 2008] works by sampling a set of deterministic “futures” of the original problem, solves each using FF, and combines their cost to estimate the costs in the original problem. HMDPP [Keyder and Geffner, 2008] introduces a self-loop determinization trick that nudges the deterministic planner into generating plans with low probability of deviation. SSiPP-FF [Trevizan and Veloso, 2014] works by creating short-sighted problems that consider only states up to a certain horizon from the current state. These smaller problems are solved optimally, and when a tip state is found during execution, the FF-Replan method is used.\nAlthough these planners are capable of quickly solving large problems, they all employ a rather crude approach to determinization, by generally relying on either the most-likelyoutcome (MLO) determinization, or the all-outcomes (AO) one. Using MLO can make planners ignore important sections of the state space (e.g., outcomes leading to dead-ends), resulting in poor policies. Using AO has other problems such as potentially wasting computation on irrelevant/unlikely sections of the state space, or treating all paths to the goal as equally important.\nRecent work has shown that the choice of determinization can sometimes have a significant impact in the quality of a\nar X\niv :1\n70 5.\n07 38\n1v 2\n[ cs\n.A I]\n2 9\nJu l 2\n01 7\ndeterminization-based planning approach [Pineda and Zilberstein, 2014]. In fact, even in some domains deemed “probabilistically interesting” [Little and Thiebaux, 2007] planning with a good determinization can actually result in optimal plans for the original SSP (e.g., the triangle tireworld domain). However, not much work has been done on generating methods for choosing a determinization that works well for a particular domain.\nIn this work, we address these issues with two main contributions. First, we present a new planner, FF-LAO*, that combines the LAO* optimal SSP solver [Hansen and Zilberstein, 2001] with the FF classical planner. FF-LAO* can leverage fast deterministic planning to estimate state values, but still partially reason about the complete probabilistic model if sodesired; it does this by relying on the reduced models framework introduced by Pineda and Zilberstein [2014].\nOur second contribution is showing that it is possible to learn a good determinization on small instances of a planning domain, such that, when applied to larger instances, significant gains in efficiency and performance can be realized. This is the first work that selects a determinization based on the anticipated performance in the original probabilistic domain.\nThe rest of the paper is structured as follows: Section 2 gives background on SSPs and reduced models, Section 3 describes the FF-LAO* algorithm, Section 4 explains how to choose a good determinization for a particular planning domain, Section 5 presents experimental results, and Section 6 summarizes the conclusions and ideas for future work."
    }, {
      "heading" : "2 Background",
      "text" : "A Stochastic Shortest Path (SSP) problem [Bertsekas and Tsitsiklis, 1991] is defined by a tuple 〈S,A, T,C, s0, G〉, where S is a finite set of states, A is a finite set of actions, T (s′|s, a) ∈ [0, 1] represents the probability of reaching state s′ when action a is taken in state s, C(s, a) ∈ [0,∞) is the cost of applying action a in state s, s0 is an initial state and G is a set of goal states satisfying ∀sg ∈ G, a ∈ A, T (sg|sg, a) = 1 ∧ C(sg, a) = 0. Moreover, we assume costs satisfy ∀s ∈ S \\G,C(s, a) > 0. Interestingly, SSPs are a variant of Markov Decision Process (MDPs) [Puterman, 1994] that has been shown to be more general than finite-horizon and infinite-horizon discounted MDPs [Bertsekas and Tsitsiklis, 1995].\nA solution to an SSP is a policy, a mapping π : S → A, indicating that action π(s) should be taken at state s. A policy π induces a value function V π : S → R that represents the expected cumulative cost of reaching sg ∈ G by following policy π from state s. An optimal policy π∗ is one that minimizes this expected cumulative cost; similarly, we use the notation V ∗ to refer to the optimal value function.\nFor an SSP to be well-defined, a policy must exist such that the goal is reachable from any state with probability 1. Under this assumption, an SSP is guaranteed to have an optimal solution, and the optimal value function is unique. This optimal value function can then be found as the fixed point of the Bellman update operator (Eq. 1).\nV (s) = min a∈A\n{ C(s, a) + ∑ s′∈S T (s′|s, a)V (s′) }\n(1)\nThere is a variety of languages to compactly describe SSPs, of which PPDDL [Younes and Littman, 2004] has been most widely used within the AI community. As it turns out, it has been shown that finding whether a plan exists in a compactly described problem is EXPTIME-complete [Littman, 1997]. As mentioned earlier, this challenging complexity has led to the development of many approximate methods for solving SSPs. Particularly relevant to our work are determinizationbased approaches, and, more generally, the reduced models framework."
    }, {
      "heading" : "2.1 Reduced Models",
      "text" : "Given an SSP, the reduced models framework creates a simplified model characterized by two parameters: the number of outcomes per action that are fully accounted for (referred to as primary outcomes), and the maximum number of occurrences of the remaining outcomes that are planned for in advance (referred to as exceptions). This type of reduction generalizes single-outcome determinization, and introduces a spectrum of reductions with varying levels of probabilistic complexity.\nThe model assumes a factored representation of SSPs in which actions are probabilistic operators of the form:\na = 〈preconditions, cost, [p1 : e1, ..., pm : em]〉,\nwhere each effect ei, for i ∈ {1, ...,m}, is associated with a probability pi of occurring when a is executed, and there exists a successor function, τ , that maps effects to successor states, so that s′ = τ(s, ei) and T (s′|s, a) = pi.\nUsing this formalism, a reduced model of an SSP 〈S,A, T,C, s0, G〉 is defined as follows:\n• The set of states is defined as S′ = S × {0, 1, .., k}, where k is a positive integer;\n• The set of actions is the original set, A;\n• The transition function is defined by Eq. (2);\n• The cost function is defined as C ′((s, j), a) = C(s, a), for all (s, j) ∈ S′ ∧ a ∈ A;\n• The initial state is (s0, 0);\n• The set of goals is defined as G′ = {(s, j) ∈ S′|s ∈ G}.\nThe transition function defined below, while seemingly complicated, describes a simple process. The value j in state (s, j) keeps counts of how many exceptions have occurred up to that point in execution. For states where the count is less than the exception bound, k, the transition function operates as the original, except that the counter is increased by one for successors labeled as exceptions. On the other hand, for states with count j = k, the transition completely ignores exceptions, and only considers transitions to primary outcomes, redistributing the ignored probabilities so that they form a proper distribution (e.g., by normalizing). The notation Pa used below refers to the set of primary effects.\nT ′((s′, j′)|(s, j), a) = pi if j < k ∧ j′ = j ∧ ei ∈ Pa pi if j < k ∧ j′ = j + 1 ∧ ei /∈ Pa p′i if j = j\n′ = k ∧ ei ∈ Pa 0 if j = j′ = k ∧ ei /∈ Pa\n(2)\nwhere s′ = τ(s, ei) and the set {p′1, ..., p′m} is any set of real numbers that satisfy\n∀i : ei ∈ Pa p′i > 0 and ∑\ni:ei∈Pa\np′i = 1 (3)\nNote that the reduced models framework encapsulates single-outcome determinization, which is simply a reduction where the set of primary outcomes has size 1 and the value of k = 0. In this work we are indeed concerned with reductions having a single primary outcome, but will include the possibility of using k > 0. We refer to these models asMk1reductions."
    }, {
      "heading" : "3 FF-LAO*",
      "text" : "Reducing an SSP can significantly accelerate planning times by pruning large sections of the state space. However, there are many domains in which solving a reduced model optimally is still prohibitively expensive. In fact, for complex domains like the ones used in IPPC [Bryce and Buffet, 2008], using determinization and k = 0 already results in problems too large to be solved optimally in a practical manner.\nTo address this issue, we present a planner that combines the flexibility of the reduced models framework with the efficiency of a classical planner. We call this planner FF-LAO*, as it is an extension of the LAO* algorithm [Hansen and Zilberstein, 2001] that leverages the FF classical planner [Hoffmann and Nebel, 2001] to accelerate computation.\nFF-LAO* (Algorithms 1-4) receives as input an Mk1-reduction, S′=〈S′, A, T ′, C ′, (s0, 0), G′〉; i.e., one that becomes deterministic after the exception bound is reached (equivalently, one where ∀a ∈ A, |Pa| = 1). The remaining inputs are the exception bound, k, and the error tolerance, . We use S to denote the original SSP from which S′ is derived.\nFF-LAO* works almost exactly as LAO*, except that FF is used to compute values and actions for states that have reached the exception bound (i.e., states of the form (s, k)). This occurs in lines 4 and 8 of Algorithm 1, where the state expansion and test convergence procedures are replaced with versions that use FF (Algorithms 2 and 3, respectively).\nReaders familiar with LAO* may notice differences with respect to the usual expansion and convergence test procedures. In particular, note the inclusion of if statements in line 7 (both procedures), where the successors of the expanded state are only added to the stack if j < k. The reason is that states (s, k) will be solved by calling FF, so there is no need to expand their successors.\nIt is possible, of course, to remove these if statements and let FF-LAO* continue the search; in that case, FF will be used\nas an inadmissible heuristic. However, this does not improve the theoretical properties of the algorithm (neither version is optimal), and results in higher computation times, so we prefer the version shown in the pseudocode.\nThe actual call to FF is done in Algorithm 4 (FF-BELLMAN-UPDATE). This procedure performs a Bellman update (Eq. 1) for any state (s, j) with j < k, and stores the updated cost estimate and best action in global variables V [(s, j)] and π[(s, j)], respectively (lines 6-7). For simplicity of presentation, we use the following action-value function: Q((s, j), a) ≡ C′((s, j), a)+ ∑\n(s′,j′)\nT ′((s′, j′)|(s, j), a)V [(s′, j′)]\nand assume, as is common for heuristic search algorithms, that the values V [(s′, j′)] are initialized using an admissible heuristic for S′.\nFor states (s, k), the FF-BELLMAN-UPDATE procedure creates a PDDL file1, denoted as D, representing the deterministic problem induced by M when j = k, with initial state s (CREATE-PDDL in line 3). The procedure then calls FF with input D (line 4) and memoizes costs and actions for all the states visited in the plan computed by FF (lines 5-7). More concretely, for each state si visited by this plan, we set V [(si, k)] to be the cost, according to C ′, of the plan computed by FF for that state (line 6), and set π[(si, k)] to be the corresponding action (line 7). Additionally, note that the estimates V [(s, k)] are not admissible, even with respect to the inputMk1-reduction, since FF is not an optimal planner for deterministic problems. Finally, in the case that FF returns failure, we set V [(s, k)] =∞ and π[(s, k)] = NOP.\nFF-BELLMAN-UPDATE also returns the residual, defined as the absolute difference between the previous cost estimate, and the estimate after applying the Bellman equation. This residual is used by FF-TEST-CONVERGENCE to check the stopping criterion of the algorithm.\nHandling plan deviations during execution While FF-LAO* solves Mk1-reductions, the ultimate goal is to solve the SSP from which the reduction is derived from. As mentioned before, we use S to denote this SSP. It is easy to see that a complete policy for S′ is not necessarily complete for S. Therefore, during execution we need to be able to handle deviations from the plan returned by FF-LAO*.\nWe use a replanning approach to address this issue, FF-LAO*-REPLAN, illustrated in Algorithm 5. The idea is simple: during execution, check if the current state has an action already computed with j = 0. If that’s the case, this action is executed (line 7). Otherwise, FF-LAO* is called to solve the reduced model with initial state (s, 0) (lines 5-6). FF-LAO*-REPLAN receives the choice of determinization as input (∆), and creates anMk1-reduction accordingly (line 1).\nNote that there are other choices for the replanning criterion. For example, checking if there is any j ∈ [0; k] such that (s, j) ∈ π. Another alternative is to keep track of exceptions\n1In practice, we create the PDDL file representingM before calling FF-LAO* and store its name in memory. CREATE-PDDL is shown for simplicity of presentation.\nAlgorithm 1: FF-LAO* input: S′=〈S′, A, T ′, C′, (s0, 0), G′〉, k, 1 while true do // Node expansion step 2 while true do 3 visited← ∅ 4 cnt← FF-EXPAND ( S′, (s, j), k, visited\n) 5 if cnt = 0 then\n// No tip nodes were expanded, so current policy is closed\nbreak // Convergence test step\n6 while true do 7 visited← ∅ 8 error←\nFF-TEST-CONVERGENCE ( S′, (s, j), k, visited ) 9 if error < then\nreturn // solution found 10 if error =∞ then\nbreak // change in partial policy, go back to expansion step\nAlgorithm 2: FF-EXPAND input: S′=〈S′, A, T ′, C′, (s0, 0), G′〉, (s, j), k, visited 1 if (s, j) ∈ visited then return 0 2 visited← visited ∪ {(s, j)} 3 cnt = 0 4 if π[(s, j)] = ∅ then // Expand this state for the first time 5 FF-BELLMAN-UPDATE ( S′, (s, j), k\n) 6 return 1 7 else if j < k then 8 forall (s′, j′) s.t. T ′((s′, j′)|(s, j), π[(s, j)]) > 0 do 9 cnt += FF-EXPAND ( S′, (s, j), k, visited\n) 10 FF-BELLMAN-UPDATE ( S′, (s, j), k\n) 11 return cnt\nduring execution, and set the value of j accordingly; in this case, j should be set to 0 after re-planning. Other alternatives are possible. We choose the one used by FF-LAO*-REPLAN because it is, in principle, the more robust choice, given that we use the maximum “look-ahead” every time2. However, if computational efficiency is a concern, other alternatives might be better. We leave a more in depth analysis of these choices for future work.\nTheoretical considerations We now show conditions under which FF-LAO* is guaranteed to succeed. The following definition will be useful: a proper policy rooted at s is one that reaches a goal state with probability 1 from every state it can reach from s.\n2Note that this is not guaranteed to be better than using j > 0, since pathological scenarios can be created where increasing k leads to worse plans.\nAlgorithm 3: FF-TEST-CONVERGENCE input: S′=〈S′, A, T ′, C′, (s0, 0), G′〉, (s, j), k, visited 1 if s ∈ visited then return 0 2 visited← visited ∪ {(s, j)} 3 error = 0 4 a← π[(s, j)] 5 if a = ∅ then the test reached a state that hasn’t been expanded yet 6 return∞ 7 else if j < k then 8 forall (s′, j′) s.t. T ′((s′, j′)|(s, j), π[(s, j)]) > 0 do 9 error = max ( error,\nFF-TEST-CONVERGENCE ( S′, (s, j), k, visited )) 10 error=max ( error, FF-BELLMAN-UPDATE ( S′, (s, j), k\n)) 11 if π(s, j) 6= a then 12 return∞ // the policy changed 13 return error\nAlgorithm 4: FF-BELLMAN-UPDATE input: S′=〈S′, A, T ′, C′, (s0, 0), G′〉, (s, j), k output: error 1 V ′ ← V [(s, j)] 2 if j = k then 3 D ← CREATE-PDDL(S′, s) 4 {s1, a1, s2, a2, ..., sL, aL} ← CALL-FF(D) 5 for 1 ≤ L do 6 V [(si, k)]← ∑ i≤x≤L C ′((sx, k), ai) 7 π[(si, k)]← ai\n8 else 9 V [(s, j)]← minaQ((s, j), a)\n10 π[(s, j)]← arg minaQ((s, j), a) 11 return |V [(s, j)]− V ′|\nProposition 1. Given an admissible heuristic for the reduced model S′, if S′ has at least one proper policy rooted at (s0, 0), then FF-LAO* is guaranteed to find one in finite time.\nProof. Whenever FF-LAO* expands a state (s, k) and calls FF on this state, if the call succeeds, the states si, for i ∈ [1, ..., L], that are part of the plan computed by FF essentially become terminal states of the problem, with costs set as in line 6. Since FF is a sub-optimal planner for deterministic problems, we have that ∑ i≤x≤L C\n′((sx, k), ai) ≥ V [(si, k)], and thus the values of all other states (s, j), with j < k, are guaranteed to be admissible with respect to the new updated value of the added terminal states. Therefore, after every successful call to FF, the resulting set of values and terminal states form a well-defined SSP, which LAO* is able to solve.\nMoreover, in the case that a call to FF fails for some state ŝ, this state will be assigned an infinite cost, and thus the improved version of LAO* will avoid ŝ as long there is some other path to the goal. Because FF is complete, any state belonging to a proper policy will be assigned a positive\nAlgorithm 5: FF-LAO*-REPLAN input: S=〈S,A, T, C, (s0, 0), G〉,∆, k, 1 S′ ← CREATE-REDUCTION(S,∆) 2 s← s0 3 while s /∈ G do 4 if (s, 0) /∈ π then 5 REPLACE-INITIAL-STATE(S′, (s, 0)) 6 FF-LAO*(S′, k, ) 7 s← EXECUTE-ACTION(s, π[(s, 0)])\ncost, so ŝ couldn’t have been part of a proper policy for M . Thus, under the conditions of the theorem, every call to FF transforms the problem becomes into an MDP with avoidable dead-ends [Kolobov et al., 2012], which LAO* is able to solve3.\nUnfortunately, as is the case for virtually all replanning algorithms, not much can be guaranteed about the quality of plans found by FF-LAO*-REPLAN for S. However, as we show in our experiments, by carefully choosing the input determinization, ∆ and the bound k, FF-LAO*-REPLAN can find successful policies extremely quickly, even in domains well-known for their computational hardness and the presence of dead-end states."
    }, {
      "heading" : "4 Choosing a Good Determinization",
      "text" : "Many stochastic domains have an inherent structure that make some of their determinizations significantly more effective than others. Consider, for instance, the triangle-tireworld domain [Little and Thiebaux, 2007]. The agent has to reach one of the vertices in a planar graph of triangular shape, but after every move there is the possibility of getting a flat tire (see Figure 1). If this happens, it must get a spare tire before being able to move again. However, spares are only available in certain locations, and there is only a single path from the start to the goal such that all locations in the path have spares. This domain has two possible determinizations, depending on whether a flat tire happens after moving or not. As it turns out, it is possible to get the optimal policy for this problem by planning as if a flat tire will always occur. The interesting part is that this is true for all instances of this problem, regardless of size.\nTriangle-tireworld is a great example of a domain where all domain instances share a probabilistic structure that can be captured by determinization. In practical terms, this means that it is possible to learn a determinization on the smaller problems, and then use it for solving larger ones.\nWe adopt this approach for choosing the input determinization to FF-LAO*-REPLAN, ∆. We assume a set of problems of varying sizes are available and that it is easy to identify the smaller ones. This is the case for the IPPC benchmarks that we consider in our experiments, as they are typically ordered\n3While this is not true for the original version of LAO*, this is true of the so-called improved version of LAO* that we use in this work, which performs Bellman backups of states in depth-first fashion, in post order traversal.\nAlgorithm 6: LEARNING-DET input: D, Sl, k output: ∆ 1 {∆1, ...,∆M} ← Create all possible determinizations of D 2 forall i ∈ {1, ...,M} do 3 Pi,Ci ← Estimate probability of successs and expected cost of executing FF-LAO*-REPLAN(Sl,∆i, k) 4 P ∗ ← maxi Pi 5 ∆← ∆mini Ci s.t. Pi=P∗\nby problem size/difficulty. However, in general, some analysis of the problem might be required, for example, by counting the number of possible grounded atoms that the problem description induces. Another possibility is to generate small problems automatically from the domain description. This has the advantage that it doesn’t require additional problems for learning the determinization to use (besides the actual problem to solve). Although this is certainly an interesting possibility, it requires some finesse and it is outside the scope of this work.\nAlgorithm 6 illustrates LEARNING-DET, a brute-force approach to learn a determinization ∆ for domain D; Sl represents the problem used for learning. This procedure does a comprehensive search over the space of all determinizations. For each, we estimate the probability of success (Pi) and the expected execution cost (Ci) of running FF-LAO*-REPLAN on Sl; the costs are estimated using Monte-Carlo simulations. Finally, we pick the determinization with the lowest expected cost, among the ones with the highest probability of success.\nThere are some subtleties involved in this process. Note that FF-LAO* is only guaranteed to terminate if the input reduction has a proper policy from its initial state. This will most likely not be the case for many of the determinizations explored by LEARNING-DET; in fact, under some determinizations the goals might be completely unreachable from any state.\nWe address this issue using a well-known technique for planning with problems involving dead-ends. In particular, we use a cap M on state costs, including the costs assigned when FF fails, and modify the Bellman backup operator used\nby FF-LAO* as V (s) = min { M,min\na∈A\n{ C(s, a) + ∑ s′∈S T (s′|s, a)V (s′) }}\nwhich guarantees the convergence of heuristic search algorithms [Kolobov et al., 2012]. While this introduces a new parameter impacting the planner’s decisions, and hides the true impact of dead-end states. Note that LEARNING-DET still attempts to maximize the multi-objective evaluation criterion typically used when unavoidable dead-ends exist [Kolobov et al., 2012; Steinmetz et al., 2016]."
    }, {
      "heading" : "5 Experiments",
      "text" : "Domains and methodology We evaluated FF-LAO* and LEARNING-DET on a set of problems taken from IPPC’08 [Bryce and Buffet, 2008]. Specifically, we used the first 10 problem instances of the following four domains: TRIANGLE-TIREWORLD, BLOCKSWORLD, EX-BLOCKSWORLD, and ZENOTRAVEL. Unfortunately, the rest of the IPPC’08 domains are not supported by our PPDDL parser [Bonet and Geffner, 2005]. Additionally, we modified the EX-BLOCKSWORLD domain to avoid the possibility of blocks to be put on top of themselves [Trevizan and Veloso, 2014].\nThe evaluation methodology was similar to the one used in past planning competitions: we give each planner 20 minutes to solve 50 rounds of each problem (i.e., reach a goal state starting from the initial state). Then we measure its performance in terms of the number of rounds that the planner was able to solve during that time. All experiments were conducted on an Intel Core i7-6820HQ machine running at 2.70GHz with a 4GB memory cutoff.\nWe evaluated the planners using the MDPSIM [Younes et al., 2005] client/server program for simulating SSPs, by having planners repeatedly perform the following three steps: 1) connect to the MDPSIM server to receive a state, 2) compute an action for the received state and send the action to the MDPSIM server, and 3) wait for the server to simulate the result of applying this action and send a new state. A simulation ends when a goal state is reached, when an invalid action is sent by the client, or after 2500 actions have been sent by the planner.\nWe compared the performance of FF-LAO* with our own implementations of FF-REPLAN and RFF, as well as the original author’s implementation of SSIPP [Trevizan and Veloso, 2014]. We evaluated two variants of FF-REPLAN, one using MLO (FFs) and another one using AO (FFa). For RFF we used MLO and the Random Goals variant, in which before every call to FF, a random subset (size 100) of the previously solved states are added as goal states. Additionally, we used a probability threshold ρ = 0.2. The choice of these parameters was informed by analysis in the original work [TeichteilKönigsbuch et al., 2010]. For SSIPP we used t = 3 and the hadd heuristic, parameters also informed by the original work [Trevizan and Veloso, 2014].\nFor FF-LAO*, we learned a good determinization to use by applying LEARNING-DET on the first problem of each\ndomain (p01), with k = 0. This choice of k was motivated both by time considerations, and by the rationale that k = 0 should better reflect the impact of each determinization (since FF-LAO* becomes a fully determinization-based planner). We used a dead-end cap D = 500 throughout our experiments. We initialized values with the non-admissible FF heuristic [Bonet and Geffner, 2005].\nWe ran LEARNING-DET offline, prior to the MDPSIM evaluation. Note, however, that the time taken by the brute force search plus the time used to solve problem p01 with the chosen determinization was, in all cases, well below the 20 minutes limit (approx. 2 minutes in the worst case). The remaining parameter for FF-LAO* is the value of k. We report the best performing configuration in the range k ∈ [0, 3], which was k = 0 for most domains, with the exception of EX-BLOCKSWORLD, which required k = 3. Note that FF-LAO* with k = 0 is essentially equivalent to FF-REPLAN, so any advantage obtained over FFs and FFa is completely derived from the choice of determinization.\nResults and Discussion Figure 2 shows the number of successful rounds obtained by each planner in the benchmarks. In general, FF-LAO* either tied for the best, or outperformed the baselines. All planners had a 100% success rate in BLOCKSWORLD, so there is not much room for comparison.\nIn the TRIANGLE-TIREWORLD domain, FF-LAO* and FFs had 100% success rate, while RFF ran out of time in the last 3 problems. On the other hand, the performance of SSIPP and FFa deteriorated quickly as the problem instance increased. It is worth pointing out that the performances of FFs and RFF in this domain are quite sensitive to tiebreaking—there are only two outcomes to choose from, each occurring with 0.5 probability. As the results of FFa suggest, a different choice would have resulted in a much worse success rate. On the other hand, the use of LEARNING-DET gets around this issue by automatically choosing the best determinization to use, a process that took seconds. While we do note that the best goals parameterization of RFF gets around this issue, its computational cost is much harder, so it’s not obvious that it would actually improve performance in this case [Teichteil-Königsbuch et al., 2010].\nIn the EX-BLOCKSWORLD domain FF-LAO* (with k = 3) and SSIPP significantly outperform the other two planners, solving 252 and 250 rounds, respectively, against 187 for both FFs and RFF, and 200 for FFa. Interestingly, in this domain the determinization found by LEARNING-DET is not sufficient to obtain good performance; in fact, only 3 problems had a non-zero success rate with k = 0. This highlights the utility of doing probabilistic reasoning with FF-LAO*. Although not shown here for space considerations, the performance with k = 1 (214 successful rounds) was already better than all the baselines, except for SSIPP.\nIn ZENOTRAVEL, FF-LAO* and FFa were remarkably better than the other two planners: they achieved 100% success rate in all domain instances, while the other baselines failed almost all of the rounds. In the case of the determinization-based planners, this is due to the goal becoming unreachable under MLO, so the choice of determinization\nhas a significant impact on performance."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this work we presented a novel perspective on the use of determinization for probabilistic planning, by showing that a careful choice of determinization can outperform stateof-the-art planners. We also introduced a new planner, FF-LAO*, that, given a choice of determinization, leverages the power of classical planning algorithms for computational efficiency, but can also reason probabilistically if desired.\nWe proposed a strategy for selecting a determinization that takes advantage of the inherent structure of a given stochastic domain. We show that the choice of determinization can generalize across problems of varying size, in particular, in terms of its impact on planning performance (probability of success and expected cost).\nWe compared our approach to state-of-the-art planners for goal-oriented probabilistic problems, using a set of benchmarks taken from the International Probabilistic Planning Competition. Our results strongly support the claim that the choice of determinization can lead to very substantial gains in performance, and position FF-LAO*—using our determinization learning approach—as a competitive planner for large stochastic domains.\nIn future work, we plan to explore ways to vary the choice of determinization according to state-space features, and to develop automated methods for generating problems useful for learning a good determinization of a given domain."
    }, {
      "heading" : "7 Acknowledgment",
      "text" : "This work was supported in part by NSF Grant No. 1524797."
    } ],
    "references" : [ {
      "title" : "Mathematics of Operations Research",
      "author" : [ "Dimitri P. Bertsekas", "John N. Tsitsiklis. An analysis of stochastic shortest path problems" ],
      "venue" : "16(3):580–595,",
      "citeRegEx" : "Bertsekas and Tsitsiklis. 1991",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Neuro-dynamic programming: an overview",
      "author" : [ "Dimitri P. Bertsekas", "John N. Tsitsiklis" ],
      "venue" : "Proceedings of the 34th IEEE Conference on Decision and Control, pages 560–564,",
      "citeRegEx" : "Bertsekas and Tsitsiklis. 1995",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "mGPT: A probabilistic planner based on heuristic search",
      "author" : [ "Blai Bonet", "Héctor Geffner" ],
      "venue" : "Journal of Artificial Intelligence Research, 24:933–944,",
      "citeRegEx" : "Bonet and Geffner. 2005",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Sixth international planning competition: Uncertainty part",
      "author" : [ "Daniel Bryce", "Olivier Buffet" ],
      "venue" : "Proceedings of the Sixth International Planning Competition (IPC’08),",
      "citeRegEx" : "Bryce and Buffet. 2008",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Modeling wildfire propagation with the stochastic shortest path: A fast simulation approach",
      "author" : [ "Mohammad Hajian", "Emanuel Melachrinoudis", "Peter Kubat" ],
      "venue" : "Environmental Modelling & Software, 82:73–88,",
      "citeRegEx" : "Hajian et al.. 2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "LAO*: A heuristic search algorithm that finds solutions with loops",
      "author" : [ "Eric A. Hansen", "Shlomo Zilberstein" ],
      "venue" : "Artificial Intelligence, 129(1-2):35– 62,",
      "citeRegEx" : "Hansen and Zilberstein. 2001",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "The FF planning system: Fast plan generation through heuristic search",
      "author" : [ "Jörg Hoffmann", "Bernhard Nebel" ],
      "venue" : "Journal of Artificial Intelligence Research, 14(1):253–302,",
      "citeRegEx" : "Hoffmann and Nebel. 2001",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "editors",
      "author" : [ "Emil Keyder", "Hector Geffner. The HMDPP planner for planning with probabilities. In D. Bryce", "O. Buffet" ],
      "venue" : "ICAPS Third International Probabilistic Planning Competition. IPPC’08,",
      "citeRegEx" : "Keyder and Geffner. 2008",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "pages 438–447",
      "author" : [ "Andrey Kolobov", "Mausam", "Daniel S. Weld. A theory of goal-oriented MDPs with dead ends. In Proceedings of the Conference on Uncertainty in Artificial Intelligence" ],
      "venue" : "Catalina Island, California,",
      "citeRegEx" : "Kolobov et al.. 2012",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Practical route planning under delay uncertainty: Stochastic shortest path queries",
      "author" : [ "Sejoon Lim", "Christian Sommer", "Evdokia Nikolova", "Daniela Rus" ],
      "venue" : "Proceedings of Robotics: Science and Systems, volume 8, pages 249–256,",
      "citeRegEx" : "Lim et al.. 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "In Proceedings of the ICAPS’07 Workshop on the International Planning Competition: Past",
      "author" : [ "Iain Little", "Sylvie Thiebaux. Probabilistic planning vs. replanning" ],
      "venue" : "Present and Future,",
      "citeRegEx" : "Little and Thiebaux. 2007",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Probabilistic propositional planning: Representations and complexity",
      "author" : [ "Michael L. Littman" ],
      "venue" : "Proceedings of the Fourteenth National Conference on Artificial Intelligence, pages 748–754, Providence, Rhode Island,",
      "citeRegEx" : "Littman. 1997",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Planning under uncertainty using reduced models: Revisiting determinization",
      "author" : [ "Pineda", "Zilberstein", "2014] Luis Pineda", "Shlomo Zilberstein" ],
      "venue" : "In Proceedings of the 24th International Conference on Automated Planning and Scheduling,",
      "citeRegEx" : "Pineda et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Pineda et al\\.",
      "year" : 2014
    }, {
      "title" : "Markov Decision Processes: Discrete Stochastic Dynamic Programming",
      "author" : [ "Martin L. Puterman" ],
      "venue" : "John Wiley & Sons, Inc., New York, NY, USA,",
      "citeRegEx" : "Puterman. 1994",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Revisiting goal probability analysis",
      "author" : [ "Steinmetz et al", "2016] Marcel Steinmetz", "Joerg Hoffmann", "Olivier Buffet" ],
      "venue" : null,
      "citeRegEx" : "al. et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2016
    }, {
      "title" : "IEEE Transactions on Smart Grid",
      "author" : [ "Xiaoqi Tan", "Yuan Wu", "Danny H.K. Tsang. A stochastic shortest path framework for quantifying the value", "lifetime of battery energy storage under dynamic pricing" ],
      "venue" : "pages 769–778,",
      "citeRegEx" : "Tan et al.. 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Incremental plan aggregation for generating policies in MDPs",
      "author" : [ "Teichteil-Königsbuch et al", "Ugur Kuter", "Guillaume Infantes" ],
      "venue" : "In Proceedings of the Ninth International Conference on Autonomous Agents and Multiagent Systems,",
      "citeRegEx" : "al. et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2010
    }, {
      "title" : "Artificial Intelligence",
      "author" : [ "Felipe W Trevizan", "Manuela M Veloso. Depth-based short-sighted stochastic shortest path problems" ],
      "venue" : "216:179– 205,",
      "citeRegEx" : "Trevizan and Veloso. 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "In Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems",
      "author" : [ "Kyle Hollins Wray", "Luis Pineda", "Shlomo Zilberstein. Hierarchical approach to transfer of control in semi-autonomous systems" ],
      "venue" : "pages 1285–1286,",
      "citeRegEx" : "Wray et al.. 2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "FF-Replan: A baseline for probabilistic planning",
      "author" : [ "Sung Wook Yoon", "Alan Fern", "Robert Givan" ],
      "venue" : "Proceedings of the Seventeenth International Conference on Automated Planning and Scheduling, pages 352– 359, Providence, Rhode Island,",
      "citeRegEx" : "Yoon et al.. 2007",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "pages 1010–1016",
      "author" : [ "Sungwook Yoon", "Alan Fern", "Robert Givan", "Subbarao Kambhampati. Probabilistic planning via determinization in hindsight. In Proceedings of the Twenty-Third National Conference on Artificial Intelligence" ],
      "venue" : "Chicago, Illinois,",
      "citeRegEx" : "Yoon et al.. 2008",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Littman",
      "author" : [ "Håkan L.S. Younes", "Michael L" ],
      "venue" : "PPDDL1.0: An extension to PDDL for expressing planning domains with probabilistic effects. Technical Report CMU-CS-04-162,",
      "citeRegEx" : "Younes and Littman. 2004",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Journal of Artificial Intelligence Research",
      "author" : [ "Håkan L.S. Younes", "Michael L. Littman", "David Weissman", "John Asmuth. The first probabilistic track of the international planning competition" ],
      "venue" : "24(1):851–887,",
      "citeRegEx" : "Younes et al.. 2005",
      "shortCiteRegEx" : null,
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "One of the most popular models for probabilistic planning is the Stochastic Shortest Path SSP) [Bertsekas and Tsitsiklis, 1991].",
      "startOffset" : 95,
      "endOffset" : 127
    }, {
      "referenceID" : 9,
      "context" : "This model has been used for a wide variety of applications, such as route planning in the presence of traffic delays [Lim et al., 2013], quantifying the value of battery energy storage systems [Tan et al.",
      "startOffset" : 118,
      "endOffset" : 136
    }, {
      "referenceID" : 15,
      "context" : ", 2013], quantifying the value of battery energy storage systems [Tan et al., 2015], modeling wildfire propagation [Hajian et al.",
      "startOffset" : 65,
      "endOffset" : 83
    }, {
      "referenceID" : 4,
      "context" : ", 2015], modeling wildfire propagation [Hajian et al., 2016], and semi-autonomous driving [Wray et al.",
      "startOffset" : 39,
      "endOffset" : 60
    }, {
      "referenceID" : 18,
      "context" : ", 2016], and semi-autonomous driving [Wray et al., 2016].",
      "startOffset" : 37,
      "endOffset" : 56
    }, {
      "referenceID" : 19,
      "context" : "These methods became popular thanks to the unexpected success of FF-Replan [Yoon et al., 2007] in the International Probabilistic Planning Competition (IPPC) [Bryce and Buffet, 2008].",
      "startOffset" : 75,
      "endOffset" : 94
    }, {
      "referenceID" : 3,
      "context" : ", 2007] in the International Probabilistic Planning Competition (IPPC) [Bryce and Buffet, 2008].",
      "startOffset" : 71,
      "endOffset" : 95
    }, {
      "referenceID" : 6,
      "context" : ", by only considering most probable outcomes), and using the FF classical planner [Hoffmann and Nebel, 2001] to solve the resulting problems.",
      "startOffset" : 82,
      "endOffset" : 108
    }, {
      "referenceID" : 20,
      "context" : "FF-Hindsight [Yoon et al., 2008] works by sampling a set of deterministic “futures” of the original problem, solves each using FF, and combines their cost to estimate the costs in the original problem.",
      "startOffset" : 13,
      "endOffset" : 32
    }, {
      "referenceID" : 7,
      "context" : "HMDPP [Keyder and Geffner, 2008] introduces a self-loop determinization trick that nudges the deterministic planner into generating plans with low probability of deviation.",
      "startOffset" : 6,
      "endOffset" : 32
    }, {
      "referenceID" : 17,
      "context" : "SSiPP-FF [Trevizan and Veloso, 2014] works by creating short-sighted problems that consider only states up to a certain horizon from the current state.",
      "startOffset" : 9,
      "endOffset" : 36
    }, {
      "referenceID" : 10,
      "context" : "In fact, even in some domains deemed “probabilistically interesting” [Little and Thiebaux, 2007] planning with a good determinization can actually result in optimal plans for the original SSP (e.",
      "startOffset" : 69,
      "endOffset" : 96
    }, {
      "referenceID" : 5,
      "context" : "First, we present a new planner, FF-LAO*, that combines the LAO* optimal SSP solver [Hansen and Zilberstein, 2001] with the FF classical planner.",
      "startOffset" : 84,
      "endOffset" : 114
    }, {
      "referenceID" : 0,
      "context" : "A Stochastic Shortest Path (SSP) problem [Bertsekas and Tsitsiklis, 1991] is defined by a tuple 〈S,A, T,C, s0, G〉, where S is a finite set of states, A is a finite set of actions, T (s′|s, a) ∈ [0, 1] represents the probability of reaching state s′ when action a is taken in state s, C(s, a) ∈ [0,∞) is the cost of applying action a in state s, s0 is an initial state and G is a set of goal states satisfying ∀sg ∈ G, a ∈",
      "startOffset" : 41,
      "endOffset" : 73
    }, {
      "referenceID" : 13,
      "context" : "Interestingly, SSPs are a variant of Markov Decision Process (MDPs) [Puterman, 1994] that has been shown to be more general than finite-horizon and infinite-horizon discounted MDPs [Bertsekas and Tsitsiklis, 1995].",
      "startOffset" : 68,
      "endOffset" : 84
    }, {
      "referenceID" : 1,
      "context" : "Interestingly, SSPs are a variant of Markov Decision Process (MDPs) [Puterman, 1994] that has been shown to be more general than finite-horizon and infinite-horizon discounted MDPs [Bertsekas and Tsitsiklis, 1995].",
      "startOffset" : 181,
      "endOffset" : 213
    }, {
      "referenceID" : 21,
      "context" : "(1) There is a variety of languages to compactly describe SSPs, of which PPDDL [Younes and Littman, 2004] has been most widely used within the AI community.",
      "startOffset" : 79,
      "endOffset" : 105
    }, {
      "referenceID" : 11,
      "context" : "As it turns out, it has been shown that finding whether a plan exists in a compactly described problem is EXPTIME-complete [Littman, 1997].",
      "startOffset" : 123,
      "endOffset" : 138
    }, {
      "referenceID" : 3,
      "context" : "In fact, for complex domains like the ones used in IPPC [Bryce and Buffet, 2008], using determinization and k = 0 already results in problems too large to be solved optimally in a practical manner.",
      "startOffset" : 56,
      "endOffset" : 80
    }, {
      "referenceID" : 5,
      "context" : "We call this planner FF-LAO*, as it is an extension of the LAO* algorithm [Hansen and Zilberstein, 2001] that leverages the FF classical planner [Hoffmann and Nebel, 2001] to accelerate computation.",
      "startOffset" : 74,
      "endOffset" : 104
    }, {
      "referenceID" : 6,
      "context" : "We call this planner FF-LAO*, as it is an extension of the LAO* algorithm [Hansen and Zilberstein, 2001] that leverages the FF classical planner [Hoffmann and Nebel, 2001] to accelerate computation.",
      "startOffset" : 145,
      "endOffset" : 171
    }, {
      "referenceID" : 8,
      "context" : "Thus, under the conditions of the theorem, every call to FF transforms the problem becomes into an MDP with avoidable dead-ends [Kolobov et al., 2012], which LAO* is able to solve3.",
      "startOffset" : 128,
      "endOffset" : 150
    }, {
      "referenceID" : 10,
      "context" : "Consider, for instance, the triangle-tireworld domain [Little and Thiebaux, 2007].",
      "startOffset" : 54,
      "endOffset" : 81
    }, {
      "referenceID" : 8,
      "context" : "which guarantees the convergence of heuristic search algorithms [Kolobov et al., 2012].",
      "startOffset" : 64,
      "endOffset" : 86
    }, {
      "referenceID" : 8,
      "context" : "Note that LEARNING-DET still attempts to maximize the multi-objective evaluation criterion typically used when unavoidable dead-ends exist [Kolobov et al., 2012; Steinmetz et al., 2016].",
      "startOffset" : 139,
      "endOffset" : 185
    }, {
      "referenceID" : 3,
      "context" : "We evaluated FF-LAO* and LEARNING-DET on a set of problems taken from IPPC’08 [Bryce and Buffet, 2008].",
      "startOffset" : 78,
      "endOffset" : 102
    }, {
      "referenceID" : 2,
      "context" : "Unfortunately, the rest of the IPPC’08 domains are not supported by our PPDDL parser [Bonet and Geffner, 2005].",
      "startOffset" : 85,
      "endOffset" : 110
    }, {
      "referenceID" : 17,
      "context" : "Additionally, we modified the EX-BLOCKSWORLD domain to avoid the possibility of blocks to be put on top of themselves [Trevizan and Veloso, 2014].",
      "startOffset" : 118,
      "endOffset" : 145
    }, {
      "referenceID" : 22,
      "context" : "We evaluated the planners using the MDPSIM [Younes et al., 2005] client/server program for simulating SSPs, by having planners repeatedly perform the following three steps: 1) connect to the MDPSIM server to receive a state, 2) compute an action for the received state and send the action to the MDPSIM server, and 3) wait for the server to simulate the result of applying this action and send a new state.",
      "startOffset" : 43,
      "endOffset" : 64
    }, {
      "referenceID" : 17,
      "context" : "We compared the performance of FF-LAO* with our own implementations of FF-REPLAN and RFF, as well as the original author’s implementation of SSIPP [Trevizan and Veloso, 2014].",
      "startOffset" : 147,
      "endOffset" : 174
    }, {
      "referenceID" : 17,
      "context" : "For SSIPP we used t = 3 and the hadd heuristic, parameters also informed by the original work [Trevizan and Veloso, 2014].",
      "startOffset" : 94,
      "endOffset" : 121
    }, {
      "referenceID" : 2,
      "context" : "We initialized values with the non-admissible FF heuristic [Bonet and Geffner, 2005].",
      "startOffset" : 59,
      "endOffset" : 84
    } ],
    "year" : 2017,
    "abstractText" : "The stochastic shortest path problem (SSP) is a highly expressive model for probabilistic planning. The computational hardness of SSPs has sparked interest in determinization-based planners that can quickly solve large problems. However, existing methods employ a simplistic approach to determinization. In particular, they ignore the possibility of tailoring the determinization to the specific characteristics of the target domain. In this work we examine this question, by showing that learning a good determinization for a planning domain can be done efficiently and can improve performance. Moreover, we show how to directly incorporate probabilistic reasoning into the planning problem when a good determinization is not sufficient by itself. Based on these insights, we introduce a planner, FF-LAO*, that outperforms stateof-the-art probabilistic planners on several wellknown competition benchmarks.",
    "creator" : "LaTeX with hyperref package"
  }
}