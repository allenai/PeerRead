{
  "name" : "1204.2718.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Leveraging Usage Data for Linked Data Movie Entity Summarization",
    "authors" : [ "Andreas Thalhammer", "Ioan Toma", "Antonio J. Roa-Valverde", "Dieter Fensel" ],
    "emails" : [ "firstname.lastname@sti2.at" ],
    "sections" : [ {
      "heading" : null,
      "text" : "The proposed approach of this paper focuses on the movie domain. It utilizes usage data in order to support measuring the similarity between movie entities. Using this similarity it is possible to determine the k-nearest neighbors of an entity. This leads to the idea that features that entities share with their nearest neighbors can be considered as significant or important for these entities. Additionally, we introduce a downgrading factor (similar to TF-IDF) in order to overcome the high number of commonly occurring features. We exemplify the approach based on a movie-ratings dataset that has been linked to Freebase entities.\nCategories and Subject Descriptors H.1.2 [User/Machine Systems]: Human factors—Usage Data Mining ; H.3.5 [On-line Information Services]: Data sharing—Linked Open Data\nGeneral Terms Human Factors, Experimentation\nKeywords linked data, entity summarization, ranking, item similarity"
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "Linked Data, which connects different pieces of machinereadable information (resources) via machine-readable relationships (properties) has rapidly grown in the past years, changing the way data is published and consumed on the Web. Data referring to real-world entities is being linked resulting into vast network of structured, interlinked descriptions that can be used to infer new knowledge. The rapid growth of Linked Data (LD) introduces however a set of new challenges. One in particular becomes very important when it comes to characterizing real world entities: their LD descriptions need to be processed and understood quickly and\neffectively. The problem known as entity summarization [5] is concerned with identifying the most important features of lengthy LD or Linked Open Data (LOD)1 descriptions. Solutions to this problem help applications and users of LD to quickly and effectively understand and work with the vast amount of data from LOD cloud.\nIn this paper we propose a novel approach that leverages usage data in order to summarize entities in the LOD space. More precisely, we perform data analysis on LD in order to identify features of entities that best characterize them. Our approach is simple and effective. We first measure similarities between entities and identify a set of nearest neighbors for each entity. For each feature of the entity we then count the number of entities having the same feature in the nearest neighbors group as well as in the set of all entities. Based on this we compute a weight for each entity, order the entities descending and select the top-n features as the summarization for each entity. To validate our approach we run a set of experiments using two datasets, namely the HetRec2011 MovieLens2k dataset [4] and data crawled from Freebase.2 Results obtained from these datasets show that our approach is capable to identify relevant features that are shared with similar entities and thus provide meaningful summarizations.\nThe remainder of this paper is organized as follows. Section 2 details our approach on leveraging usage data for linked data movie entity summarization. Section 3 presents the related work in the areas of entity summarization, usage mining and semantic representation of user profiles. Section 4 introduces the datasets used in our experiments while Section 5 discusses the preliminary results obtained, focusing more on the neighborhood formation and neighborhoodbased entity summarization results. Finally, Section 6 concludes the paper and Section 7 outlines future work that we plan based on the approach presented in this paper.\nPlease note, we use the terms item and entity interchangeable in this paper."
    }, {
      "heading" : "2. PROPOSED APPROACH",
      "text" : "The main idea introduced in this work is that propertyvalue pairs - consecutively also called features - that an entity shares with its k-nearest neighbors are more relevant than features that are shared with entities that are not in the k-nearest neighbors range. Figure 1 visualizes this situ-\n1http://www.w3.org/wiki/SweoIG/TaskForces/ CommunityProjects/LinkingOpenData 2http://freebase.com/\nar X\niv :1\n20 4.\n27 18\nv1 [\ncs .A\nI] 1\n2 A\npr 2\n01 2\nation. Two nodes (green and blue) of the same type (M) are in each other’s neighborhood. The features shared with each other (strong lines and dark gray nodes) are considered to be more important for their idendity than features they share with a node (light gray M) that is not in their respective neighborhood. The neighborhood formation of each node is based on usage data.\nA detailed problem statement of entity summarization is given in [5]. The authors of this paper define the summarization of an entity e as follows:\n“Given FS(e) and a positive integer k < |FS(e)|, the problem of entity summarization is to select Summ(e) ⊂ FS(e) such that |Summ(e)| = k. Summ(e) is called a summary of e.”3\nFS(e) denotes the feature set of a given entity e. More informally, the feature set of an entity e is defined as the property-value pair set of e. An example for such a propertyvalue pair for the entity fb : en.toy story4 is:\n(fb:film.film.production_companies, fb:en.pixar)\nIn the following, E denotes the set of all entities. Our approach to provide a summarization of a given entity e ∈ E is based on usage data and includes six steps:\n1. Generate the user-item matrix.\n2. Measure the similarity between e and other items and identify a set Nk,e ⊆ E of k-nearest neighbors of e.\n3. For each feature f ∈ FS(e) collect the items Ae,f ⊆ Nk,e that share the same feature.\n3In our approach, k is already used for the k-nearest neighbors method. Therefore, we refer to the cardinality of the summarization as n. 4fb denotes the Freebase namespace: http://rdf. freebase.com/ns/\n4. For each feature f ∈ FS(e) collect the items Be,f ⊆ E that share the same feature.\n5. The weight w of f is the following ratio:\nwe(f) = |Ae,f | × log |E| |Be,f |\n6. Order the features f ∈ FS(e) descending according to their given weight we(f). Select the n most relevant features as a summarization of e.\nThe concept of a user-item matrix (step 1) is a well-known principle in the field of recommender systems. Each column of the matrix represents a single item and each row represents a single user. The entries of the matrix are either the ratings (a numerical score) or empty if a user has not rated a particular item (which is the standard case). The column or row vectors can be used to compare items or users amongst each other respectively. For this, several similarity measures have been introduced of which cosine similarity and Pearson correlation (comparing the vectors with regard to their angular distance) are the most common techniques [1].\nIn our current implementation, we apply the log-likelihood ratio score [8] for computing item similarity (step 2). In the context of item similarity, the ratio takes into account four parameters: the number of users who rated both items, the number of users who rated the first but not the second item and vice versa, and the number of users who rated none of the two items. Note that this similarity measure does not consider the numerical values of the ratings and therefore also works with binary data like web site visits.5 Finally, with the similarity scores it is easy to identify a set of knearest neighbors (kNN) for a given item.\nListing 1 states a SPARQL6 query that is used for the retrieval of common features (property-value pairs) between the item (fb:movie.uri) and its 20 nearest neighbors (step 3). For measuring the similarity to all items in the dataset (step 4), the same query can be executed but without line 3. For each of the two result sets, the property-value pairs can be counted by occurrence. The filter rule (line 7) filters out property-value pairs that stem from the given entity (fb:movie.uri). Additionally, we also filter out the commonality of similar nearest neighbors because those features were added in the course of applying the approach and do not contribute to the summarization of the given entity.\nIn the result set of the nearest neighbors, a lot of features are frequently occurring; such as the following propertyvalue pair:\n(fb:film.film.country, fb:en.united_states)\nIf the weighting involved only counting, features like the above would be considered as highly relevant for many movies. However, as these features do not only occur often in the neighbors set but also in the overall set, they can be downgraded (step 5). As for the downgrading technique, we use the idea of the classic information retrieval method term frequency - inverse document frequency (TF-IDF). In our case\n5This is the reason why we refer to the term “usage data” rather than “rating data”: we conclude usage from the process of giving a rating. We do not consider the numerical values of the ratings. 6SPARQL W3C Recommendation - http://www.w3.org/ TR/rdf-sparql-query/\nListing 1: SPARQL query: retrieving propertyvalue pairs shared with at least one of the 20-nearest neighbors.\n1 select ?p ?o where { 2 fb:movie.uri ?p ?o. 3 fb:movie.uri knn:20 ?s. 4 ?s ?p ?o. 5 ?s rdf:type fb:film.film. 6 FILTER((?s != fb:movie.uri) && (?p != knn:20)) 7 }\na “term” is stated by a single feature and the term frequency is the frequency of the feature in the nearest neighbors set. After this step, every feature that is shared with at least one of the k-nearest neighbors has an assigned weight.\nFinally, in step 6, we select the n most relevant propertyvalue pairs in accordance to their weight."
    }, {
      "heading" : "3. RELATED WORK",
      "text" : "In the field of entity summarization, initial work has been presented in [5], where an approach called RELIN is introduced. The authors apply an adapted version of the random surfer model7 - called goal directed surfer - in order to combine informativeness and relatedness for the ranking of features. In the conclusion, it is stated that a “user-specific notion of informativeness (...) could be implemented by leveraging user profiles or feedback” in order to mitigate the issue of presenting summarizations that help domain experts but not average users. Our approach can be considered as a first step into this direction as it focuses on leveraging usage data for providing summarizations. Our summarizations are not adapted to each user individually but present a consensus that has been reached by similar behavior in the past.\n[7] uses combines hierarchical link analysis with weighted link analysis. For the latter, the authors suggest to combine PageRank with a TF-IDF-related weighting scheme. In this work, usage or feedback data is not considered as an additional source of information.\nIn the field of recommender systems, [9] propose an approach based on Latent Dirichlet Allocation (LDA) [2] for discovering hidden semantic relationships between items. This includes the extraction of what is considered to be the most important feature of an item (e.g. genre: adventure). The approach is exemplified on a movie and a real estate dataset.\nIn the field of user modeling, there exist several approaches for leveraging (weighted) semantic knowledge about items [6, 11, 10]. The approach presented in [6] proposes an aggregated presentation of user profiles by extracting and combining the domain knowledge of different items. [11] models users and items each as a feature matrix. For feature weighting in the user profile, an adapted version of TF-IDF is introduced. In the recommendation approach, the authors form neighborhoods of users based on the user-feature matrix. [10] introduces an impact measure that indicates the influences on user behavior by item features modeled as a domain ontology. The approach is presented with examples from the movie domain.\n7See also PageRank [3]."
    }, {
      "heading" : "4. DATASET",
      "text" : "For the preparation of first tests, we combined the usage data of the HetRec2011 MovieLens2k dataset [4] with Freebase.8 The usage dataset extends the original MovieLens10M dataset9 by additional metadata: directors, actors, countries, and locations have been added to the original dataset. Although this dataset already contains valuable material to perform our tests without making use of LOD (i.e. Freebase), the search space for properties and objects is very restricted. In particular, 26 properties (the four mentioned above plus 22 other properties such as the genre, year, Spanish title, rotten tomatoes10 rating etc.) are opposed to more than 240 Freebase properties. Also, the range in Freebase is much broader as - for example - more than 380 different genres (fb:film.film.genre) are covered in contrast to 20 fixed genres contained in the HetRec2011 MovieLens2k dataset.\nThe HetRec2011 MovieLens2k dataset includes IMDb11 identifiers for each movie. This makes the linking to Freebase easy as querying12 for the IMDb identifier is simple (see listing 2). Given only this query, we were able to match more than 10000 out of 10197 movies.13\nFor performance reasons, we crawled the RDF-XML14 representation from Freebase15 and stored it to a local triple store. Using the usage data, we computed the 20-nearest neighbors for each movie and stored the results also in the\n8http://freebase.com 9http://www.grouplens.org\n10http://www.rottentomatoes.com/ 11http://www.imdb.com/ 12Freebase uses a special query language called Metaweb Query Language (MQL). 13Unmatched items are mostly TV series that do not match the pattern \"type\"=\"film/film/\". 14http://www.w3.org/TR/REC-rdf-syntax/ 15http://rdf.freebase.com/\nListing 2: MQL query: retrieving the Freebase identifiers given an IMDb identifier.\n1 { 2 \"id\"= null, 3 \"imdb_id\"=\"ttIMDb_ID\", 4 \"type\"= \"/film/film\" 5 }\ntriple store; like in the following example:\n(fb:en.pulp_fiction, knn:20, fb:en.reservoir_dogs)\nUsing SPARQL queries (like in listing 1) we are able to retrieve common properties between single movies and their neighbors. The results of first tests with this setup are discussed in the following section."
    }, {
      "heading" : "5. PRELIMINARY RESULTS",
      "text" : "With the created dataset, we were able to identify and rank features that connect an entity to one of their nearest neighbors. We do not plan to conduct a separate evaluation at the level of neighborhood quality but we are currently in the process of performing comparisons on the level of quality of summarizations. In this analysis, we are also conducting different similarity measures as well as estimating the optimal size of the neighborhood. At the current stage of our work, statistics for the presentation of these results have not been produced.\nWe will discuss our findings regarding the neighborhood formation in section 5.1. Moreover, preliminary results of the entity summarization approach are presented in section 5.2."
    }, {
      "heading" : "5.1 Neighborhood formation",
      "text" : "One of the most important steps is the neighborhood formation dependent solely on usage data. An example for such a neighborhood is presented in table 1. In general the presented neighborhood of the movie “Beauty and the Beast” fits the perception of most observers and also overlaps with related movies presented in IMDb.16 The scores presented in table 1 are all very close to each other and every score is also close to a perfect match (1.0). In this respect, the question arises whether the k-nearest neighbor approach makes sense with such dense scores. An alternative could be to introduce a threshold rather than just selecting a fixed amount of neighbors (e.g. all movies that have a similarity higher than 0.95). As a matter of fact, the runtime of the SPARQL queries would turn into a gambling game as it can not be decided in advance whether there are 10 or 500 neighbors that cross the threshold. Another approach to address this question would be to introduce different or additional similarity measures that improve the result set while - at the same time - widens the range of the scores. Finally, the optimal neighborhood size is still due for evaluation. As such, the current size of 20 was selected to serve for the creation of first results.\nA particularity about the neighborhood is that one movie (fb:en.toy_story) occurs twice in the list. This is due to\n16http://www.imdb.com/title/tt0101414/, as of February 2012\nthe HetRec2011 MovieLens2k dataset that contains several duplicates with different identifiers. We suppose that these duplicates occur due to an automatic processing that has been conducted in the course of enriching the original MovieLens10M dataset with additional data."
    }, {
      "heading" : "5.2 Neighborhood-based entity summarization",
      "text" : "After the neighborhood formation step we are able to extract the 10 most important features for each entity. Tables 2 to 5 each provide an example for a movie entity summarization.\nIn general, most of the presented examples have genre as one of the strongest components. In this realm, one of the real advantages of LOD can be exemplified, i.e. data richness: genres such as “costume drama”, “crime fiction” or “parody” are missing in the HetRec2011 MovieLens2k dataset and can not be circumscribed. It is interesting to see that the property fb:film.film.written_by affects all of the presented movies. In the results, the movie “Bridget Jones’s Diary” shares with its neighbors that the scene plays in the United Kingdom while Walt Disney as the production company is surely important for the movie “Beauty and the Beast”. It is also worth to mention that, according to our results, “Pulp Fiction” is under heavy influence by its director Quentin Tarantino.\nThe mindful reader will surely notice that not a single actor influences the presented movies. At least “The Naked Gun - From the Files of Police Squad” should have as an important feature the main actor Leslie Nielsen. This is due to the fact that - in Freebase - the actors are hidden behind another node that connects movies, actors, and characters. Queries that deal with such “two-hops-each” relationships (see listing 3) are hard to resolve for triple stores and yet, we were not able to produce a result set from the triple store.17 However, for the near future we consider ways to circumvent this issue that does not only affect the actor feature and also help to improve the “hop-radius” of such queries.\nAnother issue that is visible in the results is the problem of data quality and the constant evolution of the data. Newly added property-value pairs like\n(fb:user.robert.(...).ew_rating, 92)\nare shared with one or two neighbors but - at this stage - have not been assigned to a sufficient amount of entities to be downgraded with the weighting method introduced in section 2."
    }, {
      "heading" : "6. CONCLUSION",
      "text" : "In the following we will summarize the key findings of this early stage of research. We have presented an approach that tries to leverage usage data in order to summarize movie entities in the LOD space. This part of Semantic Web research is connected to a variety of fields, including semantic user modeling, user interfaces, and information ranking.\nThe goal of our research is to provide meaningful summarizations of entities. This is the task of identifying features that“not just represent the main themes of the original data, but rather, can best identify the underlying entity” [5]. Our\n17We currently employ Sesame with the Native Java Store (see also http://www.openrdf.org/)\nListing 3: SPARQL query: retrieving propertyvalue pairs shared with at least one of the 20-nearest neighbors.\n1 select ?p ?q ?t where { 2 fb:movie.uri ?p ?o. 3 fb:movie.uri knn:20 ?s. 4 ?o ?q ?t. 5 ?s ?p ?r. 6 ?r ?q ?t. 7 ?s rdf:type fb:film.film. 8 FILTER((?s != fb:movie.uri) && (?p != knn:20)) 9 }\napproach can be considered as a further step to this direction. Properties such as rdf:label or fb:type.object.name are currently missing as they are usually not shared with any other entity. With regard to this issue, the approach can easily be combined with another feature ranking strategy. The question whether strong weights for features that are shared with a usage-data-based neighborhood enhance the state of the art is subject to an extensive evaluation that is currently in progress of being conducted.\nAdditionally, we want to discuss the fact that the presented approach is restricted to a single domain and whether it can work for multiple domains or even cross-domain. Consider a electronics web shop that includes semantic metainformation about the items to be sold. Users that search a for product that fulfills their requirements (whatever those are) provide usage data that can be used to compare two products on the basis of whether they have been browsed\nby a same set of users (each user has watched a set of items within a given time-frame). Utilizing this information with the proposed approach can lead to a ranked list of features that a product has (e.g. 12 mega pixels in the case of digital cameras). This may help to provide meaningful product summarizations rather than listing all features that it has. However, for data hubs like DBpedia and Freebase, filtering mechanisms (like restricting to rdf:type film) have to be applied for not to compare apples with pears."
    }, {
      "heading" : "7. FUTURE WORK",
      "text" : "Considering the simplicity of our current approach and the subjective quality that has already been reached, we plan to follow this track of research. In our next contributions we plan the following enhancements:\n• An extensive evaluation of the approach will be conducted: the analysis is will include an intrinsic as well as an extrinsic evaluation with user surveys.\n• Features that are specific to an entity (and not shared with others) will be considered in future versions of this approach. It has to be evaluated whether usage data can help with this task.\n• The problem of intermediate nodes needs to be addressed in order to provide a scalable solution. This could be done with a fixed set of important propertyvalue pairs (like actors and characters). Another solution would be to set up triple store indexes.\n• The ideas of diversifying the results as well as a possible adaption to user profiles and context state interesting challenges.\nTable 4: Top-10 features: Bridget Jones’s Diary\nScore Property Value\n29.67 fb:film.film.genre fb:en.romantic_comedy 29.39 fb:film.film.written_by fb:en.richard_curtis 19.40 fb:film.film.country fb:en.united_kingdom 18.43 fb:film.film.film_casting_director fb:en.michelle_guish 16.75 fb:film.film.produced_by fb:en.eric_fellner 16.50 fb:film.film.produced_by fb:en.tim_bevan 13.05 fb:user.robert.default_domain.rated_film.ew_rating 69 12.79 fb:film.film.film_format fb:en.super_35_mm_film 12.51 fb:film.film.production_companies fb:en.universal_studios 9.140 fb:film.film.story_by fb:en.helen_fielding\nTable 5: Top-10 features: Pulp Fiction\nScore Property Value\n21.58 fb:film.film.directed_by fb:en.quentin_tarantino 19.75 fb:film.film.genre fb:en.crime_fiction 19.10 fb:user.robert.default_domain.rated_film.ew_rating 92 16.94 fb:film.film.rating fb:en.r_usa 16.38 fb:film.film.featured_film_locations fb:en.los_angeles 14.12 fb:film.film.written_by fb:en.quentin_tarantino 13.72 fb:film.film.film_collections fb:en.afis_100_years_100_movies 13.48 fb:film.film.edited_by fb:en.sally_menke 13.31 fb:film.film.film_production_design_by fb:en.david_wasco 12.39 fb:film.film.produced_by fb:en.lawrence_bender\n• With enhanced versions of the presented approach we want to move forward to the direction of user interfaces and user interaction in the context of Linked Data; also in combination with Social Media such as Twitter and Blogs."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The research leading to these results has received funding from the European Union’s Seventh Framework Programme (FP7/2007-2013) under grant agreement no. 257790."
    } ],
    "references" : [ {
      "title" : "Toward the Next Generation of Recommender Systems: A Survey of the State-of-the-Art and Possible Extensions",
      "author" : [ "Gediminas Adomavicius", "Alexander Tuzhilin" ],
      "venue" : "IEEE Trans. on Knowl. and Data Eng",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2005
    }, {
      "title" : "Latent dirichlet allocation",
      "author" : [ "David M. Blei", "Andrew Y. Ng", "Michael I. Jordan" ],
      "venue" : "In: J. Mach. Learn. Res",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2003
    }, {
      "title" : "The anatomy of a large-scale hypertextual Web search engine",
      "author" : [ "Sergey Brin", "Lawrence Page" ],
      "venue" : "Proc. of the 7th intl. conf. on World Wide Web 7. WWW7. Brisbane, Australia: Elsevier Science Publishers B. V.,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1998
    }, {
      "title" : "2nd Ws. on Information Heterogeneity and Fusion in Recommender Systems (HetRec 2011)",
      "author" : [ "Iván Cantador", "Peter Brusilovsky", "Tsvi Kuflik" ],
      "venue" : "Proc. of the 5th ACM conf. on Recommender systems. RecSys",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2011
    }, {
      "title" : "RE- LIN: relatedness and informativeness-based centrality for entity summarization",
      "author" : [ "Gong Cheng", "Thanh Tran", "Yuzhong Qu" ],
      "venue" : "Proc. of the 10th intl. conf. on The semantic web - Volume Part I. ISWC’11",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2011
    }, {
      "title" : "Using ontologies to discover domain-level web usage profiles",
      "author" : [ "Honghua Dai", "Bamshad Mobasher" ],
      "venue" : "Semantic Web Mining Ws. at ECML/PKDD-2002",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2002
    }, {
      "title" : "Hierarchical Link Analysis for Ranking Web Data",
      "author" : [ "Renaud Delbru" ],
      "venue" : "The Semantic Web: Research and Applications",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2010
    }, {
      "title" : "Accurate Methods for the Statistics of Surprise and Coincidence",
      "author" : [ "Ted Dunning" ],
      "venue" : "COMPUTATIONAL LINGUISTICS",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1993
    }, {
      "title" : "A maximum entropy web recommendation system: combining collaborative and content features",
      "author" : [ "Xin Jin", "Yanzan Zhou", "Bamshad Mobasher" ],
      "venue" : "Proc. of the 11th ACM SIGKDD intl. conf. on KD in data mining",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2005
    }, {
      "title" : "Employing a domain ontology to gain insights into user behaviour",
      "author" : [ "Patricia Kearney", "Sarabjot Singh An", "Mary Shapcott" ],
      "venue" : "Proc. of the 3rd Ws. on Intelligent Techniques for Web Personalization,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2005
    }, {
      "title" : "Feature-Weighted User Model for Recommender Systems",
      "author" : [ "Panagiotis Symeonidis", "Alexandros Nanopoulos", "Yannis Manolopoulos" ],
      "venue" : "Proc. of the 11th intl. conf. on User Modeling. UM ’07. Corfu, Greece: Springer- Verlag,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "The problem known as entity summarization [5] is concerned with identifying the most important features of lengthy LD or Linked Open Data (LOD) descriptions.",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 3,
      "context" : "To validate our approach we run a set of experiments using two datasets, namely the HetRec2011 MovieLens2k dataset [4] and data crawled from Freebase.",
      "startOffset" : 115,
      "endOffset" : 118
    }, {
      "referenceID" : 4,
      "context" : "A detailed problem statement of entity summarization is given in [5].",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 0,
      "context" : "For this, several similarity measures have been introduced of which cosine similarity and Pearson correlation (comparing the vectors with regard to their angular distance) are the most common techniques [1].",
      "startOffset" : 203,
      "endOffset" : 206
    }, {
      "referenceID" : 7,
      "context" : "In our current implementation, we apply the log-likelihood ratio score [8] for computing item similarity (step 2).",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 4,
      "context" : "In the field of entity summarization, initial work has been presented in [5], where an approach called RELIN is introduced.",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 6,
      "context" : "[7] uses combines hierarchical link analysis with weighted link analysis.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "In the field of recommender systems, [9] propose an approach based on Latent Dirichlet Allocation (LDA) [2] for discovering hidden semantic relationships between items.",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 1,
      "context" : "In the field of recommender systems, [9] propose an approach based on Latent Dirichlet Allocation (LDA) [2] for discovering hidden semantic relationships between items.",
      "startOffset" : 104,
      "endOffset" : 107
    }, {
      "referenceID" : 5,
      "context" : "In the field of user modeling, there exist several approaches for leveraging (weighted) semantic knowledge about items [6, 11, 10].",
      "startOffset" : 119,
      "endOffset" : 130
    }, {
      "referenceID" : 10,
      "context" : "In the field of user modeling, there exist several approaches for leveraging (weighted) semantic knowledge about items [6, 11, 10].",
      "startOffset" : 119,
      "endOffset" : 130
    }, {
      "referenceID" : 9,
      "context" : "In the field of user modeling, there exist several approaches for leveraging (weighted) semantic knowledge about items [6, 11, 10].",
      "startOffset" : 119,
      "endOffset" : 130
    }, {
      "referenceID" : 5,
      "context" : "The approach presented in [6] proposes an aggregated presentation of user profiles by extracting and combining the domain knowledge of different items.",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 10,
      "context" : "[11] models users and items each as a feature matrix.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 9,
      "context" : "[10] introduces an impact measure that indicates the influences on user behavior by item features modeled as a domain ontology.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 2,
      "context" : "See also PageRank [3].",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 3,
      "context" : "For the preparation of first tests, we combined the usage data of the HetRec2011 MovieLens2k dataset [4] with Freebase.",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 4,
      "context" : "This is the task of identifying features that“not just represent the main themes of the original data, but rather, can best identify the underlying entity” [5].",
      "startOffset" : 156,
      "endOffset" : 159
    } ],
    "year" : 2012,
    "abstractText" : "Novel research in the field of Linked Data focuses on the problem of entity summarization. This field addresses the problem of ranking features according to their importance for the task of identifying a particular entity. Next to a more human friendly presentation, these summarizations can play a central role for semantic search engines and semantic recommender systems. In current approaches, it has been tried to apply entity summarization based on patterns that are inherent to the regarded data. The proposed approach of this paper focuses on the movie domain. It utilizes usage data in order to support measuring the similarity between movie entities. Using this similarity it is possible to determine the k-nearest neighbors of an entity. This leads to the idea that features that entities share with their nearest neighbors can be considered as significant or important for these entities. Additionally, we introduce a downgrading factor (similar to TF-IDF) in order to overcome the high number of commonly occurring features. We exemplify the approach based on a movie-ratings dataset that has been linked to Freebase entities.",
    "creator" : "LaTeX with hyperref package"
  }
}