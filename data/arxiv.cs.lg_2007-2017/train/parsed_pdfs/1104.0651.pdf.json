{
  "name" : "1104.0651.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Meaningful Clustered Forest: an Automatic and Robust Clustering Algorithm",
    "authors" : [ "Mariano Tepper", "Pablo Musé", "Andrés Almansa" ],
    "emails" : [ "mtepper@dc.uba.ar", "pmuse@fing.edu.uy", "andres.almansa@telecom-paristech.fr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords and phrases: clustering, minimum spanning tree, a contrario detection."
    }, {
      "heading" : "1. Introduction",
      "text" : "Clustering is an unsupervised learning method that seeks to group observations into subsets (called clusters) so that, in some sense, intra-cluster observations are more similar than inter-cluster ones. Despite its intuitive simplicity, there is no general agreement on the definition of a cluster. In part this is due to the fact that the notion of cluster cannot be trivially separated from the context. Consequently, in practice different authors provide different definitions, usually derived from the algorithm being used, rather than the opposite.\nUnfortunately, the lack of a unified definition makes it difficult to find a unifying clustering theory. A plethora of methods to assess or classify clustering algorithms have been developed, some of them with very interesting results. To\n1\nar X\niv :1\n10 4.\n06 51\nv3 [\ncs .L\ncite a few [7, 20, 21]. For a broad perspective on clustering techniques, we refer the reader to the excellent overview of clustering methods recently reported by Jain [18].\nHuman perception is extremely adapted to group similar visual objects. Based on psychophysical experiments using simple 2D figures, the Gestalt school studied the perceptual organization, and identified a set of rules that govern human perception [25]. Each of these rules focuses on a single quality, or gestalt, many of which have been unveiled over the years.\nOne of the earlier and most powerful gestalts is proximity, which states that spatial or temporal proximity of elements may be perceived as a single group. Of course, the notion of distance is heavily embedded in the proximity gestalt. This is clearly illustrated in Figure 1. Two possible distances between the bars B1 and B2 that could be considered are\ndM (B1, B2) = max p1∈B1 p2∈B2\n||p1 − p2||,\ndm(B1, B2) = min p1∈B1 p2∈B2\n||p1 − p2||.\nIn this particular example ||. || denotes the euclidean norm. According to distance dM , the bars are exactly at the same distance in both experiments, while according to distance dm the bars on the right are closer to each other. In this case, the distance dm seems to be more consistent with our perception.\nThe conceptual grounds on which our work is based were laid by Zahn in a seminal paper from 1971 [26]. Zahn faced the problem of finding perceptual clusters according to the proximity gestalt and proposed three key arguments:\n1. Only inter-point distances matter. This imposes graphs as the only suitable underlying structure for clustering. 2. No random steps. Results must remain stable for all runs of the detection process. In particular, random initializations are forbidden. 3. Independence from the exploration strategy. The order in which points are analyzed must not affect the outcome of the algorithm.\nThese conceptual statements, together with the preference for dm over dM or other distances between sets, led Zahn to use the Minimum Spanning Tree (MST) as a building block for clustering algorithms. (The MST is the tree structure induced by the distance dm [9].) Recently, psychophysical experiments\nperformed by Dry et al. [11] supported this choice. In these experiments individuals were asked to connect points of 30 major star constellations, to show the structure they perceive. Two examples of constellations are shown in Figure 2. The outcome of these experiments was that, among five relational geometry structures, the MST and the Relative Neighborhood Graph (RNG) exhibit the highest degree of agreement with the empirical edges. In the RNG, two points p and q are connected by an edge whenever there does not exist a third point r that is closer to both p and q than they are to each other. The MST is a subgraph of the RNG. Nonetheless the diagonal variance of both groups might suggest that sometimes other links not present nor in the MST nor in the RNG are used.\nFrom a theoretical point of view, Carlsson and Mémoli [7] proved very recently that the single-link hierarchy (i.e. a hierarchical structure built from the MST, as explained later) is the most stable choice and has good convergence properties among other hierarchical techniques.\nZahn [26] suggested to cluster a feature set by eliminating the inconsistent edges in the minimum spanning tree. That is, instead of constructing a MST and as a consequence of the eliminations, a minimum spanning forest is built.\nSince then, variations of the limited neighborhood set approaches have been extensively explored. The criteria in most works are based on local properties of the graph. Since perceptual grouping implies an assessment of local properties versus global properties, exclusively local methods must be discarded or patched. For example, Felzenszwalb and Huttenlocher [12] and Bandyopadhyay [3] make use of the MST and RNG respectively. However, in order to correct local observations and to produce a reasonable clustering, they are forced to consider additional ad hoc global criteria.\nThe computation of the MST requires previous computation of the complete graph. This is a major disadvantage of MST-based clustering methods, that\nimpose severe restrictions both on time and memory. The obvious workaround is to prune a priori the complete graph (e.g. in image segmentation, the image connexity is exploited), but unfortunately it might produce artifacts in the final solution. In a recent work Tepper et al. [24] proposed an efficient method to compute the MST on metric datasets. The use of this method allows for a significant performance boost over previous MST-based methods (e.g. [5, 12]), thus permitting to treat large datasets.\nFrom an algorithmic point of view, the main problem with the Gestalt rules is their qualitative nature. Desolneux et al. developed a detection theory which seeks to provide a quantitative assessment of gestalts [10]. This theory is often referred as Computational Gestalt Theory and it has been successfully applied to numerous gestalts and detection problems [6, 15, 22]. It is primarily based on the Helmholtz principle which states that no structure is perceived in white noise. In this approach, there is no need to characterize the elements one wishes to detect but contrarily, the elements one wishes to avoid detecting.\nIn the light of this framework, Desolneux et al. analyzed the proximity gestalt, proposing a clustering algorithm [10]. It is founded on the idea that clusters are groups of points contained in a relatively small area. In other words, by counting points and computing the area that encloses them, one can assess the exceptionality of a given group of points.\nThe method proposed by Desolneux et al. [10] suffers from some problems. First, it can only be applied to points in an Euclidean 2D space. Second, in order to compute the enclosing areas, the space has to be discretized a priori and such discretization is used to compute the enclosing areas; of course, different discretizations lead to different results. Last, two phenomena called collateral elimination and faulty union in [5] occur when an extremely exceptional cluster hides or masks other less but still exceptional ones.\nCao et al. [5] continued this line of research extending the clustering algorithm to higher dimensions and corrected the collateral elimination and faulty union issues, by introducing what they called indivisibility criterion. However, as their method is also based on counting points on a given region, it is still required that a set of candidate regions is given a priori. The set of test regions is defined to be a set R of hyper-rectangles parallel to the axes and of different edge lengths, centered at each data point. The choice of R is application specific since it is intrinsically related to cluster size/scale. For example, an exponential choice for the discretization of the rectangle space is made by Cao et al. [5] introducing a bias for small rectangles (since they are more densely sampled). Then each cluster must be fitted by an axis-aligned hyper-rectangle R ∈ R, meaning that clusters with arbitrary shapes are not detected. Even hyper-rectangular but diagonal clusters may be missed or oversegmented. A probability law modeling the number of points that fall in each hyper-rectangle R ∈ R, assuming no specific structure in the data, must be known a priori or estimated. Obviously, this probability depends on the dimension of the space to be clustered.\nRecently Tepper et al. [23] introduced the concept of graph-based a contrario clustering. A key element in this method is that the area can be computed from a weighted graph, where the edge weight represents the distance between two\npoints, using non-parametric density estimation. Since only distances are used, the dimensionality of the problem is reduced to one. However, since this method is conceived for complete graphs, it suffers from a high computational burden.\nThere is an additional concept behind clustering algorithms that was not stated before: a point, to belong to a cluster, must be similar to all points in the cluster or only to some of them? All the described region-based solutions imply choosing the first option since, in some sense, all distances within a group are inspected. Table 1 shows on which side some algorithms are. Since our goal is to detect arbitrarily shaped clusters, we must place ourselves in the second group. We can do this by using the MST.\nOur goal is to design a clustering method that can be considered a quantitative assessment of the proximity gestalt. Hence in Section 2 we propose a clustering method based on analyzing the distribution of distances of MST edges. The formulation naturally allows to detect clusters of arbitrary shapes. The use of trees, as minimally connected graphs, also leads to a fast algorithm. The approach is fully automatic in the sense that the user input only relates to the nature of the problem to be treated and not the clustering algorithm itself. Strictly speaking it involves one single parameter that controls the degree of reliability of the detected clusters. However, these methods can be considered parameter-free, as the result is not sensitive to the parameter value. Results on synthetic but challenging sets are presented in Section 3.\nAs the method relies on the sole characterization of non-clustered data, it is thus capable of detecting non-clustered data as such. In other words, in the absence of clustered data, the algorithm yields no detections. In Section 4 it is also shown that, by iteratively applying our method to datasets with clustered and unclustered data, it is possible to automatically separate both classes.\nIn Section 5, we finally illustrate a masking phenomena where a highly populated cluster might occlude or mask less populated ones, showing that the iterative application of the MST-based clustering method is able to cope with this issue, thus solving very complicated clustering problems.\nResults on three-dimensional examples of the complete process are presented in Section 6 and we expose some final remarks in Section 7."
    }, {
      "heading" : "2. A New Clustering Method: Proximal Meaningful Forest",
      "text" : "We now propose a new method to find clusters in graphs that is independent from their shape and from their dimension. We first build a weighted undirected graph G = (X,E) where X is a set of features in a metric space (M,d) and the weighting function ω is defined in terms of the corresponding distance function\nω((vi, vj)) = d(xi, xj). (1)"
    }, {
      "heading" : "2.1. The Minimum Spanning Tree",
      "text" : "Informally, the Minimum Spanning Tree (MST) of an undirected weighted graph is the tree that covers all vertices with minimum total edge cost.\nGiven a metric space (M,d) and feature set X ⊆M , we denote by G = (X,E) the undirected weighted graph where E = X × X and the graph’s weighting function ω : E → R is defined as\nω((xi, xj)) = d(xi, xj) ∀xi, xj ∈ X. (2)\nThe MST T = (X,ET ) of the feature set X is defined as the MST of G. A very important and classical property of the MST is that a hierarchy of point groups can be constructed from it.\nNotation 1. Let T = (X,ET ) be the minimum spanning tree of X. For a group of points C ∈ X, we denote\nE(C) = {(vi, vj) | vi, vj ∈ C ∧ (vi, vj) ∈ ET } (3)\nThe edges in E(C) are sorted in non-decreasing order, i.e.\n∀ ei, ej ∈ E(C), i < j ⇒ ω(ei) ≤ ω(ej)\nDefinition 1. Let T = (X,ET ) be the minimum spanning tree of X. A component C ⊆ X is a set such that the graph G = (C,E(C)) is connected and\n• ∃ v ∈ V, C = {v} or • @ C ′ ∈ X, C ⊂ C ′ ∧ ωmax(C) > ωmax(C ′),\nwhere ωmax(C) = max e∈E(C) ω(e). A single-link hierarchy T is the set of all possible components.\nIt is important to notice what the single-link hierarchy implies: given two components C1, C2 ∈ T , it suffices that there exists a pair of vertices, one in C1 and one in C2 that are sufficiently near each other to generate a new component CF ∈ T , such that C = C1 ∪ C2 and\nωmax(CF ) = min vi∈C1,vj∈C2 (vi,vj)∈ET ω((vi, vj)). (4)\nAn example is depicted in Figure 3. The direct consequence of this fact is that the use of the single-link hierarchy for clustering provides a natural way to deal with clusters of different shapes.\nAll minimum spanning tree algorithms are greedy. From Definition 1 and Equation 4, in the single-link hierarchy the component CF = C1 ∪ C2 is the father of C1 and C2 and\nωmax(CF ) ≥ ωmax(C1) (5) ωmax(CF ) ≥ ωmax(C2). (6)\nWith the objective of finding a suitable partition and to the best of our knowledge, Felzenszwalb and Huttenlocher [12] were the first to compare ωmax(CF ) with ωmax(C1) and ωmax(C2), with an additional correction factor τ . Components C1 and C2 are only merged if\nmin [ ωmax(C1) + τ(C1), ωmax(C2) + τ(C2) ] ≥ ωmax(CF ). (7)\nIn practice τ is defined as τ(C) = s/|C| where s plays the role of a scale parameter. The above definition presents a few problems. First, τ (i.e. s) is a global parameter and experiments show that clusters with different sizes and densities might not be recovered with this approach (Figure 10a). Second, there is not an easy rule to fix τ or s and, although it can be related with a scale parameter, there is no way to predict which specific value is best suited to a particular problem.\nThe exploration of similar ideas, while bearing in mind their shortcomings, leads us to a new clustering method."
    }, {
      "heading" : "2.2. Proximal Meaningful Forest",
      "text" : "First, let us observe that the edge length distribution of an MST of a configuration of clustered points differs significantly from that of an unclustered point set (Figure 4). As a general idea, by knowing how to model unclustered data, one could detect clustered data by measuring some kind of dissimilarity between both.\nIn the a contrario framework, there is no need to characterize the elements one wishes to detect but contrarily the elements one wishes to avoid detecting\n(i.e. unclustered data). To achieve such characterization we only need two elements: (1) a naive model and (2) a measurement made on structures to be potentially detected. The naive model is a probabilistic model that describes typical configurations where no structure is present and we will describe it in detail in Section 2.3. We now focus on establishing a coherent measurement for MST-based clustering.\nConcretely, we are looking to evaluate the probability of occurrence, under the background model (i.e. unclustered data), of a random set C which exhibits the characteristics of a given observed set C. Both sets have the same cardinality, i.e. |E(C)| = |E(C )| = K.\nThe general principle has been previously explored. In 1983, following the same rationale Hoffman and Jain [17] proposed a similar idea: to perform a test of randomness. They built a null hypothesis using the edge length distribution of the MST and they performed a single test analyzing whether the whole dataset belongs to the random model or not by computing the difference between the theoretical and the empirical CDF. Jain et al. [19] further refined this work, by using heuristic computations to separate the dataset into two or more subsets which were then tested using a two sample test statistic. Barzily et al. recently continued this line of work [4]. This approach introduces a bias towards the detection of compact (i.e. non-elongated) and equally sized clusters [4]. From the perspective chosen in this work these characteristics can be seen as shortcomings, and thus we build a new method.\nDefinition 2. Let P be a partition of R. We define the equivalence relation ∼ where ω1 ∼ ω2 if and only if ∃P ∈ P such that ω1 ∈ P and ω2 ∈ P .\nWe denote by ei the i-th edge of E(C) and by γi the i-th edge of E(C ). Inspired by Equation 7, which proved successful as a decision rule to detect\nclusters, and associating it with Equations 5 and 6, we compute Pr ( ωmax(C ) < ωmax(C) | ωmax(CF ) ∼ ωmax(CF ) ) . (8)\nDefinition 3. Let C ∈ X be a component of the single-link hierarchy T induced by the minimum spanning tree T = (X,ET ) such that |C| > 1. We define the probability of false alarms (PFA) of C as\nPFA(C) def = Pr ( ωmax(C ) < ωmax(C) | ωmax(CF ) ∼ ωmax(CF ) ) . (9)\nThe constraint |C| > 1 is needed since E(C) = ∅ when |C| = 1. Note that sets consisting of a single node must certainly not be detected. Conceptually, even when they are isolated, they constitute an outlier and not a cluster. We simply do not test such sets.\nTo detect unlikely dense subgraphs, a threshold is necessary on the PFA. In the classical a contrario framework, a new quantity is introduced: the Number of False Alarms (NFA), i.e. the product of the PFA by the number of tested candidate clusters. The NFA has a more intuitive meaning than the PFA, since it is an upper bound on the expectation of the number of false detections [10]. The threshold is then easily set on the NFA.\nDefinition 4 (Number of false alarms). We define the number of false alarms (NFA) of C as\nNFA(C) def = (|X| − 1) · PFA(C). (10)\nNotice that, by definition, |X| − 1 is the number of non-singleton sets in the single-link hierarchy.\nDefinition 5 (Meaningful component). A component C is ε-meaningful if\nNFA(C) < ε. (11)\nIn the following, it is important to notice a fact about the single-link hierarchy. The components are mainly determined by the sorted sequence of the edges from the original graph; this follows directly from Kruskal’s algorithm [9]. However, the components are independent of the differences between the edges in that sorted sequence: only the order matters and not the actual weights of the edges.\nWe will now state a simple proposition that motivates Definition 4.\nProposition 1. The expected number of ε-meaningful clusters in a random single-link hierarchy (i.e. issued from the background model) is lower than ε.\nThe proof is given in Appendix A in the light of the discussion in the next section."
    }, {
      "heading" : "2.3. The background model",
      "text" : "The distribution Pr ( ωmax(C ) < ωmax(C) | ωmax(CF ) ∼ ωmax(CF ) ) is not\nknown a priori. Moreover, up to our knowledge there is no analytical expression for the cumulative edge distribution under H0 for the MST [17].\nWe estimate this distribution by performing Monte Carlo simulations of the background process. However this estimation would involve extremely high computational costs.\nWe assume that the edge lengths in the cluster, given that ωmax(CF ) ∼ ωmax(CF ), are mutually conditionally independent and identically distributed. Let Ω be a random variable with this common distribution:\nFΩ ( ω, ωmax(CF ) ) = Pr ( Ω ≤ ω | ωmax(CF ) ∼ ωmax(CF ) ) . (12)\nThen, using rank statistics:\nPr ( ωmax(C ) ≤ ωmax(C) | ωmax(CF ) ∼ ωmax(CF ) ) = FΩ ( ωmax(C), ωmax(CF ) )K . (13)\nThe distribution of Ω is much easier to compute than the one of ωmax(C ) given that ωmax(CF ) ∼ ωmax(CF ) ) as it requires fewer Monte Carlo simulations and\nthus we use it as in Equation 13 to compute the PFA. Now, the independence hypothesis is clearly false even for i.i.d. graph vertices, because of the ordering structure and edge selection induced by the MST. However, the conditional dependence may be weak enough to make the model suitable in practice. This explains why naive classifiers, such as the Naive Bayes classifier, despite their simplicity, can often outperform more sophisticated classification methods [16]. Therefore we only assume conditional independence in our model. Still, the suitability of this model has to be checked; a series of experiments shows that no clusters are detected in non-clustered, unstructured data (for instance, sets of independent, uniformly distributed vertices). On the other side of the problem, meaningful clusters always constitute a clear violation of this naive independence assumption.\nThe estimation process is depicted in Algorithm 1. Classically, one defines a point process and a sampling window. Hoffman and Jain [17] point out that the sampling window for the background point process is usually unknown for a given dataset. They use the convex hull arguing that it is the maximum likelihood estimator of the true sampling window for uniformly distributed twodimensional data. In the experiments from Section 3, we simply use the minimum hiper-rectangle that contains the whole dataset as the sampling window. However, there are problems whose intrinsic characteristics allow to define other background processes that do not involve a sampling window.\nAlgorithm 1 Compute Pr ( ωmax(C ) < ωmax(C) | ωmax(CF ) ∼ ωmax(CF ) ) for\na set of N points by Q Monte Carlo simulations. for all q such that 1 ≤ q ≤ Q do\nX ← draw N points from the background point process. build the single-link hierarchy Tq from the MST of X.\nend for compute a conditional histogram from the set {Tq}q=1...Q"
    }, {
      "heading" : "2.4. Eliminating redundancy",
      "text" : "While each meaningful cluster is relevant by itself, the whole set of meaningful components exhibits, in general, high redundancy: a meaningful component C1 can contain another meaningful component C2 [5]. This question can be answered by comparing NFA(C1) and NFA(C2) using Definition 5. The group with the smallest NFA must of course be preferred. Classically, the following rule\nfor all ε-meaningful clusters C1, C2 do if C2 ⊂ C1 ∨ C1 ⊂ C2 then\neliminate arg max (NFA(C1),NFA(C2)) end if\nend for\nwould have been used to perform the pruning of the set of meaningful components. Unfortunately, it leads to a phenomenon described in [5], where it was called collateral elimination. A very meaningful component can hide another meaningful sibling, as illustrated in Figure 5.\nThe single-link hierarchy offers an alternative scheme to prune the redundant set of meaningful components, profiting from the inclusion properties of the dendrogram structure. It is non-other than the exclusion principle, defined first by Desolneux et al. [10], which states that\nLet A and B be groups obtained by the same gestalt law. Then no point x is allowed to belong to both A and B. In other words each point must either belong to A or to B.\nA simple scheme for applying the exclusion principle is shown in Algorithm 2. Since we are choosing the components that are more in accordance with the proximity gestalt, we call the resulting components Proximal Meaningful Components (PMC). Then, we say that the set of all proximal meaningful components is a Meaningful Clustered Forest (MCF).\nAlgorithm 2 Eliminate redundant components from the set M of meaningful components. 1: F ← ∅ 2: whileM 6= ∅ do 3: Cmin ← argmin\nC∈M NFA(C)\n4: eliminate Cmin fromM 5: eliminate all components C fromM such that C ⊂ Cmin 6: eliminate all components C fromM such that Cmin ⊂ C 7: add Cmin to F 8: end while 9: M← F"
    }, {
      "heading" : "3. Experiments on Synthetic examples",
      "text" : "As a sanity check, we start by testing our method with simple examples. Figure 6 present clusters which are well but not linearly separated. The meaningful clustered forest describes correctly the structure of the data.\nFigure 7 shows an example of cluster detection in a dataset overwhelmed by outliers. The data consists of 950 points uniformly distributed in the unit square, and 50 points manually added around the positions (0.4, 0.4) and (0.7, 0.7). The figure shows the result of a numerical method involving the above NFA. The\nbackground distribution is chosen to be uniform in [0, 1]2. Both visible clusters are found and their NFAs are respectively 10−15 and 10−8. Such low numbers can barely be the result of chance.\nThe case of mixture of Gaussians, shown in Figure 8, provides an interesting example. On the tails, points are obviously sparser and the distance to neighboring points grows. Since we are looking for tight components, the tail might be discarded, depending on the Gaussian variance.\nThe example in Figure 9 consists of a very complex scene, composed of clusters with different densities, shapes and sizes. Proximal components (i.e. we avoid testing NFA < ε) are displayed. Even when no decision about the statistical significance is made, the recovered clusters describe, in general, the scene accurately. Some oversplitting can be detected in proximal components. When a decision is made and only meaningful components are kept, we realize that the oversplit figures are not meaningful. As a sanity check, in Figure 9e we plot some of the detected structures superimposed to a realization of the background noise model. The input data in Figure 9a contains 764 points and for a given shape in it, with W points, we plot the shape and 764−W points drawn from the background model. Among proximal components, the meaningful ones can\nbe clearly perceived while non-meaningful ones are unnoticed. Our results are compared with Felzenszwalb and Huttenlocher’ algorithm (denoted by FH in the following) and with Mean Shift [8, 14]. Mean Shift performs a non-parametric density estimation (using sliding windows) and finds its local maxima. Clusters are determined by what Comaniciu and Meer call “bassins of attraction” [8]: points are assigned to a local maximum following an ascendent path along the density gradient 1.\nFigure 10 present an experiment were FH and Mean Shift are used, respectively, to cluster the dataset in Figure 9a. Different runs were performed, by varying the kernel/scale size. Clearly, results are suboptimal in both cases. Both algorithms share the same main disadvantage: a global scale must be chosen a priori. Such a strategy is unable to cope with clusters of different densities and spatial sizes. Choosing a fine scale causes to correctly detect dense clusters at the price of oversplitting less denser ones. On the contrary, a coarser scale corrects the oversplitting of less denser clusters but introduces undersplitting for the denser ones."
    }, {
      "heading" : "4. Handling MST Instability",
      "text" : "A seemingly obvious but interesting phenomenon occurs when noise is added to clustered data. Suppose we have data with two well separated clusters. In the absence of noise, it exists an MST edge linking both clusters. If noise is added to the data, the edge would probably disappear and be replaced by a sequence of edges. The length of the original linking edge is larger than the length of the endpoints of the sequence. The direct consequence is an increase in the NFA of the two clusters. Depending on the magnitude of that increase, the clusters might potentially be split into several proximal meaningful components. See Figure 13.\nIn short terms, noise affects the ideal topology of the MST. The oversplitting phenomenon can be corrected by iterating the following steps:\n1. detecting the meaningful clustered forest, 2. add the union of points in the meaningful clustered forest to a new input\ndataset, 3. remove the points in the meaningful clustered forest and replace them\nwith noise, 4. iterate until convergence, 5. re-detect the meaningful clustered forest on the new noise-free dataset\nbuilt along all iterations.\nThe MST of the set formed by merging the meaningful clustered forests from all iterations has the right topology. In other words this MST resembles the MST of the original data without noise. Then, detection of meaningful clustered forest\n1code available at http://www.mathworks.com/matlabcentral/fileexchange/ 10161-mean-shift-clustering\n(a) Input data (b) MST (c) MC (d) MMC\n(e) Shapes drawn against noise. Shapes are respectively plotted in red and in black on the top and bottom rows.\nFig 9. In this example, maximal meaningful components correctly describe the organization of the points configuration. Only small or less denser figures are discarded. Indeed, meaningful components are clearly perceived in noise while non-meaningful components are not.\ncan be performed without major trouble. We say that these detections form a stabilized meaningful clustered forest.\nThe above method implicitly contains a key issue in step 3. Detected points must be replaced with others which have a completely different distribution (i.e. the background distribution) but must occupy the same space. Figure 11 contains an example of the need for such a strong requirement. Pieces of background data might become “disconnected, or to be precise connected by artificially created new edges. In one dimension, these holes are easily contracted, but when the dimensionality increases the contracting scheme gains more and more degrees of freedom.\nThis noise filling procedure can be achieved by using the Voronoi diagram [2] of the original point set. In the Voronoi diagram, each point lies on a different cell. To remove a point amounts to emptying a cell. Then the set of empty cells can be used as a sampling window to draw points from the background model. Notice that this procedure is actually generic since the Voronoi tesselation can be generalized to higher dimensional metric spaces [2].\nThe process simulates replacing detected components with noise from the background model. Due to the same nature of the Voronoi diagram, the process is not perfect: in the final iteration, the resulting point set is quasi but not exactly distributed according to the background model. A small bias is introduced, causing a few spurious detections in the MCF. To correct this issue it suffices to set ε = 10−2, as these detections have NFAs slightly lower than one and actual detections have really low NFAs. Of course this new thresholding could be avoided if a more accurate flling procedure was used.\nAlgorithm 3 illustrates steps 1 to 4 of the correcting method. An example is shown in Figure 12, where four iterations are required until convergence.\nFigure 13 shows a second example of the stabilization process, followed by the detection of the stabilized meaningful clustered forest. The NFAs of the detected components are also included. The very low attained NFAs, account for the success of the procedure."
    }, {
      "heading" : "5. The Masking Challenge",
      "text" : "In 2009, Jain [18] stated that no available algorithm could cluster the dataset in Figure 14b and obtain the result in Figure 14c. The dataset is interesting\nInput data MCF MCF area Replaced MCF\nIt er a ti o n 1\nIt er a ti o n 2\nIt er a ti o n 3\nIt er a ti o n 4\n(a) In each iteration, the MCF is detected and the cells on the Voronoi diagram corresponding to points in the MCF are emptied and filled with points distributed according to the background model. In the fourth iteration, no MCF is detected and thus the algorithm stops.\nAlgorithm 3 Stabilize point set X returning the set F of non-background points. 1: F ← ∅ 2: V ← cells from Voronoi diagram of point set X intersected with the minimum rectangle\nenclosing X. 3: X′ ← X 4: M← meaningful clustered forest of X′ 5: whileM 6= ∅ do 6: V ′ ← ∅ 7: X′ ← ∅ 8: for all C ∈M do 9: for all p ∈ C do 10: add V ∈ V to V ′ such that p ∈ V . 11: if p ∈ X then 12: add p to X′. 13: add p to F . 14: end if 15: end for 16: end for 17: a←\n∑ V ∈V area(V )\n18: aM ← ∑\nV ∈V′ area(V ) 19: nM ← ∑\nC∈M |C|\n20: n← aM · (|X| −NM)/(a− aM) 21: B ← draw n points qi, 1 ≤ i ≤ n, from the background model such that (∃V ∈ V ′) qi ∈ V . 22: X′ ← X′ ∪B 23: M← meaningful clustered forest of X′ 24: end while\nbecause it brings to light a new phenomenon: a cluster with many points can “dominate” the scene and hide other clusters that could be meaningful.\nA similar behavior occurs in vanishing point detection, as pointed out by Almansa et al. [1]. A vanishing point is a point in an image to which parallel line segments not frontoparallel appear to converge; in some sense one can regard this point as a collection of such parallel line segments. Sometimes this procedure will still miss some weak vanishing points which are “masked” by stronger vanishing points composed of much more segments. These may not be perceived at first sight, but only if we manage to unmask them by getting rid of the “clutter” in one way or another. Almansa et al. propose to eliminate these detected vanishing points and look for new vanishing points among the remaining line segments.\nIn our case, this very same approach cannot be followed. Masked clusters are not completely undetected but partially detected. Removing such cluster parts and re-detecting would cause oversegmentation. We propose instead to only remove the most meaningful proximal component and then iterate. The process ends when the masking phenomenon disappears, that is:\n• when there are no unclustered points, or • no MCF is detected.\nAlgorithm 4 shows a detail of this unmasking scheme. Summarizing, first nonbackground points are detected using the stabilization process in Algorithm 3 and then the unmasking process takes place.\nThe detection of unmasked MMCs in Figure 14d correct all masking issues. Moreover, they are extremely similar to the desired clustering in Figure 14c. The difference is that clusters absorb background points that are within of near them. Indeed, these background points are statistically indistinguishable from the points from the cluster that absorbs them.\nAlgorithm 4 Compute the unmasked meaningful clustered forest U from the point set F of non-background points. 1: U ← ∅ 2: whileM 6= ∅ do 3: M← stabilized meaningful clustered forest of F 4: if |F | =\n∑ C∈M |C| then\n5: ∀C ∈M, add C to U . 6: else 7: Cmin ← argmin\nC∈M NFA(C)\n8: for all p ∈ Cmin do 9: remove p from F 10: end for 11: add Cmin to U . 12: end if 13: end while\nFrom a total number of 7000 points in Figure 14b, the outer spiral (in orange in in Figure 14c) has 2514 points, i.e. almost 36% of the points. The detection of the unmasked MCF in Figure 14d correct all masking issues. Moreover, they are extremely similar to the desired clustering in Figure 14c. The difference is that clusters absorb background points that are within of near them. Indeed, these background points are statistically indistinguishable from the points from the cluster that absorbs them."
    }, {
      "heading" : "6. Three-dimensional point clouds",
      "text" : "We tested the proposed algorithm with three-dimensional point clouds. We put two point clouds in the same scene at different positions, thus building two scenes in Figures 15 and 16. In both cases uniformly distributed noise was artificially added. The skeleton hand and the bunny are formed by 3274 and by 3595, respectively. In Figure 15, 3031 noise points were added to total 9900 points. In Figure 15, 7031 noise points were added to total 13900 points and both shapes were positioned closer to each other and in such a way that no linear separation exist between them. In both cases the result is correct\nIn Figure 15, the MCF is oversplit but the stabilization process discussed in Section 4 corrects the issue. In Figure 15, although the same phenomenon is possible, it does not occur in this realization of the noise process.\nIteration 1 Iteration 2 Iteration 3 Iteration 4 Iteration 5 Iteration 6\nIn p u t d a ta\nM C F\n(a) In each iteration, the MCF is detected and the most meaningful component is removed from the dataset. In the sixth iteration, all points are clustered and thus the algorithm stops."
    }, {
      "heading" : "7. Final Remarks",
      "text" : "In this work we propose a new clustering method that can be regarded as a numerical method to compute the proximity gestalt. The method relies on analyzing edge distances in the MST of the dataset. The direct consequence is that our approach is fully parametric on the chosen distance.\nThe proposed method present several novelties over other MST-based formulations. Some formulations have preference for compact clusters as they extract their clustering detection rule from characteristics that are not intrinsic to the MST. Our method only focuses on the length of the MST edges; hence, it does not present such preference. Other formulations analyze the data at a fixed local scale, thus introducing a new method parameter. We have shown through examples that these local methods can fail when the input data has clusters with different sizes and densities. In these same examples, our method perform well without the need of introducing any extra parameter.\nThe method is also automatic, in the sense that only a single parameter is left to the user. This parameter has an intuitive interpretation as it controls the expected number of false detections. Moreover, setting it to 1 is sufficient in practice.\nRobustness to noise is an additional but essential feature of the method. Indeed, we have shown that the iterative application of our method can be used to treat noisy data, producing quality results.\nWe also studied the masking phenomenon in which a highly populated and salient cluster dominates the scene and inhibits the detection of less-populated, but still salient, clusters. The proposed method can be iteratively used to avoid such inhibitions from happening, yielding promising results.\nAs future work, it would be interesting to study the MST edge distribution under different point distributions. From the theoretical point of view, it can bring light to the method correctness. In practice, it would allow to replace the simulated background models by their analytical counterparts."
    }, {
      "heading" : "Appendix A: Proof of Proposition 1",
      "text" : "The proof relies on the following classical lemma.\nLemma 1. Let X be a real random variable and let F (x) = P (X ≤ x) be the cumulative density function of X. Then for all t ∈ (0, 1),\nPr(F (X) < t) ≤ t (14)\nProposition 2. The expected number of ε-meaningful clusters in a random single-link hierarchy (i.e. issued from the background model) is lower than ε.\nProof. We follow the scheme of Proposition 1 from the work by Cao et al. [6]. Let T be random single-link hierarchy. For brevity let M = |X| − 1. Let Zi be\na binary random variable equal to 1 if the random cluster Ci ∈ T is meaningful and 0 else.\nLet us denote by E(X) the expectation of a random variable X in the a contrario model. We then have\nE ( M∑ i=1 Zi ) = E ( E ( M∑ i=1 Zi | M )) . (15)\nLet Yi be a binary random variable equal to 1 if M · Pr ( ωmax(C ) < ωmax(Ci) | ωmax(CF ) ∼ ωmax(CiF ) ) < ε (16)\nand 0 else. Of course, M is independent from the sets in T . Thus, conditionally to M = m, the law of ∑M i=1 Zi is the law of ∑M i=1 Yi. Let us reprise Equation 13 on p. 10,\nPr ( ωmax(C ) < ωmax(Ci) | ωmax(CF ) ∼ ωmax(CiF ) ) = FΩ ( ωmax(Ci), ωmax(CiF ) )Ki . (17)\nBy linearity of expectation,\nE ( M∑ i=1 Zi | M = m ) = E ( m∑ i=1 Yi ) = m∑ i=1 E (Yi) . (18)\nLet us denote FΩ ( ωmax(Ci), ωmax(CiF ) ) by Pr(Ci). Since Yi is a Bernoulli variable,\nE(Yi) = Pr(Yi = 1) = Pr ( M · Pr(Ci)Ki < ε ) =\n∞∑ k=0 Pr ( M · Pr(Ci)Ki < ε ∣∣∣ Ki = k) · Pr (Ki = k) . (19) We have assumed that Ki is independent from Pr(Ci). Thus, conditionally to Ki = k, Pr(Ci) Ki = Pr(Ci) k. We have\nPr ( m · Pr ( ωmax(C ) < ωmax(Ci) | ωmax(CF ) ∼ ωmax(CiF ) ) < ε )\n= Pr ( m · FΩ ( ωmax(Ci), ωmax(CiF ) )k < ε ) = Pr ( m · Pr ( Ω < ωmax(Ci) | ωmax(CF ) ∼ ωmax(CiF ) )k < ε\n) = Pr ( Pr (\nΩ < max e∈Ci ω(e) | ωmax(CF ) ∼ ωmax(CiF ) ) < ( ε m )1/k)\nM. Tepper et al./Meaningful Clustered Forest 27\n= Pr ( max e∈E(Ci) Pr ( Ω < ω(e) | ωmax(CF ) ∼ ωmax(CiF ) ) < ( ε m )1/k) =\n∏ e∈E(Ci) Pr ( Pr ( Ω < ω(e) | ωmax(CF ) ∼ ωmax(CiF ) ) < ( ε m )1/k) . (20)\nThe last equality follows from the conditional independence assumption. Now, using Lemma 1 and bearing in mind that the number of edges in Ci is Ki = k, yields\nPr ( m · Pr(Ci)k < ε ) ≤ k∏ j=1 ( ε m )1/k = ε m . (21)\nWe can now use this bound in Equation 19:\nE(Yi) = ∞∑ k=0 Pr ( M · Pr(Ci)Ki < ε ∣∣∣ Ki = k) · Pr (Ki = k) ≤ ε m ∞∑ k=0 Pr (Ki = k) = ε m . (22)\nHence,\nE ( M∑ i=1 Zi | M = m ) = m∑ i=1 E(Yi) ≤ ε. (23)\nThis finally implies E (∑M\ni=1 Zi\n) ≤ ε, what means that the expected number of\nε-meaningful clusters is less than ε."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors acknowledge financial support by CNES (R&T Echantillonnage Irregulier DCT / SI / MO - 2010.001.4673), FREEDOM (ANR07-JCJC-0048-01), Callisto (ANR-09-CORD-003), ECOS Sud U06E01, ARFITEC (07 MATRH) and STIC Amsud (11STIC-01 - MMVPSCV) and the Uruguayan Agency for Research and Innovation (ANII) under grant PR-POS-2008-003."
    } ],
    "references" : [ {
      "title" : "Vanishing point detection without any a priori information",
      "author" : [ "A. Almansa", "A. Desolneux", "S. Vamech" ],
      "venue" : "Transactions on Pattern Analysis and Machine Intelligence, 25(4):502–507,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Voronoi diagrams - a survey of a fundamental geometric data structure",
      "author" : [ "F. Aurenhammer" ],
      "venue" : "ACM Computing Surveys, 23(3):345–405, September",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "An automatic shape independent clustering technique",
      "author" : [ "S. Bandyopadhyay" ],
      "venue" : "Pattern Recognition, 37(1):33–45, January",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "On a Minimal Spanning Tree Approach in the Cluster Validation Problem",
      "author" : [ "Z. Barzily", "Z. Volkovich", "B. Akteke Öztürk", "G.W. Weber" ],
      "venue" : "Informatica, 20(2):187–202,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A Unified Framework for Detecting Groups and Application to Shape Recognition",
      "author" : [ "F. Cao", "J. Delon", "A. Desolneux", "P. Musé", "F. Sur" ],
      "venue" : "Journal of Mathematical Imaging and Vision, 27(2):91–119, February",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Extracting Meaningful Curves from Images",
      "author" : [ "F. Cao", "P. Musé", "F. Sur" ],
      "venue" : "J. Math. Imaging Vis., 22(2-3):159–181,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Characterization, Stability and Convergence of Hierarchical Clustering methods",
      "author" : [ "G. Carlsson", "F. Mémoli" ],
      "venue" : "Journal of Machine Learning Research, 11:1425–1470,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Mean shift: a robust approach toward feature space analysis",
      "author" : [ "D. Comaniciu", "P. Meer" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence, 24(5):603–619, August",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Introduction to Algorithms",
      "author" : [ "T. Cormen", "C. Leiserson", "R. Rivest", "C. Stein" ],
      "venue" : "McGraw-Hill Science / Engineering / Math, 2nd edition, December",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "From Gestalt Theory to Image Analysis, volume 34",
      "author" : [ "A. Desolneux", "L. Moisan", "J.M. Morel" ],
      "venue" : "Springer-Verlag,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "The Perceptual Organization of Point Constellations",
      "author" : [ "M. Dry", "D. Navarro", "K. Preiss", "M. Lee" ],
      "venue" : "Annual Meeting of the Cognitive Science Society,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Efficient Graph-Based Image Segmentation",
      "author" : [ "P. Felzenszwalb", "D. Huttenlocher" ],
      "venue" : "International Journal of Computer Vision, 59(2):167–181, September",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Introduction to Statistical Pattern Recognition",
      "author" : [ "K. Fukunaga" ],
      "venue" : "Academic Press, second edition,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "The estimation of the gradient of a density function, with applications in pattern recognition",
      "author" : [ "K. Fukunaga", "L. Hostetler" ],
      "venue" : "IEEE Transactions on Information Theory, 21(1):32–40, January",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1975
    }, {
      "title" : "LSD: A Fast Line Segment Detector with a False Detection Control",
      "author" : [ "R. Grompone von Gioi", "J. Jakubowicz", "J.M. Morel", "G. Randall" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2010
    }, {
      "title" : "The Elements of Statistical Learning, Second Edition: Data Mining, Inference, and Prediction",
      "author" : [ "T. Hastie", "R. Tibshirani", "J. Friedman" ],
      "venue" : "Springer Series in Statistics. Springer, 2nd edition, February",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A test of randomness based on the minimal spanning tree",
      "author" : [ "Richard Hoffman", "Anil K. Jain" ],
      "venue" : "Pattern Recognition Letters,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1983
    }, {
      "title" : "Data clustering: 50 years beyond K-means",
      "author" : [ "A.K. Jain" ],
      "venue" : "Pattern Recognition Letters, 31(8):651–666, September",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Uniformity testing using minimal spanning tree",
      "author" : [ "A.K. Jain", "Xiaowei Xu", "Tin K. Ho", "Fan Xiao" ],
      "venue" : "In International Conference on Pattern Recognition,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2002
    }, {
      "title" : "On clusterings: Good, bad and  M",
      "author" : [ "R. Kannan", "S. Vempala", "A. Vetta" ],
      "venue" : "Tepper et al./Meaningful Clustered Forest  29 spectral. Journal of the ACM, 51(3):497–515, May",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "An impossibility theorem for clustering",
      "author" : [ "J. Kleinberg" ],
      "venue" : "S. Becker, S. Thrun, and K. Obermayer, editors, Advances in Neural Information Processing Systems, pages 446–453. MIT Press,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "A Statistical Approach to the Matching of Local Features",
      "author" : [ "J. Rabin", "J. Delon", "Y. Gousseau" ],
      "venue" : "SIAM Journal on Imaging Sciences, 2(3):931–958,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Automatically finding clusters in normalized cuts",
      "author" : [ "M. Tepper", "P. Musé", "A. Almansa", "M. Mejail" ],
      "venue" : "Pattern Recognition, 44(7):1372–1386, July",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Boruvka Meets Nearest Neighbors",
      "author" : [ "M. Tepper", "P. Musé", "A. Almansa", "M. Mejail" ],
      "venue" : "Technical report, Departamento de Computación, Facultad de Ciencias Exactas y Naturales, Universidad de Buenos Aires, Argentina, April",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Laws of organization in perceptual forms, pages 71–88",
      "author" : [ "M. Wertheimer" ],
      "venue" : "Routledge and Kegan Paul,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 1938
    }, {
      "title" : "Graph-Theoretical Methods for Detecting and Describing Gestalt Clusters",
      "author" : [ "C.T. Zahn" ],
      "venue" : "Transactions on Computers, C-20(1):68–86,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 1971
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "cite a few [7, 20, 21].",
      "startOffset" : 11,
      "endOffset" : 22
    }, {
      "referenceID" : 19,
      "context" : "cite a few [7, 20, 21].",
      "startOffset" : 11,
      "endOffset" : 22
    }, {
      "referenceID" : 20,
      "context" : "cite a few [7, 20, 21].",
      "startOffset" : 11,
      "endOffset" : 22
    }, {
      "referenceID" : 17,
      "context" : "For a broad perspective on clustering techniques, we refer the reader to the excellent overview of clustering methods recently reported by Jain [18].",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 24,
      "context" : "Based on psychophysical experiments using simple 2D figures, the Gestalt school studied the perceptual organization, and identified a set of rules that govern human perception [25].",
      "startOffset" : 176,
      "endOffset" : 180
    }, {
      "referenceID" : 25,
      "context" : "The conceptual grounds on which our work is based were laid by Zahn in a seminal paper from 1971 [26].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 8,
      "context" : "(The MST is the tree structure induced by the distance dm [9].",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 10,
      "context" : "Reproduced from [11].",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 10,
      "context" : "[11] supported this choice.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 6,
      "context" : "From a theoretical point of view, Carlsson and Mémoli [7] proved very recently that the single-link hierarchy (i.",
      "startOffset" : 54,
      "endOffset" : 57
    }, {
      "referenceID" : 25,
      "context" : "Zahn [26] suggested to cluster a feature set by eliminating the inconsistent edges in the minimum spanning tree.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 11,
      "context" : "For example, Felzenszwalb and Huttenlocher [12] and Bandyopadhyay [3] make use of the MST and RNG respectively.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 2,
      "context" : "For example, Felzenszwalb and Huttenlocher [12] and Bandyopadhyay [3] make use of the MST and RNG respectively.",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 23,
      "context" : "[24] proposed an efficient method to compute the MST on metric datasets.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 4,
      "context" : "[5, 12]), thus permitting to treat large datasets.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 11,
      "context" : "[5, 12]), thus permitting to treat large datasets.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 9,
      "context" : "developed a detection theory which seeks to provide a quantitative assessment of gestalts [10].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 5,
      "context" : "This theory is often referred as Computational Gestalt Theory and it has been successfully applied to numerous gestalts and detection problems [6, 15, 22].",
      "startOffset" : 143,
      "endOffset" : 154
    }, {
      "referenceID" : 14,
      "context" : "This theory is often referred as Computational Gestalt Theory and it has been successfully applied to numerous gestalts and detection problems [6, 15, 22].",
      "startOffset" : 143,
      "endOffset" : 154
    }, {
      "referenceID" : 21,
      "context" : "This theory is often referred as Computational Gestalt Theory and it has been successfully applied to numerous gestalts and detection problems [6, 15, 22].",
      "startOffset" : 143,
      "endOffset" : 154
    }, {
      "referenceID" : 9,
      "context" : "analyzed the proximity gestalt, proposing a clustering algorithm [10].",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 9,
      "context" : "[10] suffers from some problems.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 4,
      "context" : "Last, two phenomena called collateral elimination and faulty union in [5] occur when an extremely exceptional cluster hides or masks other less but still exceptional ones.",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 4,
      "context" : "[5] continued this line of research extending the clustering algorithm to higher dimensions and corrected the collateral elimination and faulty union issues, by introducing what they called indivisibility criterion.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[5] introducing a bias for small rectangles (since they are more densely sampled).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 22,
      "context" : "[23] introduced the concept of graph-based a contrario clustering.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "a point must be similar to all points in the cluster to at least one point in the cluster k-means single-link algorithm [13] Cao et al.",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 4,
      "context" : "[5] Mean Shift [8] Tepper et al.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[5] Mean Shift [8] Tepper et al.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 22,
      "context" : "[23] Felzenszwalb and Huttenlocher [12] Table 1 Conceptually there are two different ways to form a cluster.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[23] Felzenszwalb and Huttenlocher [12] Table 1 Conceptually there are two different ways to form a cluster.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 11,
      "context" : "With the objective of finding a suitable partition and to the best of our knowledge, Felzenszwalb and Huttenlocher [12] were the first to compare ωmax(CF ) with ωmax(C1) and ωmax(C2), with an additional correction factor τ .",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 16,
      "context" : "In 1983, following the same rationale Hoffman and Jain [17] proposed a similar idea: to perform a test of randomness.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 18,
      "context" : "[19] further refined this work, by using heuristic computations to separate the dataset into two or more subsets which were then tested using a two sample test statistic.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 3,
      "context" : "recently continued this line of work [4].",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 3,
      "context" : "non-elongated) and equally sized clusters [4].",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 9,
      "context" : "The NFA has a more intuitive meaning than the PFA, since it is an upper bound on the expectation of the number of false detections [10].",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 8,
      "context" : "The components are mainly determined by the sorted sequence of the edges from the original graph; this follows directly from Kruskal’s algorithm [9].",
      "startOffset" : 145,
      "endOffset" : 148
    }, {
      "referenceID" : 16,
      "context" : "Moreover, up to our knowledge there is no analytical expression for the cumulative edge distribution under H0 for the MST [17].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 15,
      "context" : "This explains why naive classifiers, such as the Naive Bayes classifier, despite their simplicity, can often outperform more sophisticated classification methods [16].",
      "startOffset" : 162,
      "endOffset" : 166
    }, {
      "referenceID" : 16,
      "context" : "Hoffman and Jain [17] point out that the sampling window for the background point process is usually unknown for a given dataset.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 4,
      "context" : "While each meaningful cluster is relevant by itself, the whole set of meaningful components exhibits, in general, high redundancy: a meaningful component C1 can contain another meaningful component C2 [5].",
      "startOffset" : 201,
      "endOffset" : 204
    }, {
      "referenceID" : 4,
      "context" : "Unfortunately, it leads to a phenomenon described in [5], where it was called collateral elimination.",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 9,
      "context" : "[10], which states that",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 5,
      "context" : "in Figure 2 [6].",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 0,
      "context" : "background distribution is chosen to be uniform in [0, 1].",
      "startOffset" : 51,
      "endOffset" : 57
    }, {
      "referenceID" : 7,
      "context" : "Our results are compared with Felzenszwalb and Huttenlocher’ algorithm (denoted by FH in the following) and with Mean Shift [8, 14].",
      "startOffset" : 124,
      "endOffset" : 131
    }, {
      "referenceID" : 13,
      "context" : "Our results are compared with Felzenszwalb and Huttenlocher’ algorithm (denoted by FH in the following) and with Mean Shift [8, 14].",
      "startOffset" : 124,
      "endOffset" : 131
    }, {
      "referenceID" : 7,
      "context" : "Clusters are determined by what Comaniciu and Meer call “bassins of attraction” [8]: points are assigned to a local maximum following an ascendent path along the density gradient .",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 1,
      "context" : "This noise filling procedure can be achieved by using the Voronoi diagram [2] of the original point set.",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 1,
      "context" : "Notice that this procedure is actually generic since the Voronoi tesselation can be generalized to higher dimensional metric spaces [2].",
      "startOffset" : 132,
      "endOffset" : 135
    }, {
      "referenceID" : 17,
      "context" : "In 2009, Jain [18] stated that no available algorithm could cluster the dataset in Figure 14b and obtain the result in Figure 14c.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 0,
      "context" : "[1].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[6].",
      "startOffset" : 0,
      "endOffset" : 3
    } ],
    "year" : 2011,
    "abstractText" : "We propose a new clustering technique that can be regarded as a numerical method to compute the proximity gestalt. The method analyzes edge length statistics in the MST of the dataset and provides an a contrario cluster detection criterion. The approach is fully parametric on the chosen distance and can detect arbitrarily shaped clusters. The method is also automatic, in the sense that only a single parameter is left to the user. This parameter has an intuitive interpretation as it controls the expected number of false detections. We show that the iterative application of our method can (1) provide robustness to noise and (2) solve a masking phenomenon in which a highly populated and salient cluster dominates the scene and inhibits the detection of less-populated, but still salient, clusters.",
    "creator" : "LaTeX with hyperref package"
  }
}