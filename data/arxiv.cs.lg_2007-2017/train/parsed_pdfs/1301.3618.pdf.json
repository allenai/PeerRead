{
  "name" : "1301.3618.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning New Facts From Knowledge Bases With Neural Tensor Networks and Semantic Word Vectors",
    "authors" : [ "Danqi Chen", "Richard Socher", "Christopher D. Manning", "Andrew Y. Ng" ],
    "emails" : [ "danqi@stanford.edu,", "manning@stanford.edu,", "ang@stanford.edu,", "richard@socher.org" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n30 1.\n36 18\nv2 [\ncs .C\nL ]\n1 6\nM ar\nKnowledge bases provide applications with the benefit of easily accessible, systematic relational knowledge but often suffer in practice from their incompleteness and lack of knowledge of new entities and relations. Much work has focused on building or extending them by finding patterns in large unannotated text corpora. In contrast, here we mainly aim to complete a knowledge base by predicting additional true relationships between entities, based on generalizations that can be discerned in the given knowledgebase. We introduce a neural tensor network (NTN) model which predicts new relationship entries that can be added to the database. This model can be improved by initializing entity representations with word vectors learned in an unsupervised fashion from text, and when doing this, existing relations can even be queried for entities that were not present in the database. Our model generalizes and outperforms existing models for this problem, and can classify unseen relationships in WordNet with an accuracy of 75.8%."
    }, {
      "heading" : "1 Introduction",
      "text" : "Ontologies and knowledge bases such as WordNet [1] or Yago [2] are extremely useful resources for query expansion [3], coreference resolution [4], question answering (Siri), information retrieval (Google Knowledge Graph), or generally providing inference over structured knowledge to users. Much work has focused on extending existing knowledge bases [5, 6, 2] using patterns or classifiers applied to large corpora.\nWe introduce a model that can accurately learn to add additional facts to a database using only that database. This is achieved by representing each entity (i.e., each object or individual) in the database by a vector that can capture facts and their certainty about that entity. Each relation is defined by the parameters of a novel neural tensor network which can explicitly relate two entity vectors and is more powerful than a standard neural network layer.\nFurthermore, our model allows us to ask whether even entities that were not in the database are in certain relationships by simply using distributional word vectors. These vectors are learned by a neural network model [7] using unsupervised text corpora such as Wikipedia. They capture syntactic and semantic information and allow us to extend the database without any manually designed rules or additional parsing of other textual resources.\nThe model outperforms previously introduced related models such as that of Bordes et al. [8]. We evaluate on a heldout set of relationships in WordNet. The accuracy for predicting unseen relations is 75.8%. We also evaluate in terms of ranking. For WordNet, there are 38,696 different entities and we use 11 relationship types. On average for each left entity there are 100 correct entities in a specific relationship. For instance, dog has many hundreds of hyponyms such as puppy, barker or dachshund. In 20.9% of the relationship triplets, the model ranks the correct test entity in the top 100 out of 38,696 possible entities."
    }, {
      "heading" : "2 Related Work",
      "text" : "There is a vast amount of work extending knowledge bases using external corpora [5, 6, 2], among many others. In contrast, little work has been done in extensions based purely on the knowledge base itself. The work closest to ours is that by Bordes et al. [9]. We implement their approach and compare to it directly. Our model outperforms it by a significant margin in terms of both accuracy and ranking. Both models can benefit from initialization with unsupervised word vectors.\nAnother related approach is that by Sutskever et al. [10] who use tensor factorization and Bayesian clustering for learning relational structures. Instead of clustering the entities in a nonparametric Bayesian framework we rely purely on learned entity vectors. Their computation of the truth of a relation can be seen as a special case of our proposed model. Instead of using MCMC for inference, we use standard backpropagation which is modified for the Neural Tensor Network. Lastly, we do not require multiple embeddings for each entity. Instead, we consider the subunits (space separated words) of entity names. This allows more statistical strength to be shared among entities.\nMany methods that use knowledge bases as features such as [3, 4] could benefit from a method that maps the provided information into vector representations. We learn to modify unsupervised word representations via grounding in world knowledge. This essentially allows us to analyze word embeddings and query them for specific relations. Furthermore, the resulting vectors could be used in other tasks such as NER [7] or relation classification in natural language [11].\nLastly, Ranzato et al. [12] introduced a factored 3-way Restricted Boltzmann Machine which is also parameterized by a tensor."
    }, {
      "heading" : "3 Neural Tensor Networks",
      "text" : "In this section we describe the full neural tensor network. We begin by describing the representation of entities and continue with the model that learns entity relationships.\nWe compare using both randomly initialized word vectors and pre-trained 100-dimensional word vectors from the unsupervised model of Collobert and Weston [13, 7]. Using free Wikipedia text, this model learns word vectors by predicting how likely it is for each word to occur in its context. The model uses both local context in the window around each word and global document context. Similar to other local co-occurrence based vector space models, the resulting word vectors capture distributional syntactic and semantic information. For further details and evaluations of these embeddings, see [14, 13, 15].\nFor cases where the entity name has multiple words, we simply average the word vectors.\nThe Neural Tensor Network (NTN) replaces the standard linear layer with a bilinear layer that directly relates the two entity vectors. Let e1, e2 ∈ Rd be the vector representations of the two entities. We can compute a score of how plausible they are in a certain relationship R by the following NTNbased function:\ng(e1, R, e2) = U T f\n(\neT1 W [1:k] R e2 + VR\n[\ne1 e2\n]\n+ bR\n)\n, (1)\nwhere f = tanh is a standard nonlinearity. We define W [1:k] ∈ Rd×d×k as a tensor and the bilinear tensor product results in a vector h ∈ Rk, where each entry is computed by one slice of the tensor:\nhi = e T 1 W [i]e2. (2)\nThe remaining parameters for relation R are the standard form of a neural network: VR ∈ Rk×2d and U ∈ Rk, bR ∈ Rk.\nThe main advantage of this model is that it can directly relate the two inputs instead of only implicitly through the nonlinearity. The bilinear model for truth values in [10] becomes a special case of this model with VR = 0, bR = 0, k = 1, f = identity.\nIn order to train the parameters W,U, V,E, b, we minimize the following contrastive max-margin objective:\nJ(W,U, V,E, b) = N ∑\ni=1\nC ∑\nc=1\nmax(0, 1− g(e (i) 1 , R (i), e (i) 2 ) + g(e (i) 1 , R (i), ec)), (3)\nwhere N is the number of training triplets and we score the correct relation triplets higher than a corrupted one in which one of the entities was replaced with a random entity. For each correct triplet we sample C random corrupted entities.\nThe model is trained by taking gradients with respect to the five sets of parameters and using minibatched L-BFGS."
    }, {
      "heading" : "4 Experiments",
      "text" : "In our experiments, we follow the data settings of WordNet in [9]. There are a total of 38,696 different entities and 11 relations. We use 112,581 triplets for training, 2,609 for the development set and 10,544 for final testing.\nThe WordNet relationships we consider are has instance, type of, member meronym, member holonym, part of, has part, subordinate instance of, domain region, synset domain region, similar to, domain topic.\nWe compare our model with two models in Bordes et al. [9, 8], which have the same goal as ours. The model of [9] has the following scoring function:\ng(e1, R, e2) = ‖WR,lefte1 −WR,righte2‖1, (4)\nwhere WR,left,WR,right ∈ Rd×d. The model of [8] also maps each relation type to an embedding eR ∈ R d and scores the relationships by:\ng(e1, R, e2) = −(W1e1 ⊗Wrel,1eR + b1) · (W2e2 ⊗Wrel,2eR + b2), (5)\nwhere W1,Wrel,1,W2,Wrel,2 ∈ Rd×d, b1, b2 ∈ Rd×1. In the comparisons below, we call these two models the similarity model and the Hadamard model respectively. While our function scores correct triplets highly, these two models score correct triplets lower. All models are trained in a contrastive max-margin objective functions.\nOur goal is to predict “correct” relations (e1, R, e2) in the testing data. We can compute a score for each triplet (e1, R, e2). We can consider either just a classification accuracy result as to whether the relation holds, or look at a ranking of e2, for considering relative confidence in particular relations holding. We use a different evaluation set from Bordes et al. [9] because it has became apparent to us and them that there were issues of overlap between their training and testing sets which impacted the quality and interpretability of their evaluation.\nRanking\nFor each triplet (e1, R, e2), we compute the score g(e1, R, e) for all other entities in the knowledge base e ∈ E. We then sort values by decreasing order and report the rank of the correct entity e2.\nFor WordNet the total number of entities is |E| = 38, 696. Some of the questions relating to triplets are of the form “A is a type of ?” or “A has instance ?” Since these have multiple correct answers, we report the percentage of times that e2 is ranked in the top 100 of the list (recall @ 100). The higher this number, the more often the specific correct test entity has likely been correctly estimated.\nAfter cross-validation of the hyperparameters of both models on the development fold, our neural tensor net obtains a ranking recall score of 20.9% while the similarity model achieves 10.6%, and the Hadamard model achieves only 7.4%. The best performance of the NTN with random initialization instead of the semantic vectors drops to 16.9% and the similarity model and the Hadamard model only achieve 5.7% and 7.1%.\nClassification\nIn this experiment, we ask the model whether any arbitrary triplet of entities and relations is true or not. With the help of the large vocabulary of semantic word vectors, we can query whether certain WordNet relationships hold or not even for entities that were not originally in WordNet.\nWe use the development fold to find a threshold TR for each relation such that if f(e1, R, e2) ≥ TR, the relation (e1, R, e2) holds, otherwise it is considered false. In order to create negative examples,\nwe randomly switch entities and relations from correct testing triplets, resulting in a total of 2 × 10, 544 triplets. The final accuracy is based on how many of of triplets are classified correctly.\nThe Neural Tensor Network achieves an accuracy of 75.8% with semantically initialized entity vectors and 70.0% with randomly initialized ones. In comparison, the similarity based model only achieve 66.7% and 51.6%, the Hadamard model achieve 71.9% and 68.2% with the same setup. All models improve in performance if entities are represented as an average of their word vectors but we will leave experimentation with this setup to future work."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We introduced a new model based on Neural Tensor Networks. Unlike previous models for predicting relationships purely using entity representations in knowledge bases, our model allows direct interaction of entity vectors via a tensor. This architecture allows for much better performance in terms of both ranking correct answers out of tens of thousands of possible ones and predicting unseen relationships between entities. It enables the extension of databases even without external textual resources but can also benefit from unsupervised large corpora even without manually designed extraction rules."
    } ],
    "references" : [ {
      "title" : "WordNet: A Lexical Database for English",
      "author" : [ "G.A. Miller" ],
      "venue" : "Communications of the ACM",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Yago: a core of semantic knowledge",
      "author" : [ "F.M. Suchanek", "G. Kasneci", "G. Weikum" ],
      "venue" : "Proceedings of the 16th international conference on World Wide Web",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "The SphereSearch engine for unified ranked retrieval of heterogeneous XML and web documents",
      "author" : [ "J. Graupmann", "R. Schenkel", "G. Weikum" ],
      "venue" : "Proceedings of the 31st international conference on Very large data bases, VLDB",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Improving machine learning approaches to coreference resolution",
      "author" : [ "V. Ng", "C. Cardie" ],
      "venue" : "ACL",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Learning syntactic patterns for automatic hypernym discovery",
      "author" : [ "R. Snow", "D. Jurafsky", "A.Y. Ng" ],
      "venue" : "NIPS",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Identifying relations for open information extraction",
      "author" : [ "A. Fader", "S. Soderland", "O. Etzioni" ],
      "venue" : "EMNLP",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Word representations: a simple and general method for semisupervised learning",
      "author" : [ "J. Turian", "L. Ratinov", "Y. Bengio" ],
      "venue" : "Proceedings of ACL, pages 384–394",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Joint Learning of Words and Meaning Representations for Open-Text Semantic Parsing",
      "author" : [ "A. Bordes", "X. Glorot", "J. Weston", "Y. Bengio" ],
      "venue" : "AISTATS",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Learning structured embeddings of knowledge bases",
      "author" : [ "A. Bordes", "J. Weston", "R. Collobert", "Y. Bengio" ],
      "venue" : "AAAI",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Modelling relational data using Bayesian clustered tensor factorization",
      "author" : [ "I. Sutskever", "R. Salakhutdinov", "J.B. Tenenbaum" ],
      "venue" : "NIPS",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Semantic Compositionality Through Recursive Matrix-Vector Spaces",
      "author" : [ "R. Socher", "B. Huval", "C.D. Manning", "A.Y. Ng" ],
      "venue" : "EMNLP",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Factored 3-Way Restricted Boltzmann Machines",
      "author" : [ "M. Ranzato", "A. Krizhevsky G.E. Hinton" ],
      "venue" : "For Modeling Natural Images. AISTATS,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2010
    }, {
      "title" : "A unified architecture for natural language processing: deep neural networks with multitask learning",
      "author" : [ "R. Collobert", "J. Weston" ],
      "venue" : "ICML",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "A neural probabilistic language model",
      "author" : [ "Y. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2003
    }, {
      "title" : "Improving Word Representations via Global Context and Multiple Word Prototypes",
      "author" : [ "E.H. Huang", "R. Socher", "C.D. Manning", "A.Y. Ng" ],
      "venue" : "ACL",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Ontologies and knowledge bases such as WordNet [1] or Yago [2] are extremely useful resources for query expansion [3], coreference resolution [4], question answering (Siri), information retrieval (Google Knowledge Graph), or generally providing inference over structured knowledge to users.",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 1,
      "context" : "Ontologies and knowledge bases such as WordNet [1] or Yago [2] are extremely useful resources for query expansion [3], coreference resolution [4], question answering (Siri), information retrieval (Google Knowledge Graph), or generally providing inference over structured knowledge to users.",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 2,
      "context" : "Ontologies and knowledge bases such as WordNet [1] or Yago [2] are extremely useful resources for query expansion [3], coreference resolution [4], question answering (Siri), information retrieval (Google Knowledge Graph), or generally providing inference over structured knowledge to users.",
      "startOffset" : 114,
      "endOffset" : 117
    }, {
      "referenceID" : 3,
      "context" : "Ontologies and knowledge bases such as WordNet [1] or Yago [2] are extremely useful resources for query expansion [3], coreference resolution [4], question answering (Siri), information retrieval (Google Knowledge Graph), or generally providing inference over structured knowledge to users.",
      "startOffset" : 142,
      "endOffset" : 145
    }, {
      "referenceID" : 4,
      "context" : "Much work has focused on extending existing knowledge bases [5, 6, 2] using patterns or classifiers applied to large corpora.",
      "startOffset" : 60,
      "endOffset" : 69
    }, {
      "referenceID" : 5,
      "context" : "Much work has focused on extending existing knowledge bases [5, 6, 2] using patterns or classifiers applied to large corpora.",
      "startOffset" : 60,
      "endOffset" : 69
    }, {
      "referenceID" : 1,
      "context" : "Much work has focused on extending existing knowledge bases [5, 6, 2] using patterns or classifiers applied to large corpora.",
      "startOffset" : 60,
      "endOffset" : 69
    }, {
      "referenceID" : 6,
      "context" : "These vectors are learned by a neural network model [7] using unsupervised text corpora such as Wikipedia.",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 7,
      "context" : "[8].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "There is a vast amount of work extending knowledge bases using external corpora [5, 6, 2], among many others.",
      "startOffset" : 80,
      "endOffset" : 89
    }, {
      "referenceID" : 5,
      "context" : "There is a vast amount of work extending knowledge bases using external corpora [5, 6, 2], among many others.",
      "startOffset" : 80,
      "endOffset" : 89
    }, {
      "referenceID" : 1,
      "context" : "There is a vast amount of work extending knowledge bases using external corpora [5, 6, 2], among many others.",
      "startOffset" : 80,
      "endOffset" : 89
    }, {
      "referenceID" : 8,
      "context" : "[9].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "[10] who use tensor factorization and Bayesian clustering for learning relational structures.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 2,
      "context" : "Many methods that use knowledge bases as features such as [3, 4] could benefit from a method that maps the provided information into vector representations.",
      "startOffset" : 58,
      "endOffset" : 64
    }, {
      "referenceID" : 3,
      "context" : "Many methods that use knowledge bases as features such as [3, 4] could benefit from a method that maps the provided information into vector representations.",
      "startOffset" : 58,
      "endOffset" : 64
    }, {
      "referenceID" : 6,
      "context" : "Furthermore, the resulting vectors could be used in other tasks such as NER [7] or relation classification in natural language [11].",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 10,
      "context" : "Furthermore, the resulting vectors could be used in other tasks such as NER [7] or relation classification in natural language [11].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 11,
      "context" : "[12] introduced a factored 3-way Restricted Boltzmann Machine which is also parameterized by a tensor.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "We compare using both randomly initialized word vectors and pre-trained 100-dimensional word vectors from the unsupervised model of Collobert and Weston [13, 7].",
      "startOffset" : 153,
      "endOffset" : 160
    }, {
      "referenceID" : 6,
      "context" : "We compare using both randomly initialized word vectors and pre-trained 100-dimensional word vectors from the unsupervised model of Collobert and Weston [13, 7].",
      "startOffset" : 153,
      "endOffset" : 160
    }, {
      "referenceID" : 13,
      "context" : "For further details and evaluations of these embeddings, see [14, 13, 15].",
      "startOffset" : 61,
      "endOffset" : 73
    }, {
      "referenceID" : 12,
      "context" : "For further details and evaluations of these embeddings, see [14, 13, 15].",
      "startOffset" : 61,
      "endOffset" : 73
    }, {
      "referenceID" : 14,
      "context" : "For further details and evaluations of these embeddings, see [14, 13, 15].",
      "startOffset" : 61,
      "endOffset" : 73
    }, {
      "referenceID" : 9,
      "context" : "The bilinear model for truth values in [10] becomes a special case of this model with VR = 0, bR = 0, k = 1, f = identity.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 8,
      "context" : "In our experiments, we follow the data settings of WordNet in [9].",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 8,
      "context" : "[9, 8], which have the same goal as ours.",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 7,
      "context" : "[9, 8], which have the same goal as ours.",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 8,
      "context" : "The model of [9] has the following scoring function:",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 7,
      "context" : "The model of [8] also maps each relation type to an embedding eR ∈ R d and scores the relationships by: g(e1, R, e2) = −(W1e1 ⊗Wrel,1eR + b1) · (W2e2 ⊗Wrel,2eR + b2), (5) where W1,Wrel,1,W2,Wrel,2 ∈ R, b1, b2 ∈ R.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 8,
      "context" : "[9] because it has became apparent to us and them that there were issues of overlap between their training and testing sets which impacted the quality and interpretability of their evaluation.",
      "startOffset" : 0,
      "endOffset" : 3
    } ],
    "year" : 2013,
    "abstractText" : "Knowledge bases provide applications with the benefit of easily accessible, systematic relational knowledge but often suffer in practice from their incompleteness and lack of knowledge of new entities and relations. Much work has focused on building or extending them by finding patterns in large unannotated text corpora. In contrast, here we mainly aim to complete a knowledge base by predicting additional true relationships between entities, based on generalizations that can be discerned in the given knowledgebase. We introduce a neural tensor network (NTN) model which predicts new relationship entries that can be added to the database. This model can be improved by initializing entity representations with word vectors learned in an unsupervised fashion from text, and when doing this, existing relations can even be queried for entities that were not present in the database. Our model generalizes and outperforms existing models for this problem, and can classify unseen relationships in WordNet with an accuracy of 75.8%.",
    "creator" : "LaTeX with hyperref package"
  }
}