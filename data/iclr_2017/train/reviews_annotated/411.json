{"conference": "ICLR 2017 conference submission", "title": "Learning to superoptimize programs", "abstract": "Code super-optimization is the task of transforming any given program to a more efficient version while preserving its input-output behaviour. In some sense, it is similar to the paraphrase problem from natural language processing where the intention is to change the syntax of an utterance without changing its semantics. Code-optimization has been the subject of years of research that has resulted in the development of rule-based transformation strategies that are used by compilers. More recently, however, a class of stochastic search based methods have been shown to outperform these strategies. This approach involves repeated sampling of modifications to the program from a proposal distribution, which are accepted or rejected based on whether they preserve correctness, and the improvement they achieve. These methods, however, neither learn from past behaviour nor do they try to leverage the semantics of the program under consideration. Motivated by this observation, we present a novel learning based approach for code super-optimization. Intuitively, our method works by learning the proposal distribution using unbiased estimators of the gradient of the expected improvement. Experiments on benchmarks comprising of automatically generated as well as existing (``Hacker's Delight'') programs show that the proposed method is able to significantly outperform state of the art approaches for code super-optimization.", "histories": [], "reviews": [{"comments": "This work builds on top of STOKE (Schkufza et al., 2013), which is a superoptimization engine for program binaries. It works by starting with an existing program, and proposing modifications to it acc", "ORIGINALITY": 4, "IS_META_REVIEW": false, "RECOMMENDATION": 2, "APPROPRIATENESS": 2, "is_meta_review": false}, {"SUBSTANCE": 4, "comments": "Two things I really liked about this paper: 1. The whole idea of having a data-dependent proposal distribution for MCMC. I wasn't familiar with this, although it apparently was previously published. I", "SOUNDNESS_CORRECTNESS": 5, "IS_META_REVIEW": false, "RECOMMENDATION": 4, "is_meta_review": false}, {"comments": "This is an interesting and pleasant paper on superoptimization, that extends the problem approached by the stochastic search STOKE to a learned stochastic search, where the STOKE proposals are the out", "SOUNDNESS_CORRECTNESS": 5, "ORIGINALITY": 4, "IS_META_REVIEW": false, "CLARITY": 5, "is_meta_review": false}], "authors": "Rudy Bunel, Alban Desmaison, M. Pawan Kumar, Philip H.S. Torr, Pushmeet Kohli", "accepted": true, "id": "411"}