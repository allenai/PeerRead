{
  "name" : "1703.01024.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "wj80290}@alibaba-inc.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 3.\n01 02\n4v 1\n[ cs\n.C L\n] 3\nM ar\n2 01\nwith multi-GPUs cluster is widely applied in the neural network model learning currently. We present a new approach that applies exponential moving average method in large-scale parallel training of neural network model. It is a non-interference strategy that the exponential moving average model is not broadcasted to distributed workers to update their local models after model synchronization in the training process, and it is implemented as the final model of the training system. Fully-connected feed-forward neural networks (DNNs) and deep unidirectional Long short-term memory (LSTM) recurrent neural networks (RNNs) are successfully trained with proposed method for large vocabulary continuous speech recognition on Shenma voice search data in Mandarin. The character error rate (CER) of Mandarin speech recognition further degrades than state-of-the-art approaches of parallel training."
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "Over the past few years, neural networks has been widely used in some domains, such as large vocabulary continuous speech recognition (LVCSR) [1, 2, 3], image recognition [4, 5] and neural machine translation [6]. Fully-connected feed-forward deep neural networks (DNNs) and Recurrent neural networks (RNNs), especially long short-term memory (LSTM) RNNs have shown the effective performance for LVCSR [7, 8, 9, 10]. It is significant that training with larger dataset could improve recognition accuracy. As a matter of fact, larger dataset does mean more training samples and more model parameters, and it is high time consumption to train neural networks with only one computing unit. Therefore, parallel training with multi-GPUs is essential, but it leads to slower convergence. For multi-GPUs training, the key problem is how to accelerate convergence and get further improvement.\nMini-batch based stochastic gradient descent (SGD) is the most prevalent method in neural network training procedure. Several methods are proposed based on it, and achieving encouraging performance for parallel training.\nAsynchronous SGD is a successful attempt [11, 12], and it is shown that parallel training with asynchronous SGD can many times speedup without lowering the accuracy. Besides, synchronous SGD is another positive effort, where the parameter server waits for every workers to finish their computation and send their local models to it, and then it sends updated model back to all workers [13]. Synchronous SGD converges well in parallel training with data parallelism, and is also easy to be implemented.\nModel averaging is a method for large-scale parallel training, which the final model is averaged from all parameters of separated models [14, 15]. Compared with single GPU training, it achieves linear speedup, but the accuracy decreases. Moreover, blockwise model-updating filter (BMUF) provides another linear speedup approach with multi-GPUs on the basis of model averaging. It can achieve improvement or nodegradation of recognition performance compared with minibatch SGD on single GPU [16].\nIt is demonstrated that the performance of moving average of the parameters obtained by SGD is as good as that of the parameters which minimize the empirical cost, and moving average parameters can be used as the estimator of them, if there are enough training samples [17]. One pass learning is then proposed, and it is the combination of learning rate schedule and averaged SGD using moving average [18]. When the moving average model outperforms the model aggregated with model averaging, the moving average model is broadcasted to update local workers. Since the learning rate of one pass learning is difficult to be adjusted, it is challenging to train different models in different domains.\nIn this paper, we propose a new approach which applies exponential moving average (EMA) directly in large-scale synchronous-based parallel training. It is a kind of noninterference method that the EMA model is not broadcasted, after the parameters of each worker are synchronized. It is applied as the final model of the training. The exponential moving average method in parallel training will be described in Section 2. Neural network models are successfully trained for LVCSR, using this method. The experiments and results are presented in Section 3, followed by the conclusion in Section 4."
    }, {
      "heading" : "2. EXPONENTIAL MOVING AVERAGE MODEL",
      "text" : "In recognition applications, the parameters θ of neural network is trained for classification. It’s also an optimization problem:\nargmin θ\n1\nt\nt∑\ni=1\n(L(fθ(x), y))\nWhere t is the number of data points, (x, y) is the input data and correspondent target, L is the loss function, and fθ denotes the network. Let θ∗ be the parameters that minimize the empirical cost. Large scale recognition training needs to deal with the optimization problem with billions of training data, and makes it hard to find the θ∗. SGD and its variants are presented promising learning results for large scale optimization problem, and become the most popular methods of deep learning."
    }, {
      "heading" : "2.1. Model averaging and block-wise model updating filter",
      "text" : "In order to reduce the time cost of training, data parallelism is implemented. The full training dataset is partitioned into N splits without overlapping, and they are distributed to N GPUs.\nEach GPU optimizes local model in parallel with one split of training dataset. After a mini-batch training, the global model is needed to update, and it is computed with model averaging or BMUF, and consequently broadcasted to GPUs to update their local models. For model averaging method, all local models are synchronized and averaged, and then aggregated model θ̄(t) is sent back to GPUs [14, 15]. For BMUF method, the global model θg(t) is employed, instead of θ̄(t) in model averaging method. The synchronization and updating process of θg(t) in BMUF as follows:\nθ̄(t) = 1\nN\nN∑\ni=1\nθi\nG(t) = θ̄(t)− θg(t− 1)\n∆(t) = ηt∆(t− 1) + ζtG(t)\nWhere G(t) denotes model update, and ∆(t) is the globalmodel update. There are two parameters in BMUF, block momentum η, and block learning rate ζ. Then, the global model is updated as\nθg(t) = θg(t− 1) + ∆(t)\nConsequently, θg(t) is broadcasted to all GPUs to update their local models.\nIt is worth noting that when block momentum and block learning rate are set as 0 and 1, BMUF becomes model averaging. We treat model averaging and BMUF as model averaging based methods."
    }, {
      "heading" : "2.2. Moving Average and Exponential Moving Average",
      "text" : "Averaged SGD is proposed to further accelerate the convergence speed of SGD. Averaged SGD leverages the moving average (MA) θ̄ as the estimator of θ∗ [17]:\nθ̄t = 1\nt\nt∑\nτ=1\nθτ\nWhere θτ is computed by model averaging or BMUF. It is shown that θ̄t can well converge to θ ∗, with the large enough training dataset in single GPU training. It can be considered as a non-interference strategy that θ̄t does not participate the main optimization process, and only takes effect after the end of entire optimization. However, for the parallel training implementation, each θτ is computed by model averaging and BMUF with multiple models, and moving average model θ̄t does not well converge, compared with single GPU training.\nModel averaging based methods are employed in parallel training of large scale dataset, because of their faster convergence, and especially no-degradation implementation of BMUF. But combination of model averaged based methods and moving average does not match the expectation of further enhance performance and it is presented as\nθ̄gt = 1\nt\nt∑\nτ=1\nθgτ\nThe weight of each θgt is equal in moving average method regardless the effect of temporal order. But t closer to the end of training achieve higher accuracy in the model averaging based approach, and thus it should be with more proportion in final θ̄g. As a result, exponential moving average(EMA) is appropriate, which the weight for each older parameters decrease exponentially, and never reaching zero. After moving average based methods, the EMA parameters are updated recursively as\nθ̂gt = αθ̂gt−1 + (1− α)θgt\nHere α represents the degree of weight decrease, and called exponential updating rate. EMA is also a non-interference training strategy that is implemented easily, as the updated model is not broadcasted. Therefore, there is no need to add extra learning rate updating approach, as it can be appended to existing training procedure directly."
    }, {
      "heading" : "3. EXPERIMENTS AND RESULTS",
      "text" : ""
    }, {
      "heading" : "3.1. Training Data",
      "text" : "In order to present the performance of our proposed method, we trained acoustic model for LVCSR. A large quantity of labeled data is needed for training a more accurate acoustic model. We collect the 17000 hours labeled data from Shenma voice search, which is one of the most popular mobile search\nengines in China. The dataset is created from anonymous online users’ search queries in Mandarin, and all audio file’s sampling rate is 16kHz, recorded by mobile phones. This dataset consists of many different conditions, such as diverse noise even low signal-to-noise, babble, dialects, accents, hesitation and so on. The dataset is divided into training set, validation set and test set, and the quantity of them is shown in Table 1. The three sets are split according to speakers, in order to avoid utterances of same speaker appearing in three sets simultaneously. The overfitting can be prevented in time, if there is a apparent gap between the frame error rate (FER) of training and validation set."
    }, {
      "heading" : "3.2. Experimental setup",
      "text" : "LSTM RNNs outperform conventional RNNs for speech recognition system, especially deep LSTM RNNs, because of its long-range dependencies more accurately for temporal sequence conditions [19, 20]. Shenma voice search is a streaming service that intermediate recognition results displayed while users are still speaking. So as for online recognition in real time, we prefer unidirectional LSTM model rather than bidirectional one. Thus, the parallel training procedure is unidirectional LSTM-based.\nA 28-dimensional filter bank feature is extracted for each frame, and is concatenated with first and second order difference as the final input of the network. The architecture we trained consists of two LSTM layers with sigmoid activation function, followed by a full-connection layer. The out layer is a softmax layer with 11088 hidden markov model (HMM) tied-states as output classes, the loss function is cross-entropy (CE). The performance metric of the system in Mandarin is reported with character error rate (CER). The alignment of frame-level ground truth is obtained by GMM-HMM system. Mini-batched SGD is utilized with momentum trick and the network is trained for a total of 4 epochs. The block learning rate and block momentum of BMUF are set as 1 and 0.9. 5-gram language model is leveraged in decoder, and the vocabulary size is as large as 760000.\nEMA method is proposed for parallel training problem. In our training system, it is employed on the MPI-based HPC cluster where 8 GPUs are used to train neural networkmodels. Each GPU processes non-overlap subset split from the entire large scale dataset in parallel.\nLocal models from distributed workers synchronize with each other in decentralized way. In the traditional model av-\neraging and BMUF method, a parameter server waits for all workers to send their local models, aggregate them, and send the updated model to all workers. Computing resource of workers is wasted until aggregation of the parameter server done. Decentralized method makes full use of computing resource. There is no centralized parameter server, and peer to peer communication is used to transmit local models between workers. Local model θi of i-th worker in N workers cluster is split to N pieces θi,j j = 1 · · ·N , and send to corresponding worker. In the aggregation phase, j-th worker computed N splits of model θi,j i = 1 · · ·N and send updated model θ̄gj back to workers. As a result, all workers participate in aggregation and no computing resource is dissipated. It is significant to promote training efficiency, when the size of neural network model is too large. The EMA model is also updated additionally, but not broadcasting it.\nBesides, frame stacking leads to reduce the computation and training time dramatically [21]. Frames are stacked so as that the network sees multiple frames at a time. The super frame after stacking is the input feature of the network and it contains abundant information. As a result, 3 frames are stacked without overlapping in the training procedure."
    }, {
      "heading" : "3.3. Results",
      "text" : "The test set including about 9000 samples contains various real world conditions. It simulates the majority of user scenarios, and could well evaluate the performance of a trained model. BMUF based approach, which has no worse performance than the single-GPU training procedure, is the baseline of experiments. The results of MA and EMA methods on the basis of BMUF are presented, and we call them MA-based methods.\nSince the EMA is a non-interference method, the performance can not be evaluated with the real-time FER. Therefore, the FER on validation sets are computed after every epoch. In order to present the decoding performance of MAbased methods, we extract 4 temporary models from each epoch to visualize the degradation of CER. FER curves of LSTM models trained with BMUF, MA and EMA methods are shown in Figure 1. It is significant that the frame accuracy of MA-based methods are higher than those of BMUF. Frame accuracies between MA-based methods have slight difference. Though the EMA only perform better than MA slightly on FER, there is an obvious difference between the CER of EMA and MA, as shown in Figure 2 which illustrates CER curves of different models after decoding. It demonstrates that decoding result of EMA is always much better than that of BMUF, but that of MA fluctuates greatly, and even higher than that of BMUF sometimes. From the Table 2 which shows the CER of final models trained from three methods, the superiority of EMA over the others can be also observed. EMA method achieves about relative 3.9% CER reduction on test set, while MA method only achieves relative 2.1% CER\nreduction.\nMoreover, the CER of final DNN models with 8 layers are also presented in Table 2, and the CER of EMA method decreases relative 8.4% compared with baseline. Therefore, more accuracy models are trained with large-scale parallel training using EMA method, and it is more stable than MA method."
    }, {
      "heading" : "4. CONCLUSION",
      "text" : "The exponential moving average method is proposed in this paper for multi-GPUs cluster parallel training with almost linear speedup. It is demonstrated that unidirectional LSTM and DNN models trained with EMA method have better decoding results than that of BMUF and traditional moving average methods for large vocabulary continues speech recognition in Mandarin. Our future work includes 1) Employing this method to CNN, Connectionist Temporal Classification\n(CTC), attention-based neural networks and other hybrid deep neural network architecture; 2) Extending this method from frame-wise discriminative training to sequence discriminative training such as maximum mutual information (MMI) and segmental Minimum Bayes-Risk (sMBR); 3) Develop more approaches for parallel training with better performance."
    }, {
      "heading" : "5. REFERENCES",
      "text" : "[1] Alex Graves and Navdeep Jaitly, “Towards end-to-end\nspeech recognition with recurrent neural networks.,” in ICML, 2014, vol. 14, pp. 1764–1772.\n[2] Alex Graves, Navdeep Jaitly, and Abdel-rahman Mo-\nhamed, “Hybrid speech recognition with deep bidirectional lstm,” in Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 273–278.\n[3] Dario Amodei, Rishita Anubhai, Eric Battenberg, Carl\nCase, Jared Casper, Bryan Catanzaro, Jingdong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, et al., “Deep speech 2: End-to-end speech recognition in english and mandarin,” arXiv preprint arXiv:1512.02595, 2015.\n[4] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-\nton, “Imagenet classification with deep convolutional neural networks,” in Advances in neural information processing systems, 2012, pp. 1097–1105.\n[5] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun, “Deep residual learning for image recognition,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 770–778.\n[6] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio, “Neural machine translation by jointly learning to align and translate,” arXiv preprint arXiv:1409.0473, 2014.\n[7] Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl,\nAbdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al., “Deep neural networks for acoustic modeling in speech recognition: The shared views of four\nresearch groups,” IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82–97, 2012.\n[8] Alex Graves, Abdel-rahman Mohamed, and Geoffrey\nHinton, “Speech recognition with deep recurrent neural networks,” in Acoustics, speech and signal processing (icassp), 2013 ieee international conference on. IEEE, 2013, pp. 6645–6649.\n[9] Hasim Sak, AndrewW Senior, and Françoise Beaufays,\n“Long short-term memory recurrent neural network architectures for large scale acoustic modeling.,” in Interspeech, 2014, pp. 338–342.\n[10] Yu Zhang, Guoguo Chen, Dong Yu, Kaisheng Yaco,\nSanjeev Khudanpur, and James Glass, “Highway long short-term memory rnns for distant speech recognition,” in Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. IEEE, 2016, pp. 5755–5759.\n[11] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen,\nMatthieu Devin, Mark Mao, Andrew Senior, Paul Tucker, Ke Yang, Quoc V Le, et al., “Large scale distributed deep networks,” in Advances in neural information processing systems, 2012, pp. 1223–1231.\n[12] Shanshan Zhang, Ce Zhang, Zhao You, Rong Zheng,\nand Bo Xu, “Asynchronous stochastic gradient descent for dnn training,” in 2013 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2013, pp. 6660–6663.\n[13] Jianmin Chen, Rajat Monga, Samy Bengio, and Rafal\nJozefowicz, “Revisiting distributed synchronous sgd,” arXiv preprint arXiv:1604.00981, 2016.\n[14] Ryan McDonald, Keith Hall, and Gideon Mann, “Dis-\ntributed training strategies for the structured perceptron,” in Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics, 2010, pp. 456–464.\n[15] Martin Zinkevich, Markus Weimer, Lihong Li, and\nAlex J Smola, “Parallelized stochastic gradient descent,” in Advances in neural information processing systems, 2010, pp. 2595–2603.\n[16] Kai Chen and Qiang Huo, “Scalable training of deep\nlearning machines by incremental block training with intra-block parallel optimization and blockwise modelupdate filtering,” in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 5880–5884.\n[17] Boris T Polyak and Anatoli B Juditsky, “Acceleration of\nstochastic approximation by averaging,” SIAM Journal\non Control and Optimization, vol. 30, no. 4, pp. 838– 855, 1992.\n[18] Wei Xu, “Towards optimal one pass large scale learn-\ning with averaged stochastic gradient descent,” arXiv preprint arXiv:1107.2490, 2011.\n[19] Michiel Hermans and Benjamin Schrauwen, “Training\nand analysing deep recurrent neural networks,” in Advances in Neural Information Processing Systems, 2013, pp. 190–198.\n[20] Haşim Sak, Félix de Chaumont Quitry, Tara Sainath,\nKanishka Rao, et al., “Acoustic modelling with cd-ctcsmbr lstm rnns,” in Automatic Speech Recognition and Understanding (ASRU), 2015 IEEE Workshop on. IEEE, 2015, pp. 604–609.\n[21] Haşim Sak, Andrew Senior, Kanishka Rao, and\nFrançoise Beaufays, “Fast and accurate recurrent neural network acoustic models for speech recognition,” arXiv preprint arXiv:1507.06947, 2015."
    } ],
    "references" : [ {
      "title" : "Towards end-to-end speech recognition with recurrent neural networks",
      "author" : [ "Alex Graves", "Navdeep Jaitly" ],
      "venue" : "ICML, 2014, vol. 14, pp. 1764–1772.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Hybrid speech recognition with deep bidirectional lstm",
      "author" : [ "Alex Graves", "Navdeep Jaitly", "Abdel-rahman Mohamed" ],
      "venue" : "Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 273–278.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Deep speech 2: End-to-end speech recognition in english and mandarin",
      "author" : [ "Dario Amodei", "Rishita Anubhai", "Eric Battenberg", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Jingdong Chen", "Mike Chrzanowski", "Adam Coates", "Greg Diamos" ],
      "venue" : "arXiv preprint arXiv:1512.02595, 2015.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton" ],
      "venue" : "Advances in neural information processing systems, 2012, pp. 1097–1105.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 770–778.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1409.0473, 2014.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Deep neural networks for acoustic modeling in speech recognition: The shared views of four  research groups",
      "author" : [ "Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath" ],
      "venue" : "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82–97, 2012.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Speech recognition with deep recurrent neural networks",
      "author" : [ "Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton" ],
      "venue" : "Acoustics, speech and signal processing (icassp), 2013 ieee international conference on. IEEE, 2013, pp. 6645–6649.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Long short-term memory recurrent neural network architectures for large scale acoustic modeling",
      "author" : [ "Hasim Sak", "AndrewW Senior", "Françoise Beaufays" ],
      "venue" : "Interspeech, 2014, pp. 338–342.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Highway long short-term memory rnns for distant speech recognition",
      "author" : [ "Yu Zhang", "Guoguo Chen", "Dong Yu", "Kaisheng Yaco", "Sanjeev Khudanpur", "James Glass" ],
      "venue" : "Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. IEEE, 2016, pp. 5755–5759.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Large scale distributed deep networks",
      "author" : [ "Jeffrey Dean", "Greg Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Mark Mao", "Andrew Senior", "Paul Tucker", "Ke Yang", "Quoc V Le" ],
      "venue" : "Advances in neural information processing systems, 2012, pp. 1223–1231.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Asynchronous stochastic gradient descent for dnn training",
      "author" : [ "Shanshan Zhang", "Ce Zhang", "Zhao You", "Rong Zheng", "Bo Xu" ],
      "venue" : "2013 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2013, pp. 6660–6663.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Revisiting distributed synchronous sgd",
      "author" : [ "Jianmin Chen", "Rajat Monga", "Samy Bengio", "Rafal Jozefowicz" ],
      "venue" : "arXiv preprint arXiv:1604.00981, 2016.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Distributed training strategies for the structured perceptron",
      "author" : [ "Ryan McDonald", "Keith Hall", "Gideon Mann" ],
      "venue" : "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics, 2010, pp. 456–464.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Parallelized stochastic gradient descent",
      "author" : [ "Martin Zinkevich", "Markus Weimer", "Lihong Li", "Alex J Smola" ],
      "venue" : "Advances in neural information processing systems, 2010, pp. 2595–2603.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Scalable training of deep learning machines by incremental block training with intra-block parallel optimization and blockwise modelupdate filtering",
      "author" : [ "Kai Chen", "Qiang Huo" ],
      "venue" : "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 5880–5884.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Acceleration of stochastic approximation by averaging",
      "author" : [ "Boris T Polyak", "Anatoli B Juditsky" ],
      "venue" : "SIAM Journal  on Control and Optimization, vol. 30, no. 4, pp. 838– 855, 1992.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Towards optimal one pass large scale learning with averaged stochastic gradient descent",
      "author" : [ "Wei Xu" ],
      "venue" : "arXiv preprint arXiv:1107.2490, 2011.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Training and analysing deep recurrent neural networks",
      "author" : [ "Michiel Hermans", "Benjamin Schrauwen" ],
      "venue" : "Advances in Neural Information Processing Systems, 2013, pp. 190–198.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Acoustic modelling with cd-ctcsmbr lstm rnns",
      "author" : [ "Haşim Sak", "Félix de Chaumont Quitry", "Tara Sainath", "Kanishka Rao" ],
      "venue" : "Automatic Speech Recognition and Understanding (ASRU), 2015 IEEE Workshop on. IEEE, 2015, pp. 604–609.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Fast and accurate recurrent neural network acoustic models for speech recognition",
      "author" : [ "Haşim Sak", "Andrew Senior", "Kanishka Rao", "Françoise Beaufays" ],
      "venue" : "arXiv preprint arXiv:1507.06947, 2015.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Over the past few years, neural networks has been widely used in some domains, such as large vocabulary continuous speech recognition (LVCSR) [1, 2, 3], image recognition [4, 5] and neural machine translation [6].",
      "startOffset" : 142,
      "endOffset" : 151
    }, {
      "referenceID" : 1,
      "context" : "Over the past few years, neural networks has been widely used in some domains, such as large vocabulary continuous speech recognition (LVCSR) [1, 2, 3], image recognition [4, 5] and neural machine translation [6].",
      "startOffset" : 142,
      "endOffset" : 151
    }, {
      "referenceID" : 2,
      "context" : "Over the past few years, neural networks has been widely used in some domains, such as large vocabulary continuous speech recognition (LVCSR) [1, 2, 3], image recognition [4, 5] and neural machine translation [6].",
      "startOffset" : 142,
      "endOffset" : 151
    }, {
      "referenceID" : 3,
      "context" : "Over the past few years, neural networks has been widely used in some domains, such as large vocabulary continuous speech recognition (LVCSR) [1, 2, 3], image recognition [4, 5] and neural machine translation [6].",
      "startOffset" : 171,
      "endOffset" : 177
    }, {
      "referenceID" : 4,
      "context" : "Over the past few years, neural networks has been widely used in some domains, such as large vocabulary continuous speech recognition (LVCSR) [1, 2, 3], image recognition [4, 5] and neural machine translation [6].",
      "startOffset" : 171,
      "endOffset" : 177
    }, {
      "referenceID" : 5,
      "context" : "Over the past few years, neural networks has been widely used in some domains, such as large vocabulary continuous speech recognition (LVCSR) [1, 2, 3], image recognition [4, 5] and neural machine translation [6].",
      "startOffset" : 209,
      "endOffset" : 212
    }, {
      "referenceID" : 6,
      "context" : "Fully-connected feed-forward deep neural networks (DNNs) and Recurrent neural networks (RNNs), especially long short-term memory (LSTM) RNNs have shown the effective performance for LVCSR [7, 8, 9, 10].",
      "startOffset" : 188,
      "endOffset" : 201
    }, {
      "referenceID" : 7,
      "context" : "Fully-connected feed-forward deep neural networks (DNNs) and Recurrent neural networks (RNNs), especially long short-term memory (LSTM) RNNs have shown the effective performance for LVCSR [7, 8, 9, 10].",
      "startOffset" : 188,
      "endOffset" : 201
    }, {
      "referenceID" : 8,
      "context" : "Fully-connected feed-forward deep neural networks (DNNs) and Recurrent neural networks (RNNs), especially long short-term memory (LSTM) RNNs have shown the effective performance for LVCSR [7, 8, 9, 10].",
      "startOffset" : 188,
      "endOffset" : 201
    }, {
      "referenceID" : 9,
      "context" : "Fully-connected feed-forward deep neural networks (DNNs) and Recurrent neural networks (RNNs), especially long short-term memory (LSTM) RNNs have shown the effective performance for LVCSR [7, 8, 9, 10].",
      "startOffset" : 188,
      "endOffset" : 201
    }, {
      "referenceID" : 10,
      "context" : "Asynchronous SGD is a successful attempt [11, 12], and it is shown that parallel training with asynchronous SGD can many times speedup without lowering the accuracy.",
      "startOffset" : 41,
      "endOffset" : 49
    }, {
      "referenceID" : 11,
      "context" : "Asynchronous SGD is a successful attempt [11, 12], and it is shown that parallel training with asynchronous SGD can many times speedup without lowering the accuracy.",
      "startOffset" : 41,
      "endOffset" : 49
    }, {
      "referenceID" : 12,
      "context" : "Besides, synchronous SGD is another positive effort, where the parameter server waits for every workers to finish their computation and send their local models to it, and then it sends updated model back to all workers [13].",
      "startOffset" : 219,
      "endOffset" : 223
    }, {
      "referenceID" : 13,
      "context" : "Model averaging is a method for large-scale parallel training, which the final model is averaged from all parameters of separated models [14, 15].",
      "startOffset" : 137,
      "endOffset" : 145
    }, {
      "referenceID" : 14,
      "context" : "Model averaging is a method for large-scale parallel training, which the final model is averaged from all parameters of separated models [14, 15].",
      "startOffset" : 137,
      "endOffset" : 145
    }, {
      "referenceID" : 15,
      "context" : "It can achieve improvement or nodegradation of recognition performance compared with minibatch SGD on single GPU [16].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 16,
      "context" : "It is demonstrated that the performance of moving average of the parameters obtained by SGD is as good as that of the parameters which minimize the empirical cost, and moving average parameters can be used as the estimator of them, if there are enough training samples [17].",
      "startOffset" : 269,
      "endOffset" : 273
    }, {
      "referenceID" : 17,
      "context" : "One pass learning is then proposed, and it is the combination of learning rate schedule and averaged SGD using moving average [18].",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 13,
      "context" : "For model averaging method, all local models are synchronized and averaged, and then aggregated model θ̄(t) is sent back to GPUs [14, 15].",
      "startOffset" : 129,
      "endOffset" : 137
    }, {
      "referenceID" : 14,
      "context" : "For model averaging method, all local models are synchronized and averaged, and then aggregated model θ̄(t) is sent back to GPUs [14, 15].",
      "startOffset" : 129,
      "endOffset" : 137
    }, {
      "referenceID" : 16,
      "context" : "Averaged SGD leverages the moving average (MA) θ̄ as the estimator of θ∗ [17]:",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 18,
      "context" : "LSTM RNNs outperform conventional RNNs for speech recognition system, especially deep LSTM RNNs, because of its long-range dependencies more accurately for temporal sequence conditions [19, 20].",
      "startOffset" : 185,
      "endOffset" : 193
    }, {
      "referenceID" : 19,
      "context" : "LSTM RNNs outperform conventional RNNs for speech recognition system, especially deep LSTM RNNs, because of its long-range dependencies more accurately for temporal sequence conditions [19, 20].",
      "startOffset" : 185,
      "endOffset" : 193
    }, {
      "referenceID" : 20,
      "context" : "Besides, frame stacking leads to reduce the computation and training time dramatically [21].",
      "startOffset" : 87,
      "endOffset" : 91
    } ],
    "year" : 2017,
    "abstractText" : "As training data rapid growth, large-scale parallel training with multi-GPUs cluster is widely applied in the neural network model learning currently. We present a new approach that applies exponential moving average method in large-scale parallel training of neural network model. It is a non-interference strategy that the exponential moving average model is not broadcasted to distributed workers to update their local models after model synchronization in the training process, and it is implemented as the final model of the training system. Fully-connected feed-forward neural networks (DNNs) and deep unidirectional Long short-term memory (LSTM) recurrent neural networks (RNNs) are successfully trained with proposed method for large vocabulary continuous speech recognition on Shenma voice search data in Mandarin. The character error rate (CER) of Mandarin speech recognition further degrades than state-of-the-art approaches of parallel training.",
    "creator" : "LaTeX with hyperref package"
  }
}