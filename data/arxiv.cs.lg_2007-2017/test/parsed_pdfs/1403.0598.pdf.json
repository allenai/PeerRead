{
  "name" : "1403.0598.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "The Structurally Smoothed Graphlet Kernel",
    "authors" : [ "Pinar Yanardag", "S.V. N. Vishwanathan" ],
    "emails" : [ "ypinar@purdue.edu", "vishy@stat.purdue.edu" ],
    "sections" : [ {
      "heading" : "1. INTRODUCTION",
      "text" : "In this paper, we are interested in comparing graphs by computing a kernel between graphs [39]. Graph kernels are popular because many datasets from diverse domains such as bio-informatics [3, 30], chemo-informatics [2], and web data mining [41] naturally can be represented as graphs. Almost all graph kernels (implicitly or explicitly) represent a graph as a (normalized or un-normalized) vector which contains the frequency of occurrence of motifs or subgraphs1. The key idea here is that well chosen motifs can capture\n1The kernels proposed by [21] are a notable exception.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. KDD’14, August 24–27, 2014, New York City, New York USA. Copyright 2014 ACM 978-1-4503-1462-6 /12/08 ...$15.00.\nthe semantics of the graph structure while being computationally tractable. For instance, counting walks in a graph leads to the random walk graph kernel of Borgwardt et al. [3] (see Vishwanathan et al. [39] for an efficient algorithm for computing this kernel). Other popular motifs include subtrees [31], shortest paths [5], and cycles [16]. Of particular interest to us are the graphlet kernels of Shervashidze et al. [32]. The motif used in this kernel is the set of unique sub-graphs of size k, which were christened as graphlets by Przulj [28].\nObservation 1: Computing meaningful graphlet kernels that have high discriminative ability requires a careful selection of k. If k is small, then the number of unique graphlets is small (i.e., the length of the feature vector is small). See Figure 1. Consequently the feature vector does not provide meaningful discrimination between two graphs. On the other hand, if k is large then a) the set of unique graphlets grows exponentially (i.e., the feature vector is very high dimensional) but b) only a small number of unique graphlets will be observed in a given graph (i.e., the feature vector is very sparse). Moreover, the probability that two graphs will contain a given large sub-graph is very small. Consequently, a graph is similar to itself but not to any other graph in the training data. This is well known as the diagonal dominance problem in the machine learning community [17], and the resulting kernel matrix is close to the identity matrix. In other words, the graphs are orthogonal to each other in the feature space. However, it is desirable to use large values of k in order to gain better discriminative ability. One way to circumvent the diagonal dominance problem is to view the normalized graphlet-frequency vector as estimating a multinomial distribution, and use smoothing.\nObservation 2: The normalized graphlet-frequency vector exhibits power-law behavior, especially for large values of k. In other words, a few popular graphlets occur very frequently while a vast majority of graphlets will occur very rarely. Put another way, a few graphlets dominate the distribution. To see this, we randomly sampled a graph from six benchmark datasets (details of the datasets can be found in Section 6), and exhaustively computed occurrences of all graphlets of size k = 8 and plotted the resulting histogram on a log-log scale in Figure 2. As can be seen, the frequencies are approximately linear in the log-log scale which indicates power law behavior. Therefore, any smoothing technique that we use on the normalized graphlet-frequency vector must respect this power-law behaviour.\nObservation 3: The space of graphlets is structured. What we mean by this is that graphlets of different sizes are related\nar X\niv :1\n40 3.\n05 98\nv1 [\ncs .L\nG ]\n3 M\nar 2\n01 4\nto each other. While many such relationships can be derived, we will work with perhaps the simplest one which is depicted in Figure 4. Here, we construct a directed acyclic graph (DAG) with the following property: a node at depth k denotes a graphlet of size k. Given a graphlet g of size k and other graphlet g′ of size k + 1 we add an edge from g to g′ if, and only if, g can be obtained from g′ by deleting a node of g′. This shows that graphlets of size k have a strong relationship to graphlets of size k + 1 and one must respect this relationship when deriving a smoothing technique.\nOur contributions in this paper are as follows. First, we propose a new smoothing technique for graphlets which is inspired by Kneser-Ney smoothing [20] used for language models in natural language processing. Our model satisfies the desiderata that we outlined above, that is, it respects the power law behavior of the counts and yet takes into account the structure of the space of graphlets. Second, we provide a novel Bayesian version of our\nmodel that is extended from the Hierarchical Pitman-Yor process of Teh [35]. Unlike the traditional Hierarchical Pitman-Yor Process (HPYP) where the base distribution is given by another Pitman-Yor Process (PYP), in our case it is given by a transformation of a PYP that is guided by the structure of the space. Third, we perform experiments to validate and understand how smoothing affects the performance of graphlet kernels.\nThe structure of the paper is as follows. In Section 2, we discuss background on graphlet kernels and smoothing techniques. In Section 3, we introduce our Kneser-Ney-inspired smoothing technique. In Section 4, we propose an alternate Bayesian version of our model. Related work is discussed in Section 5. In Section 6, we perform experiments and discuss our findings, and we conclude the paper with Section 7."
    }, {
      "heading" : "2. BACKGROUND",
      "text" : ""
    }, {
      "heading" : "2.1 Notation",
      "text" : "A graph is a pair G = (V,E) where V = { v1, v2, . . . , v|V | } is an ordered set of vertices or nodes and E ⊆ V × V is a set of edges. Given G = (V,E) and H = (VH , EH), H is a subgraph of G iff there is an injective mapping α : VH → V such that (v, w) ∈ EH iff (α(v), α(w)) ∈ E. Two graphs G = (V,E) and G′ = (V ′, E′) are isomorphic if there exists a bijective mapping g : V → V ′ such that (vi, vj) ∈ E iff (g(vi), g(vj)) ∈ E′. Graphlets are small, connected, non-isomorphic sub-graphs of a large network. They were introduced by Przulj [28] to design a new measure of local structural similarity between biological networks. Graphlets up to size five are shown in Figure 3."
    }, {
      "heading" : "2.2 The Graphlet kernel",
      "text" : "Let Gk = {g1, g2, . . . , gnk} be the set of size-k graphlets where nk denotes the number of unique graphlets of size k. Given a graph G, we define fG as a normalized vector of length nk whose i-th component corresponds to the frequency of occurrence of gi in G:\nfG = ( c1∑nk j cj , · · · , cnk∑nk j cj )T . (1)\nHere ci denotes number of times gi occurs as a sub-graph of G. Given two graphs G and G′, the graphlet kernel kg is defined as:\nkg(G,G ′) := f>G fG′ , (2)\nwhich is simply the dot product between the normalized graphletfrequency vectors."
    }, {
      "heading" : "2.3 Smoothing multinomial distributions",
      "text" : "In this section we will briefly review smoothing techniques for multinomial distributions and show that graphlet kernels are indeed based on estimating a multinomial. Suppose we observe a sequence e1, e2, . . . , en containing n discrete events drawn from a ground set of size M and we would like to estimate the probability P (ei) of observing each event ei. Maximum likelihood estimation based on sequence counts obtained from the observations provides a way to compute P (ei):\nPMLE(ei) = ci∑ j cj , (3)\nwhere ci denotes the number of times the event ei appears in the observed sequence and ∑ j cj denotes the total number of observed events. Therefore, one can easily see that the representation used in graphlet kernels in Section 2.2 is actually an MLE estimate on the observed sequences of graphlets.\nHowever, MLE estimates of the multinomial distribution are spiky, that is, they assign zero probability to events that did not occur in the observed sequence. What this means is that an event with low probability is often estimated to have zero probability mass. This issue occurs in a number of different domains and therefore, unsurprisingly, has received significant research attention [42]; smoothing methods are typically used to address this problem. The general idea behind smoothing is to discount the probabilities of the observed events and to assign extra probability mass to unobserved events.\nLaplace smoothing is the simplest and one of the oldest smoothing methods, where only a fixed count of 1 is added to every event. This results in the estimate\nPLaplace(ei) =\n∑ j cj∑\nj cj +M PMLE + M∑ j cj +M 1 M (4)\nor equivalently,\nPLaplace(ei) = λPMLE(ei) + (1− λ) 1\nM , (5)\nwhere λ is a normalization factor which ensures that the distributions sum to one. The intuition behind Laplace smoothing is basically to interpolate a uniform distribution with the MLE distribution. Although Laplace smoothing resolves the zero-count problem, it does not produce a power law distribution which is a desirable feature in real-life models. Therefore, researchers have worked on finding smoothing techniques that respect power law behavior. The key idea behind these methods is to redistribute the probability mass using a so-called fallback model, where the fallback model is also recursively estimated.\nKneser-Ney smoothing is a fallback based smoothing method which has been identified as the state-of-the-art smoothing in natural language processing by several studies [7]. Kneser-Ney smoothing computes the probability of an event by using the raw counts that are discounted by using a fixed mass. Then, the discounted mass is re-added equally to all event probabilities by using a base distribution: PKN (ei) = max{ci − d, 0}∑\nj cj + n∑ j=1 |{ej : cj > d}| d∑ j cj P0(ei),\n(6)\nwhere d ≥ 0 is the discounting parameter, P0(·) is the base distribution and P0(ei) denotes the probability mass the base distribution assigned to event ei. The quantity ∑n j=1 |{ej : cj > d}| is\na normalization factor to ensure that the distribution sums to 1 and simply denotes the number of events the discount is applied. When discount parameter d = 0, we recover MLE estimation since no mass is taken away from any event. When d is very large, then we recover the base distribution on the events since we discount all the available mass. One should interpolate between these two extremes in order to get a reasonable smoothed estimate. In order to propose a new Kneser-Ney-based smoothing framework, one needs to specify the discount parameter d and the base distribution P0(·). In the next section we will show how one can derive a meaningful base distribution for graphlets."
    }, {
      "heading" : "3. DEFINING A BASE DISTRIBUTION",
      "text" : "The space of graphlets has an inherent structure. One can construct a directed acyclic graph (DAG) in order to show how different graphlets are related with each other. A node at depth k denotes a graphlet of size k. When it is clear from context, we will use the node of the DAG and a graphlet interchangeably. Given a graphlet gi of size k and another graphlet gj of size k + 1 we add an edge from gi to gj if, and only if, gi can be obtained from gj by deleting a node of gj . We first discuss how to construct this DAG and then discuss how we can use this DAG to define a base distribution.\nThe first step towards constructing our DAG is to obtain all unique graphlet types of size k. Therefore, we first exhaustively generate all possible graphs of size k (this involves a one timeO(2k) effort), and use Nauty [25] to obtain their canonically-labelled isomorphic representations. In order to obtain the edges of the DAG, we take a node at depth k + 1, which denotes a canonically-labeled isomorphic graphlet and delete a node to obtain a size k graphlet. We use Nauty to canonically-label the size k graph, which in turn allows us to link to a node at depth k. By deleting each node of the k+1 sized graphlet we can therefore obtain k + 1 possible links. We repeat this for all nodes at level k + 1 before proceeding to level k. Figure 4 shows the constructed DAG for size k = 5 graphlets. Since all descendants of a given graphlet at level k are at level k + 1, a topological ordering of the vertices is possible, and hence it is easy to see that the resulting graph is a DAG.\nNow, let us define the edge weight between an arbitrary graphlet gj of size k + 1 and its parent gi of size k. Let sij denote the number of times gi occurs as a sub-graph of gj and Cgi denote all the children of graphlet gi in the DAG. Then, we define the edge\nweight between graphlet gi and gj as\nwij = sij∑\ngj′∈Cgi sij′\n(7)\nNext we show how the DAG can be used to define a base distribution. Suppose we have a distribution over graphlets of size k. Then we can transform it into a distribution over size k+1 graphlets in a recursive way by exploiting edge connections in the DAG as follows:\nP0(gj) = ∑\ngi∈Pa(gj)\nwijP0(gi). (8)\nHere gj denotes a graphlet of size k + 1 and Pa(gj) denotes the parents of graphlet gj in the DAG.\nLEMMA 1. Given the set of child nodes of a graphlet gi, if the edge weights on the DAG are all non-negative and satisfy∑\ngj∈C(gi)\nwij = 1, (9)\nthen (8) defines a valid probability distribution.\nPROOF. For clarity, we introduce a few notations to facilitate the proof. Assume that there are in total J child graphlets, which we denote by g1, g2, · · · , gJ . Further assume that there are in total I parent graphlets, which we denote by g1, · · · , gI . Let Ij denote the number of parents graphlet gj has, i.e. Ij = |Pa(gj)|. Clearly, we have that ∑J j=1 Ij = I . Thus, we have\nJ∑ j=1 P0(gj) = J∑ j=1 [ Ij∑ i=1 wijP0(gi)]\n= I∑ i=1 [ ∑\n{j:gj∈C(gi)}\nwijP0(gi)]\n= I∑ i=1 P0(gi)[ ∑\n{j:gj∈C(gi)}\nwij ]\n= I∑ i=1 P0(gi) = 1,\nwhich completes the proof.\nThe base distribution we defined above respects the structural space of the graphlets. Pretend for a moment that we are given only the frequencies of occurrences of size k graphlets and are asked to infer the probability of occurrences of size k + 1 graphlets. Without any additional information, one can infer the distribution as follows: each parent graphlet casts a vote for every child graphlet based on how many times the parent occurs in the child. The votes of all parents are accumulated and this provides a distribution over size k + 1 graphlets. In other words, a natural way to infer the distribution at level k+1 is to use how likely we are to see its subgraphs. Figure 5 illustrates the relationship between a graphlet of size k+1 and its parent graphlets of size k. Here, edge weights denote how many times each parent occurs as a sub-graph of g15. In the case that we do not observe graphlet g15, it still gets probability mass proportional to the edge weight from its parents g7, g6, g5, thus overcoming the sparsity problem of unseen data. Our model combines this base distribution with the observed real data to generates the final distribution.\nThe way how the discounted mass is distributed is controlled by the edge weights between two graphlets. In Equation 7 we defined edge weights according to the number of times a parent node occurs in its children. However, one can explore different weighting schemes between the nodes on the DAG based on domain knowl-\nedge. For example, in the case of structured graphs such as social networks, one might benefit from weighting the edges according to the PageRank [26] score of the nodes. Similarly, other link analysis algorithms such as Hubs or Authority given by HITS algorithm [19] can be used in order to exploit the domain knowledge."
    }, {
      "heading" : "3.1 Kneser-Ney Smoothing with a structural distribution",
      "text" : "We now have all the components needed to explain our Structural Kneser-Ney (SKN) framework. Given an arbitrary graphlet gj of size k + 1, we estimate the probability of observing that graphlet as follows:\nPSKN (gj) = max(cj − d, 0)∑\ngj′∈Gk+1 cj′\n+ d∑\ngj′∈Gk+1 cj′∑\ngj′∈Gk+1\n|{gj′ : cj′ > d}| ∑\ni∈Pgj\nP0(gi) wij∑\ngj′∈Cgi wij′\n(10)\nAs can be seen from the equation, we first discount the count of all graphlets by d, and then redistribute this mass to all other graphlets. The amount of mass a graphlet receives is controlled by the base distribution. In order to automatically tune the discount parameter d, we use the Pitman-Yor process (a Bayesian approximation of Kneser-Ney) in the next section."
    }, {
      "heading" : "4. PITMAN-YOR PROCESS",
      "text" : "We will only give a very high level overview of a Pitman-Yor process and refer the reader to the excellent papers by Teh [35] and [12] for more details. A Pitman-Yor process P on a ground set Gk+1 of size-(k + 1) graphlets is defined via\nPk+1 ∼ PY (dk+1, θk+1, Pk), (11)\nwhere dk+1 is a discount parameter 0 ≤ dk+1 < 1, θ > −dk+1 is a strength parameter, and Pk is a base distribution. The most intuitive way to understand draws from the Pitman-Yor process is via the Chinese restaurant process (also see Figure 6). Consider a restaurant with an infinite number of tables. Customers enter the restaurant one by one. The first customer sits at the first table, and is seated at the first table. Since this table is occupied for the first time, a graphlet is assigned to it by drawing a sample from the base distribution. The label of the first table is the first graphlet drawn from the Pitman-Yor process. Subsequent customers when they enter the restaurant decide to sit at an already occupied table with probability proportional to ci − dk+1, where ci represents the number of customers already sitting at table i. If they sit at an already occupied table, then the label of that table denotes the next graphlet drawn from the Pitman-Yor process. On the other hand, with probability θk+1 + dk+1t, where t is the current number of occupied tables, a new customer might decide to occupy a new table. In this case, the base distribution is invoked to label this table with a graphlet. Intuitively the reason this process generates power-law behavior is because popular graphlets which are served on tables with a large number of customers have a higher probability of attracting new customers and hence being generated again. This self reinforcing property produces power law behavior.\nIn a hierarchical Pitman-Yor process, the base distribution Pk is recursively defined via a Pitman-Yor processPk ∼ PY (dk, θk, Pk−1). In order to label a table, we need a draw from Pk, which is obtained by inserting a customer into the corresponding restaurant.\nIn our case Pk+1 is defined over Gk+1 of size nk+1 while Pk is defined over Gk of size nk ≤ nk+1. Therefore, like we did\nin the case of Kneser-Ney smoothing we will use the DAG and Equation (8) to define a base distribution. This changes the Chinese Restaurant process as follows: When we need to label a table, we will first draw a size-k graphlet gi ∼ Pk by inserting a customer into the corresponding restaurant. Given gi, we will draw a size(k+1) graphlet gj proportional towij , wherewij is obtained from the DAG. Deletion of a customer is handled similarly. Detailed pseudo-code can be found in Algorithms 1 and 2.\nAlgorithm 1 Insert a Customer Input: dk+1, θk+1, Pk t← 0 // Occupied tables c← () // Counts of customers l← () // Labels of tables if t = 0 then t← 1 append 1 to c draw graphlet gi ∼ Pk // Insert customer in parent draw gj ∼ wij append gj to l return gj\nelse with probability ∝ max(0, cj − d) cj ← cj + 1 return lj with probability proportional to θ + dt t← t+ 1 append 1 to c draw graphlet gi ∼ Pk // Insert customer in parent draw gj ∼ wij append gj to l return gj end if\nAlgorithm 2 Delete a Customer Input: d, θ, P0, C, L, t\nwith probability ∝ cl cl ← cl − 1 gj ← lj if cl = 0 then Pk ∝ 1/wij delete cl from c delete lj from l t← t− 1 end if return g"
    }, {
      "heading" : "5. RELATED WORK",
      "text" : "The problem of estimating multinomial distributions is a classic problem. In natural language processing they occur in the following context: suppose we are given a sequence of wordsw1, . . . , wk and one is interested in asking what is the probability of observing word w next. Estimating this probability lies at the heart of language models, and many sophisticated smoothing techniques have been proposed. This is a classic multinomial estimation problem that suffers from sparsity since the event space is unbounded. Moreover, natural language exhibits power law behavior since the distribution tends to be dominated by a small number of frequently occurring words. In extensive empirical evaluation it has been found the Kneser-Ney smoothing is very effective for language\nmodels [7], [24]. Here, the base distribution is constructed using smaller context of k−1 words which naturally leads to a denser distribution. Even though language models and graphlets have some similarities, there is a significant fundamental difference between the two. In language models, one can derive the base distribution using a smaller context. However, in the case of graphlets there is no equivalent concept of a fallback model. Therefore, we need to derive the base distribution by using smaller size graphlets. However, this leads to a problem since the distribution is now defined on a smaller space. Therefore, we need to apply a transformation by using the DAG in order to convert the distribution back into to the original space. Goldwater et al. [13] and Teh [35] independently showed that Kneser-Ney can be explained in a Bayesian setting by using the Pitman-Yor Process (PYP) [27]. In the Bayesian interpretation, a hierarchical PYP where the Pitman-Yor prior comes from another PYP is used. Similar to Kneser-Ney, this interpretation is not directly applicable to our model since the previous PYP has a different space, thus we need to apply a transformation.\nGraph kernels can be considered as special cases of convolutional kernels proposed by Haussler [15]. In general, graph kernels can be categorized into three classes: graph kernels based on walks and paths [11], [18],[5], graph kernels based on limited-size subgraphs [16], [31], [34] and graph kernels based on subtree patterns [29]. Shervashidze et al. [33] performs a relaxation on the vertices and exploit labeling information embedded in the graphs to derive their so-called Weisfeiler-Lehman kernels. However, their kernel is applicable only to labeled graphs. The sparsity problem of graphlet kernels has been addressed before. Hash kernels proposed by Shi et al. [34] addresses the sparsity problem by applying a sparse projection into a lower dimensional space. The idea here is that many higher order graphlets will “collide” and therefore be mapped to the same lower dimensional representation, thus avoiding the diagonal dominance problem. Unfortunately, we find that in our experiments the hash kernel is very sensitive to the hash value used for embedding and rarely performed well as compared to the MLE estimate."
    }, {
      "heading" : "6. EXPERIMENTS",
      "text" : "To compare the efficacy of our approach, we compare our KneserNey and Pitman-Yor smoothed kernels with state-of-the-art graph kernels namel the graphlet kernel [32], the hash kernel [34], the random walk kernel [11], [18], [38], and the shortest path kernel [5]. For random walk kernel, we uniformly set the decay factor λ = 10−4, for shortest path we used the delta kernel to compare the shortest-path distances, and for the hash kernel we used a prime number of 11291. We adopted Markov chain Monte Carlo sampling based inference scheme for the hierarchical Pitman-Yor language model from [35] and modified the open source implementation of HPYP from https://github.com/redpony/cpyp.\nDue to lack of space we will only present a subset of our experimental results. Full results including the source code and experimental scripts will be made available for download from http: //cs.purdue.edu/~ypinar/kdd.\nDatasets In order to test the efficacy of our model, we applied smoothing to real-world benchmark datasets, namely MUTAG, PTC, NCI1, NCI109, ENZYMES and DD. MUTAG [9] is a binary data set of 188 mutagenic aromatic and heteroaromatic nitro compounds, labeled whether they have mutagenicity in Salmonella typhimurium. The Predictive Toxicology Challenge (PTC) [36] dataset is a chemical compound dataset that reports the carcinogenicity for male and female rats. NCI1 and NCI109 [40], (http://pubchem.ncbi.nlm.nih.gov) datasets, made publicly available by the National Cancer Institute (NCI), are two subsets of balanced data sets of chemical compounds screened for ability to suppress or inhibit the growth of a panel of human tumor cell lines. Enzymes is a data set of protein tertiary structures obtained from [4]. DD [10] is a data set of protein structures where each protein is represented by a graph and nodes are amino acids that are connected by an edge if they are less than 6 Angstroms apart. Table 2 shows summary statistics for these datasets. Note that we did not use edge or node labels in our experiments.\nExperimental Setting All data sets we work with consist of sparse graphs. However, counting all graphlets of size k for a graph with n nodes requires O(nk) effort which is intractable even for moderate values of k. Therefore, we use random sampling, as advocated by [32], in order to obtain an empirical distribution of graphlet counts that is close to the actual distribution of graphlets in the graph. For each value of for each k ∈ {2, . . . , 8} we randomly sampled 10,000 sub-graphs, and used Nauty [25] to get canonically-labeled isomorphic representations which are then used to construct the frequency representation.\nWe performed 5-fold cross-validation with C-Support Vector Machine Classification using LibSVM [6], using 4 folds for training and 1 for testing. We used a linear kernel. Since we are interested in understanding the difference in performance between smoothed and un-smoothed kernels we did not tune the value ofC; it was simply set to 1. In order to tune the discount parameter for Kneser-Ney based smoothed kernel, we tried different parameters vary from 0.01 to 10,000 and report results for the best one.\nEffect of discounting parameter on performance First, we investigate the effect of the discounting parameter on the classification performance. Since the trends are similar across different datasets, we pick PTC as a representative dataset to report results. Figure 8 shows the classification accuracy on the PTC dataset with different discounting parameters for Kneser-Ney smoothing. As expected, applying very large discounts decreases the performance because the distribution (10) degrades to the base distribution. On the other hand, applying a very small discount also decreases the accuracy since the distribution degrades to the MLE estimate. From our experiments, we observe that the best performance in all datasets is achieved by using an intermediate discount value between these two extremes. However, the specific discount value is data dependent.\nEffect of graphlet size on performance Next we investigate how the value of k affects performance. Again, we show results for a representative dataset namely PTC. Figure 7 shows the classification accuracy on the PTC dataset as a function of graphlet size k for MLE (the graphlet kernel), Pitman-Yor smoothed kernel and Kneser-Ney smoothed kernel. From the figure, we can see that small graphlet sizes such as k = 2, 3 do not perform well and are not informative since the number of unique graphlets is very small (see Table 1). On the other hand, MLE does\nnot perform well for large graphlet sizes such as k = 7, 8 because of the diagonal dominance problem. On the other hand, smoothed kernels in general obtain a balance between these two extreme situations and tend to yield better performance. Pitman-Yor smoothed kernel tends to achieve a better performance than MLE, but doesn’t perform as good as Kneser-Ney. This is expected since we didn’t tune the hyperparameters for Pitman-Yor process. Teh [35] shows that Pitman-Yor yields a better performance if one tune the hyperparameters. Therefore, our Pitman-Yor kernel is open to improvement.\nComparison with related work We compare the proposed smoothed kernel with graphlet kernel and hash kernel on the benchmark data sets in Table 1. The results for the Shortest Path and Random Walk graph kernels are included mainly to show what is the state-of-the-art using other representations. We fixed the k = 5 which is observed to be the best value for the MLE based graphlet kernel on most datasets. We randomly sample 10,000 graphlets from each graph and feed the same frequency vectors to graphlet kernel (GK), hash kernel (HK), Kneser-Ney smoothed kernel (KN), and Pitman-Yor smoothed kernel (PYP). Therefore, the differences in performance that we observe are solely due to the transformation of the frequency vectors that these kernels perform. We performed an unpaired t-test and use bold numbers to indicate that the results were statistically significant at p < 0.0001.\nWe can see that KN kernel outperforms MLE and hash kernels on all of the benchmark data sets. The accuracy of the PYP kernel is usually lower than that of the KN kernel. We conjecture that this is because the Pitman-Yor process is sensitive to the hyperparameters and we do not carefully tune the hyper-parameters in our experiments. On PTC, DD and Enzymes datasets, the KN kernel reached the highest accuracy. On MUTAG, NCI1 and NCI109 datasets, KN kernels also yield good results and got comparable classification accuracies to shortest path and random walk kernels. For the DD dataset, shortest path and random walk kernels were not able to finish in 24 hours, due to the fact that this dataset has a large maximum degree.\nTo summarize, smoothed kernels turns out to be competitive in terms of classification accuracy on all datasets and are also applicable to very large graphs.\nEffect of exhaustive sampling on performance Next, we investigate whether any of the difference in performance can be attributed to sampling a small number of graphlets. In other words, we ask do the results summarily change if we performed exhaustive sampling instead of using 10,000 samples. We give MLE an unfair advantage by performing exhaustive sampling on MUTAG, PTC, NCI and NCI109 datasets for k = 5 by using a distributed memory implementation. Table 4 shows mean, median and standard deviations of number of samples in bruteforce sampled datasets for k = 5. Here, we can see that the original frequencies of the graphlets are quite high in most of the datasets. Even though our algorithm only uses 10,000 samples, it outperforms the graphlet kernel with exhaustive sampling on MUTAG, PTC and NCI1 and achieves competitive performance on NCI109 dataset. Results are summarized in Table 3.\nEven though distribution with a small number of samples is close to the original distribution in the L1 sense, bruteforce sampling reveals that the true underlying distribution of the datasets contains a larger number of unique graphlets comparing to random sampling. Since the graphlet kernel uses a MLE estimate its performance degrades. On the other hand, our smoothing technique uses structural\ninformation to redistribute the mass and hence is able to outperform MLE even with a small number of samples."
    }, {
      "heading" : "7. DISCUSSION",
      "text" : "We presented a novel framework for smoothing normalized graphletfrequency vectors inspired by smoothing techniques from natural language processing. Although our models are inspired by work done in language models, they are fundamentally different in the way they define a fallback base distribution. We believe that our framework has applicability beyond graph kernels, and can be used in any structural setting where one can naturally define a relationship such as the DAG that we defined in Figure 4. We are currently investigating the applicability of our framework to string kernels [22, 23, 37] and tree kernels [8]. It is also interesting to investigate if our method can be extended to other graph kernels such as random walk kernels. Our framework is also applicable to nodelabeled graphs since they also suffer from similar sparsity issues. We leave the application of our framework to labelled graphs to an extended version of this paper. We are also investigating better strategies for tuning the hyper-parameters of the Pitman-Yor kernels."
    } ],
    "references" : [ {
      "title" : "Gephi: an open source software for exploring and manipulating networks",
      "author" : [ "Mathieu Bastian", "Sebastien Heymann", "Mathieu Jacomy" ],
      "venue" : "In ICWSM,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2009
    }, {
      "title" : "editors",
      "author" : [ "D. Bonchev", "D.H. Rouvray" ],
      "venue" : "Chemical Graph Theory: Introduction and Fundamentals, volume 1. Gordon and Breach Science Publishers, London, UK",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Protein function prediction via graph kernels",
      "author" : [ "K.M. Borgwardt", "C.S. Ong", "S. Schönauer", "S.V.N. Vishwanathan", "A.J. Smola", "H.-P. Kriegel" ],
      "venue" : "Proceedings of Intelligent Systems in Molecular Biology (ISMB), Detroit, USA",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Protein function prediction via graph kernels",
      "author" : [ "K.M. Borgwardt", "C.S. Ong", "S. Schonauer", "S.V.N. Vishwanathan", "A.J. Smola", "H.P. Kriegel" ],
      "venue" : "Bioinformatics (ISMB),",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2005
    }, {
      "title" : "Shortest-path kernels on graphs",
      "author" : [ "Karsten M. Borgwardt", "Hans-Peter Kriegel" ],
      "venue" : "In Proc. Intl. Conf. Data Mining,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2005
    }, {
      "title" : "LIBSVM: a library for support vector machines",
      "author" : [ "C.C. Chang", "C.J. Lin" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2001
    }, {
      "title" : "An empirical study of smoothing techniques for language modeling",
      "author" : [ "Stanley F Chen", "Joshua Goodman" ],
      "venue" : "In Proceedings  of the 34th annual meeting on Association for Computational Linguistics,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1996
    }, {
      "title" : "Convolution kernels for natural language",
      "author" : [ "M. Collins", "N. Duffy" ],
      "venue" : "T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14, pages 625–632, Cambridge, MA",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "R",
      "author" : [ "A.K. Debnath" ],
      "venue" : "L. Lopez de Compadre, G. Debnath, A. J. Shusterman, and C. Hansch. Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity. J Med Chem, 34:786–797",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Distinguishing enzyme structures from non-enzymes without alignments",
      "author" : [ "P.D. Dobson", "A.J. Doig" ],
      "venue" : "J Mol Biol,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2003
    }, {
      "title" : "On graph kernels: Hardness results and efficient alternatives",
      "author" : [ "T. Gärtner", "P.A. Flach", "S. Wrobel" ],
      "venue" : "B. Schölkopf and M. K. Warmuth, editors, Proc. Annual Conf. Computational Learning Theory, pages 129–143. Springer",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Interpolating between types and tokens by estimating power-law generators",
      "author" : [ "Sharon Goldwater", "Tom Griffiths", "Mark Johnson" ],
      "venue" : "Advances in neural information processing systems,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2006
    }, {
      "title" : "Producing power-law distributions and damping word frequencies with two-stage language models",
      "author" : [ "Sharon Goldwater", "Thomas L Griffiths", "Mark Johnson" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2011
    }, {
      "title" : "Exploring network structure, dynamics, and function using networkx",
      "author" : [ "Aric Hagberg", "Pieter Swart", "Daniel S Chult" ],
      "venue" : "Technical report, Los Alamos National Laboratory (LANL),",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2008
    }, {
      "title" : "Convolution kernels on discrete structures",
      "author" : [ "David Haussler" ],
      "venue" : "Technical Report UCS-CRL-99-10, UC Santa Cruz,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1999
    }, {
      "title" : "Cyclic pattern kernels for predictive graph mining",
      "author" : [ "T. Horvath", "T. Gärtner", "S. Wrobel" ],
      "venue" : "Proceedings of the International Conference on Knowledge Discovery and Data Mining (KDD), pages 158–167",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Reducing kernel matrix diagonal dominance using semi-definite programming",
      "author" : [ "Jaz Kandola", "Thore Graepel", "John Shawe-Taylor" ],
      "venue" : "In Proc. Annual Conf. Computational Learning Theory,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2003
    }, {
      "title" : "Kernels for graphs",
      "author" : [ "H. Kashima", "K. Tsuda", "A. Inokuchi" ],
      "venue" : "K. Tsuda, B. Schölkopf, and J.P. Vert, editors, Kernels and Bioinformatics, pages 155–170, Cambridge, MA",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Authoritative sources in a hyperlinked environment",
      "author" : [ "J. Kleinberg" ],
      "venue" : "Journal of the ACM,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1999
    }, {
      "title" : "Improved backing-off for M-gram language modeling",
      "author" : [ "R. Kneser", "H. Ney" ],
      "venue" : "In Proc. ICASSP",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1995
    }, {
      "title" : "The skew spectrum of graphs",
      "author" : [ "Risi Kondor", "Karsten Borgwardt" ],
      "venue" : "In Proceedings of the International Conference on Machine Learning,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2008
    }, {
      "title" : "Fast kernels for inexact string matching",
      "author" : [ "C. Leslie", "R. Kuang" ],
      "venue" : "Proc. Annual Conf. Computational Learning Theory",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "The spectrum kernel: A string kernel for SVM protein classification",
      "author" : [ "C. Leslie", "E. Eskin", "W.S. Noble" ],
      "venue" : "Proceedings of the Pacific Symposium on Biocomputing, pages 564–575, Singapore",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Introduction to Information Retrieval",
      "author" : [ "C.D. Manning", "P. Raghavan", "H. Schütze" ],
      "venue" : "Cambridge University Press",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Nauty userâĂŹs guide (version 2.4)",
      "author" : [ "Brendan D McKay" ],
      "venue" : "Computer Science Dept., Australian National University,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2007
    }, {
      "title" : "The pagerank citation ranking: Bringing order to the web",
      "author" : [ "L. Page", "S. Brin", "R. Motwani", "T. Winograd" ],
      "venue" : "Technical report,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 1998
    }, {
      "title" : "The two-parameter poisson-dirichlet distribution derived from a stable subordinator",
      "author" : [ "J. Pitman", "M. Yor" ],
      "venue" : "Annals of Probability, 25(2):855–900",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Biological network comparison using graphlet degree distribution",
      "author" : [ "N. Przulj" ],
      "venue" : "European Conference on Computational Biology (ECCB),",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2006
    }, {
      "title" : "Expressivity versus efficiency of graph kernels",
      "author" : [ "J. Ramon", "T. Gärtner" ],
      "venue" : "Technical report, First International Workshop on Mining Graphs, Trees and Sequences (held with ECML/PKDD’03)",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Modeling cellular machinery through biological network comparison",
      "author" : [ "R. Sharan", "T. Ideker" ],
      "venue" : "Nature Biotechnology,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2006
    }, {
      "title" : "Fast subtree kernels on graphs",
      "author" : [ "Nino Shervashidze", "Karsten Borgwardt" ],
      "venue" : "In Neural Information Processing Systems,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2010
    }, {
      "title" : "Efficient graphlet kernels for large graph comparison",
      "author" : [ "Nino Shervashidze", "S.V.N. Vishwanathan", "Tobias Petri", "Kurt Mehlhorn", "Karsten Borgwardt" ],
      "venue" : "Proc. Intl. Conference on Artificial Intelligence and Statistics. Society for Artificial Intelligence and Statistics,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2009
    }, {
      "title" : "Weisfeiler-lehman graph kernels",
      "author" : [ "Nino Shervashidze", "Pascal Schweitzer", "Erik Jan Van Leeuwen", "Kurt Mehlhorn", "Karsten M Borgwardt" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2011
    }, {
      "title" : "Hash kernels",
      "author" : [ "Q. Shi", "J. Petterson", "G. Dror", "J. Langford", "A.J. Smola", "A. Strehl", "S.V.N. Vishwanathan" ],
      "venue" : "M. Welling and D. van Dyk, editors, Proc. Intl. Conference on Artificial Intelligence and Statistics. Society for Artificial Intelligence and Statistics",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A hierarchical bayesian language model based on pitman-yor processes",
      "author" : [ "Yee Whye Teh" ],
      "venue" : "In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2006
    }, {
      "title" : "Statistical evaluation of the predictive toxicology challenge 2000-2001",
      "author" : [ "H. Toivonen", "A. Srinivasan", "R.D. King", "S. Kramer", "C. Helma" ],
      "venue" : null,
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2003
    }, {
      "title" : "Fast kernels for string and tree matching",
      "author" : [ "S.V.N. Vishwanathan", "A.J. Smola" ],
      "venue" : "S. Becker, S. Thrun, and K. Obermayer, editors, Advances in Neural Information Processing Systems 15, pages 569–576. MIT Press, Cambridge, MA",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Karsten Borgwardt",
      "author" : [ "S.V.N. Vishwanathan" ],
      "venue" : "and Nicol N. Schraudolph. Fast computation of graph kernels. In B. Schölkopf, J. Platt, and T. Hofmann, editors, Advances in Neural Information Processing Systems 19, Cambridge MA",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Nicol N",
      "author" : [ "S.V.N. Vishwanathan" ],
      "venue" : "Schraudolph, Imre Risi Kondor, and Karsten M. Borgwardt. Graph kernels. Journal of Machine Learning Research",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Comparison of descriptor spaces for chemical compound retrieval and classification",
      "author" : [ "Nikil Wale", "Ian A Watson", "George Karypis" ],
      "venue" : "Knowledge and Information Systems,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2008
    }, {
      "title" : "State of the art of graphbased data mining",
      "author" : [ "Takashi Washio", "Hiroshi Motoda" ],
      "venue" : "SIGKDD Explorations,",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2003
    }, {
      "title" : "A study of smoothing methods for language models applied to information retrieval",
      "author" : [ "C. Zhai", "J. Lafferty" ],
      "venue" : "ACM Trans. Inf. Syst., 22(2):179–214",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 31,
      "context" : "[32] uses induced sub-graphs of k nodes (christened as graphlets by Przulj [28]) as motifs in the vector representation, and computes the kernel via a dot product between these vectors.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 27,
      "context" : "[32] uses induced sub-graphs of k nodes (christened as graphlets by Przulj [28]) as motifs in the vector representation, and computes the kernel via a dot product between these vectors.",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 38,
      "context" : "In this paper, we are interested in comparing graphs by computing a kernel between graphs [39].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 2,
      "context" : "Graph kernels are popular because many datasets from diverse domains such as bio-informatics [3, 30], chemo-informatics [2], and web data mining [41] naturally can be represented as graphs.",
      "startOffset" : 93,
      "endOffset" : 100
    }, {
      "referenceID" : 29,
      "context" : "Graph kernels are popular because many datasets from diverse domains such as bio-informatics [3, 30], chemo-informatics [2], and web data mining [41] naturally can be represented as graphs.",
      "startOffset" : 93,
      "endOffset" : 100
    }, {
      "referenceID" : 1,
      "context" : "Graph kernels are popular because many datasets from diverse domains such as bio-informatics [3, 30], chemo-informatics [2], and web data mining [41] naturally can be represented as graphs.",
      "startOffset" : 120,
      "endOffset" : 123
    }, {
      "referenceID" : 40,
      "context" : "Graph kernels are popular because many datasets from diverse domains such as bio-informatics [3, 30], chemo-informatics [2], and web data mining [41] naturally can be represented as graphs.",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 20,
      "context" : "The kernels proposed by [21] are a notable exception.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 2,
      "context" : "[3] (see Vishwanathan et al.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 38,
      "context" : "[39] for an efficient algorithm for computing this kernel).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 30,
      "context" : "Other popular motifs include subtrees [31], shortest paths [5], and cycles [16].",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 4,
      "context" : "Other popular motifs include subtrees [31], shortest paths [5], and cycles [16].",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 15,
      "context" : "Other popular motifs include subtrees [31], shortest paths [5], and cycles [16].",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 31,
      "context" : "[32].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 27,
      "context" : "The motif used in this kernel is the set of unique sub-graphs of size k, which were christened as graphlets by Przulj [28].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 16,
      "context" : "This is well known as the diagonal dominance problem in the machine learning community [17], and the resulting kernel matrix is close to the identity matrix.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 19,
      "context" : "First, we propose a new smoothing technique for graphlets which is inspired by Kneser-Ney smoothing [20] used for language models in natural language processing.",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 34,
      "context" : "Second, we provide a novel Bayesian version of our model that is extended from the Hierarchical Pitman-Yor process of Teh [35].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 27,
      "context" : "They were introduced by Przulj [28] to design a new measure of local structural similarity between biological networks.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 13,
      "context" : "Plots are generated with NetworkX library [14].",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 41,
      "context" : "This issue occurs in a number of different domains and therefore, unsurprisingly, has received significant research attention [42]; smoothing methods are typically used to address this problem.",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 6,
      "context" : "Kneser-Ney smoothing is a fallback based smoothing method which has been identified as the state-of-the-art smoothing in natural language processing by several studies [7].",
      "startOffset" : 168,
      "endOffset" : 171
    }, {
      "referenceID" : 24,
      "context" : "Therefore, we first exhaustively generate all possible graphs of size k (this involves a one timeO(2) effort), and use Nauty [25] to obtain their canonically-labelled isomorphic representations.",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 0,
      "context" : "Image is generated by Gephi [1].",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 25,
      "context" : "For example, in the case of structured graphs such as social networks, one might benefit from weighting the edges according to the PageRank [26] score of the nodes.",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 18,
      "context" : "Similarly, other link analysis algorithms such as Hubs or Authority given by HITS algorithm [19] can be used in order to exploit the domain knowledge.",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 34,
      "context" : "We will only give a very high level overview of a Pitman-Yor process and refer the reader to the excellent papers by Teh [35] and [12] for more details.",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 11,
      "context" : "We will only give a very high level overview of a Pitman-Yor process and refer the reader to the excellent papers by Teh [35] and [12] for more details.",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 12,
      "context" : "[13].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 6,
      "context" : "models [7], [24].",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 23,
      "context" : "models [7], [24].",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 12,
      "context" : "[13] and Teh [35] independently showed that Kneser-Ney can be explained in a Bayesian setting by using the Pitman-Yor Process (PYP) [27].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 34,
      "context" : "[13] and Teh [35] independently showed that Kneser-Ney can be explained in a Bayesian setting by using the Pitman-Yor Process (PYP) [27].",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 26,
      "context" : "[13] and Teh [35] independently showed that Kneser-Ney can be explained in a Bayesian setting by using the Pitman-Yor Process (PYP) [27].",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 14,
      "context" : "Graph kernels can be considered as special cases of convolutional kernels proposed by Haussler [15].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 10,
      "context" : "In general, graph kernels can be categorized into three classes: graph kernels based on walks and paths [11], [18],[5], graph kernels based on limited-size subgraphs [16], [31], [34] and graph kernels based on subtree patterns [29].",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 17,
      "context" : "In general, graph kernels can be categorized into three classes: graph kernels based on walks and paths [11], [18],[5], graph kernels based on limited-size subgraphs [16], [31], [34] and graph kernels based on subtree patterns [29].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 4,
      "context" : "In general, graph kernels can be categorized into three classes: graph kernels based on walks and paths [11], [18],[5], graph kernels based on limited-size subgraphs [16], [31], [34] and graph kernels based on subtree patterns [29].",
      "startOffset" : 115,
      "endOffset" : 118
    }, {
      "referenceID" : 15,
      "context" : "In general, graph kernels can be categorized into three classes: graph kernels based on walks and paths [11], [18],[5], graph kernels based on limited-size subgraphs [16], [31], [34] and graph kernels based on subtree patterns [29].",
      "startOffset" : 166,
      "endOffset" : 170
    }, {
      "referenceID" : 30,
      "context" : "In general, graph kernels can be categorized into three classes: graph kernels based on walks and paths [11], [18],[5], graph kernels based on limited-size subgraphs [16], [31], [34] and graph kernels based on subtree patterns [29].",
      "startOffset" : 172,
      "endOffset" : 176
    }, {
      "referenceID" : 33,
      "context" : "In general, graph kernels can be categorized into three classes: graph kernels based on walks and paths [11], [18],[5], graph kernels based on limited-size subgraphs [16], [31], [34] and graph kernels based on subtree patterns [29].",
      "startOffset" : 178,
      "endOffset" : 182
    }, {
      "referenceID" : 28,
      "context" : "In general, graph kernels can be categorized into three classes: graph kernels based on walks and paths [11], [18],[5], graph kernels based on limited-size subgraphs [16], [31], [34] and graph kernels based on subtree patterns [29].",
      "startOffset" : 227,
      "endOffset" : 231
    }, {
      "referenceID" : 32,
      "context" : "[33] performs a relaxation on the vertices and exploit labeling information embedded in the graphs to derive their so-called Weisfeiler-Lehman kernels.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 33,
      "context" : "[34] addresses the sparsity problem by applying a sparse projection into a lower dimensional space.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 31,
      "context" : "To compare the efficacy of our approach, we compare our KneserNey and Pitman-Yor smoothed kernels with state-of-the-art graph kernels namel the graphlet kernel [32], the hash kernel [34], the random walk kernel [11], [18], [38], and the shortest path kernel [5].",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 33,
      "context" : "To compare the efficacy of our approach, we compare our KneserNey and Pitman-Yor smoothed kernels with state-of-the-art graph kernels namel the graphlet kernel [32], the hash kernel [34], the random walk kernel [11], [18], [38], and the shortest path kernel [5].",
      "startOffset" : 182,
      "endOffset" : 186
    }, {
      "referenceID" : 10,
      "context" : "To compare the efficacy of our approach, we compare our KneserNey and Pitman-Yor smoothed kernels with state-of-the-art graph kernels namel the graphlet kernel [32], the hash kernel [34], the random walk kernel [11], [18], [38], and the shortest path kernel [5].",
      "startOffset" : 211,
      "endOffset" : 215
    }, {
      "referenceID" : 17,
      "context" : "To compare the efficacy of our approach, we compare our KneserNey and Pitman-Yor smoothed kernels with state-of-the-art graph kernels namel the graphlet kernel [32], the hash kernel [34], the random walk kernel [11], [18], [38], and the shortest path kernel [5].",
      "startOffset" : 217,
      "endOffset" : 221
    }, {
      "referenceID" : 37,
      "context" : "To compare the efficacy of our approach, we compare our KneserNey and Pitman-Yor smoothed kernels with state-of-the-art graph kernels namel the graphlet kernel [32], the hash kernel [34], the random walk kernel [11], [18], [38], and the shortest path kernel [5].",
      "startOffset" : 223,
      "endOffset" : 227
    }, {
      "referenceID" : 4,
      "context" : "To compare the efficacy of our approach, we compare our KneserNey and Pitman-Yor smoothed kernels with state-of-the-art graph kernels namel the graphlet kernel [32], the hash kernel [34], the random walk kernel [11], [18], [38], and the shortest path kernel [5].",
      "startOffset" : 258,
      "endOffset" : 261
    }, {
      "referenceID" : 34,
      "context" : "We adopted Markov chain Monte Carlo sampling based inference scheme for the hierarchical Pitman-Yor language model from [35] and modified the open source implementation of HPYP from https://github.",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 8,
      "context" : "MUTAG [9] is a binary data set of 188 mutagenic aromatic and heteroaromatic nitro compounds, labeled whether they have mutagenicity in Salmonella typhimurium.",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 35,
      "context" : "The Predictive Toxicology Challenge (PTC) [36] dataset is a chemical compound dataset that reports the carcinogenicity for male and female rats.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 39,
      "context" : "NCI1 and NCI109 [40], (http://pubchem.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 3,
      "context" : "Enzymes is a data set of protein tertiary structures obtained from [4].",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 9,
      "context" : "DD [10] is a data set of protein structures where each protein is represented by a graph and nodes are amino acids that are connected by an edge if they are less than 6 Angstroms apart.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 31,
      "context" : "Therefore, we use random sampling, as advocated by [32], in order to obtain an empirical distribution of graphlet counts that is close to the actual distribution of graphlets in the graph.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 24,
      "context" : ", 8} we randomly sampled 10,000 sub-graphs, and used Nauty [25] to get canonically-labeled isomorphic representations which are then used to construct the frequency representation.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 5,
      "context" : "We performed 5-fold cross-validation with C-Support Vector Machine Classification using LibSVM [6], using 4 folds for training and 1 for testing.",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 34,
      "context" : "Teh [35] shows that Pitman-Yor yields a better performance if one tune the hyperparameters.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 21,
      "context" : "We are currently investigating the applicability of our framework to string kernels [22, 23, 37] and tree kernels [8].",
      "startOffset" : 84,
      "endOffset" : 96
    }, {
      "referenceID" : 22,
      "context" : "We are currently investigating the applicability of our framework to string kernels [22, 23, 37] and tree kernels [8].",
      "startOffset" : 84,
      "endOffset" : 96
    }, {
      "referenceID" : 36,
      "context" : "We are currently investigating the applicability of our framework to string kernels [22, 23, 37] and tree kernels [8].",
      "startOffset" : 84,
      "endOffset" : 96
    }, {
      "referenceID" : 7,
      "context" : "We are currently investigating the applicability of our framework to string kernels [22, 23, 37] and tree kernels [8].",
      "startOffset" : 114,
      "endOffset" : 117
    } ],
    "year" : 2014,
    "abstractText" : "A commonly used paradigm for representing graphs is to use a vector that contains normalized frequencies of occurrence of certain motifs or sub-graphs. This vector representation can be used in a variety of applications, such as, for computing similarity between graphs. The graphlet kernel of Shervashidze et al. [32] uses induced sub-graphs of k nodes (christened as graphlets by Przulj [28]) as motifs in the vector representation, and computes the kernel via a dot product between these vectors. One can easily show that this is a valid kernel between graphs. However, such a vector representation suffers from a few drawbacks. As k becomes larger we encounter the sparsity problem; most higher order graphlets will not occur in a given graph. This leads to diagonal dominance, that is, a given graph is similar to itself but not to any other graph in the dataset. On the other hand, since lower order graphlets tend to be more numerous, using lower values of k does not provide enough discrimination ability. We propose a smoothing technique to tackle the above problems. Our method is based on a novel extension of Kneser-Ney and Pitman-Yor smoothing techniques from natural language processing to graphs. We use the relationships between lower order and higher order graphlets in order to derive our method. Consequently, our smoothing algorithm not only respects the dependency between sub-graphs but also tackles the diagonal dominance problem by distributing the probability mass across graphlets. In our experiments, the smoothed graphlet kernel outperforms graph kernels based on raw frequency counts.",
    "creator" : "LaTeX with hyperref package"
  }
}