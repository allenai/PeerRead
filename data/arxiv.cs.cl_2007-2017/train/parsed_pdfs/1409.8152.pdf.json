{
  "name" : "1409.8152.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Controversy and Sentiment in Online News",
    "authors" : [ "Yelena Mejova", "Amy X. Zhang", "Nicholas Diakopoulos", "Carlos Castillo" ],
    "emails" : [ "ymejova@qf.org.qa,", "axz@mit.edu,", "nicholas.diakopoulos@gmail.com,", "chato@acm.org" ],
    "sections" : [ {
      "heading" : "1. INTRODUCTION",
      "text" : "One of the most vital functions of the news media is to serve as a place to critically examine and present information about social, political, economic, and ideological issues of the day. Many of these issues are controversial, in the sense that they provoke arguments in which people express strong opposing opinions [10].\nFor this reason, journalists must often take special precaution and make careful language choices when they write about controversial issues. This can often manifest as different ways of using language, for instance news sources will often use a series of terms to signal that a controversy exists, such as “outcry,” “furor,” and “uproar” [6]. It has been theorized that journalists can also become susceptible to the ideologies, attitudes, and pressures of their organization [13], as well as unstated rules and norms [8]. These inputs can influence the particular language used to discuss controversial issues within a particular news source. The difference in framing could be subtle enough to be unnoticeable to the casual reader. However, using computational techniques in textual analysis, we can analyze large datasets of articles for consistent differences in the way different news organizations write about controversial issues.\nComputation+Journalism Symposium 2014. October 24-25, 2014. Columbia University, New York, NY, USA.\nOur contribution. In this work, we quantify the use of emotional and biased language when presenting controversial issues in the news. We begin by building a list of controversial and noncontroversial terms in current news in the U.S. using crowdsourcing techniques. Then, we perform a large-scale analysis of millions of news articles from 15 U.S.-based news sources. We focus on the expression of sentiment using a series of lexical resources containing words conveying positive and negative emotions; this automatic analysis helps reduce the inherent subjectivity of traditional content analysis methods.\nWe demonstrate that controversial issues in news can be characterized by the use of fewer positive words and a greater presence of negative words. This finding is consistent across different media sources and confirmed with 4 different sentiment lexicons. Interestingly, we find that the use of highly emotional terms (as opposed to mild ones) is less likely in the context of controversial topics, suggesting a self-moderation on the part of the news sources.\nAdditionally, we perform an analysis based on a vocabulary of words signaling bias obtained from discussions in Wikipedia, and find that these bias terms also tend to occur more frequently in articles mentioning controversial topics, and can serve as a fairly accurate predictor of the level of controversy.\nThe next section outlines previous work related to controversy in news media. Next, we describe our dataset of online news (Section 3) and describe the process we used to label strongly controversial, somewhat controversial, and non-controversial words (Section 4). We then compare controversial and non-controversial articles in terms of a series of bias and sentiment lexicons in Section 5, and discuss the differences in the strength with which annotators perceive a topic as controversial and the treatment it received in news media in Section 6. Lastly we discuss the implications and limitations of such computational approaches to media analysis."
    }, {
      "heading" : "2. PREVIOUS WORK",
      "text" : "Controversy has been examined in both social media, including Wikipedia and Twitter, and more traditional news sites. The unique structure of Wikipedia as a collaborative endeavor has been used by Rad and Barbosa [11] who detect controversy based on mutual reverts, bi-polarity in the collaboration network, and/or mutually-reinforced scores for editors and articles (controversial editors work in controversial articles). The fact that an article on Wikipedia is controversial has then been used to evaluate the level of controversy of other documents (e.g. web pages) by mapping them to related Wikipedia articles [9].\nTaking a content-driven approach, Pennacchiotti and Popescu [10] detect controversies around celebrities in Twitter. They use a number of features including the presence of sentiment-bearing\nar X\niv :1\n40 9.\n81 52\nv1 [\ncs .C\nY ]\n2 9\nSe p\n20 14\nwords, swear words, and words in a list of controversial topics from Wikipedia.\nControversies in online news. In news sites, Choi et al. [5] detect controversial issues by looking at words that frequently appear in contexts containing positive and/or negative sentiment words. Also using lexicons, Chimmalgi [4] study controversy in user comments of news articles. Taking a more data-driven approach, Awadallah et al. [1] describe a method in which opinion holders and their opinions in news articles are identified by an iterative method based on a seed set of patterns that describe expressions of support or opposition to an idea.\nIn contrast to previous work that applies sentiment analysis to controversies, we do not assume that sentiments and controversy are related, but instead demonstrate it experimentally, thus not only providing an empirical basis for future use of sentiment lexicons, but also discovering new insight into potential self-moderation of the news sources."
    }, {
      "heading" : "3. NEWS ARTICLES",
      "text" : "Data was provided by NewsCred,1 which aggregates news content from thousands of news sources, and makes it available via an API (described further in Diakopoulos et al. [7]). We chose 15 high-volume sources from NewsCred and considered their articles in the period from March to September 2013. The main criterion we used for this selection was variety—including both national and regional sources—while at the same time we attempted to keep the list relatively brief for ease of analysis and exposition: CNN, Reuters, Usa Today, Los Angeles Times, Washington Post, Chicago Tribune, News Day, Minneapolis Star Tribune, Houston Chronicle, Philadelphia Inquirer, Honolulu Star-Advertiser, Huffington Post, New York Times, ProPublica, and Talking Points Memo.\nTwo data gathering tasks were done. First, we collected a random subset of articles for the purposes of obtaining high-frequency words. This is the list of initial topics used for the labeling task described in Section 4. Second, we searched the selected media sources for articles containing each topic word using the NewsCred API. In total, over 21 million articles were collected, with an average of around 3,000 and median of 1,000 per topic per news source. Reuters was the most prolific source with an average of 11,659 articles per topic, and ProPublica the least at 31 articles."
    }, {
      "heading" : "4. CONTROVERSIAL WORDS",
      "text" : "Whether a word is controversial or not is a highly subjective and context-dependent matter. Since controversy is socially constructed, we performed an annotation effort with a relatively large pool of annotators. We employed 25 annotators based in the U.S., hired through crowdsourcing platform CrowdFlower (https://crowdflower.com/) and paid $0.80 (USD) for every 100 words annotated, following standard pricing practices of this platform.2\nInitial Words. The initial list was composed of the top frequent words in a large random sample of articles. We filtered this list to keep only single-word nouns using a part-of-speech tagger. Next, we removed generic English stopwords, as well as frequent newsspecific stopwords (ap, broadcast, press, published, rewritten, redistributed, rights, copyright, reserved), and kept the top 2,000 most frequent terms.\n1http://newscred.com/ 2In total 40 annotators participated in the task, but only 25 of them contributed more than 100 labels. This is common in crowdsourcing platforms.\nTable 1: List of words identified during the crowdsourcing task.\nStrongly Controversial (145): abuse, administration, afghanistan, aid, america, american, army, attack, attacks, authorities, authority, ban, banks, benefits, bill, bills, border, budget, campaign, candidate, candidates, catholic, china, chinese, church, concerns, congress, conservative, control, country, court, crime, criminal, crisis, cuts, debate, debt, defense, deficit, democrats, disease, dollar, drug, drugs, economy, education, egypt, election, elections, enforcement, fighting, finance, fiscal, force, funding, gas, government, gun, health, immigration, inaccuracies, india, insurance, investigation, investigators, iran, israel, job, jobs, judge, justice, killing, korea, labor, land, law, lawmakers, laws, lawsuit, leadership, legislation, marriage, media, mexico, military, money, murder, nation, nations, news, obama, offensive, officials, oil, parties, peace, police, policies, policy, politics, poll, power, president, prices, primary, prison, progress, race, reform, republican, republicans, restrictions, rule, rules, ruling, russia, russian, school, security, senate, sex, shooting, society, spending, strategy, strike, support, syria, syrian, tax, taxes, threat, trial, unemployment, union, usa, victim, victims, violence, vote, voters, war, washington, weapons, world\nSomewhat Controversial (45): account, advantage, amount, attorney, chairman, charge, charges, cities, class, comment, companies, cost, credit, delays, effect, expectations, families, family, february, germany, goal, housing, information, investment, markets, numbers, oklahoma, parents, patients, population, price, projects, raise, rate, reason, sales, schools, sector, shot, source, sources, status, stock, store, worth\nNon-Controversial (272): 60s, 70s, addition, address, afternoon, agreed, amp, angeles, answer, april, attention, avenue, average, ball, base, bay, beach, beginning, bit, block, blue, bowl, box, boy, boys, brother, building, bus, call, calling, calls, camp, car, cars, central, cents, click, close, cloudy, club, coast, cup, dallas, date, daughter, davis, day, decade, decades, december, def, delivery, door, download, drive, eagles, end, entire, era, evening, face, faces, facility, fall, fans, father, feel, feeling, feet, fell, field, finish, floor, form, fort, francisco, friday, friend, friends, fun, girl, girls, ground, gt, guy, guys, half, hall, hand, hands, hawaii, heart, heat, heavy, hill, hits, hold, hopes, host, hotel, hour, hours, house, houston, hundreds, husband, ice, illinois, index, indiana, innings, island, january, johnson, jones, june, kansas, kind, lack, lake, leave, lee, letter, levels, light, line, lines, lot, lows, lt, main, make, makes, mark, mass, material, matter, medium, men, mid, middle, miles, mind, minneapolis, minutes, moment, monday, month, months, morning, mother, mountain, move, mph, museum, names, natural, net, night, north, note, notes, november, october, opening, park, part, parts, pass, period, person, philadelphia, pick, pitch, plant, play, player, playing, pm, point, post, practice, put, quarter, rain, read, reading, red, rest, restaurant, rise, rock, rose, round, sale, san, saturday, scene, search, season, seasons, seconds, selling, september, series, set, showers, showing, shows, sign, signs, smith, son, sox, special, spot, spring, square, stadium, stage, start, starting, starts, station, stay, step, stores, street, student, summer, sun, sunday, thing, things, thinking, thought, thousands, thunderstorms, thursday, time, title, top, total, transportation, type, unit, valley, vehicle, version, village, visit, wait, walk, wall, watch, water, ways, wednesday, week, weekend, weeks, williams, wind, winds, winner, winter, word, writer, yards, year, years, york\nCrowdsourced annotation. We used a four-point scale to classify 2,000 high-frequency terms, asking 7 “trusted” annotators per word the following question:\nYou need to be familiar with U.S. news media. Indicate if word is:\n(C3) Strongly Controversial: people often disagree and debate with opposing viewpoints. (C2) Somewhat Controversial: people sometimes disagree or have debates with opposing viewpoints. (C1) Less Controversial: people infrequently disagree or have debates with opposing viewpoints. (C0) Non-Controversial: people almost never disagree or debate with opposing viewpoints.\n“Trusted” annotators are selected by including a set of terms for which the label was known. These terms were obtained by a preliminary task in which we asked 5 crowdsourcing workers for each of 1,000 words whether they believed it to be controversial in U.S. news media or not. We selected a balanced set of 94 words for which there was perfect agreement in the preliminary task and assigned them to C0 and C1 (for non-controversial terms) and C2 and C3 (for controversial terms). Annotators who did not agree substantially with this gold standard were not considered “trusted.”\nFinally, we considered only labels for which the majority label was larger than 60% among the 7 workers. This yields Table 1 containing 145 controversial terms having label C3, 45 terms having label C2, and 272 terms having label C0. Class C1 is made of\nborderline cases and elicited very little agreement, it was thus not considered.\nControversial terms in Table 1 are part political terms referring to congress and legislation, social terms like education and unemployment, country names including russia and china, and terms such as criminal and threat. Those in the medium category may potentially belong to a controversial topic, such as february being Black History Month in the U.S. Finally, the non-controversial terms include mostly generic words."
    }, {
      "heading" : "5. CONTROVERSY AND SENTIMENT",
      "text" : "Using the dataset we describe above, we explore the vocabulary used in articles mentioning controversial/non-controversial topics using a series of lexical resources related to sentiment and bias.\nMethod. The general methodology is to consider in turn each word w in our vocabulary of 462 topics, and each media source s in our list of 15 news media. Then, collect all articles in the source s containing the word w, eliminate duplicate and near-duplicate articles (typically a by-product of articles that have multiple URLs), and combine all the articles in one topical (and very large) superarticle for analysis.\nThe analysis consists of examining this content using a lexical resource, counting the proportion of words matching a certain category in the lexical resource. We consider the following sentiment lexicons:\na. Affective Norms for English Words3 (ANEW) is a set of normative emotional ratings for 2,476 English words. We use the “valence” rating considering positive (respectively, negative) the ratings above (respectively, below) the mean. b. General Inquirer4 is a list of 1,915 words classified as positive, and 2,291 words classified as negative. c. MicroWNOp5 [3] is a list of 1,105 WordNet synsets (cognitive synonyms) classified as positive, negative, or neutral. d. SentiWordNet6 [2] assigns to each synset of WordNet (around 117,000) a positive and negative score determined by a diffusion process. Additionally, we use a bias-specific lexicon: e. Bias Lexicon7 is a list of 654 bias-related lemmas extracted\nfrom the edit history of Wikipedia by Recasens et al. [12]. Sentiment words are used as contributing features in the construction of this bias lexicon. To assess how prominent bias- and sentiment-laden terms are in controversial topics, we count the number of times each lexicon term is used in each super-article, and divide this count by the total length of the article, resulting in a proportion of the text which uses the lexicon terms.\nBias, positive and negative emotions. The distributions of the proportion of words matching each vocabulary are shown in Figure 1 in which for brevity we have selected three media: Huffington Post (HUF), CNN, and Reuters (REU). As we show in the next section, these three news media sources differ in the extent to which the use of lexicon words are used around controversial words in their articles (from most to least).\nFirst, we observe that the use of bias terms is more likely in controversial topics than non-controversial topics. This is statistically significant at p < 0.01 for all 15 news sources.\n3http://csea.phhp.ufl.edu/media/anewmessage.html 4http://www.wjh.harvard.edu/~inquirer/homecat.htm 5http://www-3.unipv.it/wnop/ 6http://sentiwordnet.isti.cnr.it/ 7http://www.mpi-sws.org/~cristian/Biased_language.html\nSecond, the use of negative terms is more likely in controversial topics than non-controversial topics. This is statistically significant at p < 0.01 for 46 out of the 60 combinations of source and lexicon (15 sources and 4 sentiment lexicons), with 9 ties (no difference significant at this level in either direction) and 5 cases in which the difference is significant at this level in the opposite direction."
    }, {
      "heading" : "CNN SWN −",
      "text" : ""
    }, {
      "heading" : "CNN SWN +",
      "text" : ""
    }, {
      "heading" : "CNN ANEW −",
      "text" : ""
    }, {
      "heading" : "CNN ANEW +",
      "text" : ""
    }, {
      "heading" : "CNN MWNOp −",
      "text" : ""
    }, {
      "heading" : "CNN MWNOp +",
      "text" : ""
    }, {
      "heading" : "CNN Gen Inq −",
      "text" : ""
    }, {
      "heading" : "CNN Gen Inq +",
      "text" : ""
    }, {
      "heading" : "CNN Bias",
      "text" : "Third, the use of positive terms is more likely in non-controversial topics than controversial topics. This is statistically significant at p < 0.01 for 40 out of the 60 conditions, with 16 ties and 4 cases with a significant difference in the opposite direction.\nStrong emotions. Three of our lexicons (ANEW, MicroWNOp and SentiWordNet) include scores that allow us to distinguish between weakly and strongly emotional terms. We observe that the use of strong emotional words is less likely in controversial topics. This is statistically significant at p < 0.01 for 32 out of 45 conditions (15 sources and 3 sentiment lexicons), with only 1 of the remaining conditions having a significant difference in the other direction.\nDifferences across sources. We find a great variety in the different treatment that controversial and non-controversial topics have, in terms of the use of biased and emotional words. In terms of statistical significance, the clearer difference between controversial and non-controversial topics was observed using (i) the lexicon of bias words, (ii) the General Inquirer strong sentiment words, and (iii) the ANEW negative words.\nWe next rank the media sources in terms of their different usage of words in these lexicons in controversial and non-controversial topics. The top 5 sources for each one are shown on Table 2. We note that several media sources repeat in this list, with Huffington Post, Washington Post, and New York Times remaining on top, indicating a consistently large difference in their usage of sentiment words around controversial topics, compared to non-controversial ones."
    }, {
      "heading" : "6. RANKING CONTROVERSY WORDS",
      "text" : "Finally, we assign a score to each topic in our list of controversial and non-controversial terms by using logistic regression, using as input features the proportion of words from each lexicon in each news source, and as training data the manually-labeled words (using only the classes C3 and C0 that represent the extreme values). This is done using feature selection, selecting 5 features out of the total 195, and training a logistic regression classifier. Then, the same classifier is applied to the training data (the purpose of this is not to generalize to unseen topics, but to understand the existing one), and a score between 0 and 1 is computed for each word (0.0 is non-controversial, 0.5 is undecided, and 1.0 is strongly controversial).\nFigure 2 depicts the training errors, which appear above the horizontal line in the plot for non-controversial topics and below it in the plot for controversial topics. To put these errors into context, we include in the figure the confidence of the manual annotation process. Note that most of the terms are classified correctly, appearing at the bottom of non-controversial and top of controversial figures. In the next section, we discuss the possible reasons for the misclassification of some of the topics.\n0.0 0.1 0.2 0.3 0.4\n0. 0\n0. 2\n0. 4\n0. 6\n0. 8\nUser Score\nC la\nss ifi\ner S\nco re\n60s 70s\naddition\naddress\nafternoon\nagreed\namp angeles\nanswer april attention\navenue\naverage\nball\nbase\nbay beach\nbeginning\nbit\nblock\nluebowlbox boy boysbrother\ncalling\ncalls\ncampcar\ncentral\ncents click\nclose cloudy clubcoastc pdallas date daughter day\ndecade\ndecades\ndecember\ndef\ndelivery\nd r downloaddriveeagles end entireeve ing\nfacefaces\nf ll fansfatherfeelf et f llfi ld finishfloor fortfranci co friday fri nd friendsfungirl girls\nground\ntguyguys h lf hallhan hands\nhawaii\nheart heatvy\nholdhopes\nhost\nhotelhourhours\nhouse\nhous n\nhundreds\nhusbandic\nillinois\nindiana\nin i gs island\njanuary\njones\njune\nkans s kind\nlack\nlake leaveleelight line lines lot lowlt\nm in make\nmakes mark\nmass\nmaterial\nmedium\nmen\nmid\nmiddle\nmil s mindminn apolis minut smoment\nmondaymonth\nmonths r i oth rou tai\nmove\nmphm seua s natural\nnet\nnight\nnorth\nnote otes\nnovember\noctober\nopeningpark\npart\nparts\npass\nperiodperson\nphiladelphia pi kitch pla t playplayerplay g pm\npoint\npost\nput quart r rain read red est restauran\nrise\nroc ros sals nsat r y sc n se ch s s s sonss cond\nsept mber\nseriess tshowers\nshowing\nsign\nsigns\nsmithn xspecialpot spring squ re tadiumstage start starting stay\nstep\nt r s street\nstudent\nsum er susunday thing things thinkingthought\nthousands\nt u d r torms\nthursday\ntime titletop\ntotal\ntype alley vehi leversion v ll gewa twalk wall w tch\nways\nwednesday\nweek\nwe k nd weeksill ams wi wi wi ter\nword\ny r\nyear years york\n(a) Non-controversial words; correctly classified words appear below the horizontal line.\n0.6 0.7 0.8 0.9 1.0\n0. 0\n0. 2\n0. 4\n0. 6\n0. 8\n1. 0\nUser Score\nC la\nss ifi\ner S\nco re\nabuse administrationafghanistanaidamerica\namerican army\nattack\nattacks\nauthorities\nauthorityban\nbanks\nbenefits\nbill\nbills\nbord r\nbudget campaignndidatec ndidatescat licchina\nchinesechurch concerns\ncon ress conserv tive\ncontrol\ncountry court crime criminal crisis\ncuts\ndebate debtdefense deficit\nde ocrat\ndisease\ndollar\ndrug drugs\neconomy education egypt election lectionsenforcement\nfighting\nfinance\nfiscal force fu ding\ngas\ngov rnment\ngun health\nimmigr tion inaccuracies\nindia insuranceinvestigation\ninvestigators\niranisrael\njob\njobs\njudge\njus ice\nkilling\nkorea labor\nland\nlaw lawmakerlaws\nlawsuit\nl de sh pl gi latio marriage\nme ia\nmilit ry\nmoney\nmurder\nnationnations\nnews\nobam\noffensive\nofficials\noil\nparties\npeace\npolice\npolici sp licy p liticsp ll\npower\npr ident\nprimary prisonprog ess race\nreform r ubli are bl srestrictions\nrule\nrules\nrulingrussia\nrussian\nschool\nsecuritysenate\nsex\nshooting\nsociety\nspending\nstrat gy\nstrike\nsupport syria yritaxtaxesthreat\ntrial\nun p ymentnion\nusa victim\nvictims\nviolencev te v t war washington w ap\nworld\n(b) Controversial words; correctly classified words appear above the horizontal line.\nFigure 2: Scores of controversial and non-controversial words including classification errors. “User score” is the confidence with which the manual labeling was done (with at least 7 annotators per element), while “classifier score” is the output of the classifier on the training data."
    }, {
      "heading" : "7. DISCUSSION",
      "text" : "In this study, we found that controversial issues are often framed using negative emotions. Many of the terms that were found to be controversial are oftentimes related to social problems and violence, such as gun, fighting, crime, victim and shooting, or war, such as strike, weapons, and army. There may not be as many opportunities to discuss some of these issues in a non-negative light. Other terms could be viewed in a negative, positive, or even a neutral light depending on the context at hand, for instance, terms such as health, security, or jobs. However, because these words often invoke larger ideological issues of government spending, privacy, or the economy, it may be that frames emerge that are more antagonistic. For instance, instead of emphasizing the positive aspects of one’s view, a writer may choose to highlight the negative aspects of the opposing view.\nAdditionally, we found that controversial topics also involve less strongly emotional words. Theories of framing posit that various organizational pressures can shape the frames different news sources employ [13]. This may point to an effort on the part of news\nagencies to more tightly control the language used when discussing controversial topics. As an overt example of organization influence on language use, many organizations today adhere to different wellknown style guides when choosing certain language around sensitive topics. The use of different style guides may point to some of the similarities and differences we saw between sources and certain controversial words. Finally, the use of biased terms become more prevalent around controversial words, with abstract notions of fairness like justice and rights, and judgment-laden terms like terrorist and criminal having a high correlation.\nBased on the above observations, we built a model to estimate to what degree a news source is treating a term as controversial. We find that some strongly controversial terms are instead classified as non-controversial—they appear in the bottom right corner of Figure 2(b). In some cases, terms that have different connotations depending on the context may have lead to incorrect classifications. For instance, oil, when referenced in a financial article may treat the subject objectively as opposed to using biased or emotional language. Another example is drug which can sometimes simply reference drug stores. However, the classification of other less ambiguous terms, such as sex and killing as non-controversial prompts re-examination of these topics as controversial. Further work is needed to model more clearly the topics represented by these terms in order to better understand their context.\nThe approach described in this work also allows us to examine the language of each agency around a controversy using a controlled vocabulary. For example, for the term democrats we compare the top 30 terms of the bias words lexicon used by each agency. We see that, along with more topical terms like obama and democratic, Huffington Post also includes more general and subjective terms like very and good, unlike, say, CNN and Reuters. The lexicons also allow us to glimpse a more local approach to news of smaller news agencies compared to national ones. When discussing murder, Reuters and CNN often mention a larger framework of government and other groups, whereas Philadelphia Inquirer, Honolulu Star-Advertiser, and Houston Chronicle mention particular people (woman, victim) and places (university, west). Further development of lexical resources for deeper understanding of news coverage beyond sentiment is an exciting future direction of this research.\nThe large-scale analysis we have conducted is an initial inquiry into quantifying how news sources differ in their framing. By being able to pick apart how news sources differ in often subtle ways, we can begin to uncover implicit biases in framing or language use by different news entities. That many of these news sources are quite large, employing hundreds and thousands of writers and editors, suggests that organization-level pressures to conform to a particular standard or world view may in fact exist. This work can also be used to inform automatic style guide checkers, serve as a reference to journalists interested in maintaining objectivity, or readers wishing to monitor their news intake. Data release: our list of strongly controversial, somewhat controversial and non-controversial terms, along with their scores is available at www.yelenamejova.com/resources/controversial_words.txt. Acknowledgments: the authors would like to thank NewsCred for making the data available for our research."
    } ],
    "references" : [ {
      "title" : "Harmony and dissonance: Organizing the people’s voices on political controversies",
      "author" : [ "R. Awadallah", "M. Ramanath", "G. Weikum" ],
      "venue" : "In Proceedings of the Fifth ACM International Conference on Web Search and Data Mining,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2012
    }, {
      "title" : "Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining",
      "author" : [ "S. Baccianella", "A. Esuli", "F. Sebastiani" ],
      "venue" : "In LREC,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2010
    }, {
      "title" : "Micro-WNOp: A gold standard for the evaluation of automatically compiled lexical resources for opinion mining",
      "author" : [ "S. Cerini", "V. Compagnoni", "A. Demontis", "M. Formentelli", "G. Gandini" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2007
    }, {
      "title" : "Controversy trend detection in social media",
      "author" : [ "R.V. Chimmalgi" ],
      "venue" : "Master’s thesis,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2010
    }, {
      "title" : "Identifying controversial issues and their sub-topics in news articles",
      "author" : [ "Y. Choi", "Y. Jung", "S.-H. Myaeng" ],
      "venue" : "Intelligence and Security Informatics,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2010
    }, {
      "title" : "Controversy as an Event Category, volume 19 of Argumentation Library, pages 75–137",
      "author" : [ "P.A. Cramer" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2011
    }, {
      "title" : "Visual analytics of media frames in online news and blogs",
      "author" : [ "N. Diakopoulos", "A.X. Zhang", "A. Salway" ],
      "venue" : "In Proc. IEEE InfoVis Workshop on Text Visualization,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2013
    }, {
      "title" : "Framing bias: Media in the distribution of power",
      "author" : [ "R.M. Entman" ],
      "venue" : "Journal of communication,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2007
    }, {
      "title" : "Detecting controversy on the web",
      "author" : [ "S.D. Hacohen", "J. Allan" ],
      "venue" : "In Proceedings of the 22nd ACM international conference on Conference on information  knowledge management,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2013
    }, {
      "title" : "Detecting controversies in Twitter: A first study",
      "author" : [ "M. Pennacchiotti", "A.M. Popescu" ],
      "venue" : "In Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics in a World of Social Media,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2010
    }, {
      "title" : "Identifying controversial articles in wikipedia: A comparative study",
      "author" : [ "H.S. Rad", "D. Barbosa" ],
      "venue" : "In Proceedings of the Eighth Annual International Symposium on Wikis and Open Collaboration, WikiSym ’12,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2012
    }, {
      "title" : "Linguistic models for analyzing and detecting biased language",
      "author" : [ "M. Recasens", "C. Danescu-Niculescu-Mizil", "D. Jurafsky" ],
      "venue" : "In Proc. of ACL,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2013
    }, {
      "title" : "Framing as a theory of media effects",
      "author" : [ "D.A. Scheufele" ],
      "venue" : "Journal of communication,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1999
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "Many of these issues are controversial, in the sense that they provoke arguments in which people express strong opposing opinions [10].",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 5,
      "context" : "This can often manifest as different ways of using language, for instance news sources will often use a series of terms to signal that a controversy exists, such as “outcry,” “furor,” and “uproar” [6].",
      "startOffset" : 197,
      "endOffset" : 200
    }, {
      "referenceID" : 12,
      "context" : "It has been theorized that journalists can also become susceptible to the ideologies, attitudes, and pressures of their organization [13], as well as unstated rules and norms [8].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 7,
      "context" : "It has been theorized that journalists can also become susceptible to the ideologies, attitudes, and pressures of their organization [13], as well as unstated rules and norms [8].",
      "startOffset" : 175,
      "endOffset" : 178
    }, {
      "referenceID" : 10,
      "context" : "The unique structure of Wikipedia as a collaborative endeavor has been used by Rad and Barbosa [11] who detect controversy based on mutual reverts, bi-polarity in the collaboration network, and/or mutually-reinforced scores for editors and articles (controversial editors work in controversial articles).",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 8,
      "context" : "web pages) by mapping them to related Wikipedia articles [9].",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 9,
      "context" : "Taking a content-driven approach, Pennacchiotti and Popescu [10] detect controversies around celebrities in Twitter.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 4,
      "context" : "[5] detect controversial issues by looking at words that frequently appear in contexts containing positive and/or negative sentiment words.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "Also using lexicons, Chimmalgi [4] study controversy in user comments of news articles.",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 0,
      "context" : "[1] describe a method in which opinion holders and their opinions in news articles are identified by an iterative method based on a seed set of patterns that describe expressions of support or opposition to an idea.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[7]).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "MicroWNOp [3] is a list of 1,105 WordNet synsets (cognitive synonyms) classified as positive, negative, or neutral.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 1,
      "context" : "SentiWordNet [2] assigns to each synset of WordNet (around 117,000) a positive and negative score determined by a diffusion process.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 11,
      "context" : "[12].",
      "startOffset" : 0,
      "endOffset" : 4
    } ],
    "year" : 2014,
    "abstractText" : "How do news sources tackle controversial issues? In this work, we take a data-driven approach to understand how controversy interplays with emotional expression and biased language in the news. We begin by introducing a new dataset of controversial and noncontroversial terms collected using crowdsourcing. Then, focusing on 15 major U.S. news outlets, we compare millions of articles discussing controversial and non-controversial issues over a span of 7 months. We find that in general, when it comes to controversial issues, the use of negative affect and biased language is prevalent, while the use of strong emotion is tempered. We also observe many differences across news sources. Using these findings, we show that we can indicate to what extent an issue is controversial, by comparing it with other issues in terms of how they are portrayed across different media.",
    "creator" : "LaTeX with hyperref package"
  }
}