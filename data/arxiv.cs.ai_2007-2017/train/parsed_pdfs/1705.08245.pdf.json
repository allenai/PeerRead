{
  "name" : "1705.08245.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Enhanced Experience Replay Generation for Efficient Reinforcement Learning",
    "authors" : [ "Vincent Huang", "Tobias Ley", "Wenfeng Hu" ],
    "emails" : [ "vincent.a.huang@ericsson.com", "tobias.ley@ericsson.com", "martha.vlachou-konchylaki@ericsson.com", "wenfeng.hu@ericsson.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In 5G telecom systems, network functions need to fulfill new network characteristic requirements, such as ultra-low latency, high robustness, quick response to changed capacity needs, and dynamic allocation of functionality. With the rise of cloud computing and data centers, more and more network functions will be virtualized and moved into the cloud. Self-optimized and self-care dynamic systems with fast and efficient scaling, workload optimization, as well as new functionality like self-healing, parameter free and zero-touch systems will assure SLA (Service Level Agreements) and reduce TCO (Total Cost of Ownership). Reinforcement learning, where an Agent learns how to act optimally given the system state information and a reward function, is a promising technology to solve such an optimization problem.\nReinforcement learning is a technology to develop self-learning SW agents, which can learn and optimize a policy based on an observed states of the environment and a reward system. An agent receives an observation from the environment in state S and selects an action to maximize the expected future reward r. Based on the expected future rewards, a value function V for each state can be calculated, and an optimal policy π that maximizes the long term value function can be derived. In a model-free environment, the RL agent needs to balance exploitation with exploration. Exploitation is the strategy to select a policy based on previously learned policies, while exploration is a strategy to search for better policies using decisions not explored previously. Exploration creates opportunities, but also induces the risk that choices done during this phase will not generate increased reward.\nIn real-time, service-critical systems, exploration can have an impact on the service quality. In addition, sparse and slow data sampling, and extended training duration put extra requirements\n*Equal contribution.\nar X\niv :1\n70 5.\n08 24\n5v 1\n[ cs\n.A I]\n2 3\non the training phase. This paper proposes a new approach for pre-training the agent based on enhanced GAN data sampling to shorten the training phase, to address the training limitation options of environments with sparse and slow data sampling.\nThe paper is organized as follows. In Section 2, we give a brief overview of reinforcement learning, generative adversarial networks and their recent development. In Section 3, we present our proposed approach of a pre-training system with enhanced GAN. Experiment results are presented in Section 4. Finally, we give concluding remarks and discussions in Section 5."
    }, {
      "heading" : "2 Background",
      "text" : ""
    }, {
      "heading" : "2.1 Reinforcement Learning",
      "text" : "Reinforcement learning is generally the problem of learning to make decisions by maximizing a numerical reward signal. [Sutton and Barto, 1998]. A reinforcement learning agent receives an observation ot from the environment it interacts in state st, and selects an action at so as to maximize the total expected discounted reward Gt. The action, drawn from the action space A, is calculated by a policy π(at|st). Every time the policy is executed, a scalar reward Ras is returned from the environment, and the agent transitions to the next state, st+1, following the state transition probability P ass′ = P (s\n′|s, a). We can define the state value function V π(s) as the expected return at state s, following policy π, and the action value function Qπ(s) as the expected return taking action a, while in state s, following policy π.\nThe reinforcement learning agent tries to maximize the expected return by maximizing the value function V π :\nV π(s) = Σa∈Aπ(a|s)(Ras + γΣs′∈SP ass′V π(s′)) (1)\nAn approach of maximizing V π(s) is using policy gradients (PG), in which the policy is parametrized and optimized by calculating the gradients using supervised learning, while iteratively adjusting the weights by backpropagating the gradients into the neural network.\nMost reinforcement learning work uses simulated environments like OpenAI Gym [Brockman et al., 2016] and achieve good results running many episodes [Duan et al., 2016, Mnih et al., 2015]. Compared to simulated environments, real environments have different characteristics and different training strategies need to be applied. The agent has access only to partial, local information, which can be formalized as a Decentralized Partial-Observable Markov Decision Process (Dec-POMDPs) [Oliehoek, 2012]. Further, it is either not possible or too expensive to do exhaustive exploration strategies in a real production system, which might cause service impact. Finally, sparse data, low data sampling rate, and slow reaction time to actions greatly limit the possibility to train an agent in an acceptable time frame [Duan et al., 2016]. New, sample efficient algorithms such as Q-Prop [Gu et al., 2016] have been proposed, that provide substantial gains in sample efficiency over trust region policy optimization (TRPO) [Schulman et al., 2015]. Methods such as actor critic algorithms [Mnih et al., 2016], as well as combinations of on-policy and off policy algorithms [O’Donoghue et al.] have been tested to beat the benchmarks. Other approaches using supervised learning have been also tested [Pinto and Gupta, 2016]. Still, the need of increasing sample efficiency to speed-up training time is imperative in real production systems that only allow for sparse data sampling."
    }, {
      "heading" : "2.2 Generative Adversarial Networks",
      "text" : "A second trend in deep learning research has been generative models, especially Generative Adversarial Nets (GAN) [Goodfellow et al., 2014], and the connection to reinforcement learning [Finn et al., 2016, Yu et al., 2017] has been discussed. GANs are used to synthesize data samples that can be used for training an RL agent. In our case, these synthesized data samples are used to pre-train a Reinforcement Learning Agent to speed-up the training time in the real production system. We will compare this method with different pre-training alternatives.\nThe essence behind Generative Adversarial Nets is an aversion between a generative model G, which learns the true data distribution, and a discriminative model D, which evaluates the probability of a sample coming from the true distribution, rather than having been generated by G.\nThe generator, modeled as a multilayer perceptron, is given inputs z, sampled from a noise distribution pz . The network G(z; θg) is trained to learn the mapping from z ∼ pz(z) to x ∼ pdata(x), where pdata is the true data distribution. The discriminator, D(x; θd), also represented by a multilayer perceptron, is given as input either the generated sample x ∼ p(x|z) or a true data point x ∼ pdata. D(x) is learning the probability of x originating from the true distribution.\nBy training both models in parallel, we can converge to a single solution where G can eventually capture the training data distribution, and D cannot discriminate between true and generated samples."
    }, {
      "heading" : "3 Enhanced GAN",
      "text" : "The object of GAN can be considered as the minmax game. The discriminator tries to maximize a value function, while the generator tries to minimize it, as shown below.\nmin G max D V (D,G) (2)\nwhere, the value function V (D,G) can be expressed as:\nV (D,G) = Ex∼pdata(x)[logD(x)] + Ez∼pz(z)[log(1−D(G(z)))] (3)\nIn our case, the training data set is the collected state(s)-{action, reward}(a) pairs. Thus, the training data can be subset to two parts:\nx = [x1, x2] = [(st, a), (st+1, r)] (4)\nCorrespondingly, the generated data also consist of two parts:\nG(z) = [G1(z), G2(z)] = [(s ′ t, a ′), (s′t+1, r ′)] (5)\nwhere s′ and a′ are the generated state(s)-{action, reward}(a) pairs. Since the state(s) and {action, reward}(a) pairs are not independent variables, there are latent relations between x1 and x2. The mutual information I between X1 and X2 can be expressed as two entropy terms:\nI(X2;X1) = H(X2)−H(X2|X1) (6)\nwhere X1 represents the (s′t, a ′) pair and X2 represents the (s′t+1, r ′) pair. The (s′t+1, r ′) pair is dependent of the (s′t, a ′) pair, therefore I(X2;X1) cannot be zero. To utilize this information, we can generate better quality experience replay data. To achieve this, we use the Kullback–Leibler divergence from Q to P , where P is the distribution of the generated action values G2(z) and Q represents the distribution of derived dependent (s′′t+1, r\n′′) pair generated from G1(z) using the mutual information I(X2;X1).\nDKL(P ||Q) = ∑ i P (i) log P (i) Q(i) (7)\nI(X2;X1) can be obtained by training from the real experience replay data. Thus, we can update the value function of V (D,G) as\nV (D,G) = Ex∼pdata(x)[logD(x)] + Ez∼pz(z)[log(1−D(G(z)))] + λDKL(P ||Q) (8)\nwhere λ is just a weighting factor. The last term is a regularization term to force the GAN to follow the relation between state and action-reward pair. When the GAN improves, G1(z) and G2(z) will follow the relations in the real experience replay data and this distance will tend to zero. The goal of the generator network is to also to minimize this term.\nThe network architecture can be realized as in figure 1. Besides the normal GAN networks, an\nadditional DNN has been added, to train the relations between state(s) and {action, reward}(a) pairs. The training procedure is shown in algorithm 1.\nAlgorithm 1: Data generation algorithm with EGAN Data: Batch of quadruplets Dr(st, a, st+1, r) drawn from the real experience Result: Unlimited experience replay samples Ds(st, a, st+1, r) which can be used for the pre-training of the reinforcement learning agent. begin\ninitialization; /* initializes the weights for generator and discriminator networks in\nGAN, as well as the enhancer network */ training GAN; /* training a GAN network with the real experience data Dr(st, a, st+1, r) */ training enhancer; /* training an enhancer network with the real experience data\nDr(st, a, st+1, r) to find the relations between Dr(st, a) and Dr(st+1, r) */ for k iterations do\ngenerate data Dt(st, a, st+1, r) with GAN; /* generate a test experience data set with GAN */ improve GAN with enhancer; /* using the enhancer to calculate the discrepancy between Dt(st, a)\nand Dt(st+1, r) and use this to update GAN */ end\nend\nIn practice, we can update the GAN with the regularization term at the same time. However, it is also possible to update the regularization term separately. In a real system, where the data collection is slow, more training on the network can be performed while waiting input of the new experience replay data. We train the relation between the state(s) and {action, reward}(a) pairs whenever new experience data comes in. After we train the GAN with the normal settings, the network weights can be updated using the trained relations.\nOnce the GAN has been trained, it is possible to generate unlimited experience replay data to pre-train the agent."
    }, {
      "heading" : "4 Results",
      "text" : "We use the CartPole environment from OpenAI Gym to evaluate the EGAN performance, as shown in figure 2, with parameter settings listed in table 1. The figure shows the training of the PG agent after it has been pre-trained, therefore we observe a small offset of the pre-trained agents on the x-axis by around 10000 samples (500 episodes), while the agent with no pre-training starts at 0. The black solid line is the 100-episode rolling average reward over the total consumed samples of a PG agent, without any pre-training mechanisms. The red dash line and the blue dot line represent the performance of\nthe PG agent with GAN and EGAN pre-training respectively. The EGAN uses 500-episode real experience, Dr, with randomly selected actions to train the GAN and Enhancer neural networks in the pre-training phase, and then generates 6000 batches of synthetic data, Dt, to update the policy network in the beginning of the training phase. For the no pre-training agent we set the total training episodes to 5500, to have a fair comparison with the EGAN over cumulative samples.\nThe samples for training the GAN and EGAN were collected using a random policy. Consequently, we expect a low initial performance for both pre-trained systems, but a more accurate value function estimation, thus a quicker learning curve since they are already initiated by generated samples. As a result, we can observe in figure 2 a faster increase of the reward for both agents pre-trained with GAN and EGAN. Both those networks can provide more modalities in the data space and since EGAN enhances the state-action-reward relation it can further improve the quality of the synthesized data, and the robustness of the system in terms of single standard deviation. We obtain, therefore, a 20% higher sample efficiency for EGAN pre-training compared to no pre-training, and a 5% improvement compare to pre-training with GAN without an enhancer. That means to reach a certain mean reward, less cumulative samples are needed, thus speeding up the training time. However,\nIn order to test the hypothesis of bootstrapping the online training with DNN, GAN, and policy network pre-initialization, we trained our system with a varying number of pre-training lengths, demonstrating the results in figure 3. The y axis represents again the 100-episode rolling average reward, while the x axis displays the online episode numbers. Figure 3 demonstrates that pre-training the generator networks with 5000 episodes results in a faster learning curve for the policy network.\nIn a real production system, the pre-training could be achieved with saving prior data to pre-initialize the system, so as to aid it to converge faster, while also achieving a more stable training. Therefore, it is of great importance to point out the fact that in real environments, where samples are expensive to produce, while also taking into consideration the episodes needed for the pre-initialization, pretraining the network with 500 episodes rather than 5000 is more efficient and cost-effective."
    }, {
      "heading" : "5 Conclusions",
      "text" : "In this work, we are tackling a fundamental problem of reinforcement learning applied to a real environment. The training normally takes long time and requires many samples. We first collected a small set of data samples from the environment, following a random policy, in order to train a GAN. The GAN is used, then, to generate unlimited synthesized data to pre-train an RL agent, so that the agent learns the basic characteristics of the environment. Using a GAN, we can cover larger variations of the random sampled data. We further improve the GAN with an enhancer, which utilizes the state-action relations in the experience replay data in order to improve the quality of the synthesized data.\nBy using the enhanced structure (EGAN) we can achieve a 20% faster than no pre-training, a 5% faster learning than pre-training with a GAN, and a more robust system in terms of standard deviation. However, further work is needed to verify and fine-tune the system for achieving optimal performance.\nOur next step is to explore and test this setup together with virtualized network functions in 5G telecom systems, where sample efficiency is crucial, and exploration can directly affect the service quality of the system."
    } ],
    "references" : [ {
      "title" : "Introduction to Reinforcement Learning",
      "author" : [ "Richard S. Sutton", "Andrew G. Barto" ],
      "venue" : null,
      "citeRegEx" : "Sutton and Barto.,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton and Barto.",
      "year" : 1998
    }, {
      "title" : "Benchmarking deep reinforcement learning for continuous control",
      "author" : [ "Yan Duan", "Xi Chen", "Rein Houthooft", "John Schulman", "Pieter Abbeel" ],
      "venue" : "In Proceedings of the 33rd International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Duan et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Duan et al\\.",
      "year" : 2016
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski" ],
      "venue" : "Nature, 518(7540):529–533,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2015
    }, {
      "title" : "Decentralized POMDPs, pages 471–503",
      "author" : [ "Frans A. Oliehoek" ],
      "venue" : "ISBN 978-3-642-27645-3",
      "citeRegEx" : "Oliehoek.,? \\Q2012\\E",
      "shortCiteRegEx" : "Oliehoek.",
      "year" : 2012
    }, {
      "title" : "Q-prop: Sample-efficient policy gradient with an off-policy critic",
      "author" : [ "Shixiang Gu", "Timothy Lillicrap", "Zoubin Ghahramani", "Richard E Turner", "Sergey Levine" ],
      "venue" : "arXiv preprint arXiv:1611.02247,",
      "citeRegEx" : "Gu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2016
    }, {
      "title" : "Trust region policy optimization",
      "author" : [ "John Schulman", "Sergey Levine", "Pieter Abbeel", "Michael Jordan", "Philipp Moritz" ],
      "venue" : "In Proceedings of the 32nd International Conference on Machine Learning",
      "citeRegEx" : "Schulman et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Schulman et al\\.",
      "year" : 2015
    }, {
      "title" : "Asynchronous methods for deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Adria Puigdomenech Badia", "Mehdi Mirza", "Alex Graves", "Timothy Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2016
    }, {
      "title" : "Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours",
      "author" : [ "Lerrel Pinto", "Abhinav Gupta" ],
      "venue" : "In Robotics and Automation (ICRA),",
      "citeRegEx" : "Pinto and Gupta.,? \\Q2016\\E",
      "shortCiteRegEx" : "Pinto and Gupta.",
      "year" : 2016
    }, {
      "title" : "Generative Adversarial Networks",
      "author" : [ "I.J. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio" ],
      "venue" : null,
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models",
      "author" : [ "Chelsea Finn", "Paul Christiano", "Pieter Abbeel", "Sergey Levine" ],
      "venue" : "arXiv preprint arXiv:1611.03852,",
      "citeRegEx" : "Finn et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Finn et al\\.",
      "year" : 2016
    }, {
      "title" : "Seqgan: sequence generative adversarial nets with policy gradient",
      "author" : [ "Lantao Yu", "Weinan Zhang", "Jun Wang", "Yong Yu" ],
      "venue" : "In Thirty-First AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Yu et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "[Sutton and Barto, 1998].",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 3,
      "context" : "The agent has access only to partial, local information, which can be formalized as a Decentralized Partial-Observable Markov Decision Process (Dec-POMDPs) [Oliehoek, 2012].",
      "startOffset" : 156,
      "endOffset" : 172
    }, {
      "referenceID" : 1,
      "context" : "Finally, sparse data, low data sampling rate, and slow reaction time to actions greatly limit the possibility to train an agent in an acceptable time frame [Duan et al., 2016].",
      "startOffset" : 156,
      "endOffset" : 175
    }, {
      "referenceID" : 4,
      "context" : "New, sample efficient algorithms such as Q-Prop [Gu et al., 2016] have been proposed, that provide substantial gains in sample efficiency over trust region policy optimization (TRPO) [Schulman et al.",
      "startOffset" : 48,
      "endOffset" : 65
    }, {
      "referenceID" : 5,
      "context" : ", 2016] have been proposed, that provide substantial gains in sample efficiency over trust region policy optimization (TRPO) [Schulman et al., 2015].",
      "startOffset" : 125,
      "endOffset" : 148
    }, {
      "referenceID" : 6,
      "context" : "Methods such as actor critic algorithms [Mnih et al., 2016], as well as combinations of on-policy and off policy algorithms [O’Donoghue et al.",
      "startOffset" : 40,
      "endOffset" : 59
    }, {
      "referenceID" : 7,
      "context" : "Other approaches using supervised learning have been also tested [Pinto and Gupta, 2016].",
      "startOffset" : 65,
      "endOffset" : 88
    }, {
      "referenceID" : 8,
      "context" : "A second trend in deep learning research has been generative models, especially Generative Adversarial Nets (GAN) [Goodfellow et al., 2014], and the connection to reinforcement learning [Finn et al.",
      "startOffset" : 114,
      "endOffset" : 139
    } ],
    "year" : 2017,
    "abstractText" : "Applying deep reinforcement learning (RL) on real systems suffers from slow data sampling. We propose an enhanced generative adversarial network (EGAN) to initialize an RL agent in order to achieve faster learning. The EGAN utilizes the relation between states and actions to enhance the quality of data samples generated by a GAN. Pre-training the agent with the EGAN shows a steeper learning curve with a 20% improvement of training time in the beginning of learning, compared to no pre-training, and an improvement compared to training with GAN by about 5% with smaller variations. For real time systems with sparse and slow data sampling the EGAN could be used to speed up the early phases of the training process.",
    "creator" : "LaTeX with hyperref package"
  }
}