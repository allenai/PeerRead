{
  "name" : "1408.6515.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Large Scale Purchase Prediction with Historical User Actions on B2C Online Retail Platform",
    "authors" : [ "Yuyu Zhang", "Liang Pang", "Lei Shi", "Bin Wang" ],
    "emails" : [ "shilei1025}@gmail.com,", "wangbin@ict.ac.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Categories and Subject Descriptors H.2.8 [Database Applications]: Data Mining\nGeneral Terms Algorithms, Experimentation\nKeywords Tmall Recommendation Prize, purchase prediction, recommender system"
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "∗These authors equally contributed to this work. †Team advisor\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. RecSys 2014, 2nd Workshop on Large Scale Recommender Systems Copyright 2014 ACM 0-89791-88-6/97/05 ...$15.00.\nTmall is a Chinese-language website for business-to-consumer (B2C) online retail, spun off from Taobao and operated in China by Alibaba Group. For B2C platform like Tmall, both sellers and buyers play the role of its customers, which should be well served at the same time. In other words, B2C platform should help sellers (brands) to precisely target potential buyers, and meanwhile help buyers (users) to easily find what they like to buy. In that sense, purchase prediction or brand recommendation is crucially important for B2C online retail platforms. On popular B2C platforms, e.g. Amazon or Tmall, the amount of active brands and users is huge. For these B2C platforms, it is challenging to build large scale recommender system. So here comes Tmall Recommendation Prize 2014, a large scale competition (over 7,000 teams participated) for large scale recommender system. Tmall provides real-world business data containing more than half billion action records from over ten million distinct users. To our best knowledge, this data set is the biggest ever among all the public competitions on recommender system. Such big data poses a big challenge. Luckily, a shared powerful cluster and corresponding development environment is provided for free during the competition. Competitors are required to implement their ideas, even the simplest ones, in MapReduce fashion and ensure programs can correctly run on distributed cluster. Due to the length limitation, details of distributed implementation will be omitted in this paper.\nThe competition task is clear and simple: recommend a set of brands to a set of users. The provided data set consists of a number of user action records, following the schema as (user ID, brand ID, action type, date). All the IDs of users and brands are hashmapped and encrypted. There are altogether 4 different types of user action: “click”, “buy”, “collect”, and “cart”. No explicit training and testing set provided. The competition requires to predict what will those users buy in the next month. To be more specific, it requires to predict a set of (user ID, brand ID) pairs which may have “buy” records (no matter how many times) during the next month. Since there is no explicit testing set, predictions are judged on F1Score for fair evaluation, which is defined based on Precision and Recall:\nPrecision = ∑N i hitBrandi∑N i pBrandi , (1)\nRecall = ∑M i hitBrandi∑M i bBrandi , (2)\nar X\niv :1\n40 8.\n65 15\nv3 [\ncs .L\nG ]\n4 M\nar 2\n01 5\nwhereN stands for the number of users in prediction,M stands for the actual number of buyers in answer, hitBrandi is the number of correctly predicted brands for user i, pBrandi is the number of predicted brands for user i, and bBrandi is the actual number of brands purchased by user i in answer.\nTo discourage overfitting, a whole new data set (drawn from the same distribution) is scheduled to be released at the last week of competition. The original data set will be replaced at that time. Since the paper deadline is earlier than the end of this competition, this solution paper is all based on the original data set.\nThe remainder parts of this paper are organized as follows. Section 2 presents our basic data analysis and preprocessing approach. In Section 3, we introduce our modeling on the purchase prediction task. Section 4 describes our feature design. Blending and ensemble algorithms are presented in Section 5.3. At last, we conclude the paper in Section 6."
    }, {
      "heading" : "2. DATA PREPROCESSING",
      "text" : "In this section, we firstly conduct basic data analysis, and accordingly introduce data splitting for offline validation, as well as data cleansing."
    }, {
      "heading" : "2.1 Data Analysis",
      "text" : "In the data set, record date ranges from April to July (from 04-15 to 08-15, year unknown). For convenience, we have a month name convention for the 4-month long data, as shown in Table 1.\nTable 2 shows the basic statistics of the data set. From this table, we see that the data is almost uniformly distributed in those 4 months. On the average, each user has about 50 action records, while each brand has about 20,000 action records. For the brand level aggregation, the comparatively abundant information can be very helpful. As for the four action types, it is obvious that click is the “cheapest” one. The overall ratio between click and buy is about 39:1."
    }, {
      "heading" : "2.2 Data Splitting",
      "text" : "Based on previous data analysis, we split the data set for local validation. Figure 1 illustrates the details. For online setting, provided data set locates in the first 4 months, and the August data is an invisible answer set used for evaluating predictions. As for local data splitting, we use records in the first three months (April, May, and June) as the local data, and use records in July as the local answer set, which is invisible and used for evaluating local predictions."
    }, {
      "heading" : "2.3 Data Cleansing",
      "text" : "Data cleansing is necessary since we have observed some users have millions of clicks but no purchases. A possible explanation is that these users may be crawlers, which automatically access to target website many times every day. Obviously, such “user”\nshould not be considered in model. We use a simple filtration rule to clean such noise in data: if a user have more than 500 clicks but 0 buys, then all the action records of this user are removed. According to our statistics, about 22 million action records are filtered out, which account for 4% of the data set."
    }, {
      "heading" : "3. PURCHASE PREDICTION MODELING",
      "text" : "In this section, we introduce the modeling of purchase prediction problem, including how to build instance and how to design time spans of feature and target."
    }, {
      "heading" : "3.1 Instance Building",
      "text" : "We model the purchase prediction task as standard machine learning problem. We take (user ID, brand ID) pair as the instance. User actions are obviously non-i.i.d. and highly dependent on historical actions. For example, user is much more likely to purchase a certain brand if she has clicked and added it to shopping cart before. Therefore, we choose to build instance in a timedependent fashion: the target of instance is decided by future actions. Here target stands for the ground truth of instance. If the task is modeled as a classification problem, the target is a binary label of purchase or not in the future. While if modeled as regression problem, the target can be either a binary label or the purchase times in the future.\nTake classification modeling as an example, for each (user ID, brand ID) pair, what we need to predict is a conditional probability:\nP (purchase in next month | historical actions), (4)\nwhere histrocial actions are the action records in previous months, including the actions of other users and brands. We use the given historical actions to build features of each instance, and set the corresponding target by checking whether or not the instance has any purchase action in next month. So that for each instance (user ID, brand ID), the records in action sequence are then divided into two parts by date: feature span and target span, as illustrated in Figure 2. In this figure, digit 0-3 stands for click, buy, collect and cart action respectively. We build features within a set of date buckets, e.g. {latest k1 days, latest k2 days, ..., latest kn days}. Details of feature design will be discussed in Section 4."
    }, {
      "heading" : "3.2 Time Span",
      "text" : "As discussed above, the time spans of feature and target are separated. We have two different ways to construct time spans:\n• Fixed time spans. In training, we use the last month as the target span and previous months as the feature span. In predicting, we use all available months as the feature span. Figure 3 illustrates the details of fixed time spans in both local and online settings.\n• Sliding time spans. In training, the feature span becomes extendable, and the target span follows the sliding end of the\nTable 2: Basic statistics of the data set.\nApril May June July Date Total (User ID, Brand ID) 52,909,766 59,299,203 49,776,687 50,188,852 195,303,359 User ID 7,513,602 8,024,680 7,415,736 7,497,824 12,500,984 Brand ID 21,305 23,779 25,742 28,036 29,706 Action Click 133,211,931 153,814,233 134,127,435 128,902,358 550,055,957 Action Buy 3,441,803 3,823,481 3,483,978 3,345,793 14,095,055 Action Collect 1,727,565 1,740,057 1,452,666 1,576,096 6,496,384 Action Cart 255,432 312,437 313,465 377,750 1,259,084 Action Total 138,636,731 159,690,208 139,377,544 134,201,997 571,906,480\n0 20 0\naction sequence of (user ID, brand ID)\n3\ndate\nlatest \uD835\uDC58\uD835\uDC5B days\nlatest \uD835\uDC581 days\nlatest \uD835\uDC58\uD835\uDC56 days\n0 1\nfeature span target span"
    }, {
      "heading" : "4. FEATURE DESIGN",
      "text" : "In this section, we introduce our design of features. Basically, for an instance (user ID, brand ID) pair, we build its features in three parts: pair features, user features, and brand features. For each part, we design 5 groups of features: counting features, ratio features, flag features, and global features. We will list each feature which parts it is applied to in detailed feature list as follows."
    }, {
      "heading" : "4.1 Counting Features",
      "text" : "We use several types of counting features as the basic statistics on various granularities, which are listed below.\n• Action count, including the count of each action type and the sum of all action types. This counting feature is applied to pair, user and brand.\n• Action day count, including the day count of each action\ntype. This is an aggregation of the action count on the day level. This counting feature is applied to pair, user and brand.\n• Valid click count, which only counts the number of clicks later than the last purchase. This counting feature is only applied to pair.\n• Distinct action user or brand count, which counts the number of distinct users or brands in each action type. This counting feature is applied to brand and user.\n• First-time action user or brand count, which counts the number of initial action of user or brand. This counting feature is applied to brand and user."
    }, {
      "heading" : "4.2 Ratio Features",
      "text" : "Ratio features are generated by division between counting features. We mainly build two types of ratio features.\n• Conversion ratio, which divides purchased related count (buy count, buy day count) by the total action count. Conversion ratio reflects the likelihood of purchase after click. This ratio feature is applied to pair, user and brand.\n• Counting ratio, which divides click or buy count by another type of click or buy count. Counting ratio is between counting features of the same action type. For example, click count / click day, buy count / distinct buy user, etc. This ratio feature is applied to user and brand.\n• Cross ratio, which is built between pair and user counting features. For example, user click count / pair click count. Cross ratio shows the user’s preference on specific brand in pair."
    }, {
      "heading" : "4.3 Flag Features",
      "text" : "Rules might be straightforward but very useful as additional predictions, which can be merged into model predictions. For example, a simple rule can be just predicting pairs with more than 10 clicks. However, our offline experiments show that it doesn’t work if directly add rule predictions on model output. Therefore, we transfer effective rules to binary flags and elegantly incorporate flag features in learning model, which works better than directly adding rule predictions. Our flag features include:\n• Whether or not this pair or user or brand has a certain type of action before. This flag feature is applied to pair, user and brand.\n• Whether or not this pair or user has consecutive purchases in adjacent two months. This flag feature is applied to pair, user and brand."
    }, {
      "heading" : "4.4 Global Features",
      "text" : "Counting, ratio, and flag features are all built based on date buckets, i.e. each feature has a copy in each date bucket and the feature value is determined by the range of date bucket. We also build global features, which are not dependent on date buckets and computed on the whole feature span. Global features have more information than date bucket dependent features. They are more robust and can describe instance more precisely. We mainly use the following global feature:\n• First and last active day, which is the distance between the first or last active day and the end of the feature span. This global feature is applied to pair and user.\n• Last purchase day, which is the distance between the last purchase day and the end of the feature span. This global feature is applied to pair and user.\n• Length of active span, which is the number of days between first and last active day. This global feature is applied to pair, user and brand.\n• Percentage of frequent users, where frequent means that a user has purchased the same brand more than once. This global feature is only applied to brand."
    }, {
      "heading" : "5. BLENDING AND ENSEMBLE",
      "text" : "In this section, we first simply introduce our individual models, and then blending and ensemble approach for aggregating those models."
    }, {
      "heading" : "5.1 Individual Models",
      "text" : "We mainly use three types of individual model: regression, classification, and global scoring. For regression, we use Gradient Boosting Regression Tree (GBRT) as a single model. For classification, we use Logistic Regression (LR) and Random Forest (RF) as individual models. Fixed time span is applied to all these three models, and sliding time span is applied to GBRT and RF. The organizers of this competition provide distributed implementation of these classical learning models. For global scoring, we use a time decay function to directly score each instance:\nscore(userID, brandID) = ∑ N ∑ T αt · I(t) · day2(n), (5)\nwhere N is the total number of the instance actions, T is the number of action types, αt is the weight of action type t, I(t) is\nan indicator showing whether the action is of type t, and day(n) is the day index of the nth action. Here day2(n) is a time decay factor."
    }, {
      "heading" : "5.2 Overall Framework",
      "text" : "With the individual models, we use a two-stage approach for model blending and ensemble. We firstly use individual models to predict the training set, and apply Logistic Regression (LR) to blend those models group by group. Then, to avoid overfitting, we just use a simple linear ensemble to the blended models, and get our final prediction model. The overall framework is shown in Figure 5."
    }, {
      "heading" : "5.3 Post Analysis",
      "text" : "We make some post analysis of our prediction result on local answer set to check the model performance in detail. We pick up all the hits in local prediction, and draw the day distribution of them, as is shown in Figure 6. This analysis shows that our model are good at predicting user purchases in the first 3 days. It is clear to see that the predicting difficulty significantly increases after the first 3 days. This is understandable since useful information becomes weaker but noise becomes stronger in farther future."
    }, {
      "heading" : "6. CONCLUSION",
      "text" : "In this paper, we present our solution to Tmall Recommendation Prize 2014. We construct instance for training and predicting in a time-dependent way, and model the purchase prediction task as a standard machine learning problem. Two ways of time span construction are designed for generating feature and target of instance. We mainly use three types of individual model: regression, classification, and global scoring. We use a twostage approach for model blending and ensemble, which effectively improve the prediction accuracy."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "This paper describes the solution of Bazinga Team for Tmall Recommendation Prize 2014. With real-world user action data provided by Tmall, one of the largest B2C online retail platforms in China, this competition requires to predict future user purchases on Tmall website. Predictions are judged on F1Score, which considers both precision and recall for fair evaluation. The data set provided by Tmall contains more than half billion action records from over ten million distinct users. Such massive data volume poses a big challenge, and drives competitors to write every single program in MapReduce fashion and run it on distributed cluster. We model the purchase prediction problem as standard machine learning problem, and mainly employ regression and classification methods as single models. Individual models are then aggregated in a two-stage approach, using linear regression for blending, and finally a linear ensemble of blended models. The competition is approaching the end but still in running during writing this paper. In the end, our team achieves F1Score 0.0611 and ranks 7th (out of 7,276 teams in total).",
    "creator" : "LaTeX with hyperref package"
  }
}