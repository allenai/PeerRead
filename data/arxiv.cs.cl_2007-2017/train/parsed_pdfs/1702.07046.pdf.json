{
  "name" : "1702.07046.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Feature Generation for Robust Semantic Role Labeling",
    "authors" : [ "Travis Wolfe", "Mark Dredze", "Benjamin Van Durme" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Feature engineering is widely recognized as an important component of robust NLP systems, with much of this engineering done by hand. Articles describing improvements in task performance over prior work tend to be methodologically driven (for example low-regret online learning algorithms and structured regularizers), with improvements in feature design often described just briefly, and as a matter of secondary importance. While the distinctions between methods of inference are formalized in the language of mathematics, most expositions of feature design employ terse, natural language descriptions, often not sufficient for reliable reproduction of the underlying factors being extracted. This has led to stagnation in feature design, and in general an attitude in some circles that features themselves are not worth exploring; i.e., we should abandon explicit, interpretable features for neural techniques which create their own representations which may not align with our own.\nFeatures sets are constructed by authors using heuristics which are often not tested. For example it is common to coarsen a feature before using it in a product because the fine grained prod-\nuct would produce “too many” features. The author may have been correct (they ran the experiment and verified that performance went down) or not, but the reader often doesn’t know which is the case, and are left with the same problem of whether to run that experiment or not. Due to the cost of running experiments, practitioners are biased towards copying the feature set verbatim.\nThis work is about removing the human from the loop of feature selection, focussing on Semantic Role Labeling (SRL) (Gildea and Jurafsky, 2002). The key challenge that we address is feature generation. Previous work has generated features by taking the Cartesian product of templates, but this is not rich enough to capture many widely used manually created features. We show that by decomposing the template even further, into atoms called featlets, we can automatically compose templates with rich, ad-hoc combinators. This process can generate many features which an expert might not consider.\nOnce we have tackled the feature generation problem, we show that we can automatically derive feature sets which match the performance of state-of-the-art feature sets created by experts. Our method uses basic statistics and requires no human expertise. We believe that models specified using featlets are easier to reproduce and offer the potential for performing feature selection with machine learning rather than domain expert knowledge, potentially at lower-cost and super-human performance.\nFeature Descriptions in the Literature For a case study on feature descriptions, consider the “voice feature” for SRL. It was first motivated and described in Gildea and Jurafsky (2002). They said that they defined 10 rules for when a verb had either active or passive voice, but never said what they were. Since then, almost every prominent pa-\nar X\niv :1\n70 2.\n07 04\n6v 1\n[ cs\n.C L\n] 2\n2 Fe\nb 20\n17\nper on SRL has listed voice as a feature or template that they use, but none of the following defined their rules for the voice feature.1 Further, discrepancies between authors is not unheard of: Gildea and Jurafsky (2002) report 5% of verbs were passive in the PTB, while Pradhan et al. (2005) report 11%. Some of these papers go into great detail about other aspects like ILP constraints and and regularization constants, but this same clarity doesn’t always extend to features.\nIn methods papers, math is used as a bridge between natural and programming languages. There is no equivalent for describing features, so this type of omission is understandable given space constraints and the clumsiness of natural language. However, given the importance of the underlying factors in a model, the lack of clarity diminishes the value of the work to other practitioners, especially among those less linguistically-inclined."
    }, {
      "heading" : "2 Featlets",
      "text" : "Our approach begins with the notion that features can be decomposed into smaller units called featlets. These units can be composed together to make a wide variety of features. We distinguish featlets from feature templates, or just templates, which are effectively sets of features. Featlets are not necessarily features, but are composed to produce features or feature templates.\nTo start with an example, the featlet WORD: given the index of a token, it returns the word at that position. A feature would not assume a token index is given, only that y and x are given, so WORD is not a feature. Featlets are also used to provide information to other featlets. For example, the featlet ARGHEAD takes the head token index of an argument span and passes it to WORD. The combination of the two, [ARGHEAD, WORD], is a template. Importantly featlets are interchangeable: the template [ARGHEAD, WORD] is related to [ARGHEAD, POS] and [ARGHEADPARENT, WORD]. Featlets are minimal to ensure that the trial and error of feature engineering falls to the machine rather than the expert.\nDefinition Featlets are operations performed on a context, which is a data structure with:\n1. Named fields which have types 1 Gildea and Jurafsky (2002) Xue and Palmer (2004) Pradhan et al. (2005) Toutanova et al. (2005) Johansson and Nugues (2008) Màrquez et al. (2008) Punyakanok et al. (2008) Das et al. (2014)\n2. A list of featlets which have been applied\n3. An output buffer of features\nIn our implementation the data fields are:\n• token1 and token2: are integers • span1 and span2: are pairs of integers\n(start, end)\n• value: an untyped object • sequence: is an untyped list\nEach of the fields in a context start out as a special value NIL. Once they are set, other featlets can read from these fields and put a feature into the output buffer. If a NIL field is read, then the featlet fails and no features are output.\nLabel Extractors This group of featlets are responsible for reading an aspect of the label y and putting it into the context. These are the only taskspecific featlets which the inference algorithm has to be aware of.\n• TARGETSPAN: sets span2 to a target • TARGETHEAD: sets token2 to the head of\nthe target span\n• ARGSPAN: sets span1 to an argument span • ARGHEAD: sets token1 to the head of an\nargument span\n• ROLE: sets value to a role • FRAMEROLE: sets value to the concatena-\ntion of a frame and a role\n• FRAME: sets value to a frame\nToken Extractors These read from token1 and output a feature.\n• WORD, POS, LEMMA • WNSYNSET: reads the lemma and POS tag\nat token1, looks up the first WordNet sense, and puts its synset id onto value.\n• BROWNCLUST: looks up a un-supervised hierarchical word cluster id for token12\n• DEPREL: output the syntactic dependency edge label connecting token1 to its parent.\n• DEPTHD: compute the depth of token1 in a dependency tree\n2One featlet for a 256 and a 1000 cluster output of Liang (2005).\nBefore moving on, it will be helpful to slightly redefine the behavior of token extractors: instead of immediately outputting a string, instead they will store that string in the value field and leave it to the OUTPUT featlet to finish the job of outputting a feature. By convention, we will assume that every string of featlets ends in a (possibly implicit) OUTPUT, so the old meaning of “token extractors output a feature” is true as long as the token extractor featlet is not followed by anything.\nValue Mutators In many cases though, we will want normalized or simplified versions of other features. For example we could want to find the shape of a word, “McDonalds” → “CcCcccccc”, or perhaps just take the first few characters, “NNP”→ “N”. Value mutators read a string from value, compute a new string, and store it back to value. This enables features like [ARGHEAD, WORD, SHAPE] or [TARGETHEAD, POS, PREFIX1].\n• LC: if value is a string, output its lowercase • SHAPE: if value is a string, output its shape • PREFIXN: sets value to a prefix of length\nAn interesting special case of value mutators are ones which filter. A featlet like CLOSEDCLASS can be applied after WORD but before OUTPUT in order to have a feature only fire for closed class words. This is achieved by CLOSEDCLASS writing NIL to value so that OUTPUT fails and no features are output. This selective firing is valuable because it can lead to expressive feature semantics (e.g. “only output the first word in a span if is in a closed class”).\nDependency Walkers Syntax lets us jump around in a sentence where structural proximity is often a more informative measure of relevance than linear proximity. Dependency walkers3 provide one way of jumping around by reading and writing token1. These can be composed as well to form walks, e.g. “grandparent” = [PARENTD, PARENTD].\n• PARENTD: set token1 to its parent in the dependency tree.\n• LEFTCHILDD: sets token1 to its left-most child in the dependency tree\n• LEFTSIBD: sets token1 to its next-to-theleft sibling in the dependency tree\n3Every time we list a LEFT featlet, we have omitted its RIGHT equivalent for space.\n• LEFTMOSTSIBD: sets token1 to its leftmost sibling in the dependency tree\nSome information is contained in the name of a dependency walker (e.g. PARENTD), other information is contained in the edge crossed (e.g. whether the parent is nsubj or dobj). To capture this information, dependency walkers also append the edge that they crossed into the sequence field. This side information can be read out later by other featlets.\nSequence Reducers Values appended to sequence are converted into features with sequence reducers.\n• NGRAMS: reads n-grams from sequence, outputs each. If value is set, prefixes every n-gram with value. Clears sequence when done.\n• BAG: special case of n-grams when n=1 • SEQN: if sequence is no longer than N,\noutput items concatenated (preserves order). Also will prepend value if set. Clears sequence regardless of length.\n• COMPRESSRUNS: Collapses X Y Y Z Z Z to X Y+ Z+, no output, doesn’t clear sequence\nDependency Representers Going back to dependency walkers for a moment, the edge that they append to sequence need not be a string like nsubj which sequence reducers can operate on. Edges are represented as tuples of (parent, child, deprel)4, and we add featlets which choose a string to represent each edge, called dependency representers. We construct an edge to string function by taking the Cartesian product of the token extractors to represent the parent and the set of functions { EDGEDIRECTIOND5, EDGELABELD6, NOEDGED7 } and make a featlet to map each of these functions over sequence.\nConstituent Walkers There is an equivalent class of featlets to the dependency walkers which instead operate on span1, span2, and a constituency tree, called constituent walker. Each of\n4Sequence reducers fail when attempting to operate nonstring values in sequence, so a representer must be called first.\n5Left or right 6Taken from dependency parse, e.g. nsubj 7A constant string: “*”\nthese operations fail if the span they read is not a constituent.\n• PARENTC: sets span1 to its parent • LEFTCHILDC: sets span1 to the left-most\nchild node\n• LEFTSIBC: sets span1 to the next-to-theleft sibling node\n• LEFTMOSTSIBC: sets span1 to the leftmost sibling node\nConstituent Representers Constituent walkers differ from dependency walkers in the values that they append to sequence, they store grammar rules like S→ NP VP. Equivalently to dependency representers, the constituency representers are { CATEGORYC8, SUBCATEGORYC9 }.\nTree Walkers Operations longer than one step typically require that a start and endpoint are known to avoid meaningless walks. Tree walkers use both dependency and constituency parses, but take shortest path walks between two endpoints, adding edges or rules to sequence.\n• TOROOTD: walks from token1 to root • COMMONPARENTD: walks from token1\nto a common parent and then token2\n• TOROOTC: walks from span1 to root • COMMONPARENTC: like COMMONPAR-\nENTD for constituency trees\n• CHILDREND: walks the children of token1, left to right\n• CHILDRENC: walks the children of span1, left to right\nLinear Walkers If syntactic trees are not available or accurate, linear walkers can provide another source of relevant information. These featlets append token indices to sequence for multi-step walks, but behave like dependency walkers otherwise, mutating a field such as token1.\n• LEFTL: moves token1 position if possible • SPAN1STARTTOENDL: walks tokens in span1\n• SPAN1LEFTTORIGHTL: like SPAN1STARTTOENDL but expands two tokens in either direction\n• HEAD1TOSPAN1STARTL\n• HEAD1TOSPAN1ENDL • SPAN1TOSPAN2L: adds any tokens between\nthe two spans to sequence\nDistance Functions Distance can be informative, but usually not clear how to represent its scale. Featlets let us address which distances to measure separately from the units used to measure them. Distance functions put a number into the value field:\n• SEQLENGTH: the number of elements in sequence10\n• DELTADEPTHD: if values in sequence are dependency nodes or token indices, put the depth of the first minus the second into value\n• DELTADEPTHC: like DELTADEPTHD for constituency nodes.\nDistance Representers Once a number has been put into value, distance representers write a string representation suitable for a feature back to value.\n• DASBUCKETS: encodes the bucket widths defined in Das et al. (2014)\n• DIRECTION: writes +1 or -1 based on the sign of the number given."
    }, {
      "heading" : "2.1 Finding Legal Templates",
      "text" : "We can figure out which strings of featlets constitute a template (most sequences don’t make sense, like [ARGSPAN, SHAPE]) by brute force search with a few heuristics. We have a rules which filter out strings of featlets like:\n• nothing can come before a token extractor • if apply a featlet fails or doesn’t change the\ncontext, stop there (this and all suffixes are invalid)\nWe do a breadth first search over all strings of featlets up to length 6 and collect all strings which are templates: those that produce output on at least 2 of 50 instances, producing 5241 templates.\nAt this point note that since featlets are functions from one context to another, they are closed under function composition.\n10This can be use to measure a variety of distances using linear walkers, like ArgSpan width or the distance between ArgHead and TargetHead."
    }, {
      "heading" : "2.2 Frequency-based Template Transforms",
      "text" : "For every template found, we produce 5 additional templates by appending the following featlets: TOP10, TOP100, TOP1000, CNT8 CNT16. The TOPN transforms a template by sorting its features by frequency, and only letting the template fire for values with a count at least as high as theN th most common feature. CNTC only lets through features observed at least C times in the training data.\nThese automatic transforms are useful for building products, since they control number of features created. In our experiments, we found that a TOPN template transform appeared in a little less than 50% of our final features and a CNTC feature appeared in a little less than 10%."
    }, {
      "heading" : "2.3 Template Composition",
      "text" : "To grow features larger than we can discover with brute force enumeration of featlet strings, we consider products of templates. It is common to represent this by string concatenation, but we define template products to be the same as featlet concatenation. This has one importance difference: a template may return no value, in which case the rest of the product returns no value. With string concatenation you can represent one template making another more specific by including more information. With featlet composition you can have a template modulate when another can fire. This is weaker than general featlet composition though, and order doesn’t matter."
    }, {
      "heading" : "2.4 Near Duplicate Removal",
      "text" : "We will generate pairs of similar and possibly redundant templates. For example: e.g. X1 = [ARGHEAD, LEFTSIBD, WORD] and X2 = [ARGHEAD, LEFTMOSTSIBD, WORD]. Principled approaches like conditional mutual information I(Y ;X2 | X1) could be used to filter, but this would require a lot of computation. It is faster to use the type level (featlet/template names) rather than the token level (values extracted on instance data). We can construct similarity functions for each of the levels of structure we’ve produced.\n1. similarity between two featlets is the normalized Levenshtein edit distance11 between their names, so LEFTSIBD is similar to LEFTMOSTSIBD but not PARENTC.\n11Operations have unit cost and we divide by the length of the longer string.\n2. similarity between two templates (strings of featlets) is again the the normalized Levenshtein edit distance, but over an alphabet of featlets, where the substitution cost is inversely related to the previous similarity function.12\n3. similarity between two features is the maxweight bipartite matching of templates, where the weight is inversely related to the previous similarity function. We don’t use edit distance here since order doesn’t matter.\nWe use this last similarity function to prune ranked lists of features produced in §3.13"
    }, {
      "heading" : "3 Feature Selection",
      "text" : "Feature generation can lead to too many features to fit in memory. Some of the features we generate may provide no signal towards a label we are trying to predict. To filter down to a manageable set of informative features, we score each template using mutual information (sometimes referred to as information gain), between a label (Bernoulli) and a template (Multinomial). Mutual information has a natural connection to Bayesian independence testing (Minka, 2003). Since computing mutual information is just counting, this task is embarrassingly parallel and can be easily implemented in frameworks like MapReduce.\nWe select a budget of how many features to search over B, and divide that budget up amongst template products up to order n such that order i features get a proportion of the budget of γi. In these experiments we set B = 3000000 and γ = 1.5, which meant that the split between features was [21%, 32%, 47%].14 For each split, features were ranked by the max of a heuristic score for each of its templates. Each templates heuristic score was its mutual information plus a Gaussian with mean 0 and standard deviation 2. Randomness was introduced for diversity and so that templates which are more useful as filters (and have low mutual information by themselves) have some chance of being picked.\n12We convert edit distance to similarity by sim(a, b) = k\nk+dist(a,b) with k = 2.\n13We consider two features redundant if the normalized max-weight matching is greater than 0.75. To normalize we divide by the shorter length (in templates) of the two features.\n14These are really maximum proportions, filled up from lowest order to highest order, with extra slots rolled over to the remaining slices proportional to the remaining weights.\nEntropy and mutual information estimation breaks down when the cardinality of the variables is large compared to the number of observations (Paninski, 2003). We observed that entropy estimates based on the maximum likelihood estimates of p(y, x) and p(x) from counts of (y, x) yielded very biased estimates of mutual information (high for sparse features). We correct for this problem by using the BUB entropy estimation method described by Paninski (2003).\nWe produce a final ranked list of features by sorting by I(Y ;X)1+βH(X) and then applying the greedy pruning described in §2.4. This expression’s limit as β → 0 is mutual information and normalized mutual information as β → ∞. In our experiments, most features had between one and nine nats of entropy, as shown in figure 3, and we created feature sets out of β ∈ {0.01, 0.1, 1, 10}"
    }, {
      "heading" : "4 Experiments",
      "text" : "In our experiments we use semantic role labeling (SRL) as a case study to test whether our automatic methods can derive feature sets which work as well as hand engineered ones. SRL is a difficult prediction task with more than one structural formulation (type of label). Sometimes arguments are represented by their head token in a dependency tree (Surdeanu et al., 2008; Hajič et\nal., 2009) and sometimes they are specified by a span or constituent (Carreras and Màrquez, 2005; Baker et al., 1998). For span-based SRL, the correspondence between the argument spans and syntactic constituents can be very tight (Kingsbury and Palmer, 2002) or not (Ruppenhofer et al., 2006). Sometimes the role labels depend on the predicate sense (Ruppenhofer et al., 2006) and sometimes they don’t (Kingsbury and Palmer, 2002). These differences indicate that there may not be one “SRL feature set” which works best, or even well, for all variants of the task.\nWe used FrameNet 1.5 (Ruppenhofer et al., 2006) and the CoNLL 2012 data derived from the OntoNotes 5.0 corpus (Pradhan et al., 2013). We used the argument candidate selection method described in Xue and Palmer (2004) as well as the extensions in Hermann et al. (2014). Annotations are provided from the Stanford CoreNLP toolset (Manning et al., 2014). Feature selection is run first on each data set to produce a few feature sets based on β and size, then we evaluate their performance using an averaged perceptron (Freund and Schapire, 1999; Collins, 2002). We ran the algorithm for up to 10 passes over the data, shuffling before each pass, and selected the averaged weights which yielded the best F1 on the dev set.15\nSRL Stages Most work on SRL breaks the problem into (at least) two stages: argument identification and role classification. The argument identification stage classifies whether a span is a semantic argument to a particular target, and then the classification stage chooses the role for each span which was selected by the identification stage. We adopt this standard architecture for efficiency: if there are O(s) spans and O(k) roles, it turns an O(sk) decoding problem into anO(s+k) decoding problem.\nGiven this stage-based architecture, we split our budget, half going to each stage. For both we define y to be a Bernoulli variable which is one on sub-structures which appear on the gold parse. For arg id the instances are spans for a particular target, and for role classification the instances are roles for the span chosen in the previous stage. During training we use gold arg id to train the role classifier. For arg id we only score features which contain a ARGHEAD, or ARGSPAN featlet, and for role classification we additionally require\n15For FrameNet data we took a random 10% slice of the training data as the dev set.\na ROLEARG or FRAMEROLEARG featlet appear."
    }, {
      "heading" : "5 Results",
      "text" : "Overall, our method seems to work about as well as experts manually designing features for SRL. Results in table 5 shows our approach matching the performance of Das et al. (2012) and Pradhan et al. (2013). Other systems achieve better performance, but these models all use global information, an orthogonal issue to the local feature set.\nIn looking at the feature sets generated, one major difference is the complexity parameter β. For FrameNet, the best value of β was 10, meaning that the features with the highest normalized mutual information were chosen, whereas with Propbank and β = 0.01, it was better to ignore the entropy of the feature. This makes sense in retrospect when you consider the size of the training sets, Propbank is about 20 times larger, but its not clear how much data is needed to justify this shift when tuning by hand.\nThis difference in selection criteria does lead to very different feature sets chosen,16 but it is another question of whether this matters towards system performance. It could be that there are many different types of feature sets which lead to good performance on either task/dataset, and only one is needed (possibly created manually). In table 5 we show the effects generating a feature set for one dataset and applying it to the other. The performance on the diagonal is considerably higher, indicating empirically that there likely isn’t one\n16There are actually only two templates in common between the best FrameNet and Propbank feature sets. Both contain the COMMONPARENTD featlet.\n“SRL feature set”. If you weight both equally, the average increase in error due to domain shift is 7.9%. This is even the case for feature selection with FrameNet, where you might expect that selecting features on Propbank, a much larger resource, could yield gains because of much lower variance without much bias.\nSensitivity by Stage Given that we can automatically generate feature sets, we can easily determine how adding or removing features from each stage will affect performance. This is useful for choosing a feature set which balances the cost of prediction time with performance, which is labor intensive and error prone when done manually. Table 5 shows that the model is more sensitive to the removal of argument id features than role classification ones. This is not a new result (Màrquez et al., 2008), but this work offers a way to respond by applying computational rather than human resources to the problem."
    }, {
      "heading" : "6 Discussion",
      "text" : "Limitations On limitation of the methods described here is finding symmetries. The product operator for templates is commutative, but this is\nnot the case for featlets. Some templates are equivalent and there is no easy way to check short of checking their denotations, which is expensive.\nAnother issue is that a lot of features are required. The best models we trained for FrameNet use over 2500 features, which is significantly more than Das et al. (2012), which used 34. Upon manual inspection of the feature sets we learned, we find most if not all of the features that Das et al. (2012) created,17 but precision is low."
    }, {
      "heading" : "7 Related Work",
      "text" : "Recently there has been a swell in interest in neural methods in NLP which use continuous representations rather than discrete feature weights. This work shares some motivation with neural methods, e.g. the desire to avoid domain expertderived features, but we diverge primarily for computational reasons. This work is about model generation and scoring, and it is not clear how to score neural models in ways that don’t involve re-training a model. Feature based models are amenable decomposition and information theoretic analysis in ways that neural models aren’t.\nWithin feature based methods, backwards selection methods are common, including a large body of work on sparsity-inducing regularization, the canonical being the lasso (Tibshirani, 1996). These methods are applicable when the entire feature set can be enumerated and scored on one machine. This is not feasible for this work, since we generate features which lie in a combinatorial space too large to fit in memory. For example, our method found the feature [ TARGETHEAD, RIGHT, WNSYNSET, ARGSPAN, SPAN1START, LEFTL, WORD, TOP10, SPAN1TOSPAN2L, SEQMAPPOS, BAG ], which is comprised of 11 featlets18.\nThe alternative are forwards selection methods which work by building bigger features from smaller ones. Björkelund et al. (2009) used forwards selection for dependency-based SRL (Hajič et al., 2009) for 7 languages based on products of templates. Their experiments showed a great diversity of the features learned for different languages and they placed second in the shared\n17It is a not trivial to match our templates to theirs. For example, the template [TARGETHEAD, LEFTL, TOP10] * [TARGETHEAD, CHILDSEQUENCED, SEQMAPDEPREL, BAG] is likely close to the passive voice template used in Das et al. (2012), since “was” and “be” are in the top 10 words to the left of a verb.\n18Technically it is 13 featlets since an OUTPUT featlet is not written after the WNSYNSET and TOP10 featlets, §2.\ntask. McCallum (2003) used forwards selection for named entity recognition, scoring new feature products using approximate model re-fitting (pseudo-likelihood), which also produced good results. In both of these works, scoring new features depended on the output of a smaller feature set. Sequential methods like this are not amenable to paralellization and take quadratic time with respect to the number of feature to be searched over.\nOur method is more similar to the work of Gormley et al. (2014) where every template is scored in parallel irrespective of a trained model.\nFuture work This work dove-tails with the approach described by Lee et al. (2007), which derives a prior or regularization constant for individual features by looking at properties of the feature (meta features). This work generates features with a lot of structure, which the learner could reflect upon to improve regularization and generalization.\nThe structure in these features can also inform parameterization. Tensor decomposition methods of fixed-order tensors have been used to great effect (Lei et al., 2014; Lei et al., 2015). Low-rank or embedding methods (e.g. RNNs) for parameterizing featlet strings, as opposed to storing a weight in a dictionary, could also improve regularization.\nStep-wise methods which select some features, fit a model, and then select more features with respect to mutual information with residuals, are another simple and promising direction."
    }, {
      "heading" : "8 Conclusion",
      "text" : "In this work we propose a general framework for generating feature sets with the goal of removing expert engineering from the machine learning loop. Our approach is based on composing units called featlets to create templates. Featlets are small functions which are task agnostic and easy to define and implement by non-experts. Featlets on one hand preserve a wide variety of nuanced feature semantics, and on the other can be enumerated automatically to derive a huge amount of novel templates and features. We validate our approach on semantic role labeling and achieve performance on par with models that had considerable expert intervention."
    } ],
    "references" : [ {
      "title" : "The berkeley framenet",
      "author" : [ "Baker et al.1998] Collin F Baker", "Charles J Fillmore", "John B Lowe" ],
      "venue" : null,
      "citeRegEx" : "Baker et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Baker et al\\.",
      "year" : 1998
    }, {
      "title" : "Multilingual semantic role labeling",
      "author" : [ "Love Hafdell", "Pierre Nugues" ],
      "venue" : "In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task,",
      "citeRegEx" : "Björkelund et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Björkelund et al\\.",
      "year" : 2009
    }, {
      "title" : "Introduction to the conll-2005 shared task: Semantic role labeling",
      "author" : [ "Carreras", "Màrquez2005] Xavier Carreras", "Lluı́s Màrquez" ],
      "venue" : "In Proceedings of the Ninth Conference on Computational Natural Language Learning,",
      "citeRegEx" : "Carreras et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Carreras et al\\.",
      "year" : 2005
    }, {
      "title" : "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms",
      "author" : [ "Michael Collins" ],
      "venue" : "In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume",
      "citeRegEx" : "Collins.,? \\Q2002\\E",
      "shortCiteRegEx" : "Collins.",
      "year" : 2002
    }, {
      "title" : "An exact dual decomposition algorithm for shallow semantic parsing with constraints. In SemEval, SemEval ’12",
      "author" : [ "Das et al.2012] Dipanjan Das", "André F.T. Martins", "Noah A. Smith" ],
      "venue" : null,
      "citeRegEx" : "Das et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Das et al\\.",
      "year" : 2012
    }, {
      "title" : "Semantic role labelling with neural network factors",
      "author" : [ "Oscar Täckström", "Kuzman Ganchev", "Dipanjan Das" ],
      "venue" : "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "FitzGerald et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "FitzGerald et al\\.",
      "year" : 2015
    }, {
      "title" : "Large margin classification using the perceptron algorithm",
      "author" : [ "Freund", "Schapire1999] Yoav Freund", "Robert E Schapire" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Freund et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Freund et al\\.",
      "year" : 1999
    }, {
      "title" : "Automatic labeling of semantic roles",
      "author" : [ "Gildea", "Jurafsky2002] Daniel Gildea", "Daniel Jurafsky" ],
      "venue" : "Computational linguistics,",
      "citeRegEx" : "Gildea et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Gildea et al\\.",
      "year" : 2002
    }, {
      "title" : "Low-resource semantic role labeling",
      "author" : [ "Margaret Mitchell", "Benjamin Van Durme", "Mark Dredze" ],
      "venue" : "In Proceedings of ACL, June",
      "citeRegEx" : "Gormley et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Gormley et al\\.",
      "year" : 2014
    }, {
      "title" : "Semantic frame identification with distributed word representations",
      "author" : [ "Dipanjan Das", "Jason Weston", "Kuzman Ganchev" ],
      "venue" : "In Proceedings of ACL. Association for Computational Linguistics",
      "citeRegEx" : "Hermann et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hermann et al\\.",
      "year" : 2014
    }, {
      "title" : "Dependency-based semantic role labeling of propbank",
      "author" : [ "Johansson", "Nugues2008] Richard Johansson", "Pierre Nugues" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Johansson et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Johansson et al\\.",
      "year" : 2008
    }, {
      "title" : "From treebank to propbank",
      "author" : [ "Kingsbury", "Palmer2002] Paul Kingsbury", "Martha Palmer" ],
      "venue" : "In LREC. Citeseer",
      "citeRegEx" : "Kingsbury et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Kingsbury et al\\.",
      "year" : 2002
    }, {
      "title" : "Learning a meta-level prior for feature relevance from multiple related tasks",
      "author" : [ "Lee et al.2007] Su-In Lee", "Vassil Chatalbashev", "David Vickrey", "Daphne Koller" ],
      "venue" : "In Proceedings of the 24th International Conference on Machine Learning,",
      "citeRegEx" : "Lee et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2007
    }, {
      "title" : "Low-rank tensors for scoring dependency structures",
      "author" : [ "Lei et al.2014] Tao Lei", "Yu Xin", "Yuan Zhang", "Regina Barzilay", "Tommi Jaakkola" ],
      "venue" : "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume",
      "citeRegEx" : "Lei et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Lei et al\\.",
      "year" : 2014
    }, {
      "title" : "High-order low-rank tensors for semantic role labeling",
      "author" : [ "Lei et al.2015] Tao Lei", "Yuan Zhang", "Lluı́s Màrquez", "Alessandro Moschitti", "Regina Barzilay" ],
      "venue" : "In Proceedings of the 2015 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Lei et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lei et al\\.",
      "year" : 2015
    }, {
      "title" : "Semi-supervised learning for natural language",
      "author" : [ "Percy Liang" ],
      "venue" : "Master’s thesis, Massachusetts Institute of Technology",
      "citeRegEx" : "Liang.,? \\Q2005\\E",
      "shortCiteRegEx" : "Liang.",
      "year" : 2005
    }, {
      "title" : "The Stanford CoreNLP natural language processing toolkit",
      "author" : [ "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky" ],
      "venue" : null,
      "citeRegEx" : "Manning et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Manning et al\\.",
      "year" : 2014
    }, {
      "title" : "Semantic role labeling: An introduction",
      "author" : [ "Xavier Carreras", "Kenneth C. Litkowski", "Suzanne Stevenson" ],
      "venue" : null,
      "citeRegEx" : "Màrquez et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Màrquez et al\\.",
      "year" : 2008
    }, {
      "title" : "Efficiently inducing features of conditional random fields",
      "author" : [ "Andrew McCallum" ],
      "venue" : "In Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "McCallum.,? \\Q2003\\E",
      "shortCiteRegEx" : "McCallum.",
      "year" : 2003
    }, {
      "title" : "Bayesian inference, entropy, and the multinomial distribution",
      "author" : [ "Tom Minka" ],
      "venue" : "Online tutorial",
      "citeRegEx" : "Minka.,? \\Q2003\\E",
      "shortCiteRegEx" : "Minka.",
      "year" : 2003
    }, {
      "title" : "Estimation of entropy and mutual information",
      "author" : [ "Liam Paninski" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Paninski.,? \\Q2003\\E",
      "shortCiteRegEx" : "Paninski.",
      "year" : 2003
    }, {
      "title" : "Support vector learning for semantic argument classification",
      "author" : [ "Kadri Hacioglu", "Valerie Krugler", "Wayne Ward", "James H Martin", "Daniel Jurafsky" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Pradhan et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Pradhan et al\\.",
      "year" : 2005
    }, {
      "title" : "Towards robust linguistic analysis using ontonotes",
      "author" : [ "Alessandro Moschitti", "Nianwen Xue", "Hwee Tou Ng", "Anders Björkelund", "Olga Uryupina", "Yuchen Zhang", "Zhi Zhong" ],
      "venue" : "In Proceedings of the Seven-",
      "citeRegEx" : "Pradhan et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Pradhan et al\\.",
      "year" : 2013
    }, {
      "title" : "The importance of syntactic parsing and inference in semantic role labeling",
      "author" : [ "Dan Roth", "Wen-tau Yih" ],
      "venue" : null,
      "citeRegEx" : "Punyakanok et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Punyakanok et al\\.",
      "year" : 2008
    }, {
      "title" : "Framenet ii: Extended theory and practice",
      "author" : [ "Michael Ellsworth", "Miriam RL Petruck", "Christopher R Johnson", "Jan Scheffczyk" ],
      "venue" : null,
      "citeRegEx" : "Ruppenhofer et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Ruppenhofer et al\\.",
      "year" : 2006
    }, {
      "title" : "The conll-2008 shared task on joint parsing of syntactic and semantic dependencies",
      "author" : [ "Richard Johansson", "Adam Meyers", "Lluı́s Màrquez", "Joakim Nivre" ],
      "venue" : "In Proceedings of the Twelfth Conference on Computa-",
      "citeRegEx" : "Surdeanu et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Surdeanu et al\\.",
      "year" : 2008
    }, {
      "title" : "Efficient inference and structured learning for semantic role labeling",
      "author" : [ "Kuzman Ganchev", "Dipanjan Das" ],
      "venue" : "Transactions of the Association for Computational Linguistics,",
      "citeRegEx" : "Täckström et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Täckström et al\\.",
      "year" : 2015
    }, {
      "title" : "Regression shrinkage and selection via the lasso",
      "author" : [ "Robert Tibshirani" ],
      "venue" : "Journal of the Royal Statistical Society. Series B (Methodological),",
      "citeRegEx" : "Tibshirani.,? \\Q1996\\E",
      "shortCiteRegEx" : "Tibshirani.",
      "year" : 1996
    }, {
      "title" : "Joint learning improves semantic role labeling",
      "author" : [ "Aria Haghighi", "Christopher D. Manning" ],
      "venue" : "In Proceedings of the 43rd Annual Meeting",
      "citeRegEx" : "Toutanova et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Toutanova et al\\.",
      "year" : 2005
    }, {
      "title" : "Calibrating features for semantic role labeling",
      "author" : [ "Xue", "Palmer2004] Nianwen Xue", "Martha Palmer" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "Xue et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Xue et al\\.",
      "year" : 2004
    }, {
      "title" : "Endto-end learning of semantic role labeling using recurrent neural networks",
      "author" : [ "Zhou", "Xu2015] Jie Zhou", "Wei Xu" ],
      "venue" : "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint",
      "citeRegEx" : "Zhou et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 21,
      "context" : "1 Further, discrepancies between authors is not unheard of: Gildea and Jurafsky (2002) report 5% of verbs were passive in the PTB, while Pradhan et al. (2005) report 11%.",
      "startOffset" : 137,
      "endOffset" : 159
    }, {
      "referenceID" : 19,
      "context" : "1 Gildea and Jurafsky (2002) Xue and Palmer (2004) Pradhan et al. (2005) Toutanova et al.",
      "startOffset" : 51,
      "endOffset" : 73
    }, {
      "referenceID" : 19,
      "context" : "1 Gildea and Jurafsky (2002) Xue and Palmer (2004) Pradhan et al. (2005) Toutanova et al. (2005) Johansson and Nugues (2008) Màrquez et al.",
      "startOffset" : 51,
      "endOffset" : 97
    }, {
      "referenceID" : 19,
      "context" : "1 Gildea and Jurafsky (2002) Xue and Palmer (2004) Pradhan et al. (2005) Toutanova et al. (2005) Johansson and Nugues (2008) Màrquez et al.",
      "startOffset" : 51,
      "endOffset" : 125
    }, {
      "referenceID" : 16,
      "context" : "(2005) Johansson and Nugues (2008) Màrquez et al. (2008) Punyakanok et al.",
      "startOffset" : 35,
      "endOffset" : 57
    }, {
      "referenceID" : 16,
      "context" : "(2005) Johansson and Nugues (2008) Màrquez et al. (2008) Punyakanok et al. (2008) Das et al.",
      "startOffset" : 35,
      "endOffset" : 82
    }, {
      "referenceID" : 4,
      "context" : "(2008) Das et al. (2014) 2.",
      "startOffset" : 7,
      "endOffset" : 25
    }, {
      "referenceID" : 15,
      "context" : "One featlet for a 256 and a 1000 cluster output of Liang (2005).",
      "startOffset" : 51,
      "endOffset" : 64
    }, {
      "referenceID" : 4,
      "context" : "• DASBUCKETS: encodes the bucket widths defined in Das et al. (2014)",
      "startOffset" : 51,
      "endOffset" : 69
    }, {
      "referenceID" : 19,
      "context" : "Mutual information has a natural connection to Bayesian independence testing (Minka, 2003).",
      "startOffset" : 77,
      "endOffset" : 90
    }, {
      "referenceID" : 20,
      "context" : "Entropy and mutual information estimation breaks down when the cardinality of the variables is large compared to the number of observations (Paninski, 2003).",
      "startOffset" : 140,
      "endOffset" : 156
    }, {
      "referenceID" : 20,
      "context" : "Entropy and mutual information estimation breaks down when the cardinality of the variables is large compared to the number of observations (Paninski, 2003). We observed that entropy estimates based on the maximum likelihood estimates of p(y, x) and p(x) from counts of (y, x) yielded very biased estimates of mutual information (high for sparse features). We correct for this problem by using the BUB entropy estimation method described by Paninski (2003).",
      "startOffset" : 141,
      "endOffset" : 457
    }, {
      "referenceID" : 25,
      "context" : "Sometimes arguments are represented by their head token in a dependency tree (Surdeanu et al., 2008; Hajič et al., 2009) and sometimes they are specified by a span or constituent (Carreras and Màrquez, 2005; Baker et al.",
      "startOffset" : 77,
      "endOffset" : 120
    }, {
      "referenceID" : 0,
      "context" : ", 2009) and sometimes they are specified by a span or constituent (Carreras and Màrquez, 2005; Baker et al., 1998).",
      "startOffset" : 66,
      "endOffset" : 114
    }, {
      "referenceID" : 24,
      "context" : "For span-based SRL, the correspondence between the argument spans and syntactic constituents can be very tight (Kingsbury and Palmer, 2002) or not (Ruppenhofer et al., 2006).",
      "startOffset" : 147,
      "endOffset" : 173
    }, {
      "referenceID" : 24,
      "context" : "Sometimes the role labels depend on the predicate sense (Ruppenhofer et al., 2006) and sometimes they don’t (Kingsbury and Palmer, 2002).",
      "startOffset" : 56,
      "endOffset" : 82
    }, {
      "referenceID" : 24,
      "context" : "5 (Ruppenhofer et al., 2006) and the CoNLL 2012 data derived from the OntoNotes 5.",
      "startOffset" : 2,
      "endOffset" : 28
    }, {
      "referenceID" : 22,
      "context" : "0 corpus (Pradhan et al., 2013).",
      "startOffset" : 9,
      "endOffset" : 31
    }, {
      "referenceID" : 16,
      "context" : "Annotations are provided from the Stanford CoreNLP toolset (Manning et al., 2014).",
      "startOffset" : 59,
      "endOffset" : 81
    }, {
      "referenceID" : 3,
      "context" : "Feature selection is run first on each data set to produce a few feature sets based on β and size, then we evaluate their performance using an averaged perceptron (Freund and Schapire, 1999; Collins, 2002).",
      "startOffset" : 163,
      "endOffset" : 205
    }, {
      "referenceID" : 18,
      "context" : "0 corpus (Pradhan et al., 2013). We used the argument candidate selection method described in Xue and Palmer (2004) as well as the extensions in Hermann et al.",
      "startOffset" : 10,
      "endOffset" : 116
    }, {
      "referenceID" : 8,
      "context" : "We used the argument candidate selection method described in Xue and Palmer (2004) as well as the extensions in Hermann et al. (2014). Annotations are provided from the Stanford CoreNLP toolset (Manning et al.",
      "startOffset" : 112,
      "endOffset" : 134
    }, {
      "referenceID" : 4,
      "context" : "6 Das et al. (2012) local 7 67.",
      "startOffset" : 2,
      "endOffset" : 20
    }, {
      "referenceID" : 4,
      "context" : "6 Das et al. (2012) local 7 67.7 59.8 63.5 Das et al. (2012) constrained 3 70.",
      "startOffset" : 2,
      "endOffset" : 61
    }, {
      "referenceID" : 20,
      "context" : "2 Pradhan et al. (2013) 7 81.",
      "startOffset" : 2,
      "endOffset" : 24
    }, {
      "referenceID" : 20,
      "context" : "2 Pradhan et al. (2013) 7 81.3 70.5 75.5 Pradhan et al. (2013) (revised) 7 78.",
      "startOffset" : 2,
      "endOffset" : 63
    }, {
      "referenceID" : 20,
      "context" : "2 Pradhan et al. (2013) 7 81.3 70.5 75.5 Pradhan et al. (2013) (revised) 7 78.5 76.7 77.5 Täckström et al. (2015) 3 80.",
      "startOffset" : 2,
      "endOffset" : 114
    }, {
      "referenceID" : 5,
      "context" : "4 FitzGerald et al. (2015) 3 80.",
      "startOffset" : 2,
      "endOffset" : 27
    }, {
      "referenceID" : 4,
      "context" : "Results in table 5 shows our approach matching the performance of Das et al. (2012) and Pradhan et al.",
      "startOffset" : 66,
      "endOffset" : 84
    }, {
      "referenceID" : 4,
      "context" : "Results in table 5 shows our approach matching the performance of Das et al. (2012) and Pradhan et al. (2013). Other systems achieve better performance, but these models all use global information, an orthogonal issue to the local feature set.",
      "startOffset" : 66,
      "endOffset" : 110
    }, {
      "referenceID" : 17,
      "context" : "This is not a new result (Màrquez et al., 2008), but this work offers a way to respond by applying computational rather than human resources to the problem.",
      "startOffset" : 25,
      "endOffset" : 47
    }, {
      "referenceID" : 4,
      "context" : "The best models we trained for FrameNet use over 2500 features, which is significantly more than Das et al. (2012), which used 34.",
      "startOffset" : 97,
      "endOffset" : 115
    }, {
      "referenceID" : 4,
      "context" : "The best models we trained for FrameNet use over 2500 features, which is significantly more than Das et al. (2012), which used 34. Upon manual inspection of the feature sets we learned, we find most if not all of the features that Das et al. (2012) created,17 but precision is low.",
      "startOffset" : 97,
      "endOffset" : 249
    }, {
      "referenceID" : 27,
      "context" : "Within feature based methods, backwards selection methods are common, including a large body of work on sparsity-inducing regularization, the canonical being the lasso (Tibshirani, 1996).",
      "startOffset" : 168,
      "endOffset" : 186
    }, {
      "referenceID" : 1,
      "context" : "Björkelund et al. (2009) used forwards selection for dependency-based SRL (Hajič et al.",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 4,
      "context" : "For example, the template [TARGETHEAD, LEFTL, TOP10] * [TARGETHEAD, CHILDSEQUENCED, SEQMAPDEPREL, BAG] is likely close to the passive voice template used in Das et al. (2012), since “was” and “be” are in the top 10 words to the left of a verb.",
      "startOffset" : 157,
      "endOffset" : 175
    }, {
      "referenceID" : 4,
      "context" : "For example, the template [TARGETHEAD, LEFTL, TOP10] * [TARGETHEAD, CHILDSEQUENCED, SEQMAPDEPREL, BAG] is likely close to the passive voice template used in Das et al. (2012), since “was” and “be” are in the top 10 words to the left of a verb. Technically it is 13 featlets since an OUTPUT featlet is not written after the WNSYNSET and TOP10 featlets, §2. task. McCallum (2003) used forwards selection for named entity recognition, scoring new feature products using approximate model re-fitting (pseudo-likelihood), which also produced good results.",
      "startOffset" : 157,
      "endOffset" : 378
    }, {
      "referenceID" : 8,
      "context" : "Our method is more similar to the work of Gormley et al. (2014) where every template is scored in parallel irrespective of a trained model.",
      "startOffset" : 42,
      "endOffset" : 64
    }, {
      "referenceID" : 12,
      "context" : "Future work This work dove-tails with the approach described by Lee et al. (2007), which derives a prior or regularization constant for individual features by looking at properties of the feature (meta features).",
      "startOffset" : 64,
      "endOffset" : 82
    }, {
      "referenceID" : 13,
      "context" : "Tensor decomposition methods of fixed-order tensors have been used to great effect (Lei et al., 2014; Lei et al., 2015).",
      "startOffset" : 83,
      "endOffset" : 119
    }, {
      "referenceID" : 14,
      "context" : "Tensor decomposition methods of fixed-order tensors have been used to great effect (Lei et al., 2014; Lei et al., 2015).",
      "startOffset" : 83,
      "endOffset" : 119
    } ],
    "year" : 2017,
    "abstractText" : "Hand-engineered feature sets are a well understood method for creating robust NLP models, but they require a lot of expertise and effort to create. In this work we describe how to automatically generate rich feature sets from simple units called featlets, requiring less engineering. Using information gain to guide the generation process, we train models which rival the state of the art on two standard Semantic Role Labeling datasets with almost no task or linguistic insight.",
    "creator" : "LaTeX with hyperref package"
  }
}