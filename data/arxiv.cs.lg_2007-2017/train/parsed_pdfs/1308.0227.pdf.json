{
  "name" : "1308.0227.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Features for Building CSP Portfolio Solvers",
    "authors" : [ "Roberto Amadini", "Maurizio Gabbrielli", "Jacopo Mauro" ],
    "emails" : [ "amadini@cs.unibo.it", "gabbri@cs.unibo.it", "jmauro@cs.unibo.it" ],
    "sections" : [ {
      "heading" : null,
      "text" : "I. INTRODUCTION The past decade has witnessed a significant increase in the number of constraint solving systems deployed for solving Constraint Satisfaction Problems (CSP). It is well recognized within the field of constraint programming that different solvers are better when solving different problem instances, even within the same problem class [14]. It has also been shown in other areas, such as satisfiability testing [34] and integer linear programming [22], that the best on-average solver can be out performed by a portfolio of possibly slower on-average solvers. Essential for the performance of a portfolio solver is the selection of (one of) the solvers composing the portfolio for solving a specific problem instance. Such a selection process is usually performed by using Machine Learning (ML) techniques based on feature data extracted from the instances that need to be solved. Thus, in general, a Portfolio Approach [14] can be seen as a methodology that exploits the significant variety in performances observed in different algorithms and combines these algorithms in order to create a globally better solver. Portfolio approaches have been extensively studied and used in the SAT solving field. Indeed the boolean satisfiability problem, the first known NPcomplete problem, has attracted a lot of attention and, over the last years, its solution has seen a tremendous progress. Problems that were considered to be completely out of reach a decade ago can now be solved by using new algorithms, better heuristics based on an extensive set of features, and refined implementation techniques. Starting from 2002, a competition was held annually to evaluate the performances of different solvers and a big set of real case, randomly generated and handcrafted instances were defined in a standard language (Dimacs format). The large number of different solvers available, the presence of a standard input language and the availability of a huge dataset of instances has fostered the study of how different solvers can be exploited in order to improve performances, thus bringing to the definition of several portfolio solvers for SAT.\nOn the other hand, no such a growth exists in the CSP field, where the only portfolio solver existing so far is CPHydra [29] that exploits a (rather small) set of 36 features extracted by Mistral [4]. There are several reasons for this gap between CSP and SAT. First of all, the CSP solving field is more complex than SAT: constraints can be arbitrary complex (e.g. global constraints like regular or bin-packing) and some of them are supported by only a few solvers. Moreover, no standard input language for CSP exists and there are no immediately available big dataset for constraint solving problems. These limitations affect also the CPHydra portfolio solver: indeed it can treat only problems expressed in the XCSP format [29] and it can extract only a limited number of features from a CSP model.\nIn this paper we address these problems and we perform a first step in the direction of filling the gap between SAT and CSP portfolio solvers. We first present a compiler xcsp2mzn that we developed for converting problem instances from the XCSP format to MiniZinc [28] that is nowadays the most used and supported language to model CSP instances. MiniZinc supports also optimization problems and is the source format used in MiniZinc challenge [32], the only surviving international competition to evaluate the performances of (also) CSP solvers. On the other hand, XCSP [30] is the format used in the International Constraint Solver Competition (ICSC) [5] that was held till 2009. Our compiler xcsp2mzn allows us to consider all the problem instances used in the extensive dataset of the ICSC.\nThe second contribution of this paper is the development of mzn2feat, a tool which allows to extract the features from CSP problems expressed by using MiniZinc (and therefore, by means of the xcsp2mzn translator, also by using XCSP). This is the major contribution of this paper: indeed, to the best of our knowledge mzn2feat is the first tool which allows to extract features from MiniZinc specifications thus improving the state of the art, since MiniZinc is strictly more expressive than XCSP. Our tool extracts 143 (static and dynamic) features thus permitting a fine tuning in the classification. We believe that the use of mzn2feat can foster the development of portfolio techniques in CSP, allowing a better and more accurate selection of the solvers to be used according to the instances that need to be solved. Moreover, mzn2feat is a flexible tool that could be easily adapted and extended for other purposes (e.g. for optimization problems).\nFinally, in order to evaluate the performances of our tool, following the approach presented in [6] we built portfolio solvers consisting of up to 11 solvers taken from those used in the last MiniZinc challenge. We used off-the-shelf machine\nar X\niv :1\n30 8.\n02 27\nv1 [\ncs .A\nI] 1\nA ug\n2 01\n3\nlearning algorithms as well as state of the art portfolio techniques in order to exploit the new extracted features. To obtain an extensive evaluation as possible the tests were conducted by using a dataset obtained by combining the CSP instances of the MiniZinc benchmark with the dataset of the last two ICSCs. Results indicate that the performances that can be obtained using the new set of features are competitive with state of the art CSP portfolios techniques.\nPaper structure. In Section II we recall some preliminary notions and we discuss the related literature. In Section III we describe the technical details of xcsp2mzn and mzn2feat (including also a detailed list of all the features it can extract) while in Section IV we discuss their empirical validation. Finally, in Section V we report some concluding remarks."
    }, {
      "heading" : "II. BACKGROUND",
      "text" : "In this Section we introduce some preliminary notions that we need later in the paper, then we discuss the related work."
    }, {
      "heading" : "A. Preliminaries",
      "text" : "A Constraint Satisfaction Problem (CSP) P = (X ,D, C) consists of a finite set of variables X , each of which associated with a domain Dx ∈ D of possible values that a variable x could take, and a set of constraints C that define the set of allowed assignments of values to the variables [23]. Given a CSP the goal is normally to find a solution, that is an assignment to the variables that satisfies all the constraints of the problem, through one suitable constraint solver.\nMachine Learning (ML) is a broad field that uses concepts from computer science, mathematics, statistics, information theory, complexity theory, biology and cognitive science [25] to “construct computer programs that automatically improve with experience”. In particular, classification is a well-known ML problem that, given a finite number of classes (or categories), consists in identifying to which class belongs each new observation. This problem is solved by using an appropriate classifier which is essentially a function mapping a new instance - characterized by one or more discrete or continuous features - to one class [25]. The classifier is defined on the basis of a training set of instances whose class is already known, trying to exploit such a knowledge to properly classify each new instance.\nAs previously mentioned a portfolio approach [14] is a general methodology that allows to exploits the synergies of different (constraint) solving algorithms in order to obtain a globally better solver. We can then consider a portfolio of solvers as a particular solver S consisting of a number m > 1 of different (constituent) solvers S1, . . . , Sm. When a new problem P is given, the portfolio tries to predict which is the best constituent solver Si for solving the specific problem P and then uses it for solving P . This solver selection process, which is clearly a fundamental part for the success of the approach, is usually performed by using ML techniques. In particular, classification techniques are often used to make predictions on the basis of the features extracted from a relevant set of problem instances. There are also hybrid approaches that integrate ML with other techniques (e.g. integer programming)\nin order to improve the accuracy of the predictions and to maximize the number of solved problems."
    }, {
      "heading" : "B. Related Work",
      "text" : "To the best of our knowledge, CPHydra [29] is currently the only CSP solver which uses a portfolio approach. For the feature extraction it uses the code of Mistral, one of its constituent solvers, that is able to extract only 36 features. In [19] the feature extraction code was improved allowing the extraction of few additional features. The main weakness of CPHydra concerns the fact that it is not scalable w.r.t. the number of the constituent solvers. Moreover it assumes that problem instances are formulated in the XCSP format, which is less expressive (and less used today) than MiniZinc.\nOn the other hand, there are several portfolio solvers for SAT. 3S [17] is a SAT solver that conjugates a fixed-time static solver schedule with the dynamic selection of one longrunning component solver. 3S solves the scalability issues of CPHydra because the scheduling is computed offline and covers only 10% of the time limit. If a given instance is not yet solved after the short runs, a designated solver is chosen at runtime (using a k-nearest neighbors algorithm) and executed.\nSATzilla [34] is a SAT solver that relies on runtime prediction models to select the solver that (hopefully) has the fastest running time on a given problem instance. Its last version [33], which consistently outperforms the previous ones, uses a weighted random forest approach provided with an explicit cost-sensitive loss function punishing misclassifications in direct proportion to their impact on portfolio performance.\nIn [24] the Instance-Specific Algorithm Configuration tool ISAC [18] has been used as solver selector. The aim of ISAC is to optimally tune the solver parameters on the basis of the given instance features, behind the primary assumption that a solver will have consistent performance on instances that are clustered together.\nIn [6] we did an empirical evaluation and comparison of these three portfolio approaches together with CPHydra approach and some other models developed on top of offthe-shelf machine learning algorithms. In order to adapt 3S, SATzilla, and ISAC to CSP field we simulated some portfolio solvers (for CSP) by using the feature extractor of CPHydra and applying their specific techniques. Different portfolio sizes and evaluation metrics were used on a dataset of XCSP instances taken from the last two ICSCs.\nOther recent works show that the interest in algorithm runtime prediction is quite general and growing. A detailed overview of the state of the art in this context is provided in [16] that also describes new features for predicting algorithm runtime for SAT, MIP (Mixed Integer Programming), and TSP (Traveling Salesperson) problems.\nIn [7], [8] ML techniques are used to enhance the performances of a single CSP solver by dynamically adapting its search heuristics. This work lists an extensive set of features to train and improve the heuristics model through Support Vector Machines. Since the goal in that paper is to select the most promising heuristic at given runtime checkpoints, most of the features are dynamic (63 out of 95 features).\nFeature filtering techniques for ISAC tool are described in [21]: instead of using traditional approaches, the authors introduce three new evaluation functions that use precomputed runtimes of a collection of untuned solvers to quickly evaluate subsets of features. Numerical results on both SAT and CSP domains show that the number of features can be significantly reduced while often providing considerable performances gains. Moreover, in [13] the authors introduce SNNAP (Solver-based Nearest Neighbors for Algorithm Portfolios), an alternative view of ISAC which uses the existing features to predict the best three solvers for a particular instance. Using knearest neighbors, this approach scans the training data to find other instances that preferred the same solvers, and uses them as a dynamically formed training set to select the best solver to use. Empirical evidences show that SNNAP can consistently outperform the traditional ISAC methodology.\nFinally, a number of tools are being developed in order to improve portfolio solvers usability. snappy (Simple Neighborhood-based Algorithm Portfolio in PYthon) [31] is a simple and training-less algorithm portfolio which relies on a nearest neighbors prediction mechanism. Its aim is to provide a tool that is effective and at the same time easy to use. Preliminary results show that this simple approach can be competitive with state of the art portfolio solvers for SAT.\nLLAMA (Leveraging Learning to Automatically Manage Algorithm) [20] is a framework that facilitates the exploration of different portfolio techniques on any problem domain. It supports the most common solver selectors, possibly combining them, and offers facilities to build, evaluate and apply portfolio algorithms in practice."
    }, {
      "heading" : "III. TOOLS",
      "text" : "In this section we present the tools that we developed in order to extract the features from a CSP instance, encoded either in the MiniZinc or in the XCSP format. These are a compiler xcsp2mzn for converting XCSP instances to MiniZinc, and a feature extractor mzn2feat, for extracting features from a MiniZinc model.\nA. xcsp2mzn and mzn2feat\nMiniZinc is nowadays the most used language to encode CSPs while XCSP was mainly used in the past for the International Constraint Solver Competition (ICSC), which ended in 2009. Nevertheless, the ICSC dataset is by far the biggest dataset of CSP instances existing today. Hence, in order to exploit such a dataset for building better portfolios, we developed a compiler from XCSP to MiniZinc. xcsp2mzn was developed by adapting x4g [26], a converter from XCSP to Gecode [1] used in particular to support the XCSP abridged notation. Since we focused mainly on CSP we did not consider XCSP extensions like weighted constraints or quantifiers over constraints. All the code is written in C++ using the well known libxml2 libraries.\nExploiting the fact that MiniZinc is more expressive than XCSP (i.e. the majority of the primitive constraint of XCSP are also primitive constraints of MiniZinc) the translation was straightforward. The only notable difference was the\ncompilation of extensional constraints (i.e. relations explicitly expressed in terms of all the allowed or not allowed tuples) which are a native feature in XCSP only. To overcome this limitation we used the table global constraint for encoding the allowed set of tuples and a conjunction of disjunctions of inequalities for mapping the forbidden set of tuples.\nAs far as global constraints are concerned, XCSP supports the majority of the global constraints defined in the Global Constraint Catalog [11]. Since in this catalog there are hundreds of global constraints, a full XCSP support means to provide an encoding for a huge number of them. However, this is out of our scope and we have chosen to support only the subset of the global constraints used in ICSC.\nmzn2feat is a tool that allows to extract from a MiniZinc model a set of 143 features: 132 are static features and are obtained by parsing the problem instance, while 11 are dynamic and are obtained by running the Gecode solver for a short run (for a detailed description of the features please see Section III-B). Since the complexity of the MiniZinc language, in particular the possibility of using control flow statements, makes the extraction of the syntactical features quite difficult, we decided to not process directly the MiniZinc instances. We instead compile them to FlatZinc [27], a lower level language having a syntax that is mostly a subset of MiniZinc and that can be obtained from MiniZinc by using the mzn2fzn tool provided by the MiniZinc suite.\nTo develop mzn2feat we first generated the FlatZinc parser of the MiniZinc suite by using the standard Flex and Bison parser tools. Then, the generated parser was extended by integrating suitable C++ code for extracting the static features (for example the number of constraints, their arity, etc.).\nThe compilation to FlatZinc raised some design choices, since global constraints defined in MiniZinc can be translated in different ways. For example, the alldifferent global constraint is decomposed by default into a conjunction of inequalities. However, if for instance the target solver of the compilation is Gecode, specific definitions can be used to avoid its decomposition. This is a key feature of MiniZinc: starting from a general model each solver can produce a specialized FlatZinc by redefining the global specifications. Since a proper treatment of global constraints can dramatically improve the solver performances, keeping track of how and what global constraints are used is rather important. For this reason we decided to consider as source FlatZinc format the one obtained by using Gecode redefinitions. Such a choice is justified by the fact that Gecode won the gold medal in all categories of the last MiniZinc Challenge [3] and it handles natively 47 different global constraints.1.\nThe specific FlatZinc models obtained in this way are also exploited to collect the dynamic features. This was done by launching Gecode interpreter fz for short runs (2 seconds). Note that while the static features are independent from the\n1Note that mzn2feat can extract the features from every FlatZinc model, the decomposition of certain global constraints may simply constitute a loss of information that could hinder the task of selecting the best solver for that particular instance.\nrunning environment, the dynamic features may vary depending on the particular system used to run Gecode.\nSummarizing, given a generic MiniZinc model M in input, mzn2feat does the following:\n1) it translates M into the corresponding FlatZinc FM specification by using Gecode global redefinitions; 2) it extracts static features from FM by using a suitable parser; 3) it extracts dynamic features from FM by running the fz interpreter of Gecode for 2 seconds.\nSteps 2) and 3) are totally independent and therefore could be parallelized or even reversed. For instance, it could be useless to compute the static features if the given instance is solved by Gecode while trying to compute the dynamic features."
    }, {
      "heading" : "B. Features description",
      "text" : "In this section we present a detailed list of all the 143 numeric features extracted by mzn2feat. We tried to collect a set of features as exhaustive as possible, taking inspiration from and adapting those presented in [16], [8]. Although some of these features are quite generic (e.g., the number of variables or constraints) others are specific to FlatZinc (e.g. search annotations) or to Gecode (the global constraints features). For more details about these technical details we defer the interested reader to [27], [10], [1].\nIn the following we denote by X the set of the unbounded variables and, if not specified, with the term “variable” we refer to unbounded variables. A variable for us is bounded if it is either bounded to a constant value (and in this case it is called constant for short) or it is bounded to another variable (i.e, it is an alias). If x1, . . . , xk are aliases of a variable x, we compute the corresponding features by assuming to replace each xi by the aliased variable x. Moreover, we denote by dom(x) the domain size of a variable x ∈ X , by deg(x) its degree (i.e. the number of constraints c ∈ C in which x occurs) and we define X0 ⊆ X as X0 = {x ∈ X : deg(x) 6= 0}.\nSimilarly, we will denote by C the set of constraints that constrain at least one variable. For each c ∈ C, we denote by V ar(c) the set of variables that occur in c, by deg(c) = |V ar(c)|, dom(c) = log ( ∏ x∈V ar(c) dom(x)), and ari(c) the arity of c (i.e. the number of its arguments; note that deg(c) = ari(c) iff all the variables occurring in c are distinct).\nStatic features We extracted 132 static features grouped in the following different categories. • Variables (27):\n– the number of variables |X |, the number cv of constants, the number av of aliases, the ratio av+cv|X | ,\nthe ratio |X ||C| ; – the number of defined variables (i.e. defined as a\nfunction of other variables), and the number of introduced variables (i.e. auxiliary variables introduced during the FlatZinc conversion);\n– log ( ∏ x∈X dom(x)), log ( ∏\nx∈X0 deg(x)); – ∑ x∈X dom(x), ∑ x∈X deg(x), ∑ x∈X0 dom(x) deg(x) ;\n– minimum, maximum, average, variation coefficient and entropy of {dom(x) : x ∈ X}; – minimum, maximum, average, variation coefficient and entropy of {deg(x) : x ∈ X}; – minimum, maximum, average, variation coefficient and entropy of {dom(x)deg(x) : x ∈ X}. • Domains (18): Since variables could have different domains, we compute the number of:\n– boolean variables bv and the ratio bv|X | ; – float variables fv and the ratio fv|X | ; – integer variables iv and the ratio iv|X | ; – set variables sv and the ratio sv|X | .\nMoreover, we compute the number of: – array constraints ac and the ratio ac|C| ; – boolean constraints bc and the ratio bc|C| ; – int constraints ic and the ratio ic|C| ; – float constraints fc and the ratio fc|C| ; – set constraints sc and the ratio sc|C| .\n• Constraints (27): – the total number of constraints |C|, the ratio |C||X | , the\nnumber of constraints using boundsZ (or bounds), boundsR, boundsD, domain or priority specific FlatZinc annotations;\n– log ( ∏ c∈C dom(c)), log ( ∏\nc∈C deg(c)); – ∑ c∈C dom(c), ∑ c∈C ari(c), ∑ c∈C dom(c) deg(c) ; – minimum, maximum, average, variation coefficient and entropy of {dom(c) : c ∈ C}; – minimum, maximum, average, variation coefficient and entropy of {deg(c) : c ∈ C}; – minimum, maximum, average, variation coefficient and entropy of {dom(c)deg(c) : c ∈ C}. • Global Constraints (29): We consider the total number gc of global constraints, the ratio gc|C| and the number of global constraints for each one of the 27 equivalence classes in which we have grouped the 47 global constraints that Gecode supports. For instance, the class bool lin includes the constraints bool_lin_eq, bool_lin_ne, bool_lin_le, bool_lin_lt, bool_lin_ge, bool_lin_gt while the class circuit contains gecode_circuit, gecode_circuit_cost, gecode_circuit_cost_array. • Graph based (20): In order to capture the interactions between variables and constraints we computed the followings non oriented graphs:\n– Constraint Graph (CG): the graph obtained adding a node for each constraint c ∈ C and an edge between c1 and c2 iff they share at least one variable (i.e. V ar(c1) ∩ V ar(c2) 6= ∅); – Variable Graph (VG): the graph obtained adding a node for each variable x ∈ X and an edge between x1 and x2 iff they occur together in at least one constraint (i.e. ∃c ∈ C. x1, x2 ∈ V ar(c)). Then, starting from the graphs and following [16], we\ncomputed minimum, maximum, average, variation coefficient and entropy of the:\n– CG nodes degree; – CG nodes clustering coefficient; – VG nodes degree; – VG nodes diameter, where by the diameter of a\nnode x we mean the maximum among the minimum distances between x and each other node y 6= x (we set to 0 the diameter of two not connected nodes)2.\nWe noticed that for huge instances the generation of the graphs was time and space consuming. To limit the time and the memory needed to extract the features we have then imposed a timeout of 2 seconds to compute both CG and VG features. In case of timeouts these features where set to the default value of -1. • Solving (11): From the solve goal we extract the following features:\n– the number of labeled variables, i.e. the variables to be assigned; – goal: it can be either 0, 1, or 2 depending on the fact that the goal is satisfy, minimize, or maximize, respectively; – search type: the number of bool_search, int_search, set_search annotations; – variable choice: the number of input order, first_fail, or other heuristics; – value choice: the number of indomain_min, indomain_max, or other heuristics.\nObviously, since in this work we focus only on satisfaction problems, for each considered CSPs the solve goal is always set to 0.\nDynamic features We extracted the following 11 dynamic features:\n1) the number of solutions found; 2) the number p of propagations performed; 3) the ratio p|C| ; 4) the number e of nodes expanded in the search tree; 5) the number f of failed nodes in the search tree; 6) the ratio fe ; 7) the maximum depth of the search stack; 8) the peak memory allocated; 9) the CPU time needed for converting from MiniZinc to\nFlatZinc; 10) the CPU time required for static features computation; 11) the total CPU time needed for extracting all the features. The first 8 features are collected by short runs (2 seconds) of Gecode with default parameters and by using -s and -time options for the fz interpreter. We noticed that in few cases the Gecode solving process ignores the time cap, probably because of a bug. In these cases we forced the interruption of fz after 5 seconds and set the default value to -1 for the 8 first dynamic features.\n2The diameter of the graph is therefore the maximum among the diameters of the nodes in the graph.\nIV. VALIDATION\nMaking a direct comparison between mzn2feat and other similar tools was not possible, since we are not aware of tools that could extract features form MiniZinc models. The only possibility was then to compare mzn2feat with the features extractor developed by Mistral and extended in [19], by using the ICSC dataset. More precisely, we measured and compared the solving time and the number of problem instances solved by different portfolios techniques using, on one hand, the features extracted by mzn2feat and, on the other hand, those extracted by Mistral.\nTaking as reference the methodology and the results of [6] we considered the most promising portfolio techniques and we evaluated their performances using a time limit of 1800 seconds, which is the same threshold used in ICSC. For building portfolios we considered the best performing SAT approaches of [6], namely, SATzilla [34] and 3S [33], and the best off-the-shelf approaches, viz. IBk, J48, PART, Random Forest, and SMO, by using the corresponding WEKA [15] classifiers with default parameters.\nPortfolios were built by using the following 11 different solvers from the MiniZinc Challenge 2012 [3]: BProlog, Fzn2smt, CPX, G12/FD, G12/LazyFD, G12/MIP, Gecode, izplus, MinisatID, Mistral3 and OR-Tools.\nEvery approach was tested by using a 5-repeated 5-fold cross-validation [9]. The dataset was randomly partitioned in 5 disjoint sets called folds. Each of these folds was treated in turn as the test set, considering the union of the 4 remaining folds as training data. In order to avoid a possible overfitting problem (i.e. a portfolio approach that adapts too well on the training data rather than learning and exploiting the generalized pattern) the random generation of the folds was repeated 5 times. For every instance of every test set we computed the solving strategy proposed by the portfolio approach and we simulated it by checking if the solving strategy was able to solve the instance within the time cap. We evaluated the performances of every approach in terms of Average Solving Time (AST) and Percentage of Solved Instances (PSI).\nIn order to compare mzn2feat w.r.t. the features extracted by Mistral we considered the instances of the ICSC successfully compiled into MiniZinc by using Gecode specifications within 900 seconds. When the compilation time exceeds such a limit it is reasonable to assume that one can use directly Gecode to solve the problem instance, since recompiling the MiniZinc model by using the specification of a different solver would end up in wasting the entire time allowed to solve the instance. We also discarded from the dataset all the instances solved by Mistral4 and Gecode in less than 2 seconds. These instances were discarded because no prediction was needed: the problems were already solved during the features extraction. In this way we end up with a benchmark\n3Note that the version of Mistral used in the MiniZinc challenge 2012 is a completely new version of the solver and it is not the one used for extracting the features from XCSP.\n4Here we ran the old version of Mistral, the one used in the ICSC competition.\nconsisting of 2595 instances which in the following will be called Benchmark A.\nMoreover, we have also tested our tool by combining the CSP instances gathered from the MiniZinc benchmark and the instances of the ICSC successfully converted into MiniZinc. As above, we discarded from this second dataset all the instances whose compilation required more than 900 seconds as well as all the instances solved by Gecode in less than 2 seconds. In this case, we ended up with a larger dataset consisting of 4642 instances: 3538 from the ICSC and 1104 from the Minizinc benchmark. In the following, we will refer to this dataset as Benchmark B.\nIn order to evaluate portfolio performances we simulated the execution of the solvers keeping track of the solving time of each solver on every CSP and the time needed for the features extraction. In average, the time needed to compute the features for the instances of Benchmark B (the larger one) was 75.61 seconds, with a maximum value of 897.26 seconds. However, the median is 6.85 seconds, hence for half of the instances the total time required for computing the features was less than 7 seconds. It is worth noting that for the other half of the instances the time needed to extract the features is strongly influenced by the compilation into FlatZinc (in average, the 47.28% of the total extraction time). This is due the fact that many of such instances are huge (even in the order of MB) and therefore their compilation is very expensive. Nevertheless, features extraction was performed in less than a minute for 73.52% of the instances.\nAll the approaches were tested with portfolios of different sizes, considering up to a maximum of 11 solvers. Denoting by Σ the set of all our solvers, for each size n = 2, . . . , 11 the portfolio composition was computed considering the subset Π ⊆ Σ with cardinality n which maximizes the number of solved instances (possible ties were broken by minimizing average solving time).\nWe would also like to note that some solvers were buggy: there were some inconsistencies among solvers outcomes (i.e. in some cases the same CSP is considered satisfiable for certain solvers and unsatisfiable for others). Where possible, we manually fixed the overcomes penalizing the solver errors; however, we leave the issue of automatically handling and correcting wrong outputs as a future work.\nFollowing [6] we elected MinisatID [2] as backup solver, since it is the one that solves the greatest number of instances within the time limit of 1800 seconds.5 The backup solver is used in case the portfolio selects a solver that fails prematurely.\nAll the code developed to conduct the experiments, together with the xcsp2mzn and mzn2feat source code, is available at http://www.cs.unibo.it/˜amadini/ ictai_2013.zip\n5 Thanks to the help of Broes De Cat we were able to use a debugged version of MinisatID w.r.t. the one that has competed in the last MiniZinc challenge. MinisatID was the elections winner by using Approval and Plurality rules, while instead Fzn2smt solver was elected by using Borda [12]. We however decided to use MinisatID as backup solver because we noticed a lot of inconsistencies between the results of Fzn2smt and those of the other solvers."
    }, {
      "heading" : "A. Test Results",
      "text" : "Figure 1 presents the results of the simulated portfolio techniques on Benchmark A by using both Mistral features and the features extracted by mzn2feat. The features for both approaches were normalized in the [−1, 1] range and then used to make predictions. Useless features, i.e. features constants for all the instances of the dataset, were removed. The plot presents the PSI obtained by using 7 different portfolio techniques: the first 5 are off-the-shelf approaches (IBk, J48, PART, Random Forest, SMO) simulated by using WEKA tool with default parameters, while the others (3S and SATzilla) comes from the SAT field. As baseline we used the Virtual Best Solver (VBS), i.e. an oracle able to choose the best solver to use for every instance.\nThe results clearly indicate that the performances achieved by using mzn2feat’s features are competitive w.r.t. those obtained by using Mistral’s features. Usually the difference is rather small even though, for instance, the SMO classifier allows to solve 6.56% of instances more by using our tool. Considering all the tested techniques and all the portfolio sizes, in the worst case we are able to solve only 1.03% less instances while, on average, 0.65% more instances could be solved by using the features extracted by mzn2feat. It is also worth noticing that the peak performances are reached by using 3S approach with the features extracted by our tool.\nPortfolio Techniques\nFigure 2 shows the performances achieved by using the best portfolios techniques and the features extracted by mzn2feat on the most extended Benchmark B. The results are similar to those presented in [6]. Indeed, also in this case the two best portfolios approaches are 3S and SATzilla while the best offthe shelf approaches (namely, Random Forest and SMO) have lower performances. It is worth noticing that we also tried different variants of default WEKA classifiers, obtained by tuning parameters, by using meta-classifiers and by performing features selection; despite this changes we did not observed very significant differences. Differently from [6], where the peak performances were reached by relatively small portfolios (about 6-8 solvers on a total of 16 solvers), here the addition of a solver almost always increases the percentage of solved instances. This is particularly relevant when looking to the off-the-shelf approaches: the performances gap with SATzilla and 3S, initially remarkable, gradually becomes smaller as the number of solvers increases. It is also worth mentioning that the empirical results clearly indicate that a portfolio approach could be far better than a single solver: considering for instance the peak performances of 3S on Benchmark B, we are able solve up to 25.12% instances more than the Single Best Solver (SBS) MinisatID, while the maximum gap w.r.t. the Virtual Best Solver is 7.61%. In particular, 3S is able to close the 89.55% of the gap between SBS and VBS.\nWe conclude this section with some considerations about features preprocessing. We tried to collect a set of features as large as possible, obtaining a number of features that is more than triple of that one of Mistral. In [21] the authors show that by using suitable evaluation functions it is possible to perform a feature filtering that, on one hand, drastically reduces the feature number and, on the other hand, also provides performances gains. On Benchmark B we tried different features selection techniques for all the off-the-shelf approaches by exploiting and tuning some WEKA algorithms (namely BestFirst, GeneticSearch, GreedyStepwise, Ranker). Unfortunately, we have not seen significant improvements: all\nthe performance gains were always below 1%. We have also tried to apply the filtering techniques of [21] exploiting ISAC code; unfortunately, this proved to be too time consuming (filtering of a single fold takes more than a day of computation). However, it is worth noting that merely removing constant features and scaling them in a given range could lead to major enhancements. Consider for instance Figure 3 that shows a comparison between the performances obtained by 3S on Benchmark B, by using normalized and not normalized features. As it can be seen, the difference is considerable: the performance gap that can be obtained by normalizing the features ranges from a minimum of 3.08% to a maximum of 4.68%. This is not so surprising: indeed, k neighborhood search that is used by 3S to select the solver to run for 90% of its time can be severely degraded by the presence of noisy or irrelevant features, or when the feature scales are not consistent with their importance."
    }, {
      "heading" : "V. CONCLUSIONS AND EXTENSIONS",
      "text" : "In this work we presented xcsp2mzn, a compiler for converting CSP instances from XCSP to MiniZinc, and mzn2feat, a flexible and extendible tool for extracting a set of 143 features from a MiniZinc model. We deem that\nthese two tools could serve as a prototype for the creation of a modern CSP solver adopting a portfolio approach. Indeed, to the best of our knowledge the only general purpose CSP solver that exploits portfolio techniques is CPHydra. As previously stated, this solver is rather limited: it process only XCSP instances using a portfolio of just 3 solvers. Moreover, it extracts only 36 features for each XCSP.\nThanks to the features extracted by mzn2feat, it should be pretty straightforward to build a CSP solver capable of exploiting a portfolio of different solvers: the only requirement is that these solvers must support MiniZinc format. As a future work, we are planning to assemble such a solver, possibly enrolling it to a MiniZinc Challenge.\nThe set of features we proposed was tested using the best performing portfolio techniques evaluated in [6]. A comparison with all the other existing techniques is out of the scope of this paper; nevertheless, we are interested in testing further portfolio approaches like the mentioned ISAC, CPHydra, and also brand new techniques like snappy and SNNAP.\nAnother future direction concerns the improvement of the quality of the features. On the one hand, it might be useful to add new and more sophisticated features to our set (SATzilla for instance uses a local search algorithm to compute some dynamic features). On the other hand, predictions accuracy could be significantly improved by reducing the feature number through the use of appropriate filtering techniques.\nOne of the most promising extension of our work is taking into account also Constraint Optimization Problems (COPs). In fact, the MiniZinc syntax of COPs is similar to CSPs: the only significant difference concerns the solve goal that, in case of optimization, defines the integer expression that need to be minimized or maximized. mzn2feat is currently already able to process MiniZinc COPs: there is a specific feature that discriminates whether the goal is to satisfy, minimize or maximize. Nevertheless, with a small effort new features could be devised and extracted by considering for instance the structure and the properties of the expression that must be optimized. It is our opinion that not all the best portfolio technique practices developed for satisfaction problems could be equally good for the optimization field. For instance, assuming that COPs in average require more time than CSPs to reach the optimal solution, an approach like 3S (which first executes short runs of different solvers for the 10% of the time limit) may not be very effective since few COPs could be solved in just 10% of the allowed time. Finally, in order to evaluate a COP solver new metrics should be considered. In fact, often in real world it is better to get a good solution in a short time rather than consume too much time to find the optimal value. Starting from this assumption, it may be reasonable to give each solver a reward proportional to the distance between the solution found and the optimal one."
    } ],
    "references" : [ {
      "title" : "An Empirical Evaluation of Portfolios Approaches for Solving CSPs",
      "author" : [ "R. Amadini", "M. Gabbrielli", "J. Mauro" ],
      "venue" : "CPAIOR",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Online heuristic selection in constraint programming",
      "author" : [ "A. Arbelaez", "Y. Hamadi", "M. Sebag" ],
      "venue" : "SoCS",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Continuous Search in Constraint Programming",
      "author" : [ "A. Arbelaez", "Y. Hamadi", "M. Sebag" ],
      "venue" : "ICTAI",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A survey of cross-validation procedures for model selection",
      "author" : [ "S. Arlot", "A. Celisse" ],
      "venue" : "Statistics Surveys, 4:40–79",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Specification of FlatZinc - Version 1.6",
      "author" : [ "R. Becket" ],
      "venue" : "http://www.minizinc. org/downloads/doc-1.6/flatzinc-spec.pdf. Retrieved July",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2013
    }, {
      "title" : "Global constraint catalogue",
      "author" : [ "N. Beldiceanu", "M. Carlsson", "S. Demassey", "T. Petit" ],
      "venue" : "Past, present and future. Constraints,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2007
    }, {
      "title" : "A short introduction to computational social choice",
      "author" : [ "Y. Chevaleyre", "U. Endriss", "J. Lang", "N. Maudet" ],
      "venue" : "In SOFSEM",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2007
    }, {
      "title" : "SNNAP: Solver-based Nearest Neighbor for Algorithm Portfolios. http://4c.ucc.ie/∼ymalitsky/ Papers/ECML-13.pdf",
      "author" : [ "M. Collautti", "Y. Malitsky", "B. O’Sullivan" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2013
    }, {
      "title" : "Algorithm portfolios",
      "author" : [ "C.P. Gomes", "B. Selman" ],
      "venue" : "Artif. Intell., 126(1- 2):43–62",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "The WEKA data mining software: an update",
      "author" : [ "M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Witten" ],
      "venue" : "SIGKDD Explor. Newsl.,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2009
    }, {
      "title" : "Algorithm Runtime Prediction: The State of the Art",
      "author" : [ "F. Hutter", "L. Xu", "H.H. Hoos", "K. Leyton-Brown" ],
      "venue" : "CoRR, abs/1211.0906",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Algorithm Selection and Scheduling",
      "author" : [ "S. Kadioglu", "Y. Malitsky", "A. Sabharwal", "H. Samulowitz", "M. Sellmann" ],
      "venue" : "CP",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "ISAC - Instance- Specific Algorithm Configuration",
      "author" : [ "S. Kadioglu", "Y. Malitsky", "M. Sellmann", "K. Tierney" ],
      "venue" : "ECAI",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "and B",
      "author" : [ "Z. Kiziltan", "L. Mandrioli", "J. Mauro" ],
      "venue" : "O’Sullivan. A classificationbased approach to managing a solver portfolio for CSPs. In AICS",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "LLAMA: leveraging learning to automatically manage algorithms",
      "author" : [ "L. Kotthoff" ],
      "venue" : "Technical Report arXiv:1306.1031,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2013
    }, {
      "title" : "Feature filtering for instance-specific algorithm configuration",
      "author" : [ "C. Kroer", "Y. Malitsky" ],
      "venue" : "ICTAI",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Learning the Empirical Hardness of Optimization Problems: The Case of Combinatorial Auctions",
      "author" : [ "K. Leyton-Brown", "E. Nudelman", "Y. Shoham" ],
      "venue" : "CP",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Consistency in Networks of Relations",
      "author" : [ "A.K. Mackworth" ],
      "venue" : "Artif. Intell., 8(1):99–118",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1977
    }, {
      "title" : "Instance-Specific Algorithm Configuration as a Method for Non-Model-Based Portfolio Generation",
      "author" : [ "Y. Malitsky", "M. Sellmann" ],
      "venue" : "CPAIOR",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Machine learning",
      "author" : [ "T.M. Mitchell" ],
      "venue" : "McGraw Hill series in computer science. McGraw-Hill",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Solving XCSP problems by using Gecode",
      "author" : [ "M. Morara", "J. Mauro", "M. Gabbrielli" ],
      "venue" : "CILC",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Converting MiniZinc to FlatZinc - Version 1.6",
      "author" : [ "N. Nethercote" ],
      "venue" : "http: //www.minizinc.org/downloads/doc-1.6/mzn2fzn.pdf. Retrieved July",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2013
    }, {
      "title" : "MiniZinc: Towards a Standard CP Modelling Language",
      "author" : [ "N. Nethercote", "P.J. Stuckey", "R. Becket", "S. Brand", "G.J. Duck", "G. Tack" ],
      "venue" : "CP",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Using case-based reasoning in an algorithm portfolio for constraint solving",
      "author" : [ "E. O’Mahony", "E. Hebrard", "A. Holland", "C. Nugent", "B. O’Sullivan" ],
      "venue" : "AICS 08,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2009
    }, {
      "title" : "XML Representation of Constraint Networks: Format",
      "author" : [ "O. Roussel", "C. Lecoutre" ],
      "venue" : "XCSP 2.1. CoRR,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2009
    }, {
      "title" : "Snappy: A simple algorithm portfolio",
      "author" : [ "H. Samulowitz", "C. Reddy", "A. Sabharwal", "M. Sellmann" ],
      "venue" : "SAT, pages 422–428",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Philosophy of the minizinc challenge",
      "author" : [ "P.J. Stuckey", "R. Becket", "J. Fischer" ],
      "venue" : "Constraints, 15(3):307–316",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Evaluating Component Solver Contributions to Portfolio-Based Algorithm Selectors",
      "author" : [ "L. Xu", "F. Hutter", "H. Hoos", "K. Leyton-Brown" ],
      "venue" : "SAT",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "SATzilla-07: The Design and Analysis of an Algorithm Portfolio for SAT",
      "author" : [ "L. Xu", "F. Hutter", "H.H. Hoos", "K. Leyton-Brown" ],
      "venue" : "CP",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "It is well recognized within the field of constraint programming that different solvers are better when solving different problem instances, even within the same problem class [14].",
      "startOffset" : 176,
      "endOffset" : 180
    }, {
      "referenceID" : 28,
      "context" : "It has also been shown in other areas, such as satisfiability testing [34] and integer linear programming [22], that the best on-average solver can be out performed by a portfolio of possibly slower on-average solvers.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 16,
      "context" : "It has also been shown in other areas, such as satisfiability testing [34] and integer linear programming [22], that the best on-average solver can be out performed by a portfolio of possibly slower on-average solvers.",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 8,
      "context" : "Thus, in general, a Portfolio Approach [14] can be seen as a methodology that exploits the significant variety in performances observed in different algorithms and combines these algorithms in order",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 23,
      "context" : "On the other hand, no such a growth exists in the CSP field, where the only portfolio solver existing so far is CPHydra [29] that exploits a (rather small) set of 36 features extracted by Mistral [4].",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 23,
      "context" : "These limitations affect also the CPHydra portfolio solver: indeed it can treat only problems expressed in the XCSP format [29] and it can extract only a limited number of features from a CSP model.",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 22,
      "context" : "We first present a compiler xcsp2mzn that we developed for converting problem instances from the XCSP format to MiniZinc [28] that is nowadays the most used and supported language to model CSP instances.",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 26,
      "context" : "MiniZinc supports also optimization problems and is the source format used in MiniZinc challenge [32], the only surviving international competition to evaluate the performances of (also) CSP solvers.",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 24,
      "context" : "On the other hand, XCSP [30] is the format used in the International Constraint Solver Competition (ICSC) [5] that was held till 2009.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 0,
      "context" : "Finally, in order to evaluate the performances of our tool, following the approach presented in [6] we built portfolio solvers consisting of up to 11 solvers taken from those used in the last MiniZinc challenge.",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 17,
      "context" : "A Constraint Satisfaction Problem (CSP) P = (X ,D, C) consists of a finite set of variables X , each of which associated with a domain Dx ∈ D of possible values that a variable x could take, and a set of constraints C that define the set of allowed assignments of values to the variables [23].",
      "startOffset" : 288,
      "endOffset" : 292
    }, {
      "referenceID" : 19,
      "context" : "Machine Learning (ML) is a broad field that uses concepts from computer science, mathematics, statistics, information theory, complexity theory, biology and cognitive science [25] to “construct computer programs that automatically improve with experience”.",
      "startOffset" : 175,
      "endOffset" : 179
    }, {
      "referenceID" : 19,
      "context" : "This problem is solved by using an appropriate classifier which is essentially a function mapping a new instance - characterized by one or more discrete or continuous features - to one class [25].",
      "startOffset" : 191,
      "endOffset" : 195
    }, {
      "referenceID" : 8,
      "context" : "As previously mentioned a portfolio approach [14] is a",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 23,
      "context" : "To the best of our knowledge, CPHydra [29] is currently the only CSP solver which uses a portfolio approach.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 13,
      "context" : "In [19] the feature extraction code was improved allowing the extraction of few additional features.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 11,
      "context" : "3S [17] is a SAT solver that conjugates a fixed-time static solver schedule with the dynamic selection of one longrunning component solver.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 28,
      "context" : "SATzilla [34] is a SAT solver that relies on runtime prediction models to select the solver that (hopefully) has the fastest running time on a given problem instance.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 27,
      "context" : "Its last version [33], which consistently outperforms the previous ones, uses a weighted random forest approach provided with an explicit cost-sensitive loss function punishing misclassifications in direct proportion to their impact on portfolio performance.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 18,
      "context" : "In [24] the Instance-Specific Algorithm Configuration tool ISAC [18] has been used as solver selector.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 12,
      "context" : "In [24] the Instance-Specific Algorithm Configuration tool ISAC [18] has been used as solver selector.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 0,
      "context" : "In [6] we did an empirical evaluation and comparison of these three portfolio approaches together with CPHydra approach and some other models developed on top of offthe-shelf machine learning algorithms.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 10,
      "context" : "A detailed overview of the state of the art in this context is provided in [16] that also describes new features for predicting algorithm runtime for SAT, MIP (Mixed Integer Programming), and TSP (Traveling Salesperson) problems.",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 1,
      "context" : "In [7], [8] ML techniques are used to enhance the performances of a single CSP solver by dynamically adapting its search heuristics.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 2,
      "context" : "In [7], [8] ML techniques are used to enhance the performances of a single CSP solver by dynamically adapting its search heuristics.",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 15,
      "context" : "Feature filtering techniques for ISAC tool are described in [21]: instead of using traditional approaches, the authors introduce three new evaluation functions that use precomputed runtimes of a collection of untuned solvers to quickly evaluate subsets of features.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 7,
      "context" : "Moreover, in [13] the authors introduce SNNAP (Solver-based Nearest Neighbors for Algorithm Portfolios), an alternative view of ISAC which uses the existing features to predict the best three solvers for a particular instance.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 25,
      "context" : "snappy (Simple Neighborhood-based Algorithm Portfolio in PYthon) [31] is a simple and training-less algorithm portfolio which relies on a nearest neighbors prediction mechanism.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 14,
      "context" : "LLAMA (Leveraging Learning to Automatically Manage Algorithm) [20] is a framework that facilitates the exploration of different portfolio techniques on any problem domain.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 20,
      "context" : "xcsp2mzn was developed by adapting x4g [26], a converter from XCSP to Gecode [1] used in particular to support the XCSP abridged notation.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 5,
      "context" : "As far as global constraints are concerned, XCSP supports the majority of the global constraints defined in the Global Constraint Catalog [11].",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 21,
      "context" : "We instead compile them to FlatZinc [27], a lower level language having a syntax that is mostly a subset of MiniZinc and that can be obtained from MiniZinc by using the mzn2fzn tool provided by the MiniZinc suite.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 10,
      "context" : "We tried to collect a set of features as exhaustive as possible, taking inspiration from and adapting those presented in [16], [8].",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 2,
      "context" : "We tried to collect a set of features as exhaustive as possible, taking inspiration from and adapting those presented in [16], [8].",
      "startOffset" : 127,
      "endOffset" : 130
    }, {
      "referenceID" : 21,
      "context" : "For more details about these technical details we defer the interested reader to [27], [10], [1].",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 4,
      "context" : "For more details about these technical details we defer the interested reader to [27], [10], [1].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 10,
      "context" : "Then, starting from the graphs and following [16], we",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 13,
      "context" : "The only possibility was then to compare mzn2feat with the features extractor developed by Mistral and extended in [19], by using the ICSC dataset.",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 0,
      "context" : "Taking as reference the methodology and the results of [6] we considered the most promising portfolio techniques and we evaluated their performances using a time limit of 1800 seconds, which is the same threshold used in ICSC.",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 0,
      "context" : "For building portfolios we considered the best performing SAT approaches of [6], namely, SATzilla [34] and 3S [33], and the best off-the-shelf approaches, viz.",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 28,
      "context" : "For building portfolios we considered the best performing SAT approaches of [6], namely, SATzilla [34] and 3S [33], and the best off-the-shelf approaches, viz.",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 27,
      "context" : "For building portfolios we considered the best performing SAT approaches of [6], namely, SATzilla [34] and 3S [33], and the best off-the-shelf approaches, viz.",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 9,
      "context" : "IBk, J48, PART, Random Forest, and SMO, by using the corresponding WEKA [15] classifiers with default parameters.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 3,
      "context" : "Every approach was tested by using a 5-repeated 5-fold cross-validation [9].",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 0,
      "context" : "Following [6] we elected MinisatID [2] as backup solver, since it is the one that solves the greatest number of instances within the time limit of 1800 seconds.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 6,
      "context" : "MinisatID was the elections winner by using Approval and Plurality rules, while instead Fzn2smt solver was elected by using Borda [12].",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 5,
      "context" : "For each different portfolio technique, the x-axis is sorted in ascending order according to the number of constituent solvers n ∈ [2, 11].",
      "startOffset" : 131,
      "endOffset" : 138
    }, {
      "referenceID" : 0,
      "context" : "The results are similar to those presented in [6].",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 0,
      "context" : "Differently from [6], where the peak performances were reached by relatively small portfolios (about 6-8 solvers on a total of 16 solvers), here the addition of a solver almost always increases the percentage of solved instances.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 15,
      "context" : "In [21] the authors show that by using suitable evaluation functions it is possible to perform a feature filtering that, on one hand, drastically reduces the feature number and, on the other hand, also provides performances gains.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 15,
      "context" : "We have also tried to apply the filtering techniques of [21] exploiting ISAC code; unfortunately, this proved to be too time consuming (filtering of a single fold takes more than a day of computation).",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 0,
      "context" : "The set of features we proposed was tested using the best performing portfolio techniques evaluated in [6].",
      "startOffset" : 103,
      "endOffset" : 106
    } ],
    "year" : 2017,
    "abstractText" : "Recent research has shown that the performances of a single arbitrarily efficient solver can be significantly outperformed by a portfolio of possibly slower on-average solvers. The solver selection is usually done by using machine learning techniques which exploit features extracted from the problem specification. In this paper we present a tool that is able to extract an extensive set of features from a Constraint Satisfaction Problem (CSP) defined either in the MiniZinc format or in the XCSP format. We also report some empirical results showing that the performances that can be obtained using these features are competitive with state of the art CSP portfolio techniques.",
    "creator" : "LaTeX with hyperref package"
  }
}