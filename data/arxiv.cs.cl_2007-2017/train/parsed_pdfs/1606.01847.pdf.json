{
  "name" : "1606.01847.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding",
    "authors" : [ "Akira Fukui", "Dong Huk Park", "Daylen Yang", "Anna Rohrbach", "Trevor Darrell", "Marcus Rohrbach" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Representation learning for text and images has been extensively studied in recent years. Recurrent neural networks (RNNs) are often used to represent sentences or phrases (Sutskever et al., 2014; Kiros et al.,\n* indicates equal contribution\n2015), and convolutional neural networks (CNNs) are often used to represent images (Donahue et al., 2013; He et al., 2015). For tasks such as visual question answering (VQA), most approaches require joining the representation of both modalities. For combining the two vector representations (multimodal pooling), current approaches in VQA or grounding rely on concatenating vectors or applying the element-wise addition or multiplication. While this generates a joint representation, it might not be expressive enough to fully capture the complex associations between the two different modalities.\nIn this paper, we propose to rely on Multimodal Compact Bilinear pooling (MCB) to get a joint representation. Bilinear pooling computes the outer product between two vectors, which allows, in contrast to element-wise multiplication, a multiplicative interaction between all elements of both vectors. Bilinear pooling models (Tenenbaum and Freeman, 2000) have recently been shown to be beneficial for finegrained classification for vision only tasks (Lin et al., 2015). However, given their high dimensional-\nar X\niv :1\n60 6.\n01 84\n7v 1\n[ cs\n.C V\n] 6\nJ un\nity (n2), bilinear pooling has so far not been widely used. In this paper, we adopt the idea from Gao et al. (2016) which shows how to efficiently compress bilinear pooling for a single modality. In this work, we discuss and extensively evaluate the extension to the multimodal case for text and visual modalities. As shown in Figure 1, Multimodal Compact Bilinear pooling (MCB) is approximated by randomly projecting the image and text representations to a higher dimensional space (using Count Sketch (Charikar et al., 2002)) and then convolving both vectors efficiently by using element-wise multiplication in Fast Fourier Transform (FFT) space. We use MCB to predict answers for the VQA task and locations for the visual grounding task. For open-ended question answering, we present an architecture for VQA which uses MCB twice, once to predict spatial attention and the second time to predict the answer. For multiplechoice question answering we introduce a third MCB to relate the encoded answer to the question-image space. Additionally, we discuss the benefit of multiple attention maps and additional training data for the VQA task. To summarize, MCB is evaluated on two tasks, four datasets, and with a diverse set of ablations and comparisons to the state-of-the-art."
    }, {
      "heading" : "2 Related Work",
      "text" : "Multimodal pooling. Current approaches to multimodal pooling involve element-wise operations or vector concatenation. In the visual question answering domain, a number of models have been proposed. Simpler models such as iBOWIMG baseline (Zhou et al., 2015) use concatenation and fully connected layers to combine the image and question modalities. Stacked Attention Networks (Yang et al., 2015) and Spatial Memory Networks (Xu et al., 2015) use LSTMs or extract soft-attention on the image features, but ultimately use element-wise product or element-wise sums to merge modalities. D-NMN (Andreas et al., 2016a) introduced REINFORCE to dynamically create a network and use element-wise multiplication to joint attentions, and elementwise sum to predict answers. Dynamic Memory Networks (DMN) (Xiong et al., 2016) pool image and question with element-wise multiplication and addition, attending to part of image and question with a Episodic Memory Module (Kumar et al., 2015). DPPnet (Noh\net al., 2015) create a Parameter Prediction Network which learns to predict the parameters of the second last visual recognition layer dynamically from the question. Lu et al. (2016) recently proposed a model that extracts multiple co-attentions on the image and question and combines the co-attentions in a hierarchical manner using all of element-wise sums, concatenation, and fully connected layers.\nFor the visual grounding task Rohrbach et al. (2016) propose an approach where language phrase embedding are concatenated with the visual features in order to predict the attention weights over multiple bounding box proposals. Similarly, Hu et al. (2016a) concatenate phrase embeddings with visual features at different spatial locations to obtain a segmentation.\nBilinear pooling. Bilinear pooling has been applied to the fine-grained visual recognition task. Lin et al. (2015) use two CNNs to extract features from an image and combine the resulting vectors using an outer product, which is fully connected to an output layer. Gao et al. (2016) address the space and time complexity of bilinear features by viewing the bilinear transformation as a polynomial kernel. Pham and Pagh (2013) describes a method to approximate the polynomial kernel using Count Sketches and convolutions.\nJoint multimodal embeddings. In order to model similarity between two modalities many prior works have learned joint multimodal spaces, or embeddings. Some of such embeddings are based on Canonical Correlation Analysis (Hardoon et al., 2004) e.g. (Gong et al., 2014; Klein et al., 2015; Plummer et al., 2015), linear models with ranking loss (Frome et al., 2013; Karpathy and Fei-Fei, 2015; Socher et al., 2014; Weston et al., 2011) or non-linear deep learning models (Kiros et al., 2014; Mao et al., 2015; Ngiam et al., 2011). Our multimodal compact bilinear pooling can be seen as a complementary operation that allows to capture different interactions between two modalities more expressively than e.g. concatenation. Consequently many embedding learning approaches could benefit from incorporating such interactions."
    }, {
      "heading" : "3 Multimodal Compact Bilinear Pooling for Visual and Textual Embeddings",
      "text" : "For the task of visual question answering (VQA) or visual grounding, we have to predict the most likely answer or location â for a given image x and question or phrase q. This can be formulated as\nâ = argmax a∈A\np(a|x,q; θ) (1)\nwith parameters θ and the set of answers or locations A. For an image embedding x = Ξ(x) (i.e. a CNN) and question embedding q = Ω(q) (i.e. an LSTM), we are interested in getting a good joint representation by pooling both representations. With a multimodal pooling Φ(x, q) that encodes the relationship between x and q well, it becomes easier to learn a classifier for Equation (1).\nIn this section, we first discuss our multimodal pooling Φ for combining representations from different modalities into a single representation (Sec. 3.1) and then detail our architectures for VQA (Sec. 3.2) and visual grounding (Sec. 3.3), further explaining how we predict â with the given image representation Ξ and text representation Ω."
    }, {
      "heading" : "3.1 Multimodal Compact Bilinear Pooling (MCB)",
      "text" : "Bilinear models (Tenenbaum and Freeman, 2000) take the outer product of two vectors x ∈ Rn1 and q ∈ Rn2 and learn a model W (here linear), i.e. z = W [x⊗ q], where ⊗ denotes the outer product (xqT ) and [ ] denotes linearizing the matrix in a vector. As discussed in the introduction, bilinear pooling is interesting because it allows all elements of both vectors to interact with each other in a multiplicative way. However, high dimensional representation\nAlgorithm 1 Multimodal Compact Bilinear 1: input: v1 ∈ Rn1 , v2 ∈ Rn2 2: output: Φ(v1, v2) ∈ Rd 3: procedure MCB(v1, v2) 4: for i← 1 . . . 2 do 5: if hi, si uninitialized then 6: for k ← 1 . . . ni do 7: sample hi[k] from {1, . . . , c} 8: sample si[k] from {1,−1} 9: v′i ← Ψ(vi, hi, si) 10: Φ = FFT−1(FFT(v′1) FFT(v′2)) 11: return Φ 12: procedure Ψ(v, h, s) 13: y ← [0, . . . , 0] 14: for j ← 1 . . . n do 15: y[h[j]]← y[h[j]] + s[j] · v[j] 16: return y\n(i.e. when n1 and n2 are large) lead to an infeasible number of parameters to learn in W . For example, we use n1 = n2 = 2048 and z ∈ R3000 for VQA. W thus would have 12.5 billion parameters which leads to very high memory consumption and high computation times.\nWe thus need a method that projects the outer product to a lower dimensional space and also avoids computing the outer product directly. As suggested by Gao et al. (2016) for a single modality, we rely on the count sketch projection function Ψ (Charikar et al., 2002), which projects a vector v ∈ Rn to y ∈ Rd. y is initialized to a vector of d zeros, and every element vi is multiplied by its corresponding value si ∈ {−1, 1} and added to the jth element in y, where h maps indices i in v to indices j in y. The h and s vectors are initialized randomly from uniform distribution, but remain fixed.\nThis allows us to project the outer product to a lower dimensional space, which reduces the number of parameters in W . To not compute the outer product explicitly, Pham and Pagh (2013) showed that the count sketch of the outer product of two vectors can be expressed as convolution of each count sketch: Ψ(x ⊗ q, h, s) = Ψ(x, h, s) ∗ Ψ(q, h, s). Additionally, the convolution theorem states that convolution in the time domain is equivalent to element-wise multiplication in the frequency domain. The convolution x′∗q′ can be rewritten as FFT−1(FFT(x′) FFT(q′)). These ideas are summarized in Figure 2 and formal-\nized in Algorithm 1, which is based on the Tensor Sketch algorithm of Pham and Pagh (2013). We invoke the algorithm with v1 = x and v2 = q."
    }, {
      "heading" : "3.2 Architectures for VQA",
      "text" : "The input to the model is an image and a question, and the goal is to answer the question. Our model extracts representations for the image and the question, pools the vectors using MCB, and arrives at the answer by treating the problem as a multi-class classification problem with 3,000 possible classes.\nWe extract image features using a 152-layer Residual Network (He et al., 2015) that is pretrained on ImageNet data (Deng et al., 2009). Images are resized to 448×448, and we use the output of the layer (“pool5”) before the 1000-way classifier. We then perform L2 normalization on the vector.\nInput questions are first tokenized into words, and the words are one-hot encoded and passed through an embedding layer. The tanh nonlinearity is used after the embedding. The embedding layer is followed by a 2-layer LSTM with 1024 units in each layer. The outputs of each LSTM layer are concatenated to form a 2048-D vector. The two vectors are then passed through MCB. The MCB is followed by an elementwise signed square-root and L2 normalization. After MCB pooling, a fully connected layer connects the resulting 16,000-D multimodal representation to the 3,000 top answers.\nAttention. To incorporate spatial information, we use soft attention on our MCB pooling method. Explored by (Xu et al., 2015) for image captioning and\nby (Xu and Saenko, 2015) and (Yang et al., 2015) for VQA, the soft attention mechanism can be easily integrated to our model.\nFor each spatial grid location in the visual representation (i.e. last convolutional layer of ResNet [res5c], last convolutional layer of VGG [conv5]), we use MCB pooling to merge with the language representation. As depicted in Figure 3, after the pooling we use two convolutional layers to reduce dimensionality and apply softmax to compute weights (soft attention maps) for each grid location. The spatial vectors in the visual representation are then weighted summed to create attention vectors. As we can see from Figure 3, we experiment with generating two attention maps to allow the model to make multiple “glimpses” and these glimpses are concatenated before being merged with the language representation through another MCB pooling for prediction. Generating attention maps directly from the MCB pooling allows the model to effectively learn how to attend to salient locations based on both visual and language representations.\nAnswer Encoding. For VQA with multiple choices, we can additionally embed the answers. For that we base our approach on the proposed MCB with attention. As can be seen from Figure 4, to deal with multiple variable-length answer choices written in natural language, we devise an architecture in which each candidate is encoded using word embedding and LSTM layers whose weights are shared across the candidates. Unlike the MCB with attention, we use an additional MCB pooling to merge the encoded\nanswer choices and the multimodal representation of the original pipeline. The resulting embedding is projected to a classification vector with a dimension equal to the number of answers."
    }, {
      "heading" : "3.3 Architecture for Visual Grounding",
      "text" : "We base our grounding approach on the fullysupervised version of GroundeR (Rohrbach et al., 2016). The overview of our model is shown in Figure 5. The input to the model is a query natural language phrase and an image along with multiple proposal bounding boxes. The goal is to predict a bounding box which corresponds to the query phrase. We replace the concatenation of the visual representation and the encoded phrase in Grounder with MCB to combine both modalities. In contrast to Rohrbach et al. (2016), we include a linear embedding of the visual representation and L2 normalization of both input modalities, instead of batch normalization (Ioffe and Szegedy, 2015), which we found to be beneficial when using MCB for the grounding task."
    }, {
      "heading" : "4 Evaluation on Visual Question Answering",
      "text" : "We evaluate the benefit of MCB with a diverse set of ablations on two visual question answering datasets."
    }, {
      "heading" : "4.1 Datasets",
      "text" : "The Visual Question Answering real-image dataset (Antol et al., 2015) consists of approximately 200,000 MSCOCO images (Lin et al., 2014), with 3 questions per image and 10 answers per question. There are 3 data splits: train (80K images), validation (40K images), and test (80K images). Additionally, there is a 25% subset of test named test-dev. Ac-\ncuracies for ablation experiments in this paper are reported on the test-dev data split. We use the VQA tool provided by Antol et al. (2015) for evaluation. We conducted most of our experiments on the openended real-image task. In Table 4, we also report our multiple-choice real-image scores.\nThe Visual Genome dataset (Krishna et al., 2016) uses 108,249 images from the intersection of YFCC100M (Thomee et al., 2015) and MSCOCO. For each image, an average of 17 question-answer pairs are collected. There are 1.7 million QA pairs of the 6W question types (what, where, when, who, why, and how).\nThe Visual7W dataset (Zhu et al., 2016) is a part of the Visual Genome. Visual7W adds a 7th which question category to accommodate visual answers, but we only evaluate the models on the Telling task which involves 6W questions. The natural language answers in Visual7W are in a multiple-choice format and each question comes with four answer candidates, with only one being the correct answer. Visual7W is composed of 47,300 images from MSCOCO and there are a total of 139,868 QA pairs."
    }, {
      "heading" : "4.2 Experimental Setup",
      "text" : "We use the Adam solver with = 0.0007, β1 = 0.9, β2 = 0.999. We use dropout after the LSTM layers and in fully connected layers. For all experiments, we train on the VQA train split, validate on the VQA validation split, and report results on the VQA testdev split. We use early stopping: if the validation score does not improve for 50,000 iterations, we stop\ntraining and evaluate the best iteration on test-dev. For the Visual7W task, we use the same hyperparameters and train settings as in the VQA experiments. We use the splits from (Zhu et al., 2016) to train, validate, and test our models. The accuracy was computed using the evaluation code released by (Zhu et al., 2016)."
    }, {
      "heading" : "4.3 Ablation Results",
      "text" : "We compare the performance of non-bilinear and bilinear pooling methods in Table 1. We see that MCB pooling outperforms all non-bilinear pooling methods, such as eltwise sum, concatenation, and eltwise product.\nOne could argue that the compact bilinear method simply has more parameters than the non-bilinear pooling methods, which contributes to its perfor-\nmance. We compensated for this by stacking fully connected layers (with 4096 units per layer, ReLU activation, and dropout) after the non-bilinear pooling methods to increase their number of parameters. However, even with similar parameter budgets, nonbilinear methods could not achieve the same accuracy as the MCB method. For example, the “Concat + FC + FC” pooling method has approximately 40962 + 40962 + 4096 × 3000 ≈ 46 million parameters, which matches the 48 million parameters available in MCB with d = 16000. However, the performance of the “Concat + FC + FC” method is only 57.10% compared to MCB’s 59.83%.\nSection 2 in Table 1 also shows that compact bilinear pooling has no impact on accuracy compared to full bilinear pooling. Section 3 in Table 1 demonstrates that the multimodal compact bilinear layer brings improvements regardless of the image CNN used. We primarily use ResNet-152 in this paper, but MCB also improves performance if VGG-19 is used. Section 4 in Table 1 shows that our soft attention model works best with MCB pooling. In fact, attending to the Concat + FC layer has the same performance as not using attention at all, while attending the MCB layer improves performance by 2.67 points. Table 2 compares different values of d, the output dimensionality of the multimodal compact bilinear feature. Approximating the bilinear feature with a 16,000-D vector yielded the highest accuracy.\nTable 3 presents results for the Visual7W multiplechoice QA task. The MCB with attention model outperforms the previous state-of-the-art by 7.9 points overall and performs better in every category. Finally, Figure 6 shows some example responses to questions by the eltwise product model and the MCB model."
    }, {
      "heading" : "4.4 Comparison to State-of-the-Art",
      "text" : "Table 4 compares our approach with the state-of-theart. Our best performance comes from an ensemble of seven models that use MCB pooling with atten-\ntion. Additionally, we augmented our training data with images and QA pairs from the Visual Genome dataset. The ensemble is 4.4 points above the next best approach on the VQA open-ended task and 4 points above the next best approach on the multiplechoice task (on Test-standard). It is important to note that even without ensembles and additional training data, our “MCB + Att.” model still outperforms the previous state-of-the-art by 2.4 points, with an accuracy of 64.2% versus 61.8% on the open-ended task (Test-dev)."
    }, {
      "heading" : "5 Evaluation on Visual Grounding",
      "text" : ""
    }, {
      "heading" : "5.1 Datasets",
      "text" : "We evaluate our visual grounding approach on two challenging datasets. The first is Flickr30k Entities (Plummer et al., 2015) which consists of 31K images from Flickr30k dataset (Hodosh et al., 2014) with 244K phrases localized with bounding boxes. We follow the experimental setup of prior work, e.g.\n1Plummer et al. (2016) achieve higher accuracy of 50.89% when taking into account box size and color. We believe our approach would also benefit from such additional features.\nwe use the same Selective Search (Uijlings et al., 2013) object proposals and the Fast R-CNN (Girshick, 2015) fine-tuned VGG16 features (Simonyan and Zisserman, 2014), as Rohrbach et al. (2016). The second dataset is ReferItGame (Kazemzadeh et al., 2014), which contains 20K images from IAPR TC12 dataset (Grubinger et al., 2006) with segmented regions from SAIAPR-12 dataset (Escalante et al., 2010) and 120K associated natural language referring expressions. For ReferItGame we follow the experimental setup of Hu et al. (2016b) and rely on their ground-truth bounding boxes extracted around the segmentation masks. We use the Edge Box (Zitnick and Dollár, 2014) object proposals and visual features (VGG16 combined with the spatial features, which encode bounding box relative position) from Hu et al. (2016b)."
    }, {
      "heading" : "5.2 Experimental Setup",
      "text" : "In all experiments we use Adam solver (Kingma and Ba, 2014) with learning rate = 0.0001. The embedding size used in our model is 500 both for visual and language embeddings. In the following we use\nd = 2048 in the MCB pooling, which we found to work best for the visual grounding task.\nThe accuracy is measured as percentage of query phrases which have been localized correctly. The phrase is localized correctly if the predicted bounding box overlaps with the ground-truth bounding box by more than 50% intersection over union (IOU)."
    }, {
      "heading" : "5.3 Results",
      "text" : "Tables 5 and 6 summarize our results in the visual grounding task. We present multiple ablations of our proposed architecture. First we replace the MCB with simple concatenation of the embedded visual feature and the embedded phrase, resulting in 46.5% on the Flickr30k Entities and 25.48% on the ReferItGame datasets. The results can be improved by replacing the concatenation with the elementwise product of both embedded features (47.41% and 27.80%). We can further slightly increase the performance by introducing additional 2048-D convolution after the elementwise product (47.86% and 27.98%). However, even with fewer parameters, our MCB pooling significantly improves over this baseline on both\ndatasets, reaching state-of the-art accuracy of 48.69% on Flickr30k Entities and 28.91% on ReferItGame dataset. Figure 6 (right) shows an example of improved phrase localization."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We propose to rely on Multimodal Compact Bilinear Pooling (MCB) to combine visual and text representations. For visual question answering, our architecture with attention and multiple MCBs gives significant improvements on two VQA datasets compared to state-of-the-art. In the visual grounding task, introducing MCB pooling leads to improved phrase localization accuracy, indicating better interaction between query phrase representations and visual representations of proposal bounding boxes."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported by DARPA, AFRL, DoD MURI award N000141110688, NSF awards IIS1427425 and IIS-1212798, and the Berkeley Vision and Learning Center."
    } ],
    "references" : [ {
      "title" : "Learning to compose neural networks for question answering",
      "author" : [ "Marcus Rohrbach", "Trevor Darrell", "Dan Klein" ],
      "venue" : "In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguis-",
      "citeRegEx" : "Andreas et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Andreas et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural module networks",
      "author" : [ "Marcus Rohrbach", "Trevor Darrell", "Dan Klein" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "citeRegEx" : "Andreas et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Andreas et al\\.",
      "year" : 2016
    }, {
      "title" : "Finding frequent items in data streams",
      "author" : [ "Kevin Chen", "Martin Farach-Colton" ],
      "venue" : "In Automata, languages and programming,",
      "citeRegEx" : "Charikar et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Charikar et al\\.",
      "year" : 2002
    }, {
      "title" : "Decaf: A deep convolutional activation feature for generic visual recognition",
      "author" : [ "Donahue et al.2013] Jeff Donahue", "Yangqing Jia", "Oriol Vinyals", "Judy Hoffman", "Ning Zhang", "Eric Tzeng", "Trevor Darrell" ],
      "venue" : null,
      "citeRegEx" : "Donahue et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Donahue et al\\.",
      "year" : 2013
    }, {
      "title" : "The segmented and annotated iapr tc-12",
      "author" : [ "Carlos A Hernández", "Jesus A Gonzalez", "Aurelio López-López", "Manuel Montes", "Eduardo F Morales", "L Enrique Sucar", "Luis Villaseñor", "Michael Grubinger" ],
      "venue" : null,
      "citeRegEx" : "Escalante et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Escalante et al\\.",
      "year" : 2010
    }, {
      "title" : "Devise: A deep visual-semantic embedding model",
      "author" : [ "Frome et al.2013] Andrea Frome", "Greg S Corrado", "Jon Shlens", "Samy Bengio", "Jeff Dean", "Tomas Mikolov" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Frome et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Frome et al\\.",
      "year" : 2013
    }, {
      "title" : "Compact bilinear pooling",
      "author" : [ "Gao et al.2016] Yang Gao", "Oscar Beijbom", "Ning Zhang", "Trevor Darrell" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "citeRegEx" : "Gao et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2016
    }, {
      "title" : "Fast R-CNN",
      "author" : [ "Ross Girshick" ],
      "venue" : "In Proceedings of the IEEE International Conference on Computer Vision (ICCV)",
      "citeRegEx" : "Girshick.,? \\Q2015\\E",
      "shortCiteRegEx" : "Girshick.",
      "year" : 2015
    }, {
      "title" : "Improving image-sentence embeddings using large weakly annotated photo collections",
      "author" : [ "Gong et al.2014] Yunchao Gong", "Liwei Wang", "Micah Hodosh", "Julia Hockenmaier", "Svetlana Lazebnik" ],
      "venue" : "In Proceedings of the European Conference on Computer Vision (ECCV)",
      "citeRegEx" : "Gong et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Gong et al\\.",
      "year" : 2014
    }, {
      "title" : "The iapr tc-12 benchmark: A new evaluation resource for visual information systems",
      "author" : [ "Paul Clough", "Henning Müller", "Thomas Deselaers" ],
      "venue" : "In International Workshop OntoImage,",
      "citeRegEx" : "Grubinger et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Grubinger et al\\.",
      "year" : 2006
    }, {
      "title" : "Canonical correlation analysis: An overview with application to learning methods",
      "author" : [ "Sandor Szedmak", "John Shawe-Taylor" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Hardoon et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Hardoon et al\\.",
      "year" : 2004
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "He et al.2015] Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : null,
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
      "author" : [ "Hodosh et al.2014] Peter Hodosh", "Alice Young", "Micah Lai", "Julia Hockenmaier" ],
      "venue" : null,
      "citeRegEx" : "Hodosh et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hodosh et al\\.",
      "year" : 2014
    }, {
      "title" : "2016a. Segmentation from natural language expressions",
      "author" : [ "Hu et al.2016a] Ronghang Hu", "Marcus Rohrbach", "Trevor Darrell" ],
      "venue" : null,
      "citeRegEx" : "Hu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2016
    }, {
      "title" : "Natural language object retrieval",
      "author" : [ "Hu et al.2016b] Ronghang Hu", "Huazhe Xu", "Marcus Rohrbach", "Jiashi Feng", "Kate Saenko", "Trevor Darrell" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "citeRegEx" : "Hu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2016
    }, {
      "title" : "A focused dynamic attention model for visual question answering",
      "author" : [ "Shuicheng Yan", "Jiashi Feng" ],
      "venue" : null,
      "citeRegEx" : "Ilievski et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ilievski et al\\.",
      "year" : 2016
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "Ioffe", "Szegedy2015] Sergey Ioffe", "Christian Szegedy" ],
      "venue" : null,
      "citeRegEx" : "Ioffe et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ioffe et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep visual-semantic alignments for generating image descriptions",
      "author" : [ "Karpathy", "Fei-Fei2015] Andrej Karpathy", "Li FeiFei" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "citeRegEx" : "Karpathy et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Karpathy et al\\.",
      "year" : 2015
    }, {
      "title" : "Referit game: Referring to objects in photographs of natural scenes",
      "author" : [ "Vicente Ordonez", "Mark Matten", "Tamara L. Berg" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Kazemzadeh et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kazemzadeh et al\\.",
      "year" : 2014
    }, {
      "title" : "Adam: A method for stochastic optimization. arXiv:1412.6980",
      "author" : [ "Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba" ],
      "venue" : null,
      "citeRegEx" : "Kingma et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2014
    }, {
      "title" : "Multimodal neural language models",
      "author" : [ "Kiros et al.2014] Ryan Kiros", "Ruslan Salakhutdinov", "Rich Zemel" ],
      "venue" : "In Proceedings of the International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Kiros et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kiros et al\\.",
      "year" : 2014
    }, {
      "title" : "Fisher vectors derived from hybrid gaussian-laplacian mixture models for image annotation",
      "author" : [ "Klein et al.2015] Benjamin Klein", "Guy Lev", "Gil Sadeh", "Lior Wolf" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "citeRegEx" : "Klein et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Klein et al\\.",
      "year" : 2015
    }, {
      "title" : "Ask me anything: Dynamic memory networks for natural language processing",
      "author" : [ "Kumar et al.2015] Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher" ],
      "venue" : null,
      "citeRegEx" : "Kumar et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2015
    }, {
      "title" : "Microsoft coco: Common objects in context",
      "author" : [ "Lin et al.2014] Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Dollár", "C Lawrence Zitnick" ],
      "venue" : "In Proceedings of the European Conference on Computer Vision (ECCV)",
      "citeRegEx" : "Lin et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2014
    }, {
      "title" : "Bilinear cnn models for finegrained visual recognition",
      "author" : [ "Lin et al.2015] Tsung-Yu Lin", "Aruni RoyChowdhury", "Subhransu Maji" ],
      "venue" : "In Proceedings of the IEEE International Conference on Computer Vision (ICCV)",
      "citeRegEx" : "Lin et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2015
    }, {
      "title" : "Hierarchical Co-Attention for Visual Question Answering",
      "author" : [ "Lu et al.2016] J. Lu", "J. Yang", "D. Batra", "D. Parikh" ],
      "venue" : null,
      "citeRegEx" : "Lu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2016
    }, {
      "title" : "Ask Your Neurons: A Deep Learning Approach to Visual Question Answering",
      "author" : [ "M. Rohrbach", "M. Fritz" ],
      "venue" : null,
      "citeRegEx" : "Malinowski et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Malinowski et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep captioning with multimodal recurrent neural networks (m-rnn)",
      "author" : [ "Mao et al.2015] Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Zhiheng Huang", "Alan Yuille" ],
      "venue" : "In Proceedings of the International Conference on Learning Representations",
      "citeRegEx" : "Mao et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mao et al\\.",
      "year" : 2015
    }, {
      "title" : "Multimodal deep learning",
      "author" : [ "Ngiam et al.2011] Jiquan Ngiam", "Aditya Khosla", "Mingyu Kim", "Juhan Nam", "Honglak Lee", "Andrew Y Ng" ],
      "venue" : "In Proceedings of the International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Ngiam et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Ngiam et al\\.",
      "year" : 2011
    }, {
      "title" : "Image question answering using convolutional neural network with dynamic parameter prediction",
      "author" : [ "Noh et al.2015] Hyeonwoo Noh", "Paul Hongsuck Seo", "Bohyung Han" ],
      "venue" : null,
      "citeRegEx" : "Noh et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Noh et al\\.",
      "year" : 2015
    }, {
      "title" : "Fast and scalable polynomial kernels via explicit feature maps",
      "author" : [ "Pham", "Pagh2013] Ninh Pham", "Rasmus Pagh" ],
      "venue" : "In Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
      "citeRegEx" : "Pham et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Pham et al\\.",
      "year" : 2013
    }, {
      "title" : "Flickr30k entities: Collecting region-to-phrase correspondences for richer image-tosentence models",
      "author" : [ "Liwei Wang", "Chris Cervantes", "Juan Caicedo", "Julia Hockenmaier", "Svetlana Lazebnik" ],
      "venue" : null,
      "citeRegEx" : "Plummer et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Plummer et al\\.",
      "year" : 2015
    }, {
      "title" : "Flickr30k entities: Collecting region-to-phrase correspondences for richer image-tosentence models. arXiv:1505.04870v3",
      "author" : [ "Liwei Wang", "Chris Cervantes", "Juan Caicedo", "Julia Hockenmaier", "Svetlana Lazebnik" ],
      "venue" : null,
      "citeRegEx" : "Plummer et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Plummer et al\\.",
      "year" : 2016
    }, {
      "title" : "Grounding of textual phrases in images",
      "author" : [ "Marcus Rohrbach", "Ronghang Hu", "Trevor Darrell", "Bernt Schiele" ],
      "venue" : null,
      "citeRegEx" : "Rohrbach et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Rohrbach et al\\.",
      "year" : 2016
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "Simonyan", "Zisserman2014] Karen Simonyan", "Andrew Zisserman" ],
      "venue" : null,
      "citeRegEx" : "Simonyan et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Simonyan et al\\.",
      "year" : 2014
    }, {
      "title" : "Grounded compositional semantics for finding",
      "author" : [ "Andrej Karpathy", "Quoc V Le", "Christopher D Manning", "Andrew Y Ng" ],
      "venue" : null,
      "citeRegEx" : "Socher et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2014
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Oriol Vinyals", "Quoc V. V Le" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "Sutskever et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Separating style and content with bilinear models",
      "author" : [ "Tenenbaum", "Freeman2000] Joshua B Tenenbaum", "William T Freeman" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Tenenbaum et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Tenenbaum et al\\.",
      "year" : 2000
    }, {
      "title" : "The new data and new challenges in multimedia research",
      "author" : [ "Thomee et al.2015] Bart Thomee", "David A. Shamma", "Gerald Friedland", "Benjamin Elizalde", "Karl Ni", "Douglas Poland", "Damian Borth", "Li-Jia Li" ],
      "venue" : null,
      "citeRegEx" : "Thomee et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Thomee et al\\.",
      "year" : 2015
    }, {
      "title" : "Selective search for object recognition",
      "author" : [ "Koen EA van de Sande", "Theo Gevers", "Arnold WM Smeulders" ],
      "venue" : null,
      "citeRegEx" : "Uijlings et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Uijlings et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning deep structure-preserving image-text embeddings",
      "author" : [ "Wang et al.2016] Liwei Wang", "Yin Li", "Svetlana Lazebnik" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "citeRegEx" : "Wang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Wsabie: Scaling up to large vocabulary image annotation",
      "author" : [ "Weston et al.2011] Jason Weston", "Samy Bengio", "Nicolas Usunier" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "Weston et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Weston et al\\.",
      "year" : 2011
    }, {
      "title" : "Ask Me Anything: Free-form Visual Question Answering Based on Knowledge from External Sources",
      "author" : [ "Wu et al.2016] Qi Wu", "Peng Wang", "Chunhua Shen", "Anton van den Hengel", "Anthony Dick" ],
      "venue" : "In Proc. IEEE Conf. Computer Vision Pattern Recognition",
      "citeRegEx" : "Wu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2016
    }, {
      "title" : "Dynamic memory networks for visual and textual question answering",
      "author" : [ "Xiong et al.2016] Caiming Xiong", "Stephen Merity", "Richard Socher" ],
      "venue" : null,
      "citeRegEx" : "Xiong et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Xiong et al\\.",
      "year" : 2016
    }, {
      "title" : "Ask, attend and answer: Exploring questionguided spatial attention for visual question answering",
      "author" : [ "Xu", "Saenko2015] Huijuan Xu", "Kate Saenko" ],
      "venue" : null,
      "citeRegEx" : "Xu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    }, {
      "title" : "Show, attend and tell: Neural image caption generation with visual attention",
      "author" : [ "Xu et al.2015] Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio" ],
      "venue" : "Proceedings of the International Conference on Machine",
      "citeRegEx" : "Xu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    }, {
      "title" : "Stacked attention networks for image question answering",
      "author" : [ "Yang et al.2015] Zichao Yang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Smola" ],
      "venue" : null,
      "citeRegEx" : "Yang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2015
    }, {
      "title" : "Visual7W: Grounded Question Answering in Images",
      "author" : [ "Zhu et al.2016] Yuke Zhu", "Oliver Groth", "Michael Bernstein", "Li Fei-Fei" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "citeRegEx" : "Zhu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2016
    }, {
      "title" : "Edge boxes: Locating object proposals from edges",
      "author" : [ "Zitnick", "Dollár2014] C Lawrence Zitnick", "Piotr Dollár" ],
      "venue" : "In Proceedings of the European Conference on Computer Vision (ECCV),",
      "citeRegEx" : "Zitnick et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zitnick et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "2015), and convolutional neural networks (CNNs) are often used to represent images (Donahue et al., 2013; He et al., 2015).",
      "startOffset" : 83,
      "endOffset" : 122
    }, {
      "referenceID" : 11,
      "context" : "2015), and convolutional neural networks (CNNs) are often used to represent images (Donahue et al., 2013; He et al., 2015).",
      "startOffset" : 83,
      "endOffset" : 122
    }, {
      "referenceID" : 24,
      "context" : "Bilinear pooling models (Tenenbaum and Freeman, 2000) have recently been shown to be beneficial for finegrained classification for vision only tasks (Lin et al., 2015).",
      "startOffset" : 149,
      "endOffset" : 167
    }, {
      "referenceID" : 2,
      "context" : "As shown in Figure 1, Multimodal Compact Bilinear pooling (MCB) is approximated by randomly projecting the image and text representations to a higher dimensional space (using Count Sketch (Charikar et al., 2002)) and then convolving both vectors efficiently by using element-wise multiplication in Fast Fourier Transform (FFT) space.",
      "startOffset" : 188,
      "endOffset" : 211
    }, {
      "referenceID" : 5,
      "context" : "In this paper, we adopt the idea from Gao et al. (2016) which shows how to efficiently compress bilinear pooling for a single modality.",
      "startOffset" : 38,
      "endOffset" : 56
    }, {
      "referenceID" : 46,
      "context" : "Stacked Attention Networks (Yang et al., 2015) and Spatial Memory Networks (Xu et al.",
      "startOffset" : 27,
      "endOffset" : 46
    }, {
      "referenceID" : 44,
      "context" : ", 2015) and Spatial Memory Networks (Xu et al., 2015) use LSTMs or extract soft-attention on the image features, but ultimately use element-wise product or element-wise sums to merge modalities.",
      "startOffset" : 36,
      "endOffset" : 53
    }, {
      "referenceID" : 43,
      "context" : "Dynamic Memory Networks (DMN) (Xiong et al., 2016) pool image and question with element-wise multiplication and addition, attending to part of image and question with a Episodic Memory Module (Kumar et al.",
      "startOffset" : 30,
      "endOffset" : 50
    }, {
      "referenceID" : 22,
      "context" : ", 2016) pool image and question with element-wise multiplication and addition, attending to part of image and question with a Episodic Memory Module (Kumar et al., 2015).",
      "startOffset" : 149,
      "endOffset" : 169
    }, {
      "referenceID" : 29,
      "context" : "DPPnet (Noh et al., 2015) create a Parameter Prediction Network which learns to predict the parameters of the second last visual recognition layer dynamically from the question.",
      "startOffset" : 7,
      "endOffset" : 25
    }, {
      "referenceID" : 0,
      "context" : "D-NMN (Andreas et al., 2016a) introduced REINFORCE to dynamically create a network and use element-wise multiplication to joint attentions, and elementwise sum to predict answers. Dynamic Memory Networks (DMN) (Xiong et al., 2016) pool image and question with element-wise multiplication and addition, attending to part of image and question with a Episodic Memory Module (Kumar et al., 2015). DPPnet (Noh et al., 2015) create a Parameter Prediction Network which learns to predict the parameters of the second last visual recognition layer dynamically from the question. Lu et al. (2016) recently proposed a model that extracts multiple co-attentions on the image and question and combines the co-attentions in a hierarchical manner using all of element-wise sums, concatenation, and fully connected layers.",
      "startOffset" : 7,
      "endOffset" : 589
    }, {
      "referenceID" : 13,
      "context" : "Similarly, Hu et al. (2016a) concatenate phrase embeddings with visual features at different spatial locations to obtain a segmentation.",
      "startOffset" : 11,
      "endOffset" : 29
    }, {
      "referenceID" : 22,
      "context" : "Lin et al. (2015) use two CNNs to extract features from an image and combine the resulting vectors using an outer product, which is fully connected to an output layer.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 6,
      "context" : "Gao et al. (2016) address the space and time complexity of bilinear features by viewing the bilinear transformation as a polynomial kernel.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 6,
      "context" : "Gao et al. (2016) address the space and time complexity of bilinear features by viewing the bilinear transformation as a polynomial kernel. Pham and Pagh (2013) describes a method to approximate the polynomial kernel using Count Sketches and convo-",
      "startOffset" : 0,
      "endOffset" : 161
    }, {
      "referenceID" : 10,
      "context" : "Some of such embeddings are based on Canonical Correlation Analysis (Hardoon et al., 2004) e.",
      "startOffset" : 68,
      "endOffset" : 90
    }, {
      "referenceID" : 8,
      "context" : "(Gong et al., 2014; Klein et al., 2015; Plummer et al., 2015), linear models with ranking loss (Frome et al.",
      "startOffset" : 0,
      "endOffset" : 61
    }, {
      "referenceID" : 21,
      "context" : "(Gong et al., 2014; Klein et al., 2015; Plummer et al., 2015), linear models with ranking loss (Frome et al.",
      "startOffset" : 0,
      "endOffset" : 61
    }, {
      "referenceID" : 31,
      "context" : "(Gong et al., 2014; Klein et al., 2015; Plummer et al., 2015), linear models with ranking loss (Frome et al.",
      "startOffset" : 0,
      "endOffset" : 61
    }, {
      "referenceID" : 5,
      "context" : ", 2015), linear models with ranking loss (Frome et al., 2013; Karpathy and Fei-Fei, 2015; Socher et al., 2014; Weston et al., 2011) or non-linear deep learning models (Kiros et al.",
      "startOffset" : 41,
      "endOffset" : 131
    }, {
      "referenceID" : 35,
      "context" : ", 2015), linear models with ranking loss (Frome et al., 2013; Karpathy and Fei-Fei, 2015; Socher et al., 2014; Weston et al., 2011) or non-linear deep learning models (Kiros et al.",
      "startOffset" : 41,
      "endOffset" : 131
    }, {
      "referenceID" : 41,
      "context" : ", 2015), linear models with ranking loss (Frome et al., 2013; Karpathy and Fei-Fei, 2015; Socher et al., 2014; Weston et al., 2011) or non-linear deep learning models (Kiros et al.",
      "startOffset" : 41,
      "endOffset" : 131
    }, {
      "referenceID" : 20,
      "context" : ", 2011) or non-linear deep learning models (Kiros et al., 2014; Mao et al., 2015; Ngiam et al., 2011).",
      "startOffset" : 43,
      "endOffset" : 101
    }, {
      "referenceID" : 27,
      "context" : ", 2011) or non-linear deep learning models (Kiros et al., 2014; Mao et al., 2015; Ngiam et al., 2011).",
      "startOffset" : 43,
      "endOffset" : 101
    }, {
      "referenceID" : 28,
      "context" : ", 2011) or non-linear deep learning models (Kiros et al., 2014; Mao et al., 2015; Ngiam et al., 2011).",
      "startOffset" : 43,
      "endOffset" : 101
    }, {
      "referenceID" : 2,
      "context" : "(2016) for a single modality, we rely on the count sketch projection function Ψ (Charikar et al., 2002), which projects a vector v ∈ Rn to y ∈ Rd.",
      "startOffset" : 80,
      "endOffset" : 103
    }, {
      "referenceID" : 5,
      "context" : "As suggested by Gao et al. (2016) for a single modality, we rely on the count sketch projection function Ψ (Charikar et al.",
      "startOffset" : 16,
      "endOffset" : 34
    }, {
      "referenceID" : 11,
      "context" : "We extract image features using a 152-layer Residual Network (He et al., 2015) that is pretrained on ImageNet data (Deng et al.",
      "startOffset" : 61,
      "endOffset" : 78
    }, {
      "referenceID" : 44,
      "context" : "Explored by (Xu et al., 2015) for image captioning and by (Xu and Saenko, 2015) and (Yang et al.",
      "startOffset" : 12,
      "endOffset" : 29
    }, {
      "referenceID" : 46,
      "context" : ", 2015) for image captioning and by (Xu and Saenko, 2015) and (Yang et al., 2015) for VQA, the soft attention mechanism can be easily integrated to our model.",
      "startOffset" : 62,
      "endOffset" : 81
    }, {
      "referenceID" : 33,
      "context" : "We base our grounding approach on the fullysupervised version of GroundeR (Rohrbach et al., 2016).",
      "startOffset" : 74,
      "endOffset" : 97
    }, {
      "referenceID" : 33,
      "context" : "We base our grounding approach on the fullysupervised version of GroundeR (Rohrbach et al., 2016). The overview of our model is shown in Figure 5. The input to the model is a query natural language phrase and an image along with multiple proposal bounding boxes. The goal is to predict a bounding box which corresponds to the query phrase. We replace the concatenation of the visual representation and the encoded phrase in Grounder with MCB to combine both modalities. In contrast to Rohrbach et al. (2016), we include a linear embedding of the visual representation and L2 normalization of both input modalities, instead of batch normalization (Ioffe and Szegedy, 2015), which we found to be beneficial when using MCB for the grounding task.",
      "startOffset" : 75,
      "endOffset" : 508
    }, {
      "referenceID" : 23,
      "context" : ", 2015) consists of approximately 200,000 MSCOCO images (Lin et al., 2014), with 3 questions per image and 10 answers per question.",
      "startOffset" : 56,
      "endOffset" : 74
    }, {
      "referenceID" : 38,
      "context" : ", 2016) uses 108,249 images from the intersection of YFCC100M (Thomee et al., 2015) and MSCOCO.",
      "startOffset" : 62,
      "endOffset" : 83
    }, {
      "referenceID" : 47,
      "context" : "The Visual7W dataset (Zhu et al., 2016) is a part of the Visual Genome.",
      "startOffset" : 21,
      "endOffset" : 39
    }, {
      "referenceID" : 47,
      "context" : "We use the splits from (Zhu et al., 2016) to train, validate, and test our models.",
      "startOffset" : 23,
      "endOffset" : 41
    }, {
      "referenceID" : 47,
      "context" : "The accuracy was computed using the evaluation code released by (Zhu et al., 2016).",
      "startOffset" : 64,
      "endOffset" : 82
    }, {
      "referenceID" : 25,
      "context" : "HieCoAtt (Lu et al., 2016) 79.",
      "startOffset" : 9,
      "endOffset" : 26
    }, {
      "referenceID" : 43,
      "context" : "1 DMN+ (Xiong et al., 2016) 80.",
      "startOffset" : 7,
      "endOffset" : 27
    }, {
      "referenceID" : 15,
      "context" : "4 FDA (Ilievski et al., 2016) 81.",
      "startOffset" : 6,
      "endOffset" : 29
    }, {
      "referenceID" : 42,
      "context" : "4 AMA (Wu et al., 2016) 81.",
      "startOffset" : 6,
      "endOffset" : 23
    }, {
      "referenceID" : 46,
      "context" : "4 SAN (Yang et al., 2015) 79.",
      "startOffset" : 6,
      "endOffset" : 25
    }, {
      "referenceID" : 26,
      "context" : "7 AYN (Malinowski et al., 2016) 78.",
      "startOffset" : 6,
      "endOffset" : 31
    }, {
      "referenceID" : 29,
      "context" : "1 DPPnet (Noh et al., 2015) 80.",
      "startOffset" : 9,
      "endOffset" : 27
    }, {
      "referenceID" : 31,
      "context" : "The first is Flickr30k Entities (Plummer et al., 2015) which consists of 31K images from Flickr30k dataset (Hodosh et al.",
      "startOffset" : 32,
      "endOffset" : 54
    }, {
      "referenceID" : 12,
      "context" : ", 2015) which consists of 31K images from Flickr30k dataset (Hodosh et al., 2014) with 244K phrases localized with bounding boxes.",
      "startOffset" : 60,
      "endOffset" : 81
    }, {
      "referenceID" : 13,
      "context" : "30 Hu et al. (2016b) 27.",
      "startOffset" : 3,
      "endOffset" : 21
    }, {
      "referenceID" : 13,
      "context" : "30 Hu et al. (2016b) 27.80 Plummer et al. (2016)1 43.",
      "startOffset" : 3,
      "endOffset" : 49
    }, {
      "referenceID" : 13,
      "context" : "30 Hu et al. (2016b) 27.80 Plummer et al. (2016)1 43.84 Wang et al. (2016) 43.",
      "startOffset" : 3,
      "endOffset" : 75
    }, {
      "referenceID" : 13,
      "context" : "30 Hu et al. (2016b) 27.80 Plummer et al. (2016)1 43.84 Wang et al. (2016) 43.89 Rohrbach et al. (2016) 47.",
      "startOffset" : 3,
      "endOffset" : 104
    }, {
      "referenceID" : 39,
      "context" : "we use the same Selective Search (Uijlings et al., 2013) object proposals and the Fast R-CNN (Girshick, 2015) fine-tuned VGG16 features (Simonyan and Zisserman, 2014), as Rohrbach et al.",
      "startOffset" : 33,
      "endOffset" : 56
    }, {
      "referenceID" : 7,
      "context" : ", 2013) object proposals and the Fast R-CNN (Girshick, 2015) fine-tuned VGG16 features (Simonyan and Zisserman, 2014), as Rohrbach et al.",
      "startOffset" : 44,
      "endOffset" : 60
    }, {
      "referenceID" : 18,
      "context" : "The second dataset is ReferItGame (Kazemzadeh et al., 2014), which contains 20K images from IAPR TC12 dataset (Grubinger et al.",
      "startOffset" : 34,
      "endOffset" : 59
    }, {
      "referenceID" : 9,
      "context" : ", 2014), which contains 20K images from IAPR TC12 dataset (Grubinger et al., 2006) with segmented regions from SAIAPR-12 dataset (Escalante et al.",
      "startOffset" : 58,
      "endOffset" : 82
    }, {
      "referenceID" : 6,
      "context" : ", 2013) object proposals and the Fast R-CNN (Girshick, 2015) fine-tuned VGG16 features (Simonyan and Zisserman, 2014), as Rohrbach et al. (2016). The second dataset is ReferItGame (Kazemzadeh et al.",
      "startOffset" : 45,
      "endOffset" : 145
    }, {
      "referenceID" : 13,
      "context" : "For ReferItGame we follow the experimental setup of Hu et al. (2016b) and rely on their ground-truth bounding boxes extracted around the segmentation masks.",
      "startOffset" : 52,
      "endOffset" : 70
    }, {
      "referenceID" : 13,
      "context" : "For ReferItGame we follow the experimental setup of Hu et al. (2016b) and rely on their ground-truth bounding boxes extracted around the segmentation masks. We use the Edge Box (Zitnick and Dollár, 2014) object proposals and visual features (VGG16 combined with the spatial features, which encode bounding box relative position) from Hu et al. (2016b).",
      "startOffset" : 52,
      "endOffset" : 352
    } ],
    "year" : 2017,
    "abstractText" : "Modeling textual or visual information with vector representations trained from large language or visual datasets has been successfully explored in recent years. However, tasks such as visual question answering require combining these vector representations with each other. Approaches to multimodal pooling include element-wise multiplication or addition, as well as concatenation of the visual and textual representations. We hypothesize that these methods are not as expressive as an outer product of the visual and textual vectors. As the outer product is typically infeasible due to its high dimensionality, we instead propose utilizing Multimodal Compact Bilinear pooling (MCB) to efficiently and expressively combine multimodal features. We extensively evaluate MCB on the visual question answering and grounding tasks. We consistently show the benefit of MCB over ablations without MCB. For visual question answering, we present an architecture which uses MCB twice, once for predicting attention over spatial features and again to combine the attended representation with the question representation. This model outperforms the state-of-the-art on the Visual7W dataset and the VQA challenge.",
    "creator" : "LaTeX with hyperref package"
  }
}