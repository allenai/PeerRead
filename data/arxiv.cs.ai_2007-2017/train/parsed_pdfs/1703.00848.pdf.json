{
  "name" : "1703.00848.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Unsupervised Image-to-Image Translation Networks",
    "authors" : [ "Ming-Yu Liu", "Thomas Breuel", "Jan Kautz" ],
    "emails" : [ "MINGYUL@NVIDIA.COM", "TBREUEL@NVIDIA.COM", "JKAUTZ@NVIDIA.COM" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Mapping images from one domain to another (image-toimage translation) has a wide range of applications. One may translate an image in a modality that is difficult to understand to a corresponding color image for better visualization; one may translate labeled images in one domain to corresponding images in a target domain, creating a training dataset that could be used for training classifiers in the target domain.\nMost of the existing image-to-image translation approaches\nare based on supervised learning. They require training datasets consisting of pairs of corresponding images in two domains (Ledig et al., 2016; Isola et al., 2016). Although they can achieve good performance in several image translation tasks (e.g. super-resolution and colorization), the required image pairs are difficult to obtain for many applications; certain applications require complex setups, involving multiple sensors, mirrors, and beam splitters, while other applications may require alterations to the apperances of human subjects.\nTo address this limitation, we propose using the UNsupervised Image-to-image Translation (UNIT) network framework to learn a two-way translation function between two image domains. Instead of requiring corresponding image pairs, we merely assume a dataset of images from each domain. The framework is based on recent deep generative models, including generative adversarial networks (GANs) and variational autoencoders (VAEs). We model each image domain using a VAE and a GAN. Through an adversarial training objective, an image fidelity function is implicitly defined for each domain. The adversarial training objective interacts with a weight-sharing constraint to generate corresponding images in two domains, while the variational autoencoders relate translated images with input images in the respective domains. Through visualization results from various unsupervised image translation tasks, we verify the effectiveness of the proposed framework. An ablation study and a hyperparameter sensitivity test reveal the critical design choices. Finally, we apply UNIT to the unsupervised domain adaptation task and achieve better results than competing algorithms do in the benchmark datasets."
    }, {
      "heading" : "2. Mathematical Motivation",
      "text" : "Let X1 and X2 be two different image domains. In a supervised image-to-image translation problem, we are given training samples (x1, x2) drawn from a joint distribution PX1,X2(x1, x2). In the unsupervised setting, we are given training samples only from the marginal distributions PX1(x1) and PX2(x2). Without any other assumptions, we could infer nothing about the joint distribution from the marginal distributions.\nar X\niv :1\n70 3.\n00 84\n8v 1\n[ cs\n.C V\n] 2\nM ar\n2 01\n7\nIn order to illustrate the motivation and idea behind our approach, let us consider a very simple model in which a nearly 1-1 correspondence between images in the two domains exists. (Of course, this is not generally true, such as the correspondence between thermal and RGB images, so we will talk about general cases in the next section.)\nRecall that we are trying to discover the relationship between two related image domains X1 and X2. Under the assumption of near perfect 1-1 correspondences between images in these two domains, given x1 ∈ X1 and x2 ∈ X2 a function φ1→2 such that x2 ≈ φ1→2(x1), and an inverse function φ2→1 for the reverse mapping x1 = φ2→1(x2) exist. For the joint density, this means that PX1,X2(x1, x2) ≈ PX1(x1)δ(x2−φ1→2(x1)), where δ is the Dirac delta function (this is reminiscent of the constraint used in (Viola & Wells III, 1997) for cross-domain image alignment). However, merely postulating a functional relationship (or even a smooth functional relationship) is not enough to infer a useful correspondence between X1 and X2 from samples drawn from the marginal distributions. We therefore require additional assumptions.\nFirst, we assume that the relationship between X1 and X2 does not only exist at the image level but also at the level of local patches or regions. Similar to other deep networks, our model realizes this assumption through the use of convolutional layers. Then, we make an additional, strong assumption—for any given images x1 and x2, there exists a common underlying representation z, such that we can recover both images from this underlying representation, and that we can compute this underlying representation from each of the two input images. That is, we postulate that\nthere exist functions E1 E2 G1 and G2 such that, given a sample (x1, x2) from the joint distribution, z ≈ E1(x1) ≈ E2(x2) and conversely, x1 ≈ G1(z) and x2 ≈ G2(z). Within this model, the function x2 = φ1→2(x1) that maps from domain X1 to domain X2 can then be represented by the composition φ1→2(x1) = E2(G1(x1)).\nIn more general image translation problems, the relationships between images and representations are only approximately functional. That is, a single thermal image may correspond to a whole range of color images, with uncertainty in both specific pixel color values (a kind of noise) and global color assignments. Therefore, instead of simply learning functional relations E1, E2, G1 and G2 between images and representations, we need to learn statistical distributions using a combination of VAEs and GANs. Furthermore, in the absence of supervised training data, we also need to devise an adversarial training procedure.\nIn the next section, we will discuss how we realize the above ideas using the UNIT framework."
    }, {
      "heading" : "3. The UNIT Framework",
      "text" : "We propose the unsupervised image translation (UNIT) network framework for the unsupervised image-to-image translation task. The framework, as illustrated in Figure 1, is motivated by recent deep generative models including variational autoencoders (VAEs) (Kingma & Welling, 2013; Rezende et al., 2014; Larsen et al., 2016) and generative adversarial networks (GANs) (Goodfellow et al., 2014; Liu & Tuzel, 2016). It consists of 6 subnetworks: including two domain image encoders E1 and E2, two domain\nimage generators G1 and G2, and two domain adversarial discriminators D1 and D2. Several ways exist to interpret the roles of the subnetworks as summarize in Table 1. We note that the UNIT network learns two-way translation in one shot. In the following, we first present the UNIT framework and then discuss the design considerations.\nVAEs: The encoder–generator pair {E1, G1} constitutes a VAE for the X1 domain, termed VAE1. For an input image x1 ∈ X1, the VAE1 first maps it to a code in a latent space Z via the encoder E1 and then decodes a random-perturbed version of the code to reconstruct the input image via the generator G1. Following the VAE design in (Kingma & Welling, 2013) we assume the components in the latent space Z are conditionally independent and Gaussian. The encoder outputs a mean vector E1,µ(x1) and a variance vector E1,σ2(x1) where the distribution of the latent code z1 is given by q1(z1|x1) ≡ N (z1|E1,µ(x1), diag(E1,σ2(x1))). The diag operator converts a vector to a diagonal matrix where diagonal elements are the components of the vector. The reconstructed image is x̃1→11 = G1(z1 ∼ q1(z1|x1)). Note that here we abused the notation where we treated the distribution of q1(z1|x1) as a random vector of N (E1,µ(x1), diag(E1,σ2(x1))) and sampled from it. This notation abuse avoided cluttered presentations, and several other instances of such a notation abuse can be found in the rest of the paper.\nSimilarly the pair of {E2, G2} constitutes a VAE for the X2 domain: VAE2. The encoder E2 outputs a mean vector E2,µ(x1) and a variance vector E2,σ2(x1) and the distribution of the latent code z2 is given by q2(z2|x2) ≡ N (z2|E2,µ(x2), diag(E2,σ2(x2))). The reconstructed image is denoted as x̃2→22 = G2(z2 ∼ q2(z2|x2)).\nUtilizing the reparameterization trick (Kingma & Welling, 2013), the non-differentiable sampling operation can be reparameterized as a differentiable arithmetic operation using auxiliary random variables. This reparameterization trick allows us to train the VAE using the standard backprop algorithm. Let η be a random vector with a multivariate Gaussian distribution: η ∼ N (η|0, I). The sampling operation of z1 ∼ q1(z1|x1) can be implemented via z1 = E1,µ(x1) + E1,σ2(x1) ◦ η where ◦ is the Hadamard product. Similarly, the sampling operation of z2 ∼ q2(z2|x2) is implemented as z2 = E2,µ(x2) + E2,σ2(x2) ◦ η.\nWeight-sharing: In order to relate the representations in the two VAEs, we enforce a weight-sharing constraint. We share the weights of the last few layers of E1 and E2 that are responsible for extracting high-level representations of the input images in the two domains based on the intuition that high-level representations of a pair of corresponding images in the two domains should be the same. Similarly, we share the weights of the first few layers of G1 and G2\nthat are responsible for decoding high-level representations for reconstructing the input images1.\nNote that the weight-sharing constraint by itself makes no guarantee that a pair of corresponding images in the two domains will have the same latent code2. The latent codes for a pair of corresponding images are different in general. Even if they were the same, the same latent component may have different semantic meanings in the two domains. Hence, the same latent code could still be decoded to output two unrelated images in the two domains. However, we will show that through adversarial training, a pair of corresponding images in the two domains will be mapped to a common latent code by E1 and E2, respectively, and a latent code will be mapped to a pair of corresponding images in the two domains by G1 and G2, respectively.\nThe shared latent space of X1 and X2 allows us to perform image-to-image translation. We can translate an image x1 in X1 to an image in X2 through applying G2(z1 ∼ q1(z1|x1)). We term such an information processing stream as the image translation stream. Two image translation streams exist in the UNIT framework: X1 → X2 and X2 → X1. The two streams are trained jointly with the image reconstruction streams. Once we ensure that a pair of corresponding images are mapped to a same latent code and a same latent code is decoded to a pair of corresponding images, (x1, G2(z1 ∼ q1(z1|x1))) will form a pair of corresponding images. In other words, the composition ofE1 andG2 functions is our φ1→2 for unsupervised image-to-image translation discussed in Section 2, and the composition of E2 and G1 function is our φ2→1.\nGANs: A UNIT network employs two adversarial discriminators: D1 and D2. For images sampled from the first domain dataset, D1 should output true, while for images generated by G1, it should output false. The images generated by G1 can be either a same domain reconstructed images x̃1→11 = G1(z1 ∼ q1(z1|x1)) or a domain translated im-\n1Note that the relative positions, with respect to the input layers, of the high-level information processing layers in the encoder and decoder subnetworks are different. The former is in the back, while the latter is in the front.\n2In the unsupervised setting, no pair of corresponding images in the two domains exists to train the network to output a same latent code for corresponding images.\nage x̃2→12 = G1(z2 ∼ q2(z2|x2)). Similarly, D2 is trained to output true for images sampled from the second domain dataset and false for images generated from G2. We share the weights of the high-level layers of D1 and D2 based on the same intuition discussed above.\nLearning: Training the UNIT network can be done through jointly solving the learning problems of the VAE1, VAE2, GAN1 and GAN2 for both the image reconstruction streams and the image translation streams:\nmin E1,G1,E2,G2 max D1,D2 LVAE1(E1, G1) + LVAE2(E2, G2)\n+ LGAN1(E1, G1, D1) + LGAN2(E2, G2, D2). (1)\nTraining a VAE is usually done via minimizing a variational upper bound of a negative log-likelihood function. In (1), the VAE object functions are given by\nLVAE1(E1, G1) =λ1Ez1∼q1(z1|x1)[− log pG1(x1|z1)] +λ2KL(q1(z1|x1)||pη(z1)) (2)\nLVAE2(E2, G2) =λ1Ez2∼q2(z2|x2)[− log pG2(x1|z1)] +λ2KL(q2(z2|x2)||pη(z2)) (3)\nwhere the hyper-parameters λ1 and λ2 control the weights of the objective functions and KL stands for the KullbackLeibler (KL) divergence. The KL divergence terms penalize deviation of the distribution of the latent code from the prior distribution. The regularization allows an easy way to sample from the latent space (Kingma & Welling, 2013). We model the conditional distribution pG1 using a Gaussian given by pG1(x1|z1) = 12 exp( −||x1−G1(z1)||2 2 ). Hence, minimizing the negative log-likelihood term is equivalent to minimizing the Euclidean distance between the image and the reconstructed image. The same modeling is applied to pG2 . The prior distribution is pη(z) = N (z|0, I).\nIn (1), the GAN objective functions are given by\nLGAN1(E1, G1, D1) = Ex1∼PX1 [logD1(x1)] + 12Ez1∼q1(z1|x1)[log(1−D1(G1(z1)))] + 12Ez2∼q2(z2|x2)[log(1−D1(G1(z2)))] (4) LGAN2(E2, G2, D2) = Ex2∼PX2 [logD2(x2)] + 12Ez2∼q2(z2|x2)[log(1−D2(G2(z2)))] + 12Ez1∼q1(z1|x1)[log(1−D2(G2(z1)))]. (5)\nThe objective functions in (4) and (5) differ from the standard GAN objective function in that generated images come from two different distributions. For (4), the two distributions are q1(z1|x1) (the distribution of the reconstructed images for the input images in X1) and q2(z2|x2) (the distribution of the translated images for the input images in X2). Optimizing (4) encourages G1 to output images sampled from both of the distributions resembling images from X1. Similarly, optimizing (5) encourages G2 to\noutput images sampled from q1(z1|x1) and q2(z2|x2) resembling images from X2.\nInheriting from GAN, the UNIT training problem is a minimax problem where the optimization is about finding a saddle point. It can be seen as a two player zero-sum game. The first player is a team consisting of the encoders and generators. The second player is a team consisting of the adversarial discriminators. In addition to defeating the second player, the first player has to minimize the VAE losses. We apply an alternating gradient update scheme similar to the one described in (Goodfellow et al., 2014) to solve (1). Specifically, we first apply a gradient ascent step to update D1 and D2 with E1, E2, G1, and G2 fixed. We then apply a gradient descent step to update E1, E2, G1, and G2 with D1 andD2 fixed. The details of the optimization algorithm are given in Algorithm 1 in the appendix.\nTranslation: Once the training is completed, we obtain two image translation functions by assembling a subset of the subnetworks in the UNIT network. We use the function φ1→2(x1) = G2(z1 ∼ q1(z1|x1)) for translating images from X1 to X2, and use the function φ2→1(x2) = G1(z2 ∼ q2(z2|x2)) for translating images from X2 to X1.\nDiscussion: We employ the VAEs in the UNIT framework for the following reasons: 1) VAEs are established generative models. 2) Sampling from the VAE latent space admits a simple form, allowing a seamless integration with GANs (Larsen et al., 2016). 3) Randomness injected in the VAE sampling step is useful for modeling the randomness in image translation: corresponding images with different appearances for the same input image can be generated with different realizations of the random perturbation3. Nevertheless, for the unsupervised image-to-image translation task, only using VAEs (instead of UNIT) would have the shortcoming that none of the terms in the objective function is defined on the translated images—no feedback is available to the image translation streams. Pure VAEs also tend to produce blurry images. In the UNIT framework, the GAN discriminators address these shortcomings. An image generator trained using the GAN loss can generate crisp sharp images (Larsen et al., 2016; Ledig et al., 2016). More importantly, the GAN discriminators are those that provide feedback to train the image translation streams.\nOptimizing the GAN loss alone is insufficient for learning the image-to-image translation function in an unsupervised manner. The weight-sharing constraint is necessary for encouraging a generated image from an image translation stream resembles a translated version of the input image. The weight-sharing constraint sets an information\n3The VAE implementation adopted in the paper is unimodal and cannot be used to model the multi-modal nature of image translation. We hypothesize a multi-modal VAE implementation would be necessary and leave it as the future work.\nbottleneck. It limits the amount of representation power available at the high-level layers in the VAEs. If a VAE latent code represents two images of two different scenes in two different domains, then the two images will have to share the neurons at the high-level layers of the VAEs. Due to the sharing, each of the images will have difficulty to encode sufficient details to deceive the respective GAN discriminator. On the other hand, if the latent code represents two images of the same scene in two different domains, both of the images can utilize more capacity to deceive the discriminators, because many of the high-level concepts of the two images are the same. The information bottleneck gives incentives to generate corresponding images in two domains. Of course, when the capacity of the network is too large, there is no incentive for sharing the representation even with the weight-sharing constraint. In this case, the UNIT network fails to learn the translation function. We verify these points in the experiment section."
    }, {
      "heading" : "4. Implementations",
      "text" : "Stochastic Skip Connections: The encoders in the UNIT network is responsible for mapping images to the latent space representing the image manifold. However, as the encoders get deeper, it becomes more difficult to preserve image details after layers of neural information processing. This results in blurry image reconstruction and translation. To overcome the issue, we apply skip connections to send intermediate image representations from the encoders to the decoders. The skip connections are applied to all the layers where the weights are shared by the two encoders. They creates channels for transmitting image representations of different granularity. Note that we do not apply skip connections to the first few layers of the encoders where the weights are not shared because the representation computed by the layers are unavailable in the test time.\nThe skip connections in the UNIT network represent stochastic sampling operations, which complies with the VAE design principle. Let K be the number of skip connections. Note that the connection from the last encoder layer to the first decoder layer is considered as a skip connection for simplifying the discussion. The representations passed by the kth skip connection in E1 and E2 are random samples drawn from z1,k ∼ q1,k(z1,k|x1) and z2,k ∼ q2,k(z2,k|x2) where q1,k and q2,k are multi-variate Gaussian distributions whose means and covariance are given by the intermediate representations of the encoding layers that the skip connections are originated. The representations passed from the encoders to the decoders are hence concatenations of samples from the set of multivariate Gaussian distributions: z1 ≡ (z1,1, ..., z1,K) and z2 ≡ (z2,1, ..., z2,K).\nSpatial Context: We incorporate spatial-context informa-\ntion for achieving a better image translation performance. For each input image, we create a y-image of the same size. The pixel values in the y-image are normalized ycoordinates where the bottom pixels have value 1 while the top pixels have value -1. The y-image is concatenated to the input image along the channel direction to create the final input image to the encoders as well as to the adversarial discriminators.\nWe trained the UNIT network using ADAM (Kingma & Ba, 2015). We set the learning rate to 0.0002 and momentums to 0.5 and 0.999, respectively, as in (Radford et al., 2016). The hyperparameters in the objective functions were set to λ1 = 0.0001 and λ2 = 0.00001 throughout the experiments. For translating large resolution images (≥480×480), the batch size was set to 2 due to the limited memory size. Otherwise, the batch size was set to 64. We trained the UNIT networks using a Tesla P100 card in an NVIDIA DGX-1 machine. Training was typically done within a day. Our implementation will be made public."
    }, {
      "heading" : "5. Experiments",
      "text" : "We first show the image translation results of the UNIT framework on several unsupervised image-to-image translation tasks. (More results are available in the appendix.) We then quantitatively analyze various design choices through an extensive set of experiments using a toy dataset. Finally, we apply the UNIT framework to the unsupervised domain adaptation task. Throughout the experiments, we emphasize no corresponding images exist in the training datasets for learning the translation funciton.\nIn the first experiment, we trained a UNIT network using the KAIST multispectral pedestrian detection benchmark (Hwang et al., 2015) for translating between daytime and night time images and between thermal IR and RGB images. The KAIST dataset contains several video sequences captured at different areas in a city at different times in a day. Some of the videos were captured at night time while the others were captured at day time. In addition to RGB video sequences, a thermal IR camera was used to capture thermal IR video sequences. The resolution of the RGB and thermal IR images in the videos were 640×512. We operated in a transductive testing setting and translated the images in their original resolution. For the day-time and night-time image translation task, we created two datasets where the first set contained the images extracted from the day-time video sequences and the second set contained the images from the night-time sequences. The dataset sizes were 54,768 and 28,657 images, respectively. We applied the learned UNIT network to translate images in the two sets. The results are shown in Figure 2 and 3. We observed that UNIT translated day-time images to realistic and corresponding night-time images. The street lights were hal-\nlucinated at plausible locations. The UNIT network also achieved good performance on night-time to day-time image translation.\nWe used the day-time video sequences in the KAIST dataset for training a UNIT for thermal IR and RGB image translation. We divided the sequences into two sets. For the first set, we only used the thermal IR images, while for the second set, we only used the RGB images. There were no corresponding thermal IR and RGB images in the two sets. The two sets contained 31,386 and 31,191 images, respectively. We applied the learned UNIT network to translate images in the two domains. The results are shown in Figure 4 and 5. We observed that the translation from thermal IR to RGB images were realistic. The color gradient in the tree region which were not observed in the thermal IR images were in the translated version. The translation from\nRGB to thermal IR images were realistic too. The cloud texture patterns were removed in the generated thermal IR images, since the region has the same thermal signature.\nWe captured two datasets of driving sequences in California for training a UNIT for sunny and rainy image translation. The first set consisted of images captured on sunny days, while the second set consisted of images captured on rainy days. The datasets contained 11,863 and 2,267 images, respectively. We applied the learned UNIT network to translate images in the two domains. The results are shown in Figure 6 and 7. We found that clouds were added to the sky region and the images appear gloomy when translating from sunny to rainy. On the other hand, clouds were replaced with sunshine when translating images from rainy day to sunny day.\nWe used the CelebFaces Attributes dataset (Liu et al., 2015) for translating face images based on attributes. Each face image in the dataset had several attributes, including blond hair, smiling, goatee, and eyeglasses. The face images with an attribute constituted the 1st domain, while those without the attribute constituted the 2nd domain. No corresponding face images between the two domains was given. We resized the images to a resolution of 132×132 and randomly sampled 128× 128 regions for training. For each attribute translation, we trained a UNIT network. In Figure 8 we visualized the results where we translated several images that do not have blond hair, eye glasses, goatee, and smiling to corresponding images with each of the individual attributes. We found that the translated face images were realistic.\nQuantitative Evaluation: Quantitative evaluation of generative models is known to be a challenging task and the popular metrics are all subject to flaws (Theis et al., 2016). Hence, we developed an evaluation protocol tailored for the unsupervised image translation task for studying impact of individual components in the UNIT framework. We used the MNIST dataset to create a toy dataset where the ground truth image translation function between two domains is known. Specifically we partitioned the MNIST training set into two equal-sized disjoint sets. For the digit images in the 1st set, we randomly colored the strokes in either red green or blue. For the images in the 2nd set, we first computed the edge images and then randomly colored the edges in either magenta yellow or cyan. We trained the UNIT network to learn the translation function between the two\ndigit domains. For our evaluation, we translated the images in the MNIST test set from one domain to the other and compared the Euclidean distance between an image that was translated by the learned UNIT translation function to the corresponding image that was translated by the ground truth translation function. Please find experimental details, performance numbers (average Euclidean distance of all the images in the MNIST test set) and an analysis on various design choices in the appendix. We briefly summarize our findings here:\n• Network Capacity: When the capacity of a UNIT network was too low (e.g., a small number of neurons in each layer), the quality of the learned translation function degraded; however, when the capacity was too high, the UNIT network failed to learn the translation function at all. We concluded that setting a proper network capacity is very important for the UNIT framework.\n• Sensitivity to Hyperparameters: We found that UNIT was not sensitive to the learning hyperparameters and a wide range of λ1 and λ2 values (see (2) and (3) for defintions) rendered comparable unsupervised image translation performance.\n• Weight-sharing: We found that applying the weightsharing constraint to the encoders and generators were essential. Without a sufficient number of weight-sharing layers, UNIT failed to the learn the translation function.\n• Ablation Study: We found that E1, E2, G1, G2, D1, andD2 were all essential to the UNIT framework. When removing D1 and D2, the resulting network became a coupled VAE network. It could still learn the translation function for the toy dataset but was inferior and outputted blurry images. When removing other subnetworks, the resulting network failed to learn the image translation function.\nUnsupervised Domain Adaptation (UDA): We applied the UNIT framework to the UDA problem, i.e., adapting a classifier trained using labeled samples in one domain (source domain) to classify samples in a new domain (target domain) where labeled samples in the new domain are unavailable. A UDA algorithm has to leverage unlabeled samples in the new domain for adapting the classifier. Early UDA works have explored ideas from subspace learning (Fernando et al., 2013) to deep learning (Ganin et al., 2016; Liu & Tuzel, 2016; Taigman et al., 2017).\nWe used a multi-task learning approach where we trained a UNIT network to translate images between the source and target domains as well as trained the adversarial discriminator in the source domain to classify samples in the source domain. Due to the weight-sharing constraint enforced in the high-level layers in the adversarial discriminators, the trained adversarial discriminator in the target domain can classify samples in the target domain, inheriting the\npower from the source domain discriminator. We did not use a separately trained source-domain classifier to classify UNIT-translated samples because the multi-task learning approach performed better in practice, possibly due to the use of unlabeled data in the target domain in training.\nWe applied the above approach to the task of adapting a classifier from the Street View House Number (SVHN) dataset (Netzer et al., 2011) to the MNIST dataset. Specifically, we trained the UNIT network to learn to translate images between the SVHN and MNIST training sets as well as to classify the digit classes in the SVHN training images using the features extracted by the SVHN domain adversarial discriminators. During test time, we applied the target domain adversarial discriminator to classify the digit class in the MNIST test set. We reported the achieved performance with comparison to the competing methods in Table 9. We found that our approach achieved a 90.53% accuracy, which was much better than 84.88% achieved by the previous state-of-the-art method (Taigman et al., 2017). The details of the network architecture and additional experiments are available in the supplementary materials."
    }, {
      "heading" : "6. Related Works",
      "text" : "Several deep generative models were recently proposed for image gneration including GANs (Goodfellow et al., 2014), VAEs (Kingma & Welling, 2013; Rezende et al., 2014), moment matching networks (Li et al., 2015), PixelCNN (van den Oord et al., 2016), and Plug&Play Generative Networks (Nguyen et al., 2016). The UNIT framework was based on GANs and VAEs but it was designed for the unsupervised image-to-image translation task. In the following, we first review several recent GAN and VAE works and then discuss related image translation works.\nGANs learn to generate images by setting up a zero-sum game played by a generator and a discriminator. The quality of images generated by GANs had improved dramatically since its introduction. (Denton et al., 2015) proposed a Laplacian pyramid implementation of GANs. (Radford et al., 2016) used a deeper convolution network architecture. (Zhang et al., 2016) stacks two generators to progressively render realistic images. InfoGAN (Chen et al., 2016) learned to a more interpretable latent representation. (Salimans et al., 2016) proposed several GAN training tricks. (Arjovsky et al., 2017) proposed the Wasserstein\nGAN framework for a more stable GAN training.\nVAEs optimize a variational bound of the image likelihood function. By improving the variational approximation, better image generation results were achieved (Maaløe et al., 2016; Kingma et al., 2016). In (Larsen et al., 2016), a VAEGAN architecture was proposed to improve image generation quality of VAEs. VAEs were applied to translate face image attribute in (Yan et al., 2016).\nImage Translation via a conditional generative model is now a popular approach for mapping an image from one domain to the other. Most of the existing works were based on supervised learning (Ledig et al., 2016; Isola et al., 2016), requiring corresponding images in two domains. Our work differed to the previous works in that we do not need corresponding images. Recently, (Taigman et al., 2017) proposed the domain transformation network (DTN) and achieved promising results on translating small resolution face and digit images. In addition to faces and digits, the UNIT network can translate large resolution natural images. It also achieved a better performance in the unsupervised domain adaptation task. In (Shrivastava et al., 2016), a conditional generative adversarial network-based approach was proposed to translate a rendering images to a real image for gaze estimation. In order to ensure the generated real image was similar to the original rendering image, the L1 distance between the generated and original image was minimized. While this approach could generate more realistic rendering images, it is not applicable to the natural image translation task. For example, for the task of converting a thermal image to a color image, the L1 distance between a translated color image and the input thermal image does not carry meaningful senses.\nWe note that image translation is different to style transfer (Gatys et al., 2016; Johnson et al., 2016) in that while style transfer focuses on translate a natural image to an abstract, artistic image, image translation also considers translating a natural image to another natural image where loss of image details is unacceptable."
    }, {
      "heading" : "7. Conclusion and Future Work",
      "text" : "We presented the UNIT framework—a general framework for unsupervised image-to-image translation. We showed it learned to translate an image from one domain to another without any corresponding images in two domains in the training dataset. In future, we plan to extend the framework to deal with the semi-supervised image-to-image translation task where supervision of domain correspondence is given either through a set of rules or a few pairs of corresponding images. We are also interested in extending the framework to the unsupervised language-to-language translation task."
    }, {
      "heading" : "A. Training",
      "text" : "We present the learning algorithm for the UNIT framework in Algorithm 1. The algorithm can be considered as an extension of the learning algorithm for the generative adversarial networks (GAN). The convergence property follows the results shown in (Goodfellow et al., 2014).\nAlgorithm 1 The alternating stochastic gradient update algorithm for training the unsupervised image-to-image translation networks.\n1: Initialize E1 E2 G1 G2 D1 and D2 with the shared network connection weights set to the same values. 2: for t = 0, 1, 2, ...,MAX ITER do 3: for k = 0, 1, 2, ...,K do 4: Draw N samples from the first domain training\ndataset, X1 = {x(1)1 , ..., x (N) 1 }\n5: DrawN samples from the second domain training dataset, X2 = {x(1)2 , ..., x (N) 2 } 6: Draw N samples from pη , Z = {z(1), ..., z(N)} 7: Perform a forward pass of the network using X1, X2, and Z. 8: Perform a backward pass of the network by back-\nprop the gradients from the derivates of\nLGAN1(E1, G1, D1) + LGAN2(E1, G2, D2) (6)\n9: Update D1 and D2 using the gradients. 10: end for 11: Draw N samples from the first domain training\ndataset, X1 = {x(1)1 , ..., x (N) 1 }\n12: Draw N samples from the second domain training dataset, X2 = {x(1)2 , ..., x (N) 2 } 13: Draw N samples from pη , Z = {z(1), ..., z(N)} 14: Perform a forward pass of the network using X1, X2, and Z. 15: Perform a backward pass of the network by back-\nprop the gradients from the derivates of\nLVAE1(E1, G1) + LVAE2(E1, G2) +LGAN1(E1, G1, D1) + LGAN2(E1, G2, D2) (7)\n16: Update E1 E2 G1 and G2 using the gradients. 17: end for"
    }, {
      "heading" : "B. Quantitative Analysis",
      "text" : "We created a toy dataset using the MNIST dataset for analyzing various components in the UNIT framework. We partitioned the MNIST training set into two equal-size disjoint sets. For each handwritten digit image in the first set, we randomly colored the strokes with a color that is either red, green, or blue. For each image in the second set, we first computed the edge map and then randomly colored the\nedges with a color that is either cyan, yellow, or magenta. Some examples images from the two sets are illustrated in Figure 9.\nThe two sets formed two different image domains. Our goal was to translate a digit image from the 1st domain to the corresponding digit image in the 2nd domain and vise versa. Since no corresponding images existed in the two sets, the translation function learning had to be performed in an unsupervised manner. For performance evaluation, we translated the images in the MNIST test set from one domain to the other. For this toy dataset, we knew precisely what the translation function between the two domains was, which allowed us to quantitatively evaluate the performance of a learned translation function4. We computed the Euclidean distance between an image that was translated with the learned UNIT translation function to an image that was translated with the ground truth translation\n4Note that we ignored the color difference as evaluating the translation performance. A digit in red color can perfectly match an edge image of the digit in either cyan, yellow, or magenta as long as the colored portion of the edge image resembles the edges extracted from the colored portion of the digit image.\nfunction. The smaller the Euclidean distance between the two images, the closer the learned translation function to the ground truth translation function. We used the average Euclidean distance between the UNIT translated images and the ground-truth translated images in the test set as the performance metric for performance evaluation.\nSensitivity to Hyperparameters: The weights on the image likelihood terms, λ1, and the KL divergence terms, λ2, are two learning hyperparameters in the UNIT framework. To study their impact on the translation performance, we trained a UNIT network with different hyperparameter values. The encoders, generators, and discriminators used in the experiments were all five-layer convolutional networks. We applied weight-sharing to the last four layers of the encoders, the first three layers of the generators, and the last four layers of the discriminators. The UNIT network had 32 convolutional filters in the first layer and the number of filters for every next layer was doubled. For the toy dataset, we did not find a need to use skip connections. The details of the UNIT network is shown in Table 3.\nIn Figure 10 we fixed λ2 = 0.0001 and plotted the translation error as a function of training iterations with different λ1 values. The λ1 values tested ranged from 0.001 to 100. The UNIT framework learned two-way translation simultaneously. We plotted the error curves for both translation directions. From the figure, we found that the performance of the translation functions did not depend on the hyperparameter values much for the toy dataset. A wide range of parameter values rendered good translation performance. However, we suspected that a hyperparameter search for the weight on the image likelihood term would still be necessary when dealing with natural image translation tasks. In Figure 11, we fixed λ1 = 1 and plotted the translation errors as a function of training iterations with different λ2 values. The λ2 values tested ranged from 0.00001 to 0.1. Similar to the λ1 case, we found that the performance of the translation functions did not depend on the hyperparameter values much. We set λ1 = 1 and λ2 = 0.0001 for the rest of the experiments in the section. Several digit translation results from this set of hyperparameters are visualized in Figure 13.\nNetwork Capacity: The weight-sharing constraint in the UNIT framework limits the capacity of the encoders and generators, which enables UNIT to unsupervisedly learn the target image-to-image translation function. To understand the relation between the capacity and the quality of the learned translation function, we trained a set of UNIT networks with different numbers of convolutional filters. We kept the architecture of the discriminators fixed as in Table 3 (i.e. starting with 32 filters) and varied the number of filters in the encoders and generators. We tested five different versions of the UNIT networks. They started with\n8, 16, 32, 64, and 128 filters in the first encoder layers, respectively. We doubled the number of filters for every next layer in the encoders. The number of convolutional filters in the first layers of the generators was set to equal to the number of convolutional filters in the last layers of the encoders. We halved the number of filters for every next layer in the generators. The UNIT network that had more filters in a convolutional layer had a higher capacity. We quantitatively evaluated the performance of the learned translation functions of the UNIT networks. The results are shown in Figure 12. From the figure, we observed that when the capacity was too low, the learned translation function did not perform well. But when the capacity was too high, the UNIT network failed to learned the translation function completely. As visualizing the translated images by the UNIT networks with high capacity, we found that the translated images were simply duplications of the input images. Figure 12 also showed that the network that starting with 32 convolutional filters rendered the best performance. However, we expected that the optimal number of filters would vary for different image translation tasks.\nWeight-sharing: We conducted the following experiment to understand the role of weight-sharing in the UNIT framework. We trained a set of UNIT networks with different number of weight-sharing layers in the encoders, generators, and discriminators. The networks used in the experiment were all based on the architecture in Table 3. We trained the networks for 10K iterations and reported the performance in Table 4. We observed that the translation performance was invariant to the number of weight-sharing layers in the adversarial discriminators. In contrast, the performance largely depended on the number of weightsharing layers in the encoders and decoders. Generally, with more weight-sharing layers, UNIT learned a better translation function. When the number of weight-sharing layers was too small, UNIT failed to learn the translation function completely.\nAblation Study: We conducted an ablation study to understand the dependency on each of the subnetworks in the UNIT framework. We trained several variants of the UNIT networks with various individual subnetworks removed. The architecture of the networks were all based on Table 3. In one experiment, we removed both of the adversarial discriminators. This rendered a pair of VAEs with a weight sharing constraint, which we refer to as the coupled VAEs. In an another experiment, we removed the encoder in the second image domain. We have also tested removing the generator and adversarial discriminator in the second domain. This broke the symmetry in the UNIT framework and forced the remaining components in the UNIT network to focus on translating images from the second domain to the first domain. We trained these variants using the same hyperparameters and reported the performance in Table 5.\nFrom the table, we found that the UNIT network achieved the best translation performance, while the coupled VAEs was ranked second. Although the coupled VAEs rendered a comparable performance for the toy dataset, it produced blurred images as applied to natural images. We hence do not recommend it for the image translation task. From the table, we also found that the versions of the networks that broke the symmetry in the architecture failed to learn the translation function. We hypothesized that the failure was due to the absent of the weight-sharing constraint."
    }, {
      "heading" : "C. Unsupervised Domain Adaptation",
      "text" : "We applied the UNIT framework to the UDA problem, which concerns adapting a classifier trained using labeled\nsamples in one domain, which is referred to as the source domain, to classify samples in a new domain, which is referred to as the target domain, where labeled samples in the new domain are unavailable. A UDA algorithm has to leverage unlabeled samples in the new domain for adapting the classifier.\nWe developed a multi-task learning approach where we trained a UNIT network to translate images between the source and target domains as well as trained the adversarial discriminator in the source domain in the UNIT network to classify samples in the source domain. Due to the weightsharing constraint enforced in the high-level layers in the adversarial discriminators, the trained adversarial discriminator in the target domain in the UNIT network can classify samples in the target domain, inheriting the classification\nTable 4: Number of weight-sharing layers versus translation performance. The numbers shown in the table are the average Euclidean distance between images translated by the learned UNIT translation function (after 10K iterations of training) and images translated by the ground truth translation function. The smaller the number the better the translation performance. In the x-axis, we vary nE , which denotes the number of weight-sharing layers in E1 and E2. In the y-axis, we vary nG and nD , which are the number of weight-sharing layers in G1 and G2 and the number of weight-sharing layers in D1 and D2, respectively.\nTranslation from nD X1 to X2 4 3 2 1 0\n(n E ,n\nG )\n(4, 3) 59.3 58.4 58.6 65.1 57.3 (3, 2) 67.7 73.0 63.7 64.4 75.9 (2, 1) 126.7 97.6 139.7 138.3 124.5 (1, 0) 180.6 180.9 179.3 182.3 180.1 (0, 0) 186.2 188.8 182.8 183.7 185.6\nTranslation from nD X2 to X1 4 3 2 1 0\n(n E ,n\nG )\n(4, 3) 47.0 49.6 49.2 60.6 46.7 (3, 2) 59.8 66.7 56.1 55.7 69.1 (2, 1) 117.2 86.2 130.4 129.6 113.7 (1, 0) 185.9 185.9 184.4 185.6 184.1 (0, 0) 197.0 197.2 195.3 193.7 194.5\nFigure 13: Visualization of the digit translation results. Left: Translation from the first domain to the second domain. Right: Translation from the second domain to the first domain. For each image pair, the one in the left-hand side is the input image, while the one in the right-hand side is the translated image.\npower from the source domain adversarial discriminator. We did not use a separately trained source-domain classifier to classify UNIT-translated samples because the multitask learning approach performed better in practice, possibly due to the use of unlabeled data in the target domain in training.\nIn the first UDA experiment, we applied the above approach to the task of adapting a classifier between the MNIST dataset and the USPS dataset. For the USPS dataset, we randomly partitioned it into two sets. One for training and the other for testing. The size of the training set was 4 times larger than the the test set. We used a UNIT network where the encoder and decoder architecture were the same as those shown in Table 3. But the adversarial discriminators were based on the LeNet architecture as shown in Table 6. The adversarial discriminators had two different output layers: the first output layer has a single output neuron representing the probability that an input sample was from the training dataset (4a in Table 6) and the other out-\nput layer has 10 output neurons representing the digit class (4b in Table 6). We used the labeled training samples from the source domain to train the classifier and used the unlabeled samples in the target domain as well as the samples in the source domain to learn the image translation function. For adapting a classifier from the MNIST domain to the USPS domain, the training data included the labeled\nMNIST training set and the unlabeled USPS training set. We reported the performance of the adapted classifier in classifying the USPS test set. For adapting a classifier from the USPS domain to the MNIST domain, the training data included the labeled USPS training set and the unlabeled MNIST training set. We reported the performance of the adapted classifier in classifying the MNIST test set. We compared the performance of the proposed method with the CoGAN algorithm (Liu & Tuzel, 2016), which was the state-of-the-art approach for this domain adaptation task. The results are reported in Table 7. From the table, we found that UNIT’s performance was comparable to CoGAN.\nIn the second UDA experiment, we applied the above approach to the task of adapting a classifier from the Street View House Number (SVHN) dataset (Netzer et al., 2011) to the MNIST dataset. Specifically, we trained the UNIT network to learn to translate images between the SVHN and MNIST training sets as well as to classify the digit classes in the SVHN training images using the features extracted from the SVHN domain adversarial discriminators. In the test time, we applied the target domain adversarial discriminator to classify the digit class for the images in the MNIST test set. We used a UNIT network where the encoder and decoder architecture were the same as those shown in Table 3. But the adversarial discriminators were based on a network architecture similar to the one used in as shown in (Taigman et al., 2017). The details of the discriminator architecture is given in Table 8. The adversarial discriminators had two different output layers: the first output layer has a single output neuron representing the probability that an input sample was from the training dataset (5a in Table 8) and the other output layer has 10 output neurons representing the digit class (5b in Table 8). We reported the\nachieved performance with comparison to the competing methods in Table 9. We found that our approach achieved a 90.53% accuracy, which was much better than 84.88% achieved by the previous state-of-the-art method (Taigman et al., 2017)."
    }, {
      "heading" : "D. Network Architecture",
      "text" : "The UNIT architecture for translating natural images presented in the experiment section is shown in Table 10."
    }, {
      "heading" : "E. Additional Translation Results",
      "text" : "Additional unsupervised image-to-image translation results are visualized in Figure 14,15, 16, 17 18, and 19."
    } ],
    "references" : [ {
      "title" : "Infogan: Interpretable representation learning by information maximizing generative adversarial nets",
      "author" : [ "Chen", "Xi", "Duan", "Yan", "Houthooft", "Rein", "Schulman", "John", "Sutskever", "Ilya", "Abbeel", "Pieter" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Chen et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep generative image models using a laplacian pyramid of adversarial networks",
      "author" : [ "Denton", "Emily L", "Chintala", "Soumith", "Fergus", "Rob" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Denton et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Denton et al\\.",
      "year" : 2015
    }, {
      "title" : "Unsupervised visual domain adaptation using subspace alignment",
      "author" : [ "Fernando", "Basura", "Habrard", "Amaury", "Sebban", "Marc", "Tuytelaars", "Tinne" ],
      "venue" : "In International Conference on Computer Vision,",
      "citeRegEx" : "Fernando et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Fernando et al\\.",
      "year" : 2013
    }, {
      "title" : "Domainadversarial training of neural networks",
      "author" : [ "Ganin", "Yaroslav", "Ustinova", "Evgeniya", "Ajakan", "Hana", "Germain", "Pascal", "Larochelle", "Hugo", "Laviolette", "François", "Marchand", "Mario", "Lempitsky", "Victor" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Ganin et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ganin et al\\.",
      "year" : 2016
    }, {
      "title" : "Image style transfer using convolutional neural networks",
      "author" : [ "Gatys", "Leon A", "Ecker", "Alexander S", "Bethge", "Matthias" ],
      "venue" : "In Conference on Computer Vision and Pattern Recognition",
      "citeRegEx" : "Gatys et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gatys et al\\.",
      "year" : 2016
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "Goodfellow", "Ian", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron", "Bengio", "Yoshua" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Multispectral pedestrian detection: Benchmark dataset and baseline",
      "author" : [ "Hwang", "Soonmin", "Park", "Jaesik", "Kim", "Namil", "Choi", "Yukyung", "So Kweon", "In" ],
      "venue" : "In Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Hwang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hwang et al\\.",
      "year" : 2015
    }, {
      "title" : "Image-to-image translation with conditional adversarial networks",
      "author" : [ "Isola", "Phillip", "Zhu", "Jun-Yan", "Zhou", "Tinghui", "Efros", "Alexei A" ],
      "venue" : "arXiv preprint arXiv:1611.07004,",
      "citeRegEx" : "Isola et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Isola et al\\.",
      "year" : 2016
    }, {
      "title" : "Perceptual losses for real-time style transfer and superresolution",
      "author" : [ "Johnson", "Justin", "Alahi", "Alexandre", "Fei-Fei", "Li" ],
      "venue" : "In European Conference in Computer Vision,",
      "citeRegEx" : "Johnson et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Kingma", "Diederik", "Ba", "Jimmy" ],
      "venue" : "In International Conference on Learning Representations,",
      "citeRegEx" : "Kingma et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2015
    }, {
      "title" : "Auto-encoding variational bayes",
      "author" : [ "Kingma", "Diederik P", "Welling", "Max" ],
      "venue" : "arXiv preprint arXiv:1312.6114,",
      "citeRegEx" : "Kingma et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2013
    }, {
      "title" : "Improving variational inference with inverse autoregressive flow",
      "author" : [ "Kingma", "Diederik P", "Salimans", "Tim", "Welling", "Max" ],
      "venue" : "arXiv preprint arXiv:1606.04934,",
      "citeRegEx" : "Kingma et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2016
    }, {
      "title" : "Autoencoding beyond pixels using a learned similarity metric",
      "author" : [ "Larsen", "Anders Boesen Lindbo", "Sønderby", "Søren Kaae", "Larochelle", "Hugo", "Winther", "Ole" ],
      "venue" : "International Conference on Machine Learning,",
      "citeRegEx" : "Larsen et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Larsen et al\\.",
      "year" : 2016
    }, {
      "title" : "Generative moment matching networks",
      "author" : [ "Li", "Yujia", "Swersky", "Kevin", "Zemel", "Richard S" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Li et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2015
    }, {
      "title" : "Coupled generative adversarial networks",
      "author" : [ "Liu", "Ming-Yu", "Tuzel", "Oncel" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Liu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep learning face attributes in the wild",
      "author" : [ "Liu", "Ziwei", "Luo", "Ping", "Wang", "Xiaogang", "Tang", "Xiaoou" ],
      "venue" : "In International Conference on Computer Vision,",
      "citeRegEx" : "Liu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2015
    }, {
      "title" : "Auxiliary deep generative models",
      "author" : [ "Maaløe", "Lars", "Sønderby", "Casper Kaae", "Søren Kaae", "Winther", "Ole" ],
      "venue" : "arXiv preprint arXiv:1602.05473,",
      "citeRegEx" : "Maaløe et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Maaløe et al\\.",
      "year" : 2016
    }, {
      "title" : "Reading digits in natural images with unsupervised feature learning",
      "author" : [ "Netzer", "Yuval", "Wang", "Tao", "Coates", "Adam", "Bissacco", "Alessandro", "Wu", "Bo", "Ng", "Andrew Y" ],
      "venue" : "In Advances in Neural Information Processing Systems workshop,",
      "citeRegEx" : "Netzer et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Netzer et al\\.",
      "year" : 2011
    }, {
      "title" : "Plug & play generative networks: Conditional iterative generation of images in latent space",
      "author" : [ "Nguyen", "Anh", "Yosinski", "Jason", "Bengio", "Yoshua", "Dosovitskiy", "Alexey", "Clune", "Jeff" ],
      "venue" : "arXiv preprint arXiv:1612.00005,",
      "citeRegEx" : "Nguyen et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2016
    }, {
      "title" : "Unsupervised representation learning with deep convolutional generative adversarial networks",
      "author" : [ "Radford", "Alec", "Metz", "Luke", "Chintala", "Soumith" ],
      "venue" : "In International Conference on Learning Representations,",
      "citeRegEx" : "Radford et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2016
    }, {
      "title" : "Stochastic backpropagation and variational inference in deep latent gaussian models",
      "author" : [ "Rezende", "Danilo Jimenez", "Mohamed", "Shakir", "Wierstra", "Daan" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Rezende et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Rezende et al\\.",
      "year" : 2014
    }, {
      "title" : "Improved techniques for training gans",
      "author" : [ "Salimans", "Tim", "Goodfellow", "Ian", "Zaremba", "Wojciech", "Cheung", "Vicki", "Radford", "Alec", "Chen", "Xi" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Salimans et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Salimans et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning from simulated and unsupervised images through adversarial training",
      "author" : [ "Shrivastava", "Ashish", "Pfister", "Tomas", "Tuzel", "Oncel", "Susskind", "Josh", "Wang", "Wenda", "Webb", "Russ" ],
      "venue" : "arXiv preprint arXiv:1612.07828,",
      "citeRegEx" : "Shrivastava et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Shrivastava et al\\.",
      "year" : 2016
    }, {
      "title" : "Unsupervised cross-domain image generation",
      "author" : [ "Taigman", "Yaniv", "Polyak", "Adam", "Wolf", "Lior" ],
      "venue" : "International Conference on Learning Representations,",
      "citeRegEx" : "Taigman et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Taigman et al\\.",
      "year" : 2017
    }, {
      "title" : "A note on the evaluation of generative models",
      "author" : [ "Theis", "Lucas", "Oord", "Aäron van den", "Bethge", "Matthias" ],
      "venue" : "International Conference on Learning Representations,",
      "citeRegEx" : "Theis et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Theis et al\\.",
      "year" : 2016
    }, {
      "title" : "Conditional image generation with pixelcnn decoders",
      "author" : [ "van den Oord", "Aaron", "Kalchbrenner", "Nal", "Espeholt", "Lasse", "Vinyals", "Oriol", "Graves", "Alex" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Oord et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Oord et al\\.",
      "year" : 2016
    }, {
      "title" : "Alignment by maximization of mutual information",
      "author" : [ "Viola", "Paul", "Wells III", "William M" ],
      "venue" : "International journal of computer vision,",
      "citeRegEx" : "Viola et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Viola et al\\.",
      "year" : 1997
    }, {
      "title" : "Attribute2image: Conditional image generation from visual attributes",
      "author" : [ "Yan", "Xinchen", "Yang", "Jimei", "Sohn", "Kihyuk", "Lee", "Honglak" ],
      "venue" : "European Conference in Computer Vision,",
      "citeRegEx" : "Yan et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Yan et al\\.",
      "year" : 2016
    }, {
      "title" : "Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks",
      "author" : [ "Zhang", "Han", "Xu", "Tao", "Li", "Hongsheng", "Shaoting", "Huang", "Xiaolei", "Wang", "Xiaogang", "Metaxas", "Dimitris" ],
      "venue" : "arXiv preprint arXiv:1612.03242,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "They require training datasets consisting of pairs of corresponding images in two domains (Ledig et al., 2016; Isola et al., 2016).",
      "startOffset" : 90,
      "endOffset" : 130
    }, {
      "referenceID" : 20,
      "context" : "The framework, as illustrated in Figure 1, is motivated by recent deep generative models including variational autoencoders (VAEs) (Kingma & Welling, 2013; Rezende et al., 2014; Larsen et al., 2016) and generative adversarial networks (GANs) (Goodfellow et al.",
      "startOffset" : 131,
      "endOffset" : 198
    }, {
      "referenceID" : 12,
      "context" : "The framework, as illustrated in Figure 1, is motivated by recent deep generative models including variational autoencoders (VAEs) (Kingma & Welling, 2013; Rezende et al., 2014; Larsen et al., 2016) and generative adversarial networks (GANs) (Goodfellow et al.",
      "startOffset" : 131,
      "endOffset" : 198
    }, {
      "referenceID" : 5,
      "context" : ", 2016) and generative adversarial networks (GANs) (Goodfellow et al., 2014; Liu & Tuzel, 2016).",
      "startOffset" : 51,
      "endOffset" : 95
    }, {
      "referenceID" : 12,
      "context" : "{E1, G1} VAE for X1 {E2, G2} VAE for X2 {E1, G2} X1 → X2 Image Translator {E2, G1} X2 → X1 Image Translator {G1, D1} GAN for X1 {G2, D2} GAN for X2 {E1, G1, D1} VAE-GAN (Larsen et al., 2016) {G1, G2, D1, D2} CoGAN (Liu & Tuzel, 2016)",
      "startOffset" : 169,
      "endOffset" : 190
    }, {
      "referenceID" : 5,
      "context" : "We apply an alternating gradient update scheme similar to the one described in (Goodfellow et al., 2014) to solve (1).",
      "startOffset" : 79,
      "endOffset" : 104
    }, {
      "referenceID" : 12,
      "context" : "2) Sampling from the VAE latent space admits a simple form, allowing a seamless integration with GANs (Larsen et al., 2016).",
      "startOffset" : 102,
      "endOffset" : 123
    }, {
      "referenceID" : 12,
      "context" : "An image generator trained using the GAN loss can generate crisp sharp images (Larsen et al., 2016; Ledig et al., 2016).",
      "startOffset" : 78,
      "endOffset" : 119
    }, {
      "referenceID" : 19,
      "context" : "999, respectively, as in (Radford et al., 2016).",
      "startOffset" : 25,
      "endOffset" : 47
    }, {
      "referenceID" : 6,
      "context" : "In the first experiment, we trained a UNIT network using the KAIST multispectral pedestrian detection benchmark (Hwang et al., 2015) for translating between daytime and night time images and between thermal IR and RGB images.",
      "startOffset" : 112,
      "endOffset" : 132
    }, {
      "referenceID" : 15,
      "context" : "We used the CelebFaces Attributes dataset (Liu et al., 2015) for translating face images based on attributes.",
      "startOffset" : 42,
      "endOffset" : 60
    }, {
      "referenceID" : 24,
      "context" : "Quantitative Evaluation: Quantitative evaluation of generative models is known to be a challenging task and the popular metrics are all subject to flaws (Theis et al., 2016).",
      "startOffset" : 153,
      "endOffset" : 173
    }, {
      "referenceID" : 2,
      "context" : "Early UDA works have explored ideas from subspace learning (Fernando et al., 2013) to deep learning (Ganin et al.",
      "startOffset" : 59,
      "endOffset" : 82
    }, {
      "referenceID" : 3,
      "context" : ", 2013) to deep learning (Ganin et al., 2016; Liu & Tuzel, 2016; Taigman et al., 2017).",
      "startOffset" : 25,
      "endOffset" : 86
    }, {
      "referenceID" : 23,
      "context" : ", 2013) to deep learning (Ganin et al., 2016; Liu & Tuzel, 2016; Taigman et al., 2017).",
      "startOffset" : 25,
      "endOffset" : 86
    }, {
      "referenceID" : 23,
      "context" : "The results of the other algorithms were duplicated from (Taigman et al., 2017)",
      "startOffset" : 57,
      "endOffset" : 79
    }, {
      "referenceID" : 2,
      "context" : "SA (Fernando et al., 2013) 59.",
      "startOffset" : 3,
      "endOffset" : 26
    }, {
      "referenceID" : 3,
      "context" : "32% DANN (Ganin et al., 2016) 73.",
      "startOffset" : 9,
      "endOffset" : 29
    }, {
      "referenceID" : 23,
      "context" : "85% DTN (Taigman et al., 2017) 84.",
      "startOffset" : 8,
      "endOffset" : 30
    }, {
      "referenceID" : 17,
      "context" : "We applied the above approach to the task of adapting a classifier from the Street View House Number (SVHN) dataset (Netzer et al., 2011) to the MNIST dataset.",
      "startOffset" : 116,
      "endOffset" : 137
    }, {
      "referenceID" : 23,
      "context" : "88% achieved by the previous state-of-the-art method (Taigman et al., 2017).",
      "startOffset" : 53,
      "endOffset" : 75
    }, {
      "referenceID" : 5,
      "context" : "Several deep generative models were recently proposed for image gneration including GANs (Goodfellow et al., 2014), VAEs (Kingma & Welling, 2013; Rezende et al.",
      "startOffset" : 89,
      "endOffset" : 114
    }, {
      "referenceID" : 20,
      "context" : ", 2014), VAEs (Kingma & Welling, 2013; Rezende et al., 2014), moment matching networks (Li et al.",
      "startOffset" : 14,
      "endOffset" : 60
    }, {
      "referenceID" : 13,
      "context" : ", 2014), moment matching networks (Li et al., 2015), PixelCNN (van den Oord et al.",
      "startOffset" : 34,
      "endOffset" : 51
    }, {
      "referenceID" : 18,
      "context" : ", 2016), and Plug&Play Generative Networks (Nguyen et al., 2016).",
      "startOffset" : 43,
      "endOffset" : 64
    }, {
      "referenceID" : 1,
      "context" : "(Denton et al., 2015) proposed a Laplacian pyramid implementation of GANs.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 19,
      "context" : "(Radford et al., 2016) used a deeper convolution network architecture.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 28,
      "context" : "(Zhang et al., 2016) stacks two generators to progressively render realistic images.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 0,
      "context" : "InfoGAN (Chen et al., 2016) learned to a more interpretable latent representation.",
      "startOffset" : 8,
      "endOffset" : 27
    }, {
      "referenceID" : 21,
      "context" : "(Salimans et al., 2016) proposed several GAN training tricks.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 16,
      "context" : "By improving the variational approximation, better image generation results were achieved (Maaløe et al., 2016; Kingma et al., 2016).",
      "startOffset" : 90,
      "endOffset" : 132
    }, {
      "referenceID" : 11,
      "context" : "By improving the variational approximation, better image generation results were achieved (Maaløe et al., 2016; Kingma et al., 2016).",
      "startOffset" : 90,
      "endOffset" : 132
    }, {
      "referenceID" : 12,
      "context" : "In (Larsen et al., 2016), a VAEGAN architecture was proposed to improve image generation quality of VAEs.",
      "startOffset" : 3,
      "endOffset" : 24
    }, {
      "referenceID" : 27,
      "context" : "VAEs were applied to translate face image attribute in (Yan et al., 2016).",
      "startOffset" : 55,
      "endOffset" : 73
    }, {
      "referenceID" : 7,
      "context" : "Most of the existing works were based on supervised learning (Ledig et al., 2016; Isola et al., 2016), requiring corresponding images in two domains.",
      "startOffset" : 61,
      "endOffset" : 101
    }, {
      "referenceID" : 23,
      "context" : "Recently, (Taigman et al., 2017) proposed the domain transformation network (DTN) and achieved promising results on translating small resolution face and digit images.",
      "startOffset" : 10,
      "endOffset" : 32
    }, {
      "referenceID" : 22,
      "context" : "In (Shrivastava et al., 2016), a conditional generative adversarial network-based approach was proposed to translate a rendering images to a real image for gaze estimation.",
      "startOffset" : 3,
      "endOffset" : 29
    }, {
      "referenceID" : 4,
      "context" : "We note that image translation is different to style transfer (Gatys et al., 2016; Johnson et al., 2016) in that while style transfer focuses on translate a natural image to an abstract, artistic image, image translation also considers translating a natural image to another natural image where loss of image details is unacceptable.",
      "startOffset" : 62,
      "endOffset" : 104
    }, {
      "referenceID" : 8,
      "context" : "We note that image translation is different to style transfer (Gatys et al., 2016; Johnson et al., 2016) in that while style transfer focuses on translate a natural image to an abstract, artistic image, image translation also considers translating a natural image to another natural image where loss of image details is unacceptable.",
      "startOffset" : 62,
      "endOffset" : 104
    } ],
    "year" : 2017,
    "abstractText" : "Most of the existing image-to-image translation frameworks—mapping an image in one domain to a corresponding image in another—are based on supervised learning, i.e., pairs of corresponding images in two domains are required for learning the translation function. This largely limits their applications, because capturing corresponding images in two different domains is often a difficult task. To address the issue, we propose the UNsupervised Image-to-image Translation (UNIT) framework, which is based on variational autoencoders and generative adversarial networks. The proposed framework can learn the translation function without any corresponding images in two domains. We enable this learning capability by combining a weight-sharing constraint and an adversarial training objective. Through visualization results from various unsupervised image translation tasks, we verify the effectiveness of the proposed framework. An ablation study further reveals the critical design choices. Moreover, we apply the UNIT framework to the unsupervised domain adaptation task and achieve better results than competing algorithms do in benchmark datasets.",
    "creator" : "LaTeX with hyperref package"
  }
}