{
  "name" : "1606.04695.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Strategic Attentive Writer for Learning Macro-Actions",
    "authors" : [ "Alexander (Sasha", "John Agapiou", "Simon Osindero", "Alex Graves", "Oriol Vinyals", "Koray Kavukcuoglu" ],
    "emails" : [ "vezhnick@google.com", "vmnih@google.com", "jagapiou@google.com", "osindero@google.com", "gravesa@google.com", "vinyals@google.com", "korayk@google.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Using reinforcement learning to train neural network controllers has recently led to rapid progress on a number of challenging control tasks [15, 17, 26]. Much of the success of these methods has been attributed to the ability of neural networks to learn useful abstractions or representations of the stream of observations, allowing the agents to generalize between similar states. Notably, these agents do not exploit another type of structure – the one present in the space of controls or policies. Indeed, not all sequences of low-level controls lead to interesting high-level behaviour and an agent that can automatically discover useful macro-actions should be capable of more efficient exploration and learning. The discovery of such temporal abstractions has been a long-standing problem in both reinforcement learning and sequence prediction in general, yet no truly scalable and successful architectures exist.\nWe propose a new deep recurrent neural network architecture, dubbed STRategic Attentive Writer (STRAW), that is capable of learning macro-actions in a reinforcement learning setting. Unlike the vast majority of reinforcement learning approaches [15, 17, 26], which output a single action after each observation, STRAW maintains a multi-step action plan. STRAW periodically updates the plan based on observations and commits to the plan between the replanning decision points. The replanning decisions as well as the commonly occurring sequences of actions, i.e. macro-actions, are learned from rewards. To encourage exploration with macro-actions we introduce a noisy communication channel between a feature extractor (e.g. convolutional neural network) and the planning modules, taking inspiration from recent developments in variational auto-encoders [10, 13, 24]. Injecting noise\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n60 6.\n04 69\n5v 1\n[ cs\n.A I]\n1 5\nJu n\n20 16\nat this level of the network generates randomness in plans updates that cover multiple time steps and thereby creates the desired effect.\nOur proposed architecture is a step towards more natural decision making, wherein one observation can generate a whole sequence of outputs if it is informative enough. This provides several important benefits. First and foremost, it facilitates structured exploration in reinforcement learning – as the network learns meaningful action patterns it can use them to make longer exploratory steps in the state space [4]. Second, since the model does not need to process observations while it is committed its action plan, it learns to allocate computation to key moments thereby freeing up resources when the plan is being followed. Additionally, the acquisition of macro-actions can aid transfer and generalization to other related problems in the same domain (assuming that other problems from the domain share similar structure in terms of action-effects).\nWe evaluate STRAW on a subset of Atari games that require longer term planning and show that it leads to substantial improvements in scores. We also demonstrate the generality of the STRAW architecture by training it on a text prediction task and show that it learns to use frequent n-grams as the macro-actions on this task.\nThe following section reviews the related work. Section 3 defines the STRAW model formally. Section 4 describes the training procedure for both supervised and reinforcement learning cases. Section 5 presents the experimental evaluation of STRAW on 8 ATARI games, 2D maze navigation and next character prediction tasks. Section 6 concludes."
    }, {
      "heading" : "2 Related Work",
      "text" : "Learning temporally extended actions and temporal abstraction in genral are long standing problems in reinforcement learning [5, 6, 7, 8, 12, 20, 21, 22, 23, 25, 27, 28]. The options framework [21, 28] provides a general formulation. An option is a sub-policy with a termination condition, which takes in enviroment observations and outputs actions until the termination condition is met. An agent picks an option using its policy-over-options and subsequently follows it until termination, at which point the policy-over-options is queried again and the process continues. Notice, that macro-action is a particular, simpler instance of options, where the action sequence (or a distribution over them) is decided at the time the macro-action is initiated. Options are typically learned using subgoals and ’pseudo-rewards’ that are provided explicitly [7, 8, 28]. Given the options, a policy-over-options can be learned using standard techniques by treating options as actions. Recently [14, 29] have demonstrated that combining deep learning with pre-defined subgoals delivers promising results in challenging environments like Minecraft and Atari, however, subgoal discovery remains an unsolved problem. Another recent work by [1] shows a theoretical possibility of learning options jointly with a policy-over-options by extending the policy gradient theorem to options, but the approach was only tested on a toy problem.\nIn contrast, STRAW learns macro-actions and a policy over them in an end-to-end fashion from only the environment’s reward signal and without resorting to explicit pseudo-rewards or hand-crafted subgoals. The macro-actions are represented implicitly inside the model, arising naturally from the interplay between action and commitment plans within the network. Our experiments demonstrate that the model scales to a variety of tasks from next character prediction in text to ATARI games."
    }, {
      "heading" : "3 The model",
      "text" : "STRAW is a deep recurrent neural network with two modules. The first module translates environment observations into an action-plan – a state variable which represents an explicit stochastic plan of future actions. STRAW generates macro-actions by committing to the action-plan and following it without updating for a number of steps. The second module maintains commitment-plan – a state variable that determines at which step the network terminates a macro-action and updates the action-plan. The action-plan is a matrix where one dimension corresponds to time and the other to the set of possible discrete actions. The elements of this matrix are proportional to the probability of taking the corresponding action at the corresponding time step. Similarly, the commitment plan represents the probabilities of terminating a macro-action at the particular step. For updating both plans we use attentive writing technique [10], which allows the network to focus on parts of a plan\nwhere the current observation is informative of desired outputs. This section formally defines the model, we describe the way it is trained later in section 4.\nThe state of the network at time t is comprised of matrices At ∈ RA×T and ct ∈ R1×T . Matrix At is the action-plan. Each element Ata,τ is proportional to the probability of outputting a at time t + τ . Here A is a total number of possible actions and T is a maximum time horizon of the plan. To generate an action at time t, the first column of At (i.e. At•0) is transformed into a distribution over possible outputs by a SoftMax function. This distribution is then sampled to generate the action at. Thereby the content of At corresponds to the plan of future actions as conceived at time t. The single row matrix ct represents the commitment-plan of the network. Let gt be a binary random variable distributed as follows: gt ∼ ct−11 . If gt = 1 then at this step the plans will be updated, otherwise gt = 0 means they will be committed to. Macro-actions are defined as a sequence of outputs {at}t2−1t1 produced by the network between steps where gt is ‘on’: i.e gt1 = gt2 = 1 and gt′ = 0,∀t1 < t′ < t2. During commitment the plans are rolled over to the next step using the matrix time-shift operator ρ, which shifts a matrix by removing the first column and appending a column filled with zeros to its rear. Applying ρ to At or ct reflects the advancement of time. Figure 1 illustrates the workflow. Notice that during commitment (step 2 and 3) the network doesn’t compute the forward pass, thereby saving computation.\nAttentive planning. An important assumption that underpins the usage of macro-actions is that one observation reveals enough information to generate a sequence of actions. The complexity of the sequence and its length can vary dramatically, even within one environment. Therefore the network has to focus on the part of the plan where the current observation is informative of desired actions. To achieve this, we apply differentiable attentive reading and writing operations [10], where attention is defined over the temporal dimension. This technique was originally proposed for image generation, here instead it used to update the plans At and ct. In the image domain, the attention operates over the spatial extent of an image, reading and writing pixel values. Here it operates over the temporal extent of a plan, and is used to read and write action probabilities. The differentiability of the attention model [10] makes it possible to train with standard backpropagation.\nAn array of Gaussian filters is applied to the plan, yielding a ‘patch’ of smoothly varying location and zoom. Let A be the total number of possible actions and K be a parameter that determines the temporal resolution of the patch. A grid of K ×A of one-dimensional Gaussian filters is positioned on the plan by specifying the co-ordinates of the grid center and the stride distance between adjacent filters. The stride controls the ‘zoom’ of the patch; that is, the larger the stride, the larger an area of the original plan will be visible in the attention patch, but the lower the effective resolution of the\npatch will be. The filtering is performed along the temporal dimension only. Let ψ be a vector of attention parameters, i.e.: grid position, stride, and standard deviation of Gaussian filters. We define the attention operations as follows:\nD = write(p, ψAt ); βt = read(A t, ψAt ) (1)\nThe write operation takes in a patch p ∈ RA×K and attention parameters ψ. It produces a matrix D of the same size as At, which contains the patch p scaled and positioned according to ψ. Analogously the read operation takes the full plan At together with attention parameters ψAt and outputs a read patch β ∈ RA×K , which is extracted from At according to ψAt . We direct readers to [10] for details.\nAction-plan update. Let zt be a feature representation (e.g. the output of a deep convolutional network) of an observation xt. Given zt, gt and the previous state At−1 STRAW computes an update to the action-plan using the Algorithm 1. Here fψ and fA are linear functions and h is two-layer perceptron. Figure 2 gives an illustration of an update to At.\nAlgorithm 1 Action-plan update Input: zt, gt,At−1 Output: At, at if gt = 1 then\nCompute attention parameters ψAt = f ψ(zt) Attentively read the current state of the action-plan βt = read(At−1, ψAt ); Compute intermediate representation ξt = h([βt, zt]) Update At = ρ(At−1) + gt · write(fA(ξt), ψAt )\nelse // gt = 0 Update At = ρ(At−1) //just advance time end if Sample an action at ∼ SoftMax(At0)\nCommitment-plan update. Now we introduce a module that partitions the action-plan into macroactions by defining the temporal extent to which the current action-plan At can be followed without re-planning. The commitment-plan ct is updated at the same time as the action-plan, i.e. when gt = 1 or otherwise it is rolled over to the next time step by ρ operator. Unlike the planning module, where At is updated additively, ct is overwritten completely using the following equations:\ngt ∼ ct−11 if gt = 0 then ct = ρ(ct−1) else ψct = f c([ψAt , ξt]) ct = Sigmoid(b+ write(e, ψc)); (2)\nHere the same attentive writing operation is used, but with only one Gaussian filter for attention over c. The patch e is therefore just a scalar, which we fix to a high value (40 in our experiments). This high value of e is chosen so that the attention parameters ψc define a time step when re-planning is guaranteed to happen. Vector b is of the same size as dt filled with a shared, learnable bias b, which defines the probability of re-planning earlier than the step implied by ψc.\nNotice that gt is used as a multiplicative gate in algorithm 1. This allows for retroactive credit assignment during training, as gradients from write operation at time t + τ directly flow into the\ncommitment module through state ct. Moreover, when gt = 0 only the computationally cheap operator ρ is invoked. Thereby more commitment significantly saves computation."
    }, {
      "heading" : "3.1 Structured exploration with macro-actions",
      "text" : "The architecture defined above is capable of generating macro-actions. This section describes how to use macro-actions for structured exploration. We introduce STRAW-explorer (STRAWe), a version of STRAW with a noisy communication channel between the feature extractor (e.g. a convolutional neural network) and STRAW planning modules. Let ζt be the activations of the last layer of the feature extractor. We use ζt to regress the parameters of a Gaussian distribution Q(zt|ζt) = N(µ(ζt), I · σ(ζt)) from which zt is sampled. Here µ is a vector and σ is a scalar. Injecting noise at this level of the network generates randomness on the level of plan updates that cover multiple time steps. This effect is reinforced by commitment, which forces STRAWe to execute the plan and experience the outcome instead of rolling the update back on the next step. In section 5 we demonstrate how this significantly improves score on games like Frostbite and Ms. Pacman."
    }, {
      "heading" : "4 Learning",
      "text" : "The training loss of the model is defined as follows:\nL = T∑ t ( Lout(At) + 1gt · αKL(Q(zt|φ(xt))|P (zt)) + λct[t] ) , (3)\nwhere Lout is a domain specific differentiable loss function defined over the network’s output. For supervised problems, like next character prediction in text, Lout can be defined as negative log likelihood of the correct output. We discuss the reinforcement learning case later in this section. The two extra terms are regularisers. The first is a cost of communication through the noisy channel, which is defined as KL divergence between latent distributions Q(zt|φ(xt)) and some prior P (zt). Since the latent distribution is a Gaussian (sec. 3.1), a natural choice for the prior is a Gaussian with a zero mean and standard deviation of one. The last term penalizes re-planning and encourages commitment.\nFor reinforcement learning we consider the standard setting where an agent is interacting with an environment in discrete time. At each step t, the agent observes the state of the environment xt and selects an action at from a finite set of possible actions. The environment responds with a new state xt+1 and a scalar reward rt. The process continues until the terminal state is reached, after which it restarts. The goal of the agent is to maximize the discounted return Rt = ∑∞ k=0 α\nkrt+k+1. The agent’s behaviour is defined by its policy π – mapping from state space into action space. STRAW produces a distribution over possible actions (a stochastic policy) by passing the first column of the action-plan At through a SoftMax function: π(at|xt; θ) = SoftMax(At•0). An action is then produced by sampling the output distribution.\nWe use a recently proposed Asynchronous Advantage Actor-Critic (A3C) method [18], which directly optimizes the policy of an agent. A3C requires a value function estimator V (xt) for variance reduction. This estimator can be produced in a number of ways, for example by a separate neural network. The most natural solution for our architecture would be to create value-plan containing the estimates. To keep the architecture simple and efficient, we simply add an auxiliary row to the action plan which corresponds to the value function estimation. It participates in attentive reading and writing during the update, thereby sharing the temporal attention with action-plan. The plan is then split into action part and the estimator before the SoftMax is applied and an action is sampled. The policy gradient update for Lout in L is defined as follows:\n∇Lout = ∇θ log π(at|xt; θ)(Rt − V (xt; θ)) + β∇θH(π(at|xt; θ)) (4)\nHere H(π(at|xt; θ)) is entropy of the policy, which stimulates the exploration of primitive actions. The network contains two random variables – zt and gt, which we have to pass gradients through. For zt we employ the re-parametrization trick [13, 24]. For gt, we set∇ct−11 ≡ ∇gt as proposed in [3]."
    }, {
      "heading" : "5 Experiments",
      "text" : "The goal of our experiments was to demonstrate that STRAW learns meaningful and useful macroactions. We use three domains of increasing complexity: supervised next character prediction in text [9], 2D maze navigation and ATARI games [2]."
    }, {
      "heading" : "5.1 Experimental setup",
      "text" : "Architecture. The read and write patches are A× 10 dimensional, and h is a 2 layer perceptron with 64 hidden units. The time horizon T = 500. For STRAWe (sec. 3.1) the Gaussian distribution for structured exploration is 128-dimensional. Ablative analysis for some of these choices is provided in section 5.5. Feature representation of the state space is particular for each domain. For 2D mazes and ATARI it is a convolutional neural net (CNN) and it is an LSTM [11] for text. We provide more details in the corresponding sections.\nBaselines. The experiments employ two baselines: a simple feed forward network (FF) and a recurrent LSTM network. FF directly regresses the action probabilities and value function estimate from feature representation. The LSTM [11] architecture is a widely used recurrent network and it was demonstrated to perform well on a suite of reinforcement learning problems [18]. It has 128 hidden units and its inputs are the feature representation of an observation and the previous action of the agent. Action probabilities and the value function estimate are regressed from its hidden state.\nOptimization. We use the A3C method [18] for all reinforcement learning experiments. It was shown to achieve state-of-the-art results on several challenging benchmarks [18]. We cut the trajectory and run backpropagation through time [19] after 40 forward passes of a network or if a terminal signal is received. The optimization process runs 32 asynchronous threads using shared RMSProp. There are 4 hyper-parameters in STRAW and 2 in the LSTM and FF baselines. For each method, we ran 200 experiments, each using randomly sampled hyperparameters. Learning rate and entropy penalty were sampled from a LogUniform(10−4, 10−3) interval. Learning rate is linearly annealed from a sampled value to 0. To explore STRAW behaviour, we sample coding cost α ∼ LogUniform(10−7, 10−4) and replanning penalty λ ∼ LogUniform(10−6, 10−2). For stability, we clip the advantageRt−V (xt; θ) (eq. 4) to [−1, 1] for all methods and for STRAW(e) we do not propagate gradients from commitment module into planning module through ψAT and ξt. We define a training epoch as one million observations."
    }, {
      "heading" : "5.2 Text",
      "text" : "STRAW is a general sequence prediction architecture. To demonstrate that it is capable of learning output patterns with complex structure we present a qualitative experiment on next character prediction using Penn Treebank dataset [16]. Actions in this case correspond to emitting characters and macroactions to their sequences. For this experiment we use an LSTM, which receives a one-hot-encoding of 50 characters as input. A STRAW module is connected on top. We omit the noisy Gaussian channel, as this task is fully supervised and does not require exploration. The actions now correspond to emitting characters. The network is trained with stochastic gradient descent using supervised\nnegative log-likelihood loss (sec. 4). At each step we feed a character to the model which updates the LSTM representation, but only update the STRAW plans according to the commitment plan ct. For a trained model we record macro-actions – the sequences of characters produced when STRAW is committed to the plan. If STRAW adequately learns the structure of the data, then its macro-actions should correspond to common n-grams. Figure 3.c plots the 20 most frequent macro-action of length 4 produced by STRAW. On x-axis is the rank of the frequency at which macro-action is used by STRAW, on y-axis is it’s actual frequency rank as an 4-gram. Notice how STRAW correctly learned to predict frequent 4-grams such as ’the’, ’ing’, ’and’ and so on."
    }, {
      "heading" : "5.3 2D mazes",
      "text" : "To investigate what macro-actions our model learns and whether they are useful for reinforcement learning we conduct an experiment on a random 2D mazes domain. A maze is a grid-world with two types of cells – walls and corridors. One of corridor cells is marked as a goal. The agent receives a small negative reward of −r every step, and double that if it tries to move through the wall. It receives as small positive reward r when it steps on the goal and the episode terminates. Otherwise episode terminates after 100 steps. Therefore to maximize return an agent has to reach the goal as fast as possible. In every training episode the position of walls, goal and the starting position of the agent are randomized. An agent fully observes the state of the maze. The remainder of this section presents two experiments in this domain. Here we use STRAWe version with structured exploration. For feature representation we use a 2-layer CNN with 3x3 filters and stride of 1, each followed by a rectifier nonlinearity.\nIn our first experiment we train a STRAWe agent on an 11 x 11 random maze environment. We then evaluate a trained agent on a novel maze with fixed geometry and only randomly varying start and goal locations. The aim is to visualize the positions in which STRAWe terminates macro-actions and re-plans. Figure 3.a shows the maze, where red intensity corresponds to the ratio of re-planning events at the cell normalized with the total amount of visits by an agent. Notice how corners and areas close to junctions are highlighted. This demonstrates that STRAW learns an adequate temporal abstractions in this domain. In the next experiment we test whether these temporal abstractions are useful.\nThe second experiment uses a larger 15 x 15 random mazes. If the goal is placed arbitrarily far from an agent’s starting position, then learning becomes extremely hard and neither LSTM nor STRAWe can reliably learn a good policy. We introduce a curriculum where the goal is first positioned very close to the starting location and is moved further away during the progress of training. More precisely, we position the goal using a random walk starting from the same point as an agent. We increase the random walks length by one every 2 million training steps, starting from 2. Although the task gets progressively harder, the temporal abstractions (e.g. follow the corridor, turn the corner) remain the same. If learnt early on, they should make adaptation easy. The Figure 3.b plots episode reward against training steps for STRAW, LSTM and the optimal policy given by Dijkstra algorithm. Notice how both learn a good policy after approximately 200 epochs, when the task is still simple. As the goal moves away LSTM has a strong decline in reward relative to the optimal policy. In contrast, STRAWe effectively uses macro-actions learned early on and stays close to the optimal policy at harder stages. This demonstrates that temporal abstractions learnt by STRAW are useful."
    }, {
      "heading" : "5.4 ATARI",
      "text" : "This section presents results on a subset of ATARI games. All compared methods used the same CNN architecture, input preprocessing, and an action repeat of 4. For feature representation we use a CNN with a convolutional layer with 16 filters of size 8 x 8 with stride 4, followed by a convolutional layer\nwith with 32 filters of size 4 x 4 with stride 2, followed by a fully connected layer with 128 hidden units. All three hidden layers were followed by a rectifier nonlinearity. This is the same architecture as in [17, 18], the only difference is that in pre-processing stage we keep colour channels.\nWe chose games that require some degree of planning and exploration as opposed to purely reactive ones: Ms. Pacman, Frostbite, Alien, Amidar, Hero, Q-bert, Crazy Climber. We also added a reactive Breakout game to the set as a sanity check. Table. 1 shows the average performance of the top 5 agents for each architecture after 500 epochs of training. Due to action repeat, here an epoch corresponds to four million frames (across all threads). STRAW or STRAWe reach the highest score on 6 out of 8 games. They are especially strong on Frostbite, Ms. Pacman and Amidar. On Frostbite STRAWe achieves more than 6× improvement over the LSTM score. Notice how structured exploration (sec. 3.1) improves the performance on all games but one. STRAW and STRAWe do perform worse than an LSTM on breakout, although they still achieves a very good score (human players score below 100). This is likely due to breakout requiring fast reaction and action precision, rather than planning or exploration. FF baseline scores worst on every game apart from Hero, where it is the best. Although the difference is not very large, this is still a surprising result, which might be due to FF having fewer parameters to learn.\nExamples of macro-actions learned by STRAWe on Frostbite are shown in figure 4. Notice that macro-actions correspond to jumping from floe to floe and picking up fish. Figure 5 demonstrates re-planning behaviour on Amidar. In this game, agent explores a maze with a yellow avatar. It has to cover all of the maze territory without colliding with green figures (enemies). Notice how the STRAW agent changes its plan as an enemy comes near and blocks its way. It backs off and then resumes the initial plan when the enemy takes a turn and danger is avoided. Also, notice that when the enemy is near, agent plans for shorter macro-actions as indicated by commitment-plan ct. Figure 3.d shows the percentage of time the best STRAWe agent is committed to a plan on 4 different games. As training progresses, STRAWe learns to commit more and converges to a stable regime after about 200 epochs. The only exception is breakout, where meticulous control is required and is beneficial to re-plan often. This shows that STRAWe is capable of adapting to the environment and learns temporal abstractions useful for each particular case."
    }, {
      "heading" : "5.5 Ablative analysis",
      "text" : "Here we examine different design choices that were made and investigate their impact on final performance. Figure 6 presents the performance curves of different versions of STRAW on Ms. Pacman game trained for 100 epochs. From left to right, the first plot shows STRAW performance given different resolution of the action patch. The higher it is, the more complex is the update that STRAW can generate for the action plan. The second plot shows the influence of the Gaussian channel’s dimensionality, which is used for structured exploration (sec. 3.1). The results show that higher dimensionality is generally beneficial, but these benefits rapidly saturate. In the third plot we investigate different possible choices for the re-planning mechanism: we compare STRAW with two simple modifications, one re-plans at every step, the other commits to the plan for a random amount of steps between 0 and 4. Re-planning at every step is not only less elegant, but also much more computationally expensive and less data efficient. The results demonstrate that learning when to commit to the plan and when to re-plan is beneficial."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We have introduced the STRategic Attentive Writer (STRAW) architecture, and demonstrated its ability to implicitly learn useful temporally abstracted macro-actions in an end-to-end manner. Furthermore, STRAW advances the state-of-the-art on several challenging Atari domains that require temporally extended planning and exploration strategies, and also has the ability to learn temporal abstractions in general sequence prediction. As such it opens a fruitful new direction in tackling an important problem area for sequential decision making and AI more broadly."
    }, {
      "heading" : "7 Acknowledgements",
      "text" : "We thank David Silver, Joseph Modayil and Nicolas Heess for many helpful discussions, suggestions and comments on the paper."
    } ],
    "references" : [ {
      "title" : "The option-critic architecture",
      "author" : [ "Pierre-Luc Bacon", "Doina Precup" ],
      "venue" : "In NIPS Deep RL Workshop,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2015
    }, {
      "title" : "The arcade learning environment: An evaluation platform for general agents",
      "author" : [ "Marc G Bellemare", "Yavar Naddaf", "Joel Veness", "Michael Bowling" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2012
    }, {
      "title" : "Estimating or propagating gradients through stochastic neurons for conditional computation",
      "author" : [ "Yoshua Bengio", "Nicholas Léonard", "Aaron Courville" ],
      "venue" : "arXiv preprint arXiv:1308.3432,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2013
    }, {
      "title" : "Hierarchically organized behavior and its neural foundations: A reinforcement learning perspective",
      "author" : [ "Matthew M Botvinick", "Yael Niv", "Andrew C Barto" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2009
    }, {
      "title" : "Prioritized goal decomposition of markov decision processes: Toward a synthesis of classical and decision theoretic planning",
      "author" : [ "Craig Boutilier", "Ronen I Brafman", "Christopher Geib" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1997
    }, {
      "title" : "Improving generalization for temporal difference learning: The successor representation",
      "author" : [ "Peter Dayan" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1993
    }, {
      "title" : "Feudal reinforcement learning",
      "author" : [ "Peter Dayan", "Geoffrey E Hinton" ],
      "venue" : "In NIPS. Morgan Kaufmann Publishers,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1993
    }, {
      "title" : "Hierarchical reinforcement learning with the maxq value function decomposition",
      "author" : [ "Thomas G Dietterich" ],
      "venue" : "J. Artif. Intell. Res.(JAIR),",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2000
    }, {
      "title" : "Generating sequences with recurrent neural networks",
      "author" : [ "Alex Graves" ],
      "venue" : "arXiv preprint arXiv:1308.0850,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2013
    }, {
      "title" : "Draw: A recurrent neural network for image generation",
      "author" : [ "Karol Gregor", "Ivo Danihelka", "Alex Graves", "Danilo Rezende", "Daan Wierstra" ],
      "venue" : "In ICML,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1997
    }, {
      "title" : "Hierarchical learning in stochastic domains: Preliminary results",
      "author" : [ "Leslie Pack Kaelbling" ],
      "venue" : "In ICML,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2014
    }, {
      "title" : "Auto-encoding variational bayes",
      "author" : [ "Diederik P Kingma", "Max Welling" ],
      "venue" : "ICLR,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation",
      "author" : [ "Tejas D. Kulkarni", "Karthik R. Narasimhan", "Ardavan Saeedi", "Joshua B. Tenenbaum" ],
      "venue" : "arXiv preprint arXiv:1604.06057,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2016
    }, {
      "title" : "End-to-end training of deep visuomotor policies",
      "author" : [ "Sergey Levine", "Chelsea Finn", "Trevor Darrell", "Pieter Abbeel" ],
      "venue" : "arXiv preprint arXiv:1504.00702,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2015
    }, {
      "title" : "Building a large annotated corpus of english: The penn treebank",
      "author" : [ "Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini" ],
      "venue" : "Computational linguistics,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1993
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A. Rusu", "Joel Veness", "Marc G. Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K. Fidjeland", "Georg Ostrovski", "Stig Petersen", "Charles Beattie", "Amir Sadik", "Ioannis Antonoglou", "Helen King", "Dharshan Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis" ],
      "venue" : "Nature, 518(7540):529–533,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2015
    }, {
      "title" : "Asynchronous methods for deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Adria Puigdomenech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2016
    }, {
      "title" : "A focused back-propagation algorithm for temporal pattern recognition",
      "author" : [ "Michael C Mozer" ],
      "venue" : "Complex systems,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1989
    }, {
      "title" : "Reinforcement learning with hierarchies of machines",
      "author" : [ "Ronald Parr", "Stuart Russell" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1998
    }, {
      "title" : "Temporal abstraction in reinforcement learning",
      "author" : [ "Doina Precup" ],
      "venue" : "PhD thesis, University of Massachusetts,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2000
    }, {
      "title" : "Planning with closed-loop macro actions",
      "author" : [ "Doina Precup", "Richard S Sutton", "Satinder P Singh" ],
      "venue" : "Technical report,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1997
    }, {
      "title" : "Theoretical results on reinforcement learning with temporally abstract options",
      "author" : [ "Doina Precup", "Richard S Sutton", "Satinder Singh" ],
      "venue" : "In European Conference on Machine Learning (ECML). Springer,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1998
    }, {
      "title" : "Stochastic backpropagation and approximate inference in deep generative models",
      "author" : [ "Danilo Jimenez Rezende", "Shakir Mohamed", "Daan Wierstra" ],
      "venue" : null,
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2014
    }, {
      "title" : "Neural sequence chunkers",
      "author" : [ "Jürgen Schmidhuber" ],
      "venue" : "Technical report,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 1991
    }, {
      "title" : "Trust region policy optimization",
      "author" : [ "John Schulman", "Sergey Levine", "Philipp Moritz", "Michael I Jordan", "Pieter Abbeel" ],
      "venue" : "In ICML,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2015
    }, {
      "title" : "Td models: Modeling the world at a mixture of time scales",
      "author" : [ "Richard S Sutton" ],
      "venue" : "In ICML,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 1995
    }, {
      "title" : "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning",
      "author" : [ "Richard S Sutton", "Doina Precup", "Satinder Singh" ],
      "venue" : "Artificial intelligence,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 1999
    }, {
      "title" : "A deep hierarchical approach to lifelong learning in minecraft",
      "author" : [ "Chen Tessler", "Shahar Givony", "Tom Zahavy", "Daniel J Mankowitz", "Shie Mannor" ],
      "venue" : "arXiv preprint arXiv:1604.07255,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "Using reinforcement learning to train neural network controllers has recently led to rapid progress on a number of challenging control tasks [15, 17, 26].",
      "startOffset" : 141,
      "endOffset" : 153
    }, {
      "referenceID" : 16,
      "context" : "Using reinforcement learning to train neural network controllers has recently led to rapid progress on a number of challenging control tasks [15, 17, 26].",
      "startOffset" : 141,
      "endOffset" : 153
    }, {
      "referenceID" : 25,
      "context" : "Using reinforcement learning to train neural network controllers has recently led to rapid progress on a number of challenging control tasks [15, 17, 26].",
      "startOffset" : 141,
      "endOffset" : 153
    }, {
      "referenceID" : 14,
      "context" : "Unlike the vast majority of reinforcement learning approaches [15, 17, 26], which output a single action after each observation, STRAW maintains a multi-step action plan.",
      "startOffset" : 62,
      "endOffset" : 74
    }, {
      "referenceID" : 16,
      "context" : "Unlike the vast majority of reinforcement learning approaches [15, 17, 26], which output a single action after each observation, STRAW maintains a multi-step action plan.",
      "startOffset" : 62,
      "endOffset" : 74
    }, {
      "referenceID" : 25,
      "context" : "Unlike the vast majority of reinforcement learning approaches [15, 17, 26], which output a single action after each observation, STRAW maintains a multi-step action plan.",
      "startOffset" : 62,
      "endOffset" : 74
    }, {
      "referenceID" : 9,
      "context" : "convolutional neural network) and the planning modules, taking inspiration from recent developments in variational auto-encoders [10, 13, 24].",
      "startOffset" : 129,
      "endOffset" : 141
    }, {
      "referenceID" : 12,
      "context" : "convolutional neural network) and the planning modules, taking inspiration from recent developments in variational auto-encoders [10, 13, 24].",
      "startOffset" : 129,
      "endOffset" : 141
    }, {
      "referenceID" : 23,
      "context" : "convolutional neural network) and the planning modules, taking inspiration from recent developments in variational auto-encoders [10, 13, 24].",
      "startOffset" : 129,
      "endOffset" : 141
    }, {
      "referenceID" : 3,
      "context" : "First and foremost, it facilitates structured exploration in reinforcement learning – as the network learns meaningful action patterns it can use them to make longer exploratory steps in the state space [4].",
      "startOffset" : 203,
      "endOffset" : 206
    }, {
      "referenceID" : 4,
      "context" : "Learning temporally extended actions and temporal abstraction in genral are long standing problems in reinforcement learning [5, 6, 7, 8, 12, 20, 21, 22, 23, 25, 27, 28].",
      "startOffset" : 125,
      "endOffset" : 169
    }, {
      "referenceID" : 5,
      "context" : "Learning temporally extended actions and temporal abstraction in genral are long standing problems in reinforcement learning [5, 6, 7, 8, 12, 20, 21, 22, 23, 25, 27, 28].",
      "startOffset" : 125,
      "endOffset" : 169
    }, {
      "referenceID" : 6,
      "context" : "Learning temporally extended actions and temporal abstraction in genral are long standing problems in reinforcement learning [5, 6, 7, 8, 12, 20, 21, 22, 23, 25, 27, 28].",
      "startOffset" : 125,
      "endOffset" : 169
    }, {
      "referenceID" : 7,
      "context" : "Learning temporally extended actions and temporal abstraction in genral are long standing problems in reinforcement learning [5, 6, 7, 8, 12, 20, 21, 22, 23, 25, 27, 28].",
      "startOffset" : 125,
      "endOffset" : 169
    }, {
      "referenceID" : 11,
      "context" : "Learning temporally extended actions and temporal abstraction in genral are long standing problems in reinforcement learning [5, 6, 7, 8, 12, 20, 21, 22, 23, 25, 27, 28].",
      "startOffset" : 125,
      "endOffset" : 169
    }, {
      "referenceID" : 19,
      "context" : "Learning temporally extended actions and temporal abstraction in genral are long standing problems in reinforcement learning [5, 6, 7, 8, 12, 20, 21, 22, 23, 25, 27, 28].",
      "startOffset" : 125,
      "endOffset" : 169
    }, {
      "referenceID" : 20,
      "context" : "Learning temporally extended actions and temporal abstraction in genral are long standing problems in reinforcement learning [5, 6, 7, 8, 12, 20, 21, 22, 23, 25, 27, 28].",
      "startOffset" : 125,
      "endOffset" : 169
    }, {
      "referenceID" : 21,
      "context" : "Learning temporally extended actions and temporal abstraction in genral are long standing problems in reinforcement learning [5, 6, 7, 8, 12, 20, 21, 22, 23, 25, 27, 28].",
      "startOffset" : 125,
      "endOffset" : 169
    }, {
      "referenceID" : 22,
      "context" : "Learning temporally extended actions and temporal abstraction in genral are long standing problems in reinforcement learning [5, 6, 7, 8, 12, 20, 21, 22, 23, 25, 27, 28].",
      "startOffset" : 125,
      "endOffset" : 169
    }, {
      "referenceID" : 24,
      "context" : "Learning temporally extended actions and temporal abstraction in genral are long standing problems in reinforcement learning [5, 6, 7, 8, 12, 20, 21, 22, 23, 25, 27, 28].",
      "startOffset" : 125,
      "endOffset" : 169
    }, {
      "referenceID" : 26,
      "context" : "Learning temporally extended actions and temporal abstraction in genral are long standing problems in reinforcement learning [5, 6, 7, 8, 12, 20, 21, 22, 23, 25, 27, 28].",
      "startOffset" : 125,
      "endOffset" : 169
    }, {
      "referenceID" : 27,
      "context" : "Learning temporally extended actions and temporal abstraction in genral are long standing problems in reinforcement learning [5, 6, 7, 8, 12, 20, 21, 22, 23, 25, 27, 28].",
      "startOffset" : 125,
      "endOffset" : 169
    }, {
      "referenceID" : 20,
      "context" : "The options framework [21, 28] provides a general formulation.",
      "startOffset" : 22,
      "endOffset" : 30
    }, {
      "referenceID" : 27,
      "context" : "The options framework [21, 28] provides a general formulation.",
      "startOffset" : 22,
      "endOffset" : 30
    }, {
      "referenceID" : 6,
      "context" : "Options are typically learned using subgoals and ’pseudo-rewards’ that are provided explicitly [7, 8, 28].",
      "startOffset" : 95,
      "endOffset" : 105
    }, {
      "referenceID" : 7,
      "context" : "Options are typically learned using subgoals and ’pseudo-rewards’ that are provided explicitly [7, 8, 28].",
      "startOffset" : 95,
      "endOffset" : 105
    }, {
      "referenceID" : 27,
      "context" : "Options are typically learned using subgoals and ’pseudo-rewards’ that are provided explicitly [7, 8, 28].",
      "startOffset" : 95,
      "endOffset" : 105
    }, {
      "referenceID" : 13,
      "context" : "Recently [14, 29] have demonstrated that combining deep learning with pre-defined subgoals delivers promising results in challenging environments like Minecraft and Atari, however, subgoal discovery remains an unsolved problem.",
      "startOffset" : 9,
      "endOffset" : 17
    }, {
      "referenceID" : 28,
      "context" : "Recently [14, 29] have demonstrated that combining deep learning with pre-defined subgoals delivers promising results in challenging environments like Minecraft and Atari, however, subgoal discovery remains an unsolved problem.",
      "startOffset" : 9,
      "endOffset" : 17
    }, {
      "referenceID" : 0,
      "context" : "Another recent work by [1] shows a theoretical possibility of learning options jointly with a policy-over-options by extending the policy gradient theorem to options, but the approach was only tested on a toy problem.",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 9,
      "context" : "For updating both plans we use attentive writing technique [10], which allows the network to focus on parts of a plan",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 9,
      "context" : "To achieve this, we apply differentiable attentive reading and writing operations [10], where attention is defined over the temporal dimension.",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 9,
      "context" : "The differentiability of the attention model [10] makes it possible to train with standard backpropagation.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 9,
      "context" : "We direct readers to [10] for details.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 17,
      "context" : "We use a recently proposed Asynchronous Advantage Actor-Critic (A3C) method [18], which directly optimizes the policy of an agent.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 12,
      "context" : "For zt we employ the re-parametrization trick [13, 24].",
      "startOffset" : 46,
      "endOffset" : 54
    }, {
      "referenceID" : 23,
      "context" : "For zt we employ the re-parametrization trick [13, 24].",
      "startOffset" : 46,
      "endOffset" : 54
    }, {
      "referenceID" : 2,
      "context" : "For gt, we set∇ct−1 1 ≡ ∇gt as proposed in [3].",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 8,
      "context" : "We use three domains of increasing complexity: supervised next character prediction in text [9], 2D maze navigation and ATARI games [2].",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 1,
      "context" : "We use three domains of increasing complexity: supervised next character prediction in text [9], 2D maze navigation and ATARI games [2].",
      "startOffset" : 132,
      "endOffset" : 135
    }, {
      "referenceID" : 10,
      "context" : "For 2D mazes and ATARI it is a convolutional neural net (CNN) and it is an LSTM [11] for text.",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 10,
      "context" : "The LSTM [11] architecture is a widely used recurrent network and it was demonstrated to perform well on a suite of reinforcement learning problems [18].",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 17,
      "context" : "The LSTM [11] architecture is a widely used recurrent network and it was demonstrated to perform well on a suite of reinforcement learning problems [18].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 17,
      "context" : "We use the A3C method [18] for all reinforcement learning experiments.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 17,
      "context" : "It was shown to achieve state-of-the-art results on several challenging benchmarks [18].",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 18,
      "context" : "We cut the trajectory and run backpropagation through time [19] after 40 forward passes of a network or if a terminal signal is received.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 15,
      "context" : "To demonstrate that it is capable of learning output patterns with complex structure we present a qualitative experiment on next character prediction using Penn Treebank dataset [16].",
      "startOffset" : 178,
      "endOffset" : 182
    }, {
      "referenceID" : 16,
      "context" : "This is the same architecture as in [17, 18], the only difference is that in pre-processing stage we keep colour channels.",
      "startOffset" : 36,
      "endOffset" : 44
    }, {
      "referenceID" : 17,
      "context" : "This is the same architecture as in [17, 18], the only difference is that in pre-processing stage we keep colour channels.",
      "startOffset" : 36,
      "endOffset" : 44
    } ],
    "year" : 2016,
    "abstractText" : "We present a novel deep recurrent neural network architecture that learns to build implicit plans in an end-to-end manner by purely interacting with an environment in reinforcement learning setting. The network builds an internal plan, which is continuously updated upon observation of the next input from the environment. It can also partition this internal representation into contiguous subsequences by learning for how long the plan can be committed to – i.e. followed without re-planing. Combining these properties, the proposed model, dubbed STRategic Attentive Writer (STRAW) can learn high-level, temporally abstracted macroactions of varying lengths that are solely learnt from data without any prior information. These macro-actions enable both structured exploration and economic computation. We experimentally demonstrate that STRAW delivers strong improvements on several ATARI games by employing temporally extended planning strategies (e.g. Ms. Pacman and Frostbite). It is at the same time a general algorithm that can be applied on any sequence data. To that end, we also show that when trained on text prediction task, STRAW naturally predicts frequent n-grams (instead of macroactions), demonstrating the generality of the approach.",
    "creator" : "LaTeX with hyperref package"
  }
}