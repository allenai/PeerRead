{
  "name" : "1510.04390.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Dual Principal Component Pursuit",
    "authors" : [ "Manolis C. Tsakiris", "René Vidal" ],
    "emails" : [ "m.tsakiris@jhu.edu", "rvidal@jhu.edu" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Principal Component Analysis (PCA) is one of the oldest [16, 11] and most fundamental techniques in data analysis, enjoying ubiquitous applications in modern science and engineering [12]. Given a data matrix X ∈ RD×L of L data points of dimension D, PCA gives a closed form solution to the problem of fitting, in the Euclidean sense, a d-dimensional linear subspace to the columns of X . Even though the optimization problem associated with PCA is non-convex, it does admit a simple solution by means of the Singular Value Decomposition (SVD) of X . In fact, the d-dimensional subspace V̂ of RD that is closest to the column span of X is precisely the subspace spanned by the first d left singular vectors of X .\nUsing V̂ as a model for the data is meaningful when the data are known to have an approximately linear structure of underlying dimension d, i.e. they lie close to a ddimensional subspace V . In practice, the principal components of X are known to be well-behaved under mild levels of noise, i.e., the angle between V̂ and V is relatively small and more importantly V̂ is optimal when the noise is Gaus-\nsian [12]. However, in the presence of even a few outliers in X , i.e., points whose angle from the underlying ground truth subspace V is large, the angle between V and its estimate V̂ will in general be large. This is to be expected since, by definition, the principal components are orthogonal directions of maximal correlation with all the points of X . This phenomenon, together with the fact that outliers are almost always present in real datasets, has given rise to the important problem of outlier detection in PCA.\nTraditional outlier detection approaches come from robust statistics and include Influence-based Detection, Multivariate Trimming,M -Estimators, Iteratively Weighted Recursive Least Squares and Random Sampling Consensus (RANSAC) [12]. These methods are usually based on non-convex optimization problems, admit limited theoretical guarantees and have high computational complexity; for example, in the case of RANSAC many trials are required. Recently, two attractive methods have appeared [23, 19] with tight connections to compressed sensing [3] and low-rank representation [14]. Both of these methods are based on convex optimization problems and admit theoretical guarantees and efficient implementations. Remarkably, the self-expressiveness method of [19] does not require an upper bound on the number of outliers as the method of [23] does. However, they are both guaranteed to succeed only in the low-rank regime: the dimension d of the underlying subspace V associated to the inliers should be small compared to the ambient dimension D.\nIn this paper we adopt a dual approach to the problem of robust PCA in the presence of outliers, which allows us to transcend the low-rank regime of modern methods such as [23, 19]. The key idea of our approach comes from the fact that, in the absence of noise, the inliers lie inside any hyperplane H1 = Span(b1)⊥ that contains the underlying linear subspace V . This suggests that, instead of attempting to fit directly a low-dimensional linear subspace to the entire data set, as done e.g. in [23], we can search for a hyperplane H1 that contains as many points of the dataset as possible. When the inliers are in general position inside the subspace, and the outliers are in general position out-\nar X\niv :1\n51 0.\n04 39\n0v 1\n[ cs\n.C V\n] 1\n5 O\nct 2\n01 5\nside the subspace, this hyperplane will ideally contain the entire set of inliers together with possibly a few outliers. After removing the points that do not lie in that hyperplane, the robust PCA problem is reduced to one with a potentially much smaller outlier percentage than in the original dataset. In fact, the number of outliers in the new dataset will be at most D − 2, an upper bound that can be used to dramatically facilitate the outlier detection process using existing methods. We think of the direction b1 of the normal to the hyperplaneH1 as a dual principal component of X , as ideally it is an element of V⊥. Naturally, one can continue by finding a second dual principal component by searching for a hyperplane H2 = Span(b2)⊥, with b2 ⊥ b1, that contains as many points as possible from X ∩ H1, and so on, leading to a Dual Principal Component Analysis of X .\nWe pose the problem of searching for such hyperplanes as an `0 cosparsity-type problem, which we relax to a nonconvex `1 problem on the sphere. We provide theoretical guarantees under which every global solution of that problem is a dual principal component. More importantly, we relax this non-convex optimization problem to a sequence of linear programming problems, which, after a finite number of steps, yields a dual principal component. Experiments on synthetic data demonstrate that the proposed method is able to handle more outliers and higher dimensional subspaces than the state-of-the-art methods [23, 19]."
    }, {
      "heading" : "2. Problem Formulation",
      "text" : "We begin by establishing our data model in Section 2.1, then we formulate our DPCP problem conceptually and computationally in Sections 2.2 and 2.3, respectively."
    }, {
      "heading" : "2.1. Data Model",
      "text" : "We employ a deterministic noise-free data model, under which the inliers consist of N points X = [x1, . . . ,xN ] ∈ RD×N that lie in the intersection of the unit sphere SD−1 with an unknown proper subspace V of RD of unknown dimension d. Accordingly, the outliers consist of M arbitrary points O = [o1, . . . ,oM ] ∈ RD×M that lie on SD−1. The dataset, that we assume given, is X̃ = [X O]Γ ∈ RD×L, where L = N + M and Γ is some permutation, indicating that the partition of the columns of X̃ into X and O is unknown. We further assume that the columns of X̃ are in general position in the following sense: First, any d-tuple of inliers and any D-tuple of outliers is linearly indepenent. Second, for any { ξ1, . . . , ξD−1 } ⊂ X̃ , of which at most d − 1 come from X , the hyperplane of RD spanned by ξ1, . . . , ξD−1 does not contain any of the remaining points."
    }, {
      "heading" : "2.2. Conceptual Formulation",
      "text" : "Notice that in our data model we have made no assumption about the dimension of V: indeed, V can be anything from a line to a (D − 1)-dimensional hyperplane. Ideally,\nwe would like to be able to partition the columns of X̃ into those that lie in V and those that don’t. But under such generality, this is not a well-posed problem since X lies inside every subspace that contains V , which in turn may contain some elements of O. In other words, given X̃ and without any other a-priori knowledge, it may be impossible to correctly partition X̃ into X and O. Instead, we formulate the following well-posed problem:\nProblem 1 Partition the columns of X̃ ∈ RD×L into two groups, such that one of the groups is a subset of X̃ with maximal cardinality, with respect to the property of lying inside a (D − 1)-dimensional hyperplane of RD. The usefulness of this formulation is that for large values of γ := M/N , where known methods for outlier detection in PCA fail, one of the groups, say X̃ 1 will contain the entire X together with precisely D − d− 1 columns of O, while the other group, say X̃ 2, will contain the remaining M − (D−d−1) columns of O. Note that the first group is structured in the sense that it must lie in a hyperplane and so in general dim Span(X̃ 1) = D−1. Having the partition X̃ = X̃ 1 ∪ X̃ 2, we can reject the unstructured group X̃ 2 and reconsider the Robust PCA problem on the group X̃ 1. But now the number of outliers has decreased from γ N to D − d − 1. In fact, we can use the upper bound D − 2 on the number of outliers to dramatically facilitate the outlier detection process using other existing methods."
    }, {
      "heading" : "2.3. Computational Formulation",
      "text" : "A natural approach towards solving Problem 1 is to solve\nmin b ||X̃ > b||0 s.t. b 6= 0. (1)\nThe idea behind (1) is that a hyperplane H = Span(b)⊥ contains a maximal number of columns of X̃ if and only if X̃ > b is as sparse as possible. Since (1) is intractable, consider\nmin b ||X̃ > b||1 s.t. ‖b‖2 = 1. (2)\nNotice that the objective in (2) is convex, while the constraint b ∈ SD−1 is non-convex, thus leading to a nonsmooth and non-convex optimization problem.\nProblem 2 When is every global solution b∗ of (2) orthogonal to Span(X )? How can we efficiently solve (2)?\nIn this paper, we propose to relax (2) by a sequence of linear programs of the form\nnk+1 := argmin b>n̂k=1\n∥∥∥X̃>b∥∥∥ 1 , (3)\nwhere n0 is some arbitrary vector and ·̂ indicates normalization to unit `2-norm. We naturally ask:\nProblem 3 Under what conditions does the sequence of (3) converge to a vector n̂∞ that is orthogonal to Span(X )?"
    }, {
      "heading" : "3. Related Work",
      "text" : "In this section, we aim to familiarize the reader with the state-of-the-art of outlier detection in modern single subspace learning (Section 3.1), as well as give a brief overview (Section 3.2) of existing work, that relates technically to the problems of interest of this paper, i.e. problems (2) and (3)."
    }, {
      "heading" : "3.1. Outlier Rejection in PCA",
      "text" : "One of the oldest and most popular outlier detection methods in PCA is Random Sampling Consensus (RANSAC) [12]. The idea behind RANSAC is simple: alternate between randomly sampling d̂ points from the dataset and computing a subspace model for these points, until a model is found that fits a maximal number of points in the entire dataset within some error ε. RANSAC is usually characterized by high performance, when not both d̂ and the oultier percentage are large; otherwise it requires a high computational time, particularly when d is unknown and d̂ is allowed to vary, since exponentially many trials are required in order to sample outlier-free subsets, and thus obtain reliable models. Moreover, its performance is very sensitive on in the input parameters d̂ and ε.\nAmong many other outlier detection methods (see Section 1), in the remaining of this section we will focus on the modern low-rank/sparse-representation theoretic methods of [23] and [19], which we will later use experimentally to compare against our proposed method.\nThe first method [23], referred to as L21, is a variation of the Robust PCA algorithm of [13, 2], which computes a (`∗ + `21)-norm decomposition1 of the data matrix, instead of the (`∗ + `1)-decomopsition in [2]. More specifically, L21 solves the convex optimization problem\nmin L,E: X̃=L+E ‖L‖∗ + λ ‖E‖21 . (4)\nIt is shown in [23] that, under certain conditions, the optimal solution to this problem is of the form L = [X 0D×M ]Γ and E = [0D×N O]Γ. That is, the nonzero columns of the L matrix give the inliers and the nonzero columns of the E matrix give the outliers. However, the theoretical conditions require the intrinsic dimension d = dimV and the outlier percentage to be small enough.\nThe second method that we consider, referred to as SE, is based on the self-expressiveness property of the data matrix, a notion popularized by the work of [4, 5] in the area of subspace clustering [22]. More specifically, if a column of X̃ is an inlier, then it can in principle be expressed as a linear combination of d other columns of X̃ , which are inliers. If the column is instead an outlier, then it will in principle be\n1Here `∗ denotes the nuclear norm of a matrix, i.e., the sum of its singular values, and `21 is defined as the sum of the Euclidean norms of the columns of a matrix.\nexpressible as a linear combination of not less than D other columns. To encourage each point to express itself as a linear combination of the smallest number of other data points, the following convex optimization problem is solved:\nmin C ‖C‖1 , s.t. X̃ = X̃C, Diag(C) = 0. (5)\nIf d is small enough with respect to D, an element is declared as an outlier if the `1 norm of its coefficient vector in C is large; see [19] for an explicit formula. SE admits theoretical guarantees [19] and efficient ADMM implementations [5]. However, as it is clear from its description, it is expected to succeed only when d is sufficiently small. In contrast though to L21, SE has the remarkable property that it can, in principle, handle an arbitrary number of outliers."
    }, {
      "heading" : "3.2. Connections with Compressed Sensing and Dictionary Learning",
      "text" : "Problems of the form\nmin b ||Ωb||0 s.t. b 6= 0, (6)\nand variants of its relaxations have appeared on several occasions and in diverse contexts in the literature, but are much less understood than the now classic sparse [1] and cosparse [15] problems of the form\nmin x ||x||0 s.t. Ax = b (7)\nmin x ||Ωx||0 s.t. Ax = b, (8)\nrespectively. The main source of difficulty is that, in contrast to (8), obtaining tight convex relaxations of (6) is a hard problem. One of the first instances where (6) was considered was in the context of blind source separation [24], where it was proposed to relax it with the problem\nmin b ||Ωb||1 s.t. ‖b‖2 ≥ 1. (9)\nThis is still a non-convex problem, and a heuristic based on quadratic programming was proposed to solve it.\nIt was not until very recently, that the convex relaxation\nmin b ||Ωb||1 s.t. b>w = 1 (10)\nwas proposed, with w taken to be a row or a sum of two rows of Ω, and theorems of correctness were given in the context of dictionary learning [20]. Notice that our proposed convex relaxations (3) can be seen as a generalization of (10). In the context of finding the sparsest vector in a subspace, which is intrinsically related to dictionary learning, an alternating direction minimization scheme was proposed in [17, 18] to solve a relaxation of the form\nmin b,x: ||b||2=1\n||Ωb− x||22 + λ ‖x‖1 . (11)\nRemarkably, under some mild conditions, this was shown to converge with high probability to a global solution of\nmin b ||Ωb||1 s.t. ‖b‖2 = 1. (12)\nThe geometry of (12) was further studied in a probabilistic framework in the recent [21], after replacing the `1-norm with a smooth surrogate."
    }, {
      "heading" : "4. Theoretical Analysis",
      "text" : "In this section we state and discuss our main theoretical results2, regarding problems (2) and (3). Before doing so though, we need to introduce additional notation and draw some interesting connections with the field of numerical integration on the sphere (Section 4.1)."
    }, {
      "heading" : "4.1. An Integration Perspective",
      "text" : "To begin with, for a vector b ∈ SD−1, denote by fb : SD−1 → R+ the function y 7→ ∣∣∣b>y∣∣∣. Then given a set of L points Y ⊂ SD−1, the quantity\n1\nL ∥∥∥Y >b∥∥∥ 1 = 1 L L∑ j=1 ∣∣∣b>yj∣∣∣ = 1L L∑ j=1 fb(yj) (13)\nis a discrete approximation of the integral∫ y∈SD−1 fb(y)dµ = ∫ y∈SD−1 |y>b|dµ, (14)\nwhere µ is the uniform measure on SD−1 and cD is the mean height of the unit hemisphere of RD, given in closed form by\ncD = (D − 2)!! (D − 1)!!\n· {\n2 π if D even 1 if D odd , (15)\nwhere the double factorial is defined as\nk!! := { k(k − 2)(k − 4) · · · 4 · 2 if k even k(k − 2)(k − 4) · · · 3 · 1 if k odd (16)\nA useful fact is that cD is a decreasing function of D and in fact tends to zero as D goes to infinity.\nNow, observe that because of the symmetry of SD−1, the integral in (14) does not depend on b. However, the integration error ∣∣∣∣∣∣cD − 1L L∑ j=1 fb(y)\n∣∣∣∣∣∣ (17) does depend both on the direction of b as well as the distribution of the points Y on SD−1. It is clear though, that\n2All proofs are omitted due to space limitations.\nthe more uniformly the points are distributed, the smaller will be the dependence of the integration error on the direction of b. We note here that the notion of uniform point set distribution on the sphere is a non-trivial one. In a deterministic setting, this is an active subject of study in the fields of combinatorial geometry and numerical integration on the sphere [9, 8]. A widely used measure of the uniformity of a point set on the sphere is the so-called point set discrepancy DSL(Y ) of the set, which can be defined in terms of spherical harmonics as\nDSL(Y ) := sup m≥1\n1\nmD max i=1,...,Z(D,m) ∣∣∣∣∣∣ 1L L∑ j=1 Sm,i(yj) ∣∣∣∣∣∣ , (18)\nwhere Z(D,m) is the dimension of the vector space of spherical harmonics of order m, and Sm,i is the i-th basis element. It is then a fact that the integration error is small if and only if DSL(Y ) is small.\nAs before, for any b ∈ SD−1 we define a vector valued function fb : SD−1 → RD by y\nfb7−→ Sign(b>y)y. Note that the image of fb is SD−1 ∪ 0 and that points that are orthogonal to b are mapped to 0. Moreover,\nLemma 1 ∫ y∈SD−1 Sign(b >y)ydµ = cD b, ∀b ∈ SD−1.\nThis result suggests that the quantity yb := 1 L ∑L j=1 Sign(b\n>yj)yj can be interpreted as a discrete approximation of the integral ∫ y∈SD−1 fb(y)dµ and so the more uniformly distributed are the points Y , the closer yb is to the quantity cD b.\nThe above discussion motivates defining the quantities O and X , to capture the uniformity of outliers and inliers, respectively:\nO := max b∈SD−1 ‖cD b− ob‖2 , (19)\nob := 1\nM M∑ j=1 Sign(b>oj)oj , (20)\nX := max v∈SD−1∩V ‖cd v − χv‖2 (21)\nχv := 1\nN N∑ j=1 Sign(v>xj)xj . (22)"
    }, {
      "heading" : "4.2. The Non-Convex Problem",
      "text" : "Before we consider the discrete non-convex problem (2), it is instructive to examine its continuous counterpart\nmin b>b=1 M ∫ o∈SD−1 ∣∣∣b>o∣∣∣ dµ+ +N\n∫ x∈V∩SD−1 ∣∣∣b>x∣∣∣ dσ, (23)\nwhere σ is the uniform measure on V ∩ SD−1. Of course this problem is only of theoretical interest and serves in establishing a first intuition for the idea behind (2). In fact,\nTheorem 1 Any global solution to problem (23) must be orthogonal to V .\nThe proof of the above theorem follows easily from the symmetry of the sphere, since the first integral appearing in (23) does not depend on b, while the second integral depends only on the angle of b from V .\nTheorem 1 suggests that under sufficiently welldistributed point sets of inliers and outliers, any global solution to the discrete problem (2) should also be orthogonal to the span of the inliers. Before stating the precise result, we need one last piece of notation:\nDefinition 1 For a set Y = [y1, . . . ,yL] ⊂ SD−1 and integer K, define RY ,K to be the maximum circumradius among all polytopes Conv ( ±yj1 ± yj2 ± · · · ± yjK ) , where j1, . . . , jK are distinct integers in [L], and Conv(·) indicates the convex hull operator.\nTheorem 2 Suppose that the quantity γ := MN satisfies\nγ < min { cd − X 2 O , cd − X − (RO,K1 +RX ,K2) /N O } ,\n(24)\nfor all positive integers K1,K2 such that K1 +K2 ≤ D − 1,K2 ≤ d − 1. Then any global solution b∗ to (2) will be orthogonal to Span(X ).\nTowards interpreting this result, consider first the asymptotic case where we allow N and M to go to infinity, while keeping the ratio γ constant. Under point set uniformity, i.e. under the hypothesis that limN→∞DSN (X ) = 0 and limM→∞D S M (O) = 0, we will have that limN→∞ X = 0 and limM→∞ O = 0, in which case (24) is satisfied. This suggests the interesting fact that when the number of inliers is a linear function of the number of outliers, then (2) will always give a normal to the inliers even for arbitrarily large number of outliers and irrespectively of the subspace dimension d. Along the same lines, for a given γ and under the point set uniformity hypothesis, we can always increase the number of inliers and outliers (thus decreasing X and O), while keeping γ constant, until (24) is satisfed, once again indicating that (2) is possible to yield a normal to the space of inliers irrespectively of their intrinsic dimension."
    }, {
      "heading" : "4.3. The Sequence of Convex Relaxations",
      "text" : "In this section we consider the sequence of convex relaxations (3); in particular, there are two important issues to be addressed. First, note that relaxing the constraint b>b = 1 in (2) with a linear constraint b>n̂ = 1 as in (10), has already been found to be of limited theoretical guarantees\n[20]. So it is natural to ask whether the idea of considering a sequence of such relaxations b>n̂k = 1, k = 0, 1, . . . has an intrinsic merit or not, irrespectively of the data distribution. For example, if the data is perfectly well distributed, yet the sequence does not yield vectors orthogonal to the\ninlier space, then we will know that a-priori the method is limited. Fortunately, this is not the case: when the data is perfectly well distributed, i.e. when we restrict our attention to the continuous analog of (3), given by\nnk+1 = argmin b>n̂k=1\n[ M ∫ o∈SD−1 ∣∣∣b>o∣∣∣ dµ + N\n∫ x∈V∩SD−1 ∣∣∣b>x∣∣∣ dσ] , (25) then the sequence {nk} achieves the property of interest:\nTheorem 3 Consider the sequence of vectors {nk} generated by recursion (25), where n̂0 ∈ SD−1 is arbitrary. Let {φk} be the corresponding sequence of angles from V . Then limk→∞ φk = π 2 , provided that n0 6∈ V .\nThis result suggests that relaxing b>b = 1 with the sequence b>n̂k = 1, k ≥ 0 is intrinsically the right idea.\nThe second issue is how the distribution of the data affects the ability of this sequence of relaxations to give vectors orthogonal to V . The answer is given by Theorem\n4, which says that when the angle between n0 and V is large enough and the data points are well distributed, the sequence (3) will consist of vectors orthogonal to the inlier space, for sufficiently large indices k.\nTheorem 4 Let φ0 be the angle between n0 and V . Suppose that condition (24) on the outlier ratio γ holds true and consider the vector sequence {n̂k} generated by recursion (3). Then after a finite number of terms n̂0, . . . , n̂K , for some K, every term of {n̂k} will be orthogonal to Span(X ), providing that\nφ0 > cos −1 ( cd − X − 2 γ O\ncd + X\n) =: φ∗0. (26)\nFirst note that if (24) is true, then the expression of (26) always defines an angle between 0 and π/2. Second, Theorem 4 can be interpreted using the same asymptotic arguments as Theorem 2. In particular, notice that the lower\nbound on the angle φ0 tends to zero as M,N go to infinity with γ constant. Note also that this result does not show convergence of the sequence n̂k: it only shows that this sequence will eventually satisfy the desired property of being orthogonal to the space of inliers; a convergence result remains yet to be established."
    }, {
      "heading" : "5. Dual Principal Component Pursuit",
      "text" : "So far we have established a mechanism of obtaining an element b1 of V⊥, where V = Span(X ): run the sequence of linear programs (3) until the function ∥∥∥X̃>b̂k∥∥∥ 1 converges within some small ; then assuming no pathological point set distributions, any vector n̂k can be taken as b1. There are two possibilities: either V is a hyperplane of dimension D − 1 or dimV < D − 1. In the first case, b1 is the unique up to scale element of V⊥, which proves that in this case the sequence of (3) in fact converges. In such a case, we can identify our subspace model with the hyperplane defined by the normal b1. Next, if dimV < D − 1, we can proceed to find a second element b2 of V⊥ that is orthogonal to b1 and so on. This naturally leads to the Dual Principal Component Pursuit shown in Algorithm 1.\nA few comments are in order. In Algorithm 1, c is an estimate for the codimension D − d of the inlier subspace Span(X ). If c is rather large, then in the computation of each bi, it is more efficient to reduce the coordinate rep-\nAlgorithm 1 Dual Principal Component Pursuit 1: procedure DPCP(X̃ , c, , Tmax) 2: B ← ∅; 3: for i = 1 : c do 4: k ← 0; ∆J0 ←∞; 5: n0 ← argminn̂:‖n̂‖2=1, n̂⊥b1,...,bi−1 ∥∥∥X̃>n̂∥∥∥ 2 ;\n6: while k ≤ Tmax and ∆J0 > do 7: k ← k + 1; 8: nk ← argminn:n>n̂k−1=1,n⊥B ∥∥∥X̃>n∥∥∥ 1 ;\n9: ∆Jk ← ∥∥∥X̃>n̂k−1∥∥∥ 1 − ∥∥∥X̃>n̂k∥∥∥ 1 ;\n10: end while 11: bi ← n̂k; 12: B ← B ∪ {bi}; 13: end for 14: return B; 15: end procedure\nresentation of the data by replacing X̃ with πi(X̃ ), where πi : RD → RD−(i−1), i ≥ 2, is the orthogonal projection onto Span(b1, . . . , bi−1)⊥, and solve the linear program in step 8 in the projected space.\nNotice further how the algorithm initializes n0: This is effectively the right singular vector of πi(X̃ )>, that corresponds to the smallest singular value. As it will be demonstrated in Section 6, this choice has the effect that the angle of n0 from the inlier subspace is typically large, in particular, larger than the smallest initial angle (26) required for the success of the principal component pursuit of (3)."
    }, {
      "heading" : "6. Experiments",
      "text" : "In this section we investigate experimentally the proposed DPCP Alg. 1. Using both synthetic (subsection 6.1) and real data (subsection 6.2), we compare DPCP to the three methods SE, L21 and RANSAC discussed in Section 3.1 as well as to the method of eq. (11) discussed in Section 3.2, which we will refer to as SVS (Sparsest Vector in a Subspace). The parameters of the methods are set to fixed values, chosen such that the methods work well across all tested dimension and outlier configurations. In particular, we use αSE = 100, τL21 = 100 and λL21 = 3/(7 √ M); see [19] and [23] for details. Regarding DPCP, we fix Tmax = 10, = 10\n−6, and unless otherwise noted, we set c equal to the true codimension of the subspace."
    }, {
      "heading" : "6.1. Synthetic Data",
      "text" : "To begin with, we evaluate the performance of DPCP in the absence of noise, for various subspace dimensions d = 1 : 1 : 29 and outlier percentagesR := M/(M+N) = 0.1 : 0.1 : 0.9. We fix the ambient dimensionD = 30, sample N = 200 inliers uniformly at random from V ∩ SD−1\nandM outliers uniformly at random from SD−1. We are interested in examining the ability of DPCP to recover a single normal vector (c = 1) to the subspace, by means of recursion (3). The results are shown in Fig. 1 for 10 independent trials. Fig. 1(a) shows whether the theoretical conditions of (24) are satisfied or not. In checking these conditions, we estimate the abstract quantities O, X ,RO,K1 ,RX ,K2 by Monte-Carlo simulation. Whenever these conditions are satisfied, we choose b0 in a controlled fashion, so that its angle φ0 from the subspace is larger than the minimal angle φ∗0 of (26), and then we run DPCP; if the conditions are not true, we do not run DPCP and report a 0. Fig 1(b) shows the angle of b10 from the subspace. We see that whenever (24) is true, DPCP returns a normal after only 10 iterations. Fig 1(c) shows that if we initialize b0 randomly, then its angle φ0 from the subspace becomes less than the minimal angle φ∗0, as d increases. Even so, Fig. 1(d) shows that DPCP still yields a numerical normal, except for the regime where both d andR are very high. Notice that this is roughly the regime where we have no theoretical guarantees in Fig. 1(a). Fig. 1(e) shows that if we initialize b0 as the right singular vector of X̃ > corresponding to the smallest singular value, then φ0 > φ ∗ 0 is true for most cases, and the corresponding performance of DPCP in Fig. 1(f) improves further. Finally, Fig. 1(g) plots φ∗0. We see that for very low d this angle is almost zero, i.e. DPCP does not depend on the initialization, even for large R. As d increases though, so does φ∗0, and in the extreme case of the upper rightmost regime, where d and R are very high, φ∗0 is close to 90\no, indicating that DPCP will succeed only if b0 is very close to V⊥.\nNext, for the same range of R and d, and still in the absence of noise, we examine the potential of each of SE, L21, SVS, RANSAC and DPCP to perfectly distinguish outliers from inliers. Note that each of these methods returns a signal α ∈ RN+M+ , which can be thresholded for the purpose of declaring outliers and inliers. For SE, α is the `1-norm of the columns of the coefficient matrix C, while for L21 it is the `2-norm of the columns of E. Since RANSAC, SVS and DPCP directly return subspace models, for these methods α is simply the distances of all points to the estimated subspace model. In Fig. 2 we depict success versus failure, where success is interpreted as the existence of a threshold on α that perfectly separates outliers and inliers. As expected, the low-rank methods SE and L21 can not cope with large dimensions even in the presence of 10 − 20% outliers. As expected, RANSAC is very successful irrespectively of dimension, when R is small, since the probability of sampling outlier-free subsets is high. But as soon as R increases, its performance drops dramatically. Moving on, SVS is the worst performing method, which we attribute to its approximate nature. Remarkably, DPCP performs perfectly irrespectively of dimension for up to 50% outliers. Note that we use the true codimension c of the subspace\nas input to DPCP; this is to ascertain the true limits of the method. Certainly, in practice only an estimate for c can be used. As we have observed from experiments, the performance of DPCP typically does not change much if the codimension is underestimated; however performance can deteriorate significantly if the true c is overestimated. Moreover, we note that while SE, L21 and SVS are extremely fast, as they rely on ADMM implementations, DPCP is much slower, even if we use an optimizer such as Gurobi [10]. Speeding up DPCP is the subject of current research.\nFinally, in Fig. 3 we show ROC curves associated with the thresholding of α for varying levels of noise and outliers. When d is small, Fig. 3(a) shows that SE, L21 and DPCP are equally robust giving perfect separation between outliers and inliers, while SVS and RANSAC perform poorly. Interestingly, for large d (Fig. 3(a)), DPCP gives considerably less False Positives (FP) than all other methods across all cases, indicating once again its unique property of being able to handle large subspace dimensions in the presence of many outliers."
    }, {
      "heading" : "6.2. Real Data",
      "text" : "In this subsection we consider an outlier detection scenario in PCA using real images. The inliers are taken to be all N = 64 face images of a single individual from the Extended Yale B dataset [7], while the M outliers are randomly chosen from Caltech101 [6]. All images are cropped to size 48 × 42 as was done in [5]. For a fair comparison, we run SE on the raw 2016-dimensional data, while all other methods use projected data onto dimension D = 50. Since it is known that face images of a single individual under different lighting conditions lie close to an approximately 9-dimensional subspace [5], we choose the codimension parameter of DPCA to be c = 41. We perform 10 independent trials for each individual across all 38 individuals for a different number of outliers M = 32, 64, 128 and report the ensemble ROC curves in Fig. 4. As is evident, DPCA is the most robust among all methods."
    }, {
      "heading" : "7. Conclusions",
      "text" : "We presented Dual Principal Component Pursuit (DPCP), a novel `1 outlier detection method, which is based on solving an `1 problem on the sphere by linear programs over a sequence of tangent spaces on the sphere. DPCP is able to handle subspaces of as low codimension as 1 in the presence of as many outliers as 50%. Future research will be concerned with speeding up the method as well as extending it to multiple subspaces and other types of data corruptions, such as missing entries and entry-wise errors."
    }, {
      "heading" : "Acknowledgement",
      "text" : "This work was supported by grant NSF 1447822."
    } ],
    "references" : [ {
      "title" : "From sparse solutions of systems of equations to sparse modeling of signals and images",
      "author" : [ "A.M. Bruckstein", "D.L. Donoho", "M. Elad" ],
      "venue" : "SIAM Review,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2009
    }, {
      "title" : "Robust principal component analysis",
      "author" : [ "E. Candès", "X. Li", "Y. Ma", "J. Wright" ],
      "venue" : "Journal of the ACM,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2011
    }, {
      "title" : "An introduction to compressive sampling",
      "author" : [ "E. Candès", "M. Wakin" ],
      "venue" : "IEEE Signal Processing Magazine,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2008
    }, {
      "title" : "Robust classification using structured sparse representation",
      "author" : [ "E. Elhamifar", "R. Vidal" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2011
    }, {
      "title" : "Sparse subspace clustering: Algorithm, theory, and applications",
      "author" : [ "E. Elhamifar", "R. Vidal" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories",
      "author" : [ "L. Fei-Fei", "R. Fergus", "P. Perona" ],
      "venue" : "Comput. Vis. Image Underst.,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2007
    }, {
      "title" : "From few to many: Illumination cone models for face recognition under variable lighting and pose",
      "author" : [ "A.S. Georghiades", "P.N. Belhumeur", "D.J. Kriegman" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intelligence,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2001
    }, {
      "title" : "Discrepancies of point sequences on the sphere and numerical integration",
      "author" : [ "P.J. Grabner", "B. Klinger", "R.F. Tichy" ],
      "venue" : "Mathematical Research,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1997
    }, {
      "title" : "Spherical designs, discrepancy and numerical integration",
      "author" : [ "P.J. Grabner", "R.F. Tichy" ],
      "venue" : "Math. Comp.,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1993
    }, {
      "title" : "Analysis of a complex of statistical variables into principal components",
      "author" : [ "H. Hotelling" ],
      "venue" : "Journal of Educational Psychology,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1933
    }, {
      "title" : "Principal Component Analysis",
      "author" : [ "I. Jolliffe" ],
      "venue" : "Springer-Verlag, 2nd edition,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2002
    }, {
      "title" : "Fast convex optimization algorithms for exact recovery of a corrupted low-rank matrix",
      "author" : [ "Z. Lin", "A. Ganesh", "J. Wright", "L. Wu", "M. Chen", "Y. Ma" ],
      "venue" : "Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP),",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2009
    }, {
      "title" : "Robust subspace segmentation by low-rank representation",
      "author" : [ "G. Liu", "Z. Lin", "Y. Yu" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2010
    }, {
      "title" : "The cosparse analysis model and algorithms",
      "author" : [ "S. Nam", "M.E. Davies", "M. Elad", "R. Gribonval" ],
      "venue" : "Applied and Computational Harmonic Analysis,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "On lines and planes of closest fit to systems of points in space",
      "author" : [ "K. Pearson" ],
      "venue" : "The London, Edinburgh and Dublin Philosphical Magazine and Journal of Science,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1901
    }, {
      "title" : "Finding a sparse vector in a subspace: Linear sparsity using alternating directions",
      "author" : [ "Q. Qu", "J. Sun", "J. Wright" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2014
    }, {
      "title" : "Finding a sparse vector in a subspace: Linear sparsity using alternating directions",
      "author" : [ "Q. Qu", "J. Sun", "J. Wright" ],
      "venue" : "CoRR, abs/1412.4659,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2014
    }, {
      "title" : "A geometric analysis of subspace clustering with outliers",
      "author" : [ "M. Soltanolkotabi", "E.J. Candès" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2012
    }, {
      "title" : "Exact recovery of sparsely-used dictionaries",
      "author" : [ "D.A. Spielman", "H. Wang", "J. Wright" ],
      "venue" : "In Proceedings of the Twenty-Third international joint conference on Artificial Intelligence,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2013
    }, {
      "title" : "Complete dictionary recovery over the sphere",
      "author" : [ "J. Sun", "Q. Qu", "J. Wright" ],
      "venue" : "CoRR, abs/1504.06785,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2015
    }, {
      "title" : "Subspace clustering",
      "author" : [ "R. Vidal" ],
      "venue" : "IEEE Signal Processing Magazine,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2011
    }, {
      "title" : "Robust PCA via Outlier Pursuit",
      "author" : [ "H. Xu", "C. Caramanis", "S. Sanghavi" ],
      "venue" : "In Neural Information Processing Systems,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2010
    }, {
      "title" : "Blind source separation by sparse decomposition in a signal dictionary",
      "author" : [ "M. Zibulevsky", "B. Pearlmutter" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2001
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "Principal Component Analysis (PCA) is one of the oldest [16, 11] and most fundamental techniques in data analysis, enjoying ubiquitous applications in modern science and engineering [12].",
      "startOffset" : 56,
      "endOffset" : 64
    }, {
      "referenceID" : 9,
      "context" : "Principal Component Analysis (PCA) is one of the oldest [16, 11] and most fundamental techniques in data analysis, enjoying ubiquitous applications in modern science and engineering [12].",
      "startOffset" : 56,
      "endOffset" : 64
    }, {
      "referenceID" : 10,
      "context" : "Principal Component Analysis (PCA) is one of the oldest [16, 11] and most fundamental techniques in data analysis, enjoying ubiquitous applications in modern science and engineering [12].",
      "startOffset" : 182,
      "endOffset" : 186
    }, {
      "referenceID" : 10,
      "context" : ", the angle between V̂ and V is relatively small and more importantly V̂ is optimal when the noise is Gaussian [12].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 10,
      "context" : "Traditional outlier detection approaches come from robust statistics and include Influence-based Detection, Multivariate Trimming,M -Estimators, Iteratively Weighted Recursive Least Squares and Random Sampling Consensus (RANSAC) [12].",
      "startOffset" : 229,
      "endOffset" : 233
    }, {
      "referenceID" : 21,
      "context" : "Recently, two attractive methods have appeared [23, 19] with tight connections to compressed sensing [3] and low-rank representation [14].",
      "startOffset" : 47,
      "endOffset" : 55
    }, {
      "referenceID" : 17,
      "context" : "Recently, two attractive methods have appeared [23, 19] with tight connections to compressed sensing [3] and low-rank representation [14].",
      "startOffset" : 47,
      "endOffset" : 55
    }, {
      "referenceID" : 2,
      "context" : "Recently, two attractive methods have appeared [23, 19] with tight connections to compressed sensing [3] and low-rank representation [14].",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 12,
      "context" : "Recently, two attractive methods have appeared [23, 19] with tight connections to compressed sensing [3] and low-rank representation [14].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 17,
      "context" : "Remarkably, the self-expressiveness method of [19] does not require an upper bound on the number of outliers as the method of [23] does.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 21,
      "context" : "Remarkably, the self-expressiveness method of [19] does not require an upper bound on the number of outliers as the method of [23] does.",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 21,
      "context" : "In this paper we adopt a dual approach to the problem of robust PCA in the presence of outliers, which allows us to transcend the low-rank regime of modern methods such as [23, 19].",
      "startOffset" : 172,
      "endOffset" : 180
    }, {
      "referenceID" : 17,
      "context" : "In this paper we adopt a dual approach to the problem of robust PCA in the presence of outliers, which allows us to transcend the low-rank regime of modern methods such as [23, 19].",
      "startOffset" : 172,
      "endOffset" : 180
    }, {
      "referenceID" : 21,
      "context" : "in [23], we can search for a hyperplane H1 that contains as many points of the dataset as possible.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 21,
      "context" : "Experiments on synthetic data demonstrate that the proposed method is able to handle more outliers and higher dimensional subspaces than the state-of-the-art methods [23, 19].",
      "startOffset" : 166,
      "endOffset" : 174
    }, {
      "referenceID" : 17,
      "context" : "Experiments on synthetic data demonstrate that the proposed method is able to handle more outliers and higher dimensional subspaces than the state-of-the-art methods [23, 19].",
      "startOffset" : 166,
      "endOffset" : 174
    }, {
      "referenceID" : 10,
      "context" : "One of the oldest and most popular outlier detection methods in PCA is Random Sampling Consensus (RANSAC) [12].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 21,
      "context" : "Among many other outlier detection methods (see Section 1), in the remaining of this section we will focus on the modern low-rank/sparse-representation theoretic methods of [23] and [19], which we will later use experimentally to compare against our proposed method.",
      "startOffset" : 173,
      "endOffset" : 177
    }, {
      "referenceID" : 17,
      "context" : "Among many other outlier detection methods (see Section 1), in the remaining of this section we will focus on the modern low-rank/sparse-representation theoretic methods of [23] and [19], which we will later use experimentally to compare against our proposed method.",
      "startOffset" : 182,
      "endOffset" : 186
    }, {
      "referenceID" : 21,
      "context" : "The first method [23], referred to as L21, is a variation of the Robust PCA algorithm of [13, 2], which computes a (`∗ + `21)-norm decomposition1 of the data matrix, instead of the (`∗ + `1)-decomopsition in [2].",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 11,
      "context" : "The first method [23], referred to as L21, is a variation of the Robust PCA algorithm of [13, 2], which computes a (`∗ + `21)-norm decomposition1 of the data matrix, instead of the (`∗ + `1)-decomopsition in [2].",
      "startOffset" : 89,
      "endOffset" : 96
    }, {
      "referenceID" : 1,
      "context" : "The first method [23], referred to as L21, is a variation of the Robust PCA algorithm of [13, 2], which computes a (`∗ + `21)-norm decomposition1 of the data matrix, instead of the (`∗ + `1)-decomopsition in [2].",
      "startOffset" : 89,
      "endOffset" : 96
    }, {
      "referenceID" : 1,
      "context" : "The first method [23], referred to as L21, is a variation of the Robust PCA algorithm of [13, 2], which computes a (`∗ + `21)-norm decomposition1 of the data matrix, instead of the (`∗ + `1)-decomopsition in [2].",
      "startOffset" : 208,
      "endOffset" : 211
    }, {
      "referenceID" : 21,
      "context" : "It is shown in [23] that, under certain conditions, the optimal solution to this problem is of the form L = [X 0D×M ]Γ and E = [0D×N O]Γ.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 3,
      "context" : "The second method that we consider, referred to as SE, is based on the self-expressiveness property of the data matrix, a notion popularized by the work of [4, 5] in the area of subspace clustering [22].",
      "startOffset" : 156,
      "endOffset" : 162
    }, {
      "referenceID" : 4,
      "context" : "The second method that we consider, referred to as SE, is based on the self-expressiveness property of the data matrix, a notion popularized by the work of [4, 5] in the area of subspace clustering [22].",
      "startOffset" : 156,
      "endOffset" : 162
    }, {
      "referenceID" : 20,
      "context" : "The second method that we consider, referred to as SE, is based on the self-expressiveness property of the data matrix, a notion popularized by the work of [4, 5] in the area of subspace clustering [22].",
      "startOffset" : 198,
      "endOffset" : 202
    }, {
      "referenceID" : 17,
      "context" : "(5) If d is small enough with respect to D, an element is declared as an outlier if the `1 norm of its coefficient vector in C is large; see [19] for an explicit formula.",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 17,
      "context" : "SE admits theoretical guarantees [19] and efficient ADMM implementations [5].",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 4,
      "context" : "SE admits theoretical guarantees [19] and efficient ADMM implementations [5].",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 0,
      "context" : "and variants of its relaxations have appeared on several occasions and in diverse contexts in the literature, but are much less understood than the now classic sparse [1] and cosparse [15] problems of the form",
      "startOffset" : 167,
      "endOffset" : 170
    }, {
      "referenceID" : 13,
      "context" : "and variants of its relaxations have appeared on several occasions and in diverse contexts in the literature, but are much less understood than the now classic sparse [1] and cosparse [15] problems of the form",
      "startOffset" : 184,
      "endOffset" : 188
    }, {
      "referenceID" : 22,
      "context" : "One of the first instances where (6) was considered was in the context of blind source separation [24], where it was proposed to relax it with the problem",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 18,
      "context" : "was proposed, with w taken to be a row or a sum of two rows of Ω, and theorems of correctness were given in the context of dictionary learning [20].",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 15,
      "context" : "In the context of finding the sparsest vector in a subspace, which is intrinsically related to dictionary learning, an alternating direction minimization scheme was proposed in [17, 18] to solve a relaxation of the form",
      "startOffset" : 177,
      "endOffset" : 185
    }, {
      "referenceID" : 16,
      "context" : "In the context of finding the sparsest vector in a subspace, which is intrinsically related to dictionary learning, an alternating direction minimization scheme was proposed in [17, 18] to solve a relaxation of the form",
      "startOffset" : 177,
      "endOffset" : 185
    }, {
      "referenceID" : 19,
      "context" : "The geometry of (12) was further studied in a probabilistic framework in the recent [21], after replacing the `1-norm with a smooth surrogate.",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 8,
      "context" : "In a deterministic setting, this is an active subject of study in the fields of combinatorial geometry and numerical integration on the sphere [9, 8].",
      "startOffset" : 143,
      "endOffset" : 149
    }, {
      "referenceID" : 7,
      "context" : "In a deterministic setting, this is an active subject of study in the fields of combinatorial geometry and numerical integration on the sphere [9, 8].",
      "startOffset" : 143,
      "endOffset" : 149
    }, {
      "referenceID" : 18,
      "context" : "[20].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "In particular, we use αSE = 100, τL21 = 100 and λL21 = 3/(7 √ M); see [19] and [23] for details.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 21,
      "context" : "In particular, we use αSE = 100, τL21 = 100 and λL21 = 3/(7 √ M); see [19] and [23] for details.",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 6,
      "context" : "The inliers are taken to be all N = 64 face images of a single individual from the Extended Yale B dataset [7], while the M outliers are randomly chosen from Caltech101 [6].",
      "startOffset" : 107,
      "endOffset" : 110
    }, {
      "referenceID" : 5,
      "context" : "The inliers are taken to be all N = 64 face images of a single individual from the Extended Yale B dataset [7], while the M outliers are randomly chosen from Caltech101 [6].",
      "startOffset" : 169,
      "endOffset" : 172
    }, {
      "referenceID" : 4,
      "context" : "All images are cropped to size 48 × 42 as was done in [5].",
      "startOffset" : 54,
      "endOffset" : 57
    }, {
      "referenceID" : 4,
      "context" : "Since it is known that face images of a single individual under different lighting conditions lie close to an approximately 9-dimensional subspace [5], we choose the codimension parameter of DPCA to be c = 41.",
      "startOffset" : 147,
      "endOffset" : 150
    } ],
    "year" : 2017,
    "abstractText" : "We consider the problem of outlier rejection in single subspace learning. Classical approaches work directly with a low-dimensional representation of the subspace. Our approach works with a dual representation of the subspace and hence aims to find its orthogonal complement. We pose this problem as an `1-minimization problem on the sphere and show that, under certain conditions on the distribution of the data, any global minimizer of this non-convex problem gives a vector orthogonal to the subspace. Moreover, we show that such a vector can still be found by relaxing the non-convex problem with a sequence of linear programs. Experiments on synthetic and real data show that the proposed approach, which we call Dual Principal Component Pursuit (DPCP), outperforms state-of-the art methods, especially in the case of high-dimensional subspaces.",
    "creator" : "LaTeX with hyperref package"
  }
}