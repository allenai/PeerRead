{
  "name" : "1602.05944.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "The Interaction of Memory and Attention in Novel Word Generalization: A Computational Investigation",
    "authors" : [ "Erin Grant", "Aida Nematzadeh", "Suzanne Stevenson" ],
    "emails" : [ "suzanne}@cs.toronto.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 2.\n05 94\n4v 1\n[ cs\n.C L\n] 1\n8 Fe\nb 20\n16"
    }, {
      "heading" : "Introduction",
      "text" : "A number of computational models have successfully mimicked child behaviors in learning the meaning of words from ambiguous input (e.g., Siskind, 1996; Yu & Ballard, 2007; Frank et al., 2007; Fazly, Alishahi, & Stevenson, 2010). However, one challenge in word-meaning acquisition that has received less attention is that of novel word generalization: i.e., correctly identifying the level of a hierarchical taxonomy that a word refers to. After hearing it only a few times, how does the child determine, for example, that the word dog refers to Dalmatians, all dogs of different breeds, or any kind of animal? This issue poses difficulties to the learner because the accumulated evidence can be compatible with more than one of these choices. In this example, all dogs are also animals, and thus the meaning “animal” might also be consistent with all the usages of the word dog.\nXu and Tenenbaum (2007) (henceforth XT07) studied novel word generalization in both children and adults by observing decisions about category membership for novel objects in various experimental settings. One of their important findings concerned how people responded having seen 1 vs. 3 labeled exemplars of a certain kind of entity within a taxonomy. For example, having seen a single Dalmatian labeled as a fep, people assumed that the novel word fep could refer to the general category of dogs. However, if people saw several Dalmatians called fep, they apparently recognized that it would be a suspicious coincidence if fep meant “dog”, but only one breed of dog was observed. In such cases, people had a lesser tendency to generalize to the higher level category than after seeing a single exemplar.\nSpencer, Perone, Smith, and Samuelson (2011) (henceforth SPSS11) investigated the effect of presentation timing in the same task. They found that presenting exemplars in sequence – as opposed to simultaneously, as in XT07– reverses the suspicious coincidence effect. That is, after sequentially viewing three exemplars consistent with a more specific level of the taxonomy (e.g., three dogs of a single breed), people have a greater tendency to generalize to the higher category than after seeing one exemplar. SPSS11 explained this reversal as an interaction of word learning with the more general cognitive processes of attention and memory, which differ in their operation across the presentation types: People attend to and remember finer-grained similarities among objects when viewed simultaneously (e.g., that they are all Dalmatians), while the sequential presentation leads to a consideration of the objects that focuses on their general commonalities (e.g., that they are all dogs).\nOur goal in this paper is to provide a computational model that accounts for both the XT07 and SPSS11 findings in a well-motivated manner, by incorporating memory and attentional constraints into an incremental model of word learning and word generalization. It is desirable to integrate together all these pieces – novel word generalization, incremental word learning, and memory and attention – because: (i) word generalization is part and parcel of learning the meaning of words, since it allows the abstraction of meaning from a sequence of specific experiences, and (ii) many word-learning behaviors are influenced by the general cognitive processes of memory and attention (e.g., Vlach et al., 2008; Samuelson & Smith, 2000). Importantly, by explicitly specifying such mechanisms in a computational model, we contribute to the precise understanding of the interactions between them that are required to account for empirical data."
    }, {
      "heading" : "Suspicious Coincidence: Data and Models",
      "text" : "XT07 and SPSS11 explored how people generalized a novel word like fep to various levels of a taxonomy of objects (including animals, vehicles, and vegetables). Basic-level categories are those whose members share a significant number of salient attributes (e.g., dogs or trucks); subordinate categories occur lower in the hierarchy, and their members share many fine-grained attributes (e.g., Dalmatians or bulldozers); superordinate categories (e.g., animals and vehicles) are higher than the basic-level, and their members have fewer attributes in common (Rosch, 1973).\nFor the sake of space, we focus only on two of the train-\ning conditions in XT07 and SPSS11– the “1-example” and “3-subordinate” conditions – in which the suspicious coincidence effect and its reversal are seen.1 The 1-example condition has one training trial in which participants observe a single object (e.g., a Dalmatian) that is labeled with a novel word (such as fep). In the 3-subordinate condition, participants observe three instances from the same subordinate category (e.g., three different Dalmatians) labeled with the novel word. In XT07’s experiment, all three instances were presented simultaneously. SPSS11 included a condition in which the three instances are shown and labeled sequentially. (Simultaneous and sequential are the same for one example.)\nAfter training, participants select all and only objects that they think are feps from a set of test items. Each test object is assessed as exactly one of the following types of match:\n• a subordinate match has the same subordinate category as a training object (e.g., a Dalmatian).\n• a basic-level match has the same basic-level category as a training object (e.g., a dog, but not a Dalmatian).\n• a superordinate match has the same superordinate category as training objects (e.g., an animal other than a dog).\nSince SPSS11 replicated the pattern found by XT07 in their simultaneous presentation condition, we report only the results of SPSS11, as shown in Fig. 1.\nSPSS11 data for (1a) simultaneous and (1b) sequential presentations. Each bar is the percent of chosen test objects of each type of match: subord(inate), basic(-level), or super(ordinate). Differences in 1-ex. across the two experiments were not statistically significant.\nIn the 1-example condition people generalized the novel word to refer to both subordinate matches (e.g., Dalmatians) and (to a lesser extent) basic-level matches (e.g., other kinds of dogs), but not to the superordinate matches (e.g., other animals). This is in line with the idea that people tend to generalize a novel word to a basic-level category such as “dog” because of the perceptual salience of this level of categorization (e.g., Markman, 1991).\nIn the 3-subordinate condition, when objects are presented simultaneously (Figure 1a), the generalization to the basic level is attenuated compared to the 1-example condition.\n1Our model replicates the results of XT07 and SPSS11 for all training conditions, but we only report the results for these two here.\nXT07 explained this behavior as the suspicious coincidence effect. However, when objects are presented sequentially (Figure 1b), there was a surprising reversal of this effect.\nWhile SPSS11 outline possible memory and attentional processes to explain their results, we know of no computational model that can account for both sets of data. XT07’s Bayesian model formed hypotheses over a detailed hierarchical taxonomy to account for their own data, but it cannot model the difference between presentation timings, as SPSS11 note. The computational word learner of Nematzadeh, Grant, and Stevenson (2015) (henceforth NGS15) can model the XT07 results without the need for elaborated knowledge of the hierarchy or a built-in basic-level bias. Instead, the results of the model arise from a general type-token frequency interaction of the sort that commonly arises in explanations of linguistic phenomena (e.g., Bybee, 1985; Croft & Cruse, 2004). However, the timing of presentations also has no effect on the NGS15 model, and so the reversal of the suspicious coincidence effect is not achieved. In the next section, we explain how the NGS15 model can be naturally extended to integrate memory and attention, and therefore sensitivity to presentation timing."
    }, {
      "heading" : "Our Computational Model",
      "text" : "We start with the NGS15 model because it uses an incremental word learning framework that mimics a range of behaviors in vocabulary acquisition (e.g., Fazly, Alishahi, & Stevenson, 2010; Fazly, Ahmadi-Fakhr, et al., 2010). This framework has recently been extended to incorporate the effects of memory and attention on word learning (Nematzadeh, Fazly, & Stevenson, 2012), presenting a natural opportunity for integrating these processes within word generalization. We describe the NGS15 model, then the novel extensions that enable our model to replicate the SPSS11 data."
    }, {
      "heading" : "Learned Meanings in the NGS15 Model",
      "text" : "The NGS15 model is a cross-situational learner that tracks weighted co-occurrences of words and semantic features across its input as in Fazly, Alishahi, and Stevenson (2010). The input to the model is intended to reflect the naturalistic input a child is exposed to, which consists of linguistic input (the words a child hears) paired with nonlinguistic data (the things a child perceives). An input pair is the set of words Ut and the set of semantic features St observed at time t:\nUt : { look, a, fep } St : { PERCEPTION, LOOK, . . . , DALMATIAN, DOG, ANIMAL }\nThe output of the model at each time t is a set of meaning probabilities, Pt( fi|w j), for each feature fi and each word w j observed up through time t. The set of all conditional probabilities Pt( fi|w j) for w j represents the meaning of w j .\nThe representation of meaning in NGS15 reflects the structure of taxonomic knowledge. Meaning features are arranged into feature groups, each corresponding to a level of the taxonomic hierarchy, as shown in Figure 2. For each word\nw j, a meaning probability distribution, Pt(.|w j), is calculated for each feature group; that is, Pt(.|w j) is normalized over the features in a group, rather than over all meaning features. The result is that features at the same level of the hierarchy, such as DALMATIAN and POODLE, or DOG and CAT, compete for probability mass; this ensures that such features, which are mutually incompatible given their taxonomic relationship, cannot simultaneously have high probability. Features at different levels of the hierarchy are in different feature groups and thus do not compete for probability mass; this ensures that meaning probabilities such as Pt(DALMATIAN|fep), Pt(DOG|fep), and Pt(ANIMAL|fep) can all be highly activated if fep is intended to refer to a Dalmatian (which is also both a dog and an animal). In this approach, the meaning of a word is the set of n distributions, Pt(.|w j), one per feature group in a taxonomy with n levels."
    }, {
      "heading" : "The NGS15 Learning Algorithm",
      "text" : "The input to the model is processed in an incremental twostep bootstrapping framework: Words and features that cooccur are aligned in proportion to the current meaning probabilities, which are then updated with the new evidence regarding strength of association, as follows.\nThe Alignment Step. For an input pair at time t, the alignment strength for each feature fi ∈ St and word w j ∈Ut is:\nat( fi,w j) = Pt−1( fi|w j)\n∑ w′∈Ut\nPt−1( fi|w′) (1)\nThese alignments are incrementally accumulated as:\nassoct( fi,w j) = ∑ t′∈T at′( fi,w j) (2)\nwhere T is all times at which fi and w j have co-occurred. In our model, if more than one instance of a feature occurs at time t, multiple instances of the alignment for that feature and word are recorded. For example, in the simultaneous presentation of three exemplars, at( fi,w j) will contribute three times to the association score for each feature fi in the input.\nUpdate of Meaning Probabilities The model next uses the association scores to update the meaning probabilities. Each meaning probability Pt( fi|w j) represents the magnitude of the fi–w j association relative to the association strength between w j and other features within the same feature group G as fi:\nPt( fi|w j) = assoct( fi,w j)+ γ tG\n∑ fm∈G\nassoct( fm,w j)+ kGγ tG (3)\nHere kG and γ tG are smoothing terms: kG reflects the expected number of features in G and γ t\nG represents the a priori ten-\ndency to observe a feature in G . While kG is a fixed parameter, γ tG is a function of the number of observed types within the feature group G , and thus changes over time (see NGS15).\nThe γ tG parameters are key to the generalization behavior of the NGS15 model because they influence how much probability mass is allocated to a feature previously unseen with a word (cf. Eqn. 3 when the assoc score is 0). A higher value for γ t\nG leads to more probability mass allocated to previously unseen features in group G , allowing for more generalization to new features in that group. Because γ tG increases with the number of types, it captures the oft-observed tendency in language that people more readily generalize categories for which a greater variety of types of items has been observed. The model matches the child data from XT07 by equating γ 0\nG\nacross feature groups. But to match the adults, who show a stronger basic-level bias, the model required that the γ 0\nG pa-\nrameters be initialized to successively higher values for feature groups successively lower in the hierarchy, entailing that, e.g., it is easier to generalize a novel word to a new breed of dog not seen in training (basic-level generalization), than to a new kind of animal not seen in training (superordinate generalization).\nWe next describe how we incorporate mechanisms of memory and attention into the model, which render it sensitive to presentation timing."
    }, {
      "heading" : "Our Extensions to Integrate Memory and Attention",
      "text" : "We adopt the general approach of Nematzadeh et al. (2012) because it integrates memory and attention seamlessly into the cross-situational word-learning mechanism. The approach was shown to account for spacing effects in word learning, which are closely related to the presentation timing factors considered by SPSS11. However, the methods must be extended to adequately meet the needs of word generalization in the NGS15 model; we describe those extensions here.\nModeling the Effects of Forgetting. To model the effect of memory, we use the association score formulation of Nematzadeh et al. (2012), which implements “forgetting” by applying a decay factor to each alignment probability (cf. Eqn. 2 above):\nassoct( fi,w j) = ∑ t′∈T\nat′( fi,w j)\n(t − t ′+ 1)dat′ (4)\nEach alignment in the sum is scaled by the temporal distance between the current time t and the time t ′ that the alignment was made, exponentiated to a decay function dat′ that is inversely proportional to the strength of alignment.\nHowever, we must extend this decay formulation to accommodate our hierarchical knowledge of feature groups.2 In particular, we find that using the same decay rate across all feature groups is not sufficient. As noted above, appropriate word generalization in the NGS15 model requires that lower levels in the taxonomy be more “open” to generalizing to new features than higher levels in the taxonomy. It is important to note that the decay of alignments also influences “openness” to generalization because it shifts probably mass away from observed word–feature pairs onto unseen events. Thus, to appropriately reflect the nature of the hierarchy – that openness increases with greater depth in the taxonomy – we must parameterize decay by feature group. Just as feature groups lower in the taxonomy must have successively higher γ values to indicate more “openness” to generalization, lower feature groups also require higher decay rate parameters.\nWe thus use the following formulation of decay:\ndat = dG\nat( fi,w j) (5)\nwhere dG controls the rate of decay for features in feature group G , and is set successively higher for lower-level feature groups in the taxonomy.\nModeling Attention to Novelty. Building on research showing that people attend more to novel stimuli in learning (e.g., Snyder et al., 2008; MacPherson & Moore, 2010; Horst et al., 2011), we use the general idea of Nematzadeh et al. (2012) in allocating more strength to alignments that are more novel (cf. Eqn. 1):\nat( fi,w j) = Pt−1( fi|w j)\n∑ w′∈U\nPt−1( fi|w′) ·noveltyt( fi,w j) (6)\nIn this model, noveltyt( fi,w j) was inversely proportional to how recently w j had been observed, and thus focused solely on novelty of words; the novelty of the feature fi was not considered. We must broaden this approach because the experiments here are focused on a single novel word.\nHere instead we consider the novelty of the observed word–feature pairing, and again draw on considerations of type–token frequencies, as in other aspects of the NGS15 model. Specifically, we scale the alignment strength by the ratio of the token frequency of fi–w j observations at time t to the total frequency of all such observations, by formulating noveltyt( fi,w j) as:\nnoveltyt( fi,w j) = tokent ( fi,w j)\n∑t′∈T tokent′ ( fi,w j) (7)\n2Nematzadeh et al. (2012) used a single meaning probability distribution over all features – i.e., there are no feature groups.\nwhere tokent ( fi,w j) is the number of tokens of feature fi that occurred at time t with word w j.\nThis formulation achieves attention to novelty as follows. Generally, earlier observations of feature fi with word w j will have a stronger alignment than later observations, where the increased number of observations will increase the denominator of noveltyt( fi,w j), and lead to attenuation of the alignment strength. Note that when the co-occurrence of fi with word w j is truly novel – i.e., the first time they are observed together – the strength of alignment is undiminished, since the numerator and denominator of the novelty factor are equal in the initial observation of fi with w j.\nSummary of Novel Extensions to the NGS15 Model In summary, we have extended both the model of NGS15, and the memory and attention mechanisms of Nematzadeh et al. (2012), by: (i) incorporating a forgetting mechanism that is sensitive to the taxonomic level of a feature group, which reflects the needs of taxonomic structure and the process of novel word generalization; and (ii) formulating a mechanism for attention to novelty of word–feature pairings, rather than just to recency of words, consistent with the key role of word– feature association statistics in the model.\nThese mechanisms have a direct impact on the processing of stimuli in simultaneous vs. sequential presentations in a novel word generalization task. The forgetting mechanism ensures that more general features, such as the kind of animal observed (e.g., dog or cat), are remembered better than more detailed features, such as particular breeds of dogs. The attention-to-novelty mechanism has the consequence that successive observations of word–feature pairings in a sequential presentation scenario are “discounted” with respect to earlier presentations. We demonstrate in our experiments below that, together, these mechanisms interact to enable the model to account for both the suspicious coincidence effect in a simultaneous presentation as found by XT07, and its reversal in a sequential presentation as found by SPSS11."
    }, {
      "heading" : "Methodology",
      "text" : "We follow the methods of NGS15, adapted where needed for our extended model on the SPSS11 data.3\nTraining the Model. We use a taxonomy with three levels, corresponding to the subordinate, basic, and superordinate categories of animals. This yields four feature groups, one per category level plus an “instance” group to distinguish multiple objects of the same subordinate category. See Figure 2. In each Ut–St input pair, Ut consists of the novel word, and St is a set of four features (one per feature group) representing a unique instance of the same subordinate category across all training trials; for example:\nUt : { fep } St : { INSTANCE1, DALMATIAN, DOG, ANIMAL }\nIn the 1-example condition, training consists of just one such\n3Our code and data are available at https://github.com/ eringrant/novel word generalization.\nUt–St pair. In the 3-subordinate condition, training has three such Ut–St pairs, differing only in the unique instance feature (i.e., instance1, instance2, instance3) in each St . In the simultaneous condition, the three Ut–St pairs are all presented at the same time t. In the sequential condition, the three Ut–St pairs are presented one at a time, at t, t + 1, and t + 2.\nTesting the Model. After training, the level of generalization of the novel word is assessed against test objects, each of which is a subordinate match, a basic-level match, or a superordinate match; for example:\nsubord. match: { INSTANCE4, DALMATIAN, DOG, ANIMAL }\nbasic match: { INSTANCE5, POODLE, DOG, ANIMAL }\nsuper. match: { INSTANCE6, TOUCAN, BIRD, ANIMAL }\nWe adapt the Pgen formula of NGS15 to test whether the model generalizes the learned meaning of the novel word w to the various levels of match at test time t (after training):\nPgen(m|w) = avgY∈m Pt(Y |w)\navgY ′∈ {sub.} Pt(Y ′|w)\nHere Pt(Y |w) is the probability of a test object Y given w, and m is the set of test objects at a certain level of match. The measure in the numerator of Pgen is the average such probability across test matches at that level, avgY∈m Pt(Y |w). This is not directly comparable to the empirical data, which are the percentages of test objects selected from each type of match. To obtain a comparable measure, we scale each probability (for each level of match) by the probability of the subordinate matches in that condition, avgY ′∈ {sub.} Pt(Y\n′|w) (the denominator of Pgen). Thus Pgen(m|w) is the relative average preference for test items at level m. This renders the subordinate match probability as 1.0 (reflecting that people generally pick close to 100% of the subordinate test items), and shows the other type of matches relative to that amount.\nModel Parameters. Since the SPSS11 participants are adults, we use the adult parameter settings of NGS15 for the four γ 0G parameters and the four kG (one each per feature group), which are tuned to achieve a match to adult data of XT07. For our decay parameters, we use:\nd inst = 0.8 dsubord = 0.5 d basic = 0.05 dsuper = 0.01"
    }, {
      "heading" : "Model Results and Discussion",
      "text" : "Figure 3 shows the results of our model in the simultaneous and sequential conditions; cf. Figure 1 for the human behavioral data in SPSS11. Following simultaneous presentation of training input (Figure 3a), our model shows the suspicious coincidence effect: Generalization to the basic level is inhibited in the 3-subordinate condition as compared to the 1-example condition. In contrast, sequential presentation reverses the suspicious coincidence effect (Figure 3b): the model exhibits greater basic-level generalization in the 3-subordinate condition. Thus, these results replicate the qualitative pattern evident in the behavioural data of SPSS11.\nOur model data for (3a) simultaneous and (3b) sequential presentations. Each bar is the probability of a type of test match: i.e., subord(inate), basic(-level), or super(ordinate), scaled by the subordinate match probability.\nThe interaction (between presentation type and amount of training) seen in the human data arises as a result of a corresponding interaction in the model. Consider the comparison of each 3-subordinate condition (simultaneous and sequential) to the 1-example condition. In the simultaneous 3-subordinate case, the attentional mechanism yields higher alignment strengths between the word and features because their three co-occurrences are all novel at the single presentation time; in addition there is little forgetting because the items are all seen at time t and test is at time t+1. This yields stronger subordinate alignments compared to the 1-example case, and therefore somewhat less basic-level generalization.\nBy contrast, in the sequential 3-subordinate case, the word–feature co-occurrences are less salient because they decrease in novelty over the three presentation times. In addition, greater forgetting occurs because there is more time between the (first two) presentation times and test time (t + 4). In this case, because subordinate features decay faster than basic features, the interaction yields weaker subordinate features compared to the 1-example case, and more basic-level generalization is achieved.\nThe interaction of memory and attention effects are required to obtain this pattern of results in the model. If the model includes only the decay mechanism, differentiated by taxonomic level, this enables it to focus more on abstract than specific features, and consequently raises the basic generalization closer to the level of the subordinate generalization in all conditions. On the other hand, using the attention mechanism alone enables the model to distinguish the sequential and simultaneous conditions, but it cannot on its own raise the basic generalization high enough. Only when the two are used together does the model produce the reversal of the suspicious coincidence effect in the sequential presentation.\nThe necessity of both memory and attention is suggestive of how word learning occurs in people. In particular, the attention mechanism in the model focuses more probability onto word–feature co-occurrences in their earlier presentations, simulating the general tendency for people to attend more to less-familiar things. In addition, the mechanism that\nincreases the decay rate for lower-level features in the taxonomy simulates the tendency in people to remember abstract features of objects over very specific features. SPSS11 contended that people are able to attend to specific features in the simultaneous condition due to the close spatial and temporal proximity of the items, and correspondingly attend only to the abstract commonalities of items in sequential presentations. Our model explains this effect as the result of general memory and attention mechanisms that have been shown to play a role in word learning more widely (cf. Nematzadeh et al. (2012); Nematzadeh, Fazly, and Stevenson (2013)). Interestingly, attention in our model is a function of the token frequency of word–feature co-occurrences (as opposed to a fixed parameter) and is therefore a response to the statistics of the data, as are other components of our word generalization formulation. All this further supports that attention, memory and statistical learning interact to produce the suspicious coincidence effect and its reversal across presentations."
    }, {
      "heading" : "Conclusions and Future Work",
      "text" : "Novel word generalization – understanding how a word maps to the appropriate level of a taxonomic hierarchy – is an important aspect of novel word learning, but one that has not received much attention in the word-learning community. We propose a unified model of word learning that accounts for the various observed patterns of novel word generalization – in particular, the suspicious coincidence effect (Xu & Tenenbaum, 2007) and its reversal under differing presentation conditions (Spencer et al., 2011). We extend the model of Nematzadeh et al. (2015) with a novel integration of the general cognitive mechanisms of memory and attention, and show that our model’s success is a result of the interaction of forgetting and attention to novelty of word–feature co-occurrences. Our approach builds on the earlier NGS15 model in highlighting the importance of type and token frequency patterns in the input to capturing interesting generalization effects, but here these patterns are manifest in our formulation of memory and attention mechanisms.\nIn incorporating these cognitive processes into our model, we drew on the approach of Nematzadeh et al. (2012), whose model had been shown to account for various spacing effects in word learning (see also Nematzadeh et al., 2013). Much further work is needed to explore whether our model can explain other such effects. For example, Vong, Perfors, and Navarro (2014) showed that people’s categorization of novel object instances depends on the distribution of training examples both that are labelled with a word as well as those that are unlabelled. Currently, our model only takes into account word–feature co-occurrences, and is therefore insensitive to features that occur without a word label. We will need to consider how to integrate learning from unlabelled data in order to better model how statistical word learning interacts with object categorization, as it does in people."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "People exhibit a tendency to generalize a novel noun to the basic-level in a hierarchical taxonomy – a cognitively salient category such as “dog” – with the degree of generalization depending on the number and type of exemplars. Recently, a change in the presentation timing of exemplars has also been shown to have an effect, surprisingly reversing the prior observed pattern of basic-level generalization. We explore the precise mechanisms that could lead to such behavior by extending a computational model of word learning and word generalization to integrate cognitive processes of memory and attention. Our results show that the interaction of forgetting and attention to novelty, as well as sensitivity to both type and token frequencies of exemplars, enables the model to replicate the empirical results from different presentation timings. Our results reinforce the need to incorporate general cognitive processes within word learning models to better understand the range of observed behaviors in vocabulary acquisition.",
    "creator" : "dvips(k) 5.991 Copyright 2011 Radical Eye Software"
  }
}