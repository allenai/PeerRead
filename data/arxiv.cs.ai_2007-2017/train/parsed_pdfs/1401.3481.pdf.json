{
  "name" : "1401.3481.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Bounds Arc Consistency for Weighted CSPs",
    "authors" : [ "Matthias Zytnicki", "Christine Gaspin", "Simon de Givry", "Thomas Schiex" ],
    "emails" : [ "Matthias.Zytnicki@versailles.inra.fr", "Christine.Gaspin@toulouse.inra.fr", "Simon.DeGivry@toulouse.inra.fr", "Thomas.Schiex@toulouse.inra.fr" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "The Weighted Constraint Satisfaction Problem (WCSP) is an extension of the crisp Constraint Satisfaction Problem (CSP) that allows the direct representation of hard constraints and cost functions. The WCSP defines a simple optimization (minimization) framework with a wide range of applications in resource allocation, scheduling, bioinformatics (Sànchez, de Givry, & Schiex, 2008; Zytnicki, Gaspin, & Schiex, 2008), electronic markets (Sandholm, 1999), etc. It also captures fundamental AI and statistical problems such as Maximum Probability Explanation in Bayesian nets and Markov Random Fields (Chellappa & Jain, 1993).\nAs in crisp CSP, the two main approaches to solve WCSP are inference and search. This last approach is usually embodied in a branch-and-bound algorithm. This algorithm estimates at each node of the search tree a lower bound of the cost of the solutions of the sub-tree.\nc©2009 AI Access Foundation. All rights reserved.\nOne of the most successful approaches to build lower bounds has been obtained by extending the notion of local consistency to WCSP (Meseguer, Rossi, & Schiex, 2006). This includes soft AC (Schiex, 2000), AC* (Larrosa, 2002), FDAC* (Larrosa & Schiex, 2004), EDAC* (Heras, Larrosa, de Givry, & Zytnicki, 2005), OSAC (Cooper, de Givry, & Schiex, 2007) and VAC (Cooper, de Givry, Sànchez, Schiex, & Zytnicki, 2008) among others. Unfortunately, the worst case time complexity bounds of the associated enforcing algorithms are at least cubic in the domain size and use an amount of space which is at least linear in the domain size. This makes these consistencies useless for problems with very large domains.\nThe motivation for designing a local consistency which can be enforced efficiently on problems with large domains follows from our interest in the RNA gene localization problem. Initially modeled as a crisp CSP, this problem has been tackled using bounds consistency (Choi, Harvey, Lee, & Stuckey, 2006; Lhomme, 1993) and dedicated propagators using efficient pattern matching algorithms (Thébault, de Givry, Schiex, & Gaspin, 2006). The domain sizes are related to the size of the genomic sequences considered and can reach hundreds of millions of values. In order to enhance this tool with scoring capabilities and improved quality of localization, a shift from crisp to weighted CSP is a natural step which requires the extension of bounds consistency to WCSP. Beyond this direct motivation, this extension is also useful in other domains where large domains occur naturally such as temporal reasoning or scheduling.\nThe local consistencies we define combine the principles of bounds consistency with the principles of soft local consistencies. These definitions are general and are not restricted to binary cost functions. The corresponding enforcing algorithms improve over the time and space complexity of AC* by a factor of d and also have the nice but rare property, for WCSP local consistencies, of being confluent.\nAs it has been done for AC-5 by Van Hentenryck, Deville, and Teng (1992) for functional or monotonic constraints, we show that different forms of cost functions (largely captured by the notion of semi-convex cost functions) can be processed more efficiently. We also show that the most powerful of these bounds arc consistencies is strictly stronger than the application of bounds consistency to the reified representation of the WCSP as proposed by Petit, Régin, and Bessière (2000).\nTo conclude, we experimentally compare the efficiency of algorithms that maintain these different local consistencies inside branch-and-bound on agile satellite scheduling problems (Verfaillie & Lemâıtre, 2001) and RNA gene localization problems (Zytnicki et al., 2008) and observe clear speedups compared to different existing local consistencies."
    }, {
      "heading" : "2. Definitions and Notations",
      "text" : "This section will introduce the main notions that will be used throughout the paper. We will define the (Weighted) Constraint Satisfaction Problems, as well as a local consistency property frequently used for solving the Weighted Constraint Satisfaction Problem: arc consistency (AC*)."
    }, {
      "heading" : "2.1 Constraint Networks",
      "text" : "Classic and weighted constraint networks share finite domain variables as one of their components. In this paper, the domain of a variable xi is denoted by D(xi). To denote a value in D(xi), we use an index i as in vi, v ′ i,. . . For each variable xi, we assume that the domain of xi is totally ordered by ≺i and we denote by inf(xi) and sup(xi) the minimum (resp. maximum) values of the domain D(xi). An assignment tS of a set of variables S = {xi1 , . . . , xir} is a function that maps variables to elements of their domains: tS = (xi1 ← vi1 , . . . , xir ← vir) with ∀i ∈ {i1, . . . , ir}, tS(xi) = vi ∈ D(xi). For a given assignment tS such that xi ∈ S, we simply say that a value vi ∈ D(xi) belongs to tS to mean that tS(xi) = vi. We denote by ℓS , the set of all possible assignments on S.\nDefinition 2.1 A constraint network (CN) is a tuple P = 〈X ,D, C〉, where X = {x1, . . . , xn} is a set of variables and D = {D(x1), . . . , D(xn)} is the set of the finite domains of each variable. C is a set of constraints. A constraint cS ∈ C defines the set of all authorized combinations of values for the variables in S as a subset of ℓS. S is called the scope of cS.\n|S| is called the arity of cS . For simplicity, unary (arity 1) and binary (arity 2) constraints may be denoted by ci and cij instead of c{xi} and c{xi,xj} respectively. We denote by d the maximum domain size, n, the number of variables in the network and e, the number of constraints. The central problem on constraint networks is to find a solution, defined as an assignment tX of all variables such that for any constraint cS ∈ C, the restriction of tX to S is authorized by cS (all constraints are satisfied). This is the Constraint Satisfaction Problem (CSP).\nDefinition 2.2 Two CNs with the same variables are equivalent if they have the same set of solutions.\nA CN will be said to be “empty” if one of its variables has an empty domain. This may happen following local consistency enforcement. For CN with large domains, the use of bounds consistency is the most usual approach. Historically, different variants of bounds consistency have been introduced, generating some confusion. Using the terminology introduced by Choi et al. (2006), the bounds consistency considered in this paper is the bounds(D) consistency. Because we only consider large domains defining intervals, this is actually equivalent to bounds(Z) consistency. For simplicity, in the rest of the paper we denote this as “bounds consistency”.\nDefinition 2.3 (Bounds consistency) A variable xi is bounds consistent iff every constraint cS ∈ C such that xi ∈ S contains a pair of assignments (t, t\n′) ∈ ℓS × ℓS such that inf(xi) ∈ t and sup(xi) ∈ t\n′. In this case, t and t′ are called the supports of the two bounds of xi’s domain.\nA CN is bounds consistent iff all its variables are bounds consistent.\nTo enforce bounds consistency on a given CN, any domain bound that does not satisfy the above properties is deleted until a fixed point is reached."
    }, {
      "heading" : "2.2 Weighted Constraint Networks",
      "text" : "Weighted constraint networks are obtained by using cost functions (also referred as “soft constraints”) instead of constraints.\nDefinition 2.4 A weighted constraint network (WCN) is a tuple P = 〈X , D, W, k〉, where X = {x1, . . . , xn} is a set of variables and D = {D(x1), . . . , D(xn)} is the set of the finite domains of each variable. W is a set of cost functions. A cost function wS ∈ W associates an integer cost wS(tS) ∈ [0, k] to every assignment tS of the variables in S. The positive number k defines a maximum (intolerable) cost.\nThe cost k, which may be finite or infinite, is the cost associated with forbidden assignments. This cost is used to represent hard constraints. Unary and binary cost functions may be denoted by wi and wij instead of w{xi} and w{xi,xj} respectively. As usually for WCNs, we assume the existence of a zero-arity cost function, w∅ ∈ [0, k], a constant cost whose initial value is usually equal to 0. The cost of an assignment tX of all variables is obtained by combining the costs of all the cost functions wS ∈ W applied to the restriction of tX to S. The combination is done using the function ⊕ defined as a⊕ b = min(k, a+ b).\nDefinition 2.5 A solution of a WCN is an assignment tX of all variables whose cost is less than k. It is optimal if no other assignment of X has a strictly lower cost.\nThe central problem in WCN is to find an optimal solution.\nDefinition 2.6 Two WCNs with the same variables are equivalent if they give the same cost to any assignments of all their variables.\nInitially introduced by Schiex (2000), the extension of arc consistency to WCSP has been refined by Larrosa (2002) leading to the definition of AC*. It can be decomposed into two sub-properties: node and arc consistency itself.\nDefinition 2.7 (Larrosa, 2002) A variable xi is node consistent iff:\n• ∀vi ∈ D(xi), w∅ ⊕ wi(vi) < k.\n• ∃vi ∈ D(xi) such that wi(vi) = 0. The value vi is called the unary support of xi."
    }, {
      "heading" : "A WCN is node consistent iff every variable is node consistent.",
      "text" : "To enforce NC on a WCN, values that violate the first property are simply deleted. Value deletion alone is not capable of enforcing the second property. As shown by Cooper and Schiex (2004), the fundamental mechanism required here is the ability to move costs between different scopes. A cost b can be subtracted from a greater cost a by the function ⊖ defined by a ⊖ b = (a − b) if a 6= k and k otherwise. Using ⊖, a unary support for a variable xi can be created by subtracting the smallest unary cost minvi∈D(xi)wi(vi) from all wi(vi) and adding it (using ⊕) to w∅. This operation that shifts costs from variables to w∅, creating a unary support, is called a projection from wi to w∅. Because ⊖ and ⊕ cancel out, defining a fair valuation structure (Cooper & Schiex, 2004), the obtained WCN is equivalent to the original one. This equivalence preserving transformation (Cooper and Schiex) is more precisely described as the ProjectUnary() function in Algorithm 1.\nWe are now able to define arc and AC* consistency on WCN.\nAlgorithm 1: Projections at unary and binary levels\nProcedure ProjectUnary(xi) [ Find the unary support of xi ]1 min ← minvi∈D(xi){wi(vi)} ;2 if (min = 0) then return;3 foreach vi ∈ D(xi) do wi(vi) ← wi(vi)⊖min ;4 w∅ ← w∅ ⊕min ;5\nProcedure Project(xi, vi, xj) [ Find the support of vi w.r.t. wij ]6 min ← minvj∈D(xj){wij(vi, vj)} ;7\nif (min = 0) then return;8 foreach vj ∈ D(xj) do wij(vi, vj) ← wij(vi, vj)⊖min ;9 wi(vi) ← wi(vi)⊕min ;10\nDefinition 2.8 A variable xi is arc consistent iff for every cost function wS ∈ W such that xi ∈ S, and for every value vi ∈ D(xi), there exists an assignment t ∈ ℓS such that vi ∈ t and wS(t) = 0. The assignment t is called the support of vi on wS. A WCN is AC* iff every variable is arc and node consistent.\nTo enforce arc consistency, a support for a given value vi of xi on a cost function wS can be created by subtracting (using ⊖) the cost mint∈ℓS ,vi∈twS(t) from the costs of all assignments containing vi in ℓS and adding it to wi(vi). These cost movements, applied for all values vi of D(xi), define the projection from wS to wi. Again, this transformation preserves equivalence between problems. It is more precisely described (for simplicity, in the case of binary cost functions) as the Project() function in Algorithm 1.\nExample 2.9 Consider the WCN in Figure 1(a). It contains two variables (x1 and x2), each with two possible values (a and b, represented by vertices). A unary cost function is associated with each variable, the cost of a value being represented inside the corresponding vertex. A binary cost function between the two variables is represented by weighted edges connecting pairs of values. The absence of edge between two values represents a zero cost. Assume k is equal to 4 and w∅ is equal to 0.\nSince the cost w1(x1 ← a) is equal to k, the value a can be deleted from the domain of x1 (by NC, first property). The resulting WCN is represented in Figure 1(b). Then, since x2 has no unary support (second line of the definition of NC), we can project a cost of 1 to w∅ (cf. Figure 1(c)). The instance is now NC. To enforce AC*, we project 1 from the binary cost function w12 to the value a of x1 since this value has no support on w12 (cf. Figure 1(d)). Finally, we project 1 from w1 to w∅, as seen on Figure 1(e). Ultimately, we note that the value b of x2 has no support. To enforce AC*, we project a binary cost of 1 to this value and remove it since it has a unary cost of 2 which, combined with w∅ reaches k = 4."
    }, {
      "heading" : "3. Bounds Arc Consistency (BAC)",
      "text" : "In crisp CSP, the bounds consistency enforcing process just deletes bounds that are not supported in one constraint. In weighted CSP, enforcement is more complex. If a similar value deletion process exists based on the first node consistency property violation (whenever w∅ ⊕ wi(vi) reaches k), additional cost movements are performed to enforce node and arc consistency.\nAs shown for AC*, these projections require the ability to represent an arbitrary unary cost function wi for every variable xi. This requires space in O(d) in general since projections can lead to arbitrary changes in the original wi cost function (even if they have an efficient internal representation). To prevent this, we therefore avoid to move cost from cost functions with arity greater than one to unary constraints. Instead of such projections, we only keep a value deletion mechanism applied to the bounds of the current domain that takes into account all the cost functions involving the variable considered. For a given variable xi involved in a cost function wS , the choice of a given value vi will at least induce a cost increase of mintS∈ℓS ,vi∈tS wS(tS). If these minimum costs, combined on all the cost functions involving xi, together with w∅, reach the intolerable cost of k, then the value can be deleted. As in bounds consistency, this is just done for the two bounds of the domain. This leads to the following definition of BAC (bounds arc consistency) in WCSP:\nDefinition 3.1 In a WCN P = 〈X ,D,W, k〉, a variable xi is bounds arc consistent iff:\nw∅ ⊕ ∑\nwS∈W,xi∈S\n{\nmin tS∈ℓS ,inf(xi)∈tS wS(tS)\n}\n< k\nw∅ ⊕ ∑\nwS∈W,xi∈S\n{\nmin tS∈ℓS ,sup(xi)∈tS wS(tS)\n}\n< k\nA WCN is bounds arc consistent if every variable is bounds arc consistent.\nOne can note that this definition is a proper generalization of bounds consistency since when k = 1, it is actually equivalent to the definition of bounds(D) consistency for crisp CSP (Choi et al., 2006) (also equivalent to bounds(Z) consistency since domains are defined as intervals).\nThe algorithm enforcing BAC is described as Algorithm 2. Because enforcing BAC only uses value deletion, it is very similar in structure to bounds consistency enforcement. We maintain a queue Q of variables whose domain has been modified (or is untested). For better efficiency, we use extra data-structures to efficiently maintain the combined cost associated with the domain bound inf(xi), denoted w\ninf(xi). For a cost function wS involving xi, the contribution of wS to this combined cost is equal to mintS∈ℓS ,inf(xi)∈tS wS(tS). This contribution is maintained in a data-structure ∆inf(xi, wS) and updated whenever the minimum cost may change because of value removals. Notice that, in Algorithm 2, the line 14 is a concise way to denote the hidden loops which initialize the winf , wsup, ∆inf and ∆sup data-structures to zero.\nDomain pruning is achieved by function PruneInf() which also resets the data-structures associated with the variable at line 35 and these data-structures are recomputed when the variable is extracted from the queue. Indeed, inside the loop of line 20, the contributions ∆inf(xi, wS) to the cost w\ninf(xi) from the cost functions wS involving xj are reset. The Function pop removes an element from the queue and returns it.\nProposition 3.2 (Time and space complexity) For a WCN with maximum arity r of the constraints, enforcing BAC with Algorithm 2 is time O(er2dr) and space O(n+ er).\nProof: Regarding time, every variable can be pushed into Q at most d + 1 times: once at the beginning, and when one of its values has been removed. As a consequence, the foreach loop on line 18 iterates O(erd) times, and the foreach loop on line 20 iterates O(er2d) times. The min computation on line 22 takes time O(dr−1) and thus, the overall time spent at this line takes time O(er2dr). PruneInf() is called at most O(er2d) times. The condition on line 32 is true at most O(nd) times and so, line 35 takes time O(ed) (resetting ∆inf(xi, ·) on line 35 hides a loop on all cost functions involving xi). The total time complexity is thus O(er2dr).\nRegarding space, we only used winf , wsup and ∆ data-structures. The space complexity is thus O(n+ er).\nNote that exploiting the information of last supports as in AC2001 (Bessière & Régin, 2001) does not reduce the worst-case time complexity because the minimum cost of a cost function must be recomputed from scratch each time a domain has been reduced and the last support has been lost (Larrosa, 2002). However, using last supports helps in practice to reduce mean computation time and this has been done in our implementation.\nCompared to AC*, which can be enforced in O(n2d3) time and O(ed) space for binary WCN, BAC can be enforced d times faster, and the space complexity becomes independent of d which is a requirement for problems with very large domains.\nAnother interesting difference with AC* is that BAC is confluent — just as bounds consistency is. Considering AC*, it is known that there may exist several different AC* closures with possibly different associated lower bounds w∅ (Cooper & Schiex, 2004). Note that although OSAC (Cooper et al., 2007) is able to find an optimal w∅ (at much higher\nAlgorithm 2: Algorithm enforcing BAC.\nProcedure BAC(X ,D,W, k)11 Q ← X ;12 winf(·) ← 0 ; wsup(·) ← 0 ; ∆inf(·, ·) ← 0 ; ∆sup(·, ·) ← 0 ;14 while (Q 6= ∅) do15 xj ← pop(Q) ;16 foreach wS ∈ W, xj ∈ S do18 foreach xi ∈ S do20 α ← mintS∈ℓS ,inf(xi)∈tS wS(tS) ;22\nwinf(xi) ← w inf(xi)⊖∆ inf(xi, wS)⊕ α ;23 ∆inf(xi, wS) ← α ;24 if PruneInf(xi) then Q ← Q ∪ {xi} ;25 α ← mintS∈ℓS ,sup(xi)∈tS wS(tS) ;26\nwsup(xi) ← w sup(xi)⊖∆ sup(xi, wS)⊕ α ;27 ∆sup(xi, wS) ← α ;28 if PruneSup(xi) then Q ← Q ∪ {xi} ;29\nFunction PruneInf(xi) : boolean30 if (w∅ ⊕ w\ninf(xi) = k) then32 delete inf(xi) ;33 winf(xi) ← 0 ; ∆ inf(xi, ·) ← 0 ;35 return true;36\nelse return false;37\nFunction PruneSup(xi) : boolean38 if (w∅ ⊕ w\nsup(xi) = k) then39 delete sup(xi) ;40 wsup(xi) ← 0 ; ∆\nsup(xi, ·) ← 0 ;41 return true;42\nelse return false;43\ncomputational cost), it is still not confluent. The following property shows that BAC is confluent.\nProposition 3.3 (Confluence) Enforcing BAC on a given problem always leads to a unique WCN.\nProof: We will prove the proposition as follows. We will first define a set of problems which contains all the problems that can be reached from the original WCN through BAC enforcement. Notice that, at each step of BAC enforcement, in the general case, several operations can be performed and no specific order is imposed. Therefore, a set of problems can be reached at each step. We will show that the set of problems has a lattice structure and ultimately show that the closure of BAC is the lower bound of this lattice, and is therefore unique, which proves the property. This proof technique is usual for proving convergence of the chaotic iteration of a collection of suitable functions and has been used for characterizing CSP local consistency by Apt (1999).\nDuring the enforcement of BAC, the original problem P = 〈X ,D,W, k〉 is iteratively transformed into a set of different problems which are all equivalent to P, and obtained by deleting values violating BAC. Because these problems are obtained by value removals, they belong to the set ℘1(P ) defined by: {〈X ,D\n′,W, k〉 : D′ ⊆ D}. We now define a relation, denoted ⊑, on the set ℘1(P ):\n∀(P1,P2) ∈ ℘ 2 1(P),P1 ⊑ P2 ⇔ ∀i ∈ [1, n], D1(xi) ⊆ D2(xi)\nIt is easy to see that this relation defines a partial order. Furthermore, each pair of elements has a greatest lower bound glb and a least upper bound lub in ℘1(P), defined by:\n∀(P1,P2) ∈ ℘ 2 1(P),\nglb(P1,P2) = 〈X , {D1(xi) ∩D2(xi) : i ∈ [1, n]},W, k〉 ∈ ℘1(P)\nlub(P1,P2) = 〈X , {D1(xi) ∪D2(xi) : i ∈ [1, n]},W, k〉 ∈ ℘1(P)\n〈℘1(P),⊑〉 is thus a complete lattice. BAC filtering works by removing values violating the BAC properties, transforming an original problem into a succession of equivalent problems. Each transformation can be described by the application of dedicated functions from ℘1(P) to ℘1(P). More precisely, there are two such functions for each variable, one for the minimum bound inf(xi) of the domain of xi and a symmetrical one for the maximum bound. For inf(xi), the associated function keeps the instance unchanged if inf(xi) satisfies the condition of Definition 3.1 and it otherwise returns a WCN where inf(xi) alone has been deleted. The collection of all those functions defines a set of functions from ℘1(P ) to ℘1(P ) which we denote by FBAC .\nObviously, every function f ∈ FBAC is order preserving:\n∀(P1,P2) ∈ ℘ 2 1(P),P1 ⊑ P2 ⇒ f(P1) ⊑ f(P2)\nBy application of the Tarski-Knaster theorem (Tarski, 1955), it is known that every function f ∈ FBAC (applied until quiescence during BAC enforcement) has at least one fixpoint, and that the set of these fixed points forms a lattice for ⊑. Moreover, the intersection of the lattices of fixed points of the functions f ∈ FBAC , denoted by ℘ ⋆ 1(P), is also\na lattice. ℘⋆1(P) is not empty since the problem 〈X , {∅, . . . ,∅},W〉 is a fixpoint for every filtering function in FBAC . ℘ ⋆ 1(P) is exactly the set of fixed points of FBAC .\nWe now show that, if the algorithm reaches a fixpoint, it reaches the greatest element of ℘⋆1(P). We will prove by induction that any successive application of elements of FBAC on P yields problems which are greater than any element of ℘⋆1(P) for the order ⊑. Let us consider any fixpoint P⋆ of ℘⋆1(P). Initially, the algorithm applies on P, which is the greatest element of ℘1(P), and thus P\n⋆ ⊑ P. This is the base case of the induction. Let us now consider any problem P1 obtained during the execution of the algorithm. We have, by induction, P⋆ ⊑ P1. Since ⊑ is order preserving, we know that, for any function f of FBAC , f(P\n⋆) = P⋆ ⊑ f(P1). This therefore proves the induction. To conclude, if the algorithm terminates, then it gives the maximum element of ℘⋆1(P). Since proposition 3.2 showed that the algorithm actually terminates, we can conclude that it is confluent.\nIf enforcing BAC may reduce domains, it never increases the lower bound w∅. This is an important limitation given that each increase in w∅ may generate further value deletions and possibly, failure detection. Note that even when a cost function becomes totally assigned, the cost of the corresponding assignment is not projected to w∅ by BAC enforcement. This can be simply done by maintaining a form of backward checking as in the most simple WCSP branch-and-bound algorithm (Freuder & Wallace, 1992). To go beyond this simple approach, we consider the combination of BAC with another WCSP local consistency which, similarly to AC*, requires cost movements to be enforced but which avoids the modification of unary cost functions to keep a reasonable space complexity. This is achieved by directly moving costs to w∅."
    }, {
      "heading" : "4. Enhancing BAC",
      "text" : "In many cases, BAC may be very weak compared to AC* in situations where it seems to be possible to infer a decent w∅ value. Consider for example the following cost function:\nw12 :\n{\nD(x1)×D(x2) → E (v1, v2) 7→ v1 + v2\nD(x1) = D(x2) = [1, 10]\nAC* can increase w∅ by 2, by projecting a cost of 2 from w12 to the unary constraint w1 on every value, and then projecting these costs from w1 to w∅ by enforcing NC. However, if w∅ = w1 = w2 = 0 and k is strictly greater than 11, BAC remains idle here. We can however simply improve BAC by directly taking into account the minimum possible cost of the cost function w12 over all possible assignments given the current domains.\nDefinition 4.1 A cost function wS is ∅-inverse consistent (∅-IC) iff:\n∃tS ∈ ℓS , wS(tS) = 0\nSuch a tuple tS is called a support for wS. A WCN is ∅-IC iff every cost function (except w∅) is ∅-IC.\nEnforcing ∅-IC can always be done as follows: for every cost function wS with a non empty scope, the minimum cost assignment of wS given the current variable domains is\ncomputed. The cost α of this assignment is then subtracted from all the tuple costs in wS and added to w∅. This creates at least one support in wS and makes the cost function ∅-IC. For a given cost function wS , this is done by the Project() function of Algorithm 3.\nIn order to strengthen BAC, a natural idea is to combine it with ∅-IC. We will call BAC∅ the resulting combination of BAC and ∅-IC. To enforce BAC∅, the previous algorithm is modified by first adding a call to the Project() function (see line 53 of Algorithm 3). Moreover, to maintain BAC whenever w∅ is modified by projection, every variable is tested for possible pruning at line 66 and put back in Q in case of domain change. Note that the subtraction applied to all constraint tuples at line 75 can be done in constant time without modifying the constraint by using an additional ∆wS data-structure, similar to the ∆ data-structure introduced by Cooper and Schiex (2004). This data-structure keeps track of the cost which has been projected from wS to w∅. This feature makes it possible to leave the original costs unchanged during the enforcement of the local consistency. For example, for any tS ∈ ℓS , wS(t) refers to wS(t) ⊖ ∆\nwS , where wS(t) denotes the original cost. Note that ∆wS , which will be later used in a confluence proof, precisely contains the amount of cost which has been moved from wS to w∅. The whole algorithm is described in Algorithm 3. We highlighted in black the parts which are different from Algorithm 2 whereas the unchanged parts are in gray.\nProposition 4.2 (Time and space complexity) For a WCN with maximum arity r of the constraints, enforcing BAC∅ with Algorithm 3 can be enforced in O(n2r2dr+1) time using O(n+ er) memory space.\nProof: Every variable is pushed at most O(d) times in Q, thus the foreach at line 51 (resp. line 55) loops at most O(erd) (resp. O(er2d)) times. The projection on line 53 takes O(dr) time. The operation at line 57 can be carried out in O(dr−1) time. The overall time spent inside the if of the PruneInf() function is bounded by O(ed). Thus the overall time spent in the loop at line 51 (resp. line 55) is bounded by O(er2dr+1) (resp. O(er2dr)).\nThe flag on line 66 is true when w∅ increases, and so it cannot be true more than k times (assuming integer costs). If the flag is true, then we spend O(n) time to check all the bounds of the variables. Thus, the time complexity under the if is bounded by O(min{k, nd}× n). To sum up, the overall time complexity is O(er2dr+1 +min{k, nd} × n), which is bounded by O(n2r2dr+1).\nThe space complexity is given by the ∆, winf , wsup and ∆wS data-structures which sums up to O(n+ re) for a WCN with an arity bounded by r.\nThe time complexity of the algorithm enforcing BAC∅ is multiplied by d compared to BAC without ∅-IC. This is a usual trade-off between the strength of a local property and the time spent to enforce it. However, the space complexity is still independent of d. Moreover, like BAC, BAC∅ is confluent.\nProposition 4.3 (Confluence) Enforcing BAC∅ on a given problem always leads to a unique WCN.\nProof: The proof is similar to the proof of Proposition 3.3. However, because of the possible cost movements induced by projections, BAC∅ transforms the original problem P in more complex ways, allowing either pruning domains (BAC) or moving costs from cost\nAlgorithm 3: Algorithm enforcing BAC∅\nProcedure BAC∅(X ,D,W, k)44 Q ← X ;45 winf(·) ← 0 ; wsup(·) ← 0 ; ∆inf(·, ·) ← 0 ; ∆sup(·, ·) ← 0 ;46 while (Q 6= ∅) do47 xj ← pop(Q) ;48 flag ← false ;49 foreach wS ∈ W, xj ∈ S do51 if Project(wS) then flag ← true ;53 foreach xi ∈ S do55 α ← mintS∈ℓS ,inf(xi)∈tS wS(tS) ;57\nwinf(xi) ← w inf(xi)⊖∆ inf(xi, wS)⊕ α ;58 ∆inf(xi, wS) ← α ;59 if PruneInf(xi) then Q ← Q ∪ {xi} ;60 α ← mintS∈ℓS ,sup(xi)∈tS wS(tS) ;61\nwsup(xi) ← w sup(xi)⊖∆ sup(xi, wS)⊕ α ;62 ∆sup(xi, wS) ← α ;63 if PruneSup(xi) then Q ← Q ∪ {xi} ;64\nif (flag) then66 foreach xi ∈ X do67 if PruneInf(xi) then Q ← Q ∪ {xi} ;68 if PruneSup(xi) then Q ← Q ∪ {xi} ;69\nFunction Project(wS) : boolean70 α ← mintS∈ℓS wS(tS) ;71 if (α > 0) then72 w∅ ← w∅ ⊕ α ;73 wS(·) ← wS(·)⊖ α ;75 return true;76\nelse return false;77\nfunctions to w∅. The set of problems that will be considered needs therefore to take this into account. Instead of being just defined by its domains, a WCN reached by BAC∅ is also characterized by the amount of cost that has been moved from each cost function wS to w∅. This quantity is already denoted by ∆\nwS in Section 4, on page 603. We therefore consider the set ℘2(P) defined by:\n{\n(〈X ,D′,W, k〉, {∆w : w ∈ W}) : ∀i ∈ [1, n], D′(xi) ⊆ D(xi),∀w ∈ W,∆ w ∈ [0, k]\n}\nWe can now define the relation ⊑ on ℘2(P):\nP1 ⊑ P2 ⇔ ((∀w ∈ W,∆ w 1 ≥ ∆ w 2 ) ∧ (∀xi ∈ X , D1(xi) ⊆ D2(xi)))\nThis relation is reflexive, transitive and antisymmetric. The first two properties can be easily verified. Suppose now that (P1,P2) ∈ ℘ 2 2(P) and that (P1 ⊑ P2) ∧ (P2 ⊑ P1). We have thus (∀w ∈ W,∆w = ∆ ′ w)∧(∀xi ∈ X , D(xi) = D\n′(xi)). This ensures that the domains, as well as the amounts of cost projected by each cost function, are the same. Thus, the problems are the same and ⊑ is antisymmetric.\nBesides, 〈℘2(P),⊑〉 is a complete lattice, since:\n∀(P1,P2) ∈ ℘ 2 2(P),\nglb(P1,P2) = (〈X , {D1(xi) ∩D2(xi) : i ∈ [1, n]},W, k〉, {max{∆ w 1 ,∆ w 2 } : w ∈ W}) lub(P1,P2) = (〈X , {D1(xi) ∪D2(xi) : i ∈ [1, n]},W, k〉, {min{∆ w 1 ,∆ w 2 } : w ∈ W})\nand both of them are in ℘2(P). Every enforcement of BAC∅ follows from the application of functions from a set of functions FBAC∅ which may remove the maximum or minimum domain bound (same definition as for BAC) or may project cost from cost functions to w∅. For a given cost function w ∈ W, such a function keeps the instance unchanged if the minimum α of w is 0 over possible tuples. Otherwise, if α > 0, the problem returned is derived from P by projecting an amount of cost α from w to w∅. These functions are easily shown to be order preserving for ⊑.\nAs in the proof of Proposition 3.3, we can define the lattice ℘⋆2(P), which is the intersection of the sets of fixed points of the functions f ∈ FBAC∅ . ℘ ⋆ 2(P) is not empty, since (〈X , {∅, . . . ,∅},W, k〉, {k, . . . , k}) is in it. As in the proof of proposition 3.3, and since Algorithm 3 terminates, we can conclude that this algorithm is confluent, and that it results in lub(℘⋆2(P))."
    }, {
      "heading" : "5. Exploiting Cost Function Semantics in BAC∅",
      "text" : "In crisp AC, several classes of binary constraints make it possible to enforce AC significantly faster (in O(ed) instead of O(ed2), as shown by Van Hentenryck et al., 1992). Similarly, it is possible to exploit the semantics of the cost functions to improve the time complexity of BAC∅ enforcement. As the proof of Proposition 4.2 shows, the dominating factors in this complexity comes from the complexity of computing the minimum of cost functions during projection at lines 53 and 57 of Algorithm 3. Therefore, any cost function property\nthat makes these computations less costly may lead to an improvement of the overall time complexity.\nProposition 5.1 In a binary WCN, if for any cost function wij ∈ W and for any subintervals Ei ⊆ D(xi), Ej ⊆ D(xj), the minimum of wij over Ei × Ej can be found in time O(d), then the time complexity of enforcing BAC∅ is O(n2d2).\nProof: This follows directly from the proof of Proposition 4.2. In this case, the complexity of projection at line 53 is only in O(d) instead of O(d2). Thus the overall time spent in the loop at line 51 is bounded by O(ed2) and the overall complexity is O(ed2+n2d) ≤ O(n2d2).\nProposition 5.2 In a binary WCN, if for any cost function wij ∈ W and for any subintervals Ei ⊆ D(xi), Ej ⊆ D(xj), the minimum of wij over Ei × Ej can be found in constant time, then the time complexity of enforcing BAC∅ is O(n2d).\nProof: This follows again from the proof of Proposition 4.2. In this case, the complexity of projection at line 53 is only in O(1) instead of O(d2). Moreover, the operation at line 57 can be carried out in time O(1) instead of O(d). Thus, the overall time spent in the loop at line 51 is bounded by O(ed) and the overall complexity is O(ed+ n2d) = O(n2d).\nThese two properties are quite straightforward and one may wonder if they have non trivial usage. They can actually be directly exploited to generalize the results presented by Van Hentenryck et al. (1992) for functional, anti-functional and monotonic constraints. In the following sections, we show that functional, anti-functional and semi-convex cost functions (which include monotonic cost functions) can indeed benefit from an O(d) speedup factor by application of Proposition 5.1. For monotonic cost functions and more generally any convex cost function, a stronger speedup factor of O(d2) can be obtained by Proposition 5.2."
    }, {
      "heading" : "5.1 Functional Cost Functions",
      "text" : "The notion of functional constraint can be extended to cost functions as follows:\nDefinition 5.3 A cost function wij is functional w.r.t. xi iff:\n• ∀(vi, vj) ∈ D(xi)×D(xj), wij(vi, vj) ∈ {0, α} with α ∈ [1, k]\n• ∀vi ∈ D(xi), there is at most one value vj ∈ D(xj) such that wij(vi, vj) = 0. When it exists, this value is called the functional support of vi.\nWe assume in the rest of the paper that the functional support can be computed in constant\ntime. For example, the cost function w=ij =\n{\n0 if xi = xj 1 otherwise is functional. In this case, the\nfunctional support of vi is itself. Note that for k = 1, functional cost functions represent functional constraints.\nProposition 5.4 The minimum of a functional cost function wij w.r.t. xi can always be found in O(d).\nProof: For every value vi of xi, one can just check if the functional support of vi belongs to the domain of xj . This requires O(d) checks. If this is never the case, then the minimum of the cost function is known to be α. Otherwise, it is 0. The result follows."
    }, {
      "heading" : "5.2 Anti-Functional and Semi-Convex Cost Functions",
      "text" : "Definition 5.5 A cost function wij is anti-functional w.r.t. the variable xi iff:\n• ∀(vi, vj) ∈ D(xi)×D(xj), wij(vi, vj) ∈ {0, α} with α ∈ [1, k]\n• ∀vi ∈ D(xi), there is at most one value vj ∈ D(xj) such that wij(vi, vj) = α. When it exists, this value is called the anti-support of vi.\nThe cost function w 6=ij =\n{\n0 if xi 6= xj 1 otherwise is an example of an anti-functional cost function.\nIn this case, the anti-support of vi is itself. Note that for k = 1, anti-functional cost functions represent anti-functional constraints.\nAnti-functional cost functions are actually a specific case of semi-convex cost functions, a class of cost functions that appear for example in temporal constraint networks with preferences (Khatib, Morris, Morris, & Rossi, 2001).\nDefinition 5.6 Assume that the domain D(xj) is contained in a set Dj totally ordered by the order <j.\nA function wij is semi-convex w.r.t. xi iff ∀β ∈ [0, k],∀vi ∈ Di, the set {vj ∈ Dj : wij(vi, vj) ≥ β}, called the β-support of vi, defines an interval over Dj according to < j.\nSemi-convexity relies on the definition of intervals defined in a totally ordered discrete set denoted Dj , and ordered by <\nj . Even if they may be identical, it is important to avoid confusion between the order ≺j over D(xj), used to define interval domains for bounds arc consistency, and the order <j over Dj used to define intervals for semi-convexity. In order to guarantee constant time access to the minimum and maximum elements of D(xj) according to <j (called the <j-bounds of the domain), we assume that <j=≺j or < j=≻j 1. In this case, the <j-bounds and the domain bounds are identical. One can simply check that anti-functional cost functions are indeed semi-convex: in this case, the β-support of any value is either the whole domain (β = 0), reduced to one point (0 < β ≤ α) or to the empty set (otherwise). Another example is the cost function wij = x 2 i − x 2 j which is semi-convex w.r.t. xi.\nProposition 5.7 The minimum of a cost function wij which is semi-convex w.r.t. one of its variables can always be found in O(d).\nProof: We will first show that, if wij is semi-convex w.r.t. to one of its variables (let say xi), then for any value vi of xi, the cost function wij must be minimum at one of the <j-bounds of Dj .\n1. This restriction could be removed using for example a doubly-linked list data-structure over the values in D(xj), keeping the domain sorted according to <\nj and allowing constant time access and deletion but this would be at the cost of linear space which we cannot afford in the context of BAC.\nAssume xi is set to vi. Let βb be the lowest cost reached on either of the two < j-bounds of the domain. Since wij is semi-convex, then {vj ∈ Dj : wij(vi, vj) ≥ βb} is an interval, and thus every cost wij(vi, vj) is not less than βb for every value of Dj . Therefore, at least one of the two <j-bounds has a minimum cost.\nIn order to find the global minimum of wij , we can restrict ourselves to the < j-bounds\nof the domain of xj for every value of xi. Therefore, only 2d costs need to be checked.\nFrom Proposition 5.1, we can conclude\nCorollary 5.8 In a binary WCN, if all cost functions are functional, anti-functional or semi-convex, the time complexity of enforcing BAC∅ is O(n2d2) only."
    }, {
      "heading" : "5.3 Monotonic and Convex Cost Functions",
      "text" : "Definition 5.9 Assume that the domain D(xi) (resp. D(xj)) is contained in a set Di (resp. Dj) totally ordered by the order < i (resp. <j).\nA cost function wij is monotonic iff:\n∀(vi, v ′ i, vj , v ′ j) ∈ D 2 i ×D 2 j , v ′ i ≤ i vi ∧ v ′ j ≥ j vj ⇒ wij(v ′ i, v ′ j) ≤ wij(vi, vj)\nThe cost function w≤ij =\n{\n0 if xi ≤ xj 1 otherwise is an example of a monotonic cost function.\nMonotonic cost functions are actually instances of a larger class of functions called convex functions.\nDefinition 5.10 A function wij is convex iff it is semi-convex w.r.t. each of its variables.\nFor example, wij = xi + xj is convex.\nProposition 5.11 The minimum of a convex cost function can always be found in constant time.\nProof: Since the cost function is semi-convex w.r.t. each of its variable, we know from the proof of Proposition 5.7 that it must reach a minimum cost on one of the <j-bounds of the domain of xj and similarly for xi. There are therefore only four costs to check in order to compute the minimum cost.\nFrom Proposition 5.2, we conclude that\nCorollary 5.12 In a binary WCN, if all cost functions are convex, then the time complexity of enforcing BAC∅ is O(n2d) only.\nOne interesting example for a convex cost function is wij = max{xi − xj + cst, 0}. This type of cost function, which can be efficiently filtered by BAC∅, may occur in temporal reasoning problems and is also used in our RNA gene localization problem for specifying preferred distances between elements of a gene."
    }, {
      "heading" : "6. Comparison with Crisp Bounds Consistency",
      "text" : "Petit et al. (2000) have proposed to transform WCNs into crisp constraint networks with extra cost variables. In this transformation, every cost function is reified into a constraint, which applies on the original cost function scope augmented by one extra variable representing the assignment cost. This reification of costs into domain variables transforms a WCN in a crisp CN with more variables and augmented arities. As proposed by Petit et al., it can be achieved using meta-constraints, i.e. logical operators applied to constraints. Given this relation between WCNs and crisp CNs and the relation between BAC∅ and bounds consistency, it is natural to wonder how BAC∅ enforcing relates to just enforcing bounds consistency on the reified version of a WCN.\nIn this section we show that BAC∅ is in some precise sense stronger than enforcing bounds consistency on the reified form. This is a natural consequence of the fact that the domain filtering in BAC is based on the combined cost of several cost functions instead of taking each constraint separately in bounds consistency. We first define the reification process precisely. We then show that BAC∅ can be stronger than the reified bounds consistency on one example and conclude by proving that it can never be weaker.\nThe following example introduces the cost reification process.\nExample 6.1 Consider the WCN in Figure 2(a). It contains two variables x1 and x2, one binary cost function w12, and two unary cost functions w1 and w2. For the sake of clarity, every variable or constraint in the reified hard model, described on Figure 2(b), will be indexed by the letter R.\nFirst of all, we model every cost function by a hard constraint, and express that assigning b to x1 yields a cost of 1. We create a new variable x1 C R, the cost variable of w1, that stores the cost of any assignment of x1. Then, we replace the unary cost function w1 by a binary constraint c1R that involves x1 and x1 C R, such that if a value v1 is assigned to x1, then x1 C R should take the value w1(v1). We do the same for the unary cost function w2. The idea is the same for the binary cost function w12: we create a new variable x12 C R, and we replace w12 by a ternary constraint c12R, that makes sure that for any assignment of x1 and x2 to v1 and v2 respectively, x12 C R takes the value w12(v1, v2). Finally, a global cost constraint cCR that states that the sum of the cost variables should be less than k is added: x1 C R + x2 C R + x12 C R < k. This completes the description of the reified cost hard constraint network.\nWe can now define more formally the reification process of a WCN.\nDefinition 6.2 Consider the WCN P = 〈X ,D,W, k〉. Let reify(P) = 〈XR, DR, WR〉 be the crisp CN such that:\n• the set XR contains one variable xiR for every variable xi ∈ X , augmented with an extra cost variable xS C R per cost function wS ∈ W − {w∅}.\n• the domains DR are:\n– DR(xiR) = D(xi) for the xiR variables, with domain bounds lbiR and ubiR, – [lbS C R, ubS C R] = [0, k − 1] for the xS C R variables.\n• the set WR of constraints contains:\n– cSR = {(t, wS(t)) : t ∈ ℓS , w∅⊕wS(t) < k}, with scope S ∪ {xS C R}, for every cost\nfunction wS ∈ W,\n– cCR is defined as (w∅ ⊕ ∑ wS∈W xS C R < k), an extra constraint that makes sure\nthat the sum of the cost variables is strictly less than k.\nIt is simple to check that the problem reify(P) has a solution iff P has a solution and the sum of the cost variables in a solution is the cost of the corresponding solution (defined by the values of the xiR variables) in the original WCN.\nDefinition 6.3 Let P be a problem, ℓ and ℓ′ two local consistency properties. Let ℓ(P) be the problem obtained after filtering P by ℓ. ℓ is said to be not weaker than ℓ′ iff ℓ′(P) emptiness implies ℓ(P) emptiness.\nℓ is said to be stronger than ℓ′ iff it is not weaker than ℓ′, and if there exists a problem P such that ℓ′(P) is not empty but ℓ(P) is empty.\nThis definition is practically very significant since the emptiness of a filtered problem is the event that generates backtracking in tree search algorithms used for solving CSP and WCSP.\nExample 6.4 Consider the WCN defined by three variables (x1, x2 and x3) and two binary cost functions (w12 and w13). D(x1) = {a, b, c, d}, D(x2) = D(x3) = {a, b, c} (we assume that a ≺ b ≺ c ≺ d). The costs of the binary cost functions are described in Figure 3. Assume that k = 2 and w∅ = 0.\nOne can check that the associated reified problem is already bounds consistent and obviously not empty. For example, a support of the minimum bound of the domain of x1R w.r.t. c12R is (a, a, 1), a support of its maximum bound is (d, a, 1). Supports of the maximum and minimum bounds of the domain of x12 C R w.r.t. c12R are (b, a, 0) and (a, a, 1) respectively. Similarly, one can check that all other variable bounds are also supported on all the constraints that involve them.\nHowever, the original problem is not BAC since for example, the value a, the minimum bound of the domain of x1, does not satisfy the BAC property:\nw∅ ⊕ ∑\nwS∈W,x1∈S\n{\nmin tS∈ℓS ,a∈tS wS(tS)\n}\n< k\nThis means that the value a can be deleted by BAC filtering. By symmetry, the same applies to the maximum bound of x1 and ultimately, the problem inconsistency will be proved by BAC. This shows that bounds consistency on the reified problem cannot be stronger than BAC on the original problem.\nWe will now show that BAC∅ is actually stronger than bounds consistency applied on the reified WCN. Because BAC∅ consistency implies non-emptiness (since it requires the existence of assignments of cost 0 in every cost function) we will start from any BAC∅ consistent WCN P (therefore not empty) and prove that filtering the reified problem reify(P) by bounds consistency does not lead to an empty problem.\nLemma 6.5 Let P be a BAC∅ consistent binary WCN. Then filtering reify(P) by bounds consistency does not produce an empty problem.\nProof: We will prove here that bounds consistency will just reduce the maximum bounds of the domains of the cost variables xS C R to a non empty set and leave all other domains unchanged.\nMore precisely, the final domain of xS C R will become [0,max{wS(t) : t ∈ ℓS , w∅⊕wS(t) < k}]. Note that this interval is not empty because the network is BAC∅ consistent which means that every cost function has an assignment of cost 0 (by ∅-IC) and w∅ < k (or else the bounds of the domains could not have supports and the problem would not be BAC).\nTo prove that bounds consistency will not reduce the problem by more than this, we simply prove that the problem defined by these domain reductions only is actually bounds consistent.\nAll the bounds consistency required properties apply to the bounds of the domains of the variables of reify(P). Let us consider every type of variable in this reified reduced problem:\n• reified variables xiR. Without loss of generality, assume that the minimum bound lbiR of xiR is not bounds consistent (the symmetrical reasoning applies to the maximum bound). This means it would have no support with respect to a given reified constraint\ncSR, xi ∈ S. However, by BAC, we have\nw∅ ⊕ min t∈ℓS ,lbiR∈t wS(t) < k\nand so ∃t ∈ ℓS , lbiR ∈ t, wS(t) ≤ max{wS(t) : t ∈ ℓS , w∅ ⊕ wS(t) < k}\nwhich means that lbiR is supported w.r.t. cSR.\n• cost variables. The minimum bound of all cost variables are always bounds consistent w.r.t. the global constraint cCR because this constraint is a “less than” inequality. Moreover, since the minimum bounds of the cost variables are set to 0, they are also consistent w.r.t. the reified constraints, by the definition of ∅-inverse consistency.\nConsider the maximum bound ubS C R of a cost variable in the reduced reified problem. Remember it is defined as max{wS(t) : t ∈ ℓS , w∅⊕wS(t) < k}, and so w∅⊕ubS C R < k. The minimum bounds of all other cost variables in the reified problem, which are 0, form a support of ubS C R w.r.t. the global constraint c C R. So ubS C R cannot be removed by bounds consistency.\nWe will now prove the final assertion:\nProposition 6.6 BAC∅ is stronger than bounds consistency.\nProof: Lemma 6.5 shows that BAC∅ is not weaker than bounds consistency. Then, example 6.4 is an instance where BAC, and therefore BAC∅ is actually stronger than bounds consistency after reification.\nA filtering related to BAC∅ could be achieved in the reified approach by an extra shaving process where each variable is assigned to one of its domain bounds and this bound is deleted if an inconsistency is found after enforcing bounds consistency (Lhomme, 1993)."
    }, {
      "heading" : "7. Other Related Works",
      "text" : "The Definition 3.1 of BAC is closely related to the notion of arc consistency counts introduced by Freuder and Wallace (1992) for Max-CSP processing. The Max-CSP can be seen as a very simplified form of WCN where cost functions only generate costs of 0 or 1 (when the associated constraint is violated). Our definition of BAC can be seen as an extension of AC counts allowing dealing with arbitrary cost functions, including the usage of w∅ and k, and applied only to domain bounds as in bounds consistency. The addition of ∅-IC makes BAC∅ more powerful.\nDealing with large domains in Max-CSP has also been considered in the Range-Based Algorithm, again designed for Max-CSP by Petit, Régin, and Bessière (2002). This algorithm uses reversible directed arc consistency (DAC) counts and exploits the fact that in Max-CSP, several successive values in a domain may have the same DAC counts. The algorithm intimately relies on the fact that the problem is a Max-CSP problem, defined by a set of constraints and actively uses bounds consistency dedicated propagators for the constraints in the Max-CSP. In this case the number of different values reachable by the DAC counters of a variable is bounded by the degree of the variable, which can be much\nsmaller than the domain size. Handling intervals of values with a same DAC cost as one value allows space and time savings. For arbitrary binary cost functions, the translation into constraints could generate up to d2 constraints for a single cost function and makes the scheme totally impractical.\nSeveral alternative definition of bounds consistency exist in crisp CSPs (Choi et al., 2006). Our extension to WCSP is based on bounds(D) or bounds(Z) consistencies (which are equivalent on intervals). For numerical domains, another possible weaker definition of bounds consistency is bounds(R) consistency, which is obtained by a relaxation to real numbers. It has been shown by Choi et al. that bounds(R) consistency can be checked in polynomial time on some constraints whereas bounds(D) or bounds(Z) is NP-hard (eg. for linear equality). The use of this relaxed version in the WCSP context together with intentional description of cost functions would have the side effect of extending the cost domain from integer to real numbers. Because extensional or algorithmical description of integer cost functions is more general and frequent in our problems, this possibility was not considered. Since cost comparison is the fundamental mechanism used for pruning in WCSP, a shift to real numbers for costs would require a safe floating number implementation both in the local consistency enforcing algorithms and in the branch and bound algorithm."
    }, {
      "heading" : "8. Experimental Results",
      "text" : "We experimented bounds arc consistency on two benchmarks translated into weighted CSPs. The first benchmark is from AI planning and scheduling. It is a mission management benchmark for agile satellites (Verfaillie & Lemâıtre, 2001; de Givry & Jeannin, 2006). The maximum domain size of the temporal variables is 201. This reasonable size and the fact that there are only binary cost functions allows us to compare BAC∅ with strong local consistencies such as EDAC*. Additionally, this benchmark has also been modeled using the reified version of WCN, thus allowing for an experimental counterpart of the theoretical comparison of Section 6.\nThe second benchmark comes from bioinformatics and models the problem of the localization of non-coding RNA molecules in genomes (Thébault et al., 2006; Zytnicki et al., 2008). Our aim here is mostly to confirm that bounds arc consistency is useful and practical on a real complex problem with huge domains, which can reach several millions."
    }, {
      "heading" : "8.1 A Mission Management Benchmark for Agile Satellites",
      "text" : "We solved a simplified version described by de Givry and Jeannin (2006) of a problem of selecting and scheduling earth observations for agile satellites. A complete description of the problem is given by Verfaillie and Lemâıtre (2001). The satellite has a pool of candidate photographs to take. It must select and schedule a subset of them on each pass above a certain strip of territory. The satellite can only take one photograph at a time (disjunctive scheduling). A photograph can only be taken during a time window that depends on the location photographed. Minimal repositioning times are required between two consecutive photographs. All physical constraints (time windows and repositioning times) must be met, and the sum of the revenues of the selected photographs must be maximized. This is equivalent to minimizing the “rejected revenues” of the non selected photographs.\nLet N be the number of candidate photographs. We define N decision variables representing the acquisition starting times of the candidate photographs. The domain of each variable is defined by the time window of its corresponding photograph plus an extra domain value which represents the fact that the photograph is not selected. As proposed by de Givry and Jeannin (2006), we create a binary hard constraint for every pair of photographs (resulting in a complete constraint graph) which enforces the minimal repositioning times if both photographs are selected (represented by a disjunctive constraint). For each photograph, a unary cost function associates its rejected revenue to the corresponding extra value.\nIn order to have a better filtering, we moved costs from unary cost functions inside the binary hard constraints in a preprocessing step. This allows bounds arc consistency filtering to exploit the revenue information and the repositioning times jointly, possibly increasing w∅ and the starting times of some photographs. To achieve this, for each variable xi, the unary cost function wi is successively combined (using ⊕) with each binary hard constraint wij that involves xi. This yields N − 1 new binary cost functions w ′ ij defined as w′ij(t) = wij(t) ⊕ wi(t[xi]), having both hard (+∞) and soft weights. These binary cost functions w′ij replace the unary cost function wi and the N − 1 original binary hard constraints wij . Notice that this transformation has the side effect of multiplying all soft weights by N − 1. This does preserve the equivalence with the original problem since all finite weights are just multiplied by the same constant (N − 1).\nThe search procedure is an exact depth-first branch-and-bound dedicated to scheduling problems, using a schedule or postpone strategy as described by de Givry and Jeannin (2006) which avoids the enumeration of all possible starting time values. No initial upper bound was provided (k = +∞).\nWe generated 100 random instances for different numbers of candidate photographs (N varying from 10 to 30)2. We compared BAC∅ (denoted by BAC0 in the experimental results) with EDAC* (Heras et al., 2005) (denoted by EDAC*). Note that FDAC* and VAC (applied in preprocessing and during search, in addition to EDAC*) were also tested on these instances, but did not improve over EDAC* (FDAC* was slightly faster than EDAC* but developed more search nodes and VAC was significantly slower than EDAC*, without improving w∅ in preprocessing). OSAC is not practical on this benchmark (for N = 20, it has to solve a linear problem with 50, 000 variables and about 4 million constraints). All the algorithms are using the same search procedure. They are implemented in the toulbar2 C++ solver3. Finding the minimum cost of the previously-described binary cost functions (which are convex if we consider the extra domain values for rejected photographs separately), is done in constant time for BAC∅. It is done in time O(d2) for EDAC* (d = 201).\nWe also report the results obtained by maintaining bounds consistency on the reified problem using meta-constraints as described by de Givry and Jeannin (2006), using the claire/Eclair C++ constraint programming solver (de Givry, Jeannin, Josset, Mattioli, Museux, & Savéant, 2002) developed by THALES (denoted by B-consistency).\nThe results are presented in Figure 4, using a log-scale. These results were obtained on a 3 GHz Intel Xeon with 4 GB of RAM. Figure 4 shows the mean CPU time in seconds and the mean number of backtracks performed by the search procedure to find the optimum\n2. These instances are available at http://www.inra.fr/mia/ftp/T/bep/. 3. See http://carlit.toulouse.inra.fr/cgi-bin/awki.cgi/ToolBarIntro.\nand prove optimality as the problem size increases. In the legends, algorithms are sorted by increasing efficiency.\nThe analysis of experimental results shows that BAC∅ was up to 35 times faster than EDAC* while doing only 25% more backtracks than EDAC* (for N = 30, no backtrack results are reported as EDAC* does not solve any instance within the time limit of 6 hours). It shows that bounds arc consistency can prune almost as many search nodes as a stronger local consistency does in much less time for temporal reasoning problems where the semantic of the cost functions can be exploited, as explained in Section 5. The second fastest approach was bounds consistency on the reified representation which was at least 2.3 worse than BAC∅ in terms of speed and number of backtracks when N ≥ 25. This is a practical confirmation of the comparison of Section 6. The reified approach used with bounds consistency introduces Boolean decision variables for representing photograph selection and uses a criteria defined as a linear function of these variables. Contrarily to BAC∅, bounds consistency is by definition unable to reason simultaneously on the combination of several constraints to prune the starting times."
    }, {
      "heading" : "8.2 Non-coding RNA Gene Localization",
      "text" : "A non-coding RNA (ncRNA) gene is a functional molecule composed of smaller molecules, called nucleotides, linked together by covalent bonds. There are four types of these nucleotides, commonly identified by a single letter: A, U, G and C. Thus, an RNA can be represented as a word built from the four letters. This sequence defines what is called the primary structure of the RNA molecule.\nRNA molecules have the ability to fold back on themselves by developing interactions between nucleotides, forming pairs. The most frequently interacting pairs are: a G interacts with a C, or a U interacts with an A. A sequence of such interactions forms a structure called a helix. Helices are a fundamental structural element in ncRNA genes and are the basis for more complex structures. The set of interactions is often displayed by a graph where vertices represent nucleotides and edges represent either covalent bonds linking successive nucleotides (represented as plain lines in Figure 5) or interacting nucleotide pairs (represented as dotted lines). This representation is usually called the molecule’s secondary structure. See the graph of a helix in Figure 5(a).\nThe set of ncRNAs that have a common biological function is called a family. The signature of a gene family is the set of conserved elements either in the sequence or the secondary structure. It can be expressed as a collection of properties that must be satisfied by a set of regions occurring on a sequence. Given the signature of a family, the problem we are interested in involves searching for new members of a gene family in existing genomes, where these members are in fact the set of regions appearing in the genome which satisfy the signature properties. Genomic sequences are themselves long texts composed of nucleotides. They can be thousand of nucleotides long for the simplest organisms up to several hundred million nucleotides for the more complex ones. The problem of searching for an occurrence of a gene signature in a genomic sequence is NP-complete for complex combinations of helix structures (Vialette, 2004).\nIn order to find ncRNAs, we can build a weighted constraint network that scans a genome, and detects the regions of the genome where the signature elements are present\nand correctly positioned. The variables are the positions of the signature elements in the sequence. The size of the domains is the size of the genomic sequence. Cost functions enforce the presence of the signature elements between the positions taken by the variables involved. Examples of cost functions are given in Figure 5.\n• The pattern(xi, xj , p) function states that a fixed word p, given as parameter, should be found between the positions indicated by the variables xi and xj . The cost given by the function is the edit distance between the word found at xi:xj and the word p (see the cost function pattern with the word ACGUA in Figure 5(b)).\n• The helix(xi, xj , xk, xl,m) function states that the nucleotides between positions xi and xj should be able to bind with the nucleotides between xk and xl. Parameter m specifies a minimum helix length. The cost given is the number of mismatches or nucleotides left unmatched (see the helix function with 5 interacting nucleotide pairs in Figure 5(c)).\n• Finally, the function, spacer(xi, xj , d1, d2, d3, d4) specifies a favorite range of distances between positions xi and xj using a trapezoidal cost function as shown in Figure 5(d).\nSee the work of Zytnicki et al. (2008) for a complete description of the cost functions.\nBecause of the sheer domain size, and given that the complex pattern matching oriented cost functions do not have any specific property that could speedup filtering, BAC alone has been used for filtering these cost functions (Zytnicki et al., 2008). The exception is the\npiecewise linear spacer cost function: its minimum can be computed in constant time for BAC∅ enforcement. The resulting C++ solver is called DARN!4.\nA typical benchmark for the ncRNA localization problem is the transfer RNA (tRNA) localization. The tRNA signature (Gautheret, Major, & Cedergren, 1990) can be modelled by 22 variables, 3 nucleotide words, 4 helices, and 7 spacers. DARN! searched for all the solutions with a cost strictly lower than the maximum cost k = 3. Just to illustrate the absolute necessity of using bounds arc consistency in this problem, we compared bounds arc consistency enforcement with AC* (Larrosa, 2002) on sub-sequences of the genome of Escherichia coli, which is 4.9 million nucleotides long. Because of their identical space complexity and because they have not been defined nor implemented on non-binary cost functions (helix is a quaternarycost function), DAC, FDAC or EDAC have not been tested (see the work of Sànchez et al., 2008, however for an extension of FDAC to ternary cost functions).\nThe results are displayed in Table 1. For different beginning sub-sequences of the complete sequence, we report the size of the sub-sequence in which the signature is searched for (10k is a sequence of 10,000 nucleotides), as well as the number of solutions found. We also show the number of backtracks and the time spent on a 3 GHz Intel Xeon with 2 GB. A “-” means the instance could not be solved due to memory reasons, and despite memory optimizations. BAC solved the complete sequence in less than 3 seconds. BAC is approximately 300, 000 (resp. 4, 400, 000) times faster than AC* for the 10k (resp. 50k) sub-sequence. More results on other genomes and ncRNA signatures can be found in the work of Zytnicki et al. (2008).\nThe reason of the superiority of BAC over AC* is twofold. First, AC* needs to store all the unary costs for every variable and projects costs from binary cost functions to unary cost functions. Thus, the space complexity of AC* is at least O(nd). For very large domains (in our experiments, greater than 100,000 values), the computer cannot allocate a sufficient memory and the program is aborted. For the same kind of projection, BAC only needs to store the costs of the bounds of the domains, leading to a space complexity of O(n).\nSecond, BAC does not care about the interior values and focuses on the bounds of the domains only. On the other hand, AC* projects all the binary costs to all the interior values,\n4. DARN!, and several genomic sequences and family signatures are available at http://carlit.toulouse.inra.fr/Darn/.\nwhich takes a lot of time, but should remove more values and detect inconsistencies earlier. However, Table 1 shows that the number of backtracks performed by AC* and BAC are the same. This can be explained as follows. Due to the nature of the cost functions used in these problems, the supports of the bounds of the domains of the variables usually are the bounds of the other variables. Thus, removing the values which are inside the domains, as AC* does, do not help removing the bounds of the variables. As a consequence, the bounds founds by BAC are the same as those found by AC*. This explains why enforcing of AC* generally does not lead to new domain wipe out compared to BAC, and finding the support inside the bounds of the domains is useless.\nNotice that the spacer cost functions dramatically reduce the size of the domains. When a single variable is assigned, all the other domain sizes are dramatically reduced, and the instance becomes quickly tractable. Moreover, the helix constraint has the extra knowledge of a maximum distance djk between its variables xj and xk (see Fig. 5(c)) which bounds the time complexity of finding the minimum cost w.r.t. djk and not the length of the sequence."
    }, {
      "heading" : "9. Conclusions and Future Work",
      "text" : "We have presented here new local consistencies for weighted CSPs dedicated to large domains as well as algorithms to enforce these properties. The first local consistency, BAC, has a time complexity which can be easily reduced if the semantics of the cost function is appropriate. A possible enhancement of this property, ∅-IC, has also been presented. Our experiments showed that maintaining bounds arc consistency is much better than AC* for problems with large domains, such as ncRNA localization and scheduling for Earth observation satellites. This is due to the fact that AC* cannot handle problems with large domains, especially because of its high memory complexity, but also because BAC∅ behaves particularly well with specific classes of cost functions.\nSimilarly to bounds consistency, which is implemented on almost all state-of-the-art CSP solvers, this new local property has been implemented in the open source toulbar2 WCSP solver.5\nBAC, BAC∅ and ∅-inverse consistency allowed us to transfer bounds consistency CSP to weighted CSP, including improved propagation for specific classes of binary cost functions. Our implementation for RNA gene finding is also able to filter non-binary constraints. It would therefore be quite natural to try to define efficient algorithms for enforcing BAC, BAC∅ or ∅-inverse consistency on specific cost functions of arbitrary arity such as the soft global constraints derived from All-Diff, GCC or regular (Régin, 1994; Van Hoeve, Pesant, & Rousseau, 2006). This line of research has been recently explored by Lee and Leung (2009).\nFinally, another interesting extension of this work would be to better exploit the connection between BAC and bounds consistency by exploiting the idea of Virtual Arc Consistency introduced by Cooper et al. (2008). The connection established by Virtual AC between crisp CNs and WCNs is much finer grained than in the reification approach considered by Petit et al. (2000) and could provide strong practical and theoretical results.\n5. Available at http://carlit.toulouse.inra.fr/cgi-bin/awki.cgi/ToolBarIntro."
    } ],
    "references" : [ {
      "title" : "The essence of constraint propagation",
      "author" : [ "K. Apt" ],
      "venue" : "Theoretical computer science,",
      "citeRegEx" : "Apt,? \\Q1999\\E",
      "shortCiteRegEx" : "Apt",
      "year" : 1999
    }, {
      "title" : "Refining the basic constraint propagation algorithm",
      "author" : [ "C. Bessière", "Régin", "J.-C" ],
      "venue" : "In Proc. of IJCAI’01,",
      "citeRegEx" : "Bessière et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Bessière et al\\.",
      "year" : 2001
    }, {
      "title" : "Markov Random Fields: Theory and Applications",
      "author" : [ "R. Chellappa", "A. Jain" ],
      "venue" : null,
      "citeRegEx" : "Chellappa and Jain,? \\Q1993\\E",
      "shortCiteRegEx" : "Chellappa and Jain",
      "year" : 1993
    }, {
      "title" : "Finite domain bounds consistency revisited",
      "author" : [ "C.W. Choi", "W. Harvey", "J.H.M. Lee", "P.J. Stuckey" ],
      "venue" : "In Proc. of Australian Conference on Artificial Intelligence,",
      "citeRegEx" : "Choi et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Choi et al\\.",
      "year" : 2006
    }, {
      "title" : "Arc consistency for soft constraints",
      "author" : [ "M. Cooper", "T. Schiex" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Cooper and Schiex,? \\Q2004\\E",
      "shortCiteRegEx" : "Cooper and Schiex",
      "year" : 2004
    }, {
      "title" : "Virtual arc consistency for weighted CSP",
      "author" : [ "M.C. Cooper", "S. de Givry", "M. Sànchez", "T. Schiex", "M. Zytnicki" ],
      "venue" : "In Proc. of AAAI’2008",
      "citeRegEx" : "Cooper et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Cooper et al\\.",
      "year" : 2008
    }, {
      "title" : "Optimal soft arc consistency",
      "author" : [ "M.C. Cooper", "S. de Givry", "T. Schiex" ],
      "venue" : "In Proc. of IJCAI’07,",
      "citeRegEx" : "Cooper et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Cooper et al\\.",
      "year" : 2007
    }, {
      "title" : "A unified framework for partial and hybrid search methods in constraint programming",
      "author" : [ "S. de Givry", "L. Jeannin" ],
      "venue" : "Computer & Operations Research,",
      "citeRegEx" : "Givry and Jeannin,? \\Q2006\\E",
      "shortCiteRegEx" : "Givry and Jeannin",
      "year" : 2006
    }, {
      "title" : "The THALES constraint programming framework for hard and soft real-time applications",
      "author" : [ "S. de Givry", "L. Jeannin", "F. Josset", "J. Mattioli", "N. Museux", "P. Savéant" ],
      "venue" : "The PLANET Newsletter, Issue",
      "citeRegEx" : "Givry et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Givry et al\\.",
      "year" : 2002
    }, {
      "title" : "Partial constraint satisfaction",
      "author" : [ "E. Freuder", "R. Wallace" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Freuder and Wallace,? \\Q1992\\E",
      "shortCiteRegEx" : "Freuder and Wallace",
      "year" : 1992
    }, {
      "title" : "Pattern searching/alignment with RNA primary and secondary structures: an effective descriptor for tRNA",
      "author" : [ "D. Gautheret", "F. Major", "R. Cedergren" ],
      "venue" : "Comp. Appl. Biosc.,",
      "citeRegEx" : "Gautheret et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Gautheret et al\\.",
      "year" : 1990
    }, {
      "title" : "Existential arc consistency: Getting closer to full arc consistency in weighted CSPs",
      "author" : [ "F. Heras", "J. Larrosa", "S. de Givry", "M. Zytnicki" ],
      "venue" : "In Proc. of IJCAI’05,",
      "citeRegEx" : "Heras et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Heras et al\\.",
      "year" : 2005
    }, {
      "title" : "Temporal constraint reasoning with preferences",
      "author" : [ "L. Khatib", "P. Morris", "R. Morris", "F. Rossi" ],
      "venue" : "In Proc. of IJCAI’01,",
      "citeRegEx" : "Khatib et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Khatib et al\\.",
      "year" : 2001
    }, {
      "title" : "Node and arc consistency in weighted CSP",
      "author" : [ "J. Larrosa" ],
      "venue" : "In Proc. of AAAI’02,",
      "citeRegEx" : "Larrosa,? \\Q2002\\E",
      "shortCiteRegEx" : "Larrosa",
      "year" : 2002
    }, {
      "title" : "Solving weighted CSP by maintaining arc-consistency",
      "author" : [ "J. Larrosa", "T. Schiex" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Larrosa and Schiex,? \\Q2004\\E",
      "shortCiteRegEx" : "Larrosa and Schiex",
      "year" : 2004
    }, {
      "title" : "Towards Efficient Consistency Enforcement for Global Constraints in Weighted Constraint Satisfaction",
      "author" : [ "J. Lee", "K. Leung" ],
      "venue" : "In Proc. of IJCAI’09",
      "citeRegEx" : "Lee and Leung,? \\Q2009\\E",
      "shortCiteRegEx" : "Lee and Leung",
      "year" : 2009
    }, {
      "title" : "Consistency techniques for numeric CSPs",
      "author" : [ "O. Lhomme" ],
      "venue" : "In Proc. of IJCAI’93,",
      "citeRegEx" : "Lhomme,? \\Q1993\\E",
      "shortCiteRegEx" : "Lhomme",
      "year" : 1993
    }, {
      "title" : "Meta-constraints on violations for over constrained problems",
      "author" : [ "T. Petit", "Régin", "J.-C", "C. Bessière" ],
      "venue" : "In Proc. of ICTAI’00,",
      "citeRegEx" : "Petit et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Petit et al\\.",
      "year" : 2000
    }, {
      "title" : "Range-based algorithm for Max-CSP",
      "author" : [ "T. Petit", "J.C. Régin", "C. Bessière" ],
      "venue" : "In Proc. of CP’02,",
      "citeRegEx" : "Petit et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Petit et al\\.",
      "year" : 2002
    }, {
      "title" : "A filtering algorithm for constraints of difference in CSPs",
      "author" : [ "Régin", "J.-C" ],
      "venue" : "In Proc. of AAAI’94,",
      "citeRegEx" : "Régin and J..C.,? \\Q1994\\E",
      "shortCiteRegEx" : "Régin and J..C.",
      "year" : 1994
    }, {
      "title" : "Mendelian error detection in complex pedigrees using weighted constraint satisfaction",
      "author" : [ "M. Sànchez", "S. de Givry", "T. Schiex" ],
      "venue" : "techniques. Constraints,",
      "citeRegEx" : "Sànchez et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Sànchez et al\\.",
      "year" : 2008
    }, {
      "title" : "An Algorithm for Optimal Winner Determination in Combinatorial Auctions",
      "author" : [ "T. Sandholm" ],
      "venue" : "In Proc. of IJCAI’99,",
      "citeRegEx" : "Sandholm,? \\Q1999\\E",
      "shortCiteRegEx" : "Sandholm",
      "year" : 1999
    }, {
      "title" : "Arc consistency for soft constraints",
      "author" : [ "T. Schiex" ],
      "venue" : "In Proc. of CP’00,",
      "citeRegEx" : "Schiex,? \\Q2000\\E",
      "shortCiteRegEx" : "Schiex",
      "year" : 2000
    }, {
      "title" : "A lattice-theoretical fixpoint theorem and its applications",
      "author" : [ "A. Tarski" ],
      "venue" : "Pacific Journal of Mathematics,",
      "citeRegEx" : "Tarski,? \\Q1955\\E",
      "shortCiteRegEx" : "Tarski",
      "year" : 1955
    }, {
      "title" : "Searching RNA motifs and their intermolecular contacts with constraint",
      "author" : [ "P. Thébault", "S. de Givry", "T. Schiex", "C. Gaspin" ],
      "venue" : "networks. Bioinformatics,",
      "citeRegEx" : "Thébault et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Thébault et al\\.",
      "year" : 2006
    }, {
      "title" : "A generic arc-consistency algorithm and its specializations",
      "author" : [ "P. Van Hentenryck", "Y. Deville", "Teng", "C.-M" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Hentenryck et al\\.,? \\Q1992\\E",
      "shortCiteRegEx" : "Hentenryck et al\\.",
      "year" : 1992
    }, {
      "title" : "On global warming: Flow-based soft global constraints",
      "author" : [ "W. Van Hoeve", "G. Pesant", "L. Rousseau" ],
      "venue" : "Journal of Heuristics,",
      "citeRegEx" : "Hoeve et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Hoeve et al\\.",
      "year" : 2006
    }, {
      "title" : "Selecting and scheduling observations for agile satellites: some lessons from the constraint reasoning community point of view",
      "author" : [ "G. Verfaillie", "M. Lemâıtre" ],
      "venue" : "In Proc. of CP’01,",
      "citeRegEx" : "Verfaillie and Lemâıtre,? \\Q2001\\E",
      "shortCiteRegEx" : "Verfaillie and Lemâıtre",
      "year" : 2001
    }, {
      "title" : "On the computational complexity of 2-interval pattern matching problems",
      "author" : [ "S. Vialette" ],
      "venue" : "Theoretical Computer Science,",
      "citeRegEx" : "Vialette,? \\Q2004\\E",
      "shortCiteRegEx" : "Vialette",
      "year" : 2004
    }, {
      "title" : "DARN! A soft constraint solver for RNA motif",
      "author" : [ "M. Zytnicki", "C. Gaspin", "T. Schiex" ],
      "venue" : "localization. Constraints,",
      "citeRegEx" : "Zytnicki et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Zytnicki et al\\.",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 21,
      "context" : "The WCSP defines a simple optimization (minimization) framework with a wide range of applications in resource allocation, scheduling, bioinformatics (Sànchez, de Givry, & Schiex, 2008; Zytnicki, Gaspin, & Schiex, 2008), electronic markets (Sandholm, 1999), etc.",
      "startOffset" : 239,
      "endOffset" : 255
    }, {
      "referenceID" : 22,
      "context" : "This includes soft AC (Schiex, 2000), AC* (Larrosa, 2002), FDAC* (Larrosa & Schiex, 2004), EDAC* (Heras, Larrosa, de Givry, & Zytnicki, 2005), OSAC (Cooper, de Givry, & Schiex, 2007) and VAC (Cooper, de Givry, Sànchez, Schiex, & Zytnicki, 2008) among others.",
      "startOffset" : 22,
      "endOffset" : 36
    }, {
      "referenceID" : 13,
      "context" : "This includes soft AC (Schiex, 2000), AC* (Larrosa, 2002), FDAC* (Larrosa & Schiex, 2004), EDAC* (Heras, Larrosa, de Givry, & Zytnicki, 2005), OSAC (Cooper, de Givry, & Schiex, 2007) and VAC (Cooper, de Givry, Sànchez, Schiex, & Zytnicki, 2008) among others.",
      "startOffset" : 42,
      "endOffset" : 57
    }, {
      "referenceID" : 16,
      "context" : "Initially modeled as a crisp CSP, this problem has been tackled using bounds consistency (Choi, Harvey, Lee, & Stuckey, 2006; Lhomme, 1993) and dedicated propagators using efficient pattern matching algorithms (Thébault, de Givry, Schiex, & Gaspin, 2006).",
      "startOffset" : 89,
      "endOffset" : 139
    }, {
      "referenceID" : 29,
      "context" : "To conclude, we experimentally compare the efficiency of algorithms that maintain these different local consistencies inside branch-and-bound on agile satellite scheduling problems (Verfaillie & Lemâıtre, 2001) and RNA gene localization problems (Zytnicki et al., 2008) and observe clear speedups compared to different existing local consistencies.",
      "startOffset" : 246,
      "endOffset" : 269
    }, {
      "referenceID" : 3,
      "context" : "Using the terminology introduced by Choi et al. (2006), the bounds consistency considered in this paper is the bounds(D) consistency.",
      "startOffset" : 36,
      "endOffset" : 55
    }, {
      "referenceID" : 21,
      "context" : "Initially introduced by Schiex (2000), the extension of arc consistency to WCSP has been refined by Larrosa (2002) leading to the definition of AC*.",
      "startOffset" : 24,
      "endOffset" : 38
    }, {
      "referenceID" : 13,
      "context" : "Initially introduced by Schiex (2000), the extension of arc consistency to WCSP has been refined by Larrosa (2002) leading to the definition of AC*.",
      "startOffset" : 100,
      "endOffset" : 115
    }, {
      "referenceID" : 13,
      "context" : "7 (Larrosa, 2002) A variable xi is node consistent iff: • ∀vi ∈ D(xi), w∅ ⊕ wi(vi) < k.",
      "startOffset" : 2,
      "endOffset" : 17
    }, {
      "referenceID" : 4,
      "context" : "As shown by Cooper and Schiex (2004), the fundamental mechanism required here is the ability to move costs between different scopes.",
      "startOffset" : 12,
      "endOffset" : 37
    }, {
      "referenceID" : 3,
      "context" : "One can note that this definition is a proper generalization of bounds consistency since when k = 1, it is actually equivalent to the definition of bounds(D) consistency for crisp CSP (Choi et al., 2006) (also equivalent to bounds(Z) consistency since domains are defined as intervals).",
      "startOffset" : 184,
      "endOffset" : 203
    }, {
      "referenceID" : 13,
      "context" : "Note that exploiting the information of last supports as in AC2001 (Bessière & Régin, 2001) does not reduce the worst-case time complexity because the minimum cost of a cost function must be recomputed from scratch each time a domain has been reduced and the last support has been lost (Larrosa, 2002).",
      "startOffset" : 286,
      "endOffset" : 301
    }, {
      "referenceID" : 6,
      "context" : "Note that although OSAC (Cooper et al., 2007) is able to find an optimal w∅ (at much higher",
      "startOffset" : 24,
      "endOffset" : 45
    }, {
      "referenceID" : 23,
      "context" : "Obviously, every function f ∈ FBAC is order preserving: ∀(P1,P2) ∈ ℘ 2 1(P),P1 ⊑ P2 ⇒ f(P1) ⊑ f(P2) By application of the Tarski-Knaster theorem (Tarski, 1955), it is known that every function f ∈ FBAC (applied until quiescence during BAC enforcement) has at least one fixpoint, and that the set of these fixed points forms a lattice for ⊑.",
      "startOffset" : 145,
      "endOffset" : 159
    }, {
      "referenceID" : 0,
      "context" : "This proof technique is usual for proving convergence of the chaotic iteration of a collection of suitable functions and has been used for characterizing CSP local consistency by Apt (1999). During the enforcement of BAC, the original problem P = 〈X ,D,W, k〉 is iteratively transformed into a set of different problems which are all equivalent to P, and obtained by deleting values violating BAC.",
      "startOffset" : 179,
      "endOffset" : 190
    }, {
      "referenceID" : 4,
      "context" : "Note that the subtraction applied to all constraint tuples at line 75 can be done in constant time without modifying the constraint by using an additional ∆S data-structure, similar to the ∆ data-structure introduced by Cooper and Schiex (2004). This data-structure keeps track of the cost which has been projected from wS to w∅.",
      "startOffset" : 220,
      "endOffset" : 245
    }, {
      "referenceID" : 25,
      "context" : "They can actually be directly exploited to generalize the results presented by Van Hentenryck et al. (1992) for functional, anti-functional and monotonic constraints.",
      "startOffset" : 83,
      "endOffset" : 108
    }, {
      "referenceID" : 17,
      "context" : "Comparison with Crisp Bounds Consistency Petit et al. (2000) have proposed to transform WCNs into crisp constraint networks with extra cost variables.",
      "startOffset" : 41,
      "endOffset" : 61
    }, {
      "referenceID" : 16,
      "context" : "A filtering related to BAC could be achieved in the reified approach by an extra shaving process where each variable is assigned to one of its domain bounds and this bound is deleted if an inconsistency is found after enforcing bounds consistency (Lhomme, 1993).",
      "startOffset" : 247,
      "endOffset" : 261
    }, {
      "referenceID" : 9,
      "context" : "1 of BAC is closely related to the notion of arc consistency counts introduced by Freuder and Wallace (1992) for Max-CSP processing.",
      "startOffset" : 82,
      "endOffset" : 109
    }, {
      "referenceID" : 9,
      "context" : "1 of BAC is closely related to the notion of arc consistency counts introduced by Freuder and Wallace (1992) for Max-CSP processing. The Max-CSP can be seen as a very simplified form of WCN where cost functions only generate costs of 0 or 1 (when the associated constraint is violated). Our definition of BAC can be seen as an extension of AC counts allowing dealing with arbitrary cost functions, including the usage of w∅ and k, and applied only to domain bounds as in bounds consistency. The addition of ∅-IC makes BAC more powerful. Dealing with large domains in Max-CSP has also been considered in the Range-Based Algorithm, again designed for Max-CSP by Petit, Régin, and Bessière (2002). This algorithm uses reversible directed arc consistency (DAC) counts and exploits the fact that in Max-CSP, several successive values in a domain may have the same DAC counts.",
      "startOffset" : 82,
      "endOffset" : 694
    }, {
      "referenceID" : 3,
      "context" : "Several alternative definition of bounds consistency exist in crisp CSPs (Choi et al., 2006).",
      "startOffset" : 73,
      "endOffset" : 92
    }, {
      "referenceID" : 24,
      "context" : "The second benchmark comes from bioinformatics and models the problem of the localization of non-coding RNA molecules in genomes (Thébault et al., 2006; Zytnicki et al., 2008).",
      "startOffset" : 129,
      "endOffset" : 175
    }, {
      "referenceID" : 29,
      "context" : "The second benchmark comes from bioinformatics and models the problem of the localization of non-coding RNA molecules in genomes (Thébault et al., 2006; Zytnicki et al., 2008).",
      "startOffset" : 129,
      "endOffset" : 175
    }, {
      "referenceID" : 7,
      "context" : "We solved a simplified version described by de Givry and Jeannin (2006) of a problem of selecting and scheduling earth observations for agile satellites.",
      "startOffset" : 47,
      "endOffset" : 72
    }, {
      "referenceID" : 7,
      "context" : "We solved a simplified version described by de Givry and Jeannin (2006) of a problem of selecting and scheduling earth observations for agile satellites. A complete description of the problem is given by Verfaillie and Lemâıtre (2001). The satellite has a pool of candidate photographs to take.",
      "startOffset" : 47,
      "endOffset" : 235
    }, {
      "referenceID" : 11,
      "context" : "We compared BAC (denoted by BAC0 in the experimental results) with EDAC* (Heras et al., 2005) (denoted by EDAC*).",
      "startOffset" : 73,
      "endOffset" : 93
    }, {
      "referenceID" : 7,
      "context" : "As proposed by de Givry and Jeannin (2006), we create a binary hard constraint for every pair of photographs (resulting in a complete constraint graph) which enforces the minimal repositioning times if both photographs are selected (represented by a disjunctive constraint).",
      "startOffset" : 18,
      "endOffset" : 43
    }, {
      "referenceID" : 7,
      "context" : "As proposed by de Givry and Jeannin (2006), we create a binary hard constraint for every pair of photographs (resulting in a complete constraint graph) which enforces the minimal repositioning times if both photographs are selected (represented by a disjunctive constraint). For each photograph, a unary cost function associates its rejected revenue to the corresponding extra value. In order to have a better filtering, we moved costs from unary cost functions inside the binary hard constraints in a preprocessing step. This allows bounds arc consistency filtering to exploit the revenue information and the repositioning times jointly, possibly increasing w∅ and the starting times of some photographs. To achieve this, for each variable xi, the unary cost function wi is successively combined (using ⊕) with each binary hard constraint wij that involves xi. This yields N − 1 new binary cost functions w ′ ij defined as w′ ij(t) = wij(t) ⊕ wi(t[xi]), having both hard (+∞) and soft weights. These binary cost functions w′ ij replace the unary cost function wi and the N − 1 original binary hard constraints wij . Notice that this transformation has the side effect of multiplying all soft weights by N − 1. This does preserve the equivalence with the original problem since all finite weights are just multiplied by the same constant (N − 1). The search procedure is an exact depth-first branch-and-bound dedicated to scheduling problems, using a schedule or postpone strategy as described by de Givry and Jeannin (2006) which avoids the enumeration of all possible starting time values.",
      "startOffset" : 18,
      "endOffset" : 1525
    }, {
      "referenceID" : 7,
      "context" : "As proposed by de Givry and Jeannin (2006), we create a binary hard constraint for every pair of photographs (resulting in a complete constraint graph) which enforces the minimal repositioning times if both photographs are selected (represented by a disjunctive constraint). For each photograph, a unary cost function associates its rejected revenue to the corresponding extra value. In order to have a better filtering, we moved costs from unary cost functions inside the binary hard constraints in a preprocessing step. This allows bounds arc consistency filtering to exploit the revenue information and the repositioning times jointly, possibly increasing w∅ and the starting times of some photographs. To achieve this, for each variable xi, the unary cost function wi is successively combined (using ⊕) with each binary hard constraint wij that involves xi. This yields N − 1 new binary cost functions w ′ ij defined as w′ ij(t) = wij(t) ⊕ wi(t[xi]), having both hard (+∞) and soft weights. These binary cost functions w′ ij replace the unary cost function wi and the N − 1 original binary hard constraints wij . Notice that this transformation has the side effect of multiplying all soft weights by N − 1. This does preserve the equivalence with the original problem since all finite weights are just multiplied by the same constant (N − 1). The search procedure is an exact depth-first branch-and-bound dedicated to scheduling problems, using a schedule or postpone strategy as described by de Givry and Jeannin (2006) which avoids the enumeration of all possible starting time values. No initial upper bound was provided (k = +∞). We generated 100 random instances for different numbers of candidate photographs (N varying from 10 to 30)2. We compared BAC (denoted by BAC0 in the experimental results) with EDAC* (Heras et al., 2005) (denoted by EDAC*). Note that FDAC* and VAC (applied in preprocessing and during search, in addition to EDAC*) were also tested on these instances, but did not improve over EDAC* (FDAC* was slightly faster than EDAC* but developed more search nodes and VAC was significantly slower than EDAC*, without improving w∅ in preprocessing). OSAC is not practical on this benchmark (for N = 20, it has to solve a linear problem with 50, 000 variables and about 4 million constraints). All the algorithms are using the same search procedure. They are implemented in the toulbar2 C++ solver3. Finding the minimum cost of the previously-described binary cost functions (which are convex if we consider the extra domain values for rejected photographs separately), is done in constant time for BAC. It is done in time O(d2) for EDAC* (d = 201). We also report the results obtained by maintaining bounds consistency on the reified problem using meta-constraints as described by de Givry and Jeannin (2006), using the claire/Eclair C++ constraint programming solver (de Givry, Jeannin, Josset, Mattioli, Museux, & Savéant, 2002) developed by THALES (denoted by B-consistency).",
      "startOffset" : 18,
      "endOffset" : 2834
    }, {
      "referenceID" : 28,
      "context" : "The problem of searching for an occurrence of a gene signature in a genomic sequence is NP-complete for complex combinations of helix structures (Vialette, 2004).",
      "startOffset" : 145,
      "endOffset" : 161
    }, {
      "referenceID" : 29,
      "context" : "Because of the sheer domain size, and given that the complex pattern matching oriented cost functions do not have any specific property that could speedup filtering, BAC alone has been used for filtering these cost functions (Zytnicki et al., 2008).",
      "startOffset" : 225,
      "endOffset" : 248
    }, {
      "referenceID" : 29,
      "context" : "See the work of Zytnicki et al. (2008) for a complete description of the cost functions.",
      "startOffset" : 16,
      "endOffset" : 39
    }, {
      "referenceID" : 13,
      "context" : "Just to illustrate the absolute necessity of using bounds arc consistency in this problem, we compared bounds arc consistency enforcement with AC* (Larrosa, 2002) on sub-sequences of the genome of Escherichia coli, which is 4.",
      "startOffset" : 147,
      "endOffset" : 162
    }, {
      "referenceID" : 13,
      "context" : "Just to illustrate the absolute necessity of using bounds arc consistency in this problem, we compared bounds arc consistency enforcement with AC* (Larrosa, 2002) on sub-sequences of the genome of Escherichia coli, which is 4.9 million nucleotides long. Because of their identical space complexity and because they have not been defined nor implemented on non-binary cost functions (helix is a quaternarycost function), DAC, FDAC or EDAC have not been tested (see the work of Sànchez et al., 2008, however for an extension of FDAC to ternary cost functions). The results are displayed in Table 1. For different beginning sub-sequences of the complete sequence, we report the size of the sub-sequence in which the signature is searched for (10k is a sequence of 10,000 nucleotides), as well as the number of solutions found. We also show the number of backtracks and the time spent on a 3 GHz Intel Xeon with 2 GB. A “-” means the instance could not be solved due to memory reasons, and despite memory optimizations. BAC solved the complete sequence in less than 3 seconds. BAC is approximately 300, 000 (resp. 4, 400, 000) times faster than AC* for the 10k (resp. 50k) sub-sequence. More results on other genomes and ncRNA signatures can be found in the work of Zytnicki et al. (2008). The reason of the superiority of BAC over AC* is twofold.",
      "startOffset" : 148,
      "endOffset" : 1285
    }, {
      "referenceID" : 13,
      "context" : "This line of research has been recently explored by Lee and Leung (2009). Finally, another interesting extension of this work would be to better exploit the connection between BAC and bounds consistency by exploiting the idea of Virtual Arc Consistency introduced by Cooper et al.",
      "startOffset" : 52,
      "endOffset" : 73
    }, {
      "referenceID" : 5,
      "context" : "Finally, another interesting extension of this work would be to better exploit the connection between BAC and bounds consistency by exploiting the idea of Virtual Arc Consistency introduced by Cooper et al. (2008). The connection established by Virtual AC between crisp CNs and WCNs is much finer grained than in the reification approach considered by Petit et al.",
      "startOffset" : 193,
      "endOffset" : 214
    }, {
      "referenceID" : 5,
      "context" : "Finally, another interesting extension of this work would be to better exploit the connection between BAC and bounds consistency by exploiting the idea of Virtual Arc Consistency introduced by Cooper et al. (2008). The connection established by Virtual AC between crisp CNs and WCNs is much finer grained than in the reification approach considered by Petit et al. (2000) and could provide strong practical and theoretical results.",
      "startOffset" : 193,
      "endOffset" : 372
    } ],
    "year" : 2009,
    "abstractText" : "The Weighted Constraint Satisfaction Problem (WCSP) framework allows representing and solving problems involving both hard constraints and cost functions. It has been applied to various problems, including resource allocation, bioinformatics, scheduling, etc. To solve such problems, solvers usually rely on branch-and-bound algorithms equipped with local consistency filtering, mostly soft arc consistency. However, these techniques are not well suited to solve problems with very large domains. Motivated by the resolution of an RNA gene localization problem inside large genomic sequences, and in the spirit of bounds consistency for large domains in crisp CSPs, we introduce soft bounds arc consistency, a new weighted local consistency specifically designed for WCSP with very large domains. Compared to soft arc consistency, BAC provides significantly improved time and space asymptotic complexity. In this paper, we show how the semantics of cost functions can be exploited to further improve the time complexity of BAC. We also compare both in theory and in practice the efficiency of BAC on a WCSP with bounds consistency enforced on a crisp CSP using cost variables. On two different real problems modeled as WCSP, including our RNA gene localization problem, we observe that maintaining bounds arc consistency outperforms arc consistency and also improves over bounds consistency enforced on a constraint model with cost variables.",
    "creator" : "gnuplot 4.2 patchlevel 3 "
  }
}