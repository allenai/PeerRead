{
  "name" : "1511.03034.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Ruitong Huang", "Bing Xu", "Dale Schuurmans" ],
    "emails" : [ "szepesva}@ualberta.ca" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Deep Neural Network (DNN) models have shown its powerful learning capacity on many visual and speech classification problems. (Krizhevsky et al., 2012; Hinton et al., 2012) Part of reason is believed to be its high expressiveness from the deep architecture. Even though the misclassification rate is the main performance metric used to evaluate classifiers, robustness is also a highly desirable property. In particular, a classifier is expected to be ‘smooth’ so that a small perturbation of a datapoint does not change the prediction of the model. However, a recent intriguing discovery suggests that DNN models do not have such property of robustness. Szegedy et al. (2013) A well performed DNN model may misclassify most of the datapoints because of a human-indistinguishable perturbation on the original dataset. We call the perturbed dataset ‘adversarial examples’. It is even more curious that a same set of such adversarial examples are consistently misclassified by a large group of DNN models which are learned with different architectures and hyperparameters.\nFollowing the paper of (Szegedy et al., 2013), more and more attentions have been attracted toward such curious ‘adversary phenomenon’ in the deep learning community. (Szegedy et al., 2013; Goodfellow et al., 2014; Fawzi et al., 2015; Miyato et al., 2015; Nøkland, 2015; Tabacof and Valle, 2015). Goodfellow et al. (2014) suggests that the reason that cause the existence of adversarial examples may be its linearity of the model in high dimension. Further exploration is conduct by Tabacof and Valle (2015), showing that in an image domain, adversarial images inhabit large ”adversarial pockets” in the pixel space. Based on these observations, different ways of finding adversarial examples are proposed, among which the most relevant one is proposed in the paper of Goodfellow et al. (2014) where a linear approximation is used and thus it does not require to solve an optimization problem. In this paper, we further investigate this problem, and proposed another simple way of finding adversarial examples. Experimental results suggest that our method is more efficient in the sense that DNN has worse performance under same magnitude of perturbation.\nThe main contribution of this paper is learn a robust classifier that can still maintain a high classification performance. Goodfellow et al. (2014) suggest using a new objective function that is a combination of the original one and the one after the datapoints are perturbed to improve the robustness of the network. While in (Nøkland, 2015), as a specific case of the method in (Goodfellow et al., 2014), it is suggested to only use the objective function that is defined on the perturbed data. However, there is no theoretical analysis to justify that the learned classifier is indeed robust. In particular, both methods are proposed heuristically. Also, the perturbed datapoints that are used to evaluate the robustness of the learned classifier in the experiments are generated from the original network that is trained with clean data. It would have been more convincing if such perturbed datapoints were generated from the current classifier, if one is to argue that the current classifier is robust. Recently a theoretical exploration about the robustness of classifiers in (Fawzi et al., 2015) suggest that, as expected, there is a trade-off between the expressive power and the robustness. This paper can be consider as a further exploration about this trade-off in the engineering side. We formulate\nar X\niv :1\n51 1.\n03 03\n4v 1\n[ cs\n.L G\n] 1\n0 N\nov 2\n01 5\nthe learning procedure as a min-max problem so that it forces the DNN model to prepare for the worst situation. In particular, we allow an adversary to play different perturbation on each datapoint, then the learning procedure is to minimize the misclassification error on the intendedly perturbed dataset. We call such learning procedure as ‘learning with adversary’. It turns out that an efficient way of finding such adversarial examples is required, as an intermediate step, to solve such min-max problem, which goes back to the first part of our paper. We observe that our learning procedure turns out to be very similar to the one proposed in (Nøkland, 2015), while both works are conduct totally independently from different understandings of this problem.\nThe organization of the rest of this paper is as follows: we propose our method of find adversarial examples in Section 2. Section 3 is devoted to our main contribution: Learning with adversary. Finally we present our experimental results on MNIST and CIFAR-10 in Section 4."
    }, {
      "heading" : "1.1 NOTATIONS",
      "text" : "We denote the samples by Z = {(x1, y1), . . . , (xN , yN )}. Let K be the number of classes in our classification problem. The loss function that is used for training is denoted by `. Given a norm ‖ ·‖, let ‖ · ‖∗ denote its duel norm that ‖u‖∗ = max‖v‖≤1 < u, v >. Denote the network by N whose last layer is a softmax layer g(x) , α = (α1, . . . , αK)."
    }, {
      "heading" : "2 FINDING ADVERSARIAL EXAMPLES",
      "text" : "Consider a network that uses softmax as its last layer for classification, denoted by N . Given an sample (x, y) ∈ X × {1, 2, . . . , T} such that N (x) = y, where y is the true label for x. Our goal is to find a small perturbation r ∈ X so that N (X + r) 6= y. This problem is first investigated in (Szegedy et al., 2013) which proposes the following learning procedure: given x,\nmin r ‖r‖\ns.t. N (X + r) 6= N (X) Our simple method to find such perturbation r is based on the linear approximation of g(x), ĝ(x + r) = g(x) + Tr, where T = ∂g∂w |x is the derivative matrix. Consider the following question: for a fixed index j 6= J , what is the minimal r(j) satisfying N (x + r(j)) = j? Replacing g by its linear approximation ĝ, one of the necessary condition for such perturbation r is: Tjr(j) − TJr(j) ≤ αJ − αj , where Tj is the j-th row of T . Therefore, the norm of the optimal r∗(j) is greater than the following objective value:\nmin r(j) ‖r(j)‖ (1)\ns.t. Tjr(j) − TJr(j) ≤ αJ − αj . Optimal solution to this problem is provided in Proposition 2.1.\nProposition 2.1. It is straight forward that the optimal objective value is ‖r(j)‖ = αJ−αj ‖Tj−TJ‖∗ . The optimal r∗(j) for common norms are :\n1. If ‖ · ‖ is L2 norm, then r∗(j) = αJ−αj ‖Tj−TJ‖22 (Tj − TJ);\n2. If ‖ · ‖ is L∞ norm, then r∗(j) = αJ−αj ‖Tj−TJ‖1 sign(Tj − TJ);\n3. If ‖ · ‖ is L1 norm, then r∗(j) = c ‖Tj−TJ‖∞ eI where I satisfies |(Tj −TJ)I | = ‖Tj −TJ‖∞. Here VI is the I-th element of V .\nHowever, such r∗(j) is necessary but NOT sufficient to guarantee that argmaxi ĝ(x+ r(j))i = j. The following proposition shows that in order to have ĝ make wrong prediction, it is enough to use the minimal one among all the r∗(j)’s.\nProposition 2.2. Let I = argmini ‖r∗(i)‖. Then r ∗ I is the solution of the following problem:\nmin r ‖r‖\ns.t. argmax i (ĝ(X + r))i 6= J.\nPutting all things together, we have an algorithm to find adversarial examples, as shown in Algorithm 1.\nAlgorithm 1 Finding Adversarial Examples input (x, y); Network N ; output r\n1: Compute T by performing forward-backward propagation from the input layer to the softmax layer g(x) 2: for j = 1, 2, . . . , d do 3: Compute r∗(j) for Equation (1) 4: end for 5: Return r = r∗(I) where I = argmini ‖r ∗ (i)‖."
    }, {
      "heading" : "3 TOWARD THE ROBUSTNESS OF NEURAL NETWORK",
      "text" : "We enhance the robustness of the neural network by preparing the network for the worst cases, as follows.\nmin f ∑ i max ‖r(i)‖≤c `(g(xi + r (i)), yi). (2)\nThe hyperparameter c that control the magnitude of the perturbation needs to be tuned. Note that when `(g(xi + r(i)), yi) = I(maxj(g(xi+r(i))j) 6=yi), the objective function is the misclassification error under perturbations. Oftentimes, ` is a surrogate loss which is differentiable and smooth. Let Li(g) = max‖r(i)‖2≤c `(g(xi + r (i)), yi). Thus the problem is to find f∗ = argminf ∑ i Li(f).\nTo solve the problem (2) using SGD, one need to compute the derivative of Li with respect to f . The following proposition suggest a way of computing this derivative. Proposition 3.1. Given f : U × V → W differentiable almost everywhere, define L(v) = maxu∈U f(u, v). Assume that L is uniformly Lipschitz-continuous as a function of v, then the following results holds almost everywhere:\n∂L ∂v (v0) = ∂f ∂v (u∗, v0),\nwhere u∗ = argmaxu f(u, v0).\nProof. Note L is uniformly Lipschitz-continuous, therefore by Rademacher’s theorem, L is differentiable almost everywhere. For v0 where L is differentiable, the Fréchet subderivative of L is actually a singleton set of its derivative.\nConsider the function L̂(v) = f(u∗, v). Since f is differentiable, ∂f∂v (u ∗, v0) is the derivative of L̂ at point v0. Also L̂(v0) = L(v0). Thus, by Proposition 2 of (Neu and Szepesvári, 2012), ∂f∂v (u ∗, v0) also belongs to the subderivative of L. Therefore, ∂L\n∂v (v0) =\n∂f ∂v (u∗, v0).\nThe differentiability of f in Proposition 3.1 usually holds. The uniformly Lipschitz-continuous of neural networks was also discussed in the paper of Szegedy et al. (2013). It still remains to compute u∗ in Proposition 3.1. In particular given (xi, yi),\nmax ‖r(i)‖≤c\n`(g(xi + r (i)), yi). (3)\nWe postpone the solution for the above problem to the end of this section. Given that we can have an approximate solution for Equation (3), a simple SGD method to compute a local solution for Equation (2) is shown in Algorithm 2.\nAlgorithm 2 Learning with Adversary input (xi, yi) for 1 ≤ i ≤ N ; Initial f0; output f̂\n1: for t = 1, 2, . . . , T do 2: for (xi, yi) in the current batch do 3: Use forward-backward propagation to compute ∂α∂x 4: Compute r∗ as the optimal perturbation to x, using the proposed methods in Section 3.1 5: Create a pseudo-sample to be (x̂i = xi + c r ∗\n‖r∗‖2 , yi)\n6: end for 7: Update the network f̂ using forward-backward propagation on the pseudo-sample (x̂i, yi) for 1 ≤ i ≤ N 8: end for 9: Return f̂ .\nFor complex data, deeper neural networks are usually proposed, which can be interpreted as consist of two parts: the lower layers of the networks learns a representation for the datapoints, while the upper layers learns a classifier. The number of layers that should be categorized as in the representation network is not clear and varies a lot for different datasets. Given such a general network, denote the representation network as Nrep and the classification network as Ncal. We propose to perform the perturbation over the output ofNrep rather than the raw data. Thus the problem of learning with adversary can be fomulated as follows:\nmin Nrep,Ncal ∑ i max ‖r(i)‖≤c ` ( Ncal ( Nrep(xi) + r(i) ) , yi ) . (4)\nSimilarly, Equation (4) can be solved by the following SGD method, as shown in Algorithm 3.\nAlgorithm 3 Learning with Adversary input (xi, yi) for 1 ≤ i ≤ N ; Initial Ncal and Nrep; output f̂\n1: for t = 1, 2, . . . , T do 2: for (xi, yi) in the current batch do 3: Use forward propagation to compute the output of Nrep, x̃i 4: Take x̃i as the input for Ncal 5: Use forward-backward propagation to compute ∂α∂x̃i 6: Compute r∗ as the optimal perturbation to x̃i, using the proposed methods in Section 3.1 7: Create a pseudo-sample to be (ˆ̃xi = x̃i + c r ∗\n‖r∗‖ , yi)\n8: end for 9: Use forward propagation to compute the output of Ncal on (ˆ̃xi, yi) for 1 ≤ i ≤ N\n10: Use backward propagation to update both Ncal and Nrep for 1 ≤ i ≤ N 11: end for 12: Return Ncal and Nrep"
    }, {
      "heading" : "3.1 COMPUTING THE PERTURBATION",
      "text" : "We propose two method based on two different principles. Our method, similar to that of (Goodfellow et al., 2014), does not require to solve an optimization problem. Experimental results show that our method, compared to the method proposed in (Goodfellow et al., 2014), is more efficient in that under the same magnitude of perturbation, the performance of the network is worse on our adversarial examples."
    }, {
      "heading" : "3.1.1 LIKELIHOOD BASED LOSS",
      "text" : "Assume the loss function `(x, y) = h(αy) where h is a non-negative decreasing function. One of the typical examples would be the logistic regression model. In fact, most of the network models use a softmax layer as the last layer and a cross-entropy objective function. All these networks can fit into this type of loss function. Recall that we would like to find\nr∗ = arg max ‖r(i)‖≤c\nh ( g(xi + r (i))yi ) ,\nwhere xi could be the raw data or the output of Nrep. Since h is decreasing, r∗ = argmin‖r(i)‖≤c g(xi + r (i))yi .\nThis problem can be still difficult in general. We propose to compute a approximate solution based on the linear approximation of the function g. Replacing g(xi + r(i))yi by its linear approximation g̃(xi + r (i))yi , i.e. g(xi + r (i))yi u g̃(xi + r(i))yi = g(xi)yi+ < Tyi , r(i) >, r∗ can be solved for g̃(xi + r (i))yi as r\n∗ = {r : ‖r‖ ≤ c; < Tyi , r(i) >= c‖Tyi‖∗}. The optimal r∗ for common norms are :\n1. If ‖ · ‖ is L2 norm, then r∗(j) = c Tyi ‖Tyi‖2 ; 2. If ‖ · ‖ is L∞ norm, then r∗(j) = c sign(Tyi);\n3. If ‖ · ‖ is L1 norm, then r∗(j) = c eI where I satisfies |Tyi | = ‖Tyi‖∞. Here VI is the I-th element of V .\nNote that the second item here is exactly the method suggested in (Goodfellow et al., 2014)."
    }, {
      "heading" : "3.1.2 MISCLASSIFICATION BASED LOSS",
      "text" : "In the case that the loss function ` is a surrogate loss for the misclassification rate, in Equation (3) it is reasonable to still use the misclassification rate as the loss function `. Thus Equation (3) is to find a perturbation r : ‖r‖ ≤ c that make N misclassify xi. In practice, in order for N to achieve good approximation, c is pick to be a small value, thus may not be large enough to force the misclassification of N . One intuitive way is to have r the same direction as the one that is found in Section 2, since such direction is arguable to be an ‘efficient’ direction for the perturbation. Therefore, r∗ = c r∗I/‖r∗I‖, where r∗I is the output of Algorithm 1."
    }, {
      "heading" : "4 EXPERIMENTAL RESULTS",
      "text" : "We use MNIST (LeCun et al., 1998b) and CIFAR-10 to test our methods of finding adversarial examples and training robust netowrks.\nMNIST dataset contains grey scale handwrite images in size of 28x28. We random choose 50,000 images for training and 10,000 for testing. We normalize each pixel into range [0, 1] by dividing 256.\n”Introduction to CIFAR-10”: Data distribution"
    }, {
      "heading" : "4.1 FINDING ADVERSARIAL EXAMPLES",
      "text" : "We test different perturbation methods on MNIST including: 1. Perturbation based on α using `2 norm as shown in Section 2 (Adv Alpha); 2. Perturbation based on Loss function using loss function using `2 norm as shown in Section 3.1 (Adv Loss); 3. Perturbation based on Loss function using loss function using `∞ norm as shown in Section 3.1 (Adv Loss Sign);. In particular, a standard Lenet is trained on MNIST, with the training and validation accuracy being 100% and 99.1%. Based\non the learned network, different validation sets are then generated by perturbing the original data with different perturbation methods. The magnitudes of the perturbations range from 0.0 to 4.0 in `2 norm. The classification accuracies on differently perturbed datasets are reported in Figure 1. As the\ngrowing of the magnitude of the perturbation, the network’s performance decreases. Experimental results suggest that Adv Alpha is consistently, but slightly, more efficient than Adv Loss, and these two method are significantly more efficient than Adv Loss Sign."
    }, {
      "heading" : "Is it reasonable to use `2 norm to measure the magnitude?",
      "text" : "One may concern that if the following case may happen: a small perturbation in `2 norm has most of its weight on some specific position, and thus change the picture distinguishably. One of this example is shown as in Figure ******. However, the above example is artificially created, and we don’t observe such phenomenon in our experiments.\nDrawback of using α to find perturbations Note that the difference in perturbation efficiency between using α and using the loss function is small. On the other hand, to compute the perturbation using α, one need to compute ∂α∂x , which is actually d times computation complexity as that of the method using the loss function and thus only need to compute ∂`∂x . Here d is the number of classes."
    }, {
      "heading" : "4.2 LEARNING WITH ADVERSARY",
      "text" : "We test our method on both MNIST and CIFAR-10.\nExperiments on MNIST: We first test different learning methods on a 2-hidden-layers neural network that has 1000 hidden nodes for each hidden layer: 1. Normal back-forward propagation training (Normal); 2. Normal back-forward propagation training with Dropout (Dropout); 3. The method in (Goodfellow et al., 2014) (Goodfellow’s method); 4. Learning with adversary at raw data (LWA); 5. Learning with adversary at representation layer (LWA Rep). The robustness of each classifier is measured on various adversarial sets. A same type of adversarial set for different learned classifiers are generated based on its targeted classifier. We generate 3 types of adversarial datasets for the above 5 classifiers corresponding to Adv Alpha, Adv Loss, and Adv Loss Sign. Moreover, we also evaluate the performances of these 5 classifiers on a fixed adversarial set which is generated based on the ‘Normal’ network using Adv Loss. Lastly, we also report the original validation accuracies of different networks. All the results are tested under perturbations of `2 norm being 1.5.\nThe normal method can not afford any perturbation on the validation set, showing that it is not robust at all. By training with the dropout technique, both performance and robustness of the neural network are improved, but its robustness is still weak. Especially, for the adversarial set that is generated by Adv Loss, its classification accuracy is only 13.5%. Goodfellow’s method improves\nthe network’s robust greatly, compared to the previous methods. The best performance and the most robustness are both achieved by LWA. In particular, on the adversarial set that is generated by our methods (Adv Loss and Adv Alpha), the performance is improved from 71.3% to 86.0%, and from 70.1% to 85.7%. The result of LWA Rep is also reported for comparison. Overall, it achieves fair comparable performance to Goodfellow’s method (Goodfellow et al., 2014).\nWe also evaluate these learning methods on LeNet (LeCun et al., 1998a), which is more complicated including convolution layers. Its learning curve is reported in Figure 2. It is interesting that we do not observe the trade-off between robustness and its performance. This phenomenon also happens to the 2-hidden-layers neural network. The final result is summarized in Table 2, which shows its\ngreat robustness. Recall that from the results in\nExperiments on CIFAR-10:\nSTAY TUNED!!!"
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "We thank Naiyan Wang and Ian Goodfellow for meaningful discussions. This work was supported by the Alberta Innovates Technology Futures and NSERC."
    } ],
    "references" : [ {
      "title" : "Analysis of classifiers’ robustness to adversarial perturbations",
      "author" : [ "Alhussein Fawzi", "Omar Fawzi", "Pascal Frossard" ],
      "venue" : "arXiv preprint arXiv:1502.02590,",
      "citeRegEx" : "Fawzi et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Fawzi et al\\.",
      "year" : 2015
    }, {
      "title" : "Explaining and harnessing adversarial examples",
      "author" : [ "Ian J Goodfellow", "Jonathon Shlens", "Christian Szegedy" ],
      "venue" : "arXiv preprint arXiv:1412.6572,",
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups",
      "author" : [ "Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath" ],
      "venue" : "Signal Processing Magazine, IEEE,",
      "citeRegEx" : "Hinton et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2012
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton" ],
      "venue" : null,
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Yann LeCun", "Léon Bottou", "Yoshua Bengio", "Patrick Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "The mnist database of handwritten digits",
      "author" : [ "Yann LeCun", "Corinna Cortes", "Christopher JC Burges" ],
      "venue" : null,
      "citeRegEx" : "LeCun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "Distributional smoothing by virtual adversarial examples",
      "author" : [ "Takeru Miyato", "Shin-ichi Maeda", "Masanori Koyama", "Ken Nakae", "Shin Ishii" ],
      "venue" : "arXiv preprint arXiv:1507.00677,",
      "citeRegEx" : "Miyato et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Miyato et al\\.",
      "year" : 2015
    }, {
      "title" : "Apprenticeship learning using inverse reinforcement learning and gradient methods",
      "author" : [ "Gergely Neu", "Csaba Szepesvári" ],
      "venue" : "arXiv preprint arXiv:1206.5264,",
      "citeRegEx" : "Neu and Szepesvári.,? \\Q2012\\E",
      "shortCiteRegEx" : "Neu and Szepesvári.",
      "year" : 2012
    }, {
      "title" : "Improving back-propagation by adding an adversarial gradient",
      "author" : [ "Arild Nøkland" ],
      "venue" : "arXiv preprint arXiv:1510.04189,",
      "citeRegEx" : "Nøkland.,? \\Q2015\\E",
      "shortCiteRegEx" : "Nøkland.",
      "year" : 2015
    }, {
      "title" : "Intriguing properties of neural networks",
      "author" : [ "Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus" ],
      "venue" : "arXiv preprint arXiv:1312.6199,",
      "citeRegEx" : "Szegedy et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2013
    }, {
      "title" : "Exploring the space of adversarial images",
      "author" : [ "Pedro Tabacof", "Eduardo Valle" ],
      "venue" : "arXiv preprint arXiv:1510.05328,",
      "citeRegEx" : "Tabacof and Valle.,? \\Q2015\\E",
      "shortCiteRegEx" : "Tabacof and Valle.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "(Krizhevsky et al., 2012; Hinton et al., 2012) Part of reason is believed to be its high expressiveness from the deep architecture.",
      "startOffset" : 0,
      "endOffset" : 46
    }, {
      "referenceID" : 2,
      "context" : "(Krizhevsky et al., 2012; Hinton et al., 2012) Part of reason is believed to be its high expressiveness from the deep architecture.",
      "startOffset" : 0,
      "endOffset" : 46
    }, {
      "referenceID" : 9,
      "context" : "Following the paper of (Szegedy et al., 2013), more and more attentions have been attracted toward such curious ‘adversary phenomenon’ in the deep learning community.",
      "startOffset" : 23,
      "endOffset" : 45
    }, {
      "referenceID" : 9,
      "context" : "(Szegedy et al., 2013; Goodfellow et al., 2014; Fawzi et al., 2015; Miyato et al., 2015; Nøkland, 2015; Tabacof and Valle, 2015).",
      "startOffset" : 0,
      "endOffset" : 128
    }, {
      "referenceID" : 1,
      "context" : "(Szegedy et al., 2013; Goodfellow et al., 2014; Fawzi et al., 2015; Miyato et al., 2015; Nøkland, 2015; Tabacof and Valle, 2015).",
      "startOffset" : 0,
      "endOffset" : 128
    }, {
      "referenceID" : 0,
      "context" : "(Szegedy et al., 2013; Goodfellow et al., 2014; Fawzi et al., 2015; Miyato et al., 2015; Nøkland, 2015; Tabacof and Valle, 2015).",
      "startOffset" : 0,
      "endOffset" : 128
    }, {
      "referenceID" : 6,
      "context" : "(Szegedy et al., 2013; Goodfellow et al., 2014; Fawzi et al., 2015; Miyato et al., 2015; Nøkland, 2015; Tabacof and Valle, 2015).",
      "startOffset" : 0,
      "endOffset" : 128
    }, {
      "referenceID" : 8,
      "context" : "(Szegedy et al., 2013; Goodfellow et al., 2014; Fawzi et al., 2015; Miyato et al., 2015; Nøkland, 2015; Tabacof and Valle, 2015).",
      "startOffset" : 0,
      "endOffset" : 128
    }, {
      "referenceID" : 10,
      "context" : "(Szegedy et al., 2013; Goodfellow et al., 2014; Fawzi et al., 2015; Miyato et al., 2015; Nøkland, 2015; Tabacof and Valle, 2015).",
      "startOffset" : 0,
      "endOffset" : 128
    }, {
      "referenceID" : 8,
      "context" : "While in (Nøkland, 2015), as a specific case of the method in (Goodfellow et al.",
      "startOffset" : 9,
      "endOffset" : 24
    }, {
      "referenceID" : 1,
      "context" : "While in (Nøkland, 2015), as a specific case of the method in (Goodfellow et al., 2014), it is suggested to only use the objective function that is defined on the perturbed data.",
      "startOffset" : 62,
      "endOffset" : 87
    }, {
      "referenceID" : 0,
      "context" : "Recently a theoretical exploration about the robustness of classifiers in (Fawzi et al., 2015) suggest that, as expected, there is a trade-off between the expressive power and the robustness.",
      "startOffset" : 74,
      "endOffset" : 94
    }, {
      "referenceID" : 0,
      "context" : ", 2012; Hinton et al., 2012) Part of reason is believed to be its high expressiveness from the deep architecture. Even though the misclassification rate is the main performance metric used to evaluate classifiers, robustness is also a highly desirable property. In particular, a classifier is expected to be ‘smooth’ so that a small perturbation of a datapoint does not change the prediction of the model. However, a recent intriguing discovery suggests that DNN models do not have such property of robustness. Szegedy et al. (2013) A well performed DNN model may misclassify most of the datapoints because of a human-indistinguishable perturbation on the original dataset.",
      "startOffset" : 8,
      "endOffset" : 533
    }, {
      "referenceID" : 0,
      "context" : ", 2014; Fawzi et al., 2015; Miyato et al., 2015; Nøkland, 2015; Tabacof and Valle, 2015). Goodfellow et al. (2014) suggests that the reason that cause the existence of adversarial examples may be its linearity of the model in high dimension.",
      "startOffset" : 8,
      "endOffset" : 115
    }, {
      "referenceID" : 0,
      "context" : ", 2014; Fawzi et al., 2015; Miyato et al., 2015; Nøkland, 2015; Tabacof and Valle, 2015). Goodfellow et al. (2014) suggests that the reason that cause the existence of adversarial examples may be its linearity of the model in high dimension. Further exploration is conduct by Tabacof and Valle (2015), showing that in an image domain, adversarial images inhabit large ”adversarial pockets” in the pixel space.",
      "startOffset" : 8,
      "endOffset" : 301
    }, {
      "referenceID" : 0,
      "context" : ", 2014; Fawzi et al., 2015; Miyato et al., 2015; Nøkland, 2015; Tabacof and Valle, 2015). Goodfellow et al. (2014) suggests that the reason that cause the existence of adversarial examples may be its linearity of the model in high dimension. Further exploration is conduct by Tabacof and Valle (2015), showing that in an image domain, adversarial images inhabit large ”adversarial pockets” in the pixel space. Based on these observations, different ways of finding adversarial examples are proposed, among which the most relevant one is proposed in the paper of Goodfellow et al. (2014) where a linear approximation is used and thus it does not require to solve an optimization problem.",
      "startOffset" : 8,
      "endOffset" : 587
    }, {
      "referenceID" : 0,
      "context" : ", 2014; Fawzi et al., 2015; Miyato et al., 2015; Nøkland, 2015; Tabacof and Valle, 2015). Goodfellow et al. (2014) suggests that the reason that cause the existence of adversarial examples may be its linearity of the model in high dimension. Further exploration is conduct by Tabacof and Valle (2015), showing that in an image domain, adversarial images inhabit large ”adversarial pockets” in the pixel space. Based on these observations, different ways of finding adversarial examples are proposed, among which the most relevant one is proposed in the paper of Goodfellow et al. (2014) where a linear approximation is used and thus it does not require to solve an optimization problem. In this paper, we further investigate this problem, and proposed another simple way of finding adversarial examples. Experimental results suggest that our method is more efficient in the sense that DNN has worse performance under same magnitude of perturbation. The main contribution of this paper is learn a robust classifier that can still maintain a high classification performance. Goodfellow et al. (2014) suggest using a new objective function that is a combination of the original one and the one after the datapoints are perturbed to improve the robustness of the network.",
      "startOffset" : 8,
      "endOffset" : 1098
    }, {
      "referenceID" : 8,
      "context" : "We observe that our learning procedure turns out to be very similar to the one proposed in (Nøkland, 2015), while both works are conduct totally independently from different understandings of this problem.",
      "startOffset" : 91,
      "endOffset" : 106
    }, {
      "referenceID" : 9,
      "context" : "This problem is first investigated in (Szegedy et al., 2013) which proposes the following learning procedure: given x,",
      "startOffset" : 38,
      "endOffset" : 60
    }, {
      "referenceID" : 7,
      "context" : "Thus, by Proposition 2 of (Neu and Szepesvári, 2012), ∂f ∂v (u ∗, v0) also belongs to the subderivative of L.",
      "startOffset" : 26,
      "endOffset" : 52
    }, {
      "referenceID" : 9,
      "context" : "The uniformly Lipschitz-continuous of neural networks was also discussed in the paper of Szegedy et al. (2013). It still remains to compute u∗ in Proposition 3.",
      "startOffset" : 89,
      "endOffset" : 111
    }, {
      "referenceID" : 1,
      "context" : "Our method, similar to that of (Goodfellow et al., 2014), does not require to solve an optimization problem.",
      "startOffset" : 31,
      "endOffset" : 56
    }, {
      "referenceID" : 1,
      "context" : "Experimental results show that our method, compared to the method proposed in (Goodfellow et al., 2014), is more efficient in that under the same magnitude of perturbation, the performance of the network is worse on our adversarial examples.",
      "startOffset" : 78,
      "endOffset" : 103
    }, {
      "referenceID" : 1,
      "context" : "Note that the second item here is exactly the method suggested in (Goodfellow et al., 2014).",
      "startOffset" : 66,
      "endOffset" : 91
    }, {
      "referenceID" : 1,
      "context" : "The method in (Goodfellow et al., 2014) (Goodfellow’s method); 4.",
      "startOffset" : 14,
      "endOffset" : 39
    }, {
      "referenceID" : 1,
      "context" : "Overall, it achieves fair comparable performance to Goodfellow’s method (Goodfellow et al., 2014).",
      "startOffset" : 72,
      "endOffset" : 97
    } ],
    "year" : 2017,
    "abstractText" : "In this paper, we propose a method, learning with adversary, to learn a robust network. Our method takes finding adversarial examples as its mediate step. A new and simple way of finding adversarial examples are presented and experimentally shown to be more ‘efficient’. Lastly, experimental results shows our learning method greatly improves the robustness of the learned network.",
    "creator" : "LaTeX with hyperref package"
  }
}