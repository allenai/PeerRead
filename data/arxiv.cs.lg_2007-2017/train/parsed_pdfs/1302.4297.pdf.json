{
  "name" : "1302.4297.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Feature Multi-Selection among Subjective Features",
    "authors" : [ "Sivan Sabato", "Adam Kalai" ],
    "emails" : [ "SIVAN.SABATO@MICROSOFT.COM", "ADAM.KALAI@MICROSOFT.COM" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "In this paper we consider prediction with subjective, vague, or noisy attributes (which are also termed ‘features’ throughout this paper). Such attributes can sometimes be useful for prediction, because they account for an important part of the signal that cannot be otherwise captured. In a crowdsourcing setting, the “wisdom of crowds” suggests that including multiple assessments of the same feature by different people may be useful. Henceforth, we refer to assessments of features as judgments. This paper introduces the problem of selecting, from a set of candidate features, which ones to use for prediction, and how many judgments to acquire for each, for a given budget limiting the total number of judgments. We give theoretically justified algorithms for this problem, and report crowdsourced experimental results, in which judgments of highly subjective features (even culturally fraught ones such as attractive) are helpful for prediction.\nAs a toy example, consider the problem of estimating the number of jelly beans in a jar based on an image of the jar. A linear regressor with multiple judgments of features might have the form,\nŷ =0.95(est. number of beans)/5 − 50(round jar)/2+ 100(monochromatic)/1 + 30(beautiful)/3.\nHere, for binary attributes, a/ra ∈ [0, 1] denotes the fraction of positive judgments out of ra judgments of attribute a. For real-valued attributes, a/ra denotes the mean of ra judgments. The shape, number of colors, and attractiveness of the jar each help correct biases in the estimated number of beans, averaged across five people. Our goal is to choose a regressor that, as accurately as possible, estimates the labels (i.e., jelly bean counts) on future objects (i.e., jars) drawn from the same distribution, while staying within a budget of feature judgment resources per evaluated object at test time. In the example above, notice that even though the monochromatic coefficient is greater than the beautiful coefficient, fewer monochromatic judgments are used, because counting the number of colors is more objective, and hence further judgments are less valuable. While this example is contrived, similar phenomena are observed in the output of our algorithms (see 2).\nWe refer to the problem of selecting the number of repetitions, ra, of each attribute, as the feature multi-selection problem, because it generalizes the feature selection problem of choosing a subset of features, i.e., ra ∈ {0, 1}, to choosing a multiset of features, i.e., ra ∈ N. Since the feature selection problem is well known to be NP-hard (Natarajan, 1995), our problem is also NP-hard in the general case. (For a formal reduction, one simply considers the “objective” case where all judgments of the same featureobject pair are identical.) Nonetheless, several successful approaches have been proposed for feature selection. The algorithms that we propose generalize two of these approaches to the problem of feature multi-selection.\nOur algorithms are theoretically motivated, and tested on synthetic and real-world data. The real world data are photos extracted from the publicly available Photographic Height/Weight Chart1, where people post pictures of themselves announcing their own height and weight.\nAs a more general motivation, consider a scientist who would like to use crowdsourcing as an alternative to themselves estimating a value for each of a large data set of objects. Say the scientist gathers multiple judgments of\n1http://www.cockeyed.com/photos/bodies/heightweight.html\nar X\niv :1\n30 2.\n42 97\nv3 [\ncs .L\nG ]\n1 4\nM ay\n2 01\n3\na number of binary or real-valued attributes for each object, and uses linear regression to predict the value of interest. In some cases, crowdsourcing is a natural source of judgments, as a great number of them may be acquired on demand, rapidly, and at very low cost. We assume the scientist has access to the following information:\n• A labeled set of objects (o, y) ∈ O × Y (with no judgments), where O is a set of objects and Y ⊆ R is a set of ground-truth labels drawn independently from a distribution D.\n• A crowd, which is a large pool of workers.\n• A possibly large set of candidate attributes A. For any attribute a ∈ A and object o ∈ O, the judgment of a random worker from the crowd may be queried at a cost.\n• A budget B, limiting the number of attribute judgments to be used when evaluating the regressor on a new unseen object.\nOur approach is as follows:\n1. Collect k ≥ 2 judgments for each candidate attribute in A, for each object in the labeled set.\n2. Based on this data and the budget, decide how many judgments of each attribute to use in the regressor.\n3. Collect additional judgments (as needed) on the labeled set so that each attribute has the number of judgments specified in the previous step.\n4. Find a linear predictor based on the average judgment of each feature.2\nStep 4 can be accomplished by simple least-squares regression. The goal in Step 2 (feature multi-selection) is to decide on a number of judgments per attribute that will hopefully yield the smallest squared error after Step 4.\nInterestingly, even given as few as k = 2 judgments per attribute, one can project an estimate of the squared error with more than k judgments of some features. We prove that these projections are accurate, for any fixed k ≥ 2, as the number of labeled objects increases. Our algorithms perform a greedy strategy for feature multi-selection, to attempt to minimize the projected loss. This greedy strategy can be seen as a generalization of the Forward Regression approach for standard feature selection (see e.g. Miller, 2002). The first algorithm operates under the assumption that different attributes are uncorrelated. In this case the\n2We focus on mean averaging, leaving to future work other aggregation statistics such as the median.\nprojection simplifies to a simple scoring rule, which incorporates attribute-label correlations as well as a natural notion of inter-rater reliability for each attribute. In this case, greedy selection is also provably optimal. While attributes are highly correlated in practice, the algorithm performs well in our experiments, possibly because Step 4 corrects for a small number of poor choices during feature multiselection. The second algorithm attempts to optimize the projection without any assumptions on the nature of correlations between features.\nWhile crowdsourcing is one motivation, the algorithms would be applicable to other settings such as learning from noisy sensor inputs, where one may place multiple sensors measuring each quantity, or social science experiments, where one may have multiple research assistants (rather than a crowd) judging each attribute.\nThe main contributions of this paper are: (a) introducing the feature multi-selection problem, (b) giving theoretically justified feature multi-selection algorithms, and (c) presenting experimental results, showing that feature multiselection can yield more accurate regressors, with different numbers of judgments for different attributes.\nRelated Work Related work spans a number of fields, including Statistics, Machine Learning, Crowdsourcing, and measurement in the social sciences. A number of researchers have studied attribute-efficient prediction (also called budgeted learning) assuming, as we do, that there is a cost to evaluating attributes and one would like to evaluate as few as possible (see, for instance, the recent work by Cesa-Bianchi et al. (2011) and references therein). In that line of work, each attribute is judged at most once. The errors-in-variables approach (e.g., Cheng & Van Ness, 1999) in statistics estimates the ‘true’ regression coefficients using noisy feature measurements. This approach is less suitable in our setting, since our final goal is to predict from noisy measurements.\nA wide variety of techniques have been studied to combine estimates of experts or the crowd of a single quantity of interest (see, e.g. Dawid & Skene, 1979; Smyth et al., 1994; Welinder et al., 2010), like estimating the number of jelly beans in a jar from a number of guesses.\nTwo recent works on crowdsourcing are very relevant. Patterson & Hays (2012) crowdsourced the mean of 3 judgments of each of 102 binary attributes on over 14,000 images, yielding over 4 million judgments. Some of their attributes are subjective, e.g., soothing. We employ their crowdsourcing protocol to label our binary attributes. Isola et al. (2011) study subjective and objective features for the task of estimating how memorable an image is, by taking the mean of 10 judgments per attribute for each im-\nage. They perform greedy feature selection over these attributes to find the best compact set of attributes for predicting memorability. The key difference between their algorithm and ours is that theirs does not choose how many judgments to average. Since that quantity is fixed for each attribute, their setting falls under the more standard feature selection umbrella. In our experiments we compare this approach to our algorithms.\nFinally, in the social sciences, a wide array of techniques have been developed for assessing inter-rater reliability of attributes, with the most popular perhaps being the α coefficient (Cronbach, 1951). A principal use of such measures is determining, by some threshold, which features may be used in content analysis. For an overview of reliability theory, see (Krippendorff, 2012)."
    }, {
      "heading" : "2. Preliminary assumptions and definitions",
      "text" : "Let there be d candidate attributes called A = [d] = {1, 2, . . . , d}. We assume that, for any object o and attribute a, there is a distribution over judgments P[X[a] | O = o], and we assume that the judgments of attributeobject pairs are conditionally independent given the sets of attributes and objects. This represents an idealized setting in which a new random crowd worker is selected for each attribute-object judgment (In our experiments, we limit the total amount of work that any one worker may perform). We assume a distribution D over labeled objects, where labels are real numbers. We denote by DO the marginal distribution over objects drawn according to D. We let P[X[a]] = PO∼DO [X[a] | O]]. Labels y are assumed to be real valued. As is standard, we assume one “true” label yi for each object oi.\nFor notational ease, we assume that in the feature multiselection phase, exactly k ≥ 2 judgments for each feature are collected. Our analysis trivially generalizes to the setting in which different attributes are judged different numbers of times. Finally, each attribute a is assumed to have an expected value of E[X[a]] = 0, where the expectation is taken across objects and judgments of a. This is done for ease of presentation, so that we do not have to track the mean vectors as well as the variance. When discussing implementation details, we describe how to remove this assumption in practice without loss of generality.\nVectors will be boldface, e.g., x = (x[1], . . . , x[d]), random variables will be capitalized, e.g.,X , and matrices will be in black-board font, e.g., X. The i’th standard unit vector is denoted by ei.\nLet r ∈ Nd represent the number of judgments for each feature, so that attribute a is judged r[a] times, and we rep-\nresent the object’s judgments by x, defined as:\nx = ( 〈x[1](j)〉r[1]j=1, . . . , 〈x[d](j)〉 r[d] j=1 ) ,\nwhere x[a](j) is the jth judgment of attribute a in x, and 〈x[a](j)〉r[a]j=1 is a vector with x[a](j) in coordinate j. We say that r is the repeat vector of x. We denote the set of all possible representations with repeat vector r by R[r].\nWe denote by Dr the distribution which draws (X, Y ) ∈ R[r] × R by first drawing a labeled object (O, Y ) from D, and then drawing a random representation X ∈ R[r] for this object. We denote by D∞ the distribution that draws (X, Y ) where X ∈ Rd by first drawing (O, Y ) fromD and then setting X[a] = E[X[a] | O]. We denote the expectation over Dr by Er = E(X,Y )∼Dr . For D∞ we denote E∞ = E(X,Y )∼D∞ .\nFor k ≥ 2, let k = (k, k, . . . , k) ∈ Nd be the repeat vector used in the first training phase. The feature multi-selection algorithm receives as input a labeled training set S = ((x1, y1), . . . , (xm, ym)) where xi ∈ Rkd and yi ∈ R, drawn from Dk. This sample is generated by first drawing a set of labeled objects ((o1, y1), . . . , (om, ym)) i.i.d. from D, and then drawing a random representation xi for object oi. The algorithm further receives as input a budget B ∈ N, which specifies the total number of feature judgments allowed for each unlabeled object at test (i.e., prediction) time. The output of the algorithm is a new vector of repeats r ∈ RB , where,\nRB ≡ { r ∈ Nd | ∑\na∈A r[a] ≤ B\n} .\nLet o be an object with a true label y, and let ŷ be a prediction of the label of o. The squared loss for this prediction is `(y, ŷ) = (y − ŷ)2. Given a function f : Z → R for some domain Z, and a distribution D over Z ×R, we denote the average loss of f on D by\n`(f,D) ≡ E(Z,Y )∼D[`(f(Z), Y )].\nThe final goal of our procedure is to find a predictor with a low expected loss on labeled objects drawn from D. This predictor must use only B feature judgments for each object, as determined by the test repeat vector r. We consider linear predictors w ∈ Rd that operate on the vector of average judgments of x ∈ R[r], defined as follows:\nx̄[a] ≡\n{ 1 r[a] ∑r[a] j=1 x[a](j) if r[a] > 0,\n0 if r[a] = 0.\nFor an input representation x, the predictor w predicts the label 〈w, x̄〉. For vector v ∈ Rd, we denote by Diag(v) ∈ Rd×d the diagonal matrix with v[a] in the ath position.\nFor a vector r ∈ Nd and a matrix S ∈ Rd×d, we denote by subr(S) the submatrix of S resulting from deleting all rows and columns a such that r[a] = 0. For a vector, subr(u) omits entries a such that r[a] = 0. Here subr(u) ∈ Rd ′ and subr(S) ∈ Rd ′×d′ , where d′ is the support size of r. We denote the pseudo-inverse of a matrix A ∈ Rn×n (see e.g. Ben-Israel & Greville, 2003) by A+."
    }, {
      "heading" : "3. Feature Multi-Selection Algorithms",
      "text" : "The input to a feature multi-selection algorithm is a budget B andm labeled examples in which each attribute has been judged k times, and the output is a repeat vector r ∈ RB . Our ultimate goal is to find r and a predictor w ∈ Rd such that `(w, Dr) is minimal. We now give intuition about the derivation of the algorithms, but their formal definition is given in Alg. 1.\nDefine the loss of a repeat vector to be `(r) ≡ minw∈Rd `(w, Dr). The goal is to minimize `(r) over r ∈ RB . We give two forward-selection algorithms, both of which begin with r = (0, . . . , 0) and greedily increment r[a] for a that most decreases an estimate of `(r). The key question is how does one estimate this projected loss `(r) since the number of judgments can exceed k. We simplify notation by first considering only r which are positive, i.e., r[a] ≥ 1 for each a. We will shortly explain how to handle r[a] = 0. Define\nb = E[XY ], and Σr = Er[X̄T X̄].\nWe call b[a] the correlation of a with the label. Note that b = Ek[X̄Y ], since linearity of expectation implies that b does not depend on k. Straightforward calculations show that, for any positive repeat vector r, If Σr is non-singular,3\n`(r) = min w\nEr [ (wT X̄− Y )2 ] = Er[Y 2]− bTΣ−1r b.\nSince E[Y 2] does not depend on r, minimizing `(r) is equivalent to maximizing bTΣ−1r b (for positive r and nonsingular Σr)."
    }, {
      "heading" : "3.1. A Scoring Algorithm",
      "text" : "The first algorithm that we propose is derived from the zero-correlation assumption, that E[X[a]X[a′]] = 0 for a 6= a′, or equivalently that the covariance matrix is diagonal. Perhaps the simplest approach to standard feature selection is to score each feature independently, based on its normalized empirical correlation with the label, and to select the B top-scoring features. If features are uncorrelated and the training sample is sufficiently large, then this efficient approach finds an optimal set of features. The feature multi-selection scoring algorithm that we propose\n3For singular Σr, the pseudo-inverse Σ+r replaces Σ−1r .\nhenceforth is optimal under similar assumptions, however it is complicated by the fact that we may include multiple repetitions of each feature. Under the zero-correlation assumption, Σr is diagonal, and its ath element, for r[a] > 0, can be expanded as\nEr[(X̄[a])2] = σ2[a] + v[a]\nr[a] , where\nv[a] ≡ EO∼DO [Var[X[a] | O]] and σ2[a] ≡ E∞ [ (X[a])2 ] .\nWe refer to v[a] as the internal variance as it measures the “inter-rater reliability” of a, and we call σ2[a] the external variance as it is the inherent variance between examples. Hence for a diagonal Σr, simple manipulation gives,\nE[Y 2]− `(r) = ∑\na:r[a]>0\n(b[a])2\nσ2[a] + v[a]r[a] . (1)\nTherefore, when Σr is diagonal, minimizing the projected loss is equivalent to maximizing the RHS above, a sum of independent terms that depend on the correlation and on the internal and external variance of each attribute, all of which can be estimated just once, for all possible repeat vectors. As one expects, greater correlation indicates a better feature, while a greater external variance indicates a worse feature. A larger internal variance indicates that more repeats are needed to achieve prediction quality.\nTo estimate Eq. (1) we estimate each of the components on the RHS. Unbiased estimation of b is straightforward, and unbiased estimation of v is also possible for k ≥ 2 samples per object, though importantly one should use the unbiased variance estimator,\nv̂[a] = 1\nm ∑ i VarEst(xi[a](1), . . . , xi[a](j)), (2)\nVarEst(α1, . . . , αn) ≡ 1 n− 1 ∑ j∈[n] (αj − 1 n ∑ j′∈[n] αj′) 2.\nUsing these estimates of v, we estimate the external variance using the equality σ2[a] = Ek [ (X̄[a])2 ] − v[a]k . A slight complication arises here, as this estimate might be negative for small samples, so we round it up to 0 when this happens. Another issue might seem to arise when the denominator of one of the summands in Eq. (1) is zero, however note that this can only occur if both the internal and the external variance are zero, which implies that the feature is constantly zero, thus zeroing its correlation as well. The same holds for the estimated ratio. In such cases we treat the ratio as equal to 0."
    }, {
      "heading" : "3.2. The Full Multi-Selection Algorithm",
      "text" : "The scoring algorithm is motivated by the assumption of zero correlation between features. However, this assump-\nAlgorithm 1 Feature multi-selection algorithms 1: Input: Budget B; ((x1, y1), . . . , (xm, ym)) ∈ Rdk+1,\nAlgorithm type: Scoring/Full. 2: Output: A repeat vector r ∈ RB . 3: x̄i[a]← 1k ∑ j∈[k] xi[a](j) for i ∈ [m], a ∈ A.\n4: b̂← 1m ∑ i yix̄i.\n5: v̂[a]← 1m ∑ i VarEst(xi[a](1), . . . , xi[a](k)). 6: if Scoring Algorithm then 7: ∀a ∈ A, σ̂2[a]← max { 0, 1m ∑ i(x̄i[a]) 2 − v̂[a]k } .\n8: Define ˆobj(r) ≡ ∑ a:r[a]>0 b̂[a]\n2/(σ̂2[a] + v̂[a]r[a] ) 9: else\n10: Σ̂← MakePSD (\n1 m ∑ i x̄ T i x̄i −Diag(v̂)/k) ) 11: Mr ≡ subr(Σ̂ + Diag( v̂[1]r[1] , . . . , v̂[d] r[d] )) 12: Define ˆobj(r) ≡ subr(b̂)TM+r subr(b̂) 13: end if 14: r0 ← (0, . . . , 0) ∈ Nd 15: for t = 1 to B do 16: Find ibest ∈ [d] such that ˆobj(rt−1+ei) is maximal. 17: rt ← rt−1 + eibest . 18: end for 19: Return rB .\ntion rarely holds in practice. Building on and paralleling the definitions and derivation above, the Full Algorithm similarly maximizes bTΣ−1r b without this assumption. For positive r, one has\nΣr = Σ + Diag(v[1]/r[1], . . . , v[d]/r[d])\nWhere Σ ≡ E∞[XTX] is the external covariance matrix, and we estimate it based on the equality Σ = Σk − Diag(v)/k. Just as in the Scoring algorithm, the estimates of σ2[a] might be negative, in the full algorithm it is possible that the estimate of Σ will not be positive semi-definite, so we analogously “round up” our estimate of Σ to the nearest PSD matrix (see implementation details below). The estimate when some of the r[a]’s are zero is formed by deleting the corresponding entries in the estimate of b and the corresponding rows and columns in the estimate of Σr."
    }, {
      "heading" : "3.3. Guarantees",
      "text" : "Under our distributional assumptions, we show that the estimated objective functions used by our algorithms converge to E[Y 2] − `(r). Thus maximizing the estimated objective approximately minimizes `(r). Formally, let ˆobjf (r) and ˆobjs(r) be the objectives used in Alg. 1 for the full algorithm and the Scoring algorithm, respectively. Note that these objectives are implicitly functions of the training sample S. For a symmetric matrix S, let λmin(S) be the smallest eigenvalues of S. We define: λ = minr∈RB λmin(subr(Σ)), and B̄ = min(B, d).\nTheorem 3.1. Suppose that all judgments and labels are in [−1, 1]. Then for any δ ∈ (0, 1), with prob. at least 1− δ over m i.i.d. training samples from Dk, for all r ∈ RB , for m ≥ Ω̃(B̄ ln(B̄d/δ)/λ2) we have\n| ˆobjf (r)− (E[Y 2]− `(r))| ≤ O ( B̄3 ln(Bd/δ)\nλ2 √ m\n) ."
    }, {
      "heading" : "If the external covariance matrix Σ is diagonal, then for",
      "text" : "m ≥ Ω̃(ln(d/δ)/λ2) we have\n| ˆobjs(r)− (E[Y 2]− `(r))| ≤ O ( ln(Bd/δ)\nλ2 √ m\n) .\nThe proof of this theorem is provided in Appendix A. The convergence rate for the full algorithm stems from two bounds: (1) If the norm of the minimizing w is at most α, then the convergence rate is at most B̄α2/ √ m; (2) With high probability, the norm of the minimizing w is at most√ B̄/λ. An additional factor of O(B̄ ln(Bd)) gets uniform convergence over r ∈ RB . The components of this result are of the same order as the equivalent results for uniform convergence of standard least-squares regression. An improved rate of √ B̄α2/m can be achieved for least-squares regression, if the algorithm exactly minimizes the sample squared loss (Srebro et al., 2010). However, our algorithm minimizes another objective, thus this result is not directly applicable. We leave it as a challenge for future work to find out whether a faster rate can be achieved in our case.\nAs always, these convergence rates are worst-case, and in practice a much smaller sample size is often sufficient to get meaningful results, as we have observed in our experiments. However, if the available training sample is too small to achieve reasonable results, one can limit the norm of the minimizer by adding regularization to the estimated covariance matrix, as in ridge regression (Hoerl & Kennard, 1970). This would allow faster convergence at the expense of a more limited class of predictors.\nAs Theorem 3.1 shows, when the zero-correlation assumption holds, the Scoring algorithm enjoys a much faster worst-case rate of convergence than the full algorithm. This is because it does not attempt to estimate the entire covariance matrix. This advantage is more significant for larger budgets. An additional advantage is that it finds the optimal value of r for its estimated objective:\nTheorem 3.2. The Scoring algorithm returns r ∈ argmaxr∈RB ˆobjs(r).\nTheorem 3.2 follows since f(r) = a/(b+ c/r) is concave and increasing in r and due to the following observation.\nLemma 3.3. Let r ∈ Nd, and let f(r) = ∑ i∈[d] gi(r[i]), where gi(·) : R+ → R are monotonic non-decreasing concave functions. Let B ∈ N. The maximum of f(r) subject\nto r ∈ RB is attained by a greedy algorithm which starts with r = (0, . . . , 0), and iteratively increases the coordinate which increases f the most.\nThe proof of this lemma is provided in Appendix A."
    }, {
      "heading" : "3.4. Implementation",
      "text" : "If our estimate of Σ is not PSD, we use the procedure ‘MakePSD’, which takes a symmetric matrix A as input, and returns the PSD closest to A in Frobenius norm. This can be done by calculating the eigenvalue decomposition A = UDUT where U is orthogonal and D is diagonal, and returning UD̃UT , where D̃ is D with zeroed negative entries (Higham, 1988). If we assume a diagonal external covariance, then this procedure is equivalent to rounding up the estimate of σ2(a) to zero, as done in the Scoring algorithm. For a budget of B, the full algorithm performs Bd SVDs to calculate pseudo-inverses. Note, however, that the largest matrix that might be decomposed here is of size min(d,B)×min(d,B). Furthermore, in practice the matrices can be much smaller, since the algorithm might choose several repeats of the same features. In our experiments, the total time for decompositions, using standard libraries on a standard personal computer, has been negligible.\nOur description of the algorithms above assumes for simplicity that the mean of all features is zero. In practice, one adds a ‘free’ feature that is always 1, to allow for biased regressors. For the Scoring algorithm, one should further subtract the empirical mean from each feature. For the full algorithm, this not necessary, because when bias is allowed, adding a constant to any feature provably will not change the output of the full algorithm."
    }, {
      "heading" : "4. Experiments",
      "text" : "We tested our approach on three regression problems. In the first problem the feature judgments were simulated. In the second and third problem they were collected from the crowd using Amazon’s Mechanical Turk.4\nFor the simulated experiment we used the UCI dataset ‘Relative location of CT slices on axial axis Data Set’ (Frank & Asuncion, 2010). In this dataset the features are histograms of spatial measurements in the image, and the label to predict is the relative location of the image on the axial axis. To simulate features with varying judgments, we collapsed each set of 8 adjacent histogram bins into a single feature, so that each judgment of the new feature was randomly chosen out of 8 possible values for this feature. The resulting dataset contained 48 noisy real-valued features per example.\nThe second and third problems were to predict the height and weight of people from a photo. 880 photos with selfdeclared height and weight were extracted from the publicly available Photographic Height/Weight Chart (Cockerham, 2013), where people post pictures of themselves announcing their own height and weight. We chose 37 attributes that we felt the crowd could judge and might be predictive. We collected judgments for these binary attributes, mainly following the judgment collection methodology of Patterson & Hays (2012), by batching the images into groups of 40, making labeling very efficient. To encourage honest workers, we promised (and delivered) bonuses for good work. We further limited the amount of work any one person could do. We used all of the collected judgments, regardless of whether the workers received bonuses for them or not. Our pay per hour was set\n4http://mturk.com. We will share our data upon request from other researchers, due to the sensitivity of judgments on people’s images.\nto average to minimum wage. We collected numerical estimates of the height and the weight in a similar fashion. Binary judgments took about one second per judgment and their cost was a fraction of a cent per attribute judgment. The numerical estimates took about four times as long and we paid four times as much for them. Accordingly, we adjusted all the algorithms to count a single numerical judgment as equal to four binary attribute judgments.\nFigure 1 and Figure 2 show the normalized correlation (b̂[a]/σ̂[a]) vs. the normalized inter-rater reliability (v̂[a]/σ̂[a]) of selected attributes. These plots demonstrate that all combinations of useful/non-useful and stable/noisy attributes exist in this data. The full data listing all the attributes and their properties is provided in Table 1.\nTable 1 Lists all the attributes that were collected for the height and weight prediction problem, their internal variance and their normalized correlation for each of the prediction tasks.\nWe compared the test error of our algorithms, denoted ‘Full’ and ‘Scoring’ in the plots, to those of several plausible baselines. In all comparisons, we set k = 2. The first baseline, denoted ‘Averages’ in the plots, is based on the “predictive” feature selection algorithm of Isola et al. (2011): We first average the 2 judgments per attribute to create a standard data set with one value for each objectattribute pair, and then greedily add attributes, one at a time, so as to minimize the least-squares error. The resulting regressor uses 2 judgments for each selected feature. The second baseline, denoted ‘Copies’, treats the 2 judgments of each feature-object pair as 2 different individual attributes, and again performs greedy forward selection on these features. Here the test repeat vector r was set according to the number of copies selected for each feature. Note that these baselines perform standard Machine Learning feature selection: ‘Averages’ considers d features and ‘Copies’ considers 2d features. For height and weight prediction, we compared the results also to the test error achieved by averaging only the height or weight estimates of the crowd, respectively. Since each numerical feature costs 4 times as much as a binary feature, we averaged over B/4 numerical judgments when the budget was set to B. We did not use regularization anywhere, thus our algorithms and the baselines are all parameter-free.\nThe test error presented in the plots was obtained as follows: r was selected based on a training set with k judgments. We then added judgments to features in the training set to get to r repeats. Finally we performed regular regression on the means of the enhanced training set to get a predictor. This predictor was then used to predict the labels of the test set with r judgments. In all the comparisons, each experiment was averaged over 50 random train/test splits. In all of the experiments, shown in figures 3-7, our\nfull algorithm achieved better test error than the baselines. The Scoring algorithm was usually somewhat worse than the Full Multi-Selection algorithm, and for small budgets also sometimes worse than the baselines, This is expected due to its zero-correlation assumption. However, when the sample size was small, the Scoring algorithm was sometimes better (see e.g., Figure 6), since it suffered from less over-fitting. This is consistent with our convergence analysis in Theorem 3.1. Analysis of training errors indicates that baseline algorithms suffer for two different reasons: (1) they are limited to a small number of repeats per feature; and (2) they suffer from greater over-fitting. The second reason is probably due to the fact that our algorithm tends to select a sparser r than do the baselines. Table 2 shows examples of predictors, with number of judgments for each attribute, learned by our full algorithm.\nIn our last experiment we tested the tradeoff between the number of training judgments per feature, and the number of training examples, in the following setting: Suppose we have a budget that allows us to collect a total of M judgments for training the feature multi-selection algorithm, and we have access to at least M/2d labeled exam-\nples. We can decide on a number k of judgments per feature, randomly select M/kd objects from our labeled pool to serve as the training set, and obtain kd judgments for each of these objects. What number k should we choose? Does this number depend on the total budget M? We compared the test error arising from different values of k over different values of M , for the slice dataset using both of our algorithms,. The results are shown in Figure 8. These results show a clear preference for a small k (which allows a large m on the same budget M ). Characterizing the optimal number k is left as an open question for future work."
    }, {
      "heading" : "5. Conclusions",
      "text" : "We introduce the problem of feature multi-selection and provide two algorithms for the case of regression with mean averaging of judgments. Future directions of research include other learning tasks, such as classification, and other types of feature aggregation, such as median averaging (which, for binary features, is equivalent to taking the majority). An additional important question for future\nwork is how to carry out feature multi-selection in an environment with a changing crowd."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We wish to thank Edith Law and Haoqi Zhang for several helpful discussions."
    }, {
      "heading" : "A. Analysis",
      "text" : "A.1. Notations\nFor a symmetric matrix S, let λmax(S) and λmin(S) be the largest and the smallest eigenvalues of S, respectively. For functions α and β, we say that α ≤ O(β) if for some constants C,C ′ > 0, α ≤ Cβ + C ′. Similarly, α ≥ Ω(β) indicates that for some constants C,C ′ > 0, α ≥ Cβ−C ′. we say that α ≤ Õ(β) if for some constants C,C ′ > 0,\nα ≤ Cβ ln(β) + C ′. Similarly, α ≥ Ω̃(β) indicates that for some constants C,C ′ > 0, α ≥ Cβ ln(β)− C ′.\nDenote by Zr a diagonal d× d matrix whose i’th diagonal entry is I[r[i] > 0]. Recall that v[i] is the internal variance of feature i. For a vector r ∈ Nd, let Vr be the diagonal matrix such that its i’th diagonal entry is zero if r[i] = 0 and equal to v[i]/r[i] otherwise. Let V̂r be defined similarly but using the sample estimate v̂[i], defined in Eq. (2), instead of v[i]. Denote by nr the number of non-zero entries in r ∈ Nd.\nLet x ∈ R[k] be an example with a repeat vector k such that k[i] ≥ 2.5 Let v̂(x)[i] = VarEst(x[i](1), . . . , x[i](k[i])). Given a repeat vector r, define the r-loss `r of the labeled example (x, y) by:\n`r(w,x, y) = (〈Zrw, x̄〉 − y)2 + ∑\ni:r[i]>0\nw[i]2v̂(x)[i]( 1 r[i] − 1 k[i] ).\nLet S = ((x1, y1), . . . , (xm, ym)) be a training set of labeled representations drawn i.i.d. from Dk. Denote the vector of training labels by y = (y1, . . . , yl). We denote the average of `r over S by\n`r(w, S) = 1\nm ∑ l∈[m] `r(w,xl, yl).\nDefine\nΣ̂ = 1\nm ∑ l∈[m] x̄lx̄ T l − V̂k,\nΣ̂r = Zr(Σ̂ + V̂r)Zr,\nb̂ = 1\nm ∑ l∈[m] ylx̄l.\nNote that the notation for Σ̂ here is different than the one used in Alg. 1, since Σ̂ is not ‘corrected’ to be PSD. We denote the corrected estimate used in the algorithm by Σ̂p. Similarly, we denote by Σ̂pr the estimate for Σr resulting from using Σ̂p instead of Σ̂. We have\n`r(w, S) = w T Σ̂rw − 2wTZrb̂ +\n1 m yTy.\nDefine λr = λmin(subr(Σ)). Note that for λ defined in the statement of Theorem 3.1, λ = minr∈RB λr.\n5Alg. 1 can be applied to k with different values per feature by simply using k[a] instead of k in step 7 and Vk instead of Diag(v̂)/k in step 10. Our analysis holds for this more general algorithm.\nA.2. Proof of Theorem 3.1\nTo prove Theorem 3.1 we require several lemmas. All of the analysis below is under the assumption of Theorem 3.1, that all judgments and labels are in [−1, 1] with probability 1.\nFirst, the following lemma links the covariance matrix of the population when averaging r judgments to the covariance matrix of the population when using the expected values of the features for each object.\nLemma A.1. Let r ∈ Nd. Then Σr = ZrΣZr + Vr.\nProof. Consider a random object O drawn from DO, and define the vector µO such that for each coordinate i, µO[i] = E[X[i]|O]. Define ΣO = µOµTO, so that Σ = EO∼DO [ΣO]. Similarly, define Σr|O = Er[X̄Tr X̄ | O], so that Σr = EO∼DO [Σr|O]. Lastly, denote the variance of a feature on object O by vO[i] = E[(X[i]− E[X[i]|O])2|O], and let Vr|O be the diagonal matrix whose i’th entry is zero if r[i] = 0, and equal to vO[i]/r[i] otherwise. Again Vr = EO[Vr|O]. We will prove that for any object O,\nΣr|O = ZrΣOZr + Vr|O. (3)\nThe desired equality will follow by averaging over O ∼ DO.\nDenote the entries of Σr|O by sik. First, whenever r[i] = 0 or r[k] = 0, entry (i, k) is zero for both sides of Eq. (3). Now, consider a non-diagonal entry sik of Σr|O for i 6= k, r[i] > 0 and r[k] > 0. The (i, k) entry on the right-hand side of Eq. (3) is µO[i]µO[k]. For the left-hand side we have\nsik = Er[X̄[i]X̄[k] | O] = Er[X̄[i] | O] · Er[X̄[k] | O] = µO[i]µO[k].\nThus the equality holds for non-diagonal entries.\nNow, consider a diagonal entry sii of Σr. If r[i] = 0 then both sides of Eq. (3) are zero. Assume r[i] > 0. We have\nsii = Er[X̄[i]2 | O] = Er[( 1\nr[i] ∑ j∈[r[i]] X[i](j))2 | O]\n= 1 r[i]2 ( ∑ j∈[r[i]] Er[(X[i](j))2 | O]+\n2 · ∑\nj<j′,j,j′∈[r[i]]\nEr[X[i](j) ·X[i](j′) | O] ) .\nConditioned onO, X[i](j) andX[i](j′) are statistically independent. Therefore\nEr[X[i](j) ·X[i](j′) | O] = Er[X[i](j) | O] · E[X[i](j′) | O] = µO[i]2.\nIn addition, Er[(X[i](j))2] = vO[i] + µO[i]2. Combining the two equalities we get\nsii = 1\nr[i] (vO[i] + µO[i]\n2) + 1\nr[i]2 · (r[i]2 − r[i])µO[i]2\n= µO[i] 2 + vO[i]/r[i].\nThis is exactly the value of entry (i, i) on the right-hand side of Eq. (3). We conclude that Eq. (3) holds for all types of entries, thus the lemma is proved.\nUsing Lemma A.1, we can show that `r(w, S) is an unbiased estimator of `(w, Dr).\nLemma A.2. Let k, r ∈ Nd, so that ∀i ∈ [d], k[i] ≥ 2. Let S be a sample drawn i.i.d. from Dk. For any w ∈ Rd, `(w, Dr) = ES [`r(w, S)].\nProof. We have\nES [`r(w, S)] = ES [wT Σ̂rw − 2wTZrb̂ + 1\nm yTy]\n= wTES [Σ̂r]w − 2wTZrES [b̂] + ES [ 1\nm yTy].\nFrom the definition of Σ̂r, we have\nES [Σ̂r] = ZrES [ 1\nm ∑ l∈[m] x̄lx̄ T l + V̂r − V̂k]Zr\n= Zr(Σk + Vr − Vk)Zr = Σr,\nWhere the last inequality follows from Lemma A.1.\nIn addition, E[b̂] = b, and E[ 1my Ty] = E[Y 2]. Therefore\nES [`r(w, S)] = wTΣrw − 2wTZrb + E[Y 2].\nOn the other hand, we have\n`(w, Dr) = Er[(〈w, X̄〉 − Y )2] = Er[(wT X̄− Y )(X̄Tw − Y )] = wTE[X̄X̄T ]w − 2wTEr[X̄Y ] + E[Y 2] = wΣrw − 2wTZrb + E[Y 2] = ES [`r(w, S)].\nThis completes the proof.\nThe next step is to bound the rate of convergence of minw∈Rd `r(w, S) to `(r) = minw∈Rd `(w, Dr). We show that as m grows, the difference between the two quantities approaches zero. The following lemma provides guarantees under the assumption that the two minimizers have a bounded norm. We will then go on to show that such a bound on the norm holds with high probability, where the bound depends on the external covariance matrix Σ.\nLemma A.3. Let α > 0, and let Wα = {w ∈ Rd | ‖w‖ ≤ α}. Let δ ∈ (0, 1). Fix some w∗ ∈ Wα, and let ŵ ∈ argminw∈Rd `r(w, S) such that ‖ŵ‖ is minimal. With probability at least 1 − δ over the draw of S, if ‖ŵ‖ ≤ α, then\n|`(w∗, Dr)− `r(ŵ, S)| ≤ O ( α2nr ln(e/δ)√\nm\n) .\nProof. By Lemma A.2, `(w, Dr) = Ek[`r(w,X, Y )]. By Rademacher complexity bounds (Bartlett & Mendelson, 2002), with probability 1− δ, for all w ∈Wα,\nEr[`r(w,X, Y )] ≤ (4) `r(w, S) +Rm(`r ◦Wα, Dk) +O ( β √ ln(1/δ)/m ) ,\nwhereRm(`r ◦Wα, Dk) is the expected Rademacher complexity of the function classWα under `r, and β is the maximal value of `r on the possible inputs. Under our assumptions, β ≤ α2nr.\nWe wish to bound the Rademacher complexity for our function class, defined as\nRm(`r◦Wα, Dk) = 1\nm ES [Eσ[| sup\nw∈Wα ∑ l∈[m] σl`r(w,xl, yl)|]],\nwhere σ = (σ1, . . . , σm) are m independent uniform {±1}-valued variables, and S = ((x1, y1), . . . , (xm, ym)) is a random sample drawn i.i.d. from Dk. Denote the components of `r by\n`ar(w,x, y) = (〈Zrw, x̄〉 − y)2, `br(w,x, y) = ∑\ni:r[i]>0\nw[i]2v̂(x)[i]( 1 r[i] − 1 k[i] ).\nWe have\nRm(`r◦Wα, Dk) ≤ Rm(`ar◦Wα, Dk)+Rm(`br◦Wα, Dk).\nWe bound each of these Rademacher complexities individually. The first term is the Rademacher complexity of the squared loss over the distribution generated by averaging over judgment vectors drawn from Dk. Standard application of the Lipschitz properties of the squared loss following Bartlett & Mendelson (2002) provides the following bound:\nRm(`ar ◦Wα, Dk) ≤ O ( α2nr√ m ) .\nFor the second term, denote uw = Zr·(w[1]2, . . . , w[d]2)T , and v̂x = Zr · (v̂(x)1, . . . , v̂(x)d)T . Then\n`br(w,x, y) = ( 1 r[i] − 1 k[i] )〈uw, v̂x〉.\nSince all feature judgments are in [−1, 1], v̂x[i] ≤ 4, thus ‖v̂x‖ ≤ 4 √ nr. In addition, ‖uw‖ ≤ ‖w‖2. Lastly, | 1r[i]− 1 k[i] | ≤ 1. Thus, by standard Rademacher complexity bounds for the linear loss,\nRm(`br ◦Wα, Dk) ≤ O (\nsupw∈Wα ‖uw‖ · supx ‖v̂x‖√ m ) ≤ O ( α2 √ nr√ m ) .\nSumming the two terms, we have shown that Rm(`r ◦Wα, Dk) ≤ O ( α2nr√ m ) .\nCombining this with Eq. (4) and taking the minimum on both sides, we get\n`(w∗, Dr) ≤ `r(ŵ, S) +O ( α2nr ln(e/δ)√\nm\n) .\nThis completes one side of the bound. To bound `r(ŵ, S)− `(w∗, Dr), we note that, by the minimality of ŵ and by Hoeffding’s inequality, with probability 1− δ\n`r(ŵ, S) ≤ `r(w∗, S) ≤ E[`r(w∗, S)] +O(β √ ln(e/δ)\nm ).\nCombining the two bounds and applying the union bound we get the statement of the lemma.\nWe now turn to bound the norm of any w considered by our algorithm. Let\nΣ̂ = 1\nm ∑ l∈[m] x̄lx̄ T l − V̂k.\nFirst, we relate the smallest eigenvalue of Σ̂ to that of the true Σ.\nLemma A.4. Let δ ∈ (0, 1). With probability at least 1−δ,\nλmin(subr(Σ̂)) ≥ (5) λr − √ O(ln(1/δ) + nr ln(nrm))/m.\nProof. It can be shown (see e.g. Sabato, 2012), by using an -net of vectors on the unit sphere, that for a symmetric random matrix S ∈ Rn×n, and positive β, , γ,\nP[λmin(S) ≤ β − γ] ≤ (6) P[λmax(S) > γ] +O((n/ )n) min\nu:‖u‖=1 P[uTSu ≤ β].\nIn our case, we have S = subr(Σ̂) and n = nr. Due to the boundedness of the judgments, all the entries of S are at most 1. Therefore λmax(S) ≤ nr. We thus let γ = nr,\nso that P[λmax(S) > γ] = 0. To bound P[uTSu ≤ β] for u such that ‖u‖ = 1, note that\nuTSu = 1\nm ( ∑ l∈[m] 〈u,Zrx̄l〉2 − ∑ i:r[i]>0 u[i]2v(xl)[i]/k[i] ) .\nBy McDiarmid’s inequality (McDiarmid, 1989), for any t > 0,\nP[uTSu ≤ E[uTSu]− t] ≤ exp(−2t2/m∆2),\nWhere ∆ is the maximal difference between uTSu with some x1, . . . ,xm and uTSu with x[i] replaced by some x′[i]. It is easy to verify that due to the boundedness of all xl, ∆ ≤ O(nr/m). Further, uTE[S]u = uT subr(Σ)u ≥ λmin(subr(Σ)) = λr. Thus,\nP[uTSu ≤ λr − t] ≤ exp(−2mt2/n2r).\nSubstituting into Eq. (6), we get\nP[λmin(S) ≤ λr − t− nr] ≤ O((nr/ )nr) exp(−2mt2/n2r) = exp(−2mt2 + nr ln(nr/ )).\nLetting = t/nr and solving for δ we get the statement of the theorem.\nUsing Lemma A.4, we can now bound the norms of a minimizer of `(w, Dr) and of a minimizer of `r(w, S) with high probability.\nLemma A.5. Let w ∈ Rd be a minimum-norm minimizer for `r(w, S), that is, let M be the set of minimizers for `r(w, S), and let ŵ ∈ argminw∈M ‖w‖. Let w∗ be a minimum-norm minimizer for `r(w, S). If m ≥ Ω̃((ln(1/δ)+nr ln(nr))/λ 2 r), then with probability at least 1− δ, subr(Σ̂) is positive definite, and\n‖ŵ‖ ≤ √ nr ( λr − √ O(ln(1/δ) + nr ln(nrm))\nm\n)−1 , and\n‖w∗‖ ≤ √ nr/λr.\nFurther, this holds simultaneously for any vector r′ ∈ Nd with the same support as r.\nProof. Any minimum-norm minimizer for `r(w, S) has w[i] = 0 whenever r[i] = 0. Thus, we may assume w.l.o.g. that r[i] > 0 for all i by deleting the coordinates with r[i] = 0. We thus have\n`r(w, S) = w T Σ̂rw − 2wT b̂ +\n1 m yTy.\nIf Σ̂r is positive definite, then the minimizer of `r(w, S) is ŵ = Σ̂−1r b̂. Thus\n‖ŵ‖ ≤ ‖b̂‖λmax(Σ̂−1r ) (7)\n= ‖b̂‖/λmin(Σ̂r) ≤ √ nr/λr.\nNow, Σ̂r = Σ̂ + V̂r. Since V̂r 0, we have λmin(Σ̂r) ≥ λmin(Σ̂). λmin(Σ̂) can be bounded from below by Lemma A.4. Substituting Eq. (5) in Eq. (7) we get the first desired inequality. For the second inequality, it suffices to note that if subr(Σ) is not singular, then w∗ = subr(Σ)\n−1b, thus ‖w∗‖ ≤ λ−1min(subr(Σ))‖b‖ ≤√ nr/λr.\nCombining Lemma A.3 and Lemma A.5 and applying the union bound we immediately get the following theorem.\nTheorem A.6. Let S be a training sample of size m drawn from Dk, where k[i] ≥ 2 for all i in [d]. Fix r ∈ Nd. Let ŵ be a minimum-norm minimizer of `r(w, S). Let δ ∈ (0, 1). Ifm ≥ Ω̃((ln(1/δ)+nr ln(nr))/λ2r), then with probability at least 1− δ, subr(Σ̂) is positive definite and\n|`(r)− `r(ŵ, S)| ≤ O ( n2rλ −2 r ln(e/δ)√\nm\n) .\nMoreover, the positive-definiteness holds simultaneously for all r′ with the same support as r.\nFinally, we prove our main result, Theorem 3.1.\nProof of Theorem 3.1. We start by proving the result for the full algorithm. Assume that m ≥ Ω̃((ln(1/δ) + nr ln(nr))/λ 2), and consider a fixed r ∈ RB . Recall that\n`r(w, S) = w T Σ̂rw − 2wTZrb̂ +\n1 m yTy.\nThe full algorithm ignores, for each examined repeat vector, all the matrix and vector entries that correspond to features with zero repeats. Thus we may assume w.l.o.g. that for all i, r[i] > 0. Theorem A.6 guarantees with probability 1− δ, that Σ̂ is positive definite with probability 1− δ. It follows that Σ̂p = Σ̂, therefore Σ̂pr = Σ̂r. We thus have\n`r(w, S) = w T Σ̂prw − 2wT b̂ +\n1 m yTy.\nThe minimizer of `r(w, S) is ŵ = (Σ̂pr) −1b̂. Substituting this solution in `r(w, S), we get minw∈Rd `r(w, S) = −b̂T (Σ̂pr)−1b̂ + 1my Ty = − ˆobjf (r) + 1my Ty. If the features are uncorrelated then\nmin w∈Rd\n`r(w, S) = −b̂T (Σ̂pr)−1b̂ + 1\nm yTy\n= − ˆobjf (r) + 1\nm yTy.\nSince all labels Y are bounded in [−1, 1], with probability 1− δ, | 1my Ty − E[Y 2]| ≤ O( √\nln(1/δ)/m). Thus it suffices to bound |`(r) − minw∈Rd `r(w, S)|. This bound is given by Theorem A.6. We now apply this bound simultaneously to all the possible r ∈ RB . There are∑ i∈[B] ( d+ i− 1 d− 1 ) = ∑ i∈[B] ( d+ i− 1 i− 1 ) ≤ B(d+B)B̄\nsuch combinations, thus the bound in Theorem A.6 holds simultaneously for all r ∈ RB , by dividing δ with this upper bound. For the requirement on the size of m we only need a union bound on the number of possible supports for r, which is bounded by dB̄ . Finally, by noting that for all r ∈ RB , nr ≤ B̄, we get the desired uniform convergence bound.\nNow, consider the Scoring algorithm. If Σ is diagonal, then `(w, Dr) decomposes into a sum of nr independent losses over single-dimensional predictors with a singledimensional covariance ‘matrix’ equal to the scalar σ2[i] for feature i. The loss minimized by the Scoring algorithm decomposes similarly, using the covariance ‘matrix’ σ̂2[i]. Thus, we apply the convergence bound of Theorem A.6 to show convergence of each of the components individually, with nr = 1 and a union bound over B possible values of r[i], and then apply a union bound over d components to get simultaneous convergence of the parts of the loss. For the requirement on the size of m we only need a union bound over the number of components d.\nA.3. Proof of Theorem 3.2\nFirst, we prove the more general Lemma 3.3. We actually prove an equivalent mirror image of Lemma 3.3, by assuming the gi are convex non-increasing and proving that a greedy algorithm minimizes f(r). We formally define the greedy algorithm as follows:\n1. r0 ← (0, . . . , 0).\n2. For t = 1 to B, let rt = rt−1 + ei, where i is the coordinate of r that decreases f(r) the most.\n3. Return rB .\nProof of Lemma 3.3. Let r∗ ∈ argminr∈RB f(r). Since gi(·) are all non-increasing, we may choose r∗ such that∑ i∈[d] r\n∗[i] = B. Let r be a solution returned by the greedy algorithm listed in the theorem statement. Consider the iterations t1 < . . . < tn ∈ [B] such that the index ik selected by the algorithm at iteration tk satisfies rtk [ik] > r\n∗[ik], so that it causes r to increase this coordinate more than its value in r∗. Let j1, . . . , jn ∈ [d] be a\nseries of alternative indices such that r = r∗ + ∑ k∈[n] eik − ∑ k∈[n] ejk .\nDenote r∗L = r ∗ + ∑ k∈[L] eik − ∑ k∈[L] ejk .\nNote that for all L < n, r[jL] < r∗L−1[jL].\nWe prove by induction that for all L ∈ [n],\nf(r∗L) = f(r ∗).\nWhen setting L = n we will get f(r) = f(r∗).\nThe claim trivially holds L = 0. Now assume it holds for L − 1, and consider L. Denote for brevity t = tL−1, i = iL and j = jL. Since the algorithm selected i over j at iteration t+ 1, we have that\nf(rt + ei) ≤ f(rt + ej).\nSubtracting ∑ l∈[d] gl(rt[l]) from both sides we get\ngi(rt[i] + 1)− gi(rt[i]) ≤ gj(rt[j] + 1)− gj(rt[j]).\nIt follows that\n0 ≤ gi(rt[i])− gi(rt[i] + 1) + gj(rt[j] + 1)− gj(rt[j]).\nSince rt[i] ≥ r∗L−1[i], by the convexity of gi,\ngi(rt[i])− gi(rt[i] + 1) ≤ gi(r∗L−1[i])− gi(r∗L−1[i] + 1).\nIn addition, rt[j] + 1 ≤ r∗L−1[j], therefore, again by convexity,\ngj(rt[j] + 1)− gj(rt[j]) ≤ gj(r∗L−1[j])− gj(r∗L−1[j]− 1).\nIt follows that\n0 ≤gi(r∗L−1[i])− gi(r∗L−1[i] + 1)+ gj(r ∗ L−1[j])− gj(r∗L−1[j]− 1).\nTherefore\n0 ≤ f(r∗L−1)− f(r∗L−1 + ei − ej).\nBy the induction hypothesis f(r∗) = f(r∗L−1). In addition, r∗L−1 + ei − ej = r∗L. it follows that\nf(r∗L) ≤ f(r∗).\nSince r∗ is optimal for f , it follows that this inequality must hold with equality, thus proving the induction hypothesis.\nProof of Theorem 3.2. We have\nˆobj(r) = ∑\ni:r[i]>0\nb̂[i]2\nσ̂2[i] + v̂i/r[i] .\nDefine fi : N+ → R by\nfi(x) = x · b̂[i]2\nx · σ̂2[i] + v̂[i] .\nThen ˆobj(r) = ∑ i:r[i]>0 fi(x).\nWe will define gi(x) : R → R so that each gi is concave and ˆobj(r) = ∑ i∈[d] gi(r[i]). (8)\nWe consider two cases: (1) If v̂[i] > 0, then gi(x) is the natural extension of fi(x) to the reals. (2) If v̂[i] = 0, fi is a positive constant for all positive integers. Let gi(x) = fi(1) for all x ≥ 1, and let gi(x) = x/fi(1).\nIn both cases, gi(x) is concave with gi(x) = fi(x) for x ∈ N+ and gi(0) = 0. Therefore Eq. (8) holds. In addition, in both cases gi(x) is monotonic non-decreasing. Therefore by Lemma 3.3, the greedy algorithm maximizes ˆobj(r) subject to r ∈ RB ."
    } ],
    "references" : [ {
      "title" : "Rademacher and Gaussian complexities: Risk bounds and structural results",
      "author" : [ "P.L. Bartlett", "S. Mendelson" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Bartlett and Mendelson,? \\Q2002\\E",
      "shortCiteRegEx" : "Bartlett and Mendelson",
      "year" : 2002
    }, {
      "title" : "Generalized inverses: theory and applications, volume 15",
      "author" : [ "A. Ben-Israel", "T.N.E. Greville" ],
      "venue" : null,
      "citeRegEx" : "Ben.Israel and Greville,? \\Q2003\\E",
      "shortCiteRegEx" : "Ben.Israel and Greville",
      "year" : 2003
    }, {
      "title" : "Efficient learning with partially observed attributes",
      "author" : [ "N. Cesa-Bianchi", "S. Shalev-Shwartz", "O. Shamir" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Cesa.Bianchi et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Cesa.Bianchi et al\\.",
      "year" : 2011
    }, {
      "title" : "Statistical regression with measurement error, volume 6",
      "author" : [ "C. Cheng", "J.W. Van Ness" ],
      "venue" : "Arnold London,",
      "citeRegEx" : "Cheng and Ness,? \\Q1999\\E",
      "shortCiteRegEx" : "Cheng and Ness",
      "year" : 1999
    }, {
      "title" : "The photographic height and weight chart",
      "author" : [ "Cockerham", "Rob" ],
      "venue" : "http://www.cockeyed.com/photos/ bodies/heightweight.html,",
      "citeRegEx" : "Cockerham and Rob.,? \\Q2013\\E",
      "shortCiteRegEx" : "Cockerham and Rob.",
      "year" : 2013
    }, {
      "title" : "Coefficient alpha and the internal structure of tests",
      "author" : [ "L. Cronbach" ],
      "venue" : "Psychometrika, 16(3):297–334,",
      "citeRegEx" : "Cronbach,? \\Q1951\\E",
      "shortCiteRegEx" : "Cronbach",
      "year" : 1951
    }, {
      "title" : "Maximum likelihood estimation of observer error-rates using the em algorithm",
      "author" : [ "A.P. Dawid", "A.M. Skene" ],
      "venue" : "Journal of the Royal Statistical Society. Series C (Applied Statistics),",
      "citeRegEx" : "Dawid and Skene,? \\Q1979\\E",
      "shortCiteRegEx" : "Dawid and Skene",
      "year" : 1979
    }, {
      "title" : "Computing a nearest symmetric positive semidefinite matrix",
      "author" : [ "N.J. Higham" ],
      "venue" : "Linear algebra and its applications,",
      "citeRegEx" : "Higham,? \\Q1988\\E",
      "shortCiteRegEx" : "Higham",
      "year" : 1988
    }, {
      "title" : "Understanding the intrinsic memorability of images",
      "author" : [ "P. Isola", "D. Parikh", "A. Torralba", "A. Oliva" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Isola et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Isola et al\\.",
      "year" : 2011
    }, {
      "title" : "Content analysis: An introduction to its methodology",
      "author" : [ "K. Krippendorff" ],
      "venue" : "Sage Publications,",
      "citeRegEx" : "Krippendorff,? \\Q2012\\E",
      "shortCiteRegEx" : "Krippendorff",
      "year" : 2012
    }, {
      "title" : "On the method of bounded differences",
      "author" : [ "C. McDiarmid" ],
      "venue" : "Surveys in Combinatorics, pp",
      "citeRegEx" : "McDiarmid,? \\Q1989\\E",
      "shortCiteRegEx" : "McDiarmid",
      "year" : 1989
    }, {
      "title" : "Subset selection in regression",
      "author" : [ "A. Miller" ],
      "venue" : "Chapman & Hall/CRC,",
      "citeRegEx" : "Miller,? \\Q2002\\E",
      "shortCiteRegEx" : "Miller",
      "year" : 2002
    }, {
      "title" : "Sparse approximate solutions to linear systems",
      "author" : [ "B.K. Natarajan" ],
      "venue" : "SIAM journal on computing,",
      "citeRegEx" : "Natarajan,? \\Q1995\\E",
      "shortCiteRegEx" : "Natarajan",
      "year" : 1995
    }, {
      "title" : "Sun attribute database: Discovering, annotating, and recognizing scene attributes",
      "author" : [ "Patterson", "Genevieve", "Hays", "James" ],
      "venue" : "In Proceeding of the 25th Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Patterson et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Patterson et al\\.",
      "year" : 2012
    }, {
      "title" : "Partial Information and DistributionDependence in Supervised Learning Models",
      "author" : [ "S. Sabato" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "Sabato,? \\Q2012\\E",
      "shortCiteRegEx" : "Sabato",
      "year" : 2012
    }, {
      "title" : "Feature multi-selection among subjective features",
      "author" : [ "S. Sabato", "A. Kalai" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Sabato and Kalai,? \\Q2013\\E",
      "shortCiteRegEx" : "Sabato and Kalai",
      "year" : 2013
    }, {
      "title" : "Inferring ground truth from subjective labelling of venus images",
      "author" : [ "P. Smyth", "U.M. Fayyad", "M.C. Burl", "P. Perona", "P. Baldi" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Smyth et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Smyth et al\\.",
      "year" : 1994
    }, {
      "title" : "Smoothness, lownoise and fast rates",
      "author" : [ "N. Srebro", "K. Sridharan", "A. Tewari" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "Srebro et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Srebro et al\\.",
      "year" : 2010
    }, {
      "title" : "The multidimensional wisdom of crowds",
      "author" : [ "Welinder", "Peter", "Branson", "Steve", "Belongie", "Serge", "Perona", "Pietro" ],
      "venue" : "In Neural Information Processing Systems Conference (NIPS),",
      "citeRegEx" : "Welinder et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Welinder et al\\.",
      "year" : 2010
    }, {
      "title" : "By Rademacher complexity bounds (Bartlett & Mendelson, 2002), with probability",
      "author" : [ "X Ek[`r(w" ],
      "venue" : null,
      "citeRegEx" : "Ek.`r.w and ...,? \\Q2002\\E",
      "shortCiteRegEx" : "Ek.`r.w and ...",
      "year" : 2002
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "This work has been published in Sabato & Kalai (2013).",
      "startOffset" : 32,
      "endOffset" : 54
    }, {
      "referenceID" : 12,
      "context" : "Since the feature selection problem is well known to be NP-hard (Natarajan, 1995), our problem is also NP-hard in the general case.",
      "startOffset" : 64,
      "endOffset" : 81
    }, {
      "referenceID" : 16,
      "context" : "A wide variety of techniques have been studied to combine estimates of experts or the crowd of a single quantity of interest (see, e.g. Dawid & Skene, 1979; Smyth et al., 1994; Welinder et al., 2010), like estimating the number of jelly beans in a jar from a number of guesses.",
      "startOffset" : 125,
      "endOffset" : 199
    }, {
      "referenceID" : 18,
      "context" : "A wide variety of techniques have been studied to combine estimates of experts or the crowd of a single quantity of interest (see, e.g. Dawid & Skene, 1979; Smyth et al., 1994; Welinder et al., 2010), like estimating the number of jelly beans in a jar from a number of guesses.",
      "startOffset" : 125,
      "endOffset" : 199
    }, {
      "referenceID" : 2,
      "context" : "A number of researchers have studied attribute-efficient prediction (also called budgeted learning) assuming, as we do, that there is a cost to evaluating attributes and one would like to evaluate as few as possible (see, for instance, the recent work by Cesa-Bianchi et al. (2011) and references therein).",
      "startOffset" : 255,
      "endOffset" : 282
    }, {
      "referenceID" : 2,
      "context" : "A number of researchers have studied attribute-efficient prediction (also called budgeted learning) assuming, as we do, that there is a cost to evaluating attributes and one would like to evaluate as few as possible (see, for instance, the recent work by Cesa-Bianchi et al. (2011) and references therein). In that line of work, each attribute is judged at most once. The errors-in-variables approach (e.g., Cheng & Van Ness, 1999) in statistics estimates the ‘true’ regression coefficients using noisy feature measurements. This approach is less suitable in our setting, since our final goal is to predict from noisy measurements. A wide variety of techniques have been studied to combine estimates of experts or the crowd of a single quantity of interest (see, e.g. Dawid & Skene, 1979; Smyth et al., 1994; Welinder et al., 2010), like estimating the number of jelly beans in a jar from a number of guesses. Two recent works on crowdsourcing are very relevant. Patterson & Hays (2012) crowdsourced the mean of 3 judgments of each of 102 binary attributes on over 14,000 images, yielding over 4 million judgments.",
      "startOffset" : 255,
      "endOffset" : 987
    }, {
      "referenceID" : 2,
      "context" : "A number of researchers have studied attribute-efficient prediction (also called budgeted learning) assuming, as we do, that there is a cost to evaluating attributes and one would like to evaluate as few as possible (see, for instance, the recent work by Cesa-Bianchi et al. (2011) and references therein). In that line of work, each attribute is judged at most once. The errors-in-variables approach (e.g., Cheng & Van Ness, 1999) in statistics estimates the ‘true’ regression coefficients using noisy feature measurements. This approach is less suitable in our setting, since our final goal is to predict from noisy measurements. A wide variety of techniques have been studied to combine estimates of experts or the crowd of a single quantity of interest (see, e.g. Dawid & Skene, 1979; Smyth et al., 1994; Welinder et al., 2010), like estimating the number of jelly beans in a jar from a number of guesses. Two recent works on crowdsourcing are very relevant. Patterson & Hays (2012) crowdsourced the mean of 3 judgments of each of 102 binary attributes on over 14,000 images, yielding over 4 million judgments. Some of their attributes are subjective, e.g., soothing. We employ their crowdsourcing protocol to label our binary attributes. Isola et al. (2011) study subjective and objective features for the task of estimating how memorable an image is, by taking the mean of 10 judgments per attribute for each im-",
      "startOffset" : 255,
      "endOffset" : 1263
    }, {
      "referenceID" : 5,
      "context" : "Finally, in the social sciences, a wide array of techniques have been developed for assessing inter-rater reliability of attributes, with the most popular perhaps being the α coefficient (Cronbach, 1951).",
      "startOffset" : 187,
      "endOffset" : 203
    }, {
      "referenceID" : 9,
      "context" : "For an overview of reliability theory, see (Krippendorff, 2012).",
      "startOffset" : 43,
      "endOffset" : 63
    }, {
      "referenceID" : 17,
      "context" : "An improved rate of √ B̄α2/m can be achieved for least-squares regression, if the algorithm exactly minimizes the sample squared loss (Srebro et al., 2010).",
      "startOffset" : 134,
      "endOffset" : 155
    }, {
      "referenceID" : 7,
      "context" : "This can be done by calculating the eigenvalue decomposition A = UDU where U is orthogonal and D is diagonal, and returning UD̃U , where D̃ is D with zeroed negative entries (Higham, 1988).",
      "startOffset" : 174,
      "endOffset" : 188
    }, {
      "referenceID" : 8,
      "context" : "The first baseline, denoted ‘Averages’ in the plots, is based on the “predictive” feature selection algorithm of Isola et al. (2011): We first average the 2 judgments per attribute to create a standard data set with one value for each objectattribute pair, and then greedily add attributes, one at a time, so as to minimize the least-squares error.",
      "startOffset" : 113,
      "endOffset" : 133
    } ],
    "year" : 2013,
    "abstractText" : "When dealing with subjective, noisy, or otherwise nebulous features, the “wisdom of crowds” suggests that one may benefit from multiple judgments of the same feature on the same object. We give theoretically-motivated feature multi-selection algorithms that choose, among a large set of candidate features, not only which features to judge but how many times to judge each one. We demonstrate the effectiveness of this approach for linear regression on a crowdsourced learning task of predicting people’s height and weight from photos, using features such as gender and estimated weight as well as culturally fraught ones such as attractive. This work has been published in Sabato & Kalai (2013).",
    "creator" : "LaTeX with hyperref package"
  }
}