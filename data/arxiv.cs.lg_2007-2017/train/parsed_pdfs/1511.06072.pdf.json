{
  "name" : "1511.06072.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Sebastian Agethen", "Winston H. Hsu" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "We present a new supervised architecture termed Mediated Mixture-of-Experts (MMoE) that allows us to improve classification accuracy of Deep Convolutional Networks (DCN). Our architecture achieves this with the help of expert networks: A network is trained on a disjoint subset of a given dataset and then run in parallel to other experts during deployment. A mediator is employed if experts contradict each other. This allows our framework to naturally support incremental learning, as adding new classes requires (re-)training of the new expert only. We also propose two measures to control computational complexity: An early-stopping mechanism halts experts that have low confidence in their prediction. The system allows to trade-off accuracy and complexity without further retraining. We also suggest to share low-level convolutional layers between experts in an effort to avoid computation of a near-duplicate feature set. We evaluate our system on a popular dataset and report improved accuracy compared to a single model of same configuration."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Deep learning methods in general, and Deep Convolutional Neural Networks (DCN) in particular, have seen a surge in popularity among researchers over the past decade or so. While the application of early DCN was limited to simple tasks such as hand-written digit recognition Cun et al. (1990), modern state-of-the-art methods can meet or exceed human-level performance on far more complex tasks, including generic image classification He et al. (2015); Krizhevsky et al. (2012); Szegedy et al. (2014). Two factors that have contributed to this development are the availability of large-scale image corpora like ImageNet Russakovsky et al. (2014), and the wide-spread availability of GPU-based computing hardware which renders computationally expensive model training procedures feasible.\nExperts systems are known to improve the classification accuracy of neural networks even further and have been studied extensively. Previous work on Mixture of Experts (MoE) Jacobs et al. (1991) is however flawed in two aspects: First, computational complexity in MoE is a multiple of that of a traditional model. To this end, we propose early-stopping of experts. Our mechanism evaluates each expert’s confidence and is influenced by a hyper-parameter, allowing a trade-off between complexity and classification accuracy without the need for retraining. Second, as recently pointed out in Xiao et al. (2014), training becomes more difficult as datasets are getting larger. Intuitively, we expect that an expert has extensive knowledge on a limited area of expertise, i.e., a subset of the training data, and thereby naturally avoids the issue. Previous MoE systems are trained on full datasets, however, as difficulties arise when a decision is needed on which expert to trust. To remedy this issue, we introduce a mediator, see Figure 1. The mediator has a full classifier, allowing it to arbitrate conflicting predictions when multiple experts are confident.\nIn this paper we propose a new architecture which we name Mediated Mixture of Experts (MMoE) that aims to increase the classification accuracy of a general DCN. We briefly discuss related research in Section 2 before formally defining our method in Section 3. We follow by summarizing the outcome of our experimental evaluation in Section 4 and give some concluding remarks in Section 5.\nar X\niv :1\n51 1.\n06 07\n2v 1\n[ cs\n.L G\n] 1\n9 N\nov 2\n01 5"
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "A great deal of literature on deep learning and in particular on DCN has been published in recent years. Image classification with deep learning has considerably profited from the availability of large datasets (in terms of both number of classes and images). In this paper, our choice falls on the popular ImageNet Russakovsky et al. (2014) dataset. Both a 22K classes and a 1K classes version exist of this dataset, we evaluate our system on the latter.\nAlexNet by Krizhevsky et al. Krizhevsky et al. (2012) achieved a significant increase in classification performance over traditional methods on ImageNet, i.e., an top-1 accuracy of 62.5%, showing the potential of deep learning for large datasets such as ImageNet. Their success is arguably founded on three factors: Depth, Rectified Linear Units (ReLU) as non-linearity and data augmentations. We use CaffeNet, a variation of this model, as a baseline to evaluate our system.\nA hierarchical architecture for incremental learning was presented in Xiao et al. (2014). The authors suggested to use a Branching layer to determine the superclass a specific problem belongs to. Following this, a leaf model provided fine-grained classification. We follow their work insofar that we also propose to use experts (leaf models) for fine-grained classification. However, we avoid branching as it introduces a conditional error that cannot be recovered. Branching error also poses a scalability issue, as it grows with the number of superclasses. Finally, the prediction process in their work has large redundancy: Full models are used for both branching prediction and fine-grained prediction.\nMixture of experts (MoE), as first proposed in Jacobs et al. (1991), have been well-known for a while. A large deal of work has been done in this area and an overview can be found in Yuksel et al. (2012). Generally, a number N of expert networks are trained together on a dataset. The experts learn discriminant features and thereby improve the overall accuracy of the system. While this can be used to improve classification results, the drawback lies in the parallel execution of all N experts, which results in longer runtime. In contrast to our proposed method, experts are trained on the full\ndataset and have a complete classifier. This is disadvantageous for incremental learning, a scenario in which new classes are added over time, as all experts need to be retrained on the new data.\nThe work of Yosinski et al. (2014) discusses generality and specialization of convolutional features, giving insight into how deep learning works. The same phenomenon has also been discussed in Zeiler & Fergus (2013) in the context of network visualization. In particular, both works show that the first few layers of a DCN produce general Gabor-like features, i.e., lines and blobs. It is only in the higher layers that these features become more class-specific. Inspired by this fact, instead of N times producing nearly identical general features, we can compute a single set of these features."
    }, {
      "heading" : "3 PROPOSED METHOD",
      "text" : "When tackling a problem, a divide-and-conquer approach can often be helpful: In deep learning, we can train experts on small problems such that the accuracy improves. This has been used in MoE Jacobs et al. (1991): Experts are trained competitively in the hope that they automatically learn discriminant features. A gating network combines the prediction results.\nEach expert’s area of expertise can also be designed with help of prior knowledge. We can utilize two methods to gain this knowledge: Spectral Clustering (as done in Xiao et al. (2014)) and explicit hierarchies (such as the ImageNet dataset provides), see also Sec. 4.1. In both cases the i-th expert is then trained exclusively on the subset of data in that superclass, which we denote as Ci."
    }, {
      "heading" : "3.1 SIMPLE BRANCHING MODEL",
      "text" : "We briefly discuss a branching network as in Xiao et al. (2014), which predicts the superclass of an image. The branching network is a DCN such as AlexNet Krizhevsky et al. (2012) with the number of outputs in the classifier reduced to N , the number of superclasses. In the branching decision, the expert corresponding to the highest activation is selected to then obtain a fine-grained prediction.\nWe trained such a system with two slim1 experts, as seen in Table 1. Even though we used two experts only, classification accuracy suffered from large branching errors: Should the wrong expert be chosen, the error cannot be recovered. Nevertheless, we also see that single experts show superior performance, confirming that we are on the right track. The model we discuss in the following is an attempt at reducing branching errors."
    }, {
      "heading" : "3.2 BRANCHED EXPERTS WITH EARLY STOPPING",
      "text" : "We propose the following framework as depicted in Fig. 1: The input is passed through a number of convolutional layers to generate general features, as defined in Yosinski et al. (2014). These layers are shared between all experts and could encompass the first two or three layers in the case of CaffeNet. Higher layer features have a higher degree of specialization, and therefore need to be finetuned for each expert. Parallel execution of these layers is costly, however, in particular for large\n1By reducing the number of neurons of both layers FC6 and FC7 to 512, in the hope to keep parameter complexity low.\nN . To this end we propose a confidence module, which determines the confidence of whether the expert is able to solve the problem. In particular, an expert i is deemed to have low confidence (in layer j) if for its given score sji and a threshold T :\nmax k 6=i\n( sjk ) − sji ≥ T, 1 ≤ i, k ≤ N (1)\nIn the following, we implement the confidence module simply as a fully connected layer with N outputs, trained on the complete dataset with the original labels replaced by a superclass label lCi ∈ [0, N − 1]. Note the placement of the confidence module: While lower layers have too general features to compute a reliable score, early stopping in higher layers diminishes the resource saving effect. We suggest an intermediate position such as after Conv4.\nLet ujCi be the activation of the confidence module in layer j of the expert on Ci. The appropriate score is then simply the i-th component of the activation vector:\nsji = u j Ci,i\nTo support our decision for a simple threshold, we run a simple branching network (as described above) on a test set, and measure the magnitude of the left-hand side of Equation (1). We then note whether the branching decision is correct or not. As is to be expected, bad branching decisions occur when this magnitude is small, while correct decisions often have higher confidence, see Table 2."
    }, {
      "heading" : "3.3 MEDIATOR",
      "text" : "Given that the inequality in (1) holds, an expert is deemed to have low confidence and stopped. His corresponding activations in the final layer are set to zero. So far, the system described above performs slightly worse than a single model with same configuration. This is due to those instances, where more than one expert is active: An expert trained on a different superclass could “mistake” the input for a particular fine-grained class of his own domain, leading to conflicting opinions. In these cases, we found it helpful to add a mediator, that is, a slim model trained on all 1000 classes to arbitrate opinions, see Figure 2. The resulting architecture then outperforms the single model baseline. Finally, the softmax probabilities of each expert are weighted and averaged: The slim 1000-classes mediator is only added when more than one expert is executed and weighted with wMed. We set wMed = 0.6 if executed or wMed = 0 otherwise. Experts are weighted according to their confidence scores normalized to [0, 1] and subsequently scaled by (1− wMed).\nOur framework can also be used in the context of incremental learning. To add a new superclass CN+1, we simply train a single expert on the new data. Following this, the confidence modules of other experts need to be fine-tuned to accommodate for the new superclass. We stress that this is significantly faster than training a full network, as the confidence module only encompasses a single layer with N outputs, rendering it very shallow and slim. In our experiments, 3 epochs were found to be sufficient. In order to update the mediator network, we can simply add extra neurons to the output and finetune the network, see the Flat Increment technique in Xiao et al. (2014)."
    }, {
      "heading" : "4 EVALUATION",
      "text" : "The evaluation is performed on the ImageNet 1K dataset. We begin by describing how the superclasses corresponding to each expert are formed."
    }, {
      "heading" : "4.1 SUPERCLASS CONSTRUCTION",
      "text" : "Superclasses can be defined in an automated or manual fashion: In the former case Spectral Clustering, as was done in Xiao et al. (2014), is a suitable choice. Alternatively, we can traverse the hierarchy available with ImageNet and join several leaf classes that are conceptually related. This provides a convenient advantage in an incremental learning scenario: We simply train one or more new experts for new data. Contrary, when using Spectral Clustering, it may be necessary to retrain some of the existing experts as well.\nDue to space limitations, we limit ourselves to manually defined superclasses and N = 2. We choose C1 as “artifact” (with synset: n00021939) and C2 as all remaining classes, resulting in a split of 517 vs. 483 leaf classes with roughly 660K and 620K images each. In other words, this simulates adding “artifact” data to an existing model. In the following, we refer to this as the Hierarchical set."
    }, {
      "heading" : "4.2 RESULTS ON HIERARCHICAL SET",
      "text" : "Network configuration. We evaluate our system on three configurations: First, a slim configuration of CaffeNet, where both FC layers are reduced to 512 neurons. In our second configuration we drop layer FC7 altogether, while reducing FC6 to 512 neurons. Finally, our third configuration uses the default CaffeNet configuration with both fully connected layers containing 4096 neurons. All configurations were finetuned from the CaffeNet model available with Caffe, see CaffeNet (2014).\nSystem accuracy. MMoE improves the top-1 classification accuracy in all three configurations. The result is strongly dependent on the value of threshold T , see Figure 3. Classification accuracy peaks at T = 6, for which our system outperforms the baseline over 2.8% and 2.7%2 (in the slim and default configuration respectively).\nEarly stopping. Experts are rarely stopped falsely, see Figure 4. For T ≥ 3, a true expert is incorrectly stopped in less than 1% of all cases, an improvement over the branching error discussed in Section 3. Table 3 shows the probability of how often an expert is stopped for all configurations.\nMediator impact. To show the importance of the Mediator, we train a slim configuration lacking mediation. As such, the system only reaches a top-1 accuracy of 50.52%, about 3.1% lower than the same configuration with mediator.\nComplexity vs. Threshold. The choice of T also influences how often an expert is run. We show this by evaluating the average number of parameters that need to be loaded, see Figure 3. For T = 4, the expected number of parameters is close to the number of parameters when using traditional Mixture-of-Experts, that is, the complexity of N experts alone. The number of computational operations, dominated by the Convolutional layers, can be significantly reduced by layer sharing as discussed earlier.\nImpact of sharing layers. We also tested how sharing different numbers of convolutional layers affects performance. To achieve this, we finetuned the experts from the mediator, while keeping the lowest k convolutional layers frozen, i.e., setting the learning rate to 0. We show our results for all three aforementioned configurations in Figure 5. Performance drops with larger number of shared layers, as is to be expected. The default configuration, having 4096 neurons in each fully connected layer, shows smaller impact: Accuracy drops by two percent going from no shared to fully shared layers, leading us to the assumption that the fully connected layers are able to mitigate the impact. The other two slim configurations however suffer a loss of more than 5% when sharing all convolutional layers."
    }, {
      "heading" : "5 CONCLUSION AND FUTURE WORK",
      "text" : "We propose a mediated expert system for deep convolutional networks that enables experts to learn on small partitions of a training set, a case given in an incremental learning scenario. In detail, experts can be stopped early, where the stopping point is controlled by a single hyper-parameter, allowing to adapt to different circumstances in terms of availability of resources. Furthermore, we avoid the branching error that occurs in prior work when training on partitioned datasets.\nIn order to better underline the benefits of our proposal, two points are left for our future work. First, a more thorough evaluation is necessary that was not possible in the scope of this paper. Second, we believe that the mediator concept can be developed further in order to reduce computational complexity to a higher degree.\n2Using caffe’s test command, we measured CaffeNet’s top-1 accuracy at 55.84% on the validation set, the reported accuracy is 57.4% however."
    } ],
    "references" : [ {
      "title" : "Handwritten digit recognition with a back-propagation network",
      "author" : [ "Cun", "Le", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Cun et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Cun et al\\.",
      "year" : 1990
    }, {
      "title" : "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification",
      "author" : [ "K. He", "X. Zhang", "S. Ren", "J. Sun" ],
      "venue" : null,
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Adaptive mixtures of local experts",
      "author" : [ "Jacobs", "Robert A", "Jordan", "Michael I", "Nowlan", "Steven J", "Hinton", "Geoffrey E" ],
      "venue" : "Neural Comput.,",
      "citeRegEx" : "Jacobs et al\\.,? \\Q1991\\E",
      "shortCiteRegEx" : "Jacobs et al\\.",
      "year" : 1991
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Error-driven incremental learning in deep convolutional neural network for large-scale image classification",
      "author" : [ "Xiao", "Tianjun", "Zhang", "Jiaxing", "Yang", "Kuiyuan", "Peng", "Yuxin", "Zheng" ],
      "venue" : "In Proceedings of the ACM International Conference on Multimedia, MM",
      "citeRegEx" : "Xiao et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Xiao et al\\.",
      "year" : 2014
    }, {
      "title" : "How transferable are features in deep neural networks",
      "author" : [ "Yosinski", "Jason", "Clune", "Jeff", "Bengio", "Yoshua", "Lipson", "Hod" ],
      "venue" : "CoRR, abs/1411.1792,",
      "citeRegEx" : "Yosinski et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Yosinski et al\\.",
      "year" : 2014
    }, {
      "title" : "Visualizing and understanding convolutional networks",
      "author" : [ "Matthew D", "Fergus", "Rob" ],
      "venue" : null,
      "citeRegEx" : "Zeiler et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Zeiler et al\\.",
      "year" : 2002
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "While the application of early DCN was limited to simple tasks such as hand-written digit recognition Cun et al. (1990), modern state-of-the-art methods can meet or exceed human-level performance on far more complex tasks, including generic image classification He et al.",
      "startOffset" : 102,
      "endOffset" : 120
    }, {
      "referenceID" : 0,
      "context" : "While the application of early DCN was limited to simple tasks such as hand-written digit recognition Cun et al. (1990), modern state-of-the-art methods can meet or exceed human-level performance on far more complex tasks, including generic image classification He et al. (2015); Krizhevsky et al.",
      "startOffset" : 102,
      "endOffset" : 279
    }, {
      "referenceID" : 0,
      "context" : "While the application of early DCN was limited to simple tasks such as hand-written digit recognition Cun et al. (1990), modern state-of-the-art methods can meet or exceed human-level performance on far more complex tasks, including generic image classification He et al. (2015); Krizhevsky et al. (2012); Szegedy et al.",
      "startOffset" : 102,
      "endOffset" : 305
    }, {
      "referenceID" : 0,
      "context" : "While the application of early DCN was limited to simple tasks such as hand-written digit recognition Cun et al. (1990), modern state-of-the-art methods can meet or exceed human-level performance on far more complex tasks, including generic image classification He et al. (2015); Krizhevsky et al. (2012); Szegedy et al. (2014). Two factors that have contributed to this development are the availability of large-scale image corpora like ImageNet Russakovsky et al.",
      "startOffset" : 102,
      "endOffset" : 328
    }, {
      "referenceID" : 0,
      "context" : "While the application of early DCN was limited to simple tasks such as hand-written digit recognition Cun et al. (1990), modern state-of-the-art methods can meet or exceed human-level performance on far more complex tasks, including generic image classification He et al. (2015); Krizhevsky et al. (2012); Szegedy et al. (2014). Two factors that have contributed to this development are the availability of large-scale image corpora like ImageNet Russakovsky et al. (2014), and the wide-spread availability of GPU-based computing hardware which renders computationally expensive model training procedures feasible.",
      "startOffset" : 102,
      "endOffset" : 473
    }, {
      "referenceID" : 0,
      "context" : "While the application of early DCN was limited to simple tasks such as hand-written digit recognition Cun et al. (1990), modern state-of-the-art methods can meet or exceed human-level performance on far more complex tasks, including generic image classification He et al. (2015); Krizhevsky et al. (2012); Szegedy et al. (2014). Two factors that have contributed to this development are the availability of large-scale image corpora like ImageNet Russakovsky et al. (2014), and the wide-spread availability of GPU-based computing hardware which renders computationally expensive model training procedures feasible. Experts systems are known to improve the classification accuracy of neural networks even further and have been studied extensively. Previous work on Mixture of Experts (MoE) Jacobs et al. (1991) is however flawed in two aspects: First, computational complexity in MoE is a multiple of that of a traditional model.",
      "startOffset" : 102,
      "endOffset" : 810
    }, {
      "referenceID" : 0,
      "context" : "While the application of early DCN was limited to simple tasks such as hand-written digit recognition Cun et al. (1990), modern state-of-the-art methods can meet or exceed human-level performance on far more complex tasks, including generic image classification He et al. (2015); Krizhevsky et al. (2012); Szegedy et al. (2014). Two factors that have contributed to this development are the availability of large-scale image corpora like ImageNet Russakovsky et al. (2014), and the wide-spread availability of GPU-based computing hardware which renders computationally expensive model training procedures feasible. Experts systems are known to improve the classification accuracy of neural networks even further and have been studied extensively. Previous work on Mixture of Experts (MoE) Jacobs et al. (1991) is however flawed in two aspects: First, computational complexity in MoE is a multiple of that of a traditional model. To this end, we propose early-stopping of experts. Our mechanism evaluates each expert’s confidence and is influenced by a hyper-parameter, allowing a trade-off between complexity and classification accuracy without the need for retraining. Second, as recently pointed out in Xiao et al. (2014), training becomes more difficult as datasets are getting larger.",
      "startOffset" : 102,
      "endOffset" : 1224
    }, {
      "referenceID" : 2,
      "context" : "AlexNet by Krizhevsky et al. Krizhevsky et al. (2012) achieved a significant increase in classification performance over traditional methods on ImageNet, i.",
      "startOffset" : 11,
      "endOffset" : 54
    }, {
      "referenceID" : 2,
      "context" : "AlexNet by Krizhevsky et al. Krizhevsky et al. (2012) achieved a significant increase in classification performance over traditional methods on ImageNet, i.e., an top-1 accuracy of 62.5%, showing the potential of deep learning for large datasets such as ImageNet. Their success is arguably founded on three factors: Depth, Rectified Linear Units (ReLU) as non-linearity and data augmentations. We use CaffeNet, a variation of this model, as a baseline to evaluate our system. A hierarchical architecture for incremental learning was presented in Xiao et al. (2014). The authors suggested to use a Branching layer to determine the superclass a specific problem belongs to.",
      "startOffset" : 11,
      "endOffset" : 565
    }, {
      "referenceID" : 2,
      "context" : "Mixture of experts (MoE), as first proposed in Jacobs et al. (1991), have been well-known for a while.",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 2,
      "context" : "Mixture of experts (MoE), as first proposed in Jacobs et al. (1991), have been well-known for a while. A large deal of work has been done in this area and an overview can be found in Yuksel et al. (2012). Generally, a number N of expert networks are trained together on a dataset.",
      "startOffset" : 47,
      "endOffset" : 204
    }, {
      "referenceID" : 3,
      "context" : "151% – – Baseline Krizhevsky et al. (2012) 49.",
      "startOffset" : 18,
      "endOffset" : 43
    }, {
      "referenceID" : 5,
      "context" : "The work of Yosinski et al. (2014) discusses generality and specialization of convolutional features, giving insight into how deep learning works.",
      "startOffset" : 12,
      "endOffset" : 35
    }, {
      "referenceID" : 5,
      "context" : "The work of Yosinski et al. (2014) discusses generality and specialization of convolutional features, giving insight into how deep learning works. The same phenomenon has also been discussed in Zeiler & Fergus (2013) in the context of network visualization.",
      "startOffset" : 12,
      "endOffset" : 217
    }, {
      "referenceID" : 2,
      "context" : "This has been used in MoE Jacobs et al. (1991): Experts are trained competitively in the hope that they automatically learn discriminant features.",
      "startOffset" : 26,
      "endOffset" : 47
    }, {
      "referenceID" : 2,
      "context" : "This has been used in MoE Jacobs et al. (1991): Experts are trained competitively in the hope that they automatically learn discriminant features. A gating network combines the prediction results. Each expert’s area of expertise can also be designed with help of prior knowledge. We can utilize two methods to gain this knowledge: Spectral Clustering (as done in Xiao et al. (2014)) and explicit hierarchies (such as the ImageNet dataset provides), see also Sec.",
      "startOffset" : 26,
      "endOffset" : 382
    }, {
      "referenceID" : 3,
      "context" : "We briefly discuss a branching network as in Xiao et al. (2014), which predicts the superclass of an image.",
      "startOffset" : 45,
      "endOffset" : 64
    }, {
      "referenceID" : 3,
      "context" : "The branching network is a DCN such as AlexNet Krizhevsky et al. (2012) with the number of outputs in the classifier reduced to N , the number of superclasses.",
      "startOffset" : 47,
      "endOffset" : 72
    }, {
      "referenceID" : 5,
      "context" : "1: The input is passed through a number of convolutional layers to generate general features, as defined in Yosinski et al. (2014). These layers are shared between all experts and could encompass the first two or three layers in the case of CaffeNet.",
      "startOffset" : 108,
      "endOffset" : 131
    }, {
      "referenceID" : 4,
      "context" : "In order to update the mediator network, we can simply add extra neurons to the output and finetune the network, see the Flat Increment technique in Xiao et al. (2014).",
      "startOffset" : 149,
      "endOffset" : 168
    }, {
      "referenceID" : 4,
      "context" : "Superclasses can be defined in an automated or manual fashion: In the former case Spectral Clustering, as was done in Xiao et al. (2014), is a suitable choice.",
      "startOffset" : 118,
      "endOffset" : 137
    } ],
    "year" : 2015,
    "abstractText" : "We present a new supervised architecture termed Mediated Mixture-of-Experts (MMoE) that allows us to improve classification accuracy of Deep Convolutional Networks (DCN). Our architecture achieves this with the help of expert networks: A network is trained on a disjoint subset of a given dataset and then run in parallel to other experts during deployment. A mediator is employed if experts contradict each other. This allows our framework to naturally support incremental learning, as adding new classes requires (re-)training of the new expert only. We also propose two measures to control computational complexity: An early-stopping mechanism halts experts that have low confidence in their prediction. The system allows to trade-off accuracy and complexity without further retraining. We also suggest to share low-level convolutional layers between experts in an effort to avoid computation of a near-duplicate feature set. We evaluate our system on a popular dataset and report improved accuracy compared to a single model of same configuration.",
    "creator" : "LaTeX with hyperref package"
  }
}