{
  "name" : "1604.04315.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Moving Beyond the Turing Test with the Allen AI Science Challenge",
    "authors" : [ "Carissa Schoenick", "Peter Clark", "Oyvind Tafjord", "Peter Turney", "Oren Etzioni" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "Moving Beyond the Turing Test with the Allen AI Science Challenge",
      "text" : ""
    }, {
      "heading" : "Carissa Schoenick, Peter Clark, Oyvind Tafjord, Peter Turney, Oren Etzioni",
      "text" : "Given recent successes in AI (e.g., AlphaGo’s victory against Lee Sedol in the game of GO), it’s become increasingly important to assess: how close are AI systems to human-level intelligence? This paper describes the Allen AI Science Challenge—an approach towards that goal which led to a unique Kaggle Competition, its results, the lessons learned, and our next steps.\nMeasuring Artificial Intelligence The famous Turing test developed by Alan Turing in 1950 proposes that if a system can exhibit question-answering behavior that is indistinguishable from that of a human during a conversation, that system could be considered intelligent. As the field of artificial intelligence grows, this approach to evaluating a system has become less and less appropriate or meaningful. Current systems have revealed just how gameable this assessment of AI can be, as some chatbots have improved in recent years to the point where one could argue a few of them could pass the Turing test [1][2]. As The New York Times’ John Markoff puts it, “the Turing test is a test of human gullibility.” It is difficult to define intelligence, but it is certainly more than the ability to trick a human. Intelligence involves reasoning, creativity, insight, general-purpose strategies, deduction, induction, and the ability to explain why an answer is correct, rather than producing unexplained answers like an oracle. Other popular assessments of the skills of an AI system also fall short in demonstrating this aspect of AI by focusing on narrow problems that are solvable by combining raw computing power with elaborate algorithms over huge amounts of data. IBM’s Watson demonstrated excellent performance in the context of a Jeopardy! game, but behind the answers you won’t find reasoning or comprehension; natural language processing over massive corpora, coupled with statistical training and some specific, purpose-built strategies for accommodating the many quirks of Jeopardy!-style questions and puns produces a system that can perform marvelously on this specific task. Google’s AlphaGo recently performed the impressive feat of defeating the world champion Go player using several advanced techniques, but like Watson, this system is also a highly specific demonstration of a powerful algorithm, not necessarily an expression of intelligence. Neither Watson nor AlphaGo could engage with a human in dialog, or even give that human an explanation for the success of their own design. So if the Turing test is readily subverted, and the game-playing approach is limited, what other, richer ways can we use to successfully measure the progress of AI technology as it continues to expand and evolve? Rather than a single test, cognitive scientist Gary Marcus of NYU and others recently proposed the notion of series of tests, a Turing Olympics of sorts, that could assess the full gamut of AI from robotics to NLP [3][4]. Among the assessments proposed, Peter Clark and Oren Etzioni from the Allen Institute of Artificial Intelligence (AI2) are advocating a more multifaceted, meaningful approach: give an AI system a standardized test, such as a science exam [5]. Peter Clark, who leads AI2’s Aristo Project, states that “unlike the Turing Test,\nstandardized tests provide us with a way to measure a machine’s current ability to reason about basic scientific concepts, to understand questions, and to compare its abilities with that of a human.” AI2 considers standardized tests such a useful challenge problem for AI because they require significant advances in AI technology while also being accessible, measurable, understandable, and compelling.\nAI vs 8th Grade: The Allen AI Science Challenge To put this approach to the test, AI2 designed and hosted “The Allen AI Science Challenge,” a four month long competition in partnership with Kaggle.com that concluded in February of 2016. Researchers worldwide were invited to build AI software that could answer standard 8th grade multiple choice science questions. The competition aimed to assess the state of the art in AI systems utilizing natural language understanding and knowledge-based reasoning--how accurately the participants’ models could answer the exam questions would serve as an indicator of how far the field has come in these areas. According to AI2’s Oren Etzioni, “The Allen AI Science Challenge is an important step towards a rational, quantitative assessment of AI’s capabilities, and how these progress over time.” One of the most interesting and appealing aspects of science exams is their graduated and multi-faceted nature: different questions explore different types of knowledge, and they vary substantially in difficulty (especially for a computer). There are questions that can be easily addressed with a simple fact lookup, like this one:\nHow many chromosomes does the human body cell contain? (A) 23 (B) 32 (C) 46 (D) 64\nAnd then there are questions requiring extensive understanding of the world, such as this example:\nCity administrators can encourage energy conservation by (A) lowering parking fees (B) building larger parking lots (C) decreasing the cost of gasoline (D) lowering the cost of bus and subway fares\nThis question requires the knowledge that certain activities and incentives result in human behaviors, which in turn result in more or less energy being consumed. Understanding this question also requires recognizing that “energy” in this context refers to resource consumption for the purposes of transportation (as opposed to other forms of energy one might find in a science exam, like electrical, kinetic/potential, etc.).\nThe final test set for the competition contained total of 2,283 8th grade multiple choice science questions. The three teams that achieved the highest scores on the challenge’s test set received prizes of $50,000, $20,000, and $10,000 respectively. Participants had to design and finalize their models on a smaller validation set of questions before the large final set was released and scores were newly calculated. The final outcome was quite close, with the top three teams achieving scores with a spread of only 1.05%.\nFirst Place Top prize went to Chaim Linhart of Israel (Kaggle username Cardal). His model achieved a final score of 59.31% on the test question set using a combination of 15 gradient boosting models, each of which used a different subset of features. Unlike the other winners’ models, Chaim’s model predicts the correctness of each answer option individually. There were two general categories of features used to make these predictions; the first category was made up of information retrieval (IR) based features, applied by searching over corpora he compiled from various sources such as study guide or quiz building websites, open source textbooks, and wikipedia. His searches used various weightings and stemmings to optimize performance. The other flavor of feature used in his ensemble of 15 models was based on properties of the questions themselves, such as the length of the question and answer, the form of the answers (e.g., characteristics like numeric answer options, answers that contained referential clauses like “none of the above” as an option), and the relationships between answer options. Chaim explained that he used several smaller gradient boosting models instead of one big model in order to maximize diversity. One big model tends to ignore some important features because it requires a very large training set to require it to pay attention to all of the potentially useful features present--using several small models requires that the learning algorithm to use features that it would otherwise ignore, given the more limited training data available in this competition. The IR-based features alone could achieve scores as high as 55% by Chaim’s estimation. His question-form features fill in some remaining gaps to bring the system up to about 60% correct. The 15 models were combined by a simple weighted average to yield the final score for each choice. Chaim credited careful corpus selection as one of the primary elements driving the success of his model.\nSecond Place The second place team with a score of 58.34% was a group of people from a social media analytics company based in Luxembourg called Talkwalker, led by Benedikt Wilbertz (Kaggle username poweredByTalkwalker). Benedikt’s team built a relatively large corpus as compared to other winning models, which used 180GB of disk space after indexing with Lucene. They utilized several feature types, including IR-based features using their large corpus, vector-based features (scoring question-answer similarity by comparing vectors from word2vec and Glove), pointwise mutual information (PMI) features (measured between the question and target answer, calculated on their large corpus), and string hashing features in which term-definition pairs were hashed and then a supervised\nlearner was trained to classify pairs as correct or incorrect. A final model uses these various features to learn pairwise ranking between the answer options using the XGBoost gradient boosting library. The use of string hashing features by the poweredByTalkwalker team is unique; this methodology was not tried by either of the other two competition winners, nor is it used in AI2’s Project Aristo. The team used a corpus of terms and definitions obtained from an educational flashcard building site, and then created negative examples by mixing terms with random definitions. A supervised classifier was trained on these incorrect pairs, and then the output was used to generate features for input to XGBoost.\nThird Place The third place winner was Alejandro Mosquera from the UK (Kaggle username Alejandro Mosquera), with a score of 58.26%. Alejandro approached the challenge as a three-way classification problem for each pair of answer options (i.e., first pair element is correct, second is correct, neither is correct), and utilized logistic regression to select the top candidate answer for each question. His learning objective was classification accuracy. He made use of three types of features: IR-based features based on scores from Elastic Search using Lucene over a corpus, vector-based features that measured question-answer similarity by comparing vectors from word2vec, and question-form features that considered things such as the structure of a question, the length of the question and the answer choices. Alejandro also noted that careful corpus selection was crucial to his model’s success.\nCompetition Lessons In the end, each of the winning models found the most benefit in information retrieval based methods. This is indicative of the state of AI technology in this area of research; we can’t ace an 8th grade science exam because we do not currently have AI systems capable of going beyond the surface text to a deeper understanding of the meaning underlying each question, and then successfully using reasoning to find the appropriate answer. All three winners expressed that it was clear that applying a deeper, semantic level of reasoning with scientific knowledge to the questions and answers would be the key to achieving scores of 80% and beyond, and to demonstrating what might be considered true artificial intelligence. A few other example questions from the competition that each of the top three models got wrong highlight the the more interesting, complex nuances of language and chains of reasoning an AI system will need handle in order to answer these questions correctly, and for which IR methods aren’t sufficient:\nWhat do earthquakes tell scientists about the history of the planet? (A) Earth's climate is constantly changing. (B) The continents of Earth are continually moving. (C) Dinosaurs became extinct about 65 million years ago. (D) The oceans are much deeper today than millions years ago.\nThis question digs into the causes behind earthquakes and the larger geographic phenomena of plate tectonics, and cannot be easily solved by looking up a single fact. Additionally, other true facts appear in the answer options (“Dinosaurs became extinct about 65 million years ago.”), but must be intentionally identified and discounted as being incorrect in the context of the question.\nWhich statement correctly describes a relationship between the distance from Earth and a characteristic of a star? (A) As the distance from Earth to the star decreases, its size increases. (B) As the distance from Earth to the star increases, its size decreases. (C) As the distance from Earth to the star decreases, its apparent brightness increases. (D) As the distance from Earth to the star increases, its apparent brightness increases.\nThis question requires general common-sense type knowledge of the physics of distance and perception, as well as the semantic ability to relate one statement to another within each answer option to find the right directional relationship.\nOther Attempts While there are numerous question-answering systems that have emerged from the AI community, none address the challenges of scientific and commonsense reasoning exhibited by the example questions above. Question-answering systems developed for the MUC (message understanding) conferences [6] and TREC (text retrieval) conferences [7] focused on retrieving answers from text, the former from newswire articles and the latter from various large corpora such as the Web, microblogs, and clinical data. More recent work has focused on answer retrieval from structured data, e.g., \"In which city was Bill Clinton born?\" from FreeBase [8,9,10]. These systems rely on the information being stated explicitly in the underlying data, however, and are unable to perform the reasoning steps that would be required to conclude this information from indirect supporting evidence. There are a few systems that attempt some form of reasoning: Wolfram Alpha [11] answers mathematical questions, providing they are stated either equationally or with relatively simple English; Evi [12] is able to combine facts together to answer simple questions (e.g., Who is older, Barack or Michelle Obama?); and START [13] will similarly answer simple inference questions using Web-based databases (e.g., What South-American country has the largest population?). However, none of these systems attempt the level of complex question processing and reasoning that will be required to successfully answer many of the science questions in the Allen AI Challenge.\nLooking Forward Project Aristo at AI2 is intently focused on this problem of successfully demonstrating artificial intelligence using standardized science exams. As the 2015 Allen AI Science Challenge helps to clearly illustrate, achieving a high score on a science exam is going to require a system that can do more than merely sophisticated information retrieval. Beyond text-only multiple choice\nscience questions, Aristo is also tackling questions with free-response answers (e.g., “Explain two ways in which plants can disperse their seeds”), as well as questions with associated diagrams that require computer vision. For a demonstration of Aristo’s early work on diagram interpretation as well as selecting correct, explainable answers to science questions, visit Aristo’s demo page at aristo-demo.allenai.org. More information about the project can be found at allenai.org/aristo.\n2016 Allen AI Challenge AI2 is planning to continue to engage the wider community with the interesting problem of science question answering with AI. In 2016, AI2 plans to launch a new, $1 million challenge, inviting the wider world to take the next big steps in AI with us as we work to move beyond information retrieval and into intelligent inferencing over multiple facts, concepts, and relationships to produce correct, explainable answers to these questions. Follow AI2 on Twitter @allenai_org to keep up to date with our plans for the next big challenge.\nReferences [1] BBC. Computer AI passes Turing test in 'world first' Turing Test. BBC News. 9 June 2014. http://www.bbc.com/news/technology-27762088 [2] Aron, J. Software tricks people into thinking it is human. New Scientist (Issue 2829), Sept 2011. [3] Marcus, G., Rossi, F., Veloso, M. (Eds), Beyond the Turing Test (AI Magazine Special Edition), AI Magazine, 37 (1), Spring 2016. [4] Turk, V. The Plan to Replace the Turing Test with a ‘Turing Olympics’. Motherboard. 28 January 2015. http://motherboard.vice.com/read/the-plan-to-replace-the-turing-test-with-a-turing-olympics [5] Clark, P., Etzioni, O. My Computer is an Honor Student - But how Intelligent is it? Standardized Tests as a Measure of AI. In AI Magazine 37 (1), Spring 2016. [6] Grishman, R., Sundheim, B. Message Understanding Conference-6: A Brief History. In COLING (Vol. 96, pp. 466-471), 1996. [7] Voorhees, E., Ellis, A. (Eds)Proc. 24th Text REtrieval Conference (TREC 2015), Publication SP 500- 319, NIST (http://trec.nist.gov/ ), 2015. [8] Yao, X., Van Durme, B. Information Extraction over Structured Data: Question Answering with Freebase. In ACL (1) (pp. 956-966), 2014. [9] Berant, J., Chou, A., Frostig, R., Liang, P. Semantic Parsing on Freebase from Question-Answer Pairs. In EMNLP (Vol. 2, No. 5, p. 6), 2013. [10] Fader, A., Zettlemoyer, L., & Etzioni, O. Open question answering over curated and extracted\nknowledge bases. In Proc 20th ACM SIGKDD Int Conf on Knowledge Discovery and Data Mining (pp. 1156-1165). ACM, 2014.\n[11] Wolfram, S. Making the World's Data Computable. Proc. Wolfram Data Summit, 2010. (http://blog.stephenwolfram.com/2010/09/making-the-worlds-data-computable/ ) [12] Simmons, J. True Knowledge: The Natural Language Question Answering Wikipedia for Facts. In: Semantic Focus, Feb 2008. [13] Katz, B., Borchardt, G., Felshin, S. Natural Language Annotations for Question Answering. Proc 19th Int FLAIRS Conference (FLAIRS 2006), 2006. (http://start.csail.mit.edu) [14] Sahan, M., Dubey, A., Xing, E. Science Question Answering using Instructional Materials. arXiv preprint at arXiv:1602.04375 http://arxiv.org/pdf/1602.04375.pdf"
    } ],
    "references" : [ {
      "title" : "Software tricks people into thinking it is human",
      "author" : [ "J. Aron" ],
      "venue" : "New Scientist (Issue",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2011
    }, {
      "title" : "The Plan to Replace the Turing Test with a ‘Turing Olympics",
      "author" : [ "V. Turk" ],
      "venue" : "Motherboard",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2015
    }, {
      "title" : "My Computer is an Honor Student - But how Intelligent is it? Standardized Tests as a Measure of AI",
      "author" : [ "P. Clark", "O. Etzioni" ],
      "venue" : "In AI Magazine",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2016
    }, {
      "title" : "Message Understanding Conference-6: A Brief History",
      "author" : [ "R. Grishman", "B. Sundheim" ],
      "venue" : "In COLING (Vol",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1996
    }, {
      "title" : "Eds)Proc. 24th Text REtrieval Conference (TREC 2015), Publication SP 500- 319, NIST (http://trec.nist.gov",
      "author" : [ "E. Voorhees", "A. Ellis" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2015
    }, {
      "title" : "Information Extraction over Structured Data: Question Answering with Freebase",
      "author" : [ "X. Yao", "B. Van Durme" ],
      "venue" : "(pp. 956-966),",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2014
    }, {
      "title" : "Semantic Parsing on Freebase from Question-Answer Pairs",
      "author" : [ "J. Berant", "A. Chou", "R. Frostig", "P. Liang" ],
      "venue" : "In EMNLP (Vol. 2,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2013
    }, {
      "title" : "Open question answering over curated and extracted knowledge bases",
      "author" : [ "A. Fader", "L. Zettlemoyer", "O. Etzioni" ],
      "venue" : "Proc 20th ACM SIGKDD Int Conf on Knowledge Discovery and Data Mining (pp. 1156-1165)",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2014
    }, {
      "title" : "Making the World's Data Computable",
      "author" : [ "S. Wolfram" ],
      "venue" : "Proc. Wolfram Data Summit,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2010
    }, {
      "title" : "True Knowledge: The Natural Language Question Answering Wikipedia for Facts",
      "author" : [ "J. Simmons" ],
      "venue" : "Semantic Focus,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2008
    }, {
      "title" : "Natural Language Annotations for Question Answering",
      "author" : [ "B. Katz", "G. Borchardt", "S. Felshin" ],
      "venue" : "Proc 19th Int FLAIRS Conference (FLAIRS",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Current systems have revealed just how gameable this assessment of AI can be, as some chatbots have improved in recent years to the point where one could argue a few of them could pass the Turing test [1][2].",
      "startOffset" : 204,
      "endOffset" : 207
    }, {
      "referenceID" : 1,
      "context" : "So if the Turing test is readily subverted, and the game-playing approach is limited, what other, richer ways can we use to successfully measure the progress of AI technology as it continues to expand and evolve? Rather than a single test, cognitive scientist Gary Marcus of NYU and others recently proposed the notion of series of tests, a Turing Olympics of sorts, that could assess the full gamut of AI from robotics to NLP [3][4].",
      "startOffset" : 430,
      "endOffset" : 433
    }, {
      "referenceID" : 2,
      "context" : "Among the assessments proposed, Peter Clark and Oren Etzioni from the Allen Institute of Artificial Intelligence (AI2) are advocating a more multifaceted, meaningful approach: give an AI system a standardized test, such as a science exam [5].",
      "startOffset" : 238,
      "endOffset" : 241
    }, {
      "referenceID" : 3,
      "context" : "Question-answering systems developed for the MUC (message understanding) conferences [6] and TREC (text retrieval) conferences [7] focused on retrieving answers from text, the former from newswire articles and the latter from various large corpora such as the Web, microblogs, and clinical data.",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 4,
      "context" : "Question-answering systems developed for the MUC (message understanding) conferences [6] and TREC (text retrieval) conferences [7] focused on retrieving answers from text, the former from newswire articles and the latter from various large corpora such as the Web, microblogs, and clinical data.",
      "startOffset" : 127,
      "endOffset" : 130
    }, {
      "referenceID" : 5,
      "context" : ", \"In which city was Bill Clinton born?\" from FreeBase [8,9,10].",
      "startOffset" : 55,
      "endOffset" : 63
    }, {
      "referenceID" : 6,
      "context" : ", \"In which city was Bill Clinton born?\" from FreeBase [8,9,10].",
      "startOffset" : 55,
      "endOffset" : 63
    }, {
      "referenceID" : 7,
      "context" : ", \"In which city was Bill Clinton born?\" from FreeBase [8,9,10].",
      "startOffset" : 55,
      "endOffset" : 63
    }, {
      "referenceID" : 8,
      "context" : "There are a few systems that attempt some form of reasoning: Wolfram Alpha [11] answers mathematical questions, providing they are stated either equationally or with relatively simple English; Evi [12] is able to combine facts together to answer simple questions (e.",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 9,
      "context" : "There are a few systems that attempt some form of reasoning: Wolfram Alpha [11] answers mathematical questions, providing they are stated either equationally or with relatively simple English; Evi [12] is able to combine facts together to answer simple questions (e.",
      "startOffset" : 197,
      "endOffset" : 201
    }, {
      "referenceID" : 10,
      "context" : ", Who is older, Barack or Michelle Obama?); and START [13] will similarly answer simple inference questions using Web-based databases (e.",
      "startOffset" : 54,
      "endOffset" : 58
    } ],
    "year" : 2016,
    "abstractText" : "The famous Turing test developed by Alan Turing in 1950 proposes that if a system can exhibit question-answering behavior that is indistinguishable from that of a human during a conversation, that system could be considered intelligent. As the field of artificial intelligence grows, this approach to evaluating a system has become less and less appropriate or meaningful. Current systems have revealed just how gameable this assessment of AI can be, as some chatbots have improved in recent years to the point where one could argue a few of them could pass the Turing test [1][2]. As The New York Times’ John Markoff puts it, “the Turing test is a test of human gullibility.”",
    "creator" : "Word"
  }
}