{
  "name" : "1205.0622.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "No-Regret Learning in Extensive-Form Games with Imperfect Recall",
    "authors" : [ "Marc Lanctot", "Richard Gibson", "Neil Burch", "Martin Zinkevich", "Michael Bowling" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "gorithm for decision problems modeled as extensive games. CFR’s regret bounds depend on the requirement of perfect recall: players always remember information that was revealed to them and the order in which it was revealed. In games without perfect recall, however, CFR’s guarantees do not apply. In this paper, we present the first regret bound for CFR when applied to a general class of games with imperfect recall. In addition, we show that CFR applied to any abstraction belonging to our general class results in a regret bound not just for the abstract game, but for the full game as well. We verify our theory and show how imperfect recall can be used to trade a small increase in regret for a significant reduction in memory in three domains: die-roll poker, phantom tic-tac-toe, and Bluff."
    }, {
      "heading" : "1 Introduction",
      "text" : "Many real-world problems can be modeled as a repeated decision-making task. For problems involving multiple agents, one can model the repeated task as a normal-form game. When the task incorporates sequential decisions involving imperfect information or stochastic events, an extensive game is a useful alternative. In such decision problems, a typical goal is to minimize regret: the amount of utility lost by playing a past sequence of strategies, versus playing the best, stationary strategy in hindsight.\nIn this paper, we consider the problem of minimizing regret in an extensive game. A common approach to achieving low regret in extensive games is the Counterfactual Regret Minimization (CFR) [Zinkevich et al., 2008] algorithm. CFR uses a regret minimizer at every decision point with an alternative notion of regret, which provably minimizes regret in the entire extensive game. However, convergence is limited to games exhibiting perfect recall: players never forget information that was revealed to them, nor the order in the which the information was revealed. For games with imperfect recall, CFR’s original analysis provides no general guarantees.\nImperfect recall brings about a number of complications. In games with perfect recall, every mixed strategy (probability distribution over pure strategies) has a utility-equivalent behavioral strategy (probability distribution over actions at each decision point) [Kuhn, 1953]. While certain lossless imperfect recall games share this\nar X\niv :1\n20 5.\n06 22\nv1 [\ncs .G\nT ]\n3 M\nproperty [Kaneko and Kline, 1995], it is not true for imperfect recall games in general [Piccione and Rubinstein, 1996]. In addition, the decision problem of determining if a player can assure themself a certain payoff in an imperfect recall game is NPcomplete [Koller and Megiddo, 1992]. Two-player zero-sum games can be solved by constructing an appropriate linear program [Koller et al., 1994] or minimizing regret [Zinkevich et al., 2008], provided the game has perfect recall. Without perfect recall, however, the problem becomes exponential in the worst case [Koller et al., 1994].\nOn the other hand, imperfect recall extensive games are more versatile than perfect recall games for modelling large real-world problems. While perfect recall requires all past information to be remembered, imperfect recall allows irrelevant information to be forgotten so that the size of the game is smaller. As CFR’s memory requirements are linear in the size of the game, more games become feasible through imperfect recall. Despite the complications above, CFR has empirically been shown to work well when applied to imperfect recall abstractions of Texas Hold’em poker [Waugh et al., 2009b], but there is currently no theory to suggest why this is so.\nThis paper presents theoretical groundings for applying CFR to games exhibiting imperfect recall. We define a general class of imperfect recall games and provide a bound on CFR’s regret in such games. For a subset of this class, CFR minimizes average regret in the extensive game. Moreover, our results also provide regret guarantees when applying CFR to an abstract game, provided the abstract game belongs to our general class. We test our theory in three different domains: die-roll poker, phantom tic-tac-toe, and Bluff. To the best of our knowledge, this work demonstrates the first theoretically-grounded, practical use of imperfect recall in extensive games."
    }, {
      "heading" : "2 Background",
      "text" : "An extensive-form game Γ with imperfect information [Osborne and Rubinstein, 1994] is a tuple 〈N,A,H,Z, P, σc, u, I〉, where N is a finite set of players. A is a finite set of actions. H is a finite set of histories: a subset of the set of sequences of elements in A. A prefix of a history h′ ∈ H is a history h ∈ H where h′ begins with the sequence h; we denote prefix histories by h v h′. For every h ∈ H , define A(h) = {a : a ∈ A, ha ∈ H}, the set of valid actions at history h; P (h) ∈ N ∪ {c} is the player to act at the history h, or chance if P (h) = c; and Hi = {h | h ∈ H,P (h) = i}. Z ⊆ H is the set of terminal histories. A terminal history z ∈ Z is a history where there does not exist any history h ∈ H , h 6= z such that z v h. The utility function ui : Z → R gives the utility to player i ∈ N , for each terminal history. If |N | = 2 and for all z ∈ Z, ∑ i∈N ui(z) = 0, we say the game is zero-sum.\nFor each player i ∈ N , Ii is a partition of Hi with the property that A(h) = A(h′) whenever h and h′ are in the same member of the partition. We call Ii the information partition of player i, and a set I ∈ Ii is an information set of player i. A player, when taking actions, cannot distinguish between two histories in the same information set. For I ∈ Ii we denote A(I) as the set A(h) for any h ∈ I . Define I(h) to be the information set containing h. In this paper, we restrict ourselves to games where players cannot reach the same information set twice in a single game. Thus, we assume\nthat for all i ∈ N and h, h′ ∈ Hi,\nh v h′, h 6= h′ ⇒ I(h) 6= I(h′). (1)\nFinally, σc is the fixed “strategy” of the special player chance. σc(h, a) gives the probability that chance event a occurs at h. For all h ∈ Hc, ∑ a∈A(h) σc(h, a) = 1 and the decisions at any h are independent of the decision at any other h′ 6= h. Given a history h, define Xi(h) to be the sequence of information set, action pairs such that (I, a) ∈ Xi(h) if I ∈ Ii and there exists h′ v h such that h′ ∈ I and h′a v h. The order of the pairs in Xi(h) is the order in which they occur in h. Define X(h) to be the sequence of information set, action pairs belonging to all players in the order in which they occur in h, and X−i(h) similarly, by removing player i’s information set, action pairs from X(h). Also, define X(h, h′) to be the sequence of information set, action pairs belonging to all players that start at h and end at h′ when h v h′; if h 6v h′, X(h, h′) is defined to be the empty sequence. Finally, Xi(h, h′) and X−i(h, h′) are similarly defined.\nDefinition 1. An extensive game has perfect recall if for every player i ∈ N , for every information set I ∈ Ii, for any h, h′ ∈ I : Xi(h) = Xi(h′). Otherwise, the game has imperfect recall.\nIntuitively, with perfect recall every player has an infallible memory: they cannot “forget” anything during a play of the game that they once knew. Hence, what a player knows at I is a composition of what the player has discovered in the past up to this point and the precise order in which information was discovered. Note that every perfect recall game satisfies equation (1), but not every imperfect recall game does.\nA (behavioral) strategy σi for player i is a function such that for each history h ∈ Hi, σi(h) is a probability distribution over A(h). Furthermore, it is required that σi(h) = σi(h\n′) for all h, h′ ∈ I , and we denote that as σi(I). The set of all such strategies for player i is denoted by Σi. A strategy profile σ ∈ Σ is a collection of strategies, one for each player, i.e. in a two-player game σ = (σ1, σ2). By notational convention, σ−i refers to the set of strategies including every strategy in σ except player i’s strategy.\nFor any σ ∈ Σ, i ∈ N ∪{c}, and h ∈ H , define πσi (h) = ∏ h′avh,P (h′)=i σi(h\n′, a) to be the probability that player i plays to reach history h under σ. We can then define πσ(h) = ∏ i∈N∪{c} π σ i (h) to be the probability that history h is reached under σ. Let πσ−i(h) be the product of all players’ contribution (including chance) except that of player i. Furthermore, let πσi (h, h\n′) be the probability of player i playing to reach history h′ after h, given h has occurred. Let πσ(h, h′) and πσ−i(h, h\n′) be defined similarly. Finally, we can define the expected utility of a strategy profile σ for player i to be\nui(σ) = Ez∈Z [ui(z)] = ∑ z∈Z ui(z)π σ(z).\nWe will say that a game Γ′ = 〈N,A′, H, Z, P, σc, u, I ′〉 is an abstraction, or an abstract game, of Γ = 〈N,A,H,Z, P, σc, u, I〉 if for all i ∈ N and h, k ∈ Hi, A′(h) ⊆ A(h) and I(h) = I(k) implies I ′(h) = I ′(k). In this paper, we only consider abstractions where A = A′. A typical use of abstraction is to reduce the size of the game by ensuring that |I ′| < |I|."
    }, {
      "heading" : "3 Example: Die-Roll Poker",
      "text" : "We now introduce a game that we will use as a running example throughout the paper.\nDie-roll poker (DRP) is a simplified two-player poker game that uses dice rather than cards. To begin, each player antes one chip to the pot. There are two betting rounds, where at the beginning of each round, players roll a private six-sided die. The game has imperfect information due to the players not seeing the result of the opponent’s die rolls. During a betting round, a player may fold (forfeit the game), call (match the current bet), or raise (increase the current bet) by a fixed number of chips, with a maximum of two raises per round. In the first round, raises are worth two chips, whereas in the second round, raises are worth four chips. If both players have not folded by the end of the second round, a showdown occurs where the player with the largest sum of their two dice wins all of the chips in the pot.\nDRP is naturally a game with perfect recall; players remember the exact sequence of bets made and the exact outcome of each die roll from both rounds. However, consider an imperfect recall version of DRP, DRP-IR, where at the beginning of the second round, both players “forget” their first die roll and only know the sum of their two dice. In other words, DRP-IR is an abstraction of DRP where any two histories are in the same abstract information set if and only if the sum of the player’s private dice is the same and the sequence of betting is the same. DRP-IR has imperfect recall since histories that were distinguishable in the first round (for example, a roll of 1 and a roll of 4) are no longer distinguishable in the second round (for example, a roll of 1 followed by a roll of 5, and a roll of 4 followed by a roll of 2)."
    }, {
      "heading" : "4 Counterfactual Regret Minimization",
      "text" : "Given a sequence of strategy profiles σ1, σ2, ..., σT , the (external) regret for player i,\nRTi = max σ′∈Σi T∑ t=1 ( ui(σ ′, σt−i)− ui(σti , σt−i) ) ,\nis the amount of utility player i could have gained had she played the best single strategy in hindsight for all time steps t ∈ {1, 2, ..., T}. An algorithm minimizes regret, or is a no-regret algorithm, for player i if the average positive regret approaches zero; i.e., limT→∞R T,+ i /T = 0, where x\n+ = max{x, 0}. Having no regret is a desirable property. For example, it is well known that in a zero-sum game, if both players’ average regret is bounded above by , then the average of the strategy profiles generated is a 2 -Nash equilibrium.\nCounterfactual Regret Minimization (CFR) is an iterative no-regret learning algorithm for extensive-form games having perfect recall. On each iteration t, CFR recursively traverses the entire game tree, computing the expected utility for player i at each information set I ∈ Ii under the current profile σt, assuming player i plays to reach I . This expectation is the counterfactual value for player i,\nvi(σ, I) = ∑ z∈ZI ui(z)π σ −i(z[I])π σ(z[I], z),\nwhere ZI is the set of terminal histories passing through I and z[I] is the prefix of z contained in I (z[I] is unique by equation (1)). For each action a ∈ A(I), these values determine the counterfactual regret at iteration t, rti(I, a) = vi(σtI→a, I)− vi(σt, I), where σI→a is the profile σ except at I , action a is always taken. The regret rti(I, a) measures how much player i would rather play action a at I than play σt. Finally, σt is updated by applying regret matching [Hart and Mas-Colell, 2000, Zinkevich et al., 2008] to the immediate counterfactual regrets,RTi (I, a) = ∑T t=1 r t i(I, a), according to\nσT+1(I, a) = RT,+i (I, a)∑\nb∈A(I)R T,+ i (I, b)\n,\nwith actions chosen uniformly at random when the denominator is zero. Regret matching is a no-regret learner that minimizes the per-information set immediate counterfactual regret,\nmax a∈A(I)\nRTi (I, a)\nT ≤ ∆i √ |A(I)|√ T , (2)\nwhere ∆i = maxz,z′∈Z ui(z)−ui(z′). In games having perfect recall, minimizing the immediate counterfactual regrets at every information set in turn minimizes average regret, RTi /T . This is because perfect recall implies that the regret is bounded by the sum of the positive parts of the immediate counterfactual regrets [Zinkevich et al., 2008],\nRTi ≤ ∑ I∈Ii max a∈A(I) RT,+i (I, a), (3)\nand thus RTi T ≤\n∆i|Ii| √ |Ai|√\nT , (4)\nwhere |Ai| = maxI∈Ii |A(I)|. CFR must store the immediate counterfactual regret for each information set, action pair, and thus CFR’s memory requirements are O(|Ii||Ai|).\nWhile equation (2) still holds in imperfect recall games, equation (3) and consequently equation (4) are not guaranteed to hold. An example game where CFR would exhibit high regret is provided in Section 7. Consequently, the regret for playing according to the CFR algorithm is unknown in general for imperfect recall games. However, the advantage of applying CFR to DRP-IR, for example, is that this imperfect recall game contains fewer information sets than the full game, and thus less memory is required by CFR. Although DRP is a toy example and is small enough to run CFR on the full game, this example is useful for understanding the concepts in the rest of this paper."
    }, {
      "heading" : "5 CFR with Imperfect Recall",
      "text" : "In this section, we investigate the application of CFR to games with imperfect recall. We begin by showing that CFR minimizes regret for a class of games that we call “wellformed games.” We then present a bound on the average regret for a more general class of imperfect recall games that we call “skew well-formed games.”"
    }, {
      "heading" : "5.1 Well-formed Games",
      "text" : "For games Γ = 〈N,A,H,Z, P, σc, u, I〉 and Γ̆ = 〈N,A,H,Z, P, σc, u, Ĭ〉, we say that Γ̆ is a perfect recall refinement of Γ if Γ̆ has perfect recall and Γ is an abstraction of Γ̆. So, the information available to players in Γ̆ is never forgotten, and is at least as informative as the information available to them in Γ. For example, DRP is a perfect recall refinement of DRP-IR. Every game has at least one perfect recall refinement by simply making Γ̆ a perfect information game (Ĭ = {h} for all Ĭ ∈ Ĭi). Furthermore, a perfect recall game is a perfect recall refinement of itself. For I ∈ Ii, we define\nP̆(I) = {Ĭ | Ĭ ∈ Ĭi, Ĭ ⊆ I}\nto be the set of all information sets in Ĭi that are subsets of I . Note that our notion of refinement is similar to the one described by Kaneko & Kline (1995). Our definition differs in that we consider any possible refinement, whereas Kaneko & Kline consider only the coarsest such refinement.\nDefinition 2. For a game Γ and a perfect recall refinement Γ̆, we say that Γ is a wellformed game with respect to Γ̆ if for all i ∈ N , I ∈ Ii, Ĭ , Ĭ ′ ∈ P̆(I), there exists a bijection φ : ZĬ → ZĬ′ and constants kĬ,Ĭ′ , `Ĭ,Ĭ′ ∈ [0,∞) such that for all z ∈ ZĬ :\n(i) ui(z) = kĬ,Ĭ′ui(φ(z)),\n(ii) πc(z) = `Ĭ,Ĭ′πc(φ(z)),\n(iii) In Γ, X−i(z) = X−i(φ(z)), and\n(iv) In Γ, Xi(z[Ĭ], z) = Xi(φ(z)[Ĭ ′], φ(z)).\nWe say that Γ is a well-formed game if it is well-formed with respect to some perfect recall refinement.\nRecall that ZI is the set of terminal histories containing a prefix in the information set I , and that z[I] is that prefix. Intuitively, a game is well-formed if for each information set I ∈ Ii, the structures around each Ĭ , Ĭ ′ ∈ P̆(I) of some perfect recall refinement are isomorphic across four conditions. Conditions (i) and (ii) state that the corresponding utilities and chance frequencies at each terminal history are proportional. Condition (iii) asserts that the opponents can never distinguish the corresponding histories at any point in Γ. Finally, condition (iv) states that player i cannot distinguish between corresponding histories from Ĭ and Ĭ ′ until the end of the game.\nConsider again DRP as a perfect recall refinement of DRP-IR. In DRP, the available actions are independent of dice outcomes, and the final utilities are only dependent on the final sum of the players’ dice. Therefore, in DRP the utilities are equivalent between, for example, the terminal histories where player i rolled a 1 followed by a 5, and the terminal histories where player i rolled a 4 followed by a 2 (condition (i)). In addition, the chance probabilities of reaching each terminal history are equal (condition (ii)). Furthermore, the opponents can never distinguish between two isomorphic histories since player i’s rolls are private (condition (iii)). Finally, in DRP-IR, player i\nnever remembers the outcome of the first roll from the second round on (condition (iv)). Thus, DRP-IR is well-formed with respect to DRP, with constants kĬ,Ĭ′ = `Ĭ,Ĭ′ = 1.\nAny perfect recall game is well-formed with respect to itself since P̆(I) = {I}, φ equal to the identity bijection, and kĬ,Ĭ′ = `Ĭ,Ĭ′ = 1 satisfies Definition 2. However, many imperfect recall games are also well-formed, with DRP-IR being one example. An additional example is presented in Section 6.\nWe now show that CFR can be applied to any well-formed game to minimize average regret. A sketch of the proof is described below, while a full proof is provided as supplementary material.\nTheorem 1. If Γ is well-formed with respect to Γ̆, then the average regret in Γ̆ for player i of choosing strategies according to CFR in Γ is bounded by\nR̆Ti T ≤\n∆iK √ |Ai|√\nT ,\nwhere K = ∑ I∈Ii maxĬ,Ĭ′∈P̆(I) kĬ,Ĭ′`Ĭ,Ĭ′ .\nProof sketch. One can show that conditions (i) to (iv) of Definition 2 imply that the positive regrets are proportional between any two information sets in Γ̆ that are merged in the well-formed game, Γ. In other words, for all I ∈ Ii, Ĭ , Ĭ ′ ∈ P̆(I), and a ∈ A(I),\nRT,+i (Ĭ , a) = kĬ,Ĭ′`Ĭ,Ĭ′R T,+ i (Ĭ ′, a).\nSince regrets between Γ and Γ̆ are additive, i.e.,\nRTi (I, a) = ∑\nĬ∈P̆(I)\nRTi (Ĭ , a) for all I ∈ Ii,\nthe proportionality implies that minimizing regret at each I ∈ Ii minimizes regret at each Ĭ ∈ Ĭi. Because Γ̆ has perfect recall, applying equation (3) gives the result.\nSince the strategy space is more expressive in Γ̆ than in Γ (Σ ⊆ Σ̆), RTi ≤ R̆Ti and thus it immediately follows that the average regret in Γ is minimized. In the case when Γ has perfect recall, because Γ is well-formed with respect to itself, Theorem 1 with K = |Ii| is a direct generalization of the original CFR bound in equation (4). Theorem 1 not only guarantees regret minimization for perfect recall games, but also for well-formed imperfect recall games."
    }, {
      "heading" : "5.2 Skew Well-formed Games",
      "text" : "We now present a generalization of well-formed games to which a regret bound can still be derived.\nDefinition 3. For a game Γ and a perfect recall refinement Γ̆, we say that Γ is a skew well-formed game with respect to Γ̆ if for all i ∈ N , I ∈ Ii, Ĭ , Ĭ ′ ∈ P̆(I), there exists a bijection φ : ZĬ → ZĬ′ and constants kĬ,Ĭ′ , δĬ,Ĭ′ , `Ĭ,Ĭ′ ∈ [0,∞) such that for all z ∈ ZĬ :\n(i) ∣∣∣ui(z)− kĬ,Ĭ′ui(φ(z))∣∣∣ ≤ δĬ,Ĭ′ ,\n(ii) πc(z) = `Ĭ,Ĭ′πc(φ(z)),\n(iii) In Γ, X−i(z) = X−i(φ(z)), and\n(iv) In Γ, Xi(z[Ĭ], z) = Xi(φ(z)[Ĭ ′], φ(z)).\nWe say that Γ is a skew well-formed game if it is skew well-formed with respect to some perfect recall refinement.\nThe only difference between Definitions 2 and 3 is in condition (i). While utilities must be exactly proportional in a well-formed game, utilities in a skew well-formed game must only be proportional up to a constant δĬ,Ĭ′ . Note that any well-formed game is skew well-formed by setting δĬ,Ĭ′ = 0.\nFor example, consider a new version of DRP called Skew-DRP(δ) with slightly modified payouts at the end of the game. Whenever the game reaches a showdown, player 1 receives a bonus δ times the number of chips in the pot from player 2 if player 1’s second die roll was even; otherwise, no bonus is awarded. The pot is then awarded to the player with the highest dice sum as usual. Analogously, define SkewDRP-IR(δ) to be the imperfect recall abstraction of Skew-DRP(δ) where in the second round, players only remember the sum of their two dice. Now, Skew-DRP-IR(δ) is not well-formed with respect to Skew-DRP(δ). To see this, note that the utilities resulting from the rolls 1,5 and the rolls 4,2 and the same sequence of betting are not exactly proportional because the second roll 5 is odd but 2 is even (utilities are off by δ times the pot size). However, Skew-DRP-IR(δ) is skew well-formed with respect to SkewDRP(δ) with δĬ,Ĭ′ = δ times the maximum pot size attainable from I .\nUnfortunately, there is no guarantee that regret will be minimized by CFR in a skew well-formed game. However, we can still bound regret in a predictable manner according to the degree that the utilities are skewed:\nTheorem 2. If Γ is skew well-formed with respect to Γ̆, then the average regret in Γ̆ for player i of choosing strategies according to CFR in Γ is bounded by\nR̆Ti T ≤\n∆iK √ |Ai|√ T + ∑ I∈Ii |P̆(I)|δI ,\nwhere K = ∑ I∈Ii maxĬ,Ĭ′∈P̆(I) kĬ,Ĭ′`Ĭ,Ĭ′ and δI = maxĬ,Ĭ′∈P̆(I) δĬ,Ĭ′`Ĭ,Ĭ′ .\nThe proof is similar to that of Theorem 1. Theorem 8 shows that as T approaches infinity, the bound on our regret approaches ∑ I∈Ii |P̆(I)|δI . Our experiments in Section 6 demonstrate that as the skew δ grows, so does our regret in Skew-DRP(δ) after a fixed number of iterations.\nRemarks. Theorems 1 and 8 are, to our knowledge, the first to provide such theoretical guarantees in imperfect recall settings. However, these results are also relevant with regards to regret in the full game when CFR is applied to an abstraction. Recall that if Γ has perfect recall, then Γ is a perfect recall refinement of any (skew) wellformed abstract game. Thus, if we choose an abstraction that yields a (skew) wellformed game, then applying CFR to the abstract game achieves a bound on the average\nregret in the full game, Γ. This is true regardless of whether the abstraction exhibits perfect recall or imperfect recall. Previous counterexamples show that abstraction in general provides no guarantees in the full game [Waugh et al., 2009a]. In contrast, our results show that applying CFR to an abstract game leads to bounded regret in the full game, provided we restrict ourselves to (skew) well-formed abstractions. If such an abstract game is much smaller than the full game, a significant amount of memory is saved when running CFR."
    }, {
      "heading" : "6 Empirical Evaluation",
      "text" : "To complement our theoretical results, we apply CFR to both players simultaneously in several zero-sum imperfect recall (abstract) games, and measure the sum of the average regrets for both players in a perfect recall refinement (the full game). Along with the small DRP domain and its variants, we also consider the challenging domains of phantom tic-tac-toe and Bluff, which we now describe.\nPhantom tic-tac-toe. As in regular tic-tac-toe, phantom tic-tac-toe (PTTT) is played on a 3-by-3 board, initially empty, where the goal is to claim three squares along the same row, column, or diagonal. However, in PTTT, players’ actions are private. Each turn, a player attempts to take a square of their choice. If they fail due to the opponent having taken that square on a previous turn, the same player keeps trying to take an alternative square until they succeed. Players are not informed about how many attempts the opponent made before succeeding. The game ends immediately if there is ever a connecting line of squares belonging to the same player. The winner receives a payoff of +1, while the losing player receives−1. In PTTT, the total number of histories |H| ≈ 1010.\nBluff. Bluff, also known as Liar’s Dice, Dudo, and Perudo, is a dice-bidding game. In our version, Bluff(D1,D2), each die has six sides with faces 1 to 6. Each player i rolls Di of these dice and looks at them without showing them to the opponent. Each round, players alternate by bidding on the outcome of all dice in play until one player claims that the other is bluffing (i.e., claims that the bid does not hold). A bid consists of a quantity of dice and a face value. A face of 6 is considered “wild” and counts as matching any other face. For example, the bid 2x5 represents the claim that there are at least two dice with a face of 5 (or 6) among both players’ dice. To place a new bid, the player must increase either the quantity or face value of the current bid; in addition, lowering the face is allowed if the quantity is increased. The player calling bluff wins the round if the opponent’s last bid is incorrect, and loses otherwise. The losing player removes one of their dice from the game and a new round begins, starting with the player who won the previous round. When a player has no more dice left, they have lost the game. A utility of +1 is given for a win and −1 for a loss. In this paper, we restrict ourselves to the case where D1 = D2 = 2. Note that since Bluff(2,2) is a multi-round game, the expected values of Bluff(1,1) are precomputed for payoffs at the leaves of Bluff(2,1), which is then solved for leaf payoffs in the full Bluff(2,2) game. In Bluff(2,2), the total number of histories |H| ≈ 1010."
    }, {
      "heading" : "6.1 Results",
      "text" : "We consider several different imperfect recall abstractions for DRP, Skew-DRP(δ), PTTT, and Bluff. For the DRP games, we apply DRP-IR and Skew-DRP-IR(δ) respectively as described in Section 5. Our PTTT and Bluff experiments, however, also investigate the effects of imperfect recall beyond skew well-formed games. In the full, perfect recall version of PTTT, each player remembers the order of every failed and every successful move she makes throughout the entire game. In our first abstract game, FOSF, players forget the order of successive failures within the same turn. Clearly, there is an isomorphism between any two merged information sets Ĭ , Ĭ ′ ∈ P̆(I) since the order of the actions does not affect the available future moves or utilities. Players still remember which turn each success and each failure occurred, and so the opponent’s sequences of actions must be equal across the isomorphism. Thus, FOSF is wellformed. Our remaining PTTT abstractions, however, are not even skew well-formed. In FOI, players independently remember the sequence of failures and the sequence of successful actions, but not how the actions interleave. In FOS, players remember the order of failed actions, but not the order of successes. Finally, in FOE, players only know what actions they have taken and remember nothing about the order in which they were taken. FOI, FOS, and FOE are not skew well-formed because no isomorphism can preserve the order of the opponent’s previous information set, action pairs (breaking condition (iii) of Definitions 2 and 3). In Bluff, we use abstractions described by Neller and Hnath (2011) that force players to forget everything except the last r bids. Similarly, these abstract games are not skew well-formed because the players forget information that the opponent could previously distinguish. The size of each DRP, PTTT, and Bluff game is given in Table 1. Here, A = {(I, a) : i ∈ N, I ∈ Ii, a ∈ A(I)} is the set of all information set, action pairs. Note that Skew-DRP(δ) is the same size as\nDRP regardless of the skew, and recall that CFR requires space linear in |A|. For each game, we ran CFR1 on both players, meaning that each player’s opponent was an identical copy of the same no-regret learner. The sum of the average regrets for each player over number of iterations is shown in Figure 1. The SkewDRP-IR(δ) experiments show that as δ increases, so does the regret as predicted by Theorem 8, though ∑ I∈Ii\n∣∣∣P̆(I)∣∣∣ δI appears to be a very loose bound on the final regret. In PTTT, regret diverges from zero for FOI, FOS, and FOE, where FOS appears to provide slightly better strategies than FOI and FOE. While our theory cannot explain why FOS performs better, this does match our intuition that remembering information about the opponent’s moves is important. For a small increase in average regret, FOS reduces the space required by 87% compared to FOSF’s 20% reduction. Note that for both DRP and PTTT, running CFR on the full, perfect recall game achieves the same regret as in the well-formed abstractions (Skew-DRP-IR(0) and FSOF) and is thus not shown. In Bluff, we see that regret consistently worsens as fewer previous bids are remembered. This suggests that a result similar to Theorem 8 for skew-well-formed games may hold if condition (iii) of Definition 2 is less constrained, though the proper formulation for such a relaxation remains unclear. Nonetheless, choosing r = 8 saves 85% of the memory with only a very small increase in average regret after millions of iterations."
    }, {
      "heading" : "7 Discussion",
      "text" : "Well-formed games are described by four conditions provided in Definition 2. Recall that Koller & Megiddo (1992) prove that determining a player’s guaranteed payoff in an imperfect recall game is NP-complete. However, Koller & Megiddo’s NP-hardness reduction creates an imperfect recall game that breaks conditions (i), (iii), and (iv) of Definition 2. In this section, we discuss the following question: For minimizing regret, how important is it to satisfy each individual condition of Definition 2?\nSkew well-formed games and Theorem 8 show that one can relax condition (i) of Definition 2 and still derive a bound on the average regret. In addition, most of our PTTT and Bluff abstractions from the previous section do not satisfy condition (iii), but CFR still produces reliable results. This suggests that it may be possible to relax condition (iii) in a similar manner to the relaxation of condition (i) introduced by skew well-formed games. While we leave this question open, we now demonstrate that breaking condition (iii) can lead CFR to a dead-lock situation where one player has constant average regret.\nLet us walk through the process of applying CFR to the game in Figure 2. Note that this game satisfies all of the conditions of Definition 2, except for condition (iii). To begin, the current strategy profile σ1 is set to be uniform random at every information set. Under this profile, when player 1 is at I3, each of the four histories are equally likely. Thus, vi(σ1(I3→l), I3) = vi(σ 1 (I3→r), I3) = vi(σ\n1, I3) = 0, and so r11(I3, l) = r 1 1(I3, r) = 0. In addition, under σ 1, the counterfactual value of the pass\n1Similar to Zinkevich et al. (2008), we used the chance sampling variant of CFR.\n(p) and continue (c) actions at both I1 and I2 is zero, and thus the immediate counterfactual regrets at I1 and I2 on iteration 1 are also zero. Player 2, however, has positive immediate counterfactual regret for passing (p) at histories ac and ec (to always receive ξ utility) and for continuing (c) at bc and de (to always avoid receiving −ξ utility), and has negative immediate counterfactual regret for continuing at ac and ec and for passing at bc and de. Therefore, the next profile σ2 still has player 1 playing uniformly random everywhere, but player 2 now always passes at ac and ec, and always continues at bc and dc. On the second iteration of CFR, the positive regrets for player 1 at I3 remain the same because the histories bcc and dcc are equally likely. Also, player 2’s positive regrets remain the same at all four histories in H2. However, player 1’s expected utility for continuing at I1 or I2 is now negative since player 2 now passes at ac and ec. Thus, player 1 gains positive regret for passing at both I1 and I2. This leads us to the next profile σ3 = {(I1, p) = 1, (I2, p) = 1, (ac, p) = 1, (bc, p) = 0, (dc, p) = 0, (ec, p) = 1, (I3, l) = 0.5}. One can check that running CFR for more iterations yields σt = σ3 for all t ≥ 3. The average regret for playing this way will be constant and hence does not approach zero because player 1 would rather play σ′1 = {(I1, p) = 1, (I2, p) = 0, (I3, l) = 0} and get u1(σ′1, σ32) = (1− ξ)/4 > u1(σ3) for ξ ∈ (0, 1). A similar example can be constructed where condition (iii) holds, but chance’s probabilities are not proportional (breaking condition (ii)).\nDespite the problem of breaking condition (iii), condition (iv) of Definition 2 can be relaxed. Rather than enforcing player i’s future information to be the same across the bijection φ, we only require that the corresponding subtrees be isomorphic, allowing\nplayer i to re-remember information that was previously forgotten. The details for this relaxation are in the supplementary material. However, it is not clear that this relaxation is possible in skew well-formed games, nor does it seem to provide any practical advantage."
    }, {
      "heading" : "8 Conclusion",
      "text" : "We have provided the first set of theoretical guarantees for CFR in imperfect recall games. We defined well-formed and skew well-formed games and provided bounds on the average regret that results from applying CFR to such games. In addition, our theory shows that we can achieve low average regret in a full, perfect recall game when employing CFR on an abstract version of the game, provided the abstract game is skew well-formed (with or without imperfect recall). Our DRP experiments confirm these theoretical results, while our PTTT and Bluff experiments hint that it may be possible to still bound regret in other types of imperfect recall games. Future work will look to expand on the set of imperfect recall games to which CFR can be reliably applied. In particular, it may be possible to derive regret bounds for a new class of games where conditions (ii) and (iii) of Definition 2 are relaxed."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank the Computer Poker Research Group at the University of Alberta for their helpful discussions that contributed to this work. This work was supported by NSERC, Alberta Innovates – Technology Futures, and the use of computing resources provided by WestGrid and Compute Canada."
    }, {
      "heading" : "Appendix A",
      "text" : "In this section, we will prove Theorems 1 and 2 of the main paper. Note that by the definition of counterfactual value, the regrets between Γ and a perfect recall refinement Γ̆ are additive; specifically, for I ∈ Ii in Γ,\nRTi (I, a) = ∑\nĬ∈P̆(I)\nRTi (Ĭ , a). (5)\nFirst, we provide a lemma that generalizes Theorem 4 of (Zinkevich et al., 2008) by showing that if the immediate counterfactual regrets of each Ĭ ∈ P̆(I) are proportional up to some difference D, then the average regret can be bounded above:\nLemma A. Let Γ̆ be a perfect recall refinement of a game Γ. If for all I ∈ Ii, Ĭ , Ĭ ′ ∈ P̆(I), and a ∈ A(I), there exist constants CĬ,Ĭ′,a, DĬ,Ĭ′,a ∈ [0,∞) such that\n1\nT\n∣∣∣RT,+i (Ĭ , a)− CĬ,Ĭ′,aRT,+i (Ĭ ′, a)∣∣∣ ≤ DĬ,Ĭ′,a, (6)\nthen the average regret in Γ̆ is bounded by\nR̆Ti T ≤ ∆iC √ |Ai|√ T + ∑ I∈I |P̆(I)|DI ,\nwhere C = ∑ I∈Ii max Ĭ,Ĭ′∈P̆(I),a∈A(I) CĬ,Ĭ′,a\nand DI = max\nĬ,Ĭ′∈P̆(I),a∈A(I) DĬ,Ĭ′,a.\nProof. R̆Ti ≤ ∑ Ĭ∈Ĭi max a∈A(I) RT,+i (Ĭ , a) by Theorem 3 of (Zinkevich et al., 2008)\n= ∑ I∈Ii ∑ Ĭ∈P̆(I) max a∈A(I) RT,+i (Ĭ , a) by definition of a perfect recall refinement\n≤ ∑ I∈Ii |P̆(I)|RT,+i (Ĭ ∗, a∗) where Ĭ∗ = arg max Ĭ∈P̆(I) max a∈A(I) RT,+i (Ĭ , a)\nand a∗ = arg max a∈A(I) RT,+i (Ĭ ∗, a)\n≤ ∑ I∈Ii |P̆(I)| ( CĬ∗,Ĭ∗∗,a∗R T,+ i (Ĭ ∗∗, a∗) + TDĬ∗,Ĭ∗∗,a∗ ) by (6),\nwhere Ĭ∗∗ = arg min Ĭ∈P̆(I) RTi (Ĭ , a ∗)\n≤ ∑ I∈Ii |P̆(I)|CĬ∗,Ĭ∗∗,a∗  1 |P̆(I)| ∑ Ĭ∈P̆(I) RTi (Ĭ , a ∗) + + T ∑ I∈Ii |P̆(I)|DI\nbecause the minimum is less than the average and (·)+ is monotone increasing = ∑ I∈Ii CĬ∗,Ĭ∗∗,a∗R T,+ i (I, a ∗) + T ∑ I∈Ii |P̆(I)|DI by (5) ≤ ∑ I∈Ii CĬ∗,Ĭ∗∗,a∗T √√√√√ ∑ a∈A(I) ( RT,+i (I, a) T )2 + T ∑ I∈Ii |P̆(I)|DI\n≤ ∑ I∈Ii CĬ∗,Ĭ∗∗,a∗∆i √ |A(I)| √ T + T ∑ I∈Ii |P̆(I)|DI\nby Theorem 6 of (Lanctot et al., 2009) ≤ ∆iC √ |Ai| √ T + T ∑ I∈Ii |P̆(I)|DI .\nDividing both sides by T establishes the lemma.\nNote that if Γ has perfect recall, then the constants CI,I,a = 1 and DI,I,a = 0 for all I ∈ Ii and a ∈ A(I) satisfy the condition of Lemma A. In this case, C = |Ii| and DI = 0, and so RTi /T ≤ ∆i|Ii| √ |Ai|/ √ T , recovering Theorem 4 of (Zinkevich et al., 2008). We now use Lemma A to prove Theorems 1 and 2:\nTheorem 2. If Γ is skew well-formed with respect to Γ̆, then the average regret in Γ̆ for player i of choosing strategies according to CFR in Γ is bounded by\nR̆Ti T ≤\n∆iK √ |Ai|√ T + ∑ I∈Ii |P̆(I)|δI ,\nwhere K = ∑ I∈Ii maxĬ,Ĭ′∈P̆(I) kĬ,Ĭ′`Ĭ,Ĭ′ and δI = maxĬ,Ĭ′∈P̆(I) δĬ,Ĭ′`Ĭ,Ĭ′ .\nProof. We will show that for all I ∈ Ii, Ĭ , Ĭ ′ ∈ P̆(I), and a ∈ A(I),\n1\nT ∣∣∣RT,+i (Ĭ , a)− kĬ,Ĭ′`Ĭ,Ĭ′RT,+i (Ĭ ′, a)∣∣∣ ≤ δĬ,Ĭ′`Ĭ,Ĭ′ , (7) which, by Lemma A, proves the theorem.\nFix I ∈ Ii, Ĭ , Ĭ ′ ∈ P̆(I), and a ∈ A(I). Firstly, for all z ∈ ZĬ and σ ∈ Σ, by conditions (ii) and (iii) of Definition 3, we have\nπσ−i(z) = πc(z) ∏\n(I,a)∈X−i(z)\nσ(I, a)\n= `Ĭ,Ĭ′πc(φ(z)) ∏\n(I,a)∈X−i(φ(z))\nσ(I, a)\n= `Ĭ,Ĭ′π σ −i(φ(z)) (8)\nand by condition (iv) of Definition 3, we similarly have\nπσi (z[Ĭ], z) = π σ i (φ(z)[Ĭ ′], φ(z)) (9)\nand πσi (z[Ĭ]a, z) = π σ i (φ(z)[Ĭ ′]a, φ(z)). (10)\nWe can then bound the positive part of the immediate counterfactual regretRT,+i (Ĭ , a) above by\nRT,+i (Ĭ , a) = ( T∑ t=1 rti(Ĭ , a) )+\n=  T∑ t=1 ∑ z∈ZĬ πσ−i(z)(π σ i (z[Ĭ]a, z)− πσi (z[Ĭ], z))ui(z) + ≤ ( T∑ t=1 ∑ z∈ZĬ `Ĭ,Ĭ′π σ −i(φ(z))(π σ i (φ(z)[Ĭ ′]a, φ(z))\n− πσi (φ(z)[Ĭ ′], φ(z)))(kĬ,Ĭ′ui(φ(z)) + δĬ,Ĭ′)) +\nby equations (8), (9), (10), and condition (i) of Definition 3\n= ( T∑ t=1 ∑ z∈ZĬ′ `Ĭ,Ĭ′π σ −i(z)(π σ i (z[Ĭ ′]a, z)\n− πσi (z[Ĭ ′], z))(kĬ,Ĭ′ui(z) + δĬ,Ĭ′)) +\nsince φ is a bijection\n≤  T∑ t=1 ∑ z∈ZĬ′ kĬ,Ĭ′`Ĭ,Ĭ′π σ −i(z)(π σ i (z[Ĭ]a, z)− πσi (z[Ĭ], z))ui(z) +\n+  T∑ t=1 ∑ z∈ZĬ′ δĬ,Ĭ′`Ĭ,Ĭ′π σ −i(z)(π σ i (z[Ĭ]a, z)− πσi (z[Ĭ], z)) +\n≤ kĬ,Ĭ′`Ĭ,Ĭ′R T,+ i (Ĭ ′, a) + T∑ t=1 δĬ,Ĭ′`Ĭ,Ĭ′π σ −i(Ĭ ′) ≤ kĬ,Ĭ′`Ĭ,Ĭ′R T,+ i (Ĭ ′, a) + TδĬ,Ĭ′`Ĭ,Ĭ′ , (11)\nwhere the last line follows because πσ−i(Ĭ ′) = ∑ z∈ZĬ′ πσ−i(z[Ĭ ′]) ≤ 1 in a perfect\nrecall game Γ̆. Similarly,\nRT,+i (Ĭ , a) ≥ kĬ,Ĭ′`Ĭ,Ĭ′R T,+ i (Ĭ ′, a)− TδĬ,Ĭ′`Ĭ,Ĭ′ , (12)\nwhich together with equation (11) and dividing by T establishes (7), completing the proof. Note that Theorem 1 immediately follows from Theorem 2 since a well-formed game is skew well-formed with δĬ,Ĭ′ = 0 for all Ĭ , Ĭ ′ ∈ P̆(I)."
    }, {
      "heading" : "Appendix B",
      "text" : "In this section, we consider an alternative extension of well-formed games that relaxes condition (iv) of Definition 2. For a subset of histories S ⊆ Hi, define\nDi(S) = {I | I ∈ Ii,∃h ∈ S, h′ ∈ I such that h v h′}\nto be the set of all information sets descending from any history in S.\nDefinition 4. For a game Γ and a perfect recall refinement Γ̌, we say that Γ is a nearly well-formed game with respect to Γ̌ if for all i ∈ N , I ∈ Ii, Ǐ , Ǐ ′ ∈ P̌(I), J ∈ Di(Ǐ), there exist bijections φ : ZǏ → ZǏ′ , ψ : Di(Ǐ) → Di(Ǐ ′), ω : A(J) → A(ψ(J)) and constants kǏ,Ǐ′ , `Ǐ,Ǐ′ ∈ [0,∞) such that for all z ∈ ZǏ :\n(i) ui(z) = kǏ,Ǐ′ui(φ(z)),\n(ii) πc(z) = `Ǐ,Ǐ′πc(φ(z)),\n(iii) In Γ, X−i(z) = X−i(φ(z)), and\n(iv) Xi(z[Ǐ], z) = (J1, a1), ..., (Jm, am) if and only if Xi(φ(z)[Ǐ ′], φ(z)) = (ψ(J1), ω(a1)), ..., (ψ(Jm), ω(am)).\nWe say that Γ is a nearly well-formed game if it is nearly well-formed with respect to some perfect recall refinement.\nIn a nearly well-formed game, condition (iv) says that player i may now remember information that was once forgotten, provided the descendants from Ǐ and Ǐ ′ are isomorphic across φ. This relaxes the corresponding condition for a well-formed game where player i could never remember information once it was forgotten. Clearly, any well-formed game is nearly well-formed by choosing ψ and ω to be the identity bijections.\nFor example, consider a longer version of DRP, DRP-3, that consists of three betting rounds instead of two where a third die is rolled at the beginning of round 3. We then define DRP-IR-3 to be the imperfect recall abstraction of DRP-3 where during round 2, players only know the sum of their two dice. In round 3, players once again know the outcome of each individual die roll, recovering information from the first round that was forgotten in the second. For instance, corresponding histories where player i’s first two rolls were 1,5 and where her first two rolls were 4,2 will be in the same information set during round 2, but will be in different information sets in round 3. However, betting is independent of dice rolls and utilities are only dependent on the final sum of the three dice. Therefore, the descendants from these histories are isomorphic across φ and thus DRP-IR-3 is nearly well-formed with respect to DRP-3.\nCFR guarantees that the average regret is also minimized in nearly well-formed games:\nTheorem 3. If Γ is nearly well-formed with respect to Γ̆, then the average regret in Γ̆ for player i of choosing strategies according to CFR in Γ is bounded by\nŘTi T ≤\n∆iK √ |Ai|√\nT ,\nwhere K = ∑ I∈Ii maxǏ,Ǐ′∈P̌(I) kǏ,Ǐ′`Ǐ,Ǐ′ .\nProof. Fix I ∈ Ii, Ĭ , Ĭ ′ ∈ P̆(I), and a ∈ A(I). By conditions (ii) and (iii) of Definition 4, equation (8) holds.\nClaim: RTi (J, b) = kĬ,Ĭ′`Ĭ,Ĭ′R T i (ψ(J), ω(b)) for all J ∈ Di(Ĭ), b ∈ A(J), T ≥ 0.\nProvided the claim is true, we have\nσT+1(J, b) =  RT,+i (J,b)∑ d∈A(J) R T,+ i (J,d) if ∑ d∈A(J)R T,+ i (J, d) > 0\n1 |A(J)| otherwise\n=  kĬ,Ĭ′`Ĭ,Ĭ′R T,+ i (ψ(J),ω(b))∑ d∈A(J) kĬ,Ĭ′`Ĭ,Ĭ′R T,+ i (ψ(J),ω(b)) if ∑ d∈A(J) kĬ,Ĭ′`Ĭ,Ĭ′R T,+ i (ψ(J), ω(b)) > 0\n1 |A(ψ(J))| otherwise\nsince ω is a bijection\n= σT+1(ψ(J), ω(b)) (13)\nfor all J ∈ Di(Ĭ), b ∈ A(J), T ≥ 0. Therefore, for t ≥ 1,\nπσ t i (z[Ĭ], z) = ∏\n(J,b)∈Xi(z[Ĭ],z)\nσt(J, b)\n= ∏\n(J,b)∈Xi(z[Ĭ],z)\nσt(ψ(J), ω(b))\n= ∏\n(J,b)∈Xi(φ(z)[Ĭ′],φ(z))\nσt(J, b) by condition (iv) of Definition 4\n= πσ t i (φ(z)[Ĭ ′], φ(z)),\nand thus equation (9) and similarly equation (10) hold for σ = σt. By following the proof of Theorem 2, we then have that equations (11) and (12) with δĬ,Ĭ′ = 0 hold, and hence equation (7) with δĬ,Ĭ′ = 0 holds. This establishes the theorem by Lemma A.\nTo complete the proof, we are left to show that the claim holds. We will do so by induction on T . The base case T = 0 holds since R0i (I, a) = 0 for all I ∈ Ii, a ∈ A(I). For the inductive step, assume that RT−1i (J, b) = kĬ,Ĭ′`Ĭ,Ĭ′R T−1 i (ψ(J), ω(b)) for all J ∈ Di(Ĭ), b ∈ A(J). We will show that RTi (J, b) = kĬ,Ĭ′`Ĭ,Ĭ′RTi (ψ(J), ω(b)) for all J ∈ Di(Ĭ), b ∈ A(J).\nFix J ∈ Di(Ĭ) and b ∈ A(J). By equation (13), we have for all z ∈ ZJ ,\nπσ T i (z[J ], z) = ∏\n(J′,b′)∈Xi(z[J],z)\nσT (J ′, b′)\n= ∏\n(J′,b′)∈Xi(z[J],z)\nσT (ψ(J ′), ω(b′)) by equation (13)\n= ∏\n(J′,b′)∈Xi(φ(z)[ψ(J)],φ(z))\nσT (J ′, b′)\nby condition (iv) of Definition 4 since Xi(z[J ], z) is a subsequence\n(more precisely, a suffix) of Xi(z[Ĭ], z)\n= πσ T\ni (φ(z)[ψ(J)], φ(z)) (14)\nand similarly πσ T\ni (z[J ]b, z) = π σT i (φ(z)[ψ(J)]ω(b), φ(z)). (15)\nNow consider the counterfactual regret at time T ,\nrTi (J, b) = ∑ z∈ZJ πσ T −i (z)(π σT i (z[J ]b, z)− πσ T i (z[J ], z))ui(z)\n= ∑ z∈ZJ `Ĭ,Ĭ′π σT −i (φ(z))(π σT i (φ(z)[ψ(J)]ω(b), φ(z))\n− πσ T\ni (φ(z)[ψ(J)], φ(z)))kĬ,Ĭ′ui(φ(z))\nby equations (14), (15) and conditions (i), (ii), and (iii) of Definition 4\n= `Ĭ,Ĭ′kĬ,Ĭ′r T i (ψ(J), ω(b)).\nFinally,\nRTi (J, b) = T∑ t=1 rti(J, b)\n= RT−1i (J, b) + r T i (J, b) = `Ĭ,Ĭ′kĬ,Ĭ′(R T−1 i (ψ(J), ω(b)) + r T i (ψ(J), ω(b)))\nby the induction hypothesis and the above\n= `Ĭ,Ĭ′kĬ,Ĭ′ T∑ t=1 rti(ψ(J), ω(b)) = `Ĭ,Ĭ′kĬ,Ĭ′R T i (ψ(J), ω(b)),\nestablishing the inductive step. This completes the proof."
    } ],
    "references" : [ {
      "title" : "A simple adaptive procedure leading to correlated",
      "author" : [ "Sergiu Hart", "Andreu Mas-Colell" ],
      "venue" : "equilibrium. Econometrica,",
      "citeRegEx" : "Hart and Mas.Colell.,? \\Q2000\\E",
      "shortCiteRegEx" : "Hart and Mas.Colell.",
      "year" : 2000
    }, {
      "title" : "Behavior strategies, mixed strategies and perfect recall",
      "author" : [ "Mamoru Kaneko", "J. Jude Kline" ],
      "venue" : "International Journal of Game Theory,",
      "citeRegEx" : "Kaneko and Kline.,? \\Q1995\\E",
      "shortCiteRegEx" : "Kaneko and Kline.",
      "year" : 1995
    }, {
      "title" : "The complexity of two-person zero-sum games in extensive form",
      "author" : [ "Daphne Koller", "Nimrod Megiddo" ],
      "venue" : "Games and Economic Behavior,",
      "citeRegEx" : "Koller and Megiddo.,? \\Q1992\\E",
      "shortCiteRegEx" : "Koller and Megiddo.",
      "year" : 1992
    }, {
      "title" : "Fast algorithms for finding randomized strategies in game trees",
      "author" : [ "Daphne Koller", "Nimrod Megiddo", "Bernhard von Stengel" ],
      "venue" : "In Proceedings of the 26th ACM Symposium on Theory of Computing (STOC",
      "citeRegEx" : "Koller et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Koller et al\\.",
      "year" : 1994
    }, {
      "title" : "Extensive games and the problem of information",
      "author" : [ "Harold W. Kuhn" ],
      "venue" : "Contributions to the Theory of Games,",
      "citeRegEx" : "Kuhn.,? \\Q1953\\E",
      "shortCiteRegEx" : "Kuhn.",
      "year" : 1953
    }, {
      "title" : "Monte carlo sampling for regret minimization in extensive games. Technical Report TR09-15",
      "author" : [ "Marc Lanctot", "Kevin Waugh", "Martin Zinkevich", "Michael Bowling" ],
      "venue" : "University of Alberta,",
      "citeRegEx" : "Lanctot et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Lanctot et al\\.",
      "year" : 2009
    }, {
      "title" : "Approximating optimal Dudo play with fixedstrategy iteration counterfactual regret minimization",
      "author" : [ "Todd W. Neller", "Steven Hnath" ],
      "venue" : "In Computers and Games,",
      "citeRegEx" : "Neller and Hnath.,? \\Q2011\\E",
      "shortCiteRegEx" : "Neller and Hnath.",
      "year" : 2011
    }, {
      "title" : "A Course in Game Theory",
      "author" : [ "Martin J. Osborne", "Ariel Rubinstein" ],
      "venue" : null,
      "citeRegEx" : "Osborne and Rubinstein.,? \\Q1994\\E",
      "shortCiteRegEx" : "Osborne and Rubinstein.",
      "year" : 1994
    }, {
      "title" : "On the interpretation of decision problems with imperfect recall",
      "author" : [ "Michele Piccione", "Ariel Rubinstein" ],
      "venue" : "In Proceedings of the 6th Conference on Theoretical Aspects of Rationality and Knowledge,",
      "citeRegEx" : "Piccione and Rubinstein.,? \\Q1996\\E",
      "shortCiteRegEx" : "Piccione and Rubinstein.",
      "year" : 1996
    }, {
      "title" : "Abstraction pathologies in extensive games",
      "author" : [ "Kevin Waugh", "Dave Schnizlein", "Michael Bowling", "Duane Szafron" ],
      "venue" : "In he Eight International Conference on Autonomous Agents and Multiagent Systems,",
      "citeRegEx" : "Waugh et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Waugh et al\\.",
      "year" : 2009
    }, {
      "title" : "A practical use of imperfect recall",
      "author" : [ "Kevin Waugh", "Martin Zinkevich", "Michael Johanson", "Morgan Kan", "David Schnizlein", "Michael Bowling" ],
      "venue" : "In Proceedings of SARA 2009: The Eighth Symposium on Abstraction, Reformulation and Approximation,",
      "citeRegEx" : "Waugh et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Waugh et al\\.",
      "year" : 2009
    }, {
      "title" : "Regret minimization in games with incomplete information",
      "author" : [ "Martin Zinkevich", "Michael Johanson", "Michael Bowling", "Carmelo Piccione" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Zinkevich et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Zinkevich et al\\.",
      "year" : 2008
    }, {
      "title" : "We now use Lemma A to prove Theorems 1 and 2: Theorem 2. If Γ is skew well-formed with respect to Γ̆, then the average regret in Γ̆",
      "author" : [ ],
      "venue" : "(Zinkevich et al.,",
      "citeRegEx" : "√,? \\Q2008\\E",
      "shortCiteRegEx" : "√",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "A common approach to achieving low regret in extensive games is the Counterfactual Regret Minimization (CFR) [Zinkevich et al., 2008] algorithm.",
      "startOffset" : 109,
      "endOffset" : 133
    }, {
      "referenceID" : 4,
      "context" : "In games with perfect recall, every mixed strategy (probability distribution over pure strategies) has a utility-equivalent behavioral strategy (probability distribution over actions at each decision point) [Kuhn, 1953].",
      "startOffset" : 207,
      "endOffset" : 219
    }, {
      "referenceID" : 1,
      "context" : "property [Kaneko and Kline, 1995], it is not true for imperfect recall games in general [Piccione and Rubinstein, 1996].",
      "startOffset" : 9,
      "endOffset" : 33
    }, {
      "referenceID" : 8,
      "context" : "property [Kaneko and Kline, 1995], it is not true for imperfect recall games in general [Piccione and Rubinstein, 1996].",
      "startOffset" : 88,
      "endOffset" : 119
    }, {
      "referenceID" : 2,
      "context" : "In addition, the decision problem of determining if a player can assure themself a certain payoff in an imperfect recall game is NPcomplete [Koller and Megiddo, 1992].",
      "startOffset" : 140,
      "endOffset" : 166
    }, {
      "referenceID" : 3,
      "context" : "Two-player zero-sum games can be solved by constructing an appropriate linear program [Koller et al., 1994] or minimizing regret [Zinkevich et al.",
      "startOffset" : 86,
      "endOffset" : 107
    }, {
      "referenceID" : 11,
      "context" : ", 1994] or minimizing regret [Zinkevich et al., 2008], provided the game has perfect recall.",
      "startOffset" : 29,
      "endOffset" : 53
    }, {
      "referenceID" : 3,
      "context" : "Without perfect recall, however, the problem becomes exponential in the worst case [Koller et al., 1994].",
      "startOffset" : 83,
      "endOffset" : 104
    }, {
      "referenceID" : 7,
      "context" : "An extensive-form game Γ with imperfect information [Osborne and Rubinstein, 1994] is a tuple 〈N,A,H,Z, P, σc, u, I〉, where N is a finite set of players.",
      "startOffset" : 52,
      "endOffset" : 82
    }, {
      "referenceID" : 11,
      "context" : "This is because perfect recall implies that the regret is bounded by the sum of the positive parts of the immediate counterfactual regrets [Zinkevich et al., 2008], R i ≤ ∑",
      "startOffset" : 139,
      "endOffset" : 163
    }, {
      "referenceID" : 6,
      "context" : "In Bluff, we use abstractions described by Neller and Hnath (2011) that force players to forget everything except the last r bids.",
      "startOffset" : 43,
      "endOffset" : 67
    }, {
      "referenceID" : 11,
      "context" : "In addition, under σ , the counterfactual value of the pass 1Similar to Zinkevich et al. (2008), we used the chance sampling variant of CFR.",
      "startOffset" : 72,
      "endOffset" : 96
    } ],
    "year" : 2012,
    "abstractText" : "Counterfactual Regret Minimization (CFR) is an efficient no-regret learning algorithm for decision problems modeled as extensive games. CFR’s regret bounds depend on the requirement of perfect recall: players always remember information that was revealed to them and the order in which it was revealed. In games without perfect recall, however, CFR’s guarantees do not apply. In this paper, we present the first regret bound for CFR when applied to a general class of games with imperfect recall. In addition, we show that CFR applied to any abstraction belonging to our general class results in a regret bound not just for the abstract game, but for the full game as well. We verify our theory and show how imperfect recall can be used to trade a small increase in regret for a significant reduction in memory in three domains: die-roll poker, phantom tic-tac-toe, and Bluff.",
    "creator" : "LaTeX with hyperref package"
  }
}