{
  "name" : "1603.04930.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Deep Fully-Connected Networks for Video Compressive Sensing",
    "authors" : [ "Michael Iliadis", "Leonidas Spinoulas", "Aggelos K. Katsaggelos" ],
    "emails" : [ "miliad@u.northwestern.edu", "leonisp@u.northwestern.edu", "aggk@eecs.northwestern.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Video compressive sensing (CS) refers to the problem of recovering an unknown spatio-temporal volume from limited samples. It comes in two incarnations, namely, spatial CS and temporal CS. Spatial video CS architectures stem from the well-known single-pixel-camera [7], which performs spatial multiplexing per measurement, and enable video recovery by expediting the capturing process. They either employ fast readout circuitry to capture information at video rates [4] or parallelize the single-pixel architecture using multiple sensors, each one responsible for sampling a separate spatial area of the scene [3, 37].\nIn this work, we focus on temporal CS where multiplexing occurs across the time dimension. Figure 1 depicts this process, where a spatio-temporal volume of size Wf ×Hf × t = Nf is modulated by t binary random masks during the exposure time of a single capture, giving rise to a coded frame of size Wf ×Hf = Mf . We denote the vectorized versions of the unknown signal and the captured frame as x : Nf × 1 and y : Mf × 1, respectively. Each vectorized sampling mask is expressed as φ1, . . . ,φt giving rise to the measurement model\ny = Φx, (1)\nwhere Φ = [diag(φ1), . . . , diag(φt)] : Mf × Nf and diag(·) creates a diagonal matrix from its vector argument.\n∗Indicates equal contribution.\nar X\niv :1\n60 3.\n04 93\n0v 1\n[ cs\n.C V\n] 1\n6 M\n∗ ∫ =\nHfWf × × t\nMeasurement matrix (Φ)\nHfWf ×\nCaptured frame (y)\ndt\nSpatio-Temporal volume (x)\nHfWf × × t\nFigure 1: Temporal compressive sensing measurement model.\nVarious successful temporal CS architectures have been proposed. Their differences mainly involve the implementation of the random masks on the optical path (i.e., the measurement matrix in Figure 1). Digital micromirror devices (DMD), spatial light modulators (SLM) and liquid crystal on silicon (LCoS) were used in [3, 37, 9, 21, 29] while translating printed masks were employed in [15, 23]. Moreover, a few architectures have eliminated additional optical elements by directly programming the chip’s readout mode through hardware circuitry modifications [8, 26, 33].\nDespite their reasonable performance, temporal CS architectures lack practicality. The main drawback is that existing reconstruction algorithms (e.g., using sparsity models [3, 12], combining sparsity and dictionary learning [22] or using Gaussian mixture models [41, 42]) are often too computationally intensive, rendering the reconstruction process painfully slow. Even with parallel processing, recovery times make video CS prohibitive for modern commercial camera architectures.\nIn this work, we address this problem by employing deep learning and show that video frames can be recovered in a few seconds at significantly improved reconstruction quality compared to existing approaches.\nOur contributions are summarized as follows:\n1. We present a novel temporal video CS reconstruction approach, based on fully-connected neural networks, which learns to map directly temporal CS measurements to video frames. For such task to be practical, a measurement mask with a repeated pattern is proposed.\n2. We show that a simple linear regression-based approach learns to reconstruct video frames adequately at a minimal computational cost. Such reconstruction could be used as an initial point to other video CS algorithms.\n3. The learning parading is extended to deeper architectures exhibiting reconstruction quality and computational cost improvements compared to previous methods."
    }, {
      "heading" : "2 Motivation and Related Work",
      "text" : "Deep learning [18] is a burgeoning research field which has demonstrated state-of-the-art performance in a multitude of machine learning and computer vision tasks, such as image recognition [11] or object detection [28].\nIn simple words, deep learning tries to mimic the human brain by training large multi-layer neural networks with vast amounts of training samples, describing a given task. Such networks have proven very successful in problems where analytical modeling is not easy or straightforward (e.g., a variety of computer vision tasks [16, 20]).\nThe popularity of neural networks in recent years has led researchers to explore the capabilities of deep architectures even in problems where analytical models often exist and are well understood (e.g., restoration problems [2, 32, 39]). Even though performance improvement is not as pronounced as in classification problems, many proposed architectures have achieved state-of-the-art performance in problems such as deconvolution, denoising, inpainting, and super-resolution.\nMore specifically, investigators have employed a variety of architectures: deep fully-connected networks or multi-layer perceptrons (MLPs) [2, 32]; stacked denoising auto-encoders (SDAEs) [1, 5, 36, 39], which are MLPs whose layers are pre-trained to provide improved weight initializa-\ntion; convolutional neural networks (CNNs) [6, 19, 30, 34, 37, 40] and recurrent neural networks (RNNs) [13].\nBased on such success in restoration problems, we wanted to explore the capabilities of deep learning for the video CS problem. However, the majority of existing architectures involve outputs whose dimensionality is smaller than the input (e.g., classification) or have the same size (e.g., denoising/deblurring). Hence, devising an architecture that estimates Nf unknowns, given Mf inputs, where Mf Nf is not necessarily straightforward. Two recent studies, utilizing SDAEs [24] or CNNs [17], have been presented on spatial CS for still images exhibiting promising performance. Our work constitutes the first attempt to apply deep learning on temporal video CS. Our approach differs from prior 2D image restoration architectures [2, 32] since we are recovering a 3D volume from 2D measurements."
    }, {
      "heading" : "3 Deep Networks for Compressed Video",
      "text" : ""
    }, {
      "heading" : "3.1 Linear mapping",
      "text" : "We started our investigation by posing the question: can training data be used to find a linear mapping W such that x = Wy? Essentially, this question asks for the inverse of Φ in equation (1) which, of course, does not exist. Clearly, such a matrix would be huge to store but, instead, one can apply the same logic on video blocks [22].\nWe collect a set of training video blocks denoted by xi, i ∈ N of size wp × hp × t = Np. Therefore, the measurement model per block is now yi = Φpxi with size Mp × 1, where Mp = wp × hp and Φp refers to the corresponding measurement matrix per block.\nCollecting a set of N video blocks, we obtain the matrix equation\nY = ΦpX, (2)\nwhere Y = [y1, . . . ,yN ], X = [x1, . . . ,xN ] and Φp is the same for all blocks. The linear mapping X = WpY we are after can be calculated as\nmin Wp ‖X −WpY ‖22 →Wp =\n( XY T ) ( Y Y T )−1 , (3)\nwhere Wp is of size Np ×Mp. Intuitively, such an approach would not necessarily be expected to even provide a solution due to ill-posedness. However, it turns out that, if N is sufficiently large and the matrix Φp has at least one nonzero in each row (i.e., sampling each spatial location at least once over time), the estimation of xi’s by the yi’s provides surprisingly good performance.\nSpecifically, we obtain measurements from a test video sequence applying the same Φp per video block and then reconstruct all blocks using the learnt Wp. Figure 2 depicts the average peak signalto-noise ratio (PSNR) and structural similarity metric (SSIM) [38] for the reconstruction of 14 video\nsequences using 2 different realizations of the random binary matrix Φp for varying percentages of nonzero elements. The empty bars for 10− 20% and 10− 30% of nonzeros in realizations 1 and 2, respectively, refer to cases when there was no solution due to the lack of nonzeros at some spatial location. In these experiments wp × hp × t was selected as 8× 8× 16 simulating the reconstruction of 16 frames by a single captured frame and N = 106."
    }, {
      "heading" : "3.2 Measurement Matrix Construction",
      "text" : "Based on the performance in Figure 2, investigating the extension of the linear mapping in (3) to a nonlinear mapping using deep networks seemed increasingly promising. In order for such an approach to be practical, though, reconstruction has to be performed on blocks and each block must be sampled with the same measurement matrix Φp. Furthermore, such a measurement matrix should be realizable in hardware. Hence we propose constructing a Φ which consists of repeated identical building blocks of size ws × hs × t, as presented in Figure 3. Such a matrix can be straightforwardly implemented on existing systems employing DMDs, SLMs or LCoS [3, 9, 21, 29, 37]. At the same time, in systems utilizing translating masks [15, 23], a repeated mask can be printed and shifted appropriately to produce the same effect.\nIn the remainder of this paper, we select a building block of size ws×hs×t = 4×4×16 as a random binary matrix containing 50% of nonzero elements and set wp × hp × t = 8 × 8 × 16, such that Np = 1024 and Mp = 64. Therefore, the compression ratio is 1/16. In addition, for the proposed matrix Φ, each 4× 4× 16 block is the same allowing reconstruction for overlapping blocks of size 8× 8× 16 with spatial overlap of 4× 4. Such overlap can usually aid at improving reconstruction quality. The selection of 50% of nonzeros was just a random choice since the results of Figure 2 did not suggest that a specific percentage is particularly beneficial in terms of reconstruction quality."
    }, {
      "heading" : "3.3 Multi-layer Network Architecture",
      "text" : "In this section, we extend the linear formulation to MLPs and investigate the performance in deeper structures. We consider an MLP architecture to learn a nonlinear function f(·) that maps a measured frame patch yi via several hidden layers to a video block xi, as illustrated in Figure 4.\nEach hidden layer Lk, k = 1, . . . ,K is defined as\nhk(y) = σ(bk +Wky), (4)\nwhere bk ∈ RNp is the bias vector and Wk is the output weight matrix, containing linear filters. W1 ∈ RNp×Mp connects yi to the first hidden layer, while for the remaining hidden layers, W2−K ∈ RNp×Np . The last hidden layer is connected to the output layer via bo ∈ RNp and Wo ∈ RNp×Np without nonlinearity. The non-linear function σ(·) is the rectified linear unit (ReLU) [25] defined as, σ(y) = max(0, y).\nThe MLP architecture was chosen due to the following considerations; there is a need for at least one fully-connected layer (the first one) that would provide a 3D signal from the compressed 2D measurements. Following that, one could argue that the subsequent layers could be 3D Convolutional layers [35]. Although that would sound reasonable, in practice, the small size of blocks used in this paper (8×8×16) do not allow for convolutions to be effective. Such small block sizes have provided good reconstruction quality in dictionary learning approaches used for CS video reconstruction [22].\nThus, MLPs were considered more reasonable in our work and we found that when applied in 8×8×16 blocks they capture the motion and spatial details of videos adequately. Besides, increasing the size of blocks would dramatically increase the network complexity in 3D volumes such as in videos.\nTo train the proposed MLP, we learn all the weights and biases of the model. The set of parameters is denoted as θ = {b1−K ,bo,W1−K ,Wo} and is updated by the backpropagation algorithm [31] minimizing the quadratic error between the set of training mapped measurements f(yi; θ) and the corresponding video blocks xi. The loss function is the mean squared error (MSE) which is given by\nL(θ) = 1\nN\nN∑\ni=1\n‖f(yi; θ)− xi‖22. (5)\nThe MSE was used in this work since our goal is to optimize the PSNR which is directly related to the MSE."
    }, {
      "heading" : "4 Experiments",
      "text" : "We compare our proposed deep architecture with state-of-the-art approaches both quantitatively and qualitatively. The proposed approaches are evaluated assuming noiseless measurements or under the presence of measurement noise. Finally, we investigate the performance of our methods under different network parameters (e.g., number of layers) and size of training samples. The metrics used for evaluation were the PSNR and SSIM."
    }, {
      "heading" : "4.1 Training Data Collection",
      "text" : "For deep neural networks, increasing the number of training samples is usually synonymous to improved performance. We collected a diverse set of training samples using 400 high-definition videos from Youtube, depicting natural scenes. The video sequences contain more than 105 frames which were converted to grayscale. All videos are unrelated to the test set. We randomly extracted 10 million video blocks of size wp × hp × t while keeping the amount of blocks extracted per video proportional to its duration. This data was used as output while the corresponding input was obtained by multiplying each sample with the measurement matrix Φp (see subsection 3.2 for details). Example frames from the video sequences used for training are shown in Figure 5."
    }, {
      "heading" : "4.2 Implementation Details",
      "text" : "Our networks were trained for up to 4× 106 iterations using a mini-batch size of 200. We normalized the input per-feature to zero mean and standard deviation one. The weights of each layer were initialized to random values uniformly distributed in (−1/√s, 1/√s), where s is the size of the previous layer [10]. We used Stochastic Gradient Descent (SGD) with a starting learning rate of 0.01, which was divided by 10 after 3× 106 iterations. The momentum was set to 0.9 and we further used\n`2 norm gradient clipping to keep the gradients in a certain range. Gradient clipping is a widely used technique in recurrent neural networks to avoid exploding gradients [27]. The threshold of gradient clipping was set to 10."
    }, {
      "heading" : "4.3 Comparison with Previous Methods",
      "text" : "We compare our method with the state-of-the-art video compressive sensing methods:\n• GMM-TP, a Gaussian mixture model (GMM)-based algorithm [42]. • MMLE-GMM, a maximum marginal likelihood estimator (MMLE), that maximizes the\nlikelihood of the GMM of the underlying signals given only their linear compressive measurements [41].\nFor temporal CS reconstruction, data driven models usually perform better than standard sparsitybased schemes [41, 42]. Indeed, both GMM-TP and MMLE-GMM have demonstrated superior performance compared to existing approaches in the literature such as Total-Variation (TV) or dictionary learning [22, 41, 42], hence we did not include experiments with the latter methods.\nIn GMM-TP [42] we followed the settings proposed by the authors and used our training data (randomly selecting 20, 000 samples) to train the underlying GMM parameters. We found that our training data provided better performance compared to the data used by the authors. In our experiments we denote this method by GMM-4 to denote reconstruction of overlapping blocks with spatial overlap of 4× 4 pixels, as discussed in subsection 3.2. MMLE [41] is a self-training method but it is sensitive to initialization. A satisfactory performance is obtained only when MMLE is combined with a good starting point. In [41], the GMM-TP [42] with full overlapping patches (denoted in our experiments as GMM-1) was used to initialize the MMLE. We denote the combined method as GMM-1+MMLE. For fairness, we also conducted experiments in the case where our method is used as a starting point for the MMLE.\nIn our methods, a collection of overlapping patches of size wp × hp is extracted by each coded measurement of size Wf × Hf and subsequently reconstructed into video blocks of size wp × hp × t. Overlapping areas of the recovered video blocks are then averaged to obtain the final video reconstruction results, as depicted in Figure 4. The step of the overlapping patches was set to 4× 4 due to the special construction of the utilized measurement matrix, as discussed in subsection 3.2.\nWe consider six different architectures:\n• W-10M, a simple linear mapping (equation (3)) trained on 10× 106 samples. • FC4-1M, a K = 4 MLP trained on 1× 106 samples (randomly selected from our 10× 106\nsamples). • FC4-10M, a K = 4 MLP trained on 10× 106 samples. • FC7-1M, a K = 7 MLP trained on 1× 106 samples (randomly selected from our 10× 106\nsamples). • FC7-10M, a K = 7 MLP trained on 10× 106 samples. • FC7-10M+MMLE, a K = 7 MLP trained on 10 × 106 samples which is used as an\ninitialization to the MMLE [41] method.\nNote that the subset of randomly selected 1 million samples used for training FC4-1M and FC7-1M was the same.\nOur test set consists of 14 video sequences. They involve a set of videos that were used for dictionary training in [22], provided by the authors, as well as the “Basketball” video sequence used by [41]. All video sequences are unrelated to the training set (see subsection 4.1 for details). For fair comparisons, the same measurement mask was used in all methods, according to subsection 3.2. All code implementations are publicly available provided by the authors."
    }, {
      "heading" : "4.4 Reconstruction Results",
      "text" : "Quantitative reconstruction results for all video sequences with all tested algorithms are illustrated in Table 1 and average performance is summarized in Figure 7. The presented metrics refer to average performance for the reconstruction of the first 32 frames of each video sequence, using 2 consequtive captured coded frames through the video CS measurement model of equation (1). In both, Table 1 and Figure 7, results are divided in two parts. The first part lists reconstruction performance of the tested approaches without the MMLE step, while the second compares the performance of the best candidate proposed and previous methods with a subsequent MMLE step [41]. In Table 1 the best performing algorithms are highlighted for each part while the bottom row presents average reconstruction time requirements for the recovery of 16 video frames using 1 captured coded frame.\nOur FC7-10M and FC7-10M+MMLE yield the highest PSNR and SSIM values for all video sequences. Specifically, the average PSNR improvement of FC7-10M over the GMM-1 [41] is 2.15 dB. When these two methods are used to initialize the MMLE [41] algorithm, the average PSNR gain of FC7-10M+MMLE over the GMM-1+MMLE [41] is 1.67 dB. Notice also that the FC7-10M achieves 1.01 dB higher than the combined GMM-1+MMLE. The highest PSNR and SSIM values are reported in the FC7-10M+MMLE method with 33.58 dB average PSNR over all test sequences. However, the average reconstruction time for the reconstruction of 16 frames using this method is\nalmost two hours while for the second best, the FC7-10M, is about 12 seconds, with average PSNR 32.93 dB. We conclude that, when time is critical, FC7-10M should be the preferred reconstruction method.\nQualitative results of selected video frames are shown in Figure 6. The proposed MLP architectures, including the linear regression model, favorably recover motion while the additional hidden layers emphasize on improving the spatial resolution of the scene. One can clearly observe the sharper edges and high frequency details produced by the FC7-10M and FC7-10M+MMLE methods compared to previously proposed algorithms.\nDue to the extremely long reconstruction times of previous methods, the results presented in Table 1 and Figure 7 refer to only the first 32 frames of each video sequence, as mentioned above. Figure 9 compares the PSNR for all the frames of 3 video sequences using our FC7-10M algorithm and the fastest previous method GMM-4 [42], while Figure 8 depicts representative snapshots for some of them. The varying PSNR performance across the frames of a 16 frame block is consistent for both algorithms and is reminiscent of the reconstruction tendency observed in other video CS papers in the literature [15, 23, 41, 42]."
    }, {
      "heading" : "4.5 Reconstruction Results with Noise",
      "text" : "Previously, we evaluated the proposed algorithms assuming noiseless measurements. In this subsection, we investigate the performance of the presented deep architectures under the presence of measurement noise. Specifically, the measurement model of equation (1) is now modified to\ny = Φx + n, (6)\nwhere n : Mf × 1 is the additive measurement noise vector. We employ our best architecture utilizing K = 7 hidden layers and follow two different training schemes. In the first one, the network is trained on the 10×106 samples, as discussed in subsection 4.3 (i.e., the same FC7-10M network as before) while in the second, the network is trained using the same data pairs {yi,xi} after adding random Gaussian noise to each vector yi. Each vector yi was corrupted with a level of noise such that signal-to-noise ratio (SNR) is uniformly selected in the range between 20− 40 dB giving rise to a set of 10× 106 noisy samples for training. We denote the network trained on the noisy dataset as FC7N-10M.\nWe now compare the performance of the two proposed architectures with the previous methods GMM-4 and GMM-1 using measurement noise. We did not include experiments with the MMLE counterparts of the algorithms since, as we observed earlier, the performance improvement is always related to the starting point of the MMLE algorithm. Figure 10 shows the average performance comparison for the reconstruction of the first 32 frames of each tested video sequence under different levels of measurement noise while Figure 11 depicts example reconstructed frames.\nAs we can observe, the network trained on noiseless data (FC7-10M) provides good performance for low measurement noise (e.g., 40 dB) and reaches similar performance to GMM-1 for more severe noise levels (e.g., 20 dB). The network trained on noisy data (FC7N-10M), proves more robust to noise severity achieving better performance than GMM-1 under all tested noise levels.\nDespite proving more robust to noise, our algorithms in general recover motion favorably but, for high noise levels, there is additive noise throughout the reconstructed scene (observe results for 20 dB noise level in Figure 11). Such degradation could be combated by cascading our architecture with a denoising deep architecture (e.g., [2]) or denoising algorithm to remove the noise artifacts. Ideally, for a specific camera system, data would be collected using this system and trained such that the deep architecture incorporates the noise characteristics of the underlying sensor."
    }, {
      "heading" : "4.6 Run Time",
      "text" : "Run time comparisons for several methods are illustrated at the bottom row of Table 1. All previous approaches are implemented in MATLAB. Our deep learning methods are implemented in Caffe package [14] and all algorithms were executed by the same machine. We observe that the deep learning approaches significantly outperform the previous approaches in order of several magnitudes.\nNote that a direct comparison between the methods is not trivial due to the different implementations. Nevertheless, previous methods solve an optimization problem during reconstruction while our MLP is a feed-forward network that requires only few matrix-vector multiplications."
    }, {
      "heading" : "4.7 Number of Layers and Dataset Size",
      "text" : "From Figure 7 we observe that as the number of training samples increase the performance consistently improves. However, the improvement achieved by increasing the number of layers (from 4 to 7) for architectures trained on small datasets (e.g., 1M) is not significant. This is perhaps expected as one may argue that in order to achieve higher performance with extra layers (thus, more parameters to train) more training data would be required. Intuitively, adding hidden layers enables the network to learn more complex functions. Indeed, reconstruction performance in our 10 million dataset is higher in FC7-10M than in FC4-10M. Increasing the number of hidden layers further did not help in our experiments as we did not observe any additional performance improvement."
    }, {
      "heading" : "5 Conclusions",
      "text" : "To the best of our knowledge, this work constitutes the first deep learning architecture for temporal video compressive sensing reconstruction. We demonstrated superior performance compared to existing algorithms while reducing reconstruction time to a few seconds. At the same time, we focused on the applicability of our framework on existing compressive camera architectures suggesting that their commercial use could be viable. We believe that this work can be extended in three directions: 1) exploring the performance of variant architectures such as RNNs, 2) investigate the training of deeper architectures and 3) finally, examine the reconstruction performance in real video sequences acquired by a temporal compressive sensing camera."
    } ],
    "references" : [ {
      "title" : "Adaptive multi-column deep neural networks with application to robust image denoising",
      "author" : [ "F. Agostinelli", "M.R. Anderson", "H. Lee" ],
      "venue" : "Adv. Neural Inf. Process. Syst. 26, pages 1493–1501.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Image denoising: Can plain neural networks compete with BM3D? In Proc",
      "author" : [ "H.C. Burger", "C.J. Schuler", "S. Harmeling" ],
      "venue" : "IEEE Conf. Comp. Vision Pattern Recognition, pages 2392–2399, June",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "FPA-CS: Focal plane array-based compressive imaging in short-wave infrared",
      "author" : [ "H. Chen", "M.S. Asif", "A.C. Sankaranarayanan", "A. Veeraraghavan" ],
      "venue" : "Proc. IEEE Conf. Comp. Vision Pattern Recognition, pages 2358–2366, June",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "High speed single-pixel imaging via time domain compressive sampling",
      "author" : [ "H. Chen", "Z. Weng", "Y. Liang", "C. Lei", "F. Xing", "M. Chen", "S. Xie" ],
      "venue" : "CLEO: 2014, page JTh2A.132. Optical Society of America,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Deep network cascade for image super-resolution",
      "author" : [ "Z. Cui", "H. Chang", "S. Shan", "B. Zhong", "X. Chen" ],
      "venue" : "Computer Vision – ECCV 2014, volume 8693 of Lecture Notes in Computer Science, pages 49–64. Springer International Publishing,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Image super-resolution using deep convolutional networks",
      "author" : [ "C. Dong", "C. Loy", "K. He", "X. Tang" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell., 38(2):295–307, Feb.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Single-Pixel imaging via compressive sampling",
      "author" : [ "M.F. Duarte", "M.A. Davenport", "D. Takhar", "J.N. Laska", "T. Sun", "K.F. Kelly", "R.G. Baraniuk" ],
      "venue" : "IEEE Signal Process. Mag., 25(2):83–91, Mar.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Smart pixel imaging with computational-imaging arrays",
      "author" : [ "C. Fernandez-Cull", "B.M. Tyrrell", "R. D’Onofrio", "A. Bolstad", "J. Lin", "J.W. Little", "M. Blackwell", "M. Renzi", "M. Kelly" ],
      "venue" : "In Proc. SPIE,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2014
    }, {
      "title" : "Single-Shot compressed ultrafast photography at one hundred billion frames per second",
      "author" : [ "L. Gao", "J. Liang", "C. Li", "L.V. Wang" ],
      "venue" : "Nature, 516:74–77,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "X. Glorot", "Y. Bengio" ],
      "venue" : "Proc. Int. Conf. Artificial Intelligence and Statistics,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "K. He", "X. Zhang", "S. Ren", "J. Sun" ],
      "venue" : "CoRR, abs/1512.03385,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Flutter shutter video camera for compressive sensing of videos",
      "author" : [ "J. Holloway", "A.C. Sankaranarayanan", "A. Veeraraghavan", "S. Tambe" ],
      "venue" : "IEEE Int. Conf. Computational Photography, pages 1–9, April",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Bidirectional recurrent convolutional networks for multi-frame super-resolution",
      "author" : [ "Y. Huang", "W. Wang", "L. Wang" ],
      "venue" : "Adv. Neural Inf. Process. Syst. 28, pages 235–243.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Caffe: Convolutional architecture for fast feature embedding",
      "author" : [ "Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell" ],
      "venue" : "Proc. ACM Int. Conf. Multimedia, MM ’14, pages 675–678, New York, NY, USA,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "High spatio-temporal resolution video with compressed sensing",
      "author" : [ "R. Koller", "L. Schmid", "N. Matsuda", "T. Niederberger", "L. Spinoulas", "O. Cossairt", "G. Schuster", "A.K. Katsaggelos" ],
      "venue" : "Opt. Express, 23(12):15992– 16007, June",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "Adv. Neural Inf. Process. Syst. 25, pages 1097–1105.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "ReconNet: Non-iterative reconstruction of images from compressively sensed random measurements",
      "author" : [ "K. Kulkarni", "S. Lohit", "P.K. Turaga", "R. Kerviche", "A. Ashok" ],
      "venue" : "CoRR, abs/1601.06892,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Deep learning",
      "author" : [ "Y. LeCun", "Y. Bengio", "G. Hinton" ],
      "venue" : "Nature, 521(7553):436–444, May",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Backpropagation applied to handwritten zip code recognition",
      "author" : [ "Y. LeCun", "B. Boser", "J. Denker", "D. Henderson", "R. Howard", "W. Hubbard", "L. Jackel" ],
      "venue" : "Neural Computation, 1(4):541–551, Dec.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "Proc. IEEE, 86(11):2278–2324, Nov.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Efficient space-time sampling with pixel-wise coded exposure for high speed imaging",
      "author" : [ "D. Liu", "J. Gu", "Y. Hitomi", "M. Gupta", "T. Mitsunaga", "S.K. Nayar" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell., 99:1,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Efficient space-time sampling with pixel-wise coded exposure for high-speed imaging",
      "author" : [ "D. Liu", "J. Gu", "Y. Hitomi", "M. Gupta", "T. Mitsunaga", "S.K. Nayar" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell., 36(2):248–260, Feb",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Coded aperture compressive temporal imaging",
      "author" : [ "P. Llull", "X. Liao", "X. Yuan", "J. Yang", "D. Kittle", "L. Carin", "G. Sapiro", "D.J. Brady" ],
      "venue" : "Opt. Express, 21(9):10526–10545, May",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A deep learning approach to structured signal recovery",
      "author" : [ "A. Mousavi", "A.B. Patel", "R.G. Baraniuk" ],
      "venue" : "CoRR, abs/1508.04065,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Rectified linear units improve restricted boltzmann machines",
      "author" : [ "V. Nair", "G.E. Hinton" ],
      "venue" : "Proc. Int. Conf. Machine Learning, pages 807–814,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Real time compressive sensing video reconstruction in hardware",
      "author" : [ "G. Orchard", "J. Zhang", "Y. Suo", "M. Dao", "D.T. Nguyen", "S. Chin", "C. Posch", "T.D. Tran", "R. Etienne- Cummings" ],
      "venue" : "IEEE Trans. Emerg. Sel. Topics Circuits Syst., 2(3):604–615, Sept.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "On the difficulty of training recurrent neural networks",
      "author" : [ "R. Pascanu", "T. Mikolov", "Y. Bengio" ],
      "venue" : "ICML (3), volume 28 of JMLR Proceedings, pages 1310–1318. JMLR.org,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Learning to segment object candidates",
      "author" : [ "P.O. Pinheiro", "R. Collobert", "P. Dollár" ],
      "venue" : "CoRR, abs/1506.06204,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "P2C2: Programmable pixel compressive camera for high speed imaging",
      "author" : [ "D. Reddy", "A. Veeraraghavan", "R. Chellappa" ],
      "venue" : "Proc. IEEE Conf. Comp. Vision Pattern Recognition, pages 329–336, June",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Shepard convolutional neural networks",
      "author" : [ "J.S. Ren", "L. Xu", "Q. Yan", "W. Sun" ],
      "venue" : "Adv. Neural Inf. Process. Syst. 28, pages 901–909.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Neurocomputing: Foundations of research",
      "author" : [ "D.E. Rumelhart", "G.E. Hinton", "R.J. Williams" ],
      "venue" : "chapter Learning Representations by Back-propagating Errors, pages 696–699. MIT Press, Cambridge, MA, USA,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "A machine learning approach for non-blind image deconvolution",
      "author" : [ "C. Schuler", "H. Burger", "S. Harmeling", "B. Scholkopf" ],
      "venue" : "Proc. IEEE Conf. Comp. Vision Pattern Recognition, pages 1067–1074, June",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Video compressive sensing with on-chip programmable subsampling",
      "author" : [ "L. Spinoulas", "K. He", "O. Cossairt", "A. Katsaggelos" ],
      "venue" : "Proc. IEEE Conf. Comp. Vision Pattern Recognition Workshops, pages 49–57, June",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Learning a convolutional neural network for non-uniform motion blur removal",
      "author" : [ "J. Sun", "W. Cao", "Z. Xu", "J. Ponce" ],
      "venue" : "Proc. IEEE Conf. Comp. Vision Pattern Recognition, pages 769–777, June",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Learning spatiotemporal features with 3d convolutional networks",
      "author" : [ "D. Tran", "L. Bourdev", "R. Fergus", "L. Torresani", "M. Paluri" ],
      "venue" : "IEEE Int. Conf. Computer Vision, pages 4489–4497, Dec",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion",
      "author" : [ "P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.-A. Manzagol" ],
      "venue" : "J. Mach. Learn. Res., 11:3371–3408, Dec.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Lisens- a scalable architecture for video compressive sensing",
      "author" : [ "J. Wang", "M. Gupta", "A.C. Sankaranarayanan" ],
      "venue" : "Proc. IEEE Conf. Comp. Photography, pages 1–9, April",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Image quality assessment: from error visibility to structural similarity",
      "author" : [ "Z. Wang", "A.C. Bovik", "H. Sheikh", "E.P. Simoncelli" ],
      "venue" : "IEEE Trans. Image Process., 13(4):600–612, April",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Image denoising and inpainting with deep neural networks",
      "author" : [ "J. Xie", "L. Xu", "E. Chen" ],
      "venue" : "Adv. Neural Inf. Process. Syst. 25, pages 341–349.",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Deep convolutional neural network for image deconvolution",
      "author" : [ "L. Xu", "J.S. Ren", "C. Liu", "J. Jia" ],
      "venue" : "Adv. Neural Inf. Process. Syst. 27, pages 1790–1798.",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Compressive sensing by learning a gaussian mixture model from measurements",
      "author" : [ "J. Yang", "X. Liao", "X. Yuan", "P. Llull", "D.J. Brady", "G. Sapiro", "L. Carin" ],
      "venue" : "IEEE Trans. Image Processing, 24(1):106–119, Jan.",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Video compressive sensing using gaussian mixture models",
      "author" : [ "J. Yang", "X. Yuan", "X. Liao", "P. Llull", "D.J. Brady", "G. Sapiro", "L. Carin" ],
      "venue" : "IEEE Trans. Image Processing, 23(11):4863–4878, Nov.",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "Spatial video CS architectures stem from the well-known single-pixel-camera [7], which performs spatial multiplexing per measurement, and enable video recovery by expediting the capturing process.",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 3,
      "context" : "They either employ fast readout circuitry to capture information at video rates [4] or parallelize the single-pixel architecture using multiple sensors, each one responsible for sampling a separate spatial area of the scene [3, 37].",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 2,
      "context" : "They either employ fast readout circuitry to capture information at video rates [4] or parallelize the single-pixel architecture using multiple sensors, each one responsible for sampling a separate spatial area of the scene [3, 37].",
      "startOffset" : 224,
      "endOffset" : 231
    }, {
      "referenceID" : 36,
      "context" : "They either employ fast readout circuitry to capture information at video rates [4] or parallelize the single-pixel architecture using multiple sensors, each one responsible for sampling a separate spatial area of the scene [3, 37].",
      "startOffset" : 224,
      "endOffset" : 231
    }, {
      "referenceID" : 2,
      "context" : "Digital micromirror devices (DMD), spatial light modulators (SLM) and liquid crystal on silicon (LCoS) were used in [3, 37, 9, 21, 29] while translating printed masks were employed in [15, 23].",
      "startOffset" : 116,
      "endOffset" : 134
    }, {
      "referenceID" : 36,
      "context" : "Digital micromirror devices (DMD), spatial light modulators (SLM) and liquid crystal on silicon (LCoS) were used in [3, 37, 9, 21, 29] while translating printed masks were employed in [15, 23].",
      "startOffset" : 116,
      "endOffset" : 134
    }, {
      "referenceID" : 8,
      "context" : "Digital micromirror devices (DMD), spatial light modulators (SLM) and liquid crystal on silicon (LCoS) were used in [3, 37, 9, 21, 29] while translating printed masks were employed in [15, 23].",
      "startOffset" : 116,
      "endOffset" : 134
    }, {
      "referenceID" : 20,
      "context" : "Digital micromirror devices (DMD), spatial light modulators (SLM) and liquid crystal on silicon (LCoS) were used in [3, 37, 9, 21, 29] while translating printed masks were employed in [15, 23].",
      "startOffset" : 116,
      "endOffset" : 134
    }, {
      "referenceID" : 28,
      "context" : "Digital micromirror devices (DMD), spatial light modulators (SLM) and liquid crystal on silicon (LCoS) were used in [3, 37, 9, 21, 29] while translating printed masks were employed in [15, 23].",
      "startOffset" : 116,
      "endOffset" : 134
    }, {
      "referenceID" : 14,
      "context" : "Digital micromirror devices (DMD), spatial light modulators (SLM) and liquid crystal on silicon (LCoS) were used in [3, 37, 9, 21, 29] while translating printed masks were employed in [15, 23].",
      "startOffset" : 184,
      "endOffset" : 192
    }, {
      "referenceID" : 22,
      "context" : "Digital micromirror devices (DMD), spatial light modulators (SLM) and liquid crystal on silicon (LCoS) were used in [3, 37, 9, 21, 29] while translating printed masks were employed in [15, 23].",
      "startOffset" : 184,
      "endOffset" : 192
    }, {
      "referenceID" : 7,
      "context" : "Moreover, a few architectures have eliminated additional optical elements by directly programming the chip’s readout mode through hardware circuitry modifications [8, 26, 33].",
      "startOffset" : 163,
      "endOffset" : 174
    }, {
      "referenceID" : 25,
      "context" : "Moreover, a few architectures have eliminated additional optical elements by directly programming the chip’s readout mode through hardware circuitry modifications [8, 26, 33].",
      "startOffset" : 163,
      "endOffset" : 174
    }, {
      "referenceID" : 32,
      "context" : "Moreover, a few architectures have eliminated additional optical elements by directly programming the chip’s readout mode through hardware circuitry modifications [8, 26, 33].",
      "startOffset" : 163,
      "endOffset" : 174
    }, {
      "referenceID" : 2,
      "context" : ", using sparsity models [3, 12], combining sparsity and dictionary learning [22] or using Gaussian mixture models [41, 42]) are often too computationally intensive, rendering the reconstruction process painfully slow.",
      "startOffset" : 24,
      "endOffset" : 31
    }, {
      "referenceID" : 11,
      "context" : ", using sparsity models [3, 12], combining sparsity and dictionary learning [22] or using Gaussian mixture models [41, 42]) are often too computationally intensive, rendering the reconstruction process painfully slow.",
      "startOffset" : 24,
      "endOffset" : 31
    }, {
      "referenceID" : 21,
      "context" : ", using sparsity models [3, 12], combining sparsity and dictionary learning [22] or using Gaussian mixture models [41, 42]) are often too computationally intensive, rendering the reconstruction process painfully slow.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 40,
      "context" : ", using sparsity models [3, 12], combining sparsity and dictionary learning [22] or using Gaussian mixture models [41, 42]) are often too computationally intensive, rendering the reconstruction process painfully slow.",
      "startOffset" : 114,
      "endOffset" : 122
    }, {
      "referenceID" : 41,
      "context" : ", using sparsity models [3, 12], combining sparsity and dictionary learning [22] or using Gaussian mixture models [41, 42]) are often too computationally intensive, rendering the reconstruction process painfully slow.",
      "startOffset" : 114,
      "endOffset" : 122
    }, {
      "referenceID" : 17,
      "context" : "Deep learning [18] is a burgeoning research field which has demonstrated state-of-the-art performance in a multitude of machine learning and computer vision tasks, such as image recognition [11] or object detection [28].",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 10,
      "context" : "Deep learning [18] is a burgeoning research field which has demonstrated state-of-the-art performance in a multitude of machine learning and computer vision tasks, such as image recognition [11] or object detection [28].",
      "startOffset" : 190,
      "endOffset" : 194
    }, {
      "referenceID" : 27,
      "context" : "Deep learning [18] is a burgeoning research field which has demonstrated state-of-the-art performance in a multitude of machine learning and computer vision tasks, such as image recognition [11] or object detection [28].",
      "startOffset" : 215,
      "endOffset" : 219
    }, {
      "referenceID" : 15,
      "context" : ", a variety of computer vision tasks [16, 20]).",
      "startOffset" : 37,
      "endOffset" : 45
    }, {
      "referenceID" : 19,
      "context" : ", a variety of computer vision tasks [16, 20]).",
      "startOffset" : 37,
      "endOffset" : 45
    }, {
      "referenceID" : 1,
      "context" : ", restoration problems [2, 32, 39]).",
      "startOffset" : 23,
      "endOffset" : 34
    }, {
      "referenceID" : 31,
      "context" : ", restoration problems [2, 32, 39]).",
      "startOffset" : 23,
      "endOffset" : 34
    }, {
      "referenceID" : 38,
      "context" : ", restoration problems [2, 32, 39]).",
      "startOffset" : 23,
      "endOffset" : 34
    }, {
      "referenceID" : 1,
      "context" : "More specifically, investigators have employed a variety of architectures: deep fully-connected networks or multi-layer perceptrons (MLPs) [2, 32]; stacked denoising auto-encoders (SDAEs) [1, 5, 36, 39], which are MLPs whose layers are pre-trained to provide improved weight initializa-",
      "startOffset" : 139,
      "endOffset" : 146
    }, {
      "referenceID" : 31,
      "context" : "More specifically, investigators have employed a variety of architectures: deep fully-connected networks or multi-layer perceptrons (MLPs) [2, 32]; stacked denoising auto-encoders (SDAEs) [1, 5, 36, 39], which are MLPs whose layers are pre-trained to provide improved weight initializa-",
      "startOffset" : 139,
      "endOffset" : 146
    }, {
      "referenceID" : 0,
      "context" : "More specifically, investigators have employed a variety of architectures: deep fully-connected networks or multi-layer perceptrons (MLPs) [2, 32]; stacked denoising auto-encoders (SDAEs) [1, 5, 36, 39], which are MLPs whose layers are pre-trained to provide improved weight initializa-",
      "startOffset" : 188,
      "endOffset" : 202
    }, {
      "referenceID" : 4,
      "context" : "More specifically, investigators have employed a variety of architectures: deep fully-connected networks or multi-layer perceptrons (MLPs) [2, 32]; stacked denoising auto-encoders (SDAEs) [1, 5, 36, 39], which are MLPs whose layers are pre-trained to provide improved weight initializa-",
      "startOffset" : 188,
      "endOffset" : 202
    }, {
      "referenceID" : 35,
      "context" : "More specifically, investigators have employed a variety of architectures: deep fully-connected networks or multi-layer perceptrons (MLPs) [2, 32]; stacked denoising auto-encoders (SDAEs) [1, 5, 36, 39], which are MLPs whose layers are pre-trained to provide improved weight initializa-",
      "startOffset" : 188,
      "endOffset" : 202
    }, {
      "referenceID" : 38,
      "context" : "More specifically, investigators have employed a variety of architectures: deep fully-connected networks or multi-layer perceptrons (MLPs) [2, 32]; stacked denoising auto-encoders (SDAEs) [1, 5, 36, 39], which are MLPs whose layers are pre-trained to provide improved weight initializa-",
      "startOffset" : 188,
      "endOffset" : 202
    }, {
      "referenceID" : 5,
      "context" : "tion; convolutional neural networks (CNNs) [6, 19, 30, 34, 37, 40] and recurrent neural networks (RNNs) [13].",
      "startOffset" : 43,
      "endOffset" : 66
    }, {
      "referenceID" : 18,
      "context" : "tion; convolutional neural networks (CNNs) [6, 19, 30, 34, 37, 40] and recurrent neural networks (RNNs) [13].",
      "startOffset" : 43,
      "endOffset" : 66
    }, {
      "referenceID" : 29,
      "context" : "tion; convolutional neural networks (CNNs) [6, 19, 30, 34, 37, 40] and recurrent neural networks (RNNs) [13].",
      "startOffset" : 43,
      "endOffset" : 66
    }, {
      "referenceID" : 33,
      "context" : "tion; convolutional neural networks (CNNs) [6, 19, 30, 34, 37, 40] and recurrent neural networks (RNNs) [13].",
      "startOffset" : 43,
      "endOffset" : 66
    }, {
      "referenceID" : 36,
      "context" : "tion; convolutional neural networks (CNNs) [6, 19, 30, 34, 37, 40] and recurrent neural networks (RNNs) [13].",
      "startOffset" : 43,
      "endOffset" : 66
    }, {
      "referenceID" : 39,
      "context" : "tion; convolutional neural networks (CNNs) [6, 19, 30, 34, 37, 40] and recurrent neural networks (RNNs) [13].",
      "startOffset" : 43,
      "endOffset" : 66
    }, {
      "referenceID" : 12,
      "context" : "tion; convolutional neural networks (CNNs) [6, 19, 30, 34, 37, 40] and recurrent neural networks (RNNs) [13].",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 23,
      "context" : "Two recent studies, utilizing SDAEs [24] or CNNs [17], have been presented on spatial CS for still images exhibiting promising performance.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 16,
      "context" : "Two recent studies, utilizing SDAEs [24] or CNNs [17], have been presented on spatial CS for still images exhibiting promising performance.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 1,
      "context" : "Our approach differs from prior 2D image restoration architectures [2, 32] since we are recovering a 3D volume from 2D measurements.",
      "startOffset" : 67,
      "endOffset" : 74
    }, {
      "referenceID" : 31,
      "context" : "Our approach differs from prior 2D image restoration architectures [2, 32] since we are recovering a 3D volume from 2D measurements.",
      "startOffset" : 67,
      "endOffset" : 74
    }, {
      "referenceID" : 21,
      "context" : "Clearly, such a matrix would be huge to store but, instead, one can apply the same logic on video blocks [22].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 37,
      "context" : "Figure 2 depicts the average peak signalto-noise ratio (PSNR) and structural similarity metric (SSIM) [38] for the reconstruction of 14 video",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 2,
      "context" : "Such a matrix can be straightforwardly implemented on existing systems employing DMDs, SLMs or LCoS [3, 9, 21, 29, 37].",
      "startOffset" : 100,
      "endOffset" : 118
    }, {
      "referenceID" : 8,
      "context" : "Such a matrix can be straightforwardly implemented on existing systems employing DMDs, SLMs or LCoS [3, 9, 21, 29, 37].",
      "startOffset" : 100,
      "endOffset" : 118
    }, {
      "referenceID" : 20,
      "context" : "Such a matrix can be straightforwardly implemented on existing systems employing DMDs, SLMs or LCoS [3, 9, 21, 29, 37].",
      "startOffset" : 100,
      "endOffset" : 118
    }, {
      "referenceID" : 28,
      "context" : "Such a matrix can be straightforwardly implemented on existing systems employing DMDs, SLMs or LCoS [3, 9, 21, 29, 37].",
      "startOffset" : 100,
      "endOffset" : 118
    }, {
      "referenceID" : 36,
      "context" : "Such a matrix can be straightforwardly implemented on existing systems employing DMDs, SLMs or LCoS [3, 9, 21, 29, 37].",
      "startOffset" : 100,
      "endOffset" : 118
    }, {
      "referenceID" : 14,
      "context" : "At the same time, in systems utilizing translating masks [15, 23], a repeated mask can be printed and shifted appropriately to produce the same effect.",
      "startOffset" : 57,
      "endOffset" : 65
    }, {
      "referenceID" : 22,
      "context" : "At the same time, in systems utilizing translating masks [15, 23], a repeated mask can be printed and shifted appropriately to produce the same effect.",
      "startOffset" : 57,
      "endOffset" : 65
    }, {
      "referenceID" : 24,
      "context" : "The non-linear function σ(·) is the rectified linear unit (ReLU) [25] defined as, σ(y) = max(0, y).",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 34,
      "context" : "Following that, one could argue that the subsequent layers could be 3D Convolutional layers [35].",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 21,
      "context" : "Such small block sizes have provided good reconstruction quality in dictionary learning approaches used for CS video reconstruction [22].",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 30,
      "context" : "The set of parameters is denoted as θ = {b1−K ,bo,W1−K ,Wo} and is updated by the backpropagation algorithm [31] minimizing the quadratic error between the set of training mapped measurements f(yi; θ) and the corresponding video blocks xi.",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 9,
      "context" : "The weights of each layer were initialized to random values uniformly distributed in (−1/√s, 1/√s), where s is the size of the previous layer [10].",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 26,
      "context" : "Gradient clipping is a widely used technique in recurrent neural networks to avoid exploding gradients [27].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 41,
      "context" : "We compare our method with the state-of-the-art video compressive sensing methods: • GMM-TP, a Gaussian mixture model (GMM)-based algorithm [42].",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 40,
      "context" : "• MMLE-GMM, a maximum marginal likelihood estimator (MMLE), that maximizes the likelihood of the GMM of the underlying signals given only their linear compressive measurements [41].",
      "startOffset" : 176,
      "endOffset" : 180
    }, {
      "referenceID" : 40,
      "context" : "For temporal CS reconstruction, data driven models usually perform better than standard sparsitybased schemes [41, 42].",
      "startOffset" : 110,
      "endOffset" : 118
    }, {
      "referenceID" : 41,
      "context" : "For temporal CS reconstruction, data driven models usually perform better than standard sparsitybased schemes [41, 42].",
      "startOffset" : 110,
      "endOffset" : 118
    }, {
      "referenceID" : 21,
      "context" : "Indeed, both GMM-TP and MMLE-GMM have demonstrated superior performance compared to existing approaches in the literature such as Total-Variation (TV) or dictionary learning [22, 41, 42], hence we did not include experiments with the latter methods.",
      "startOffset" : 174,
      "endOffset" : 186
    }, {
      "referenceID" : 40,
      "context" : "Indeed, both GMM-TP and MMLE-GMM have demonstrated superior performance compared to existing approaches in the literature such as Total-Variation (TV) or dictionary learning [22, 41, 42], hence we did not include experiments with the latter methods.",
      "startOffset" : 174,
      "endOffset" : 186
    }, {
      "referenceID" : 41,
      "context" : "Indeed, both GMM-TP and MMLE-GMM have demonstrated superior performance compared to existing approaches in the literature such as Total-Variation (TV) or dictionary learning [22, 41, 42], hence we did not include experiments with the latter methods.",
      "startOffset" : 174,
      "endOffset" : 186
    }, {
      "referenceID" : 41,
      "context" : "In GMM-TP [42] we followed the settings proposed by the authors and used our training data (randomly selecting 20, 000 samples) to train the underlying GMM parameters.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 40,
      "context" : "MMLE [41] is a self-training method but it is sensitive to initialization.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 40,
      "context" : "In [41], the GMM-TP [42] with full overlapping patches (denoted in our experiments as GMM-1) was used to initialize the MMLE.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 41,
      "context" : "In [41], the GMM-TP [42] with full overlapping patches (denoted in our experiments as GMM-1) was used to initialize the MMLE.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 40,
      "context" : "• FC7-10M+MMLE, a K = 7 MLP trained on 10 × 10 samples which is used as an initialization to the MMLE [41] method.",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 40,
      "context" : "Figure 6: Qualitative reconstruction comparison of frames from two video sequences between our methods and GMM-1 [41], GMM-1+MMLE [41].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 40,
      "context" : "Figure 6: Qualitative reconstruction comparison of frames from two video sequences between our methods and GMM-1 [41], GMM-1+MMLE [41].",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 21,
      "context" : "They involve a set of videos that were used for dictionary training in [22], provided by the authors, as well as the “Basketball” video sequence used by [41].",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 40,
      "context" : "They involve a set of videos that were used for dictionary training in [22], provided by the authors, as well as the “Basketball” video sequence used by [41].",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 40,
      "context" : "The first part lists reconstruction performance of the tested approaches without the MMLE step, while the second compares the performance of the best candidate proposed and previous methods with a subsequent MMLE step [41].",
      "startOffset" : 218,
      "endOffset" : 222
    }, {
      "referenceID" : 41,
      "context" : "Reconstruction Method Video Sequence Metric W-10M FC7-10M GMM-4 [42] GMM-1 [41] FC7-10M +MMLE GMM-1 +MMLE [41] Electric Ball PSNR 40.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 40,
      "context" : "Reconstruction Method Video Sequence Metric W-10M FC7-10M GMM-4 [42] GMM-1 [41] FC7-10M +MMLE GMM-1 +MMLE [41] Electric Ball PSNR 40.",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 40,
      "context" : "Reconstruction Method Video Sequence Metric W-10M FC7-10M GMM-4 [42] GMM-1 [41] FC7-10M +MMLE GMM-1 +MMLE [41] Electric Ball PSNR 40.",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 40,
      "context" : "Specifically, the average PSNR improvement of FC7-10M over the GMM-1 [41] is 2.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 40,
      "context" : "When these two methods are used to initialize the MMLE [41] algorithm, the average PSNR gain of FC7-10M+MMLE over the GMM-1+MMLE [41] is 1.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 40,
      "context" : "When these two methods are used to initialize the MMLE [41] algorithm, the average PSNR gain of FC7-10M+MMLE over the GMM-1+MMLE [41] is 1.",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 41,
      "context" : "Figure 8: Qualitative reconstruction performance of video frames between the proposed method FC7-10M and the previous method GMM-4 [42].",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 41,
      "context" : "Figure 9: PSNR comparison for all the frames of 3 video sequences between the proposed method FC7-10M and the previous method GGM-4 [42].",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 41,
      "context" : "Figure 9 compares the PSNR for all the frames of 3 video sequences using our FC7-10M algorithm and the fastest previous method GMM-4 [42], while Figure 8 depicts representative snapshots for some of them.",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 14,
      "context" : "The varying PSNR performance across the frames of a 16 frame block is consistent for both algorithms and is reminiscent of the reconstruction tendency observed in other video CS papers in the literature [15, 23, 41, 42].",
      "startOffset" : 203,
      "endOffset" : 219
    }, {
      "referenceID" : 22,
      "context" : "The varying PSNR performance across the frames of a 16 frame block is consistent for both algorithms and is reminiscent of the reconstruction tendency observed in other video CS papers in the literature [15, 23, 41, 42].",
      "startOffset" : 203,
      "endOffset" : 219
    }, {
      "referenceID" : 40,
      "context" : "The varying PSNR performance across the frames of a 16 frame block is consistent for both algorithms and is reminiscent of the reconstruction tendency observed in other video CS papers in the literature [15, 23, 41, 42].",
      "startOffset" : 203,
      "endOffset" : 219
    }, {
      "referenceID" : 41,
      "context" : "The varying PSNR performance across the frames of a 16 frame block is consistent for both algorithms and is reminiscent of the reconstruction tendency observed in other video CS papers in the literature [15, 23, 41, 42].",
      "startOffset" : 203,
      "endOffset" : 219
    }, {
      "referenceID" : 1,
      "context" : ", [2]) or denoising algorithm to remove the noise artifacts.",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 13,
      "context" : "Our deep learning methods are implemented in Caffe package [14] and all algorithms were executed by the same machine.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 41,
      "context" : "Figure 11: Qualitative reconstruction comparison between our methods and GMM-4 [42], GMM1 [41] under different levels of measurement noise.",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 40,
      "context" : "Figure 11: Qualitative reconstruction comparison between our methods and GMM-4 [42], GMM1 [41] under different levels of measurement noise.",
      "startOffset" : 90,
      "endOffset" : 94
    } ],
    "year" : 2016,
    "abstractText" : "In this work we present a deep learning framework for video compressive sensing. The proposed formulation enables recovery of video frames in a few seconds at significantly improved reconstruction quality compared to previous approaches. Our investigation starts by learning a linear mapping between video sequences and corresponding measured frames which turns out to provide promising results. We then extend the linear formulation to deep fully-connected networks and explore the performance gains using deeper architectures. Our analysis is always driven by the applicability of the proposed framework on existing compressive video architectures. Extensive simulations on several video sequences document the superiority of our approach both quantitatively and qualitatively. Finally, our analysis offers insights into understanding how dataset sizes and number of layers affect reconstruction performance while raising a few points for future investigation.",
    "creator" : "LaTeX with hyperref package"
  }
}