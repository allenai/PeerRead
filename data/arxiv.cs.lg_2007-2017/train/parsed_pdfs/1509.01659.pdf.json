{
  "name" : "1509.01659.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Introduction to Gravitational Clustering",
    "authors" : [ "Armen Aghajanyan" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 9.\n01 65\n9v 1\n[ cs\n.L G\n] 5\nS ep\n2 01\n5 PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. Y, FEBRUARY 2015 1\nKeywords—Machine Learning, Classification, Clustering.\nI. Introduction\nThe name of this algorithm is derived from the metaphor that the algorithm was built upon. Each cluster is symbolic of a planet,and each planet has a mass and a radius as well as the class that it represents. But unlike real life planets, our planets are static with respect to other planets. The process of training can be conceptually thought of as building a universe. The process of predicting is simply placing a mass in the universe and tracing what planet it will appear on.\nThis algorithm exhibits three nice properties:\n1) Ability to learn from a few samples. 2) Ability to weight the importance of training vectors. 3) The nature of the algorithm makes it resilient to\noverfitting.\nThe ability to weight the importance of training vectors as well as the ability to learn from a few samples allows us to model a system that supports the notion of prototypes, e.g. Eleanor Rosch (Lakoff, 1987)(P. 41)."
    }, {
      "heading" : "II. Definition",
      "text" : "Let us start by mathematically defining what each one of our symbolic structures will be. The most important structure is our cluster or our planet. We will define the planet as containing a dynamic mass m, dynamic radius r,\ndynamic position −→x and a static class θ. Mathematically:\nm ∈ R\nr ∈ R −→x ∈ Rn\nθ ∈ Z\nP = {m, r,−→x , θ}\n(1)\nOur universe will simply consist of a set of planets. The universe will also hold a couple of global constants. The initial radius of a planet that has just been created which we will denote with r′. The so called percent step, which represents the amount a test mass moves before recalculating the new forces on the test mass. We will denote this with the Greek α. The amount of steps taken or iterations will be denoted with β. The distance between planets will be calculated with the function denoted D(−→x ,−→y )."
    }, {
      "heading" : "III. Training Model",
      "text" : "One of the better aspects of the model is its ability to rate your feature vectors. To do so, let us define a hybrid feature vector h.\nh = {−→x ,m, θ} (2)\nThe m variable allows us to rate the value of the feature vector. For example if you have a probabilistic diagnosis, each feature vector will contain the class of the diagnosis as well as the probability of the diagnosis represented by the mass. The training is quite simple. Below is the pseudocode.\nnearplanets ← Find Planets in Radius of −→ h x; nearplanets ← nearplanets where Pθ = hθ ; if nearplanets is Empty then\nUniverse Add Planet {m = hm, r = r ′,−→x = −→ h x, θ = hθ}\nelse p ← planet that generates most force ∈ nearplanets ; Universe update p ←\n{m = pm + hm, r = m pr pm ,−→x = pm m −→p x + hm m −→ h x}\nend Algorithm 1: Training Algorithm\nThe new position is a weighted sum of the two position vectors with respect to their weight."
    }, {
      "heading" : "A. Asymptotic Analysis",
      "text" : "Our training simply traverses through all of the planets in the universe and computes the distance from the training sample. Saying N is the amount of planets and D is the dimensionality of our feature vectors. Assuming that the planet exists, we get\nO (D ∗N) = O (N) (3)\nUsing a KD-Tree (Bentley, 1975) will allow us to train with the average asymptotic of\nO (D ∗ logN) = O (logN) (4)\nOn the flip side, assuming we have to add the planet:\nO (D ∗N +Nnear) = O (N +Nnear) (5)\nKD-Tree (Bentley, 1975)\nO (D ∗ logN +Nnear) = O (logN +Nnear) (6)\nThis is the asymptotic of adding a single train vector. Stating that Ns is the number of samples we end up with the final equation being.\nO (Ns(logN +Nn)) (7)"
    }, {
      "heading" : "B. Comparison of Training Times",
      "text" : "Gravitational Clustering\nK-Means SVM Decision Trees\nBig O O (Ns(log N + Nn)) O(nDk+1 log n) O(n3 ) O(nsD log(ns)) Online Training Yes Yes No Partial Variant Importance Yes No No No\n• Nn is synonymous with Nnear • Ns is synonymous with Nsamples"
    }, {
      "heading" : "IV. Simulation Testing Model",
      "text" : "Metaphorically, predicting the class of a new point is equivalent to dropping a piece of mass into the universe and tracing the mass until it collides with a planet. In this metaphor, we assume that the planets are infinitely small and therefore there will be no interference. Our test point will simply be defined as l = {−→x } Let us first define getting the normalized directional force vector. Recall from physics that the gravitational force between two planets is\nF = G m1m2\nr2 (8)\nIn our case, we will assume that the mass of each test point is equal to every other, therefore we can disregard the mass. We can also remove the G constant. Our hybrid force equation per planet p is now:\nF = pm\nr2 (9)\nWhere r is D(−→p x, −→ l x). We define the total normalized force on our test mass with the custom equation.\nFnet = ∑ p∈Universe pm∗( −→p x− −→ l x) r2 Fnorm = α\n||Fnet|| Fnet\n(10)\nTo restate, α is the percent step taken with respect to the force. Now let us describe the simulation algorithm:\npos ← −→ l x; for i in i [0, β] step 1 do\nforce ← ∑\np∈Universe pm∗( −→p x−pos) r2\n; ; norm ← α||force||force ; pos ← pos + norm\nend nearplanets ← Find Planets in Radius of pos; if nearplanets is not Empty then\nreturn mode[nearplanets θ] else\nreturn [planet closest to pos] θ end"
    }, {
      "heading" : "A. Asymptotic Analysis of Simulation Testing Model",
      "text" : "Let us state that N is the number of planets and D is the dimensionality of our feature vector. Calculating the force takes up\nO (4D ∗N) = O (N) (11)\nThe 4 comes from the vector arithmetic that needed to be done. One subtraction, one multiplication, one distance squared, one division. The N term came from the summation. The total simulation next becomes.\nO (7D ∗N ∗ β +N) (12)\nThe 3 more D terms come from: finding the magnitude, multiplying by force (simultaneously multiplying by α) and the update summation. The next N came from finding the planets with the radius containing pos. We can disregard the final if statement since they do not directly affect N. We get:\nO (7D ∗N ∗ β +N) = O (N(7D ∗ β + 1)) = O(N) (13)"
    }, {
      "heading" : "V. Probabilistic Non-Simulating Model",
      "text" : "We propose an different method of computing the class of the test point, without the need of simulation and through purely statistical methods. We first make an assumption that a planet or cluster is normally distributed from the center and the standard deviation is some function of the radius of the planet σ(pr). Therefore let us the define the probability density function.\nPDFp = 1\n2π ∗ σ(pr) e\n−D(−→p x, −→ l x) 2\n2σ(pr)2 (14)\nNow to define our prediction equation:\nMAXθ[\nUniverse∏\np | pθ=θn\n1\n2π ∗ σ(pr) e\n−D(−→p x, −→ l x) 2\n2σ(pr)2 ] (15)\nTo account for the fact that different classes have different amounts of planets, we will transform this function into:\nMAXθ[ log\n∏Universe p | pθ=θn e ( −D(−→p x, −→ l x) 2 pm2σ(pr)2 ) ]\n|p | pθ = θn| (16)\nWe removed the normalization constant, due to the fact that this is a relative measure. The bottom of the fraction is the number of planets per class which insures that there is no bias due to the different amounts of clusters with varying radius’s. The mass term is added to insure that greater planets have a greater impact on the rating.\nThrough trial and error we found the best function for σ(pr) was simply p 2 r.\nThe asymptotic will simply be\nO (DN) = O (N) (17)"
    }, {
      "heading" : "VI. Testing Results",
      "text" : "We tested the algorithm out on the Wisconsin breast cancer data-set (Wolberg and Mangasarian, 1990) (Lichman, 2013). Below are the results.\nGravitational Clustering r′ = 50 α = 0.01 β = 100\nr′ = 5000 α = 0.001 β = 1000\nSimulated Model 89.65% 90.59% Probabilistic Model 92.78% 72.41%\nIt is interesting to note that the larger the clusters and smaller the amount of clusters the less accurate the probabilistic model will be. Unless of course the clusters perfectly model the data that they encapsulate. We continued our testing by comparing the outputs of some popular out of the box methods. All the other algorithms were implemented in the scikit-learn library (Pedregosa et al., 2011). The data-sets we used were the popular Iris data-set (Lichman, 2013), digits data-set(Pedregosa et al., 2011), Ollivetti data-set (Bevilacqua et al., 2006).\nAlgorithm\nData-sets GC Prob GC Sim SVM (poly)\nSVM (rbf)\nNaive Bayes (Gaussian)\nIris 98.41% 96.82% 94.66% 97.33% 96% Digits 86.95% 91.04% 98.99% 25.61% 83.85% Olivetti 65.5% 77.5% 7.5% 8.5% 99.5%\nTo show that our algorithm can handle very few samples, we tested the following data-sets again, but this time we only used 1 sample per each class as the training data. Below are the results.\nAccuracy Per Data-set Algorithm Type GC Prob GC Sim Iris 93.33% 92.00% Digits 59.96% 58.18% Olivetti 63.5% 53.75%"
    }, {
      "heading" : "VII. Conclusion",
      "text" : "In this paper we introduced a novel technique to clustering and supervised learning that can learn from a few samples, while maintaining a low asymptotic run-time and inherently allowing for arbitrary sample weighting. We compared it to current techniques for classification and showed both the strengths of the algorithm as well as the weaknesses. From the test results we can infer that our algorithm acts consistently in both low and high dimensional data, as well as staying consistent in a range of multi-class data-sets. All the code written, including the tests and the algorithm itself can be found on https://github.com/ArmenAg/GravitationalClustering/\nThank you for reading."
    } ],
    "references" : [ {
      "title" : "Conjugate-gradient neural networks in classification of multisource and very-high-dimensional remote sensing data",
      "author" : [ "J.A. Benediktsson", "P.H. SWAIN", "O.K. ERSOY" ],
      "venue" : "International Journal of Remote Sensing, 14(15):2883–2903.",
      "citeRegEx" : "Benediktsson et al\\.,? 1993",
      "shortCiteRegEx" : "Benediktsson et al\\.",
      "year" : 1993
    }, {
      "title" : "Multidimensional binary search trees used for associative searching",
      "author" : [ "J.L. Bentley" ],
      "venue" : "Commun. ACM, 18(9):509–517.",
      "citeRegEx" : "Bentley,? 1975",
      "shortCiteRegEx" : "Bentley",
      "year" : 1975
    }, {
      "title" : "Hidden markov models for recognition using artificial neural networks",
      "author" : [ "V. Bevilacqua", "G. Mastronardi", "A. Pedone", "G. Romanazzi", "D. Daleno" ],
      "venue" : "4113:126– 134.",
      "citeRegEx" : "Bevilacqua et al\\.,? 2006",
      "shortCiteRegEx" : "Bevilacqua et al\\.",
      "year" : 2006
    }, {
      "title" : "Support-vector networks",
      "author" : [ "C. Cortes", "V. Vapnik" ],
      "venue" : "Machine Learning, 20(3):273–297.",
      "citeRegEx" : "Cortes and Vapnik,? 1995",
      "shortCiteRegEx" : "Cortes and Vapnik",
      "year" : 1995
    }, {
      "title" : "Women, Fire, and Dangerous Things",
      "author" : [ "G. Lakoff" ],
      "venue" : "The University of Chicago Press.",
      "citeRegEx" : "Lakoff,? 1987",
      "shortCiteRegEx" : "Lakoff",
      "year" : 1987
    }, {
      "title" : "Some methods for classification and analysis of multivariate observations",
      "author" : [ "J. MacQueen" ],
      "venue" : "pages 281–297.",
      "citeRegEx" : "MacQueen,? 1967",
      "shortCiteRegEx" : "MacQueen",
      "year" : 1967
    }, {
      "title" : "Scikit-learn: Machine learning in Python",
      "author" : [ "F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay" ],
      "venue" : null,
      "citeRegEx" : "Pedregosa et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Pedregosa et al\\.",
      "year" : 2011
    }, {
      "title" : "Multisurface method of pattern separation for medical diagnosis applied to breast cytology",
      "author" : [ "W. Wolberg", "O. Mangasarian" ],
      "venue" : "pages 9193–9196.",
      "citeRegEx" : "Wolberg and Mangasarian,? 1990",
      "shortCiteRegEx" : "Wolberg and Mangasarian",
      "year" : 1990
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Abstract—The downfall of many supervised learning algorithms, such as neural networks, is the inherent need for a large amount of training data (Benediktsson et al., 1993).",
      "startOffset" : 144,
      "endOffset" : 171
    }, {
      "referenceID" : 3,
      "context" : "Other methods such as support vector machines, although capable of dealing with few samples, are inherently binary classifiers (Cortes and Vapnik, 1995), and are in need of learning strategies such as One vs All in the case of multi-classification.",
      "startOffset" : 127,
      "endOffset" : 152
    }, {
      "referenceID" : 5,
      "context" : "Unlike traditional methods such as K-Means (MacQueen, 1967), Gravitational Clustering does not require the initial number of clusters, and automatically builds the clusters, individual samples can be arbitrarily weighted and it requires only few samples while staying resilient to over-fitting.",
      "startOffset" : 43,
      "endOffset" : 59
    }, {
      "referenceID" : 4,
      "context" : "Eleanor Rosch (Lakoff, 1987)(P.",
      "startOffset" : 14,
      "endOffset" : 28
    }, {
      "referenceID" : 1,
      "context" : "Using a KD-Tree (Bentley, 1975) will allow us to train with the average asymptotic of",
      "startOffset" : 16,
      "endOffset" : 31
    }, {
      "referenceID" : 1,
      "context" : "KD-Tree (Bentley, 1975)",
      "startOffset" : 8,
      "endOffset" : 23
    }, {
      "referenceID" : 7,
      "context" : "We tested the algorithm out on the Wisconsin breast cancer data-set (Wolberg and Mangasarian, 1990) (Lichman, 2013).",
      "startOffset" : 68,
      "endOffset" : 99
    }, {
      "referenceID" : 6,
      "context" : "All the other algorithms were implemented in the scikit-learn library (Pedregosa et al., 2011).",
      "startOffset" : 70,
      "endOffset" : 94
    }, {
      "referenceID" : 6,
      "context" : "The data-sets we used were the popular Iris data-set (Lichman, 2013), digits data-set(Pedregosa et al., 2011), Ollivetti data-set (Bevilacqua et al.",
      "startOffset" : 85,
      "endOffset" : 109
    }, {
      "referenceID" : 2,
      "context" : ", 2011), Ollivetti data-set (Bevilacqua et al., 2006).",
      "startOffset" : 28,
      "endOffset" : 53
    } ],
    "year" : 2015,
    "abstractText" : "The downfall of many supervised learning algorithms, such as neural networks, is the inherent need for a large amount of training data (Benediktsson et al., 1993). Although there is a lot of buzz about big data, there is still the problem of doing classification from a small data-set. Other methods such as support vector machines, although capable of dealing with few samples, are inherently binary classifiers (Cortes and Vapnik, 1995), and are in need of learning strategies such as One vs All in the case of multi-classification. In the presence of a large number of classes this can become problematic. In this paper we present, a novel approach to supervised learning through the method of clustering. Unlike traditional methods such as K-Means (MacQueen, 1967), Gravitational Clustering does not require the initial number of clusters, and automatically builds the clusters, individual samples can be arbitrarily weighted and it requires only few samples while staying resilient to over-fitting. Keywords—Machine Learning, Classification, Clustering.",
    "creator" : "LaTeX with hyperref package"
  }
}