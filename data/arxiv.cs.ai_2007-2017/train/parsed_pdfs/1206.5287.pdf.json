{
  "name" : "1206.5287.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Policy Iteration for Relational MDPs",
    "authors" : [ "Chenggang Wang" ],
    "emails" : [ "cwan@cs.tufts.edu", "roni@cs.tufts.edu" ],
    "sections" : null,
    "references" : [ {
      "title" : "Algebraic decision diagrams and their applications",
      "author" : [ "R.I. Bahar", "E.A. Frohm", "C.M. Gaona", "G.D. Hachtel", "E. Macii", "A. Pardo", "F. Somenzi" ],
      "venue" : "Proc. of the International Conference on Computer-Aided Design",
      "citeRegEx" : "Bahar et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Bahar et al\\.",
      "year" : 1993
    }, {
      "title" : "Stochastic dynamic programming with factored representations",
      "author" : [ "C. Boutilier", "T. Dean", "M. Goldszmidt" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Boutilier et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Boutilier et al\\.",
      "year" : 2000
    }, {
      "title" : "Symbolic dynamic programming for first-order MDPs",
      "author" : [ "C. Boutilier", "R. Reiter", "B. Price" ],
      "venue" : "Proc. of the International Joint Conference of Artificial Intelligence",
      "citeRegEx" : "Boutilier et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Boutilier et al\\.",
      "year" : 2001
    }, {
      "title" : "Approximate policy iteration with a policy language bias",
      "author" : [ "A. Fern", "S. Yoon", "R. Givan" ],
      "venue" : "International Conference on Neural Information Processing Systems",
      "citeRegEx" : "Fern et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Fern et al\\.",
      "year" : 2003
    }, {
      "title" : "Exploiting first-order regression in inductive policy selection",
      "author" : [ "C. Gretton", "S. Thiebaux" ],
      "venue" : "Proceedings of the International Conference on Machine Learning",
      "citeRegEx" : "Gretton and Thiebaux,? \\Q2004\\E",
      "shortCiteRegEx" : "Gretton and Thiebaux",
      "year" : 2004
    }, {
      "title" : "Generalizing plans to new environments in relational MDPs",
      "author" : [ "C. Guestrin", "D. Koller", "C. Gearhart", "N. Kanodia" ],
      "venue" : "Proceedings of the International Joint Conference of Artificial Intelligence",
      "citeRegEx" : "Guestrin et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Guestrin et al\\.",
      "year" : 2003
    }, {
      "title" : "A logic-based approach to dynamic programming. AAAI-04 workshop on learning and planning in Markov Processes – advances and challenges",
      "author" : [ "S. Hölldobler", "O. Skvortsova" ],
      "venue" : null,
      "citeRegEx" : "Hölldobler and Skvortsova,? \\Q2004\\E",
      "shortCiteRegEx" : "Hölldobler and Skvortsova",
      "year" : 2004
    }, {
      "title" : "Markov decision processes: Discrete stochastic dynamic programming",
      "author" : [ "M.L. Puterman" ],
      "venue" : null,
      "citeRegEx" : "Puterman,? \\Q1994\\E",
      "shortCiteRegEx" : "Puterman",
      "year" : 1994
    }, {
      "title" : "Approximate linear programming for first-order MDPs",
      "author" : [ "S. Sanner", "C. Boutilier" ],
      "venue" : "Proceedings of the Conference on Uncertainty in Artificial Intelligence",
      "citeRegEx" : "Sanner and Boutilier,? \\Q2005\\E",
      "shortCiteRegEx" : "Sanner and Boutilier",
      "year" : 2005
    }, {
      "title" : "Practical linear valueapproximation techniques for first-order MDPs",
      "author" : [ "S. Sanner", "C. Boutilier" ],
      "venue" : "Proc. of the Conference on Uncertainty in Artificial Intelligence",
      "citeRegEx" : "Sanner and Boutilier,? \\Q2006\\E",
      "shortCiteRegEx" : "Sanner and Boutilier",
      "year" : 2006
    }, {
      "title" : "First order decision diagrams for relational MDPs",
      "author" : [ "C. Wang", "S. Joshi", "R. Khardon" ],
      "venue" : "Proceedings of the International Joint Conference of Artificial Intelligence",
      "citeRegEx" : "Wang et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Recent work on RMDPs includes exact solutions with value iteration (Boutilier et al., 2001; Kersting et al., 2004; Hölldobler & Skvortsova, 2004; Wang et al., 2007), and several approximate or heuristic methods (Guestrin et al.",
      "startOffset" : 67,
      "endOffset" : 164
    }, {
      "referenceID" : 10,
      "context" : "Recent work on RMDPs includes exact solutions with value iteration (Boutilier et al., 2001; Kersting et al., 2004; Hölldobler & Skvortsova, 2004; Wang et al., 2007), and several approximate or heuristic methods (Guestrin et al.",
      "startOffset" : 67,
      "endOffset" : 164
    }, {
      "referenceID" : 5,
      "context" : ", 2007), and several approximate or heuristic methods (Guestrin et al., 2003; Fern et al., 2003; Gretton & Thiebaux, 2004; Sanner & Boutilier, 2005; Sanner & Boutilier, 2006) using different representation languages and algorithms.",
      "startOffset" : 54,
      "endOffset" : 174
    }, {
      "referenceID" : 3,
      "context" : ", 2007), and several approximate or heuristic methods (Guestrin et al., 2003; Fern et al., 2003; Gretton & Thiebaux, 2004; Sanner & Boutilier, 2005; Sanner & Boutilier, 2006) using different representation languages and algorithms.",
      "startOffset" : 54,
      "endOffset" : 174
    }, {
      "referenceID" : 10,
      "context" : "Motivated by the success of using decision diagrams for propositionally factored MDPs, we have previously introduced First Order Decision Diagrams (FODD) and developed a value iteration algorithm working with this representation (Wang et al., 2007).",
      "startOffset" : 229,
      "endOffset" : 248
    }, {
      "referenceID" : 7,
      "context" : "We assume familiarity with standard notions of MDPs and commonly used solution methods (Puterman, 1994).",
      "startOffset" : 87,
      "endOffset" : 103
    }, {
      "referenceID" : 7,
      "context" : "When V is Vn and the output is taken as Vn+1 we get the successive approximation algorithm (Puterman, 1994) calculating the value function V π that is associated with the policy π.",
      "startOffset" : 91,
      "endOffset" : 107
    }, {
      "referenceID" : 7,
      "context" : "For computational efficiency, Puterman (1994) introduced Modified Policy Iteration (MPI) where the sequence mn of non-negative integers controls the number of updates in policy evaluation steps:",
      "startOffset" : 30,
      "endOffset" : 46
    }, {
      "referenceID" : 10,
      "context" : "We only present the parts needed in our construction and refer the reader to (Wang et al., 2007) for further details.",
      "startOffset" : 77,
      "endOffset" : 96
    }, {
      "referenceID" : 0,
      "context" : "As in propositional algebraic decision diagrams (Bahar et al., 1993) we use a a total order over node labels, and we have a set of reduction operators to minimize representation size.",
      "startOffset" : 48,
      "endOffset" : 68
    }, {
      "referenceID" : 10,
      "context" : "Due to space constraints we omit these details that can be found in (Wang et al., 2007).",
      "startOffset" : 68,
      "endOffset" : 87
    }, {
      "referenceID" : 10,
      "context" : "Probabilities, rewards, and value functions can be represented directly using FODDs, with the restriction that probability FODDs are either propositional or depend directly on the action parameters (Wang et al., 2007).",
      "startOffset" : 198,
      "endOffset" : 217
    }, {
      "referenceID" : 1,
      "context" : "Following Boutilier et al. (2001) we specify stochastic actions as a randomized choice among deterministic alternatives.",
      "startOffset" : 10,
      "endOffset" : 34
    }, {
      "referenceID" : 2,
      "context" : "The general first order value iteration algorithm (Boutilier et al., 2001) works as follows: given the reward function R and the action model as input, we set V0 = R, n = 0 and repeat the procedure Rel-greedy until termination: Procedure 2 Rel-greedy 1.",
      "startOffset" : 50,
      "endOffset" : 74
    }, {
      "referenceID" : 10,
      "context" : "As discussed in (Wang et al., 2007) block replacement may not be efficient since it will necessitate reordering nodes and reductions.",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 10,
      "context" : "We have shown (Wang et al., 2007) that this algorithm correctly calculates regression and hence VI for relational domains.",
      "startOffset" : 14,
      "endOffset" : 33
    }, {
      "referenceID" : 10,
      "context" : "Lemma 1 (Wang et al., 2007) Let (Vn+1, π) = Rel-greedy(Vn), then Vn+1 = Q π Vn .",
      "startOffset" : 8,
      "endOffset" : 27
    }, {
      "referenceID" : 1,
      "context" : "Our relational policy regression generalizes an algorithm from (Boutilier et al., 2000) where propositional decision trees were used to solve factored MDPs.",
      "startOffset" : 63,
      "endOffset" : 87
    }, {
      "referenceID" : 10,
      "context" : "It is instructive to review the effect of reductions from (Wang et al., 2007) in this case to see that significant compaction can occur in the process.",
      "startOffset" : 58,
      "endOffset" : 77
    }, {
      "referenceID" : 1,
      "context" : "This is in contrast with the propositional case (Boutilier et al., 2000) where they are necessarily the same since all the leaves of a single Q-function have the same label and every interpretation follows exactly one path.",
      "startOffset" : 48,
      "endOffset" : 72
    }, {
      "referenceID" : 2,
      "context" : "This is possible when one is using a representation expressive enough to define explicit state partitions that are mutually exclusive, as for example in SDP (Boutilier et al., 2001).",
      "startOffset" : 157,
      "endOffset" : 181
    }, {
      "referenceID" : 7,
      "context" : "Note that it follows from this lemma that y is achievable by some (possibly non-stationary) policy and since stationary policies are sufficient (Puterman, 1994) we have",
      "startOffset" : 144,
      "endOffset" : 160
    }, {
      "referenceID" : 7,
      "context" : "Lemma 6 and the fact that if v ≥ u then v̂ ≥ û where v̂ = greedy(v) and û = greedy(u) (Puterman, 1994) imply:",
      "startOffset" : 86,
      "endOffset" : 102
    } ],
    "year" : 2009,
    "abstractText" : "Relational Markov Decision Processes are a useful abstraction for complex reinforcement learning problems and stochastic planning problems. Recent work developed representation schemes and algorithms for planning in such problems using the value iteration algorithm. However, exact versions of more complex algorithms, including policy iteration, have not been developed or analyzed. The paper investigates this potential and makes several contributions. First we observe two anomalies for relational representations showing that the value of some policies is not well defined or cannot be calculated for restricted representation schemes used in the literature. On the other hand, we develop a variant of policy iteration that can get around these anomalies. The algorithm includes an aspect of policy improvement in the process of policy evaluation and thus differs from the original algorithm. We show that despite this difference the algorithm converges to the optimal policy.",
    "creator" : "Adobe InDesign CS2 (4.0.4)"
  }
}