{
  "name" : "0911.5372.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Maximin affinity learning of image segmentation",
    "authors" : [ "Srinivas C. Turaga", "Kevin L. Briggman" ],
    "emails" : [ "sturaga@mit.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Images can be segmented by first using a classifier to predict an affinity graph that reflects the degree to which image pixels must be grouped together and then partitioning the graph to yield a segmentation. Machine learning has been applied to the affinity classifier to produce affinity graphs that are good in the sense of minimizing edge misclassification rates. However, this error measure is only indirectly related to the quality of segmentations produced by ultimately partitioning the affinity graph. We present the first machine learning algorithm for training a classifier to produce affinity graphs that are good in the sense of producing segmentations that directly minimize the Rand index, a well known segmentation performance measure. The Rand index measures segmentation performance by quantifying the classification of the connectivity of image pixel pairs after segmentation. By using the simple graph partitioning algorithm of finding the connected components of the thresholded affinity graph, we are able to train an affinity classifier to directly minimize the Rand index of segmentations resulting from the graph partitioning. Our learning algorithm corresponds to the learning of maximin affinities between image pixel pairs, which are predictive of the pixel-pair connectivity."
    }, {
      "heading" : "1 Introduction",
      "text" : "Supervised learning has emerged as a serious contender in the field of image segmentation, ever since the creation of training sets of images with “ground truth” segmentations provided by humans, such as the Berkeley Segmentation Dataset [15]. Supervised learning requires 1) a parametrized algorithm that map images to segmentations, 2) an objective function that quantifies the performance of a segmentation algorithm relative to ground truth, and 3) a means of searching the parameter space of the segmentation algorithm for an optimum of the objective function.\nIn the supervised learning method presented here, the segmentation algorithm consists of a parametrized classifier that predicts the weights of a nearest neighbor affinity graph over image pixels, followed by a graph partitioner that thresholds the affinity graph and finds its connected components. Our objective function is the Rand index [18], which has recently been proposed as a quantitative measure of segmentation performance [23]. We “soften” the thresholding of the classifier output and adjust the parameters of the classifier by gradient learning based on the Rand index.\n∗sturaga@mit.edu\nar X\niv :0\n91 1.\n53 72\nv1 [\ncs .C\nV ]\n2 8\nBecause maximin edges of the affinity graph play a key role in our learning method, we call it maximin affinity learning of image segmentation, or MALIS. The minimax path and edge are standard concepts in graph theory, and maximin is the opposite-sign sibling of minimax. Hence our work can be viewed as a machine learning application of these graph theoretic concepts. MALIS focuses on improving classifier output at maximin edges, because classifying these edges incorrectly leads to genuine segmentation errors, the splitting or merging of segments.\nTo the best of our knowledge, MALIS is the first supervised learning method that is based on optimizing a genuine measure of segmentation performance. The idea of training a classifier to predict the weights of an affinity graph is not novel. Affinity classifiers were previously trained to minimize the number of misclassified affinity edges [9, 16]. This is not the same as optimizing segmentations produced by partitioning the affinity graph. There have been attempts to train affinity classifiers to produce good segmentations when partitioned by normalized cuts [17, 2]. But these approaches do not optimize a genuine measure of segmentation performance such as the Rand index. The work of Bach and Jordan [2] is the closest to our work. However, they only minimize an upper bound to a renormalized version of the Rand index. Both approaches require many approximations to make the learning tractable.\nIn other related work, classifiers have been trained to optimize performance at detecting image pixels that belong to object boundaries [16, 6, 14]. Our classifier can also be viewed as a boundary detector, since a nearest neighbor affinity graph is essentially the same as a boundary map, up to a sign inversion. However, we combine our classifier with a graph partitioner to produce segmentations. The classifier parameters are not trained to optimize performance at boundary detection, but to optimize performance at segmentation as measured by the Rand index.\nThere are also methods for supervised learning of image labeling using Markov or conditional random fields [10]. But image labeling is more similar to multi-class pixel classification rather than image segmentation, as the latter task may require distinguishing between multiple objects in a single image that all have the same label.\nIn the cases where probabilistic random field models have been used for image parsing and segmentation, the models have either been simplistic for tractability reasons [12] or have been trained piecemeal. For instance, Tu et al. [22] separately train low-level discriminative modules based on a boosting classifier, and train high-level modules of their algorithm to model the joint distribution of the image and the labeling. These models have never been trained to minimize the Rand index."
    }, {
      "heading" : "2 Partitioning a thresholded affinity graph by connected components",
      "text" : "Our class of segmentation algorithms is constructed by combining a classifier and a graph partitioner (see Figure 1). The classifier is used to generate the weights of an affinity graph. The nodes of the graph are image pixels, and the edges are between nearest neighbor pairs of pixels. The weights of the edges are called affinities. A high affinity means that the two pixels tend to belong to the same\nsegment. The classifier computes the affinity of each edge based on an image patch surrounding the edge.\nThe graph partitioner first thresholds the affinity graph by removing all edges with weights less than some threshold value θ. The connected components of this thresholded affinity graph are the segments of the image.\nFor this class of segmentation algorithms, it’s obvious that a single misclassified edge of the affinity graph can dramatically alter the resulting segmentation by splitting or merging two segments (see Fig. 1). This is why it is important to learn by optimizing a measure of segmentation performance rather than affinity prediction.\nWe are well aware that connected components is an exceedingly simple method of graph partitioning. More sophisticated algorithms, such as spectral clustering [20] or graph cuts [3], might be more robust to misclassifications of one or a few edges of the affinity graph. Why not use them instead? We have two replies to this question.\nFirst, because of the simplicity of our graph partitioning, we can derive a simple and direct method of supervised learning that optimizes a true measure of image segmentation performance. So far learning based on more sophisticated graph partitioning methods has fallen short of this goal [17, 2].\nSecond, even if it were possible to properly learn the affinities used by more sophisticated graph partitioning methods, we would still prefer our simple connected components. The classifier in our segmentation algorithm can also carry out sophisticated computations, if its representational power is sufficiently great. Putting the sophistication in the classifier has the advantage of making it learnable, rather than hand-designed.\nThe sophisticated partitioning methods clean up the affinity graph by using prior assumptions about the properties of image segmentations. But these prior assumptions could be incorrect. The spirit of the machine learning approach is to use a large amount of training data and minimize the use of prior assumptions. If the sophisticated partitioning methods are indeed the best way of achieving good segmentation performance, we suspect that our classifier will learn them from the training data. If they are not the best way, we hope that our classifier will do even better."
    }, {
      "heading" : "3 The Rand index quantifies segmentation performance",
      "text" : "Image segmentation can be viewed as a special case of the general problem of clustering, as image segments are clusters of image pixels. Long ago, Rand proposed an index of similarity between two clusterings [18]. Recently it has been proposed that the Rand index be applied to image segmentations [23]. Define a segmentation S as an assignment of a segment label si to each pixel i. The indicator function δ(si, sj) is 1 if pixels i and j belong to the same segment (si = sj) and 0 otherwise. Given two segmentations S and Ŝ of an image with N pixels, define the function\n1− RI(Ŝ, S) = (\nN 2 )−1 ∑ i<j ∣∣δ(si, sj)− δ(ŝi, ŝj)∣∣ (1) which is the fraction of image pixel pairs on which the two segmentations disagree. We will refer to the function 1− RI(Ŝ, S) as the Rand index, although strictly speaking the Rand index is RI(Ŝ, S), the fraction of image pixel pairs on which the two segmentations agree. In other words, the Rand index is a measure of similarity, but we will often apply that term to a measure of dissimilarity.\nIn this paper, the Rand index is applied to compare the output Ŝ of a segmentation algorithm with a ground truth segmentation S, and will serve as an objective function for learning. Figure 1 illustrates why the Rand index is a sensible measure of segmentation performance. The segmentation of affinity graph #1 incurs a huge Rand index penalty relative to the ground truth. A single wrongly classified edge of the affinity graph leads to an incorrect merger of two segments, causing many pairs of image pixels to be wrongly assigned to the same segment. On the other hand, the segmentation corresponding to affinity graph #2 has a perfect Rand index, even though there are misclassifications in the affinity graph. In short, the Rand index makes sense because it strongly penalizes errors in the affinity graph that lead to split and merger errors."
    }, {
      "heading" : "4 Connectivity and maximin affinity",
      "text" : "Recall that our segmentation algorithm works by finding connected components of the thresholded affinity graph. Let Ŝ be the segmentation produced in this way. To apply the Rand index to train our classifier, we need a simple way of relating the indicator function δ(ŝi, ŝj) in the Rand index to classifier output. In other words, we would like a way of characterizing whether two pixels are connected in the thresholded affinity graph.\nTo do this, we introduce the concept of maximin affinity, which is defined for any pair of pixels in an affinity graph (the definition is generally applicable to any weighted graph). Let Aklbe the affinity of pixels k and l. Let P ij be the set of all paths in the graph that connect pixels i and j. For every path P in Pij, there is an edge (or edges) with minimal affinity. This is written as min〈k,l〉∈P Akl , where 〈k, l〉 ∈ P means that the edge between pixels k and l are in the path P. A maximin path P∗ij is a path between pixels i and j that maximizes the minimal affinity,\nP∗ij = arg maxP∈Pij min 〈k,l〉∈P Akl (2)\nThe maximin affinity of pixels i and j is the affinity of the maximin edge, or the minimal affinity of the maximin path,\nA∗ij = maxP∈P ij min 〈k,l〉∈P Akl (3)\nWe are now ready for a trivial but important theorem. Theorem 1. A pair of pixels is connected in the thresholded affinity graph if and only if their maximin affinity exceeds the threshold value.\nProof. By definition, a pixel pair is connected in the thresholded affinity graph if and only if there exists a path between them. Such a path is equivalent to a path in the unthresholded affinity graph for which the minimal affinity is above the threshold value. This path in turn exists if and only if the maximin affinity is above the threshold value.\nAs a consequence of this theorem, pixel pairs can be classified as connected or disconnected by thresholding maximin affinities. Let Ŝ be the segmentation produced by thresholding the affinity graph Aij and then finding connected components. Then the connectivity indicator function is\nδ(ŝi, ŝj) = H(A∗ij − θ) (4)\nwhere H is the Heaviside step function.\nMaximin affinities can be computed efficiently using minimum spanning tree algorithms [8]. A maximum spanning tree is equivalent to a minimum spanning tree, up to a sign change of the weights.\nAny path in a maximum spanning tree is a maximin path. For our nearest neighbor affinity graphs, the maximin affinity of a pixel pair can be computed in O(|E| · α(|V|)) where |E| is the number of graph edges and |V| is the number of pixels and α(·) is the inverse Ackerman function which grows sub-logarithmically. The full matrix A∗ij can be computed in time O(|V|2) since the computation can be shared. Note that maximin affinities are required for training, but not testing. For segmenting the image at test time, only a connected components computation need be performed, which takes time linear in the number of edges |E|."
    }, {
      "heading" : "5 Optimizing the Rand index by learning maximin affinities",
      "text" : "Since the affinities and maximin affinities are both functions of the image I and the classifier parameters W, we will write them as Aij(I; W) and A∗ij(I; W), respectively. By Eq. (4) of the previous section, the Rand index of Eq. (1) takes the form\n1− RI(S, I; W) = (\nN 2 )−1 ∑ i<j ∣∣∣δ(si, sj)− H(A∗ij(I; W)− θ)∣∣∣ Since this is a discontinuous function of the maximin affinities, we make the usual relaxation by replacing |δ(si, sj) − H(A∗ij(I; W) − θ)| with a continuous loss function l(δ(si, sj), A∗ij(I; W)). Any standard loss such as the such as the square loss, 12 (x− x̂)2, or the hinge loss can be used for l(x, x̂). Thus we obtain a cost function suitable for gradient learning,\nE(S, I; W) = (\nN 2 )−1 ∑ i<j l(δ(si, sj), A∗ij(I; W))\n= (\nN 2 )−1 ∑ i<j l(δ(si, sj), max P∈Pij min 〈k,l〉∈P Akl(I; W)) (5)\nThe max and min operations are continuous and differentiable (though not continuously differentiable). If the loss function l is smooth, and the affinity Akl(I; W) is a smooth function, then the gradient of the cost function is well-defined, and gradient descent can be used as an optimization method.\nDefine (k, l) = mm(i, j) to be the maximin edge for the pixel pair (i, j). If there is a tie, choose between the maximin edges at random. Then the cost function takes the form\nE(S, I; W) = (\nN 2 )−1 ∑ i<j l(δ(si, sj), Amm(i,j)(I; W))\nIt’s instructive to compare this with the cost function for standard affinity learning\nEstandard(S, I; W) = 2\ncN ∑〈i,j〉 l(δ(si, sj), Aij(I; W))\nwhere the sum is over all nearest neighbor pixel pairs 〈i, j〉 and c is the number of nearest neighbors [9]. In contrast, the sum in the MALIS cost function is over all pairs of pixels, whether or not they are adjacent in the affinity graph. Note that a single edge can be the maximin edge for multiple pairs of pixels, so its affinity can appear multiple times in the MALIS cost function. Roughly speaking, the MALIS cost function is similar to the standard cost function, except that each edge in the affinity graph is weighted by the number of pixel pairs that it causes to be incorrectly classified."
    }, {
      "heading" : "6 Online stochastic gradient descent",
      "text" : "Computing the cost function or its gradient requires finding the maximin edges for all pixel pairs. Such a batch computation could be used for gradient learning. However, online stochastic gradient\nlearning is often more efficient than batch learning [13]. Online learning makes a gradient update of the parameters after each pair of pixels, and is implemented as described in the box.\nMaximin affinity learning 1. Pick a random pair of (not necessarily nearest neighbor) pixels i and j from a randomly drawn training image I.\n2. Find a maximin edge mm(i, j) 3. Make the gradient update: W ←W + η ddW l(δ(si, sj), Amm(i,j)(I; W))\nStandard affinity learning 1. Pick a random pair of nearest neighbor pixels i and j from a randomly drawn training image I\n2. Make the gradient update: W ←W + η ddW l(δ(si, sj), Aij(I; W))\nFor comparison, we also show the standard affinity learning [9]. For each iteration, both learning methods pick a random pair of pixels from a random image. Both compute the gradient of the weight of a single edge in the affinity graph. However, the standard method picks a nearest neighbor pixel pair and trains the affinity of the edge between them. The maximin method picks a pixel pair of arbitrary separation and trains the minimal affinity on a maximin path between them.\nEffectively, our connected components performs spatial integration over the nearest neighbor affinity graph to make connectivity decisions about pixel pairs at large distances. MALIS trains these global decisions, while standard affinity learning trains only local decisions. MALIS is superior because it truly learns segmentation, but this superiority comes at a price. The maximin computation requires that on each iteration the affinity graph be computed for the whole image. Therefore it is slower than the standard learning method, which requires only a local affinity prediction for the edge being trained. Thus there is a computational price to be paid for the optimization of a true segmentation error."
    }, {
      "heading" : "7 Application to electron microscopic images of neurons",
      "text" : ""
    }, {
      "heading" : "7.1 Electron microscopic images of neural tissue",
      "text" : "By 3d imaging of brain tissue at sufficiently high resolution, as well as identifying synapses and tracing all axons and dendrites in these images, it is possible in principle to reconstruct connectomes, complete “wiring diagrams” for a brain or piece of brain [19, 4, 21]. Axons can be narrower than 100 nm in diameter, necessitating the use of electron microscopy (EM) [19]. At such high spatial resolution, just one cubic millimeter of brain tissue yields teravoxel scale image sizes. Recent advances in automation are making it possible to collect such images [19, 4, 21], but image analysis remains a challenge. Tracing axons and dendrites is a very large-scale image segmentation problem requiring high accuracy. The images used for this study were from the inner plexiform layer of the rabbit retina, and were taken using Serial Block-Face Scanning Electron Microscopy [5]. Two large image volumes of 1003 voxels were hand segmented and reserved for training and testing purposes."
    }, {
      "heading" : "7.2 Training convolutional networks for affinity classification",
      "text" : "Any classifier that is a smooth function of its parameters can be used for maximin affinity learning. We have used convolutional networks (CN), but our method is not restricted to this choice. Convolutional networks have previously been shown to be effective for similar EM images of brain tissue [11].\nWe trained two identical four-layer CNs, one with standard affinity learning and the second with MALIS. The CNs contained 5 feature maps in each layer with sigmoid nonlinearities. All filters in the CN were 5× 5× 5 in size. This led to an affinity classifier that uses a 17× 17× 17 cubic image patch to classify a affinity edge. We used the square-square loss function l(x, x̂) = x ·max(0, 1− x̂−m)2 + (1− x) ·max(0, x̂−m)2, with a margin m = 0.3. As noted earlier, maximin affinity learning can be significantly slower than standard affinity learning, due to the need for computing the entire affinity graph on each iteration, while standard affinity training need only predict the weight of a single edge in the graph. For this reason, we constructed a proxy training image dataset by picking all possible 21× 21× 21 sized overlapping sub-images\nfrom the original training set. Since each 21× 21× 21 sub-image is smaller than the original image, the size of the affinity graph needed to be predicted for the sub-image is significantly smaller, leading to faster training. A consequence of this approximation is that the maximum separation between image pixel pairs chosen for training is less than about 20 pixels. A second means of speeding up the maximin procedure is by pretraining the maximin CN for 500,000 iterations using the fast standard affinity classification cost function. At the end, both CNs were trained for a total of 1,000,000 iterations by which point the training error plateaued."
    }, {
      "heading" : "7.3 Maximin learning leads to dramatic improvement in segmentation performance",
      "text" : "We benchmarked the performance of the standard and maximin affinity classifiers by measuring the the pixel-pair connectivity classification performance using the Rand index. After training the standard and MALIS affinity classifiers, we generated affinity graphs for the training and test images. In principle, the training algorithm suggests a single threshold for the graph partitioning. In practice, one can generate a full spectrum of segmentations leading from over-segmentations to under-segmentations by varying the threshold parameter. In Fig. 3, we plot the Rand index for segmentations resulting from a range of threshold values.\nIn images with large numbers of segments, most pixel pairs will be disconnected from one another leading to a large imbalancing the number of connected and disconnected pixel pairs. This is reflected in the fact that the Rand index is over 95% for both segmentation algorithms. While this imbalance between positive and negative examples is not a significant problem for training the affinity classifier, it can make comparisons between classifiers difficult to interpret. Instead, we can use the ROC and precision-recall methodologies, which provide for accurate quantification of the accuracy of classifiers even in the presence of large class imbalance. From these curves, we observe that our maximin affinity classifier dramatically outperforms the standard affinity classifier.\nOur positive results have an intriguing interpretation. The poor performance of the connected components when applied to a standard learned affinity classifier could be interpreted to imply that 1) a local classifier lacks the context important for good affinity prediction; 2) connected components is a poor strategy for image segmentation since mistakes in the affinity prediction of just a few edges can merge or split segments. On the contrary, our experiments suggest that when trained properly, thresholded affinity classification followed by connected components can be an extremely competitive method of image segmentations."
    }, {
      "heading" : "8 Discussion",
      "text" : "In this paper, we have trained an affinity classifier to produce affinity graphs that result in excellent segmentations when partitioned by the simple graph partitioning algorithm of thresholding followed by connected components. The key to good performance is the training of a segmentation-based cost function, and the use of a powerful trainable classifier to predict affinity graphs. Once trained, our segmentation algorithm is fast. In contrast to classic graph-based segmentation algorithms where\nthe partitioning phase dominates, our partitioning algorithm is simple and can partition graphs in time linearly proportional to the number of edges in the graph. We also do not require any prior knowledge of the number of image segments or image segment sizes at test time, in contrast to other graph partitioning algorithms [7, 20].\nThe formalism of maximin affinities used to derive our learning algorithm has connections to singlelinkage hierarchical clustering, minimum spanning trees and ultrametric distances. Felzenszwalb and Huttenlocher [7] describe a graph partitioning algorithm based on a minimum spanning tree computation which resembles our segmentation algorithm, in part. The Ultrametric Contour Map algorithm [1] generates hierarchical segmentations nearly identical those generated by varying the threshold of our graph partitioning algorithm. Neither of these methods incorporates a means for learning from labeled data, but our work shows how the performance of these algorithms can be improved by use of our maximin affinity learning."
    }, {
      "heading" : "Acknowledgements",
      "text" : "SCT and HSS were supported in part by the Howard Hughes Medical Institute and the Gatsby Charitable Foundation."
    } ],
    "references" : [ {
      "title" : "Boundary extraction in natural images using ultrametric contour maps",
      "author" : [ "P. Arbelaez" ],
      "venue" : "Proc. POCV,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Learning spectral clustering, with application to speech separation",
      "author" : [ "F. Bach", "M. Jordan" ],
      "venue" : "The Journal of Machine Learning Research, 7:1963–2001,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Fast approximate energy minimization via graph cuts",
      "author" : [ "Y. Boykov", "O. Veksler", "R. Zabih" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 23(11):1222–1239,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Towards neural circuit reconstruction with volume electron microscopy techniques",
      "author" : [ "K.L. Briggman", "W. Denk" ],
      "venue" : "Curr Opin Neurobiol, 16(5):562–70,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Serial block-face scanning electron microscopy to reconstruct three-dimensional tissue nanostructure",
      "author" : [ "W. Denk", "H. Horstmann" ],
      "venue" : "PLoS Biol, 2(11):e329,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Supervised learning of edges and object boundaries",
      "author" : [ "P. Dollár", "Z. Tu", "S. Belongie" ],
      "venue" : "CVPR, June",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Efficient Graph-Based Image Segmentation",
      "author" : [ "P. Felzenszwalb", "D. Huttenlocher" ],
      "venue" : "International Journal of Computer Vision, 59(2):167–181,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Clustering with the connectivity kernel",
      "author" : [ "B. Fischer", "V. Roth", "J. Buhmann" ],
      "venue" : "Advances in Neural Information Processing Systems 16: Proceedings of the 2003 Conference. Bradford Book,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Learning affinity functions for image segmentation: combining patch-based and gradient-based approaches",
      "author" : [ "C. Fowlkes", "D. Martin", "J. Malik" ],
      "venue" : "Computer Vision and Pattern Recognition, 2003. Proceedings. 2003 IEEE Computer Society Conference on, 2,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Multiscale conditional random fields for image labeling",
      "author" : [ "X. He", "R. Zemel", "M. Carreira-Perpinan" ],
      "venue" : "IEEE Computer Society Conference on Computer Vision and Pattern Recognition, volume 2. IEEE Computer Society; 1999,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Supervised learning of image restoration with convolutional networks",
      "author" : [ "V. Jain", "J. Murray", "F. Roth", "S. Turaga", "V. Zhigulin", "K. Briggman", "M. Helmstaedter", "W. Denk", "H. Seung" ],
      "venue" : "ICCV 2007,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Discriminative random fields: a discriminative framework for contextual interaction in classification",
      "author" : [ "S. Kumar", "M. Hebert" ],
      "venue" : "Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on, pages 1150–1157,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Efficient backprop",
      "author" : [ "Y. LeCun", "L. Bottou", "G. Orr", "K. Müller" ],
      "venue" : "Lecture notes in computer science, pages 9–50,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Using contours to detect and localize junctions in natural images",
      "author" : [ "M. Maire", "P. Arbelaez", "C. Fowlkes", "J. Malik" ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition, 2008. CVPR 2008, pages 1–8,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics",
      "author" : [ "D. Martin", "C. Fowlkes", "D. Tal", "J. Malik" ],
      "venue" : "Proc. Eighth Int’l Conf. Computer Vision, volume 2, pages 416–423,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Learning to detect natural image boundaries using local brightness, color, and texture cues",
      "author" : [ "D.R. Martin", "C.C. Fowlkes", "J. Malik" ],
      "venue" : "IEEE Trans Pattern Anal Mach Intell, 26(5):530–549, May",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Learning segmentation by random walks",
      "author" : [ "M. Meila", "J. Shi" ],
      "venue" : "ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS, pages 873–879,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Objective criteria for the evaluation of clustering methods",
      "author" : [ "W. Rand" ],
      "venue" : "Journal of the American Statistical association, pages 846–850,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1971
    }, {
      "title" : "Reading the Book of Memory: Sparse Sampling versus Dense Mapping of Connectomes",
      "author" : [ "H. Seung" ],
      "venue" : "Neuron, 62(1):17–29,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Normalized cuts and image segmentation",
      "author" : [ "J. Shi", "J. Malik" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(8):888–905,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Circuit reconstruction tools today",
      "author" : [ "S.J. Smith" ],
      "venue" : "Curr Opin Neurobiol, 17(5):601–608, Oct",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Image parsing: Unifying segmentation, detection, and recognition",
      "author" : [ "Z. Tu", "X. Chen", "A. Yuille", "S. Zhu" ],
      "venue" : "International Journal of Computer Vision, 63(2):113–140,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Toward objective evaluation of image segmentation algorithms",
      "author" : [ "R. Unnikrishnan", "C. Pantofaru", "M. Hebert" ],
      "venue" : "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTEL- LIGENCE, pages 929–944,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "Supervised learning has emerged as a serious contender in the field of image segmentation, ever since the creation of training sets of images with “ground truth” segmentations provided by humans, such as the Berkeley Segmentation Dataset [15].",
      "startOffset" : 238,
      "endOffset" : 242
    }, {
      "referenceID" : 17,
      "context" : "Our objective function is the Rand index [18], which has recently been proposed as a quantitative measure of segmentation performance [23].",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 22,
      "context" : "Our objective function is the Rand index [18], which has recently been proposed as a quantitative measure of segmentation performance [23].",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 8,
      "context" : "Affinity classifiers were previously trained to minimize the number of misclassified affinity edges [9, 16].",
      "startOffset" : 100,
      "endOffset" : 107
    }, {
      "referenceID" : 15,
      "context" : "Affinity classifiers were previously trained to minimize the number of misclassified affinity edges [9, 16].",
      "startOffset" : 100,
      "endOffset" : 107
    }, {
      "referenceID" : 16,
      "context" : "There have been attempts to train affinity classifiers to produce good segmentations when partitioned by normalized cuts [17, 2].",
      "startOffset" : 121,
      "endOffset" : 128
    }, {
      "referenceID" : 1,
      "context" : "There have been attempts to train affinity classifiers to produce good segmentations when partitioned by normalized cuts [17, 2].",
      "startOffset" : 121,
      "endOffset" : 128
    }, {
      "referenceID" : 1,
      "context" : "The work of Bach and Jordan [2] is the closest to our work.",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 15,
      "context" : "In other related work, classifiers have been trained to optimize performance at detecting image pixels that belong to object boundaries [16, 6, 14].",
      "startOffset" : 136,
      "endOffset" : 147
    }, {
      "referenceID" : 5,
      "context" : "In other related work, classifiers have been trained to optimize performance at detecting image pixels that belong to object boundaries [16, 6, 14].",
      "startOffset" : 136,
      "endOffset" : 147
    }, {
      "referenceID" : 13,
      "context" : "In other related work, classifiers have been trained to optimize performance at detecting image pixels that belong to object boundaries [16, 6, 14].",
      "startOffset" : 136,
      "endOffset" : 147
    }, {
      "referenceID" : 9,
      "context" : "There are also methods for supervised learning of image labeling using Markov or conditional random fields [10].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 11,
      "context" : "In the cases where probabilistic random field models have been used for image parsing and segmentation, the models have either been simplistic for tractability reasons [12] or have been trained piecemeal.",
      "startOffset" : 168,
      "endOffset" : 172
    }, {
      "referenceID" : 21,
      "context" : "[22] separately train low-level discriminative modules based on a boosting classifier, and train high-level modules of their algorithm to model the joint distribution of the image and the labeling.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "More sophisticated algorithms, such as spectral clustering [20] or graph cuts [3], might be more robust to misclassifications of one or a few edges of the affinity graph.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 2,
      "context" : "More sophisticated algorithms, such as spectral clustering [20] or graph cuts [3], might be more robust to misclassifications of one or a few edges of the affinity graph.",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 16,
      "context" : "So far learning based on more sophisticated graph partitioning methods has fallen short of this goal [17, 2].",
      "startOffset" : 101,
      "endOffset" : 108
    }, {
      "referenceID" : 1,
      "context" : "So far learning based on more sophisticated graph partitioning methods has fallen short of this goal [17, 2].",
      "startOffset" : 101,
      "endOffset" : 108
    }, {
      "referenceID" : 17,
      "context" : "Long ago, Rand proposed an index of similarity between two clusterings [18].",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 22,
      "context" : "Recently it has been proposed that the Rand index be applied to image segmentations [23].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 7,
      "context" : "Maximin affinities can be computed efficiently using minimum spanning tree algorithms [8].",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 8,
      "context" : "where the sum is over all nearest neighbor pixel pairs 〈i, j〉 and c is the number of nearest neighbors [9].",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 12,
      "context" : "learning is often more efficient than batch learning [13].",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 8,
      "context" : "For comparison, we also show the standard affinity learning [9].",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 18,
      "context" : "By 3d imaging of brain tissue at sufficiently high resolution, as well as identifying synapses and tracing all axons and dendrites in these images, it is possible in principle to reconstruct connectomes, complete “wiring diagrams” for a brain or piece of brain [19, 4, 21].",
      "startOffset" : 261,
      "endOffset" : 272
    }, {
      "referenceID" : 3,
      "context" : "By 3d imaging of brain tissue at sufficiently high resolution, as well as identifying synapses and tracing all axons and dendrites in these images, it is possible in principle to reconstruct connectomes, complete “wiring diagrams” for a brain or piece of brain [19, 4, 21].",
      "startOffset" : 261,
      "endOffset" : 272
    }, {
      "referenceID" : 20,
      "context" : "By 3d imaging of brain tissue at sufficiently high resolution, as well as identifying synapses and tracing all axons and dendrites in these images, it is possible in principle to reconstruct connectomes, complete “wiring diagrams” for a brain or piece of brain [19, 4, 21].",
      "startOffset" : 261,
      "endOffset" : 272
    }, {
      "referenceID" : 18,
      "context" : "Axons can be narrower than 100 nm in diameter, necessitating the use of electron microscopy (EM) [19].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 18,
      "context" : "Recent advances in automation are making it possible to collect such images [19, 4, 21], but image analysis remains a challenge.",
      "startOffset" : 76,
      "endOffset" : 87
    }, {
      "referenceID" : 3,
      "context" : "Recent advances in automation are making it possible to collect such images [19, 4, 21], but image analysis remains a challenge.",
      "startOffset" : 76,
      "endOffset" : 87
    }, {
      "referenceID" : 20,
      "context" : "Recent advances in automation are making it possible to collect such images [19, 4, 21], but image analysis remains a challenge.",
      "startOffset" : 76,
      "endOffset" : 87
    }, {
      "referenceID" : 4,
      "context" : "The images used for this study were from the inner plexiform layer of the rabbit retina, and were taken using Serial Block-Face Scanning Electron Microscopy [5].",
      "startOffset" : 157,
      "endOffset" : 160
    }, {
      "referenceID" : 10,
      "context" : "Convolutional networks have previously been shown to be effective for similar EM images of brain tissue [11].",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 6,
      "context" : "We also do not require any prior knowledge of the number of image segments or image segment sizes at test time, in contrast to other graph partitioning algorithms [7, 20].",
      "startOffset" : 163,
      "endOffset" : 170
    }, {
      "referenceID" : 19,
      "context" : "We also do not require any prior knowledge of the number of image segments or image segment sizes at test time, in contrast to other graph partitioning algorithms [7, 20].",
      "startOffset" : 163,
      "endOffset" : 170
    }, {
      "referenceID" : 6,
      "context" : "Felzenszwalb and Huttenlocher [7] describe a graph partitioning algorithm based on a minimum spanning tree computation which resembles our segmentation algorithm, in part.",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 0,
      "context" : "The Ultrametric Contour Map algorithm [1] generates hierarchical segmentations nearly identical those generated by varying the threshold of our graph partitioning algorithm.",
      "startOffset" : 38,
      "endOffset" : 41
    } ],
    "year" : 2009,
    "abstractText" : "Images can be segmented by first using a classifier to predict an affinity graph that reflects the degree to which image pixels must be grouped together and then partitioning the graph to yield a segmentation. Machine learning has been applied to the affinity classifier to produce affinity graphs that are good in the sense of minimizing edge misclassification rates. However, this error measure is only indirectly related to the quality of segmentations produced by ultimately partitioning the affinity graph. We present the first machine learning algorithm for training a classifier to produce affinity graphs that are good in the sense of producing segmentations that directly minimize the Rand index, a well known segmentation performance measure. The Rand index measures segmentation performance by quantifying the classification of the connectivity of image pixel pairs after segmentation. By using the simple graph partitioning algorithm of finding the connected components of the thresholded affinity graph, we are able to train an affinity classifier to directly minimize the Rand index of segmentations resulting from the graph partitioning. Our learning algorithm corresponds to the learning of maximin affinities between image pixel pairs, which are predictive of the pixel-pair connectivity.",
    "creator" : "LaTeX with hyperref package"
  }
}