{
  "name" : "1509.08038.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Deep Trans-layer Unsupervised Networks for Representation Learning",
    "authors" : [ "Wentao Zhu", "Jun Miao", "Laiyun Qing", "Xilin Chen" ],
    "emails" : [ "wentao.zhu@vipl.ict.ac.cn", "jmiao@ict.ac.cn", "lyqing@ucas.ac.cn", "xlchen@ict.ac.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 9.\n08 03\n8v 1\n[ cs\n.N E\n] 2\nLearning features from massive unlabelled data is a vast prevalent topic for highlevel tasks in many machine learning applications. The recent great improvements on benchmark data sets achieved by increasingly complex unsupervised learning methods and deep learning models with lots of parameters usually requires many tedious tricks and much expertise to tune. However, filters learned by these complex architectures are quite similar to standard hand-crafted features visually. In this paper, unsupervised learning methods, such as PCA or auto-encoder, are employed as the building block to learn filter banks at each layer. The lower layer responses are transferred to the last layer (trans-layer) to form a more complete representation retaining more information. In addition, some beneficial methods such as local contrast normalization and whitening are added to the proposed deep trans-layer networks to further boost performance. The trans-layer representations are followed by block histograms with binary encoder schema to learn translation and rotation invariant representations, which are utilized to do high-level tasks such as recognition and classification. Compared to traditional deep learning methods, the implemented feature learning method has much less parameters and is validated in several typical experiments, such as digit recognition on MNIST and MNIST variations, object recognition on Caltech 101 dataset, face verification on LFW dataset. The deep trans-layer unsupervised learning achieves 99.45 % accuracy on MNIST dataset, 67.11 % accuracy on 15 samples per class and 75.98 % accuracy on 30 samples per class on Caltech 101 dataset, 87.10 % on LFW dataset.\nKeywords: Unsupervised feature learning, deep representation learning, trans-layer neural networks, no fine-tuning representation learning 2010 MSC: 00-01, 99-00\nEmail addresses: wentao.zhu@vipl.ict.ac.cn (Wentao Zhu), jmiao@ict.ac.cn (Jun Miao), lyqing@ucas.ac.cn (Laiyun Qing), xlchen@ict.ac.cn (Xilin Chen)\nPreprint submitted to **** September 29, 2015"
    }, {
      "heading" : "1. Introduction",
      "text" : "Almost all high-layer tasks such as classification, recognition and verification require us to design fine representations for their specific aims. For classification of images taken from the wild, numerous factors in the environment, such as different lighting conditions, occlusions, corruptions and deformations, lead to large amount of intra-class variability in images. Good representations should reduce such non-informative intra-class variability, whilst preserving discriminative information across classes. However, designing good feature representations is a quite tough and difficult procedure for pattern recognition tasks, which is a hot topic in machine learning field.\nThe research of feature representations mainly contains two aspects, handcrafted feature designing and automatic feature learning. Researchers and engineers made enormous efforts to devise robust feature representations at their own domains a decade ago. Many successful features are proposed such as SIFT [1] and HoG [2] features in computer vision domain. However, these hand-crafted features have poor transfer ability over domains. Novel features need to be redesigned elaborately when the domain of application is changed. The other way is representation learning, which is a quite prevalent topic after deep learning coming out [3]. Nevertheless, these fully learned representations by multi-layer unsupervised learning followed by a fine-tuning procedure have too many parameters to be tuned, and require much expertise knowledge and sophisticated hardware support to train a long time.\nIn this paper, we demonstrate a novel trans-layer neural network with quite simple and the most classical unsupervised learning method, PCA or autoencoder, as the building block. Different from the PCANet [4], a one-by-one two layer PCA network, the responses of the previous layer of our model are concatenated to that of the last layer to form a more complete representation. Such trans-layer connections make up the rapid information loss in the cascade unsupervised learning effectively. In addition, the local contrast normalization [5] and whitening are added in our trans-layer unsupervised network to boost its learning ability, which are commonly used in deep neural networks [6]. The difference between the implemented deep trans-layer unsupervised network and conventional networks is that the deep trans-layer unsupervised network requires no back propagation information to fine-tune the feature banks.\nExperimental results indicate that the implemented trans-layer connection scheme boosts the deep trans-layer unsupervised network effectively, and commonly used local contrast normalization and whitening also contribute to the performances. The demonstrated deep trans-layer unsupervised network is validated on digit recognition and object recognition tasks. Quite surprisingly, the stacked conventional unsupervised learning with trans-layer representations achieves 99.45 % accuracy on MNIST dataset, and 67.11 % accuracy on 15 samples per class and 75.98 % accuracy on 30 samples per class on Caltech 101 dataset [7].\nWe will start by reviewing the related works on feature learning and representation in Section 2. Then the idea of the deep trans-layer unsupervised\nnetwork, including the pre-processing and trans-layer unsupervised learning, is illustrated detailedly in Section 3. How to use the deep trans-layer unsupervised network to extract features and tackle applications is also described in Section 3. The experimental results and comparative analysis on MNIST, MNIST variations and Caltech 101 datasets are presented on Section 4. Finally, discussions, conclusions and the future work are summarized in Section 5 and Section 6."
    }, {
      "heading" : "2. Related works",
      "text" : "Much research has been conducted to pursuit a good representation by manually designing elaborative low-level features, such as LBPH feature [8], SIFT feature [1] and HoG feature [2] in computer vision field. However, these handcrafted features cannot be easily adapted to new conditions and tasks, and redesigning them usually requires novel expertise knowledge and tough studies.\nLearning good feature representations is probably a promising way to handle the required elaborative and expertise problem in devising hand-crafted features. Much recent work in machine learning has focused on how to learn good feature representations from massive unlabelled data, and great progresses have been made by the methods [9, 10]. The main idea of deep models is to learn multi-level features at different layers. High-level features generated in the upper-layer are expected to extract more complex and abstract semantics of data, and more invariance to intra-class variability, which is quite useful to highlevel tasks. These deep learning methods typically learn multi-level features by greedily “pre-training” each layer using the specific unsupervised learning, and then fine-tuning the pre-trained features by stochastic gradient descent (SGD) method with supervised information [3], [5]. However, these deep architectures have numerous parameters such as the number of features to learn and parameters of unsupervised learning in each layer. Besides, the SGD also has various parameters such as momentum, weight decay rate, learning rate, and extra parameters including the Dropout rate or DropConnet rate in recently proposed convolution deep neural networks (ConvNets) [9, 10].\nThere is also some work on conventional unsupervised learning methods with only single layer [6, 11]. The main idea of these methods is to learn over complete representations with dense features. Although these methods have made much progress on benchmark datasets with almost no hyper parameters, these single layer unsupervised representational learning models require over complete features of dimensions as high as possible, and the parameters need to be elaborately chosen in order to obtain satisfactory results [6].\nA major drawback of deep learning methods with fine-tuning for stacking representations is the consuming of expensive computational resources and high complexities of the models. One intuition is that, since the elaborately learned features are quite similar to some conventional unsupervised features, such as wavelets, PCA and auto encoder, why not jump over the tough and timeconsuming parameter fine-tuning procedure and take those features stacked as the representation directly. Furthermore, more robust invariant features can be\nbetter devised other than various pooling strategies. Wavelet scattering networks (ScatNet) are such networks with pre-fixed wavelet filter banks in the deep convolution architectures [12]. The ScatNets have quite solid mathematical analysis of their rotation and translation invariants at any scale. More surprisingly, superior performance over ConvNet and DNNs is obtained by the ScatNets with no fine-tuning phase. However, the ScatNet is shown to have inferior performance in large intra-class variability including great illumination changing and corruption such as face related tasks [4].\nThe other non-propagation deep network with pre-fixed feature banks is the PCANet [4]. The PCANet uses two layer cascaded linear networks with näıve PCA filter banks to extract more complex features. The output of the two layer cascaded PCA network is processed by the quantized histogram units. The PCANet presents a superior or highly comparable performance over other methods such as ScatNet [12], ConvNet [9] and HSC [13], especially in face recognition tasks with large occlusion, illumination, expression and pose changes. In addition, the PCANet has quite fewer parameters, much faster learning speed and much more reliable than the currently widely researched ConvNets and DNNs, which is much more convenient and more practical to applications [4]. However, the cascaded PCA structure in PCANet will face great information loss and corruption after multi-layer transformation, which will be illustrated in Section 5. The current prevalent deep networks are also probably facing the same problem that lower layer’s discriminative information will be lost after layers’ transformation. This leads to inferior results of PCANet on conventional object recognition tasks.\nThis paper will tackle the multi-layer information loss problem by demonstrating a novel trans-layer structure based on multi-layer conventional unsupervised filter banks. The local contrast normalization and whitening operations are applied to ameliorate the unsupervised learning in the deep trans-layer network. Thus the trans-layer unsupervised network forms a more complete and effective representation, whilst retaining the advantages such as fewer parameters, faster learning speed and more reliable performance. Also, histogram operation is adopted to preserve translation and rotation invariance after binary quantization. Experimental results confirm that the deep trans-layer unsupervised network boosts the performance of conventional unsupervised learning, and it learns effective feature representations that achieve 99.45 % accuracy on MNIST dataset, and 67.11 % accuracy on 15 samples per class and 75.98 % accuracy on 30 samples per class on Caltech 101 dataset."
    }, {
      "heading" : "3. Deep trans-layer unsupervised network",
      "text" : "In this section, we present a novel framework, the deep trans-layer unsupervised network, for feature learning and representation. The framework of proposed deep trans-layer unsupervised network is illustrated in Figure 1. The procedures of the deep trans-layer unsupervised network is similar to other commonly used frameworks in computer vision [14] and feature learning work [15]\nas well. Different from the traditional methods, the deep trans-layer unsupervised network utilizes the unsupervised learning methods, such as the PCA or auto encoder, to learn the local receptive filter banks, and needs no fine-tuning procedure to adjust those local filter banks. Besides, the previous layer’s unsupervised feature maps are concatenated to the last layer to form a much more completed representation, which is shown quite effective for the following tasks.\nAs a high level, the proposed deep trans-layer unsupervised performs the following steps to learn a feature representation.\nThe first layer\n• Extract random patches from the training images.\n• Apply local contrast normalization (LCN) and whitening operations to each extracted patches.\n• Learn the first layer’s filter banks by the unsupervised learning, such as PCA or auto encoder, on the extracted patches.\n• Use the learned feature banks to generate the feature maps of each training image.\nThe second layer\n• Extract random patches from the obtained feature maps in the first layer.\n• Apply local contrast normalization and whitening operations to each extracted patches from feature maps.\n• Learn the second layer’s filter banks by the unsupervised learning on the extracted feature map patches.\nGiven test images, we can extract stacked and concatenated feature representations and apply them to classification.\nThe first layer\n• Apply the unsupervised learning feature banks the first layer learned to each test image to generate its feature maps after local contrast normalization and whitening processing operations.\nThe second layer\n• Apply the unsupervised learning feature banks the second layer learned to each first layer feature maps after local contrast normalization and whitening processing operations.\nThe output layer\n• Generate the response maps by amalgamating the first layer feature maps with the second layer feature maps.\n• Use binary coding operation to facilitate the next histogram procedure.\n• Compose block-wise histogram of the coded response maps of each test image as its feature representation.\n• Apply dimensionality reduction methods or distance metrics to the learned deep trans-layer unsupervised feature representations, or directly train a classifier based the representations to tackle the applications.\nIn the following paragraphs, we will describe the components of the above network pipeline in more details."
    }, {
      "heading" : "3.1. Representation learning",
      "text" : "The structure of deep trans-layer unsupervised network is partially similar to convolution neural network where convolution operations are done in small patches. In deep trans-layer unsupervised network, the local filters are learned by unsupervised learning, such as PCA or auto encoder, which requires no fine tuning process through error back propagation.\nIn each unsupervised layer (the first and second layer), the system begins with extracting a large number of random patches from unlabelled input images. Suppose the images used here are all gray images. Each patch has a receptive field size or dimension of k1-by-k2. If the images are color images with d channels, the patch dimension is k1-by-k2-by-d. Just process the other d − 1 channels the same as following procedures step by step independently. Then a dataset of m patches is constructed, X = {x(1), . . . ,x(m)}, where x(i) ∈ Rk1×k2 stands for the ith patch extracted from the input images in the first layer or the feature maps in the second layer. Sequentially, we apply the preprocessing of local contrast normalization (LCN) and whitening, and then unsupervised learning in the first and second layer, respectively."
    }, {
      "heading" : "3.1.1. Local contrast normalization and whitening",
      "text" : "In the pre-processing of each layer’s unsupervised learning, we perform several simple operations effectively in the implemented network.\nThe first is local contrast normalization (LCN) [5]. For each local patch x(i) in the extracted patch dataset X, we normalize the patch x(i) by subtracting its mean and dividing by its standard deviation as,\ny(i)j,k = (x (i) j,k − 1\nk1k2\nk1 ∑\nj=1\nk2 ∑\nk=1\nx(i)j,k)/\n(\n√\n1 k1k2\nk1 ∑\nj=1\nk2 ∑\nk=1\n(x(i)j,k − 1\nk1k2\nk1 ∑\nj=1\nk2 ∑\nk=1\nx(i)j,k)2 + C),\nj = 1, · · · , k1; k = 1, · · · , k2; i = 1, · · · ,m,\n(1)\nwhere C is a constant integer to make the model more robust, which is commonly manipulated in practice. In our work, C is set to 10.\nThe LCN has explicit explanations both in physics and physiology. The mean of local patch stands for local brightness, and the standard deviation represents contrast normalization. By LCN, the illumination and material optimal property effects are removed. On the other hand, the LCN has an effect similar to lateral inhibition found in real neurons. The LCN operator inhabits the responses within each local patch, whilst activating responses in the same location of these patches.\nFollowing the LCN, whitening is the second preprocessing method for each unsupervised learning layer. Whitening is commonly used in various application and is a decorrelation operator, which reduces redundant representation of images. The whitening operator transforms the patches as,\n[D,U] = eig(cov(Y)) z(i) = U(D+ diag(ǫ))−1/2UTy(i), i = 1, . . . ,m, (2)\nwhere Y is formed by m patches y(i), cov() stands for covariance function and the size of output data is k1 ∗k2, eig() is the eigenvalue decomposition function, D and U are eigenvalues and eigenvectors respectively, ǫ is set as 0.1 here to make the operator more robust. The ZCA whitening also has biological explanation and has been proved its effectiveness by a lot of work."
    }, {
      "heading" : "3.1.2. Trans-layer unsupervised learning",
      "text" : "After pre-processing for each layer, unsupervised learning, such as PCA or auto encoder, is used to learn feature banks in the trans-layer network.\nThe first unsupervised layer Assuming that the number of feature banks in the first layer is L1, flatten each pre-processed patch z (i) 1 extracted from input images, and put the flattened vectors together. Extracted patch matrix from the first layer will be obtained as\nZ1 = [ z1 (1), z1 (2), · · · , z1 (m) ] ∈ Rk1k2×m. (3)\nIf the used unsupervised learning is PCA, the PCA unsupervised learning aims to\nmin W1∈RL1×k1k2\n∥ ∥ ∥ Z1 −W1 TW1Z1 ∥ ∥ ∥ 2\n2\ns.t. W1W1 T = IL1 ,\n(4)\nwhere W1 is the PCA transformation weights, and IL1 is an identity matrix of size L1 × L1. We get the solution of the above PCA constraints as\nW1 = [ w1 1, w1 2, · · · , w1 L1 ]T , (5)\nwhere w1 1, w1 2, · · · , w1 L1 are the first L1 eigenvectors of Z1Z T 1 with L1 largest variances. Then, we should make these eigenvectors changing back to k1 × k2 dimensions to facilitate the next PCA feature mapping operation.\nThe first PCA unsupervised layer’s feature maps are generated by applying these L1 filters to convolute with the input images. For each input image, L1 feature maps can be obtained by\nI1 (i) = I ∗w1i, i = 1, · · · , L1 , (6)\nwhere I stands for an input image with zero padded to make I (i) 1 have the same size as the input image, and ∗ stands for a convolution operator. If the used unsupervised learning is auto encoder, the auto encoder unsupervised learning aims to\nmin W1∈RL1×k1k2 ,b1∈RL1\nC ∥ ∥\n∥ Z1 − σ\n(\nW1 Tσ\n( W1Z̃1 + b1i ) + b′1i )∥ ∥ ∥ 2\n2 + ‖W1‖\n2 2\n(7) where W1 is the auto encoder transformation weights, b1 is the encoder bias, C is the tradeoff between errors and model complexity, the used activation function σ() is the hyperbolic tangent function, b′1 is the decoder bias, i is a column\nvector of size m full of elements 1, and Z̃1 is obtained by randomly turning 10% of the elements in Z1 into 0. Then the encoder weights W1 and bias b1 are calculated by stochastic gradient decent method. The solution of the above de-noising auto encoders is as\nW1 = [ w1 1, w1 2, · · · , w1 L1 ]T ,b1 = [ b1 1, b1 2, · · · , b1 L1 ]T (8)\nwhere W1 i is the weights of the ith neuron in the encoder layer, and bi1 stands for the bias of the ith neurons in the encoder layer. Then, we should make these weights changing back to k1 × k2 dimensions to facilitate the next feature mapping operation.\nThe first de-noising auto encoders unsupervised layer’s feature maps are generated by applying the encoder layer to convolute with the input images. For each input image, L1 feature maps can be obtained by\nI1 (i) = σ\n(\nI ∗w1 i + bi11\n)\n, i = 1, · · · , L1 , (9)\nwhere I stands for an input image with zero padded to make I (i) 1 have the same size as the input image, and 1 is a matrix of the same size as I (i) 1 full of elements 1. The second unsupervised layer\nPatches should be extracted from feature maps I (i) 1 obtained from the first unsupervised layer. These patches are also the pre-processed by LCN and whitening operations. By flattening these patches, the patch matrix is constructed as\nZ2 = [ z2 (1), z2 (2), · · · , z2 (m) ] ∈ Rk1k2×m, (10)\nwhere z (i) 2 stands for flattened patch from the first layer’s feature maps after pre-processing, and m stands for the number of extracted patches, and k1, k2 stand for the size of extracted patches.\nThe unsupervised learning method is applied to the patch matrix Z2 the same as the first layer. Assuming that the number of the second layer’s filters is L2, its solution can be obtained as 5 or 8 by solving 4 or 7. The second layer’s feature maps based on the first layer’s feature map are calculated as 6 or 9.\nIf the used unsupervised learning is PCA, the second layer’s unsupervised feature maps are calculated by\nI2 (i) = I1 ∗w2 i, i = 1, · · · , L2 , (11)\nwhere I1 is the zero padded image of the first layer’s feature map, and w i 2 is the second layer’s feature bank weights. If the unsupervised learning is auto encoder, the second layer’s unsupervised feature maps are calculated by\nI2 (i) = σ\n(\nI1 ∗w2 i + bi21\n)\n, i = 1, · · · , L2 , (12)\nwhere w2 i and bi2 are the weights and bias of the ith neuron in the second auto encoder layer. For an input image, we get L1 × (L2 + 1) feature maps after the two convolution layers by concatenating the first layer maps to the second layer. That is\n{I1 (1), I1 (2), · · · , I1 (L1), I2 (1) 1, I2 (2) 1, · · · ,\nI2 (L2) 1, · · · , I2 (1) L1 , I2 (2) L1 , · · · , I2 (L2) L1}, (13)\nwhere I1 (i) stands for the ith feature map of the first layer, and I2 (j) i stands for the jth feature map of the second layer for the ith feature map of the first layer ."
    }, {
      "heading" : "3.1.3. Block-wise histogram",
      "text" : "The third layer of the network is illustrated in Figure 2. The third layer is quite similar to that of LBPH [8]. For an input training image, the first step is to encoder the L1× (L2+1) real valued feature maps with binary values, 0 and 1. The operation converts these feature maps into binary images.\nThe second step is to compress these binary feature maps by quantizing each L1 binary feature maps. Here the number of second layer filters L2 is set as 8, and the number of first layer filters L1 is also set as 8. That is, we compress each L1 binary feature maps into one feature map, and the compressed feature maps have pixel values from 0 to 255. Then we get L2 + 1 compressed feature maps for each training image as\n{\nIc (1), Ic (2), · · · , Ic (L2), Ic\n(L2+1) }\n, (14)\nwhere Ic (i) represents the ith compressed feature map.\nThe third step is to construct block-wise histogram illustrated as the third procedure in Figure 2. First, we should partition each compressed feature map. Assuming that the size of compressed feature is x × y, and the size of block is w1 × w2 with strides s1 × s2, each compressed feature map is partitioned into ⌊(x − w1)/s1 + 1⌋ × ⌊(y − w2)/s2 + 1⌋ blocks. For all the L2 + 1 compressed feature maps of each input image, we get blocks as\n{ B1,B2, · · · ,B(L2+1)×⌊(x−w1)/s1+1⌋×⌊(y−w2)/s2+1⌋ } , (15)\nwhere Bi stands for the ith block. Next step is to build histograms in each of the blocks. In the deep trans-layer PCA network, we set the number of bins in the histogram to 2L1. It means that each integer of the pixel values is set as a bin and a sparse vector representing the histogram is constructed. Then concatenate these NB = (L2+1)×⌊(x−w1)/s1+1⌋×⌊(y−w2)/s2+1 histograms to form a more complete representation of the input image as\nf(Image) =\n[hist(B1) T , hist(B2) T , · · · , hist(BNB) T ]T ∈ RNB(2\nL1), (16)\nwhere hist() stands for histogram operators. Then we use the deep trans-layer unsupervised network representation of each training image to learn a dimensional reduction weight, or to train a classifier to tackle the next applications directly."
    }, {
      "heading" : "3.2. Feature extraction and classification",
      "text" : "After the above representation learning phase, the unsupervised convolution feature banks have been learned. Given a test image, the feature extraction\nis to map the image to trans-layer representation with NB(2 L1) dimensions by these feature banks."
    }, {
      "heading" : "3.2.1. Convolutional extraction and block-wise histogram",
      "text" : "Given a test image, we should zero-pad the image for the sake of keeping the same size between feature maps and input image. Then the pre-processing of LCN and whitening is applied to the zero-padded image. Feature maps are calculated by the first unsupervised layer with L1 filters of size k1 × k2.\nFor the second unsupervised layer, the procedure is the same as the first layer. First, zero-pad each feature map. Then, pre-process these feature maps. Next, put these feature maps into unsupervised learning of L2 feature banks of size k1×k2. Finally, merge the first layer’s feature maps with the second layer’s feature maps to form a more complete trans-layer representation.\nThe last phase is block-wise histogram. Binary encoding is used to tackle these real valued feature maps. And then form a compact representation by quantizing each L1 binary feature maps into one feature map with pixel values from 0 to 2L1 − 1. Then partition these compact feature maps into blocks, and construct histogram representation within each block. Concatenate these histograms to form the deep trans-layer unsupervised representation of translation and rotation invariance."
    }, {
      "heading" : "3.2.2. Classification",
      "text" : "For real applications in the next experiments, classifiers or dimension reduction methods are following the deep trans-layer unsupervised representation. Due to the relative high dimensions of this representation, whitening PCA (WPCA) is used to reduce the representation in object recognition tasks. The WPCA is conducted by conventional PCA weights weighted by the inverse of their corresponding squared root energies. Also, the deep trans-layer unsupervised representation can be directly used to train a classifier to tackle recognition tasks. In our experiments of digit recognition and object recognition, a simple linear SVM classifier with no parameter tuned is used following the deep translayer unsupervised network. The parameter, the cost factor C, in the used linear SVM software kit LIBLINEAR is 1 as default [16]."
    }, {
      "heading" : "4. Experimental results",
      "text" : "In the experiment, we will validate the performance of deep trans-layer PCA network (using PCA unsupervised learning the local receptive features) and deep trans-layer auto encoder network (using de-noising auto encoder unsupervised learning the local receptive features). The proposed deep trans-layer unsupervised network has two key phases, LCN in pre-processing and trans-layer concatenation. We will validate the two phases in the MNIST variations data set [17] using the deep tran-layer PCA network. Also, other parameters such as block size and stride size are chosen through cross validation or validation set. Benchmark experiments are conducted on digit recognition of MNIST [18] and\nMNIST variations [17] data sets, and object recognition of Caltech 101 data set [7]."
    }, {
      "heading" : "4.1. Effect of local contrast normalization",
      "text" : "We first validate the effect of LCN followed by conventional PCA in our network. Experiments about deep trans-layer PCA network with LCN and without LCN are conducted on the MNIST variations data sets.\nThe MNIST data set contains 60,000 training samples and 10,000 test samples of gray images with size 28× 28 pixels. The data set is a subset of NIST, which contains hand-written digits in real world. The recognition targets have been size-normalized and centered in the images [18]. MNIST variations data sets are created by applying simple controllable factor variations on MNIST digits [17]. The data sets are effective ways to study the invariance ability of representation learning methods. Details about recognition tasks, numbers of classes and samples are included in Table 1.\nThe parameters of the block size, the stride size and the patch size are determined on validation set experimentally. The validation sets are typically partitioned from the training set in consistence with the related work [17]. On the digit recognition tasks, the filter size is set to 7 × 7 pixels, the number of filters is set to L1=L2=8 and the size of strides is the half size of the block. For MNIST, MNIST basic, mnist-rotation, and rectangles-image data sets, the block size is 7× 7 pixels. For mnist-back-rand, mnist-back-image and mnist-rot-backimage data sets, the block size is 4× 4 pixels. For rectangles data set, the block size is 14×14 pixels. For convex data set, the block size is 28×28 pixels. These validated tiny parameters are fixed in the following digit recognition tasks. A simple linear SVM with default parameters is connected to the deep trans-layer PCA representation to do recognition task [16].\nTwo groups of experiments are conducted on MNIST basic, mnist-rotation, mnist-back-rand, mnist-back-image-rotation data sets to validate the effect of LCN. The performance of deep trans-layer PCA Network with and without LCN is reported in Table 2.\nFrom Table 2, we observe that our model with LCN achieves better performance on these data sets except for mnist-back-rand data set. The results prove that LCN helps conventional PCA performance, which is explained in section 3.1.1. The main reason why LCN performs worse in mnist-back-rand data set is that, the mean and standard deviation in the data set are all corrupted noise information due to large area random noise in the background, which degrades the performance. The LCN boosts the performance in natural images, which is validated by the improvements in MNIST basic, mnist-rotation, mnist-back-image and mnist-back-image-rotation data sets."
    }, {
      "heading" : "4.2. Effect of trans-layer connection",
      "text" : "The effect of trans-layer connection is validated on MNIST basic, mnistrotation, mnist-back-rand, mnist-back-image-rotation data sets in the second experiment. The parameters are set as that of section 4.1. The performance of deep trans-layer PCA network with and without trans-layer connection is recorded in Table 3.\nFrom Table 3, we observe that deep trans-layer PCA networks with translayer connection have a consistently better performance than those of without trans-layer connection. Trans-layer connection boosts almost 0.1 percentage performance even on the hardest task of MNIST basic data set, and 1% performance for mnist-back-image-rotation data set. The trans-layer connection in deep trans-layer PCA network provides a more complete representation that is always helpful for recognition.\n4.3. Digit recognition on MNIST and MNIST variations data sets\nWe report the performance of the implemented model onMNIST andMNIST variations data sets compared with other methods such as convolution network (ConvNet) [5] and ScatNet-2 [12]. The performance of these methods is recorded in Table 4 of MNIST data set and Table 5 of MNIST variations data sets respectively. For fair comparison, the following results do not include the results using augmented samples.\nFrom Table 4, the results show that the deep trans-layer unsupervised network is only inferior to ScatNet-2 and enhanced Convolution Network related methods. It is worthy to mention that the performance of ScatNet-2 is achieved by connected with a non-linear SVM with RBF kernels with tuned parameters, but our model is connected with a linear SVM with all default parameters in LIBLINEAR software kit [16]. Our model’s performance (0.55) is highly comparable to that of Convolution Network (0.53) on MNIST data set [5]. Because MNIST data set contains too many training samples with small intra-class variability, most of methods work well on this data set and the tiny difference is not much meaningful statistically. Despite this, the deep trans-layer PCA network boosts almost 0.1% performance higher than that of PCA related method,\n1The matlab code can be downloaded from https://github.com/wentaozhu/Deep-trans-layer-unsupervised-network.git\nPCANet. From Table 5, we observe that the deep trans-layer unsupervised network achieves the best performance on six data sets with a simple linear SVM classifier. Our model has a highly superior performance than other methods. It is sufficient to prove that the proposed structure of LCN pre-processing and trans-layer connection work well in the convolution structure with unsupervised filters."
    }, {
      "heading" : "4.4. Object recognition on Caltech 101 data set",
      "text" : "We also evaluate the model for object recognition task on Caltech 101 data set. Caltech 101 data set contains color images belonging to 102 categories including a background class. The number of each class’s images varies from 31 to 800 [7]. The pre-processing of the data set is to convert the images into gray scale, and adjust the longer side of the image to 300 with preserved aspect ratio. Two typical tasks are conducted. One is with a training set of 15 samples per class. The other is with training set of 30 samples per class. The training sets are randomly sampled from Caltech 101, and the rest are test set. Five rounds of experiments are recorded, and the performance is recorded as the average of the five rounds of results.\nParameters are set as follows. The filter size is set to 7×7 pixels, the number of filters is set to L1 = L2 = 8, the block size is set to a quarter of the image size and the size of strides is set to the half size of the blocks. The WPCA is used to reduce the dimension of each block’s trans-layer representation from 256 to 64. Linear SVM with default parameters in LIBLINEAR [16] is used to tackle the recognition task. Comparison results on gray raw images are recorded in Table 6.\nThe Table 6 shows that the deep trans-layer unsupervised network gets the impressive performance trained by 15 samples per class and 30 samples per class tasks respectively by simply using the unsupervised methods. More surprisingly,\nour model with no elaborately tuned parameters gets about 2% upper than that of HSC [13] on 30 samples per class task. The proposed simple network really makes a high progress for the data set."
    }, {
      "heading" : "4.5. Face verification on LFW-a data set",
      "text" : "Face verification task is conducted with our model on LFW-a data set [30]. The LFW data set contains more than 13,000 faces of 5,749 different individuals, and these images were collected from the web of unconstrained conditions. In the LFW data set, the unsupervised setting is used for the deep trans-layer PCA network for sufficiently validating of representation effectiveness. The LFW-a data set with alignment is used, and we cropped the face images into 150× 80 pixels. The standard evaluation protocol on LFW is followed for performance evaluation. The histogram block size is 15 × 13 with non-overlapping. Other parameters are set the same as before. The WPCA is used to reduce the dimension of trans-layer PCA representation to 3,200 after additional square-root operation in the data set. Then use NN classifier with cosine distance to tackle the verification task. The performance is recorded in Table 7 for unsupervised setting with a single descriptor.\nTable 7 shows that the deep trans-layer PCA network achieves the best performance with the unsupervised setting on the LFW-a data set. Also, the boosted performance of our model in contrast to PCANet with the same 3,200 dimensions reveals that, the trans-layer architecture provides uncorrelated information in the increased dimensions, which is beneficial for tasks."
    }, {
      "heading" : "5. Discussion",
      "text" : "One of the typical cases to illustrate why the trans-layer representation works is that the local information of naevi in our faces may vanish in the upper layer’s unsupervised representation for the size of receptive fields are increased and the local discriminative information may be neglected. However, the naevi is of highly discriminative features to recognize each person. In conventional deep neural networks, such discriminative details also hard to preserve with the\nincreasing of layer numbers. The trans-layer representation from the bottom layer preserves these details and is helpful for recognition.\nThe trans-layer representation scheme is quite successful for the convolution network of PCA and auto encoder filters, because the number of filters is relatively too small to preserve enough information for tasks. And information will be lost rapidly with the increase of layer numbers, as illustrated in Fig. 3. If the model uses only the second layer’s feature maps, there would be much noise and lose much useful information. Therefore, the second layer’s feature maps are not enough for classification, and the trans-layer connection is quite helpful.\nThe deep trans-layer PCA network also has some problems to be improved despite that LCN and trans-layer representation are added into it. The main problem is the high dimensions of the trans-layer representation. Although good representation for tasks needs high dimensions, we should make the dimensions of representation as low as possible providing good representational ability. So the future work could include that other translation and rotation invariance preserving methods to reduce the dimensions of the trans-layer representations such as pooling operations [5, 18, 21]."
    }, {
      "heading" : "6. Conclusion",
      "text" : "In this paper, a novel feature learning and representation model, the deep trans-layer unsupervised network, is demonstrated. Several key elements are added to boost the deep unsupervised learning such as the LCN operation and trans-layer representation. Several experiments on digit recognition and object recognition tasks have shown that, the proposed method based on layer-wise unsupervised learning the local receptive features also obtains the impressive results by trans-layer representation. The deep trans-layer unsupervised network has a quite simple structure with much few parameters, in which only the cascaded local receptive filters need to be learned by unsupervised methods. The structure of trans-layer network also provides a novel direction for us to mine."
    } ],
    "references" : [ {
      "title" : "Distinctive image features from scale-invariant keypoints",
      "author" : [ "D.G. Lowe" ],
      "venue" : "International journal of computer vision 60 (2) ",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Histograms of oriented gradients for human detection",
      "author" : [ "N. Dalal", "B. Triggs" ],
      "venue" : "in: Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, Vol. 1, IEEE",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Reducing the dimensionality of data with neural networks",
      "author" : [ "G.E. Hinton", "R.R. Salakhutdinov" ],
      "venue" : "Science 313 (5786) ",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "What is the best multistage architecture for object recognition",
      "author" : [ "K. Jarrett", "K. Kavukcuoglu", "M. Ranzato", "Y. LeCun" ],
      "venue" : "in: Computer Vision, 2009 IEEE 12th International Conference on, IEEE",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Learning feature representations with k-means",
      "author" : [ "A. Coates", "A.Y. Ng" ],
      "venue" : "in: Neural Networks: Tricks of the Trade, Springer",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories",
      "author" : [ "L. Fei-Fei", "R. Fergus", "P. Perona" ],
      "venue" : "Computer Vision and Image Understanding 106 (1) ",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Multiresolution gray-scale and rotation invariant texture classification with local binary patterns",
      "author" : [ "T. Ojala", "M. Pietikainen", "T. Maenpaa" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on 24 (7) ",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "in: Advances in neural information processing systems",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Regularization of neural networks using dropconnect",
      "author" : [ "L. Wan", "M. Zeiler", "S. Zhang", "Y.L. Cun", "R. Fergus" ],
      "venue" : "in: Proceedings of the 30th International Conference on Machine Learning (ICML-13)",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Ica with reconstruction cost for efficient overcomplete feature learning",
      "author" : [ "Q.V. Le", "A. Karpenko", "J. Ngiam", "A.Y. Ng" ],
      "venue" : "in: Advances in Neural Information Processing Systems",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Invariant scattering convolution networks",
      "author" : [ "J. Bruna", "S. Mallat" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on 35 (8) ",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Learning image representations from the pixel level via hierarchical sparse coding",
      "author" : [ "K. Yu", "Y. Lin", "J. Lafferty" ],
      "venue" : "in: Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, IEEE",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Learning mid-level features for recognition",
      "author" : [ "Y.-L. Boureau", "F. Bach", "Y. LeCun", "J. Ponce" ],
      "venue" : "in: Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, IEEE",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations",
      "author" : [ "H. Lee", "R. Grosse", "R. Ranganath", "A.Y. Ng" ],
      "venue" : "in: Proceedings of the 26th Annual International Conference on Machine Learning, ACM",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Liblinear: A library for large linear classification",
      "author" : [ "R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin" ],
      "venue" : "The Journal of Machine Learning Research 9 ",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "An empirical evaluation of deep architectures on problems with many factors of variation",
      "author" : [ "H. Larochelle", "D. Erhan", "A. Courville", "J. Bergstra", "Y. Bengio" ],
      "venue" : "in: Proceedings of the 24th international conference on Machine learning, ACM",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "Proceedings of the IEEE 86 (11) ",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Shape matching and object recognition using shape contexts",
      "author" : [ "S. Belongie", "J. Malik", "J. Puzicha" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on 24 (4) ",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Deformation models for image recognition",
      "author" : [ "D. Keysers", "T. Deselaers", "C. Gollan", "H. Ney" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on 29 (8) ",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Contractive autoencoders: Explicit invariance during feature extraction",
      "author" : [ "S. Rifai", "P. Vincent", "X. Muller", "X. Glorot", "Y. Bengio" ],
      "venue" : "in: Proceedings of the 28th International Conference on Machine Learning (ICML-11)",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Learning and selecting features jointly with point-wise gated {B} oltzmann machines",
      "author" : [ "K. Sohn", "G. Zhou", "C. Lee", "H. Lee" ],
      "venue" : "in: Proceedings of The 30th International Conference on Machine Learning",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Learning convolutional feature hierarchies for visual recognition",
      "author" : [ "K. Kavukcuoglu", "P. Sermanet", "Y.-L. Boureau", "K. Gregor", "M. Mathieu", "Y.L. Cun" ],
      "venue" : "in: Advances in neural information processing systems",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Deconvolutional networks",
      "author" : [ "M.D. Zeiler", "D. Krishnan", "G.W. Taylor", "R. Fergus" ],
      "venue" : "in: Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, IEEE",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "The hierarchical beta process for convolutional factor analysis and deep learning",
      "author" : [ "B. Chen", "G. Polatkan", "G. Sapiro", "L. Carin", "D.B. Dunson" ],
      "venue" : "in: Proceedings of the 28th International Conference on Machine Learning (ICML- 11)",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Deep learning of invariant features via simulated fixations in video",
      "author" : [ "W. Zou", "S. Zhu", "K. Yu", "A.Y. Ng" ],
      "venue" : "in: Advances in Neural Information Processing Systems",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Effective unconstrained face recognition by combining multiple descriptors and learned background statistics",
      "author" : [ "L. Wolf", "T. Hassner", "Y. Taigman" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on 33 (10) ",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Enhanced patterns of oriented edge magnitudes for face recognition and image matching",
      "author" : [ "N.-S. Vu", "A. Caplier" ],
      "venue" : "Image Processing, IEEE Transactions on 21 (3) ",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Blessing of dimensionality: Highdimensional feature and its efficient compression for face verification",
      "author" : [ "D. Chen", "X. Cao", "F. Wen", "J. Sun" ],
      "venue" : "in: Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, IEEE",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Fusing robust face region descriptors via multiple metric learning for face recognition in the wild",
      "author" : [ "Z. Cui", "W. Li", "D. Xu", "S. Shan", "X. Chen" ],
      "venue" : "in: Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, IEEE",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "F",
      "author" : [ "S.U. Hussain", "T. Napoléon" ],
      "venue" : "Jurie, et al., Face recognition using local quantized patterns, in: British Machive Vision Conference",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Fast high dimensional vector multiplication face recognition",
      "author" : [ "O. Barkan", "J. Weill", "L. Wolf", "H. Aronowitz" ],
      "venue" : "in: Computer Vision ",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 1967
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Many successful features are proposed such as SIFT [1] and HoG [2] features in computer vision domain.",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 1,
      "context" : "Many successful features are proposed such as SIFT [1] and HoG [2] features in computer vision domain.",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 2,
      "context" : "The other way is representation learning, which is a quite prevalent topic after deep learning coming out [3].",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 3,
      "context" : "In addition, the local contrast normalization [5] and whitening are added in our trans-layer unsupervised network to boost its learning ability, which are commonly used in deep neural networks [6].",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 4,
      "context" : "In addition, the local contrast normalization [5] and whitening are added in our trans-layer unsupervised network to boost its learning ability, which are commonly used in deep neural networks [6].",
      "startOffset" : 193,
      "endOffset" : 196
    }, {
      "referenceID" : 5,
      "context" : "98 % accuracy on 30 samples per class on Caltech 101 dataset [7].",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 6,
      "context" : "Much research has been conducted to pursuit a good representation by manually designing elaborative low-level features, such as LBPH feature [8], SIFT feature [1] and HoG feature [2] in computer vision field.",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 0,
      "context" : "Much research has been conducted to pursuit a good representation by manually designing elaborative low-level features, such as LBPH feature [8], SIFT feature [1] and HoG feature [2] in computer vision field.",
      "startOffset" : 159,
      "endOffset" : 162
    }, {
      "referenceID" : 1,
      "context" : "Much research has been conducted to pursuit a good representation by manually designing elaborative low-level features, such as LBPH feature [8], SIFT feature [1] and HoG feature [2] in computer vision field.",
      "startOffset" : 179,
      "endOffset" : 182
    }, {
      "referenceID" : 7,
      "context" : "Much recent work in machine learning has focused on how to learn good feature representations from massive unlabelled data, and great progresses have been made by the methods [9, 10].",
      "startOffset" : 175,
      "endOffset" : 182
    }, {
      "referenceID" : 8,
      "context" : "Much recent work in machine learning has focused on how to learn good feature representations from massive unlabelled data, and great progresses have been made by the methods [9, 10].",
      "startOffset" : 175,
      "endOffset" : 182
    }, {
      "referenceID" : 2,
      "context" : "These deep learning methods typically learn multi-level features by greedily “pre-training” each layer using the specific unsupervised learning, and then fine-tuning the pre-trained features by stochastic gradient descent (SGD) method with supervised information [3], [5].",
      "startOffset" : 263,
      "endOffset" : 266
    }, {
      "referenceID" : 3,
      "context" : "These deep learning methods typically learn multi-level features by greedily “pre-training” each layer using the specific unsupervised learning, and then fine-tuning the pre-trained features by stochastic gradient descent (SGD) method with supervised information [3], [5].",
      "startOffset" : 268,
      "endOffset" : 271
    }, {
      "referenceID" : 7,
      "context" : "Besides, the SGD also has various parameters such as momentum, weight decay rate, learning rate, and extra parameters including the Dropout rate or DropConnet rate in recently proposed convolution deep neural networks (ConvNets) [9, 10].",
      "startOffset" : 229,
      "endOffset" : 236
    }, {
      "referenceID" : 8,
      "context" : "Besides, the SGD also has various parameters such as momentum, weight decay rate, learning rate, and extra parameters including the Dropout rate or DropConnet rate in recently proposed convolution deep neural networks (ConvNets) [9, 10].",
      "startOffset" : 229,
      "endOffset" : 236
    }, {
      "referenceID" : 4,
      "context" : "There is also some work on conventional unsupervised learning methods with only single layer [6, 11].",
      "startOffset" : 93,
      "endOffset" : 100
    }, {
      "referenceID" : 9,
      "context" : "There is also some work on conventional unsupervised learning methods with only single layer [6, 11].",
      "startOffset" : 93,
      "endOffset" : 100
    }, {
      "referenceID" : 4,
      "context" : "Although these methods have made much progress on benchmark datasets with almost no hyper parameters, these single layer unsupervised representational learning models require over complete features of dimensions as high as possible, and the parameters need to be elaborately chosen in order to obtain satisfactory results [6].",
      "startOffset" : 322,
      "endOffset" : 325
    }, {
      "referenceID" : 10,
      "context" : "Wavelet scattering networks (ScatNet) are such networks with pre-fixed wavelet filter banks in the deep convolution architectures [12].",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 10,
      "context" : "The PCANet presents a superior or highly comparable performance over other methods such as ScatNet [12], ConvNet [9] and HSC [13], especially in face recognition tasks with large occlusion, illumination, expression and pose changes.",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 7,
      "context" : "The PCANet presents a superior or highly comparable performance over other methods such as ScatNet [12], ConvNet [9] and HSC [13], especially in face recognition tasks with large occlusion, illumination, expression and pose changes.",
      "startOffset" : 113,
      "endOffset" : 116
    }, {
      "referenceID" : 11,
      "context" : "The PCANet presents a superior or highly comparable performance over other methods such as ScatNet [12], ConvNet [9] and HSC [13], especially in face recognition tasks with large occlusion, illumination, expression and pose changes.",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 12,
      "context" : "The procedures of the deep trans-layer unsupervised network is similar to other commonly used frameworks in computer vision [14] and feature learning work [15]",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 13,
      "context" : "The procedures of the deep trans-layer unsupervised network is similar to other commonly used frameworks in computer vision [14] and feature learning work [15]",
      "startOffset" : 155,
      "endOffset" : 159
    }, {
      "referenceID" : 3,
      "context" : "The first is local contrast normalization (LCN) [5].",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 6,
      "context" : "The third layer is quite similar to that of LBPH [8].",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 14,
      "context" : "The parameter, the cost factor C, in the used linear SVM software kit LIBLINEAR is 1 as default [16].",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 15,
      "context" : "We will validate the two phases in the MNIST variations data set [17] using the deep tran-layer PCA network.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 16,
      "context" : "Benchmark experiments are conducted on digit recognition of MNIST [18] and",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 16,
      "context" : "Table 1: Details of the 9 used data sets for digit recognition on MNIST [18] and MNIST variations [17] Data sets Recognition tasks #Classes #Train-Test MNIST Handwritten digits from 0 to 9 10 60000-10000 mnist-basic Smaller subset of MNIST 10 12000-50000 mnist-rot Same as basic with rotation 10 12000-50000 mnist-back-rand Same as basic with random background 10 12000-50000 mnist-back-image Same as basic with image background 10 12000-50000 mnist-rot-back-image Same as basic with rotation and image background 10 12000-50000 rectangles Tall or wide rectangles 2 1200-50000 rectangles-image Same as rect.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 15,
      "context" : "Table 1: Details of the 9 used data sets for digit recognition on MNIST [18] and MNIST variations [17] Data sets Recognition tasks #Classes #Train-Test MNIST Handwritten digits from 0 to 9 10 60000-10000 mnist-basic Smaller subset of MNIST 10 12000-50000 mnist-rot Same as basic with rotation 10 12000-50000 mnist-back-rand Same as basic with random background 10 12000-50000 mnist-back-image Same as basic with image background 10 12000-50000 mnist-rot-back-image Same as basic with rotation and image background 10 12000-50000 rectangles Tall or wide rectangles 2 1200-50000 rectangles-image Same as rect.",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 15,
      "context" : "MNIST variations [17] data sets, and object recognition of Caltech 101 data set [7].",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 5,
      "context" : "MNIST variations [17] data sets, and object recognition of Caltech 101 data set [7].",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 16,
      "context" : "The recognition targets have been size-normalized and centered in the images [18].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 15,
      "context" : "MNIST variations data sets are created by applying simple controllable factor variations on MNIST digits [17].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 15,
      "context" : "The validation sets are typically partitioned from the training set in consistence with the related work [17].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 14,
      "context" : "A simple linear SVM with default parameters is connected to the deep trans-layer PCA representation to do recognition task [16].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 3,
      "context" : "Digit recognition on MNIST and MNIST variations data sets We report the performance of the implemented model onMNIST andMNIST variations data sets compared with other methods such as convolution network (ConvNet) [5] and ScatNet-2 [12].",
      "startOffset" : 213,
      "endOffset" : 216
    }, {
      "referenceID" : 10,
      "context" : "Digit recognition on MNIST and MNIST variations data sets We report the performance of the implemented model onMNIST andMNIST variations data sets compared with other methods such as convolution network (ConvNet) [5] and ScatNet-2 [12].",
      "startOffset" : 231,
      "endOffset" : 235
    }, {
      "referenceID" : 17,
      "context" : "Methods MNIST error rates (%) K-NN-SCM [19] 0.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 18,
      "context" : "63 K-NN-IDM [20] 0.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 13,
      "context" : "54 CDBN [15] 0.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 11,
      "context" : "82 HSC [13] 0.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 3,
      "context" : "77 ConvNet [5] 0.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 10,
      "context" : "45 ScatNet-2 (SVMrbf) [12] 0.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 19,
      "context" : "CAE-2 [23] 2.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 20,
      "context" : "50 - PGBM +DN-1 [25] - - 6.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 10,
      "context" : "ScatNet-2 (SVMrbf) [12] 1.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 14,
      "context" : "It is worthy to mention that the performance of ScatNet-2 is achieved by connected with a non-linear SVM with RBF kernels with tuned parameters, but our model is connected with a linear SVM with all default parameters in LIBLINEAR software kit [16].",
      "startOffset" : 244,
      "endOffset" : 248
    }, {
      "referenceID" : 3,
      "context" : "53) on MNIST data set [5].",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 13,
      "context" : "Methods 15 samples per class (%) 30 samples per class (%) CDBN [15] 57.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 21,
      "context" : "50 ConvNet [26] 57.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 22,
      "context" : "50 DeconvNet [27] 58.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 23,
      "context" : "[28] 58.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "[29] 66.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "50 HSC [13] 74.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 5,
      "context" : "The number of each class’s images varies from 31 to 800 [7].",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 14,
      "context" : "Linear SVM with default parameters in LIBLINEAR [16] is used to tackle the recognition task.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 26,
      "context" : "Methods Verification accuracy (%) POEM [31] 82.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 27,
      "context" : "LBP [32] 84.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 27,
      "context" : "LE [32] 84.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 28,
      "context" : "58 SFRD [33] 84.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 29,
      "context" : "81 I-LQP [34] 86.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 30,
      "context" : "46 OCLBP [35] 86.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 11,
      "context" : "our model with no elaborately tuned parameters gets about 2% upper than that of HSC [13] on 30 samples per class task.",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 25,
      "context" : "Face verification on LFW-a data set Face verification task is conducted with our model on LFW-a data set [30].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 3,
      "context" : "So the future work could include that other translation and rotation invariance preserving methods to reduce the dimensions of the trans-layer representations such as pooling operations [5, 18, 21].",
      "startOffset" : 186,
      "endOffset" : 197
    }, {
      "referenceID" : 16,
      "context" : "So the future work could include that other translation and rotation invariance preserving methods to reduce the dimensions of the trans-layer representations such as pooling operations [5, 18, 21].",
      "startOffset" : 186,
      "endOffset" : 197
    } ],
    "year" : 2015,
    "abstractText" : "Learning features from massive unlabelled data is a vast prevalent topic for highlevel tasks in many machine learning applications. The recent great improvements on benchmark data sets achieved by increasingly complex unsupervised learning methods and deep learning models with lots of parameters usually requires many tedious tricks and much expertise to tune. However, filters learned by these complex architectures are quite similar to standard hand-crafted features visually. In this paper, unsupervised learning methods, such as PCA or auto-encoder, are employed as the building block to learn filter banks at each layer. The lower layer responses are transferred to the last layer (trans-layer) to form a more complete representation retaining more information. In addition, some beneficial methods such as local contrast normalization and whitening are added to the proposed deep trans-layer networks to further boost performance. The trans-layer representations are followed by block histograms with binary encoder schema to learn translation and rotation invariant representations, which are utilized to do high-level tasks such as recognition and classification. Compared to traditional deep learning methods, the implemented feature learning method has much less parameters and is validated in several typical experiments, such as digit recognition on MNIST and MNIST variations, object recognition on Caltech 101 dataset, face verification on LFW dataset. The deep trans-layer unsupervised learning achieves 99.45 % accuracy on MNIST dataset, 67.11 % accuracy on 15 samples per class and 75.98 % accuracy on 30 samples per class on Caltech 101 dataset, 87.10 % on LFW dataset.",
    "creator" : "LaTeX with hyperref package"
  }
}