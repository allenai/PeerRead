{
  "name" : "1605.01335.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning from the memory of Atari 2600",
    "authors" : [ "Jakub Sygnowski", "Henryk Michalewski" ],
    "emails" : [ "J.Sygnowski@students.mimuw.edu.pl", "H.Michalewski@mimuw.edu.pl" ],
    "sections" : [ {
      "heading" : null,
      "text" : "As the benchmark we used the convolutional model proposed in [12] and received comparable results in all considered games. Quite surprisingly, in the case of Seaquest we were able to train RAM-only agents which behave better than the benchmark screen-only agent. Mixing screen and RAM did not lead to an improved performance comparing to screen-only and RAM-only agents."
    }, {
      "heading" : "1 Introduction",
      "text" : "An Atari 2600 controller can perform one of 18 actions1 and in this work we are intended to learn which of these 18 actions is the most profitable given a state of the screen or memory. Our work is based on deep Q-learning [12] – a reinforcement learning algorithm that can learn to play Atari games using\n∗J.Sygnowski@students.mimuw.edu.pl †H.Michalewski@mimuw.edu.pl 1For some games only some of these 18 actions are used in the gameplay. The number\nof available actions is 4 for Breakout, 18 for Seaquest and 6 for Bowling.\nar X\niv :1\n60 5.\n01 33\n5v 1\n[ cs\n.L G\n] 4\nonly input from the screen. The deep Q-learning algorithm builds on the Q-learning [24] algorithm, which in its simplest form (see [14, Figure 21.8]) iteratively learns values Q(state, action) for all state-action pairs and lets the agent choose the action with the highest value. In the instance of Atari 2600 games this implies evaluating all pairs (screen, action) where action is one of the 18 positions of the controller. This task is infeasible and similarly, learning all values Q(state, action) is not a realistic task in other real-world games such as chess or Go.\nThis feasibility issues led to generalizations of the Q-learning algorithm which are intended to limit the number of parameters on which the function Q depends. One can arbitrarily restrict the number of features which can be learned2, but instead of using manually devised features, the deep Qlearning algorithm3 presented in [12] builds them in the process of training of the neural network. Since every neural network is a composition of a priori unknown linear maps and fixed non-linear maps, the aim of the deep Q-learning algorithm is to learn coefficients of the unknown linear maps.\nIn the deep Q-learning algorithm the game states, actions and immediate rewards are passed to a deep convolutional network. This type of network abstracts features of the screen, so that various patterns on the screen can be identified as similar. The network has a number of output nodes – one for each possible action – and it predicts the cumulative game rewards after making moves corresponding to actions.\nA number of decisions was made in the process of designing of the deep Q-learning algorithm (see [12, Algorithm 1] for more details): (1) in each step there is some probability ε of making a random move and it decreases from ε = 1 to ε = 0.1 in the course of training, (2) the previous game states are stored in the replay memory; the updates in the Q-learning are limited to a random batch of events polled from that memory, (3) the updates of unknown linear maps in the neural network are performed according to the gradient of the squared loss function which measures discrepancy between the estimation given by the network and the actual reward. In this work we use the same computational infrastructure as in [12], including the above decisions (1)-(3).\n2E.g. we may declare that Q(state, action) = θ1f1(state, action) + θ2f2(state, action), where f1, f2 are some fixed pre-defined functions, for example f1 may declare value 1 to the state-action pair (screen, fire) if a certain shape appears in the bottom-left corner of the screen and 0 otherwise and f2 may declare value 1 to (screen, left) if an enemy appeared on the right and 0 otherwise. Then the Q-learning algorithm learns the best values of θ1, θ2.\n3This algorithm is also called a deep q-network or DQN.\nRelated work\nThe original algorithm in [12] was improved in a number of ways in [21, 11, 9]. This includes changing network architecture, choosing better hyperparameters and improving the speed of algorithm which optimizes neural network’s loss function. These attempts proved to be successful and made the deep Q-learning algorithm the state-of-the-art method for playing Atari games.\nInstead of the screen one can treat the RAM state of the Atari machine as the game state. The work [10] implemented a classical planning algorithm on the RAM state. Since the Atari 2600 RAM consists of only 128 bytes, one can efficiently search in this low-dimensional state space. Nevertheless, the learning in [10] happens during the gameplay, so it depends on the time limit for a single move. In contrast, in [12] the learning process happens before the gameplay - in particular the agent can play in the real-time. To the best of our knowledge the only RAM-based agent not depending on search was presented in [2]. We cite these results as ale ram.\nIn our work we use the deep Q-learning algorithm, but instead of using screens as inputs to the network, we pass the RAM state or the RAM state and the screen together. In the following sections we describe the games we used for evaluation, as well as the network architectures we tried and hyperparameters we tweaked.\nThe work [12] as the main benchmark\nThe changes to the deep Q-learning algorithm proposed in [11] came at a cost of making computations more involved comparing to [12]. In this work we decided to use as the reference result only the basic work [12], which is not the state of the art, but a single training of a neural network can be contained in roughly 48 hours using the experimental setup we describe below. This decision was also motivated by a preliminary character of our study – we wanted to make sure that indeed the console memory contains useful data which can be extracted during the training process using the deep Q-learning algorithm. From this perspective the basic results in [12] seem to be a perfect benchmark to verify feasibility of learning from RAM. We refer to this benchmark architecture as nips through this paper."
    }, {
      "heading" : "2 The setting of the experiment",
      "text" : ""
    }, {
      "heading" : "2.1 Games",
      "text" : "Bowling – simulation of the game of bowling; the player aims the ball toward the pins and then steers the ball; the aim is to hit the pins [4, 17].\nBreakout – the player bounces the ball with the paddle towards the layer of bricks; the task is to destroy all bricks; a brick is destroyed when the ball hits it [6, 18].\nSeaquest – the player commands a submarine, which can shoot enemies and rescue divers by bringing them above the water-level; the player dies if he fails to get a diver up before the air level of submarine vanishes [15, 20].\nWe’ve chosen these games, because each of them offers a distinct challenge. Breakout is a relatively easy game with player’s actions limited to moves along the horizontal axis. We picked Breakout because disastrous results of learning would indicate a fundamental problem with the RAM learning. The deep Q-network for Seaquest constructed in [12] plays at an amateur human level and for this reason we consider this game as a tempting target for improvements. Also the game state has some elements that possibly can be detected by the RAM-only network (e.g. oxygen-level meter or the number of picked divers). Bowling seems to be a hard game for all deep Q-network models. It is an interesting target for the RAM-based networks, because visualizations suggest that the state of the RAM is changing only very slightly."
    }, {
      "heading" : "2.2 Technical architecture",
      "text" : "By one experiment we mean a complete training of a single deep Q-network. In this paper we quote numerical outcomes of 30 experiments which we performed4. For our experiments we made use of Nathan Sprague’s implementation of the deep Q-learning algorithm [13] in Theano [3] and Lasagne [8]. The code uses the Arcade Learning Environment [2] – the standard framework for evaluating agents playing Atari games. Our code with instructions how to run it can be found on github [19]. All experiments were performed on a Linux machine equipped with Nvidia GTX 480 graphics card. Each of the experiments lasted for 1− 3 days. A single epoch of a RAM-only training lasted approximately half of the time of the screen-only training for an architecture with the same number of layers."
    }, {
      "heading" : "2.3 Network architectures",
      "text" : "We performed experiments with four neural network architectures which accept the RAM state as (a part of) the input. The RAM input is scaled by 256, so all the inputs are between 0 and 1.\nAll the hyperparameters of the network we consider are the same as in [12], if not mentioned otherwise (see Appendix A). We only changed the size of the replay memory to ≈ 105 items, so that it fits into 1.5GB of Nvidia GTX 480 memory5."
    }, {
      "heading" : "3 Plain approach",
      "text" : "Here we present the results of training the RAM-only networks just ram and big ram as well as the benchmark model nips.\nNeural network 1: just ram(outputDim)\nInput: RAM Output: A vector of length outputDim\n1 hiddenLayer1← DenseLayer(RAM, 128, rectify) 2 hiddenLayer2← DenseLayer(hiddenLayer1, 128, rectify) 3 output← DenseLayer(hiddenLayer2, outputDim, no activation) 4 return output\n4The total number of experiments exceeded 100, but this includes experiments involving other models and repetitions of experiments described in this paper.\n5We have not observed a statistically significant change in results when switching between replay memory size of 105 and 5 · 105.\nThe next considered architecture consists of the above network with two additional dense layers:\nNeural network 2: big ram(outputDim)\nInput: RAM Output: A vector of length outputDim\n1 hiddenLayer1← DenseLayer(RAM, 128, rectify) 2 hiddenLayer2← DenseLayer(hiddenLayer1, 128, rectify) 3 hiddenLayer3← DenseLayer(hiddenLayer2, 128, rectify) 4 hiddenLayer4← DenseLayer(hiddenLayer3, 128, rectify) 5 output← DenseLayer(hiddenLayer4, outputDim, no activation) 6 return output\nThe training process consists of epochs, which are interleaved by test periods. During a test period we run the model with the current parameters, the probability of doing a random action ε = 0.05 and the number of test steps (frames) being 10 000. Figures 2, 3 and 4 show the average result per episode (full game, until player’s death) for each epoch.\nFigures 2,3,4 show that there is a big variance of the results between epochs, especially in the RAM models. Because of that, to compare the models, we chose the results of the best epoch6. We summarized these results\n6For Breakout we tested networks with best training-time results. The test consisted of choosing other random seeds and performing 100 000 steps. For all networks, including nips, we received results consistently lower by about 30%.\nin Table 1, which also include the results of ale ram7.\nIn Breakout the best result for the big ram model is weaker than those obtained by the network nips. In Seaquest the best result obtained by the big ram network is better than the best result obtained by the network nips. In Bowling our methods give a slight improvement over the network nips, yet in all considered approaches the learning as illustrated by Figure 4 seem to be poor and the outcome in terms of gameplay is not very satisfactory. We decided to not include in this paper further experiments with Bowling and leave it as a topic of a further research."
    }, {
      "heading" : "4 Regularization",
      "text" : "Training a basic RAM-only network leads to high variance of the results (see the figures in the previous section) over epochs. This can be a sign of overfitting. To tackle this problem we’ve applied dropout [16], a standard regularization technique for neural networks.\nDropout is a simple, yet effective regularization method. It consists of “turning off” with probability p each neuron in training, i.e. setting the output of the neuron to 0, regardless of its input. In backpropagation, the parameters of switched off nodes are not updated. Then, during testing, all neurons are set to “on” – they work as in the course of normal training, with the exception that each neuron’s output is multiplied by p to make up for the skewed training. The intuition behind the dropout method is that it forces each node to learn in absence of other nodes. The work [23] shows an experimental evidence that the dropout method indeed reduces the variance of the learning process.\n7The ale ram’s evaluation method differ – the scores presented are the average over 30 trials consisting of a long period of learning and then a long period of testing, nevertheless the results are much worse than of any DQN-based method presented here.\nWe’ve enabled dropout with probability of turning off a neuron p = 1 2 . This applies to all nodes, except output ones. We implemented dropout for two RAM-only networks: just ram and big ram. This method offers an improvement for the big ram network leading to the best result for Seaquest in this paper. The best epoch results are presented in the Table 2 and the intermediate training results for Seaquest are shown in Figure 5."
    }, {
      "heading" : "5 Decreasing learning rate",
      "text" : "We also tried to reduce the variance of the learner through reduction of the learning rate from 0.0002 to 0.0001.\nThe learning rate is a parameter of the algorithm rmsprop that decides how parameters are changed in each step. Bigger learning rates correspond to moving faster in the parameter space, making learning faster, but more noisy.\nWe expected that the drastic changes in performance between consecutive epochs, as illustrated by Figures 2 and 3, may come from stepping over optimal values when taking too big steps. If it is the case, decreasing the step size should lead to slower learning combined with higher precision of finding minima of the loss function.\nThe results of these experiments can be found in Table 3. Comparing to the training without regularization, scores improved only in the case of Breakout and the just ram network, but not by a big margin."
    }, {
      "heading" : "6 Frame skip",
      "text" : "Atari 2600 was designed to use an analog TV as the output device with 60 new frames appearing on the screen every second. To simplify the search space we impose a rule that one action is repeated over a fixed number of frames. This fixed number is called the frame skip. The standard frame skip used in [12] is 4. For this frame skip the agent makes a decision about the next move every 4 · 1\n60 = 1 15 of a second. Once the decision is made, then it\nremains unchanged during the next 4 frames. Low frame skip allows the network to learn strategies based on a superhuman reflex. High frame skip will limit the number of strategies, hence learning may be faster and more successful.\nIn the benchmark agent nips, trained with the frame skip 4, all 4 frames are used for training along with the sum of the rewards coming after them. This is dictated by the fact that due to hardware limitations, Atari games sometimes ”blink”, i.e. show some objects only every few frames. For example, in the game Space Invaders, if an enemy spaceship shoots in the direction of the player, then shots can be seen on the screen only every second frame and an agent who sees only the frames of the wrong parity would have no access to a critical part of the game information.\nIn the case of learning from memory we are not aware of any critical loses of information when intermediate RAM states are ignored. Hence in our models we only passed to the network the RAM state corresponding to the\nlast frame corresponding to a given action8. The work [5] suggests that choosing the right frame skip can have a big influence on the performance of learning algorithms (see also [7]). Figure 6 and Table 4 show a significant improvement of the performance of the just ram model in the case of Seaquest. Quite surprisingly, the variance of results appeared to be much lower for higher FRAME SKIP.\nAs noticed in [5], in the case of Breakout high frame skips, such as FRAME SKIP = 30, lead to a disastrous performance. Hence we tested only lower FRAME SKIP and for FRAME SKIP = 8 we received results slightly weaker than those with FRAME SKIP = 4.\n8We also tried to pass all the RAM states as a (128 ∗ FRAME SKIP)-dimensional vector, but this didn’t lead to an improved performance."
    }, {
      "heading" : "7 Mixing screen and memory",
      "text" : "One of the hopes of future work is to integrate the information from the RAM and information from the screen in order to train an ultimate Atari 2600 agent. In this work we made some first steps towards this goal. We consider two mixed network architectures. The first one is mixed ram, where we just concatenate the output of the last hidden layer of the convolutional network with the RAM input and then in the last layer apply a linear transformation without any following non-linearity.\nNeural network 3: mixed ram(outputDim)\nInput: RAM,screen Output: A vector of length outputDim\n1 conv1← Conv2DLayer(screen, rectify) 2 conv2← Conv2DLayer(conv1, rectify) 3 hidden← DenseLayer(conv2, 256, rectify) 4 concat← ConcatLayer(hidden,RAM) 5 output← DenseLayer(concat, outputDim, no activation) 6 return output\nThe other architecture is a deeper version of mixed ram. We allow more dense layers which are applied in a more sophisticated way as described below.\nNeural network 4: big mixed ram(outputDim)\nInput: RAM,screen Output: A vector of length outputDim\n1 conv1← Conv2DLayer(screen, rectify) 2 conv2← Conv2DLayer(conv1, rectify) 3 hidden1← DenseLayer(conv2, 256, rectify) 4 hidden2← DenseLayer(RAM, 128, rectify) 5 hidden3← DenseLayer(hidden2, 128, rectify) 6 concat← ConcatLayer(hidden1, hidden3) 7 hidden4← DenseLayer(concat, 256, rectify) 8 output← DenseLayer(hidden4, outputDim, no activation) 9 return output\nThe obtained results presented in Table 5 are reasonable, but not particularly impressive. In particular we did not notice any improvement over the benchmark nips network, which is embedded into both mixed architectures. This suggests that in the mixed ram and big mixed ram models the additional information from the memory is not used in a productive way."
    }, {
      "heading" : "8 RAM visualization",
      "text" : "We visualized the first layers of the neural networks in an attempt to understand how they work. Each column in Figure 7 corresponds to one of 128 nodes in the first layer of the trained big ram network and each row corresponds to one of 128 memory cells. The color of a cell in a given column describes whether the high value in this RAM cell negatively (blue) or positively (red) influence the activation level for that neuron. Figure 7 suggests that the RAM cells with numbers 95−105 in Breakout and 90−105 in Seaquest are important for the gameplay – the behavior of big ram networks depend to the the high extent on the state of these cells."
    }, {
      "heading" : "9 Conclusions",
      "text" : "We trained a number of neural networks capable of playing Atari 2600 games: Bowling, Breakout and Seaquest. The novel aspect of this work is that the networks use information stored in the memory of the console. In all games the RAM agents are on a par with the screen-only agent nips. The RAM\nagents trained using methods described in this work were unaware of more abstract features of the games, such as counters controlling amount of oxygen or the number of divers in Seaquest.\nIn the case of Seaquest, even a simple just ram architecture with an appropriately chosen FRAME SKIP parameter as well as the big ram agent with standard parameters, performs better than the benchmark nips agent. In the case of Breakout, the performance is below the screen-only agent nips, but still reasonable. In the case of of Bowling methods presented in [12] as well as those in this paper are not very helpful – the agents play at a rudimentary level."
    }, {
      "heading" : "10 Future work",
      "text" : "10.1 Games with more refined logic\nSince in the case of Seaquest the performance of RAM-only networks is quite good, a natural next target would be games such as Pacman or Space Invaders, which similarly to Seaquest offer interesting tactical challenges."
    }, {
      "heading" : "10.2 More sophisticated architecture and better hy-",
      "text" : "perparameters\nThe recent papers [11, 21, 9, 25] introduce more sophisticated ideas to improve deep Q-networks. We would like to see whether these improvements also apply to the RAM models.\nIt would be also interesting to tune hyperparameters in a way which would specifically address the needs of RAM-based neural networks. In particular we are interested in:\n• better understanding what the deep Q-network learns about specific memory cells; can one identify critical cells in the memory?\n• improving stability of learning and reducing variance and overfitting,\n• more effective joining of information from the screen and from the memory,\n• trying more complex, deeper architectures for RAM."
    }, {
      "heading" : "10.3 Recurrent neural networks and patterns of mem-",
      "text" : "ory usage\nReading the RAM state while running the deep Q-learning algorithm gives us an access to a practically unlimited stream of Atari 2600 memory states. We can use this stream to build a recurrent neural network which takes into account previous RAM states.\nIn our view it would also be interesting to train an autoencoder. It may help to identify RAM patterns used by Atari 2600 programmers and to find better initial parameters of the neural network [22]."
    }, {
      "heading" : "10.4 Patterns as finite state machines",
      "text" : "The work of D. Angluin [1] introduced the concept of learning the structure of a finite state machine through queries and counter-examples. A game for Atari 2600 can be identified with a finite state machine which takes as input the memory state and action and outputs another memory state. We are interested in devising a neural network which would learn the structure of this finite state machine. The successive layers of the network would learn about sub-automata responsible for specific memory cells and later layers would join the automata into an automaton which would act on the whole memory state."
    }, {
      "heading" : "11 Acknowledgements",
      "text" : "This research was carried out with the support of grant GG63-11 awarded by the Interdisciplinary Centre for Mathematical and Computational Modelling (ICM) University of Warsaw.\nWe would like to express our thanks to Marc G. Bellemare for suggesting this research topic."
    }, {
      "heading" : "A Parameters",
      "text" : "The list of hyperparameters and their descriptions. Most of the descriptions come from [11].\nhyperparameter value description\nminibatch size 32 Number of training cases over which each stochastic gradient descent (SGD) update is computed.\nreplay memory size 100 000 SGD updates are randomply sampled from this number of most recent frames.\nphi length 4 The number of most recent frames experienced by the agent that are given as input to the Q network in case of the networks that accept screen as input.\nupdate rule rmsprop Name of the algorithm optimizing the neural network’s objective function\nlearning rate 0.0002 The learning rate for rmsprop\ndiscount 0.95 Discount factor γ used in the Q-learning update. Measures how much less do we value our expectation of the value of the state in comparison to observed reward.\nepsilon start 1.0 The probability (ε) of choosing a random action at the beginning of the training.\nepsilon decay 1000000 Number of frames over which the ε is faded out to its final value.\nepsilon min 0.1 The final value of ε, the probability of choosing a random action.\nreplay start size 100 The number of frames the learner does just the random actions to populate the replay memory.\nTable 6: Parameters"
    } ],
    "references" : [ {
      "title" : "Learning Regular Sets from Queries and Counterexamples",
      "author" : [ "Dana Angluin" ],
      "venue" : "Inf. Comput",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1987
    }, {
      "title" : "The Arcade Learning Environment: An Evaluation Platform for General Agents",
      "author" : [ "M.G. Bellemare" ],
      "venue" : "Journal of Artificial Intelligence Research",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2013
    }, {
      "title" : "Theano: a CPU and GPU Math Expression Compiler",
      "author" : [ "James Bergstra" ],
      "venue" : "Proceedings of the Python for Scientific Computing Conference (SciPy). Oral Presentation",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2010
    }, {
      "title" : "Frame Skip Is a Powerful Parameter for Learning to Play Atari",
      "author" : [ "Alex Braylan" ],
      "venue" : "Workshop on Learning for General Competency in Video Games",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2015
    }, {
      "title" : "A Comparison of learning algorithms on the Arcade Learning Environment",
      "author" : [ "Aaron Defazio", "Thore Graepel" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2014
    }, {
      "title" : "State of the Art Control of Atari Games Using Shallow Reinforcement Learning",
      "author" : [ "Yitao Liang" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "Classical Planning with Simulators: Results on the Atari Video Games",
      "author" : [ "Nir Lipovetzky", "Miquel Ramirez", "Hector Geffner" ],
      "venue" : "In: International Joint Conference on Artificial Intelligence (IJCAI)",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "Volodymyr Mnih" ],
      "venue" : "Nature",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2015
    }, {
      "title" : "Playing Atari With Deep Reinforcement Learning",
      "author" : [ "Volodymyr Mnih" ],
      "venue" : "NIPS Deep Learning Workshop",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2013
    }, {
      "title" : "Artificial Intelligence - A Modern Approach (3",
      "author" : [ "Stuart J. Russell", "Peter Norvig" ],
      "venue" : "internat. ed.) Pearson Education,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2010
    }, {
      "title" : "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
      "author" : [ "Nitish Srivastava" ],
      "venue" : "Journal of Machine Learning Research",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2014
    }, {
      "title" : "Deep reinforcement learning with double Q-learning",
      "author" : [ "Hado Van Hasselt", "Arthur Guez", "David Silver" ],
      "venue" : null,
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2015
    }, {
      "title" : "Extracting and Composing Robust Features with Denoising Autoencoders",
      "author" : [ "Pascal Vincent" ],
      "venue" : "Proceedings of the 25th International Conference on Machine Learning. ICML ’08. Helsinki, Finland: ACM,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2008
    }, {
      "title" : "An empirical analysis of dropout in piecewise linear networks",
      "author" : [ "David Warde-Farley" ],
      "venue" : "url: http://arxiv.org/abs/1312",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2014
    }, {
      "title" : "Technical Note Q- Learning",
      "author" : [ "Christopher J.C.H. Watkins", "Peter Dayan" ],
      "venue" : "Machine Learning",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1992
    }, {
      "title" : "Dueling Network Architectures Learning",
      "author" : [ ],
      "venue" : null,
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "As the benchmark we used the convolutional model proposed in [12] and received comparable results in all considered games.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 8,
      "context" : "Our work is based on deep Q-learning [12] – a reinforcement learning algorithm that can learn to play Atari games using ∗J.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 14,
      "context" : "The deep Q-learning algorithm builds on the Q-learning [24] algorithm, which in its simplest form (see [14, Figure 21.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 8,
      "context" : "One can arbitrarily restrict the number of features which can be learned, but instead of using manually devised features, the deep Qlearning algorithm presented in [12] builds them in the process of training of the neural network.",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 8,
      "context" : "In this work we use the same computational infrastructure as in [12], including the above decisions (1)-(3).",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 8,
      "context" : "The original algorithm in [12] was improved in a number of ways in [21, 11, 9].",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 11,
      "context" : "The original algorithm in [12] was improved in a number of ways in [21, 11, 9].",
      "startOffset" : 67,
      "endOffset" : 78
    }, {
      "referenceID" : 7,
      "context" : "The original algorithm in [12] was improved in a number of ways in [21, 11, 9].",
      "startOffset" : 67,
      "endOffset" : 78
    }, {
      "referenceID" : 5,
      "context" : "The original algorithm in [12] was improved in a number of ways in [21, 11, 9].",
      "startOffset" : 67,
      "endOffset" : 78
    }, {
      "referenceID" : 6,
      "context" : "The work [10] implemented a classical planning algorithm on the RAM state.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 6,
      "context" : "Nevertheless, the learning in [10] happens during the gameplay, so it depends on the time limit for a single move.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 8,
      "context" : "In contrast, in [12] the learning process happens before the gameplay - in particular the agent can play in the real-time.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 1,
      "context" : "To the best of our knowledge the only RAM-based agent not depending on search was presented in [2].",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 8,
      "context" : "The work [12] as the main benchmark",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 7,
      "context" : "The changes to the deep Q-learning algorithm proposed in [11] came at a cost of making computations more involved comparing to [12].",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 8,
      "context" : "The changes to the deep Q-learning algorithm proposed in [11] came at a cost of making computations more involved comparing to [12].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 8,
      "context" : "In this work we decided to use as the reference result only the basic work [12], which is not the state of the art, but a single training of a neural network can be contained in roughly 48 hours using the experimental setup we describe below.",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 8,
      "context" : "From this perspective the basic results in [12] seem to be a perfect benchmark to verify feasibility of learning from RAM.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 8,
      "context" : "The deep Q-network for Seaquest constructed in [12] plays at an amateur human level and for this reason we consider this game as a tempting target for improvements.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 2,
      "context" : "For our experiments we made use of Nathan Sprague’s implementation of the deep Q-learning algorithm [13] in Theano [3] and Lasagne [8].",
      "startOffset" : 115,
      "endOffset" : 118
    }, {
      "referenceID" : 1,
      "context" : "The code uses the Arcade Learning Environment [2] – the standard framework for evaluating agents playing Atari games.",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 8,
      "context" : "All the hyperparameters of the network we consider are the same as in [12], if not mentioned otherwise (see Appendix A).",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 10,
      "context" : "To tackle this problem we’ve applied dropout [16], a standard regularization technique for neural networks.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 13,
      "context" : "The work [23] shows an experimental evidence that the dropout method indeed reduces the variance of the learning process.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 8,
      "context" : "The standard frame skip used in [12] is 4.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 3,
      "context" : "The work [5] suggests that choosing the right frame skip can have a big influence on the performance of learning algorithms (see also [7]).",
      "startOffset" : 9,
      "endOffset" : 12
    }, {
      "referenceID" : 4,
      "context" : "The work [5] suggests that choosing the right frame skip can have a big influence on the performance of learning algorithms (see also [7]).",
      "startOffset" : 134,
      "endOffset" : 137
    }, {
      "referenceID" : 3,
      "context" : "As noticed in [5], in the case of Breakout high frame skips, such as FRAME SKIP = 30, lead to a disastrous performance.",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 8,
      "context" : "In the case of of Bowling methods presented in [12] as well as those in this paper are not very helpful – the agents play at a rudimentary level.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 7,
      "context" : "The recent papers [11, 21, 9, 25] introduce more sophisticated ideas to improve deep Q-networks.",
      "startOffset" : 18,
      "endOffset" : 33
    }, {
      "referenceID" : 11,
      "context" : "The recent papers [11, 21, 9, 25] introduce more sophisticated ideas to improve deep Q-networks.",
      "startOffset" : 18,
      "endOffset" : 33
    }, {
      "referenceID" : 5,
      "context" : "The recent papers [11, 21, 9, 25] introduce more sophisticated ideas to improve deep Q-networks.",
      "startOffset" : 18,
      "endOffset" : 33
    }, {
      "referenceID" : 15,
      "context" : "The recent papers [11, 21, 9, 25] introduce more sophisticated ideas to improve deep Q-networks.",
      "startOffset" : 18,
      "endOffset" : 33
    }, {
      "referenceID" : 12,
      "context" : "It may help to identify RAM patterns used by Atari 2600 programmers and to find better initial parameters of the neural network [22].",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 0,
      "context" : "Angluin [1] introduced the concept of learning the structure of a finite state machine through queries and counter-examples.",
      "startOffset" : 8,
      "endOffset" : 11
    } ],
    "year" : 2016,
    "abstractText" : "We train a number of neural networks to play games Bowling, Breakout and Seaquest using information stored in the memory of a video game console Atari 2600. We consider four models of neural networks which differ in size and architecture: two networks which use only information contained in the RAM and two mixed networks which use both information in the RAM and information from the screen. As the benchmark we used the convolutional model proposed in [12] and received comparable results in all considered games. Quite surprisingly, in the case of Seaquest we were able to train RAM-only agents which behave better than the benchmark screen-only agent. Mixing screen and RAM did not lead to an improved performance comparing to screen-only and RAM-only agents.",
    "creator" : "LaTeX with hyperref package"
  }
}