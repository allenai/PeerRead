{
  "name" : "533.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "SURPRISE-BASED INTRINSIC MOTIVATION FOR DEEP REINFORCEMENT LEARNING",
    "authors" : [ "Joshua Achiam", "Shankar Sastry" ],
    "emails" : [ "jachiam@berkeley.edu,", "sastry@coe.berkeley.edu" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "A reinforcement learning agent uses experiences obtained from interacting with an unknown environment to learn behavior that maximizes a reward signal. The optimality of the learned behavior is strongly dependent on how the agent approaches the exploration/exploitation trade-off in that environment. If it explores poorly or too little, it may never find rewards from which to learn, and its behavior will always remain suboptimal; if it does find rewards but exploits them too intensely, it may wind up prematurely converging to suboptimal behaviors, and fail to discover more rewarding opportunities. Although substantial theoretical work has been done on optimal exploration strategies for environments with finite state and action spaces, we are here concerned with problems that have continuous state and/or action spaces, where algorithms with theoretical guarantees admit no obvious generalization or are prohibitively impractical to implement.\nSimple heuristic methods of exploring such as -greedy action selection and Gaussian control noise have been successful on a wide range of tasks, but are inadequate when rewards are especially sparse. For example, the Deep Q-Network approach of Mnih et al. [13] used -greedy exploration in training deep neural networks to play Atari games directly from raw pixels. On many games, the algorithm resulted in superhuman play; however, on games like Montezuma’s Revenge, where rewards are extremely sparse, DQN (and its variants [25], [26], [15], [12]) with -greedy exploration failed to achieve scores even at the level of a novice human. Similarly, in benchmarking deep reinforcement learning for continuous control, Duan et al.[5] found that policy optimization algorithms that explored by acting according to the current stochastic policy, including REINFORCE and Trust Region Policy Optimization (TRPO), could succeed across a diverse slate of simulated robotics control tasks with well-defined, non-sparse reward signals (like rewards proportional to the forward velocity of the robot). Yet, when tested in environments with sparse rewards—where the agent would only be able to attain rewards after first figuring out complex motion primitives without reinforcement—every algorithm failed to attain scores better than random agents. The failure modes in all of these cases pertained to the nature of the exploration: the agents encountered reward signals so infrequently that they were never able to learn reward-seeking behavior.\nOne approach to encourage better exploration is via intrinsic motivation, where an agent has a task-independent, often information-theoretic intrinsic reward function which it seeks to maximize in addition to the reward from the environment. Examples of intrinsic motivation include empowerment, where the agent enjoys the level of control it has about its future; surprise, where the agent is excited to see outcomes that run contrary to its understanding of the world; and novelty, where the agent is excited to see new states (which is tightly connected to surprise, as shown in [2]). For in-depth reviews of the different types of intrinsic motivation, we direct the reader to [1] and [17].\nRecently, several applications of intrinsic motivation to the deep reinforcement learning setting (such as [2], [7], [22]) have found promising success. In this work, we build on that success by exploring scalable measures of surprise for intrinsic motivation in deep reinforcement learning. We formulate surprise as the KL-divergence of the true transition probability distribution from a transition model which is learned concurrently with the policy, and consider two approximations to this divergence which are easy to compute in practice. One of these approximations results in using the surprisal of a transition as an intrinsic reward; the other results in using a measure of learning progress which is closer to a Bayesian concept of surprise. Our contributions are as follows:\n1. we investigate surprisal and learning progress as intrinsic rewards across a wide range of environments in the deep reinforcement learning setting, and demonstrate empirically that the incentives (especially surprisal) result in efficient exploration,\n2. we evaluate the difficulty of the slate of sparse reward continuous control tasks introduced by Houthooft et al. [7] to benchmark exploration incentives, and introduce a new task to complement the slate,\n3. and we present an efficient method for learning the dynamics model (transition probabilities) concurrently with a policy.\nWe distinguish our work from prior work in a number of implementation details: unlike Bellemare et al. [2], we learn a transition model as opposed to a state-action occupancy density; unlike Stadie et al. [22], our formulation naturally encompasses environments with stochastic dynamics; unlike Houthooft et al. [7], we avoid the overhead of maintaining a distribution over possible dynamics models, and learn a single deep dynamics model.\nIn our empirical evaluations, we compare the performance of our proposed intrinsic rewards with other heuristic intrinsic reward schemes and to recent results from the literature. In particular, we compare to Variational Information Maximizing Exploration (VIME) [7], a method which approximately maximizes Bayesian surprise and currently achieves state-of-the-art performance on continuous control with sparse rewards. We show that our incentives can perform on the level of VIME at a lower computational cost."
    }, {
      "heading" : "2 PRELIMINARIES",
      "text" : "We begin by introducing notation which we will use throughout the paper. A Markov decision process (MDP) is a tuple, (S,A,R, P, µ), where S is the set of states, A is the set of actions, R : S × A × S → R is the reward function, P : S × A × S → [0, 1] is the transition probability function (where P (s′|s, a) is the probability of transitioning to state s′ given that the previous state was s and the agent took action a in s), and µ : S → [0, 1] is the starting state distribution. A policy π : S × A → [0, 1] is a distribution over actions per state, with π(a|s) the probability of selecting a in state s. We aim to select a policy π which maximizes a performance measure, L(π), which usually takes the form of expected finite-horizon total return (sum of rewards in a fixed time period), or expected infinite-horizon discounted total return (discounted sum of all rewards forever). In this paper, we use the finite-horizon total return formulation."
    }, {
      "heading" : "3 SURPRISE INCENTIVES",
      "text" : "To train an agent with surprise-based exploration, we alternate between making an update step to a dynamics model (an approximator of the MDP’s transition probability function), and making a policy update step that maximizes a trade-off between policy performance and a surprise measure.\nThe dynamics model step makes progress on the optimization problem\nmin φ − 1 |D| ∑ (s,a,s′)∈D logPφ(s ′|s, a) + αf(φ), (1)\nwhere D is is a dataset of transition tuples from the environment, Pφ is the model we are learning, f is a regularization function, and α > 0 is a regularization trade-off coefficient. The policy update step makes progress on an approximation to the optimization problem\nmax π L(π) + η E s,a∼π\n[DKL(P ||Pφ)[s, a]] , (2)\nwhere η > 0 is an explore-exploit trade-off coefficient. The exploration incentive in (2), which we select to be the on-policy average KL-divergence of Pφ from P , is intended to capture the agent’s surprise about its experience. The dynamics model Pφ should only be close to P on regions of the transition state space that the agent has already visited (because those transitions will appear in D and thus the model will be fit to them), and as a result, the KL divergence of Pφ and P will be higher in unfamiliar places. Essentially, this exploits the generalization in the model to encourage the agent to go where it has not gone before. The surprise incentive in (2) gives the net effect of performing a reward shaping of the form\nr′(s, a, s′) = r(s, a, s′) + η (logP (s′|s, a)− logPφ(s′|s, a)) , (3) where r(s, a, s′) is the original reward and r′(s, a, s′) is the transformed reward, so ideally we could solve (2) by applying any reinforcement learning algorithm with these reshaped rewards. In practice, we cannot directly implement this reward reshaping because P is unknown. Instead, we consider two ways of finding an approximate solution to (2).\nIn one method, we approximate the KL-divergence by the cross-entropy, which is reasonable when H(P ) is finite (and small) and Pφ is sufficiently far from P 1; that is, denoting the cross-entropy by H(P, Pφ)[s, a] . = Es′∼P (·|s,a)[− logPφ(s′|s, a)], we assume\nDKL(P ||Pφ)[s, a] = H(P, Pφ)[s, a]−H(P )[s, a] ≈ H(P, Pφ)[s, a].\n(4)\nThis approximation results in a reward shaping of the form\nr′(s, a, s′) = r(s, a, s′)− η logPφ(s′|s, a); (5) here, the intrinsic reward is the surprisal of s′ given the model Pφ and the context (s, a).\nIn the other method, we maximize a lower bound on the objective in (2) by lower bounding the surprise term:\nDKL(P ||Pφ)[s, a] = DKL(P ||Pφ′)[s, a] + E s′∼P\n[ log Pφ′(s ′|s, a)\nPφ(s′|s, a) ] ≥ E s′∼P [ log Pφ′(s ′|s, a)\nPφ(s′|s, a)\n] .\n(6)\nThe bound (6) results in a reward shaping of the form\nr′(s, a, s′) = r(s, a, s′) + η (logPφ′(s ′|s, a)− logPφ(s′|s, a)) , (7)\nwhich requires a choice of φ′. From (6), we can see that the bound becomes tighter by minimizing DKL(P ||Pφ′). As a result, we choose φ′ to be the parameters of the dynamics model after k updates based on (1), and φ to be the parameters from before the updates. Thus, at iteration t, the reshaped rewards are\nr′(s, a, s′) = r(s, a, s′) + η ( logPφt(s ′|s, a)− logPφt−k(s′|s, a) ) ; (8)\nhere, the intrinsic reward is the k-step learning progress at (s, a, s′). It also bears a resemblance to Bayesian surprise; we expand on this similarity in the next section.\nIn our experiments, we investigate both the surprisal bonus (5) and the k-step learning progress bonus (8) (with varying values of k).\n1On the other hand, if H(P )[s, a] is non-finite everywhere—for instance if the MDP has continuous states and deterministic transitions—then as long as it has the same sign everywhere, Es,a∼π[H(P )[s, a]] is a constant with respect to π and we can drop it from the optimization problem anyway."
    }, {
      "heading" : "3.1 DISCUSSION",
      "text" : "Ideally, we would like the intrinsic rewards to vanish in the limit as Pφ → P , because in this case, the agent should have sufficiently explored the state space, and should primarily learn from extrinsic rewards. For the proposed intrinsic reward in (5), this is not the case, and it may result in poor performance in that limit. The thinking goes that when Pφ = P , the agent will be incentivized to seek out states with the noisiest transitions. However, we argue that this may not be an issue, because the intrinsic motivation seems mostly useful long before the dynamics model is fully learned. As long as the agent is able to find the extrinsic rewards before the intrinsic reward is just the entropy in P , the pathological noise-seeking behavior should not happen. On the other hand, the intrinsic reward in (8) should not suffer from this pathology, because in the limit, as the dynamics model converges, we should have Pφt ≈ Pφt−k . Then the intrinsic reward will vanish as desired. Next, we relate (8) to Bayesian surprise. The Bayesian surprise associated with a transition is the reduction in uncertainty over possibly dynamics models from observing it ([1],[8]):\nDKL (P (φ|ht, at, st+1)||P (φ|ht)) . Here, P (φ|ht) is meant to represent a distribution over possible dynamics models parametrized by φ given the preceding history of observed states and actions ht (so ht includes st), and P (φ|ht, at, st+1) is the posterior distribution over dynamics models after observing (at, st+1). By Bayes’ rule, the dynamics prior and posterior are related to the model-based transition probabilities by\nP (φ|ht, at, st+1) = P (φ|ht)P (st+1|ht, at, φ)\nEφ∼P (·|ht) [P (st+1|ht, at, φ)] ,\nso the Bayesian surprise can be expressed as E\nφ∼Pt+1 [logP (st+1|ht, at, φ)]− log E φ∼Pt [P (st+1|ht, at, φ)] , (9)\nwhere Pt+1 = P (·|ht, at, st+1) is the posterior and Pt = P (·|ht) is the prior. In this form, the resemblance between (9) and (8) is clarified. Although the update from φt−k to φt is not Bayesian— and is performed in batch, instead of per transition sample—we can imagine (8) might contain similar information to (9)."
    }, {
      "heading" : "3.2 IMPLEMENTATION DETAILS",
      "text" : "Our implementation usesL2 regularization in the dynamics model fitting, and we impose an additional constraint to keep model iterates close in the KL-divergence sense. Denoting the average divergence as\nD̄KL(Pφ′ ||Pφ) = 1 |D| ∑\n(s,a)∈D\nDKL(Pφ′ ||Pφ)[s, a], (10)\nour dynamics model update is\nφi+1 = arg min φ − 1 |D| ∑ (s,a,s′)∈D logPφ(s ′|s, a) + α‖φ‖22 : D̄KL(Pφ||Pφi) ≤ κ. (11)\nThe constraint value κ is a hyper-parameter of the algorithm. We solve this optimization problem approximately using a single second-order step with a line search, as described by [20]; full details are given in supplementary material. D is a FIFO replay memory, and at each iteration, instead of using the entirety of D for the update step we sub-sample a batch d ⊂ D. Also, similarly to [7], we adjust the bonus coefficient η at each iteration, to keep the average bonus magnitude upper-bounded (and usually fixed). Let η0 denote the desired average bonus, and r+(s, a, s′) denote the intrinsic reward; then, at each iteration, we set\nη = η0 max (\n1, 1|B| ∣∣∣∑(s,a,s′)∈B r+(s, a, s′)∣∣∣) , where B is the batch of data used for the policy update step. This normalization improves the stability of the algorithm by keeping the scale of the bonuses fixed with respect to the scale of the extrinsic rewards. Also, in environments where the agent can die, we avoid the possibility of the intrinsic rewards becoming a living cost by translating all bonuses so that the mean is nonnegative. The basic outline of the algorithm is given as Algorithm 1. In all experiments, we use fully-factored Gaussian distributions for the dynamics models, where the means and variances are the outputs of neural networks.\nAlgorithm 1 Reinforcement Learning with Surprise Incentive Input: Initial policy π0, dynamics model Pφ0 repeat\ncollect rollouts on current policy πi add rollout (s, a, s′) tuples to replay memory D compute reshaped rewards using (5) or (8) with dynamics model Pφi normalize η by the average intrinsic reward of the current batch of data update policy to πi+1 using any RL algorithm with the reshaped rewards update the dynamics model to Pφi+1 according to (11)\nuntil training is completed"
    }, {
      "heading" : "4 EXPERIMENTS",
      "text" : "We evaluate our proposed surprise incentives on a wide range of benchmarks that are challenging for naive exploration methods, including continuous control and discrete control tasks. Our continuous control tasks include the slate of sparse reward tasks introduced by Houthooft et al. [7]: sparse MountainCar, sparse CartPoleSwingup, and sparse HalfCheetah, as well as a new sparse reward task that we introduce here: sparse Swimmer. (We refer to these environments with the prefix ‘sparse’ to differentiate them from other versions which appear in the literature, where agents receive non-sparse reward signals.) Additionally, we evaluate performance on a highly-challenging hierarchical sparse reward task introduced by Duan et al [5], SwimmerGather. The discrete action tasks are several games from the Atari RAM domain of the OpenAI Gym [4]: Pong, BankHeist, Freeway, and Venture.\nEnvironments with deterministic and stochastic dynamics are represented in our benchmarks: the continuous control domains have deterministic dynamics, while the Gym Atari RAM games have stochastic dynamics. (In the Atari games, actions are repeated for a random number of frames.)\nWe use Trust Region Policy Optimization (TRPO) [20], a state-of-the-art policy gradient method, as our base reinforcement learning algorithm throughout our experiments, and we use the rllab implementations of TRPO and the continuous control tasks [5]. Full details for the experimental set-up are included in the appendix.\nOn all tasks, we compare against TRPO without intrinsic rewards, which we refer to as using naive exploration (in contrast to intrinsically motivated exploration). For the continuous control tasks, we also compare against intrinsic motivation using the L2 model prediction error,\nr+(s, a, s ′) = ‖s′ − µφ(s, a)‖2, (12)\nwhere µφ is the mean of the learned Gaussian distribution Pφ. The model prediction error was investigated as intrinsic motivation for deep reinforcement learning by Stadie et al [22], although they used a different method for learning the model µφ. This comparison helps us verify whether or not our proposed form of surprise, as a KL-divergence from the true dynamics model, is useful. Additionally, we compare our performance against the performance reported by Houthooft et al. [7] for Variational Information Maximizing Exploration (VIME), a method where the intrinsic reward associated with a transition approximates its Bayesian surprise using variational methods. Currently, VIME has achieved state-of-the-art results on intrinsic motivation for continuous control.\nAs a final check for the continuous control tasks, we benchmark the tasks themselves, by measuring the performance of the surprisal bonus without any dynamics learning: r+(s, a, s′) = − logPφ0(s′|s, a), where φ0 are the original random parameters of Pφ. This allows us to verify whether our benchmark tasks actually require surprise to solve at all, or if random exploration strategies successfully solve them."
    }, {
      "heading" : "4.1 CONTINUOUS CONTROL RESULTS",
      "text" : "Median performance curves are shown in Figure 1 with interquartile ranges shown in shaded areas. Note that TRPO without intrinsic motivation failed on all tasks: the median score and upper quartile range for naive exploration were zero everywhere. Also note that TRPO with random exploration bonuses failed on most tasks, as shown separately in Figure 2. We found that surprise was not needed to solve MountainCar, but was necessary to perform well on the other tasks.\nThe surprisal bonus was especially robust across tasks, achieving good results in all domains and substantially exceeding the other baselines on the more challenging ones. The learning progress bonus for k = 1 was successful on CartpoleSwingup and HalfCheetah but it faltered in the others. Its weak performance in MountainCar was due to premature convergence of the dynamics model, which resulted in the agent receiving intrinsic rewards that were identically zero. (Given the simplicity of the environment, it is not surprising that the dynamics model converged so quickly.) In Swimmer, however, it seems that the learning progress bonuses did not inspire sufficient exploration. Because the Swimmer environment is effectively a stepping stone to the harder SwimmerGather, where the agent has to learn a motion primitive and collect target pellets, on SwimmerGather, we only evaluated the intrinsic rewards that had been successful on Swimmer.\nBoth surprisal and learning progress (with k = 1) exceeded the reported performance of VIME on HalfCheetah by learning to solve the task more quickly. On CartpoleSwingup, however, both were more susceptible to getting stuck in locally optimal policies, resulting in lower median scores than VIME. Surprisal performed comparably to VIME on SwimmerGather, the hardest task in the slate—in the sense that after 1000 iterations, they both reached approximately the same median score—although with greater variance than VIME.\nOur results suggest that surprisal is a viable alternative to VIME in terms of performance, and is highly favorable in terms of computational cost. In VIME, a backwards pass through the dynamics model must be computed for every transition tuple separately to compute the intrinsic rewards, whereas our surprisal bonus only requires forward passes through the dynamics model for intrinsic\nreward computation. (Limitations of current deep learning tool kits make it difficult to efficiently compute separate backwards passes, whereas almost all of them support highly parallel forward computations.) Furthermore, our dynamics model is substantially simpler than the Bayesian neural network dynamics model of VIME. To illustrate this point, in Figure 3 we show the results of a speed comparison making use of the open-source VIME code [6], with the settings described in the VIME paper. In our speed test, our bonus had a per-iteration speedup of a factor of 3 over VIME.2 We give a full analysis of the potential speedup in Appendix C."
    }, {
      "heading" : "4.2 ATARI RAM DOMAIN RESULTS",
      "text" : "Median performance curves are shown in Figure 4, with tasks arranged from (a) to (d) roughly in order of increasing difficulty.\nIn Pong, naive exploration naturally succeeds, so we are not surprised to see that intrinsic motivation does not improve performance. However, this serves as a sanity check to verify that our intrinsic rewards do not degrade performance. (As an aside, we note that the performance here falls short of the standard score of 20 for this domain because we truncate play at 5000 timesteps.)\nIn BankHeist, we find that intrinsic motivation accelerates the learning significantly. The agents with surprisal incentives reached high levels of performance (scores > 1000) 10% sooner than naive exploration, while agents with learning progress incentives reached high levels almost 20% sooner.\nIn Freeway, the median performance for TRPO without intrinsic motivation was adequate, but the lower quartile range was quite poor—only 6 out of 10 runs ever found rewards. With the learning progress incentives, 8 out of 10 runs found rewards; with the surprisal incentive, all 10 did. Freeway is a game with very sparse rewards, where the agent effectively has to cross a long hallway before it can score a point, so naive exploration tends to exhibit random walk behavior and only rarely reaches the reward state. The intrinsic motivation helps the agent explore more purposefully.\n2We compute this by comparing the marginal time cost incurred just by the bonus in each case: that is, if Tvime, Tsurprisal, and Tnobonus denote the times to 15 iterations, we obtain the speedup as\nTvime − Tnobonus Tsurprisal − Tnobonus .\nIn Venture, we obtain our strongest results in the Atari domain. Venture is extremely difficult because the agent has to navigate a large map to find very sparse rewards, and the agent can be killed by enemies interspersed throughout. We found that our intrinsic rewards were able to substantially improve performance over naive exploration in this challenging environment. Here, the best performance was again obtained by the surprisal incentive, which usually inspired the agent to reach scores greater than 500."
    }, {
      "heading" : "4.3 COMPARING INCENTIVES",
      "text" : "Among our proposed incentives, we found that surprisal worked the best overall, achieving the most consistent performance across tasks. The learning progress-based incentives worked well on some domains, but generally not as well as surprisal. Interestingly, learning progress with k = 10 performed much worse on the continuous control tasks than with k = 1, but we observed virtually no difference in their performance on the Atari games; it is unclear why this should be the case.\nSurprisal strongly outperformed the L2 error based incentive on the harder continuous control tasks, learning to solve them more quickly and without forgetting. Because we used fully-factored Gaussians for all of our dyanmics models, the surprisal had the form\n− logPφ(s′|s, a) = n∑ i=1\n( (s′i − µφ,i(s, a))2\n2σ2φ,i(s, a) + log σφ,i(s, a)\n) + k\n2 log 2π,\nwhich essentially includes the L2-squared error norm as a sub-expression. The relative difference in performance suggests that the variance terms confer additional useful information about the novelty of a state-action pair."
    }, {
      "heading" : "5 RELATED WORK",
      "text" : "Substantial theoretical work has been done on optimal exploration in finite MDPs, resulting in algorithms such as E3 [10], R-max [3], and UCRL [9], which scale polynomially with MDP size. However, these works do not permit obvious generalizations to MDPs with continuous state and action spaces. C-PACE [18] provides a theoretical foundation for PAC-optimal exploration in MDPs with continuous state spaces, but it requires a metric on state spaces. Lopes et al. [11] investigated exploration driven by learning progress and proved theoretical guarantees for their approach in the finite MDP case, but they did not address the question of scaling their approach to continuous or high-dimensional MDPs. Also, although they formulated learning progress in the same way as (8), they formed intrinsic rewards differently. Conceptually and mathematically, our work is closest to prior work on curiosity and surprise [8, 19, 23, 24], although these works focus mainly on small finite MDPs.\nRecently, several intrinsic motivation strategies that deal specifically with deep reinforcement learning have been proposed. Stadie et al. [22] learn deterministic dynamics models by minimizing Euclidean loss—whereas in our work, we learn stochastic dynamics with cross entropy loss—and use L2 prediction errors for intrinsic motivation. Houthooft et al. [7] train Bayesian neural networks to approximate posterior distributions over dynamics models given observed data, by maximizing a variational lower bound; they then use second-order approximations of the Bayesian surprise as intrinsic motivation. Bellemare et al. [2] derived pseudo-counts from CTS density models over states and used those to form intrinsic rewards, notably resulting in dramatic performance improvement on Montezuma’s Revenge, one of the hardest games in the Atari domain. Mohamed and Rezende [14] developed a scalable method of approximating empowerment, the mutual information between an agent’s actions and the future state of the environment, using variational methods. Oh et al. [16] estimated state visit frequency using Gaussian kernels to compare against a replay memory, and used these estimates for directed exploration."
    }, {
      "heading" : "6 CONCLUSIONS",
      "text" : "In this work, we formulated surprise for intrinsic motivation as the KL-divergence of the true transition probabilities from learned model probabilities, and derived two approximations—surprisal and k-step\nlearning progress—that are scalable, computationally inexpensive, and suitable for application to high-dimensional and continuous control tasks. We showed that empirically, motivation by surprisal and 1-step learning progress resulted in efficient exploration on several hard deep reinforcement learning benchmarks. In particular, we found that surprisal was a robust and effective intrinsic motivator, outperforming other heuristics on a wide range of tasks, and competitive with the current state-of-the-art for intrinsic motivation in continuous control."
    }, {
      "heading" : "ACKNOWLEDGEMENTS",
      "text" : "We thank Rein Houthooft for interesting discussions and for sharing data from the original VIME experiments. We also thank Rocky Duan, Carlos Florensa, Vicenc Rubies-Royo, Dexter Scobee, and Eric Mazumdar for insightful discussions and reviews of the preliminary manuscript.\nThis work is supported by TRUST (Team for Research in Ubiquitous Secure Technology) which receives support from NSF (award number CCF-0424422)."
    }, {
      "heading" : "A SINGLE STEP SECOND-ORDER OPTIMIZATION",
      "text" : "In our experiments, we approximately solve several optimization problems by using a single secondorder step with a line search. This section will describe the exact methodology, which was originally given by Schulman et al. [20].\nWe consider the optimization problem\np∗ = max θ L(θ) : D(θ) ≤ δ, (13)\nwhere θ ∈ Rn, and for some θold we have D(θold) = 0,∇θD(θold) = 0, and∇2θD(θold) 0; also, ∀θ,D(θ) ≥ 0. We suppose that δ is small, so the optimal point will be close to θold. We also suppose that the curvature of the constraint is much greater than the curvature of the objective. As a result, we feel justified in approximating the objective to linear order and the constraint to quadratic order:\nL(θ) ≈ L(θold) + gT (θ − θold) g . = ∇θL(θold)\nD(θ) ≈ 1 2 (θ − θold)TA(θ − θold) A . = ∇2θD(θold).\nWe now consider the approximate optimization problem,\np∗ ≈ max θ gT (θ − θold) :\n1 2 (θ − θold)TA(θ − θold) ≤ δ.\nThis optimization problem is convex as long as A 0, which is an assumption that we make. (If this assumption seems to be empirically invalid, then we repair the issue by using the substitution A→ A+ I , where I is the identity matrix, and > 0 is a small constant chosen so that we usually have A+ I 0.) This problem can be solved analytically by applying methods of duality, and its optimal point is\nθ∗ = θold +\n√ 2δ\ngTA−1g A−1g. (14)\nIt is possible that the parameter update step given by (14) may not exactly solve the original optimization problem (13)—in fact, it may not even satisfy the constraint—so we perform a line search between θold and θ∗. Our update with the line search included is given by\nθ = θold + s k\n√ 2δ\ngTA−1g A−1g, (15)\nwhere s ∈ (0, 1) is a backtracking coefficient, and k is the smallest integer for which L(θ) ≥ L(θold) and D(θ) ≤ δ. We select k by checking each of k = 1, 2, ...,K, where K is the maximum number of backtracks. If there is no value of k in that range which satisfies the conditions, no update is performed.\nBecause the optimization problems we solve with this method tend to involve thousands of parameters, inverting A is prohibitively computationally expensive. Thus in the implementation of this algorithm that we use, the search direction x = A−1g is found by using the conjugate gradient method to solve Ax = g; this avoids the need to invert A.\nWhen A and g are sample averages meant to stand in for expectations, we employ an additional trick to reduce the total number of computations necessary to solve Ax = g. The computation of A is more expensive than g, and so we use a smaller fraction of the population to estimate it quickly. Concretely, suppose that the original optimization problem’s objective is Ez∼P [L(θ, z)], and the constraint is Ez∼P [D(θ, z)] ≤ δ, where z is some random variable and P is its distribution; furthermore, suppose that we have a dataset of samples D = {zi}i=1,...,N drawn on P , and we form an approximate optimization problem using these samples. Defining g(z) .= ∇θL(θold, z) and A(z)\n. = ∇2θD(θold, z), we would need to solve(\n1 |D| ∑ z∈D A(z)\n) x = 1\n|D| ∑ z∈D g(z)\nto obtain the search direction x. However, because the computation of the average Hessian is expensive, we sub-sample a batch b ⊂ D to form it. As long as b is a large enough set, then the approximation\n1 |b| ∑ z∈b A(z) ≈ 1 |D| ∑ z∈D A(z) ≈ E z∼P [A(z)]\nis good, and the search direction we obtain by solving( 1\n|b| ∑ z∈b A(z)\n) x = 1\n|D| ∑ z∈D g(z)\nis reasonable. The sub-sample ratio |b|/|D| is a hyperparameter of the algorithm."
    }, {
      "heading" : "B EXPERIMENT DETAILS",
      "text" : "B.1 ENVIRONMENTS\nThe environments have the following state and action spaces: for the sparse MountainCar environment, S ⊆ R2, A ⊆ R1; for the sparse CartpoleSwingup task, S ⊆ R4, A ⊆ R1; for the sparse HalfCheetah\ntask, S ⊂ R20, A ⊆ R6; for the sparse Swimmer task, S ⊆ R13, A ⊆ R2; for the SwimmerGather task, S ⊆ R33, A ⊆ R2; for the Atari RAM domain, S ⊆ R128, A ⊆ {1, ..., 18}. For the sparse MountainCar task, the agent receives a reward of 1 only when it escapes the valley. For the sparse CartpoleSwingup task, the agent receives a reward of 1 only when cos(β) > 0.8, with β the pole angle. For the sparse HalfCheetah task, the agent receives a reward of 1 when xbody ≥ 5. For the sparse Swimmer task, the agent receives a reward of 1 + |vbody| when |xbody| ≥ 2. Atari RAM states, by default, take on values from 0 to 256 in integer intervals. We use a simple preprocessing step to map them onto values in (−1/3, 1/3). Let x denote the raw RAM state, and s the preprocessed RAM state:\ns = 1\n3 ( x 128 − 1 ) .\nB.2 POLICY AND VALUE FUNCTIONS\nFor all continuous control tasks we used fully-factored Gaussian policies, where the means of the action distributions were the outputs of neural networks, and the variances were separate trainable parameters. For the sparse MountainCar and sparse CartpoleSwingup tasks, the policy mean networks had a single hidden layer of 32 units. For sparse HalfCheetah, sparse Swimmer, and SwimmerGather, the policy mean networks were of size (64, 32). For the Atari RAM tasks, we used categorical distributions over actions, produced by neural networks of size (64, 32).\nThe value functions used for the sparse MountainCar and sparse CartpoleSwingup tasks were neural networks with a single hidden layer of 32 units. For sparse HalfCheetah, sparse Swimmer, and SwimmerGather, time-varying linear value functions were used, as described by Duan et al. [5]. For the Atari RAM tasks, the value functions were neural networks of size (64, 32). The neural network value functions were learned via single second-order step optimization; the linear baselines were obtained by least-squares fit at each iteration.\nAll neural networks were feed-forward, fully-connected networks with tanh activation units.\nB.3 TRPO HYPERPARAMETERS\nFor all tasks, the MDP discount factor γ was fixed to 0.995, and generalized advantage estimators (GAE) [21] were used, with the GAE λ parameter fixed to 0.95.\nIn the table below, we show several other TRPO hyperparameters. Batch size refers to steps of experience collected at each iteration. The sub-sample factor is for the second-order optimization step, as detailed in Appendix A.\nB.4 EXPLORATION HYPERPARAMETERS\nFor all tasks, fully-factored Gaussian distributions were used as dynamics models, where the means and variances of the distributions were the outputs of neural networks.\nFor the sparse MountainCar and sparse CartpoleSwingup tasks, the means and variances were parametrized by single hidden layer neural networks with 32 units. For all other tasks, the means and variances were parametrized by neural networks with two hidden layers of size 64 units each. All networks used tanh activation functions.\nFor all continuous control tasks except SwimmerGather, we used replay memories of size 5, 000, 000, and a KL-divergence step size of κ = 0.001. For SwimmerGather, the replay memory was the same size, but we set the KL-divergence size to κ = 0.005. For the Atari RAM domain tasks, we used replay memories of size 1, 000, 000, and a KL-divergence step size of κ = 0.01.\nFor all tasks except SwimmerGather and Venture, 5000 time steps of experience were sampled from the replay memory at each iteration of dynamics model learning to take a stochastic step on (11), and a sub-sample factor of 1 was used in the second-order step optimizer. For SwimmerGather and Venture, 10, 000 time steps of experience were sampled at each iteration, and a sub-sample factor of 0.5 was used in the optimizer.\nFor all continuous control tasks, the L2 penalty coefficient was set to α = 1. For the Atari RAM tasks except for Venture, it was set to α = 0.01. For Venture, it was set to α = 0.1.\nFor all continuous control tasks except SwimmerGather, η0 = 0.001. For SwimmerGather, η0 = 0.0001. For the Atari RAM tasks, η0 = 0.005."
    }, {
      "heading" : "C ANALYSIS OF SPEEDUP COMPARED TO VIME",
      "text" : "In this section, we provide an analysis of the time cost incurred by using VIME or our bonuses, and derive the potential magnitude of speedup attained by our bonuses versus VIME.\nAt each iteration, bonuses based on learned dynamics models incur two primary costs:\n• the time cost of fitting the dynamics model, • and the time cost of computing the rewards.\nWe denote the dynamics fitting costs for VIME and our methods as T fitvime and T fit ours. Although the Bayesian neural network dynamics model for VIME is more complex than our model, the fit times can work out to be similar depending on the choice of fitting algorithm. In our speed test, the fit times were nearly equivalent, but used different algorithms.\nFor the time cost of computing rewards, we first introduce the following quantities:\n• n: the number of CPU threads available, • tf : time for a forward pass through the model, • tb: time for a backward pass through the model, • N : batch size (number of samples per iteration), • k: the number of forward passes that can be performed simultaneously.\nFor our method, the time cost of computing rewards is\nT rewours = Ntf kn .\nFor VIME, things are more complex. Each reward requires the computation of a gradient through its model, which necessitates a forward and a backward pass. Because gradient calculations cannot be efficiently parallelized by any deep learning toolkits currently available3, each (s, a, s′) tuple requires its own forward/backward pass. As a result, the time cost of computing rewards for VIME is:\nT rewvime = N(tf + tb)\nn .\nThe speedup of our method over VIME is therefore\nT fitvime + N(tf+tb) n\nT fitours + Ntf kn\n.\nIn the limit of large N , and with the approximation that tf ≈ tb, the speedup is a factor of ∼ 2k. 3If this is not correct, please contact the authors so that we can issue a correction! But to the best of our knowledge, this is currently true, at time of publication."
    } ],
    "references" : [ {
      "title" : "Novelty or Surprise",
      "author" : [ "Andrew Barto", "Marco Mirolli", "Gianluca Baldassarre" ],
      "venue" : "Frontiers in Psychology,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2013
    }, {
      "title" : "Unifying Count-Based Exploration and Intrinsic Motivation",
      "author" : [ "Marc G Bellemare", "Sriram Srinivasan", "Georg Ostrovski", "Tom Schaul", "David Saxton", "Google Deepmind", "Rémi Munos" ],
      "venue" : "arXiv, (Im),",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2016
    }, {
      "title" : "R-max – A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning",
      "author" : [ "Ronen I Brafman", "Moshe Tennenholtz" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2002
    }, {
      "title" : "Benchmarking Deep Reinforcement Learning for Continuous Control",
      "author" : [ "Yan Duan", "Xi Chen", "John Schulman", "Pieter Abbeel" ],
      "venue" : "The 33rd International Conference on Machine Learning",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2016
    }, {
      "title" : "VIME Open-Source Code",
      "author" : [ "Rein Houthooft" ],
      "venue" : "https://github.com/openai/vime,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2016
    }, {
      "title" : "Variational Information Maximizing Exploration",
      "author" : [ "Rein Houthooft", "Xi Chen", "Yan Duan", "John Schulman", "Filip De Turck", "Pieter Abbeel" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2016
    }, {
      "title" : "Bayesian surprise attracts human attention",
      "author" : [ "Laurent Itti", "Pierre Baldi" ],
      "venue" : "Vision Research,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2009
    }, {
      "title" : "Near-optimal Regret Bounds for Reinforcement Learning",
      "author" : [ "Thomas Jaksch", "Ronald Ortner", "Peter Auer" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2010
    }, {
      "title" : "Near Optimal Reinforcement Learning in Polynomial Time",
      "author" : [ "Michael Kearns", "Satinder Singh" ],
      "venue" : "Proceedings of the 15th International Conference on Machine Learning,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1998
    }, {
      "title" : "Exploration in model-based reinforcement learning by empirically estimating learning progress",
      "author" : [ "Manuel Lopes", "Tobias Lang", "Marc Toussaint", "Py Oudeyer" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2012
    }, {
      "title" : "Asynchronous Methods for Deep Reinforcement Learning",
      "author" : [ "Volodymyr Mnih", "Adrià Puigdomènech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P. Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu" ],
      "venue" : "In ICML,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2016
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei a Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski", "Stig Petersen", "Charles Beattie", "Amir Sadik", "Ioannis Antonoglou", "Helen King", "Dharshan Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis" ],
      "venue" : "Nature, 518(7540):529–533,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2015
    }, {
      "title" : "Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning",
      "author" : [ "Shakir Mohamed", "Danilo J Rezende" ],
      "venue" : "In Proceedings of the 29th Conference on Neural Information Processing Systems",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2015
    }, {
      "title" : "Massively Parallel Methods for Deep Reinforcement Learning",
      "author" : [ "Arun Nair", "Praveen Srinivasan", "Sam Blackwell", "Cagdas Alcicek", "Rory Fearon", "Alessandro De Maria", "Mustafa Suleyman", "Charles Beattie", "Stig Petersen", "Shane Legg", "Volodymyr Mnih", "David Silver" ],
      "venue" : "ICML Deep Learning Workshop 2015,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2015
    }, {
      "title" : "Action- Conditional Video Prediction using Deep Networks in Atari Games",
      "author" : [ "Junhyuk Oh", "Guo Xiaoxiao", "Lee Honglak", "Lewis Richard", "Singh Satinder" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2015
    }, {
      "title" : "How can we define intrinsic motivation",
      "author" : [ "Pierre-Yves Oudeyer", "Frederic Kaplan" ],
      "venue" : "In 8th International Conference on Epigenetic Robotics,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2008
    }, {
      "title" : "PAC Optimal Exploration in Continuous Space Markov Decision Processes",
      "author" : [ "Jason Pazis", "Ronald Parr" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2013
    }, {
      "title" : "Curious Model-Building Control Systems",
      "author" : [ "Jürgen Schmidhuber" ],
      "venue" : "International Joint Conference on Neural Networks,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1991
    }, {
      "title" : "Trust Region Policy Optimization",
      "author" : [ "John Schulman", "Philipp Moritz", "Michael Jordan", "Pieter Abbeel" ],
      "venue" : "In ICML,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2015
    }, {
      "title" : "High- Dimensional Continuous Control Using Generalized Advantage Estimation",
      "author" : [ "John Schulman", "Philipp Moritz", "Sergey Levine", "Michael Jordan", "Pieter Abbeel" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2016
    }, {
      "title" : "Incentivizing Exploration In Reinforcement Learning",
      "author" : [ "Bradly C. Stadie", "Sergey Levine", "Pieter Abbeel" ],
      "venue" : "With Deep Predictive Models. arXiv,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2015
    }, {
      "title" : "Reinforcement driven information acquisition in non-deterministic environments",
      "author" : [ "Jan Storck", "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "In Proceedings of the International . . . ,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1995
    }, {
      "title" : "Planning to be surprised: Optimal Bayesian exploration in dynamic environments",
      "author" : [ "Yi Sun", "Faustino Gomez", "Jürgen Schmidhuber" ],
      "venue" : "In International Conference on Artificial General Intelligence,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2011
    }, {
      "title" : "Deep Reinforcement Learning with Double Q-learning",
      "author" : [ "Hado van Hasselt", "Arthur Guez", "David Silver" ],
      "venue" : "In AAAI 2016,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "[13] used -greedy exploration in training deep neural networks to play Atari games directly from raw pixels.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "On many games, the algorithm resulted in superhuman play; however, on games like Montezuma’s Revenge, where rewards are extremely sparse, DQN (and its variants [25], [26], [15], [12]) with -greedy exploration failed to achieve scores even at the level of a novice human.",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 13,
      "context" : "On many games, the algorithm resulted in superhuman play; however, on games like Montezuma’s Revenge, where rewards are extremely sparse, DQN (and its variants [25], [26], [15], [12]) with -greedy exploration failed to achieve scores even at the level of a novice human.",
      "startOffset" : 172,
      "endOffset" : 176
    }, {
      "referenceID" : 10,
      "context" : "On many games, the algorithm resulted in superhuman play; however, on games like Montezuma’s Revenge, where rewards are extremely sparse, DQN (and its variants [25], [26], [15], [12]) with -greedy exploration failed to achieve scores even at the level of a novice human.",
      "startOffset" : 178,
      "endOffset" : 182
    }, {
      "referenceID" : 3,
      "context" : "[5] found that policy optimization algorithms that explored by acting according to the current stochastic policy, including REINFORCE and Trust Region Policy Optimization (TRPO), could succeed across a diverse slate of simulated robotics control tasks with well-defined, non-sparse reward signals (like rewards proportional to the forward velocity of the robot).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "Examples of intrinsic motivation include empowerment, where the agent enjoys the level of control it has about its future; surprise, where the agent is excited to see outcomes that run contrary to its understanding of the world; and novelty, where the agent is excited to see new states (which is tightly connected to surprise, as shown in [2]).",
      "startOffset" : 340,
      "endOffset" : 343
    }, {
      "referenceID" : 0,
      "context" : "For in-depth reviews of the different types of intrinsic motivation, we direct the reader to [1] and [17].",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 15,
      "context" : "For in-depth reviews of the different types of intrinsic motivation, we direct the reader to [1] and [17].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 1,
      "context" : "Recently, several applications of intrinsic motivation to the deep reinforcement learning setting (such as [2], [7], [22]) have found promising success.",
      "startOffset" : 107,
      "endOffset" : 110
    }, {
      "referenceID" : 5,
      "context" : "Recently, several applications of intrinsic motivation to the deep reinforcement learning setting (such as [2], [7], [22]) have found promising success.",
      "startOffset" : 112,
      "endOffset" : 115
    }, {
      "referenceID" : 20,
      "context" : "Recently, several applications of intrinsic motivation to the deep reinforcement learning setting (such as [2], [7], [22]) have found promising success.",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 5,
      "context" : "[7] to benchmark exploration incentives, and introduce a new task to complement the slate, 3.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "[2], we learn a transition model as opposed to a state-action occupancy density; unlike Stadie et al.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 20,
      "context" : "[22], our formulation naturally encompasses environments with stochastic dynamics; unlike Houthooft et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 5,
      "context" : "[7], we avoid the overhead of maintaining a distribution over possible dynamics models, and learn a single deep dynamics model.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "In particular, we compare to Variational Information Maximizing Exploration (VIME) [7], a method which approximately maximizes Bayesian surprise and currently achieves state-of-the-art performance on continuous control with sparse rewards.",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 0,
      "context" : "A Markov decision process (MDP) is a tuple, (S,A,R, P, μ), where S is the set of states, A is the set of actions, R : S × A × S → R is the reward function, P : S × A × S → [0, 1] is the transition probability function (where P (s′|s, a) is the probability of transitioning to state s′ given that the previous state was s and the agent took action a in s), and μ : S → [0, 1] is the starting state distribution.",
      "startOffset" : 172,
      "endOffset" : 178
    }, {
      "referenceID" : 0,
      "context" : "A Markov decision process (MDP) is a tuple, (S,A,R, P, μ), where S is the set of states, A is the set of actions, R : S × A × S → R is the reward function, P : S × A × S → [0, 1] is the transition probability function (where P (s′|s, a) is the probability of transitioning to state s′ given that the previous state was s and the agent took action a in s), and μ : S → [0, 1] is the starting state distribution.",
      "startOffset" : 368,
      "endOffset" : 374
    }, {
      "referenceID" : 0,
      "context" : "A policy π : S × A → [0, 1] is a distribution over actions per state, with π(a|s) the probability of selecting a in state s.",
      "startOffset" : 21,
      "endOffset" : 27
    }, {
      "referenceID" : 0,
      "context" : "The Bayesian surprise associated with a transition is the reduction in uncertainty over possibly dynamics models from observing it ([1],[8]): DKL (P (φ|ht, at, st+1)||P (φ|ht)) .",
      "startOffset" : 132,
      "endOffset" : 135
    }, {
      "referenceID" : 6,
      "context" : "The Bayesian surprise associated with a transition is the reduction in uncertainty over possibly dynamics models from observing it ([1],[8]): DKL (P (φ|ht, at, st+1)||P (φ|ht)) .",
      "startOffset" : 136,
      "endOffset" : 139
    }, {
      "referenceID" : 18,
      "context" : "We solve this optimization problem approximately using a single second-order step with a line search, as described by [20]; full details are given in supplementary material.",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 5,
      "context" : "Also, similarly to [7], we adjust the bonus coefficient η at each iteration, to keep the average bonus magnitude upper-bounded (and usually fixed).",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 5,
      "context" : "[7]: sparse MountainCar, sparse CartPoleSwingup, and sparse HalfCheetah, as well as a new sparse reward task that we introduce here: sparse Swimmer.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : ") Additionally, we evaluate performance on a highly-challenging hierarchical sparse reward task introduced by Duan et al [5], SwimmerGather.",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 18,
      "context" : ") We use Trust Region Policy Optimization (TRPO) [20], a state-of-the-art policy gradient method, as our base reinforcement learning algorithm throughout our experiments, and we use the rllab implementations of TRPO and the continuous control tasks [5].",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 3,
      "context" : ") We use Trust Region Policy Optimization (TRPO) [20], a state-of-the-art policy gradient method, as our base reinforcement learning algorithm throughout our experiments, and we use the rllab implementations of TRPO and the continuous control tasks [5].",
      "startOffset" : 249,
      "endOffset" : 252
    }, {
      "referenceID" : 20,
      "context" : "The model prediction error was investigated as intrinsic motivation for deep reinforcement learning by Stadie et al [22], although they used a different method for learning the model μφ.",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 5,
      "context" : "[7] for Variational Information Maximizing Exploration (VIME), a method where the intrinsic reward associated with a transition approximates its Bayesian surprise using variational methods.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[7], reproduced here with permission.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "To illustrate this point, in Figure 3 we show the results of a speed comparison making use of the open-source VIME code [6], with the settings described in the VIME paper.",
      "startOffset" : 120,
      "endOffset" : 123
    }, {
      "referenceID" : 8,
      "context" : "Substantial theoretical work has been done on optimal exploration in finite MDPs, resulting in algorithms such as E [10], R-max [3], and UCRL [9], which scale polynomially with MDP size.",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 2,
      "context" : "Substantial theoretical work has been done on optimal exploration in finite MDPs, resulting in algorithms such as E [10], R-max [3], and UCRL [9], which scale polynomially with MDP size.",
      "startOffset" : 128,
      "endOffset" : 131
    }, {
      "referenceID" : 7,
      "context" : "Substantial theoretical work has been done on optimal exploration in finite MDPs, resulting in algorithms such as E [10], R-max [3], and UCRL [9], which scale polynomially with MDP size.",
      "startOffset" : 142,
      "endOffset" : 145
    }, {
      "referenceID" : 16,
      "context" : "C-PACE [18] provides a theoretical foundation for PAC-optimal exploration in MDPs with continuous state spaces, but it requires a metric on state spaces.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 9,
      "context" : "[11] investigated exploration driven by learning progress and proved theoretical guarantees for their approach in the finite MDP case, but they did not address the question of scaling their approach to continuous or high-dimensional MDPs.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 6,
      "context" : "Conceptually and mathematically, our work is closest to prior work on curiosity and surprise [8, 19, 23, 24], although these works focus mainly on small finite MDPs.",
      "startOffset" : 93,
      "endOffset" : 108
    }, {
      "referenceID" : 17,
      "context" : "Conceptually and mathematically, our work is closest to prior work on curiosity and surprise [8, 19, 23, 24], although these works focus mainly on small finite MDPs.",
      "startOffset" : 93,
      "endOffset" : 108
    }, {
      "referenceID" : 21,
      "context" : "Conceptually and mathematically, our work is closest to prior work on curiosity and surprise [8, 19, 23, 24], although these works focus mainly on small finite MDPs.",
      "startOffset" : 93,
      "endOffset" : 108
    }, {
      "referenceID" : 22,
      "context" : "Conceptually and mathematically, our work is closest to prior work on curiosity and surprise [8, 19, 23, 24], although these works focus mainly on small finite MDPs.",
      "startOffset" : 93,
      "endOffset" : 108
    }, {
      "referenceID" : 20,
      "context" : "[22] learn deterministic dynamics models by minimizing Euclidean loss—whereas in our work, we learn stochastic dynamics with cross entropy loss—and use L2 prediction errors for intrinsic motivation.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 5,
      "context" : "[7] train Bayesian neural networks to approximate posterior distributions over dynamics models given observed data, by maximizing a variational lower bound; they then use second-order approximations of the Bayesian surprise as intrinsic motivation.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "[2] derived pseudo-counts from CTS density models over states and used those to form intrinsic rewards, notably resulting in dramatic performance improvement on Montezuma’s Revenge, one of the hardest games in the Atari domain.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 12,
      "context" : "Mohamed and Rezende [14] developed a scalable method of approximating empowerment, the mutual information between an agent’s actions and the future state of the environment, using variational methods.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 14,
      "context" : "[16] estimated state visit frequency using Gaussian kernels to compare against a replay memory, and used these estimates for directed exploration.",
      "startOffset" : 0,
      "endOffset" : 4
    } ],
    "year" : 2017,
    "abstractText" : "Exploration in complex domains is a key challenge in reinforcement learning, especially for tasks with very sparse rewards. Recent successes in deep reinforcement learning have been achieved mostly using simple heuristic exploration strategies such as -greedy action selection or Gaussian control noise, but there are many tasks where these methods are insufficient to make any learning progress. Here, we consider more complex heuristics: efficient and scalable exploration strategies that maximize a notion of an agent’s surprise about its experiences via intrinsic motivation. We propose to learn a model of the MDP transition probabilities concurrently with the policy, and to form intrinsic rewards that approximate the KL-divergence of the true transition probabilities from the learned model. One of our approximations results in using surprisal as intrinsic motivation, while the other gives the k-step learning progress. We show that our incentives enable agents to succeed in a wide range of environments with high-dimensional state spaces and very sparse rewards, including continuous control tasks and games in the Atari RAM domain, outperforming several other heuristic exploration techniques.",
    "creator" : "LaTeX with hyperref package"
  }
}