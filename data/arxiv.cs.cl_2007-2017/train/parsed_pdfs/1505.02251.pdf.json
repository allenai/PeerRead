{
  "name" : "1505.02251.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Probabilistic Cascading for Large Scale Hierarchical Classification",
    "authors" : [ "Aris Kosmopoulos", "Georgios Paliouras" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 5.\n02 25\n1v 1\n[ cs\nHierarchies are frequently used for the organization of objects. Given a hierarchy of classes, two main approaches are used, to automatically classify new instances: flat classification and cascade classification. Flat classification ignores the hierarchy, while cascade classification greedily traverses the hierarchy from the root to the predicted leaf. In this paper we propose a new approach, which extends cascade classification to predict the right leaf by estimating the probability of each root-to-leaf path. We provide experimental results which indicate that, using the same classification algorithm, one can achieve better results with our approach, compared to the traditional flat and cascade classifications."
    }, {
      "heading" : "1 Introduction",
      "text" : "Machine learning is often used to estimate classification models for a set of predefined categories. Most of the times, these categories are assumed\nto be independent. When independence cannot be assumed we may either construct artificial hierarchies (hierarchical clustering), or classify new instances onto a hierarchy that is given, typically representing is-a relations.\nIn this paper we study cases where the hierarchy is already provided. Furthermore, the hierarchy is a tree and the classification nodes are always the leaves of the hierarchy. We also assume that each instance belongs to only one category (single-label classification).\nMany researchers approach hierarchical classification problems [2, 8] using flat classification, i.e. ignoring the hierarchy.\nHierarchical classification approaches, typically divide the problem into smaller ones, usually one classification for each node of the hierarchy. For each of these problems fewer features and instances are required to train a good classifier, compared to the respective flat approaches. This can be very important, especially in cases of large scale classification, where the number of categories and instances can increase to thou-\nsands and millions respectively. In such cases, a hierarchical approach would require much fewer resources than a flat one.\nThe main issue in hierarchical classification is to combine the decisions of the node-specific classifiers appropriately, in order to predict a category for an instance. The most common approach is that of cascade classification. In this case, we start at the root of the hierarchy and greedily select the most probable descendant. This continues until we reach a leaf, which is chosen as the predicted node. The main disadvantage of this approach is that any mistake done during the descent deterministically leads to the wrong final decision. Therefore the cascade is very sensitive to the quality of the inner node classifiers. In this paper we propose a new approach, which is as fast as cascade regarding training but leads to better results compared to cascade and flat classification, using the same classification algorithms.\nIn the next Section we present the related work, while in Section 3 we introduce our approach. Section 4 discusses our experimental results. Finally, Section 5 concludes and points to future work."
    }, {
      "heading" : "2 Related Work",
      "text" : "Although hierarchical classification has many advantages, typically researchers resort to mildly hierarchical or even flat approaches [3]. One reason for this is that flat classification is well studied, so it is easier to transfer methods from this field. On the other hand on large scale problems, the flat use of traditional classifiers, such as SVMs, is often prohibitively expensive computationally [4].\nEarly work in hierarchical classification fo-\ncused on approaches such as shrinkage [5] and hierarchical mixture models [7]. Unfortunately most of these approaches cannot be applied to large scale problems, at least in the form described in the original papers. New methods based on similar ideas, such as that of latent concepts [6], continue to appear in the literature, taking also into account scalability issues. But still most of the proposed methods are tested on rather small datasets with small hierarchies.\nMildly hierarchical approaches, typically make limited use of the hierarchy. Methods such as [9] use only some levels of the hierarchy, flattening the rest. Other approaches such as [1], alter the initial hierarchy before performing cascading in order to minimize errors at the upper levels of the hierarchy."
    }, {
      "heading" : "3 Probabilistic Cascading",
      "text" : "In our method following the cascading approach, we train one binary classifier for each node of the hierarchy. For example, using the hierarchy of Figure 1 we would train one classifier for each of the nodes Arts, Health, Music, Dance, Fitness and Medicine. The binary classifier of a node N is trained using as positive examples the instances belonging to the leaf descendants of N and as negative examples the instances of its siblings. For example, the binary classifier of node Music would use all instances belonging to Music as positive examples and all instances belonging to Dance as negative examples. Similarly for the binary classifier of node Arts, all instances belonging to Music and Dance would be positive examples, while all instances belonging to Fitness and Medicine would be negative.\nThese binary classifiers require fewer resources to be trained compared to flat ones. They can\nalso be more accurate, since they aim to distinguish between fewer categories. For example, if we have 10,000 leaves, each binary classifier would need to separate one class from 9.999 others. In the case of cascading,it would only need to separate between the sibling categories. Such classifiers would also require fewer features to train on, an important characteristic if we consider large datasets.\nThe main disadvantage of cascading is that any mistake is carried over. For example if an instance belonging to category of Music, gets a higher probability by the classifier of Health than that of Arts, is classified wrongly, without taking into consideration the classifiers of Music and Dance. In contrast, our method computes the probability of each root-to-leaf path for a testing instance and we classify it to the most probable path, which we call Ppath. As an example, the probability of an instance d belonging to Music:\nP (Music|d) = P (Arts|d)P (Music|Arts, d)\nP (Arts|Music, d) (1)\nbut since P (Arts|Music, d) = 1:\nP (Music|d) = P (Arts|d)P (Music|Arts, d) (2)\nSimilarly:\nP (Arts|d) = P (Root|d)P (Arts|Root, d)\nP (Root|Arts, d) (3)\nbut since P (Music|Root, d) = 1 and P (Root|d) = 1:\nP (Arts|d) = P (Arts|Root, d) (4)\nBy combining (2) and (4) we get:\nP (Music|d) = P (Arts|Root, d)P (Music|Arts, d) (5)\nThese conditional probabilities are in fact the ones computed by the binary classifiers of each node. So given a document d, a leaf C and a set S of all the ancestors of C:\nP (C|d) =\n|S|∏\ni=1\nP (Si|Ancestor(Si), d) (6)\nand we define Ppath as the:\nPpath(d) = argmax C P (C|d) (7)\nLet’s get back to our initial example where document d belonged to Music. Lets assume that we have the following probabilities:\n• P (Arts|Root, d) = 0.2\n• P (Health|Root, d) = 0.21\n• P (Music|Arts, d) = 0.9\n• P (Dance|Arts, d) = 0.6\n• P (Fitness|Health, d) = 0.1\n• P (Medicine|Health, d) = 0.2\nIf we used standard cascading, document d would be classified to category Medicine. Using Ppath we get:\n• P (Music|d) = 0.18\n• P (Dance|d) = 0.12\n• P (Fitness|d) = 0.021\n• P (Medicine|d) = 0.042\nand Ppath would assign d to class Music. The cost that we have to pay, compared to standard cascading, is that we have to compute all the P (C|d), in order to select the one with the highest probability."
    }, {
      "heading" : "4 Experimental results",
      "text" : "In order to compare our approach against flat and cascade classification, we used the Task 1 dataset of the first Large Scale Hierarchical Text Classification Challenge (LSHTC1).1 This dataset contains 93,505 instances (split into train and validation files), composed of 55,765 distinct features and belonging to 12,294 categories. Classification is only allowed to the leaves of the hierarchy, which is a tree. Each instance belongs to only one category. The testing instances are 34,880 and the results are evaluated using the evaluation measures of the challenge (an Oracle is provided by the organizers) which are the following:\n• Accuracy\n1http://lshtc.iit.demokritos.gr/node/1\n• Macro F-measure\n• Macro Precision\n• Macro Recall\n• Tree Induced Error\nAs a classifier we used a L2 Regularized Logistic Regression with the regularization parameter C set to 1 (usually the default value). We also conducted experiments with other regularization methods and other values of C, but the results were similar. All the experiments were conducted using TF/IDF instead TF features, as our experiments indicated better performance with this feature set.\nThe goal of our experiments was to illustrate that the proposed method can improve the results of flat and cascade classification, using the same algorithm, L2 Regularized Logistic Regression in this case. Further experimentation and engineering could make the method competitive to the best-performing systems, in the challenge. However, we consider this exercise beyond the scope of the paper.\nFor flat classification, we trained one binary classifier (one versus all) for each leaf. We then assigned each testing document to the class with the highest probability. For cascade classification we trained a binary classifier for each node of the hierarchy. We used as positive examples, all the instances belonging to all the descendant leaves of the node and as negative, all the descendant leaves of its siblings. This results in more classifiers than the for flat classification, but each of these classifiers was much easier to train, since it was trained on fewer instances.\nIn table 1, we present the results of each approach, for each evaluation measure. The main observation is that Ppath outperforms both Flat\nand Cascade. Another interesting result is, that Flat is the worst approach, according to Tree Induced Error. This is an indication that by ignoring the hierarchy (flat classification), the mistakes tend to be located further from the correct category in the hierarchy. This is very important in hierarchical classification, since different mistakes carry different weight. Misclassifying an instance to a sibling of the correct category is a smaller error than if it was classified to a category 5 nodes away. Flat evaluation measures, generally fail to capture this, so tree induced error, being the only hierarchical measure of the five that we use is more suitable for comparing the three approaches.\nGiven that our hierarchy is a tree and each instance belongs to only a single class, there is no need to take into account more complex hierarchical evaluation measures and tree induced error is sufficient for safe conclusions.\nBoth Ppath and Flat classification produce a probability for each leaf and the highest one is returned as the predicted category. But what if we evaluated the list of categories, ranked according o their probability? In order to obtain such an assessment in Figure 4 we calculate the recall for the K most-probable categories, with K ranging from 1 to 10. As expected the probability of success increases rapidly with K. This\nis very important, for a realistic semi-automated classification scenario, where a human annotator selects the correct label between thousands of categories. Such a system would allow the annotator to select only between five or ten suggestions. The second observation is that for all values of K, Ppath performs better than the Flat one.\nRegarding the scalability of the approaches, during the two cascading approaches (standard and Ppath) require fewer resources than the flat classifiers. During classification, Ppath is slower than Cascade, since it takes into account all the root-to-leaf paths, and is similar to the cost of Flat classification."
    }, {
      "heading" : "5 Conclusions",
      "text" : "In this paper we present the Ppath method for hierarchical classification. Ppath addresses the disadvantages of traditional flat and cascade classification. Flat classification can be very computational demanding in large scale problems and\nalso ignores completely the hierarchy information which can be exploited for better results. Standard cascading on the other hand is much more computational efficient, but suffers from the problem of early misclassification at the top levels of the hierarchy.\nOur approach has the same training computational complexity as the Cascade, while achieving better scores according to all the tested evaluation measures. However, it is slower during classification, having a complexity is similar to that of flat classification.\nThe version presented in this paper is designed for tree hierarchies. As a future work, we plan to extend the idea of Ppath to DAG hierarchies. Furthermore in this paper we focused on singlelabel classification. Although the idea of Ppath seems compatible with multi-label approaches, further experiments need to be conducted in this direction."
    } ],
    "references" : [ {
      "title" : "Maximum-margin framework for training data synchronization in large-scale hierarchical classification",
      "author" : [ "Rohit Babbar", "Ioannis Partalas", "Eric Gaussier", "Massih-Reza Amini" ],
      "venue" : "In Neural Information Processing,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2013
    }, {
      "title" : "Document classification by computing an echo in a very simple neural network",
      "author" : [ "C. Brouard" ],
      "venue" : "In ICTAI,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2012
    }, {
      "title" : "The ecir 2010 large scale hierarchical classification workshop",
      "author" : [ "A. Kosmopoulos", "É. Gaussier", "G. Paliouras", "S. Aseervatham" ],
      "venue" : "SI- GIR Forum,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2010
    }, {
      "title" : "Support vector machines clas-  sification with a very large-scale taxonomy",
      "author" : [ "T. Liu", "Y. Yang", "H. Wan", "H. Zeng", "Z. Chen", "W. Ma" ],
      "venue" : "SIGKDD Explor. Newsl.,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2005
    }, {
      "title" : "Improving text classification by shrinkage in a hierarchy of classes",
      "author" : [ "A. McCallum", "R. Rosenfeld", "T.M. Mitchell", "A.Y. Ng" ],
      "venue" : "In ICML,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1998
    }, {
      "title" : "Hierarchical text classification with latent concepts",
      "author" : [ "X. Qiu", "X. Huang", "Z. Liu", "J. Zhou" ],
      "venue" : "In ACL (Short Papers),",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2011
    }, {
      "title" : "Text classification in a hierarchical mixture model for small training sets",
      "author" : [ "K. Toutanova", "F. Chen", "K. Popat", "T. Hofmann" ],
      "venue" : "In CIKM,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2001
    }, {
      "title" : "Large-scale semantic indexing of biomedical publications",
      "author" : [ "G. Tsoumakas", "M. Laliotis", "N. Markantonatos", "I.P. Vlahavas" ],
      "venue" : "In BioASQ@CLEF,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2013
    }, {
      "title" : "Deep classification in large-scale text hierarchies",
      "author" : [ "G. Xue", "D. Xing", "Q. Yang", "Y. Yu" ],
      "venue" : "In SIGIR,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Many researchers approach hierarchical classification problems [2, 8] using flat classification, i.",
      "startOffset" : 63,
      "endOffset" : 69
    }, {
      "referenceID" : 7,
      "context" : "Many researchers approach hierarchical classification problems [2, 8] using flat classification, i.",
      "startOffset" : 63,
      "endOffset" : 69
    }, {
      "referenceID" : 2,
      "context" : "Although hierarchical classification has many advantages, typically researchers resort to mildly hierarchical or even flat approaches [3].",
      "startOffset" : 134,
      "endOffset" : 137
    }, {
      "referenceID" : 3,
      "context" : "On the other hand on large scale problems, the flat use of traditional classifiers, such as SVMs, is often prohibitively expensive computationally [4].",
      "startOffset" : 147,
      "endOffset" : 150
    }, {
      "referenceID" : 4,
      "context" : "Early work in hierarchical classification focused on approaches such as shrinkage [5] and hierarchical mixture models [7].",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 6,
      "context" : "Early work in hierarchical classification focused on approaches such as shrinkage [5] and hierarchical mixture models [7].",
      "startOffset" : 118,
      "endOffset" : 121
    }, {
      "referenceID" : 5,
      "context" : "New methods based on similar ideas, such as that of latent concepts [6], continue to appear in the literature, taking also into account scalability issues.",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 8,
      "context" : "Methods such as [9] use only some levels of the hierarchy, flattening the rest.",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 0,
      "context" : "Other approaches such as [1], alter the initial hierarchy before performing cascading in order to minimize errors at the upper levels of the hierarchy.",
      "startOffset" : 25,
      "endOffset" : 28
    } ],
    "year" : 2015,
    "abstractText" : "Hierarchies are frequently used for the organization of objects. Given a hierarchy of classes, two main approaches are used, to automatically classify new instances: flat classification and cascade classification. Flat classification ignores the hierarchy, while cascade classification greedily traverses the hierarchy from the root to the predicted leaf. In this paper we propose a new approach, which extends cascade classification to predict the right leaf by estimating the probability of each root-to-leaf path. We provide experimental results which indicate that, using the same classification algorithm, one can achieve better results with our approach, compared to the traditional flat and cascade classifications.",
    "creator" : "LaTeX with hyperref package"
  }
}