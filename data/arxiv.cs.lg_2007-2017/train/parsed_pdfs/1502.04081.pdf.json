{
  "name" : "1502.04081.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Linear Dynamical System Model for Text",
    "authors" : [ "David Belanger", "Sham Kakade" ],
    "emails" : [ "BELANGER@CS.UMASS.EDU", "SKAKADE@MICROSOFT.COM" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "In many NLP applications, there is limited available labeled training data, but tremendous quantities of unlabeled, in-domain text. An effective semi-supervised learning technique is to learn word embeddings on the unlabeled data, which map every word to a low dimensional dense vector (Bengio et al., 2006; Mikolov et al., 2013; Penning-\nton et al., 2014), and then use these as features for supervised training on the labeled data (Turian et al., 2010; Passos et al., 2014; Bansal et al., 2014). Furthermore in many deep architectures for NLP, the first layer maps words to low-dimensional vectors, and these parameters are initialized with unsupervised embeddings (Collobert et al., 2011; Socher et al., 2013; Vinyals et al., 2014).\nMost of these methods embed word types, i.e., words independent of local context, as opposed to work tokens, i.e., instances of words within their context. Ideally we would have a different representation per token. For example, depending on the context, “bank” is the side of a river or a financial institution. Furthemore, we would like such embeddings to come from a probablistic sequence model that allows us to study the transition dynamics of text generation in low dimensional space.\nWe present a method for obtaining such context-dependent token embeddings, using a generative model with a vectorvalued latent variable per token and performing posterior inference for each sentence. Specifically, we employ a Gaussian linear dynamical system (LDS), with efficient inference from a Kalman filter. To learn the LDS parameters, we use a two-stage procedure, initializing with the method of moments, and then performing EM with the approximate second order statistics (ASOS) technique of Martens (2010). Overall, after taking a single pass over the training corpus, our the runtime of our approximate maximumlikelihood estimation (MLE) procedure is independent of the amount of training data since it operates on aggregate co-occurrence counts. Furthermore, performing inference to obtain token embeddings has the same time complexity as widely-used discrete first-order sequence models.\nWe fit the LDS to a one-hot encoding of each token in the input text sequence. Therefore, the LDS is a mis-specified generative model, since draws from it are not proper indicator vectors. However, we embrace this multivariate Gaussian model instead of a continuous-state dynamical system with a multinomial link function because the Gaussian LDS offers several desirable scalability properties: (1) Kalman\nar X\niv :1\n50 2.\n04 08\n1v 1\n[ st\nat .M\nL ]\n1 3\nFe b\n20 15\nfilter inference is simple and efficient (2) using ASOS, the cost of our learning iterations does not scale with the corpus size, (3) we can initialize EM using a method-of-moments estimator that requires a single SVD of a co-occurrence matrix, (4) our M-step updates are simple least-squares problems, solvable in closed form, (4) if we had used a multinomial link function, we would have performed inference using extended Kalman filtering, which makes a secondorder approximation of the log-likelihood, and thus leads to a Gaussian LDS anyway (Ghahramani & Roweis, 1999), and (5) by using EM, we avoid stochastic-gradient-based optimization, which requires careful tuning for nonconvex problems. A naive application of our method scales to large amounts of training data, but not high-dimensional observations. In response, we contribute a variety of new methods for scaling up our learning techniques to handle large input vocabularies.\nWe employ our inferred token embeddings as features for part of speech (POS) and named entity recognition (NER) taggers. For POS, we obtain a 30% relative error reduction when applying a local classifier to our context-dependent embeddings rather than Word2Vec context-independent embeddings (Mikolov et al., 2013). When using our token embeddings as additional features in lexicalized POS and NER taggers, which already have explicit features and test-time inference for context-dependence, we obtain signficant gains over the baseline, performing as well as using Word2Vec embeddings. We also present experiments demonstrating that the transition dynamics of the LDS capture salient patterns, such as transforming first names into last names.\nFinally, the functional form of the Kalman filter update equations for our LDS are identical to the updates of a recurrent neural network (RNN) language model without non-linearities (Mikolov, 2012). A key difference between the LDS and an RNN, however, is that the LDS provides a natural backwards pass, using Kalman smoothing, where a token’s embedding depends on text to both the right and left. Drawing on the parallelism between filtering and the RNN, we use the LDS parameters to initialize gradientbased optimization of a nonlinear RNN, where the LDS training time is a small fraction of the RNN time. The RNN initialized with the LDS obtains a signficant decrease in perplexity v.s. the baseline RNN, and only requires 70% as many training epochs, saving 1 day on a single CPU core."
    }, {
      "heading" : "2. Related Work",
      "text" : "We provide a continuous analog of popular discrete-state generative models used in NLP for inducing class membership for tokens, including class-based language models (Brown et al., 1992; Chelba & Jelinek, 2000) and induction of POS tags (Christodoulopoulos et al., 2010). In par-\nticular, Brown clusters (Brown et al., 1992) are commonly used by practioners with lots of unlabeled in-domain data.\nOur learning algorithm is very scalable because it operates on aggregate count matrices, rather than individual tokens. Similar algorithms have been proposed for obtaining type-level embeddings via matrix factorization (Pennington et al., 2014; Levy & Goldberg, 2014). However, these are context-independent and ignore the transition dynamics that link tokens’ embeddings. Furthermore, they require careful tuning of stochastic gradient algorithms. Previous methods for token-level embeddings either use a rigid set of prototypes (Huang et al., 2012; Neelakantan et al., 2014) or embed the token’s context, ignoring the token itself (Dhillon et al., 2011).\nFor learning discrete-state latent variable models, spectral learning methods also use count matrices, and thus are similarly scalable (Anandkumar et al., 2014). However, an LDS offers key advantages: we do not use third-order moments, which are difficult to estimate, and perform approximate MLE, rather than the method of moments, which exhibits poor statistical efficiency.\nRecently, RNNs have been used to provide impressive results in NLP applications including translation (Sutskever et al., 2014), language modeling (Mikolov et al., 2014), and parsing (Vinyals et al., 2014). We do not attempt to replace these with a Kalman filter, as we expect non-linearities are crucial for capturing long-term interactions and rigid, combinatorial constraints in the outputs. However, RNNs training can take days, even on GPUs, and requires careful tuning of stochastic gradient algorithms. Given the scalability of our parameter-free training algorithm, and our favorable preliminary results using the LDS to initialize a nonlinear RNN, we encourage further work on using linear latent variable models and the Gaussian approximations of multinomial data to develop sophisticated initialization methods. Already, practitioners have started using such techniques for initializing simple nonlinear deep neural networks using the recommendations of Saxe et al. (2014). Finally, our work differs from Pasa & Sperduti (2014), who initialize an RNN using spectral techniques, in that we perform maximum-likelihood learning. We found this crucial for good performance in our NLP experiments."
    }, {
      "heading" : "3. Background: Gaussian Linear Dynamical Systems",
      "text" : "We consider sequences of observations w1, . . . , wn, where each wi is a V -dimensional vector. An Gaussian LDS follows the following generative model (Kalman, 1960; Roweis & Ghahramani, 1999):\nxt = Axt−1 + η (1) wt = Cxt + , (2)\nwhere h < V is the dimensionality of the hidden states xt and ∼ N(0, D), η ∼ N(0, Q). For simplicity, we assume x0 is constant.\nThe latent space for x is completely unobserved and we could choose any coordinate system for it, while maintaining the same data likelihood. Therefore, without loss of generality, we can either fix A = I or Q = I , and we fix Q. Furthermore, note that the magnitude of the maximum eigenvalue of A must be no larger than 1 if the system is stable. We assume that the data we fit to has been centered, in which case the maximum eigenvalue is strictly less than 1, since this implies xt is asymptotically mean zero (independent of x0), so that xt is also asymptotically mean zero.\nFinally, define the covariance at lag k to be\nΨk = E[wt+kw>t ], (3)\nwhich is valid because we assume the data to be mean zero. Our learning algorithms require only a few Ψk (up to about k = 10 in practice) as input. These matrices can be gathered using a single pass over the data, and their size does not depend on the amount of data. Furthermore, constructing these matrices can be accelerated by splitting the data into chunks, and aggregating separate matrices afterwards."
    }, {
      "heading" : "3.1. Inference",
      "text" : "The xt are distributed as a multivariate Gaussian under both the LDS prior and posterior (conditional on observations w), so they can be fully characterized by a mean and variance. Therefore, we will use x̂t and St for the mean and covariance under the posterior for xt given w1:(t−1), computed using Kalman filtering, and x̄t and ST when considering the posterior for xt given all the data w1:T , computed using Kalman smoothing. In Appendix B.1 we provide the full filtering and smoothing updates, which compute different means and variances for every timestep. Note that the updates require inverting a V × V matrix in every step.\nWe employ the widely-used ’steady-state’ approximation, which yields substantially more efficient filtering and smoothing updates (Rugh, 1996). A key property of filtering and smoothing is that the updates to St and ST do not depend on the actual observations, but only on the model’s parameters. Furthermore, they will converge quickly to time-independent ‘steady-state’ values. Define Σ1 = E[x̂tx̂>t |w1:(t−1)] to be the aymptotic limit of the covariance St under the posterior for each xt given its history (at steady state, this is shared for all t). Here, expectation is taken with respect to both time and the posterior for the latent variables. This satisfies\nΣ1 = AΣ1A > +Q,\nwhich can be solved for quickly using fixed point iteration. Similarly, we can solve for Σ0 = E[x̂tx̂>t |w1:t]. Note that\nsteady state, a property of the posterior, is unrelated to the stationary distribution of the LDS, which is unconditional on observations.\nUnder, the steady state assumption, we can perform filtering and smoothing using substantially more efficient updates. We have:\nx̂t = (A−KCA)x̂t−1 +Kwt (4) x̄t = Jx̄t+1 + (I − JA)x̂t (5)\nHere, the steady-state Kalman gain matrix is:\nK = Σ1C >S−1ss ∈ Rh×V , (6)\nwhere we define\nSss = CΣ1C > +D, (7)\nthe unconditional prior covariance for w under the model. Note that (A−KCA) is data-independent and can be precomputed, as can the smoothing matrix J = Σ0A>(Σ1)−1. For long sequences, steady-state filtering provides asymptotically exact inference. However, for short sequences it is an approximation."
    }, {
      "heading" : "3.2. Learning: Expectation-Maximization",
      "text" : "See Ghahramani & Hinton (1996) for a full exposition on learning the parameters of an LDS using EM. Under the steady-state assumption, the M step requires:\nE[x̄tx̄>t ], E[x̄tx̄>t+1], E[x̄tw>t ], (8)\nwhere the expectation is taken with respect to time and the posterior for the latent variables. This can be computed using Kalman smoothing and then averaging over time. The M step can then be done in closed form, since it is solving least-squares regressions for xt+1 against xt andwt against xt to obtain A and C. Lastly, D can be recovered using:\nD = Ψ0 − CE [ x̄tw > t ] − E [ wtx̄ > t ] C> + CE [ x̄x̄>t ] C> (9)"
    }, {
      "heading" : "3.3. Learning: EM with ASOS (Martens, 2010)",
      "text" : "EM requires recomputing the second order statistics (8) in every iteration. While these can be computed using Kalman smoothing on the entire training set, we are interested in datasets with billions of timesteps. Fortunately, we can avoid smoothing by employing the ASOS (approximate second order statistics) method of Martens (2010), which directly performs inference about the time-averaged second order statistics.\nUnder the steady-state assumption, this is doable because we can recursively define relationships between second order statistics at lag k and at lag k + 1 using the recursive\nrelationships of the underlying dynamical system. Namely, rather than performing posterior inference by recursively applying the linear operations (4) and (5), and then averaging over time, we switch the order of these operations and apply the linear operators to time-averaged second order statistics. For example, the following equality is an immediate consequences of the filtering equation (4) (where expectation is with respect to t and the posterior for x):\nE[x̂ttw>t ] = (A−KCA)E[x̂t−1t−1w>t ] +KE[wtw>t ] (10)\nASOS uses a number of such recursions, along with methods for estimating covariances at a time horizon r. These covariances can be approximated by assuming that they are exactly described by the current estimate of the model parameters. Therefore, unlike standard EM, performing EM with ASOS allows us to precompute an empirical estimate of the Ψk at various lags (up to about r = 10) and then never touch the data again. Furthermore, (Martens, 2010) demonstrates that the ASOS approximation is consistent. Namely, the error in approximating the time-averaged second order statistics vanishes with infinite data when evaluated at the MLE parameters. Overall, ASOS scales linearly with r and the cost of multiplying by the Ψk."
    }, {
      "heading" : "3.4. Learning: Subspace Identification",
      "text" : "We initialize EM using Subspace Identification (SSID), a family of method-of-moments estimators that use spectral decomposition to recover LDS parameters (Van Overschee & De Moor, 1996). The rationale for such a combination is that the method of moments is statistically consistent, so performing it on reasonably-sized datasets will yield parameters in the neighborhood of the global optimum, and then EM will perform local hill climbing to find a local optimum of the marginal likelihood. For LDS, this combination yields empirical accuracy gains in Smith et al. (1999). A related two-stage estimator, where the local search of EM is replaced with a single Newton step on the local likelihood surface, is known to be minimax optimal, under certain local asymptotic normality conditions (Le Cam, 1974).\nFor our particular application, we use SSID as an approximate method, where it is not statistically consistent, due to the mis-specification of fitting indicator-vector data as a multivariate Gaussian. In our experiments, we discuss the superiority of SSID+EM rather than just SSID. We do not present results using EM initialized randomly rather than with SSID, since we found it very difficult for our high dimensional problems to generate initial parameters that allowed EM to reach high likelihoods.\nWe employ the ‘n4sid’ algorithm of Van Overschee & De Moor (1994). Define r to be a small integer. Define the (rV ) × h matrix Γr = [ C ; CA ; CA2 ; . . . ; CAr−1 ] , where ‘;’ denotes vertical concatenation. Also define the\nAlgorithm 1 Learning an LDS for Text Input: Text Corpus, approximation horizon r (e.g., 10) Output: LDS parameters and filtering matrices: (A,C,D,K, J)\nGather the matrices Ψk = E[wt+kw>t ] (k < r) W ← Ψ− 1 2\n0 (diagonal whitening matrix) Ψk ←WΨkW> (whitening) Params← Subspace ID(Ψ0, . . . ,Ψr) Params← ASOS EM(Params,Ψ0, . . . ,Ψr)\nx-w covariance at lag 0: G = E[xt+1y>t ] and the h× (rV ) matrix ∆r = [ Ar−1G Ar−2G . . . AG G ] .\nNext, define the Hankel matrix\nHr =  Ψr Ψr−1 Ψr−2 . . . Ψ1 Ψr+1 Ψr Ψr−1 . . . Ψ2 . . .\nΨ2r−1 Ψ2r−2 Ψr−3 . . . Ψr  . (11) Then, we have Hr = Γr∆r.\nLet (U, S, V ) result from a rank-h SVD of an empirical estimate of Hr, from which we set Γr = US 1 2 and ∆r = S 1 2V >. To recover the LDS parameters, we first define ∆1:(r−1)r to be the submatrix of ∆r corresponding to the first (r − 1) blocks. Similarly, define ∆2:rr . From the definition of ∆r, we have that A∆2:rr = ∆ 1:(r−1) r , so we can estimate A as A = ∆1:(r−1)r (∆2:rr ) +. Next, one can read off an estimate for E as the first block of Γr. Alternatively, since the previous step gives us a value for A one can set up a regression problem similar to the previous step to solve for C by invoking the block structure in Γr.\nFinally, we need to recover the covariance matrix D. We first find the asymptotic latent covariance Σ1 using fixedpoint iteration Σ1 = AΣ1A> + Q. From this, we set D using a similar update as (12), except we use Σ1, unconditional on data, rather than statistics of the LDS posterior:\nD = Ψ0 − CΣ1C>. (12)"
    }, {
      "heading" : "4. Linear Dynamical Systems for Text",
      "text" : "We fit an LDS to text using SSID to initialize EM, where the E step is performed using ASOS. A summary of the procedure is provided in Algorithm 1. SSID and ASOS scale to extremely large training sets, since they only require the Ψk matrices, for small k. However, they can not directly handle the very high dimensionality of text observations (vocabulary size V ≈ 105). In this section, we first describe particular properties of the data distribution. Then,\nwe describe novel techniques for leveraging these properties to yield scalable learning algorithms.\nDefine w̃t as an indicator vector that is 1 in the index of the word at time t and define µi to be the corpus frequency of word type i. We fit to the mean-zero observations wt = w̃t − µ. Note that the LDS will not generate observations with the structure of a one-hot vector shifted by a constant mean, so we cannot use it directly as a generative language model. On the other hand, we can still fit models to training data with this structure, perform posterior inference given observations, assess the likelihood of a corpus, etc. In our experiments, we demonstrate the usefulness of these in a variety of applications. We have:\nΨ0 = E[wtw>t ] = E[w̃tw̃>t ]− µµ> = diag(µ)− µµ>, (13)\nwhile at higher lags,\nΨk = E[wtw>t+k] = E[w̃tw̃>t+k]− µµ>. (14)\nApproximating these covariances from a length-T corpus:\nµi = Ex[w̃t] = 1\nT #(word i appears), (15)\nwhere #() denotes the count of an event. We also have\nE[w̃tw̃>t+k]i,j = (16) 1 T #(word i appears with word j k positions to the right).\nFor real-world data, (16) will be extremely sparse, with the number of nonzeros substantially less than both V 2 and the length of the corpus. The fact that (14) is sparse-minuslow-rank and (13) is diagonal-minus-low-rank is critical for scaling up the learning algorithms. First of all, we do not instantiate these as V × V dense matrices, but operate directly on their factorized structure. Second, in Sec. 4.2 we show how the structure of (13) allows us to model fullrank V × V noise covariance matrices implicitly. Strictly speaking, the number of nonzeros will in (16) will increase as the corpus size increases, due to heavy-tailed word cooccurence statistics. However, this growth is sublinear in T and can be mitigated by ignoring rare words.\nUnfortunately, each Ψk is rank-deficient. Not only is E[wt] = 0, but also the sum of everywt is zero (because w̃t is a one-hot vector and µ is a vector of word frequencies). Define 1 to be the length-V vector of ones. Our data lives on 1⊥, the d−1 dimensional subspace, orthogonal to 1. Doing maximum likelihood in RV instead of 1⊥ will lead to a degenerate likelihood function, since the empirical variance in the 1 direction is 0. However, projecting the data to this subspace breaks the special structure described above, so we instead work in RV and perform projections onto 1⊥\nimplicitly as-needed. Fortunately, both SSID and EM find C that lies in the column space of the data, so iterations of our learning algorithm will maintain that 1 /∈ col(C). In Appendix A.2, we describe how to handle this rank deficiency when computing the Kalman gain.\nNote that we could have used pretrained type-level embeddings to project our corpus and then train an LDS on lowdimensional dense observations. However, this is vulnerable to the subspace of the type-level embeddings, which are not trained to maximize the likelihod of a sequence model, and thus might not capture proper syntactic and semantic information. We will release the code of our implementation. SSID requires simple scripting on top of a sparse linear algebra library. Our EM implementation consists of small modifications to Martens’ public code."
    }, {
      "heading" : "4.1. Scalable Spectral Decomposition",
      "text" : "SSID requires a rank-h SVD of the very large block Hankel matrix Hr (11). We employ the randomized approximate SVD algorithm of Halko et al. (2011). To factorize a matrix X , this requires repeated multiplication by X and by X>. All the submatrices in Hr are sparse-minus-low-rank, so we handle the sparse and low-rank terms individually within the multiplication subroutines."
    }, {
      "heading" : "4.2. Modeling Full-Rank Noise Covariance",
      "text" : "The noise covariance matrix D is V × V , which is unmanageably large for our application, and thus it is reasonable to employ a spherical D = dI or diagonal D = diag(d1, . . . , dV ) approximation. For our problem, however, we found that these approximations performed poorly. Because of the property 1>wt = 0, off-diagonal elements ofD are critical for modeling the anti-correlations between coordinates. This would have been captured we passed wt through a logistic multinomial link function. However, this prevents simple inference using Kalman filter. To maintain conjugacy, practitioners sometimes employ the quadratic upper bound to a logistic multinomial likelihood introduced in Böhning (1992), which hard-codes the coordinate-wise anticorrelations via D = 12 [ I − 1V+111 > ] . However, we found this data-independent estimator performed poorly.\nInstead, we exploit a particular property of the SSID and EM estimators forD in (12) and (9). Namely, both setD to Ψ0 minus a low-rank matrix, and thusD is diagonal-minuslow-rank, due to the structure in (13). For the LDS, we mostly seek to manipulate the precision matrixD−1. While instantiating this dense V × V matrix is infeasible, multiplication by D−1 and evaluation of det(D−1) can both be done efficiently using the matrix inversion lemma, a.k.a. the ShermanWoodbury-Morrison formula (Appendix B.2). In Appendix A.3, we also leverage the lemma to efficiently\nevaluate the training likelihood. These uses of the lemma differ from its common usage for LDS, when not using the steady-state assumption and the posterior precision matrix needs to be updated using rank one updates to the covariance. Our technique is particular to fitting indicator-vector data as a multivariate Gaussian."
    }, {
      "heading" : "4.3. Whitening",
      "text" : "Before applying our learning algorithms, we first whiten the Ψ matrices with the diagonal transformation.\nW = Ψ − 12 0 = diag(µ − 12 1 , . . . , µ − 12 V ). (17)\nFitting to WΨkW>, rather than Ψk, maintains the data’s sparse-minus-low-rank and diagonal-minus-lowrank structures. Furthermore, EM is unaffected, i.e., applying EM to linearly-transformed data is equivalent to learning on the original data and then transforming post-hoc.\nOn the other hand, the SSID output is affected by whitening, since the squared reconstruction loss that SVD implicitly minimizes depends on the coordinate system of the data. We found such whitening crucial for obtaining high-quality initial parameters. Whitening for SSID, which is recommended by Van Overschee & De Moor (1996), solves a very similar factorization problem as canonical correlation analysis between words and their contexts, which has been used successfully to learn word embeddings (Dhillon et al., 2011; 2012) and identify the parameters of class-based language models (Stratos et al., 2014)).\nIn Appendix A.1 we also provide an algorithm, which relies on whitening, for manually ensuring the D returned by SSID is PSD, without needing to factorize a V ×V matrix. Such manual correction is unnecessary during EM, since the estimator (9) is guaranteed to be PSD."
    }, {
      "heading" : "5. Embedding Tokens using the LDS",
      "text" : "The only data-dependent term in the steady-state filtering and smoothing equations (4) and (5) is Kwt. Since wt can take on only V possible values, we precompute these word-type-level vectors. The computational cost of filtering/smoothing a length T sequence is O(Th2), which is identical to the cost of inference on a discrete first-order sequence model. (6) is not directly usable to obtain K, due to the data’s rank-deficiency, and we provide an efficient alternative in Appendix A.2. This also requires the matrix inversion lemma to avoid instantiating S−1 in (6).\nIn our experiments we use the latent space to define features for tokens. However, distances in this space are not well-defined, since the likelihood is invariant to any linear transformation of the latent variables. To place xt in reasonable coordinates, we compute the empirical posterior covariance M = E[x̄x̄>] on the training data (using\nASOS). Then, we whiten xt using M− 1 2 and project the result onto the unit sphere."
    }, {
      "heading" : "6. Relation to Recurrent Neural Networks",
      "text" : "We now highlight the similarity between the parametrization of an RNN architecture commonly used for language modeling and our Kalman filter. This allows us to use our LDS as a novel method for initializing the parameters of a non-linear RNN, which we explore in Sec. 7.4. Following (Mikolov, 2012) we consider the network structure:\nht = σ(Aht−1 +Bwt−1) (18) wt ∼ SoftMax(Cht), (19)\nHere, we employ the SoftMax transformation of a vector v as vi → exp(vi)/ ∑ k exp(vk). The coordinate-wise nonlinearity σ(·) is, for example, a sigmoid, and the network is initialized with some fixed vector h0.\nConsider the use of the steady-state Kalman filter (4) as an online predictor, where the mean prediction for wt is given by Cx̂t. Then, if we replace σ and SoftMax with the identity, the Kalman filter and the RNN have the same set of parameters, where weB corresponds toK andA corresponds to (A − KCA). In terms of the state dynamics, the LDS may provide parameters that are reasonable for a nonlinear RNN, since the sigmoid σ has a regime for inputs close to zero where it behaves like the identity. A linear approximation of SoftMax() ignores mutual exclusivity. However, we discuss in Section 4.2 that using a full-rank D captures some coordinate-wise anti-correlations. Also, (19) does not affect the state evolution in (18).\nA key difference between the LDS and the RNN is that the LDS provides a backwards pass, using Kalman smoothing, where x̄t depends on words to the right. For RNNs, this would requires separate model (Schuster & Paliwal, 1997)."
    }, {
      "heading" : "7. Experiments",
      "text" : ""
    }, {
      "heading" : "7.1. LDS Transition Dynamics",
      "text" : "Many popular word embedding methods learn word-tovector mapping, but do not learn the dynamics of text’s evolution in the latent space. Using the specific LDS model we describe in the next section, we employ the transition matrix A to explore properties of these dynamics. Because the state evolution is linear, it can be studied easily using a spectral decomposition. Namely, A converts its left singular vectors into (scaled) right singular vectors. For each vector, we find the words that are most likely to be generated from this state. In Table 1 we present these singular vector pairs. We find that they reflect interpretable transition dynamics. In all but the last block, the vectors reflect strict state transitions. However, in the case of final block,\nabout food, we found that it contains topical information invariant under A. Overall, we did not find such salient structure in the parameters estimated using SSID."
    }, {
      "heading" : "7.2. POS Tagging",
      "text" : "Unsupervised learning of generative discrete sequence models for text has been shown to capture part-of-speech (POS) information (Christodoulopoulos et al., 2010). In response, we assess the ability of the LDS to also capture POS structure. Token embeddings can be used to predict POS in two ways: (1) by applying a local classifier to each token’s embedding, or (2) by including each token’s embedding as additional features in a lexicalized tagger. For both, we train the tagging model on the Penn Treebank (PTB) train set, which is not included for LDS training. Token embeddings are obtained from Kalman smoothing. We evaluate tagging accuracy on the PTB test set using the 12 ‘universal’ POS tags (Petrov et al., 2011) and the original tags. We contrast the LDS with type embeddings from Word2Vec, trained on the LDS data (Mikolov et al., 2013).\nWe fit our LDS using a combination of the APNews, New York Times, and RCV1 newswire corpora, about 1B tokens total. We maintain punctuation and casing of the text, but replace all digits with “NUM’ and all but the most 200k frequent types with “OOV.” We employ r = 4 for SSID, r = 7 for EM, and h = 200. We add 1000 psuedocounts for each type, by adding 1000T to each coordinate of µ.\nThe LDS hyperparameters were selected by maximizing the accuracy of a local classifier on the PTB dev set. This also included when to terminate EM. For Word2Vec, we performed a broad search over hyperparameters, again maximizing for local POS tagging. Our local classifier was\na two-layer neural network with 25 hidden units, which outperformed a linear classifier. The best Word2Vec configuration used the CBOW architecture with a window width of 3. The lexicalized tagger’s hyperparameters were also tuned on the PTB dev set. For the local tagging, we ignored punctuation and few common words types such as “and” in training. Instead, we classified them directly using their majority tag in the training data.\nOverall, we found that the LDS and Word2Vec took about 12 hours to train on a single-core CPU. Since the Word2Vec algorithm is simple and the code is heavily optimized, it performs well, but our learning algorithm would have been substantially faster given a larger training set, since the Ψk matrices can be gathered in parallel and the cost of SSID and ASOS is sublinear in the corpus size. In Section 7.4, training the LDS is order of magnitude faster than an RNN.\nOur results are shown in Table 2. Left to right, we compare Word2Vec (W2V), SSID, EM initialized with SSID (EM), our baseline lexicalized tagger (LEX), the lexicalized tagger with extra features from LDS token embeddings (LEX + EM), and the lexicalized tagger with typelevel Word2Vec embeddings (LEX + SSID).\nThe first 3 columns perform local classification. First, while SSID is crucial for EM initialization, we found it performed poorly on its own. However, EM outperforms Word2Vec substantially. We expect this is because the LDS embeds tokens v.s. types and EM explicitly maximizes the likelihood of text sequences, and thus it forces token embeddings to capture the transition dynamics of syntax. All accuracy differences are statistically significant at a .05 significance level using the exact binomial test. In Appendix C, we present a figure demonstrating the importance of SSID v.s. random initialization.\nThe final 3 columns use a carefully-engineered tagger. For universal tags, LDS and Word2Vec both contribute a statistically-significant gain in accuracy over the baseline (Lex). However, their difference is not significant. In the case of the original PTB tags, we find that Word2Vec achieves a statistically significant gain over LEX, but the LDS does not. We expect that our context-dependent embeddings perform as well as context-independent embeddings since the taggers have features and test-time inference for capturing non-local dependencies."
    }, {
      "heading" : "7.3. Named Entity Recognition",
      "text" : "In Table 3 we consider the effect of unsupervised token features for NER on the Conll 2003 dataset using a lexicalized tagger (Lex). We use the same LDS and Word2Vec models as in the previous section, and also compare to the Brown clusters used for NER in Ratinov & Roth (2009). As before, we find that Word2Vec and LDS provide significant accuracy improvements over the baseline. We expect that the reason the LDS does not outperform Word2Vec is that NER relies mainly on performing local pattern matching, rather than capturing long-range discourse structure."
    }, {
      "heading" : "7.4. RNN initialization",
      "text" : "As highlighted in Section 2, RNNs can provide impressive accuracy in various applications. We consider the simple RNN architecture of Sec. 6, since it permits natural initialization with an LDS and because Mikolov et al. (2014) demonstrate that small variants of it can outperform LSTMs as a language model (LM). Note that the ‘context units’ of Mikolov et al. (2014) could also be learned using our EM procedure, by restricting the parametrization of A. We leave exploration of hierarchical softmax observations (Mnih & Hinton, 2009), and other alternative architectures, for future work.\nWe evaluate the usefulness of the LDS for initializing the RNN under two criteria: (1) whether it improves the perplexity of the final model, and (2) whether it leads to faster optimization. A standard dataset for comparing language models is the Penn Treebank (PTB) (Sundermeyer et al., 2012; Pachitariu & Sahani, 2013; Mikolov et al., 2014). We first train a baseline, obtaining the same test set perplexity as Mikolov (2012), with 300 hidden dimensions. This initializes parameters randomly, with lengthscales tuned as in Mikolov (2012). Next, we use the LDS to initialize an\nRNN. In order to maintain a fair comparison v.s. the baseline, we train the LDS on the same PTB data, though in practice one should train it on a substantially larger corpus.\nWe use the popular RNN learning rate schedule where it is kept constant until performance on held-out data fails to improve, and then it is decreased geometrically until the held-out performance again fails to improve (Mikolov, 2012). We tuned both the initial value and decay rate. When initializing with the LDS, it is important to use a small learning rate, since otherwise the optimization immediately jumps far from where it started.\nIn Figure 1, we plot perplexity on the dev set v.s. the number of training epochs. The time to train the LDS, about 30 minutes, is inconsequential compared to training the RNN (4 days) on a single CPU core. LDS training on the PTB is faster than our experiments above with 1B tokens because we use a small vocabulary and run far fewer EM iterations, in order to prevent overfitting. The RNN baseline converged after 17 training epochs, while using the LDS for initialization allowed it to converge after 12, which amounts to about a day of savings on a single CPU core. Next, in Table 4 we compare the final perplexities on the dev and test sets. We find that initializing with the LDS also provides a better model. We encourage further exploration of LDS initialization for RNN LMs, partiularly using larger training sets for both steps.\nWe found that initializing the RNN with LDS parameters trained using SSID, rather than SSID+EM, performed no better than the baseline. Specifically, the best performance was obtained using a high initial learning rate, which allows gradient descent to ignore the SSID values. We expect this is because the method of moments requires lots of data, and the PTB is small. In a setting where one trains the LDS on a very large corpus, it is possible that SSID is effective. Overall, we did not explore initializing the RNN using type-level embeddings such as Word2Vec, since it is unclear how to initialize A and how to set K v.s. C."
    }, {
      "heading" : "8. Conclusion and Future Work",
      "text" : "We have contributed a scalable method for assigning word tokens context-specific low-dimensional representations. These representations, and the transition dynamics of the LDS parameters, capture useful syntactic and semantic structure. Our algorithm requires a single pass over the training data and no painful tuning of learning rates.\nNext, we will apply our LDS to challenging data such as Twitter and look to extend ASOS to new model structures. Furthermore, we will investigate improving initialization for alternative RNN architectures, including those with hierarchical softmaxes, by leveraging not just the LDS parameters, but also the LDS posterior on the training data."
    }, {
      "heading" : "A. Scaling UP LDS Learning to Text",
      "text" : "As discussed in Section 4.3, we whiten our data using\nW = Ψ − 12 0 = diag(µ − 12 1 , . . . , µ − 12 V ). (20)\nBesides improving the empirical performance of SSID, working in the whitened coordinate system also simplifies various details used in Section 4 when scaling up LDS learning for text. Under this transformation, we have Ψ0 = diag(µ)−µµ>. This simplifies various steps because our estimators (12) and (9) are of the form I − [low rank matrix], rather than diag(µ)− [low rank matrix]. In the whitened coordinates, the data are orthogonal to µ 12 , rather than 1.\nA.1. Recovering PSD D in SSID\nWhile SSID is consistent, for finite data the procedure is not guaranteed to yield a positive semidefinite (PSD) estimate for D, which is required because it is a covariance matrix. In our particular case, the D we seek will be singular on the span of µ 1 2 , but Subspace ID will still not guarantee that D will be PSD on µ 1 2 ⊥ .\nThis is critical because if D is not PSD on this subspace, then we can not define a valid Kalman filtering procedure for the model (see Sec. A.2). However, due to the structure of our data distribution, D can easily be fixed post-hoc.\nFrom (12) we have the estimator\nD = I − µ 12µ 12 > − CΣ1C> (21)\nNext, define Dα = I−µ 1 2µ 1 2 > − (1−α)CΣ1C> and define the PSD estimator D′ = Dα0 , where α0 is the minimal value such that Dα is PSD on µ 1 2 ⊥ . We next show how to find α0.\nWe have that Dα is PSD on µ 1 2 ⊥ iff the maximum eigenvalue of (1−α)WCΣ1C>W> is less than 1. This is because µ 1 2 is a unit vector and we can ignore any cross terms between µ 1 2µ 1 2 > and (1− α)CΣ1C> because col(C) = µ 1 2 ⊥\n, which is true because the data lies in this subspace. Therefore we can find α0 using the following procedure:\n1. Find s0, the maximal eigenvalue of CΣ1C>, using power iteration.\n2. If s0 < 1, set α0 = 0. Otherwise, set α0 = s0−1s0 .\nA.2. Efficiently Computing the Kalman Gain Matrix\nNext, recall our expression (6) for the steady state Kalman gain K = Σ1C>S−1ss , which comes from solving the system\nKSss = Σ1C >, (22)\nwhere\nSss = CΣ1C > +D (23)\nFurthermore, note that both of our estimators for D, (12) and (9), maintain the property that µ 1 2 is an eigenvector of eigenvalue 0 for D.\nSince µ 1 2 is also orthogonal to col(C), we have that µ 1 2 /∈ Col(Sss). Therefore, we cannot use (6) directly because Sss is not invertible along this direction. However, we can still solve (22) as K = Σ1C>S+ss. This pseudoinverse can be characterized as:\nS+ss = [inversion of Sss within col(Sss)] [projection onto col(Sss)] (24)\nFurthermore, note that both estimators for D have the form that\nD = Ψ0 − (PSD, low rank, and ⊥ µ 1 2 ) (25)\n= I − µ 12µ 12 > − (PSD, low rank and ⊥ µ 12 ) (26)\n:= I − µ 12µ 12 > − L (27)\nTherefore, it remains to define the pseudoinverse of\nSss = I − µ 1 2µ\n1 2 >\n+ C(Σ1 −M)C>). (28)\nFurthermore, since col(L) = col(C) = µ 1 2 ⊥ , we can define L = CMC> for some positive definite M , so we consider\nSss = I − µ 1 2µ\n1 2 >\n+ C(Σ1 −M)C>). (29)\nObserve that\n(I + C(Σ1 −M)C>)−1 (30)\nis a valid inverse for Sss on µ 1 2 ⊥ . This follows from the orthogonality of µ 1 2 and col(C), so we can effectively ignore the µ 1 2 term in (29) when inverting it on µ 1 2 ⊥ .\nTherefore, we employ\n(Sss) + = (I + C(Σ1 −M)C>)−1(I − µ 1 2µ 1 2 ), (31)\nwhere the right term is an orthogonal projection onto µ 1 2 ⊥ .\nThe term in the inverse (31) is diagonal-plus-low-rank and can be manipulated efficiently using the matrix inversion lemma formula (53):\n(I + C(Σ1 −M)C>)−1 = I − C((Σ1 −M)−1 + C ′C)−1C>. (32)\nTherefore we can obtain K without instantiating an intermediate matrix of size V × V .\nRecall the filtering equation (4):\nx̂tt = (A−KCA)x̂t−1t−1 +Kwt.\nWe seek to avoid anyO(V ) (or worse) computation at test time when filtering. First of all, we can precompute (A−KCA). For the second term, there are only V possible values for the unwhitened inputwt = w̃t−µ, so we would like to precompute KW (w̃t − µ) for every possible value that the indicator w̃t can take on. Let w̃t = ei, we have:\nKW (w̃t − µ) = Σ1C>S+ssW (ei − µ) (33)\n= Σ1C >(I + C(Σ1 −M)C>)−1(I − µ 1 2µ\n1 2 >\n)(Wei − µ 1 2 ) (34)\n= Σ1C >(I + C(Σ1 −M)C>)−1(I − µ 1 2µ\n1 2 >\n)Wei (35)\n= Σ1C >(I + C(Σ1 −M)C>)−1Wei (36) = [ Σ1C >(I + C(Σ1 −M)C>)−1W ] i , (37)\n(38)\nIn the final line, the subscript i denotes the ith column of a matrix.\nA.3. Likelihood Computation\nSss is also used when computing the log-likelihood of input data (w1, . . . , wT ):\nLL = −TV log(2π)− 1 2 log det(Sss) + >∑ t=1 (wpredt − wt)>S−1ss (w pred t − wt). (39)\nHere, wpredt = CAx̂t, where x̂t is the posterior mean for xt given observations w1:(t−1). Sss is only invertible along µ 1 2 ⊥\n, but (wpredt − wt) varies only on this subspace, so we can effectively ignore the zero-variance direction µ 1 2 . Therefore, we just use (30) as S−1ss in (39).\nFor the data-dependent term in our likelihood, we have:\n− 1 2 >∑ t=1 (wpredt − wt)>S−1ss (w pred t − wt) (40)\n= −1 2 tr ( S−1ss Et[(w pred t − wt)(w pred t − wt)>] ) (41)\n= −1 2 tr ( S−1ss Et[(wt − CAx̂t)(wt − CAx̂t)>] ) (42) = −1 2 ( tr ( S−1ss Et[wtw>t ] ) − 2tr ( S−1ss Et[wtx̂>t ]A>C> ) + tr ( S−1ss CAEt[x̂tx̂>t ]A>C> )) (43) = −1 2 ( tr ( S−1ss I ) − 2tr ( S−1ss Et[wtx̂>t ]A>C> ) + tr ( S−1ss CAEt[x̂tx̂>t ]A>C> )) (44)\nNote that the Et[x̂tx̂>t ] term above is different from Σ1, since the former is from the posterior distribution given the input data and Σ1 is from the prior.\nThe first term can be computed using (57). The latter two terms are of the form tr ( S−1ss ZW >), where Z and W are both V × k, so we can invoke (58). For the log det(Sss) term, we consider Sss only on µ 1 2 ⊥\n, so we compute − log det(S−1ss ), where S−1ss comes from (30) and we employ the formula (55)."
    }, {
      "heading" : "B. Background",
      "text" : "B.1. Non-Steady-State Kalman Filtering and Smoothing\nWe will use x̂τt and S τ t for the mean and variance under the posterior for xt given w1:τ . We will use x̄t and S T t when considering the posterior for xt given all the data w1:T . The following are the forward ‘filtering’ steps (Kalman, 1960; Ghahramani & Hinton, 1996):\nx̂t−1t = Ax̂ t−1 t−1 (45) St−1t = AS t−1 t−1A > +Q (46)\nKt = S t−1 t C ′(CSt−1t−1C > +D)−1 (47)\nx̂tt = x̂ t t−1 +Kt(wt − Cx̂t−1t ) (48) St−1t = S t−1 t −KtCSt−1t (49)\nNext, we have the backwards ‘smoothing’ steps:\nJt−1 = S t−1 t−1A ′(St−1t ) −1 (50) x̄t−1 = x̂ t−1 t−1 + Jt−1(x̄ T t −Ax̂t−1t−1) (51) STt−1 = S t−1 t−1 + Jt−1(S > t − St−1t )JTt−1 (52)\nNote that the updates for the variances S are data-independent and just depend on the parameters of the model. They will converge quickly to time-independent ‘steady state’ quantities.\nB.2. Matrix Inversion Lemma\nFollowing Press et al. (1987), we have\n(A+ USV >)−1 = A−1 −A−1U(S−1 + V >A−1U)−1V >A−1 (53)\nand the related expression for determinants:\ndet(A+ USV >) = det(S) det(A) det(S−1 + V >A−1U). (54)\ni.e. log det(A+ USV >) = log det(S) + log det(A) + log det(S−1 + V >A−1U). (55)\nExpression (53) is useful if we already have a an inverse for A and want to efficiently compute the inverse of a lowrank perturbation of A. It is also useful in order to be able to do linear algebra using (A + USV >)−1 without actually instantiating a V × V matrix, which can be unmanageable in terms of both time and space for large V . For example, let M be an V × m matrix with m << V , then we can compute M(A + USV >)−1 using (53) by carefully placing our parentheses such that no V × V matrix is required. In our application, A is diagonal, so computing its inverse is trivial. Also, note that (53) can be used recursively, if A is defined as another sum of an easily invertible matrix and a low rank matrix.\nAlong these lines, here are a few additional useful identities that follow from (53) for quantities that can be computed without V 2 time or storage. Here, we assume that both A−1 and tr(A−1) can be computed inexpensively (e.g., A is diagonal).\nFor any product XY >, where X and Y are V × k matrices, note that we can compute tr(XY T ) in O(V k) time as\ntr(XY T ) = ∑ i ∑ j XijYij . (56)\nWe can use this to compute the trace of the inverse of a matrix implicitly defined via the matrix inversion lemma:\ntr [ (A+ USV >)−1 ] = tr(A−1)− tr A−1U(S−1 + V >A−1U)−1︸ ︷︷ ︸ X V >A−1︸ ︷︷ ︸ Y >  . (57) More generally, Let Z and W be V × k matrices, then we compute\ntr [ (A+ USV >)−1ZW> ] = tr(A−1Z︸ ︷︷ ︸\nX W>︸︷︷︸ Y > )− tr A−1U(S−1 + V >A−1U)−1︸ ︷︷ ︸ X V >A−1ZW>︸ ︷︷ ︸ Y >  (58) We use (58) when computing the Likelihood in Section A.3."
    }, {
      "heading" : "C. SSID Initialization v.s. Random Initialization",
      "text" : "In Figure 2, we contrast the progress of EM, in terms of the log-likelihood of the training data, when initializing with SSID v.s. initializing randomly (Random). Note that the initial values of SSID and Random are nearly identical. This is due to model mispecification, and the fact that we chose the lengthscales of the random parameters post-hoc, by looking at the lengthscales of the SSID parameters. Over the course of 100 EM iterations, the model initialized with SSID climbs quickly and begins leveling out, whereas it takes a long time for the Random model to begin climbing at all. We truncate at 100 EM iterations, since we actually use the SSID-initialized model after the 50th iteration. After that, we find that local POS tagging accuracy diminished."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Low dimensional representations of words allow<lb>accurate NLP models to be trained on limited<lb>annotated data. While most representations ig-<lb>nore words’ local context, a natural way to in-<lb>duce context-dependent representations is to per-<lb>form inference in a probabilistic latent-variable<lb>sequence model. Given the recent success of<lb>continuous vector space word representations,<lb>we provide such an inference procedure for con-<lb>tinuous states, where words’ representations are<lb>given by the posterior mean of a linear dynam-<lb>ical system. Here, efficient inference can be<lb>performed using Kalman filtering. Our learn-<lb>ing algorithm is extremely scalable, operating<lb>on simple cooccurrence counts for both param-<lb>eter initialization using the method of moments<lb>and subsequent iterations of EM. In our exper-<lb>iments, we employ our inferred word embed-<lb>dings as features in standard tagging tasks, ob-<lb>taining significant accuracy improvements. Fi-<lb>nally, the Kalman filter updates can be seen as a<lb>linear recurrent neural network. We demonstrate<lb>that using the parameters of our model to ini-<lb>tialize a non-linear recurrent neural network lan-<lb>guage model reduces its training time by a day<lb>and yields lower perplexity.",
    "creator" : "LaTeX with hyperref package"
  }
}