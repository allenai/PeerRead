{
  "name" : "1609.09365.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Deep Tracking on the Move: Learning to Track the World from a Moving Vehicle using Recurrent Neural Networks",
    "authors" : [ "Julie Dequaire", "Dushyant Rao", "Peter Ondrúška", "Dominic Zeng Wang", "Ingmar Posner" ],
    "emails" : [ "ingmar@robots.ox.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "I. INTRODUCTION\nThe safe and effective operation of an autonomous vehicle depends on its ability to interpret its surroundings and track and predict the state of the environment over time. Many tracking systems employ multiple hand-engineered stages (e.g. object detection, semantic classification, data association, state estimation and motion modelling, occupancy grid generation) in order to represent the state and evolution of the world ([3], [4], [5]). However, as the tasks assigned to robots become more complex, this approach becomes increasingly infeasible.\nRecent advances in machine learning, particularly those of deep neural networks, have demonstrated the ability to capture complex structure in the real world, and have led to significant improvements in numerous computer vision and natural language processing applications ([6], [7], [8]). Such approaches would however typically require large, taskspecific corpora of annotated ground-truth labels to master the desired task. This becomes difficult when learning a model of the environment without access to corresponding ground truth, as is often the case for object tracking in crowded urban environments.\nIn recent work, [2] took an alternative approach and presented an end-to-end fully and efficiently trainable framework for learning a model of the world dynamics, building on the original DeepTracking work by [1]. We considered the specific problem of learning to track and classify moving objects in a complex and only partially-observable realworld scenario, as viewed from a static sensor. Here, we\nAuthors are from the Mobile Robotics Group at the University of Oxford, United Kingdom: julie, dushyant, ingmar@robots.ox.ac.uk\nadvance this work and address the problem of tracking from a moving platform. We extend the neural network architecture proposed in [2] to account for the egomotion of the sensor frame as it moves in the world frame, and demonstrate improved tracking accuracy as compared to previous work.\nWe demonstrate the system on laser point cloud data collected in a busy urban environment, with an array of static and dynamic objects, including cars, buses, pedestrians and cyclists. The model not only bypasses the sequence of handengineered steps typical of traditional tracking approaches, but is empirically shown to successfully predict the future evolution of objects in the environment, even when they are completely occluded.\nThe rest of the paper is structured as follows. Section II\nar X\niv :1\n60 9.\n09 36\n5v 1\n[ cs\n.C V\n] 2\n9 Se\np 20\n16\nhighlights related work and Section III summarises the problem definition and DeepTracking framework first presented in ([1], [2]). Section IV describes the models used to perform tracking in real-world scenarios considering both static and dynamic sensors. Section V presents an empirical evaluation of our methods, and Section VI concludes the paper and discusses the future implications of our findings."
    }, {
      "heading" : "II. RELATED WORK",
      "text" : "A number of previous works have explored the problem of model-free tracking of objects in the environment of an autonomous vehicle ([3], [4], [5]). Typically, these approaches follow the traditional paradigm of a multi-component pipeline, with separate components to parametrise and detect objects, associate new measurements to existing tracks, and estimate the state of each individually tracked object. The use of multiple stages in the framework is cumbersome and introduces extra unnecessary failure modes for the tracking algorithm.\nRecent work proposes to replace these multiple stages with an end-to-end learning framework known as DeepTracking, by leveraging neural networks to directly learn the mapping from raw laser input to an unoccluded occupancy grid ([1], [2]), even with relatively small amounts of data. The approach utilises an RNN architecture using gated recurrent units [9] to capture the state and evolution of the world in a sequence of laser scan frames. Another work [10] considers recurrent flow networks and takes a different angle to predicting occupancy in dynamic environments. They explicitly encode a range of velocities in the hidden layers of a recurrent network, and use Bayesian optimization to learn the network parameters which update velocity estimation and occupancy prediction. However, the model does not explicitly track objects through occlusion.\nDeepTracking shares similarities with deep learning approaches to predictive video modelling ([11], [12]), in that it is trained to predict the future state of the world based on current input data. This is particularly important, because in order to successfully capture the future location of dynamic objects in the scene, the model must implicitly store the position and velocity of each object in its internal memory state.\nWhile this eliminates the need to design individual components by hand, the model assumes a static vehicle [2]. Extending the problem to a moving platform is a challenging task, as it introduces an array of complex relative motions between the vehicle and objects in its environment. As DeepTracking ignores the motion of the vehicle, the model is forced to learn all possible motion interactions between the vehicle and its environment as if the vehicle were stationary. For a moving platform, we leverage estimates of egomotion as a proxy for vehicle motion. We scale up the RNN-based models proposed by ([1], [2]) for real-world application on dynamic vehicles, and exploit Spatial Transformer modules [13], which allow the internal memory state representations to be spatially transformed according to the estimated egomotion.\nThe main contributions of this work are as follows: 1) A more in-depth analysis of the performance of Deep-\nTracking in the case of a static vehicle, building on the experiments presented in [2]. 2) The use of Spatial Transformer modules to exploit estimates of visual egomotion in the DeepTracking framework. 3) Demonstration of end-to-end tracking of a variety of object classes through occlusion, on a moving vehicle in crowded urban scenes."
    }, {
      "heading" : "III. TRACKING PROBLEM FORMULATION",
      "text" : "The problem we address in this paper is to uncover the true state of the world, in terms of a 2D occupancy grid yt, given a sequence of partially observed states of the environment x1:t computed from raw sensor measurements. In particular, we solve for P (yt|x1:t), the probability of the true unoccluded state of the world at time t given a sequence of partial observations at all previous time steps. This formulation can also be used to predict future states by solving for P (yt+n|x1:t), given empty input for xt+1:t+n.\nP (yt|x1:t) = P (yt|ht) (1)\nEvolution of this latent state ht, which includes propagating model dynamics and integrating new sensor measurements, is modelled by the update equation:\nht = f(ht−1, xt) (2)\nThe key element here is that both the latent state update f(ht−1, xt), and the decoding step to produce the output P (yt|ht) are modelled as parts of a single neural network and are trained jointly. Equations 1 and 2 can then be performed repeatedly as the building block of a recurrent neural network that continuously updates the belief state ht, and uses it as network memory to predict yt. This makes it suitable for online stream filtering of sensor input.\nWhen the output ground-truth yt is not readily available, as is the case in real-world scenarios, the network can be trained in an self-supervised fashion. This is made possible by considering that predicting the movement of objects in occlusion at time t is similar to predicting a future state yt+n provided no input is given to the network, i.e xt+n = . Lack of input observation equates to complete occlusion of the scene. As only observable ground truth is available, we reduce the problem of predicting yt+n to that of predicting the directly observable part of yt+n, denoted y′t+n, which equates to the observed input xt+n. Training the network to instead predict P (y′t+n|x1:t) is equivalent to computing and backpropagating the errors only on the observable parts of the scene. We refer the reader to [1] and [2] for further details on the RNN and the training procedure.\nEach input observation xt ∈ {0, 1}2×M×M is represented as a pair of discretised 2D binary grids of size M × M , parallel to the ground and locally built around the sensor. The first matrix encodes whether a cell is directly observable by the sensor at time t, while the second matrix encodes whether\na cell is observed to be free (value of 0) or occupied (value of 1). We refer to these two matrices as xt,vis and xt,occ, the visibility and occupancy grids respectively. The output we wish to obtain is an occlusion-free state of the world yt ∈ {0, 1}M×M , and is represented by an occupancy matrix similar to that of the input occupancy grid of xt.\nIn the next section, we build upon [2] to deploy the DeepTracking paradigm on a real-world moving platform."
    }, {
      "heading" : "IV. TECHNICAL OVERVIEW",
      "text" : ""
    }, {
      "heading" : "A. Deep Tracking from a Static Sensor",
      "text" : "First, we revisit [2] to detail the baseline DeepTracking architecture for real world application, which we extend to a moving platform.\nAt each time step t, the partially observed grid xt used as input to the network is computed from raw 2D laser scans by ray tracing. Cells in which a measurement ends are marked as occupied, all cells from the sensor origin up to the end of the ray are marked as free, and cells beyond the ray are marked to be unobserved.\nThe input xt is processed by a multi-layer network illustrated in Figure 2, with the Spatial Transformer module only utilised in the moving vehicle scenario. The architecture originally proposed in [1] is scaled up with dilated convolutions [14] and a variant of gated recurrent units ([9], [15]) to allow for the simultaneous tracking of different-sized objects such as cars and pedestrians. Each layer l at time t is updated considering its own activations at time t−1 and those of layer l−1 below at time t, thus implementing the recurrence. This allows the network to extract and remember the information from the past and to use it for prediction of occluded objects or future states of the world. An additional static memory can also be utilised, in which the network is able to learn individual pixel-wise biases to add to the output of every convolutional operation. The output of the final layer is then converted into the resulting output of cell occupancy yt via a simple convolutional decoder."
    }, {
      "heading" : "B. Deep Tracking from a Moving Vehicle",
      "text" : "Tracking a dynamic scene from a moving platform poses the challenge of decoupling the motion of the vehicle from\nthe motion of objects in the global environment. In the static scenario, information related to an object located at index {i, j} in the input xt would typically be stored at the corresponding neighbouring spatial location {i+∆i, j+∆j} in the layers of the latent representation, with the neighbourhood ∆i,j based on the receptive field of each neuron in the hidden state. The latent state update would then pass the information along spatially in accordance with the observed relative motion of the input object (Figure 4). In the static scenario, the dynamics of the scene as viewed from the world frame are coherent with that viewed from the local sensor frame.\nWhen tracking from a moving vehicle however, the spatial update of information within the latent representation would additionally need to account for the sensor’s egomotion as it will affect the position of the object relative to the sensor frame. Although this is a major drawback of the baseline DeepTracking architecture, it can be compensated by transforming the memory state in accordance to the egomotion. In other words, a static obstacle situated at position {it−1, jt−1} in the sensor frame at t−1, will be moved to position {it, jt} in the sensor frame at time t such that:\n[xt, yt, 1] T = Tt,t−1 × [xt−1, yt−1, 1]T (3)\nwhere Tt,t−1 is the SE(2) forward transformation of the sensor source frame at t−1 into the sensor destination frame at t. This formulation naturally extends to 3D motion with SE(3).\nWe aid the network in decoupling egomotion and object motion in this way by introducing a Spatial Transformer [13] module (STM) in the hidden state (Figure 2). In the original work by [13], the STM is introduced as a learnable module for actively spatially transforming feature maps to aid tasks such as the classification of distorted datasets. However, in the context of tracking from a moving platform where egomotion is readily available (e.g. visual odometry), the STM can be used just to forward transform the hidden feature maps centred in the sensor source frame at time t− 1, into the sensor destination frame at time t, using transformation Tt,t−1 (Equation 3).\nThus, the STM is introduced in the hidden state and performs a transformation on all feature maps of ht−1, set in frame t − 1, into frame t before update with the new incoming input xt to form the new memory at t."
    }, {
      "heading" : "C. Training",
      "text" : "In both static and dynamic cases, the network is presented the input sequence x1:t and trained to predict the next n input frames xt+1:t+n. The binary cross-entropy loss is calculated and backpropagated only on the visible part of the output, which is achieved by simply masking the prediction yt with xt,vis and multiplying the resulting grid element-wise with the occupancy part grid xt,occ. By using a loss that encourages the model to predict the state of the environment in the future, the model is forced to capture the motion of each object in the hidden representation.\nIn the static sensor scenario, the error is only backpropagated on the ground truth available, i.e the visible part of the space. In the case of a moving sensor, however, an additional constraint needs to be imposed to account for the fact that as the robot moves in future frames, it will discover new space that falls outside the field of view of the current frame. Given this fact, the model should not be falsely penalised for failing to accurately guess objects located within this new space when the input is blanked out. This is similar in nature to the static case, where the input grid also represents a frontier between what the robot can perceive and understand of the scene and the unknown world outside its field of view.\nTo address this, we apply an additional mask at training time on the cost computation and error backpropagation to represent the predictable space in future frames. Accounting for this field of view drift is crucial in terms of tracking performance, as it corrects for an objective function that is otherwise skewed towards the incredibly difficult task of predicting objects outside the field of view. This mask has been overlaid in transparency over the ground truth comparison outputs of Figure 1, and indicates the predictable free space shrinking on future timesteps."
    }, {
      "heading" : "V. EXPERIMENTAL RESULTS",
      "text" : "In this section, we perform experimental validation of both the baseline DeepTracking and the STM-based variant proposed in this paper. With a stationary vehicle, no spatial transform is necessary and the two are identical."
    }, {
      "heading" : "A. Static Vehicle",
      "text" : "For the static case, we consider an architecture with three hidden layers of 16 GRU feature maps each. Computation of the hidden state consists of 3×3 convolutional filters, applied on the three layers (from bottom up) with strides of 1, 2, and 4 pixels, and receptive fields of 3 × 3, 7 × 7, and 11 × 11, respectively. We consider an input field of view of 20 × 20 m2 discretised into cells of 20 × 20 cm2, which translates into a H ×W = 101× 101 input grid. With a hidden state consisting of 48 feature maps, the additional static memory contributes to H×W×D = 489, 648 of the total 1, 506, 865 parameters of the network.\nThe evaluation dataset consists of a 75 minute log collected from a stationary Hokuyo UMT-30LX 2D laser scanner, positioned in the middle of a busy urban intersection. The area features dense traffic composed of buses, cars, cyclists and pedestrians, which results in extensive amounts of occlusion. Consequently, at no point in time is the complete scene fully observable. The dataset is subsampled at 8Hz and split into a 65 minute set for training, and a 10 minute set for testing occupancy prediction.\nThe full model was trained for 100 epochs over three days on an Nvidia Tesla K40 GPU, using the unsupervised training procedure described in Section III. The training set is split into mini-batch sequences of 40 frames (5 seconds). For every mini-batch, the network is shown 10 frames and trained to predict the next 10 frames, leading to two such sequences per 40-frame mini-batch. This length of sequence\ncovers the typical lengths of occlusions observed but can be tuned accordingly.\n1) Quantitative results: We first look to quantify the gain in performance achieved by scaling up the original DeepTracking network with the proposed architecture. We compare a number of different architectures ranging from the original [1] to the proposed model in [2], and compare performance on the task of predicting the observable nearfuture scene occupancy y′t+1:t+n, given the input sequence x1:t. The predicted output occupancy P (yt+n|x1:t) is compared to xt+n,vis = y′t+n, which corresponds to the visible (hence known) part of the world at time t. A threshold of 0.5 is used to determine whether a cell is predicted as occupied or free, and an F1 measure computed for each frame.\nFigure 3 compares the F1 measures computed on each of n = 10 blacked out future frames, given the 10 frames in the past. All model predictions decrease over time as would be expected, as the uncertainty of the state of the world increases with the prediction horizon. There is a notable step change in performance with neural architectures compared to a state-of-the art model-free tracking pipeline approach by [4]. If we increase the capacity of the original RNN architecture of [1] (RNN16) from 16 to 48 feature maps (RNN48), we do not find any significant performance increase. In comparison, replacing the standard RNN hidden state with the modified gated recurrent units (GRU3) detailed in [2] provides significant improvement in prediction ability. Finally, although incorporating static memory capacity (GRU3Bias) does yield some improvement over GRU3 alone, it is the dilated convolutions (GRU3DilConv) that, in concert with GRUs in the hidden state, yields the highest performance with significant margin. The performance of the full model (GRU3BiasDilConv), however, is commensurate to that of GRU3Dil, and the learned static bias values may convey useful information such as that of the static background layout.\n2) Qualitative results: To better understand what the network has learned, we also qualitatively analyse a typical test sequence of length 3 seconds, along with the network output and selected hidden state feature maps in Figure 4. The network is able to track pedestrians through full occlusion\nand the unobserved hallucinated tracks are represented in blue in the output sequence.\nIt can be seen that the GRU1 feature maps appear to have learned to capture the static background, as activations remain stationary during the sequence (i), and track pedestrians, as highlighted through the pink circles (ii). The GRU2 feature map also captures the motion of pedestrians moving upwards to the left, while, interestingly, the GRU3 unit seems to activate only on the car that appears to the top right at frame 2 (indicated by the orange box). This provides empirical support for the use of Dilated Convolutions, as they allow the model to learn patterns from a wider receptive field.\nIn general, we observe that information regarding objects in the scene is captured in the hidden state, and moves\nspatially through the feature maps according to the motion of the object in the input. This can be problematic when extending tracking to a moving platform, as the object motion and vehicle motion are coupled. We address this concern in the following section."
    }, {
      "heading" : "B. Moving vehicle",
      "text" : "In this section we compare the best performing model from the static case, which utilises GRU3 and Dilated Convolutions (Baseline DT), with an equivalent architecture that incorporates the Spatial Transformer module into the hidden belief state (STM). As with the static case, we evaluate the ability to predict future frames given blacked out input, and illustrate the achieved occlusion-free tracking performance on a series of examples selected from the test set.\nThe evaluation dataset was collected over a 35 minute period, from a moving vehicle equipped with two Velodyne HDL64E lasers, resulting in a 360 degree field of view. The 3D point clouds were reduced to a 2D scan by considering the range of points within 0.6-1.5 meters height from the ground.\nThe network was trained on mini-batches of 40 sequences with a frame rate of 10Hz, alternating between 5 inputs shown, and 5 inputs hidden. This higher frequency is better adapted to the moving vehicle case given the input field of view of 18 × 18 m2 and a vehicle mean velocity of 20 miles per hour. Longer occluded sequences would lead to an increase loss in useful memory due to the aforementioned drift of field of view. Finally, we performed an 80/20 split of the data into training and test set with no overlap in location, and trained our GRU3DilConv model for 100 epochs over 2 days on an Nvidia Tesla K40 GPU until convergence. Our architecture is implemented on Torch and uses the Spatial Transformer GPU-implementation of [16].\na) Quantitative Results: Figure 5 represents the F1 measure as computed with STM and with the baseline DT. The baseline DT is identical to STM with the exception that no egomotion is taken into account. In other words, no additional mask is applied to the cost computation and backpropagation, and hidden states are not forward transformed into the next sensor frame.\nWith no egomotion information, one might expect the F1 measure for the baseline to be very poor. Surprisingly however, it does very well as illustrated in Figure 5. We suggest two explanations for this. Firstly, we posit that this is due to the dataset being relatively benign in terms of vehicle motion patterns. As most of the driving occurs down straight roads and at relatively constant velocity (∼ 20 mph) the baseline may have learned a constant velocity model that could be used to correct the hidden state update. Our preliminary work on synthetic datasets reinforces this argument: training a model on a constant velocity vehicle achieves very good tracking performance, randomised sensor motion yields very poor performance. Secondly, the F1 measure here may be less informative, as the dataset is dominated by static objects such as walls and buildings. As such, a large fraction of the F1 score can be attained by learning the vehicle’s motion and merely capturing static scenes. Nonetheless, the STM offers a\nclear improvement over the baseline DT in all future frames. 1) Qualitative Results: To qualitatively evaluate the model, we show a selection of sequences where the model does well, and where it does more poorly. As the dataset does not contain as many occlusions and for as long as the static set due to the setting of a vehicle driving through an urban environment, we look at the ability of the network to predict what happens in occlusion by maintaining the blacking out of the input every 5 frames at test time.\nFigure 6 shows a compelling example of STM accurately tracking both dynamic and static objects through occlusion. In particular, the model accurately predicts the position of two static objects of different sizes when future frames are blacked out, and is also able to maintain the tracks when both objects are fully occluded.\nFigure 7 illustrates how STM accurately predicts the tracks of both a moving vehicle (red circle) and two occluded pedestrians (orange rectangle) whereas the baseline model fails. For the latter, the predicted track of the vehicle gradually shifts from the ground truth until complete failure in frame 5 (false negative area), and it fails to separate and track the pedestrians through occlusion by the vehicle."
    }, {
      "heading" : "VI. CONCLUSION",
      "text" : "In this paper, we have proposed an approach to perform object tracking for a mobile robot travelling in crowded urban environments, building on the previously proposed DeepTracking framework ([1], [2]). Crucially, unlike classical techniques which employ a multi-stage pipeline, this approach is learned end-to-end with limited architectural choices. By employing a Spatial Transformer module, the model is able to exploit noisy estimates of visual egomotion as a proxy for true vehicle motion. Experimental results demonstrate that our method performs favourably to DeepTracking in terms of accurately predicting future states, and show that the model can capture the location and motion of cars, pedestrians, cyclists, and buses, even when in complete occlusion.\nFuture work will look to estimate ego-motion, explore modalities such as radar, and extend the approach to 3D."
    }, {
      "heading" : "ACKNOWLEDGEMENT",
      "text" : "The authors would like to gratefully acknowledge support of this work by the UK Engineering and Physical Sciences Research Council (EPSRC) Doctoral Training Programme (DTP) and Programme Grant DFR-01420, as well as the Advanced Research Computing services at the University of Oxford for providing access to their computing cluster."
    } ],
    "references" : [ {
      "title" : "Deep tracking: Seeing beyond seeing using recurrent neural networks",
      "author" : [ "P. Ondrúška", "I. Posner" ],
      "venue" : "The Thirtieth AAAI Conference on Artificial Intelligence (AAAI), Phoenix, Arizona USA, February 2016.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "End-to-end tracking and semantic segmentationusing recurrent neural networks",
      "author" : [ "P. Ondrúška", "J. Dequaire", "D.Z. Wang", "I. Posner" ],
      "venue" : "arXiv preprint arXiv:1604.05091, 2016.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Model-free detection and tracking of dynamic objects with 2d lidar",
      "author" : [ "D.Z. Wang", "I. Posner", "P. Newman" ],
      "venue" : "The International Journal of Robotics Research, vol. 34, no. 7, pp. 1039–1063, 2015.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Model based vehicle detection and tracking for autonomous urban driving",
      "author" : [ "A. Petrovskaya", "S. Thrun" ],
      "venue" : "Autonomous Robots, vol. 26, no. 2, pp. 123–139, 2009.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "Advances in neural information processing systems, 2012, pp. 1097–1105.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Context-dependent pretrained deep neural networks for large-vocabulary speech recognition",
      "author" : [ "G.E. Dahl", "D. Yu", "L. Deng", "A. Acero" ],
      "venue" : "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 20, no. 1, pp. 30–42, 2012.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "End-to-end text recognition with convolutional neural networks",
      "author" : [ "T. Wang", "D.J. Wu", "A. Coates", "A.Y. Ng" ],
      "venue" : "Pattern Recognition (ICPR), 2012 21st International Conference on. IEEE, 2012, pp. 3304–3308.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "author" : [ "K. Cho", "B. Van Merriënboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio" ],
      "venue" : "arXiv preprint  arXiv:1406.1078, 2014.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Spatio-temporal video autoencoder with differentiable memory",
      "author" : [ "V. Patraucean", "A. Handa", "R. Cipolla" ],
      "venue" : "arXiv preprint arXiv:1511.06309, 2015.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Deep predictive coding networks for video prediction and unsupervised learning",
      "author" : [ "W. Lotter", "G. Kreiman", "D. Cox" ],
      "venue" : "arXiv preprint arXiv:1605.08104, 2016.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Spatial transformer networks",
      "author" : [ "M. Jaderberg", "K. Simonyan", "A. Zisserman" ],
      "venue" : "Advances in Neural Information Processing Systems, 2015, pp. 2017–2025.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Multi-scale context aggregation by dilated convolutions",
      "author" : [ "F. Yu", "V. Koltun" ],
      "venue" : "arXiv preprint arXiv:1511.07122, 2015.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Convolutional lstm network: A machine learning approach for precipitation nowcasting",
      "author" : [ "S. Xingjian", "Z. Chen", "H. Wang", "D.-Y. Yeung", "W.-k. Wong", "W.-c. WOO" ],
      "venue" : "Advances in Neural Information Processing Systems, 2015, pp. 802–810.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Module for spatial transformer networks",
      "author" : [ "M. Oquab" ],
      "venue" : "https://github. com/qassemoquab/stnbhwd/.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 0
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Inspired by the recently presented DeepTracking approach ([1], [2]), we employ a recurrent neural network (RNN) to capture the temporal evolution of the state of the environment, and propose to use Spatial Transformer modules to exploit estimates of the egomotion of the vehicle.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 1,
      "context" : "Inspired by the recently presented DeepTracking approach ([1], [2]), we employ a recurrent neural network (RNN) to capture the temporal evolution of the state of the environment, and propose to use Spatial Transformer modules to exploit estimates of the egomotion of the vehicle.",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 2,
      "context" : "object detection, semantic classification, data association, state estimation and motion modelling, occupancy grid generation) in order to represent the state and evolution of the world ([3], [4], [5]).",
      "startOffset" : 192,
      "endOffset" : 195
    }, {
      "referenceID" : 3,
      "context" : "object detection, semantic classification, data association, state estimation and motion modelling, occupancy grid generation) in order to represent the state and evolution of the world ([3], [4], [5]).",
      "startOffset" : 197,
      "endOffset" : 200
    }, {
      "referenceID" : 4,
      "context" : "Recent advances in machine learning, particularly those of deep neural networks, have demonstrated the ability to capture complex structure in the real world, and have led to significant improvements in numerous computer vision and natural language processing applications ([6], [7], [8]).",
      "startOffset" : 274,
      "endOffset" : 277
    }, {
      "referenceID" : 5,
      "context" : "Recent advances in machine learning, particularly those of deep neural networks, have demonstrated the ability to capture complex structure in the real world, and have led to significant improvements in numerous computer vision and natural language processing applications ([6], [7], [8]).",
      "startOffset" : 279,
      "endOffset" : 282
    }, {
      "referenceID" : 6,
      "context" : "Recent advances in machine learning, particularly those of deep neural networks, have demonstrated the ability to capture complex structure in the real world, and have led to significant improvements in numerous computer vision and natural language processing applications ([6], [7], [8]).",
      "startOffset" : 284,
      "endOffset" : 287
    }, {
      "referenceID" : 1,
      "context" : "In recent work, [2] took an alternative approach and presented an end-to-end fully and efficiently trainable framework for learning a model of the world dynamics, building on the original DeepTracking work by [1].",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 0,
      "context" : "In recent work, [2] took an alternative approach and presented an end-to-end fully and efficiently trainable framework for learning a model of the world dynamics, building on the original DeepTracking work by [1].",
      "startOffset" : 209,
      "endOffset" : 212
    }, {
      "referenceID" : 1,
      "context" : "We extend the neural network architecture proposed in [2] to account for the egomotion of the sensor frame as it moves in the world frame, and demonstrate improved tracking accuracy as compared to previous work.",
      "startOffset" : 54,
      "endOffset" : 57
    }, {
      "referenceID" : 0,
      "context" : "highlights related work and Section III summarises the problem definition and DeepTracking framework first presented in ([1], [2]).",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 1,
      "context" : "highlights related work and Section III summarises the problem definition and DeepTracking framework first presented in ([1], [2]).",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 2,
      "context" : "A number of previous works have explored the problem of model-free tracking of objects in the environment of an autonomous vehicle ([3], [4], [5]).",
      "startOffset" : 137,
      "endOffset" : 140
    }, {
      "referenceID" : 3,
      "context" : "A number of previous works have explored the problem of model-free tracking of objects in the environment of an autonomous vehicle ([3], [4], [5]).",
      "startOffset" : 142,
      "endOffset" : 145
    }, {
      "referenceID" : 0,
      "context" : "Recent work proposes to replace these multiple stages with an end-to-end learning framework known as DeepTracking, by leveraging neural networks to directly learn the mapping from raw laser input to an unoccluded occupancy grid ([1], [2]), even with relatively small amounts of data.",
      "startOffset" : 229,
      "endOffset" : 232
    }, {
      "referenceID" : 1,
      "context" : "Recent work proposes to replace these multiple stages with an end-to-end learning framework known as DeepTracking, by leveraging neural networks to directly learn the mapping from raw laser input to an unoccluded occupancy grid ([1], [2]), even with relatively small amounts of data.",
      "startOffset" : 234,
      "endOffset" : 237
    }, {
      "referenceID" : 7,
      "context" : "The approach utilises an RNN architecture using gated recurrent units [9] to capture the state and evolution of the world in a sequence of laser scan frames.",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 8,
      "context" : "DeepTracking shares similarities with deep learning approaches to predictive video modelling ([11], [12]), in that it is trained to predict the future state of the world based on current input data.",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 9,
      "context" : "DeepTracking shares similarities with deep learning approaches to predictive video modelling ([11], [12]), in that it is trained to predict the future state of the world based on current input data.",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 1,
      "context" : "While this eliminates the need to design individual components by hand, the model assumes a static vehicle [2].",
      "startOffset" : 107,
      "endOffset" : 110
    }, {
      "referenceID" : 0,
      "context" : "We scale up the RNN-based models proposed by ([1], [2]) for real-world application on dynamic vehicles, and exploit Spatial Transformer modules [13], which allow the internal memory state representations to be spatially transformed according to the estimated egomotion.",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 1,
      "context" : "We scale up the RNN-based models proposed by ([1], [2]) for real-world application on dynamic vehicles, and exploit Spatial Transformer modules [13], which allow the internal memory state representations to be spatially transformed according to the estimated egomotion.",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 10,
      "context" : "We scale up the RNN-based models proposed by ([1], [2]) for real-world application on dynamic vehicles, and exploit Spatial Transformer modules [13], which allow the internal memory state representations to be spatially transformed according to the estimated egomotion.",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 1,
      "context" : "1) A more in-depth analysis of the performance of DeepTracking in the case of a static vehicle, building on the experiments presented in [2].",
      "startOffset" : 137,
      "endOffset" : 140
    }, {
      "referenceID" : 0,
      "context" : "We refer the reader to [1] and [2] for further details on the RNN and the training procedure.",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 1,
      "context" : "We refer the reader to [1] and [2] for further details on the RNN and the training procedure.",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 1,
      "context" : "In the next section, we build upon [2] to deploy the DeepTracking paradigm on a real-world moving platform.",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 1,
      "context" : "First, we revisit [2] to detail the baseline DeepTracking architecture for real world application, which we extend to a moving platform.",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 0,
      "context" : "The architecture originally proposed in [1] is scaled up with dilated convolutions [14] and a variant of gated recurrent units ([9], [15]) to allow for the simultaneous tracking of different-sized objects such as cars and pedestrians.",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 11,
      "context" : "The architecture originally proposed in [1] is scaled up with dilated convolutions [14] and a variant of gated recurrent units ([9], [15]) to allow for the simultaneous tracking of different-sized objects such as cars and pedestrians.",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 7,
      "context" : "The architecture originally proposed in [1] is scaled up with dilated convolutions [14] and a variant of gated recurrent units ([9], [15]) to allow for the simultaneous tracking of different-sized objects such as cars and pedestrians.",
      "startOffset" : 128,
      "endOffset" : 131
    }, {
      "referenceID" : 12,
      "context" : "The architecture originally proposed in [1] is scaled up with dilated convolutions [14] and a variant of gated recurrent units ([9], [15]) to allow for the simultaneous tracking of different-sized objects such as cars and pedestrians.",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 10,
      "context" : "We aid the network in decoupling egomotion and object motion in this way by introducing a Spatial Transformer [13] module (STM) in the hidden state (Figure 2).",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 10,
      "context" : "In the original work by [13], the STM is introduced as a learnable module for actively spatially transforming feature maps to aid tasks such as the classification of distorted datasets.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 2,
      "context" : "75 1 Model-free tracker [4]",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 1,
      "context" : "RNN16 [2]",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 0,
      "context" : "We compare a number of different architectures ranging from the original [1] to the proposed model in [2], and compare performance on the task of predicting the observable nearfuture scene occupancy y′ t+1:t+n, given the input sequence x1:t.",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 1,
      "context" : "We compare a number of different architectures ranging from the original [1] to the proposed model in [2], and compare performance on the task of predicting the observable nearfuture scene occupancy y′ t+1:t+n, given the input sequence x1:t.",
      "startOffset" : 102,
      "endOffset" : 105
    }, {
      "referenceID" : 2,
      "context" : "There is a notable step change in performance with neural architectures compared to a state-of-the art model-free tracking pipeline approach by [4].",
      "startOffset" : 144,
      "endOffset" : 147
    }, {
      "referenceID" : 0,
      "context" : "If we increase the capacity of the original RNN architecture of [1] (RNN16) from 16 to 48 feature maps (RNN48), we do not find any significant performance increase.",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 1,
      "context" : "In comparison, replacing the standard RNN hidden state with the modified gated recurrent units (GRU3) detailed in [2] provides significant improvement in prediction ability.",
      "startOffset" : 114,
      "endOffset" : 117
    }, {
      "referenceID" : 13,
      "context" : "Our architecture is implemented on Torch and uses the Spatial Transformer GPU-implementation of [16].",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 0,
      "context" : "In this paper, we have proposed an approach to perform object tracking for a mobile robot travelling in crowded urban environments, building on the previously proposed DeepTracking framework ([1], [2]).",
      "startOffset" : 192,
      "endOffset" : 195
    }, {
      "referenceID" : 1,
      "context" : "In this paper, we have proposed an approach to perform object tracking for a mobile robot travelling in crowded urban environments, building on the previously proposed DeepTracking framework ([1], [2]).",
      "startOffset" : 197,
      "endOffset" : 200
    } ],
    "year" : 2016,
    "abstractText" : "This paper presents an end-to-end approach for tracking static and dynamic objects for an autonomous vehicle driving through crowded urban environments. Unlike traditional approaches to tracking, this method is learned end-to-end, and is able to directly predict a full unoccluded occupancy grid map from raw laser input data. Inspired by the recently presented DeepTracking approach ([1], [2]), we employ a recurrent neural network (RNN) to capture the temporal evolution of the state of the environment, and propose to use Spatial Transformer modules to exploit estimates of the egomotion of the vehicle. Our results demonstrate the ability to track a range of objects, including cars, buses, pedestrians, and cyclists through occlusion, from both moving and stationary platforms, using a single learned model. Experimental results demonstrate that the model can also predict the future states of objects from current inputs, with greater accuracy than previous work.",
    "creator" : "LaTeX with hyperref package"
  }
}