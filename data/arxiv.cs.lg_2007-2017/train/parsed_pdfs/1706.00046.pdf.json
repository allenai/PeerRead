{
  "name" : "1706.00046.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning Time-Efficient Deep Architectures with Budgeted Super Networks",
    "authors" : [ "Tom Veniat" ],
    "emails" : [ "tom.veniat@lip6.fr,", "ludovic.denoyer@lip6.fr" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In the Deep Learning community, finding the best architecture of a neural network for a given task is a key problem that is mainly addressed by hand or using validation techniques. For instance, in computer vision, this has lead to particularly well-known models like GoogleNet [21] or ResNet [8]. More recently, there is a surge of interest in developing techniques able to automatically discover efficient neural network architectures. Different algorithms have been proposed including evolutionary methods [20, 16, 17] or reinforcement learning-based approaches [24]. But in all cases, this selection is usually made solely based on the final predictive performance of the model such as the accuracy.\nWhen facing real-world applications, this predictive performance is not the only measure that matters. Indeed, learning a very good predictive model with the help of a cluster of GPUs might lead to a neural network that can be incompatible with low-resources mobile devices: it would involve too many complex operations resulting in a very low computation speed at use. Another example concerns distributed models in which part of the computation is made in the cloud and the other part is made on the device. In that case, an efficient architecture would have to predict accurately while minimizing the amount of exchanged messages between the cloud and the device. One important research direction is thus to propose models that can learn to take into account the computation complexity in addition to the quality of the prediction.\nWe propose to tackle this issue as a problem of automatically learning a neural network architecture that is efficient in terms of both prediction and computation time at inference. This task has been recently targeted by different models [1, 15, 12], with different learning techniques. The main difference of our approach is that it can be used with any computation cost function like the time in milliseconds or the number of operations made by the network, and only requires the user to specify a maximum authorized cost. For that, we propose a budgeted learning approach that integrates\nSubmitted to 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nar X\niv :1\n70 6.\n00 04\n6v 1\n[ cs\n.L G\n] 3\n1 M\nay 2\nthis computation cost directly in the learning objective function. Our model called Budgeted Super Network (BSN) is based on the following principles: (i) the user provides a (big) Super Network (see Section 3) that defines a very large set of possible final network architectures as well as the maximum inference cost. (ii) Since finding the best time-efficient architecture in this set is an intractable combinatorial problem (Section 4.1), we relax this optimization problem and propose a stochastic model (called Stochastic Super Networks – Section 4.2) that can be optimized using reinforcement learning-inspired methods (Section 4.3). We show that the optimal solution of this new problem corresponds to the optimal network architecture (Proposition 1). At last, we evaluate our approach on both image classification and image segmentation problems on which we show that our model is able to reduce the computation time while keeping a very high accuracy (Section 5). The related work is presented in Section 2."
    }, {
      "heading" : "2 Related Work",
      "text" : "Different authors have proposed to provide networks with the ability to learn to select the computations that will be applied. This is the case for example for classification in [2, 24] based on Reinforcement learning techniques, in [19] based on gating mechanisms, in [17] based on evolutionary algorithms or even in [4] based on both RL and evolutionary techniques. One strong difference with our approach is that these models are only guided by the final predictive performance of the network. For example in [24], a controller neural net can propose a “child” model architecture, which can then be trained and evaluated before trying a new architecture. The process is repeated iteratively until a good architecture is obtained. Moreover, in this approach each generated architecture is learned to convergence on the training set, resulting in a very low training speed while our model learns the parameters of the modules and the architecture simultaneously.\nWhen the objective is to learn time or memory efficient models, different methods have been proposed. The most common approach is to simplify the learned network typically by removing some connections between neurons. The oldest approach is certainly the Optimal Brain Surgeon [7] which removes weights in a classical neural network architecture. The problem of network compression can also be seen as a way to speedup a particular architecture, for example by using quantization of the weights of the network [22]. Pruning and quantization techniques can also be combined [6] to obtain impressive performances.\nOther algorithms include the use of hardware efficient operations that allow a high speedup. For example [3] combines algorithmic optimizations and architecture improvements to speed up the response time of a network while keeping high accuracy. This technique is effective but requires a strong expertise to find the best optimization and is also highly specific to each architecture and/or\nhardware. Note that the computation speed can be reduced by using parallelized inference approaches which is not the topic of this paper.\nIn some other works, authors learn time-efficient models in an end-to-end manner. For example, the model proposed in [1] can be seen as an extension of dropout where the probability of sampling neurons is conditioned on currently processed input (conditional computation). In the same vein, the model proposed in [15] aims at learning routes in Super networks. It is certainly the most similar approach to ours, while keeping several important differences. First, the model is based on an explicit computation cost formulation while our approach can be used for any type of cost. Moreover, while our model converges to a static architecture, allowing to keep only the blocks used by the trained model, their model learns dynamic routes, causing an overhead in the computation cost and forcing to keep the whole network to do further inferences. At last, the number of possible architectures explored is smaller in this approach than in ours."
    }, {
      "heading" : "3 Super Networks",
      "text" : "We consider the classical supervised learning problem defined by an input space X and an output space Y . In the following, input and output spaces correspond to multi-dimensional real-valued spaces (vectors, matrices or larger tensors). The training set is denoted asD = {(x1, y1), ..., (x`, y`)} where xi ∈ X , yi ∈ Y and ` is the number of supervised examples. At last, we consider a model f : X → Y that predicts an output given a particular input. We first describe a family of models called Super Networks (S-network)1 since our contribution presented in Section 4 will be a (stochastic) extension of this model. Note that the principle of Super Networks is not new and the same ideas have been already proposed in the literature under different names, e.g Deep Sequential Neural Networks [2], Neural Fabrics [18], or even PathNet [4] – and for different tasks.\nA Super Network is composed of a set of layers connected together in a direct acyclic graph structure. Each edge is a (small) neural network, and the Super Network corresponds to a particular combination of these neural networks and defines a computation graph. Examples of S-networks are given in Figure 1. More formally, let us denote l1, ...., lN a set of layers, N being the number of layers, such that each layer li is associated with a particular representation space Xi which is a multi-dimensional real-valued space. l1 will be the input layer while lN will be the output layer. We also consider a set of (differentiable) functions fi,j associated to each possible pair of layers such that fi,j : Xi → Xj . Each function fi,j will be referred as a module in the following: it takes data from Xi as inputs and transforms these data to Xj . Note that each fi,j will make complex disk/memory/network operations and their computation will have consequences on the inference speed of the S-network. Each fi,j module is associated with parameters in θ, θ being implicit in the notation for sake of clarity.\nOn top of this structure, a particular architecture E = {ei,j}(i,j)∈[1;N ]2 is a binary adjacency matrix over the N layers such that E defines a directed acyclic graph (DAG) with a single source node is l1 and a single sink node lN . Different matrices E will thus correspond to different super network architectures – see Figure 1 that shows different possible architectures within the same super network. A S-network will be denoted (E, θ) in the following, θ being the parameters of the different modules, and E being the architecture of the super network.\nPredicting with S-networks: The computation of the output f(x,E, θ) given an input x and a super network (E, θ) is made through a classic forward algorithm, the main idea being that the output of modules fi,j and fk,j leading to the same layer lj will be added in order to compute the value of lj . Let us denote li(x,E, θ) the value of layer li for input x, the computation is recursively defined as:\nInput:l1(x,E, θ) = x,Layer Computation: li(x,E, θ) = ∑ k ek,ifk,i(lk(x,E, θ)) (1)\nNote that learning the parameters θ can be made using classical back-propagation and gradient-descent techniques.\n1The name Super Network comes from [4] which presents an architecture close to ours."
    }, {
      "heading" : "4 Learning Time-efficient architectures",
      "text" : "Our main idea is the following: we now consider that the structure E of the S-network (E, θ) describes not a single neural network architecture but a set of possible architectures. Indeed, each sub-graph of E (subset of edges) corresponds to a S-network. A sub-graph of edges will be denoted H E where H corresponds to a binary matrix in charge of selecting the edges in E, denoting the Hadamard product between H and E. Our objective will thus be to identify the best matrix H such that the corresponding S-network (H E, θ) will be a network efficient both in terms of predictive quality and in terms of computation speed.\nThe next sections are organized as follows: (i) First, we formalize this problem as a combinatorial problem where one wants to discover the best matrix H in the set of all possible binary matrices of size N ×N . Since this optimization problem is intractable, we propose a new family of models called Stochastic Super Networks where the edges of E are sampled following a parametrized distribution Γ before each prediction. We then show that the resulting budgeted learning problem is continuous and that its solution corresponds to the optimal solution of the initial budgeted problem (Proposition 1). We then propose a practical learning algorithm based on reinforcement learning techniques to learn Γ and θ simultaneously."
    }, {
      "heading" : "4.1 Budgeted Architectures Learning",
      "text" : "Let us consider H a binary matrix of size N ×N . Let us denote C(H E) ∈ R+ the computation cost2 associated to the computation of the S-network (H E, θ) which can be of different nature (see Section 5). Let us also define C the maximum cost the user would allow to the wanted model. For instance, when solving the problem of learning a model with a computation time lower than 200 ms then C is equal to 200ms. We aim at solving the following budgeted learning problem:\nH∗, θ∗ = arg min H,θ\n1\n` ∑ i ∆(f(xi, H E, θ), yi) under constraints: C(H E) ≤ C (2)\nIn this article, we propose to focus on a soft version of the problem written as:\nH∗, θ∗ = arg min H,θ\n1\n` ∑ i ∆(f(xi, H E, θ), yi) +λmax(0, C(H E)−C) (3)\nwhere λ corresponds to the importance of the cost penalty. Note that the proposed learning model directly includes the computational cost which will be specific to the particular infrastructure on which the model is used. For instance, if C is the cost in milliseconds, the value of C(H E) will not be the same depending on the device on which the model is used. While previous works make assumptions on the structure of the cost function [15], our approach does not rely on any constraint concerning C(H E) except the fact that this cost can be measured during training. In other words, solving this problem on a mobile device will produce a different structure that the one obtained when solving this problem on a cluster of GPUs which is an important property of our model.\nFinding a solution to this learning problem is not trivial since it involves the computation of all possible architectures which is prohibitive (O(2N ) in the worst case). We explain in the next section how this problem can be solved using Stochastic Super Networks."
    }, {
      "heading" : "4.2 Stochastic Super Networks",
      "text" : "Now, given a particular architectureE, we consider the following stochastic model – called Stochastic Super Network (SS-network) – that computes a prediction in two steps: (i) first a sub-graph H is sampled based on a distribution with parameters Γ. This operation will be denoted H ∼ Γ (ii) The final prediction is made using this sub-graph i.e by computing f(x,H E, θ). This inference algorithm is described in Algorithm 1. A SS-network is thus defined by a triplet (E,Γ, θ), Γ and θ being both learned on a training set.\nWe can rewrite the budgeted learning objective of Equation 3 as:\nΓ∗, θ∗ = arg min Γ,θ\n1\n` ∑ i EH∼Γ [ ∆(f(xi, H E, θ), yi) + λmax(0, C(H E)−C) ] (4)\n2Note that we consider that the cost only depends on the network architecture. The model could easily be extended to costs that depend on the input x to process\nAlgorithm 1 Stochastic Super Network forward algorithm 1: procedure SSN-FORWARD(x,E,Γ, θ) 2: H ∼ Γ . as explained in Section 4.2 3: For i ∈ [1..N ], li ←Ø EndFor . Init layers values 4: l1 ← x . Set the input in the first layer 5: For i ∈ [2..N ],li ←\n∑ k<i ek,ihk,ifk,i(lk) EndFor\n6: return lN 7: end procedure\nProposition 1. When The solution of Equation 4 is reached, then the models sampled following (Γ∗) and using parameters θ∗ are optimal solution of the problem of Equation 3.\nThe demonstration of the proposition is given in supplementary material.\nEdge Sampling: For each layer li visited following the DAG order of E (from the first layer to the last one) and for all k < i: If lk is connected to the input layer l1 based on the previously sampled edges, then hk,i is sampled following a Bernoulli distribution with probability3 γk,i. In the other cases then hk,i = 0. This sampling procedure avoids sampling edges that would not help in the output computation i.e edges not connected to the input layer."
    }, {
      "heading" : "4.3 Learning Algorithm",
      "text" : "We consider a generic case where the cost-function C(H E) is unknown and can only be observed at the end of the computation of the model over an input x e.g the computation time in seconds. Note that this case also includes stochastic costs where the cost is a random variable, caused by some network latency during distributed computation for example. We now describe the case where C is deterministic, its extension to stochastic values being simple (see supplementary material).\nLet us denote D(x, y, θ, E,H) the quality of the model (H E, θ) on a given training pair (x, y):\nD(x, y, θ, E,H) = ∆(f(x,H E, θ), y) + λmax(0, C(H E)−C) (5)\nWe propose to use a REINFORCE-inspired algorithm as in [2, 1] for learning. In that case, let us denote L(x, y,Γ, θ) the expectation of D over the possible sampled matrices H:\nL(x, y, E,Γ, θ) = EH∼ΓD(x, y, θ, E,H) (6)\nThe gradient of L can be written as4: ∇θ,ΓL(x, y, E,Γ, θ) = ∑ H P (H|Γ) [(∇θ,Γ logP (H|Γ))D(x, y, θ, E,H)]\n+ ∑ H P (H|Γ) [∇θ,Γ∆(f(x,H E, θ), y)] (7)\nThe first term corresponds to the gradient over the log-probability of the sampled structure H while the second term is the gradient of the prediction loss given the sampled structure H E. Learning can be made using back-propagation and stochastic-gradient descent algorithms as it is made in Deep Reinforcement Learning models. Note that in practice, in order to reduce the variance of the estimator, the update is made following:\n∇θ,ΓL(x, y, E,Γ, θ) ≈ (∇θ,Γ logP (H|Γ))(D(x, y, θ, E,H)− D̃) +∇θ,Γ∆(f(x,H E, θ), y) (8) where H is sampled following Γ, and where D̃ is the average value of D(x, y, θ, E,H) computed on a batch of learning examples.\n3Note that γk,i is obtained by applying a logistic function over a continuous parameter value, but this is made implicit in our notations.\n4details provided in supplementary material"
    }, {
      "heading" : "5 Experiments",
      "text" : "We propose to focus on two tasks: Image classification using MNIST and CIFAR-10 [11, 10] and Image Segmentation using the Part Labels dataset [9]. Standard train/validation/test splits are used. The base Super Network used in the experimental section is a Convolutional Neural Fabric (CNF) [18] – see Figure 1 (a). In this model (details in the supplementary material section), the layers are organized in a W ×H matrix where W is the width of the matrix, and H is the height. In our experiments, W = 8 and H = 6. Each level in H corresponds to a particular image resolution. Edges correspond to a simple NN composed of a BatchNorm module followed by a 2D-convolution with kernels of size 3, stride and padding depending on the position of the module. We use 64 filters per convolution for both MNIST and Part Labels and 128 filters for CIFAR-10. The input layer is the top-left layer taking images on dimension 32x32 for the MNIST and CIFAR-10 datasets and 256x256 for Part Labels. In classification the output layer is the bottom-right layer followed by a linear + log softmax layer transforming the data to a vector of size K, K = 10 being the number of categories. For the Image Segmentation task, the output layer is the top-right layer followed by a log softmax transformation, each pixel being associated with its (log) probability of belonging to one of the K = 3 possible pixel categories.\nWe perform experiments considering two different costs: (i) The computational cost (Flop) corresponding to the number of operations made by the convolution modules which is proportional to the number of pixels in the output map produced by the convolution. Intuitively, in the CNF architecture, top edges represents convolutions over high resolution images, being much more expensive than bottom edges which represents convolutions over lower resolution inputs – see Figure 1 (a). (ii) The Time cost (T-cost) corresponds to the time spent in milliseconds to compute the prediction. In comparison to the Flop-cost, this cost includes all the extra operations made by the model, and is evaluated in milliseconds after each prediction (measured on the CPU). For example, the Time cost includes rescaling operations (diagonal edges oriented to the top-right direction) that are expensive.\nWe test the Budgeted Super Network model (BSN) with different values of the maximum allowed cost C and we thus obtain different learned models evaluated in terms of both computation cost and accuracy. We compute the Pareto front of the cost/accuracy curves on the validation set, and report the corresponding results obtained on the test set (see supplementary material).\nWe compare our algorithm with the CNF model which is equivalent to BSN where the maximum cost C is unlimited – all edges are kept. Following the description of [18], we also compare our approach to pruned versions of CNF (denoted P-CNF). The pruned algorithm consists in (i) removing the edges of a learned CNF ordered increasingly by their average absolute value of the weights in the convolution kernels – the convolution kernel weights acting as a measure of the importance of each edge – and (ii) in fine-tuning the obtained pruned architecture on the dataset. This pruning procedure has many drawbacks: it is specific to convolution filters and cannot be applied to any S-network architecture while our model is generic to any S-network, it is time-consuming since a fine-tuning must be made after each pruning, and it may produce S-network architectures that are inconsistent since it might remove all the paths between the input layer and the output layer."
    }, {
      "heading" : "5.1 Image Classification",
      "text" : "MNIST: Table 1 presents the results obtained on MNIST with the Flop cost and the Time cost compared to (pruned) CNF. On the Flop cost, our model is able to gain 97.8 % of the maximum cost while having 0.48 % of error rate. In terms of time, BSN is able to find a model which computation time is 5.17 ms (in comparison to 68.12 ms for the full CNF) while keeping an error rate of 0.43 %. These two results show the ability of BNF to discover time-efficient architectures, and to handle different cost functions. The pruned version of CNF has a lower performance when the pruning becomes more important. This is mainly due to the fact that the pruning does not consider the overall cost, but also because this heuristic pruning procedure tends to produce architectures that are inconsistent, i.e architectures where there is no path between the input layer and the output layer due to a too strong pruning.\nCIFAR-10: Figure 2 (a) shows the performance on CIFAR for the Flop cost. One can see that, in comparison with Pruned CNF, our model obtains a better accuracy at any given cost level. The error only moves from 9.21 % to 13.66% while computing 98% less Flops. On the CIFAR-10 dataset,\nthe CNF pruning method tends to early remove unexpected edges, thus producing inconsistent architectures. To avoid this effect, we also compare the model with a ’Smart’ pruning algorithm of our invention where the edges are removed by their inverse order of importance (the average value of the convolution absolute weights) as in [18], but if and only if the remaining architecture stays consistent. In that case, the Smart P-CNF model obtains an error of 12.8 % while BSN has a 11.03% error rate at the same cost level (cost gain = 90%). Here again, our algorithm outperforms this baseline. Note that like P-CNF, Smart P-CNF is also fine-tuned at each cost level, resulting in a very slow training procedure, BSN learning an order of magnitude faster. The BNF best learned architecture is illustrated in Figure 2 (b1) (for Flop gain = 98%).The majority of the expensive edges (top edges) are ’naturally’ removed by our model since they correspond to a too low accuracy/cost trade-off.\nOther costs: To measure the ability of BNF to learn with complex costs, we train our model on architectures where one specific cost is associated to each edge, the overall cost being the sum of the costs of the edges used for prediction. Figure 3 (b) illustrates two examples of costs (left part), and the resulting learned architectures (Figure 3 (b) left) on MNIST. The first example (Figure 3-b1 ) is particularly interesting since it simulates an architecture where some edges are very expensive, it can for instance corresponds to a decentralized model that needs to exchange information on the Internet i.e expensive edges being communication edges. In that case, our model is able to find an architecture that minimizes the number of used expensive edges. In the random cost case (Figure 3-b2 ), our model is able to discover a complex structure also avoiding expensive edges."
    }, {
      "heading" : "5.2 Image Segmentation",
      "text" : "We also perform experiments on the image segmentation task. Note that, in that case, the model computes a map of pixel probabilities, the output layer being now located at the top-right position of the CNF matrix. It is thus more difficult to reduce the overall cost. On the Part Labels dataset, we are able to learn a BSN model with a Flop gain of 40%. In that task, forcing the model to increase the Flop gain by decreasing the value of C results in inconsistent models. At a Flop gain level of 40%, BSN obtains an error rate of 4.57%, which can be compared with the error of 4.94% for the full model. Due to its pruning procedure, the P-CNF model is unable to produce consistent architectures if the desired Flop gain is greater than 20%. Even at 20% its error rate is higher than ours (5.5%). The Smart P-CNF achieves higher Flop gains, but at the price of a clearly higher error rate. On this task, BNF also outperforms the baselines. The BNF best learned architecture is illustrated in Figure 2 (b2) and shows that BNF is able to learn an architecture adapted to the task."
    }, {
      "heading" : "6 Conclusion and Perspectives",
      "text" : "We proposed a new model called Budgeted Super Network able to automatically discover a timeefficient neural network architecture for a given problem by specifying a maximum authorized computation cost. The experiments in the computer vision domain show the effectiveness of our approach. Its main advantage is that BSN can be used for any computation costs (computation time, number of operations, extra-time occurred by some network latency,..). It can also be used with costs of another nature like memory consumption for example which has not been studied in this paper. This work opens many research directions. One direction is to evaluate the ability of this model to support inherent cost over a large distributed cluster, and thus to discover architectures that are efficient on large clusters. A second long-term direction is to study whether this model could be adapted in order to reduce the training time (instead of the test computation time). This could for example be done using meta-learning approaches."
    }, {
      "heading" : "Supplementary Material",
      "text" : ""
    }, {
      "heading" : "Model Selection Protocol",
      "text" : "As explained in the article, the selection of the model is obtained by learning many different models, by computing the Pareto front of the accuracy/cost curve on the validation set, and then by reporting the performance obtained on the test set. This is illustrated in the following figure where many different models are reported (blue circles) and the corresponding performance on test (red crosses). The line corresponds to the front obtained on the test models, this front being used to build the table of results presented in the article."
    }, {
      "heading" : "Demonstration of Proposition 1",
      "text" : "Let us consider the stochastic optimization problem defined in Equation 4. The schema of the proof is the following:\n• First, we lower bound the value of Equation 4 by the optimal value of Equation 3 • Then we show that this lower bound can be reached by some particular values of Γ and θ in\nEquation 4. Said otherwise, the solution of Equation 4 is equivalent to the solution of 3.\nLet us denote:\nB(H E, θ, λ) = 1 ` ∑ i ∆(f(xi, H E, θ), yi) + λmax(0, C(H E)− C) (9)\nGiven a value of Γ, let us denote supp(Γ) all the H matrices that can be sampled following Γ. The objective function of Equation 4 can be written as:\nEH∼Γ[B(H,E, θ, λ)] = ∑\nH∈supp(Γ)\nB(H E, θ, λ)P (H|Γ)\n≥ ∑\nH∈supp(Γ)\nB((H E)∗, θ∗, λ)P (H|Γ)\n= B((H E)∗, θ∗, λ)\n(10)\nwhere (H E)∗ and θ∗ correspond to the solution of:\n(H E)∗, θ∗ = arg min H,θ B(H E, θ, λ) (11)\nNow, it is easy to show that this lower bound can be reached by considering a value of Γ∗ such that ∀H ∈ supp(Γ), H E = (H E)∗. This corresponds to a value of Γ where all the probabilities associated to edges in E are equal to 0 or to 1.\nConsidering stochastic costs in the REINFORCE algorithm:\nAs explained previously, the proposed algorithm can also be used when the cost C(H E) is a stochastic function that depends of the environment e.g the network latency, (or even on the input data x). Our algorithm is still able to learn with such stochastic costs since the only change in the learning objective is that the expectation is now made on both H and C (and x if needed). This property is interesting since it allows to discover efficient architecture on stochastic operational infrastructure."
    }, {
      "heading" : "Additional Architecture Details",
      "text" : "The network we use in our experiments is based on the dense Convolutional Neural Fabrics, which can be seen as a multi-layer and multi-scale convolutional neural network. As shown in Figure 1, this architecture spans on 2 axis: The first axis represents the different layers L of the network while the second axis corresponds to different scales S of output feature maps, the first scale being the size of the input images, each subsequent scale being of a size reduced by a factor of 2 up to the last scale corresponding to a single scalar.\nEach layer (l, s) in this fabric takes its input from three different layer of the preceding column: One with a finer scale (l − 1, s − 1), one with the same scale (l − 1, s) and one with a coarser scale (l − 1, s+ 1). The first and last columns are the only ones which have vertical connections within scales of the same layer (as can be seen in Figure 1). This is made to allow the propagation of the information to all nodes in the first column and to aggregate the activations of the last column to compute the final prediction. A more detailed description of this architecture can be found in the CNF original article."
    }, {
      "heading" : "Additional Learning Details",
      "text" : ""
    }, {
      "heading" : "Datasets",
      "text" : "MNIST. The MNIST dataset is composed of 70k images of 28x28 pixels representing a single handwritten digit between 0 and 9. The training set contains 60k training samples and the testing set contains 10k samples. We use the standard split of 50k training samples and 10k validation samples from the training set. Each image is first padded with zeros to a 36x36 images before applying a random 32x32 crop to obtain the final resolution. Images are normalized in the range [-1,1].\nCIFAR10. The CIFAR10 dataset consists of 50k training images and 10k testing images with 10 classes. We split the training set following the standard, i.e 45k training samples and 5k validation samples. We use two data augmentation techniques : padding the image to 36x36 pixels before extracting a random crop of size 32x32 and horizontally flipping. Images are then normalized in the range [-1,1].\nPart Labels. The Part Labels dataset is a subset of the LFW dataset composed of 2927 face images in which each pixel is labeled as one of the Hair/Skin/Background classes. The standard split contains 1500 training samples, 500 validation samples and 927 test samples. Images are zero padded from 250x250 to 256x256 and horizontally flipped before being normalized in the range [-1,1]."
    } ],
    "references" : [ {
      "title" : "Conditional computation in neural networks for faster models",
      "author" : [ "Emmanuel Bengio", "Pierre-Luc Bacon", "Joelle Pineau", "Doina Precup" ],
      "venue" : "CoRR, abs/1511.06297,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2015
    }, {
      "title" : "Sharp Models on Dull Hardware: Fast and Accurate Neural Machine Translation",
      "author" : [ "Jacob Devlin" ],
      "venue" : "Decoding on the CPU,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2017
    }, {
      "title" : "Pathnet: Evolution channels gradient descent in super neural networks",
      "author" : [ "Chrisantha Fernando", "Dylan Banarse", "Charles Blundell", "Yori Zwols", "David Ha", "Andrei A. Rusu", "Alexander Pritzel", "Daan Wierstra" ],
      "venue" : "CoRR, abs/1701.08734,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2017
    }, {
      "title" : "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding",
      "author" : [ "Song Han", "Huizi Mao", "William J Dally" ],
      "venue" : "In International Conference on Learning Representations (ICLR’16 best paper award),",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2015
    }, {
      "title" : "Second order derivatives for network pruning: Optimal brain surgeon",
      "author" : [ "Babak Hassibi", "David G. Stork" ],
      "venue" : "In Advances in Neural Information Processing Systems 5, [NIPS Conference],",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1993
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "CoRR, abs/1512.03385,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2015
    }, {
      "title" : "Augmenting CRFs with Boltzmann machine shape priors for image labeling",
      "author" : [ "Andrew Kae", "Kihyuk Sohn", "Honglak Lee", "Erik Learned-Miller" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2013
    }, {
      "title" : "Learning multiple layers of features from tiny images",
      "author" : [ "Alex Krizhevsky" ],
      "venue" : "Technical report,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2009
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "In Intelligent Signal Processing,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2001
    }, {
      "title" : "Dynamic deep neural networks: Optimizing accuracy-efficiency trade-offs by selective",
      "author" : [ "Lanlan Liu", "Jia Deng" ],
      "venue" : "execution. CoRR,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2017
    }, {
      "title" : "Multi-objective convolutional learning for face labeling",
      "author" : [ "Sifei Liu", "Jimei Yang", "Chang Huang", "Ming-Hsuan Yang" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2015
    }, {
      "title" : "Deciding how to decide: Dynamic routing in artificial neural networks",
      "author" : [ "Mason McGill", "Pietro Perona" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2017
    }, {
      "title" : "Large-scale evolution of image",
      "author" : [ "Esteban Real", "Sherry Moore", "Andrew Selle", "Saurabh Saxena", "Yutaka Leon Suematsu", "Quoc V. Le", "Alex Kurakin" ],
      "venue" : "classifiers. CoRR,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2017
    }, {
      "title" : "Efficient reinforcement learning through evolving neural network topologies",
      "author" : [ "Kenneth O. Stanley", "Risto Miikkulainen" ],
      "venue" : "In GECCO 2002: Proceedings of the Genetic and Evolutionary Computation Conference,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2002
    }, {
      "title" : "Improving the speed of neural networks on cpus",
      "author" : [ "Vincent Vanhoucke", "Andrew Senior", "Mark Z. Mao" ],
      "venue" : "In Deep Learning and Unsupervised Feature Learning Workshop,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2011
    }, {
      "title" : "Regularization of neural networks using dropconnect",
      "author" : [ "Li Wan", "Matthew D. Zeiler", "Sixin Zhang", "Yann LeCun", "Rob Fergus" ],
      "venue" : "In ICML (3),",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2013
    }, {
      "title" : "Neural architecture search with reinforcement learning",
      "author" : [ "Barret Zoph", "Quoc V. Le" ],
      "venue" : "CoRR, abs/1611.01578,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "For instance, in computer vision, this has lead to particularly well-known models like GoogleNet [21] or ResNet [8].",
      "startOffset" : 112,
      "endOffset" : 115
    }, {
      "referenceID" : 13,
      "context" : "Different algorithms have been proposed including evolutionary methods [20, 16, 17] or reinforcement learning-based approaches [24].",
      "startOffset" : 71,
      "endOffset" : 83
    }, {
      "referenceID" : 12,
      "context" : "Different algorithms have been proposed including evolutionary methods [20, 16, 17] or reinforcement learning-based approaches [24].",
      "startOffset" : 71,
      "endOffset" : 83
    }, {
      "referenceID" : 16,
      "context" : "Different algorithms have been proposed including evolutionary methods [20, 16, 17] or reinforcement learning-based approaches [24].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 0,
      "context" : "This task has been recently targeted by different models [1, 15, 12], with different learning techniques.",
      "startOffset" : 57,
      "endOffset" : 68
    }, {
      "referenceID" : 11,
      "context" : "This task has been recently targeted by different models [1, 15, 12], with different learning techniques.",
      "startOffset" : 57,
      "endOffset" : 68
    }, {
      "referenceID" : 9,
      "context" : "This task has been recently targeted by different models [1, 15, 12], with different learning techniques.",
      "startOffset" : 57,
      "endOffset" : 68
    }, {
      "referenceID" : 16,
      "context" : "This is the case for example for classification in [2, 24] based on Reinforcement learning techniques, in [19] based on gating mechanisms, in [17] based on evolutionary algorithms or even in [4] based on both RL and evolutionary techniques.",
      "startOffset" : 51,
      "endOffset" : 58
    }, {
      "referenceID" : 12,
      "context" : "This is the case for example for classification in [2, 24] based on Reinforcement learning techniques, in [19] based on gating mechanisms, in [17] based on evolutionary algorithms or even in [4] based on both RL and evolutionary techniques.",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 2,
      "context" : "This is the case for example for classification in [2, 24] based on Reinforcement learning techniques, in [19] based on gating mechanisms, in [17] based on evolutionary algorithms or even in [4] based on both RL and evolutionary techniques.",
      "startOffset" : 191,
      "endOffset" : 194
    }, {
      "referenceID" : 16,
      "context" : "For example in [24], a controller neural net can propose a “child” model architecture, which can then be trained and evaluated before trying a new architecture.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 4,
      "context" : "The oldest approach is certainly the Optimal Brain Surgeon [7] which removes weights in a classical neural network architecture.",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 14,
      "context" : "The problem of network compression can also be seen as a way to speedup a particular architecture, for example by using quantization of the weights of the network [22].",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 3,
      "context" : "Pruning and quantization techniques can also be combined [6] to obtain impressive performances.",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 1,
      "context" : "For example [3] combines algorithmic optimizations and architecture improvements to speed up the response time of a network while keeping high accuracy.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 0,
      "context" : "For example, the model proposed in [1] can be seen as an extension of dropout where the probability of sampling neurons is conditioned on currently processed input (conditional computation).",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 11,
      "context" : "In the same vein, the model proposed in [15] aims at learning routes in Super networks.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 2,
      "context" : "g Deep Sequential Neural Networks [2], Neural Fabrics [18], or even PathNet [4] – and for different tasks.",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 2,
      "context" : "The name Super Network comes from [4] which presents an architecture close to ours.",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 11,
      "context" : "While previous works make assumptions on the structure of the cost function [15], our approach does not rely on any constraint concerning C(H E) except the fact that this cost can be measured during training.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 0,
      "context" : "We propose to use a REINFORCE-inspired algorithm as in [2, 1] for learning.",
      "startOffset" : 55,
      "endOffset" : 61
    }, {
      "referenceID" : 8,
      "context" : "We propose to focus on two tasks: Image classification using MNIST and CIFAR-10 [11, 10] and Image Segmentation using the Part Labels dataset [9].",
      "startOffset" : 80,
      "endOffset" : 88
    }, {
      "referenceID" : 7,
      "context" : "We propose to focus on two tasks: Image classification using MNIST and CIFAR-10 [11, 10] and Image Segmentation using the Part Labels dataset [9].",
      "startOffset" : 80,
      "endOffset" : 88
    }, {
      "referenceID" : 6,
      "context" : "We propose to focus on two tasks: Image classification using MNIST and CIFAR-10 [11, 10] and Image Segmentation using the Part Labels dataset [9].",
      "startOffset" : 142,
      "endOffset" : 145
    }, {
      "referenceID" : 15,
      "context" : "[23] 0 % 9.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "al [13] 0 % 4.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 0,
      "context" : "Images are normalized in the range [-1,1].",
      "startOffset" : 35,
      "endOffset" : 41
    }, {
      "referenceID" : 0,
      "context" : "Images are then normalized in the range [-1,1].",
      "startOffset" : 40,
      "endOffset" : 46
    }, {
      "referenceID" : 0,
      "context" : "Images are zero padded from 250x250 to 256x256 and horizontally flipped before being normalized in the range [-1,1].",
      "startOffset" : 109,
      "endOffset" : 115
    } ],
    "year" : 2017,
    "abstractText" : "Learning neural network architectures is a way to discover new highly predictive models. We propose to focus on this problem from a different perspective where the goal is to discover architectures efficient in terms of both prediction quality and computation cost, e.g time in milliseconds, number of operations... For instance, our approach is able to solve the following task: find the best neural network architecture (in a very large set of possible architectures) able to predict well in less than 100 milliseconds on my mobile phone. Our contribution is based on a new family of models called Budgeted Super Networks that are learned using reinforcement-learning inspired techniques applied to a budgeted learning objective function which includes the computation cost during disk/memory operations at inference. We present a set of experiments on computer vision problems and show the ability of our method to discover efficient architectures in terms of both predictive quality and computation time.",
    "creator" : "LaTeX with hyperref package"
  }
}