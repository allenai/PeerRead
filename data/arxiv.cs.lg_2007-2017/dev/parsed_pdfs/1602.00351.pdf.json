{
  "name" : "1602.00351.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Adaptive Subgradient Methods for Online AUC Maximization",
    "authors" : [ "Yi Ding", "Peilin Zhao", "Steven C.H. Hoi", "Yew-Soon Ong" ],
    "emails" : [ "dingy@uchicago.edu.", "chhoi@smu.edu.sg." ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms—AUC maximization, second-order online learning, adaptive gradient, high-dimensional, sparsity.\nF"
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "AUC (Area Under ROC curve) [1] is an important measure forcharacterizing machine learning performances in many realworld applications, such as ranking, and anomaly detection tasks, especially when misclassification costs are unknown. In general, AUC measures the probability for a randomly drawn positive instance to have a higher decision value than a randomly sample negative instance. Many efforts have been devoted recently to developing efficient AUC optimization algorithms for both batch and online learning tasks [2], [3], [4], [5], [6], [7].\nDue to its high efficiency and scalability in real-world applications, online AUC optimization for streaming data has been actively studied in the research community in recent years. The key challenge for AUC optimization in online setting is that AUC is a metric represented by the sum of pairwise losses between instances from different classes, which makes conventional online learning algorithms unsuitable for direct use in many real world scenarios. To address this challenge, two core types of Online AUC Maximization (OAM) frameworks have been proposed recently. The first framework is based on the idea of buffer sampling [6], [8], which stores some randomly sampled historical examples in a buffer to represent the observed data for calculating the pairwise loss functions. The other framework focuses on one-pass AUC optimization [7], where the algorithm\n• Yi Ding is with the Department of Computer Science, The University of Chicago, Chicago, IL, USA, 60637, E-mail: dingy@uchicago.edu.\n• Corresponding author: Steven C.H. Hoi is with the School of Information Systems, Singapore Management University, Singapore 178902, E-mail: chhoi@smu.edu.sg.\n• Peilin Zhao is with the Data Analytics Department, Institute for Infocomm Research, A*STAR, Singapore 138632, E-mail: zhaop@i2r.a-star.edu.sg.\n• Yew-Soon Ong is with the School of Computer Engineering, Nanyang Technological University, Singapore 639798, E-mail: ASYSOng@ntu.edu.sg.\nscan through the training data only once. The benefit of one-pass AUC optimization lies in the use of squared loss to represent the AUC loss function while providing proofs on its consistency with the AUC measure [9].\nAlthough these algorithms have been shown to be capable of achieving fairly good AUC performances, they share a common trait of employing the online gradient descent technique, which fail to take advantage of the geometrical property of the data observed from the online learning process, while recent studies have shown the importance of exploiting this information for online optimization [10]. To overcome the limitation of the existing works, we propose a novel framework of Adaptive Online AUC maximization (AdaOAM), which considers the adaptive gradient optimization technique for exploiting the geometric property of the observed data to accelerate online AUC maximization tasks. Specifically, the technique is motivated by a simple intuition, that is, the frequently occurring features in online learning process should be assigned with low learning rates while the rarely occurring features should be given high learning rates. To achieve this purpose, we propose the AdaOAM algorithm by adopting the adaptive gradient updating framework proposed by [10] to control the learning rates for different features. We theoretically prove that the regret bound of the proposed algorithm is better than those of the existing non-adaptive algorithms. We also empirically compared the proposed algorithm with several state-of-the-art online AUC optimization algorithms on both benchmark datasets and real-world online anomaly detection datasets. The promising results validate the effectiveness and efficiency of the proposed AdaOAM.\nTo further handle high-dimensional sparse tasks in practice, we investigate an extension of the AdaOAM method, which is labeled here as the Sparse AdaOAM method (SAdaOAM). The motivation is that because the regular AdaOAM algorithm assumes every feature is relevant and thus most of the weights for corresponding\nar X\niv :1\n60 2.\n00 35\n1v 1\n[ cs\n.L G\n] 1\nF eb\n2 01\n6\n2 features are often non-zero, which leads to redundancy and low efficiency when rare features are informative for high dimension tasks in practice. To make AdaOAM more suitable for such cases, the SAdaOAM algorithm is proposed by inducing sparsity in the learning weights using adaptive proximal online gradient descent. To the best of our knowledge, this is the first effort to address the problem of keeping the online model sparse in online AUC maximization task. Moreover, we have theoretically analyzed this algorithm, and empirically evaluated it on an extensive set of real-world public datasets, compared with several state-of-theart online AUC maximization algorithms. Promising results have been obtained that validate the effectiveness and efficacy of the proposed SAdaOAM.\nThe rest of this paper is organized as follows. We first review the related works from three core areas: online learning, AUC maximization, and sparse online learning, respectively. Then, we present the formulations of the proposed approaches for handling both regular and high-dimensional sparse data, and their theoretical analysis; we further show and discuss the comprehensive experimental results, the sensitivity of the parameters, and tradeoffs between the level of sparsity and AUC performances. Finally, we conclude the paper with a brief summary of the present work."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "Our work is closely related to three topics in the context of machine learning, namely, online learning, AUC maximization, and sparse online learning. Below we briefly review some of the important related work in these areas.\nOnline Learning. Online learning has been extensively studied in the machine learning communities [11], [12], [13], [14], [15], mainly due to its high efficiency and scalability to large-scale learning tasks. Different from conventional batch learning methods that assume all training instances are available prior to the learning phase, online learning considers one instance each time to update the model sequentially and iteratively. Therefore, online learning is ideally appropriate for tasks in which data arrives sequentially. A number of first-order algorithms have been proposed including the well-known Perceptron algorithm [16] and the Passive-Aggressive (PA) algorithm [12]. Although the PA introduces the concept of “maximum margin” for classification, it fails to control the direction and scale of parameter updates during online learning phase. In order to address this issue, recent years have witnessed some second-order online learning algorithms [17], [18], [19], [20], which apply parameter confidence information to improve online learning performance. Further, in order to solve the costsensitive classification tasks on-the-fly, online learning researchers have also proposed a few novel online learning algorithms to directly optimize some more meaningful cost-sensitive metrics [21], [22], [23].\nAUC Maximization. AUC (Area Under ROC curve) is an important performance measure that has been widely used in imbalanced data distribution classification. The ROC curve explains the rate of the true positive against the false positive at various range of threshold. Thus, AUC represents the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one. Recently, many algorithms have been developed to optimize AUC directly [2], [3], [4], [6], [7]. In [4], the author firstly presented a general framework for optimizing multivariate nonlinear performance measures such as the AUC, F1,\netc. in a batch mode. Online learning algorithms for AUC maximization involving large-scale applications have also been studied. Among the online AUC maximization approaches, two core online AUC optimization frameworks have been proposed very recently. The first framework is based on the idea of buffer sampling [6], [8], which employed a fixed-size buffer to represent the observed data for calculating the pairwise loss functions. A representative study is available in [6], which leveraged the reservoir sampling technique to represent the observed data instances by a fixed-size buffer where notable theoretical and empirical results have been reported. Then, [8] studied the improved generalization capability of online learning algorithms for pairwise loss functions with the framework of buffer sampling. The main contribution of their work is the introduction of the stream subsampling with replacement as the buffer update strategy. The other framework which takes a different perspective was presented by [7]. They extended the previous online AUC maximization framework with a regressionbased one-pass learning mode, and achieved solid regret bounds by considering square loss for the AUC optimization task due to its theoretical consistency with AUC.\nSparse Online Learning. The high dimensionality and high sparsity are two important issues for large-scale machine learning tasks. Many previous efforts have been devoted to tackling these issues in the batch setting, but they usually suffer from poor scalability when dealing with big data. Recent years have witnessed extensive research studies on sparse online learning [24], [25], [26], [27], which aim to learn sparse classifiers by limiting the number of active features. There are two core categories of methods for sparse online learning. The representative work of the first type follows the general framework of subgradient descent with truncation. Taking the FOBOS algorithm [25] as an example, which is based on the Forward-Backward Splitting method to solve the sparse online learning problem by alternating between two phases: (i) an unconstraint stochastic subgradient descent step with respect to the loss function, and (ii) an instantaneous optimization for a tradeoff between keeping close proximity to the result of the first step and minimizing `1 regularization term. Following this strategy, [24] argues that the truncation at each step is too aggressive and thus proposes the Truncated Gradient (TG) method, which alleviates the updates by truncating the coefficients at every K steps when they are lower than a predefined threshold. The second category of methods are mainly motivated by the dual averaging method [28]. The most popular method in this category is the Regularized Dual Averaging (RDA) [26], which solves the optimization problem by using the running average of all past subgradients of the loss functions and the whole regularization term instead of the subgradient. In this manner, the RDA method has been shown to exploit the regularization structure more easily in the online phase and obtain the desired regularization effects more efficiently.\nDespite the extensive works in these different fields of machine learning, to the best of our knowledge, our current work represents the first effort to explore adaptive gradient optimization and second order learning techniques for online AUC maximization in both regular and sparse online learning settings."
    }, {
      "heading" : "3 ADAPTIVE SUBGRADIENT METHODS FOR OAM",
      "text" : ""
    }, {
      "heading" : "3.1 Problem Setting",
      "text" : "We aim to learn a linear classification model that maximizes AUC for a binary classification problem. Without loss of generality,\n3 we assume positive class to be less than negative class. Denote (xt, yt) as the training instance received at the t-th trial, where xt ∈ Rd and yt ∈ {−1,+1}, and wt ∈ Rd is the weight vector learned so far.\nGiven this setting, let us define the AUC measurement [1] for binary classification task. Given a dataset D = {(xi, yi) ∈ Rd × {−1,+1}| i ∈ [n]}, where [n] = {1, 2, . . . , n}, we divide it into two sets naturally: the set of positive instances D+ = {(x+i ,+1)| i ∈ [n+]} and the set of negative instances D− = {(x−j ,−1)| j ∈ [n−]}, where n+ and n− are the numbers of positive and negative instances, respectively. For a linear classifier w ∈ Rd, its AUC measurement on D is defined as follows:\nAUC(w) =\n∑n+ i=1 ∑n− j=1 I(w·x+i >w·x−j ) + 1 2 I(w·x+i =w·x−j )\nn+n− ,\nwhere Iπ is the indicator function that outputs a ′1′ if the prediction π holds and ′0′ otherwise. We replace the indicator function with the following convex surrogate, i.e., the square loss from [7] due to its consistency with AUC [9]\n`(w,x+i − x − j ) = (1−w · (x + i − x − j )) 2,\nand find the optimal classifier by minimizing the following objective function\nL(w) = λ 2 ‖w‖22 +\nn+∑ i=1 n−∑ j=1 `(w,x+i − x − j )\n2n+n− . (1)\nwhere λ2 ‖w‖ 2 2 is introduced to regularize the complexity of the\nlinear classifier. Note, the optimal w∗ satisfies ‖w∗‖2 ≤ 1/ √ λ according to the strong duality theorem."
    }, {
      "heading" : "3.2 Adaptive Online AUC Maximization",
      "text" : "Here, we shall introduce the proposed Adaptive Online AUC Maximization (AdaOAM) algorithm. Following the similar approach in [7], we modify the loss function L(w) in (1) as a sum of losses\nfor individual training instance T∑ t=1 Lt(w) where\nLt(w) = λ\n2 ‖w‖22 +\nt−1∑ i=1 I[yi 6= yt](1− yt(xt − xi)>w)2\n2|i ∈ [t− 1] : yiyt = −1| , (2)\nfor i.i.d. sequence St = {(xi, yi)|i ∈ [t]}, and it is an unbiased estimation to L(w). X+t and X−t are denoted as the sets of positive and negative instances of St respectively, and T+t and T−t are their respective cardinalities. Besides, Lt(w) is set as 0 for T+t T − t = 0. If yt = 1, the gradient of Lt is\n∇Lt(w) =λw + xtx>t w − xt\n+\n∑ i:yi=−1 xi+ (xix > i −xix>t −xtx>i )w\nT−t .\nIf using c−t = 1 T−t ∑ i:yi=−1 xi and S − t = 1 T−t ∑ i:yi=−1 (xix > i − c−t [c − t ] >) to refer to the mean and covariance matrix of negative class, respectively, the gradient of Lt can be simplified as\n∇Lt(w) =λw− xt+ c−t + (xt − c−t )(xt − c−t )>w+ S−t w. (3)\nSimilarly, if yt = −1,\n∇Lt(w) =λw+ xt− c+t + (xt − c+t )(xt − c+t )>w+ S+t w, (4)\nwhere c+t = 1 T+t ∑ i:yi=1 xi and S + t = 1 T+t ∑ i:yi=1 (xix > i − c+t [c + t ] >) are the mean and covariance matrix of positive class, respectively. Upon obtaining gradient gt = ∇Lt(wt), a simple solution is to move the weight wt in the opposite direction of gt, while keeping ‖wt+1‖ ≤ 1/ √ λ via the projected gradient update [29]\nwt+1 = Π 1√ λ (wt − ηgt) = arg min ‖w‖≤ 1√\nλ\n‖w − (wt − ηgt)‖22,\ndue to ‖w∗‖ ≤ 1/ √ λ.\nHowever, the above scheme is clearly insufficient, since it simply assigns different features with the same learning rate. In order to perform feature-wise gradient updating, we propose a second-order gradient optimization method, i.e., Adaptive Gradient Updating strategy, as inspired by [10]. Specifically, we denote g1:t = [g1...gt] as the matrix obtained by concatenating the gradient sequences. The i-th row of this matrix is g1:t,i, which is also a concatenation of the i-th component of each gradient. In addition, we define the outer product matrix Gt = ∑t τ=1 gτg > τ . Using these notations, the generalization of the standard adaptive gradient descent leads to the following weight update\nwt+1 = Π G 1/2 t 1√ λ (wt − ηG−1/2t gt),\nwhere ΠA1√ λ (u) = arg min‖w‖≤ 1√ λ ‖w − u‖A = arg min‖w‖≤ 1√ λ 〈w − u, A(w − u)〉, which is the Mahalanobis norm to denote the projection of a point u onto {w|‖w‖ ≤ 1√ λ }.\nHowever, an obvious drawback of the above update lies in the significantly large amount of computational efforts needed to handle high-dimensional data tasks since it requires the calculations of the root and inverse root of the outer product matrix Gt. In order to make the algorithm more efficient, we use the diagonal proxy of Gt and thus the update becomes\nwt+1 = Π diag(Gt)\n1/2\n1√ λ\n(wt − ηdiag(Gt)−1/2gt). (5)\nIn this way, both the root and inverse root of diag(Gt) can be computed in linear time. Furthermore, as we discuss later, when the gradient vectors are sparse, the update above can be conducted more efficiently in time proportional to the support of the gradient.\nAnother issue with the updating rule (5) to be concern is that the diag(Gt) may not be invertible in all coordinates. To address this issue, we replace it with Ht = δI+diag(Gt)1/2, where δ > 0 is a smooth parameter. The parameter δ is introduced to make the diagonal matrix invertible and the algorithm robust, which is usually set as a very small value so that it has little influence on the learning results. Given Ht, the update of the feature-wise adaptive update can be computed as:\nwt+1 = Π Ht 1√ λ (wt − ηH−1t gt). (6)\nThe intuition of this update rule (6) is very natural, which considers the rare occurring features as more informative and discriminative than those frequently occurring features. Thus, these informative rare occurring features should be updated with higher learning rates by incorporating the geometrical property of the data observed in earlier stages. Besides, by using the\n4 previously observed gradients, the update process can mitigate the effects of noise and speed up the convergence rate intuitively.\nSo far, we have reached the key framework of the basic update rule for model learning except the details on gradient calculation. From the gradient derivation equations (3) and (4), we need to maintain and update the mean vectors and covariance matrices of the incoming instance sequences observed. The mean vectors are easy to be computed and stored here, while the covariance matrices are a bit difficult to be updated due to the online setting. Therefore, we provide a simplified update scheme for covariance matrix computation to address this issue. For an incoming instance sequence x1,x2, . . . ,xt, the covariance matrix St is given by\nSt = t∑ i=1 xix > i − ct[ct]> t\n= t∑ i=1 xix > i t − ct[ct]>\n= t− 1 t t−1∑ i=1 xix > i t− 1 + xtx > t t − ct[ct]>\n= t− 1 t ( t−1∑ i=1 xix > i t− 1 − ct−1[ct−1] > + ct−1[ct−1] >) + xtx > t t\n− ct[ct]>\n= t− 1 t (St−1 + ct−1[ct−1] >) + xtx > t t − ct[ct]> =St−1 − 1\nt (St−1 + ct−1[ct−1]\n> − xtx>t ) + ct−1[ct−1]>\n− ct[ct]>.\nThen, in our gradient update, if setting Γ+t = S + t and Γ − t =\nS−t , the covariance matrices are updated as follows:\nΓ+t = Γ + t−1+c + t−1[c + t−1] >−c+t [c+t ]>+ xtx > t− Γ+t−1− c + t−1[c + t−1] >\nT+t ,\nΓ−t = Γ − t−1+ c − t−1[c − t−1] >− c−t [c−t ]>+ xtx > t−Γ−t−1−c − t−1[c − t−1] >\nT−t .\nIt can be observed that the above updates fit the online setting well. Finally, Algorithm 1 summarizes the proposed AdaOAM method."
    }, {
      "heading" : "3.3 Fast AdaOAM Algorithm for High-dimensional Sparse Data",
      "text" : "A characteristic of the proposed AdaOAM algorithm described above is that it exploits the full features for weight learning, which may not be suitable or scalable for high-dimensional sparse data tasks. For example, in spam email detection tasks, the length of the vocabulary list can reach the million scale. Although the number of the features is large, many feature inputs are zero and do not provide any information to the detection task. The research work in [30] has shown that the classification performance saturates with dozens of features out of tens of thousands of features.\nTaking the cue, in order to improve the efficiency and scalability of the AdaOAM algorithm on working with highdimensional sparse data, we propose the Sparse AdaOAM algorithm (SAdaOAM) which learns a sparse linear classifier that contains a limited size of active features. In particular, SAdaOAM addresses the issue of sparsity in the learned model and maintains\nAlgorithm 1 The Adaptive OAM Algorithm (AdaOAM) Input: The regularization parameter λ, the learning rate {ηt}Tt=1, the smooth parameter δ ≥ 0. Output: Updated classifier wt. Variables: s ∈ Rd, H ∈ Rd×d, g1:t,i ∈ Rt for i ∈ {1, . . . , d}.\nInitialize w0 = 0, c+0 = c − 0 = 0,Γ + 0 = Γ − 0 = [0]d×d, T + 0 = T−0 = 0, g1:0 = [ ]. for t = 1, 2, . . . , T do\nReceive an incoming instance (xt, yt); if yt = +1 then T+t = T + t−1 + 1, T − t = T − t−1;\nc+t = c + t−1 + 1 T+t (xt − c+t−1) and c − t = c − t−1; Update Γ+t and Γ − t = Γ − t−1; Receive gradient gt = ∇Lt(w); Update g1:t = [g1:t−1 gt], st,i = ‖g1:t,i‖2; Ht = δI + diag(st); ĝt = H −1 t gt;\nelse T−t = T − t−1 + 1, T + t = T + t−1;\nc−t = c − t−1 + 1 T−t (xt − c−t−1) and c + t = c + t−1; Update Γ−t and Γ + t = Γ + t−1; Receive gradient gt = ∇Lt(w); Update g1:t = [g1:t−1 gt], st,i = ‖g1:t,i‖2; Ht = δI + diag(st); ĝt = H −1 t gt;\nend if wt = Π\nHt 1√ λ (wt−1 − ηtĝt); end for\nthe efficacy of the original AdaOAM at the same time. To summarize, SAdaOAM has two benefits over the original AdaOAM algorithm: simple covariance matrix update and sparse model update. Next, we introduce these properties separately.\nFirst, we employ a simpler covariance matrix update rule in the case of handling high-dimensional sparse data when compared to AdaOAM. The motivating factor behind a different update scheme here is because using the original covariance update rule of the AdaOAM method on high-dimensional data would lead to extreme high computational and storage costs, i.e. several matrix operations among multiple variables in the Γ update formulations would be necessary. Therefore, we fall back to the standard definition of the covariance matrix and consider a simpler method for updates. Since the standard definition of the covariance matrix\nis St = t∑ i=1 xix > i t − ct[ct] >, we just need to maintain the mean vector and the outer product of the instance at each iteration for the\ncovariance update. In this case, we denote Z+t = t∑ i=1 x+i [x + i ] >\nand Z−t = t∑ i=1 x−i [x − i ] >. Then, the covariance matrices S+t and S−t can be formulated as\nS+t = Z + t /T + t − c+t [c+t ]> and S−t = Z−t /T−t − c−t [c−t ]>.\nAt each iteration, one only needs to update Zytt with Z yt t = Zytt−1 +xt[xt] > and the mean vectors of the positive and negative instances, respectively, in the covariance matrices S+t and S − t . With the above update scheme, a lower computational and storage\n5 costs is attained since most of the elements in the covariance matrices are zero on high-dimensional sparse data.\nAfter presenting the efficient scheme of updating the covariance matrices for high-dimensional sparse data, we proceed next to present the method for addressing the sparsity in the learned model and second-order adaptive gradient updates simultaneously. Here we consider to impose the soft-constraint `1 norm regularization ϕ(w) = θ‖w‖1 to the objective function (2). So, the new objective function is\nLt(w) = λ\n2 ‖w‖22 +\nt−1∑ i=1 I[yi 6= yt](1− yt(xt − xi)>w)2\n2|i ∈ [t− 1] : yiyt = −1| + θ‖w‖1, (7)\nIn order to optimize this objective function, we apply the composite mirror descent method [31] that is able to achieve a trade-off between the immediate adaptive gradient term gt and the regularizer ϕ(w). We denote the i-th diagonal element of the matrix Ht as Ht,ii = δI + ‖g1:t,i‖2. Then, we give the derivation for the composite mirror descent gradient updates with `1 regularization.\nFollowing the framework of the composite mirror descent update in [10], the update needed to solve is\nwt+1 = arg min ‖w‖≤ 1√\nλ\n{η〈gt,w〉+ ηϕ(w) +Bψt(w,wt)}, (8)\nwhere gt = ∇Lt(w) andBψt(w,wt) is the Bregman divergence associated with ψt(gt) = 〈gt, Htgt〉 (see details in the proof of Theorem 1). After the expansion, this update amounts to\nmin w\nη〈gt,w〉+ ηϕ(w) + 1\n2 〈w −wt, Ht(w −wt)〉. (9)\nFor easier derivation, we rearrange and rewrite the above objective function as\nmin w 〈ηgt −Htwt,w〉+\n1 2 〈w, Htw〉+ 1 2 〈wt, Htwt〉+ ηθ‖w‖1.\nLet ŵ denote the optimal solution of the above optimization problem. Standard subgradient calculus indicates that when |wt,i − ηHt,iigt,i| ≤ ηθ Ht,ii , the solution is ŵi = 0. Similarly, when wt,i − ηHt,iigt,i < − ηθ Ht,ii\n, then ŵi < 0, the objective is differentiable, and the solution is achieved by setting the gradient to zero:\nηgt,i −Ht,iiwt,i −Ht,iiŵi − ηθ = 0,\nso that\nŵi = η\nHt,ii gt,i −wt,i −\nηθ\nHt,ii .\nSimilarly, when wt,i − ηHt,iigt,i > ηθ Ht,ii , then ŵi > 0, and the solution is\nŵi = wt,i − η\nHt,ii gt,i −\nηθ\nHt,ii .\nCombining these three cases, we obtain the coordinate-wise update results for wt+1,i :\nwt+1,i = sign(wt,i − η\nHt,ii gt,i)[|wt,i −\nη\nHt,ii gt,i| −\nηθ\nHt,ii ]+.\nThe complete sparse online AUC maximization approach using the adaptive gradient updating algorithm (SAdaOAM) is summarized in Algorithm 2.\nAlgorithm 2 The Sparse AdaOAM Algorithm (SAdaOAM) Input: The regularization parameters λ and θ, the learning rate {ηt}Tt=1, the smooth parameter δ ≥ 0. Output: Updated classifier wt+1. Variables: s ∈ Rd, H ∈ Rd×d, g1:t,i ∈ Rt for i ∈ {1, . . . , d}.\nInitialize w0 = 0, c+0 = c − 0 = 0, Z + 0 = Z − 0 = [0]d×d, T + 0 = T − 0 = 0, g1:0 = [ ].for t = 1, 2, . . . , T do\nReceive an incoming instance (xt, yt); if yt = +1 then T+t = T + t−1 + 1, T − t = T − t−1;\nc+t = c + t−1 + 1 T+t (xt − c+t−1) and c − t = c − t−1; Update Z+t = Z + t−1 + xt[xt] > and Z−t = Z − t−1; Receive gradient gt = ∇Lt(w); Update g1:t = [g1:t−1 gt], st,i = ‖g1:t,i‖2; Ht,ii = δI + ‖g1:t,i‖2; wt+1,i = sign(wt,i − ηHt,iigt,i)[|wt,i − η Ht,ii\ngt,i| − ηθ Ht,ii\n]+; else T−t = T − t−1 + 1, T + t = T + t−1;\nc−t = c − t−1 + 1 T−t (xt − c−t−1) and c + t = c + t−1; Update Z−t = Z − t−1 + xt[xt] > and Z+t = Z − t−1; Receive gradient gt = ∇Lt(w); Update g1:t = [g1:t−1 gt], st,i = ‖g1:t,i‖2; Ht,ii = δI + ‖g1:t,i‖2; wt+1,i = sign(wt,i − ηHt,iigt,i)[|wt,i − η Ht,ii\ngt,i| − ηθ Ht,ii\n]+; end if\nend for\nFrom the Algorithm 2, it is observed that we perform “lazy” computation when the gradient vectors are sparse [10]. Suppose that, from iteration step t0 to t, the i-th component of the gradient is “0”. Then, we can evaluate the updates on demand since Ht,ii remains intact. Therefore, at iteration step t when wt,i is needed, the update will be\nwt,i = sign(wt0 , i)[|wt0 , i| − ηθ\nHt0,ii (t− t0)]+,\nwhere [t]+ means max(0, t). Obviously, this type of ”lazy” updates enjoys high efficiency."
    }, {
      "heading" : "4 THEORETICAL ANALYSIS",
      "text" : "In this section, we provide the regret bounds for the proposed set of AdaOAM algorithms for handling both regular and highdimensional sparse data, respectively."
    }, {
      "heading" : "4.1 Regret Bounds with Regular Data",
      "text" : "Firstly, we introduce two lemmas as follows, which will be used to facilitate our subsequent analyses.\nLemma 1. Let gt, g1:t and st be defined same in the Algorithm 1. Then\nT∑ t=1 〈gt, diag(st)−1gt〉 ≤ 2 d∑ i=1 ‖g1:T,i‖2.\nLemma 2. Let the sequence {wt, g1:t} ⊂ Rd be generated by the composite mirror descent update in Equation (12) and assume that\n6 supw,u∈χ‖w−u‖∞ ≤ D∞. Using learning rate η = D∞/ √\n2, for any optimal w∗, the following bound holds\nT∑ t=1 [Lt(wt)− Lt(w∗)] ≤ √ 2D∞ d∑ i=1 ‖g1:T,i‖2.\nThese two lemmas are actually the Lemma 4 and Corollary 1 in the paper [10].\nUsing these two lemmas, we can derive the following theorem for the proposed AdaOAM algorithm. Theorem 1. Assume ‖wt‖ ≤ 1/ √ λ, (∀t ∈ [T ]) and the diameter of χ = {w|‖w‖ ≤ 1√ λ } is bounded via supw,u∈χ‖w− u‖∞ ≤ D∞, we have T∑ t=1 [Lt(wt)− Lt(w∗)] ≤ 2D∞ d∑ i=1 √√√√ T∑ t=1 [(λwt,i)2 + C(rt,i)2],\nwhere C ≤ (1 + 2√ λ )2, and rt,i = maxj<t |xj,i − xt,i|. Proof. We first define w∗ as w∗ = arg min\nw\n∑ t Lt(w).\nBased on the regularizer λ2 ‖w‖ 2, it is easy to obtain ‖w∗‖2 ≤ 1/λ due to the strong convexity property, and it is also reasonable to restrict wt with ‖wt‖2 ≤ 1/λ. Denote the projection of a point w onto ‖u‖2 ≤ 1√λ according to norm ‖ · ‖Ht by Π Ht 1√ λ (w) = arg min‖u‖≤ 1√ λ ‖u−w‖Ht , the AdaOAM actually employs the following update rule:\nwt+1 = Π Ht 1√ λ (wt − ηH−1t gt), (10)\nwhere Ht = δI + diag(st) and δ ≥ 0. If we denote ψt(gt) = 〈gt, Htgt〉, and the dual norm of ‖·‖ψt by ‖ · ‖ψ∗t , in which case ‖gt‖ψ∗t = ‖gt‖H−1t , then it is easy to check the update rule 10 is the same with the following composite mirror descent method:\nwt+1 = arg min ‖w‖≤ 1√\nλ\n{η〈gt,w〉+ ηϕ(w) +Bψt(w,wt)}, (11)\nwhere the regularization function ϕ ≡ 0, and Bψt(w,wt) is the Bregman divergence associated with a strongly convex and differentiable function ψt\nBψt(w,wt) = ψt(w)− ψt(wt)− 〈∇ψt(wt),w −wt〉.\nSince we have ϕ ≡ 0 in the case of regular data, the regret bound R(T ) = T∑ t=1\n[Lt(wt) − Lt(w∗)]. Then, we follow the derivation results of [10] and attain the following regret bound\nT∑ t=1 [Lt(wt)− Lt(w∗)] ≤ √ 2D∞ d∑ i=1 ‖g1:T,i‖2,\nwhere χ = {w|‖w‖ ≤ 1√ λ } is bounded via supw,u∈χ‖w − u‖∞ ≤ D∞. Next, we would like to analyze the features’ dependency on the data of the gradient. Since\n(gt,i) 2 ≤ [ λwt,i +\nt−1∑ j=1 (1− yt〈xt − xj ,w〉)yt(xj,i − xt,i)\nT−t ]2 ≤ 2(λwt,i)2 + 2C(xj,i − xt,i)2 = 2(λwt,i)2 + 2C(rt,i)2,\nwhere C ≤ (1 + 2√ λ )2 is a constant to bound the scalar of the second term in the right side of the equation, and with rt,i = maxj<t |xj,i − xt,i|, we have\nd∑ i=1 ‖g1:T,i‖2 = d∑ i=1 √√√√ T∑ t=1 (gt,i)2\n≤ √ 2 d∑ i=1 √√√√ T∑ t=1 [(λwt,i)2 + C(rt,i)2].\nFinally, combining the above inequalities, we arrive at\nT∑ t=1 [Lt(wt)− Lt(w∗)] ≤ 2D∞ d∑ i=1 √√√√ T∑ t=1 [(λwt,i)2 + C(rt,i)2].\nFrom the proof above, we can conclude that Algorithm 1 should have a lower regret than non-adaptive algorithms due to its dependence on the geometry of the underlying data space. If the features are normalized and sparse, the gradient terms in\nthe bound d∑ i=1 ‖g1:T,i‖2 should be much smaller than √ T , which leads to lower regret and faster convergence. If the feature space is relative dense, then the convergence rate will be O(1/ √ T ) for the general case as in OPAUC and OAM methods."
    }, {
      "heading" : "4.2 Regret Bounds with High-dimensional Sparse Data",
      "text" : "Theorem 2. Assume ‖wt‖ ≤ 1/ √ λ, (∀t ∈ [T ]) and the diameter of χ = {w|‖w‖ ≤ 1√ λ } is bounded via supw,u∈χ‖w− u‖∞ ≤ D∞, the regret bound with respect to `1 regularization term is\nT∑ t=1 [Lt(wt) + θ‖wt‖1 − Lt(w∗)− θ‖w∗‖1]\n≤ 2D∞ d∑ i=1 √√√√ T∑ t=1 [(λwt,i)2 + C(rt,i)2],\nwhere w∗ = arg minw ∑T t=1[Lt(w)+θ‖w‖1], C ≤ (1+ 2√λ )\n2, and rt,i = maxj<t |xj,i − xt,i|. Proof. In the case of high-dimensional sparse adaptive online AUC maximization, the regret we plan to bound with respect to the optimal weight w∗ is formulated as\nR(T ) = T∑ t=1 [Lt(wt) + ϕ(wt)− Lt(w∗)− ϕ(w∗)],\nwhere ϕ(w) = θ‖w‖1 is the `1 regularization term to impose sparsity to the solution. Similarly, if denote ψt(gt) = 〈gt, Htgt〉, and the dual norm of ‖ · ‖ψt by ‖ · ‖ψ∗t , in which case ‖gt‖ψ∗t = ‖gt‖H−1t , it is easy to check the updating rule of SAdaOAM\nwt+1,i = sign(wt,i − η\nHt,ii gt,i)[|wt,i −\nη\nHt,ii gt,i| −\nηθ\nHt,ii ]+,\nis the same with the following one\nwt+1 = arg min ‖w‖≤ 1√\nλ\n{η〈gt,w〉+ ηϕ(w) +Bψt(w,wt)}, (12)\nwhere ϕ(w) = θ||w||1. From [10], we have\nR(T ) ≤ 1 2η max t≤T ‖w∗ −wt‖2∞ d∑ i=1 ‖g1:T,i‖2 + η d∑ i=1 ‖g1:T,i‖2.\n7 Furthermore, we assume supw,u∈χ ‖w − u‖∞ ≤ D∞ and set η = D/ √ 2, the final regret bound is\nR(T ) ≤ √\n2D∞ d∑ i=1 ‖g1:T,i‖2.\nThis theoretical result shows that the regret bound for sparse solution is the same as that in the case when ϕ ≡ 0.\nAs discussed above, the SAdaOAM algorithm should have lower regret bound than non-adaptive algorithms do on highdimensional sparse data, though this depends on the geometric property of the underlying feature distribution. If some features\nappear much more frequently than others, then d∑ i=1 ‖g1:T,i‖2 indicates that we could have remarkably lower regret by using higher learning rates for infrequent features and lower learning rates for often occurring features."
    }, {
      "heading" : "5 EXPERIMENTAL RESULTS",
      "text" : "In this section, we evaluate the proposed set of the AdaOAM algorithms in terms of AUC performance, convergence rate, and examine their parameter sensitivity. The main framework of the experiments is based on the LIBOL, an open-source library for online learning algorithms 1 [32]."
    }, {
      "heading" : "5.1 Comparison Algorithms",
      "text" : "We conduct comprehensive empirical studies by comparing the proposed algorithms with various AUC optimization algorithms for both online and batch scenarios. Specifically, the algorithms considered in our experiments include:\n• Online Uni-Exp: An online learning algorithm which optimizes the (weighted) univariate exponential loss [33]; • Online Uni-Log: An online learning algorithm which optimizes the (weighted) univariate logistic loss [33]; • OAMseq: The OAM algorithm with reservoir sampling and sequential updating method [6]; • OAMgra: The OAM algorithm with reservoir sampling and online gradient updating method [6]; • OPAUC: The one-pass AUC optimization algorithm with square loss function [7]; • SVM-perf: A batch algorithm which directly optimizes AUC [4]; • CAPO: A batch algorithm which trains nonlinear auxiliary classifiers first and then adapts auxiliary classifiers for specific performance measures [34]; • Batch Uni-Log: A batch algorithm which optimizes the (weighted) univariate logistic loss [33]; • Batch Uni-Squ: A batch algorithm which optimizes the (weighted) univariate square loss; • AdaOAM: The proposed adaptive gradient method for online AUC maximization. • SAdaOAM: The proposed sparse adaptive subgradient method for online AUC maximization.\nIt is noted that the OAMseq , OAMgra, and OPAUC are the state-of-the-art methods for AUC maximization in online settings. For batch learning scenarios, CAPO and SVM-perf are both strong baselines to compare against.\n1. http://libol.stevenhoi.org/"
    }, {
      "heading" : "5.2 Experimental Testbed and Setup",
      "text" : "To examine the performance of the proposed AdaOAM in comparison to the existing state-of-the-art methods, we conduct extensive experiments on sixteen benchmark datasets by maintaining consistency to the previous studies on online AUC maximization [6], [7]. Table 1 shows the details of 16 binary-class datasets in our experiments. All of these datasets can be downloaded from the LIBSVM 2 and UCI machine learning repository 3. Note that several datasets (svmguide4, vehicle) are originally multi-class, which were converted to class-imbalanced binary datasets for the purpose of in our experimental studies.\nIn the experiments, the features have been normalized fairly, i.e., xt ← xt/‖xt‖, which is reasonable since instances are received sequentially in online learning setting. Each dataset has been randomly divided into 5 folds, in which 4 folds are for training and the remaining fold is for testing. We also generate 4 independent 5-fold partitions per dataset to further reduce the effects of random partition on the algorithms. Therefore, the reported AUC values are the average results of 20 runs for each dataset. 5-fold cross validation is conducted on the training sets to decide on the learning rate η ∈ 2[−10:10] and the regularization parameter λ ∈ 2[−10:6]. For OAMgra and OAMseq , the buffer size is fixed at 100 as suggested in [6]. All experiments for online setting comparisons were conducted with MATLAB on a computer workstation with 16GB memory and 3.20GHz CPU. On the other hand, for fair comparisons in batch settings, the core steps of the algorithms were implemented in C++ since we directly use the respective toolboxes 4 5 provided by the respective authors of the SVM-perf and CAPO algorithms."
    }, {
      "heading" : "5.3 Evaluation of AdaOAM on Benchmark Datasets",
      "text" : "Table 2 summarizes the average AUC performance of the algorithms under studied over the 16 datasets for online setting. In this table, we use •/◦ to indicate that AdaOAM is significantly better/worse than the corresponding method (pairwise t-tests at 95% significance level).\nFrom the results in Table 2, several interesting observations can be drawn. Firstly, the win/tie/loss counts show that the AdaOAM is clearly superior to the counterpart algorithms considered for comparison, as it wins in most cases and has zero loses in terms of AUC performance. This indicates that the proposed AdaOAM is the most effective online AUC optimization algorithm among all others considered. Secondly, AdaOAM outperforms the first-order online AUC maximization algorithms, including, OPAUC, OAMseq , and OAMgra, thus demonstrating that secondorder information can help significantly improve the learning efficacy of existing online AUC optimization algorithms. In addition, on svmguide4, balance scale, and poker hand datasets, the optimization methods based on pairwise loss functions including AdaOAM, OPAUC, OAMseq , and OAMgra perform far better than those methods based on univariate loss functions including Uni-Log and Uni-Exp. This highlights the significance and effectiveness of methods based on pairwise loss function optimization over univariate loss function ones.\nTo study the efficiency of the proposed AdaOAM algorithm, Figure 1 depicts the running time (in milliseconds) of AdaOAM\n2. http://www.csie.ntu.edu.tw/∼cjlin/libsvmtools/ 3. http://www.ics.uci.edu/∼mlearn/MLRepository.html 4. http://www.cs.cornell.edu/people/tj/svm light/svm perf.html 5. http://lamda.nju.edu.cn/code CAPO.ashx\n8TABLE 1 Details of benchmark machine learning datasets.\ndatasets # inst # dim T−/T+ datasets # inst # dim T−/T+ glass 214 9 2.057 vehicle 846 18 3.251 heart 270 13 1.250 german 1,000 24 2.333\nsvmguide4 300 10 5.818 svmguide3 1,243 22 3.199 liver-disorders 345 6 1.379 a2a 2,265 123 2.959\nbalance 625 4 11.755 magic04 19,020 10 1.843 breast 683 10 1.857 cod-rna 59,535 8 2.000\naustralian 690 14 1.247 acoustic 78,823 50 3.316 diabetes 768 8 1.865 poker 1025,010 11 10.000\nversus other online learning algorithms on all the 16 benchmark datasets.\nFrom the results in Figure 1, it can be observed that the empirical computational complexity of AdaOAM is in general comparable to the other online learning algorithms, while being more efficient than OAMseq and OAMgra on some of the datasets, such as, glass, heart, etc. This indicates that the proposed algorithm is scalable and efficient, making it more attractive to large-scale real-world applications.\nNext we move from the online setting to a batch learning mode. In particular, Table 3 summarizes the average AUC performance of the algorithms under comparison over the 16 datasets in a batch setting. Note that here •/◦ is used to indicate that AdaOAM is significantly better/worse than the corresponding method.\nFrom Table 3, the following observations have been observed. Firstly, the win/tie/loss counts show that the AdaOAM is superior to batch Uni-Log and batch Uni-Squ in many cases. Since batch Uni-Log and batch Uni-Squ operate on optimizing univariate loss functions, the results demonstrates the significance of adopting pair-wise loss function for AUC maximization. Secondly, the performance of AdaOAM is competitive to CAPO, but underperformed SVM-perf, which is expected since AdaOAM trades efficacy with efficiency.\nTo analyze the efficiency of the proposed AdaOAM algorithm, we summarize the running time (in milliseconds) of AdaOAM and the other batch learning algorithms on all the benchmark datasets in Figure 2. Since the core steps of the SVM-perf and CAPO are implemented based on the specific toolboxes 6 7 developed by the respective authors in C++, we have implemented the core steps of\n6. http://www.cs.cornell.edu/people/tj/svm light/svm perf.html 7. http://lamda.nju.edu.cn/code CAPO.ashx\nthe AdaOAM, batch Uni-Log, and batch Uni-Squ in C++ in our work to obtain the time cost comparisons reported here.\nSeveral core observations can be drawn from Figure 2. First of all, the results on time costs highlights the higher efficiency of the AdaOAM against the other batch learning algorithms in general. Second, the empirical computational time cost of AdaOAM is noted to be significantly lower than the batch Uni-Log and batch Uni-Squ, which is attributed to the difference between an online setting and a batch setting. Third, CAPO is noted to be less efficient than SVM-perf in most cases. This is because CAPO being an ensemble learning method for AUC optimization is expected to incur higher training time. Besides, SVM-perf and CAPO need more time costs than that of the AdaOAM especially on large datasets although both of them are designed to be fast algorithms for performance measure optimization tasks. One exception from the results where the time cost of AdaOAM on the “a2a” dataset is observed to be higher than that of the CAPO method. The reason is that “a2a” dataset being a highly sparse dataset is a clear advantage for the CAPO since it operates under the framework of Core Vector Machine method [35], which is a very fast batch algorithm for training SVM model involving sparse data. On the other hand, AdaOAM is not optimally designed to deal with sparse dataset and since it need to compute the covariance matrices when updating the model, and at the same time C++ programming is not appropriate and efficient to deal with matrix computations."
    }, {
      "heading" : "5.4 Evaluation of Online Performance",
      "text" : "Next we study the online performance of AdaOAM versus the other online learning algorithms and highlight 6 datasets for illustration. Specifically, Figure 3(a)-(h) report the average AUC performance (across 20 independent runs) of the online model on the testing datasets. From the results, AdaOAM is once again\n9\nFig. 1. Running time (in milliseconds) of AdaOAM and the other online learning algorithms on all 16 benchmark datasets. Note that the y -axis is presented in log-scale.\n10\nshown to significantly outperform all the other three counterparts in the online learning process, which is consistent to our theoretical analysis that AdaOAM can more effectively exploit second order information to achieve improved regret bounds and robust performance."
    }, {
      "heading" : "5.5 Evaluation of Parameter Sensitivity",
      "text" : "In this subsection, we proceed to examine the parameter sensitivity of the AdaOAM algorithm. In our study, we experimented the AdaOAM with a set of different learning rates that lies in the wide range of η ∈ 2[−8:4]. The average test AUC results of AdaOAM across the wide range of learning rates after a single pass through the training data of the respective benchmark datasets are then summarized in Figure 4(a)-(h). Due to the space constraints, the results of 8 datasets are reported here for illustrations. Since the AdaOAM algorithm provides a per-feature adaptive learning rate at each iteration, it is less sensitive to the learning rate η than the standard SGD.\nIn [7], the authors claimed that OPAUC was insensitive to the parameter settings. From Figure 4, it can be observed that AdaOAM is clearly more robust or insensitive to the learning rate than the OPAUC. The updating strategy by OPAUC is based on simple SGD, which usually requires quite some efforts of tuning the learning rate parameter sufficiently. On the other hand, the adaptive gradient strategy of AdaOAM is theoretically sound for learning rate adaptation since it takes full advantage of the historical gradient information available in the learning process. As such, AdaOAM is less sensitive to the parameter settings. Moreover, AdaOAM exhibits a natural phenomena of decreasing learning rate with increasing iterations."
    }, {
      "heading" : "5.6 Evaluation of SAdaOAM on High-dimensional Sparse Datasets",
      "text" : "In this subsection, we move on to evaluate the empirical performance of various online AUC maximization algorithms on the publicly available high-dimensional sparse datasets as summarized in Table 4. The pcmac dataset is downloaded from SVMLin 8. The farm ads dataset is generated based on the text ads found on twelve websites that deal with various farm animal related topics [36] and downloaded from UCI Machine Learning Repository. The Reuters dataset is the ModApte version 9 upon removing documents with multiple category labels, and contains 8293 documents in 65 categories. The sector, rcv1m, and news20 datasets are taken from the LIBSVM dataset website. Note that the original sector, Reuters, rcv1m, and news20 are multi-class datasets; in our experiments, we randomly group the multiple classes into two meta-class in which each contains the same number of classes.\nIn consistent to the previous experimental settings, we conduct 5 fold cross validation on the training sets to identify the most appropriate learning rate η ∈ 2[−8:4] and sparse regularization parameter θ ∈ 10[−8:−1]. We fix the parameter λ as 10−6 since its effect on the learning performance is negligible, especially if it is sufficiently small. The performance of all the algorithms are evaluated across 4 independent trials of 5 fold cross validation, and then the reported AUC values are the average of the 20 runs. To showcase the benefit of the SAdaOAM algorithm on highdimensional sparse datasets, the original AdaOAM is also included for comparison, in addition to the state-of-the-art online learning algorithms considered in the earlier sections. The experimental results of SAdaOAM and other online learning algorithms in terms of AUC performance on all the testing data are then reported in Table 5.\nFrom the results in Table 5, the following observations can be drawn. Firstly, all the pairwise loss function optimization algorithms have been observed to outperform the univariate loss function optimization algorithms i.e., Uni-Log and Uni-Exp, which stresses the benefits and high efficacy of pairwise loss function optimization for AUC maximization task. In addition, it is evident from the results that our proposed algorithm SAdaOAM is superior to the non-adaptive or non-sparse methods in most cases, which indicates our proposed algorithm with sparsity is potentially more effective than existing online AUC maximization algorithms that do not exploit the sparsity in the data.\nTo bring deeper insights on the mechanisms of the proposed algorithm, we take a further to analyze the sparsity level of the final learned model generated by SAdaOAM and the other online learning algorithms. The sparsity of the learned model plays a significant role in large-scale machine learning tasks in terms of both storage cost and efficiency. A sparse learned model not only speeds up the training process, but also reduces the storage cost requirements of large-scale systems. Here, we measure the sparsity level of a learned model based on the ratio of zero elements in the model and the results of the corresponding online algorithms are summarized in Table 6.\nSeveral observations can be drawn from Table 6. To begin with, we found that all the algorithms failed to produce high sparsity level solutions for online AUC maximization task except the proposed SAdaOAM algorithm. In contrast, the SAdaOAM approach gives consideration to both performance and sparsity level for all the cases. In particular, it is found that the higher the feature dimension is, the larger the sparsity is achieved by the proposed algorithm. To sum up, the SAdaOAM algorithm is an ideal method of online AUC optimization for those highdimensional sparse data."
    }, {
      "heading" : "5.7 Evaluation of SAdaOAM on Sparsity-AUC Tradeoffs",
      "text" : "In this set of experiments, we study the tradeoffs between the sparsity level and the AUC performance for the SAdaOAM algorithm. To achieve this, we set the regularization parameter for `1 with the range θ ∈ 10[−8:0] for the SAdaOAM method. We apply the same experimental setting executed above and record the average test AUC performance versus the sparsity level designated by the proportion of non-zeros in the final weight solution after a single pass through the training data. These experimental settings make the final model solutions ranging from an almost dense weight to a nearly all-zero one. We randomly choose three datasets for this set of experiments and show the results in Figure 5.\n11\n12\nFrom this figure, we can observe that there are indeed tradeoffs between the level of sparsity and the AUC performance. With high regularization parameter θ, the SAdaOAM shows poor performance as expected since the weight vector is overly sparse and exhibits poor generalization. However, when the regularization parameter decreases, the learned weight becomes less sparse and eventually exceed the OPAUC’s performance. More importantly, when the sparsity is small enough, the AUC performance of the SAdaOAM algorithm tend to become saturated for some datasets, such as farm ads, where further decreasing the sparsity of the model has very limited improvement on the AUC value. This implies that SAdaOAM can effectively learn a sparse model with small fraction of informative features, which can help remove those redundant features and reduce the testing time complexity."
    }, {
      "heading" : "5.8 Application to Real World Online Anomaly Detection Task",
      "text" : "Online AUC maximization can be potentially applied to a wide range of applications. In this subsection, we showcase an application of the proposed algorithm namely, AdaOAM, for solving online anomaly detection tasks. In particular, we begin with an introduction of the applications followed by a presentation of the empirical results. To be precise, consider the following four domains: Webspam: We apply the AdaOAM to detect malicious web pages using the “webspam-u” dataset with unigram format from the subset used in the Pascal Large Scale Learning Challenge [37]; Sensor Faults: We apply the AdaOAM to identify sensor faults in buildings with the “smartBuilding” dataset [38], where the sensors monitor the concentration of the contaminant of interest (such as CO2) in different zones in a building; Malware App: We apply the AdaOAM to detect mobile malware app with a “malware” app permission dataset, which is built from the Android Malware Genome Project 10 [39]. In our experiment, we adopt the dataset\n10. http://www.malgenomeproject.org/\npreprocessed by [40] after data cleansing and duplication removal; Bioinformatics: We apply the AdaOAM to solve a bioinformatics problem with the “protein-h” dataset from the prediction task of the KDD Cup 2004 [41]. The aim is to predict which proteins are homologous to a native (query) sentence. Non-homologous sequences are labeled as anomalies.\nTable 7 summarizes the details of these datasets related to the above four different domains.\nTable 8 and Figure 6 have shown the performance and efficiency of the proposed algorithm for online anomaly detection task respectively. From Table 8, we observe that the proposed AdaOAM algorithm also outperforms other methods. Although OAMseq and OAMgra obtain comparably good results, their computational costs are very high, which are impractical for realworld learning tasks. Again, the AdaOAM proves its efficiency for real-world applications."
    }, {
      "heading" : "6 CONCLUSION",
      "text" : "In this paper, we have proposed two adaptive subgradient online AUC maximization approaches for handling both regular and high-dimensional sparse data, which considered the historical component-wise gradient information for more efficient and adaptive learning. Our proposed algorithms employ the second order information to speed up online AUC maximization, and are less sensitive to parameter setting than that of the simple SGD strategy. Theoretically, we have derived and analyzed the regret bound of the adaptive online AUC maximization approaches and\n13\nTABLE 8 AUC performance evaluation (mean±std.) of AdaOAM versus the other online algorithms on anomaly detection datasets. •/◦ indicates that AdaOAM is significantly better/worse than the corresponding method (pairwise t-tests at 95% significance level).\ndatasets AdaOAM OPAUC OAMseq OAMgra online Uni-Log online Uni-Exp webspam-u .964 ± .005 .959 ± .006• .963 ± .005 .962 ± .005 .923 ± .005• .920 ± .006•\nsmartBuilding .838 ± .044 .629 ± .070• .629 ± .069• .631 ± .069• .749 ± .020• .758 ± .022• malware .967 ± .008 .919 ± .008• .959 ± .009• .953 ± .009• .695 ± .009• .765 ± .009• protein-h .972 ± .004 .958 ± .005• .970 ± .007 .968 ± .007• .890 ± .009• .915 ± .009•\nwin/tie/loss 4/0/0 2/2/0 3/1/0 4/0/0 4/0/0\nFig. 6. Comparison of the running time (in seconds) of AdaOAM and other online learning algorithms on anomaly detection datasets. Notice that the y -axis is in log-scale.\nverified that the proposed algorithms would achieve lower regret bound when handling both regular and high-dimensional sparse data. Empirically, we have also conducted extensive experimental studies with comparisons to a number of competing online AUC optimization algorithms on diverse types of data including many benchmark datasets, high-dimensional sparse datasets, and several real-world anomaly detection tasks. Overall, the obtained empirical results observations agree with our theoretical analyses and the results also verified the effectiveness and efficiency of the proposed AdaOAM and SAdaOAM algorithms."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "This research is partially supported by the Multi-plAtform Game Innovation Centre (MAGIC) in Nanyang Technological University. MAGIC is funded by the Interactive Digital Media Programme Office (IDMPO) hosted by the Media Development Authority of Singapore. IDMPO was established in 2006 under the mandate of the National Research Foundation to deepen Singapore’s research capabilities in interactive digital media (IDM), fuel innovation and shape the future of media. In addition, the last author is grateful to the support provided by the Singapore MOE tier 1 research grant (C220/MSS14C003)."
    } ],
    "references" : [ {
      "title" : "An introduction to roc analysis",
      "author" : [ "T. Fawcett" ],
      "venue" : "Pattern Recogn. Lett., vol. 27, pp. 861–874, 2006.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Auc optimization vs. error rate minimization",
      "author" : [ "C. Cortes", "M. Mohri" ],
      "venue" : "NIPS, 2003.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Efficient auc optimization for classification",
      "author" : [ "T. Calders", "S. Jaroszewicz" ],
      "venue" : "PKDD, 2007, pp. 42–53.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "A support vector method for multivariate performance measures",
      "author" : [ "T. Joachims" ],
      "venue" : "ICML, 2005, pp. 377–384.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Margin-based ranking and an equivalence between adaboost and rankboost",
      "author" : [ "C. Rudin", "R.E. Schapire" ],
      "venue" : "Journal of Machine Learning Research, vol. 10, pp. 2193–2232, 2009.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Online AUC maximization",
      "author" : [ "P. Zhao", "S.C.H. Hoi", "R. Jin", "T. Yang" ],
      "venue" : "Proceedings of the 28th International Conference on Machine Learning, ICML 2011, Bellevue, Washington, USA, June 28 - July 2, 2011, 2011, pp. 233–240.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "One-pass auc optimization",
      "author" : [ "W. Gao", "R. Jin", "S. Zhu", "Z.-H. Zhou" ],
      "venue" : "ICML (3), 2013, pp. 906–914.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "On the generalization ability of online learning algorithms for pairwise loss functions",
      "author" : [ "P. Kar", "B.K. Sriperumbudur", "P. Jain", "H. Karnick" ],
      "venue" : "Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, 2013, pp. 441–449.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "On the consistency of auc optimization",
      "author" : [ "W. Gao", "Z.-H. Zhou" ],
      "venue" : "arXiv preprint arXiv:1208.0645, 2012.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "J.C. Duchi", "E. Hazan", "Y. Singer" ],
      "venue" : "Journal of Machine Learning Research, vol. 12, pp. 2121–2159, 2011.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Prediction, learning, and games",
      "author" : [ "N. Cesa-Bianchi", "G. Lugosi" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2006
    }, {
      "title" : "Online passive-aggressive algorithms",
      "author" : [ "K. Crammer", "O. Dekel", "J. Keshet", "S. Shalev-Shwartz", "Y. Singer" ],
      "venue" : "Journal of Machine Learning Research, vol. 7, pp. 551–585, 2006.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Double updating online learning",
      "author" : [ "P. Zhao", "S.C.H. Hoi", "R. Jin" ],
      "venue" : "Journal of Machine Learning Research, vol. 12, pp. 1587–1615, 2011.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Online multiple kernel classification",
      "author" : [ "S.C.H. Hoi", "R. Jin", "P. Zhao", "T. Yang" ],
      "venue" : "Machine Learning, vol. 90, no. 2, pp. 289–316, 2013.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Online transfer learning",
      "author" : [ "P. Zhao", "S.C.H. Hoi", "J. Wang", "B. Li" ],
      "venue" : "Artif. Intell., vol. 216, pp. 76–102, 2014.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "The perceptron: a probabilistic model for information storage and organization in the brain.",
      "author" : [ "F. Rosenblatt" ],
      "venue" : "Psychological review,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1958
    }, {
      "title" : "Confidence-weighted linear classification",
      "author" : [ "M. Dredze", "K. Crammer", "F. Pereira" ],
      "venue" : "Machine Learning, Proceedings of the Twenty-Fifth International Conference (ICML 2008), Helsinki, Finland, June 5-9, 2008, 2008, pp. 264–271.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Adaptive regularization of weight vectors",
      "author" : [ "K. Crammer", "A. Kulesza", "M. Dredze" ],
      "venue" : "Advances in Neural Information Processing Systems 22: 23rd Annual Conference on Neural Information Processing Systems 2009. Proceedings of a meeting held 7-10 December 2009, Vancouver, British Columbia, Canada., 2009, pp. 414–422.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "New adaptive algorithms for online classification",
      "author" : [ "F. Orabona", "K. Crammer" ],
      "venue" : "Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems 2010. Proceedings of a meeting held 6-9 December 2010, Vancouver, British Columbia, Canada., 2010, pp. 1840–1848.  14",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Exact soft confidence-weighted learning",
      "author" : [ "J. Wang", "P. Zhao", "S.C.H. Hoi" ],
      "venue" : "Proceedings of the 29th International Conference on Machine Learning, ICML 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012, 2012.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Cost-sensitive online classification",
      "author" : [ "——" ],
      "venue" : "IEEE Trans. Knowl. Data Eng., vol. 26, no. 10, pp. 2425–2438, 2014.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Cost-sensitive online active learning with application to malicious URL detection",
      "author" : [ "P. Zhao", "S.C.H. Hoi" ],
      "venue" : "The 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 2013, Chicago, IL, USA, August 11-14, 2013, 2013, pp. 919–927.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Cost-sensitive double updating online learning and its application to online anomaly detection",
      "author" : [ "S.C.H. Hoi", "P. Zhao" ],
      "venue" : "Proceedings of the 13th SIAM International Conference on Data Mining, May 2-4, 2013. Austin, Texas, USA., 2013, pp. 207–215.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Sparse online learning via truncated gradient",
      "author" : [ "J. Langford", "L. Li", "T. Zhang" ],
      "venue" : "Journal of Machine Learning Research, vol. 10, pp. 777–801, 2009.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Efficient online and batch learning using forward backward splitting",
      "author" : [ "J.C. Duchi", "Y. Singer" ],
      "venue" : "Journal of Machine Learning Research, vol. 10, pp. 2899–2934, 2009.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Dual averaging methods for regularized stochastic learning and online optimization",
      "author" : [ "L. Xiao" ],
      "venue" : "Journal of Machine Learning Research, vol. 11, pp. 2543–2596, 2010.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Stochastic methods for l1-regularized loss minimization",
      "author" : [ "S. Shalev-Shwartz", "A. Tewari" ],
      "venue" : "Journal of Machine Learning Research, vol. 12, pp. 1865–1892, 2011.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 1865
    }, {
      "title" : "Primal-dual subgradient methods for convex problems",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Math. Program., vol. 120, no. 1, pp. 221–259, 2009.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Online convex programming and generalized infinitesimal gradient ascent",
      "author" : [ "M. Zinkevich" ],
      "venue" : "Machine Learning, Proceedings of the Twentieth International Conference (ICML 2003), August 21-24, 2003, Washington, DC, USA, 2003, pp. 928–936.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Spam email classification using an adaptive ontology",
      "author" : [ "S. Youn", "D. McLeod" ],
      "venue" : "JSW, vol. 2, no. 3, pp. 43–55, 2007.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "On accelerated proximal gradient methods for convex-concave optimization",
      "author" : [ "P. Tseng" ],
      "venue" : "Technical report, Department of Mathematics, University of Washington, 2008.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "LIBOL: a library for online learning algorithms",
      "author" : [ "S.C.H. Hoi", "J. Wang", "P. Zhao" ],
      "venue" : "Journal of Machine Learning Research, vol. 15, no. 1, pp. 495–499, 2014.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Bipartite ranking through minimization of univariate loss",
      "author" : [ "W. Kotlowski", "K. Dembczynski", "E. Hüllermeier" ],
      "venue" : "Proceedings of the 28th International Conference on Machine Learning, ICML 2011, Bellevue, Washington, USA, June 28 - July 2, 2011, 2011, pp. 1113–1120.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Efficient optimization of performance measures by classifier adaptation",
      "author" : [ "N. Li", "I.W. Tsang", "Z. Zhou" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell., vol. 35, no. 6, pp. 1370–1382, 2013.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Core vector machines: Fast SVM training on very large data sets",
      "author" : [ "I.W. Tsang", "J.T. Kwok", "P. Cheung" ],
      "venue" : "Journal of Machine Learning Research, vol. 6, pp. 363–392, 2005.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Active learning using on-line algorithms",
      "author" : [ "C. Mesterharm", "M.J. Pazzani" ],
      "venue" : "In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD, 2011.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Evolutionary study of web spam: Webb spam corpus 2011 versus webb spam corpus 2006",
      "author" : [ "D. Wang", "D. Irani", "C. Pu" ],
      "venue" : "8th International Conference on Collaborative Computing: Networking, Applications and Worksharing, CollaborateCom 2012, Pittsburgh, PA, USA, October 14- 17, 2012, 2012, pp. 40–49.",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "SNAP: fault tolerant event location estimation in sensor networks using binary data",
      "author" : [ "M.P. Michaelides", "C.G. Panayiotou" ],
      "venue" : "IEEE Trans. Computers, vol. 58, no. 9, pp. 1185–1197, 2009.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Dissecting android malware: Characterization and evolution",
      "author" : [ "Y. Zhou", "X. Jiang" ],
      "venue" : "IEEE Symposium on Security and Privacy, 2012, pp. 95–109.",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Using probabilistic generative models for ranking risks of android apps",
      "author" : [ "H. Peng", "C.S. Gates", "B.P. Sarma", "N. Li", "Y. Qi", "R. Potharaju", "C. Nita- Rotaru", "I. Molloy" ],
      "venue" : "ACM Conference on Computer and Communications Security, 2012, pp. 241–252.",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "AUC (Area Under ROC curve) [1] is an important measure for characterizing machine learning performances in many realworld applications, such as ranking, and anomaly detection tasks, especially when misclassification costs are unknown.",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 1,
      "context" : "Many efforts have been devoted recently to developing efficient AUC optimization algorithms for both batch and online learning tasks [2], [3], [4], [5], [6], [7].",
      "startOffset" : 133,
      "endOffset" : 136
    }, {
      "referenceID" : 2,
      "context" : "Many efforts have been devoted recently to developing efficient AUC optimization algorithms for both batch and online learning tasks [2], [3], [4], [5], [6], [7].",
      "startOffset" : 138,
      "endOffset" : 141
    }, {
      "referenceID" : 3,
      "context" : "Many efforts have been devoted recently to developing efficient AUC optimization algorithms for both batch and online learning tasks [2], [3], [4], [5], [6], [7].",
      "startOffset" : 143,
      "endOffset" : 146
    }, {
      "referenceID" : 4,
      "context" : "Many efforts have been devoted recently to developing efficient AUC optimization algorithms for both batch and online learning tasks [2], [3], [4], [5], [6], [7].",
      "startOffset" : 148,
      "endOffset" : 151
    }, {
      "referenceID" : 5,
      "context" : "Many efforts have been devoted recently to developing efficient AUC optimization algorithms for both batch and online learning tasks [2], [3], [4], [5], [6], [7].",
      "startOffset" : 153,
      "endOffset" : 156
    }, {
      "referenceID" : 6,
      "context" : "Many efforts have been devoted recently to developing efficient AUC optimization algorithms for both batch and online learning tasks [2], [3], [4], [5], [6], [7].",
      "startOffset" : 158,
      "endOffset" : 161
    }, {
      "referenceID" : 5,
      "context" : "The first framework is based on the idea of buffer sampling [6], [8], which stores some randomly sampled historical examples in a buffer to represent the observed data for calculating the pairwise loss functions.",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 7,
      "context" : "The first framework is based on the idea of buffer sampling [6], [8], which stores some randomly sampled historical examples in a buffer to represent the observed data for calculating the pairwise loss functions.",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 6,
      "context" : "The other framework focuses on one-pass AUC optimization [7], where the algorithm",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 8,
      "context" : "The benefit of one-pass AUC optimization lies in the use of squared loss to represent the AUC loss function while providing proofs on its consistency with the AUC measure [9].",
      "startOffset" : 171,
      "endOffset" : 174
    }, {
      "referenceID" : 9,
      "context" : "Although these algorithms have been shown to be capable of achieving fairly good AUC performances, they share a common trait of employing the online gradient descent technique, which fail to take advantage of the geometrical property of the data observed from the online learning process, while recent studies have shown the importance of exploiting this information for online optimization [10].",
      "startOffset" : 391,
      "endOffset" : 395
    }, {
      "referenceID" : 9,
      "context" : "To achieve this purpose, we propose the AdaOAM algorithm by adopting the adaptive gradient updating framework proposed by [10] to control the learning rates for different features.",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 10,
      "context" : "Online learning has been extensively studied in the machine learning communities [11], [12], [13], [14], [15], mainly due to its high efficiency and scalability to large-scale learning tasks.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 11,
      "context" : "Online learning has been extensively studied in the machine learning communities [11], [12], [13], [14], [15], mainly due to its high efficiency and scalability to large-scale learning tasks.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 12,
      "context" : "Online learning has been extensively studied in the machine learning communities [11], [12], [13], [14], [15], mainly due to its high efficiency and scalability to large-scale learning tasks.",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 13,
      "context" : "Online learning has been extensively studied in the machine learning communities [11], [12], [13], [14], [15], mainly due to its high efficiency and scalability to large-scale learning tasks.",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 14,
      "context" : "Online learning has been extensively studied in the machine learning communities [11], [12], [13], [14], [15], mainly due to its high efficiency and scalability to large-scale learning tasks.",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 15,
      "context" : "A number of first-order algorithms have been proposed including the well-known Perceptron algorithm [16] and the Passive-Aggressive (PA) algorithm [12].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 11,
      "context" : "A number of first-order algorithms have been proposed including the well-known Perceptron algorithm [16] and the Passive-Aggressive (PA) algorithm [12].",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 16,
      "context" : "In order to address this issue, recent years have witnessed some second-order online learning algorithms [17], [18], [19], [20], which apply parameter confidence information to improve online learning performance.",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 17,
      "context" : "In order to address this issue, recent years have witnessed some second-order online learning algorithms [17], [18], [19], [20], which apply parameter confidence information to improve online learning performance.",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 18,
      "context" : "In order to address this issue, recent years have witnessed some second-order online learning algorithms [17], [18], [19], [20], which apply parameter confidence information to improve online learning performance.",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 19,
      "context" : "In order to address this issue, recent years have witnessed some second-order online learning algorithms [17], [18], [19], [20], which apply parameter confidence information to improve online learning performance.",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 20,
      "context" : "Further, in order to solve the costsensitive classification tasks on-the-fly, online learning researchers have also proposed a few novel online learning algorithms to directly optimize some more meaningful cost-sensitive metrics [21], [22], [23].",
      "startOffset" : 229,
      "endOffset" : 233
    }, {
      "referenceID" : 21,
      "context" : "Further, in order to solve the costsensitive classification tasks on-the-fly, online learning researchers have also proposed a few novel online learning algorithms to directly optimize some more meaningful cost-sensitive metrics [21], [22], [23].",
      "startOffset" : 235,
      "endOffset" : 239
    }, {
      "referenceID" : 22,
      "context" : "Further, in order to solve the costsensitive classification tasks on-the-fly, online learning researchers have also proposed a few novel online learning algorithms to directly optimize some more meaningful cost-sensitive metrics [21], [22], [23].",
      "startOffset" : 241,
      "endOffset" : 245
    }, {
      "referenceID" : 1,
      "context" : "Recently, many algorithms have been developed to optimize AUC directly [2], [3], [4], [6], [7].",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 2,
      "context" : "Recently, many algorithms have been developed to optimize AUC directly [2], [3], [4], [6], [7].",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 3,
      "context" : "Recently, many algorithms have been developed to optimize AUC directly [2], [3], [4], [6], [7].",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 5,
      "context" : "Recently, many algorithms have been developed to optimize AUC directly [2], [3], [4], [6], [7].",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 6,
      "context" : "Recently, many algorithms have been developed to optimize AUC directly [2], [3], [4], [6], [7].",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 3,
      "context" : "In [4], the author firstly presented a general framework for optimizing multivariate nonlinear performance measures such as the AUC, F1, etc.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 5,
      "context" : "The first framework is based on the idea of buffer sampling [6], [8], which employed a fixed-size buffer to represent the observed data for calculating the pairwise loss functions.",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 7,
      "context" : "The first framework is based on the idea of buffer sampling [6], [8], which employed a fixed-size buffer to represent the observed data for calculating the pairwise loss functions.",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 5,
      "context" : "A representative study is available in [6], which leveraged the reservoir sampling technique to represent the observed data instances by a fixed-size buffer where notable theoretical and empirical results have been reported.",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 7,
      "context" : "Then, [8] studied the improved generalization capability of online learning algorithms for pairwise loss functions with the framework of buffer sampling.",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 6,
      "context" : "The other framework which takes a different perspective was presented by [7].",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 23,
      "context" : "Recent years have witnessed extensive research studies on sparse online learning [24], [25], [26], [27], which aim to learn sparse classifiers by limiting the number of active features.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 24,
      "context" : "Recent years have witnessed extensive research studies on sparse online learning [24], [25], [26], [27], which aim to learn sparse classifiers by limiting the number of active features.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 25,
      "context" : "Recent years have witnessed extensive research studies on sparse online learning [24], [25], [26], [27], which aim to learn sparse classifiers by limiting the number of active features.",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 26,
      "context" : "Recent years have witnessed extensive research studies on sparse online learning [24], [25], [26], [27], which aim to learn sparse classifiers by limiting the number of active features.",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 24,
      "context" : "Taking the FOBOS algorithm [25] as an example, which is based on the Forward-Backward Splitting method to solve the sparse online learning problem by alternating between two phases: (i) an unconstraint stochastic subgradient descent step with respect to the loss function, and (ii) an instantaneous optimization for a tradeoff between keeping close proximity to the result of the first step and minimizing `1 regularization term.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 23,
      "context" : "Following this strategy, [24] argues that the truncation at each step is too aggressive and thus proposes the Truncated Gradient (TG) method, which alleviates the updates by truncating the coefficients at every K steps when they are lower than a predefined threshold.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 27,
      "context" : "The second category of methods are mainly motivated by the dual averaging method [28].",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 25,
      "context" : "The most popular method in this category is the Regularized Dual Averaging (RDA) [26], which solves the optimization problem by using the running average of all past subgradients of the loss functions and the whole regularization term instead of the subgradient.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 0,
      "context" : "Given this setting, let us define the AUC measurement [1] for binary classification task.",
      "startOffset" : 54,
      "endOffset" : 57
    }, {
      "referenceID" : 6,
      "context" : ", the square loss from [7] due to its consistency with AUC [9]",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 8,
      "context" : ", the square loss from [7] due to its consistency with AUC [9]",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 6,
      "context" : "Following the similar approach in [7], we modify the loss function L(w) in (1) as a sum of losses",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 28,
      "context" : "Upon obtaining gradient gt = ∇Lt(wt), a simple solution is to move the weight wt in the opposite direction of gt, while keeping ‖wt+1‖ ≤ 1/ √ λ via the projected gradient update [29]",
      "startOffset" : 178,
      "endOffset" : 182
    }, {
      "referenceID" : 9,
      "context" : ", Adaptive Gradient Updating strategy, as inspired by [10].",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 29,
      "context" : "The research work in [30] has shown that the classification performance saturates with dozens of features out of tens of thousands of features.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 30,
      "context" : "In order to optimize this objective function, we apply the composite mirror descent method [31] that is able to achieve a trade-off between the immediate adaptive gradient term gt and the regularizer φ(w).",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 9,
      "context" : "Following the framework of the composite mirror descent update in [10], the update needed to solve is",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 9,
      "context" : "From the Algorithm 2, it is observed that we perform “lazy” computation when the gradient vectors are sparse [10].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 9,
      "context" : "These two lemmas are actually the Lemma 4 and Corollary 1 in the paper [10].",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 9,
      "context" : "derivation results of [10] and attain the following regret bound",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 9,
      "context" : "From [10], we have",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 31,
      "context" : "The main framework of the experiments is based on the LIBOL, an open-source library for online learning algorithms 1 [32].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 32,
      "context" : "• Online Uni-Exp: An online learning algorithm which optimizes the (weighted) univariate exponential loss [33]; • Online Uni-Log: An online learning algorithm which optimizes the (weighted) univariate logistic loss [33]; • OAMseq: The OAM algorithm with reservoir sampling and sequential updating method [6]; • OAMgra: The OAM algorithm with reservoir sampling and online gradient updating method [6]; • OPAUC: The one-pass AUC optimization algorithm with square loss function [7]; • SVM-perf: A batch algorithm which directly optimizes AUC [4]; • CAPO: A batch algorithm which trains nonlinear auxiliary classifiers first and then adapts auxiliary classifiers for specific performance measures [34]; • Batch Uni-Log: A batch algorithm which optimizes the (weighted) univariate logistic loss [33]; • Batch Uni-Squ: A batch algorithm which optimizes the (weighted) univariate square loss; • AdaOAM: The proposed adaptive gradient method for online AUC maximization.",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 32,
      "context" : "• Online Uni-Exp: An online learning algorithm which optimizes the (weighted) univariate exponential loss [33]; • Online Uni-Log: An online learning algorithm which optimizes the (weighted) univariate logistic loss [33]; • OAMseq: The OAM algorithm with reservoir sampling and sequential updating method [6]; • OAMgra: The OAM algorithm with reservoir sampling and online gradient updating method [6]; • OPAUC: The one-pass AUC optimization algorithm with square loss function [7]; • SVM-perf: A batch algorithm which directly optimizes AUC [4]; • CAPO: A batch algorithm which trains nonlinear auxiliary classifiers first and then adapts auxiliary classifiers for specific performance measures [34]; • Batch Uni-Log: A batch algorithm which optimizes the (weighted) univariate logistic loss [33]; • Batch Uni-Squ: A batch algorithm which optimizes the (weighted) univariate square loss; • AdaOAM: The proposed adaptive gradient method for online AUC maximization.",
      "startOffset" : 215,
      "endOffset" : 219
    }, {
      "referenceID" : 5,
      "context" : "• Online Uni-Exp: An online learning algorithm which optimizes the (weighted) univariate exponential loss [33]; • Online Uni-Log: An online learning algorithm which optimizes the (weighted) univariate logistic loss [33]; • OAMseq: The OAM algorithm with reservoir sampling and sequential updating method [6]; • OAMgra: The OAM algorithm with reservoir sampling and online gradient updating method [6]; • OPAUC: The one-pass AUC optimization algorithm with square loss function [7]; • SVM-perf: A batch algorithm which directly optimizes AUC [4]; • CAPO: A batch algorithm which trains nonlinear auxiliary classifiers first and then adapts auxiliary classifiers for specific performance measures [34]; • Batch Uni-Log: A batch algorithm which optimizes the (weighted) univariate logistic loss [33]; • Batch Uni-Squ: A batch algorithm which optimizes the (weighted) univariate square loss; • AdaOAM: The proposed adaptive gradient method for online AUC maximization.",
      "startOffset" : 304,
      "endOffset" : 307
    }, {
      "referenceID" : 5,
      "context" : "• Online Uni-Exp: An online learning algorithm which optimizes the (weighted) univariate exponential loss [33]; • Online Uni-Log: An online learning algorithm which optimizes the (weighted) univariate logistic loss [33]; • OAMseq: The OAM algorithm with reservoir sampling and sequential updating method [6]; • OAMgra: The OAM algorithm with reservoir sampling and online gradient updating method [6]; • OPAUC: The one-pass AUC optimization algorithm with square loss function [7]; • SVM-perf: A batch algorithm which directly optimizes AUC [4]; • CAPO: A batch algorithm which trains nonlinear auxiliary classifiers first and then adapts auxiliary classifiers for specific performance measures [34]; • Batch Uni-Log: A batch algorithm which optimizes the (weighted) univariate logistic loss [33]; • Batch Uni-Squ: A batch algorithm which optimizes the (weighted) univariate square loss; • AdaOAM: The proposed adaptive gradient method for online AUC maximization.",
      "startOffset" : 397,
      "endOffset" : 400
    }, {
      "referenceID" : 6,
      "context" : "• Online Uni-Exp: An online learning algorithm which optimizes the (weighted) univariate exponential loss [33]; • Online Uni-Log: An online learning algorithm which optimizes the (weighted) univariate logistic loss [33]; • OAMseq: The OAM algorithm with reservoir sampling and sequential updating method [6]; • OAMgra: The OAM algorithm with reservoir sampling and online gradient updating method [6]; • OPAUC: The one-pass AUC optimization algorithm with square loss function [7]; • SVM-perf: A batch algorithm which directly optimizes AUC [4]; • CAPO: A batch algorithm which trains nonlinear auxiliary classifiers first and then adapts auxiliary classifiers for specific performance measures [34]; • Batch Uni-Log: A batch algorithm which optimizes the (weighted) univariate logistic loss [33]; • Batch Uni-Squ: A batch algorithm which optimizes the (weighted) univariate square loss; • AdaOAM: The proposed adaptive gradient method for online AUC maximization.",
      "startOffset" : 477,
      "endOffset" : 480
    }, {
      "referenceID" : 3,
      "context" : "• Online Uni-Exp: An online learning algorithm which optimizes the (weighted) univariate exponential loss [33]; • Online Uni-Log: An online learning algorithm which optimizes the (weighted) univariate logistic loss [33]; • OAMseq: The OAM algorithm with reservoir sampling and sequential updating method [6]; • OAMgra: The OAM algorithm with reservoir sampling and online gradient updating method [6]; • OPAUC: The one-pass AUC optimization algorithm with square loss function [7]; • SVM-perf: A batch algorithm which directly optimizes AUC [4]; • CAPO: A batch algorithm which trains nonlinear auxiliary classifiers first and then adapts auxiliary classifiers for specific performance measures [34]; • Batch Uni-Log: A batch algorithm which optimizes the (weighted) univariate logistic loss [33]; • Batch Uni-Squ: A batch algorithm which optimizes the (weighted) univariate square loss; • AdaOAM: The proposed adaptive gradient method for online AUC maximization.",
      "startOffset" : 541,
      "endOffset" : 544
    }, {
      "referenceID" : 33,
      "context" : "• Online Uni-Exp: An online learning algorithm which optimizes the (weighted) univariate exponential loss [33]; • Online Uni-Log: An online learning algorithm which optimizes the (weighted) univariate logistic loss [33]; • OAMseq: The OAM algorithm with reservoir sampling and sequential updating method [6]; • OAMgra: The OAM algorithm with reservoir sampling and online gradient updating method [6]; • OPAUC: The one-pass AUC optimization algorithm with square loss function [7]; • SVM-perf: A batch algorithm which directly optimizes AUC [4]; • CAPO: A batch algorithm which trains nonlinear auxiliary classifiers first and then adapts auxiliary classifiers for specific performance measures [34]; • Batch Uni-Log: A batch algorithm which optimizes the (weighted) univariate logistic loss [33]; • Batch Uni-Squ: A batch algorithm which optimizes the (weighted) univariate square loss; • AdaOAM: The proposed adaptive gradient method for online AUC maximization.",
      "startOffset" : 695,
      "endOffset" : 699
    }, {
      "referenceID" : 32,
      "context" : "• Online Uni-Exp: An online learning algorithm which optimizes the (weighted) univariate exponential loss [33]; • Online Uni-Log: An online learning algorithm which optimizes the (weighted) univariate logistic loss [33]; • OAMseq: The OAM algorithm with reservoir sampling and sequential updating method [6]; • OAMgra: The OAM algorithm with reservoir sampling and online gradient updating method [6]; • OPAUC: The one-pass AUC optimization algorithm with square loss function [7]; • SVM-perf: A batch algorithm which directly optimizes AUC [4]; • CAPO: A batch algorithm which trains nonlinear auxiliary classifiers first and then adapts auxiliary classifiers for specific performance measures [34]; • Batch Uni-Log: A batch algorithm which optimizes the (weighted) univariate logistic loss [33]; • Batch Uni-Squ: A batch algorithm which optimizes the (weighted) univariate square loss; • AdaOAM: The proposed adaptive gradient method for online AUC maximization.",
      "startOffset" : 792,
      "endOffset" : 796
    }, {
      "referenceID" : 5,
      "context" : "To examine the performance of the proposed AdaOAM in comparison to the existing state-of-the-art methods, we conduct extensive experiments on sixteen benchmark datasets by maintaining consistency to the previous studies on online AUC maximization [6], [7].",
      "startOffset" : 247,
      "endOffset" : 250
    }, {
      "referenceID" : 6,
      "context" : "To examine the performance of the proposed AdaOAM in comparison to the existing state-of-the-art methods, we conduct extensive experiments on sixteen benchmark datasets by maintaining consistency to the previous studies on online AUC maximization [6], [7].",
      "startOffset" : 252,
      "endOffset" : 255
    }, {
      "referenceID" : 5,
      "context" : "For OAMgra and OAMseq , the buffer size is fixed at 100 as suggested in [6].",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 34,
      "context" : "The reason is that “a2a” dataset being a highly sparse dataset is a clear advantage for the CAPO since it operates under the framework of Core Vector Machine method [35], which is a very fast batch algorithm for training SVM model involving sparse data.",
      "startOffset" : 165,
      "endOffset" : 169
    }, {
      "referenceID" : 6,
      "context" : "In [7], the authors claimed that OPAUC was insensitive to the parameter settings.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 35,
      "context" : "The farm ads dataset is generated based on the text ads found on twelve websites that deal with various farm animal related topics [36] and downloaded from UCI Machine Learning Repository.",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 36,
      "context" : "To be precise, consider the following four domains: Webspam: We apply the AdaOAM to detect malicious web pages using the “webspam-u” dataset with unigram format from the subset used in the Pascal Large Scale Learning Challenge [37]; Sensor Faults: We apply the AdaOAM to identify sensor faults in buildings with the “smartBuilding” dataset [38], where the sensors monitor the concentration of the contaminant of interest (such as CO2) in different zones in a building; Malware App: We apply the AdaOAM to detect mobile malware app with a “malware” app permission dataset, which is built from the Android Malware Genome Project 10 [39].",
      "startOffset" : 227,
      "endOffset" : 231
    }, {
      "referenceID" : 37,
      "context" : "To be precise, consider the following four domains: Webspam: We apply the AdaOAM to detect malicious web pages using the “webspam-u” dataset with unigram format from the subset used in the Pascal Large Scale Learning Challenge [37]; Sensor Faults: We apply the AdaOAM to identify sensor faults in buildings with the “smartBuilding” dataset [38], where the sensors monitor the concentration of the contaminant of interest (such as CO2) in different zones in a building; Malware App: We apply the AdaOAM to detect mobile malware app with a “malware” app permission dataset, which is built from the Android Malware Genome Project 10 [39].",
      "startOffset" : 340,
      "endOffset" : 344
    }, {
      "referenceID" : 38,
      "context" : "To be precise, consider the following four domains: Webspam: We apply the AdaOAM to detect malicious web pages using the “webspam-u” dataset with unigram format from the subset used in the Pascal Large Scale Learning Challenge [37]; Sensor Faults: We apply the AdaOAM to identify sensor faults in buildings with the “smartBuilding” dataset [38], where the sensors monitor the concentration of the contaminant of interest (such as CO2) in different zones in a building; Malware App: We apply the AdaOAM to detect mobile malware app with a “malware” app permission dataset, which is built from the Android Malware Genome Project 10 [39].",
      "startOffset" : 630,
      "endOffset" : 634
    }, {
      "referenceID" : 39,
      "context" : "org/ preprocessed by [40] after data cleansing and duplication removal; Bioinformatics: We apply the AdaOAM to solve a bioinformatics problem with the “protein-h” dataset from the prediction task of the KDD Cup 2004 [41].",
      "startOffset" : 21,
      "endOffset" : 25
    } ],
    "year" : 2016,
    "abstractText" : "Learning for maximizing AUC performance is an important research problem in Machine Learning and Artificial Intelligence. Unlike traditional batch learning methods for maximizing AUC which often suffer from poor scalability, recent years have witnessed some emerging studies that attempt to maximize AUC by single-pass online learning approaches. Despite their encouraging results reported, the existing online AUC maximization algorithms often adopt simple online gradient descent approaches that fail to exploit the geometrical knowledge of the data observed during the online learning process, and thus could suffer from relatively larger regret. To address the above limitation, in this work, we explore a novel algorithm of Adaptive Online AUC Maximization (AdaOAM) which employs an adaptive gradient method that exploits the knowledge of historical gradients to perform more informative online learning. The new adaptive updating strategy of the AdaOAM is less sensitive to the parameter settings and maintains the same time complexity as previous non-adaptive counterparts. Additionally, we extend the algorithm to handle high-dimensional sparse data (SAdaOAM) and address sparsity in the solution by performing lazy gradient updating. We analyze the theoretical bounds and evaluate their empirical performance on various types of data sets. The encouraging empirical results obtained clearly highlighted the effectiveness and efficiency of the proposed algorithms.",
    "creator" : "LaTeX with hyperref package"
  }
}